\documentclass[11pt,a4paper]{article}
\usepackage[a4paper,margin=2cm,head=15pt]{geometry}
\usepackage{mathtools,amsmath,amsthm,amsfonts,amssymb,latexsym}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}

\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{indentfirst}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{multirow}

\usepackage{breqn}
\newtheorem{Theorem}{Theorem}
\newtheorem{Proposition}[Theorem]{Theorem}
\newtheorem{Example}[Theorem]{Example}
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{corollary}[Theorem]{Corollary}
\newtheorem{Notation}[Theorem]{Notation}
\newtheorem{Tm}{Theorem}
\newtheorem{Lem}{Lemma}
\newtheorem{Cr}{Corollary}
\newtheorem{Rm}{Remark}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Algorithm}[Theorem]{Algorithm}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\newtheorem{Problem}[Theorem]{Problem}
\newtheorem{fact}[Theorem]{Fact}
\newtheorem{Assumption}[Theorem]{Assumption}
\newtheorem{Remark}[Theorem]{Remark}
\newtheorem{Remarks}[Theorem]{Remarks}
\newtheorem{Note}[Theorem]{Note}
\newtheorem{Claim}[Theorem]{Claim}
\newtheorem{ack}{Acknowledgment}  \renewcommand{\theack}{}
\newtheorem{remark}{Remark}[section]
\newtheorem{Layer}[Theorem]{Layer}
\newtheorem{Deduction}{Deduction}
\newtheorem{Definition}[Theorem]{Definition}
\newtheorem{definition}{Definition}




\setlength{\headheight}{15pt}
\pagestyle{fancy}





\newcommand{\theyear}{2020}
\newcommand{\themonth}{April}
\newcommand{\thejournal}{IJCCC}
\newcommand{\thejournallong}{INTERNATIONAL JOURNAL OF COMPUTERS COMMUNICATIONS \& CONTROL}
\newcommand{\theonlineissn}{1841-9844}
\newcommand{\theissn}{1841-9836}
\newcommand{\thevolume}{15}
\newcommand{\theissue}{2}
\newcommand{\thedoi}{doi.org/10.15837/ijccc.2020.2.3862}
\newcommand{\theartnum}{3862}



\fancypagestyle{firstpage}
{
    \fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \renewcommand{\footrulewidth}{0pt}
}

\fancyhf{}\renewcommand{\headrulewidth}{0pt}



\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\rhead{ \thepage}
\lhead{ \thedoi }
\renewcommand{\headrulewidth}{0.4pt}





\begin{document}


\thispagestyle{firstpage}
\vspace*{\dimexpr-\headheight-\headsep}\hrule\vspace{0.1cm}\hrule\vspace{0.2cm}
            \thejournallong\\
            Online ISSN \theonlineissn, ISSN-L \theissn, Volume: \thevolume, Issue: \theissue, Month: \themonth, Year: \theyear\\
            Article Number: \theartnum, \thedoi\\
        \hrule\vspace{0.1cm}\hrule\vspace{0.2cm}
        \includegraphics[width=2.8cm]{./Logo/logoccc-eps-converted-to.pdf}
            \hspace{3.2cm}
        \includegraphics[width=3.4cm]{./Logo/sigla-eps-converted-to.pdf}
            \hspace{3.2cm}
        \includegraphics[width=2.7cm]{./Logo/siglaedagora-eps-converted-to.pdf}\\
        \hrule\



\title{Pose Manipulation with Identity Preservation}


\author{A. T. Ardelean, L. M. Sasu}
\date{}
{\let\newpage\relax 
\maketitle}
\thispagestyle{firstpage} 

{\small

\begin{flushleft}
\textbf{Andrei-Timotei Ardelean*}\\
Transilvania University of Brasov\\
500036 Brasov, B-dul Eroilor, 29, Romania\\
{*}Corresponding author: andrei.ardelean@student.unitbv.ro
\end{flushleft}


\begin{flushleft}
\textbf{Lucian Mircea Sasu}\\

1. Transilvania University of Brasov\\
500036 Brasov, B-dul Eroilor, 29, Romania\\
lmsasu@unitbv.ro\\
2. Xperi Corporation\\
500152 Brasov, Turnului, 5, Romania\\
lucian.sasu@xperi.com\\
\end{flushleft}




\begin{abstract}
This paper describes a new model which generates images in novel poses e.g. by altering face expression and orientation, from just a few instances of a human subject. Unlike previous approaches which require large datasets of a specific person for training, our approach may start from a scarce set of images, even from a single image. To this end, we introduce Character Adaptive Identity Normalization GAN (CainGAN) which uses spatial characteristic features extracted by an embedder and combined across source images. The identity information is propagated throughout the network by applying conditional normalization. After extensive adversarial training, CainGAN receives figures of faces from a certain individual and produces new ones while preserving the person's identity. Experimental results show that the quality of generated images scales with the size of the input set used during inference. Furthermore, quantitative measurements indicate that CainGAN performs better compared to other methods when training data is limited.

{\bf Keywords:} pose manipulation, image generation, adaptive normalization, Generative Adversarial Network.
\end{abstract}
} 




\section{Introduction}
Developing a way to easily manipulate face expression and head pose of an individual has been the focus of many research groups in the last decade. The baseline solution for this task involves creating a 3D model and animate it accordingly. However, building a photo-realistic head model using standard techniques can take substantial amount of time for a human artist. Many industries could benefit from optimizing this process such as cinematic, advertising and video games; furthermore, it has potential for image enhancement and editing software. 

A traditional automatic method for face manipulation is based on 3DMM fitting \cite{Blanz:1999:MMS:311535.311556}. Parameters can be estimated from a single image and then changed to obtain different expressions. This method is not sufficient by itself to work with hidden regions, e.g. teeth and closed eyes \cite{yuan2019face}.

Other approaches rely on warping from one or more source images to the desired pose to generate guided head images \cite{Wiles18}. A drawback of this solution is the limited amount of variation between the source and target pose it can manage without great loss of quality \cite{zakharov2019fewshot}.

New approaches for face image generation have been brought by recent advances in generative adversarial networks (GANs) \cite{NIPS2014_5423}. Seminal papers in this area \cite{karras2018progressive,tero2018style} show that one can generate high resolution realistic figures of human faces using GANs.

Our contribution is a new model, Character Adaptive Identity Normalization GAN (CainGAN), that receives figures of faces from a certain individual and produces new ones while preserving the person's identity. CainGAN generates images in novel poses starting from a small set of source pictures with the individual, i.e. a few-shot setting, without any fine-tuning as found in \cite{finn2017model,zakharov2019fewshot}.

We conducted experiments to compare images generated by CainGAN,  with alternative systems using image-to-image translation \cite{pix2pix2017,wang2018pix2pixHD} and conditioning based on Adaptive Instance Normalization \cite{huang2017adain} using computed embeddings \cite{zakharov2019fewshot}. By performing quantitative measurements on the self-reenactment task we show that our model is able to achieve state-of-the-art results using less data compared to other methods and without fine-tuning.  

The rest of the paper is structured as follows: In section \ref{sec:related} we make a literature review; the subsequent section describes CainGAN in detail; section \ref{sec:experiments} contains a comparison between different methods and an ablation study. Eventually, we summarize our contributions in section \ref{sec:conclusion}. Code for the implementation of CainGAN is available at \href{https://github.com/TArdelean/CainGAN}{https://github.com/TArdelean/CainGAN}

\section{Related Work}
\label{sec:related}
A number of works on face generation with preservation of identity focus on the talking face task, i.e. the area of interest is the mouth region with motion driven by either audio sources \cite{chen2018lip,XuSoundTV,suwajanakorn2017obama,zhou2019talking} or video to be imitated \cite{ijcai2019-129,zhou2019talking}. These methods cannot be easily extended to synthesize full head images that require handling more variation between poses or hidden elements in the source images. While it is possible to replace the face from an existing head footage, by using a face modeling approach as in \cite{thies2018face2face}, the result of pose manipulation would be limited to face expression.

Providing conditional information to several layers of the generator has been widely used to prevent input constraints from vanishing. Good results were obtained especially by modulating activations using AdaINs \cite{chen2018on,huang2017adain,tero2018style} and SPADE \cite{park2019SPADE} that employs spatial denormalization to introduce semantic map constraints.

Our model is based on a conditional GAN framework, i.e. instead of generating starting from noise as done traditionally \cite{NIPS2014_5423}, the input of the generative network can take different forms including images \cite{pix2pix2017,park2019SPADE} as done in this work. The use of multiple discriminators to stabilize GAN training has been recently studied in several works \cite{carneiro2019,Durugkar2016GenerativeMN,NIPS2017_6860}. Specifically our model uses two discriminators with different objectives.

\section{The CainGAN model}
In this section we present the architecture of the proposed model, followed by the training algorithm. Eventually we give some implementation details.
\subsection{The Architecture}
The goal is to train a generative model that is able to synthesize new images starting from  existing source pictures with the same person. Let  denote the -th image of an image sequence ; we uniformly sample  distinct images from . The model receives as input  along with their corresponding landmark images \cite{bulat2017far}  and target landmark , then generates a new image  that must follow target landmark and preserve identity of the person from the  input images. The generated image is expected to be similar to the ground truth image .

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{Figures/arhitecture.png}
    \caption{Full architecture and model workflow. Top side depicts the embedding which is used at each upsampling step to spatially modulate activations. The generator (shown in the bottom side of the figure) starts from the target landmarks  to synthesize a new image .  and  represent the identity and pose discriminators, respectively.}
    \label{fig:full_arhi}
\end{figure}

We propose CainGAN, a model that consists of 4 networks (Figure \ref{fig:full_arhi}) which we will describe in the following.  

The embedder computes a spatial embedding  from a single input image: . Implementation details can be found in section \ref{sec:implementation}. In order to use multiple source images, a method to combine the embeddings must be devised. Zakharov et al. in \cite{zakharov2019fewshot} simply averages the one-dimensional tensors computed by the embedder. We observed that weighing the features by their relevance helps to better capture the identity, therefore a responsibility based combining method was developed (eq. \eqref{eq:1}, Figure \ref{fig:comb_emb}). To achieve this, the embedder will also output a weighing tensor  representing the certainty for the computed features. The final identity embedding is calculated by a function  as:
  
We explored the use of target landmarks  as input to the embedder along with  and  and found this helpful for the generation process, since the identity features can be aligned to the final pose earlier. Hence, this is the version used in our experiments, denoted Targeted Embedder, as illustrated in Figure \ref{fig:temb}.

\begin{figure}[!t]
\centering
\begin{subfigure}{0.57\textwidth}
    \includegraphics[height=12em]{Figures/comb_emb.png}
    \caption{Combining embeddings based on responsibility, as in equation \eqref{eq:1}. The weights of the Targeted Embedder (E) are the same for each of the  inputs. The Targeted Embedder is detalied in Figure \ref{fig:temb}.}
    \label{fig:comb_emb}
\end{subfigure}\hspace{0.04\textwidth}
\begin{subfigure}{0.37\textwidth}
    \includegraphics[height=12em]{Figures/t_embedder.png}
        \caption{The structure of the Targeted Embedder. The real frame , its landmark image  and target landmarks  are concatenated and processed to get  and .}
    \label{fig:temb}
\end{subfigure}
\caption[]{Embedder network}
\label{fig:embedder}
\end{figure}

The generator starts from the target landmark and generates a new image . The landmark image is provided through the input layer while the combined spatial embedding  is used to modulate the activations at several resolutions with SPADE blocks.
In order to assess the quality of the generated image we employ two discriminators: identity discriminator and pose discriminator.

Identity discriminator  follows the multi-scale architecture from pix2pixHD \cite{wang2018pix2pixHD} and is used to estimate identity resemblance. At this stage we only consider characteristic traits and the results are expected to be invariant to pose. Thus, the input consists of two RGB images of the same person in arbitrary positions.

Pose discriminator  has the same architecture as  with its own set of parameters. The network receives a frame and the appropriate target landmarks and checks the correspondence.  

While both discriminators will also assess general realism of a given image,  is used specifically to avoid pose mismatch, whereas  encourages identity preservation. This disengagement allows us to assign different importance factors to each discriminator while training. Different combinations may give results that correlate better with human visual perception.

\subsection{Training}
Videos are used as image sequences during training. For each epoch we sample a sequence of  distinct frames for each training video and compute landmark images:  using a pretrained face alignment network \cite{bulat2017far}. The embedding tensor  is then computed according to (\ref{eq:1}). Using  and , CainGAN generates the new image  which is further fed to the identity discriminator  along with a source frame . The choice of the identity frame index  is arbitrary as the generation process is not influenced by the order of input images.  and  are also fed to the pose discriminator . Importance factors  and  control the weight of each discriminator in the objective function given by the hinge loss \cite{lim2017geometric,miyato2018spectral}:





The identity and pose discriminators are updated using  and , respectively. The weights of the generator and the embedder are updated together using the full objective:


 and  represent hyperparameters that control the importance of the loss in the full objective.  is a perceptual loss that compares features extracted at several layers by a pretrained VGGNet \cite{simonyan2014very} from the original and the generated image.
Feature matching loss  is also a perceptual loss, comparing activations in the layers of the discriminator according to \eqref{eq:6}. Importance factors  and  also affect the weight of feature matching losses. The FM loss is similar to the one used in \cite{wang2018pix2pixHD} as we employ multiscale discriminators:

where  represents the number of scales,  is the number of layers in ,  is the number of elements in layer  and  denotes the standard Manhattan distance.  is either the value of  when using the perceptual loss induced by the pose discriminator, or  for the identity discriminator, respectively.

To ease the training process we start with a low importance factor for the identity discriminator and linearly increase it to its maximum value over the first 10 epochs. This allows the model to learn the easy task first, generating realistic face images in given pose, after which we gradually impose identity preservation.

We alternate between (, ) and  updates, with twice more steps for the discriminator and using two time-scale update rule \cite{heusel2017gans} to stabilize training.  

\subsection{Implementation Details}
\label{sec:implementation}
The generator resembles an encoder-decoder architecture for image translation. There are 4 downsamplings residual blocks with learned skip connections, 3 same resolution residual blocks and 4 upsampling layers. Instance normalization \cite{ulyanov2016instance} is used after every downsampling and upsampling layer. A SPADE residual block with spectral normalization \cite{miyato2018spectral,zhang2019self} is used after each upsampling. Nearest interpolation is used to bring the spatial embeddings to the appropriate resolution for each SPADE block. The discriminators are based on the architecture proposed by Wang et al. in \cite{wang2018pix2pixHD}, and use 2 scales as the images are relatively small. The embedder consists of 2 downsampling and 4 same resolution residual blocks which are shared while computing  and . Two independent same resolution residual blocks are then used to get  and .


\section{Experiments and Results}
\label{sec:experiments}

To evaluate our approach, we conduct extensive experiments on the VoxCeleb2 \cite{Chung18b} dataset. To emphasize the ability of our model to learn from less data, we only use a small subset of the actual dataset. Originally, the train set contains almost 6000 different speakers featuring more than a million videos. For our experiments we randomly selected 150 speakers and their corresponding videos (around 30,000), less than 3\% of the grand total. A video dataset was used since it is an accessible way to obtain multiple images with the same identity.

Quantitative comparison is performed against two baselines: pix2pixHD \cite{wang2018pix2pixHD} and previously state-of-the-art method for talking head generation \cite{zakharov2019fewshot} denoted FSHM (Few-shot Head Models). We trained the pix2pixHD model from scratch as described in the original paper and official implementation. In order to use the model without fine-tuning, the network input consists of all source frames and their landmarks as well as the target landmark; these are also given to the discriminator. We also implemented a version of FSHM (feed-forward only) in order to assess the results in a limited training data setting. 

Three different metrics are used to compare the described methods: structural similarity metric (SSIM) between the ground truth and the generated image is used to measure low-level structural similarity, cosine similarity (CSIM) between embedding vectors as computed by a pretrained face recognition network and Fr\'echet Inception Distance (FID) \cite{heusel2017gans} measuring perceptual realism which usually better captures the similarity of real and fake images. We follow the same training setup presented by Zakharov et al. \cite{zakharov2019fewshot}, using 50 video sequences with 32 test frames for each. 

\setlength{\tabcolsep}{1em}
\begin{table}[t]
\centering
\begin{tabular}{l c c c}
     Method (K) & SSIM  & CSIM  & FID   \\
     \hline
     pix2pixHD(1) & 0.66 & 0.80 & 72.26 \\ 
     FSHM (1) & 0.64 & 0.72 & 93.17 \\
     FSHM-FF-full (1) & 0.61 & N/A & 46.61 \\
     FSHM-FT-full (1) & 0.64 & N/A & 48.5 \\
     CainGAN (1) & \textbf{0.69} & \textbf{0.85} & \textbf{35} \\
     \hline
     pix2pixHD(8) & 0.66 & 0.81 & 71.89 \\ 
     FSHM (8) & 0.65 & 0.73 & 83.13 \\
     FSHM-FF-full (8) & 0.64 & N/A & 42.2 \\
     FSHM-FT-full (8) & 0.68 & N/A & 42.2 \\
     CainGAN (8) & \textbf{0.77} & \textbf{0.91} & \textbf{24.92} \\
     \hline
\end{tabular}
\caption{K is the number of source frames used for testing. For SSIM and CSIM higher is better, for FID lower is better. CainGAN (8) was stopped after 20 epochs to avoid overfitting, all other models were trained for 30 epochs. The ``full'' suffix refers to the models being trained on the entire dataset. These results are taken from \cite{zakharov2019fewshot}. CSIM is not reported here, as a different face recognition network was used for the original results.
}
\label{tab:exp}
\end{table}

\begin{figure}[!t]
    \centering    
    \advance\leftskip-1.8cm
    \newlength\wid
    \setlength{\wid}{6.5em}
    \addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{m{3em}m{\wid}m{\wid}m{\wid}m{\wid}m{\wid}}

        \centering{} &
        \multicolumn{1}{c}{\textbf{Source}} & 
        \multicolumn{1}{c}{\textbf{pix2pixHD}} & 
        \multicolumn{1}{c}{\textbf{FSHM}} & 
        \multicolumn{1}{c}{\textbf{CainGAN}} & 
        \multicolumn{1}{c}{\textbf{Real Image}} \\ 
        
        \centering{\textbf{K=1}}&
        \includegraphics[width=\wid]{Figures/d150/source.png} &
        \includegraphics[width=\wid]{Figures/d150/p2p-os.png}&
        \includegraphics[width=\wid]{Figures/d150/fshm1.png}&
        \includegraphics[width=\wid]{Figures/d150/cain1.png}&
        \includegraphics[width=\wid]{Figures/d150/gt.png}\\
        
        \centering{\textbf{K=8}}&
        \includegraphics[width=\wid]{Figures/d150/source.png}&
        \includegraphics[width=\wid]{Figures/d150/p2p-8shot.png}&
        \includegraphics[width=\wid]{Figures/d150/fshm8.png}&
        \includegraphics[width=\wid]{Figures/d150/cain8.png}&
        \includegraphics[width=\wid]{Figures/d150/gt.png}\\
        
        \centering{\textbf{K=1}}&
        \includegraphics[width=\wid]{Figures/d39/source.png} &
        \includegraphics[width=\wid]{Figures/d39/p2p-os.png}&
        \includegraphics[width=\wid]{Figures/d39/fshm1.png}&
        \includegraphics[width=\wid]{Figures/d39/cain1.png}&
        \includegraphics[width=\wid]{Figures/d39/gt.png}\\
        
        \centering{\textbf{K=8}}&
        \includegraphics[width=\wid]{Figures/d39/source.png}&
        \includegraphics[width=\wid]{Figures/d39/p2p-8shot.png}&
        \includegraphics[width=\wid]{Figures/d39/fshm8.png}&
        \includegraphics[width=\wid]{Figures/d39/cain8.png}&
        \includegraphics[width=\wid]{Figures/d39/gt.png}\\
        
    \end{tabular}\vspace{-2pt}
    \caption{Visual assessment on the VoxCeleb2 dataset. First column represents the number of source frames, the next column illustrates one of the  source images and the last column contains the ground truth () images. In between are the generated frames by different methods. The figure is best viewed in color.}
    \label{fig:qab}
\end{figure}


The comparison given in Table \ref{tab:exp} shows that CainGAN is able to get better quantitative results using only a fraction of the dataset. Additionally, the method is able to generate realistic images in the desired pose with a good preservation of identity. From qualitative comparison in Figure \ref{fig:qab} we can see that while FSHM can synthesize the face with the right alignment there is a high identity mismatch. Clearly, small amounts of training images severely affect the ability of the FSHM model to generalize to unseen faces. We also obtain the uncanny artifacts present in images generated by pix2pixHD, as reported in \cite{zakharov2019fewshot}.

\subsection{Ablation Study}
We performed an ablation study to analyze the influence of different components of our method. Quantitative results of the experiments are visible in Table \ref{tab:abl}. The variants are: CainGAN without targeting (CainGAN w/o T) where only the source frame and its landmarks are given to the embedder, CainGAN without discriminator importance weighing (CainGAN w/o I) where  are fixed and CainGAN without responsibility based embedding mixing (CainGAN w/o R) where the weighted version in equation \eqref{eq:1} is replaced by:

This variant is not applicable for  as in this case the two expressions yield the same result.

\setlength{\tabcolsep}{1em}
\begin{table}[!t]
\centering
\begin{tabular}{l c c c}
     Method (K) & SSIM  & CSIM  & FID  \\
     \hline
     CainGAN w/o T (1) & \textbf{0.69} & \textbf{0.85} & 36.26 \\ 
     CainGAN w/o I (1) & 0.68 & 0.84 & 46.44 \\
     CainGAN (1) & \textbf{0.69} & \textbf{0.85} & \textbf{35} \\
     \hline
     CainGAN w/o T (8) & 0.72 & 0.87 & 38.08 \\ 
     CainGAN w/o I (8) & 0.76 & \textbf{0.91} & 28.72 \\
     CainGAN w/o R (8) & 0.75 & 0.90 & 30.05 \\
     CainGAN (8) & \textbf{0.77} & \textbf{0.91} & \textbf{24.92} \\
     \hline
\end{tabular}
\caption{Ablation study on selection of VoxCeleb2 dataset. All models were trained for 30 epochs, the best result between epochs 20 and 30 was reported.  is the number of source frames.}
\label{tab:abl}
\end{table}

We can observe that all components are essential to obtain the best results. Using targeted embedder has a greater influence in the  setting, which is expected since more images can benefit from early alignment.

\section{Conclusions and Future Work}
\label{sec:conclusion}
We introduced a new method for synthesizing images in novel poses while preserving the identity of a given subject. CainGAN  uses spatially adaptive normalization with a proper combining function of spatial feature maps in the embedding space. Experimental results show that CainGAN behaves better on scarce training sets compared to other methods. Furthermore, realistic images can be generated without the need for fine-tuning. The ablation study demonstrates CainGAN's non-redundant structure whereas the difference between scores in the  and  settings illustrate the ability to capitalize on more source images when available.
Further development directions include designing a method to extend the applicability of CainGAN beyond the task of self-reenactment and closing the gap between one-shot and multi-shot results by improving on single source image generation.

\begin{thebibliography}{11}


\bibitem{carneiro2019}
 Albuquerque, I.; Monteiro, J.; Doan T.; Considine B.; Falk T.; Mitliagkas I. (2019).
 Multi-objective training of Generative Adversarial Networks with multiple discriminators,
 \emph{arXiv preprint arXiv:1901.08680},
 2019.

\bibitem{Blanz:1999:MMS:311535.311556}
 Blanz, V.; Vetter, T. (1999).
 A Morphable Model for the Synthesis of {3D} Faces,
 \emph{Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques},
 187--194, 1999.
 
\bibitem{bulat2017far}
 Bulat, A.; Tzimiropoulos, G. (2017).
 How far are we from solving the 2D \& 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks),
 \emph{International Conference on Computer Vision},
 2017.

\bibitem{chen2018lip}
 Chen, L.; Li, Z.; Maddox, R.K.; Duan, Z.; Xu, C. (2018).
 Lip movements generation at a glance,
 \emph{Proceedings of the European Conference on Computer Vision (ECCV)},
 520--535, 2018.
 
\bibitem{XuSoundTV}
 Chen, L.; Zheng, H.; Maddox, R.K.; Duan, Z.; Xu, C. (2019).
 Sound to Visual: Hierarchical Cross-Modal Talking Face Video Generation,
 \emph{IEEE Computer Society Conference on Computer Vision and Pattern Recognition workshops},
 2019.
 
\bibitem{chen2018on}
 Chen, T.; Lucic, M.; Houlsby, N.; Gelly, S. (2019).
 On Self Modulation for Generative Adversarial Networks,
 \emph{International Conference on Learning Representations},
 2019.
 
\bibitem{Chung18b}
 Chung, J.~S.; Nagrani, A.; Zisserman, A. (2018).
 VoxCeleb2: Deep Speaker Recognition,
 \emph{INTERSPEECH},
 2018

\bibitem{Durugkar2016GenerativeMN}
 Durugkar I. P.; Gemp, I.; Mahadevan, S. (2016).
 Generative Multi-Adversarial Networks,
 \emph{arXiv preprint arXiv:1611.01673},
 2016.

\bibitem{finn2017model}
 Finn, C.; Abbeel, P.; Levine, S. (2017).
 Model-agnostic meta-learning for fast adaptation of deep networks,
 \emph{Proceedings of the 34th International Conference on Machine Learning-Volume 70},
 1126-1135, 2017.
 
\bibitem{NIPS2014_5423}
 Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. (2014).
 Generative Adversarial Nets,
 \emph{Advances in Neural Information Processing Systems 27},
 2672--2680, 2014.
 
\bibitem{heusel2017gans}
 Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; Hochreiter, S. (2017).
 GANs trained by a two time-scale update rule converge to a local nash equilibrium,
 \emph{Advances in Neural Information Processing Systems},
 6626--6637, 2017.

\bibitem{huang2017adain}
 Huang, X.; Belongie, S. (2017).
 Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,
 \emph{Proceedings of the IEEE International Conference on Computer Vision},
 1501--1510, 2017.

\bibitem{pix2pix2017}
 Isola, P.; Zhu, J.Y.; Zhou, T.; Efros, A.A. (2017).
 Image-to-Image Translation with Conditional Adversarial Networks,
 \emph{Proceedings of the IEEE conference on computer vision and pattern recognition},
 1125--1134, 2017.
 
\bibitem{karras2018progressive}
 Karras, T.; Aila, T.; Laine, S.; Lehtinen, J. (2018).
 Progressive Growing of {GAN}s for Improved Quality, Stability, and Variation,
 \emph{International Conference on Learning Representations},
 2018.
 
\bibitem{tero2018style}
 Karras, T.; Laine, S.; Aila, T. (2018).
 A Style-Based Generator Architecture for Generative Adversarial Networks,
 \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 4396--4405, 2018.
 
\bibitem{lim2017geometric}
 Lim, J.H.; Ye, J.C. (2017),
 Geometric gan,
 \emph{arXiv preprint arXiv:1705.02894},
 2017.
 
\bibitem{miyato2018spectral}
 Miyato, T.; Kataoka, T.; Koyama, M.; Yoshida, Y. (2018).
 Spectral Normalization for Generative Adversarial Networks,
 \emph{arXiv preprint arXiv:1802.05957},
 2018.

\bibitem{NIPS2017_6860}
 Nguyen, T.; Le, T.; Vu, H.; Phung, D. (2017).
 Dual Discriminator Generative Adversarial Nets,
 \emph{Advances in Neural Information Processing Systems},
 2670--2680, 2017.

\bibitem{park2019SPADE}
 Park, T.; Liu M.; Wang T.C.; Zhu, J.Y. (2019),
 Semantic Image Synthesis with Spatially-Adaptive Normalization,
 \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 2337--2346, 2019.
 
\bibitem{simonyan2014very}
 Simonyan, K.; Zisserman, A. (2014).
 Very deep convolutional networks for large-scale image recognition,
 \emph{arXiv preprint arXiv:1409.1556},
 2014.

\bibitem{ijcai2019-129}
 Song, Y.; Zhu, J.; Li, D.; Wang, A.; Qi, H. (2019).
 Talking Face Generation by Conditional Recurrent Adversarial Network,
 \emph{Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19},
 919--925, 2019.

\bibitem{suwajanakorn2017obama}
 Suwajanakorn, S; Seitz, S.; Kemelmacher, I. (2017).
 Synthesizing Obama: learning lip sync from audio,
 \emph{ACM Transactions on Graphics},
 36, 1--13, 2017.

\bibitem{thies2018face2face}
 Thies, J.; Zollhöfer, M.; Stamminger, M.; Theobalt, C.; Nießner, M. (2018).
 Face2Face: Real-time face capture and reenactment of RGB videos,
 \emph{Communications of the ACM},
 62, 96--104, 2018.


\bibitem{ulyanov2016instance}
 Ulyanov, D.; Vedaldi, A.; Lempitsky, V. (2016).
 Instance normalization: The missing ingredient for fast stylization,
 \emph{arXiv preprint arXiv:1607.08022},
 2016.
 
\bibitem{wang2018pix2pixHD}
 Wang T.C.; Liu M.Y.; Zhu J.Y.; Tao A.; Kautz J.; Catanzaro B. (2018).
 High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,
 \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 8798--8807, 2018.
 
\bibitem{Wiles18}
 Wiles, O.; Koepke, A.S.; Zisserman, A. (2018).
 X2Face: A network for controlling face generation,
 \emph{European Conference on Computer Vision}
 670--686, 2018.
 
\bibitem{yuan2019face}
 Yuan, X.; Park, I.K., (2019).
 Face De-occlusion using 3D Morphable Model and Generative Adversarial Network,
 \emph{Proceedings of the IEEE International Conference on Computer Vision},
 10062--10071, 2019.

\bibitem{zakharov2019fewshot}
 Zakharov, E.; Shysheya, A.; Burkov, E.; Lempitsky, V. (2019).
 Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,
 \emph{arXiv preprint arXiv:1905.08233},
 2019.
 
\bibitem{zhang2019self}
 Zhang, H.; Goodfellow, I.; Metaxas, D.; Odena, A. (2018),
 Self-Attention Generative Adversarial Networks,
 \emph{arXiv preprint arXiv:1805.08318},
 2018
 
\bibitem{zhou2019talking}
 Zhou, H.; Liu, Y.; Liu, Z.; Luo, P.; Wang, X. (2019).
 Talking Face Generation by Adversarially Disentangled Audio-Visual Representation,
 \emph{AAAI Conference on Artificial Intelligence},
 33, 9299--9306, 2019


\end{thebibliography}

\begin{figure*}[h!]
\includegraphics[width=2.2cm]{./Logo/cc-eps-converted-to.pdf}\\
Copyright 2020 by the authors. Licensee Agora University, Oradea, Romania.\\
 This is an open access article distributed under the terms and conditions of the Creative Commons Attribution-NonCommercial 4.0 International License.\\
Journal's webpage: http://univagora.ro/jour/index.php/ijccc/ \\
\end{figure*}
\begin{figure*}[h!]
\centering
\includegraphics[width=3cm]{./Logo/cope-eps-converted-to.pdf}\\
This journal is a member of, and subscribes to the principles of,\\
 the Committee on Publication Ethics (COPE).\\
 https://publicationethics.org/members/international-journal-computers-communications-and-control
\end{figure*}

\label{LastPage}


\end{document} 