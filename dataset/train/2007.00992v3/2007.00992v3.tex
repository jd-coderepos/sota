\begin{figure*}[t]
\small
\centering
\hspace{-4mm}
\begin{subfigure}[ht!]{0.24\linewidth}
\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1.0\linewidth]{figures/sources/final_1x1_expand.pdf}
\vspace{-6mm}
\caption{\small 1×1 convolution}
\label{subfig:layer1x1}
\end{subfigure}
\begin{subfigure}[ht!]{0.24\linewidth}
\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1.0\linewidth]{figures/sources/final_3x3_expand.pdf}
\vspace{-6mm}
\caption{\small 3×3 convolution}
\label{subfig:layer3x3}
\end{subfigure}
\begin{subfigure}[ht!]{0.24\linewidth}
\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1.0\linewidth]{figures/sources/final_IB_conv_16.pdf}
\vspace{-6mm}
\caption{\small Inverted Bot. (with conv)}
\label{subfig:IB_conv}
\end{subfigure}
\begin{subfigure}[ht!]{0.24\linewidth}
\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1.0\linewidth]{figures/sources/final_IB_dwconv_16.pdf}
\vspace{-6mm}
\caption{\small Inverted Bot. (with dwconv)}
\label{subfig:IB_dwconv}
\end{subfigure}
\vspace{-3mm}
\caption{\small {\bf Visualization of the output rank}. We measure the rank ratio (i.e., rank/output channel dimension) vs. channel dimension ratio (i.e., input channel dimension/output channel dimension) from diverse architectures averaged over 1,000 random-sized networks with various nonlinear functions: (a) A single 1×1 convolution; (b) A single 3×3 convolution; (c) An inverted bottleneck with a 3×3 convolution; (d) An inverted bottleneck with a 3×3 depthwise convolution~\cite{mobilenetv2}. We fundamentally observe that all the ranks are expanded above the input channel dimensions by the nonlinear functions with different network architectures.}
\label{fig:rank_test}
\vspace{-5mm}
\end{figure*}

 
\section{Designing an Expansion Layer}
\label{section:sec3}
In this section, we study how to design a layer properly considering the expressiveness, which is essential to design an entire network architecture.

\subsection{Preliminary}
\label{subsection:preliminary}
\paragraph{Estimating the expressiveness.} 
In language modeling, the authors~\cite{yang2017breaking} firstly highlighted that the softmax layer may suffer from turning the logits to the entire class probability due to the rank deficiency. This stems from the low input dimensionality of the final classifier and the vanished nonlinearity at the softmax layer when computing the log-probability. The authors proposed a remedy of enhancing the expressiveness, which improved the model accuracy. This implies that a network can be improved by dealing with the lack of expressiveness at certain layers. Estimating the expressiveness was studied in a model compression work~\cite{zhang2015efficient}. The authors compressed a model at layer-level by a low-rank approximation; investigated the amount of compression by computing the singular values of each feature. Inspired by the works, we conjecture that the rank may be closely related to the expressiveness of a network, and studying it may provide an effective layer design guide.
\vspace{-4mm}

\paragraph{Layer designs in practice.} ResNet families~\cite{resnet, preresnet,resnext} have bottleneck blocks doubling the input channel dimensions (i.e., 64-128-256-512 in order) to make the final dimension (2048) above the number of classes at last. The recent efficient models~\cite{fbnet, chamnet, proxylessnas,mobilenetv3,mnasnet, tan2019mixconv,efficientnet, fbnetv2, mei2020atomnas, chu2019fairnas, chu2019fairdarts, liang2019dartsplus} increase the channel dimensions steadily in inverted bottlenecks; therefore, they commonly involve a large expansion layer at the penultimate layer. The output dimension of the stem is set to 32 which expands the 3-dimensional input. Both of the inverted bottleneck~\cite{mobilenetv2} and bottleneck block~\cite{resnet} have the convolutional expansion layer with the predefined expansion ratio (mostly 6 or 4). Are these layers designed correctly and just need to design a new model accordingly? 

\vspace{-1mm}
\subsection{Empirical study}\label{subsection:sec_empirical_study}
\vspace{-1mm}
\paragraph{Sketch of the study.}
We aim to study a design guide of a single expansion layer that expands the input dimension. We measure the rank of the output features from the diverse architectures (over 1,000 random-sized networks) and see the trend as varying the input dimensions towards the output dimensions. The rank is originally bounded to the input dimension, but the subsequent nonlinear function will increase the rank above the input dimension~\cite{amini2011low, yang2017breaking}. However, a certain network fails to expand the rank close to the output dimension, and the feature will not be fully utilized. We statistically verify the way of avoiding failure when designing the network. The study further uncovers the effect of complicated nonlinear functions such as ELU~\cite{elu} or SiLU (Swish-1)~\cite{gelu,swish} and where to use them when designing lightweight models.

\vspace{-4mm}
\paragraph{Materials.}
We generate a bunch of networks with the building blocks consists of 1) a single 1×1 or 3×3 convolution layer; 2) an inverted bottleneck block~\cite{mobilenetv2} with a 3×3 convolution or 3x3 depthwise convolution inside. We have the layer output (i.e., feature)  with  and the input , where  denotes a nonlinear 
function\footnote{We use ReLU~\cite{relu}, ReLU6~\cite{mobilenetv2},
LeakyReLU~\cite{leakyrelu},
ELU~\cite{elu}, SoftPlus~\cite{softplus}, Hard Swish~\cite{mobilenetv3}, and SiLU (Swish-1)~\cite{gelu,swish}.} with the normalization (we use a BN~\cite{BN} here).  is randomly sampled to realize a random-sized network, and  is proportionally adjusted for each {\it channel dimension ratio} (i.e., ) in the range .  denotes the batch size, where we set .
We compute {\it rank ratio} (i.e., ) for each model and average them. An inverted bottleneck block is similarly handled as a single convolution layer to compute rank\footnote{We denote the input and output of an inverted bottleneck as the input of the first 1×1 convolution; the output after the addition operation of the shortcut and the bottleneck, respectively.}. 

\vspace{-4mm}
\paragraph{Observations.} Figure~\ref{fig:rank_test} shows how the rank changes with respect to the input channel dimension on average. Dimension ratio () on x-axis denotes the reciprocal of the expansion ratio~\cite{mobilenetv2}. We observe the followings: 
\vspace{-2mm}
\begin{enumerate}[label=(\roman*)]
    \item {\it Drastic channel expansion harms the rank.} This holds for a single convolution layer and an inverted bottleneck both as shown in Figure~\ref{fig:rank_test}. The impact gets bigger with 1) an inverted bottleneck (see Figure~\ref{subfig:IB_conv}); 2) a depthwise convolution (see Figure \ref{subfig:IB_dwconv})\footnote{Actually, many recent lightweight models avoid increasing the channel dimension drastically (i.e., dimension ratio is usually higher than 0.5) at an inverted bottleneck; the models have inverted bottlenecks where the first 1×1 convolution has the expansion ratio 3, 4, or 6. Figure~\ref{subfig:layer1x1} tells us the design factors are tenable; the architects may know this empirically, but we believe that we have provided an underlying reason through the study.};
    \vspace{-2mm}
    \item {\it Nonlinearities expand rank.} Figure~\ref{fig:rank_test} shows averaged rank is expanded above the input channel dimension by nonlinear functions. They expand rank more at smaller dimension ratio, and complicated ones such as ELU, or SiLU do more;
    \vspace{-2mm}
    \item {\it Nonlinearities are critical for convolutions.} Nonlinearities expand the rank of 1×1 and 3×3 single convolutions more than an inverted bottleneck (see Figure~\ref{subfig:layer1x1} and \ref{subfig:layer3x3} vs. Figure~\ref{subfig:IB_conv} and \ref{subfig:IB_dwconv}). \vspace{-4mm}
\end{enumerate}

\paragraph{What we learn from the observations.}
We learned the followings: 1) an inverted bottleneck is needed to design with the expansion ratio of 6 or smaller values at the first 1×1 convolution; 2) each inverted bottleneck with a depthwise convolution in a lightweight model needs a higher channel dimension ratio; 3) a complicated nonlinearity such as ELU and SiLU needs to be placed after 1×1 convolutions or 3×3 convolutions (not depthwise convolutions). Based on the knowledge, in the following section, we perform channel dimension searches to find an effective channel configuration for entire channel dimensions. This is to uncover whether the conventional way of configuring channels shown in Table~\ref{table:channel_dim_setting} is optimal or not, albeit the models have worked well with high accuracy. 



\begin{table}[t]
\fontsize{8.5}{9.5}\selectfont
\centering
\tabcolsep=0.1cm
\hspace{-1.25mm}
\begin{tabular}{@{}l|c|c|c@{}}
Network & FLOPs & Top-1 & Nuc. norm \\
\midrule
Baseline  & 103M & 48.0\% & 5997.5  \\ 
+ Increase DR of 1×1 conv () & 99M & 52.1\% & 6655.9\\
+ Increase DR of IB () & 105M & 53.8\% & 6703.2\\ 
+ Replace ReLU6 with SiLU & 105M & 54.6\%  & 6895.9\\
\end{tabular}
\quad
\vspace{-3mm}
\caption{\small {\bf Factor analysis of the study}. We use four different models with similar computational complexity and report the accuracy and rank represented by the nuclear norm of the final feature. We average all the numbers over three models trained on CIFAR-100~\cite{cifar}. Each factor successively improves the accuracy and expands the rank without extra computational cost.}
\label{table:toy_exp1}
\vspace{-5mm}
\end{table}
 \vspace{-4mm}
\paragraph{Verification of the study.} We finally provide an experimental backup to make sure what we have learned contributes to improving accuracy. We train the models consisting of two inverted bottlenecks (IBs)\footnote{The models with a single IB cannot be similar in computational complexity with the fixed stem, so the models should contain at least two IBs.} to adjust the channel dimension ratio (DR) of IBs and the first 1×1 convolutions in each IB. Starting from the baseline with the low DR 1/20, we successively study through the picked models with 1) increasing DR of the first 1×1 conv to 1/6; 2) increasing DR at every IB from 0.22 to 0.8; 3) replacing the first ReLU6 with SiLU in each IB. Table~\ref{table:toy_exp1} shows each factor works well, and the rank and accuracy increase together.






