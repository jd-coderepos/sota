\documentclass{LMCS}                                                                                   
\def\doi{8 (1:28) 2012}                                                  
\lmcsheading {\doi}                                                                      
{1--44}                                                                    
{}                                                                         
{}                                                                        
{Oct.~\phantom04, 2011}                                                    
{Mar.~26, 2012}                                                            
{}                                                                                                      

\usepackage[usenames,dvipsnames]{color}                                                                
\usepackage{amssymb}                                                                                   
\usepackage{amsmath}                                                                                   
\usepackage{stmaryrd}                                                                                  
\usepackage{bussproofs}                                                                                
\usepackage{url}                                                                                       
\usepackage{proof}                                                                                     
\usepackage{fancybox}                                                                                  
\usepackage{ulem}                                                                                     
\usepackage{enumerate,hyperref}                                                                        

                                                                                                       
\newcommand{\mathsmall}[1]{\ensuremath{\mbox{\small{}}}}
\newcommand{\usm}{{\tt R_{\mX}}}
\newcommand{\lamBig}{'\!\Lambda\mbox{\textsc{big}}}
\newcommand{\shane}[1]{{\color{BrickRed} {#1}}}
\newcommand{\Shane}[1]{{\bf Shane: #1}}
\newcommand{\ShaneIgnore}[1]{}
\newcommand{\ALCes}{\ALC \es}
\newcommand{\Btwo}{{\B\B}}
\newcommand{\ltwoes}{\lam\B\es}
\newcommand{\fresh}[1]{\widehat{#1}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Er}{\mathcal{ E}}
\def\itmath#1{\leavevmode\ifmmode{\mbox{\it#1} }\else{\it#1 }\fi}
\def\sfmath#1{\leavevmode\ifmmode{\mbox{\sf#1} }\else{\sf#1 }\fi}
\def\condmath#1{\leavevmode\ifmmode{#1}\else{}\fi}

\newcommand{\height}[1]{{\tt h}(#1)} 
\newcommand{\p}[1]{{\tt p}(#1)} 
\newcommand{\nombreprof}{height} 
\newcommand{\lab}[2]{[ \! [ #1/ #2 ] \! ]}
\newcommand{\labu}[2]{[\underline{#1}/\underline{#2}]}


\newcommand{\ie}{{\it  i.e.}~}
\newcommand{\cf}{{\it  cf.}~}
\newcommand{\eg}{{\it  e.g.}~}



\newenvironment{compactitem}
  {\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}}
  {\end{itemize}}

\newenvironment{compactenumerate}{\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}}
  {\end{enumerate}}

\newenvironment{compactdescription}{\begin{description}\setlength{\itemsep}{0.1pt}\setlength{\parskip}{0pt}\setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}}
  {\end{description}}





\newcommand{\evv}[1]{{\color{Plum} {#1}}}
\newcommand{\evo}[1]{{\color{RedOrange} {#1}}}

\newcommand{\stefano}[1]{{\color{green} {#1}}}

\newcommand{\adv}[1]{{\color{green} {#1}}}
\newcommand{\SOdelia}[1]{{  \color{blue} \sout{ #1}}}


\newcommand{\commentd}[1]{{\color{blue} {#1}}}
\newcommand{\cfabien}[2]{{\color{red} {#2}}}


\newcommand{\sep}{\hspace*{0.5cm}}
\renewcommand{\>}{\rightarrow}



\def\lam{\lambda}
\def\lu{\underline{\lambda}}
\def\sig{\sigma}
\def\Gam{\Gamma}
\def\Del{\Delta}
\def\Lam{\Lambda}


\newcommand{\Rew}[1]{\rightarrow_{#1}}
\newcommand{\rRew}[1]{\mapsto_{#1}}
\newcommand{\simRew}[1]{\leftrightarrow_{#1}}
\newcommand{\equ}[1]{\equiv_{#1}}
\newcommand{\redu}[1]{\Rightarrow_{#1}}
\newcommand{\Rewn}[2][*]{\rightarrow^{#1}_{#2}}
\newcommand{\Rewnmod}[2]{\rightarrow^*_{#1/#2}}
\newcommand{\Rewplusmod}[2]{\rightarrow^+_{#1/#2}}
\newcommand{\Rewp}[1]{\rightarrow^{+}_{#1}}
\newcommand{\x}{{\tt x}}
\newcommand{\xw}{{\tt xw}}
\newcommand{\lx}{\lam{\tt x}}
\newcommand{\lZ}{\lam_{\tt Z}}


\newcommand{\s}{{\tt s}}
\newcommand{\sw}{{\tt sw}}
\newcommand{\es}{{\tt es}}
\newcommand{\esw}{{\tt esw}}

\newcommand{\ws}{{\tt rxw}}
\newcommand{\les}{\lam \es}

\newcommand{\ex}{{\tt ex}}
\newcommand{\lex}{\lam \ex}


\newcommand{\labs}{{\tt \mathbb{E}\mathbb{X}}}
\newcommand{\labst}{{\tt \mathbb{X}}}

\newcommand{\labx}{{\tt \underline{x}}}
\newcommand{\labss}{{\tt \underline{s}}}
\newcommand{\llex}{\lam \underline{\ex}}
\newcommand{\uex}{\underline{\ex}}


\newcommand{\llexv}{\lam\SX}
\newcommand{\SX}{\ds}
\newcommand{\dv}{\mathbb{V}}
\newcommand{\ds}{\mathbb{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\ucX}{\underline{\mathcal{X}}}


\newcommand{\llexi}{\llex^i}
\newcommand{\llexe}{\llex^e}

\newcommand{\lexi}{\uex^i}
\newcommand{\lexe}{\uex^e}

\newcommand{\lm}{\lambda_{sub}}
\newcommand{\m}{{sub}}
\newcommand{\lws}{\lam \ws}
\newcommand{\lesw}{\lam \esw}

\newcommand{\Es}{{\tt E}_{\s}}
\newcommand{\Esw}{{\tt E}_{\sw}}
\newcommand{\mB}{{\tt min  (\B)}}

\newcommand{\sB}{{\tt safe (\B)}}

\newcommand{\lst}{{\tt lst}}
\newcommand{\nelst}{{\tt nelst}}
\newcommand{\llxr}{\lam {\tt lxr}}


\newcommand{\xe}{{\tt xe}}
\newcommand{\lxe}{\lam{\tt xe}}
\newcommand{\lambdax}{\lam{x}}
\newcommand{\subs}[2]{[#1 / #2]}
\newcommand{\as}[2]{[ \! \! [ #1 / #2 ] \! \! ]}

\newcommand{\su}[2]{ [ \! [ #1 / #2 ]  \! ] }

\newcommand{\dsub}[1]{ [ \! [  #1  ] \! ]  }

\newcommand{\isubs}[1]{ \{ #1  \} }
\newcommand{\lisubs}[1]{ \{\!\! \{ #1  \} \!\! \}}
\newcommand{\SN}[1]{\mathcal{SN}_{#1}}
\newcommand{\WN}[1]{\mathcal{WN}_{#1}}
\newcommand{\NF}[1]{\mathcal{ NF}_{#1}}
\newcommand{\ISN}{\mathcal{ ISN}}
\newcommand{\multiset}[1]{ [ #1 ] }
\newcommand{\pos}[2]{{\tt pos}_{#1}(#2)}
\newcommand{\posi}[3]{{\tt pos}_{#2}^{#1}(#3)}
\newcommand{\allposi}[2]{{\tt allpos}^{#1}(#2)}
\newcommand{\Ch}[1]{{\tt paths}(#1)}
\newcommand{\allCh}[1]{{\tt allpaths}(#1)}

\newcommand{\nat}{{\mathbb N}}
\newcommand{\seq}[3]{#1_{#2} \ldots #1_{#3}}
\newcommand{\deriv}[1]{\mathcal{D}(#1)}

\newcommand{\varcase}{{\tt (var)}}
\newcommand{\abscase}{{\tt (abs)}}
\newcommand{\subscase}{{\tt (subs)}}
\newcommand{\appcase}{{\tt (app)}}


\newcommand{\lambdachi}{\lam_{\varsigma}}
\newcommand{\lambdad}{\lam_{\dd}}
\newcommand{\lambdadn}{\lam_{\dn}}
\newcommand{\lambdae}{\lam_{\e}}
\newcommand{\lambdaf}{\lam_{\f}}
\newcommand{\lambdag}{\lam_{\g}}
\newcommand{\lambdav}{\lam_{\v}}
\newcommand{\lsigma}{\lam_{\sigma}}
\newcommand{\lambdasigmalift}{\lam_{\sigma_{\Uparrow}}}
\newcommand{\lambdas}{\lam_{s}}
\newcommand{\lambdatau}{\lam_{\tau}}

\newcommand{\dd}{d}
\newcommand{\dn}{dn}
\newcommand{\e}{{\tt e}}
\newcommand{\en}{{\tt e}^{-}}
\newcommand{\f}{{\tt f}}
\newcommand{\g}{g}
\renewcommand{\v}{\upsilon}
\newcommand{\sigmalift}{\sigma_{\Uparrow}}


\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\B}{{\tt dB}}
\newcommand{\ttE}{{\tt E}}
\newcommand{\ttF}{{\tt F}}
\newcommand{\tts}{{\tt s}}
\newcommand{\ttv}{{\tt v}}
\newcommand{\ttS}{{\tt S}}
\newcommand{\ttR}{{\tt R}}
\newcommand{\Bl}{{\tt dB}^l}
\newcommand{\Bg}{{\tt gdB}}
\newcommand{\Betag}{{\tt gd}\beta}

\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}

\newcommand{\uB}{\underline{\B}}
\newcommand{\shB}{{\tt B}}

\newcommand{\Bc}{{\tt Bc}}
\newcommand{\C}{{\tt C}}
\renewcommand{\H}{{\tt H}}
\newcommand{\J}{{\tt J}}
\newcommand{\T}{{\tt T}}
\newcommand{\ii}{{\tt I}}
\newcommand{\lang}{\mathcal{L}}

\newcommand{\cal}{\it}
\newcommand{\D}{D}
\newcommand{\E}{{\tt E}}
\newcommand{\Eq}{\mathcal{E}}
\newcommand{\F}{F}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\Q}{\mathcal{Q}}
 
\newcommand{\tw}{\mbox{\tt{w}}}
\newcommand{\tc}{\mbox{{\tt{c}}}}
\newcommand{\tcs}{\mbox{\tt{cs}}}
\newcommand{\tcw}{\mbox{\tt{cw}}}
\newcommand{\tsw}{\mbox{\tt{sw}}}
\newcommand{\tcsw}{\mbox{\tt{csw}}}
\newcommand{\dis}{{\tt j}}
\renewcommand{\cong}{\f}

\newcommand{\labdis}{\underline{{\tt j}}}
\newcommand{\edis}{{\tt e}\dis}

\newcommand{\disngc}{{\tt ned}}
\newcommand{\dismr}{\dis_{\rmod}}
\newcommand{\discl}{\dis_{\equiv_L}}
\newcommand{\ldis}{\lam{\dis}}
\newcommand{\ldisNU}{\lam{\dis {\tt n} \unboxed}}
\newcommand{\ldisNUfS}{\lam \modulo{{\dis {\tt n} \unboxed}}{\fsymb\Sigma}}
\newcommand{\lj}{\lam{\dis}}
\newcommand{\ljlab}{\lam{\dis}^{l}}
\newcommand{\ldisc}{\lam{\modulo{\dis}{\cong}}}
\newcommand{\ldisngc}{\lam{\disngc}}
\newcommand{\ldisunb}{\ldis{\tt ub}}
\newcommand{\ldiswc}{\lam{\dis+}{\osymb}}
\newcommand{\ldisw}{\lam{\dis\Gc}}
\newcommand{\diswc}{{\dis\Gc\cong}}
\newcommand{\res}{{\tt r}}
\newcommand{\rldis}{\lam{\dis}{\res}}
\newcommand{\rdis}{{\dis}{\res}}
\newcommand{\rldism}{\lam{\dis}{\res}_\equiv}
\newcommand{\ldisngcm}{\lam{\disngc}{\res}_\equiv}
\newcommand{\ldisngcmr}{\lam{\disngc}{\res}_{\rmod}}
\newcommand{\dism}{{\tt d_\equiv}}
\newcommand{\disngcm}{{\disngc}{\res}_\equiv}
\newcommand{\disngcmr}{{\disngc}{\res}_{\rmod}}
\renewcommand{\mod}{\equiv}
\newcommand{\rmod}{\equiv_{\sigma}}

\newcommand{\ldism}{\lam{\dism}}
\newcommand{\ldismr}{\lam{\dismr}}
\newcommand{\ldiscl}{\lam{\discl}}
\newcommand{\ldisr}{\lam{\dis}_{\rightarrow}}
\newcommand{\ldisri}{\lam{\dis\prop_i}}
\newcommand{\rldisri}{\lam{\dis\res}_{\rightarrow i}}
\newcommand{\ljdag}{\lam\jop\mbox{-dag}}
\newcommand{\nest}{\sqsubset}
\newcommand{\tsc}{\mbox{\tt{sc}}}
\newcommand{\tsm}{\rldis_{\equiv_{\tt r}}}
\newcommand{\rGB}{\lam\Gcc_{\equiv_{\tt r}}}
\newcommand{\cm}{\mbox{\tt{m}}}
\newcommand{\cmw}{\mbox{\tt{m1}}}
\newcommand{\cmd}{\mbox{\tt{m2}}}
\newcommand{\cmt}{\mbox{\tt{m3}}}
\newcommand{\tscr}{\mbox{\tt{scr}}}
\newcommand{\ta}{\mbox{\tt{a}}}
\newcommand{\tb}{\mbox{\tt{b}}}

\newcommand{\ORTH}[1]{\condmath{#1\lneg}}
\def\lneg{^\perp}

\newcommand{\pair}[2]{\langle #1, #2 \rangle}
\newcommand{\trip}[3]{\langle #1, #2, #3\rangle}

\newcommand{\typtra}[1]{\condmath{{#1}^{*}}} 
\newcommand{\pretra}[1]{\condmath{\mathcal{T}^{-}(#1)}} 
\newcommand{\tra}[1]{\condmath{T(#1)}}
\newcommand{\pntra}[1]{\condmath{W(#1)}}
\newcommand{\pnlestra}[1]{\condmath{Z(#1)}}
\newcommand{\nombrepnlestra}{Z}
\newcommand{\nombretra}{T} 

\newcommand{\Type}[1]{\Lambda_{#1}}
\newcommand{\Strongly}{\mathcal{SN}} 
\newcommand{\SET}{\mathcal{R}} 


\newcommand{\Terminos}[1]{\Lambda_{#1}}
\newcommand{\Sustituciones}[1]{\mathcal{S}_{#1}}
\newcommand{\Objetos}[1]{{\cal O}_{#1}}
\newcommand{\Appp}{\widehat{App}}
\newcommand{\Apps}{\widetilde{App}}
\newcommand{\Lambdap}{\widehat{Lambda}}
\newcommand{\Lambdas}{\widetilde{Lambda}}
\newcommand{\FVarLiftp}{\widehat{FVarLift}}
\newcommand{\FVarLifts}{\widetilde{FVarLift}}
\newcommand{\ShiftLiftp}{\widehat{ShiftLift}}
\newcommand{\ShiftLifts}{\widetilde{ShiftLift}}
\newcommand{\Cont }{{\tt SC}}
\newcommand{\CAbs}{{\tt CL}}
\newcommand{\CAppa}{{\tt CA_L}}
\newcommand{\CAppb}{{\tt CA_R}}
\newcommand{\CSubs}{{\tt CS}}
\newcommand{\WAppa }{{\tt AW_l}}
\newcommand{\WAppb }{{\tt AW_r}}
\newcommand{\WSubs }{{\tt SW}}
\newcommand{\Merge }{{\tt CW_1}}
\newcommand{\CRen}{{\tt CGc}}
\newcommand{\Ren}[3]{R_{#3}^{#2}({#1})}
\newcommand{\ren}[4]{R^{#2, #3}_{#4}(#1)}


\newcommand{\un}[1]{\underline{#1}}
\newcommand{\nombrets}{\#}
\newcommand{\lip}[2]{\Uparrow^{#1}(#2)}
\newcommand{\li}[1]{\Uparrow(#1)}
\newcommand{\nombreli}{\Uparrow}
\def\sh{\uparrow}
\newcommand{\shp}[1]{[\sh]^{#1}}
\def\uno{\un{1}}
\def\ext{ext}
\newcommand{\id}{id}

\newcommand{\Rewpos}[2]{\stackrel{#1}{\longrightarrow_{#2}\;}}
\newcommand{\Rewnpos}[2]{\stackrel{#1}{\longrightarrow^{*}_{#2}\;}}
\newcommand{\Rewpluspos}[2]{\stackrel{#1}{\longrightarrow^{+}_{#2}\;}}

\newcommand{\Rewoc}[1]{\stackrel{\longrightarrow\;}{{\mbox{}_{#1}}}}
\newcommand{\Rewnoc}[1]{\stackrel{\longrightarrow^{*}\;}{{\mbox{}_{#1}}}}
\newcommand{\Rewplusoc}[1]{\stackrel{\longrightarrow^{+}\;}{{\mbox{}_{#1}}}}


\newcommand{\Rewplus}[1]{\rightarrow^{+}_{#1}}
\newcommand{\LRewplus}[1]{\; \mbox{}_{\stackrel{#1}{+}}{\leftarrow} \;}


\newcommand{\LRew}[1]{\mbox{}_{#1}{\leftarrow} }
\newcommand{\LRewn}[1]{\mbox{}^{*}_{#1}{\leftarrow}}

\newcommand{\Rewnumber}[2]{\stackrel{#1}{\rightarrow_{#2}}}

\newcommand{\de}[1]{de(#1)}
\newcommand{\nombrede}{de}

\newcommand{\dnd}[1]{dnd(#1)}
\newcommand{\nombrednd}{dnd}

\newcommand{\nombreef}{ef}

\newcommand{\vf}[1]{vf(#1)}
\newcommand{\nombrevf}{vf}

\newcommand{\M}{\mathcal{M}}
\newcommand{\constructor}{\xi}

\newcommand{\skeleton}[1]{Sk(#1)}
\newcommand{\tri}{\:\triangleright\:}
\newcommand{\irule}[2]
   {\renewcommand{\arraystretch}{1.2}
    \begin{array}{c} \mbox{} \\ \hline \mbox{} \end{array}}


\newcommand{\Calc}{W}


\newcommand{\fv}[1]{{\tt fv}(#1)}
\newcommand{\lfv}[1]{{\mathbb L}{\tt fv}(#1)}
\newcommand{\nlfv}[1]{{\mathbb N}{\mathbb L}{\tt fv}(#1)}
\newcommand{\Lsub}[1]{{\mathbb L}{\tt sub}(#1)}
\newcommand{\Lsubvar}[2]{{\mathbb L}{\tt sub}_{#1}(#2)}
\newcommand{\Lsubnovar}[2]{{\mathbb L}{\tt sub}_{\neg #1}(#2)}

\newcommand{\tfv}[1]{\texttt{fv}^{+}(#1)}
\newcommand{\ntfv}[2]{|\texttt{fv}^{+}(#2)|_{#1}}
\newcommand{\ffv}{\texttt{fv}^{-}}

\newcommand{\ufv}{\texttt{ufv}}
\newcommand{\bv}[1]{\texttt{bv}(#1)}
\newcommand{\cfv}[1]{{\tt cfv}_{#1}}
\newcommand{\vlx}{\vdash_{\lx}}
\newcommand{\vles}{\vdash_{\les}}
\newcommand{\vd}{\vdash}
\newcommand{\vdbr}{\vdash_{\Br}}
\newcommand{\wfbr}{\Vdash_{\Br}}

\newcommand{\delambda}{{\tt A}}
\newcommand{\alambda}{{\tt L}}
\newcommand{\cut}{ {\tt cut}}
\newcommand{\proj}{\wfc}
\newcommand{\projl}{\mathbb{P}}
\newcommand{\projt}{ {\tt P}}
\newcommand{\subscut}{ {\tt subs}}
\newcommand{\ini}{\cap \; {\tt I}}
\newcommand{\ine}{\cap \; {\tt E}}

\newcommand{\axiom}{ {\tt ax}}
\newcommand{\aaxiom}{ {\tt ax}^{+}}
\newcommand{\maxiom}{ {\tt ax}^{*}}

\newcommand{\abs}{ {\tt abs}}
\newcommand{\aabs}{ {\tt abs}^{+}}
\newcommand{\mabs}{ {\tt abs}^{*}}
\newcommand{\absi}{ {\tt abs}_1}
\newcommand{\absii}{ {\tt abs}_2}
\newcommand{\mabsi}{ {\tt abs}^{*}_1}
\newcommand{\mabsii}{ {\tt abs}^{*}_2}
\newcommand{\app}{ {\tt a}}
\newcommand{\aapp}{ {\tt app}^{+}}
\newcommand{\mapp}{ {\tt app}^{*}}

\newcommand{\substr}{ {\tt subs}}
\newcommand{\asubstr}{ {\tt subs}^{+}}
\newcommand{\msubstr}{ {\tt subs}^{*}}
\newcommand{\subsi}{ {\tt subs}_1}
\newcommand{\subsii}{ {\tt subs}_2}
\newcommand{\msubsi}{ {\tt subs}^{*}_1}
\newcommand{\msubsii}{ {\tt subs}^{*}_2}
\newcommand{\Var}{{\tt d}}
\newcommand{\Varl}{{\tt d}^l}
\newcommand{\uVar}{{\tt \underline{d}}}
\newcommand{\Varc}{{\tt Vc}}
\newcommand{\GVar}{{\tt GV}}
\newcommand{\DSubs}{{\tt c}}
\newcommand{\DSubsl}{{\tt c}^l}

\newcommand{\LAB}{{\tt LAB}}
\newcommand{\MIX}{{\tt MIX}}
\newcommand{\nLAB}{{\tt \neg LAB}}
\newcommand{\INT}{{\tt F}}
\newcommand{\EXT}{{\tt P}}

\newcommand{\uDSubs}{{\tt \underline{c}}}
\newcommand{\udis}{\underline{\dis}}
\newcommand{\out}{{\tt out}}
\newcommand{\outv}{{\tt out\overline{d}}}
\newcommand{\iinn}{{\tt in}}
\newcommand{\propi}{\prop {\tt i}}
\newcommand{\prope}{\prop {\tt e}}
\newcommand{\uprop}{\underline{\prop}}
\newcommand{\ualpha}{\underline{\alpha}}
\newcommand{\ue}{\underline{\e}}
\newcommand{\uee}{\underline{\underline{\e}}}
\newcommand{\inside}{\INT:{\tt i}}
\newcommand{\ldisboxed}{\modulo{\lam \dis_{\boxed}}{\e}}
\newcommand{\ldisunboxed}{\modulo{\lam \dis_{\unboxed}}{\e}}
\newcommand{\ldisunbox}{\modulo{\B,\Var,\DSubs,\unbox}{\e}}

\newcommand{\ldisout}{\lam \dis_{\out}}
\newcommand{\ldisprop}{\modulo{\lam \dis_{ \prop}}{\e}}
\newcommand{\ldisin}{\lam \dis_{\iinn}}
\newcommand{\disout}{\dis \out}
\newcommand{\disin}{\dis \iinn}
\newcommand{\disprop}{\dis \prop}
\newcommand{\dispropi}{\dis \propi}
\newcommand{\udisout}{\underline{\dis \out}}
\newcommand{\udisprop}{\underline{\dis \prop}}
\newcommand{\udisin}{\underline{\dis \iinn}}
\newcommand{\uedis}{\underline{{\tt e} \dis}}
\newcommand{\ueprop}{\underline{{\tt e} \prop}}
\newcommand{\uedisout}{\underline{{\tt e} \dis \out}}
\newcommand{\uedisin}{\underline{{\tt e} \dis \iinn}}
\newcommand{\uedisprop}{\underline{{\tt e} \dis \prop}}
\newcommand{\uledisprop}{\underline{\lam {\tt e} \dis \prop}}
\newcommand{\uledisprope}{\underline{\lam {\tt e} \dis \prope}}
\newcommand{\uludisprope}{\underline{\lam \dis \prope}}
\newcommand{\ludisout}{\lam\underline{\dis \out}}
\newcommand{\ludisprop}{\lam\underline{\dis \prop}}
\newcommand{\ludis}{\lam\underline{\dis}}
\newcommand{\uludisprop}{\underline{\lam \dis \prop}}
\newcommand{\ludisin}{\lam\underline{\dis \iinn}}
\newcommand{\ul}{\underline{\lam}}
\newcommand{\und}[1]{\underline{#1}}


\newcommand{\DSubsc}{{\tt Dupc}}
\newcommand{\DApp}{{\tt DupApp}}
\newcommand{\DSubst}{{\tt DupSubs}}
\newcommand{\SCS}{{\tt SC}}

\newcommand{\Gc}{{\tt w}}
\newcommand{\uGc}{{\tt \underline{w}}}
\newcommand{\uNew}{{\tt \underline{\New}}}
\newcommand{\Gcc}{{\tt Gcc}}

\newcommand{\Varx}{\underline{{\tt Var}}}
\newcommand{\Gcx}{\underline{{\tt SGc}}}
\newcommand{\Appix}{\underline{{\tt App}_1}}
\newcommand{\Appiix}{\underline{{\tt App}_2}}
\newcommand{\Lv}{{\tt El}}
\newcommand{\us}{{\tt R}}
\newcommand{\AC}{{\tt CC}_{\mathcal{A}}}
\newcommand{\CCb}{{\tt C}_{\mathcal{C}}}
\newcommand{\CC}{{\tt CC}_{\mathcal{C}}}
\newcommand{\CW}{{\tt WW}_{\mathcal{C}}}
\newcommand{\CS}{{\tt CS}}
\newcommand{\uCS}{{\tt \underline{CS}}}
\newcommand{\mCS}{{\tt mCS}}
\newcommand{\CWL}{\fabien{A modifier}}
\newcommand{\CWAa}{\fabien{ A modifier}}
\newcommand{\CWAb}{\fabien{ A modifier}}


\newcommand{\Cross}{{\tt CW_2}}


\newcommand{\Beta}{{\tt \beta}}
\newcommand{\RBeta}{\rightarrow_{{\tt \beta}}}
\newcommand{\Abs}{{\tt SL}}
\newcommand{\uAbs}{{\tt \underline{SL}}}
\newcommand{\sAbs}{{\tt SL}{'}}
\newcommand{\Appa}{{\tt SA_L}}
\newcommand{\uAppa}{{\tt \underline{SA}_L}}
\newcommand{\sAppa}{{\tt SA_L}{'}}
\newcommand{\Appb}{{\tt SA_R}}
\newcommand{\uAppb}{{\tt \underline{SA}_R}}
\newcommand{\sAppb}{{\tt SA_R}{'}}


\newcommand{\App}{{\tt App}}
\newcommand{\Appx}{\underline{\tt App}}
\newcommand{\DSubsx}{\underline{{\tt DSubs}}}

\newcommand{\Appi}{{\tt App}_1}
\newcommand{\Appii}{{\tt App}_2}
\newcommand{\Appiii}{{\tt App}_3}
\newcommand{\Comp}{{\tt Comp}}
\newcommand{\uComp}{{\tt \underline{Comp}}}
\newcommand{\sComp}{{\tt Comp}{'}}
\newcommand{\Compx}{\underline{\tt Comp}}

\newcommand{\Compi}{{\tt Comp}_1}
\newcommand{\Compii}{{\tt Comp}_2}
\newcommand{\Lamb}{{\tt Lamb}}
\newcommand{\Lambx}{\underline{\tt Lamb}}
\newcommand{\Weaka}{{\tt SW_1}}
\newcommand{\Weakc}{{\tt SW_2}}
\newcommand{\WAbs}{{\tt LW}}

\newcommand{\Axcut}{{\tt ax}\mbox{-}{\tt cut}}
\newcommand{\lparltimes}{\lpar\mbox{-}\ltimes}
\newcommand{\cutwb}{{\tt w}\mbox{-}{\tt b}}
\newcommand{\db}{{\tt d}\mbox{-}{\tt b}}
\newcommand{\cb}{{\tt c}\mbox{-}{\tt b}}
\newcommand{\wc}{{\tt U}}
\newcommand{\wb}{{\tt V}}
\newcommand{\bb}{{\tt b}\mbox{-}{\tt b}}
\newcommand{\re}{\rpn/\epn}
\newcommand{\rpn}{R}
\newcommand{\epn}{E}

\def\filepath{./figures/}

\newcommand{\dessinp}[2]{
  \scalebox{#1}{
    \input \filepath#2.eepic
  }
  }

\newcommand{\dessin}[1]{
  \scalebox{.60}{
    \input \filepath#1.eepic
  }
  }

\newcommand{\scon}[1]{[\! \! [ #1 ] \! \! ]}

\newcommand{\con}[4]{\mathcal{C}_{#1}^{#2|#3}(#4)}
\newcommand{\weak}[2]{\mathcal{W}_{#1}(#2)}
\newcommand{\so}[1]{\langle   #1 \rangle }
\newcommand{\eps}{\epsilon}
\newcommand{\ov}[1]{\overline{#1}}

\newcommand{\dom}{{\tt dom}}
\newcommand{\codom}{{\tt codom}}

\newcommand{\froml}[1]{ {\cal B}(#1)}
\newcommand{\fl}{ {\cal F}}
\newcommand{\fli}{ {\tt fl}_1}
\newcommand{\flii}{ {\tt fl}_2}
\newcommand{\fliii}{{\tt fl}_3}
\newcommand{\fliv}{ {\tt fl}_4}
\newcommand{\sq}{{\tt seq}}


\newcommand{\ns}{\# s}
\newcommand{\sm}{{\tt sm}}
\newcommand{\ms}{{\tt ss}}
\newcommand{\sr}{\lam {\tt nss}}
\newcommand{\ri}{{\tt n}_1}
\newcommand{\rii}{{\tt n}_2}
\newcommand{\riii}{{\tt n}_3}
\newcommand{\riv}{{\tt n}_4}
\newcommand{\resour}[1]{{\tt res}(#1)}
\newcommand{\nf}[1]{{\tt nf}(#1)}
\newcommand{\X}{\mathcal{ X}}
\def\RI#1#2{{#1}\;\mathcal{I}\;{#2}}
\def\LI{\Lambda_I}
\newcommand{\corner}[1]{\langle #1 \rangle}
\newcommand{\set}[1]{ \{ #1 \}}
\newcommand{\al}{{\tt A}}
\newcommand{\size}{{\tt size}}
\newcommand{\maxsize}[2]{{\tt maxk}_{#1}(#2)}
\newcommand{\type}{{\tt type}}
\newcommand{\nmul}{{\tt P}}
\newcommand{\mul}[2]{{\tt P}_{#2}(#1)}
\newcommand{\mest}[1]{\underline{#1}}
\newcommand{\mesd}[1]{{\tt mes}(#1)}
\newcommand{\mesdnovar}[2]{{\tt mes}_{\neg#1}(#2)}
\newcommand{\mesdvar}[2]{{\tt mes}_{#1}(#2)}

\newcommand{\scomp}[1]{{\tt S}(#1)}
\newcommand{\I}[1]{{\tt I}(#1)}
\newcommand{\La}[1]{{\tt L}(#1)}
\newcommand{\prof}{{ \tt P}}
\newcommand{\level}[2]{{ \tt level}(#1,#2)}
\newcommand{\nbw}{{\tt nbw}}
\newcommand{\nbb}[1]{\#(#1)}
\newcommand{\back}[1]{{\tt C}(#1)}
\newcommand{\nombreback}{{\tt C}}
\newcommand{\ems}{\emptyset}
\newcommand{\eml}{ [ \; ] }

\newcommand{\cs}[1]{{\tt cs}[\!\! [ #1 ]\!\! ]}
\newcommand{\csp}[2]{{\tt cs}_{[#1]}[\!\! [ #2 ]\!\! ]}
\newcommand{\csfl}[1]{{\tt cs}_{\fl}[\!\! [ #1 ]\!\! ]}
\newcommand{\cspfl}[1]{{\tt cs'}_{\fl}[\!\! [ #1 ]\!\! ]}
\newcommand{\comment}[1]{{\bf \Large  #1}}
\newcommand{\tob}{{\tt to}_{\lam}}
\newcommand\hrsv{{\mathcal V}}
\newcommand\hrst{{\mathcal T}}
\newcommand{\paralp}[1]{\Rrightarrow_{#1}}
\newcommand{\lparalp}[1]{\; \mbox{}_{#1}{\Lleftarrow}\ }
\newcommand{\paralpn}[1]{\Rrightarrow^{*}_{#1}}
\newcommand{\mX}{\mathbb{X}}
\newcommand{\mY}{\mathbb{Y}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\ALC}{{\tt ALC}}
\newcommand{\ih}{i.h.}
\newcommand{\wcap}[1]{[ \! \! [ #1 ] \! \! ]}

\renewcommand{\vec}[1]{\overline{#1}}
\newcommand{\capp}[1]{\cap_{#1}}
\newcommand{\add}{{\tt add}}
\newcommand{\mult}{{\tt mul}}
\newcommand{\addl}{{\tt add}_{\lam}}
\newcommand{\multl}{{\tt mul}_{\lam}}
\newcommand{\addls}{{\tt add}_{\ls}}
\newcommand{\multls}{{\tt mul}_{\ls}}
\newcommand{\cwc}[1]{{[} \! {[} #1 {]}  \! {]}}

\newcommand{\per}{\rightsquigarrow}
\newcommand{\pers}{\rightsquigarrow_{s}}
\newcommand{\eqdef}{=_{def}}


\newcommand{\perbase}{\mbox{{\tt \small (p-var)}}}
\newcommand{\perabs}{\mbox{{\tt \small (p-abs)}}}
\newcommand{\perappi}{\mbox{{\tt \small (p-app1)}}}
\newcommand{\perappii}{\mbox{{\tt\small  (p-app2)}}}
\newcommand{\perB}{\mbox{{\tt \small (p-B)}}}
\newcommand{\persubi}{\mbox{{\tt \small (p-subs1)}}}
\newcommand{\persubii}{\mbox{{\tt \small (p-subs2)}}}
\newcommand{\persubiii}{\mbox{{\tt \small (p-subs3)}}}
\newcommand{\persub}{\mbox{{\tt \small (p-subs)}}}
\newcommand{\persubsn}{\mbox{{\tt \small (p-subs1)}}}
\newcommand{\persubnsn}{\mbox{{\tt \small (p-subs2)}}}


\newcommand{\mes}[1]{{\tt S}(#1)}
\newcommand{\mesl}[1]{\mathcal{ L}(#1)}
\newcommand{\mesp}[3]{{\tt S}_{#1}^{#3}(#2)}
\newcommand{\terms}{\mathcal{T}}
\newcommand{\termsv}{\mathcal{T}_{\tt v}}
\newcommand{\termsaux}{\terms_{\tt aux}}
\newcommand{\termslab}{\mathbb{T}}
\newcommand{\termslambda}{\terms_{\lam}}
\newcommand{\lterms}{\mathbb{T}}
\newcommand{\tterms}[2]{ \mathcal{ T}_{#1}^{#2}}
\newcommand{\preterms}[1]{ \mathcal{ P}_{#1}}

\newcommand{\unl}[1]{{\tt U}(#1)}
\newcommand{\perm}{{\tt Per}}
\newcommand{\lamb}[1]{{\tt lam}_{#1}}
\newcommand{\sub}{{\tt sub}}
\newcommand{\lieur}[1]{{\tt mu}_{#1}}
\newcommand{\nka}{{ \tt k}}
\newcommand{\bon}[1]{{\tt b}(#1)}
\newcommand{\cl}[1]{[#1]}
\newcommand{\dep}[1]{{ \tt dep}(#1)}
\newcommand{\dr}[1]{{ \tt dr}(#1)}
\newcommand{\dm}[1]{\dis {\tt m}(#1)}

\newcommand{\cdep}[2]{{ \tt D}(#1, #2)}
\newcommand{\cdepu}[1]{{ \tt D}(#1)}
\newcommand{\cdept}[2]{\mathbb{D}(#1, #2)}
\newcommand{\ndep}{{ \tt dep}}
\newcommand{\arx}[2]{{ \tt af}_{#2}(#1)}
\newcommand{\narx}[1]{{ \tt af}_{#1}}
\newcommand{\nnarx}{{ \tt af}}
\newcommand{\sd}{\setminus}
\newcommand{\fsd}{\setminus \!\! \! \setminus}
\newcommand{\msd}{\fatbslash}
\newcommand{\nfin}[1]{{\tt in}(#1)}
\newcommand{\nnfin}{{\tt in}}

\newcommand{\inter}[1]{| \! | #1 | \! |}
\newcommand{\ls}{\lam_{\ts}}
\newcommand{\lsc}{\lam_{\tsc}}
\newcommand{\lls}{\lam{\underline{{\tt s}}}}
\newcommand{\llsi}{\lls^i}
\newcommand{\llse}{\lls^e}
\newcommand{\efo}[1]{{\tt exp}(#1)}
\newcommand{\fo}[2]{\ttv_{#2}(#1)}
\newcommand{\fop}[1]{\ttv_{#1}}
\newcommand{\xc}{{\tt nvs}}
\newcommand{\deft}[1]{{\bf #1}}
\newcommand{\Sr}{\EuScript{S}}
\newcommand{\Rr}{\EuScript{R}}
\newcommand{\Ar}{\EuScript{A}}
\newcommand{\Br}{\EuScript{B}}
\newcommand{\Brp}{\EuScript{B'}}
\newcommand{\emp}{\emptyset}
\newcommand{\pre}[1]{\langle #1 \rangle}
\newcommand{\Add}[2]{{\tt AR}_{#1}(#2)}
\newcommand{\Rem}[2]{{\tt RR}_{#1}(#2)}
\newcommand{\Ab}[1]{\Add{\Br}{#1}}
\newcommand{\Aa}[1]{\Add{\Ar}{#1}}
\newcommand{\As}[1]{\Add{\ts}{#1}}
\newcommand{\Aw}[1]{\Add{\tw}{#1}}
\newcommand{\nfw}{{\tt nfw}}
\newcommand{\nfc}{{\tt nfc}}
\newcommand{\Ac}[1]{\Add{\tc}{#1}}
\newcommand{\Jb}[1]{\Rem{\tb}{#1}}
\newcommand{\Ja}[1]{\Rem{\Ar}{#1}}
\newcommand{\Jw}[1]{\Rem{\tw}{#1}}
\newcommand{\Jbb}[2]{{\tt RR}_{\tw}^{#1}(#2)}
\newcommand{\Pc}[1]{{\tt Pc}(#1)}
\newcommand{\nPc}{{\tt Pc}}
\newcommand{\Jc}[1]{{\tt RR}_{\tc}(#1)}
\newcommand{\Rc}[1]{{\tt cs}(#1)}

\newcommand{\temoin}{{\tt temoin}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vxp}{\vec{x_1}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vxv}{\isubs{\vec{x}/ \vec{v}}}

\newcommand{\vxu}{[ \vec{x}/ \vec{u} ] }
\newcommand{\vxpu}{[ \vec{x_1}/ \vec{u} ] }
\newcommand{\vxpup}{[ \vec{x'}/ \vec{u'} ] }
\newcommand{\vyv}{[ \vec{y}/ \vec{v} ] }
\newcommand{\sxu}{\isubs{{x}/ {u}}}
\newcommand{\syv}{\isubs{{y}/ {v}}}
\newcommand{\rest}{{\tt compl}}
\newcommand{\regles}{\mathcal{ R}}
\newcommand{\equations}{\mathcal{ E}}
\newcommand{\brc}{BBBB}
\newcommand{\PO}[1]{{\tt PO}(#1)}
\newcommand{\tapp}[2]{{\tt app}(#1,#2)}
\newcommand{\tsub}[2]{{\tt sub}(#1,#2)}
\newcommand{\tabs}[1]{{\tt abs}(#1)}
\newcommand{\tcon}[1]{{\tt con}(#1)}
\newcommand{\tweak}[1]{{\tt weak}(#1)}
\newcommand{\grpo}{>_{rpo}}
\newcommand{\vi}[1]{\vec{x_{ #1 }}}
\newcommand{\indsub}[4]{[\vec{#1}/\vec{#2}]^{#3}_{#4}}
\newcommand{\iindsub}[4]{\{ \vec{#1}/\vec{#2} \} ^{#3}_{#4}}
\newcommand{\idindsub}[4]{\{ \! \! \{ \vec{#1}/\vec{#2} \} \! \! \} ^{#3}_{#4}}
\newcommand{\ev}[2]{#1_{[#2]}}
\newcommand{\soitdans}[3]{{\tt let}\ #1 = #2\ {\tt in}\ #3}
\newcommand{\Soitdans}[5]{{\tt let}\ #1 = #2\ \mbox{{\tt and let}}\ #3 = #4\ {\tt in}\ #5}
\newcommand{\pw}{\uparrow^{\mathcal{W}}}
\newcommand{\isubst}[1]{ \{ #1  \}  }
\newcommand{\disubst}[1]{ \{ \! \!  \{ #1  \} \! \! \} }
\newcommand{\dvxv}{\disubst{\vec{x}/ \vec{v}}}
\newcommand{\dvxu}{\disubst{\vec{x}/ \vec{u}}}

\newcommand{\esn}{[\vec{x}/\vec{u}]}
\newcommand{\isn}{\disubst{\vec{x}/\vec{u}}}
\newcommand{\ef}[1]{{\tt E}(#1)}
\newcommand{\w}[2]{{\tt f}_{#1}(#2)}
\newcommand{\sRen}[3]{{\tt S}^{#2}_{#3}(#1)}
\newcommand{\iep}{{\bf IE}}
\newcommand{\wiep}{{\bf WIE}}
\newcommand{\giep}{{\bf GIE}}
\newcommand{\viep}{{\bf VIE}}
\newcommand{\CB}{C^{\B}}
\newcommand{\Cw}{t^{w}}
\newcommand{\Cd}{t^{d}}
\newcommand{\Cc}{t^{c}}

\newcommand{\SupDev}[1]{#1^{\circ\circ}}
\newcommand{\Dev}[1]{#1^{\circ}}
\newcommand{\esDev}[1]{#1^{\bullet}}
\newcommand{\esDevP}[1]{#1^{\bullet\bullet}}
\newcommand{\fc}{\dis}
\newcommand{\wfc}{\Gc\dis}
\newcommand{\fcNF}[1]{\fc (#1)}
\newcommand{\sBNF}[1]{\shB(#1)}
\newcommand{\BNF}[1]{\B(#1)}
\newcommand{\VarE}{{\tt m\Var}}
\newcommand{\TTNF}[1]{\TypTwo(#1)}
\newcommand{\TypTwo}{{\tt A}}
\newcommand{\DS}[1]{{\tt S}(#1)}
\newcommand{\DD}[1]{{\tt D}(#1)}
\newcommand{\paral}{\gg}
\newcommand{\RL}[1]{{\tt L}(#1)}
\newcommand{\VF}{{\tt V-Form}}
\newcommand{\AF}{{\tt A-Form}}
\newcommand{\LF}{{\tt L-Form}}

\newcommand{\jop}{{\tt j}}


\newcommand{\katerms}[1]{{ \tt K}(#1)}
\newcommand{\ka}[1]{ \mathbb{K}(#1)}
\newcommand{\ind}{[\ov{x}/\ov{u}]}
\newcommand{\nind}{[\ov{y}/\ov{v}]}
\newcommand{\push}{{\cal P}}
\newcommand{\pushR}{{\cal R}}
\newcommand{\dout}{{\tt out}}

\newcommand{\neutral}[2]{#1^{\overline{#2}}}
\newcommand{\sensible}[2]{#1^#2}

\newcommand{\orient}{{\tt r}}
\newcommand{\usig}{\underline{\sig}}
\newcommand{\sign}{\sig^{-}}
\newcommand{\gam}{\gamma}
\newcommand{\ugam}{\underline{\gamma}}
\newcommand{\urho}{\underline{\rho}}
\newcommand{\kao}[1]{{\mathbb Ko}(#1)}
\newcommand{\kai}[1]{{\mathbb Ki}(#1)}
\newcommand{\nl}{\B\dis\prop\e}
\newcommand{\nb}[1]{{\tt nss}(#1)}


\newcommand{\WB}{\tt WB}
\newcommand{\DB}{\tt DB}
\renewcommand{\CB}{\tt CB}

\newcommand{\ux}{\underline{x}}
\newcommand{\uy}{\underline{y}}
\newcommand{\uz}{\underline{z}}
\newcommand{\px}{? x}
\newcommand{\py}{? y}
\newcommand{\pz}{? z}

\newcommand{\substitution}{{\tt s}}
\renewcommand{\boxed}{{\tt b}}
\newcommand{\unboxed}{{\tt u}}
\newcommand{\lboxed}{\underline{{\tt b}}}
\newcommand{\lunboxed}{\underline{{\tt u}}}
\newcommand{\lunbox}{\underline{{\tt u}}}
\newcommand{\abox}{\app\boxed}
\newcommand{\sbbox}{\substitution\boxed}
\newcommand{\aubox}{\app\unboxed}
\newcommand{\sbubox}{\substitution\unboxed}
\newcommand{\labox}{\underline{\app\boxed}}
\newcommand{\lsbbox}{\underline{\substitution\boxed}}
\newcommand{\laubox}{\underline{\app\unboxed}}
\newcommand{\lsbubox}{\underline{\substitution\unboxed}}

\newcommand{\SF}{{\tt SF}}
\newcommand{\sif}[1]{{\tt sf}(#1)}
\newcommand{\wf}[2]{{\tt SN}\mathbb{L}(#1,#2)}
\newcommand{\nwfp}{{\tt SN}\mathbb{L}}
\newcommand{\wfp}[3]{{\tt SN}\mathbb{L}_{#1}(#2,#3)}
\newcommand{\wfu}[1]{{\tt SN}\mathbb{L}(#1)}
\newcommand{\wft}[2]{{\tt ST}(#1,#2)}
\newcommand{\wflt}{\mathbb{W}\mathbb{F}}
\newcommand{\true}{{\tt true}}
\newcommand{\mb}[1]{\mathbb{B}(#1)}
\newcommand{\mbi}[2]{\mathbb{B}\mathbb{I}(#1,#2)}
\newcommand{\lprop}{\underline{{\tt p}}}
\newcommand{\slist}{{\tt L}}
\newcommand{\lslist}{\mathbb{L}}
\newcommand{\snlabelled}{{\tt SN}-labelled}
\newcommand{\modulo}[2]{#1/#2}
\newcommand{\vars}[1]{{\tt var}(#1)}
\renewcommand{\sp}[4]{\{#1_{i}/#2_{i}\}^{#3}_{#4}}
\newcommand{\esp}[4]{[{#1}_i/{#2}_i]^{#3}_{#4}}
\newcommand{\espv}[4]{[\void/{#2}_i]^{#3}_{#4}}
\newcommand{\espw}[4]{[{#1}_i/\proj({#2}_i)]^{#3}_{#4}}
\newcommand{\ovl}[3]{\overline{#1}^{#2}_{#3}}
\newcommand{\jump}{jump}
\newcommand{\jumps}{jumps}
\newcommand{\boxing}{boxing}
\newcommand{\Boxing}{Boxing}
\newcommand{\unboxing}{unboxing}
\newcommand{\Unboxing}{Unboxing}
\newcommand{\jumping}{jumping}
\newcommand{\maxi}[2]{{\tt max}(#1,#2)}
\newcommand{\mmax}[1]{\maxi{1}{#1}\cdot}

\newcommand{\development}{development}
\newcommand{\Development}{Development}
\newcommand{\superdevelopment}{{\tt L}-\development}
\newcommand{\Superdevelopment}{{\tt L}-\development}
\newcommand{\supersuperdevelopment}{{\tt XL}-\development}
\newcommand{\Supersuperdevelopment}{{\tt XL}-\development}

\newcommand{\WC}{{\tt WC}}
\newcommand{\Weaki}{{\tt Weak1}}
\newcommand{\Weakii}{{\tt Weak2}}
\newcommand{\WPush}{{\tt WPush}}
\def\LI{\Lambda_I}
\def\nRI{\mathcal{I}}
\newcommand{\tat}[2]{#1(#2)}
\newcommand{\arrow}{\rightarrow}
\newcommand{\emul}{[ \, ] }
\newcommand{\pder}[1]{\Vdash_{#1}}
\newcommand{\der}{\vdash}
\newcommand{\Set}{\mathcal{ S}}
\newcommand{\union}{\odot  }
\newcommand{\K}{{\tt K}}
\newcommand{\unbox}{{\tt u}}
\newcommand{\unboxp}{{\tt u}^+}
\newcommand{\unboxn}{{\tt u}^-}
\newcommand{\modn}{\modulo{\unboxn}{\en}}
\newcommand{\uunbox}{\underline{{\tt u}}}
\newcommand{\gunbox}{\underline{{\tt gu}}}
\newcommand{\gbeta}{\beta_g}
\newcommand{\Bne}{\B, \fcnogc}
\newcommand{\gbetam}{\modulo{\beta_g,\unboxn}{\en}}
\newcommand{\z}{\bullet}
\renewcommand{\P}[1]{{\tt P}(#1)}
\newcommand{\fcnogc}{{\tt fc}^+}
\newcommand{\cI}[1]{\; \mathcal{I}_{#1}\ }


\newcommand{\weakmes}[1]{|#1|_{\nGc}} 
\newcommand{\nGc}{{\neg\Gc}}
\newcommand{\nGcl}{{\neg\Gc}^l}
\newcommand{\pdfs}[2]{\texorpdfstring{#1}{#2}}
\newcommand{\espp}[4]{[\cdot]^{#3}_{#4}}
\newcommand{\spp}[4]{\{\cdot\}^{#3}_{#4}}
 \newcommand{\ignore}[1]{}
\newcommand{\mellies}{Melli{\`e}s}
\newcommand{\sigeq}{\equiv_{\sigma}}
\newcommand{\eqw}[1]{\equiv_{#1}}
\newcommand{\eqo}{\equiv_\osym}
\newcommand{\eqttE}{\equiv_{\ttE}}
\newcommand{\preeq}{\sim}
\newcommand{\preeqw}[1]{\sim_{#1}}
\newcommand{\lunb}{\hat\unboxed}
\def\rsig{\hat{\sigma}}
\newcommand{\runboxed}{\hat{\unboxed}}
\newcommand{\oline}[1]{{\overline{#1}}}
\newcommand{\osym}{{\tt o}}
\newcommand{\eos}{\equiv_{\osym}}
\newcommand{\preeqsig}{\preeq_{\sigma}}
\newcommand{\preeqsigu}{\preeq_{\sigma_1}}
\newcommand{\preeqsigt}{\preeq_{\sigma_2}}
\newcommand{\eqsig}{\equiv_{\sigma}}
\newcommand{\eqsigu}{\equiv_{\sigma_1}}
\newcommand{\eqsigt}{\equiv_{\sigma_2}}
\newcommand{\ldiso}{\ldis/{\osym}}
\newcommand{\ldisttE}{\ldis/{\ttE}}
\newcommand{\ldisf}{\lam \modulo{\dis}{\fsymb}}
\newcommand{\ldisfS}{\ldis/{\fsymb\Sigma}}
\newcommand{\ldisfz}{\ldis/\fz}
\newcommand{\fsymb}{\osymb\boite}
\newcommand{\osymb}{{\tt o}}
\newcommand{\eqf}{\eqw{\fsymb}}
\newcommand{\eqfz}{\equiv_\fz}
\newcommand{\fz}{{\tt n}}
\newcommand{\aux}{{\tt void}}
\newcommand{\laux}{\lam\aux}
\newcommand{\lauxm}{\lam\modulo{ \aux }{ \osymb }}
\newcommand{\New}{{\tt h}}
\newcommand{\nNew}{\neg\New}
\newcommand{\mlist}{{\tt M}}
\newcommand{\snsu}[1]{\mathbb{S}(#1)}
\newcommand{\snsudd}[2]{\mathbb{SNSJ}_{#1}(#2)}
\newcommand{\lsnsu}[1]{\mathbb{L}\mathbb{S}(#1)}
\newcommand{\snsud}[2]{\mathbb{S}(#1,#2)}
\newcommand{\ctx}[2]{#1 [ \! [#2] \! ]}
\newcommand{\bs}[1]{{\tt bs}(#1)}
\newcommand{\ctxu}[2]{\mathbb{#1} \{  \! \{  #2 \}  \! \} }
\newcommand{\nil}{nil}
\newcommand{\etam}{\mathbb{M}}
\newcommand{\etamd}[2]{\mathbb{MSJ}_{#1}(#2)}
\newcommand{\letam}{\mathbb{L}\mathbb{M}}
\newcommand{\st}{{\tt st}}
\newcommand{\eqcs}{\equiv_\CS}
\newcommand{\List}{{\tt L}}
\definecolor{LightGray}{gray}{.80}
\definecolor{DarkGray}{gray}{.60}
\newcommand{\grisar}[1]{\colorbox{LightGray}{\ensuremath{#1}}}
\newcommand{\grisarOscuro}[1]{\colorbox{DarkGray}{\ensuremath{#1}}}
\newcommand{\surf}[2]{\mathbb{T}_{#1}(#2)}
\newcommand{\preetamd}[2]{\mathbb{SJ}_{#1}(#2)}
\newcommand{\eA}{{\mathbb A}}
\newcommand{\inn}{\modulo{{\tt in}}{\CS}}
\newcommand{\innn}{{\tt in}}
\newcommand{\comp}{{\tt comp}}
\newcommand{\outm}{\modulo{{\tt out}}{\CS}}
\newcommand{\lsub}{\lam {\tt sub}}

\newcommand{\boite}{{\tt box}}
\newcommand{\sigt}{\boite_1}
\newcommand{\sigq}{\boite_2}
\newcommand{\rsigt}{\widehat{\boite}}
\newcommand{\void}{\_}


\newcommand{\gm}{\sqsupset}
\newcommand{\geqm}{\sqsupseteq}
\newcommand{\subt}{\triangleleft}
\newcommand{\surt}{\triangleright}

\newcommand{\psymb}{{\tt P}}
\newcommand{\eqp}{\eqw{\psymb}}
\newcommand{\Psymb}{\Pi}
\newcommand{\eqP}{\eqw{\Psymb}}
\newcommand{\mc}[1]{{\tt M}(#1)}
\newcommand{\mt}[2]{{\tt T}(#1)_{#2}}

\newcommand{\hole}{\Box}

\title[PSN  modulo permutations for 
the structural lambda calculus]{Preservation of Strong Normalisation modulo permutations for 
the structural -calculus}

\author[B.~Accattoli]{Beniamino Accattoli\rsuper a}
\address{{\lsuper a}INRIA Saclay and LIX (\'Ecole Polytechnique)}
\email{beniamino.accattoli@gmail.com}
\author[D.~Kesner]{Delia Kesner\rsuper b}
\address{{\lsuper b}Univ. Paris Diderot, Sorbonne Paris Cit\'e, PPS, CNRS}
\email{delia.kesner@pps.jussieu.fr}

\keywords{Lambda-calculus, explicit substitutions, preservation of strong normalisation}
\subjclass{F.3.2, D.1.1, F.4.1}

\begin{document}

\begin{abstract}
Inspired by a recent  graphical formalism for -calculus based on
linear   logic  technology,   we  introduce   an   untyped  structural
-calculus, called  , which  combines actions at  a distance
with  exponential  rules  decomposing  the substitution  by  means  of
weakening,  contraction  and   derelicition.   First,  we  prove  some
fundamental properties of   such as confluence and preservation
of   -strong  normalisation.    Second,  we   add   a  strong
bisimulation  to    by  means  of an  equational  theory  which
captures in particular Regnier's -equivalence.  We then complete
this  bisimulation   with  two  more   equations  for
(de)composition  of  substitutions and  we  prove  that the  resulting
calculus  still preserves  -strong normalization.   Finally, we
discuss some  consequences of our results.
\end{abstract}


\maketitle


\section*{Introduction}



Linear Logic~\cite{LL} has been very influential in computer science,
especially because it provides a tool to explicitly control the use of
resources by limiting the use of the \textit{structural rules} of
weakening and contraction.  Erasure (weakening) and duplication
(contraction) are restricted to formulas marked with an
\textit{exponential} modality, and can
only interact with non-linear proofs marked with a
\textit{bang} modality.  Intuitionistic
and Classical Logic can thus be encoded by a fragment containing such
modalities as, for example, the Multiplicative Exponential Linear
Logic (MELL).



MELL proofs can be represented by sequent trees, but MELL
Proof-Nets~\cite{LL} provide a better geometrical representation of
proofs, eliminating irrelevant syntactical details.  They have been
used extensively to develop different encodings of intuitionistic
logic/lambda-calculus, giving rise to the geometry of
interaction~\cite{GOI}.

Normalisation of proofs (\ie\ \textit{cut elimination}) in MELL
Proof-Nets is performed in particular by \textit{exponential} and
\textit{commutative} rules.  Non-linear proofs are distinguished by
surrounding \textit{boxes}; the exponential rules handle all the
possible operations on them: erasure, duplication and linear replacement,
corresponding respectively to a cut elimination step involving a box
and either a \textit{weakening}, a \textit{contraction} or a
\textit{dereliction}. The commutative rule instead \textit{composes}
non-linear resources.




Different cut elimination systems~\cite{DCKP03,KL07,Kes07}, defined as
\textit{explicit substitution} (ES) calculi, were explained in terms
of, or were inspired from, the fine notion of reduction of MELL
Proof-Nets. They all use the idea that the content of a
substitution/cut is a non-linear resource, \ie\ a box that can be
composed with another one by means of some commutative rules.
They also share common operational semantics defined in terms of
a \textit{propagation system} in which a substitution traverses a term
until the variables are reached.\medskip
  

\deft{The  structural -calculus}.  A graphical  representation for
-terms,  s, has  been  recently proposed~\cite{AG09}.   It
denies  boxes  by  representing  them  with  additional  edges  called
\textit{{\tt j}umps}, and does not need any commutative reduction rule
to  compose non-linear  proofs.  This  paper studies  the term
formalism,  called  -calculus,   resulting  from  reading  back
s  (and  their correspondent  reductions)  by  means of  their
sequentialisation  theorem~\cite{AG09}.  The  deep connection  between
s   and   Danos    and   Regnier's   Pure   (\textit{untyped})
Proof-Nets~\cite{Danos99opt}     has      been     already     studied
in~\cite{AccattoliTh}.

Beyond this  graphical and logical interpretation,  the peculiarity of
-calculus  is  that  it  uses  two 
features  which  were  never  combined before:  \textit{action  at  a
  distance} and \textit{multiplicities}.

\textit{Action at a distance} means
  that rewriting rules are specified by means of
  some constructors which are arbitrarily far away from each other. This
approach could be understood as inconvenient
but this is only apparent because rewriting rules can be
locally implemented by means of s. The distance rules of  do not propagate
substitutions through the term except
for the linear ones which are evaluated exactly as meta-level substitutions,
regardless the distance between the  involved constructors (variable and jump).

\textit{Multiplicities} are intended to count the number of
occurrences of a given variable affected by a jump,
  \ie\ the  rewriting rule to be applied for reducing a term of the
form  depends on , the number
  of free occurrences of the variable  in the term
  . Indeed, we distinguish three cases,
,  and , which
correspond, respectively,  to 
 weakening-box, dereliction-box and contraction-box
cut-elimination rules in Proof Nets.
It is because of the weakening and
contraction rules that we call our language the \textit{structural}
-calculus.\medskip


\deft{Content of the paper}.
We start by showing that  admits a simple and elegant
theory \ie\  it enjoys  confluence, full composition (FC), and preservation of
-strong normalisation (PSN). The proof of PSN is particularly
concise because of the distance approach.

The main result of the paper is that the theory of  admits a
modular extension with respect to propagations of jumps:
an equational theory is added on top
of  and the obtained extension is  shown to
preserve all the good properties we mentioned
  before. Actually, we focus on PSN, since FC and confluence for the
extended -calculus result as straightforward.

In the literature there is a huge number of calculi with expicit
substitutions,  constructs or environments, most of them
use some rule to specify commutation (also called
propagation or permutation). In order to encompass these formalisms we
do not approach propagations as \textit{rewriting rules}, but as
equations (which can be used from left to right or vice-versa)
  defining an \textit{equivalence relation}  on terms.

This is only possible because propagations are
not needed in  to compute normal forms, a fact which is a by-product
of the
  \textit{distance} notion.  Moreover, any particular orientation
of the equations (from left to
  right \textit{or} from right to left) results in a terminating
rewriting relation, which implies that the system containing \textit{any} orientation of the 
equations still enjoys PSN.

Equations are introduced in two steps.  We first consider commutations
between independent jumps
and between jumps and  abstractions or left sides of applications. 
This equivalence, written , turns out to be a strong
bisimulation, \ie\ a 
reduction  relation
which is length preserving; thus PSN
for the reduction system  modulo  --- noted  --- immediately follows. We also
show that  can be seen as a projection of Regnier's
-equivalence~\cite{regnier94} on a syntax with
jumps. Actually,  can be understood as the quotient induced by
the translation~\cite{AccattoliTh} of -terms to Pure
Proof-Nets, which is why it is so well-behaved, and why we call it the
\textit{graphical equivalence}.


The second step is to extend  with general commutations between
jumps and right sides of applications and
contents of jumps.  
 The resulting \textit{substitution equivalence } does not
  only subsume \textit{composition} of jumps, but also
  \textit{decomposition}.  The equations of  correspond
  exactly to the commutative box-box case of Proof-Nets, but they are
  here considered as an \textit{equivalence} --- which is a novelty ---
  and not as a rewriting rule. The reduction relation of  is a
  rich rewriting system with subtle behaviour, particularly because
   affects reduction lengths, and thus is not a strong
  bisimulation. Nonetheless, we show that  enjoys PSN.

This result is non-trivial, and constitutes the main contribution of the paper.
The technique used to obtain PSN for  consists in
\begin{enumerate}
\item Projecting  reductions into a calculus that we call ,
\item Proving PSN for ,
\item Infering PSN for  from (1) and (2).
\end{enumerate}
Actually,  can be understood as a \textit{memory}
calculus specified by means of \textit{void} jumps --- \ie\ jumps  where  --- 
which generalises Klop's -calculus~\cite{Klo}. 
Despite 
the fact that it  appears only
as a technical tool we claim that 
it is a calculus interesting
on its own and can be used for proving termination
results beyond those of this paper.

The last part of the paper presents some interesting consequences of
our main result concerning different variations on
.  \medskip

\deft{Road Map}.

\begin{enumerate}[]
\item Section~\ref{sec:general-notion} recalls some general 
notions about abstract rewriting.

\item Section~\ref{s:structural-lj} presents the -calculus and 
  shows  that  it   enjoys  basic properties such as full composition,
  simulation   of   one-step
  -reduction, and confluence.

\item Section~\ref{s:lj-psn} studies    preservation of 
-strong normalisation (PSN). The     PSN    property is proved   using     a   modular  
  technique developed in~\cite{Kes09}, which  results in a
  very short  formal argument in our  case. 
 
\item Section~\ref{s:eq-th} first considers  
  enriched with the 
  equivalence , which is related to Regnier's
  -equivalence~\cite{regnier94},  and 
then with the equivalence , which also contains composition of
  jumps.

\item Section~\ref{sec:psn} is devoted to the proof of PSN for  modulo ,
which  is the main  contribution of the  paper.

\item Section~\ref{s:cons} discusses some consequences of the PSN result of
  Section~\ref{sec:psn}.
\end{enumerate}

This paper
covers some basic results in~\cite{AK10} by extending them
considerably. Indeed, the propagation systems considered
in~\cite{AK10} are just particular cases of the general equational
theory  studied in this paper. The proof technique used here to
show PSN for  modulo 
puts in evidence another calculus  that has interest in itself.   
Moreover, interesting consequences of the main result
are included in Section~\ref{s:cons}.  \medskip




\ignore{
One of the interesting features of  is that no rule
  of   propagates cuts, as the constructors  in a term
interact \textit{at  a distance}, \ie\   they work modulo  positions of
cuts.    Action   at   a   distance   is   not   a   complete
novelty~\cite{Milner07,deBruijn87,Ned92},  but none of  the previous
approaches  faithfully  reflects the  behaviour  of control  of
  resources in Linear Logic.   We propose to recognise this behaviour
as a new  paradigm, more primitive than ES, particularly because propagations can be further
  added on top  of the action at a distance  system (as we shall show in this paper).  Despite  the absence  of 
commutative  rules in  ,  cuts can be composed,  but  in a  new
(more natural)  way.

Similarly to formalisms~\cite{KR09} inspired by Proof-Nets, cut
elimination is defined in  terms of the  number of free  occurrences of
variables  in  a  term,  called  here  \textit{multiplicities}.   More
precisely,   the  weakening-box  rule  (resp.  dereliction-box  and
contraction-box)  handles terms  of the  form   when 
(resp.     and  ).   The  computation  is  however
performed  without  propagating  ,   which  is  here  called  a
\textit{\jump} to stress that the action at a distance 
is deeply different from  the propagation
in  ES calculi.  The reduction  rules of
 then  combines actions at  a distance,
due to the graphical flavour, and exponential rules, due to the
strong affinity with Proof-Nets. It is because of the weakening and contraction rules that we call our language the \textit{structural} -calculus.




Some  calculi employing distance or
multiplicities already exist, but not  both, so that it is  not possible to disclose
the full power  of their combination. Indeed, ~\cite{deBruijn87,Ned92}
use   distance  rules   to  refine   -reduction,  but   
ES is  added to the syntax, and no
distinction between dereliction and  contraction is considered.  This causes
the  formalism to  be less  expressive  than   as discussed  in~\cite{AK10}.    Milner
defines     a     -calculus      with     ES     inspired     from
another graphical formalisms,  Bigraphs~\cite{Milner07},  where  cuts   act   also  at   a
distance.  Again,  neither   a  distinction  between  dereliction  and
contraction, nor a  notion of application of functions  at distance is
given.   Same                    remarks
about~\cite{severi94definitions,OConchuir06}.
}

\deft{Related Work}.  Action at a distance has already been used
in~\cite{Milner07,deBruijn87,Ned92}, but none of the previous
approaches takes advantage of distance plus control of resources by
means of multiplicities.  Other works use multiplicities~\cite{KR09}
but not distance so that the resulting formalism contains a lot of
rules, which is really less manageable.  We think that our combined
approach is more primitive than ES, and the resulting theory is much
simpler.  Using distance \textit{and} multiplicities also provides
modularity: the substitution rules become independent from the set of
constructors of the calculus, and thus any change in the language does
not cause any changes in the associated rewriting rules. Our combined
approach does not only capture the well-known notions of
developments~\cite{Hindley78} and superdevelopments~\cite{KvOvR93}, but also allows us to
introduce \supersuperdevelopment s, a more powerful
notion of development defined in~\cite{AK10}.

In the literature there are many calculi which dealt with permutations
of constructors in intuitionistic calculi, but all use \textit{reduction} rules
rather than \textit{equations}, which is less powerful. 
Some  that can be captured by our graphical equivalence appear
in~\cite{Kamareddine00, regnier94,KfouryW95}
and those captured by our substitution equivalence
are~\cite{espiritoSanto2011,HZ09,Yoshida93}.
Intuitionistic calculi inspired from Linear Logic Proof Nets
appear for example in~\cite{KL05,Kes09,KR09}.

\section{Preliminary notions}
\label{sec:general-notion}

As several reduction notions are used along the paper, we first
introduce general definitions of rewriting. \medskip 

A \deft{reduction system} is a pair  consisting of a set 
and a binary relation  on  called a \deft{reduction relation}. When  we write
 and we say that  \deft{-reduces} to . 
The inverse of  is written
, \ie\  iff
. The reflexive and transitive (resp. transitive) closure
of  is written  (resp. ). Composition
of relations is denoted by juxtaposition. 
 Given , we write  iff 
 is -related to  in  steps, \ie\  if  and  if  s.t.   and . 


Given a \deft{reduction system} ,  we use the following reduction notions: 
\begin{enumerate}[]
  \item  is \deft{locally confluent} if 
,  \ie\ 
if  and , then  s.t.
 and .
 \item  is \deft{confluent} if ,  \ie\ 
if  and , then  s.t.
 and .
\item  is in \deft{-normal form}, written  \deft{-nf}, if there is no  such that .  
\item  has an \deft{-normal form} iff there exists -nf
such that . 
When   has a \textit{unique} -normal form, this one is denoted by .
\item  is  \deft{-weakly normalizing}, written ,
iff  has an  -normal form.
\item     is  \deft{-strongly  normalizing} or
\deft{-terminating},   written  , if  there is no infinite  -reduction sequence starting
  at  .
 \item    is  \deft{-finitely branching}
       if the set  is finite.
\item If  is  -strongly  normalizing
and -finitely branching
then   
denotes  the
  \deft{maximal length of an -reduction sequence starting at }.
  This notion is  extended to  lists of terms
   by .
\item  is \deft{weakly normalizing} (resp. \deft{strongly  normalizing} or \deft{terminating})
if every  is.
\end{enumerate}

A \deft{strong bisimulation between} two
reduction
systems 
 and  is a
relation  s.t. for any pair
:
\begin{enumerate}[]
   \item If  then 
        there is   s.t. 
         and , and conversely:
   \item If  then there is  s.t.  and .
  \end{enumerate}

A  \deft{strong bisimulation for } is a 
strong bisimulation between  and itself.
In particular we shall make use of the following property 
 whose proof is straightforward:

\begin{lem}
\label{l:lamj-bis-psn-dif-sys}
Let  be a strong bisimulation between two reduction systems 
 and . 
\begin{enumerate}
  \item The relation  preserves reduction lengths, \ie\ for any  
\begin{enumerate}[]
\item If  then 
       s.t.  and .
\item If  then  
       s.t.  and .
\end{enumerate}
\item \label{l:sbisim-pres-psn}The relation  preserves strong normalization, \ie for any  if and only if .
\end{enumerate}
\end{lem}


Given a reduction relation   and
an equivalence relation  both on , the reduction relation ,
called \deft{reduction  modulo }, is defined by
 iff .


\begin{lem}
\label{l:bisim-conf}
Let  be a strong bisimulation for 
. Then, 
\begin{enumerate}
  \item \label{l:bisim-conf-i} The relation   can be postponed
w.r.t , \ie\ 
.

  \item \label{l:bisim-conf-ii} If  is confluent then  
   is
    confluent.
\item \label{l:bisim-conf-iii}  If , then .
\end{enumerate}
\end{lem}


\begin{proof}
Point~\ref{l:bisim-conf-i} is straightforward by induction on the
length of  using the definition of strong
bisimulation. Points~\ref{l:bisim-conf-ii} and~\ref{l:bisim-conf-iii}
follow from Point~\ref{l:bisim-conf-i}.
\end{proof}


We conclude this section by giving an abstract theorem that we will
use to prove strong normalisation for different notions of reduction
modulo.

 
\begin{thm}[Termination for reduction modulo by interpretation]
\label{t:equational-abstract}
 Let consider three reduction systems ,    
and . Let 
 (resp. ) be an equivalence on  (resp. ). 
Consider a relation . Suppose
that for all 


\begin{enumerate}[\bf(P1)]
\item[{\bf (P0)}]  imply  
      s.t. .
\item[{\bf (P1)}]  imply   
      s.t. .
\item[{\bf (P2)}]  imply   
      s.t. .
\item[{\bf (P3)}] The reduction relation  is  
terminating.
\end{enumerate}

Then,  imply  .
\end{thm}

\begin{proof}
Suppose . Then, there is an
infinite -reduction sequence starting at ,
and since  is a terminating reduction  relation by
, this reduction has necessarily the form:

And can be projected by ,  and  into an infinite  reduction sequence as follows:


Since , then we get a contradiction. 
  \end{proof}



\section{The structural -calculus}
\label{s:structural-lj}


We introduce in this section the structural -calculus,
which can simply be understood as a refinement of  -calculus.  To be self-contained, 
we start this section by recalling the syntax and semantics of -calculus. 
The set of -terms,  written  ,
is generated by the following grammar:

Dynamics of -terms is   given by -reduction (noted ) 
which is  defined as the closure by contexts of
the following reduction rule:
  
where the meta-operation  on -terms is just a particular case
of the meta-operation on -terms given below.



The \deft{structural -calculus} is given by a set of
terms and a set of reduction rules. The set of -terms,  written  ,
is generated by the following grammar:


The term  is \deft{variable},  an \deft{abstraction}, 
an \deft{application} and  a \deft{substituted term}. The object
, which is not a term, is called a \deft{jump}.  The terms  and  bind  in , \ie\ the sets of \deft{free/bound
  variables} of a term are given by the following definitions:
 
A jump  in a
term  is called \deft{void} if .  The
equivalence relation generated by the renaming of bound variables is
called \deft{-conversion}.  Thus for example .  
The notation  is used for the empty sequence of terms if  and
for the sequence  otherwise;  
means that all the elements of the sequence belong to the set .
If  we use  for the term  if  and 
 otherwise; similarly,  
denotes the term  if  and 
 otherwise; 
 denotes the application 
; 

The meta-level \deft{substitution} operation is defined by
induction on terms by using the following equations on -equivalence classes:
 

We write  or  when  is a (strict) subterm of .
\deft{Positions} of terms are defined as expected (see~\cite{Terese03}, p. 643, for details);   denotes
the \deft{subterm of  at position }
and  denotes the \deft{set of all the positions}  of
 s.t. . 


We use  to denote the size of . We write  for the
number of free occurrences of the variable  in the term ,
called the \deft{multiplicy of  in }. We extend this notion to sets of variables by
  . 
A key notion used to define the semantics 
of the -calculus is that of renaming:
given a term  and a subset
, we write  for
the term  verifying  if 
and  if . Thus for example,
. 

When , we write  for any
\deft{non-deterministic replacement} of 
occurrences of  in  by a {\it fresh} variable , \ie\
 denotes  any term  s.t. 
 and . 
 Thus for
example,  may denote  or
 but not .

\deft{Contexts} are generated by the following grammar:

We write  to denote the term obtained by replacing the
hole  in  by the term . Thus for example  
 (remark that 
capture of variables is possible).

The  \deft{binding set} of a context 
is defined as follows: 
 


We now consider the
rewriting rules of the structural -calculus
(Figure~\ref{f:lambdaj}), which decompose the -rule into a
finer set of rules. The letter  in the rule 
  denotes a list  of jumps with
   (so potentially ) such that .  The  rule extends the usual  rule
   by allowing to introduce some
  distance between the abstraction  and the argument 
  which is specified by means of a list of substitutions
  . This natural extension comes from reading back a
  multiplicative cut in -dags or Pure
  Proof-Nets~\cite{AG09,AccattoliTh}.

The substitution
  rules also deserve some explanation. The side conditions ,
   and  are global on terms but local on graphs,
  simply because in the graph all the occurrences of the same variable are grouped
  together.  Also, the (global) meta-substitution
  operation  used in the right-hand side of the rule
   is completely local on graphs. Similarly, the meta-operation
   used in the right-hand side of the -rule is an
  algebraic notation for the local operation on graphs which splits  the co-located
  occurrences of  into two disjoint and
  non-empty sets, one of which corresponds  to , while the other is associated to
  the fresh variable . Thus, the structural
-calculus can be seen as an algebraic language useful to study
-dags and Pure Proof-Nets. 

\begin{figure}[ht]

\caption{The -reduction system}
\label{f:lambdaj}
\end{figure}



We close these rules by contexts, as usual:
 denotes the contextual closure of , for . We write  for the
reduction relation .  The \deft{reduction
  relation}  (resp.  ) is generated by all
(resp.  all expect ) the previous rewriting rules modulo
-conversion. \medskip


An expected property of  is that the reduction relation  is  stable by substitution. 
\begin{lem} Let .
\begin{enumerate}[]
\item If , then . 
\item If , then . 
\end{enumerate}
\end{lem}


In the rest of this section we shall prove the following properties of :
full composition (Lemma~\ref{l:fc}), 
simulation of one step -reduction (Lemma~\ref{l:l-sim}),
termination and uniqueness of normal forms of the  substitution calculus 
(Lemmas~\ref{l:dis-terminates} and~\ref{l:uniqueness-j}),
postponement of erasing reductions (Lemma~\ref{l:w-postponement})
and confluence of  (Theorem~\ref{t:confluence}).



\subsection{Jumps and Multiplicities}
\label{ss:struct-subs-and-mul}
The first property we show in this section 
is full composition, stating  that any 
jump  in a substituted term   can be
reduced to its implicit form .  There are two
  interesting points. The first is that in contrast with most calculi
  of explicit substitutions, full composition holds with no need of
  equivalences. The second is that the proof is by
  induction on  and not on the structure of .

\begin{lem}[Full Composition (FC)]
\label{l:fc}
Let  .  Then  . Moreover,  implies
.
\end{lem}


\proof
By induction on .
\begin{enumerate}[]
\item If , then .
\item If , then 
      .
\item If , then 
\bigskip
\end{enumerate}


Due to the very general form of the duplication rule
of , we get the following corollary which together with full composition can be seen as a generalised composition property: 

\begin{cor}
Given   s.t.  , then , where  is a
fresh variable. 
\end{cor}



\begin{proof}
The term   -reduces to 
. We conclude
by full composition.
\end{proof}

 Thus for example .  Note that this property is not enjoyed by
  traditional explicit substitution calculi: for instance, in
  ~\cite{BlooRoselx}, the term  cannot be reduced to
  .  However, it holds in
  calculi with partial substitutions, as Milner's calculus ~\cite{Milner07}. It is not
  difficult (see \eg~\cite{OConchuirKesner}) to define a translation
  {\tt T} on terms such that  implies .  This property allows in particular to
  deduce normalisation properties for   from those  of  .  \\

The one-step simulation of -calculus follows directly from full composition:

\begin{lem}[Simulation of -calculus]
\label{l:l-sim}
Let .  If  then  .
\end{lem}

\begin{proof}
By induction on .
Let  , then  . All the other cases are straightforward.
\end{proof}


We now introduce a notion that will be useful in various proofs. 
It counts the maximal number of free occurrences of a variable  that may appear during
a -reduction sequence from a term . 

The \deft{potential multiplicity} of the variable  in the term , written , 
is defined on -equivalence classes as follows:  
if  , then 
; otherwise:

We can formalise the intuition behind  as follows.

\begin{lem}
\label{l:mul-occ} Let .   Then 
\begin{enumerate}[\rm(1)]
\item \label{l:mo-one}.
\item If  is a -nf then .
\end{enumerate}
\end{lem}

\begin{proof}
Both points  are by induction  on the definition of  . The
only  interesting  case  is  when  : the  \ih\  gives
,                        and
, from which we conclude with the first point. 
For the second one, if  is a -nf
every   relation   given   by    the   \ih\   is   an   equality   and
,    otherwise    there    would    be    a
-redex.                Then               we               get
.
\end{proof}

Potential multiplicities enjoy the following properties.

\begin{lem}
\label{l:properties-mul-for-terms}
Let . Let  be pairwise distinct variables.
\begin{enumerate}[\rm(1)]
\item \label{l:pmt-1} If  and ,
then .
\item \label{l:pmt-2}  If ,
then  and  
, where the two
occurrences of the term  denote exactly the same term. 
\item \label{l:pmt-3} 
If  , then .
\end{enumerate}
\end{lem}

\begin{proof}
By induction on .
\end{proof}

By exploiting potential multiplicities we can define a measure of the
global degree of sharing of a given term, and use this
  measure to prove that the -reduction subsystem terminates.

We consider multisets of integers. We use  to denote the empty
multiset,  to denote multiset union, and 
  (resp. ) for the standard order (resp. strict order) on
  multisets~\cite{Nipkow-Baader}.  Given an integer
 and a multiset ,  denotes  if 
and the multiset 
 if . The
\textbf{-measure} of , written , is given
by:
 

Note that  for . Potential
multiplicities are 
decreasing by -reduction, and we are going to show that the
-measure is strictly decreasing; however  both can be incremented by
-steps. For example, consider .  We get ,
  ,  and .

The fact that the -measure decreases by -reduction is proved as follows:

\begin{lem}
\label{l:properties-dm-for-terms}
Let . Then, 
\begin{enumerate}[\rm(1)]
\item \label{l:pdmt-1} .
\item \label{l:pdmt-2} If , then
.
\end{enumerate}
\end{lem}

\proof By induction on . The proof of the  first property is 
straightforward.
For the second one we show , which proves
the desired property.
\begin{enumerate}[]
\item . Then 
. 
\item . W.l.g we assume . 

If , we reason as follows:


If , then   by Lemma \ref{l:mul-occ}:\ref{l:mo-one} and so 
. Therefore:


\item All the other cases are straightforward.
\qed 
\end{enumerate}


\begin{lem}
\label{l:dm-decreases}
Let . Then, 
\begin{enumerate}[\rm(1)]
\item  implies .
\item  implies .
\end{enumerate}
\end{lem}

\proof
By induction on the relations. The first point is straightforward, hence
we only show the second one. We reason by cases. 
\begin{enumerate}[]
\item , with . 
Then . 
 
\item , with . 
Then . 

\item , with 
and  fresh. Then, Lemma~\ref{l:properties-mul-for-terms}:\ref{l:pmt-2}
gives  and  thus:
 
       
       
\item , where . Then:
      

\ignore{
\item , where . Then
      
}
       
\item All the other cases are straightforward. 
\qed
\end{enumerate}


The last lemma obviously implies:

\begin{lem}
\label{l:dis-terminates}
The -calculus terminates.
\end{lem}

Furthermore:

\begin{lem}
\label{l:uniqueness-j}
The -reduction relation is confluent and terminating. Moreover,
if  denotes the (unique) -normal form of , then the following
properties hold:

\end{lem}

\begin{proof}
One easily shows that  is locally confluent, then
Lemma~\ref{l:dis-terminates} allows to apply Newman's Lemma~\cite{Terese03} to conclude with the first
part of the statament.  The second part can be shown by induction on
the structure of terms. Particularly, when  one has
. It is then sufficient to
note that -normal forms are stable by substitutions of
-normal forms.
\end{proof}



We conclude this section by showing
another important property of  concerning
the  postponement  of  erasing  steps. We first need the following lemma:

\begin{lem}
\label{l:struct-unary-wop-post}
Let . Then:
\begin{enumerate}[\rm(1)]
\item \label{l:struct-unary-wop-post-i}  implies . 
\item \label{l:struct-unary-wop-post-iv}  implies 
\end{enumerate}
\end{lem}

\begin{proof}
Point~\ref{l:struct-unary-wop-post-i}
is  by induction on the relations and case analysis.
Point~\ref{l:struct-unary-wop-post-iv} is by induction on the length
of  using 
Point~\ref{l:struct-unary-wop-post-i}.
\end{proof}

Let us use  as a notation for a reduction sequence, the symbol ';' for the concatenation of reduction sequences and  for the number of  steps in . Then we obtain:

\begin{lem}[-postponement]
\label{l:w-postponement}
Let . If  then  s.t. .
\end{lem}

\begin{proof}
By   induction  on  .   The  case     is
  straightforward.  Let .  If 
then   simply   conclude  using   the   \ih\   on  the   sub-reduction
.    Otherwise  the sequence    starts   with  a
  -step. If all the steps  in  are , then we trivially
  conclude.      Otherwise     where
 is  the maximal  prefix of   made out  of weakening
steps  only.   By  Lemma~\ref{l:struct-unary-wop-post}:\ref{l:struct-unary-wop-post-iv}
we  get  that
  and  we  conclude  by  applying  the
\ih\ to .
\end{proof}

\subsection{Confluence}
\label{ss:struct-confluence}
Confluence of calculi with  ES can be easily proved
by using Tait  and Martin L\"of's technique (see for  example the
case of ~\cite{Kes07}). This  technique is based
on the definition of a simultaneous reduction relation
 which enjoys the  diamond property. It is completely standard so we give the statements of the lemmas and omit the proofs. \medskip

The \deft{simultaneous reduction relation } is defined on terms in -normal form as follows:
\begin{enumerate}[]
\item 
\item If , then 
\item If  and , then 
\item If  and
         , then 
\end{enumerate}
Note that the third and fourth cases overlap, thus for example,

and , where  denotes the identity
function .

 A first lemma ensures that  can be simulated by .

\begin{lem}
\label{l:paral-Rewn}
If , then . 
\end{lem}

\begin{proof}
By induction on . 
\end{proof}

A second lemma ensures that  can be projected through  on .

\begin{lem}
\label{l:Rew-xc-paral}
If , then . 
\end{lem}

\begin{proof}
By induction on .
\end{proof}

The two lemmas combined essentially say that  is
confluent if and only if  is confluent. Then we show the
diamond property for , which implies that
 is confluent:

\begin{lem}
\label{l:paral-diamond}
The relation  enjoys the diamond property.
\end{lem}

\begin{proof}
By induction on  and case analysis.
\end{proof}

Then we conclude:

\begin{thm}[Confluence]
\label{t:confluence}
For all , for all  s.t. ,
 s.t. . 
\end{thm}

\begin{proof}
Let  for . Lemma~\ref{l:Rew-xc-paral}
gives  for . Lemma~\ref{l:paral-diamond}
implies  is confluent so that  such that
 for . We can then close the diagram
with  by Lemma~\ref{l:paral-Rewn}.
\end{proof}

While confluence holds for all calculi with explicit substitutions,
\deft{metaconfluence} does not. 
The idea is to
switch to  an enriched language  with a new  kind of (meta)variable  of the
form , to  be intended as a named  context hole expected to
be  replaced by  terms whose  free  variables form a subset of . 
This form of metaterm is for example used in  the
framework of higher-order unification~\cite{HuetThEtat}. In
presence  of   meta-variables  not   all  the  substitutions   can  be
computed.  For instance in the metaterm  the jump   is
blocked.  Consider:
 

These metaterms are different normal forms. 
However, it is  enough to  add the following equation  to recover
confluence:
  

A proof of confluence of  modulo  for metaterms can be found in~\cite{Renaudth}.

\section{Preservation of -Strong Normalization for }
\label{s:lj-psn}

A reduction system  for a language containing the set
 of all -terms is said to enjoy the \deft{PSN
  property} iff every -term which is
-strongly normalizing is also
-strongly normalizing. Formally, for all
, if , then . 

The PSN property, when it holds, is usually non-trivial to prove. We are
going to show that  enjoys PSN by giving a particularly compact proof. The proof
  technique has been developed by D.~Kesner~\cite{Kes09}; it reduces PSN to a
  property called , which relates termination of
\deft{I}mplicit substitution to termination of \deft{E}xplicit
substitution. It is an abstract technique not
depending on the particular rules of the calculus with
explicit substitutions.

A reduction system  for a language  containing the
set  is said to enjoy the \deft{ property} iff for
 and for all , :


Of course one generally considers a system  which can simulate the
-calculus,  so that the following properties seem to be natural requirements to get PSN.




\begin{thm}[Natural Requirements for PSN]
\label{t:ie-implies-psn}
Let  be a calculus verifying the following facts:
\begin{enumerate}[\bf(F1)]
\item[{\bf (F0)}] \label{f:uno} If , then .
\item[{\bf (F1)}] \label{f:dos} If , then .
\item[{\bf (F2)}] \label{f:tres} If , then 
.

\end{enumerate}
Then,  enjoys PSN. 
\end{thm}


\proof
We show that  implies  by induction on
the pair , using the lexicographic
ordering. We reason by cases.
\begin{enumerate}[]
\item If , then  
and . We have 
 by the  \ih\ and thus   by fact {\bf F0}.
\item If , then  
and . 
We have 
 by the  \ih\ and thus   by fact {\bf F1}.
\item If , 
then  and . 
Indeed, 
and  . 
We have that 
both terms are in  by  the  \ih\
Then {\bf F2} guarantees that .
\qed
\end{enumerate}
Now we show that 
  satisfies the three natural requirements of the last
theorem, and thus it satisfies PSN.

\begin{lem}[Adequacy of ] 
\label{l:adequacy}
If  verifies , then  satisfies PSN.
\end{lem}

\begin{proof} By Theorem~\ref{t:ie-implies-psn} it is sufficient to
show {\bf F0}, {\bf F1} and {\bf F2}.  The first two properties are
  straightforward.  For the third one,  assume 
  and .
  Then in particular . We show that  by induction on .  For that, it is sufficient to show
  that every -reduct of  is in .  If the
  -reduct of  is internal we conclude by the \ih\ Otherwise
   which is in  by the
  \iep\ property.
\end{proof}



As a consequence, in order to get PSN for  we only need to prove 
the \iep\ property. For that, we first generalise
the \iep\ property in order to deal with  possibly many
substitutions.   


A reduction system  for a language  containing
  the set  is said to enjoy the \deft{Generalised  property}, written \deft{, } iff for
  all  in
  , if , then
  , where  for  and  for .  


\begin{thm}[ for ]
\label{t:ieg}
The -calculus enjoys the \giep\ property.
\end{thm}


\textbf{Notation}: To improve readability of the proof we shall
abbreviate the notation  by . Similarly for implicit substitutions.


\proof 
Suppose .
We show  by
induction on:

where
 and 
 .

To show  it is sufficient to show that
every -reduct of  is in . 


\begin{enumerate}[]
\item  with .  Then we get:
  \begin{enumerate}[]
\item ,
 \item  does not change, and
 \item .
 \end{enumerate}
We conclude by the \ih\  since  and  our hypothesis  is equal or reduces to  (depending on ).


\item  with . Then we have that:

We conclude by the \ih\ since 
  .


\item  with .
 Then we have that:

We conclude by the \ih\ since 
  .


\item , with . Then we have that:

But .
We conclude by the \ih\ since 

by hypothesis.

\item 
with . Note that .  Then we get:

Since the \jump s
are independent, then  implies
.
   We conclude since  by hypothesis.


\item 
with  and  fresh. Then, \\ 
  and\\
. 
In order to apply the \ih\ to   we need:  
\begin{enumerate}[]
\item . This holds by hypothesis.
\item . 
This holds since the term is equal to  which is  by hypothesis.
\end{enumerate}

Note that this is the case that forces the
use of a  generalised sequence of substitutions: if we were proving the
statement for  using as hypothesis  then there
would be no way to use the \ih\ to get .

\item . We
     have that:
     
     holds by hypothesis. Then:

Thus  and . Since  by hypothesis we
can apply the \ih\ and get .
\qed\medskip
\end{enumerate}

\noindent The following is a consequence of Thereom~\ref{t:ieg}: just take
the number of substitutions  to be  and consider only
the  property for . 



\begin{cor}[\iep\ for ]
\label{t:ieldis}
The -calculus enjoys the \iep\ property.
\end{cor}


Corollary~\ref{t:ieldis}, then Lemma~\ref{l:adequacy}
and finally  Theorem~\ref{t:ie-implies-psn} imply:

\begin{cor}[PSN for ]
\label{coro:struct-psn-lj}
The -calculus enjoys PSN, \ie\ if , then .
\end{cor}

Note that Lemma \ref{l:adequacy} and Theorem \ref{t:ieg}, which
contains the arguments for PSN, do not use full composition, nor
termination of , confluence or postponement of erasures:
none of the properties of  plays a role in this compact proof of
PSN, which is quite surprising. The crucial point is the formulation
at a distance of the rewriting rules. Indeed, we will later show that
such a simple proof does not longer work when rules propagating jumps
are added to the system.

\section{An equational theory for }
\label{s:eq-th}
Sections~\ref{s:structural-lj} and~\ref{s:lj-psn} show  that the basic
theory of  enjoys good properties such as full composition,
confluence and PSN. In most calculi with explicit
substitutions, where substitutions are propagated through constructors
and do not act at a distance, full composition can only be obtained by
adding an equivalence relation  defined as the contextual and
reflexive-transitive closure of the following equation:
 
Otherwise  a term like  cannot reduce to its implicit form 

(and so full
composition does not hold). Interestingly,  enjoys full
composition without using equation , which is remarkable since 
plain
rewriting is much easier than rewriting
modulo an equivalence relation.\medskip

However, as mentioned at the end of
Section~\ref{ss:struct-confluence}, the equation  is necessary to
recover confluence on metaterms.  It is then natural to wonder what
happens when  is added to . The answer is extremely
positive since  preserves all the good properties of ,
and this holds in a very strong sense. In fact,  is a strong
bisimulation for  (\cf\ Lemma~\ref{l:eqo-bisim}),
so that  can be postponed w.r.t. 
(\cf\ Lemma~\ref{l:bisim-conf})  and  modulo  enjoys
  PSN
  (\cf\ Lemma~\ref{l:lamj-bis-psn-dif-sys}:\ref{l:sbisim-pres-psn}).


As already mentioned in the introduction, 
 -terms and
-dags~\cite{AccattoliTh} are strongly bisimilar, 
but the translation of -terms to -dags is not
injective, \ie\ there are different -terms 
which are mapped to the \textit{same} -dag. 
It is then interesting to characterise  the quotient induced by the
translation~\cite{AccattoliTh}, which turns out to be : indeed  if and only if  and
 are mapped to the same -dag , and since they both behave
like  (\ie\  are strongly bisimilar to ), then  they behave the same
(\ie\  they are strongly bisimilar).\medskip

The -calculus is also 
interesting since it can be mapped
 to another graphical language, Danos' and Regnier's \deft{Pure
  Proof-Nets}, being able to capture
\textit{untyped} -calculus. It is possible to endow Pure Proof-Nets
with an operational semantics\footnote{Danos' and Regnier's original
  operational semantics does not match exactly  because they use
  a big-steps rule for eliminating exponential cuts, which corresponds
  to use just one substitution rule
  . However, the refinement of Pure
  Proof-Nets where duplications are done small-steps is very natural
  from an explicit substitution point of view, altough --- to our
  knowledge --- it has never been considered before.} which makes them
strongly bisimilar to . The quotient
induced by the translation  from -terms into Pure Proof-Nets is
given by the \deft{graphical equivalence}  
which is the contextual
and reflexive-transitive closure of the 
equations in Figure~\ref{f:eqo}. 


\begin{figure}[ht]

\caption{The graphical  equivalence } 
\label{f:eqo}
\end{figure}

This means that Pure Proof-Nets quotient more than
-dags\footnote{-dags can be mapped on Pure Proof-Nets, and
  once again the map is a strong bisimulation.}. As
for ,  is a strong bisimulation
(\cf\ Lemma~\ref{l:eqo-bisim}), and thus confluence and PSN
of  
automatically lift to 
(\cf\ Theorem~\ref{thm:eqo-conf-psn}), which is the reduction relation
 modulo .\medskip

Another way to explain the  -equivalence 
is by  means of \textit{linear} constructors. Indeed, 
the body of an abstraction cannot be duplicated nor erased by the
abstraction itself---in this sense an abstraction is \textit{linear}
in its body. Similarly, explicit substitutions are linear with
respect to their left subterm, while they are \textit{non-linear} with
respect to their right subterm, \ie\ the content of the jump, which may be
duplicated or discarded. Applications are linear in their left
subterm but they are non-linear in their argument, 
because they can wrap
it in a jump. This linear/non-linear classification reflects the fact
that jumps and arguments (and only them) are associated
to -boxes in Proof-Nets, the non-linear construction of Linear Logic.  The
equations defining  can be understood as a permutation between a jump 
and a linear subterm of the
adjacent constructor.

It is then natural to wonder if  can be extended with
equations permuting jumps with non-linear subterms (see Figure
\ref{f:boite}, page \pageref{f:boite}), without breaking confluence
and PSN. The answer is yes; the obtained equational
theory is called the \textbf{substitution equivalence} , and the
fact that  modulo  enjoys PSN is the main result of this
paper.\medskip

Extending  to non-linear permutations
is delicate from a termination point of view, since the use of
non-linear equations affects reduction lengths. Indeed, the natural
but na\"{\i}ve extension of  breaks PSN. By analyzing a
counter-example to PSN we define  so that PSN turns out to be true. The
proof of this fact, however, is more involved than that for  and
, mainly because  is not a strong bisimulation.
Therefore, we shall develop a new technique for proving
PSN modulo .\medskip

Section~\ref{s:regnier} starts over by explaining
the equivalence  in terms of Regnier's -equivalence~\cite{regnier94}, 
providing a different point of view with respect to what was already
mentioned. Section \ref{s:propp-intro} discusses 
how to extend  to  by
    showing the difficulties to prove PSN for  the obtained extension.  Section~\ref{sec:psn} develops the proof of PSN for  modulo .


\subsection{The graphical equivalence}
\label{s:regnier}

\deft{Regnier's equivalence}  is the smallest
equivalence on -terms closed by contexts and containing the 
equations in Figure~\ref{f:regnier}. 
\begin{figure}[ht]

\caption{The  equivalence  }
\label{f:regnier}
\end{figure}

\noindent Regnier proved that two -equivalent terms have
essentially the same {\it operational} behavior:  is contained
in the equational theory generated by -reduction, \ie\ 
, and if  then 
the maximal -reduction
sequences from  and  have the same length (the
so-called \textit{Barendregt's norm}). That is why Regnier calls  an \textit{operational
  equivalence}. \medskip

It is then natural to expect that 
the previous property can be \textit{locally} reformulated in terms of a
strong
bisimulation, namely, 
 

Unfortunately, this is not
the case. Consider the following example, where 
grey boxes are used to help the identification of redexes and their reductions: 



The
term  has only one redex whose reduction gives 
which is not -equivalent to ,  the reduct of . The
diagram can be completed only by unfolding the whole reduction:
 


Note that the second step from  reduces a {\it created} redex. \medskip

We are now going to analyze  
in the framework of .  For that, Regnier's equivalence
can be understood on -terms by first removing the -redexes. Indeed,
let us take the clauses defining  and let us make a 
-reduction step on both sides, thus eliminating the multiplicative
redexes as in Regnier's definition:
 

Now,   can be seen
as a change of the positions of jumps in a given term and particularly
as a permutation equivalence of jumps concerning the linear constructors of the
calculus.

This is not so surprising since such permutations turn into simple
equalities when one extends the standard translation of -calculus
into Linear Logic Proof-Nets to -terms (see for
example~\cite{KL07}). Another interesting observation is the relationship between  and
the equivalence  introduced in
Section~\ref{ss:struct-confluence}. To understand this point we
proceed the other way around by expanding jumps into -redexes:
 
Note that 
the relation between the
resulting terms is
contained in , that is why it was not visible
in -calculus:
 

In~\cite{AccattoliTh} it has been proved that two -terms 
  and  are translated to the same Pure Proof-Net if and only if
  . More precisely, this relation
can be given by the \deft{graphical equivalence}
 already defined  in Figure~\ref{f:eqo}. 
\ignore{
\begin{figure}[ht]
 
\caption{The -equivalence relation}
\label{f:eqo}
\end{figure}
}

The equations defining  are specified by 
means of \textit{local}
permutations, but it is 
also possible to define
 in terms of global permutations. First, define 
a \deft{spine context}  as:

and then define  as the context closure of the following equation :


The two definitions are easily seen to be equivalent.  We shall now
prove that  is a strong bisimulation, which will immediately
imply (Lemma~\ref{l:lamj-bis-psn-dif-sys}) that  preserves
reduction lengths. This property is stronger than the one proved by
Regnier for , since it holds for any reduction sequence,
not only for the maximal ones.  \medskip

\begin{lem}
\label{l:eqo-stability}
Let  be the equivalence relation  or , and  s.t. . Let . Then:
\begin{enumerate}[\rm(1)]
  \item \label{l:eqo-stability-minus} .
  \item \label{l:eqo-stability-zero} For all  
  there is 
   s.t.
   and  
  . 
  \item \label{l:eqo-stability-two}  .
  \item \label{l:eqo-stability-one}  .
  \end{enumerate}      
 \end{lem}

\begin{proof}
 Straightforward inductions.
\end{proof}



\begin{lem}
\label{l:eqo-bisim}
The relations  and  are   strong bisimulations for . 
\end{lem}




\proof
We prove the statement for . The proof for  is obtained by 
simply forgetting the cases .
Assume  holds in -steps, which is written as
, and let . We show 
s.t.    by induction on
.  

The inductive step for  is straightforward. For 
  we reason by induction on the definition of
, given by the closure under contexts of the equations
.
 


We only show here the cases where  is contextual, all the other ones being straightforward. 
\begin{enumerate}[]
\item If ,
where  and ,  then we close the diagram by . 
\item The case ,
where  and  is analogous to the previous  one.
\item If , 
where , then by the \ih\  s.t. . 
We close the diagram by  .
\item The case ,
where  is analogous to the previous one.
\item If , where
 and ,  then .
\item If , where 
       and ,  then the previous remark implies
       and we close the diagram by  .
\item If , where  and , then we
  close the diagram by .
\item  If , where  and , then
  we first write  as , where
   and .
  Lemma~\ref{l:eqo-stability}:\ref{l:eqo-stability-minus}
  gives
   and Lemma~\ref{l:eqo-stability}:\ref{l:eqo-stability-zero}
  gives  verifying  and 
  . 
  Then, we close the diagram with
  .
\item If , where  and , then  ,  where the last equivalence 
  holds by Lemma~\ref{l:eqo-stability}:\ref{l:eqo-stability-one}.
\item If ,
  where  and . Then,  where the last equivalence holds by
  Lemma~\ref{l:eqo-stability}:\ref{l:eqo-stability-minus}-\ref{l:eqo-stability-two}.
\qed\medskip
\end{enumerate}

\noindent A consequence (\cf\ Lemma~\ref{l:bisim-conf}) of the previous lemma is that
both  and   can be postponed, which implies in particular the following.

\begin{thm}
\label{thm:eqo-conf-psn}
The reduction systems  
and  are both confluent and enjoy PSN.
\end{thm}



\begin{proof}
Confluence follows from Lemma~\ref{l:eqo-bisim} and
Theorem~\ref{t:confluence} by application of
Lemma~\ref{l:bisim-conf}:\ref{l:bisim-conf-ii}, while PSN follows from
Lemma~\ref{l:eqo-bisim} and Corollary~\ref{coro:struct-psn-lj} by
application of Lemma \ref{l:lamj-bis-psn-dif-sys}.
\end{proof}


Actually,  is equal to . In
  the framework of rewriting modulo an equivalence relation there are
  various, non-equivalent, forms of confluence. The one given by Theorem \ref{thm:eqo-conf-psn} is
  the weakest one, but the 
  \textit{Church-Rosser modulo} property also holds in our framework. 

\begin{thm}[Church-Rosser modulo  and ]
\label{cr-modulo}
Let  be the equivalence relation  or . 
If ,  and
, then  s.t.
, 
and .
\end{thm} 


\begin{proof}
By  Lemma~\ref{l:eqo-bisim} and Lemma~\ref{l:bisim-conf}. 
\end{proof}

We finish this section with the following interesting property.

\begin{lem}
\label{l:dis-o}
The reduction relation  is strongly normalizing.
\end{lem}

\begin{proof} The proof uses the measure  used to prove
Lemma~\ref{l:dis-terminates} and the fact that 
 implies .
\end{proof}



\ignore{
Actually, it is possible to show that  is much more than
confluent modulo . When dealing with reduction modulo the
three notions of
\begin{enumerate}
  \item  being confluent,
\item  being confluent modulo , 
\item  being Church-Rosser modulo 
\end{enumerate}

do not coincide,
namely  but  (see Terese~\cite{Terese03}). It is easy to show
that given a strong bisimulation  for a confluent system 
then  is Church-Rosser modulo , thus  is
Church-Rosser Modulo  see~\cite{AccattoliTh}. But in our case we
can get an even stronger result: the definition of \textit{being
  Church-Rosser modulo} requires the diagram to be closed on
equivalent terms, but since  concerns jumps only, and since
jumps can always be reduced to their implicit form without using
, we get that the diagram can be closed without ever using
. To our knowledge this stronger property has no name in the
literature, and more generally there is no abstract study of how
different forms of bisimulation transport confluence or
normalization.\\ It is also worth to note that  alone is a
strong bisimulation too, so that the confluence results also hold in
the case of  modulo  (PSN is contained in
Theorem~\ref{thm:eqo-conf-psn}, while a priori confluence or
Church-Rosser modulo  may not hold). Said differently,
 is independent from the other two equations
 defining .}
  
\ignore{We showed that the relation
  , defined as , is confluent. In
  the framework of rewriting modulo an equivalence relation there are
  various, non-equivalent, forms of confluence. The one we showed is
  the weakest one. The strongest one is the so-called
  \textit{Church-Rosser modulo} property (see~\cite{terese} for its
  definition). It can easily be shown (see~\cite{phdaccattoli}) that
   is Church-Rosser Modulo  (it follows from 
  being a strong bisimulation)\footnote{Actually, in order to close
    the Church-Rosser diagram  is not necessary so that one gets
    an even stronger confluence property, which has no name in the
    literature.}}


\ignore{The fact that  is a strong bisimulation means that if we
  prove a normalisation result for a term  then the result
  immediately lifts to all the terms in , \ie, the
  -equivalence class of . In order to simplify some
  reasonings we shall restrict to consider only canonical
  representants of -equivalence classes. Consider the reduction
  relations:
 
And let  be the relation obtained as the context closure
of . For us a term in
\deft{-form} will be a -normal form. Let us show
that -forms make sense:

\begin{lem}
. The relation  is strongly normalizing and confluent modulo .
\end{lem}
}

\subsection{The  substitution equivalence}
\label{s:propp-intro}
Composition of explicit substitutions is a sensible topic in the
literature, it is interesting to know if
 can be extended with a safe notion of (structural) composition.

The structural -calculus is peculiar 
since composition of substitution is 
provided natively, but only \textit{implicitly} and at a distance.  Indeed, a term
 s.t.    reduces in various
steps to:
 but not to the \textit{explicit
  composition} . One of the aims of this paper is to
prove that adding explicit composition to  preserves PSN and confluence.

The second aim concerns \textit{explicit
  decomposition}. Indeed, some calculi~\cite{OH06,MaraistOTW99,Schw99,HZ09,Hasegawa} explicitly
\textit{decompose} substitutions, \ie\ reduce  to
. We show that PSN and confluence hold even when extending
 with such a rule.\medskip

More generally, having a core system, , whose operational
semantics does not depend on propagations, we study how to modularly add
propagations  by  keeping the good properties. We have
already shown that  is stable with respect to 
the graphical equivalence, which can be seen as handling propagations
of jumps with respect to \textit{linear} constructors. We proved that  is confluent and
enjoys PSN (Theorem~\ref{thm:eqo-conf-psn}). What we investigate here is if we
can extend it to propagations with respect to \textit{non-linear}
constructors.\medskip

The idea is to extend  to , where 
is the the contextual and reflexive-transitive closure
of the relation
generated by  plus:

In terms of global permutations  can be defined as the
  context closure of  where
  , and  is \textit{any}
  context (not just a spine context) which does not capture the
  variables of .  These equations are constructor preserving (same
kind and number of constructors), in contrast to more traditional
explicit substitution calculi containing for instance the following 
rule:

which achieves two actions at the same time: 
duplication and propagation of a jump. In  there is a
neat separation between propagations and duplications, so that no
propagation affects the number of constructors.  The rule 
can be simulated in  only in the very special case where 
and  both have occurrences of . In our opinion this is not a
limitation: the rule  is particularly inefficient since it
duplicates even if there is no occurrence of  at all, thus it is
rather a good sign that  cannot simulate .\medskip

The reduction relation  does not enjoy PSN, since it is a bit na\"ive
on the way it handles void substitutions. The following counter-example has been found by Stefano
Guerrini. Let  , then:

The term  reduces to a term containing  and so there is a loop
of the form .  Now, take ,
which is strongly normalizing in the -calculus.  Since 
-reduces to ,  is not -strongly normalizing
and thus  does not enjoy PSN. It is worth to note that, in
contrast to \mellies\ counterexample for ~\cite{Mellies1995a}, the -rule has
no role in building the diverging reduction: the fault comes only from the
jump subsystem  modulo .\medskip


The key point of the previous counter-example is that the jump 
is free to float everywhere in the term since  has no occurrence in
. Such behavior can be avoided by imposing the constraint "" to  and . This has also a natural graphical
justification in terms of Pure Proof-Nets (\cite{AccattoliTh}, Chapter 6, page 149), since such constraint 
turns  and  into the exact analogous \ignore{form} of the
commutative box-box rule of Linear Logic Proof-Nets, but used here as
an equivalence relation.  We then modify
 and  by
  introducing the equivalence  as the contextual and
  reflexive-transitive closure of the  equations in Figure~\ref{f:boite}.
\begin{figure}[ht]

\caption{The equivalence }
\label{f:boite}
\end{figure}

\noindent Now, we redefine  in the following way. The \deft{substitution
  equivalence}  is the smallest equivalence closed by contexts
containing all the equations in Figure~\ref{f:eqf}. 

\begin{figure}[ht]

\caption{The substitution equivalence }
\label{f:eqf}
\end{figure}

\noindent Alternatively,
   can be defined by the context closure of the following
  global permutating equations:

where  is any context and  is a spine context.\medskip

It is now natural to study -reduction
modulo .  It is easy to prove that the
jump calculus terminates with respect to the
new equivalence  so that the previous counterexample to PSN is
ruled out. We need an auxiliary lemma about potential multiplicities and the -measure. 

\begin{lem}
\label{l:mul-for-eq}
Let . If   then:
\begin{enumerate}[\rm(1)]
\item  for every variable .
\item \label{p:mul-for-eq-two} .
\end{enumerate}
\end{lem}

\begin{proof}
By induction on . The base cases are easy calculations, the
inductive cases use the \ih\ 
\ignore{We show the base case, the
  inductive ones follow from the \ih:
\begin{itemize}   
\item , with  and . We get:



\item , where  and . First, let us show that
  . If
   then both expression are equal to
  , as , otherwise are both
  equal to . Then:


\end{itemize}}
\end{proof}


Lemma~\ref{l:dm-decreases} and
Lemma~\ref{l:mul-for-eq}:\ref{p:mul-for-eq-two} together proves the
following corollary.

\begin{cor}
\label{l:dis-f}
The reduction relation  is terminating.
\end{cor}

\ignore{
For the na\"ive equations it is possible to show that  implies
, while   does not  imply , as Lemma \ref{xxx} will show.
}


\section{Preservation of -Strong Normalization for }
\label{sec:psn}

The structural -calculus modulo  is an incredibly subtle and complex
rewriting system, and proving PSN is 
is not an easy task. Some of the difficulties are:
  
\begin{enumerate}[]
\item \textit{The relation  is not a strong bisimulation}.  It
  is not difficult to see that  is confluent modulo 
  (essentially the same proof than for ). However,  does
  not preserve reduction lengths to normal form, \ie\  it is not a
  strong bisimulation. Two  examples can be given by analysing the
  interaction  between  with erasure and duplication. 
  Here is an example for erasure:  
   
and here another  one for duplication:
      
Indeed, if  would have been a strong bisimulation, then in both diagrams
the two terms of the second column would be -equivalent, while
they are not (remark that  preserves the number of constructors
so that those terms cannot be -equivalent).


\item \textit{The relation  cannot be postponed}. The last
  example shows also that  cannot be postponed. 
This is illustrated by the upper left corner of
the previous figure:

Observe that this phenomenon is caused by
the equation . Remark that both
composition (\ie\ ) and decomposition 
() are used in Guerrini's
counterexemple. 
\ignore{Surprisingly, from our study we learned that dealing
with decomposition is more complicate than dealing with composition. 
\item \textit{Erasures cannot be postponed}.  Consider 

the two steps cannot be permuted. This is a phenomenon
concerning  too, except that  can be postponed
with respect to  (Lemma~\ref{l:eqo-bisim}),
and then  can be postponed
with respect to . Unfortunately,  cannot be delayed in  and so 
neither  can.
}
\item \label{it:in-out-instability}\textit{There is no canonical representant of equivalence
  classes which is stable by reduction}. Indeed, there are two natural
  canonical representants in . Given  we can define  as the term obtained by moving all substitution towards
  the variables as much as possible and  the term obtained moving substitutions far from variables
  as much as possible. Consider 
,
then 

does not reduce to .
Similarly, for the other representative
since 
does not reduce to .\medskip
\end{enumerate}


\noindent In~\cite{AK10} we proved that  enjoys PSN in the cases where
the equations  are both oriented from left to
right or from right to left. Here we prove PSN considering them as
equivalences. Surprisingly, the proof of this more
  general result is relatively more compact
  and concise than the one(s) in~\cite{AK10}. Indeed, even if we need
  to pass through an auxiliary calculus, such a calculus can be proved
  to enjoy PSN without using labels, in contrast to our previous
  result and proof. \medskip



Let us explain our technique. Even if there is no canonical representative form of an
-equivalence class which is stable by reduction
(\cf\ Section~\ref{s:propp-intro}), there is an even more natural way
to reason about PSN in the presence of the non-trivial 
equations
 which consists in projecting
 over a simpler equational calculus. Since both the calculus
and the projection are quite peculiar we introduce them
gradually.\medskip

A usual na\"{\i}ve idea consists in projecting  
into the -calculus by means of a function computing the complete
unfolding of jumps. This gives the following diagram:

This principle could be easily exploited in order to prove some
properties of  (such as confluence), however, this projection
erases divergent sub-terms, therefore it cannot be used to prove
PSN. For instance, consider 
(where  is a non-terminating term), which is only
-weakly normalizing, whereas  is in normal form. It
is easy to show that projection of terms without void jumps preserves
divergence and thus PSN.  Unfortunately, erasures cannot be postponed
in .\medskip

Roughly speaking, the projection gives  so
that there are some steps  s.t. .  It
is not really a problem if such (erased) steps are finite, but here
there may be infinite sequences of such (erased) steps. It is then quite natural to
change the complete unfolding  into a \textit{non-erasing}
unfolding , which does not project void jumps:

 
Note that there are still some erased steps, as for instance
, where , but intuition
tells that  preserves divergence, because diverging terms
  are no longer erased by the projection. Note also that the image of the projection of
the previous reduction step  is no longer a reduction
step in the -calculus, so that we need to specify which are the
rewriting rules and the equations of the image of the
translation.  \medskip

For didactive purpose let us assume that we are able to turn the image
of the projection into a calculus --- let us say  --- such that
 projects  into  and preserves divergence. Two
important remarks are: since  preserves divergence, then
PSN for  implies PSN for ; also, the -calculus
does not contain the equations  because
they were turned into equalities thanks to their side conditions.


It is then reasonable  to expect that proving PSN for 
 is easier
than PSN for . Our proof technique can then be summarised as follows:
\begin{enumerate}[(1)]
  \item Introduce ;
  \item Prove PSN for ; 
  \item Show that  preserves divergence from  to ;
  \item Conclude PSN for .
\end{enumerate}

Section~\ref{s:lauxm} presents the rewriting rules of
, thus completing point 1. Section~\ref{s:iep-for-lauxm} deals with point
2 and Section \ref{s:projection} with points 3 and 4.\medskip

We believe that the isolation of  is an important contribution
of this paper. Indeed, it is easy to
see that  should contain at least the three following rewriting rules:

 

More precisely, 

\begin{enumerate}[]
  \item The reduction step  projects into . 
\item The reduction step  should map to itself, \ie\ . 
\item The reduction step  
 should map to itself, \ie\
.
\end{enumerate}
However,  projecting on
such a simple calculus still does not work. There are 
three  phenomena  we should
take care of:


\begin{enumerate}[(1)]
\item 
\deft{Equations}. 
As we already mentioned  and 
vanish, that is, 
 implies
. The graphical equivalence, instead,
do not vanish, and must be added to the intermediate calculus, thus 
getting  the reduction  relation to be considered modulo .


  \item \deft{Generalised erasure}. Consider:

where  and . 
Hence the -rule  if 
must be generalised in order to replace
the jump  by many (eventually none) jumps containing subterms of .  We shall
then use the following (Hydra like) rule :
 
Where  means that  is a subterm of . The condition upon
free variables is necessary in order to avoid unwanted captures
inducing degenerated behaviors. Note that the particular case  gives
the -rule.
\item \deft{Unboxing}: An erasing step
 can cause jumps
to \textit{move} towards the \textit{root} of the term. Consider:

where  and .  
Hence, to project this step over  we need a rule moving
jumps towards the \textit{root} of the term, which could have in
principle the general form:
 
This rule is the one that shall demand a more involved --- but
still reasonable --- technical development. Indeed, 
reduction that moves  \textit{any} jump towards the root  modulo   may cause non-termination:
 
In order to avoid this problem we
restrict the general form of the rule to a certain kind of contexts, those
whose hole is contained in at least one \textit{box}, \ie\  
the argument of an application or the argument of a
jump. \medskip
\end{enumerate}

\noindent We now develop a PSN proof for . 
Section~\ref{s:lauxm}  formally defines the intermediate calculus
, 
while Section~\ref{s:iep-for-lauxm} proves PSN
for the intermediate calculus   and
Section~\ref{s:projection} proves the properties of the projection which allows us to conclude PSN for . 

Let us conclude this section by observing that the 
  generalised erasure and the unboxing rules are 
introduced to project the -rule and not the 
equations 
. Said in other words, 
to prove PSN of the simpler calculus  (resp. ) through the 
  projection into  (resp. ), one still needs the generalised erasure and the unboxing rules. That is why we
  believe that the technique  developed here is really interesting by itself.


\subsection{The -calculus}
\label{s:lauxm}

The -calculus can be understood as a memory calculus
based on \textit{void} jumps. It is given by a set of
  terms, written ,  generated by the following grammar, where only
void jumps are allowed:

The notation  just means that the constant 
has no (free) occurrence in the term  and  
denotes a list of void jumps . 

To define the operational semantics we need to define a particular
kind of context. More precisely, if  denotes a context then a
\deft{boxed context}  is given by the following grammar:

We now consider the  reduction rules
 and equations in Figure~\ref{f:laux}. 
The notation  in the rules  and 
means a list  
of void jumps where   (so
potentially ).

\begin{figure}[ht]
 
\caption{The -reduction system\label{f:lauxm}}
\label{f:laux}
\end{figure}



 \noindent Note that the -rule  with 
 of  is a particular case of the -rule.  Remark
 also that the unboxing rule of  moves \textit{void} jumps
 outside terms, which was forbidden in the equation
    of . However, this does not break PSN because 
 there is no \textit{boxing} rule in . Indeed, Guerrini's counterexample
 uses both boxing and unboxing.

We write  iff , where
 is the reduction relation generated by the previous
rewriting rules  and  is the 
equivalence relation defined in Figure~\ref{f:eqo}
but restricted here to the -syntax. As before,  denotes the contextual closure
of , for  .

We now show some properties of the new memory reduction system which are 
used in Section~\ref{s:iep-for-lauxm} to show PSN.

 
\begin{lem}
\label{l:sous-terme-substitution}
Let . If  and , then
. 
\end{lem}

\begin{proof} By induction on . 
\end{proof}



\begin{lem}
\label{l:newu-pass-to-sub} 
Let . 
\begin{enumerate}[]
\item If  then .
\item   If  then .
\end{enumerate}
\end{lem}


\begin{proof}
Straightforward.
\end{proof}







\begin{lem}
\label{l:gen-new} 
Let .
Let .
Then , where  
      and  and .      
\end{lem}

\begin{proof}
Straightforward, the case  happening in particular
when  and  is empty.
\end{proof}


\begin{lem}
\label{l:out-subs}
Let . If  then
. 
\end{lem}

\begin{proof}
By induction on . 
\end{proof}



\begin{lem}
\label{l:substituion-new-u-o}
Let . 
If ,
 and ,
then . 
\end{lem}


\proof
By induction on the number of steps from  to , and in the
base case by induction on the reduction step from  to . 
\begin{enumerate}[]
\item ,  
      where 
      and . 
     Then  and  and   so that 
     
\item , where .
        Then,
           
  \item , where .
        Then,
         
  \item , where .
 Then, 
          
\item All the remaining cases are straightforward.       
\qed 
\end{enumerate}



\begin{cor}
\label{c:stability-substitution}
Let . 
If ,  and ,
then . 
\end{cor}

\proof
By induction on the number of -steps in the reduction 
.
Note first that  and  do not loose free variables.

\begin{enumerate}[] 
\item If there is only one -step, then the reduction is of the form .  We have .
\item If there are  -steps, then the reduction is of the
  form , with  -steps from  to
   we consider two cases. 


If , then  is lost in the subsequence 
. We thus have
.

If , then
.
\qed
\end{enumerate}

\subsection{Preservation of -Strong Normalization for  }
\label{s:iep-for-lauxm}

The proof of PSN for  we are going to develop in this
section is based on the \iep\ property (\cf\ Section~\ref{s:lj-psn}) and follows the main lines of
that of Theorem~\ref{t:ieg}.  
Indeed, given  and
 we show that
 by using a measure on terms
which decreases for every  one-step -reduct of .
However, 
PSN for  is much more involved: first because of the nature of the reduction rules , 
second because of the equivalence . 

A first remark is that jumps
in  are all void  so in particular 
they
cannot be duplicated. As a consequence, 
there is no need at first sight to generalise the \iep\ property to terms of
the form  as we did before 
(Theorem~\ref{t:ieg}). However,
there are now new ways of getting jumps 
\textit{on the surface} of
the term. Indeed, if  and  one 
has 
Things are even more complicated since jumps
can also be moved between the arguments 
as in: 
 
The opposite phenomenon can happen too, \ie\
the jump  can enter inside , for instance:
 
The main point is that the
measure we shall use to develop the proof of the \iep\ property needs to be stable by 
the equivalence , \ie\ 
if , then  and 
must have this same measure. 

In order to handle this phenomenon we are going to split  in two
parts: the multiset  of \deft{jumps} of 
which \textit{are} or \textit{can} get to the \deft{surface}, and the 
\deft{trunk}
, \ie\  the term obtained from  by removing all the jumps in
. This \textit{splitting} of the term is then used to
generalise the statement of the \iep\ as follows:

\begin{center}
If  and  for every 
 then .
\end{center}



An intuition behind the scheme of this proof is that the term
 and the jumps in  are dynamically
independent, in the sense that any reduction of  can be seen as an
interleaving of a reduction (eventually empty) of  and
reductions (eventually empty) of elements of . Indeed,
the void jumps in  \textit{cannot be affected} by a
reduction of , since none of their free variables is bound
in , and \textit{cannot affect} a reduction of  since
they are void. The unboxing rule slightly complicates things, but
morally that is why the new generalised form of the
\iep\ property holds.\medskip

The attentive reader
may wonder why we cannot handle the equivalence  by using a strong
bisimulation argument, as in the case of  (\cf\ Theorem~\ref{thm:eqo-conf-psn}). 
Unfortunately,  is not a strong bisimulation 
for  as the following example shows:
 

Before starting with the technical details of the proof let us add two
more important remarks. First, we have just used  and
 for didactic purposes, the actual definitions are
parametrised with respect to a set of variables (those which can be
captured in the context containing ).  Moreover, in order to
simplify the proofs we will not work  directly with :
we are going to define a parametrised predicate ,
which is true when all the jumps in  are in , and
a parametrised measure ,  built out from the elements
of .  Second, the unboxing rule makes some inductive
reasonings non-trivial, so we isolate them in an intermediate lemma
(Lemma.~\ref{l:lauxmes-red-unb2}).\medskip


Given  and 
a set of variables , 
the trunk  is given by the following inductive definition:
 
Note that  and  implies
. Next, we define a predicate on  which is true when all surface jumps contain terminating terms: 
 
Observe that  implies in particular
 for any set .

For any term  s.t.  we define the following multiset measure:
 
Now, we can reformulate a generalised statement for the \iep\ property on {\bf V}oid jumps as follows:

\begin{center}
(\viep)  For all  , if    and 
, then . 
\end{center}





Some lemmas about basic properties of ,  and  follow.

\begin{lem}
\label{l:surf-snsudd-no-var}
Let  and . Then 
 and 
 iff .
\end{lem}

\begin{proof}
Straightforward.
\end{proof}


\begin{lem}
\label{l:surf-sub}
Let  s.t. . Then,
\begin{enumerate}[\rm(1)]
  \item \label{p:surf-sub-one}.
  \item \label{p:surf-sub-two}If  then 
        .
\end{enumerate}
\end{lem}

\proof \hfill
\begin{enumerate}[(1)]
\item Straightforward induction on . 
\item By induction on .
\begin{enumerate}[]
  \item .  Then . 
  \item .  Then .
\item The cases  and  are straightforward using the \ih
 \item . 
Let us analyse one particular case in detail, the other ones being similar
    can be proved by application of the definitions and the \ih\ Let us suppose 
  ,  and
  . Then 



. 
The  \ih\ gives 

  
  and so we conclude with
 
\end{enumerate}
\end{enumerate}


\begin{lem}
\label{l:surf-pred-sub}
Let  and . If ,
 and  then .
\end{lem}

\begin{proof} 
 By induction on  using Lemma~\ref{l:surf-snsudd-no-var}.
\ignore{
\begin{itemize}
\item . Then
 which holds by hypothesis.
\item . Then
 which always holds. 

\item . Straightforward by the \ih\

\item . W.l.g we assume . We have that
   and .
  Moreover,  and so
  .

The \ih\ then gives .


\item . Then
   iff
  . We reason by cases:

\begin{enumerate}
\item \underline{}:  iff  . In order to apply the \ih\ to conclude we need to prove that
\begin{enumerate}
\item  holds, which follows from the
  hypothesis since  iff

  .
\item , which
  follows from the hypothesis
  since 
\end{enumerate}
\item \underline{,  }:
   iff
   and . We have by the
  hypothesis , which is equivant to
   and . In order to
  apply the \ih\ to  to conclude we need
  to show , which
  holds by the hypothesis because
  .
\item \underline{,  and }:  exactly as case 1.
\item \underline{,  and
  }:
   iff
   and . We
  have by the hypothesis ,
which is equivalent to . The term
   can
  be written
  , 
  from which we get  and
  , which allows
  to apply the \ih\ and conclude.

\end{enumerate}
\end{itemize}
}
\end{proof}


\begin{lem}
\label{l:mes-eqo}
Let  s.t.    and 
. If  then 
 and 
and . Thus in particular
the equality  holds.
\end{lem}


\begin{proof}
By induction on .
\end{proof}

The next lemma deals with the unboxing rule, which requires a complex induction.

\begin{lem}
\label{l:lauxmes-red-unb2}
Let  s.t.    and 
. If , 
where  does not bind ,  then  and: 
\begin{enumerate}[]
  \item If  then 
  \begin{enumerate}[]
  \item Either  and ,
  \item Or ;
  \end{enumerate}
  
  \item If  then
  .
\end{enumerate}
\end{lem}

\proof
By induction on .
\begin{enumerate}[]
\item Base cases:
\begin{enumerate}[]
\item : then 
  . Hence
   and
   iff . There are two cases:
\begin{enumerate}[(1)]
  \item \underline{}: then 
    . To show 
     we need  . The first point is equivalent to
    , which holds by hypothesis, the second 
    holds since   is a
    subterm of .
\item \underline{}: then 
  . To show 
   we need  , which holds by hypothesis. 
 \end{enumerate}
\item : there are four cases:
\begin{enumerate}[(1)]

\item \emph{}:
    then 
    . Also, 
    so that  
    . To show 
     we need 
    , which clearly
    follows from . We still 
    need to  show that
     which holds because
     is just  where 
    the multiset  is replaced by the strictly smaller multiset
    . 

\item \emph{}: 
   then 
   
  Also, 
   implies . 
  To show 
   we need ,
  which then holds by hypothesis and because  is a subterm of
  .


 \item \emph{}: then
   

   Also,  
    implies  . To show
    we need 
   , which holds
   by the hypothesis and the fact that  is a subterm of a -reduct of 
   . 


\item \emph{}: then
  

  Also 
   implies  , which is equivalent to 
  . 

\end{enumerate}
\end{enumerate}
\item Inductive cases:
\begin{enumerate}[]

\item : 
  We have .
  Also 
   and the hypothesis
    and  imply  in particular
   and .
  The \ih\ gives  and we distinguish 
  several  cases: 
  \begin{enumerate}[(1)]
  \item \emph{}: 
        The hypothesis   implies in particular
         and the \ih\ 
         gives
        , so 
        we conclude also . We now consider two cases:
        \begin{enumerate}[(a)]
        \item If 
              and , 
        then  and 
        .
        
        \item If , then .
        \end{enumerate}        
  \item \emph{}: 
        The  \ih\ 
         gives
        , so 
        we conclude also . We now consider two cases:  
         
        \begin{enumerate}[(a)]
        \item If 
              and , 
        then  and 
        .
        \item If , then 
            
        \end{enumerate}   
  \item \emph{}: Then the \ih\ gives
    . We consider the following cases.

    \begin{enumerate}[(a)]
    \item \emph{}:  then
\begin{center}\end{center}

Also  
          implies  and the \ih\ 
          implies  , we thus conclude
          . 
    \item \emph{}: then
\begin{center}\end{center}

  Also, the  \ih\ 
          implies  , we therefore conclude
          .
          

    \end{enumerate}

  \end{enumerate}


\item The cases  and
              are similar to the previous ones. 

\ignore{
\item : we have
   and by definition
  . We also have that
  , the hypothesis 
  imply  and
  . Thus, the 
   \ih\ gives   and for the rest we reason  by cases:

  \begin{enumerate}
  \item \emph{}: the hypothesis
     is equivalent to
    , 
    \ie\ . Then:
  
  \begin{enumerate}
  \item If , then
    
   
  \item If  and
  
  then

   


  Then .
     \end{enumerate}

\item \emph{}: then 
 iff , 
which follows from the \ih\  The \ih\ also gives 
, thus 



\end{enumerate}  

\item : we have also 
   and 
  the hypothesis imply 
   and
  . Thus, the 
  \ih\ gives  and the rest is by cases:

\begin{enumerate}
  \item \emph{}: then
     iff
     iff
    .

\begin{enumerate}
\item If  then 


\item If  and  then 

 
We get that
 and
,
hence .
\end{enumerate}

\item \emph{}: then
   iff
   iff
  . The \ih\ gives
  , so that 


\end{enumerate} }
\qed
\end{enumerate} 
\end{enumerate}


The following lemma states that the measure we use for proving
\viep\ for  decreases with every rewriting step.

\begin{lem}
\label{l:lauxmes-red}
Let  s.t.    and 
. If  then  and
\begin{enumerate}[]
  \item Either  or 
  \item  and .
\end{enumerate}
\end{lem}

\proof
By induction on .
\begin{enumerate}[]
\item Base cases:
\begin{enumerate}[]
  \item If , where . 

Let ,
 and
. Define   the sublist of  containing only the elements in .  We have
\begin{enumerate}[]
  \item  iff  and
     for every .
  \item .
\end{enumerate}
There are two cases:
\begin{enumerate}[(1)]
  \item \emph{}. We have
    . Then
    . Moreover,
     iff  and 
    for every  , which holds by 
    the hypothesis .
\item \emph{}. We have
  .  Then .  Moreover, 
  iff  and  and  for every . The first and third parts follow
from the hypothesis  while the second one follows from the hypothesis
  .
\end{enumerate}

  \item , where .  

Let , ,  and  be as in the previous case.
We have 
\begin{enumerate}[]
  \item  iff  and
     for every .
  \item  with .
\end{enumerate}

Then 
    . Thus in particular .

 Since  is a
    subterm of , then   and so
    . Then  iff  and  for every . The first part  holds by  Lemma~\ref{l:surf-pred-sub}, the second one from the
hypothesis . 

\item , where 
  ,  for all  and . There are two cases:
  \begin{enumerate}[(1)]
  \item \emph{}: we have that  implies
    . Then , moreover the multiset
     of
 is replaced by the following multiset of
        : 
      .  Since  and  we thus
        conclude .

\item \emph{}: let  and 
   as in de -case. Then   iff
  the terms in  are  and 
  holds: the former requirement holds because
   and so , the
  latter because   iff 
  . Last, ,
  where  is the list of substitutions associated to the elements
  in , then 


  \end{enumerate}

\item . This case holds by Lemma~\ref{l:lauxmes-red-unb2}.
\end{enumerate}

\item Inductive cases:
\begin{enumerate}[]
\item 
  where .  We consider three cases.
 
  \begin{enumerate}[(1)]
  \item  \emph{}: 
  We have .
  Also   implies  so that 
    and thus .
  Finally, 
  
  
\item \emph{ } : We have
  .  Also
   and
  , thus
  .
 

\item  \emph{}:   
 We have that  implies  , 
so that  and . 

Then .  

   \end{enumerate}
\ignore{
\item , where .
This case is straightforward  by the \ih\
\item , where .
This case is also straightforward  by the \ih\
\item : just use the \ih\
\item : just use the \ih }
\item All the other cases are straightforward.
\qed
\end{enumerate}
\end{enumerate}



\begin{thm}[\viep\ for ]
\label{t:ie-laux}
Let  s.t.    and 
, then . 
\end{thm}

\begin{proof}
We proceed by induction on the measure . 
To show  it is sufficient to show
 for every -reduct of .
Take any of such reducts . Then Lemmas~\ref{l:mes-eqo} and ~\ref{l:lauxmes-red}
guarantee 
 and 
. Moreover,  preserves  and 
 strictly decreases . We thus
apply the \ih\ to conclude. 
\end{proof}


The following  is a  consequence of the  previous theorem: let  -terms   and .  If  and  holds, \ie\ ,
    then . Hence:


\begin{cor}[\iep\ for ]
\label{t:ielaux}
The -calculus enjoys the \iep\ property.
\end{cor}


\begin{cor}[PSN for ]
\label{c:psn-lauxm}
The -calculus enjoys PSN, \ie\ if , then 
.
\end{cor}

\begin{proof} By Theorem~\ref{t:ie-implies-psn} it is sufficient to
show {\bf F0}, {\bf F1} and {\bf F2}. The first two properties are straightforward. 
To show  assume  and
, both are -terms.
Then in particular . We show that
 by induction on
.
For that, it is sufficient to show that every -reduct of  is
in .  If the -reduct of  is internal we conclude
by the \ih\ If  with , then  by hypothesis. If , then  by the
\iep\ property (Corollary~\ref{t:ielaux}).  There is no other possible -reduct of 
which is a  -term and has no jumps.
\end{proof}

\subsection{Projecting   into }
\label{s:projection}

In order to relate the  and the  calculi 
we define a projection function from -terms to -terms:

Notice that . Also,  if . 


We now state some basic static properties of .


\begin{lem}
\label{l:basic-properties-proj} Let . Then, 
.
\end{lem}

\begin{proof}
By induction on .
\end{proof}



\begin{lem}[Projection]
\label{l:projection}
Let . Then, 
\begin{enumerate}[\rm(1)]
\item \label{ppp-i}  implies .
\item\label{ppp-iii}   implies 
.
\item \label{ppp-iv}  implies .
\item \label{ppp-v}  implies .
\end{enumerate}
\end{lem}


\proof \hfill
\begin{enumerate}[]
\item Base cases:
\begin{enumerate}[]
\item .

Let  (resp. ) be the sequence of jumps
(resp. the meta-level substitution) resulting from the 
projection of , \ie\ .

If , then:
 

 

If , then: 
 

\item   where  .  
Then . 
\item    where  .
Then .
\item    where 
. Then 
. 
\item   where  .
There are two cases:  

If  or , then we obtain .

If  and , then 
  


\item    
where .
There are two  cases:

If , then .

If , then .

\item    where 
.

There are two cases:

If , then 
. 

If  , then 
.



\item . Then trivially  .


\end{enumerate}

\item The inductive cases:  

\begin{enumerate}[]
\item ,
  where . If  or  then the
  property is straightforward by the \ih\ So let us suppose  (so that the reduction step is
  necessarily a -step). We have . But  
  implies 
  so that in particular we have  .
Then  holds by Corollary~\ref{c:stability-substitution}.


\ignore{
\item If  is an abstraction or an application the property is straightforward
      by the \ih\
\item , 
      where ,
      then the property is also straightforward
      by the \ih\
}
 \item All the other cases are straightforward. 
\qed
\end{enumerate}
\end{enumerate}



Here are some interesting examples:




The previous property allows us to conclude with one of the main
results of this paper. 

\begin{thm}[PSN for ]
\label{t:psn-ldisf}
The -calculus enjoys PSN, \ie\ if , then 
.
\end{thm}

\begin{proof} 
We apply Theorem~\ref{t:equational-abstract}, where ,
, 
,  is
,  is  and
. Property {\bf (P0)} holds by
Lemma~\ref{l:projection}:\ref{ppp-iv}-\ref{ppp-v}, Property {\bf (P1)}
holds by Lemma~\ref{l:projection}:\ref{ppp-iii}, Property {\bf (P2)}
holds by Lemma~\ref{l:projection}:\ref{ppp-i} and
Property {\bf (P3)} holds by Corollary~\ref{l:dis-f}.  Now, take  so that Corollary~\ref{c:psn-lauxm}
gives . Since , then  by application of the Theorem.
\end{proof}

\section{Consequences of the main result}
\label{s:cons}

In this section we show how the strong result obtained in
  Section~\ref{s:projection} can be used to
  prove PSN for different variants of
the -calculus.
\subsection{Adding  to }
\label{s:ldisf+n+u}
We show that the reduction relation
 of
 can be added to  without breaking PSN. The main point
of this extension is to show that it is safe to consider unboxing (for void jumps)
 together with the box equations (for non-void jumps). 
For that, we first extend the 
rule 
to act on the whole set  and not only on  (but
  they still concern void substitutions only). Boxed contexts are extended to non-void jumps as expected, namely:

Then the rule is given by:
 
 Indeed, the  function maps -reduction steps of
  into -reduction steps of , as the next lemma shows.

\begin{lem}[Extended Projection]
\label{l:projectionext}
Let . Then, 
 implies  .
\end{lem}

\proof
By induction on the reduction relations. 
\begin{enumerate}[]
\item   where  does not
  bind   and .  We show a stronger property, namely: 

 If   where  does not
  bind  and , then .   
Then the property we want show is just a particular case of the stronger property. By -conversion
  we assume w.l.g. 
  that
   is not even free in .

We reason by induction on .
  \begin{enumerate}[]
    \item . Then  so that . 
    \item .  
       \ignore{We have .

        If , then: 
         
      
        If , then: 
          
      }
       Then we conclude by using the \ih\ and the
        equivalence . 
      
      \item .  
        \ignore{We have 
        . 

        If , then: 
          
      
        If , then: 
       
        }
        Then we conclude by using the \ih\ and the
        reduction  . 
        
  \item .  
        \ignore{We have .

        If , then: 
        .
      
        If , then .
        }
        Then we conclude by using the \ih\ and the
        equivalence . 
        
\item  . 
        We reason by cases. 


        If , then:
         

If , then:
         
        

\ignore{        
Now suppose .  Again we reason by cases. 

If , then:
         

If , then:
         
}
\item  .  Note that , otherwise the rule cannot be applied.         We reason by cases. 


        If , then:
         

If , then:
         
           
\ignore{If  , 
Then we conclude by using the \ih\ and 
 Lemma~\ref{l:newu-pass-to-sub} if . }



     \end{enumerate}
\item The inductive cases for the  abstraction, the application 
      and reduction inside substitution are straightforward. 

\ignore{
\item , where  (resp. ).
        
        If , then:
         
       
        If , then:

          

}       

\item , where  (resp. ).
       Since  preserves free variables, then
   or  
so that the
  property is straightforward by the \ih\
\qed
\end{enumerate}




\begin{thm}
\label{t:psn-enriched}
The -calculus enjoys PSN, \ie\ , then .
\end{thm}

\begin{proof}
We apply Theorem~\ref{t:equational-abstract}, where ,
, ,
 is ,  is  and
. Property {\bf (P0)} holds by
Lemma~\ref{l:projection}:\ref{ppp-iv}-\ref{ppp-v}, Property {\bf (P1)}
holds by Lemmas~\ref{l:projection}:\ref{ppp-iii}
and~\ref{l:projectionext}, Property {\bf (P2)} holds by
Lemmas~\ref{l:projection}:\ref{ppp-i}. To show Property {\bf (P3)} we
proceed as follows. First of all notice that
 is trivially terminating, then show that
 is terminating by showing that  implies , where the first component of the pair is compared
with respect to the multiset order, the second with respect to the
terminating relation , and the
stability of  by , which is given by Lemma
\ref{l:mul-for-eq}:\ref{p:mul-for-eq-two}.  Now, take  so that Corollary~\ref{c:psn-lauxm}
  gives . Since , then  by application of the
  Theorem. 
\end{proof}


\subsection{Orienting the axioms of }

Another interesting result concerns a more
traditional form of explicit substitutions calculus, called here the 
\deft{inner structural -calculus},  and noted ,  whose rules appear in Figure~\ref{fig:inn-oriented-f}.

\begin{figure}

\caption{\label{fig:inn-oriented-f} The inner structural -calculus }
\end{figure}


Let  be the context closure of the rules  modulo .

\begin{lem}
\label{l:in-terminates}
The reduction relation  is strongly normalising.
\end{lem}

\begin{proof}
Define  to be the sum of all the sizes of the subterms of  directly
affected by jumps. It is easily seen that 
such a measure strictly decreases 
by one-step rewriting and is invariant by 
.
\end{proof}

\begin{cor}
The inner structural -calculus   enjoys PSN.
\end{cor}

\begin{proof}
By application of Theorem~\ref{t:equational-abstract},
where the required properties 
of the projection of  into 
are guaranteed by Lemmas~\ref{l:projection}
and~\ref{l:in-terminates}.
\end{proof}

\noindent The inner structural -calculus can be seen as a refinement of
Kesner's ~\cite{Kes07}, an explicit
substitution calculus related to Proof-Nets, whose rules are in
Figure~\ref{fig:les-rules}.

\begin{figure}
\begin{center}
 \begin{tabular}{llllllll}
&&  \\
&& \\
&&   & if  \\
&& & if  and \\
&& & if  and \\
&&& if  and \\
&&\\
    & &   & if  and \\
    & &   & if  and \\\\
 & &  & if  and  \\ &&&(and )
\end{tabular}
\end{center}
\caption{The -calculus\label{fig:les-rules}}
\end{figure}

Indeed, only rules  are not particular
cases of rules of , but they can be decomposed 
by using duplication followed by propagations 
as follows:


It is then straightforward to simulate  inside , so we get:


\begin{cor}
\label{cor:les-psn}
The -calculus enjoys PSN.
\end{cor}

The second author shows in~\cite{Kes09}
that from PSN of  one can infer PSN of a wide range of calculi,
, Kesner's  and ~\cite{Kes07},
Milner's calculus ~\cite{Milner07}, David's
and Guillaume's ~\cite{DBLP:journals/mscs/DavidG01}, the
calculus with director strings of~\cite{SinotFM03}. Hence PSN for  encompasses
most results of PSN in the literature of explicit substitutions.

\noindent The interesting feature of  with respect to  is that
the propagation subsystem  is not needed in order to
compute a normal form. Propagations are rather (linear)
re-arrangements of term constructors which may be used as the basis of
some term transformations used for compilation or
optimisation.\medskip

The strength of a splitting of the whole calculus into a core and 
propagation system lies in the fact that 
the latter can be changed without affecting the former. In
particular, it is possible to orient the axioms  in the opposite direction 
by getting the \textit{outer} structural -calculus
, whose rules are in Figure~\ref{fig:out-oriented-f}.

Observe that  in contrast  to the inner  calculus the outer  box rules
act also on  void jumps,  \ie\ they  are not
just an orientation  of the box equations, but  an extension too. This
is  possible because  --- as  showed  earlier (Theorem
\ref{t:psn-enriched})       ---       extending             with
unboxing for void jumps is safe (while we
do  not know  whether it  is safe  to extend   with
boxing for  void  jumps).    Let
  be  the derived  context  closure  of  the outer  rules
 modulo .


\begin{lem}
\label{l:out-terminates}
The reduction relation  is strongly normalising.
\end{lem}

\ignore{
\begin{proof}
Informal proof: let  denote the size of a context. Then define 
, \ie, the sum of the size of the contexts containing each jump of . It is easily seen that such a measure decreases with any -step and is invariant by .
\end{proof}}


\begin{cor}
The outer structural -calculus  enjoys PSN.
\end{cor}

\begin{proof}
By application of Theorem~\ref{t:equational-abstract},
where the required properties 
of the projection of  into 
are guaranteed by Lemmas~\ref{l:projection}
and~\ref{l:out-terminates}.
\end{proof}


\begin{figure}

\caption{\label{fig:out-oriented-f} The outer structural -calculus }
\end{figure}

In fact, it is easily seen that no matter how the axioms  are oriented that they get a terminating
rewriting system. As for  and ,  
PSN can also be proved for the remaining 14
derived calculi, even if it is not clear to what extent
they  would be interesting.

\ignore{
It is easily seen that  is confluent, let us note  its normal form (modulo ). As we showed in Section~\ref{s:eq-th} (example at page \pageref{it:in-out-instability}) -normal forms are not stable by reduction, \ie  does not imply . However:

\begin{lem}
\label{l:out-red}
If  then , namely:
\begin{enumerate}
\item  then  for .
\item  then  for .
\end{enumerate}
\end{lem}

\begin{proof}
By induction on .
\end{proof}

Observe that there are cases where a duplication/erasure step on  is simulated by more than one step on , for instance:



This means that duplications and erasures in  act on smaller parts of the term than in  (unless ), without loosing anything. Hence,  has \deft{minimum jumps}. 

Dually, the relation  of the inner  is confluent and one gets that jumps in  are \deft{maximum}:

\begin{lem}
\label{l:inn-red}
If  then , namely:
\begin{enumerate}
\item  then  for .
\item  then  for .
\end{enumerate}
\end{lem}

\begin{proof}
By induction on .
\end{proof}

As before there are cases where a duplication/erasure step on  is simulated by more than one step on , for instance:



It is possible to modify -dags so that all the terms of an
-equivalence class are mapped on the same graph. Furthermore, on
such a syntax it is possible to always reduce the minimum or the maximum
jump associated to a substitution. From the fact that -classes
are identified follows that one thus gets  and . This is indeed one
of the motivation which lead us to consider the equivalence relation . A
forthcoming paper about -dags will prove these facts. }

\subsection{Adding equations to -terms}
\label{ssec:rene}
We briefly present here the results of~\cite{AKLPAR}, which
extends and complement those of this paper. As discussed in
Section~\ref{s:regnier}, the equations  and
 can be seen as a jump reformulation of Regnier's
-equivalence on -terms after the elimination of -redexes. It is
also possible to apply the -rule in the other sense (\ie\ as a -expansion)
to the equations  in order to obtain other
equations  on -terms. If 
and , the equation  can be -expanded to the new equation :
 
Axiom  is a more general
instance of the rule called 
~\cite{Moggi89,DBLP:journals/corr/abs-0806-4859,DBLP:journals/tcs/David11}
(which usually is not taken modulo but oriented from right to left). The axiom
 -expands to a special case of
, and thus it is subsumed 
by it. Indeed:
 
Last, one can turn the unboxing rule into its -calculus form, getting:
\vskip2 pt

\noindent Let  be defined as the smallest equivalence relation containing
 and . In \cite{AKLPAR} we show
that the -calculus
in Figure~\ref{fig:final-calc} enjoys PSN. The proof is obtained via a
simple function which eliminates -redexes, and that project this
calculus over the -calculus,
whose PSN is given by Theorem \ref{t:psn-enriched}. The main result of
\cite{AKLPAR}, however, is that the the -calculus is also Church-Rosser modulo the
whole equational theory. This is proved via -developments, a
new notion of development taking advantage of jumps. Actually, in
\cite{AKLPAR} we use a macro-steps substitution rule  instead of our subsystem : we do so
because the fine granularity of  plays no role in the
proof of these properties, their refinement to  is
straightforward.\medskip

Let us call \deft{permutative -calculus} (see Figure
\ref{fig:perm-lambda}) the set of -terms plus the operational
semantics given by , where
 is the smallest equivalence relation containin ,
, .  Such a calculus can be (strictly) simulated into
the -calculus and
thus it enjoys PSN. This result generalises all known results in the
literature about PSN for -calculus extended with permutative
conversion
\cite{DBLP:journals/tcs/David11,DBLP:journals/tcs/Santo11,DBLP:journals/corr/abs-0806-4859}. In
\cite{AKLPAR} we also prove that it is Church-Rosser modulo .


\begin{figure}[ht]

\caption{\label{fig:final-calc} The structural -calculus modulo}
\end{figure}


\begin{figure}

\caption{\label{fig:perm-lambda} The permutative -calculus}
\end{figure}

\ignore{
\subsection{Turning  into an equality}

The fact that in  each term as an equivalent representation with minimum jumps can be strengthened by turning  into an axiom. The idea is that for instance in the term  where  it is possible to bring  out of the jump by means of , indeed:



so that when the jump on  has to be duplicated a smaller term is involved. This subject is sensible in sharing implementations of weak -calculi (il faut citer ici les papiers cit par thibaut dans son nouveau papier). In particular, one can get minimum bodies for -abstractions and jumps. To formalise this let us orient  in the unusual direction, defining  if  and  is not a variable. Now consider .

\begin{lem}
 is strongly normalising and confluent modulo .
\end{lem}

Then let us note  the -normal form of a term . Similarly to the previous section one gets that:

\begin{lem}
\label{l:out-red-var}
If  then , namely:
\begin{enumerate}
\item  then .
\item  then  for .
\end{enumerate}
\end{lem}

\begin{proof}
...
\end{proof}

Observe that there are cases where a duplication/erasure step on  is simulated by more than one step on , for instance: if  then



The projection  maps reductions  into equalities (\ie ). To infer that the calculus  modulo  enjoys PSN we just need to show that  modulo  is strongly normalising, which is trivial 
(c'est vrai? J'en suis pas sur, la measure qu'on utilise pour montrer que  termine decroit avec les pas , probablement il faut juste la modifier). Thus:

\begin{thm}
The calculus  modulo  enjoys PSN.
\end{thm}

Similarly, it is possible to turn  or  into 
an axiom and show that PSN still hold. Actually, it is also possible
to turn any two reductions out of  into
congruences at the same time and still get PSN. Instead, it does not
make sense to turn the three of them into congruences, otherwise the
reduction (where ):


would be collapsed into an equality.
}


\section{Conclusions}



We  have introduced  the  structural -calculus,  a concise  but
expressive   -calculus  with  jumps   admitting
graphical interpretations  by means of s  and Pure Proof-Nets.
Even if   has strong linear logic background,  the calculus can
be understood as a particular reduction system, based on the notion of
multiplicity and reduction at a distance, and being independent from
any  logic or  type system.  We established  different  properties for
 such as confluence  and PSN.  Moreover, full composition holds
without any  need of  structural composition nor  commutation of
jumps.  The -calculus  admits a graphical operational equivalence
 allowing to commute jumps with linear constructs.  The relation
 can  be naturally understood  as Regnier's -equivalence
on -terms  and turns out  to be a strong  bisimulation.  Moreover,
 can be further  extended to the substitution equivalence 
allowing  to  commute  also  jumps  and  non-linear  constructs.   The
resulting  calculus enjoys PSN,  a non-trivial  result from  which one
derives several known PSN results.

PSN  of   modulo    is shown  by  means  of an  auxiliary
calculus    which  can  be  understood  as  a  \textit{memory}
calculus specified by  means of \textit{void} substitutions.  
A memory calculus due to Klop ~\cite{Klo} is often used for termination
arguments. Its syntax is usually presented as follows:

where  for every term  and the memory construct  
is used to collect in  the  arguments of the erasing -redexes. The rule associated to this calculus are:

If       one      interprets              as         then   Klop's
calculus  can be  mapped into  :   maps to   and
         becomes        the         reduction        rule
,  which is  subsumed  by
the equation  of . 
Indeed,  
is more expressive than  Klop's calculus.  We
claim that   is  interesting on  its own and  can be  used for
proving termination results beyond those of this paper.

We do not know whether 
   extended with unrestricted \textit{boxing}, in contrast to
     extended with unrestricted \textit{unboxing} presented in 
  Section~\ref{s:ldisf+n+u}, enjoys PSN.  The
  point is delicate, indeed from the literature (\cite{Mellies1995a}) we know
  that unrestricted boxing together with the following
  traditional explicit substitution rule (without side condition on
  ):


\noindent break PSN. Now, the rule  cannot be simulated in
, so it would be interesting to
 understand if  plus unrestricted boxing
enjoys PSN. 

An  interesting research  direction  would be  to  formalise the  link
between  ,  linear  logic  and abstract  machines.  Indeed,  in
contrast  to  explicit substitution  calculi,  
naturally     expresses     the      notion     of     linear     head
reduction~\cite{Danos99opt},  which  relates   in  a  simpler  way  to
Krivine's  Abstract  Machine~\cite{Krivinemachine}. This
is  because  linear head  reduction  performs  the  minimal amount  of
substitutions necessary  to find  which occurrences of  variables will
stand in head positions. While this is not a reduction strategy in the
usual  sense of  -calculus, it  can  be seen  as a  clever way  to
implement -reduction  by means of  proof-nets technology, which
can be reformulated in the -calculus as a strategy.


The  structural -calculus  has   been  used in~\cite{AK10}  to  specify  {\tt
  XL}-developments,  a terminating  notion  of reduction  generalising
those           of           development~\cite{Hindley78}          and
superdevelopment~\cite{KvOvR93}. It would be interesting to better understand {\tt
  XL}-developments.

It would also be interesting to exploit distance and multiplicities in
other frameworks dealing  for example with pattern matching,
continuations  or  differential  features.   A direction  which  seems
particularly  challenging is standardization
for . It  would be interesting  in particular to obtain  a
notion    of     standard    reduction   which     is    stable    by
-equivalence (or  at least  , so that  the result
would    pass   to   -dags).   Indeed,
classical  notions as leftmost-outermost  reduction do
not  easily generalise  to   modulo , where  jumps can  be
swapped and permuted with linear constructors.



\section*{Acknowledgements}
We would like to thank Stefano Guerrini
for stimulating discussions.


\bibliographystyle{is-abbrv}


\begin{thebibliography}{10}
\ifx \showCODEN  \undefined \def \showCODEN #1{CODEN #1}  \fi
\ifx \showISBN   \undefined \def \showISBN  #1{ISBN #1}   \fi
\ifx \showISSN   \undefined \def \showISSN  #1{ISSN #1}   \fi
\ifx \showLCCN   \undefined \def \showLCCN  #1{LCCN #1}   \fi
\ifx \showPRICE  \undefined \def \showPRICE #1{#1}        \fi
\ifx \showURL    \undefined \def \showURL {URL }          \fi
\ifx \path       \undefined \input path.sty               \fi
\ifx \ifshowURL \undefined
     \newif \ifshowURL
     \showURLtrue
\fi

\bibitem{AccattoliTh}
B.~Accattoli.
\newblock {\em Jumping around the box: graphical and operational studies on
  {L}ambda {C}alculus and {L}inear {L}ogic.}
\newblock Ph.{D}. {T}hesis, {U}niversit\`a di Roma La Sapienza, 2011.

\bibitem{AG09}
B.~Accattoli and S.~Guerrini.
\newblock Jumping {B}oxes. representing {L}ambda-{C}alculus {B}oxes by {J}umps.
\newblock In E.~Gr{\"a}del and R.~Kahle, editors, {\em Proc. of 18th Computer
  Science Logic (CSL)}, volume 5771 of {\em Lecture Notes in Computer Science},
  pages 55--70. Springer-Verlag, Sept. 2009.

\bibitem{AK10}
B.~Accattoli and D.~Kesner.
\newblock The structural lambda-calculus.
\newblock In A.~Dawar and H.~Veith, editors, {\em Proc. of 24thComputer Science
  Logic (CSL)}, volume 6247 of {\em Lecture Notes in Computer Science}, pages
  381--395. Springer-Verlag, Aug. 2010.

\bibitem{AKLPAR}
B.~Accattoli and D.~Kesner.
\newblock The permutative -calculus, 2012.
\newblock In N.~Bjorner and A.~Voronkov, editors, {\em Proc. of 18th  Int. Conference on Logic for Programming Artificial Intelligence and Reasoning (LPAR)}, volume  7180 of {\em Lecture Notes in Computer Science}, pages
  23--36. Springer-Verlag, March 2012.

\bibitem{Nipkow-Baader}
F.~Baader and T.~Nipkow.
\newblock {\em Term Rewriting and {\it All That}}.
\newblock Cambridge University Press, 1998.

\bibitem{BlooRoselx}
R.~Bloo and K.~Rose.
\newblock Preservation of strong normalization in named lambda calculi with
  explicit substitution and garbage collection.
\newblock In {\em Computing Science in the Netherlands}, pages 62--72. NCSRF,
  1995.

\bibitem{Danos99opt}
V.~Danos and L.~Regnier.
\newblock Reversible, irreversible and optimal lambda-machines.
\newblock {\em Theoretical Computer Science}, 227\penalty0 (1):\penalty0
  79--97, 1999.

\bibitem{DBLP:journals/tcs/David11}
R.~David.
\newblock A short proof that adding some permutation rules to preserves sn.
\newblock {\em Theoretical Computer Science}, 412\penalty0 (11):\penalty0 1022--1026,
  2011.

\bibitem{DBLP:journals/mscs/DavidG01}
R.~David and B.~Guillaume.
\newblock A lambda-calculus with explicit weakening and explicit substitution.
\newblock {\em Mathematical Structures in Computer Science}, 11\penalty0
  (1):\penalty0 169--206, 2001.

\bibitem{deBruijn87}
N.~G. de~Bruijn.
\newblock {G}eneralizing {A}utomath by {M}eans of a {L}ambda-{T}yped {L}ambda
  {C}alculus.
\newblock In {\em Mathematical Logic and Theoretical Computer Science}, number
  106 in Lecture Notes in Pure and Applied Mathematics, pages 71--92. Marcel
  Dekker, 1987.

\bibitem{DCKP03}
R.~Di~Cosmo, D.~Kesner, and E.~Polonovski.
\newblock Proof nets and explicit substitutions.
\newblock {\em Mathematical Structures in Computer Science}, 13\penalty0
  (3):\penalty0 409--450, 2003.

\bibitem{espiritoSanto2011}
J.~Esp\'{\i}rito~Santo.
\newblock A note on preservation of strong normalisation in the
  -calculus.
\newblock {\em Theoretical Computer Science}, 412\penalty0 (12-14):\penalty0
  169--183, 2011.

\bibitem{LL}
J.-Y. Girard.
\newblock Linear logic.
\newblock {\em Theoretical Computer Science}, 50, 1987.

\bibitem{GOI}
J.-Y. Girard.
\newblock Geometry of interaction {I}: an interpretation of system {F}.
\newblock {\em Proc. of the Logic Colloquium}, 88:\penalty0 221--260, 1989.

\bibitem{Hasegawa}
M.~Hasegawa.
\newblock {\em Models of Sharing Graphs: A Categorical Semantics of let and
  letrec}, volume Distinguished Dissertation Series.
\newblock Springer-Verlag, 1999.

\bibitem{HZ09}
H.~Herbelin and S.~Zimmermann.
\newblock An operational account of call-by-value minimal and classical
  lambda-calculus in "natural deduction" form.
\newblock In P.-L. Curien, editor, {\em Proc. of 9th Typed Lambda Calculus and
  Applications (TLCA)}, volume 5608 of {\em Lecture Notes in Computer Science},
  pages 142--156. Springer-Verlag, July 2009.

\bibitem{Hindley78}
J.~R. Hindley.
\newblock Reductions of residuals are finite.
\newblock {\em Transactions of the American Mathematical Society},
  240:\penalty0 345--361, 1978.

\bibitem{HuetThEtat}
G.~Huet.
\newblock {\em R\'esolution d'\'equations dans les langages d'ordre 1, 2, . . .
  , ?}
\newblock Th\`ese de doctorat d'\'etat, {U}niversit\'e {P}aris VII, 1976.

\bibitem{Kamareddine00}
F.~Kamareddine.
\newblock Postponement, conservation and preservation of strong normalization
  for generalized reduction.
\newblock {\em Journal of Logic and Computation}, 10\penalty0 (5):\penalty0
  721--738, 2000.

\bibitem{Kes07}
D.~Kesner.
\newblock The theory of calculi with explicit substitutions revisited.
\newblock In J.~Duparc and T.~A. Henzinger, editors, {\em Proc. of 16th
  Computer Science Logic (CSL)}, volume 4646 of {\em Lecture Notes in Computer
  Science}, pages 238--252. Springer-Verlag, Sept. 2007.

\bibitem{Kes09}
D.~Kesner.
\newblock A theory of explicit substitutions with safe and full composition.
\newblock {\em Logical Methods in Computer Science}, 5\penalty0 (3:1):\penalty0
  1--29, 2009.

\bibitem{OConchuirKesner}
D.~Kesner and S.~O. Conch\'uir.
\newblock Milner's lambda calculus with partial substitutions, 2008.

\bibitem{KL05}
D.~Kesner and S.~Lengrand.
\newblock Extending the explicit substitution paradigm.
\newblock In J.~Giesl, editor, {\em 16th International Conference on Rewriting
  Techniques and Applications (RTA)}, volume 3467 of {\em Lecture Notes in
  Computer Science}, pages 407--422. Springer-Verlag, Apr. 2005.

\bibitem{KL07}
D.~Kesner and S.~Lengrand.
\newblock Resource operators for lambda-calculus.
\newblock {\em Information and Computation}, 205\penalty0 (4):\penalty0
  419--473, 2007.

\bibitem{KR09}
D.~Kesner and F.~Renaud.
\newblock The prismoid of resources.
\newblock In R.~Kr{\'a}lovic and D.~Niwinski, editors, {\em Proc. of the 34th
  Mathematical Foundations in Computer Science}, volume 5734 of {\em Lecture
  Notes in Computer Science}, pages 464--476. Springer-Verlag, Aug. 2009.

\bibitem{KfouryW95}
A.~J. Kfoury and J.~B. Wells.
\newblock New notions of reduction and non-semantic proofs of beta-strong
  normalization in typed lambda-calculi.
\newblock In D.~Kozen, editor, {\em 10th Annual {IEEE} Symposium on Logic in
  Computer Science (LICS)}, pages 311--321. IEEE Computer Society Press, June
  1995.

\bibitem{Klo}
J.-W. Klop.
\newblock {\em Combinatory Reduction Systems}, volume 127 of {\em Mathematical
  Centre Tracts}.
\newblock Mathematisch Centrum, Amsterdam, 1980.
\newblock PhD Thesis.

\bibitem{KvOvR93}
J.-W. Klop, V.~van Oostrom, and F.~van Raamsdonk.
\newblock Combinatory reduction systems: introduction and survey.
\newblock {\em Theoretical Computer Science}, 121\penalty0 (1/2):\penalty0
  279--308, 1993.

\bibitem{Krivinemachine}
J.-L. Krivine.
\newblock Un interpr\'eteur du lambda-calcul.
\newblock Available on \url{http://www.pps.jussieu.fr/~krivine/articles/}.

\bibitem{DBLP:journals/corr/abs-0806-4859}
S.~Lengrand.
\newblock Termination of lambda-calculus with the extra call-by-value rule
  known as assoc.
\newblock {\em CoRR}, abs/0806.4859, 2008.

\bibitem{MaraistOTW99}
J.~Maraist, M.~Odersky, D.~N. Turner, and P.~Wadler.
\newblock Call-by-name, call-by-value, call-by-need and the linear lambda
  calculus.
\newblock {\em Theoretical Computer Science}, 228\penalty0 (1-2):\penalty0
  175--210, 1999.

\bibitem{Mellies1995a}
P.-A. Melli\`es.
\newblock Typed lambda-calculi with explicit substitutions may not terminate.
\newblock In M.~Dezani-Ciancaglini and
               G.~D.~Plotkin, editors, {\em Proc. of 2nd Typed Lambda Calculus and Applications (TLCA)},
  volume 902 of {\em Lecture Notes in Computer Science}, pages 328--334.
  Springer-Verlag, Apr. 1995.

\bibitem{Milner07}
R.~Milner.
\newblock Local {B}igraphs and {C}onfluence: {T}wo {C}onjectures (extended
  abstract).
\newblock {\em Electronic Notes in Theoretical in Computer Science},
  175\penalty0 (3):\penalty0 65--73, 2007.

\bibitem{Moggi89}
E.~Moggi.
\newblock Computational lambda-calculus and monads.
\newblock In R.~Parikh, editor, {\em 4th Annual {IEEE} Symposium on Logic in
  Computer Science (LICS)}, pages 14--23. IEEE Computer Society Press, June
  1989.

\bibitem{Ned92}
R.~P. Nederpelt.
\newblock The fine-structure of lambda calculus.
\newblock Technical Report CSN 92/07, Eindhoven Univ. of Technology, 1992.

\bibitem{OH06}
Y.~Ohta and M.~Hasegawa.
\newblock A terminating and confluent linear lambda calculus.
\newblock In F.~Pfenning, editor, {\em Rewriting Techniques and Applications
  (RTA)}, volume 4098 of {\em Lecture Notes in Computer Science}, pages
  166--180. Springer-Verlag, 2006.

\bibitem{regnier94}
L.~Regnier.
\newblock Une \'equivalence sur les lambda-termes.
\newblock {\em Theoretical Computer Science}, 2\penalty0 (126):\penalty0
  281--292, 1994.

\bibitem{Renaudth}
F.~Renaud.
\newblock {\em Les ressources explicites vues par la th\'eorie de la
  r\'e\'ecriture}.
\newblock Ph.{D}. {T}hesis, {U}niversit\'e {P}aris-{D}iderot, 2011.

\bibitem{DBLP:journals/tcs/Santo11}
J.~E. Santo.
\newblock A note on preservation of strong normalisation in the -calculus.
\newblock {\em Theoretical Computer Science}, 412\penalty0 (11):\penalty0 1027--1032,
  2011.

\bibitem{Schw99}
H.~Schwichtenberg.
\newblock Termination of permutative conversions in intuitionistic {G}entzen
  calculi.
\newblock {\em Theoretical Computer Science}, 212\penalty0 (1-2):\penalty0
  247--260, 99.

\bibitem{SinotFM03}
F.-R. Sinot, M.~Fern{\'a}ndez, and I.~Mackie.
\newblock Efficient reductions with director strings.
\newblock In R.~Nieuwenhuis, editor, {\em 14th International Conference on
  Rewriting Techniques and Applications (RTA)}, volume 2706 of {\em Lecture
  Notes in Computer Science}, pages 46--60. Springer-Verlag, June 2003.

\bibitem{Terese03}
Terese.
\newblock {\em Term Rewriting Systems}, volume~55 of {\em Cambridge Tracts in
  Theoretical Computer Science}.
\newblock Cambridge University Press, 2003.

\bibitem{Yoshida93}
N.~Yoshida.
\newblock Optimal reduction in weak-lambda-calculus with shared environments.
\newblock In {\em Proc. of Int. Conference on Functional Programming Languages
  and Computer Architecture}, pages 243--252. ACM Press, June 1993.

\end{thebibliography}





\end{document}
