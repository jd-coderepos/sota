\documentclass[10pt]{llncs}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usetikzlibrary{arrows,automata}

\usepackage{amsmath,latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{authblk}
\usepackage{algpseudocode}
\usepackage{algorithm}

    \renewcommand{\topfraction}{0.9}	\renewcommand{\bottomfraction}{0.8}	\setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     \setcounter{dbltopnumber}{2}    \renewcommand{\dbltopfraction}{0.9}	\renewcommand{\textfraction}{0.07}	\renewcommand{\floatpagefraction}{0.7}	\renewcommand{\dblfloatpagefraction}{0.7}	

\newtheorem{thrm}{Theorem}
\newtheorem{lemm}[thrm]{Lemma}
\newtheorem{defn}[thrm]{Definition}
\newtheorem{examp}[thrm]{Example}
\newtheorem{obs}[thrm]{Observation}
\newenvironment{dig}{\\ [6pt]\noindent {\bf Digression}}{~\\ [6pt]\indent}
\newenvironment{dig1}{\noindent {\bf Digression}}{~\\ [6pt]\indent}
\newtheorem{alg}{\hspace{1.3in} Algorithm}
\newenvironment{my_itemize}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}{\end{itemize}
}

\def\proof{\bigbreak\noindent {\sl Proof.\/}\enspace}
\def\qedbox#1#2{\vbox{\hrule height.2pt
  \hbox{\vrule width.2pt height#2pt \kern#1pt \vrule width.2pt}
  \hrule height.2pt}}
\def\qed{\hfill \quad\qedbox46\newline}


\def\dd{\mathinner{\ldotp\ldotp}}
\def\s#1{\mbox{\boldmath }}
\def\floor#1{\lfloor #1 \rfloor}
\def\bfloor#1{\big\lfloor #1 \big\rfloor}
\def\Bfloor#1{\Big\lfloor #1 \Big\rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\bceil#1{\big\lceil #1 \big\rceil}
\def\+{\!+\!}
\def\-{\!-\!}
\def\plmi{\!\pm\!}
\def\m{\!-\!}
\def\u#1{\underline{#1}}
\def\o#1{\overline{#1}}
\def\itbf#1{\textit{\textbf{#1}}}
\def\match{\approx}
\def\cP{\mathcal{P}}
\def\ACTIVE{\mbox{ACTIVE}}
\def\push{\bf{push}}
\def\pop{\bf{pop}}
\def\peek{\bf{peek}}

\def\bproc{{\bf procedure\ }}
\def\bfunc{{\bf function\ }}
\def\bvar{{\bf var\ }}
\def\barray{{\bf array\ }}
\def\bof{{\bf of\ }}
\def\bfor{{\bf for\ }}
\def\bto{{\bf to\ }}
\def\bdownto{{\bf downto\ }}
\def\bwhile{{\bf while\ }}
\def\brep{{\bf repeat\ }}
\def\buntil{{\bf until\ }}
\def\band{{\bf and\ }}
\def\bor{{\bf or\ }}
\def\bdo{{\bf do\ }}
\def\bif{{\bf if\ }}
\def\bthen{{\bf then\ }}
\def\belse{{\bf else\ }}
\def\belsif{{\bf elsif\ }}
\def\bnot{{\bf not\ }}
\def\bgoto{{\bf goto\ }}
\def\breturn{{\bf return\ }}
\def\boutput{{\bf output}}
\def\la{\leftarrow}
\def\ra{\rightarrow}
\def\llra{\Leftrightarrow}
\def\q{\quad}
\def\qq{\qquad}
\def\com#1{{\bf }\hspace{6pt}{\sl #1}}
\def\rem#1{\hspace{24pt}{\sl #1}}
\def\pref(#1,#2){ is a prefix of }
\def\suff(#1,#2){ is a suffix of }
\def\FIND{\mbox{FIND}}
\def\SA{\mbox{SA}}
\def\ISA{\mbox{ISA}}
\def\offset{\mbox{offset}}
\def\prev{\mbox{prev}}
\def\period{\mbox{period}}
\def\per{\textbf{period}}
\def\nextequal{\mbox{nextequal}}
\def\nexteq{\textbf{nextequal}}
\def\reg(#1,#2){ is -regular}
\def\notreg(#1,#2){ is not -regular}
\def\top{\tt{top}}
\def\pop{\tt{pop}}
\def\true{\tt{true}}
\def\false{\tt{false}}
\def\UPDATE\_F{\tt{UPDATE\_F}}
\def\LEAST{\tt{LEAST}}
\def\MERGE{\tt{MERGE}}
\def\L{\mathcal{L}}
\def\LCOMPUTE{\rm{LCOMPUTE}}
\def\ACTIVE{\rm{ACTIVE}}
\def\MVR*{\rm{MVR^*}}
\def\MVR{\rm{MVR}}
\def\MVRISA{\rm{MVRISA}}
\def\bpush{{\bf push}}
\def\bpeek{{\bf peek}}
\def\bpop{{\bf pop}}
\def\O{\mathcal{O}}
\def\MATCH{\rm{MATCH}}
\def\EXIT{\rm{EXIT}}
\def\COMP{\rm{COMP}}
\def\NSV{\mbox{NSV}}
\def\PNSV{\mbox{PNSV}}
\def\NPNSV{\mbox{NPNSV}}
\def\RM{\rm{RM}}
\def\freq{\rm{freq}}


\newif\ifProofs
\Proofsfalse


\newif\ifRev
\Revfalse

\begin{document}

\pagestyle{headings}
\title{Algorithms to Compute the Lyndon Array\thanks
{This work was supported in part by the
Natural Sciences \& Engineering Research Council of Canada.
The authors wish to thank Maxime Crochemore and Hideo Bannai
for helpful discussions.}
}
\author{Frantisek Franek\inst{1}
\and
A.\ S.\ M.\ Sohidull Islam\inst{2}
\and M.\ Sohel Rahman\inst{3}
\and \\ 
W.\ F.\ Smyth\inst{1,3,4}
}
\institute{Algorithms Research Group \\ 
Department of Computing \& Software \\ 
McMaster University, Hamilton, Canada \\
\email{franek/smyth@mcmaster.ca} \\ 
{\ } \\ 
School of Computational Science \& Engineering \\ 
McMaster University, Hamilton, Canada \\ 
\email{sohansayed@gmail.com} \\ 
{\ } \\ 
Department of Computer Science \& Engineering \\
Bangladesh University of Engineering \& Technology \\
\email{msrahman@cse.buet.ac.bd} \\
{\ } \\ 
School of Engineering \& Information Technology \\
Murdoch University, Perth, Australia
}

\maketitle

\begin{abstract}
In the Lyndon array  of a string ,
 is the length of the longest Lyndon word starting at
position  of \s{x}.
The computation of  has recently become of
great interest, since it was shown
(Bannai {\it et al.}, {\bf The ``Runs'' Theorem} \cite{BIINTT14})
that the runs in \s{x} are computable in linear time
from .
Here we first describe three algorithms for computing 
that have been suggested in the literature,
but for which no structured exposition has been given.
Two of these algorithms execute in  time in the worst case;
the third achieves  time,
but at the expense of prior computation of both the suffix array
and the inverse suffix array of \s{x}.
We then go on to describe two variants of a new algorithm that
avoids prior computation of global data structures
and executes in worst-case  time.
Experimental evidence suggests that all but one of these five
algorithms require only linear execution time in practice,
with the two new algorithms faster by a small factor.
We conjecture that there exists a fast
and worst-case linear-time algorithm to compute the Lyndon array
that is also ``elementary''
(making no use of global data structures such as the suffix array).
\end{abstract}

\section{Introduction}
\label{sect-intro}
If  for some \s{u} and nonempty \s{v},
then \s{vu} is said to be the  \itbf{rotation} of \s{x},
written .
If there exists a string \s{u} and an integer 
such that ,
then \s{x} is said to be a \itbf{repetition};
otherwise \s{x} is \itbf{primitive}.
A primitive string \s{x} that is
lexicographically least among all its rotations
,
is said to be a \itbf{Lyndon word}.

The \itbf{Lyndon array} 
(equivalently, )
of a given nonempty string 
gives at each position  the length
(equivalently, the end position)
of the longest Lyndon word starting at :


The Lyndon array has recently become of interest since
Bannai {\it et al.} \cite{BIINTT14} showed that it could
be used to efficiently compute all the maximal periodicities (``runs'')
in a string.
In this paper we describe four algorithms to compute ,
three of them shown experimentally to be running in  time in practice.
Section~\ref{sect-prelim} makes various observations that
apply generally to the Lyndon array and its computation.
In Section~\ref{sect-folklore} we describe three
algorithms, two that require  time in the worst case,
of which one is very fast and apparently linear in practice,
the other supralinear in practice and  in the average case on binary strings.
The third algorithm is simple and worst-case linear-time,
but requires suffix array construction and so is a little slower.
Section~\ref{sect-newalgs} describes two variants of a new algorithm
that uses only elementary data structures (no suffix arrays).
One variant is  in the worst case,
the other guarantees  time,
but with no clear advantage in processing time.
Section~\ref{sect-exp} describes the results of
preliminary experiments on the algorithms;
Section~\ref{sect-future} outlines future work.

\section{Preliminaries}
\label{sect-prelim}
Here we make various observations that apply to
the algorithms described below.
\begin{obs}
\label{obs-decomp}
Let  be the Lyndon decompostion \cite{CFL58,D83} of \s{x},
with Lyndon words .
Then every Lyndon word  of length 
is a substring of some .
\end{obs}
\begin{proof}
For some , consider \s{w_h} with nonempty proper suffix \s{v_h},
and for some , consider \s{w_{h+t}} with nonempty prefix \s{u_{h+t}}.
Since \s{w_h} is a Lyndon word, ,
and by lexorder, .  Thus
,
and so 
cannot be a Lyndon word for any choice of  or .  \qed
\end{proof}
Therefore to compute  it suffices to consider separately
each distinct element \s{w_h} in the Lyndon decomposition of \s{x}.
Hence, without loss of generality suppose \s{x} is a Lyndon word
and write it in the form ,
where for each ,  and

while for ,

We call \s{x_r} a \itbf{range} in \s{x} and the boundary between
\s{x_r} and \s{x_{r+1}} a \itbf{drop}.
We identify a position  in range \s{x_r}, ,
with its equivalent position  in \s{x} by writing
.
\begin{obs}
\label{obs-endrange}
Let  be a position in \s{x} that corresponds to position 
in range \s{x_r}.
\begin{itemize}
\item[]
If , then .
\item[]
Otherwise, , where  is the final position in some range ;
that is, .
\end{itemize}
\end{obs}
\begin{proof}
(a) is an immediate consequence of (\ref{range}) and (\ref{drop}).
To prove (b), suppose that  is a maximum-length Lyndon word,
where  falls within range  but .
Since by (\ref{range}) ,
there are two consecutive Lyndon words 
that by the Lyndon decomposition theorem \cite{CFL58}
can be merged into a single Lyndon word .
Thus  is not maximum-length, a contradiction.  \qed
\end{proof}
We see then that if ,
then  is a (not necessarily maximum-length) Lyndon word,
and for , :


More generally, the vectors  satisfy a ``Monge'' property
that is exploited by Algorithm  (Section~\ref{sect-newalgs}):
\begin{obs}
\label{obs-monge}
Suppose positions  in  satisfy .
Then either  or :
the vectors  and  are nonintersecting.
\end{obs}
\begin{proof}
Suppose two such vectors do intersect.
Then the maximum-length Lyndon words
 and 
have a nonempty overlap,
so that we can write
 for some nonempty \s{v}.
But then, by well-known properties of Lyndon words,
,
implying that  is a Lyndon word,
contradicting the assumption that \s{w_1} is maximum-length.  \qed
\end{proof}

Expressing a string in terms of its ranges has the same useful
lexorder property that writing it in terms of its letters does:
\begin{obs}
\label{obs-compare}
Suppose strings \s{x} and \s{y} are expressed in terms of their ranges:
.
Suppose further that for some least integer ,
.
Then  (respectively, )
according as  (respectively, ).
\end{obs}
\begin{proof}
If , then either
\begin{itemize}
\item[]
\s{x_r} is a nonempty proper prefix of \s{y_r}; or
\item[]
there is some least position  such that .
\end{itemize}
In case (a), if , then \s{x} is actually a prefix of \s{y},
so that ,
while if , then by (\ref{drop}),
, and again .
In case (b) the result is immediate.
The proof for  is similar.  \qed
\end{proof}

\section{Basic Algorithms}
\label{sect-folklore}
Here we outline three algorithms
for which no clear exposition is available in the literature.
We remark that the Lyndon array computation is equivalent to ``Lyndon bracketing'',
for which an  algorithm has been described \cite{SR03}.

\subsection{Folklore --- Iterated MaxLyn}
\label{subsect-maxlyn}
For a string \s{x} of length ,
recall that the \itbf{prefix table} 
is an integer array in which for every ,
 is the length of the longest substring beginning at position 
of \s{x} that matches a prefix of \s{x}.
Given a nonempty string \s{x} on alphabet , let us define \ for every letter .

\begin{obs}
\label{obs2}
\s{x} is a Lyndon word if and only if for every ,
, where .
\end{obs}
This result forms the basis of the algorithm
given in Figure~\ref{fig-maxlyn} that computes
the length 
of the longest Lyndon factor at a given position  in .
Its efficiency is a consequence of the instruction
 that skips over positions in the range ,
effectively assuming that for every position 
in that range, .
Lemma~\ref{lemm-maxlyn}, given in Appendix 1, justifies this assumption.
Simply repeating MaxLyn at every position  of \s{x}
gives a simple, fast  time and  additional space
algorithm to compute .

\begin{figure}[ht]
{\leftskip=2.5cm\obeylines\sfcode`;=3000
\bproc MaxLyn

\bwhile  \bdo
\q 
\q \bwhile  \bdo
\qq 
\q \bif  \bthen
\qq 
\q \belse
\qq \breturn max
}
\caption{Algorithm MaxLyn}
\label{fig-maxlyn}
\end{figure}
Recent work on the prefix table
\cite{BKS13,CRSW15} has confirmed its importance as a data structure
for string algorithms.
In this context it is interesting to find that Lyndon words \s{x}
can be characterized in terms of :
\begin{obs}
\label{obs-prefix}
Suppose  is a string on alphabet 
such that  is the least letter in \s{x}.
Then \s{x} is a Lyndon word over  if and only if
for every ,
\begin{itemize}
\item[]
; and
\item[]
for every ,
.
\end{itemize}
\end{obs}

\subsection{Recursive Duval Factorization: Algorithm RDuval}
Rather than independently computing the maximum-length Lyndon factor
at each position , as MaxLyn does, Algorithm RDuval
recursively computes the Lyndon decomposition into
maximum factors, at each step taking advantage of the fact that
 is known for the first position  in each factor,
then recomputing with the first letters removed.
By Observation~\ref{obs-decomp},
whenever  is a Lyndon word,
we know that .
Thus computing the Lyndon decomposition ,
,
allows us to assign ,
where  is the first position of \s{w_j}, .

Algorithm RDuval applies this strategy recursively,
by assigning ,
then removing the first letter  from each  to form ,
to which the Lyndon decomposition is applied in the next recursive step.
This process continues until each Lyndon word is reduced to a single letter.

The asymptotic time required for RDuval is bounded above by
 times the maximum depth of the recursion, thus  in the worst case --- 
consider, for example, the string .
However, to estimate expected behaviour, we can make use of a result
of Bassino {\it et al.} \cite{BassinoCN05}.
Given a Lyndon word \s{w}, they call 
the \itbf{standard factorization} of \s{w} if \s{u} and \s{v}
are both Lyndon words and \s{v} is of maximum size.
They then show that if \s{w} is a binary string (),
the average length of \s{v} is asymptotically .
Thus each recursive application of RDuval yields a left Lyndon factor
of expected length 
and a remainder of length  to be factored further.
It follows that the expected number of recursive calls
of RDuval is .  Hence
\begin{lemm}
\label{lemm-RD}
On binary strings
RDuval executes in  time on average.
\end{lemm}
\begin{examp}
For

the factors considered are first 1--12, then
\begin{itemize}
\item[]
2--3 and 4--12 in the  first level of recursion;
\item[]
3, 5--7, 8--10 and 11--12 in the second level;
\item[]
 in the third level.
\end{itemize}
Positions are assigned as follows:
.
\end{examp}

\subsection{NSV Applied to the Inverse Suffix Array}
\label{subsect-nsvisa}
The idea of the ``next smaller value'' (NSV) array for a given array (string) \s{x}
has been proposed in various forms and under various names
\cite{AGKR02,FMN08,OG11,GB13}.
\begin{defn}[Next Smaller Value]
Given an array  of ordered values,
 is the \itbf{next smaller value array} of \s{x}
if and only if for every , , where
\begin{itemize}
\item[]
for every ; and
\item[]
either  or .
\end{itemize}
\end{defn}
\begin{examp}

\end{examp}
As shown in various contexts in \cite{GB13},
 can be computed in  time using a stack.
Our main observation here, touched upon in \cite{HR03},
is that  can be computed merely by applying NSV
to the inverse suffix array .
Proof of this claim can be found in Appendix 2;
here we present the very simple -time, -space algorithm for this calculation:
\begin{figure}[ht]


{\leftskip=3.4cm\obeylines\sfcode`;=3000
\bproc NSVISA
Compute \ \ \ \ (see \cite{NZC09,PST07})
Compute  from  in place\ \ \ \ (see \cite{PST07})
 NSV (in place)
}
\caption{Apply NSV to }
\label{fig-3line}
\end{figure}

\section{Elementary Computation of  Using Ranges}
\label{sect-newalgs}

In this section we describe an approach to the computation of 
that applies a variant of the NSV idea to the ranges of \s{x}.
Figure~\ref{nsv*} gives pseudocode for Algorithm 
that uses the NSV stack ACTIVE to compute .
The processing identifies ranges in a single left-to-right scan
of \s{x}, making use of two range comparison routines, COMP and MATCH.
COMP compares adjacent individual ranges \s{x_r} and \s{x_{r+1}}, returning 
according as , , .
MATCH similarly returns  for adjacent {\it sequences} of ranges; that is,




Algorithm  is based on the idea encapsulated in Lemma~\ref{maxlyn-suf}
of Appendix 2,
the main basis of the correctness of Algorithm NSVISA.
We process \s{x} from left to right, using a stack ACTIVE initialized with index 1.
At each iteration, the top of the stack (say, )
is compared with the current index (say, ).
In particular, we need to compare  with ,
where .
As long as ,
 pushes the current index and continues to the next.
When ,
it pops the stack and puts
appropriate values in the corresponding indices of .
As noted above, especially Observations~\ref{obs-decomp}--\ref{obs-monge},
ranges are employed to expedite these suffix comparisons.

\begin{figure}[ht]
{\leftskip=1.0cm\obeylines\sfcode`;=3000
\bproc NSV* 

\bpush
\com{\Sigmai \la 2n\+ 1\prev \la 0;\ j \la(\ACTIVE)i,j\delta_1 \la \COMP(\s{x}[j],\s{x}[i]);\ \delta_2 \la 1\delta_1 \ge 0\delta_2 > 0\delta_1 = 0\delta_2 \la \MATCH(\s{x}[j],\s{x}[i])\delta_2 > 0\prev = 0\nextequal[j] \neq \prev\s{\lambda}[j] \la i\m j\s{\lambda}[j] \la \offset \la \prev\m j\period[\prev]=0\s{\lambda}[\prev] > \offset\s{\lambda}[j] \la \s{\lambda}[j]\+ \s{\lambda}[\prev]\nextequal[j]=\prev\offset \neq \s{\lambda}[\prev]\s{\lambda}[j] \la \s{\lambda}[j]\+ \period[\prev]\s{\lambda}[\prev] = \offset\period[\prev] = 0\period[j] \la \period[\prev]+2\times \offset\period[j] \la \period[\prev]\+ \offset(\ACTIVE)\prev \la j;\ j \la \peek(\ACTIVE)j = 0\delta_1 \la \COMP(\s{x}[j],\s{x}[i])i\delta_2 = 0\nextequal[j] \la i(\ACTIVE) \la i\s{\lambda}_xji\delta_2 = 0\nextequal[j] \la ii\delta_2 = 0\Theta(n)\O(\sigma)\sigma = |\Sigma|\Sigma = \{\mu_1,\mu_2,\ldots,\mu_{\sigma}\}P_r[1..\sigma]P_r[j]\mu_j\s{x_r}P_r, r = 1,2,\ldots,mP_{r,j}\s{x_r}[j..\ell_r]\O(\sigma)\O(\max(\ell_r,\ell_{r'}))\NSV^*\NSV^*\NSV^*\O(n \log n)\O(n \log n)\NSV^*\s{x_0} = a^hba^hc_0,\ h \ge 1c_0 > b > a\s{x^{(h)}_k} = \s{x_k} = \s{x_{k-1}x^*_{k-1}},\ k = 1,2,\ldots,c_k > c_{k-1}c_{k-1}n = (h\+ 1)mm = 2^{k+1}\s{x_k}\NSV^*\O(n\log n)\PNSV^*\O(\sigma)\O(\sigma n\log n)\O(n\log n)\sigma\NPNSV^*\O(n\log n)\NSV^*\NSV^*\NSV^*\NSV^*\NSV^*\O(n\log n)n \in 11..22n10^{-4}i\s{x}[1..n]k = \pi[i] \ge 2j \in i+1\dd i+k-1\pi[j] \le i+k-ji+k = n+1i+k \le nj \in i+1\dd i+k-1\pi[j] > i+k-j\s{x}[j-i+1\dd k] = \s{x}[j\dd i+k-1]\s{x}\s{x}[1+k] \prec \s{x}[i+k]\s{x}[1..k+1]\s{x}[j-i+1..k+1]\s{x}[j-i+1..k+1] \prec \s{x}[1..i+k-j+1]\s{x} = \s{x}[1..n]\Sigma\prec\ISA = \ISA_{\s{x}}^{\prec}i \in 1..n\s{x}[i..j]\prec(a)h \in i\+ 1..j\ISA[j] < \ISA[h](b)j = n\ISA[j\+ 1] < \ISA[i]\s{x} \in \Sigma^+\Sigma\prec\s{x} = \s{u}^r\s{u_1}br \ge 1b \ne \s{u}[|\s{u_1}|\+ 1](a)b\prec \s{u}[|\s{u_1}|{+}1](b)b\succ \s{u}[|\s{u_1}|{+}1]\prec\s{x}[1..n]\s{s}_{\s{x}}(i) = \s{x}[i..n]i\s{s}(i)\s{x}=\s{x}[1\dd n]\Sigma\prec\s{x}[i\dd j]\s{x}i\s{s}_{\s{x}}(i)\prec \s{s}_{\s{x}}(k)k \in i\+ 1..jj=n\s{s}_{\s{x}}(j{+}1) \prec \s{s}_{\s{x}}(i)\s{x}[i\dd j]i<k\leq j\s{x}[i\dd j]\prec \s{x}[k\dd j]\s{s}(i)\prec s(k)j=nj < n\s{s}(j{+}1)\prec \s{s}(i)\s{s}(j{+}1)\not\prec \s{s}(i)\s{s}(i)\s{s}(j{+}1)\s{s}(i)\prec \s{s}(j{+}1)d=lcp(\s{s}(i),\s{s}(j{+}1))+10\leq d\leq j-ii\leq i+d\leq j\s{x}[i\dd i{+}d{-}1]=\s{x}[j{+}1\dd j{+}d]\s{x}[i{+}d]\prec \s{x}[j{+}1{+}d]j<k\leq j{+}1{+}d\s{x}[i\dd j{+}1{+}d]\prec \s{x}[k\dd j{+}1{+}d]\s{x}[i\dd j]\s{x}[i\dd j]\prec \s{x}[k\dd j]\s{x}[i\dd j{+}1{+}d]\prec \s{x}[k\dd j{+}1{+}d]i<k\leq j\s{x}[i\dd j{+}1{+}d]\s{x}[i\dd j]i0 < j-i\leq dd = r(j-i)+d_10\leq d_1<j-ir\geq 1\s{x}[i\dd j{+}1{+}d]=\s{u}^r\s{u_1}b\s{u}=\s{x}[i\dd j]\s{x}[i\dd j]\s{x}[i{+}d]\prec \s{x}[j{+}1{+}d]\s{x}[i\dd j{+}1{+}d]\s{x}[i\dd j]i\s{s}(j{+}1)\prec \s{s}(i)\s{x} = \s{x}[1\dd n]\Sigma\prec\s{x}[i\dd j]\prec\s{s}_{\s{x}}(i)\prec \s{s}_{\s{x}}(k)k \in i+1..jj=n\s{s}_{\s{x}}(j{+}1) \prec \s{s}_{\s{x}}(i)\{\s{x}[i\dd j]\s{x}\}\{\s{s}(i)\prec \s{s}(k)1\leq k\leq j\s{s}(j{+}1)\prec \s{s}(i)\}\Rightarrow\Rightarrow\s{x}[i\dd k]ik<j\s{s}(k{+}1)\prec \s{s}(i)k{+}1\leq jk>j\s{s}(i)\prec \s{s}(j{+}1)j{+}1\leq kk=j\s{x}[i\dd j]\ISA\s{s}(i)\prec s(j) \Longleftrightarrow \ISA[i] < \ISA[j]\theta(n)\NSV^*\s{x}[1..n]\mathcal D_t, 0 \leq t \leq \log n\mathcal D_t\s{x}2^t\mathcal D_t[i]\s{x}[i..i + 2^t - 1]O(n \log n)O(n \log n)\mathcal D_0\s{x}O(n \log n)\s{x}\mathcal D_t\mathcal D_{t+1}O(n)u[i..i+2^{t+1} -1]u[i..i+2^t -1]u[i+2^t..i+2^{t+1} -1]\mathcal DRM^{(h)}(k)\NSV^*\s{\lambda}_{\s{x_k}}n = (h\+ 1)mm = 2^{k+1}RM^{(h)}(k) = m(\log_2 m\- 1) \+ 1 \in \Theta(n\log n)\s{s_0} = a^hba^hc_k\NSV^*a^hb < a^hc_k(1,0)\s{s_1} = a^hba^hc_0a^hba^hc_k(2,2,1,0)\NSV^*a^hba^hc_0a^hba^hc_0a^hba^hc_{k-1}(3,3,3,3)(3,3,3,3,2,2,1,0)(4,4,4,4,4,4,4,4)RM^{(h)}(k) = m(\log_2 m \- 1) \+ 1(1,0)(2,2)(3,3,3,3)\sigma\NSV^*\s{\lambda}_{\s{x}}\O(n\log n)\NPNSV^*\s{x_k}\PNSV^*h\+ 1h = 12h > 2h(h\+ 1)/2O(n\log n)O(n\log n)\NPNSV^*\s{\lambda}_{\s{x}}\O(n\log n)$ time for all \s{x}.
\end{lemm}

\end{document}
