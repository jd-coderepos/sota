\pdfoutput=1
\documentclass[a4paper]{article}
\usepackage{amsmath,graphicx,amssymb,xcolor,hyperref}
\usepackage[flushleft]{threeparttable}
\usepackage{INTERSPEECH2021}
\usepackage{array,mathtools}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\title{Improving Perceptual Quality by Phone-Fortified Perceptual Loss using Wasserstein Distance for Speech Enhancement}
\name{Tsun-An Hsieh , Cheng Yu, Szu-Wei Fu , Xugang Lu, and Yu Tsao}
\address{
  Research Center for Information Technology Innovation, Academia Sinica, Taiwan \\
National Institute of Information and Communications Technology, Japan}
\email{\{tahsieh, chengyu\_citi, jasonfu, yu.tsao\}@citi.sinica.edu.tw, xugang.lu@nict.go.jp}

\begin{document}

\maketitle
\begin{abstract}
Speech enhancement (SE) aims to improve speech quality and intelligibility, which are both related to a smooth transition in speech segments that may carry linguistic information, e.g. phones and syllables. In this study, we propose a novel phone-fortified perceptual loss (PFPL) that takes phonetic information into account for training SE models. To effectively incorporate the phonetic information, the PFPL is computed based on latent representations of the \textit{wav2vec} model, a powerful self-supervised encoder that renders rich
phonetic information. To more accurately measure the distribution distances of the latent representations, the PFPL adopts the Wasserstein distance as the distance measure. Our experimental results first reveal that the PFPL is more correlated with the perceptual evaluation metrics, as compared to signal-level losses. Moreover, the results showed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance in terms of standardized quality and intelligibility evaluations on the Voice Bank--DEMAND dataset.
\end{abstract}
\noindent\textbf{Index Terms}: Speech enhancement, perceptual loss, contrastive predictive coding, representation learning, self-supervised learning

\section{Introduction}

In real-world speech-related applications, speech signals may be contaminated by environmental noise, and thus constrain the achievable performance on target tasks. To address this issue, speech enhancement (SE) has been studied for decades. Numerous signal processing-based methods \cite{boll1979suppression, lim1979enhancement, paliwal1987speech, loizou2013speech} have been proposed. 
These methods are based on the assumed statistical properties of speech and noise signals. Unfortunately, SE performance may drop drastically when these assumptions are unfulfilled.
With recent advances in neural network (NN) models, SE performance has increased notably. Well-known NN models, such as deep denoising autoencoder (DDAE) \cite{lu2013speech}, deep neural networks (DNNs) \cite{xu2014regression}, recurrent neural networks (RNNs) \cite{weninger2014single}, long short-term memory (LSTM) \cite{weninger2015speech}, convolutional neural networks (CNNs) \cite{zhao2018convolutional}, fully convolutional networks (FCNs) \cite{fu2018end, pandey2019tcnn}, convolutional recurrent neural networks (CRNNs) \cite{tan2018convolutional}, and generative adversarial networks (GANs) \cite{pascual2017segan, soni2018time, pandey2018adversarial, baby2019sergan, qin2018improved, su2020hifi, fu2019metricgan} have made notable improvements over traditional signal processing-based SE methods. \par

For these NN-based SE approaches, designing a suitable objective function is a crucial factor. Traditionally, point-wise distances are often used to form the objective functions. Point-wise distances, such as  and/or  norms between paired noisy-clean speech signals, attempt to recover information on a signal level. Recent studies have revealed that objective functions based on point-wise distances may not fully reflect the perceptual difference between noisy and clean speech signals. As the purpose of SE is to recover speech quality and intelligibility, objective functions that consider perceptual metrics have been investigated for NN-based SE. In some studies, perceptual metrics were modified to their differentiable alternatives for convenient gradient calculations to optimize the NN parameters. Some notable works are the perceptual evaluation-based loss function \cite{martin2018deep}, joint source-to-distortion ratio (SDR) perceptual evaluation for speech quality optimization \cite{kim2019end}, and modified short-time objective intelligibility (STOI) loss functions for network optimization \cite{fu2018end, kolbaek2018monaural, zhao2018perceptually}. Along this line, several studies focus on training NN models with target metrics for SE tasks \cite{fu2019learning}, as well as with GAN approaches like HiFi-GAN \cite{su2020hifi} and MetricGAN \cite{fu2019metricgan}.
Another class of approaches focused on building loss functions in the spaces mapped by certain pre-trained classifiers. 
For example, in style transfer studies of computer vision, \cite{johnson2016perceptual} proposed training feed-forward networks based on perceptual loss. In \cite{germain2019speech}, the authors proposed utilizing an acoustic scene (AS) recognition network's latent spaces for the loss function, termed deep feature loss (DFL). For further improvement over DFL, \cite{kataria2020perceptual} proposed the perceptual ensemble regularization loss (PERL) as a variant of DFL that gathers several pre-trained models related to speech or acoustic tasks, achieving state-of-the-art performance in terms of quality. Despite the success, it remains unclear how acoustic event (AE) or AS classifiers benefit SE. \par



In this paper, we propose a novel phone-fortified perceptual loss (PFPL) for training SE models. The PFPL modifies the original DFL in two aspects. First, the PFPL intends to consider the phonetic information embedded in the speech signals. Therefore, rather than using the AS recognition models, the PFPL is computed based on the latent representations of the \textit{wav2vec}
 model \cite{schneider2019wav2vec}, a powerful self-supervised encoder that renders rich phonetic information.  Second, as the distance used in the original DFL ignores the geometry of the distributions of the latent representations, PFPL adopts the Wasserstein distance \cite{OLKIN1982257}
as the distance measure. In this way, the SE training can be seen as an optimal transport problem that transforms the distributions of noisy speech to that of clean speech.
Experimental results first confirmed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance on the Voice Bank--DEMAND dataset. A series of ablation studies investigated the effectiveness of individual parts in the PFPL.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/model_pfpl.pdf}
\caption{A demonstration of the proposed network. The enhancement model estimates a cRM by the noisy spectrum, and consequently produces an enhanced spectrum. The PFPL then compares the semantic difference of clean speech and the enhanced one.}
    \label{fig:model}
\end{figure*}

\section{Related Works}

\label{sec:relatedworks}
In this section, we first present DFL and PERL, which was mentioned in the previous section, with a more detailed discussion in Section \ref{ssec:dfl}. We then review the perceptual metrics approximated with trained networks. Such a network can work as a discriminator in a GAN or a stand-alone metric. Last but not least, we review methods that maximize the mutual information between contexts in Section \ref{ssec:cpc_w2v}. 

\subsection{DFL and PERL}
\label{ssec:dfl}
The idea to incorporate AS recognition in SE was first proposed in DFL \cite{germain2019speech}. According to \cite{su2020hifi}, the latent features from a pre-trained recognition network (used for machine perception) are used to approximate human perception (SE, in this case). Analogous to DFL, \cite{kataria2020perceptual} extend the idea to ensemble of six types of pre-trained networks, including AE classifiers and speech encoders. In spite of that PERL-AE uses AE classifier alone and yields the best result, it remains unexplainable due to the complication of characteristics in AEs, which are too difficult to be analyzed.


\subsection{MetricGAN and HiFi-GAN}
\label{ssec:gan}
 MetricGAN \cite{fu2019metricgan} applies a discriminator (also called Quality-Net \cite{fu2018quality}) to approximate the behavior of the evaluation functions of interest. The predicted score can also be treated as a special case of perceptual loss, with an embedding dimension equal to 1. Due to the limited dimensions, Quality-Net is easily fooled by the speech generated by the updated generator. Therefore, MetricGAN needs to alternatively train between the generator and the discriminator which slows down its efficiency. HiFi-GAN \cite{su2020hifi} incorporates the idea of GAN and deep feature loss. However, its deep feature loss is based on the discriminator, which may not be highly related to human perception.

\subsection{Contrastive Predictive Coding (CPC) and \textit{wav2vec}}
\label{ssec:cpc_w2v}
Recent studies of representation learning have shown capabilities in extracting representative features without supervision.
For instance, CPC \cite{oord2018representation} is an self-supervised method that proposes to extract task-agnostic features from high-dimensional data. It was the contrastive loss that helps capture features which maximize the amount of underlying shared information of the observation and its latent representation.
As a result, self-supervised methods that can extract features with phonetic information from speech signals drew our attention.
We then focus on speech-related applications that utilizes representation learning approaches.
\par
The self-supervised automatic speech recognition (ASR) \textit{wav2vec} \cite{schneider2019wav2vec}, utilizing the CPC technique, has shown great performance in recognition accuracy and thus fits our interests. In practice, speech signals are first encoded with an encoder network that extracts features rich in phonetic information.
An ASR decoder is then trained based on these features as inputs.
\subsection{Wasserstein Distance}
\label{ssec:wasserstein}
The Wasserstein distance \cite{OLKIN1982257} is a measurement of two probability distributions on a metric space  with  a metric on . The Wasserstein distance of two densities  and  is defined as:

where  denotes a set of all possible measures (or {\it couplings}) on  with marginals  and . Here,  is the coupling, that is, a joint distribution of marginals  and , representing any possible transport plan from  to . Comparing to  distances that only regard the amount of mass transported, Eq. \eqref{eq:wass} shows that Wasserstein distance additionally takes the transport method into account. 
\section{Proposed framework}
\label{sec:proposedframework}
In this section, we start with introducing a complex U-Net adopted, which was widely utilized in several studies \cite{YaoA19coarse, hu2020dccrn, choi2018phase} that has been confirmed to achieve promising results. In the followings, we will describe the PFPL, which is a perceptual loss incorporated with Wasserstein distance in detail.

\subsection{Model Architecture}
\label{ssec:enhancement_model}

Inspired by the deep complex U-Net (DCUnet) in \cite{choi2018phase}, we designed a modified framework that estimates complex ratio masks (cRM) for a noisy complex spectrum with a different normalization mechanism. More specifically, as shown in Fig. \ref{fig:model}, a noisy speech signal is first converted to a complex spectrum through short-time Fourier transform (STFT), and the enhancement model generates a cRM. Subsequently, the noisy spectrum is multiplied by the cRM in a point-wise manner to derive the final enhanced spectrum, which is transformed to a waveform by inverse STFT (iSTFT). Here, according to \cite{choi2018phase}, a scheme that produces cRM with a complex neural network (cRMn) is used in this work, and we take the Large-DCUnet-20 as a reference architecture for our enhancement model. As a number of previous works \cite{ulyanov2016instance, conf/iccv/HuangB17} have indicated that instance normalization outperforms batch normalization on generation tasks by preserving the independence of samples in a mini-batch, we substitute the batch normalization layers with instance normalization layers. To describe the enhancement process precisely, given the noisy input speech signal , the noisy spectrum  is produced by STFT, such that . Then, the enhancement model generates a cRM  to produce the enhanced spectrum that , and transforms it to the enhanced waveform  by iSTFT.
\subsection{Phone-Fortified Perceptual Loss}
\label{ssec:problem_definition}


For training SE models, point-wise loss functions are commonly used in either time domain or time-frequency domain. Despite that these approaches have achieved promising results, point-wise losses remain numerically inconsistent with perceptual evaluations such as PESQ or STOI. Unlike point-wise losses that measure distances in the signal level, the perceptual loss is devised to measure the distance in the latent space \cite{johnson2016perceptual}. \par
The design of perceptual losses requires an appropriate feature extractor. In the SE scenario, the estimates of a system are speech signals. It is thus our desire to carry out loss computation that is able to preserve attributes in speech signals (i.e., phones, speaker characteristics, etc.) in the training stage. Some proposed a supervised pre-trained encoder for loss computation, like \cite{germain2019speech}. However, these methods can suffer from the drawback of one-hot encoding in which the correlations between categories were ignored since the labels are in an orthogonal (high-dimensional) space \cite{rodriguez2018beyond}. As a consequence, the correlations between phones could be underestimated and thus restrict the capability of preserving attributes. Hence, we employ {\it wav2vec}, a self-supervised encoder, to compute the PFPL in a non-orthogonal (low-dimensional) space that is more capable of preserving attributes. Owing to the fact that speech signals carry linguistic information (e.g., phones, syllables, etc.) more often than noises do, we prefer to use models that generate features which are representative for phonetic information. Meanwhile, note that CNNs are known for the shift-invariance, which is analogous to the perceptual evaluation of speech quality (PESQ) \cite{PESQ} which is insensitive to shifts in a short-time. As stated above, we believe the CNN-based \textit{wav2vec} encoder (denoted as ) is suitable for the design of our loss function. \par
In contrast to previous works on perceptual loss that utilize activations in multiple layers, we merely extract the final outputs for efficiency. Formally, as the densities remain unknown, we define the PFPL by the Kantorovich--Rubinstein \cite{optimaltransport} dual form of Wasserstein distance:

where  and  are the features of the clean speech  and the enhanced speech , respectively. Here,  and  are the densities of  and  in the latent space, and  is a function belonging to a set  of all 1-Lipschitz functions. For the given paired enhanced speech and clean speech, the PFPL minimizes the distance between the distributions of the estimates and the corresponding targets in a space of phonetic representations. Please note Eq.~(\ref{eq:loss}) that the PFPL includes an mean absolute error (MAE) loss to measure the signal-level difference. The effect of the MAE loss will be discussed in Section \ref{ssec:qualitative_analysis}. 
\begin{figure}[ht]
\includegraphics[width=\linewidth]{figs/tsne_lr_l25.pdf}
    \centering 
\caption{t-SNE analysis on \textit{wav2vec} encoded feature map. (a) Feature map of five phone classes. (b) Feature map of clean and noisy utterances.}
    \label{fig:tsne}
\end{figure}
\section{Experiments}
\label{sec:experiments}

In this section, we begin with the selected dataset and the evaluation metrics that were used as a standard benchmark. Next, we provide visualizations that demonstrate that the features generated by the PFPL are correlated with PESQ and STOI. Finally, it is shown that the proposed modification achieves competitive performance in terms of qualities and intelligibility.



\subsection{Voice Bank--DEMAND Dataset}
\label{ssec:voice_bank_demand_dataset}
To compare our proposed SE system with other recent approaches, the Voice Bank--DEMAND dataset \cite{voicebank, demand} was used for evaluation. In this dataset, utterances recorded by 28 speakers out of a total 30 speakers were used for training, and the utterances from the remaining 2 speakers were used for testing. In the training set, noisy mixtures were synthesized using 10 types of noise at 4 different SNR levels, ranging from 0 dB to 15 dB, and 5 types of unseen noises, ranging from 2.5 dB to 17.5 dB were added to the testing set.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.87\textwidth]{figs/pesq_metrics.pdf}
\caption{Illustration the correlations of PESQ and STOI to different losses. To quantify how much a loss is correlated to a metric, we note the Pearson correlation coefficient in the parentheses. The higher absolute value of PCC indicates higher correlation.}
\label{fig:relation}
\end{figure*}

\subsection{Evaluation Metrics}
\label{ssec:evaluation_metrics}
Following prior works evaluated on the Voice Bank--DEMAND dataset, we used five metrics, which were CSIG, CBAK, COVL, introduced in \cite{metric}, PESQ, and STOI to measure the performance of the proposed method. CSIG, CBAK, and COVL demonstrate the signal distortion, background intrusiveness, and the overall quality with the same scale of mean opinion score, respectively. PESQ and STOI quantify the perceptual quality and the intelligibility of a speech signal. All of the above-mentioned metrics are better with higher scores.

\subsection{Regarding SE as an Optimal Transport Problem}
\label{ssec:visulaization_on_wav2vec_features}
As mentioned in Section \ref{ssec:problem_definition}, latent representations of \textit{wav2vec} render rich phonetic information. Fig. \ref{fig:tsne}(a) demonstrates a t-SNE analysis of five phones, which are properly separated, confirming that the latent representations generated by \textit{wav2vec} carry rich phonetic information. Fig. \ref{fig:tsne}(b) shows that most of the noisy and clean speech are highly distinguishable in the latent space. Based on the observations from Fig. \ref{fig:tsne}, we can consider the training procedure of SE as an optimal transport problem that aims to search for a transformation mapping the distributions of noisy speech signals to that of the clean ones. Based on this concept, we decide to replace the  distance and use the Wasserstein distance as the distance measure to compute the perceptual loss for the PFPL.






\begin{table}[t]
\begin{threeparttable}
\caption{Our proposed method versus some well performing methods with respect to different metrics. DFL\tnote{} shows the results from the official source code and released parameters.}
\centering
\small
\begin{tabular*}{\linewidth}{l|P{5.8mm} P{5.8mm} P{5.8mm} P{5.8mm} P{5.8mm}}
\hline
Model  & PESQ & CSIG & CBAK & COVL & STOI\\
\hline
\hline
\textbf{Noisy} & 1.97 & 3.35 & 2.44 & 2.63 & 0.92\\
\textbf{Wiener \cite{paliwal1987speech}} & 2.22 & 3.23 & 2.68 & 2.67 & --\\
\textbf{SEGAN \cite{pascual2017segan}} & 2.16 & 3.48 & 2.94 & 2.80 & 0.93\\
\textbf{DFL \cite{germain2019speech}} & -- & 3.86 & 3.33 & 3.22 & --\\
\textbf{DFL}\tnote{} & 2.58 & 3.80 & 2.72 & 3.19 & 0.93\\
\textbf{MetricGAN \cite{fu2019metricgan}} & 2.86 & 3.99 & 3.18 & 3.42 & 0.94\\
\textbf{HiFi-GAN \cite{su2020hifi}} & 2.94 & 4.07 & 3.07 & 3.49 & --\\
\textbf{SDR-PESQ \cite{kim2019end}} & 3.01 & 4.09 & 3.54 & 3.55 & --\\
\textbf{T-GSA \cite{kim2020t}} & 3.06 & {\bf 4.18} & 3.59 & 3.62 & --\\
\textbf{PERL-{\it wav2vec} \cite{kataria2020perceptual}} & 2.92 & 4.16 & 3.37 & 3.54 & 0.94\\
\hline
\hline
\textbf{PFP} & \textbf{3.15} & \textbf{4.18} & \textbf{3.60} & \textbf{3.67} & \textbf{0.95}\\
\hline
\end{tabular*}
\begin{tablenotes}
\scriptsize
\item[] \href{https://github.com/francoisgermain/SpeechDenoisingWithDeepFeatureLosses.git}{https://github.com/francoisgermain/SpeechDenoisingWithDeepFeatureLosses.git}.
\end{tablenotes}
\label{tab:denoise}
\end{threeparttable}
\end{table}

\subsection{Correlation of Perceptual Metrics to Losses}
\label{ssec:relation_of_pesq_to_losses}
To analyze the relation between perceptual metrics and other losses, we compared several different losses to the corresponding metric scores on the testing set. Here, we illustrate the correlations of PESQ and STOI to five losses including, MAE, mean squared error (MSE), weighted source-to-distortion ratio (wSDR), DFL, and the proposed PFPL. Each point represents an utterance. From Fig. \ref{fig:relation}, we note that MAE and MSE have similar correlations to PESQ and STOI, and the four groups of points of wSDR represent the four SNR levels in the testing set. For the first three losses, there is no obvious correlation to the two metrics. However, the more obvious tendencies are that DFL and PFPL correlate to PESQ and STOI. Here, the Pearson correlation coefficient (PCC) is utilized to quantify the correlation between metrics and losses. The PCCs of losses are shown inside the parentheses in Fig. \ref{fig:relation}. The PCC of PFPL is much higher than all the other metrics' being compared. From Table \ref{tab:denoise} and Table \ref{tab:losses}, although DFL is more correlated with PESQ than the other three signal-level metrics, it has similar results to wSDR, MAE, and MSE. Because the PFPL measures how different the features are in terms of phonetic information salient to the human auditory system, it is reasonable that the PFPL is highly correlated with PESQ and STOI.
\subsection{Ablation Study on the PFPL}
\label{ssec:qualitative_analysis}
 In Table \ref{tab:denoise}, we compare prior approaches using GAN-based methods and specialized losses for auditory perception. Our approach achieved the highest PESQ score among all the compared methods. To understand PFPL, we compare several losses with the same model structure and conduct an ablation study on the PFPL. In Table \ref{tab:denoise}, {\bf PFPL}- denotes {\bf PFPL} using the  distance, and {\bf PFPL}--MAE denotes {\bf PFPL}- without using the MAE loss. From Table \ref{tab:losses}, point-wise losses (\textbf{wSDR}, \textbf{MSE}, and \textbf{MAE}) yield lower PESQ but higher CBAK comparing to the perceptual loss alone (i.e., {\bf PFPL}--MAE). The low CBAK performance is caused by the point-wise difference ignored during training. This problem can be solved by adding MAE (i.e., the first term in Eq.~\eqref{eq:loss}) to the objective function, and accordingly {\bf PFPL}- yields an improved CBAK score from 3.05 to 3.52. 
 Finally, by comparing {\bf PFPL} and {\bf PFPL}-,  the effect of the Wasserstein distance is confirmed, and our best results in terms of quality and intelligibility is attained by {\bf PFPL}.




\begin{table}[t]
\caption{Comparison of the ablations of PFPL and the point-wise losses with respect to evaluation metrics.}
\centering
\small
\begin{tabular*}{\linewidth}{l|P{7.2mm} P{7.2mm} P{7.2mm} P{7.2mm} P{7.2mm}}
\hline
Loss & PESQ & CSIG & CBAK & COVL & STOI\\
\hline
\hline
\textbf{wSDR \cite{choi2018phase}} & 2.58 & 3.00 & 3.18 & 2.76 & 0.93\\
\textbf{MSE} & 2.60 & 3.31 & 3.19 & 2.94 & 0.93\\
\textbf{MAE} & 2.62 & 3.47 & 3.20 & 3.02 & 0.93\\
\hline
\hline
\textbf{PFPL}--MAE & 3.09 & \textbf{4.22} & 3.05 & \textbf{3.67} & 0.94\\
\textbf{PFPL}- & 3.11 & 4.15 & 3.52 & 3.64 & \textbf{0.95}\\
\textbf{PFPL}& \textbf{3.15} & 4.18 & \textbf{3.60} & \textbf{3.67} & \textbf{0.95}\\
\hline
\end{tabular*}
\label{tab:losses}
\end{table}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we have proposed a novel PFPL loss for training SE models. The PFPL is derived based on the latent representations of the {\it wav2vec} model, which carry rich phonetic information. Meanwhile, the PFPL uses the Wasserstein distance as the distance measure. Accordingly, the SE training can be seen as an optimal transport problem that aims to move the latent representation distributions of noisy speech to that of clean speech. The experimental results first revealed that the PFPL has very high correlations with perceptual metrics as  compared to other related loss functions. Moreover, the SE model trained with the PFPL outperforms several well-known and related works in terms of standardized evaluation metrics. 
\vfill\pagebreak

\bibliographystyle{IEEEtran}

\bibliography{mybib}



\end{document}
