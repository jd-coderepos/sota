\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{array}
\usepackage{footnote}
\newcommand{\myparagraph}[1]{\vspace{0.1em}\noindent\textbf{#1}}


\usepackage{color}
\newcommand{\xinzhe}[1]{\textcolor[rgb]{0,0.5,0.5}{Xinzhe: #1}}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{array, caption, threeparttable}
\captionsetup[table]{
  singlelinecheck=false,}

\title{Learning to Self-Train for Semi-Supervised\\ Few-Shot Classification}


\author{Xinzhe Li \quad Qianru Sun
\quad Yaoyao Liu \quad Shibao Zheng \\ \\
\textbf{Qin Zhou}
\quad
\textbf{Tat-Seng Chua} \quad \textbf{Bernt Schiele}\\
\\
\small Shanghai Jiao Tong University
 Singapore Management University
 Tianjin University Alibaba Group \\
 \small
 National University of Singapore
 Max Planck Institute for Informatics, Saarland Informatics Campus
}

\begin{document}

\maketitle

\begin{abstract}

Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called \textbf{learning to self-train (LST)} 
that leverages unlabeled data and specifically meta-learns how to cherry-pick and label such unsupervised data to further improve performance. To this end, we train the LST model through a large number of semi-supervised few-shot tasks. On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization. We evaluate our LST method on two ImageNet benchmarks for semi-supervised few-shot classification and achieve large improvements over the state-of-the-art method. Code is at \href{https://github.com/xinzheli1217/learning-to-self-train}{github.com/xinzheli1217/learning-to-self-train}.

\end{abstract}
 \section{Introduction}
\label{introduction}

Today's deep neural networks require large amounts of labeled data for supervised training and best performance~\cite{Lecun2015, HeZRS16, ShelhamerLD17}. 
Their potential applications to the small-data regimes are thus limited.
There has been growing interest in reducing the required amount of data, e.g. to only -shot~\cite{FeiFeiFP06}.
One of the most powerful methods is meta-learning that transfers the \emph{experience} learned from similar tasks to the target task~\cite{FinnAL17}. 
Among different meta strategies, gradient descent based methods are particularly promising for today's neural networks~\cite{FinnAL17, SunCVPR2019, RusuICLR2019}.
Another intriguing idea is to additionally use unlabeled data.
Semi-supervised learning using unlabeled data with a relatively small set of labeled ones has obtained good performance on standard datasets~\cite{Chapelle2006semi_supervise, OliverNIPS18semi_survey}.
A classic, intuitive and simple method is e.g. self-training. It first trains a supervised model with labeled data, and then enlarges the labeled set based on the most confident predictions (called pseudo labels) on unlabeled data~\cite{Yarowsky95self_training, TrigueroGH15self_labeled, OliverNIPS18semi_survey}.
It can outperform regularization based methods~\cite{MiyatoDG16VAT, GrandvaletNIPS04_entmin, LaineICLR2017pi_model}, especially when labeled data is scarce.



The focus of this paper is thus on the semi-supervised few-shot classification (SSFSC) task. Specifically, there are few labeled data and a much larger amount of unlabeled data for training classifiers.
To tackle this problem, we propose a new SSFSC method called \textbf{learning to self-train (LST)} that successfully embeds a well-performing semi-supervised method, i.e. self-training, into the meta gradient descent paradigm.
However, this is non-trivial, as directly applying self-training recursively may result in gradual drifts 
and thus adding noisy pseudo-labels~\cite{ZhangICLR2017noisy}.
To address this issue, we propose both to meta-learn
a soft weighting network (SWN) to automatically reduce the effect of noisy labels, as well as to fine-tune the model with only labeled data after every self-training step. 




Specifically, our LST method consists of inner-loop self-training (for one task) and outer-loop meta-learning (over all tasks). LST meta-learns both \emph{to initialize a self-training model} and \emph{how to cherry-pick from noisy labels} for each task. 
An inner loop starts from the meta-learned initialization by which a task-specific model can be fast adapted with few labeled data. Then, this model is used to predict pseudo labels, and labels are weighted by the meta-learned soft weighting network (SWN).
Self-training consists of re-training using weighted pseudo-labeled data and fine-tuning on few labeled data.
In the outer loop, the performance of these meta-learners are evaluated via an independent validation set,
and  parameters are optimized using the corresponding validation loss.


In summary, our LST method learns to accumulate \emph{self-supervising experience} from SSFSC tasks in order to quickly adapt to a new few-shot task.
\textbf{Our contribution} is three-fold. 
(i) A novel self-training strategy 
that prevents the model from drifting due to label noise and enables robust recursive training.
(ii) A novel meta-learned cherry-picking method that optimizes the weights of pseudo labels particularly for fast and efficient self-training.
(iii) Extensive experiments on two versions of ImageNet benchmarks -- miniImageNet~\cite{VinyalsBLKW16} and tieredImageNet~\cite{RenICLR2018_semisupervised}, in which our method achieves top performance.


\section{Related works}
\myparagraph{Few-shot classification (FSC).}
Most FSC works are based on supervised learning. They can be roughly divided into four categories:
(1) data augmentation based methods~\cite{Mehrotra2017, SchwartzNIPS18, WangCVPR2018, XianCVPR2019a} generate data or features in a conditional way for few-shot classes;
(2) metric learning methods~\cite{VinyalsBLKW16, SnellSZ17, SungCVPR2018} learn a similarity space of image features in which the classification should be efficient with few examples;
(3) memory networks~\cite{MunkhdalaiICML2017, SantoroBBWL16, OreshkinNIPS18, MishraICLR2018} design special networks to record training ``experience'' from seen tasks, aiming to generalize that to the learning of unseen ones; 
and 
(4) gradient descent based methods~\cite{FinnAL17, FinnNIPS2018, AntoniouICLR19, RaviICLR2017, LeeICML18, GrantICLR2018, ZhangNIPS2018MetaGAN, SunCVPR2019}
learn a meta-learner in the outer loop to initialize a base-learner for the inner loop that is then trained on a novel few-shot task.
In our LST method, the outer-inner loop optimization is based on the gradient descent method. Different to previous works, we propose a novel meta-learner that assigns weights to pseudo-labeled data, particularly for semi-supervised few-shot learning.


\myparagraph{Semi-supervised learning (SSL).} 
SSL methods aim to leverage unlabeled data to obtain decision boundaries that better fit the underlying data structure~\cite{OliverNIPS18semi_survey}.
The -Model applies a simple consistency regularization~\cite{LaineICLR2017pi_model}, e.g. by using dropout, adding noise and data augmentation, in which data is automatically ``labeled''.
Mean Teacher is more stable version of the -Model by making use of a moving average technique~\cite{TarvainenNIPS17mean_teacher}.
Visual Adversarial Training (VAT) regularizes the network against the adversarial perturbation, and it has been shown to be an effective regularization~\cite{MiyatoDG16VAT}. 
Another popular method is Entropy Minimization that uses a loss term to encourage low-entropy (more confident) predictions for unlabeled data, regardless of their real classes~\cite{GrandvaletNIPS04_entmin}.
Pseudo-labeling is a self supervised learning method that relies on the predictions of unlabeled data, i.e. pseudo labels~\cite{Lee2013pseudo_label}.
It can outperform regularization based methods, especially when labeled data is scarce~\cite{OliverNIPS18semi_survey} as in our envisioned setting.
We thus use this method in our inner loop training.




\myparagraph{Semi-supervised few-shot classification (SSFSC).}
Semi-supervised learning on FSC tasks aims to improve the classification accuracy by adding a large number of unlabeled data in training.
Ren \emph{et al}. proposed three semi-supervised variants of ProtoNets~\cite{SnellSZ17}, basically using Soft -Means method to tune clustering centers with unlabeled data.
A more recent work used
the transductive propagation network (TPN)~\cite{LiuICLR2019transductive} to propagate labels from labeled data to unlabeled ones, and
meta-learned the key hyperparameters of TPN.
Differently, we build our method based on the simple and classical self-training~\cite{Yarowsky95self_training} and meta gradient descent method~\cite{FinnAL17, SunCVPR2019} without requiring a new design of a semi-supervised network.
Rohrbach \emph{et al}.~\cite{RohrbachNIPS13transfer} proposed to further leverage external knowledge, such as the semantic attributes of categories, to solve not only few-shot but also zero-shot problems. Similarly, we expect further gains of our approach when using similar external knowledge in our future work.



\section{Problem definition and denotation}

In conventional few-shot classification (FSC), each task has a small set of labeled training data called support set , and another set of unseen data for test, called query set . 
Following~\cite{RenICLR2018_semisupervised}, we denote another set of unlabeled data as  to be used for semi-supervised learning (SSL).  may or may not contain data of distracting classes (not included in ).

Our method follows the uniform episodic formulation of meta-learning~\cite{VinyalsBLKW16} that is different to traditional classification in three aspects. (1)~Main phases are meta-train and meta-test (instead of train and test), each of which includes training (and
self-training in our case) and test. (2)~Samples in meta-train and meta-test are not datapoints but episodes (SSFSC tasks in our case). (3)~Meta objective is not to classify unseen datapoints but to fast adapt the classifier on a new task. 
Let's detail the denotations. 
Given a dataset  for meta-train, we first sample SSFSC tasks  from a distribution  such that each  has few samples from few classes, e.g.  classes and  sample per class. 
 has a support set  plus an unlabeled set  (with a larger number of samples) to train a task-specific SSFSC model, and a query set  to compute a validation loss used to optimize meta-learners.
For meta-test, given an unseen new dataset , we sample a new SSFSC task . ``Unseen'' means there is no overlap of image classes (including distracting classes) between meta-test and meta-train tasks . We first initialize a model and weight pseudo labels for this unseen task, then self-train the model on  and .
We evaluate the self-training performance on a query set . 
If we have multiple unseen tasks, we report average accuracy as the final evaluation.








\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/figure_overall_one_task.eps}
     \caption{The pipeline of the proposed \textbf{LST} method on a single (-class, -shot) task. 
     The prototype of a class is the mean feature in the class, and SWN is the soft weighting network whose optimization procedure is given in Figure~\ref{figure_meta_learned_swn} and Section~\ref{sec_retraining}.}
  \label{figure_overall_one_task}
  \vspace{-4mm}
\end{figure*}
 

\section{Learning to self-train (LST)}

The computing flow of applying LST to a single task is given in Figure~\ref{figure_overall_one_task}.
It contains
\emph{pseudo-labeling} unlabeled samples by a few-shot model pre-trained on the support set; 
\emph{cherry-picking} pseudo-labeled samples by hard selection and soft weighting; \emph{re-training} on picked ``cherries'', followed by a fine-tuning step;
and the \emph{final test} on a query set.
On a meta-train task, \emph{final test} acts as a validation to output a loss for optimizing meta-learned parameters of LST, as shown in Figure~\ref{figure_meta_learned_swn}.


\subsection{Pseudo-labeling \& cherry-picking unlabeled data}
\label{sec_pseudolabeling}

\myparagraph{Pseudo-labeling.}
This step deploys a supervised few-shot method to train a task-specific classifier  on the support set . 
Pseudo labels of the unlabeled set  are then predicted by .
Basically, we can use different methods to learn .
We choose a top-performing one -- meta-transfer learning (MTL)~\cite{SunCVPR2019} (for fair comparison we also evaluate this method as a component of other semi-supervised methods~\cite{RenICLR2018_semisupervised, LiuICLR2019transductive})
that is based on simple and elegant gradient descent optimization~\cite{FinnAL17}.
In the outer loop meta-learning, MTL learns scaling and shifting parameters  to fast adapt a large-scale pre-trained network  (e.g. for  classes and  images per class on miniImageNet~\cite{VinyalsBLKW16}) to a new learning task.
In the inner loop base-learning, MTL takes the last fully-connected layer as classifier  and trains it with . 



In the following, we detail the pseudo-labeling process on a task . 
Given the support set ,
its loss is used to optimize the task-specific base-learner (classifier)  by gradient descent:

where  is the iteration index and . The initialization  is given by  which is meta-learned (see Section~\ref{sec_retraining}). 
Once trained, we feed  with unlabeled samples 
to get pseudo labels  as follows,

where  indicates the classifier function with parameters  and feature extractor with parameters  (the frozen  is omitted for simplicity).



\myparagraph{Cherry-picking.}
As directly applying self-training on pseudo labels  may result in gradual drifts due to label noises,
we propose two countermeasures in our LST method.
The first is to meta-learn the SWN that automatically reweighs the data points to up-weight the more promising ones and down-weighs the less promising ones, i.e. learns to cherry-pick.
Prior to this step we also perform hard selection to only use the most confident predictions~\cite{TrigueroGH15self_labeled}.
The second countermeasure is to fine-tune the model with only labeled data (in ) after every self-training step (see Section~\ref{sec_retraining}). 


Specifically, we refer to the confident scores of  to pick-up the top  samples per class. 
Therefore, we have  samples from  classes in this pseudo-labeled dataset, namely .
Before feeding  to re-training, we compute their soft weights by a meta-learned soft weighting network (SWN), in order to reduce the effect of noisy labels.
These weights should reflect the relations or distances between pseudo-labeled samples and the representations of  classes. 
We refer to a supervised method called RelationNets~\cite{SungCVPR2018} which makes use of relations between support and query samples for traditional few-shot classification.


First, we compute the prototype feature of each class by averaging the features of all its samples.
In the -shot case, we use the unique sample feature as prototype.
Then, given a pseudo-labeled sample , we concatenate its feature with  prototype features, then feed them to SWN. The weight on the -th class is as follows,

where  is the class index and ,  is the sample index in one class and , , and  denotes the parameters of SWN whose optimization procedure is given in Section~\ref{sec_retraining}.
Note that  have been normalized over  classes through a softmax layer in SWN.




\subsection{Self-training on cherry-picked data}
\label{sec_retraining}

As shown in Figure~\ref{figure_meta_learned_swn} (inner loop), our self-training contains two main stages. 
The first stage contains a few steps of re-training on the pseudo-labeled data  in conjunction with support set ,
and the second are fine-tuning steps with only .

We first initialize the
classifier parameters as , where  is meta-optimized by previous tasks in the outer loop.
We then update  by gradient descent on  and .
Assuming there are  iterations, re-training takes the first  iterations and fine-tuning takes the rest . 
For , we have

where  is the base learning rate.  denotes the classification losses that are different for samples from different sets, as follows,

where  is the cross-entropy loss.
It is computed in a standard way on . For a pseudo-labeled sample in , its predictions are weighted by  before going into the softmax layer.
For ,  is fine-tuned on  as




\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/figure_meta_learned_swn.eps}
     \caption{Outer-loop and inner-loop training procedures in our LST method.
The inner loop in the red box contains the  steps of re-training (with  and ) and  steps of fine-tuning (with only ).
     In recursive training, the fine-tuned  replaces the initial MTL learned  (see Section~\ref{sec_pseudolabeling}) for the pseudo-labeling at the next stage.}
  \label{figure_meta_learned_swn}
  \vspace{-4mm}
\end{figure*} 

\myparagraph{Iterating self-training using fine-tuned model.}
Conventional self-training often follows an iterative procedure, aiming to obtain a gradually enlarged labeled set~\cite{Yarowsky95self_training, TrigueroGH15self_labeled}.
Similarly, our method can be iterated once a fine-tuned model  is obtained, i.e. to use  to predict better pseudo labels on  and re-train  again. 
There are two scenarios: (1) the size of  is small, e.g.  samples per class, so that self-training can only be repeated on the same data; and (2) that size is infinite (at least big enough, e.g.  samples per class), we can split it into multiple subsets (e.g.  subsets and each one has  samples) and do the recursive learning each time on a new subset. In this paper, we consider the second scenario. We also validate in experiments that first splitting subsets and then recursive training is better than using the whole set for one re-training round.


\myparagraph{Meta-optimizing ,  and .}
Gradient descent base methods typically use  to compute the validation loss on query set  used for optimizing meta-learner~\cite{SunCVPR2019, FinnAL17}.
In this paper, we have multiple meta-learners with the parameters ,  and .
We propose to update them by the validation losses calculated at different self-training stages, aiming to optimize them particularly towards specific purposes. 
 and  work for feature extraction and final classification affecting on the whole self-training. We optimize them by the loss of the final model .
While,  produces soft weights to refine the re-training steps, and its quality should be evaluated by re-trained classifier .
We thus use the loss of  to optimize it.
Two optimization functions are as follows, 

where  and  are meta learning rates that are manually set in experiments.
 
% \section{Experiments}
\label{expriments}


We evaluate the proposed \textbf{LST} method in terms of few-shot image classification accuracy in semi-supervised settings.
Below we describe the two benchmarks we evaluate on, details of settings, comparisons to state-of-the-art methods, and an ablation study.


\subsection{Datasets and implementation details}
\label{sec_dataset_settings}

\myparagraph{Datasets.} We conduct our experiments on two subsets of ImageNet~\cite{Russakovsky2015}.
\textbf{miniImageNet} was firstly proposed by Vinyals \emph{et al}.~\cite{VinyalsBLKW16} and has been widely used in supervised FSC works~\cite{FinnAL17, RaviICLR2017, SunCVPR2019, RusuICLR2019, GrantICLR2018, FranceschiICML18}, as well as semi-supervised works~\cite{LiuICLR2019transductive, RenICLR2018_semisupervised}. 
In total, there are  classes with  samples of  color images per class. 
In the uniform setting, these classes are divided into , , and  respectively for meta-train, meta-validation, and meta-test.
\textbf{tieredImageNet} was proposed by Ren \emph{et al}.~\cite{RenICLR2018_semisupervised}. 
It includes a larger number of categories,   classes, than miniImageNet.
These classes are from  super-classes which are divided into  for meta-train ( classes),  for meta-validation ( classes), 
and  for meta-test ( classes).
The average image number per class is , which is much bigger than that on miniImageNet. 
All images are resized to . On both datasets, we follow the semi-supervised task splitting method used in previous works~\cite{RenICLR2018_semisupervised, LiuICLR2019transductive}.
We consider the -way classification, and sample -way, -shot (-shot) task to contain  () samples as the support set  and  samples (a uniform number) samples as the query set . 
Then, on the -shot (-shot) task, we have  () unlabeled images per class in the unlabeled set .
After hard selection, we filter out  () samples and only use the rest  () confident ones to do soft weighting and then re-training.
In the recursive training, we use a larger unlabeled data pool containing  samples from which each iteration we can sample a number of samples, i.e.  () samples for 1-shot (5-shot).

\myparagraph{Network architectures} of  and  are based on ResNet-12 (see details of MTL~\cite{SunCVPR2019}) which 
consist of  residual blocks and each block has  CONV layers with  kernels. 
At the end of each block, a  max-pooling layer is applied. The number of filters starts from  and is doubled every next block.
Following residual blocks, a mean-pooling layer is applied to compress the feature maps to a -dimension embedding. 
The architecture of SWN consists of  CONV layers with  kernels in  filters, followed by  FC layers with the dimensionality of  and , respectively.  

\myparagraph{Hyperparameters}. 
We follow the settings used in MTL~\cite{SunCVPR2019}. 
Base-learning rate  (in Eq.~\ref{eq_base_classifier}, Eq.~\ref{mixed_training_update} and Eq.~\ref{finetune_update}) is set to .
Meta-learning rates  and  (in Eq.~\ref{meta_update_1} and Eq.~\ref{meta_update_2}) are set to  initially and decay to the half value every  meta iterations until a minimum value  is reached.
We use a meta-batch size of  and run  meta iterations. 
In recursive training, we use  () recursive stages for -shot (-shot) tasks.
Each recursive stage contains  re-training and  fine-tuning steps. 


\myparagraph{Comparing methods.}
In terms of SSFSC, we have two methods, namely Soft Masked -Means~\cite{RenICLR2018_semisupervised} and TPN~\cite{LiuICLR2019transductive}
to compare with.
Their original models used a shallow, i.e. 4CONV~\cite{FinnAL17} trained from scratch, for feature extraction. 
For fair comparison, we implement the MTL as a component of their models in order to use deeper nets and pre-trained models which have been proved better.
In addition, we run these experiments using the maximum budget of unlabeled data, i.e.  samples per class.
We also compare to the state-of-the-art supervised FSC models which are closely related to ours.
They are based on either data augmentation~\cite{Mehrotra2017, SchwartzNIPS18} or gradient descent~\cite{FinnAL17, RaviICLR2017, GrantICLR2018, FranceschiICML18, ZhangNIPS2018MetaGAN, MunkhdalaiICML18, RusuICLR2019, SunCVPR2019, LeeCVPR19svm}.




\myparagraph{Ablative settings.}
In order to show the effectiveness of our LST method, we design following settings belonging to two groups: with and without meta-training.
Following are the detailed ablative settings.
\emph{no selection} denotes the baseline of once self-training without any selection of pseudo labels.
\emph{hard} denotes hard selection of pseudo labels. \emph{hard} with meta-training means meta-learning only .
\emph{soft} denotes soft weighting on selected pseudo labels by meta-learned SWN. 
\emph{recursive} applies multiple iterations of self-training based on fine-tuned models, see Section~\ref{sec_retraining}. Note that this \emph{recursive} is only for the meta-test task, as the meta-learned SWN can be repeatedly used.
We also have a comparable setting to \emph{recursive} called \emph{mixing} in which we mix all unlabeled subsets used in \emph{recursive} and run only one re-training round (see the last second paragraph of Section~\ref{sec_retraining}).
\subsection{Results and analyses}
\begin{table*}
  \small
  \centering
  \begin{tabular}{l l lcc}
    \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Few-shot Learning Method}}} & \multirow{2}{*}{\textbf{Backbone}} &  \multicolumn{2}{c}{\textbf{miniImageNet (test)}} \\
     &&& -shot & -shot \\
    \midrule
    \multirow{2}{*}{\emph{Data augmentation}}
    & Adv. ResNet, \cite{Mehrotra2017} & WRN-40 (pre) & 55.2 & 69.6 \\
    & Delta-encoder, \cite{SchwartzNIPS18} & VGG-16 (pre) & 58.7 & 73.6 \\
    \midrule  
    \multirow{8}{*}{\emph{Gradient descent}}
    & MAML, \cite{FinnAL17} & 4 CONV & 48.70   & 63.11   \\
    & Meta-LSTM, \cite{RaviICLR2017} & 4 CONV & 43.56   & 60.60   \\
    & Bilevel Programming, \cite{FranceschiICML18} & ResNet-12   & 50.54    & 64.53  \\
    & MetaGAN, \cite{ZhangNIPS2018MetaGAN} & ResNet-12 & 52.71    & 68.63   \\
    & adaResNet, \cite{MunkhdalaiICML18} & ResNet-12   & 56.88   & 71.94   \\
    & LEO, \cite{RusuICLR2019} & WRN-28-10 (pre) & 61.76   & 77.59   \\
    & MTL, \cite{SunCVPR2019} & ResNet-12 (pre) & 61.2    & 75.5   \\
    & MetaOpt-SVM, \cite{LeeCVPR19svm} & ResNet-12 & 62.64    & 78.63   \\
    \midrule
\textbf{LST (Ours)} & \emph{recursive}, \emph{hard}, \emph{soft} & ResNet-12 (pre) & \textbf{70.1}    & \textbf{78.7}    \\
    \midrule\midrule
    \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Few-shot Learning Method}}} & \multirow{2}{*}{\textbf{Backbone}} &  \multicolumn{2}{c}{\textbf{tieredImageNet (test)}} \\
     &&& -shot & -shot \\
    \midrule
\multirow{4}{*}{\emph{Gradient descent}}
    & MAML, \cite{FinnAL17} (by~\cite{LiuICLR2019transductive}) & ResNet-12 & 51.67   & 70.30   \\
    & LEO, \cite{RusuICLR2019} & WRN-28-10 (pre) & 66.33   & 81.44   \\
    & MTL, \cite{SunCVPR2019} (by us) & ResNet-12 (pre) & 65.6    & 78.6   \\
    & MetaOpt-SVM, \cite{LeeCVPR19svm} & ResNet-12 & 65.99    & 81.56   \\
\midrule
\textbf{LST (Ours)} & \emph{recursive}, \emph{hard}, \emph{soft}  & ResNet-12 (pre) & \textbf{77.7}   & \textbf{85.2}   \\
\bottomrule
    \multicolumn{5}{l}{Additional 2 convolutional layers { } One additional convolutional layer} \\
    \multicolumn{5}{l}{Using 15-shot training samples on every meta-train task.} \\
\end{tabular}
\vspace{-0.1cm}
  \caption{The -way, -shot and -shot classification accuracy () on miniImageNet and tieredImageNet datasets. ``pre'' means pre-trained for a single classification task using all training datapoints.}
    \label{table_mini}
    \vspace{-0.1cm}
\end{table*}
 \begin{table*}[t]
\centering
\small
\begin{tabular}{llccccccccccc}
\toprule 
&& \multicolumn{2}{c}{mini}
&& \multicolumn{2}{c}{tiered} && \multicolumn{2}{c}{mini w/}
&& \multicolumn{2}{c}{tiered w/} \\
\cmidrule{3-4}\cmidrule{6-7} \cmidrule{9-10}\cmidrule{12-13}
&& 1(shot) & 5 && 1 & 5 && 1 & 5 && 1 & 5 \\
\midrule[1pt]
\multicolumn{2}{l}{fully supervised (upper bound)} & 80.4 & 83.3 && 86.5 & 88.7  && - & -  && - & -  \\
\midrule[1pt]
\multirow{3}{*}{no meta}& \emph{no selection} & 59.7 & 75.2  && 67.4 & 81.1  && 54.4 & 73.3  && 66.1 & 79.4  \\
&
\emph{hard} & 63.0 & 76.3  && 69.8 & 81.5 && 61.6 & 75.3  && 68.8 & 81.1  \\
&\emph{recursive,hard} & 64.6 & 77.2 && 72.1 & 82.4  && 61.2 & 75.7  && 68.3 & 81.1  \\
\midrule[1pt]
\multirow{5}{*}{meta}&
\emph{hard} () & 64.1 & 76.9  && 74.7 & 83.2  && 62.9 & 75.4  && 73.4 & 82.5  \\
&\emph{soft} & 62.8 & 75.9  && 73.1 & 82.8  && 61.1 & 74.6  && 72.1 & 81.7  \\
&\emph{hard,soft} & 65.0 & 77.8  && 75.4 & 83.4  && 63.7 & 76.2  && \textbf{74.1} & 82.9  \\
&\emph{recursive,hard,soft} & \textbf{70.1} & \textbf{78.7}  && \textbf{77.7} & \textbf{85.2}  && 64.1 & \textbf{77.4}  && 73.5 & 83.4  \\
&\emph{mixing,hard,soft} & 66.2 & 77.9  && 75.6 & 84.6  && \textbf{64.5} & 76.5  && 73.6 & \textbf{83.8}  \\
\midrule[1pt]
\multicolumn{2}{l}{Masked Soft -Means \emph{with} MTL} & 62.1 & 73.6  && 68.6 & 81.0 && 61.0 & 72.0 && 66.9 & 80.2  \\
\multicolumn{2}{l}{TPN \emph{with} MTL} & 62.7 & 74.2  && 72.1 & 83.3  && 61.3 & 72.4 && 71.5 & 82.7  \\
\multicolumn{2}{l}{Masked Soft -Means~\cite{RenICLR2018_semisupervised}} & 50.4 & 64.4  && 52.4 & 69.9 && 49.0 & 63.0 && 51.4 & 69.1  \\
\multicolumn{2}{l}{TPN~\cite{LiuICLR2019transductive}} & 52.8 & 66.4  && 55.7 &  71.0 && 50.4 &  64.9 && 53.5 & 69.9  \\
\bottomrule[1pt]
\end{tabular}
\caption{Classification accuracy () in ablative settings (middle blocks) and related SSFSC works (bottom block), on miniImageNet (``mini'') and tieredImageNet (``tiered'').
``fully supervised'' means the labels of unlabeled data are used. ``w/'' means using unlabeled data from  distracting classes that are \textbf{excluded} in the support set~\cite{LiuICLR2019transductive, RenICLR2018_semisupervised}. 
The results of using a small unlabeled set,  per class~\cite{RenICLR2018_semisupervised}, are given in the supplementary materials.}
\label{table_ablation}
 \vspace{-0.2cm}
\end{table*} 

We conduct extensive experiments on semi-supervised few-shot classification. In Table~\ref{table_mini}, we present our results compared to the state-of-the-art FSC methods, respectively on miniImageNet and tieredImageNet. In Table~\ref{table_ablation}, we provide experimental results for ablative settings and comparisons with the state-of-the-art SSFSC methods. In Figure~\ref{plot_experiment}, we show the effect of using different numbers of re-training steps (i.e. varying  in Figure~\ref{figure_meta_learned_swn}).

\myparagraph{Overview for two datasets with FSC methods}. 
In the upper part of Table~\ref{table_mini}, we present SSFSC results on miniImageNet. We can see that LST achieves the best performance for the -shot () setting, compared to all other FSC methods. Besides, it tackles the -shot episodes with an accuracy of . This result is slightly better than  reported by~\cite{LeeCVPR19svm}, which uses various regularization techniques like data augmentation and label smoothing.
Compared to the baseline method MTL~\cite{SunCVPR2019}, LST improves the accuracies by  and  respectively for -shot and -shot, which proves the efficiency of LST using unlabeled data. In the lower part of Table~\ref{table_mini}, we present the results on tieredImageNet. Our LST performs best in both -shot () and 5-shot () and surpasses the state-of-the-art method~\cite{LeeCVPR19svm} by  and  respectively for -shot and -shot. 
Compared to MTL~\cite{SunCVPR2019}, LST improves the results by  and  respectively for -shot and -shot.

\myparagraph{Hard selection}. In Table~\ref{table_ablation}, we can see that the hard selection strategy often brings improvements. For example, compared to \emph{no selection}, \emph{hard} can boost the accuracies of -shot and -shot by  and  respectively on miniImageNet,  and  respectively on tieredImageNet. This is due to the fact that selecting more reliable samples can relieve the disturbance brought by noisy labels. 
Moreover, simply repeating this strategy (\emph{recursive,hard}) brings about  average gain.


\myparagraph{SWN}. The meta-learned SWN is able to reduce the effect of noisy predictions in a soft way, leading to better performance. When using SWN individually, \emph{soft} achieves comparable results with two previous SSFSC methods~\cite{RenICLR2018_semisupervised, LiuICLR2019transductive}. When using SWN in cooperation with hard selection (\emph{hard,soft}) achieves  improvement on miniImageNet for both -shot and -shot compared to \emph{hard}(), which also shows that SWN and the hard selection strategy are complementary.

\myparagraph{Recursive self-training}. Comparing the results of \emph{recursive,hard} with \emph{hard}, we can see that by doing recursive self-training when updating , the performances are improved in both ``meta'' and ``no meta'' scenarios. E.g., it boosts the results by  when applying recursive training to \emph{hard,soft} for miniImageNet -shot. 
However, when using \emph{mixing,hard,soft} that learns all unlabeled data without \emph{recursive}, the improvement reduces by .
These observations show that recursive self-training can successfully leverage unlabeled samples. 
However, this method sometimes brings undesirable results in the cases with distractors. E.g., compared to \emph{hard}, the \emph{recursive,hard} brings  and  reduction for -shot on miniImagenet and tieredImagenet respectively, which might be due to the fact that disturbances caused by distractors in early recursive stages propagate to later stages. 

\myparagraph{Comparing with the state-of-the-art SSFSC methods}. We can see that Masked Soft -Means~\cite{RenICLR2018_semisupervised} and TPN~\cite{LiuICLR2019transductive} improve their performances by a large margin (more than  for -shot and  for 5-shot) when they are equipped with MTL and use more unlabeled samples ( per class). Compared with them, our method (\emph{recursive,hard,soft}) achieves more than  and  improvements respectively for -shot and -shot cases with the same amount of unlabeled samples on miniImagenet. Similarly, our method also surpasses TPN by  and  for -shot and -shot on tieredImagenet. 
Even though our method is slightly more effected when adding distractors to the unlabeled dataset, we still obtain the best results compared to others.


\begin{figure}
\newcommand{\plotexpone}[1]{\includegraphics[height=1.36in]{#1}}
\plotexpone{plots/figure3a.eps}
\plotexpone{plots/figure3b.eps}
\plotexpone{plots/figure3c.eps}\\
\hspace*{2.31cm} (a) \hspace{4.18cm}(b) \hspace{4.18cm}(c)\\
\vspace{-0.5cm}
\caption{Classification accuracy on -shot miniImageNet using different numbers of re-training steps, e.g.  means using  steps f re-training and  steps ( steps in total) of fine-tuning at every recursive stage. 
Each curve shows the results obtained at the final stage. Methods are (a) our LST; (b) \emph{recursive, hard} (no meta) with MTL~\cite{SunCVPR2019}; and (c) \emph{recursive, hard} (no meta) simply initialized by pre-trained ResNet-12 model~\cite{SunCVPR2019}. Results on tieredImageNet are given in the supplementary.
}
\vspace{-0.5cm}
\label{plot_experiment}
\end{figure} 
In order to better understand our method and validate the robustness, we present more in-depth results regarding the key components, namely re-training steps, distracting classes, pseudo labeling accuracies, and using different architectures as backbone, in the following texts.

\myparagraph{Number of re-training steps}. In Figure~\ref{plot_experiment}, we present the results for different re-training steps. Figure~\ref{plot_experiment}(a), (b) and (c) show different settings respectively: LST; \emph{recursive,hard} that uses the off-the-shelf MTL method; and \emph{recursive,hard} that replaces MTL with pre-trained ResNet-12 model.
All three figures show that re-training indeed achieves better results, but too many re-training steps may lead to drifting problems and cause side effects on performance. The first two settings reach best performance at  re-training steps while the third one needs  re-training steps. That means MTL-based methods (LST and the \emph{recursive,hard}) achieve faster convergence compared to the one directly using pre-trained ResNet-12 model.


\myparagraph{Quantitative analyses on the number of distracting classes}. 
In Figure~\ref{distractors_experiment}, we show the effects of distracting classes on our LST and related methods (improved versions \emph{with} MTL)~\cite{RenICLR2018_semisupervised, TarvainenNIPS17mean_teacher}.
More distracting classes cause more performance deduction for all methods.
Our LST achieves the top performance, especially more than  higher than TPN~\cite{LiuICLR2019transductive} in the hardest case with  distracting classes. 
Among our different settings, we can see that LST with less re-training steps, i.e., a smaller  value, works better for reducing the effect from a larger number of distracting classes.
\begin{figure}[t]
\newcommand{\plotexpone}[1]{\includegraphics[height=1.38in]{#1}}
\hspace*{0.5cm}
\plotexpone{plots/mini_plot.eps}
\hspace{1cm}
\plotexpone{plots/tiered_plot.eps}\\
\hspace*{3.30cm} (a) \hspace{6.55cm}(b)\\
\vspace{-0.5cm}
\caption{Classification accuracy on miniImageNet 1-shot (a) and tieredImageNet 1-shot (b), using different numbers of distracting classes.}
\vspace{-0.3cm}
\label{distractors_experiment}
\end{figure} 
\myparagraph{Displaying the performance of pseudo-labeling.}. Taking the miniImageNet 1-shot as an example, we record the accuracy of pseudo labeling for meta-training and meta-test (based on our best model \emph{recursive, hard, soft}), in Table~\ref{table_pl_train} and Table~\ref{table_pl_test}, respectively. 
In meta-training, we can see the accuracy grows from  (iter=) to  (iter=), and it reaches saturation after  iterations.
There are  recursive stages during meta-test. From stage- to stage-, the average accuracy of  meta-test tasks using our best method increases from  to . 


\begin{table*}[h]
\vspace{-3pt}
\footnotesize
\centering
\begin{tabular}{ccccccccc}
\toprule
Iteration &  &  &  &  &  &  &  & \\
\midrule[1pt]
Accuracy & 47.0 & 64.1 & 65.9 & 70.0 & 71.2 & 70.9 & 71.3 & 71.5 \\
\bottomrule[1pt]
\end{tabular}
\vspace{-0.15cm}
\caption{Pseudo-labeling accuracies () during the meta-training process, on miniImageNet 1-shot.}
\label{table_pl_train}
\end{table*} \begin{table*}[h]
\vspace{-5pt}
\footnotesize
\centering
\begin{tabular}{ccccccc}
\toprule
Stage & 1 & 2 & 3 & 4 & 5 & 6\\
\midrule[1pt]
Accuracy & 59.8 & 63.6 & 65.1 & 66.9 & 67.9 & 68.8 \\
\bottomrule[1pt]
\end{tabular}
\vspace{-0.15cm}
\caption{Pseudo-labeling accuracies () at six recursive stages of meta-test, on miniImageNet 1-shot. Stage- is initialization.}
\label{table_pl_test}
\end{table*} 

\myparagraph{Generalization ability}.
Our LST approach is in principle able to generalize to other optimization-based FSC methods. To validate this, we replace MTL with a classical method called MAML~\cite{FinnAL17}.
We implement the experiments of MAML-based LST (using \emph{recursive,hard,soft}) and compare with the same 4CONV-arch model TPN~\cite{LiuICLR2019transductive}. 
On miniImagenet -shot,
our method gets the accuracy of  ( for w/), outperforming TPN by   ( for w/).
On the more challenging dataset tieredImageNet (1-shot) we achieve even higher superiority, i.e.,  ( for w/). 
 
% 

\section{Conclusions}
We propose a novel \textbf{LST} approach for semi-supervised few-shot classification. A novel recursive-learning-based self-training strategy is proposed for robust convergence of the inner loop, while a cherry-picking network is meta-learned to select and label the unsupervised data optimized in the outer loop. 
Our method is general in the sense that any optimization-based few-shot method with different base-learner architectures can be employed. On two popular few-shot benchmarks, we found consistent improvements over both state-of-the-art FSC and SSFSC methods.


\section*{Acknowledgments}
This research is part of NExT research which is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.
It is also partially supported by German Research Foundation (DFG CRC 1223), and National Natural Science Foundation of China (61772359, 61671289, 61771301, 61521062).




\bibliography{egbib.bib}{}
\bibliographystyle{plain}

\clearpage



\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\noindent
{\Large {\textbf{Supplementary materials}}}
\\

These supplementary materials include 
the additional results of using different numbers of stages in the recursive training in our LST method (\emph{recursive, hard, soft}) (\S\ref{sec_stages}), 
the supplementary results (on the tieredImageNet dataset) of Figure~3 in the main paper (\S\ref{sec_supple_fig3}),
and the comparable results using a very limited number of unlabeled data, i.e.  unlabeled samples per class (\S\ref{sec_fewer_data}). There are also experimental results about when our LST method is equiped with different backbones (\S\ref{sec_different_backbones}).


\section{Using different numbers of recursive stages}
\label{sec_stages}

During meta-validation, we test our method using different numbers of recursive stages, and show the results in Figure~\ref{figure_diff_recursive_stage}.
We observe that the performance of our method is saturated when running after e.g.  stages. 
In experiments, we split  samples (per class) as the unlabeled dataset.
At each recursive stage, we sample a subset, i.e.  for 1-shot and  for 5-shot. After a few stages, the model has sampled and learned all unlabeled samples, therefore, its performance gets saturated.
We choose the peak values, so we use  stages for 1-shot and  for 5-shot during meta-test, on both miniImageNet and tieredImageNet.


\begin{figure}[h]
\newcommand{\plotexpone}[1]{\includegraphics[height=2.1in]{#1}}
\plotexpone{supple_figures/figureS1a.eps}
\plotexpone{supple_figures/figureS1b.eps}\\
\hspace*{3.40cm} (a) \hspace{6.70cm}(b) \\
\vspace{-0.2cm}
\caption{Meta-validation results (classification accuracy) using different numbers of recursive stages, in the 1-shot (a) and 5-shot (b) settings on the miniImageNet dataset.}
\label{figure_diff_recursive_stage}
\end{figure} 

\section{Using different numbers of re-training steps}
\label{sec_supple_fig3}

In Figure~\ref{figure_re_train}, we report the results on tieredImageNet 1-shot, using different numbers of re-training steps, as the supplementary 
of Figure 3 in the main paper.
The same as in Figure 3, each curve shows the results obtained at the final recursive stage. Corresponding methods are (a) our LST, (b) our ablative method  (no meta) with off-the-shelf MTL model~\cite{SunCVPR2019}, and (c) the  (no meta) that directly uses pre-trained ResNet-12 model~\cite{SunCVPR2019}.
We can observe that very few re-training steps, i.e.  steps, are enough for our LST model to converge to the best performance, similar to the conclusion drawn from the results on miniImageNet.


\begin{figure}[t]
\newcommand{\plotexpone}[1]{\includegraphics[height=1.35in]{#1}}
\plotexpone{supple_figures/figureSa.eps}
\plotexpone{supple_figures/figureSb.eps}
\plotexpone{supple_figures/figureSc.eps}\\
\hspace*{2.31cm} (a) \hspace{4.18cm}(b) \hspace{4.18cm}(c)\\
\hspace*{2.40cm} 
\vspace{-0.2cm}
\caption{Classification accuracy in the 1-shot tieredImageNet, using different numbers of re-training steps, e.g.  means using  steps of re-training and  steps ( steps in total) of fine-tuning at every recursive stage. 
Each curve shows the results obtained at the final stage. Methods are (a) our LST; (b) \emph{recursive, hard} (no meta) with MTL~\cite{SunCVPR2019}; and (c) \emph{recursive, hard} (no meta) simply initialized by pre-trained ResNet-12 model~\cite{SunCVPR2019}.
}
\label{figure_re_train}
\end{figure} 


\section{Using a small number of unlabeled samples}
\label{sec_fewer_data}

We also consider using limited number of unlabeled samples ( per class) in the experiments.
In this setting, we evaluate our LST method (the version without \emph{recursive} due to few unlabeled data) as well as related methods, Masked Soft -Means and TPN.
Note that same with Table 2 in the main paper, these related methods are equipped with MTL, i.e. using pre-trained ResNet- as backbone, and using more efficient meta operations (scaling and shifting) in the feature extraction part.
As shown in Table~\ref{table_fewer_un}, our method achieves the best performance compared to other two methods, on both benchmarks.


\begin{table}[h]
\centering
\small
\begin{tabular}{llccccccccccc}
\toprule 
&& \multicolumn{2}{c}{mini}
&& \multicolumn{2}{c}{tiered} && \multicolumn{2}{c}{mini w/}
&& \multicolumn{2}{c}{tiered w/} \\
\cmidrule{3-4}\cmidrule{6-7} \cmidrule{9-10}\cmidrule{12-13}
&& 1(shot) & 5 && 1 & 5 && 1 & 5 && 1 & 5 \\
\midrule[1pt]
\multicolumn{2}{l}{\emph{hard, soft} (Ours w/o \emph{recursive})} & 61.9 & 75.3 && 72.1 & 82.4  && 60.3 & 75.0  && 70.7 & 82.0  \\
\midrule[1pt]
\multicolumn{2}{l}{Masked Soft k-Means~\cite{RenICLR2018_semisupervised} \emph{with} MTL} & 58.2 & 71.9  && 65.3 & 79.8 && 56.8 & 71.1 && 63.6 & 79.2  \\
\multicolumn{2}{l}{TPN~\cite{LiuICLR2019transductive} \emph{with} MTL} & 59.3 & 71.9  && 67.4 & 80.7  && 58.7 & 70.6 &&  67.2 & 80.5  \\
\bottomrule[1pt]
\end{tabular}
\vspace{0.1cm}
\caption{Classification accuracy (\%) using a limited number of unlabeled samples ( per class) on two benchmarks -- miniImageNet (``mini'') and tieredImageNet (``tiered''). ``w/'' means using unlabeled data from distracting classes that are \textbf{excluded} in the support set~\cite{RenICLR2018_semisupervised, LiuICLR2019transductive}.}
\label{table_fewer_un}
\end{table} 

\section{Generalization ability}
\label{sec_different_backbones}
We incorporate the 4CONV arch. of MAML~\cite{FinnAL17} and the recent FSC method LEO~\cite{RusuICLR2019} into our LST, respectively. The results are shown in Table~\ref{table_backbones}. For example, on tieredImageNet 1-shot, LST-MAML-4CONV outperforms TPN-4CONV~\cite{LiuICLR2019transductive} by  and  (). LST-LEO-ResNet12 outperforms TPN-ResNet12 by  and  ().

\begin{table*}[h]
\centering
\begin{tabular}{lcccccccc}
\toprule 
& \multicolumn{2}{c}{MAML~\cite{FinnAL17}}
&& \multicolumn{2}{c}{LEO~\cite{RusuICLR2019}}\\
\cmidrule{2-3}\cmidrule{5-6}
& mini(1-shot) / D & tiered / D  && mini / D & tiered / D\\
\midrule[1pt]
\midrule[1pt]
\emph{recursive,hard,soft} & 54.8 / 52.0 & 58.6 / 55.5  && 66.0 / 63.5  & 75.9 / 74.3 \\
TPN~\cite{LiuICLR2019transductive} & 52.8 / 50.4  & 55.7 / 53.5 && 62.7 / 61.3 &  72.1 / 71.5\\
\bottomrule[1pt]
\end{tabular}
\vspace{-0.15cm}
\caption{5-way, 1-shot classification accuracy () by replacing our base network MTL(ResNet-12)~\cite{SunCVPR2019} with MAML(4CONV)~\cite{FinnAL17} and LEO(ResNet-12)~\cite{RusuICLR2019}.}
\label{table_backbones}
\end{table*}  
\end{document}