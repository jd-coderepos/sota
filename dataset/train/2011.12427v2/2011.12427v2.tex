\section{Results and Discussion}
\label{sec:results}

This section presents the benchmark results for the identification and verification tasks.
We first describe the experimental setup used to perform the benchmark.
Then, we report and discuss the results achieved by each~approach.

\subsection{Experimental Setup}

Inspired by several recent works~\cite{Luz2018, zanlorensi2018impact, reddy2018comparison, wang2019cross, boyd2019fine, zanlorensi2020deep, boutros2020fusing, diaz2020spectrum, hern2020crossspectral}, we perform the benchmark employing pre-trained models on ImageNet and also for face recognition~(VGG16-Face and ResNet50-Face).
Afterward, we fine-tuned these models using the UFPR-Periocular dataset.
Similar to recent works on ocular recognition~\cite{Luz2018, zanlorensi2018impact, silva2018multimodal, zanlorensi2019ocular}, we modify all models by adding a fully convolutional layer before the last layer (softmax) to generate a feature vector with a size of $256$ for each image.
The default input size of the models is $224\times224\times3$, except for the InceptionResNet and Xception models, which have an input size of~$299\times299\times3$. 
Note that the input dimensions are different because we are using pre-trained models and therefore our fine-tuning process should follow the original architectures' input size.
In this way, for training and evaluation, the periocular images were resized to fit the input size required for each method, i.e., $299\times299\times3$ for both InceptionResNet and Xception and $244\times244\times3$ for the remaining~models. 

For all methods, the training was performed during $60$ epochs with a learning rate of $10^{-3}$ for the first $15$ epochs and $5\times10^{-4}$ for the remaining epochs using the Stochastic Gradient Descent~(SGD) optimizer.
Then, we used the weights from the epoch that achieves the lower loss in the validation set to perform the~evaluation.

We employ Rank $1$ and Rank $5$ accuracy for the identification task, and the Area Under the Curve~(AUC), Equal Error Rate~(EER), and Decidability~(DEC) metrics for verification.
Furthermore, to generate the verification scores, we compute the cosine distance between the deep representations generated by each \gls*{cnn} model.
As described and applied in several works with state-of-the-art results~\cite{Luz2018, zanlorensi2018impact, zanlorensi2020deep, zanlorensi2020attnormalization}, the cosine distance is computed by the cosine angle between two vectors, being invariant to scalar transformation.
This measure gives more attention to the orientation than to the coefficient of magnitude of the representations, being an interesting metric to compute the similarity between two vectors.
The cosine metric distance is given by:
\begin{equation}
d_{c}(A,B) = 1 - \frac{\sum_{j=1}^{N}A_{j}B_{j}}{\sqrt{\sum_{j=1}^{N}A_{j}^2} \sqrt{\sum_{j=1}^{N}B_{j}^2}} \,,
\end{equation}
\noindent where $A$ and $B$ stand for the feature vectors.

Regarding the models explicitly developed for the verification tasks, i.e., the Siamese and the Pairwise Filters networks, as this task has unbalanced samples of genuine and impostors pairs, selecting the best samples to perform the training is challenging.
Thus, trying to fit the models by feeding them samples as diverse as possible, we employed all genuine pairs and randomly selected the same number from the impostor pairs for each epoch.
Hence, each epoch may have different impostor samples.
However, for a fair comparison, we generated the random impostor pairs only once for each epoch and fold, and used the same samples for training both~models.

The reported results are from five repetitions for each fold, except for the Siamese and Pairwise filter networks, in which we ran only three repetitions due to the high computational cost.
All experiments were performed on a computer with an AMD Ryzen Threadripper $1920$X $3.5$GHz ($4.0$GHz Turbo)~CPU, $64$~GB of RAM and an NVIDIA Quadro RTX~$8000$ GPU~($48$~GB).
All~\gls*{cnn} models were implemented in Python using the Tensorflow ({\small\url{https://www.tensorflow.org/}}) and Keras ({\small\url{https://keras.io/}})~frameworks.

\subsection{Benchmark results}

The results obtained by each approach in the closed-world and open-world protocols are presented in this section.
An ablation study were performed evaluating each task's influence in the identification mode on the Multi-task learning network.
Table~\ref{tab:modelstats} shows the size and the number of trainable parameters of each \gls*{cnn} model used as a benchmark.
This information was extracted from the models employed in the closed-world protocol since they have more neurons on the last layer than the open-world protocol models.
We also report the results achieved by employing the state-of-the-art method that achieved first place in the VISOB 2 competition on mobile ocular biometric recognition~\cite{nguyen2021visob}.
This method~\cite{zanlorensi2020deep} consists of an ensemble of five ResNet-50 models pre-trained for face recognition and fine-tuned using the periocular images of our dataset and employing the same experimental protocol described in this work.

\begin{table}[!ht]
\centering
\caption{Size~(MB) and number of trainable parameters of the \gls*{cnn}~models used in the benchmark.}
\label{tab:modelstats}

\vspace{-2mm}
\resizebox{.7\columnwidth}{!}{
\begin{tabular}{@{}lrr@{}}
\toprule

Model              & Size (MB)       & Trainable parameters   \\


\midrule
VGG16              & $1088$  & $135{,}886{,}084$  \\
VGG16-Face         & $1088$  & $135{,}886{,}084$  \\ 
InceptionResNet    & $445$   & $55{,}246{,}372$  \\
ResNet50V2         & $400$   & $49{,}786{,}436$  \\
ResNet50           & $198$   & $24{,}609{,}284$  \\
ResNet50-Face      & $198$   & $24{,}609{,}284$  \\
Xception           & $176$   & $21{,}908{,}204$  \\
DenseNet121        & $64$    & $7{,}792{,}964$  \\
MobileNetV2        & $26$    & $3{,}128{,}516$  \\
\midrule
Multi-task         & $37$    & $4{,}494{,}230$  \\
\midrule
Siamese            & $21$    & $2{,}551{,}808$  \\
Pairwise           & $20$    & $2{,}349{,}479$  \\
\bottomrule
\end{tabular}}
\end{table}

As can be seen, the benchmark has a great diversity of models with different sizes and parameters due to their difference in structure, depth, concept, and~architectures. 

\subsubsection{Closed-World Protocol}
\label{sec:closed}
We perform the benchmark for both the identification and verification tasks in the closed-world protocol.
All results are presented in Table~\ref{tab:benchclosed} and Fig.~\ref{fig:roc_closed}.
Even though the MobileNetV2 is the shortest model in size and trainable parameters, it achieved the best results for identification and verification tasks.
Therefore, we employed MobileNetV2 as the base model for the Multi-task, Siamese, and Pairwise Filters networks.

\begin{table*}[!ht]
\centering
\caption{Benchmark results in the closed-world protocol for the identification and verification tasks.}
\label{tab:benchclosed}

\vspace{-2mm}
\resizebox{.7\linewidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule

\centering \multirow{2}{*}{Model} & \multicolumn{2}{c}{Identification ($1$:$N$)} & \multicolumn{3}{c}{Verification ($1$:$1$)}  \\

\cmidrule{2-6}

                   & Rank 1 (\%)     & Rank 5 (\%)     & AUC (\%)           & EER (\%)        & Decidability       \\


\midrule
VGG16              & $50.56\pm3.30$  & $68.73\pm3.01$  & $99.41\pm0.11$  & $3.59\pm0.32$   & $4.4544\pm0.1502$ \\
VGG16-Face         & $56.29\pm1.62$  & $73.84\pm1.48$  & $99.43\pm0.08$  & $3.44\pm0.28$   & $4.5069\pm0.1379$ \\
Xception           & $57.43\pm1.43$  & $75.88\pm1.52$  & $99.77\pm0.04$  & $2.19\pm0.18$   & $4.2470\pm0.0538$ \\
ResNet50V2         & $63.18\pm2.14$  & $77.79\pm1.81$  & $99.74\pm0.04$  & $2.24\pm0.18$   & $4.9382\pm0.1184$ \\
InceptionResNet    & $65.16\pm2.45$  & $81.53\pm1.99$  & $99.78\pm0.15$  & $1.85\pm0.40$   & $4.5561\pm0.1183$ \\
ResNet50           & $71.06\pm1.14$  & $85.22\pm0.82$  & $99.89\pm0.02$  & $1.41\pm0.10$   & $5.1242\pm0.0634$ \\
ResNet50-Face      & $73.76\pm1.43$  & $86.86\pm1.02$  & $99.83\pm0.03$  & $1.74\pm0.12$   & $5.2400\pm0.0837$ \\
DenseNet121        & $75.54\pm1.36$  & $88.53\pm0.97$  & $99.93\pm0.02$  & $1.11\pm0.09$   & $5.1730\pm0.0497$ \\
MobileNetV2        & $77.98\pm1.08$  & $90.19\pm0.79$  & $99.93\pm0.01$  & $1.13\pm0.07$   & $5.2477\pm0.0650$ \\
\midrule
\textbf{Multi-task}& $\mathbf{84.32\pm0.71}$  & $\mathbf{94.55\pm0.58}$  & $\mathbf{99.96\pm0.01}$  & $\mathbf{0.81\pm0.06}$   & $5.1978\pm0.0340$ \\
Visob 2.0 Winner~\cite{zanlorensi2020deep, nguyen2021visob} & -  & -  & $99.94\pm0.01$  & $1.02\pm0.09$   & $6.0345\pm0.0788$ \\
\midrule
Siamese            & $-$             & $-$             & $98.94\pm0.22$  & $4.86\pm0.44$  & $3.0005\pm0.1871$  \\
Pairwise           & $-$             & $-$             & $99.44\pm0.66$  & $3.06\pm1.84$  & $\mathbf{6.4503\pm1.2270}$  \\
\bottomrule
\end{tabular}}
\end{table*}


\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/closedRoc300.pdf}

\vspace{-3mm}
\caption{\acrfull*{roc} curve to compare methods in the closed-world protocol.}
\label{fig:roc_closed}
\end{figure}

The Multi-task model achieved the best results in Rank~$1$, Rank~$5$, AUC, and~EER metrics.
We emphasize that we only explored other tasks such as --~age, gender, eye side, and mobile device model~-- at the training stage of this model.
We extracted the representations only for the classification task to evaluate identification (using the softmax layer) and verification (using the cosine distance) tasks.
The Siamese network obtained the worst results in the benchmark.
In contrast, the Pairwise Filters network reached the higher Decidability index, indicating that it was the most useful to separate genuine and impostors distributions.
Nevertheless, it did not achieve the best results in terms of AUC and~EER.

The models pre-trained for face recognition generally achieve best results than those pre-trained on the ImageNet~dataset as stated in some previous works~\cite{Luz2018, boyd2019fine}.


\subsubsection{Open-World Protocol}
\label{sec:open}

The main idea of the open-world protocol is to evaluate the capability of the methods to extract discriminant features from samples of classes that are not present in the training stage.
Thus, for this protocol, we perform a benchmark only for the verification task.
The results are shown in Table~\ref{tab:benchopen} and Fig.~\ref{fig:roc_open}.


\begin{table*}[!ht]
\centering
\caption{Benchmark results in the open-world protocol for the verification task.}
\label{tab:benchopen}

\vspace{-2mm}
\resizebox{.6\linewidth}{!}{

\begin{tabular}{@{}lcccc@{}}
\toprule

\centering \multirow{2}{*}{Model} & \centering \multirow{2}{*}{Validation} & \multicolumn{3}{c}{Verification (1:1)} \\

\cmidrule{3-5}

                   &               & AUC (\%)            & EER (\%)        & Decidability       \\


\midrule
VGG16              & Closed-World  & $97.38\pm0.53$  & $8.52\pm0.92$   & $2.9599\pm0.1572$ \\
VGG16-Face         & Closed-World  & $97.70\pm0.42$  & $7.78\pm0.75$   & $3.0327\pm0.1428$ \\
ResNet50           & Closed-World  & $98.60\pm0.28$  & $5.98\pm0.67$   & $3.3702\pm0.1413$ \\
ResNet50V2         & Closed-World  & $98.73\pm0.28$  & $5.69\pm0.64$   & $3.4312\pm0.1459$ \\
Xception           & Closed-World  & $98.93\pm0.16$  & $5.23\pm0.42$   & $3.3493\pm0.0712$ \\
InceptionResNet    & Closed-World  & $99.10\pm0.24$  & $4.61\pm0.65$   & $3.4982\pm0.1208$ \\
ResNet50-Face      & Closed-World  & $99.18\pm0.16$  & $4.38\pm0.47$   & $3.8319\pm0.1239$ \\
DenseNet121        & Closed-World  & $99.51\pm0.12$  & $3.39\pm0.46$   & $3.8646\pm0.1215$ \\
MobileNet          & Closed-World  & $99.56\pm0.08$  & $3.17\pm0.33$   & $3.9868\pm0.1067$ \\
\midrule
\textbf{Multi-task }& \textbf{Closed-World}  & $\mathbf{99.67\pm0.08}$  & $\mathbf{2.81\pm0.39}$   & $3.9263\pm0.0921$ \\
Visob 2.0 Winner~\cite{zanlorensi2020deep, nguyen2021visob}& -  & $99.65\pm0.09$  & $2.96\pm0.26$  & $4.3666\pm0.1453$ \\
\midrule
Siamese            & Closed-World  & $97.27\pm0.64$  & $8.10\pm1.01$   & $2.6678\pm0.2433$ \\
Pairwise           & Closed-World  & $98.62\pm0.72$  & $5.77\pm1.57$   & $\mathbf{4.4404\pm0.5834}$ \\
\midrule
Siamese            & Open-World    & $96.85\pm0.70$  & $8.87\pm1.14$   & $2.6218\pm0.1514$ \\
Pairwise           & Open-World    & $97.80\pm2.03$  & $7.11\pm3.66$   & $4.1977\pm1.0663$ \\
\bottomrule
\end{tabular}}
\end{table*}


\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/openRoc300.pdf}

\vspace{-3mm}
\caption{\gls*{roc} curve to compare methods in the open-world protocol.}
\label{fig:roc_open}
\end{figure}

As in the closed-world protocol, the Multi-task model achieved the best results in Rank~$1$, Rank~$5$, AUC, and EER, and the Pairwise network achieved the best Decidability index.
The Siamese and Pairwise Filters networks trained using the closed-world validation split reached better results than when trained using the open-world validation split.
We believe this occurred due to the fact that there are fewer classes in the training set in the open-world validation split than in the closed-world validation split.
Although the open-world validation split corresponds to a more realistic scenario regarding the test set, the networks trained with samples from a larger number of classes can reach a higher capability of generalization, producing discriminative representations even for samples from classes not present in the training stage.

\subsubsection{Multi-Task Learning}
\label{sec:multi}

The Multi-task model reached the best results in the closed- and open-world protocols.
As this network simultaneously learns different tasks, we perform an ablation study by running some experiments with $4$ new models created by removing one of the tasks at a time.
The experiments were carried out in the closed-world protocol evaluating the performance for identification and verification.
We also evaluated the results achieved by all models in each~task.

\begin{table*}[!ht]
\centering
\caption{Results (\%) from several Multi-task models trained to predict different tasks. The device model concerns the task of identifying the smartphone model with which the image was taken. The age, gender, and eye side regard the tasks of classifying the input image into age ranges, gender (male or female), and eye side (left or right), respectively.}
\label{tab:multitask}

\vspace{-2mm}
\resizebox{.9\linewidth}{!}{

\begin{tabular}{@{}lccccccc@{}}
\toprule

Model                   & Rank 1     & Rank 5      & Device Model  & Age             & Gender          & Eye Side        \\


\midrule
Multi-task (no model)   & $80.76\pm0.94$  & $91.96\pm0.51$  & $-$               & $82.14\pm0.83$  & $97.72\pm0.17$  & $\mathbf{99.99\pm0.01}$  \\
Multi-task (no age)     & $81.93\pm0.99$  & $93.51\pm0.69$  & $87.20\pm0.63$    & $-$             & $97.65\pm0.20$  & $\mathbf{99.99\pm0.01}$  \\
Multi-task (no gender)  & $82.48\pm0.64$  & $93.55\pm0.52$  & $86.71\pm0.54$    & $83.17\pm0.54$  & $-$             & $\mathbf{99.99\pm0.01}$  \\
Multi-task (no side)    & $83.72\pm0.61$  & $94.07\pm0.54$  & $87.22\pm0.79$    & $83.75\pm0.53$  & $97.70\pm0.20$  & $-$             \\
\textbf{Multi-task}     & $\mathbf{84.32\pm0.71}$  & $\mathbf{94.55\pm0.58}$  & $\mathbf{87.42\pm0.65}$    & $\mathbf{84.34\pm0.71}$  & $\mathbf{97.80\pm0.21}$  & $99.98\pm0.02$  \\



\bottomrule
\end{tabular}}
\end{table*}

According to Table~\ref{tab:multitask}, the Multi-task network without the prediction of the mobile device model was the most penalized for the identification task, followed by the network variations without age, gender, and eye side estimation, respectively.
All models handled the gender and eye side classification tasks well, while the device model and age range classification tasks proved to be more challenging.
One problem in the device model and age range classification is the unbalanced number of samples per class.
Such bias probably contributed to the lower results being achieved in these two~tasks.

Note that we only employed the class prediction for the matching in both closed-world and open-world protocols.
However, as shown in Table~\ref{tab:multitask}, the multi-task architecture also achieved promising results in the other tasks.
In this sense, it may be possible to further improve the recognition results by adopting heuristic rules based on the scores of the other~tasks.

\subsubsection{Subjective evaluation}
\label{sec:subjective}

In this section, we perform a subjective evaluation through visual inspection on the pairs of images erroneously classified by the Multi-task model, which achieved the best result in the verification task in the closed-world protocol.
The best impostors (impostors classified as genuine) and the worst genuines (genuine classified as impostors) pairs are presented in Fig.~\ref{fig:pairserror}.

\begin{figure}[!tb]
\centering
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{cccc}

    \multicolumn{4}{c}{Wrong Genuines (Best Impostors)} \\
    
    \scriptsize $0.98$ & \scriptsize $0.96$ & \scriptsize $0.95$ & \scriptsize $0.95$\\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0022S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0023S3I13R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0486S3I13L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0487S3I13L.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0199S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0958S3I11R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0478S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1045S3I14R.jpg}}\\

    \scriptsize $0.95$ & \scriptsize$0.95$ & \scriptsize $0.94$ & \scriptsize $0.94$\\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0271S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0281S3I12R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0854S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1118S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0653S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0698S3I12R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0647S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1026S3I12R.jpg}}\\
    
     \scriptsize $0.94$ & \scriptsize $0.94$ & \scriptsize $0.93$ & \scriptsize$0.93$\\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0658S3I14L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0659S3I14L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0876S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1056S3I11R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0063S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0525S3I13R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0121S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0286S3I14R.jpg}}\\[1.75ex] 
    
    \multicolumn{4}{c}{Wrong Impostors (Worst Genuines)} \\
    
    \scriptsize $0.66$ & \scriptsize $0.68$ & \scriptsize $0.69$ & \scriptsize $0.69$\\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0981S3I11L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0981S3I12L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0110S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0110S3I15R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0964S3I11L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0964S3I15L.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0865S3I12L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0865S3I14L.jpg}}\\

    \scriptsize $0.69$ & \scriptsize $0.70$ & \scriptsize $0.70$ & \scriptsize $0.70$\\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0714S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0714S3I13R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0393S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0393S3I14R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0120S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0120S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0160S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0160S3I12R.jpg}}\\
    
    \scriptsize $0.70$ & \scriptsize $0.71$ & \scriptsize $0.72$ & \scriptsize $0.73$\\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0998S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0998S3I15R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0526S3I12L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0526S3I15L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0521S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0521S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0694S3I13L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0694S3I14L.jpg}}\\

\end{tabular}
\caption{Pairwise images wrongly classified by the model that obtained the best result in the verification task in the open-world protocol. Higher scores mean that the pair of periocular images is more likely to be~genuine.}
\label{fig:pairserror}
\end{figure}

Performing a visual analysis of all pairwise errors, it is clear that hair occlusion, age, eyeglasses, and eye shape were the most influential factors that led the model to the wrong classification of genuine pairs (intra-class comparison).
In pairs wrongly classified as impostors (inter-class comparison), we saw that lighting, blur, eyeglasses, off-angle, eye-gaze, reflection, and facial expression caused the main difference between the images.
We hypothesize that some errors caused by lightning, blur, reflection, and occlusion can be reduced by employing some data augmentation techniques in the training stage.
Attribute normalization~\cite{zanlorensi2020attnormalization} can also reduce the errors caused by attributes present in the periocular region such as eyeglasses, eye gaze, makeup, and some types of occlusion.
Although some methods can be applied to reduce the matching errors, there are still several characteristics in these images that make the mobile periocular recognition a challenging~task, mainly to the high intra-class variations.