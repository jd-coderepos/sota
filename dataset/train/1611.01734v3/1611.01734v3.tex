\documentclass[11pt,a4paper]{article}
\usepackage{iclr2017_conference,times}
\bibliographystyle{iclr2017_conference}
\usepackage{url}
\usepackage{amsmath}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{nn}
\usepackage{tikz-dependency}
\mathchardef\mhyphen="2D
\hyphenation{hyper-param-eter}
\hyphenation{Bi-LSTM}
\setlength{\headheight}{14.6pt}

\usepackage{url}

\title{Deep Biaffine Attention for Neural\\Dependency Parsing}
\author{
  Timothy Dozat\\
  Stanford University\\
  {\tt tdozat@stanford.edu} \\\And
  Christopher D.\ Manning\\
  Stanford University\\
  {\tt manning@stanford.edu}\\
}
\date{}

\iclrfinalcopy
\begin{document}
\maketitle
\begin{abstract}
  This paper builds off recent work from \citet{KiperwasserGoldberg2016} using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7\%{} UAS and 94.1\%{} LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming \citet{KiperwasserGoldberg2016} by 1.8\%{} and 2.2\%{}---and comparable to the highest performing transition-based parser \citep{KuncoroBallesterosetal2016}, which achieves 95.8\%{} UAS and 94.6\%{} LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.
\end{abstract}

\section{Introduction}
Dependency parsers---which annotate sentences in a way designed to be easy for humans and computers alike to understand---have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way \citep{Bowmanetal2016, Angelietal2015, LevyGoldberg2014, Toutanovaetal2016, ParikhPoonToutanova2015}. However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks.

The current state-of-the-art transition-based neural dependency parser \citep{KuncoroBallesterosetal2016} substantially outperforms many much simpler neural graph-based parsers. We modify the neural graph-based approach first proposed by \citet{KiperwasserGoldberg2016} in a few ways to achieve competitive performance: we build a network that's larger but uses more regularization; we replace the traditional MLP-based attention mechanism and affine label classifier with biaffine ones; and rather than using the top recurrent states of the LSTM in the biaffine transformations, we first put them through MLP operations that reduce their dimensionality. Furthermore, we compare models trained with different architectures and hyperparameters to motivate our approach empirically. The resulting parser maintains most of the simplicity of neural graph-based approaches while approaching the performance of the SOTA transition-based one.

\section{Background and Related Work}
Transition-based parsers---such as shift-reduce parsers---parse sentences from left to right, maintaining a ``buffer'' of words that have not yet been parsed and a ``stack'' of words whose head has not been seen or whose dependents have not all been fully parsed. At each step, transition-based parsers can access and manipulate the stack and buffer and assign arcs from one word to another. One can then train any multi-class machine learning classifier on features extracted from the stack, buffer, and previous arc actions in order to predict the next action.

\begin{figure}
\begin{center}
\begin{dependency}[theme=default]
  \begin{deptext}
    \texttt{root}/ROOT \& Casey/NNP \& hugged/VBD \& Kim/NNP \\
  \end{deptext}
  \depedge{1}{3}{root}
  \depedge{3}{2}{nsubj}
  \depedge{3}{4}{dobj}
\end{dependency}
\end{center}
\caption{A dependency tree parse for \emph{Casey hugged Kim}, including part-of-speech tags and a special \texttt{root} token. Directed edges (or arcs) with labels (or relations) connect the verb to the root and the arguments to the verb head.}
\label{dep}
\end{figure}
 
\citet{ChenManning2014} make the first successful attempt at incorporating deep learning into a transition-based dependency parser. At each step, the (feedforward) network assigns a probability to each action the parser can take based on word, tag, and label embeddings from certain words on the stack and buffer. A number of other researchers have attempted to address some limitations of \citeauthor{ChenManning2014}'s \citeauthor{ChenManning2014} parser by augmenting it with additional complexity: \citet{Weissetal2015} and \citet{Andoretal2016} augment it with a beam search and a conditional random field loss objective to allow the parser to ``undo'' previous actions once it finds evidence that they may have been incorrect; and \citet{Dyeretal2015} and \citep{KuncoroBallesterosetal2016} instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together.

Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don't use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges.  \citet{KiperwasserGoldberg2016} present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as \citet{BahdanauChoBengio2014} for machine translation. In \citeauthor{KiperwasserGoldberg2016}'s \citeyear{KiperwasserGoldberg2016} model, the (bidirectional) LSTM's recurrent output vector for each word is concatenated with each possible head's recurrent vector, and the result is used as input to an MLP that scores each resulting arc. The predicted tree structure at training time is the one where each word depends on its highest-scoring head. Labels are generated analogously, with each word's recurrent output vector and its gold or predicted head word's recurrent vector being used in a multi-class MLP.

Similarly, \citet{Hashimotoetal2016} include a graph-based dependency parser in their multi-task neural model. In addition to training the model with multiple distinct objectives, they replace the traditional MLP-based attention mechanism that \citet{KiperwasserGoldberg2016} use with a bilinear one (but still using an MLP label classifier). This makes it analogous to \citeauthor{Luongetal2015}'s \citeyear{Luongetal2015} proposed attention mechanism for neural machine translation. \citet{Chengetal2016} likewise propose a graph-based neural dependency parser, but in a way that attempts to circumvent the limitation of other neural graph-based parsers being unable to condition the scores of each possible arc on previous parsing decisions. In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word, they have additional, unidirectional recurrent networks (left-to-right and right-to-left) that keep track of the probabilities of each previous arc, and use these together to predict the scores for the next arc.

\section{Proposed Dependency Parser}\label{architecture}
\subsection{Deep biaffine attention}
\begin{figure*}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[every node/.style={anchor=center}]
  \bivec{r0}{}
  \bivec{r1}{right=.67cm of r0}
  \node[right=.67cm of r1] (dots) {\ldots};
  \bivec{rn}{right=.67cm of dots}
  \bivec{rnp1}{right=.67cm of rn}
  
  \wordvec{x1}{below=.33cm of r1}
  \wordvec{xn}{below=.33cm of rn}
  
  \mlpvec{m1-1}{above=.33cm of r1-1}
  \mlpvec{m1-2}{above=.33cm of r1-2}
  \mlpvec{mn-1}{above=.33cm of rn-1}
  \mlpvec{mn-2}{above=.33cm of rn-2}
  
  \node[below left=.33cm and -.67cm of x1-1] (w1) {\texttt{root}};
  \node[below right=.33cm and -.67cm of x1-2] (t1) {ROOT};
  \node[below left=.33cm and -.67cm of xn-1] (wn) {Kim};
  \node[below right=.33cm and -.67cm of xn-2] (tn) {NNP};
  
  \catstack{dep}{above=1.25cm of r1}
  \matstack{head}{above=1.25cm of rn}
  \tensor{ten}{above=1.75cm of dots}
  \node[right=0cm of head.north east] {};
  \bigstack{scores}{right=1.5cm of head}
  \path (dep) -- node{} (ten);
  \path (ten) -- node{} (head);
  \path (head) -- node{} (scores);
  
  \node[left=.5cm of r0] (lstm) {BiLSTM: };
  \node at (x1 -| lstm) {Embeddings: };
  \node at (m1-1 -| lstm) {MLP: };
  \node[above=.33cm of dep] (H) {};
  \node at (H -| ten) {};
  \node at (H -| head) {};
  \node at (H -| scores) {};
  
  \draw[<->, >=stealth] (r0) -- (r1);
  \draw[<->, >=stealth] (r1) -- (dots);
  \draw[<->, >=stealth] (dots) -- (rn);
  \draw[<->, >=stealth] (rn) -- (rnp1);
  
  \draw[->, >=stealth] (x1) -- (r1);
  \draw[->, >=stealth] (xn) -- (rn);
  
  \draw[->, >=stealth] (w1) -- (x1-1);
  \draw[->, >=stealth] (t1) -- (x1-2);
  \draw[->, >=stealth] (wn) -- (xn-1);
  \draw[->, >=stealth] (tn) -- (xn-2);
  
  \draw[->, >=stealth] (r1) -- (m1-1);
  \draw[->, >=stealth] (r1) -- (m1-2);
  \draw[->, >=stealth] (rn) -- (mn-1);
  \draw[->, >=stealth] (rn) -- (mn-2);
  
  \draw[->, >=stealth] (m1-1) -- (dep);
  \draw[->, >=stealth] (mn-1) -- (dep.south east);
  \draw[->, >=stealth] (m1-2) -- (head.south west);
  \draw[->, >=stealth] (mn-2) -- (head);
\end{tikzpicture}}
\end{center}
\caption{BiLSTM with deep biaffine attention to score each possible head for each dependent, applied to the sentence ``Casey hugged Kim''. We reverse the order of the biaffine transformation here for clarity.}
\label{proposed model}
\end{figure*}

We make a few modifications to the graph-based architectures of \citet{KiperwasserGoldberg2016}, \citet{Hashimotoetal2016}, and \citet{Chengetal2016}, shown in Figure \ref{proposed model}: we use biaffine attention instead of bilinear or traditional MLP-based attention; we use a biaffine dependency label classifier; and we apply dimension-reducing MLPs to each recurrent output vector  before applying the biaffine transformation.\footnote{In this paper we follow the convention of using lowercase italic letters for scalars and indices, lowercase bold letters for vectors, uppercase italic letters for matrices, uppercase bold letters for higher order tensors. We also maintain this notation when indexing; so row  of matrix  would be represented as .} The choice of biaffine rather than bilinear or MLP mechanisms makes the classifiers in our model analogous to traditional affine classifiers, which use an affine transformation over a single LSTM output state  (or other vector input) to predict the vector of scores  for all classes (\ref{tra-class}). We can think of the proposed biaffine attention mechanism as being a traditional affine classifier, but using a  linear transformation of the stacked LSTM output  in place of the weight matrix  and a  transformation  for the bias term  (\ref{biaff-att}). 

In addition to being arguably simpler than the MLP-based approach (involving one bilinear layer rather than two linear layers and a nonlinearity), this has the conceptual advantage of directly modeling both the prior probability of a word  receiving any dependents in the term  and the likelihood of  receiving a specific dependent  in the term . Analogously, we also use a biaffine classifier to predict dependency labels given the gold or predicted head  (\ref{biaff-class}).

This likewise directly models each of the prior probability of each class, the likelihood of a class given just word  (how probable a word is to take a particular label), the likelihood of a class given just the head word  (how probable a word is to take dependents with a particular label), and the likelihood of a class given both word  and its head (how probable a word is to take a particular label given that word's head). 
 
Applying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision. That is, every top recurrent state  will need to carry enough information to identify word 's head, find all its dependents, exclude all its non-dependents, assign itself the correct label, and assign all its dependents their correct labels, as well as transfer any relevant information to the recurrent states of words before and after it. Thus  necessarily contains significantly more information than is needed to compute any individual score, and training on this superfluous information needlessly reduces parsing speed and increases the risk of overfitting. Reducing dimensionality and applying a nonlinearity (\ref{arc-dep} - \ref{arc-score}) addresses both of these problems. We call this a \emph{deep} bilinear attention mechanism, as opposed to \emph{shallow} bilinear attention, which uses the recurrent states directly.


We apply MLPs to the recurrent states before using them in the label classifier as well. As with other graph-based models, the predicted tree at training time is the one where each word is a dependent of its highest scoring head (although at test time we ensure that the parse is a well-formed tree via the MST algorithm).

\subsection{Hyperparameter configuration}
\begin{table}[ht]
  \begin{center}\small
  \begin{tabular}{llll}
    \bf Param & \bf Value & \bf Param & \bf Value\\
    Embedding size & 100 & Embedding dropout & 33\%\\
    LSTM size & 400 & LSTM dropout & 33\%\\
    Arc MLP size & 500 & Arc MLP dropout & 33\%\\
    Label MLP size & 100 & Label MLP dropout & 33\%\\
    LSTM depth & 3 & MLP depth & 1\\
     &  & , & .9 \\
    Annealing &  &  & 50,000
  \end{tabular}
  \caption{Model hyperparameters}
  \label{Hyperparams}
  \end{center}
\end{table}

Aside from architectural differences between ours and the other graph-based parsers, we make a number of hyperparameter choices that allow us to outperform theirs, laid out in Table \ref{Hyperparams}. We use 100-dimensional uncased word vectors\footnote{We compute a ``trained'' embedding matrix composed of words that occur at least twice in the training dataset and add these embeddings to their corresponding pretrained embeddings. Any words that don't occur in either embedding matrix are replaced with a separate OOV token.} and POS tag vectors; three BiLSTM layers (400 dimensions in each direction); and 500- and 100-dimensional ReLU MLP layers. We also apply dropout at every stage of the model: we drop words and tags (independently); we drop nodes in the LSTM layers (input and recurrent connections), applying the same dropout mask at every recurrent timestep (cf.\ the Bayesian dropout of \citet{GalGhaharmani2015}); and we drop nodes in the MLP layers and classifiers, likewise applying the same dropout mask at every timestep. We optimize the network with annealed Adam \citep{KingmaBa2014} for about 50,000 steps, rounded up to the nearest epoch.

\section{Experiments \&{} Results}
\subsection{Datasets}
We show test results for the proposed model on the English Penn Treebank, converted into Stanford Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter (PTB-SD 3.3.0 and PTB-SD 3.5.0); the Chinese Penn Treebank; and the CoNLL 09 shared task dataset,\footnote{We exclude the Japanese dataset from our evaluation because we do not have access to it.} following standard practices for each dataset. We omit punctuation from evaluation only for the PTB-SD and CTB. For the English PTB-SD datasets, we use POS tags generated from the Stanford POS tagger \citep{Toutanovaetal2003}; for the Chinese PTB dataset we use gold tags; and for the CoNLL 09 dataset we use the provided predicted tags. Our hyperparameter search was done with the PTB-SD 3.5.0 validation dataset in order to minimize overfitting to the more popular PTB-SD 3.3.0 benchmark, and in our hyperparameter analysis in the following section we report performance on the PTB-SD 3.5.0 test set, shown in Tables \ref{Dev results 1} and \ref{Dev results 2}.

\subsection{Hyperparameter choices}
\begin{table}
  \begin{center}\small
  \begin{tabular}{llll@{\hskip 2em}lllllll}
    \multicolumn{4}{c}{\bf Classifier} & \multicolumn{4}{c}{\bf Size}\\
    \bf Model & \bf UAS & \bf LAS & \bf Sents/sec  & \bf Model & \bf UAS & \bf LAS & \bf Sents/sec \\
    Deep & \bf 95.75 & \bf 94.22 & \bf 410.91      & 3 layers, 400d & 95.75 & 94.22 & 410.91       \\
    Shallow & 95.74 & 94.00* & 298.99              & 3 layers, 300d & 95.82 & \bf 94.24 & 460.01   \\
    Shallow, 50\%{} drop & 95.73 & 94.05* & 300.04 & 3 layers, 200d & 95.55* & 93.89* & 469.45     \\
    Shallow, 300d & 95.63* & 93.86* & 373.24       & 2 layers, 400d & 95.62* & 93.98* & \bf 497.99 \\
    MLP & 95.53* & 93.91* & 367.44                 & 4 layers, 400d & \bf 95.83 & 94.22 & 362.09      \1ex]
    \multirow{3}{*}{Transition} & \citet{Ballesterosetal2016} & 93.56 & 91.42  & 87.65 & 86.21\\
    &\citet{Andoretal2016} & 94.61 & 92.79 & -- & --\\
    &\citet{KuncoroBallesterosetal2016} &\bf 95.8 & \bf 94.6 & -- & --\1ex]
    \citeauthor{Andoretal2016} & 92.67 & 89.83 & 84.72 & 80.85 & 88.94 & 84.56\\
    Deep Biaffine & \bf 94.69 & \bf 92.02 & \bf 88.90 & \bf 85.38 & \bf 92.08 & \bf 87.38\1ex]
    \citeauthor{Andoretal2016} & 93.22 & 91.23 & 90.91 & 89.15 & 92.62 & 89.95\\
    Deep Biaffine & \bf 95.21 & \bf 93.20 & \bf 93.46 & \bf 91.44 & \bf 94.34 & \bf 91.65
  \end{tabular}
  \caption{Results on the CoNLL '09 shared task datasets}
  \end{center}
\end{table}

\subsection{Results}
Our model gets nearly the same UAS performance on PTB-SD 3.3.0 as the current SOTA model from \citet{KuncoroBallesterosetal2016} in spite of its substantially simpler architecture, and gets SOTA UAS performance on CTB 5.1\footnote{We'd like to thank Zhiyang Teng for finding a bug in the original code that affected the CTB 5.1 dataset} as well as SOTA performance on all CoNLL 09 languages. It is worth noting that the CoNLL 09 datasets contain many non-projective dependencies, which are difficult or impossible for transition-based---but not graph-based---parsers to predict. This may account for some of the large, consistent difference between our model and \citeauthor{Andoretal2016}'s \citeyear{Andoretal2016} transition-based model applied to these datasets.

Where our model appears to lag behind the SOTA model is in LAS, indicating one of a few possibilities. Firstly, it may be the result of inefficiencies or errors in the GloVe embeddings or POS tagger, in which case using alternative pretrained embeddings or a more accurate tagger might improve label classification. Secondly, the SOTA model is specifically designed to capture phrasal compositionality; so another possibility is that ours doesn't capture this compositionality as effectively, and that this results in a worse label score. Similarly, it may be the result of a more general limitation of graph-based parsers, which have access to less explicit syntactic information than transition-based parsers when making decisions. Addressing these latter two limitations would require a more innovative architecture than the relatively simple one used in current neural graph-based parsers.

\section{Conclusion}
In this paper we proposed using a modified version of bilinear attention in a neural dependency parser that increases parsing speed without hurting performance. We showed that our larger but more regularized network outperforms other neural graph-based parsers and gets comparable performance to the current SOTA transition-based parser. We also provided empirical motivation for the proposed architecture and configuration over similar ones in the existing literature. Future work will involve exploring ways of bridging the gap between labeled and unlabeled accuracy and augment the parser with a smarter way of handling out-of-vocabulary tokens for morphologically richer languages.

\bibliography{tdozat}
\end{document}