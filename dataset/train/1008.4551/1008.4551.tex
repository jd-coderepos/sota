


\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{fancyhdr}
\usepackage{subfigure}
\usepackage{mdwlist}

\newcommand{\Xbar}{{\overline X}}
\renewcommand{\labelenumi}{{\bf Step \arabic{enumi}}}






\makeatletter


\newcommand{\BlackBox}{\rule{2.6mm}{2.6mm}}
\newenvironment{proof}{\paragraph{Proof:}}{\hspace*{\fill}}
\newenvironment{argument}{\paragraph{Argument:}}{\hspace*{\fill}}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}{Algorithm}
\newtheorem{condition}{Condition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newcommand{\fillblackbox}{\hspace*{\fill}}
\newcommand{\fillbox}{\hspace*{\fill}}
\newcommand{\fig}[1]{Figure~\ref{#1}}
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}
\newcommand{\num}[1]{(\romannumeral#1)}
\def\noflash#1{\setbox0=\hbox{#1}\hbox to 1\wd0{\hfill}}

\newcommand{\trust}{}
\newcommand{\suspect}{}

\newlength{\oulen}





\newcommand{\comment}[1]{}
\newcommand{\printcomment}[1]{{\bf #1}}
\newcommand{\nocomment}[1]{}

\newcommand{\Iitemize}{\begin{itemize}
	{\setlength{\itemsep}{-6pt}}
}

\newcommand{\ls}[1]
   {\dimen0=\fontdimen6\the\font 
    \lineskip=#1\dimen0
    \advance\lineskip.5\fontdimen5\the\font
    \advance\lineskip-\dimen0
    \lineskiplimit=.9\lineskip
    \baselineskip=\lineskip
    \advance\baselineskip\dimen0
    \normallineskip\lineskip
    \normallineskiplimit\lineskiplimit
    \normalbaselineskip\baselineskip
    \ignorespaces
   }
\newcommand{\nospecial}{\renewcommand{\special}[1]{\noflash{##1}}}
\def\ifundefined#1{\expandafter\ifx\csname#1\endcsname\relax}
\ifundefined{newblock}{
 \def\newblock{\hskip .11em plus .33em minus .07em}}\fi

\newcommand{\printname}[1]{\begin{center}\fbox{Figure
is in {\bf #1}}\end{center}}


\newcommand{\psget}[1]{\epsffile{#1}}

\newcommand{\bibdir}{/afs/crhc.illinois.edu/user/nhv/bibliography}

\newcommand{\psmp}[3]{\hfill \epsfxsize = #2\textwidth
\begin{minipage}[t]{#2\textwidth}
\epsffile{#1} \end{minipage} \begin{minipage}[t]{#3
\textwidth} ~ \end{minipage}}

\newcommand{\pspairnull}[4]{
\begin{minipage}[t]{0.48\textwidth}
\begin{center}
\epsffile{#1}
~\\ #2
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{center}
\epsffile{#3}
~\\ #4
\end{center}
\end{minipage}}

\newcommand{\pspair}[4]{
\epsfxsize = 0.48\textwidth
\begin{minipage}[t]{0.48\textwidth}
\begin{center}
\epsffile{#1}
~\b) #4
\end{center}
\end{minipage}}

\newcommand{\pspaircd}[4]{
\epsfxsize = 0.48\textwidth
\begin{minipage}[t]{0.48\textwidth}
\begin{center}
\epsffile{#1}
~\d) #4
\end{center}
\end{minipage}}



\newcommand{\thesis}{/user/vaidya/tape_dir/thesis}

\newcommand{\eqref}[1]{Equation~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\figrefs}[1]{Figures~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}
\newcommand{\sectref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\figurespace}[2]{
\begin{figure}[tbhp]
\vspace*{0.5in}
\centerline{++ figure here ++++  ++++ }
\vspace{0.5in}
\caption{#1}
\label{#2}
\end{figure}
}


\newcommand{\mybox}[1]{\centerline{\framebox{\parbox[c]{5in}{\em #1}}}}



\setlength {\topmargin}{-0.6in}						\setlength {\textheight}{9.2in}                	\setlength {\oddsidemargin}{-10pt}              	\setlength {\evensidemargin}{-10pt}             	\setlength {\textwidth}{488pt}                 	\setlength {\parskip}{1pt plus 1pt minus 1pt}	\setlength {\parindent}{2em}			


\renewcommand{\psmp}[3]{
\centering
\includegraphics[width=#2\textwidth]{#1}
}





\begin{document}

\title{Deterministic Consensus Algorithm\\ with Linear Per-Bit Complexity  \footnote{\normalsize This research is supported
in part by Army Research Office grant
W-911-NF-0710287. Any opinions, findings, and conclusions or recommendations expressed here are those of the authors and do not
necessarily reflect the views of the funding agencies or the U.S. government.}}


\date{\today}
\author{Guanfeng Liang and Nitin Vaidya\\ \normalsize Department of Electrical and Computer Engineering, and\\ \normalsize Coordinated Science Laboratory\\ \normalsize University of Illinois at Urbana-Champaign\\ \normalsize gliang2@illinois.edu, nhv@illinois.edu\\~\\Technical Report}







\maketitle


\thispagestyle{empty}

\newpage

\setcounter{page}{1}

\section{Introduction}
In this report, building on the deterministic multi-valued one-to-many Byzantine agreement ({\bf broadcast}) algorithm in our recent technical report
 \cite{techreport_BA_complexity}, we introduce a deterministic multi-valued all-to-all Byzantine agreement algorithm ({\bf consensus}), with linear complexity per bit agreed upon. The discussion in this note is not self-contained, and relies heavily on the material in \cite{techreport_BA_complexity} -- please refer to
\cite{techreport_BA_complexity} for the necessary background.

Consider a synchronous fully connected network with  nodes, namely . At most  nodes can be faulty. Every node  is given an initial value of  bits. The goal of a {\em consensus} algorithm is to allow each node to decide (or agree) on a value consisting of  bits,
 while satisfying the following three requirements:

\begin{itemize}
\item Every fault-free node eventually decides on a value (termination);
\item The decided values of all fault-free nodes are equal (consistency);
\item If every fault-free node holds the same initial value , the decided value equals to  (validity).
\end{itemize}


Our algorithm achieves consensus on a long value of  bits deterministically. Similar to the one-to-many algorithm in \cite{techreport_BA_complexity}, the proposed all-to-all Byzantine agreement (or {\em consensus})
 algorithm  progresses in generations. In each generation,  bits are being agreed upon, with the total number of generations being .
For convenience, we assume  to be an integral multiple of .

\section{Consensus Algorithm}\label{sec:algorithm}
In the proposed {\em consensus}\, algorithm, we use the same technique of ``diagnosis graph'' to narrow down the locations of faulty nodes as in \cite{techreport_BA_complexity}. If a node  is accused by at least  other nodes,  must be faulty. Then it is isolated,
and does not perform the algorithm below. When a new node is isolated,
essentially  decreases by 1, and  also decreases by 1. For the
reduced network, the condition that  will continue to hold, if
it held previously. In the
following, we consider the reduced network with the reduced values of
, and assume that no node in the reduced network is accused by  nodes.
When we say ``network'', we mean the reduced network below.


The following steps are performed for the  bits of information of the current generation.
Let the  bits at node  be denoted as .
\begin{enumerate}

\item This step is performed by each node :
We use a  distance- code, wherein each codeword
consists of  symbols, each symbol of size  bits.
Such a code exists, provided that the symbol size is large enough.
Let us denote this code as .
With a symbol size of  bits, the -bit value at node 
can be viewed as  symbols. Encode  into a
codeword  from the code .
The -th symbol in the codeword is denoted as .
Send  to all other nodes that it trusts.
Thus, node  sends -th symbol of its codeword
to all nodes {\em that it trusts}.

{\bf Note for future reference:}
Since code  has distance ,
any punctured  code obtained from 
has distance , where . Let  denote the
punctured  code of distance  obtained by removing the
last  symbols of the original  code above.
By ``last''  symbols, we refer to symbols with index  through .

\item This step is performed by each node :
Denote by  the symbol received from node  in step 1.
If  trusts  and , then set ; else .
 is a ``match'' vector, and records whether 's information matches
with the symbols sent by the other nodes.

\item Each node  uses traditional Byzantine agreement (one-to-many) algorithm
to broadcast  to all the nodes. 

\item Now each node  has received  from each node .
Due to the use of BA in the previous step, all fault-free nodes
receive identical  vectors.
Each node  attempts to find a set  containing exactly  nodes that are
``collectively consistent''. That is, for every pair
of nodes , .


There are two cases:
\begin{itemize}
\item No such subset  exists: Note that if all fault-free nodes
(at least  of them exist) have identical initial value, then a set
 must exist. (Fault-free nodes always trust each other.)
Thus, if no such  exists, that implies that the
fault-free nodes do not have identical value. Thus, the fault-free
nodes can agree on a default value, and terminate the algorithm.

\item At least one such subset  exists:
In this case, all fault-free nodes identify one such set  using
a deterministic algorithm (thus, all nodes should identify the same ).
Since all fault-free nodes can compute  identically,
without loss of generality, suppose that  contains nodes
0 through . Thus, the nodes {\em not}\, in  are  through .
(In other words, the nodes are renumbered after  is computed.)
Thus, 

and define

Let the -symbol received vector at node  consisting of the
symbols received from the  nodes in  be called .\footnote{In vector ,
the symbols are arranged in increasing order of the identifiers of the nodes that sent them.}

Since  contains  nodes and there are at most  faulty nodes, at least  of these nodes
must be fault-free.
Consider two fault-free nodes  and  in .
By definition of  and , nodes  and 
find these vectors ``consistent'' with their own values
 and , respectively. In other words,
 and  are codewords in .

There are at least   fault-free nodes in , which must
have sent the same symbols to nodes  and  in step 1.
Thus, the -symbol vectors  and  must be identical
in at least  positions, and differ in at most  positions.

Given that (i)  is a distance  code, (ii) 
and  are both codewords in , and (iii) 
and  differ in at most  positions, it follows that
 and  must be identical. This, in turn, implies that
 and  must be identical as well.
This proves the following claim:

~

{\bf Claim 1:} All fault-free nodes in  have identical -bit
values. In other words, for all fault-free nodes , 
.

\end{itemize}

\item \label{step_y} Now consider a node .
Identify any node  in  such that  and  trust each other.
If no such  exists, that implies that  is accused by all
 nodes in , and therefore,  must be already identified
as faulty, and must have
been isolated previously.
Thus,  exists.


For each ,
node  transmits  symbols  through  to node
.\footnote{For complexity analysis presented later, note that there are  nodes in ,
each of which is sent  symbols each consisting of  bits.}
Each fault-free node  
forms a vector using the  symbols 
received in step 1, and the above  symbols received from node .
Suppose that the -symbol vector thus formed at fault-free node  is
denoted .

\paragraph{Failure detection rule:}
If  is {\em not}\, a valid codeword from the  code , then
node  detects a failure. This failure observation is distributed
to other nodes in the next step. (Justification for this failure detection
mechanism is presented below.)

\item All nodes in  broadcast (using a traditional BA algorithm)
a single bit notification announcing whether they detected a failure
in the above step.

\paragraph{Decision rule:}
If no failure detection is announced by anyone, then each fault-free node  in 
decides (agrees) on its own value , and each fault-free node  decides
on the value corresponding to the codeword .

If a failure is detected by anyone, then the failure is narrowed down using
a ``full broadcast'' procedure described in \cite{techreport_BA_complexity}, and agreement on the  bits is also achieved
as a part of this full broadcast. The diagnosis graph is updated, and we return
to step 1 for next set of  bits. 

\end{enumerate}

\paragraph{Justification for the failure detection and decision rules:}
Consider any fault-free node  and any fault-free node .
Now let us compare  with .
\begin{itemize}
\item Consider the first  symbols of these vectors (elements with
index 0 through ). Observe that, for fault-free node ,
, for , by definition of set .
Since at least  of the symbols with index  come from
fault-free nodes,  can differ from  (and ) in at most
 positions with index .
\item Consider the last  symbols of vectors  and .
Since  may be faulty and could have sent arbitrary  symbols to 
to  in step~\ref{step_y}, vectors  and  may differ
in all of these  positions.
\end{itemize}
Thus,  and  may differ in at most  positions. 
Now let us make two observations:
\begin{itemize}
\item
Observation 1:
By definition of ,  is a valid codeword from the
distance- code . 
Since  differs from valid codeword  only in  places,
it follows that: either (i) 
(and both are valid codewords), or (ii)  is not a valid codeword.

\item
{Observation 2:}
To derive this observation,
consider the case where all the nodes in  are fault-free.
Clearly, in this case, all these nodes must have same value (from
claim 1 above). Then  is identical for all ,
and thus the (fault-free) nodes in X send symbols consistent with
the common value in step 1 to the nodes in .
 () consists entirely of symbols sent to it
by nodes in . Thus, clearly,  will be equal to  for all
 when all nodes in  are fault-free.
It then follows that  is a codeword from the  code

when all nodes in  are fault-free.
\end{itemize}
The above two observations imply that: (i) if  is not
a codeword then all the nodes in  cannot be fault-free (that is,
at least one of these nodes must have behaved incorrectly), and
(ii) if  is a codeword, then the value corresponding to 
matches with the values at all the fault-free nodes in .  

Thus, when the failure detection rule above detects a failure,
a failure must have indeed occurred. Also, while Claim 1
shows that the fault-free nodes in  will agree with each other
using the above decision rule, observation 1 implies that
fault-free nodes in  will also agree with them.

\section{Complexity Analysis}\label{sec:complexity}
We have finished describing the proposed consensus algorithm above. Now let us study the communication complexity of this algorithm.

\begin{itemize}
\item In step 1, every node sends at most  symbols of  bits. So at most 

are transmitted. Notice that this value decreases when both  and  are reduces by the same amount. As a result, no more than  bits will be transmitted in step 1 when some nodes are isolated.

\item In step 3, every node broadcasts a ``match'' vector of   bits, using a traditional Byzantine agreement (one-to-many) algorithm. Let us denote  as the bit-complexity to broadcast 1 bit. So in step 3, at most

are transmitted.

\item If no  is found in step 4, the algorithm terminates and nothing is transmitted any more. So we only consider the case when  exists. As we have seen before, in step \ref{step_y}, every node in  receives  symbols of  bits, which results in  bits being transmitted. 
Additionally, in step 6, every node in  broadcasts a 1-bit notification, which requires  bits being transmitted. So if no failure is detected, at most

are transmitted in steps 5 and 6. Again, this value decreases when both  and  are reduced by the same amount. So if some nodes are isolated, fewer bits will be transmitted.

\item If a failure is detected in step 6, every node broadcasts all symbols it has sent and has received through steps 1 to \ref{step_y}. In step 1,  symbols are transmitted. In step \ref{step_y},  symbols are transmitted. So  symbols are being broadcast after a failure is detected, which results in 

being transmitted.  Again, this value decreases when both  and  are reduced by the same amount. So if some nodes are isolated, fewer bits will be transmitted.
\end{itemize}

Now we can compute an upper bound of the complexity of the proposed algorithm. Notice that  bits are being agreed on in every generation, so there are  generations in total\footnote{To simplify the presentation, we assume that  is an integer multiple of  here. For other values of , the analysis and results are still valid by applying the ceiling function  to the number of generations.}. Thus, excluding the broadcast after failures are detected, no more than

are transmitted. In addition, similar to our one-to-many algorithm, all faulty nodes will be identified after failures are detected in at most  generations. So the ``full broadcast'' will be performed at most  times throughout the whole execution of the algorithm. So the total number of bits transmitted in the ``full broadcast'' in all generations is at most 

An upper bound on the communication complexity of the proposed algorithm, denoted as  is then computed as

For a large enough value of , with a suitable choice of 

we have


Notice that deterministic broadcast algorithms of complexity  are known \cite{bit_optimal_89}, so we assume . Then the complexity of our algorithm for all  is upper bounded by


For a given network with size , the per-bit communication complexity of our algorithm is upper bounded by



 


\bibliographystyle{abbrv}
\bibliography{PaperList}

\end{document}
