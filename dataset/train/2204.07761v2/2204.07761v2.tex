\section{Experiments}
\label{sec:results}

We evaluate our approach for language-grounded pre-training with state-of-the-art alternatives for 3D semantic segmentation on our \OURS{} benchmark.
For our method and all baselines, we use the same 80M parameter sparse 3D U-Net backbone implemented with MinkowskiNet~\cite{minkowski}.

\begin{table}[!ht]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{l|ccc|c?ccc|c?ccc|c}
		\multicolumn{1}{l}{\textbf{}} & \multicolumn{4}{c}{\textbf{mIoU}} & \multicolumn{4}{c}{\textbf{Precision}} & \multicolumn{4}{c}{\textbf{Recall}} \\
		\multicolumn{1}{l}{\textbf{}} & Head  & Common & Tail  & All   & Head  & Common & Tail  & All   & Head  & Common & Tail  & All    \\ \hline \hline
		Scratch             & 48.29 & 19.08  & 7.86 & 25.02 & 68.81 & 66.29  & 39.88 & 58.32 & 60.45 & 25.50  & 15.06 & 33.67 \\ \hline
		Ins. samp.         & 48.46 & 18.97  & 9.22 & 25.49 & 70.04 &  62.98 & 49.41 & 60.81 & 59.64 & 24.66  &  19.25 & 34.52 \\ \hline
		C-Focal        &   48.10    &   20.28     &   9.38    &   25.86    &   68.10    &    65.64    &   47.43    &  60.39     &   60.08    &     26.28   &  19.14     &   35.48   \\ \hline
		SupCon~\cite{supcontrast}    & 48.55 & 19.17  & 10.34 & 26.02 & 69.52 & 65.42  & 40.62 & 58.52 & 60.27 & 26.28  & 19.14  & 35.23 \\ \hline
		CSC~\cite{scene_contrast}    & 49.43 & 19.52  & 10.28 & 26.41 & 70.00 & 67.75  & 40.78 & 59.51 & 61.01 & 25.75  & 17.62  & 34.79 \\ \hline  
		Ours (CLIP only)            & 50.39 & \textbf{22.84}  & 10.10 & 27.73 & 71.64 & \textbf{69.72}  & 44.47 & 61.94 & 62.20 & \textbf{29.37}  & 17.35 & 36.16\\ \hline
		\textbf{Ours}             & \textbf{51.51} & 22.68  & \textbf{12.41} & \textbf{28.87} & \textbf{72.72} & 66.69  & \textbf{58.30} & \textbf{65.90} & \textbf{62.50} & 29.09  & \textbf{26.61} & \textbf{39.40} \\ \hline 
		\end{tabular}}
	\caption{Comparison to state of the art on \OURS{}. Our language-grounded 3D feature learning enables improved performance across frequent and infrequently seen categories in comparison with pure data augmentation or loss balancing techniques as well as state-of-the-art 3D pre-training. 
	Our approach achieves over 5\% mIoU performance over training from scratch, more than double the performance improvement of CSC~\cite{scene_contrast}.}
	\label{tab:large_model_results}
\end{table}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/results/limited_annotation_results_figure.jpg}
\caption{
    3D semantic segmentation under varying amounts of limited annotations.
    Even when considering only a small number of annotated surface points for our supervised language-guided 3D pre-training, our approach improves notably over the state-of-the-art 3D pre-training of CSC~\cite{scene_contrast}. }
    \label{fig:limitedann}
\end{figure}

\smallskip
\noindent \textbf{Comparison to the state of the art.}
We compare with a state-of-the-art  pre-training approaches Contrastive Scene Contexts (CSC) \cite{scene_contrast} and Supervised Contrastive Learning (SupCon) \cite{supcontrast}, along with our instance-based data balancing and focal loss~\cite{focalloss} training in Table~\ref{tab:large_model_results}.
For CSC, we use the same pre-training experimental setup as proposed by the authors for our 3D backbone. 
For SupCon, we sample 5 positive and 5 negative candidates from the training scene for each source point and train it for 300 epochs with the same optimization parameters as our method.
Our instance sampling, as well as focal loss, individually help to improve performance, particularly for lesser-seen class categories.
Additionally, all pre-training approaches improve performance over training from scratch, while our language-grounded feature learning enables more effective semantic reasoning with consistent improvements over baselines and across common and rarely seen class categories.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/results/method_comparisons_v2.jpg}
\caption{Qualitative semantic segmentation results on ScanNet~\cite{scannet} scenes. 
    In comparison to training from scratch, class-balance focal loss, and the 3D pre-training of CSC~\cite{scene_contrast}, our language-grounded 3D feature learning enables more consistent and accurate semantic segmentation, even for challenging less frequently seen class categories (e.g., \textit{``dish rack"} in row 4, \textit{``telephone"} in the last row).}
    \label{fig:methods_comparison}
\end{figure}


\smallskip
\noindent \textbf{Limited annotation semantic segmentation.}
As data annotations remain expensive to acquire, we additionally evaluate our approach in comparison with state of the art in the limited annotation scenario of our \OURS{} Benchmark described in Sec.~\ref{sec:benchmark}.
Figure~\ref{fig:limitedann} shows performance over varying amounts of labeled annotation data available (5\%, 10\%, 50\%, 100\%).
Note that since our pre-training leverages text labels to guide pre-training, we only pre-train with the available annotations, whereas CSC is pre-trained with all geometric data available for the train scenes and fine-tuned with the limited annotation data.
Our approach enables more robust semantic segmentation on this challenging benchmark, consistently improving and recovering the performance of training from scratch with only 5\% of the annotations. Moreover, in the very low annotation regime, we see significant improvements on tail categories, with an increase of +8 mIoU from the state-of-the-art 3D pre-training of CSC with 5\% of annotations available.


\smallskip
\noindent \textbf{How much does a class-balanced focal loss help?}
We evaluate the effect of our class-balanced  focal loss~\cite{focalloss} variant (\emph{C-Focal}) in Table~\ref{tab:large_model_results}, which helps to improve performance over training from scratch with a standard cross entropy loss.
Additionally, we see a consistent improvement with a smaller 3D backbone model in Table 3 in supplementary material, particularly for tail categories. We note that the class-balanced focal loss improves notably over both the original focal loss formulation (both using $\gamma = 2$), as well as a class-balanced cross entropy.

\smallskip
\noindent \textbf{What is the impact of data balancing with instance sampling?}
We additionally evaluate the effect of applying data balancing by our instance sampling during training in Table~\ref{tab:large_model_results} (\emph{Ins. samp}) as well as for a smaller 3D backbone in supplemental Table 3. 
We find that this instance sampling consistently provides a small improvement in performance across common and rare class categories.

\smallskip
\noindent \textbf{What is the effect of our language-grounded pre-training?}
Table~\ref{tab:large_model_results} shows that our language-grounded pretraining to text-based CLIP~\cite{clip} embeddings without focal loss or instance sampling already improves over all baselines.
Our full approach with focal loss and instance sampling in addition to text-anchored pre-training enables consistent, effective improvement in comparison to alternative approaches.

\smallskip
\noindent \textbf{3D instance segmentation task.}
In addition to 3D semantic segmentation, we also analyze a 3D instance segmentation task in Table \ref{tab:instseg_results}, showing that our approach generalizes across multiple downstream tasks with consistent performance improvement.
We use the same pre-trained 3D backbones and fine-tune them for instance segmentation by predicting an offset vector for every scene point as a voting mechanism together with the semantic labels. These directional distance vectors are optimized during train time, while the clustering of the instances is calculated only at test time. For the task and clustering algorithm, we adopt the paradigms of  \cite{jiang2020pointgroup,scene_contrast} to our \OURS{} benchmark. For this task, we train our models with a batch size of $8$ for $300$ epochs and momentum SGD optimizer with the same parameters as in the semantic segmentation experiments, except for a smaller starting learning rate of $0.02$.
\begin{table}
\centering
\resizebox{0.5\textwidth}{!}{\begin{tabular}{lc?c?c}
  &
  \textbf{Precision} &
  \textbf{mIoU} &
  \textbf{mAP@0.5} \\ \hline \hline
\multicolumn{1}{p{2cm}|}{Scratch} &
  \multicolumn{1}{>{\centering\arraybackslash}p{2cm}?}{61.04} &
  \multicolumn{1}{>{\centering\arraybackslash}p{2cm}?}{25.37} &
  \multicolumn{1}{>{\centering\arraybackslash}p{2cm}|}{24.47} \\ \hline
\multicolumn{1}{l|}{CSC~\cite{scene_contrast}} &
  \multicolumn{1}{c?}{63.13} &
  \multicolumn{1}{c?}{25.92} &
  \multicolumn{1}{c|}{25.24} \\ \hline
\multicolumn{1}{l|}{CLIP only} &
  \multicolumn{1}{c?}{64.24} &
  \multicolumn{1}{c?}{27.58} &
  \multicolumn{1}{c|}{\textbf{27.91}} \\ \hline \hline
\multicolumn{1}{l|}{Ours} &
  \multicolumn{1}{c?}{\textbf{65.32}} &
  \multicolumn{1}{c?}{\textbf{27.72}} &
  \multicolumn{1}{c|}{26.09} \\ \hline
\end{tabular}}
\caption{3D instance segmentation, in comparison with training from scratch and state-of-the-art 3D pre-training approach CSC~\cite{scene_contrast}. Our language-grounded pre-training improves over both baselines.}
\label{tab:instseg_results}
\end{table}

\noindent \textbf{Learned feature representation space.}
We analyze the pre-trained representation spaces by visualizing a t-SNE projection of the learned features in Figure~\ref{fig:representation_spaces}.
By anchoring 3D feature learning to a richly-structured text embedding space, we can learn a more structured 3D feature representation space. 

\begin{figure}[!ht]
    \centering
\begin{subfigure}[c]{.24\linewidth}
        \includegraphics[width=\linewidth]{figures/features/CSC-features.png}
        \caption{CSC~\cite{scene_contrast}}
    \end{subfigure}
\begin{subfigure}[c]{.24\linewidth}
        \includegraphics[width=\linewidth]{figures/features/supcon_feats.png}
        \caption{SupCon~\cite{supcontrast}}
    \end{subfigure}
\begin{subfigure}[c]{.24\linewidth}
        \includegraphics[width=\linewidth]{figures/features/pos_only.png}
        \caption{Ours (only pos.)}
    \end{subfigure}
\begin{subfigure}[c]{.24\linewidth}
        \includegraphics[width=\linewidth]{figures/features/uniform_negative.png}
        \caption{Ours}
    \end{subfigure}
\caption{We show a comparison with the representation learned by CSC \cite{scene_contrast}, SupCon~\cite{supcontrast}, as well as our approach when training with only positive samples. Our full language-grounded pre-training results in a more structured feature representation space with improved semantic segmentation performance.}
    \label{fig:representation_spaces}
\end{figure}

\paragraph{Limitations and Future Work.}
We believe our language-grounded 3D feature learning provides a promising step towards more robust and general 3D scene understanding, though several important limitations remain.
It is often the case that infrequently observed objects are small and their geometric resolution is limited, so while tail category performance has improved using only geometric input, there is still much room for improvement.
In particular, we note that color image observations could provide significantly higher resolution signals to explore for more accurate tail category recognition.
Additionally, text encodings are used to anchor learned 3D feature representations, but currently, only the semantic labels of each object are considered, whereas text caption or object attribute descriptions could potentially provide a richer signal.
