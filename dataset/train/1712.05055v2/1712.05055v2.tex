

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 


\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{url}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{amsthm}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem*{proof*}{Proof}
\newtheorem{remark}{Remark}


\usepackage{balance}



\usepackage[accepted]{icml2018}


\newcommand{\lu}[1]{\textcolor{red}{[lu:#1]}}
\newcommand{\supp}[1]{\textbf{\textcolor{blue}{#1}}}


\SetAlgoCaptionSeparator{.}
\renewcommand\AlCapFnt{\small}


\newcommand{\eg}{\emph{e.g.}} \newcommand{\Eg}{\emph{E.g}}
\newcommand{\ie}{\emph{i.e.}} \newcommand{\Ie}{\emph{I.e}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\etal}{\emph{et al.}}


\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}


\icmltitlerunning{MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{MentorNet: Learning Data-Driven Curriculum \\ for Very Deep Neural Networks
on Corrupted Labels}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lu Jiang}{goo}
\icmlauthor{Zhengyuan Zhou}{stan}
\icmlauthor{Thomas Leung}{goo}
\icmlauthor{Li-Jia Li}{goo}
\icmlauthor{Li Fei-Fei}{goo,stan}
\end{icmlauthorlist}

\icmlaffiliation{goo}{Google Inc., Mountain View, United States}
\icmlaffiliation{stan}{Stanford University, Stanford, United States}
\icmlcorrespondingauthor{Lu Jiang}{lujiang@google.com}

\icmlkeywords{Curriculum Learning, Regularization, Very Deep Neural Networks, Noisy Labels}
\vskip 0.3in
]



\printAffiliationsAndNotice{}  

\begin{abstract}
Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at {\small \url{https://github.com/google/mentornet}}.
\end{abstract}


\vspace{-5mm}
\section{Introduction}
Zhang \etal~\yrcite{zhang2017understanding} found that deep convolutional neural networks (CNNs) are capable of memorizing the entire data even with corrupted labels, where some or all true labels are replaced with random labels. It is a consensus that deeper CNNs usually lead to better performance. However, the ability of deep CNNs to overfit or memorize the corrupted labels can lead to very poor generalization performance~\cite{zhang2017understanding}. Recently, Neyshabur \etal~\yrcite{neyshabur2017exploring} and Arpit \etal~\yrcite{arpit2017closer} proposed deep learning generalization theories to explain this interesting phenomenon.

This paper studies how to overcome the corrupted label for deep CNNs, so as to improve generalization performance on the clean test data. Although learning models on weakly labeled data might not be novel, improving deep CNNs on corrupted labels is clearly an under-studied problem and worthy of exploration, as deep CNNs are more prone to overfitting and memorizing corrupted labels~\cite{zhang2017understanding}. To address this issue, we focus on training very deep CNNs from scratch, such as resnet-101~\cite{he2016deep} or inception-resnet~\cite{szegedy2017inception} which has a few hundred layers and orders-of-magnitude more parameters than the number of training samples. These networks can achieve the state-of-the-art result but perform poorly when trained on corrupted labels.

Inspired by the recent success of Curriculum Learning (CL), this paper tackles this problem using CL~\cite{bengio2009curriculum}, a learning paradigm inspired by the cognitive process of human and animals, in which a model is learned gradually using samples ordered in a meaningful sequence. A curriculum specifies a scheme under which training samples will be gradually learned. CL has successfully improved the performance on a variety of problems. In our problem, our intuition is that a curriculum, similar to its role in education, may provide meaningful supervision to help a student overcome corrupted labels. A reasonable curriculum can help the student focus on the samples whose labels have a high chance of being correct.

However, for the deep CNNs, we need to address two limitations of the existing CL methodology. First, existing curriculums are usually predefined and remain fixed during training, ignoring the feedback from the student. The learning procedure of deep CNNs is quite complicated, and may not be accurately modeled by the predefined curriculum. Second, the alternating minimization, commonly used in CL and self-paced learning~\cite{kumar2010self} requires alternative variable updates, which is difficult for training very deep CNNs via mini-batch stochastic gradient descent.

To this end, we propose a method to learn the curriculum from data by a network called \emph{MentorNet}. MentorNet learns a data-driven curriculum to supervise the base deep CNN, namely \emph{StudentNet}. MentorNet can be learned to approximate an existing predefined curriculum or discover new data-driven curriculums from data. The learned data-driven curriculum can be updated a few times taking into account of the StudentNet's feedback. Whenever MentorNet is learned or updated, we fix its parameter and use it together with StudentNet to minimize the learning objective, where MentorNet controls the timing and attention to learn each sample. At the test time, StudentNet makes predictions alone without MentorNet.

The proposed method improves existing curriculum learning in two aspects. First, our curriculum is learned from data rather than predefined by human experts. It takes into account of the feedback from StudentNet and can be dynamically adjusted during training. Intuitively, this resembles a ``collaborative'' learning paradigm, where the curriculum is determined by the teacher and student together. Second, in our algorithm, the learning objective is jointly minimized using MentorNet and StudentNet via mini-batch stochastic gradient descent. Therefore, the algorithm can be conveniently parallelized to train deep CNNs on big data. We show the convergence and empirically verify it on large-scale benchmarks.

We verify our method on four benchmarks. Results show that it can significantly improve the performance of deep CNNs trained on both controlled and real-world corrupted training data. Notably, to the best of our knowledge, it achieves the best-published result on WebVision~\cite{li2017webvision}, a large benchmark containing 2.2 million images of real-world noisy labels. To summarize, the contribution of this paper is threefold:

\begin{itemize}
\vspace{-3mm}
    \item We propose a novel method to learn data-driven curriculums for deep CNNs trained on corrupted labels.
\vspace{-5mm}
    \item We discuss an algorithm to perform curriculum learning for deep networks via mini-batch stochastic gradient descent.
\vspace{-2mm}
    \item We verify our method on 4 benchmarks and achieve the best-published result on the WebVision benchmark.
\vspace{-5mm}
\end{itemize}

\vspace{-5mm}
\section{Preliminary on Curriculum Learning}
We formulate our problem based on the model in~\cite{kumar2010self} and~\cite{jiang2015self}. Consider a classification problem with the training set , where  denotes the  observed sample and  is the noisy label vector over  classes. Let  denote the discriminative function of a neural network called \emph{StudentNet}, parameterized by . Further, let , a -dimensional column vector, denote the loss over  classes. Introduce the latent weight variable, , and optimize the objective:
-1.0ex]
\frac{1}{n}\sum_{i=1}^n \mathbf{v}_i^T \mathbf{L}(\mathbf{y}_i,\!g_s(\mathbf{x}_i,\!\mathbf{w})) + G(\mathbf{v}; \lambda) + \theta \|\mathbf{w}\|_2^2

\label{eq:selfpaced_sol_v}
v_i^* = \mathds{1}(\ell_i \le \lambda), \forall i \in [1,n],
\vspace{-2mm}

\label{eq:gm_obj}
g_m(\mathbf{z}_i; \Theta^*) = \arg\min_{v_i \in [0,1]} \mathbb{F} (\mathbf{w}, \mathbf{v}), \forall i \in [1,n]
\vspace{-2mm}

\label{eq:mentornet_learning_obj_explicit}
\arg\min_{\Theta}  
\sum_{(\mathbf{x}_i,y_i) \in \mathcal{D}} g_m (\mathbf{z}_i;\Theta) \ell_i +  G(g_m (\mathbf{z}_i; \Theta); \lambda)
\vspace{-1mm}

\label{eq:predefined_curriculum}
G(\mathbf{v}; \lambda) = \sum_{i=1}^n  \frac{1}{2} \lambda_2 v_i^2  - (\lambda_1+\lambda_2) v_i,
\vspace{-1mm}

f(v_i) = v_i \ell_i + \frac{1}{2} \lambda_2 v_i^2 - (\lambda_1+\lambda_2) v_i
\vspace{-2mm}

\label{eq:predefined_closed_form}
g_m(\mathbf{z}_i; \Theta^*) =\begin{cases}
\mathds{1}(\ell_i \le \lambda_1)  &  \lambda_2 = 0 \\
\min(\max (0, 1- \frac{\ell_i - \lambda_1}{\lambda_2}), 1) & \lambda_2 \ne 0\\
\end{cases},

\label{eq:focal_loss_weight}
v_i^* = [1-\exp\{-\ell_i\}]^\gamma,
\vspace{-2mm}

\label{eq:v_shorthand}
v^*(\lambda, x) = \text{argmin}_{v \in [0,1]} \ \ v x + G(v, \lambda).
\vspace{-1mm}

\label{eq:latent_f_lambda}
F_\lambda(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n \int_0^{\ell_i}v^*(\lambda,x) d x,
\vspace{-3mm}

\label{eq:f_lambda_robust}
F_\lambda (\mathbf{w}) \!=\! \frac{1}{n}\sum_{i=1}^n\!\begin{cases}
\ell_i & \ell_i \le \lambda_1\\
(\lambda_2 + 2\lambda_1)/ 2 & \ell_i \ge \lambda_2 + \lambda_1 \\
\theta \ell_i \!-\! \ell_i^2 /(2 \lambda_2) \!-\! \frac{(\theta-1)^2 \lambda_2}{2} & \text{otherwise}\\
\end{cases}
\vspace{-1mm}

F(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n \rho(\ell_i),
\vspace{-3mm}

\mathbf{v}_{\Xi}^{t} \!=\! g_m (\phi(\Xi_t, \mathbf{w}^{t-1})) \!=\! \arg\min_{\mathbf{v}_{\Xi}} \mathbb{F} (\mathbf{w}^{t-1},\! \mathbf{v}^{t-1}),
\vspace{-1mm}

where  is the feature extraction function defined in Eq.~\eqref{eq:gm_obj}.  denotes the learned MentorNet discussed in Section~\ref{sec:learning_curriculum}.

As shown in Algorithm~\ref{alg:overall}, for , a stochastic gradient is computed (via a mini-batch) and applied (Step 12), where  is the learning rate. For the latent weight variables , gradient descent is only applied to a small subset thereof parameters corresponding only to the mini-batch (Step 9 or 11). The partial gradient update on weight parameters is performed when  is used (Step 9). Otherwise, we directly apply the weights computed by the learned MentorNet (Step 11). In both cases, the weights are computed on-the-fly within a mini-batch and thus do not need to be fixed. As a result, the algorithm can be conveniently parallelized across multiple machines.

{
\vspace{-2mm}
\begin{algorithm}[ht]
\footnotesize
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\LinesNumbered
\Input{Dataset , a predefined  or a learned }
\Output{The model parameter  of StudentNet.}
Initialize , \;

\While{Not Converged}{
	Fetch a mini-batch  uniformly at random\;
	
	For every  in  compute \;
	
	\If{update curriculum}{
	    , where  is learned in Sec.~\ref{sec:learning_curriculum}
	}
	
    \If{ is used}{
	    
	}\lElse{
	    
    }


	\;
	
    
}
\Return 
\caption{{\small SPADE for minimizing Eq.~\eqref{eq:datareg_obj}}}
\label{alg:overall}
\end{algorithm}
\vspace{-2mm}
}



The curriculum can change during training. MentorNet is updated a few times in Algorithm~\ref{alg:overall}. In Step 6, the MentorNet parameter  is updated to adapt to the most recent model parameters of StudentNet. In experiments, we update  twice after the learning rate is changed. Each time, a data-driven curriculum is learned from the data generated by the most recent  using the method discussed in Section~\ref{sec:learning_curriculum}. The update is consistent with existing curriculum learning methodology~\cite{bengio2009curriculum,kumar2010self} and the difference here is that for each update, the curriculum is learned rather than specified by human experts.

Under standard assumptions, Theorem 1 shows that the algorithm stabilizes and converges to a stationary point (convergence to global/local minima cannot be guaranteed unless in specially structured non-convex objectives~\cite{chen2018gradient,zhou2017stochastic,zhou2017mirror}). The proof is in Appendix B. The theorem is a characterization of stability of the model parameters . For the weight parameters , as it is restricted in a compact set, convergence to a stationary point is not always guaranteed. As the model parameters are more important, we only provide a detailed characterization of the model parameter.

\begin{theorem}
\label{theorem:1}
Let the objective  defined in Eq.~\eqref{eq:datareg_obj} be differentiable,  be Lipschitz continuous in  and  be Lipschitz continuous in .
Let  be iterates from Algorithm 1 and  . Then,
.
\vspace{-2mm}
\end{theorem}

For the manually designed curriculums, it may be unclear where or even whether such predefined curriculum would converge via mini-batch training. Theorem~\ref{theorem:1} shows that the learned curriculum can converge and produce a stable StudentNet model. The algorithm can be used to replace the alternating minimization method in related work. 


\vspace{-3mm}
\section{Experiments}
This section empirically verifies the proposed method on four benchmarks of controlled corrupted labels in Section~\ref{sec:exp_part2} and real-world noisy labels in Section~\ref{sec:exp_part3}. The code can be found at {\small \url{https://github.com/google/mentornet}}.


\vspace{-2mm}
\subsection{Experiments on controlled corrupted labels}\label{sec:exp_part2}
\vspace{-2mm}
This section validates MentorNet on the controlled corrupted label. We follow a common setting in~\cite{zhang2017understanding} to train deep CNNs, where the label of each image is independently changed to a uniform random class with probability , where  is noise fraction and is set to 0.2, 0.4 and 0.8. The labels of validation data remain clean for evaluation.


\noindent\textbf{Dataset and StudentNet}: We use the same benchmarks in~\cite{zhang2017understanding}: CIFAR-10, CIFAR-100 and ImageNet. CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} consist of 32  32 color images arranged in 10 and 100 classes. Both datasets contain 50,000 training and 10,000 validation images. ImageNet ILSVRC2012~\cite{deng2009imagenet} contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.

We employ 3 recent deep CNNs as our StudentNets: inception~\cite{szegedy2016rethinking}, resnet-101~\cite{he2016deep} with wide filters~\cite{zagoruyko2016wide} and inception-resnet v2~\cite{szegedy2017inception}. Table~\ref{tab:studentnet} shows their \#model parameters, training, and validation accuracy when we train them on the clean training data (noise). As shown, they achieve reasonable accuracy on each task.


\begin{table}[ht]
\vspace{-5mm}
\centering
\footnotesize
\caption{\label{tab:studentnet}StudentNet and their accuracies on the clean training data.}
\begin{tabular}{cl|ccc}
\hline
\multicolumn{1}{l}{Dataset} & Model             & \#para & {\footnotesize train acc} & {\footnotesize val acc} \\
\hline
\multirow{2}{*}{CIFAR10}    & {\footnotesize inception}     &1.7M         & 0.83         & 0.81        \\
                            & {\footnotesize resnet101}         & 84M       & 1.00         & 0.96        \\
\hline
\multirow{2}{*}{CIFAR100}   & {\footnotesize inception}     &1.7M              &0.64                &0.49               \\
                            & {\footnotesize resnet101}        & 84M   & 1.00         & 0.79        \\
\hline
\multirow{1}{*}{ImageNet}   & {\footnotesize inception\_resnet}  &59M             & 0.88         & 0.77        \\
\hline
\end{tabular}
\vspace{-3mm}
\end{table}




\begin{table*}[ht]
\vspace{-4mm}
\centering
\caption{Comparison of validation accuracy on CIFAR-10 and CIFAR-100 under different noise fractions.}
\label{tab:cifar_comparison}
\begin{tabular}{|l|ccc|ccc|ccc|ccc|}
\hline
                & \multicolumn{6}{c|}{Resnet-101 StudentNet}  & \multicolumn{6}{c|}{Inception StudentNet}\\
                & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{CIFAR-10} \\
Method & 0.2      & 0.4     & 0.8     & 0.2      & 0.4      & 0.8     & 0.2      & 0.4     & 0.8     & 0.2      & 0.4      & 0.8     \\
\hline
FullModel&0.60 & 0.45 & 0.08 &0.82 & 0.69 & 0.18 & 0.43 & 0.38 & 0.15 & 0.76 & 0.73 & 0.42\\
Forgetting&0.61&0.44&0.16&0.78&0.63&0.35&0.42&0.37&0.17&0.76&0.71&0.44\\
Self-paced& 0.70 & 0.55  &0.13 & 0.89&0.85&0.28 &0.44&0.38&0.14&\textbf{0.80}&0.74&0.33 \\     
Focal Loss&0.59&0.44&0.09&0.79&0.65&0.28&0.43&0.38&0.15&0.77&0.74&0.40 \\
Reed Soft&0.62&0.46&0.08&0.81&0.63&0.18&0.42&0.39&0.12&0.78&0.73&0.39\\
MentorNet PD &0.72&0.56&0.14&0.91&0.77&0.33&0.44&0.39&0.16&0.79&0.74&0.44\\
MentorNet DD &\textbf{0.73}& \textbf{0.68}  & \textbf{0.35} &  \textbf{0.92} & \textbf{0.89} &\textbf{0.49} &\textbf{0.46}& \textbf{0.41}&\textbf{0.20}&0.79&\textbf{0.76}&\textbf{0.46}\\
\hline
\end{tabular}
\vspace{-4mm}
\end{table*}


\noindent \textbf{Baselines}: MentorNet is compared against the following baselines: \textbf{FullMode} is the standard StudentNet trained using  weight decay, dropout~\cite{srivastava2014dropout} and data augmentation~\cite{krizhevsky2012imagenet}. The hyper-parameters are set to the best ones found on the clean training data. Unless specified otherwise, for a fair comparison, the StudentNet with the same hyperparameters is used in all baseline and our model. \textbf{Forgetting} was introduced in~\cite{arpit2017closer}, in which the dropout parameter is searched in the range of (0.2-0.9). \textbf{Self-paced}~\cite{kumar2010self} and \textbf{Focal Loss}~\cite{lin2017focal} represent well-known predefined curriculums in the literature. We implemented \textbf{Reed}~\yrcite{reed2014training} and \textbf{Goldberger}~\cite{goldberger2017training} as the recent weakly-supervised learning methods. The above baseline methods are a mixture of the curriculum learning and the recent methods dealing with corrupted labels.
 
\noindent \textbf{Our Model}: \textbf{MentorNet PD} is the network learned using our \emph{predefined} curriculum in Eq.~\eqref{eq:predefined_curriculum} using no additional clean labels. \textbf{MentorNet DD} is the learned \emph{data-driven} curriculum. It is trained on 5,000 images of true labels, randomly sampled from the CIFAR-10 training set. The same data are used to learn MentorNet DD on CIFAR-100. Note CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different number of classes. Therefore, it is fair to compare MentorNet DD with other methods using no true labels on CIFAR-100. Algorithm~\ref{alg:overall} is used to optimize the StudentNet. The decay factor in computing the loss moving average is set to 0.95. The loss percentile in the moving average is set by the cross-validation. As mentioned, a burn-in process is used in the first 20\% training epoch for both MentorNet DD and MentorNet PD. More details are discussed in Appendix E.

We first show the comparison to the baseline method on CIFAR-10 and CIFAR-100 in Table~\ref{tab:cifar_comparison}. On both datasets, each method is verified with two StudentNets (resnet-101 and inception) under the noise fraction of 0.2, 0.4, and 0.8. As we see on both datasets, MentorNet improves FullModel across different noise fractions, and the learned data-driven curriculum (MentorNet DD) achieves the best results. The improvement is more significant for the deeper CNN model resnet-101. For example, on the CIFAR-10 of 40\% noise, MentorNet DD (with resnet-101) yields an absolute 20\% gain over FullModel. After inspecting the result, we found that it may be because Mentor DD learns a more appropriate curriculum to give high weights to samples of correct labels. As a result, it helps the StudentNet focus on samples of correct labels. The results indicate that the learned MentorNet can improve the generalization performance of recent deep CNNs, and outperform the predefined curriculums (Self-paced and Focal Loss).

\begin{figure}[ht]
\vspace{-2mm}
\centering
\subfigure[{Test error on clean validation}]
{
\label{fig:exp_generation_a}
\includegraphics[width=0.7\linewidth, height=28mm]{figures/new_cifar100_acc.pdf}
\vspace{-5mm}
}
\subfigure[ {Training error on corrupted labels}]
{
\label{fig:exp_generation_b}
\includegraphics[width=0.7\linewidth, height=28mm]{figures/new_cifar100_loss.pdf}
\vspace{-5mm}
}
\vspace{-5mm}
\caption{\label{fig:exp_generation}Error of resnet trained on CIFAR-100 of 40\% noise.}
\vspace{-7mm}
\end{figure}

Fig.~\ref{fig:exp_generation} plots the training and test error on the clean validation data, under a representative setting: resnet-101 on CIFAR-100 of 40\% noise, where the -axis denotes the training iteration. The -axis is the validation error on the clean validation in Fig.~\ref{fig:exp_generation_a} and the mini-batch training error on corrupted labels in Fig.~\ref{fig:exp_generation_b}. For MentorNet, the training error is computed by . The figure shows two insights. First, the training error of MentorNet approaches zero. This empirically verifies the convergence of the model. Second, MentorNet can overcome the overfitting to the corrupted label. While the training error is decreasing, the test error does not increase in Fig.~\ref{fig:exp_generation_a}. It suggests that the learned curriculum is beneficial for StudentNet. The sharp change around the 20k iteration in Fig.~\ref{fig:exp_generation} is due to the learning rate change. Besides, our result is consistent with~\cite{zhang2017understanding} that deep CNNS is able to get 0 training error on the corrupted training data. Forgetting (the dashed curve) is the only one that does not converge within 30k steps. As indicated in~\cite{arpit2017closer}, it is because forgetting reduces the speed at which DNNs memorize. As suggested in~\cite{zhang2017mixup}, a \emph{not} converged model might yield a better result, \eg, stop the model at 20K in Fig.~\ref{fig:exp_generation}. However, as it is hard to predetermine the time for early stopping, our focus is comparing the converged model. 


Fig.~\ref{fig:visual_curriculum} illustrates the best learned data-driven curriculum in our experiments, where the -axis denotes the weights computed by ; the  and  axes denote the sample loss and the loss difference to the moving average, where  is the loss moving average. Two observations can be found in Fig.~\ref{fig:visual_curriculum}. First, the learned curriculum changes during the training of the StudentNet. Fig.~\ref{fig:visual_curriculum} (a) and (b) are MentorNet learned at different epochs. As shown, (a) assigns greater weights to samples of big loss more aggressively. Second, the learned curriculums in Fig.~\ref{fig:visual_curriculum} generally satisfy the condition in Proposition 1, \ie, the weight generally decreases with the loss. It suggests that joint learning of StudentNet and MentorNet optimizes an underlying robust objective.

\begin{figure}[ht]
\vspace{-1mm}
\includegraphics[width=1\linewidth]{figures/visual_curriculum.pdf}
\vspace{-6mm}
\caption{\label{fig:visual_curriculum} The data-driven curriculums learned by MentorNet with the resnet-101 at epoch 21 in (a) and 76 in (b).}
\vspace{-5mm}
\end{figure}

Table~\ref{tab:exp_compare_soa} compares to recent published results under the setting: CIFAR of 40\% noise fraction. We cite the number in~\cite{azadi2016auxiliary}, and implement other methods using the same resnet-101 StudentNet. The results show that our result is comparable and even better than the state-of-the-art.

\begin{table}[ht]
\vspace{-5mm}
\centering
\small
\caption{\label{tab:exp_compare_soa} Validation accuracy comparison to representative published results on CIFAR of 40\% noise.}
\vspace{1mm}
\begin{tabular}{ll|cc}
\hline
ID&Method & CIFAR-10 & CIFAR-100 \\
\hline
1&Reed Hard (Resnet)  &  0.62       & 0.47  \\
2&Reed Soft (Resnet)&  0.61       & 0.46  \\
3&Goldberger (Resnet) &  0.70       & 0.46  \\
4&Azadi (AlexNet)~\yrcite{azadi2016auxiliary} &0.75& -\\
5&\textbf{MentorNet (Resnet)} &\textbf{0.89}&\textbf{0.68}\\
\hline
\end{tabular}
\vspace{-3mm}
\end{table}


To verify MentorNet for large-scale training, we apply our method on the ImageNet ILSVRC12~\cite{deng2009imagenet} benchmark to improve the inception-resnet v2~\cite{szegedy2017inception} model. We train the model on the ImageNet of 40\% noise. Inspired by~\cite{zhang2017understanding}, we start with an inception-resnet (NoReg) with no regularization (NoReg) and add weight decay, dropout, and data augmentation to the model. Table~\ref{tab:exp_imagenet} shows the comparison. As shown, MentorNet improves the performance of both the inception-resnet without regularization (NoReg) and with full regularization (FullModel). It also outperforms the forgetting baseline (dropout keep probability = 0.2). The results suggest that MentorNet can improve deep CNNs on the large-scale training on corrupted labels.

\begin{table}[ht]
\vspace{-3mm}
\centering
\caption{\label{tab:exp_imagenet} Accuracy on the clean ImageNet ImageNet validation set. Models are trained on the ImageNet training data of 40\% noise.}
\small
\begin{tabular}{l|cc}
\hline
Method & P@1 & P@5 \\
\hline
NoReg &  0.538 & 0.770  \\
NoReg+WeDecay &  0.560 & 0.809  \\
NoReg+Dropout & 0.575&0.807\\
NoReg+DataAug&  0.522 & 0.772  \\
NoReg+MentorNet&0.590& 0.814\\
\hline
FullModel&  0.612 & 0.844  \\
Forgetting(FullModel) &0.628&0.845\\
\textbf{MentorNet(FullModel)} &\textbf{0.651}&\textbf{0.859}\\
\hline
\end{tabular}
\vspace{-5mm}
\end{table}



\vspace{-2mm}
\subsection{Experiments on real-world noisy labels}
\label{sec:exp_part3}
\vspace{-2mm}
To verify MentorNet on real-world noisy labels, we conduct experiments on the large WebVision benchmark~\cite{li2017webvision}. It contains 2.4 million images of real-world noisy labels, crawled from the web using the 1,000 concepts in ImageNet ILSVRC12. We download the resized images from the official website\footnote{https://www.vision.ee.ethz.ch/webvision/download.html}. The inception-resenet v2~\cite{szegedy2017inception} is used as our StudentNet, trained using a distributed asynchronized momentum optimizer on 50 GPUs. Since the dataset is very big, for quick experiments, we compare baseline methods using the Google image subset on the first 50 classes. We use Mini to denote this subset and Entire for the entire WebVision. All the models are evaluated on the clean ILSVRC12 and WebVision validation set.

Table~\ref{tab:exp_real_world_noise} lists the comparison result. As we see, the proposed MentorNet significantly improves baseline methods on real-world noisy labels. The method marked by the start indicates it uses a pre-trained ImageNet model to obtain additional 30k labels for 118 classes. Following the same protocol, MentorNet* is trained using the additional labels. The results show that our method outperforms the baseline methods on real-world noisy labels. To the best of our knowledge, it achieves the best-published result on the WebVision~\cite{li2017webvision} benchmark.


\begin{table}[ht]
\vspace{-5mm}
\caption{\label{tab:exp_real_world_noise}Validation accuracy on the ImageNet ILSVRC12 and WebVision validation set. The number outside (inside) the parentheses denotes top-1 (top-5) classification accuracy (\%). * marks the method trained using additional verification labels.}
\centering
\small
\begin{tabular}{cl|cc}
\hline
Dataset& Method & ILSVRC12 & WebVision\\
\hline
Entire& Li \etal~\yrcite{li2017webvision} &0.476 (0.704)&0.570 (0.779)\\
Entire& Forgetting &0.590 (0.808) & 0.666 (0.856)\\
Entire& Lee \etal~\yrcite{lee2017cleannet}* &0.602 (0.811) & 0.685 (0.865)\\
Entire& MentorNet &0.625 (0.830) & 0.708 (0.880)\\
Entire& \textbf{MentorNet*} &\textbf{0.642} (\textbf{0.848})& \textbf{0.726} (\textbf{0.889})\\
\hline
Mini&FullModel &0.585 (0.818)&-\\
Mini&Forgetting &0.562 (0.816)&-\\
Mini&Reed Soft&0.565 (0.827)&-\\
Mini&Self-paced &0.576 (0.822)& -\\
Mini& \textbf{MentorNet} &\textbf{0.638} (\textbf{0.858})&-\\
\hline
\end{tabular}
\vspace{-5mm}
\end{table}

\vspace{-1mm}
\section{Related Work}
\vspace{-1mm}
Curriculum learning (CL), proposed by Bengio \etal~\yrcite{bengio2009curriculum}, is a learning paradigm in which a model is learned by gradually including from easy to complex samples in training so as to increase the learning entropy~\cite{bengio2009curriculum}. From the human behavioral perspective, Khan \etal~\yrcite{khan2011humans} have shown that CL is consistent with the principle of human teaching. CL has been empirically verified in a variety of problems, such as computer vision~\cite{supancic2013self,chen2015webly}, natural language processing~\cite{turian2010word}, multitask learning~\cite{graves2017automated}. A common CL approach is to predefine a curriculum. For example, Kumar \etal~\yrcite{kumar2010self} proposed a curriculum called self-paced learning which favors training samples of smaller loss. After that, many predefined curriculums were proposed, \eg, in \cite{supancic2013self,jiang2014self,jiang2015self,sangineto2016self,chang2017active,ma2017self,ma2017convergence}. For example, Jiang \etal~\yrcite{jiang2014self} introduced a curriculum of using easy and diverse samples. Fan \etal~\yrcite{fan2017self} proposed to use predefined sample weighting schemes as an implicit way to define a curriculum. Previous work has shown that predefined curriculums are useful in overcoming noisy labels~\cite{chen2015webly,liang2016learning,lin2017active}. In parallel to CL, the sample weighting schemes were also studied in~\cite{lin2017active,wang2017robust,fan2018learning,dehghani2018fidelityweighted}. Compared to the existing work, our paper presents a new way of learning data-driven curriculums for deep networks trained on corrupted labels.




Our work is related to the weakly-supervised learning methods. Among recent contributions, Reed \etal~\yrcite{reed2014training} developed a robust loss to model ``prediction consistency''. Menon \etal~\yrcite{menon2015learning} used class-probability estimation to study the corruption process. Sukhbaatar \etal~\yrcite{sukhbaatar2014training} proposed a noise transformation to estimate the noise distribution. The transformation matrix needs to be periodically updated and is non-trivial to learn. To address the issue, Goldberger \etal~\yrcite{goldberger2017training} proposed to add an additional softmax layer end-to-end with the base model. Azadi \etal~\yrcite{azadi2016auxiliary} tackled this problem by a regularizer called AIR. This method was shown to be effective but it relied on additional clean labels to train the representation.
More recently, methods utilized additional labels for label cleaning~\cite{Veit2017}, knowledge distillation~\cite{Li2017ICCV} or semi-supervised learning~\cite{vahdat2017toward,dehghani2017avoiding}. Different from previous work, we focus on learning curriculum to train very deep CNNs on corrupted labels from scratch. In addition, clean labels are not always needed for our method. In Section~\ref{sec:exp_part2}, the MentorNet is learned on a small subset of CIFAR-10 and applied to CIFAR-100

\vspace{-2mm}
\section{Conclusions}
\vspace{-2mm}
In this paper, we presented a novel method for training deep CNNs on corrupted labels. Our work was built on curriculum learning and advanced the methodology by proposing to learn data-driven curriculum via a neural network called MentorNet. We proposed an algorithm for jointly optimizing deep CNNs with MentorNet on large-scale data. We conducted comprehensive experiments on datasets of controlled and real-world noise. Our empirical results showed that generalization performance of deep CNNs trained on corrupted labels can be effectively improved by the learned data-driven curriculum.


\section*{Acknowledgements}
The authors would like to thank anonymous reviewers for helpful comments and Deyu Meng, Sergey Ioffe, and Chong Wang for meaningful discussions and kind support.

\bibliography{example_paper}
\bibliographystyle{icml2018}

\balance
\end{document}
