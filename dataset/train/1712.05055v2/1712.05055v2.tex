

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 


\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{url}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{amsthm}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem*{proof*}{Proof}
\newtheorem{remark}{Remark}


\usepackage{balance}



\usepackage[accepted]{icml2018}


\newcommand{\lu}[1]{\textcolor{red}{[lu:#1]}}
\newcommand{\supp}[1]{\textbf{\textcolor{blue}{#1}}}


\SetAlgoCaptionSeparator{.}
\renewcommand\AlCapFnt{\small}


\newcommand{\eg}{\emph{e.g.}} \newcommand{\Eg}{\emph{E.g}}
\newcommand{\ie}{\emph{i.e.}} \newcommand{\Ie}{\emph{I.e}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\etal}{\emph{et al.}}


\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}


\icmltitlerunning{MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{MentorNet: Learning Data-Driven Curriculum \\ for Very Deep Neural Networks
on Corrupted Labels}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lu Jiang}{goo}
\icmlauthor{Zhengyuan Zhou}{stan}
\icmlauthor{Thomas Leung}{goo}
\icmlauthor{Li-Jia Li}{goo}
\icmlauthor{Li Fei-Fei}{goo,stan}
\end{icmlauthorlist}

\icmlaffiliation{goo}{Google Inc., Mountain View, United States}
\icmlaffiliation{stan}{Stanford University, Stanford, United States}
\icmlcorrespondingauthor{Lu Jiang}{lujiang@google.com}

\icmlkeywords{Curriculum Learning, Regularization, Very Deep Neural Networks, Noisy Labels}
\vskip 0.3in
]



\printAffiliationsAndNotice{}  

\begin{abstract}
Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at {\small \url{https://github.com/google/mentornet}}.
\end{abstract}


\vspace{-5mm}
\section{Introduction}
Zhang \etal~\yrcite{zhang2017understanding} found that deep convolutional neural networks (CNNs) are capable of memorizing the entire data even with corrupted labels, where some or all true labels are replaced with random labels. It is a consensus that deeper CNNs usually lead to better performance. However, the ability of deep CNNs to overfit or memorize the corrupted labels can lead to very poor generalization performance~\cite{zhang2017understanding}. Recently, Neyshabur \etal~\yrcite{neyshabur2017exploring} and Arpit \etal~\yrcite{arpit2017closer} proposed deep learning generalization theories to explain this interesting phenomenon.

This paper studies how to overcome the corrupted label for deep CNNs, so as to improve generalization performance on the clean test data. Although learning models on weakly labeled data might not be novel, improving deep CNNs on corrupted labels is clearly an under-studied problem and worthy of exploration, as deep CNNs are more prone to overfitting and memorizing corrupted labels~\cite{zhang2017understanding}. To address this issue, we focus on training very deep CNNs from scratch, such as resnet-101~\cite{he2016deep} or inception-resnet~\cite{szegedy2017inception} which has a few hundred layers and orders-of-magnitude more parameters than the number of training samples. These networks can achieve the state-of-the-art result but perform poorly when trained on corrupted labels.

Inspired by the recent success of Curriculum Learning (CL), this paper tackles this problem using CL~\cite{bengio2009curriculum}, a learning paradigm inspired by the cognitive process of human and animals, in which a model is learned gradually using samples ordered in a meaningful sequence. A curriculum specifies a scheme under which training samples will be gradually learned. CL has successfully improved the performance on a variety of problems. In our problem, our intuition is that a curriculum, similar to its role in education, may provide meaningful supervision to help a student overcome corrupted labels. A reasonable curriculum can help the student focus on the samples whose labels have a high chance of being correct.

However, for the deep CNNs, we need to address two limitations of the existing CL methodology. First, existing curriculums are usually predefined and remain fixed during training, ignoring the feedback from the student. The learning procedure of deep CNNs is quite complicated, and may not be accurately modeled by the predefined curriculum. Second, the alternating minimization, commonly used in CL and self-paced learning~\cite{kumar2010self} requires alternative variable updates, which is difficult for training very deep CNNs via mini-batch stochastic gradient descent.

To this end, we propose a method to learn the curriculum from data by a network called \emph{MentorNet}. MentorNet learns a data-driven curriculum to supervise the base deep CNN, namely \emph{StudentNet}. MentorNet can be learned to approximate an existing predefined curriculum or discover new data-driven curriculums from data. The learned data-driven curriculum can be updated a few times taking into account of the StudentNet's feedback. Whenever MentorNet is learned or updated, we fix its parameter and use it together with StudentNet to minimize the learning objective, where MentorNet controls the timing and attention to learn each sample. At the test time, StudentNet makes predictions alone without MentorNet.

The proposed method improves existing curriculum learning in two aspects. First, our curriculum is learned from data rather than predefined by human experts. It takes into account of the feedback from StudentNet and can be dynamically adjusted during training. Intuitively, this resembles a ``collaborative'' learning paradigm, where the curriculum is determined by the teacher and student together. Second, in our algorithm, the learning objective is jointly minimized using MentorNet and StudentNet via mini-batch stochastic gradient descent. Therefore, the algorithm can be conveniently parallelized to train deep CNNs on big data. We show the convergence and empirically verify it on large-scale benchmarks.

We verify our method on four benchmarks. Results show that it can significantly improve the performance of deep CNNs trained on both controlled and real-world corrupted training data. Notably, to the best of our knowledge, it achieves the best-published result on WebVision~\cite{li2017webvision}, a large benchmark containing 2.2 million images of real-world noisy labels. To summarize, the contribution of this paper is threefold:

\begin{itemize}
\vspace{-3mm}
    \item We propose a novel method to learn data-driven curriculums for deep CNNs trained on corrupted labels.
\vspace{-5mm}
    \item We discuss an algorithm to perform curriculum learning for deep networks via mini-batch stochastic gradient descent.
\vspace{-2mm}
    \item We verify our method on 4 benchmarks and achieve the best-published result on the WebVision benchmark.
\vspace{-5mm}
\end{itemize}

\vspace{-5mm}
\section{Preliminary on Curriculum Learning}
We formulate our problem based on the model in~\cite{kumar2010self} and~\cite{jiang2015self}. Consider a classification problem with the training set $\mathcal{D} = \{(\mathbf{x}_1,\mathbf{y}_1),\cdots, (\mathbf{x}_n,\mathbf{y}_n)\}$, where $\mathbf{x}_i$ denotes the $i^{th}$ observed sample and $\mathbf{y}_i \in \{0,1\}^m$ is the noisy label vector over $m$ classes. Let $g_{s}(\mathbf{x}_i,\mathbf{w})$ denote the discriminative function of a neural network called \emph{StudentNet}, parameterized by $\mathbf{w} \in \mathbb{R}^d$. Further, let $\mathbf{L}(\mathbf{y}_i,g_s(\mathbf{x}_i,\mathbf{w}))$, a $m$-dimensional column vector, denote the loss over $m$ classes. Introduce the latent weight variable, $\mathbf{v} \in \mathbb{R}^{n \times m}$, and optimize the objective:
\begin{align}
\label{eq:datareg_obj}
\min_{\mathbf{w}  \in \mathbb{R}^d ,\mathbf{v}\in [0,\!1]^{n \times m}}\!\!\mathbb{F}(\mathbf{w},\!\mathbf{v})
&= \nonumber\\[-1.0ex]
\frac{1}{n}\sum_{i=1}^n \mathbf{v}_i^T \mathbf{L}(\mathbf{y}_i,\!g_s(\mathbf{x}_i,\!\mathbf{w})) + G(\mathbf{v}; \lambda) + \theta \|\mathbf{w}\|_2^2
\end{align}
where $\|\cdot\|_2$ is the $l_2$ norm for weight decay, and data augmentation and dropout are subsumed inside $g_s$. $\mathbf{v}_i \in [0,1]^{m \times 1}$ is a vector to represent the latent weight variable for the $i$-th sample. The function $G$ defines a \emph{curriculum}, parameterized by $\lambda$. This paper focuses on the one-hot label. For notation convenience, denote the loss $\mathbf{L}(\mathbf{y}_i,\!g_s(\mathbf{x}_i,\!\mathbf{w}))\!=\!\ell_i$, $\mathbf{v}_i$ as a scalar $v_i$, and $\mathbf{y}_i$ as an integer $y_i \in [1,m]$. 

In the existing literature, alternating minimization~\cite{csiszar1984information}, or its related variants, is commonly employed to minimize the training objective, e.g. in~\cite{kumar2010self,ma2017self,jiang2014self}. This is an algorithmic paradigm where $\mathbf{w}$ and $\mathbf{v}$ are alternatively minimized, one at a time while the other is held fixed. When $\mathbf{v}$ is fixed, the weighted loss is typically minimized by stochastic gradient descent. When $\mathbf{w}$ is fixed, we compute $\mathbf{v}^{k} = \arg\min_{\mathbf{v}} \mathbb{F} (\mathbf{v}^{k-1}, \mathbf{w}^{k})$ using the most recently updated $\mathbf{w}^{k}$ at epoch $k$. For example, Kumar \etal~\yrcite{kumar2010self} employed $G(\mathbf{v}) = - \lambda \|\mathbf{v}\|_1$. When $\mathbf{w}$ is fixed, the optimal $\mathbf{v}$ can be easily derived by:
\vspace{-2mm}
\begin{equation}
\label{eq:selfpaced_sol_v}
v_i^* = \mathds{1}(\ell_i \le \lambda), \forall i \in [1,n],
\vspace{-2mm}
\end{equation}
where $\mathds{1}$ is the indicator function. Eq.~\eqref{eq:selfpaced_sol_v} intuitively explains the predefined curriculum in~\cite{kumar2010self}, known as self-paced learning. First, when updating $\mathbf{v}$ with a fixed $\mathbf{w}$, a sample of smaller loss than the threshold $\lambda$ is treated as an ``easy" sample, and will be selected in training ($v_i^*=1$). Otherwise, it will not be selected ($v_i^*=0$). Second, when updating $\mathbf{w}$ with a fixed $\mathbf{v}$, the classifier is trained only on the selected ``easy" samples. The hyperparameter $\lambda$ controls the learning pace and corresponds to the ``age'' of the model. When $\lambda$ is small, only samples of small loss will be considered. As $\lambda$ grows, more samples of larger loss will be gradually added to train a more ``mature'' model.

As shown, the function $G$ specifies a curriculum, \ie, a sequence of samples with their corresponding weights to be used in training. When $\mathbf{w}$ is fixed, its optimal solution, \eg~ Eq.~\eqref{eq:selfpaced_sol_v}, computes the time-varying weight that controls the timing and attention to learn every sample. Recent studies discovered multiple predefined curriculums and verified them in many real-world applications, \eg, in~\cite{fan2017self,ma2017self,sangineto2016self,fan2017self,chang2017active}.

This paper studies learning curriculum from data. In the rest of this paper, Section~\ref{sec:mentornet} presents an approach to learn data-driven curriculum by \emph{MentorNet}. Section~\ref{sec:algorithm} discusses an algorithm to optimize Eq.~\eqref{eq:datareg_obj} using MentorNet and StudentNet together via mini-batch training.






\vspace{-3mm}
\section{Learning Curriculum from Data}\label{sec:mentornet}
Existing curriculums are either predetermined as an analytic expression of $G$ or a function to compute sample weights. Such predefined curriculums cannot be adjusted accordingly, taking into account of the feedback from the student. This section discusses a new way to learn data-driven curriculum by a neural network, called \emph{MentorNet}. The MentorNet $g_m$ is learned to compute time-varying weights for each training sample. Let $\Theta$ denote the parameters in $g_m$. Given a fixed $\mathbf{w}$, our goal is to learn an $\Theta^*$ to compute the weight:
\vspace{-1mm}
\begin{equation}
\label{eq:gm_obj}
g_m(\mathbf{z}_i; \Theta^*) = \arg\min_{v_i \in [0,1]} \mathbb{F} (\mathbf{w}, \mathbf{v}), \forall i \in [1,n]
\vspace{-2mm}
\end{equation}
where $\mathbf{z}_i = \phi(\mathbf{x}_i, y_i, \mathbf{w})$ indicates the input feature to MentorNet about the $i$-th sample.

\subsection{Learning Curriculum}\label{sec:learning_curriculum}
MentorNet can be learned to 1) approximate existing curriculums or 2) discover new curriculums from data. 

\textbf{Learning to approximate predefined curriculums.} Our first task is to learn a MentorNet to approximate a predefined curriculum. To do so, we minimize the objective in Eq.~\eqref{eq:datareg_obj}:
\vspace{-2mm}
\begin{equation}
\label{eq:mentornet_learning_obj_explicit}
\arg\min_{\Theta}  
\sum_{(\mathbf{x}_i,y_i) \in \mathcal{D}} g_m (\mathbf{z}_i;\Theta) \ell_i +  G(g_m (\mathbf{z}_i; \Theta); \lambda)
\vspace{-1mm}
\end{equation}
Eq.~\eqref{eq:mentornet_learning_obj_explicit} applies for both convex and non-convex $G$. This paper employs the following predefined curriculum. It is derived from~\cite{jiang2015self} and works well in our experiments. As will be discussed later, it is also related to robust non-convex penalties. 
\vspace{-3mm}
\begin{equation}
\label{eq:predefined_curriculum}
G(\mathbf{v}; \lambda) = \sum_{i=1}^n  \frac{1}{2} \lambda_2 v_i^2  - (\lambda_1+\lambda_2) v_i,
\vspace{-1mm}
\end{equation}
where $\lambda_1, \lambda_2 \ge 0$ are hyper-parameters. As $G$ is convex, there exists a closed-form solution for the optimal value of Eq.~\eqref{eq:gm_obj}. Given a fixed $\mathbf{w}$, define $\mathbb{F}_{\mathbf{w}}(\mathbf{v}) = \sum_{i=1}^n f(v_i)$:
\vspace{-2mm}
\begin{equation}
f(v_i) = v_i \ell_i + \frac{1}{2} \lambda_2 v_i^2 - (\lambda_1+\lambda_2) v_i
\vspace{-2mm}
\end{equation}
The minima are obtained at $\nabla_{\mathbf{v}} \mathbb{F}_{\mathbf{w}} (\mathbf{v}) = 0$, and can be decoupled by setting $\partial f / \partial v_i = 0$. We then have:
\begin{equation}
\label{eq:predefined_closed_form}
g_m(\mathbf{z}_i; \Theta^*) =\begin{cases}
\mathds{1}(\ell_i \le \lambda_1)  &  \lambda_2 = 0 \\
\min(\max (0, 1- \frac{\ell_i - \lambda_1}{\lambda_2}), 1) & \lambda_2 \ne 0\\
\end{cases},
\end{equation}
where $\Theta^*$ is the optimal MentorNet parameter obtained by SGD. The closed-form solution in Eq.~\eqref{eq:predefined_closed_form} gives some intuitions about the curriculum. When $\lambda_2=0$, it is similar to self-paced learning~\cite{kumar2010self} \ie~only ``easy'' samples of $\ell_i < \lambda_1$ will be selected in training ($g_m(\mathbf{z}_i; \Theta^*)=1$). When $\lambda_2 \ne 0$, samples of loss $\ell_i \ge \lambda_2 + \lambda_1$ will not be selected in training. These samples represent the ``hard'' samples of greater loss. Otherwise, samples will be weighted linearly w.r.t. $1-(\ell_i-\lambda_1)/\lambda_2$. As in~\cite{kumar2010self}, the hyper-parameters $\lambda_1$ and $\lambda_2$ control the learning pace.

\textbf{Learning data-driven curriculums.} Our next task is to learn a curriculum solely derived from labeled data. To this end, $\Theta$ is learned on another dataset $\mathcal{D}' = \{(\phi(\mathbf{x}_i, y_i, \mathbf{w}), v_i^*)\}$, where $(\mathbf{x}_i, y_i)$ is sampled from $\mathcal{D}$ and $|\mathcal{D}'| \ll |\mathcal{D}|$. $v_i^*$ is a given annotation and we assume it approximates the optimal weight, \ie, $v_i^* \simeq \arg\min_{v_i \in [0,1]} \mathbb{F} (\mathbf{v}, \mathbf{w})$. In this paper, we assign binary labels to $v_i^*$, where $v_i^*=1$ iff $y_i$ is a correct label. As $v_i^*$ is binary, $\Theta$ is learned by minimizing the cross-entropy loss between $v_i^*$ and $g(\mathbf{z}_i; \Theta)$. Intuitively, this process is similar to a mock test for the teacher (MentorNet) to learn to update her teaching strategy (curriculum). The student (StudentNet) provides features $\phi(\cdot, \cdot, \mathbf{w})$ for the mock test using the latest model $\mathbf{w}$. The teacher can learn an updated curriculum from the data to better supervise the latest student model. The learned curriculum is jointly determined by the teacher and student together.

The information on the correct label may not always be available on the target dataset $\mathcal{D}$. In this case, we learn the curriculum on a different small dataset where the correct labels are available. Intuitively, it resembles first learning a teaching strategy with the student on one topic and transfer the strategy on a similar topic. Empirically, Section~\ref{sec:exp_part2} substantiates that the learned curriculum on a small subset of CIFAR-10 can be applied to the target CIFAR-100 dataset.

A burn-in period is introduced before learning $\Theta$. In the first 20\% training epoch of the StudentNet, MentorNet is initialized and fixed as $g_m(\mathbf{z}_i ;\Theta^*)=r_i$, where $r_i \sim \text{Bernoulli}(p)$ is the Bernoulli random variable. This is equivalent to randomly dropping out $p$\% training samples. We found that the burn-in process helps StudentNet stabilize the prediction and focus on learning simple and common patterns.

\textbf{MentorNet architecture.} We found that MentorNet can have a simple architecture. Appendix D shows that even MentorNet based on the two-layer perceptron can reasonably approximate the existing curriculum in the literature. Nevertheless, we use a MentorNet architecture shown in Fig.~\ref{fig:mentornet_arch}, which works reasonably well compared to classical network architectures. It takes the input of a mini-batch of samples, and outputs their corresponding sample weights. The feature $\mathbf{z}_i = \phi(\mathbf{x}_i, y_i, \mathbf{w})$ includes the loss, loss difference to the moving average, label and epoch percentage. $\ell_{pt}$ maintains an exponential moving average on the $p$-th percentile of the loss in each mini-batch. For a sample, its loss $\ell$ and loss difference $\ell-\ell_{pt}$ over the last few epochs can be encoded by a bidirectional LSTM network to capture the prediction variance~\cite{chang2017active}. We verify the LSTM encoder in the experiments in Appendix D. For simplicity, we set the step size of the LSTM to 1 in Section~\ref{sec:exp_part2} and only consider the loss and the loss difference of the current epoch.

\begin{figure}[ht]
\vspace{-3mm}
\centering
\includegraphics[width=0.9\linewidth]{figures/TrainMentorNet.pdf}
\vspace{-5mm}
\caption{\label{fig:mentornet_arch}The MentorNet architecture used in experiments. \textit{emb}, \textit{fc} and \textit{prob sampling} stand for the embedding, fully-connected and probabilistic sampling layer.}
\vspace{-5mm}
\end{figure}

The label and the training epoch percentage are encoded by two separate embedding layers. The epoch percentage is represented as an integer between 0 and 99. It is used to indicate the StudentNet's training progress, where 0 represents the first and 99 represents the last training epoch. The concatenated outputs from the LSTM and the embedding layers are fed into two fully-connected layers $fc_1, fc_2$, where $fc_2$ uses the sigmoid activation to ensure the output weights bounded between 0 and 1. The last layer in Fig.~\ref{fig:mentornet_arch} is a probabilistic sampling layer, and is used to implement the sample dropout in the burn-in process on the already learned MentorNet. 

\vspace{-2mm}
\subsection{Discussions}\label{sec:robust_learning}
MentorNet is a general framework for both predefined and data-driven curriculum learning, where various curriculums can be learned by the same MentorNet structure with different parameters. This framework is conceptually general and practically flexible as we can switch curriculums by attaching different MentorNets without modifying the pipeline. Therefore, we also learn MentorNets for predefined curriculums. For predefined curriculums where $G$ is unknown, we directly minimize the error between the MentorNet's outputs and desired weights. For example, the desired weight for focal loss~\cite{lin2017focal} is computed by: 
\begin{equation}
\label{eq:focal_loss_weight}
v_i^* = [1-\exp\{-\ell_i\}]^\gamma,
\vspace{-2mm}
\end{equation}
where $\gamma$ is a hyperparameter for smoothing the distribution.

This paper tackles the problem of overcoming corrupted labels. It is interesting to analyze why the learned curriculum can improve the generalization performance. It turns out that StudentNet, when jointly learned with MentorNet, may optimize an underlying robust objective and the objective is also related to the robust M-estimator~\cite{huber2011robust}.

To show this, let $v^*(\lambda,x)$ represent the optimal weight function for a loss variable $x$, and we define:
\begin{equation}
\label{eq:v_shorthand}
v^*(\lambda, x) = \text{argmin}_{v \in [0,1]} \ \ v x + G(v, \lambda).
\vspace{-1mm}
\end{equation}
As $g_m$ is an approximator to Eq.~\eqref{eq:v_shorthand}, its property can then be analyzed by the function $v^*(\lambda, x)$. Meng \etal \yrcite{meng2015objective} investigated the insights of self-paced objective function, and proved that the optimization of SPL algorithm is intrinsically equivalent to minimizing a robust loss function. They showed that given a fixed $\lambda$ and a decreasing $v^*(\lambda, x)$ with respect to $x$, the underlying objective of Eq.~\eqref{eq:datareg_obj} can be obtained by:
\begin{equation}
\label{eq:latent_f_lambda}
F_\lambda(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n \int_0^{\ell_i}v^*(\lambda,x) d x,
\vspace{-3mm}
\end{equation}

Based on it, the underlying learning objective of the curriculum in Eq.~\eqref{eq:predefined_curriculum} can then be derived.
\begin{remark}
When $\lambda_1, \lambda_2$ are fixed and $\lambda_2 \ne 0$, the underlying objective function of the curriculum in Eq.~\eqref{eq:predefined_curriculum} is calculated from:
\vspace{-1mm}
\begin{equation}
\label{eq:f_lambda_robust}
F_\lambda (\mathbf{w}) \!=\! \frac{1}{n}\sum_{i=1}^n\!\begin{cases}
\ell_i & \ell_i \le \lambda_1\\
(\lambda_2 + 2\lambda_1)/ 2 & \ell_i \ge \lambda_2 + \lambda_1 \\
\theta \ell_i \!-\! \ell_i^2 /(2 \lambda_2) \!-\! \frac{(\theta-1)^2 \lambda_2}{2} & \text{otherwise}\\
\end{cases}
\vspace{-1mm}
\end{equation}
where $\theta = (\lambda_2+\lambda_1)/\lambda_2$. When $\theta=1$ it is equivalent to the minimax concave penalty~\cite{zhang2010nearly}.
\vspace{-2mm}
\end{remark}

As shown in Eq.~\eqref{eq:f_lambda_robust}, the underlying objective has a form of $F_\lambda(\mathbf{w}) = \sum_i \rho(\ell_i)/n$, where $\rho$ is the penalty function in M-estimator~\cite{candes2008enhancing}. Particularly, when $\theta=1$, $\rho(\ell)$ is equivalent to the minimax concave plus penalty~\cite{zhang2010nearly}, a popular non-convex robust loss. The result indicates the learned MentorNet that approximates our predefined curriculum in Eq.~\eqref{eq:predefined_curriculum} leads to an underlying robust objective of the StudentNet.

For the data-driven curriculum, if the learned MentorNet satisfies certain conditions, we have:
\begin{proposition}
Suppose $(\mathbf{x}, y)$ denotes a training sample and its corrupted label. For simplicity, let the MentorNet input $\phi(\mathbf{x}, y, \mathbf{w}) = \ell$ be the loss computed by the StudentNet model parameter $\mathbf{w}$. The MentorNet $g_m(\ell;\Theta) = v$, where $v$ is the sample weight. If $g_m$ decreases with respect to $\ell$, then there exists an underlying robust objective $F$:
\vspace{-2mm}
\[
F(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n \rho(\ell_i),
\vspace{-3mm}
\]
where $\rho(\ell_i)  = \int_0^{\ell_i} g_m(x;\Theta) d x$. In the special cases, $\rho(\ell)$ degenerates to the robust M-estimator: Huber~\cite{huber1964robust} and the log-sum
penalty~\cite{candes2008enhancing}.
\end{proposition}




\begin{figure}[ht]
\vspace{-3mm}
\centering
\includegraphics[width=0.8\linewidth]{figures/loss_curve.pdf}
\vspace{-3mm}
\caption{\label{fig:loss_curve}Visualization of sample loss and learning objective. The learned curriculum represents the best data-driven curriculum found in experiments.}
\vspace{-2mm}
\end{figure}

The proposition indicates that there exist some learned MentorNets that are related to the robust M-estimator. On noisy data, the effect of the robust objective is evident, \ie, preventing StudentNet from being dominated by corrupted labels. Fig.~\ref{fig:loss_curve} visualizes curves of the sample loss $\ell=y_i-g_s(\mathbf{x}_i, \mathbf{w})$ and the learning objective for the Huber loss~\cite{huber1964robust}, log-sum penalty~\cite{candes2008enhancing}, self-paced~\cite{kumar2010self}, and our learned data-driven curriculum. We use the best learned curriculum $\Theta^*$ on CIFAR-10 in our experiments and plot $|g_m(\phi(\mathbf{x},y,\mathbf{w});\Theta^*) \times \ell|$ since the $G$ in the objective function is unknown. As shown, all curves are robust to great loss to different extents. The corrupted labels in our problem are harmful. As the sample loss grows bigger beyond some value, MentorNet starts to sharply decrease the sample's weight. The subtlety of learned curriculum is difficult to be predefined by the analytic expression. Proposition 1 does not guarantee there is an underlying robust objective for every learned MentorNet. Instead, it shows MentorNet's capability of learning such robust objective. 



\vspace{-2mm}
\section{The Algorithm}\label{sec:algorithm}
\vspace{-1mm}

The alternating minimization algorithm~\cite{csiszar1984information} used in related work is intractable for deep CNNs, especially on big datasets, for two important reasons. First, in the subroutine of minimizing $\mathbf{w}$ when fixing $\mathbf{v}$, stochastic gradient descent often takes many steps before converging. This means that it can take a long time before moving past this single sub-step. However, such computation is often wasteful, particularly in the initial part of training, because, when $\mathbf{v}$ is far away from the optimal point, there is not much gain in finding the exact optimal $\mathbf{w}$ corresponding to this $\mathbf{v}$. Second, the subroutine of minimizing $\mathbf{v}$ when fixing $\mathbf{w}$ is often difficult, because the fixed vector $\mathbf{v}$ may not only consume a considerable amount of the memory but also hinder the parallel training on multiple machines. Therefore, optimizing the objective with deep CNNs requires some thought on the algorithmic level.

To minimize Eq.~\eqref{eq:datareg_obj}, we propose an algorithm called \emph{SPADE} (Scholastic gradient PArtial DEscent). The algorithm optimizes the StudentNet model parameter $\mathbf{w}$ jointly with a given MentorNet. It provides a simple and elegant way to minimize $\mathbf{w}$ and $\mathbf{v}$ stochastically over mini-batches. As a general approach, it can also take an input of $G$. Let $\Xi_t = \{(\mathbf{x}_j, y_j)\}_{j=1}^b$ denotes a mini-batch of $b$ samples, fetched uniformly at random and $\mathbf{v}_{\Xi}^{t} = [v_1^{t}, ... , v_b^{t}]$ represent the sample weights in $\Xi_t$. The MentorNet computes:
\vspace{-1mm}
\begin{equation}
\mathbf{v}_{\Xi}^{t} \!=\! g_m (\phi(\Xi_t, \mathbf{w}^{t-1})) \!=\! \arg\min_{\mathbf{v}_{\Xi}} \mathbb{F} (\mathbf{w}^{t-1},\! \mathbf{v}^{t-1}),
\vspace{-1mm}
\end{equation}
where $\phi$ is the feature extraction function defined in Eq.~\eqref{eq:gm_obj}. $\Theta$ denotes the learned MentorNet discussed in Section~\ref{sec:learning_curriculum}.

As shown in Algorithm~\ref{alg:overall}, for $\mathbf{w}$, a stochastic gradient is computed (via a mini-batch) and applied (Step 12), where $\alpha_t$ is the learning rate. For the latent weight variables $\mathbf{v}$, gradient descent is only applied to a small subset thereof parameters corresponding only to the mini-batch (Step 9 or 11). The partial gradient update on weight parameters is performed when $G$ is used (Step 9). Otherwise, we directly apply the weights computed by the learned MentorNet (Step 11). In both cases, the weights are computed on-the-fly within a mini-batch and thus do not need to be fixed. As a result, the algorithm can be conveniently parallelized across multiple machines.

{
\vspace{-2mm}
\begin{algorithm}[ht]
\footnotesize
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\LinesNumbered
\Input{Dataset $\mathcal{D}$, a predefined $G$ or a learned $g_m(\cdot;\Theta)$}
\Output{The model parameter $\mathbf{w}$ of StudentNet.}
Initialize $\mathbf{w}^{0}, \mathbf{v}^{0}$, $t = 0$\;

\While{Not Converged}{
	Fetch a mini-batch $\Xi_t$ uniformly at random\;
	
	For every $(\mathbf{x}_i, y_i)$ in $\Xi_t$ compute $\phi(\mathbf{x}_i, y_i, \mathbf{w}^t)$\;
	
	\If{update curriculum}{
	    $\Theta \leftarrow \Theta^*$, where $\Theta^*$ is learned in Sec.~\ref{sec:learning_curriculum}
	}
	
    \If{$G$ is used}{
	    $\mathbf{v}_\Xi^{t} \leftarrow  \mathbf{v}_\Xi^{t-1} - \alpha_t  \nabla_{\mathbf{v}}
	    \mathbb{F}(\mathbf{w}^{t-1}, \mathbf{v}^{t-1})\rvert_{\Xi_t}$
	}\lElse{
	    $\mathbf{v}_\Xi^{t} \leftarrow g_m(\phi(\Xi_t, \mathbf{w}^{t-1}); \Theta)$
    }


	$\mathbf{w}^{t}  \leftarrow \mathbf{w}^{t-1} - \alpha_t  \nabla_{\mathbf{w}} \mathbb{F}(\mathbf{w}^{t-1}, \mathbf{v}^{t}) \rvert_{\Xi_t}$\;
	
    $t \leftarrow t + 1$
}
\Return $\mathbf{w}^{t}$
\caption{{\small SPADE for minimizing Eq.~\eqref{eq:datareg_obj}}}
\label{alg:overall}
\end{algorithm}
\vspace{-2mm}
}



The curriculum can change during training. MentorNet is updated a few times in Algorithm~\ref{alg:overall}. In Step 6, the MentorNet parameter $\Theta$ is updated to adapt to the most recent model parameters of StudentNet. In experiments, we update $\Theta$ twice after the learning rate is changed. Each time, a data-driven curriculum is learned from the data generated by the most recent $\mathbf{w}$ using the method discussed in Section~\ref{sec:learning_curriculum}. The update is consistent with existing curriculum learning methodology~\cite{bengio2009curriculum,kumar2010self} and the difference here is that for each update, the curriculum is learned rather than specified by human experts.

Under standard assumptions, Theorem 1 shows that the algorithm stabilizes and converges to a stationary point (convergence to global/local minima cannot be guaranteed unless in specially structured non-convex objectives~\cite{chen2018gradient,zhou2017stochastic,zhou2017mirror}). The proof is in Appendix B. The theorem is a characterization of stability of the model parameters $\mathbf{w}$. For the weight parameters $\mathbf{v}$, as it is restricted in a compact set, convergence to a stationary point is not always guaranteed. As the model parameters are more important, we only provide a detailed characterization of the model parameter.

\begin{theorem}
\label{theorem:1}
Let the objective $\mathbb{F}(\mathbf{w}, \mathbf{v})$ defined in Eq.~\eqref{eq:datareg_obj} be differentiable, $L(\cdot)$ be Lipschitz continuous in $\mathbf{w}$ and $\nabla_{\mathbf{v}}G(\cdot)$ be Lipschitz continuous in $\mathbf{v}$.
Let $\mathbf{w}^{t}, \mathbf{v}^{t}$ be iterates from Algorithm 1 and $\sum_{t=0}^{\infty} \alpha_t = \infty, \sum_{t=0}^{\infty} \alpha_t^2 < \infty$ . Then,
$\lim_{t\rightarrow \infty} \mathbb{E}[\|\nabla_{\mathbf{w}} \mathbb{F}(\mathbf{w}^{t}, \mathbf{v}^{t})\|_2^2] = 0$.
\vspace{-2mm}
\end{theorem}

For the manually designed curriculums, it may be unclear where or even whether such predefined curriculum would converge via mini-batch training. Theorem~\ref{theorem:1} shows that the learned curriculum can converge and produce a stable StudentNet model. The algorithm can be used to replace the alternating minimization method in related work. 


\vspace{-3mm}
\section{Experiments}
This section empirically verifies the proposed method on four benchmarks of controlled corrupted labels in Section~\ref{sec:exp_part2} and real-world noisy labels in Section~\ref{sec:exp_part3}. The code can be found at {\small \url{https://github.com/google/mentornet}}.


\vspace{-2mm}
\subsection{Experiments on controlled corrupted labels}\label{sec:exp_part2}
\vspace{-2mm}
This section validates MentorNet on the controlled corrupted label. We follow a common setting in~\cite{zhang2017understanding} to train deep CNNs, where the label of each image is independently changed to a uniform random class with probability $p$, where $p$ is noise fraction and is set to 0.2, 0.4 and 0.8. The labels of validation data remain clean for evaluation.


\noindent\textbf{Dataset and StudentNet}: We use the same benchmarks in~\cite{zhang2017understanding}: CIFAR-10, CIFAR-100 and ImageNet. CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} consist of 32 $\times$ 32 color images arranged in 10 and 100 classes. Both datasets contain 50,000 training and 10,000 validation images. ImageNet ILSVRC2012~\cite{deng2009imagenet} contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.

We employ 3 recent deep CNNs as our StudentNets: inception~\cite{szegedy2016rethinking}, resnet-101~\cite{he2016deep} with wide filters~\cite{zagoruyko2016wide} and inception-resnet v2~\cite{szegedy2017inception}. Table~\ref{tab:studentnet} shows their \#model parameters, training, and validation accuracy when we train them on the clean training data (noise$=0$). As shown, they achieve reasonable accuracy on each task.


\begin{table}[ht]
\vspace{-5mm}
\centering
\footnotesize
\caption{\label{tab:studentnet}StudentNet and their accuracies on the clean training data.}
\begin{tabular}{cl|ccc}
\hline
\multicolumn{1}{l}{Dataset} & Model             & \#para & {\footnotesize train acc} & {\footnotesize val acc} \\
\hline
\multirow{2}{*}{CIFAR10}    & {\footnotesize inception}     &1.7M         & 0.83         & 0.81        \\
                            & {\footnotesize resnet101}         & 84M       & 1.00         & 0.96        \\
\hline
\multirow{2}{*}{CIFAR100}   & {\footnotesize inception}     &1.7M              &0.64                &0.49               \\
                            & {\footnotesize resnet101}        & 84M   & 1.00         & 0.79        \\
\hline
\multirow{1}{*}{ImageNet}   & {\footnotesize inception\_resnet}  &59M             & 0.88         & 0.77        \\
\hline
\end{tabular}
\vspace{-3mm}
\end{table}




\begin{table*}[ht]
\vspace{-4mm}
\centering
\caption{Comparison of validation accuracy on CIFAR-10 and CIFAR-100 under different noise fractions.}
\label{tab:cifar_comparison}
\begin{tabular}{|l|ccc|ccc|ccc|ccc|}
\hline
                & \multicolumn{6}{c|}{Resnet-101 StudentNet}  & \multicolumn{6}{c|}{Inception StudentNet}\\
                & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{CIFAR-10} \\
Method & 0.2      & 0.4     & 0.8     & 0.2      & 0.4      & 0.8     & 0.2      & 0.4     & 0.8     & 0.2      & 0.4      & 0.8     \\
\hline
FullModel&0.60 & 0.45 & 0.08 &0.82 & 0.69 & 0.18 & 0.43 & 0.38 & 0.15 & 0.76 & 0.73 & 0.42\\
Forgetting&0.61&0.44&0.16&0.78&0.63&0.35&0.42&0.37&0.17&0.76&0.71&0.44\\
Self-paced& 0.70 & 0.55  &0.13 & 0.89&0.85&0.28 &0.44&0.38&0.14&\textbf{0.80}&0.74&0.33 \\     
Focal Loss&0.59&0.44&0.09&0.79&0.65&0.28&0.43&0.38&0.15&0.77&0.74&0.40 \\
Reed Soft&0.62&0.46&0.08&0.81&0.63&0.18&0.42&0.39&0.12&0.78&0.73&0.39\\
MentorNet PD &0.72&0.56&0.14&0.91&0.77&0.33&0.44&0.39&0.16&0.79&0.74&0.44\\
MentorNet DD &\textbf{0.73}& \textbf{0.68}  & \textbf{0.35} &  \textbf{0.92} & \textbf{0.89} &\textbf{0.49} &\textbf{0.46}& \textbf{0.41}&\textbf{0.20}&0.79&\textbf{0.76}&\textbf{0.46}\\
\hline
\end{tabular}
\vspace{-4mm}
\end{table*}


\noindent \textbf{Baselines}: MentorNet is compared against the following baselines: \textbf{FullMode} is the standard StudentNet trained using $l_2$ weight decay, dropout~\cite{srivastava2014dropout} and data augmentation~\cite{krizhevsky2012imagenet}. The hyper-parameters are set to the best ones found on the clean training data. Unless specified otherwise, for a fair comparison, the StudentNet with the same hyperparameters is used in all baseline and our model. \textbf{Forgetting} was introduced in~\cite{arpit2017closer}, in which the dropout parameter is searched in the range of (0.2-0.9). \textbf{Self-paced}~\cite{kumar2010self} and \textbf{Focal Loss}~\cite{lin2017focal} represent well-known predefined curriculums in the literature. We implemented \textbf{Reed}~\yrcite{reed2014training} and \textbf{Goldberger}~\cite{goldberger2017training} as the recent weakly-supervised learning methods. The above baseline methods are a mixture of the curriculum learning and the recent methods dealing with corrupted labels.
 
\noindent \textbf{Our Model}: \textbf{MentorNet PD} is the network learned using our \emph{predefined} curriculum in Eq.~\eqref{eq:predefined_curriculum} using no additional clean labels. \textbf{MentorNet DD} is the learned \emph{data-driven} curriculum. It is trained on 5,000 images of true labels, randomly sampled from the CIFAR-10 training set. The same data are used to learn MentorNet DD on CIFAR-100. Note CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different number of classes. Therefore, it is fair to compare MentorNet DD with other methods using no true labels on CIFAR-100. Algorithm~\ref{alg:overall} is used to optimize the StudentNet. The decay factor in computing the loss moving average is set to 0.95. The loss percentile in the moving average is set by the cross-validation. As mentioned, a burn-in process is used in the first 20\% training epoch for both MentorNet DD and MentorNet PD. More details are discussed in Appendix E.

We first show the comparison to the baseline method on CIFAR-10 and CIFAR-100 in Table~\ref{tab:cifar_comparison}. On both datasets, each method is verified with two StudentNets (resnet-101 and inception) under the noise fraction of 0.2, 0.4, and 0.8. As we see on both datasets, MentorNet improves FullModel across different noise fractions, and the learned data-driven curriculum (MentorNet DD) achieves the best results. The improvement is more significant for the deeper CNN model resnet-101. For example, on the CIFAR-10 of 40\% noise, MentorNet DD (with resnet-101) yields an absolute 20\% gain over FullModel. After inspecting the result, we found that it may be because Mentor DD learns a more appropriate curriculum to give high weights to samples of correct labels. As a result, it helps the StudentNet focus on samples of correct labels. The results indicate that the learned MentorNet can improve the generalization performance of recent deep CNNs, and outperform the predefined curriculums (Self-paced and Focal Loss).

\begin{figure}[ht]
\vspace{-2mm}
\centering
\subfigure[{Test error on clean validation}]
{
\label{fig:exp_generation_a}
\includegraphics[width=0.7\linewidth, height=28mm]{figures/new_cifar100_acc.pdf}
\vspace{-5mm}
}
\subfigure[ {Training error on corrupted labels}]
{
\label{fig:exp_generation_b}
\includegraphics[width=0.7\linewidth, height=28mm]{figures/new_cifar100_loss.pdf}
\vspace{-5mm}
}
\vspace{-5mm}
\caption{\label{fig:exp_generation}Error of resnet trained on CIFAR-100 of 40\% noise.}
\vspace{-7mm}
\end{figure}

Fig.~\ref{fig:exp_generation} plots the training and test error on the clean validation data, under a representative setting: resnet-101 on CIFAR-100 of 40\% noise, where the $x$-axis denotes the training iteration. The $y$-axis is the validation error on the clean validation in Fig.~\ref{fig:exp_generation_a} and the mini-batch training error on corrupted labels in Fig.~\ref{fig:exp_generation_b}. For MentorNet, the training error is computed by $\sum_i v_i \ell_i$. The figure shows two insights. First, the training error of MentorNet approaches zero. This empirically verifies the convergence of the model. Second, MentorNet can overcome the overfitting to the corrupted label. While the training error is decreasing, the test error does not increase in Fig.~\ref{fig:exp_generation_a}. It suggests that the learned curriculum is beneficial for StudentNet. The sharp change around the 20k iteration in Fig.~\ref{fig:exp_generation} is due to the learning rate change. Besides, our result is consistent with~\cite{zhang2017understanding} that deep CNNS is able to get 0 training error on the corrupted training data. Forgetting (the dashed curve) is the only one that does not converge within 30k steps. As indicated in~\cite{arpit2017closer}, it is because forgetting reduces the speed at which DNNs memorize. As suggested in~\cite{zhang2017mixup}, a \emph{not} converged model might yield a better result, \eg, stop the model at 20K in Fig.~\ref{fig:exp_generation}. However, as it is hard to predetermine the time for early stopping, our focus is comparing the converged model. 


Fig.~\ref{fig:visual_curriculum} illustrates the best learned data-driven curriculum in our experiments, where the $z$-axis denotes the weights computed by $g_m$; the $y$ and $x$ axes denote the sample loss and the loss difference to the moving average, where $\lambda$ is the loss moving average. Two observations can be found in Fig.~\ref{fig:visual_curriculum}. First, the learned curriculum changes during the training of the StudentNet. Fig.~\ref{fig:visual_curriculum} (a) and (b) are MentorNet learned at different epochs. As shown, (a) assigns greater weights to samples of big loss more aggressively. Second, the learned curriculums in Fig.~\ref{fig:visual_curriculum} generally satisfy the condition in Proposition 1, \ie, the weight generally decreases with the loss. It suggests that joint learning of StudentNet and MentorNet optimizes an underlying robust objective.

\begin{figure}[ht]
\vspace{-1mm}
\includegraphics[width=1\linewidth]{figures/visual_curriculum.pdf}
\vspace{-6mm}
\caption{\label{fig:visual_curriculum} The data-driven curriculums learned by MentorNet with the resnet-101 at epoch 21 in (a) and 76 in (b).}
\vspace{-5mm}
\end{figure}

Table~\ref{tab:exp_compare_soa} compares to recent published results under the setting: CIFAR of 40\% noise fraction. We cite the number in~\cite{azadi2016auxiliary}, and implement other methods using the same resnet-101 StudentNet. The results show that our result is comparable and even better than the state-of-the-art.

\begin{table}[ht]
\vspace{-5mm}
\centering
\small
\caption{\label{tab:exp_compare_soa} Validation accuracy comparison to representative published results on CIFAR of 40\% noise.}
\vspace{1mm}
\begin{tabular}{ll|cc}
\hline
ID&Method & CIFAR-10 & CIFAR-100 \\
\hline
1&Reed Hard (Resnet)  &  0.62       & 0.47  \\
2&Reed Soft (Resnet)&  0.61       & 0.46  \\
3&Goldberger (Resnet) &  0.70       & 0.46  \\
4&Azadi (AlexNet)~\yrcite{azadi2016auxiliary} &0.75& -\\
5&\textbf{MentorNet (Resnet)} &\textbf{0.89}&\textbf{0.68}\\
\hline
\end{tabular}
\vspace{-3mm}
\end{table}


To verify MentorNet for large-scale training, we apply our method on the ImageNet ILSVRC12~\cite{deng2009imagenet} benchmark to improve the inception-resnet v2~\cite{szegedy2017inception} model. We train the model on the ImageNet of 40\% noise. Inspired by~\cite{zhang2017understanding}, we start with an inception-resnet (NoReg) with no regularization (NoReg) and add weight decay, dropout, and data augmentation to the model. Table~\ref{tab:exp_imagenet} shows the comparison. As shown, MentorNet improves the performance of both the inception-resnet without regularization (NoReg) and with full regularization (FullModel). It also outperforms the forgetting baseline (dropout keep probability = 0.2). The results suggest that MentorNet can improve deep CNNs on the large-scale training on corrupted labels.

\begin{table}[ht]
\vspace{-3mm}
\centering
\caption{\label{tab:exp_imagenet} Accuracy on the clean ImageNet ImageNet validation set. Models are trained on the ImageNet training data of 40\% noise.}
\small
\begin{tabular}{l|cc}
\hline
Method & P@1 & P@5 \\
\hline
NoReg &  0.538 & 0.770  \\
NoReg+WeDecay &  0.560 & 0.809  \\
NoReg+Dropout & 0.575&0.807\\
NoReg+DataAug&  0.522 & 0.772  \\
NoReg+MentorNet&0.590& 0.814\\
\hline
FullModel&  0.612 & 0.844  \\
Forgetting(FullModel) &0.628&0.845\\
\textbf{MentorNet(FullModel)} &\textbf{0.651}&\textbf{0.859}\\
\hline
\end{tabular}
\vspace{-5mm}
\end{table}



\vspace{-2mm}
\subsection{Experiments on real-world noisy labels}
\label{sec:exp_part3}
\vspace{-2mm}
To verify MentorNet on real-world noisy labels, we conduct experiments on the large WebVision benchmark~\cite{li2017webvision}. It contains 2.4 million images of real-world noisy labels, crawled from the web using the 1,000 concepts in ImageNet ILSVRC12. We download the resized images from the official website\footnote{https://www.vision.ee.ethz.ch/webvision/download.html}. The inception-resenet v2~\cite{szegedy2017inception} is used as our StudentNet, trained using a distributed asynchronized momentum optimizer on 50 GPUs. Since the dataset is very big, for quick experiments, we compare baseline methods using the Google image subset on the first 50 classes. We use Mini to denote this subset and Entire for the entire WebVision. All the models are evaluated on the clean ILSVRC12 and WebVision validation set.

Table~\ref{tab:exp_real_world_noise} lists the comparison result. As we see, the proposed MentorNet significantly improves baseline methods on real-world noisy labels. The method marked by the start indicates it uses a pre-trained ImageNet model to obtain additional 30k labels for 118 classes. Following the same protocol, MentorNet* is trained using the additional labels. The results show that our method outperforms the baseline methods on real-world noisy labels. To the best of our knowledge, it achieves the best-published result on the WebVision~\cite{li2017webvision} benchmark.


\begin{table}[ht]
\vspace{-5mm}
\caption{\label{tab:exp_real_world_noise}Validation accuracy on the ImageNet ILSVRC12 and WebVision validation set. The number outside (inside) the parentheses denotes top-1 (top-5) classification accuracy (\%). * marks the method trained using additional verification labels.}
\centering
\small
\begin{tabular}{cl|cc}
\hline
Dataset& Method & ILSVRC12 & WebVision\\
\hline
Entire& Li \etal~\yrcite{li2017webvision} &0.476 (0.704)&0.570 (0.779)\\
Entire& Forgetting &0.590 (0.808) & 0.666 (0.856)\\
Entire& Lee \etal~\yrcite{lee2017cleannet}* &0.602 (0.811) & 0.685 (0.865)\\
Entire& MentorNet &0.625 (0.830) & 0.708 (0.880)\\
Entire& \textbf{MentorNet*} &\textbf{0.642} (\textbf{0.848})& \textbf{0.726} (\textbf{0.889})\\
\hline
Mini&FullModel &0.585 (0.818)&-\\
Mini&Forgetting &0.562 (0.816)&-\\
Mini&Reed Soft&0.565 (0.827)&-\\
Mini&Self-paced &0.576 (0.822)& -\\
Mini& \textbf{MentorNet} &\textbf{0.638} (\textbf{0.858})&-\\
\hline
\end{tabular}
\vspace{-5mm}
\end{table}

\vspace{-1mm}
\section{Related Work}
\vspace{-1mm}
Curriculum learning (CL), proposed by Bengio \etal~\yrcite{bengio2009curriculum}, is a learning paradigm in which a model is learned by gradually including from easy to complex samples in training so as to increase the learning entropy~\cite{bengio2009curriculum}. From the human behavioral perspective, Khan \etal~\yrcite{khan2011humans} have shown that CL is consistent with the principle of human teaching. CL has been empirically verified in a variety of problems, such as computer vision~\cite{supancic2013self,chen2015webly}, natural language processing~\cite{turian2010word}, multitask learning~\cite{graves2017automated}. A common CL approach is to predefine a curriculum. For example, Kumar \etal~\yrcite{kumar2010self} proposed a curriculum called self-paced learning which favors training samples of smaller loss. After that, many predefined curriculums were proposed, \eg, in \cite{supancic2013self,jiang2014self,jiang2015self,sangineto2016self,chang2017active,ma2017self,ma2017convergence}. For example, Jiang \etal~\yrcite{jiang2014self} introduced a curriculum of using easy and diverse samples. Fan \etal~\yrcite{fan2017self} proposed to use predefined sample weighting schemes as an implicit way to define a curriculum. Previous work has shown that predefined curriculums are useful in overcoming noisy labels~\cite{chen2015webly,liang2016learning,lin2017active}. In parallel to CL, the sample weighting schemes were also studied in~\cite{lin2017active,wang2017robust,fan2018learning,dehghani2018fidelityweighted}. Compared to the existing work, our paper presents a new way of learning data-driven curriculums for deep networks trained on corrupted labels.




Our work is related to the weakly-supervised learning methods. Among recent contributions, Reed \etal~\yrcite{reed2014training} developed a robust loss to model ``prediction consistency''. Menon \etal~\yrcite{menon2015learning} used class-probability estimation to study the corruption process. Sukhbaatar \etal~\yrcite{sukhbaatar2014training} proposed a noise transformation to estimate the noise distribution. The transformation matrix needs to be periodically updated and is non-trivial to learn. To address the issue, Goldberger \etal~\yrcite{goldberger2017training} proposed to add an additional softmax layer end-to-end with the base model. Azadi \etal~\yrcite{azadi2016auxiliary} tackled this problem by a regularizer called AIR. This method was shown to be effective but it relied on additional clean labels to train the representation.
More recently, methods utilized additional labels for label cleaning~\cite{Veit2017}, knowledge distillation~\cite{Li2017ICCV} or semi-supervised learning~\cite{vahdat2017toward,dehghani2017avoiding}. Different from previous work, we focus on learning curriculum to train very deep CNNs on corrupted labels from scratch. In addition, clean labels are not always needed for our method. In Section~\ref{sec:exp_part2}, the MentorNet is learned on a small subset of CIFAR-10 and applied to CIFAR-100

\vspace{-2mm}
\section{Conclusions}
\vspace{-2mm}
In this paper, we presented a novel method for training deep CNNs on corrupted labels. Our work was built on curriculum learning and advanced the methodology by proposing to learn data-driven curriculum via a neural network called MentorNet. We proposed an algorithm for jointly optimizing deep CNNs with MentorNet on large-scale data. We conducted comprehensive experiments on datasets of controlled and real-world noise. Our empirical results showed that generalization performance of deep CNNs trained on corrupted labels can be effectively improved by the learned data-driven curriculum.


\section*{Acknowledgements}
The authors would like to thank anonymous reviewers for helpful comments and Deyu Meng, Sergey Ioffe, and Chong Wang for meaningful discussions and kind support.

\bibliography{example_paper}
\bibliographystyle{icml2018}

\balance
\end{document}
