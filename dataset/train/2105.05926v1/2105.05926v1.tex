
\begin{center} \begin{Large}{Appendix} \end{Large} \end{center}
\label{appendix}

\section{More Experiments}
\label{sec:more_experiments}
\subsection{Backbone Variations}

In our experiments we use TResNet-M \cite{ridnik2021tresnet} as a backbone for our visual model, due to its efficiency and reported high accuracy on several competitive computer vision datasets. To further extend our analysis and comparison with prior works we also explore two popular backbone architectures, VGG19 \cite{simonyan2014very} and ResNet50 \cite{he2016deep} in Table \ref{tab:backbone}. We report results using our approach as well as adding a comparison to Fast0Tag \cite{zhang2016fast} loss function with our E2E training scheme as a baseline. As can be seen, using our approach with VGG19 as a backbone, the results in terms of mAP for both zero-shot and generalized zero-shot are superior compared to prior works but lower than our current backbone, while using ResNet50 as a backbone improves over VGG19 in all metrics. Best results are achieved using TResNet-M backbone. In addition it can also be seen that the results in terms of mAP for tag-based image retrieval using different backbone variations are higher than current prior works, suggesting that our training scheme extends and may improve the quality of various model architectures. 

\begin{table}[h!]
\centering
\caption{Results using alternative backbones on NUS-WIDE test set. We report the results in terms of F1(), F1(), and mAP for ZSL and GZSL tasks. Best results are in bold.}\vspace{0.2em} 
\setlength{\tabcolsep}{12pt}
\adjustbox{width=1\linewidth}{
\begin{tabular}{cccccc} 
\toprule[0.15em]
\textbf{Backbone} & \textbf{Method} &\textbf{Task} & \begin{tabular}[c]{@{}c@{}} \textbf{F1()} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{F1()} \end{tabular} & \textbf{mAP} \\
\toprule[0.15em]
\multirow{2}{*}{VGG19 \cite{simonyan2014very}} & \multirow{2}{*}{Fast0Tag \cite{zhang2016fast}} & ZSL & 24.2 & 22.2 & 20.2 \\
 & &GZSL & 11.7 & 13.0 & 6.6 \\ 
\cmidrule(r){3-6}
\multirow{2}{*}{TResNet-M \cite{ridnik2021tresnet}} & \multirow{2}{*}{Fast0Tag \cite{zhang2016fast}} & ZSL & 25.7 & 23.3 & 21.6 \\
 & &GZSL & 15.4 & 16.6 & 9.7 \\ 
\cmidrule(r){3-6}
\multirow{2}{*}{VGG19 \cite{simonyan2014very}} & \multirow{2}{*}{Ours} & ZSL & 29.0 & 26.5 & 24.2 \\
 & &GZSL & 16.8 & 19.0 & 9.9 \\ 
\cmidrule(r){3-6}
\multirow{2}{*}{ResNet50 \cite{he2016deep}} & \multirow{2}{*}{Ours} &ZSL & 30.0 & 27.6 & 24.4 \\
 & &GZSL & 17.7 & 20.1 & 11.2 \\ 
\cmidrule(r){3-6}
\multirow{2}{*}{{TResNet-M \cite{ridnik2021tresnet}}} & \multirow{2}{*}{Ours} &ZSL & \textbf{{30.5}} & \textbf{27.8} & \textbf{{25.9}} \\
 & &GZSL & {\textbf{18.5}} & \textbf{{21.0}} & \textbf{{12.1}} \\
 
\bottomrule[0.1em]
\end{tabular}}
\vspace{-0.2cm}
\label{tab:backbone}
\end{table}


\section{Reproduciblity}
To support future research in the field, we currently work to publish our trained models and share a fully reproducible training code on GitHub.

\section{Additional Qualitative Results}
We present in figure \ref{fig:more_qualitative} additional qualitative results using our proposed method for several sample images from NUS-WIDE test set. It can be seen that in several cases the unseen tags (marked by asterisks) are ranked in the top-10. In addition, while some of the unseen tags are incorrect based on the ground truth annotation, in most cases there exists a noticeable semantic relation between these tags to the image.

\begin{figure*}
\centering
\begin{tabular}{lll}
\subcaptionbox*{\textbf{graffiti} \\ \textbf{art} \\ \textbf{London} \\ mural \\ England \\ urban \\ green \\ paint \\ war \\ politics}{\includegraphics[height = 2.6cm]{images/sup/187.png}} &
\subcaptionbox*{officers \\ protesters \\ riot \\ politics \\ *\textbf{police}* \\ \textbf{London} \\ men \\ roadblock \\ *protest* \\ soldier}{\includegraphics[height = 2.6cm]{images/sup/149.png}} &
\subcaptionbox*{firefighter \\ \textbf{demonstration} \\ France \\ Canada \\ \textbf{riot} \\ action \\ officers \\ winter \\ sport \\ *\textbf{protest}* \\ }{\includegraphics[height = 2.6cm]{images/sup/119.png}} \\
\subcaptionbox*{wildlife \\ nature \\ deer \\ \textbf{moose} \\ *elk* \\ Canada \\ wild \\ animals \\ park \\ \textbf{Alaska}}{\includegraphics[height = 2.6cm]{images/sup/180.png}} &

\subcaptionbox*{football \\ crowd \\ \textbf{cheering} \\ *soccer* \\ baseball \\ red \\ England \\ game \\ parade \\ basketball}{\includegraphics[height = 2.6cm]{images/sup/185.png}} &

\subcaptionbox*{\textbf{bride} \\ *\textbf{wedding}* \\ Hawaii \\ sea \\ \textbf{bravo} \\ \textbf{beautiful} \\ white \\ couple \\ groom \\ dress }{\includegraphics[height = 2.6cm]{images/sup/117.png}}

\end{tabular}
\caption{Qualitative results showing the top-10 tags retrieved using our proposed method. Bold text represents the correct tags according to the provided ground truth in NUS-WIDE test set. Asterisks mark unseen tags.}
\label{fig:more_qualitative}
\end{figure*}



