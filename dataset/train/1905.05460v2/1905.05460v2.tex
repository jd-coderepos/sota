\subsection{Dataset}\label{sec:dataset}
We use the full-wiki setting of HotpotQA to conduct our experiments. 
112,779 questions are collected by crowdsourcing based on the first paragraphs in Wikipedia documents, 84\% of which require multi-hop reasoning. The data are split into a training set (90,564 questions), a development set (7,405 questions) and 
a test set (7,405 questions).
All questions in development and test sets are \textit{hard multi-hop} cases.

In the training set, for each question, an answer and paragraphs of 2 \textit{gold} (useful) entities are provided, with multiple \textit{supporting facts}, sentences containing key information for reasoning, marked out. There are also 8 unhelpful \textit{negative paragraphs} for training. During evaluation, only questions are offered and meanwhile supporting facts are required besides the answer.  

To construct cognitive graphs for training, edges in gold-only cognitive graphs are inferred from supporting facts by fuzzy matching based on Levenshtein distance~\cite{navarro2001guided}.\label{fuzzy_match}
For each supporting fact in , if any gold entity or the answer, denoted as , is fuzzy matched with a span in the supporting fact, edge  is added.

\subsection{Experimental Details}
We use pre-trained BERT-base model released by \cite{devlin2018bert} in System 1. The hidden size  is 768, unchanged in node vectors of GNN and predictors. All the activation functions in our model are \emph{gelu}~\cite{hendrycks2016bridging}. We train models on Task \#1 for 1 epoch and then on Task \#1 and \#2 jointly for 1 epoch. Hyperparameters in training are as follows: 
\begin{center}
\begin{small}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccc}
\hline
    Model&Task&batch size&learning rate&weight decay\\
    \hline
    BERT & \#1,\#2 & 10 &  & 0.01\\
    GNN & \#2 & graph &  & 0 \\
\hline
\end{tabular}
\end{small}
\end{center}

BERT and GNN are optimized by two different Adam optimizers, where . The predictors share the same optimizer as GNN. The learning rate for parameters in BERT warmup over the first 10\% steps, and then linearly decays to zero.

To select out supporting facts, we just regard the sentences in the  of any node in graph as supporting facts. 
In the initialization of , these 1-hop spans exist in the question and can also be detected by fuzzy matching with supporting facts in training set. The extracted 1-hop entities by our framework can improve the retrieval phase of other models (See \S~\ref{sec:baseline}), which motivated us to separate out the extraction of 1-hop entities to another BERT-base model for the purpose of reuse in implementation.
\subsection{Baselines}\label{sec:baseline}
\begin{table*}[]
\small{
    \centering
    \begin{tabular}{|c|c|cccc|cccc|cccc|c|}
    \hline
        \multirow{2}*{}&\multirow{2}*{Model} & \multicolumn{4}{|c|}{Ans} & \multicolumn{4}{|c|}{Sup} &         \multicolumn{4}{|c|}{Joint} \\
        \cline{3-14}
        ~ & ~& EM &  & Prec & Recall & EM &  & Prec & Recall & EM &  & Prec & Recall\\
        \hline
        \multirow{5}*{Dev}&~\citet{yang2018hotpotqa} & 23.9 & 32.9 & 34.9 & 33.9 & 5.1 & 40.9 & 47.2 & 40.8 & 2.5 & 17.2 & 20.4 & 17.8\\
        ~&~\citet{yang2018hotpotqa}-IR&24.6 & 34.0 & 35.7 & 34.8 & 10.9 & 49.3 & 52.5 & 52.1 & 5.2 & 21.1 & 22.7 & 23.2\\
        ~&BERT& 22.7 & 31.6 & 33.4 & 31.9 & 6.5 & 42.4 & 54.6 & 38.7 & 3.1 & 17.8 & 24.3 & 16.2\\
\cline{2-14}
        ~&\name-sys1&33.6 & 45.0 & 47.6 & 45.4 & \textbf{23.7} & 58.3 & \textbf{67.3} & 56.2 & 12.3 & 32.5 & 39.0 & 31.8\\
        ~&\name-onlyR&34.6 & 46.2 & 48.8 & 46.7 & 14.7 & 48.2 & 56.4 & 47.7 & 8.3 & 29.9 & 36.2 & 30.1\\
        ~&\name-onlyQ&30.7 & 40.4 & 42.9 & 40.7 & 23.4 & 49.9 & 56.5 & 48.5 & \textbf{12.4} & 30.1 & 35.2 & 29.9\\
        \cline{2-14}
        ~&\name&\textbf{37.6} & \textbf{49.4} & \textbf{52.2} & \textbf{49.9} & 23.1 & \textbf{58.5} & 64.3 & \textbf{59.7} & 12.2 & \textbf{35.3} & \textbf{40.3} & \textbf{36.5}\\
        
        \hline
        \multirow{4}*{Test}&~\citet{yang2018hotpotqa} & 24.0 & 32.9 & -  & - & 3.86 & 37.7 &-&-& 1.9 & 16.2&-&-\\
        ~&QFE & 28.7 & 38.1 & -  & - & 14.2 & 44.4 &-&-& 8.7 & 23.1&-&-\\
        ~&DecompRC & 30.0 & 40.7 & -  & - & N/A & N/A & - & - & N/A & N/A &-&-\\
        ~&MultiQA & 30.7 & 40.2 & - & - & N/A & N/A & - & - & N/A & N/A &-&-\\
        ~&GRN & 27.3 & 36.5 & - & - & 12.2 & 48.8 & - & - & 7.4 & 23.6 &-&-\\
        ~&\name& \textbf{37.1} & \textbf{48.9} & - &- & \textbf{22.8}&\textbf{57.7} & - & - & \textbf{12.4}& \textbf{34.9}& - & -\\
\hline
    \end{tabular}
    }
    \addtolength{\belowcaptionskip}{-12pt}
    \caption{Results on HotpotQA (fullwiki setting). The test set is not public. The maintainer of HotpotQA only offers EM and  for every submission. N/A means the model cannot find supporting facts.}
    \label{tab:result}
\end{table*}
The first category is previous work or competitor:
\begin{compactitem}
\setlength{\itemsep}{-12pt}
\item\textbf{\citet{yang2018hotpotqa}} The strong baseline model proposed in the original HotpotQA paper~\cite{yang2018hotpotqa}. It follows the retrieval-extraction framework of DrQA~\shortcite{chen2017reading} and subsumes the advanced techniques in QA, such as self-attention, character-level model, bi-attention.\\ 
\item\textbf{GRN}, \textbf{QFE}, \textbf{DecompRC}, \textbf{MultiQA}  The other models on the leaderboard.\footnote{All these models are unpublished before this paper.}\\ 
\item\textbf{BERT} State-of-art model on single-hop QA. BERT in original paper requires single-paragraph input and pre-trained BERT can barely handle paragraphs of at most 512 tokens, much fewer than the average length of concatenated paragraphs. We add relevant sentences from predecessor nodes in the cognitive graph to every paragraphs and report the answer span with maximum start probability in all paragraphs.\\
\item\textbf{\citet{yang2018hotpotqa}-IR\ } \citet{yang2018hotpotqa} with \textbf{I}mproved \textbf{R}etrieval. \citet{yang2018hotpotqa} uses traditional \textit{inverted index filtering} strategy to retrieve relevant paragraphs. The effectiveness might be challenged due to its failures to find out entities mentioned in question sometimes. The main reason is that word-level matching in retrieval usually neglect language models, which indicates importance and POS of words. We improve the retrieval by adding 1-hop entities spotted in the question by our model, increasing the coverage of supporting facts from 56\% to 72\%. 
\end{compactitem}
Another category is for ablation study:
\begin{compactitem}
\setlength{\itemsep}{-12pt}
\item\textbf{\name-onlyR} model initializes  with the same entities retrieved in~\citet{yang2018hotpotqa} as 1-hop entities, mainly for fair comparison.\\ \item\textbf{\name-onlyQ} initializes  only with 1-hop entities extracted from question, free of retrieved paragraphs. Complete \name~implementation uses both.\\
\item\textbf{\name-sys1} only retains \sysone~and lacks cascading reasoning in \systwo.
\end{compactitem}

\subsection{Results}
Following ~\citet{yang2018hotpotqa}, the evaluation of answer and supporting facts consists of two metrics: Exact Match (EM) and  score. Joint EM is 1 only if answer string and supporting facts are both strictly correct. Joint precision and recall are the products of those of Ans and Sup, and then joint  is calculated. All results of these metrics are averaged over the test set.\footnote{Thus it is possible that overall  is lower than both precision and recall.} Experimental results show superiority of our method in multiple aspects:

\vpara{Overall Performance}Our \name~outperforms all baselines on all metrics by a significant margin (See Table~\ref{tab:result}). The leap of performance mainly results from the superiority of the \name~framework over traditional retrieval-extraction methods. Since paragraphs that are multi-hop away may share few common words literally or even little semantic relation with the question, retrieval-extraction framework fails to find the paragraphs that become related only after the reasoning clues connected to them are found.
Our framework, however, gradually discovers relevant entities following clues.




\vpara{Logical Rigor} QA systems are often criticized to answer questions with shallow pattern matching, not based on reasoning. To evaluate logical rigor of QA, we use , the proportion of ``joint correct answers'' in correct answers. The joint correct answers are those deduced from all necessary and correct supporting facts. Thus, this proportion stands for logical rigor of reasoning. The proportion of our method is up to , far outnumbering 7.9\% of ~\citet{yang2018hotpotqa} and  of QFE. 





\begin{figure}[hbt]
    \centering
    \includegraphics[width=\linewidth]{img/question-types.pdf}
    \addtolength{\belowcaptionskip}{-13pt}
    \addtolength{\abovecaptionskip}{-16pt}
    \caption{Model performance on 8 types of questions with different hops. 
}
    \label{fig:question_types}
\end{figure}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{img/case.pdf}
    \addtolength{\belowcaptionskip}{-13pt}
    \addtolength{\abovecaptionskip}{-16pt}
    \caption{Case Study. Different forms of cognitive graphs in our results, i.e., Tree, Directed Acyclic Graph (DAG), Cyclic Graph. Circles are candidate answer nodes while rounded rectangles are hop nodes. Green circles are the final answers given by \name~and check marks represent the annotated ground truth.
}
    \label{fig:case}
\end{figure*}
\vpara{Multi-hop Reasoning} Figure~\ref{fig:question_types} illustrates joint  scores and average hops of 8 types of questions, including general, alternative and special questions with different interrogative word. 
As the hop number increases, the performance of ~\citet{yang2018hotpotqa} and ~\citet{yang2018hotpotqa}-IR drops dramatically, while our approach is surprisingly robust.
However, there is no improvement in alternative and general questions, because the evidence for judgment cannot be inferred from supporting facts, leading to lack of supervision. Further human labeling is needed to answer these questions.  

\vpara{Ablation Studies}
To study the impacts of initial entities in cognitive graphs, \name-onlyR begins with the same initial paragraphs as \cite{yang2018hotpotqa}. We find that \name-onlyR still performs significantly better. The performance decreases slightly compared to \name, indicating that the contribution mainly comes from the framework. 

To compare against the retrieval-extraction framework, \name-onlyQ is designed that it only starts with the entities that appear in the question. 
Free of elaborate retrieval methods, this setting can be regarded as a natural thinking pattern of human being, in which only explicit and reliable relations are needed in reasoning.  
\name-onlyQ still outperforms all the baselines, which may reveal the superiority of \name~framework over the retrieval-extraction framework.




BERT is not the key factor of improvement, although plays a necessary role. 
Vanilla BERT
performs similar or even slightly poorer to ~\cite{yang2018hotpotqa} in this multi-hop QA task, possibly because of the pertinently designed architectures in ~\citet{yang2018hotpotqa} to better leverage supervision of supporting facts.


To investigate the impacts of the absence of System 2, we design a System 1 only approach, \name-sys1, which inherits the iterative framework
but outputs answer spans with maximum predicted probability.
On \textit{Ans} metrics, the improvement over the best competitor decreases about 50\%, highlighting the reasoning capacity of GNN on cognitive graphs.


\vpara{Case Study}We show how the cognitive graph clearly explains complex reasoning processes in our experiments in Figure~\ref{fig:case}. The cognitive graph highlights the heart of the question in case (1) -- i.e., to choose between the number of members in two houses. \name~makes the right choice based on semantic similarity between ``Senate'' and ``upper house''. Case (2) illustrates that the robustness of the answer can be boosted by exploring parallel reasoning paths. Case (3) is a \emph{semantic retrieval} question without any entity mentioned, which is intractable for \name-onlyQ or even human. Once combined with information retrieval, our model finally gets the answer ``Marijus Adomaitis'' while the annotated ground truth is ``Ten Walls''. However, when backtracking the reasoning process in cognitive graph, we find that the model has already reached ``Ten Walls'' and answers with his real name, which is acceptable and even more accurate. Such explainable advantages are not enjoyed by black-box models.

