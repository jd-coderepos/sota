\documentclass{article}

\usepackage{hyperref}
\usepackage{microtype}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{subfig}
 \usepackage{booktabs}
\usepackage{url}            \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[symbol]{footmisc}
\usepackage{tabularx,colortbl}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{booktabs}
\usepackage{balance}
\usepackage{listings}
\usepackage{lstautogobble}  \usepackage{zi4}            \definecolor{bluekeywords}{rgb}{0.13, 0.13, 1}
\definecolor{greencomments}{rgb}{0, 0.5, 0}
\definecolor{redstrings}{rgb}{0.9, 0, 0}
\definecolor{graynumbers}{rgb}{0.5, 0.5, 0.5}
\lstset{
    autogobble,
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\footnotesize,
    tabsize=4,
    captionpos=b
}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  ndkeywords={class, export, boolean, throw, implements, import, this},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  sensitive=true
}

\newcommand{\thou}[0]{}
\newcommand{\million}[0]{}

\newcommand{\todo}[1]{{\color{red} #1 }}

\usepackage{array}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\definecolor{Gray}{HTML}{ABD5FF}  \newcommand{\codesnip}[1]{\texttt{\small #1}}

\usepackage[accepted]{icml2020}

\newcommand{\ours}[0]{ContraCode}
\newcommand{\ourslong}[0]{Contrastive Code Representation Learning}


\icmltitlerunning{\ourslong{}}

\begin{document}
\twocolumn[
\icmltitle{\ourslong{}}
\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Paras Jain}{equal,ucb}
\icmlauthor{Ajay Jain}{equal,ucb}
\icmlauthor{Tianjun Zhang}{ucb}
\icmlauthor{Pieter Abbeel}{ucb}
\icmlauthor{Joseph E. Gonzalez}{ucb}
\icmlauthor{Ion Stoica}{ucb}
\end{icmlauthorlist}
\icmlaffiliation{ucb}{University of California, Berkeley}
\icmlcorrespondingauthor{Paras Jain}{parasj@berkeley.edu}
\icmlcorrespondingauthor{Ajay Jain}{ajayj@berkeley.edu}
\icmlkeywords{Machine Learning, Programming Languages}
\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like summarizing code in English, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based BERT model is sensitive to source code edits, \textit{even when the edits preserve semantics}. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training improves JavaScript summarization and TypeScript type inference accuracy by 2\% to 13\%. We also propose a new zero-shot JavaScript code clone detection dataset, showing that \ours{} is both more robust and semantically meaningful. On it, we outperform RoBERTa by 39\% AUROC in an adversarial setting and up to 5\% on natural code.
\end{abstract}

\section{Introduction}


Programmers increasingly rely on machine-aided programming tools to aid software development~\citep{refactoring_kim2012field}. These tools analyze or transform code automatically. Traditionally, code analysis uses hand-written rules, though the wide diversity of programs encountered in practice can limit their generality. Recent work uses machine learning to improve performance through richer language understanding, such as learning to detect bugs \cite{pradel2018deepbugs} and predict performance \cite{mendis2019ithemal}.

\begin{figure}[t]
    \centering
    \includegraphics[trim={0 6mm 0 0},clip,width=0.85\linewidth]{figures/robust_ap_mp.pdf}
    \caption{\textbf{Robust code clone detection:} When trained on source code, \textbf{BERT is not robust to simple label-preserving code edits} like renaming variables. Adversarially selecting between possible edits lowers performance below random guessing. Contrastive pre-training with \ours{} learns a more robust representation of functionality that is consistent across code transforms.}
    \label{fig:bert_motivation_robust}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/conceptual_ordered.pdf}
    \caption{For many learned analyses, programs with the same functionality should have similar representations. \ours{} learns such representations by pre-training an encoder to retrieve equivalent, transformed programs among many distractors.}
    \label{fig:conceptual}
\end{figure}

Still, program datasets suffer from scarce annotations due to the time and expertise needed to label code. Synthetic auto-generated labels are used for method naming~\citep{alon2018code2seq,alon2019code2vec} and bug detection~\citep{ferenc2018public,pradel2018deepbugs,benton2019defexts}. However, synthetic code datasets have duplication issues~\citep{10.1145/3359591.3359735} and biases~\citep{shin2019synthetic} that can degrade generalization.
Moreover, supervised models are not robust to adversarial code edits, suffering significant accuracy loss when variables are renamed~\cite{yefet2019adversarial}, statements are permuted, or other semantics-preserving transformations are applied~\cite{wang2019coset, wang2019learning, rabin2020evaluation}.

\begin{figure}[t]
    \centering
    \includegraphics[trim={0 90mm 120mm 0},clip,width=\linewidth]{figures/roberta_contracode_embeddings.pdf}
    \caption{A UMAP visualization of JavaScript method representations learned by RoBERTa and \ours{}, in . Programs with the same functionality share color and number. RoBERTa's embeddings often do not cluster by functionality, suggesting that it is sensitive to implementation details. For example, many different programs {\color[HTML]{7030A0}\textbf{overlap}}, and renaming the variables of Program 19 significantly {\color[HTML]{C00000}\textbf{changes the embedding}}. In contrast, variants of Program 19 cluster in \ours{}'s embedding space.}
    \label{fig:bert_motivation_tsne}
\end{figure}

In contrast, self-supervised models can acquire knowledge from large open-source repositories such as GitHub without annotations.
Inspired by the success of pre-training in natural language processing, \citet{ben2018neural} use self-supervision to learn code token embeddings like word2vec. More recently, the popular BERT model family~\cite{devlin2018bert} has been applied to code~\citep{cuBERT,feng2020codebert,guo2020graphcodebert}. BERT pre-trains a Transformer~\cite{vaswani2017attention} on programs by reconstructing masked or replaced tokens from context. This objective is called masked language modeling (MLM).

However, we find that BERT is sensitive to implementation details. Figure~\ref{fig:bert_motivation_robust} shows the performance of two self-supervised models on a binary classification task: detecting whether two programs solve the same problem. We mine these programs from the HackerRank interview preparation website. While BERT has comparable performance on the original user-submitted programs (0 edits), BERT's performance greatly degrades when the programs are adversarially transformed \textit{e.g.} by renaming variables and deleting dead code (1-16 edits). These transforms do not change the functionality of the programs, so are label-preserving. Qualitatively, program representations are not invariant to edits (Figure~\ref{fig:bert_motivation_tsne}). This could be because accurate reconstructions during pre-training mostly depend on syntactic and program implementation details.

Motivated by the sensitivity of supervised learning and reconstruction-based pre-training, we develop \ours{}: a self-supervised learning algorithm that explicitly optimizes for representations of program functionality.
We hypothesize that \emph{programs with the same functionality should have similar underlying representations} for downstream code understanding tasks.
\ours{} generates syntactically diverse but functionally similar programs with source-to-source compiler transformation techniques (\textit{e.g.}, dead code elimination, obfuscation and constant folding).
\ours{} uses these programs in a challenging discriminative pretext task that requires the model to identify equivalent programs out of a large dataset of distractors, illustrated in Figure~\ref{fig:conceptual}. 
To solve this task, the model has to embed code semantics rather than syntax. 
In essence, we specify domain knowledge about desired invariances through code transformations. \ours{} improves robustness even under the most adversarial setting in Figure~\ref{fig:bert_motivation_robust}, and consistently improves downstream code understanding on other tasks.
The contributions of our work include:
\begin{enumerate}
    \item the novel use of compiler-based transformations as data augmentations for code,
    \item the concept of program representation learning based on functional equivalence, and
    \item a detailed analysis of architectures, code transforms and pre-training strategies, showing \ours{} improves type inference top-1 accuracy by 9\%, learned inference by 2\%--13\%, summarization F1 score by up to 8\% and clone detection AUROC by 2\%--46\%.
\end{enumerate}

\section{Related Work}
\textbf{Self-supervised learning} (SSL) is a general representation learning strategy where some dimensions or attributes of a datapoint are predicted from the remaining parts. These methods are unsupervised in the sense that they do not rely on labels, but SSL tasks often adapt losses and architectures designed for supervised learning.
Self-supervised pre-training has yielded large improvements in both NLP~\citep{nlp_howard2018universal,devlin2018bert,nlp_radford2018improving,nlp_radford2019language} and computer vision~\citep{weaksupervision_mahajan2018exploring} by improving generalization \citep{erhan2010does, hao2019visualizing}. 
Weak visual features, such as orientation~\citep{rotnet_gidaris2018unsupervised}, color~\citep{zhang2016colorful}, and context~\citep{pathak2016context}, are meaningful signals for representations~\citep{weaksupervision_mahajan2018exploring}.

\textbf{Contrastive learning} unifies many past SSL approaches that compare pairs or collections of similar and dissimilar items~\citep{hadsell2006dimensionality}. Rather than training the network to predict labels or reconstruct data, contrastive methods minimize the distance between the representations of similar examples (positives) while maximizing the distance between dissimilar examples (negatives). Examples include Siamese networks~\citep{bromley1994signature} and triplet losses~\citep{schroff2015facenet}. Contrastive predictive coding~\citep{cpcv1_oord2018representation, henaff2019data} learns to encode chunks of sequential data to predict future chunks with the InfoNCE loss, a variational lower bound on mutual information between views of the data~\citep{tian2019contrastive, wu2020mutual} inspired by noise-constrastive estimation~\citep{gutmann2010noise}. In instance discrimination tasks~\citep{wu2018unsupervised}, views and not pieces of an entire image are compared. SimCLR~\citep{chen2020simple} and Momentum Contrast~\citep{he2019momentum, chen2020improved} recently made progress by using many negatives for dense loss signal. Beyond images, InfoNCE has been applied to NLP~\citep{chuang2020debiased, giorgi2020declutr}, but may require supervision~\citep{Fang_2020}.

\textbf{Code representation learning}~~Many works apply machine learning to code~\citep{allamanis2018survey}. We address code clone detection~\cite{white2016deep}, variable type inference~\citep{hellendoorn2018deep}, and summarization~\cite{alon2018code2seq}. Others have also explored ML for summarization~\citep{movshovitz2013natural, allamanis2016convolutional, iyer2016summarizing} and type inference~\citep{pradel2019typewriter, p2020opttyper, Wei2020LambdaNet, allamanis2020typilus, bielik2020adversarial} with various languages and datasets.
The tree or graph structure of code can be exploited to encode invariances in the representation. Inst2vec~\citep{ben2018neural} locally embeds individual statements in LLVM IR by processing a contextual flow graph with a context prediction objective~\citep{mikolov2013distributed}.
Tree-Based CNN embeds the Abstract Syntax Tree (AST) nodes of high-level source code. 
Code2seq \citep{alon2018code2seq} embeds AST paths with an attention-based encoder and LSTM decoder for supervised sequence-to-sequence tasks.
\citet{cuBERT} and \citet{feng2020codebert} pre-train a Transformer~\citep{vaswani2017attention} on code using variants of the masked language modeling objective~\citep{devlin2018bert}, an instance of the cloze task~\citep{taylor1953cloze} for reconstructing corrupted tokens. Recurrent networks have also been pre-trained on code~\citep{Hussain_2020} as language models~\citep{Peters:2018, karampatsis2020scelmo}.


\section{Approach}
Understanding global program functionality is important for difficult semantic tasks. For these problems, learned representations should be similar for functionally equivalent programs and dissimilar for non-equivalent programs.
The principle of contrastive learning offers a simple approach for learning such representations if data can be organized into pairs of \textit{similar positives} and \textit{dissimilar negatives}~\cite{pmlr-v97-saunshi19a}. We use these to shape representation space, drawing positives together and pushing negatives apart. A major question remains: \textit{given an unlabeled corpus of programs, how do we identify or generate similar programs for positives?} We address this question in \S\ref{sec:data_aug} and \S\ref{sec:diverse_aug}, then introduce our learning framework in \S\ref{infonce_loss}.

\begin{table}[t]
	\resizebox{\linewidth}{!}{
    \begin{tabular}{@{}llll@{}}
    \toprule
           & \textbf{\large Code compression}          &        & \textbf{\large Identifier modification}           \\
    \cmark & Reformatting (R)                   & \cmark &  Variable renaming (VR)             \\
    \cmark & Beautification (B)                 & \cmark &  Identifier mangling (IM)           \\
    \cmark & Compression (C)                    &        & \textbf{\large Regularization}             \\ 
    \cmark & Dead-code elimination (DCE)        & \cmark & Dead-code insertion (DCI)           \\
    \cmark & Type upconversion (T)              & \cmark & Subword regularization (SW)         \\
    \cmark & Constant folding (CF)              & \xmark & Line subsampling (LS)               \\ \bottomrule \vspace{-2mm} \\
    \multicolumn{4}{l}{\cmark~= semantics-preserving transformation~~~~\xmark~= lossy transformation} \\
    \end{tabular}
    }
    \caption{We augment programs with 11 automated source-to-source compiler transformations. 10 are correct-by-construction and preserve operational semantics. More details are in Sec. \ref{sec:appendix:program_transformations}.}
    \label{tab:list_of_transformations}
\end{table} 
\subsection{Compilation as data augmentation} \label{sec:data_aug}

Modern programming languages afford great flexibility to software developers, allowing them to implement the same desired functionality in different ways. Crowdsourced datasets mined from developers, such as GitHub repositories, have many near-duplicates in terms of textual similarity~\citep{10.1145/3359591.3359735}, and are bound to contain even more functional equivalences for common tasks. Satisfiability solvers can identify these equivalent programs~\citep{10.1145/512529.512566, 10.1145/1168857.1168906}, but functional equivalence is undecidable in general~\citep{rice1953classes}. Also, formal documentation of semantics is required. Programs can instead be compared approximately using test-cases~\citep{massalin1987superoptimizer}, but this is costly and requires executing untrusted code.

Instead of searching for equivalences, we propose correct by construction data augmentation. Our insight is to apply source-to-source compiler transformations to unlabeled code to generate many variants with the same functionality.
For example, dead-code elimination (DCE) is a common compiler optimization that removes operations that leave the output of a function unchanged. While DCE preserves program functionality, \citet{wang2019coset} find that up to 12.7\% of the predictions of current supervised algorithm classification models change after DCE. Supervised datasets were insufficient to acquire the domain knowledge that DCE does not change the algorithm.

\begin{figure*}[t]
\centering
\begin{minipage}{.69\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figures/codetransform.pdf}
  \captionof{figure}{A JavaScript method from our unlabeled training set with two automatically generated semantically-equivalent programs. The method is from the StackEdit Markdown editor.}
    \label{fig:augmentation_examples}
\end{minipage}\hfill \begin{minipage}{.29\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figures/aug_histogram.pdf}
  \captionof{figure}{Histogram of the number of unique transformed variants per JavaScript method during pre-training.}
  \label{fig:augmentation_histogram}
\end{minipage}
\end{figure*} 
We unambiguously parse a particular source code sequence, e.g. \codesnip{W*x~+~b} into a tree-structured representation \codesnip{(+~(*~W~x)~b)} called an Abstract Syntax Tree (AST).
This tree is then transformed by automated traversal algorithms.
A rich body of prior programming language work explores parsing then transforming ASTs to optimize a program prior to machine code generation. If source code is emitted by the compiler rather than machine code, this is called source-to-source transformation. Source-to-source transformations are common for optimization and obfuscation purposes in dynamic languages like JavaScript. Further, if each transformation preserves code functionality, then any composition also preserves code functionality.

We leverage the Babel and Terser compiler infrastructure tools for JavaScript~\citep{babel_github,terser_github} to perform different transformations on method bodies. Example transformations are shown in Figure~\ref{fig:augmentation_examples}. Table~\ref{tab:list_of_transformations} and the supplement list all transformations, but we broadly group program transformations into three categories.

\textbf{Code compression} passes change the syntactic structure of code and perform correct-by-construction transformations such as pre-computing constant expressions at compile time. \textbf{Identifier modification} transformations substitute method and variable names with random or short tokens, masking part of the human-readable information in a program but leaving its functionality unchanged. Finally, transformations for \textbf{Regularization} improve model generalization by reducing the number of trivial positive pairs with high text overlap. The line subsampling pass in this group potentially modifies program semantics.

\subsection{Transformation dropout for view diversity} \label{sec:diverse_aug}
Computer vision datasets are often augmented with altered images like crops. Back-translations have been used as data augmentations for natural language~\cite{sennrich-etal-2016-improving}. Similarly, each compiler transformation is an augmentation to a program. Each transformation is a function , where the space of programs  is composed of both the set of valid ASTs and the set of programs in source form.

Stochastic augmentations like random crops generate many views of an image, but most of our compiler-based transformations are deterministic. To produce a diverse set of transformed programs, we randomly apply a subset of available compiler passes in a pre-specified order, applying transform  with probability . Intermediate programs are converted between AST and source form as needed for the compiler. Algorithm~\ref{alg:transformation} details our transformation dropout procedure.

Figure~\ref{fig:augmentation_histogram} measures the resulting diversity in programs. We precompute up to 20 augmentations of 1.8\million{} JavaScript methods from GitHub. Algorithm~\ref{alg:transformation} deduplicates method variants before pre-training since some transforms will leave the program unchanged. 89\% of the methods have more than one alternative after applying 20 random sequences of transformations.
The remaining methods without syntactically distinct alternatives include one-line functions that are obfuscated.
We apply subword regularization~\citep{kudo2018subword} as a final transformation to derive different tokenizations every batch, so pairs derived from the same original method will still differ.
All transformations are fast; our compiler transforms 300 functions per second on a single CPU core.

\begin{algorithm}
\small
   \caption{\textbf{Transformation dropout}: Stochastic program augmentation with two encodings (AST or source).}
   \label{alg:transformation}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Program source , transformation functions , transform probabilities , count 
   \STATE {\bfseries Returns:}  variants of 
   \STATE , a set of augmented program variants
   \FOR{\textsc{Sample} }
        \STATE 
\FOR{transform }
   		    \STATE Sample 
   		    \IF{}
       		    \STATE \textbf{if} \textsc{RequiresAST} and \textsc{IsAST} \textbf{then} 
       		    \STATE \textbf{else if} \textsc{RequiresAST} and \textsc{IsAST} \textbf{then} 
\STATE 
    	    \ENDIF{}
   		\ENDFOR{}
   		\STATE \textbf{if}  \textbf{then} 
   		\STATE 
   \ENDFOR{}
   \STATE {\bfseries return} 
\end{algorithmic}
\end{algorithm} 
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/pipeline_architecture_mlp.pdf}
    \caption{\ours{} pre-trains a neural program encoder  and transfers it to downstream tasks. \textbf{A-B.} Unlabeled programs are transformed \textbf{C.} into augmented variants. \textbf{D.} We pre-train  by maximizing similarity of projected embeddings of \textit{positive} program pairs--variants of the same program--and minimizing similarity with a queue of cached negatives. \textbf{E.} \ours{} supports any architecture for  that produces a global program embedding such as Transformers and LSTMs.  is then fine-tuned on smaller labeled datasets.}
    \label{fig:training}
\end{figure*}

\subsection{Learning an encoder with contrastive pre-training} \label{infonce_loss}
While BERT pre-trains a neural program encoder by reconstructing tokens (a generative task), we apply constrative learning to code by shaping the representation at the method level. Contrastive learning is a natural framework to induce invariances into a model by attracting positives while repelling negatives. To adapt recent contrastive learning objectives for images to code representation learning, we leverage the augmentations discussed in Section~\ref{sec:data_aug}-\ref{sec:diverse_aug} to define the positive program pairs. Dissimilar negatives are randomly sampled from other programs.
We extend the Momentum Contrast method~\citep{he2019momentum} that was designed for image representation learning. In our case, we learn a program encoder  that maps a sequence of program tokens to a single, fixed dimensional embedding. This embedding is projected with a small MLP before computing the pre-training objective.

\textbf{Pre-training objective}~~~~The contrastive objective maximizes the similarity of positives without collapsing onto a single representation. Like \citet{he2019momentum}, we use InfoNCE~\citep{cpcv1_oord2018representation}, a tractable objective that frames contrastive learning as a classification task: can the positives be identified among a batch of sampled negatives? InfoNCE computes the probability of selecting the positive (transformed program) by taking the softmax of projected embedding similarities across a batch of negatives.
Eq. \eqref{eq:constrastive_loss} shows the InfoNCE loss for instance discrimination, a function whose value is low when  is similar to the positive key embedding  and dissimilar to negative key embeddings .  is a temperature hyperparameter~\cite{wu2018unsupervised}.

The query representation  is computed by the encoder network , and  is a query program. Likewise,  using the EMA key encoder . Views  depend on the specific domain and pretext task. In our case, the views are tokenized representations of the augmented programs, and the summation  in the normalizing denominator is taken over the queue of pre-computed negatives as well as other non-matching keys in the batch.

To reduce memory consumption, we enqueue past batches to cache activations for negative samples. These cached samples are valid negatives if the queue is smaller than the dataset size. Following \citet{he2019momentum}, the query encoder  is trained via gradient descent while the key encoder  is trained slowly via an exponential moving average (EMA) of the query encoder parameters. The EMA update stabilizes the pre-computed key embeddings across training iterations. Since keys are only embedded once per epoch, we use a very large set of negatives, over , with minimal additional computational cost and no explicit hard negative mining.

\ours{} is agnostic to the architecture of the program encoder . We evaluate contrastive pre-training of 6-layer Transformer~\citep{vaswani2017attention} and 2-layer BiLSTM~\citep{schuster1997bidirectional, huang2015bidirectional} architectures, with specific details in Section~\ref{sec:experiments}.

\textbf{Transfer learning}~~~~After pre-training converges, the encoder  is transferred to downstream tasks. For code clone detection, we transfer the representation  in zero-shot, without fine-tuning. For tasks where the output space differs from the encoder, we add a task-specific MLP or Transformer decoder after , then fine-tune the resulting network end-to-end on labeled task data.

\section{Evaluation}
\label{sec:experiments}
We evaluate whether self-supervised pre-training with \ours{} improves JavaScript and TypeScript code analysis through (1) code clone detection~\citep{Baker92aprogram}, (2) extreme code summarization~\citep{allamanis2016convolutional} and (3) type inference~\citep{hellendoorn2018deep} tasks.

\begin{table*}[t]
\centering
\caption{\textbf{Zero-shot code clone detection} with cosine similarity probe. Contrastive and hybrid representations improve clone detection AUROC on unmodified (natural) HackerRank programs by  and  AUROC over a heuristic textual similarity probe, respectively, suggesting they are predictive of functionality. Contrastive representations are also the most robust to adversarial code transformations.}
\small
\label{tab:code_clone}
\begin{tabular}{lccccccHH} \toprule
& \multicolumn{2}{c}{\textbf{Natural code}} & \multicolumn{2}{c}{\textbf{Adversarial} (=4)} & \multicolumn{2}{c}{\textbf{Adversarial} (=16)} \\
 & \textbf{AUROC} & \textbf{AP} & \textbf{AUROC} & \textbf{AP} & \textbf{AUROC} & \textbf{AP} & \textbf{AUROC} & \textbf{AP} \\ \midrule
Edit distance heuristic   & 69.55\tiny{0.81} & 73.75 & 31.63\tiny{0.82} & 42.85 & 12.11\tiny{0.54} & 32.46 \\
Randomly initialized Transformer & 72.31\tiny{0.79} & 75.82 & 22.72\tiny{0.20} & 37.73 & 3.09\tiny{0.28} & 30.95 & 16.53 & 36.66 \\
~~~~+ RoBERTa MLM pre-train & 74.04\tiny{0.77} & 77.65 & 25.83\tiny{0.21} & 39.46 & 4.51\tiny{0.33} & 31.17 & 18.78 & 37.57 \\
~~~~+ \cellcolor{Gray}ContraCode pre-train & \cellcolor{Gray}75.73\tiny{0.75} & \cellcolor{Gray}78.02 & \cellcolor{Gray}\textbf{64.97}\tiny{0.24} & \cellcolor{Gray}\textbf{66.23} & \cellcolor{Gray}\textbf{58.32}\tiny{0.88} & \cellcolor{Gray}\textbf{59.66} & \textbf{62.72} & \textbf{64.06} \\
~~~~+ \cellcolor{Gray}ContraCode + RoBERTa MLM & \cellcolor{Gray}\textbf{79.39}\tiny{0.70} & \cellcolor{Gray}\textbf{81.47} & \cellcolor{Gray}37.81\tiny{0.24} & \cellcolor{Gray}51.42 & \cellcolor{Gray}10.09\tiny{0.50} & \cellcolor{Gray}32.52 & 27.52 & 44.19\\ \bottomrule
\end{tabular}
\end{table*} 
Clone detection experiments show that contrastive and hybrid representations with our compiler-based augmentations are predictive of program functionality in-the-wild, and that contrastive representations are the most robust to adversarial edits (\S\ref{sec:experiments_code_clone}).
Contrastive pre-training outperforms baseline supervised and self-supervised methods on all three tasks (\S\ref{sec:experiments_code_clone}-\ref{sec:experiments_code_summarization}). Finally, ablations suggest it is better to augment unlabeled programs during pre-training rather than augmenting smaller supervised datasets (\S\ref{sec:experiments_augmentation}).

\textbf{Experimental setup}~~~~
Models are pre-trained on CodeSearchNet, a large corpus of methods extracted from popular GitHub repositories~\citep{husain2019codesearchnet}. CodeSearchNet contains 1,843,099 JavaScript programs. Only 81,487 methods have both a documentation string and a method name. The asymmetry between labeled and unlabeled programs stems from JavaScript coding practices where anonymous functions are widespread. The pre-training dataset described in Section~\ref{sec:data_aug} is the result of augmenting all 1.8\million{} programs.

As our approach supports any encoder, we evaluate two architectures: a 2-layer Bidirectional LSTM with 18\million{} parameters, similar to the supervised model used by \citet{hellendoorn2018deep}, and a 6-layer Transformer with 23\million{} parameters.
For a baseline self-supervised approach, we pre-train both architectures with the RoBERTa MLM objective, then transfer it to downstream tasks.

\subsection{Evaluating Functionality and Robustness: Zero-shot Code Clone Detection}
\label{sec:experiments_code_clone}

\ours{} learns to match variants of programs with similar functionality. While transformations produce highly diverse token sequences~(quantified in the supplement),
they are artificial and do not change the underlying algorithm. Human programmers can solve a problem with many data structures, algorithms and programming models. Are pre-trained representations consistent across programs written by different people? We benchmark on \textit{code clone detection}, a binary classification task to detect whether two programs solve the same problem or different ones. This is useful for deduplicating, refactoring and retrieving code, as well as checking approximate code correctness.

Datasets exist like BigCloneBench~\citep{10.1109/ICSME.2014.77}, but to the best of our knowledge, there is no benchmark for the JavaScript programming language. We collected 274 in-the-wild JavaScript programs correctly solving 33 problems from the HackerRank interview preparation website. 
There are 2065 pairs solving the same problem and 70\thou{} pairs solving different problems, which we randomly subsample to 2065 to balance the classes. Since we probe zero-shot performance based on pre-trained representations, there is no training set. Instead, we threshold cosine similarity of pooled representations of the programs  and : .
Many traditional code analysis methods for clone detection measure textual similarity~\cite{Baker92aprogram}. As a baseline heuristic, we threshold the dissimilarity score, a scaled Levenshtein edit distance between normalized and tokenized programs that excludes formatting changes.

Table~\ref{tab:code_clone} reports the area under the ROC curve (AUROC) and average precision (AP, area under Precision-Recall). All continuous representations improve clone detection over the heuristic on natural code. Self-supervision through RoBERTa MLM pre-training improves over a randomly initialized network by +1.7\% AUROC. Contrastive pre-training achieves +3.4\% AUROC over the same baseline. A hybrid objective combining both the contrastive loss and MLM has the best performance with +7.0\% AUROC (+5.4\% over MLM alone). This indicates that \ours{} learns a a more useful representation of functionality than MLM, though both objectives are useful for natural code.

However, are these representations robust to code edits? We adversarially edit one program in each pair by applying the loss-maximizing code compression and identifier modification transformation among  samples from Algorithm~\ref{alg:transformation}. These transformations preserve program functionality, so ground-truth labels are unchanged. With only 4 possible edits, RoBERTa performs worse than the heuristic (-5.8\% AUROC) and worse than random guessing (50\% AUROC), indicating it is highly sensitive to these kinds of implementation details. \ours{} retains much of its performance (+39\% AUROC over RoBERTa) as it explicitly optimizes for invariance to code edits. Surprisingly, the hybrid model is less robust than \ours{} alone, perhaps indicating that MLM learns non-robust features~\cite{NEURIPS2019_e2c420d9}.

\subsection{Fine-tuning for Type Inference}
\label{sec:experiments_type_inference}
JavaScript is a dynamically typed language, where variable types are determined at runtime based on the values they represent. Manually annotating code with types helps tools flag possible bugs before runtime by detecting incompatible types. Annotations also help programmers document code. However, annotations are tedious to maintain. Type inference tools automatically predict types from context.

\begin{table}[t]
\caption{\textbf{Type inference accuracy on TypeScript programs} in the \citet{hellendoorn2018deep} dataset. \ours{} (BiLSTM) outperforms baseline top-1 accuracies by 2.28\% to 13.16\%. As \ours{} does not modify model architecture, contrastive pre-training can be combined with each baseline. Compared with TypeScript's built-in type inference, we improve accuracy by 8.9\%.}
\setlength\tabcolsep{3.5pt}
\label{tab:types}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llcc} \toprule
    \textbf{Baseline} & \textbf{Method}  & \textbf{Acc@1} & \textbf{Acc@5} \\ 
    \midrule
    \multirow{2}{*}{\shortstack[l]{Static\\analysis}} & TypeScript CheckJS \citep{bierman2014understanding} & 45.11\% & --- \\
    & Name only \citep{hellendoorn2018deep} & 28.94\% & 70.07\% \\ \midrule
    \multirow{2}{*}{Transformer} & Transformer (supervised) & 45.66\% & 80.08\% \\
    & \cellcolor{Gray}~~~~+ \ours{} pre-train & \cellcolor{Gray}\textbf{46.86\%} & \cellcolor{Gray}\textbf{81.85\%}\\\midrule
    \multirow{2}{*}{RoBERTa-6} & Transformer (RoBERTa MLM pre-train) & 40.85\% & 75.76\%\\
    & \cellcolor{Gray}~~~~+ \ours{} pre-train (hybrid) & \cellcolor{Gray}\textbf{47.16\%} & \cellcolor{Gray}\textbf{81.44\%}\\\midrule
    \multirow{4}{*}{\shortstack[l]{DeepTyper\
    \text{dissimilarity}_{D}(x_q, x_k) = \frac{D(x_q, x_k)}{\max(|x_q|, |x_k|)}
    \label{eq:dissimilarity}


Dissimilarity ranges from 0\% for programs with the same sequence of tokens, to 100\% for programs without any shared tokens.
Note that whitespace transformations do not affect the metric because the tokenizer collapses repeated whitespace. For the positives, we estimate dissimilarity by sampling one pair per source program in the CodeSearchNet dataset (1.6M source programs with at least one pair). We sample the same number of negative pairs.

Figure~\ref{fig:tokendistance} shows a histogram of token dissimilarity. Positive pairs have  mean dissimilarity, while negatives have  mean dissimilarity. Negatives are more dissimilar on average as source sequences could have different lengths, idioms and functionality. Still, the transformations generated quite different positive sequences, with less than half of their tokens shared. The 25th, median and 75th percentile dissimilarity is 59\%, 66\% and 73\% for positives, and 82\%, 87\% and 90\% for negatives.

\section{Experimental setup}
\textbf{Architectures}~~~~
The Transformer encoder has 6 layers (23M parameters) in all experiments. For code summarization experiments, we add 4 decoder layers with causal masking to generate the natural language summary.
We leverage the default positional embedding function (, ) as used in the original Transformer architecture. The network originally proposed in DeepTyper \citep{hellendoorn2018deep} had 11M parameters with a 300 dimensional hidden state. We increase the hidden state size to 512 to increase model capacity, so our BiLSTM for type prediction has 17.5M parameters.
During fine-tuning, across all experiments, we optimize parameters using Adam with linear learning rate warmup and decay. For the Transformer, the learning rate is linearly increased for 5,000 steps from 0 to a maximum of . For the bidirectional LSTM, the learning rate is increased for between 2,500 and 10,000 steps to a maximum of . Type inference hyperparameters are selected by validation top-1 accuracy.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.86\linewidth]{figures/diagrams/code_clone_example.pdf}
    \caption{Code clone detection example. These programs solve the same HackerRank coding challenge (reading and summing two integers), but use different coding conventions. The neural code clone detector should classify this pair as a positive, \textit{i.e.} a clone.}
    \label{fig:code_clone_example}
\end{figure*}

\textbf{\ours{} pre-training}~~~~The InfoNCE objective is minimized with temperature  following~\citet{he2019momentum}. Also following \citet{he2019momentum}, the key encoder's parameters are computed with the momentum update equation , equivalent to an EMA of the query encoder parameters . To pretrain a Transformer using the \ours{} objective, we first embed each token in the program using the Transformer. However, the InfoNCE objective is defined in terms of a single embedding for the full program. The \ours{} Transformer is pre-trained with a batch size of 96. Our model averages the 512-dimensional token embeddings across the sequence, then applies a two-layer MLP with 512 hidden units and a ReLU activation to extract a 128-dimensional program embedding for the loss.

The DeepTyper bidirectional LSTM architecture offers two choices for extracting a global program representation. We aggregate a 1024-dimensional global representation of the program by concatenating its four terminal hidden states (from two sequence processing directions and two stacked LSTM layers), then apply the same MLP architecture as before to extract a 128-dimensional program representation. Alternatively, we can average the hidden state concatenated from each direction across the tokens in the sequence before applying the MLP head. We refer to the hidden-state configuration as a global representation and the sequence averaging configuration as a local representation in Table~\ref{tab:mean_hidden_ablation}. We pre-train the BiLSTM with large batch size of 512 and apply weight decay.

\textbf{Code clone detection on HackerRank programs}~~~~Figure~\ref{fig:code_clone_example} shows two programs sampled from the HackerRank clone detection dataset. These programs successfully solve the same problem, so they are clones. We report metrics that treat code clone detection as a binary classification task given a pair of programs. 2065 pairs of programs solving the same HackerRank problem and 2065 pairs of programs solving different problems are sampled to construct an evaluation dataset. We use the area under the Receiver Operating Characteristic (AUROC) metric and Average Precision (AP) metrics. The standard error of the AUROC is reported according to the Wilcoxon statistic~\cite{10.5555/1089508.1089530}. Average Precision is the area under the Precision-Recall curve. AUROC and AP are both computed using the \codesnip{scikit-learn} library~\cite{scikit-learn}.

A Transformer predicts contextual embeddings of each token in a program, but our thresholded cosine similiarity classifier requires fixed length embeddings of whole programs. To determine if two programs that may differ in length are clones, we pool the token representations across the sequence. We evaluated both mean pooling and max pooling the representation. For the hybrid model pre-trained with both RoBERTa (MLM) and contrastive objectives, mean pooling achieved the best AUROC and AP. For other models, max pooling performed the best.

\textbf{Type prediction}~~~~Following DeepTyper~\citep{hellendoorn2018deep}, our regenerated dataset for type prediction has
187 training projects with 15,570 TypeScript files, totaling 6,902,642 tokens. We tune hyperparameters on a validation set of 23 distinct projects with 1,803 files and 490,335 tokens, and evaluate on a held-out test set of 24 projects with 2,206 files and 958,821. The training set is smaller than originally used in DeepTyper as several projects were made private or deleted from GitHub before May 2020 when we downloaded the data, but we used the same commit hashes for available projects so our splits are a subset of the original. We have released the data with our open-source code to facilitate further work on a stable benchmark as more repositories are deleted over time. We perform early stopping to select the number of training epochs. We train each model for 100 epochs and select the checkpoint with the minimum accuracy@1 metric (all types, including \codesnip{any}) on the validation set. Except for the model learned from scratch, the Transformer architectures are pre-trained for 240\thou{} steps. Models with the DeepTyper architecture converge faster on the pre-training tasks and are pre-trained for 20\thou{} iterations (unless otherwise noted).

\begin{figure}[t!]
    \centering
    \subfloat[Character length per code sample]{{\includegraphics[width=0.3\textwidth]{figures/code_length.pdf} }}\qquad
    \subfloat[Character length per method name]{{\includegraphics[width=0.3\textwidth]{figures/identifier_length.pdf} }}\caption{CodeSearchNet code summarization dataset statistics: (a) The majority of code sequences are under 2000 characters, but there is long tail of programs that span up to 15000 characters long, (b) JavaScript method names are relatively short compared to languages like C and Java.}
    \label{fig:dataset_statistics_identifier}
\end{figure} 
\textbf{Extreme code summarization by method name prediction}~~~~We train method prediction models using the labeled subset of CodeSearchNet. Neither method names nor docstrings are provided as input to the model: the docstring is deleted, and the method name is replaced with the token `\codesnip{x}'. Thus, the task is to predict the method name using the method body and comments alone.

To decode method names from all models except the code2vec and code2seq baselines which implement their own decoding procedures, we use a beam search with a beam of size  and a maximum target sequence length of  subword tokens. We detail the cumulative distribution of program lengths in Figure~\ref{fig:dataset_statistics_identifier}. The \ours{} summarization Transformer only needed to be pre-trained for 20\thou{} iterations, with substantially faster convergence than RoBERTa (240\thou{} iterations). During fine-tuning, we apply the LS,SW,VR,DCI augmentations to \ours{}.



\section{Baselines}
\label{sec:appendix:baseline_js}
Baselines for code summarization and type prediction trained their models on an inconsistent set of programming languages and datasets. In order to normalize the effect of datasets, we selected several diverse state-of-the-art baselines and reimplemented them on the JavaScript dataset.

\textbf{AST-based models}~~~~The authors of code2vec~\citep{alon2019code2vec} and code2seq~\citep{alon2018code2seq}, AST-based code understanding models, made both data and code available, but train their model on the Java programming language. In order to extend the results in their paper to JavaScript for comparison with our approach, we generated an AST path dataset for the CodeSearchNet dataset.
The sensitivity of path-mining embeddings to different datasets is documented in prior work, so published F1 scores are not directly comparable; F1 scores for code2vec~\citep{alon2019code2vec} vary between 19 \citep{alon2018code2seq} and 43 \citep{alon2019code2vec} depending on the dataset used. Therefore, we use the same dataset generation code as the authors for fair comparison. We first parse the source functions using the Babel compiler infrastructure. Using the original code on these ASTs, up to 300 token-to-token (leaf-to-leaf) paths are extracted from each function's AST as a precomputed dataset. Then, we generate a token and AST node vocabulary using the same author-provided code, and train the models for 20 epochs, using early stopping for code2seq. We observed that code2vec overfits after 20 epochs, and longer training was not beneficial.

\textbf{DeepTyper}~\citep{hellendoorn2018deep}~~~~DeepTyper uses a two layer GRU with a projection over possible classes, with an embedding size of 300 and hidden dimension of 650. However, we found improved performance by replacing the GRU with a bidirectional LSTM (BiLSTM). We normalize the LSTM parameter count to match our model, and therefore use a hidden dimension size of 512. We also use subword tokenization rather than space delimited tokens according to \citet{kudo2018subword}, as subwords are a key part of state-of-the-art models for NLP~\citep{sennrich2015neural}.

\textbf{RoBERTa}~~~~We pre-trained an encoder using RoBERTa's masked language modeling loss on our augmented version of CodeSearchNet, the same data used to pre-train \ours{}. This model is then fine-tuned on downstream datasets. Unlike the original BERT paper which cuBERT~\citep{cuBERT} is based on, hyperparameters from RoBERTa have been found to produce better results during pre-training. RoBERTa pre-trains using a masked language modeling (MLM) objective, where 15\% of tokens in a sentence are masked or replaced and are reconstructed by the model. We did not use the BERT Next Sentence Prediction (NSP) loss which RoBERTa finds to be unnecessary. We normalize baseline parameter count by reducing the number of Transformer layers from 24 to 6 for a total of 23M parameters.

\section{Additional results and ablations}
\label{sec:appendix_ablations}

\textbf{Code clone detection ROC and PR curves}~~~~Figure~\ref{fig:codeclone_curves} plots true postive rate vs false positive rate and precision vs recall for different zero-shot classifiers on the code clone detection downstream tasks. These classifiers threshold a similarity score given by token-level edit distance for the heuristic approach or cosine similarity for the neural network representations. The hybrid self-supervised model combining \ours{}'s contrastive objective and masked language modeling achieves better tradeoffs than the other approaches. Figure~\ref{fig:codeclone_auroc_ap} shows the AUROC and Average Precision of four Transformer models on the same task under adversarial transformations of one input program. Untrained models as well as models pre-trained with RoBERTa's MLM objective are not robust to these code transformations. However, the model pre-trained with \ours{} preserves much of its performance as the adversarial attack is strengthened.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/codeclone_plot_nonadv.pdf}
    \caption{Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves for non-adversarial classifiers on the code clone detection task. Equal F1 score curves are shown on right.}
    \label{fig:codeclone_curves}
\end{figure}

\textbf{Which part of the model should be transferred?}~~~~
SimCLR \citep{chen2020simple} proposed using a small MLP head to reduce the dimensionality of the representation used in the InfoNCE loss during pre-training, and did not transfer the MLP to the downstream image-classification task. In contrast, we find it beneficial to transfer part of the contrastive MLP head to type inference, showing a  improvement in top-5 accuracy over transferring the encoder only (Table~\ref{tab:mlp_transfer_ablation}). We believe the improvement stems from fine-tuning both the encoder and MLP which allows feature adaptation, while SimCLR trained a linear model on top of frozen features. We only transferred the MLP when contrasting the mean of token embeddings during pre-training, not the terminal hidden states, as the dimensionality of the MLP head differs. These representations are compared next.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/codeclone_auroc_ap_adv.pdf}
    \caption{Adversarial AUROC and Average Precision for four models on the code clone detection task: a randomly initialized transformer, and transformers pre-trained on code with the RoBERTa MLM objective, our contrastive objective, or both. Representations learned by the contrastive model transfer robustly.}
    \label{fig:codeclone_auroc_ap}
\end{figure}


\begin{table}
\caption{If local representations are learned, transferring part of the Contrastive MLP head improves type inference. The encoder is a 2-layer BiLSTM (d=512), with a 2-layer MLP head for both pre-training purposes and type inference. The mean hidden state representation is optimized for 10\thou{} iterations for the purposes of this ablation.}
\setlength\tabcolsep{3.5pt}
\label{tab:mlp_transfer_ablation}
\centering
\begin{tabular}{lccHH} \toprule
    \textbf{Warm-started layers} & \textbf{Acc@1} & \textbf{Acc@5} & Acc@1 & Acc@5\\
    \midrule
    BiLSTM & \textbf{49.32\%} & 80.03\% & 59.75\% & 77.98\% \\
    BiLSTM, 1 layer of MLP & 49.15\% & \textbf{82.58\%} & \textbf{60.88\%} & \textbf{79.41\%} \\
    \bottomrule 
\end{tabular}
\end{table}

\begin{table*}
\caption{Contrasting global, sequence-level representations outperforms contrasting local representations. We compare using the terminal (global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level (local) hidden states.}
\small
\setlength\tabcolsep{3.5pt}
\label{tab:mean_hidden_ablation}
\centering
\begin{tabular}{clccHH} \toprule
    \textbf{Representation} & \textbf{Optimization} & \textbf{Acc@1} & \textbf{Acc@5} & Acc@1 & Acc@5\\
    \midrule
    \multirow{2}{*}{Global} & InfoNCE with terminal hidden state, 20\thou{} steps & \textbf{52.65\%} & \textbf{84.60\%} & \textbf{63.35\%} & \textbf{79.69\%} \\
    & InfoNCE with terminal hidden state, 10\thou{} steps & 51.70\% & 83.03\% & 62.16\% & 79.56\% \\ \midrule
    Local & InfoNCE with mean token rep., 10\thou{} steps & 49.32\% & 80.03\% & 59.75\% & 77.98\% \\
    \bottomrule 
\end{tabular}
\end{table*}

\begin{table*}
\caption{Training time and decoder depth ablation on the method name prediction task. Longer pre-training significantly improves downstream performance when a shallow, 1 layer decoder is used.} 
\label{tab:summarization_depth_ablation}
\footnotesize
\setlength\tabcolsep{3.5pt}
\centering
\begin{tabular}{llccccccccccc} \toprule
    \multirow{2}{*}{Decoder} & Pre-training & Supervision & \multirow{2}{*}{Precision} & \multirow{2}{*}{Recall} & \multirow{2}{*}{F1}\\    
    & (1.8M programs) & (81k programs) & \\
    \midrule
    Transformer, 1 layer & MoCo, 10k steps & Original set & 11.91\% & 5.96\% & 7.49\% & \\
    Transformer, 1 layer & MoCo, 45k steps & Original set & \textbf{17.71\%} & \textbf{12.57\%} & \textbf{13.79\%} & \\
    Transformer, 4 layers & MoCo, 45k steps & Original set & \textbf{18.21\%} & \textbf{13.21\%} & \textbf{14.56\%} \\
     \bottomrule 
\end{tabular}
\end{table*}


\textbf{Should we pre-train global or local representations?}~~~~We compare pre-training DeepTyper with two variants of \ours{}. We either use the mean of token hidden states across the program (averaging local features), or the terminal hidden states as input to the MLP used to extract the contrastive representation  (global features). Token-level features might capture more syntactic details, but averaging pooling ignores order. Table~\ref{tab:mean_hidden_ablation} shows the accuracy of a BiLSTM pre-trained with each strategy. Using the global features for pre-training yields significantly improved performance, +2.38\% acc@1 after 10\thou{} iterations of pre-training (not converged for the purposes of ablation). The global pre-training strategy achieves our best results.

\textbf{Do pre-trained encoders help more with shallow decoders?}~~~~
For the sequence-to-sequence code summarization task, \ours{} only pre-trains the encoder of the Transformer. In Table~\ref{tab:summarization_depth_ablation}, we ablate the depth of the decoder to understand how much shallow decoders benefit from contrastive pre-training of the encoder. Similar experiments were performed in a vision context by~\cite{erhan2010does}, where different numbers of layers of a classifier are pre-trained. After 45k pre-training steps, the 4-layer decoder achieves  higher precision,  higher recall and  higher F1 score than the 1-layer model, so additional decoder depth is helpful for the downstream task. The 1-layer decoder model also benefits significantly from longer pre-training, with a  increase in F1 from 10k to 45k iterations. This large of an improvement indicates that \ours{} could be more helpful for pre-training when the number of randomly initialized parameters at the start of fine-tuning is small. For larger decoders, more parameters must be optimized during-finetuning, and the value of pre-training is diminished.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/LossPlot_crop.pdf}
    \caption{Pre-training quickly converges if negative programs in the queue are frequently changed.}
    \label{fig:constrastive_pretraining_accuracy}
\end{figure}
 
\textbf{Contrastive representation learning strategies}~~~~In Figure~\ref{fig:constrastive_pretraining_accuracy}, we compare two strategies of 
refreshing the MoCo queue of key embeddings (the dictionary of negative program representations assumed to be non-equivalent to the batch of positives). 
In the first strategy, we add 8 items out of the batch to the queue (1), while in the second we add 96 items (12). In addition, we use a larger queue (65k versus 125k keys) and a slightly larger batch size (64 versus 96).
We observe that for the baseline queue fill rate, the accuracy decreases for the first 8125 iterations as the queue fills. This decrease in accuracy is expected as the task becomes more difficult due to the increasing number of negatives during queue warmup. However, it is surprising that accuracy grows so slowly once the queue is filled. 
We suspect this is because the key encoder changes significantly over thousands of iterations: with a momentum term , the original key encoder parameters are decayed by a factor of  by the moving average. If the queue is rapidly refreshed, queue embeddings are predicted by recent key encoders, not old parameters. This also indicates that a large diversity of negative, non-equivalent programs are helpful for rapid convergence of \ours{} pre-training.


\section{Qualitative results}
\label{sec:qualitative}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/type_qualitative_vert.pdf}
    \caption{Our model, a variant of DeepTyper pretrained with \ours{}, generates type annotations for two programs in the held-out set. The model consistently predicts the correct return type of functions, and even predicts project-specific types imported at the top of the file. The model corresponds to the top row of Table~\ref{tab:mean_hidden_ablation}, though is not our best performing model.}
    \label{fig:qual_types}
\end{figure*}
 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/tsne_comparison.pdf}
    \caption{t-SNE~\citep{tsne2008} plot of mean pooled program representations learned with masked language modeling (RoBERTa), contrastive learning (\ours{}), and a hybrid loss (RoBERTa + \ours{}). Transformed variants of the same program share the same color. Note that colors may be similar across different programs.}\label{fig:tsne}
\end{figure}
 \begin{figure}
\begin{minipage}{0.5\linewidth}
\begin{footnotesize}
\begin{lstlisting}[language=JavaScript,basicstyle=\ttfamily\tiny]
function x(url, callback, error) {
  var img = new Image();
  img.src = url;
  if(img.complete){
    return callback(img);
  }
  img.onload = function(){
    img.onload = null;
    callback(img);
  };
  img.onerror = function(e){
    img.onerror = null;
    error(e);
  };
}	
\end{lstlisting}
\end{footnotesize}
\end{minipage}\hfill \begin{minipage}{0.5\linewidth}
\begin{small}
\textit{Ground truth}: \texttt{loadImage}\\
\textit{Prediction:} {\color{ForestGreen} \texttt{loadImage} }\\

Other predictions:
{\tiny
\begin{enumerate}
    \item \texttt{getImageItem}
    \item \texttt{createImage}
    \item \texttt{loadImageForBreakpoint}
    \item \texttt{getImageSrcCSS}
\end{enumerate}
}
\end{small}
\end{minipage}
\caption{A JavaScript program from the CodeSearchNet dataset not seen during training and the predicted method names from a Transformer pre-trained with \ours{}. \ours{} predicts the correct method name as its most likely decoding.}
\label{fig:qual_method_name}
\end{figure} 
\paragraph{t-SNE visualization of representations} We qualitatively inspect the structure of the learned representation space by visualizing self-supervised representations of variants of 28 programs using t-SNE~\citep{tsne2008} in Figure~\ref{fig:tsne}. Representations of transformed variants of the same program are plotted with the same color. \ours{} (BiLSTM) clusters variants closely together. Indeed, contrastive learning learns representations that are invariant to a wide class of automated compiler-based transformations. In comparison, the representations learned by masked language modeling (RoBERTa) show more overlap between different programs, and variants do not cleanly cluster. With a hybrid loss combining masked language modeling and contrastive learning, representations of variants of the same program once again cluster.

\paragraph{Code summaries} Figure~\ref{fig:qual_method_name} shows a qualitative example of predictions for the code summarization task. The JavaScript method is not seen during training. A Transformer pre-trained with \ours{} predicts the correct method name through beam search. The next four predictions are reasonable, capturing that the method processes an image. The 2nd and 3rd most likely decodings, \codesnip{getImageItem} and \codesnip{createImage}, use \codesnip{get} and \codesnip{create} as synonyms for \codesnip{load}, though the final two unlikely decodings include terms not in the method body.

\paragraph{Type inferences} We can also visualize outputs of the type inference model. Figure~\ref{fig:qual_types} shows two TypeScript programs from the held-out test set. User-provided type annotations are removed from the programs, and the model is provided with a tokenized form without access to dependencies. We visualize predictions from a variant of DeepTyper pre-trained with \ours{}. This corresponds to the best-performing model in Table~\ref{tab:mean_hidden_ablation}.

In the first program, our model consistently predicts the correct return and parameter type. While a tool based on static analysis could infer the \codesnip{void} return types, the type of the \codesnip{message} argument is ambiguous without access to the imported \codesnip{write} method signature. Still, the model correctly predicts with high confidence that the variable \codesnip{message} is a string. In the second program, \ours{} correctly predicts 4 of 8 types including the \codesnip{ViewContainerRef} and \codesnip{ChangeDetectorRef} types, each imported from the AngularJS library. As this sample is held-out from the training set, these predictions show generalization from other repositories using AngularJS.
 

\end{document}