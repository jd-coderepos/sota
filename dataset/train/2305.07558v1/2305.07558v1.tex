\section{Experimental Setup} \label{app:exp_setup}

In this section, we provide further details on the experimental setups that we used for our studies.

\begin{table*}[t!]
    \setlength{\tabcolsep}{3.5pt}
    \small
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{llc|lrrr}
\toprule
\multicolumn{3}{c}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Data}} \\
Name & ViT & Img Res & Datasets & \# Img & \# Cap & \# Ann \\
\midrule
\albefb         & DeiT-B/16 & 256$\times$256 & 4M: COCO+SBU+VG+CC$_\text{3M}$ & 4.0M & 5.1M & - \\
\albefl         & DeiT-B/16 & 256$\times$256 & 14M: 4M + CC$_\text{12M}$ & 14.1M & 15.2M & - \\
\midrule
\blipl          & ViT-B/16 & 224$\times$224 & \capfiltb{}(14M) & 14.1M & 15.2M & - \\
\blipxl         & ViT-B/16 & 224$\times$224 & \capfiltb{}(14M + LAION) & 129.1M & 130.2M & - \\
\blipxlfilt     & ViT-B/16 & 224$\times$224 & \capfiltl{}(14M + LAION) & 129.1M & 130.2M & -  \\
\blipvitxl      & ViT-L/16 & 224$\times$224 & \capfiltl{}(14M + LAION) & 129.1M & 130.2M & - \\
\midrule
\pevlpre        & \albefl & 256$\times$256 & 14M$\rightarrow$RefCOCO\{,+,g\}+F30KE+GQA+VCR+VG & 14.4M & 15.2M & 4.7M \\
\pevlgrd        & \pevlpre & 512$\times$512 & \pevlpre$\rightarrow$RefCOCO\{,+,g\}+F30KE & 14.4M & 15.2M & 4.7M \\
\pevlvrd        & \pevlpre & 512$\times$512 & \pevlpre$\rightarrow$VG & 14.4M & 15.2M & 6.2M \\
\midrule
\xvlmb          & Swin-B/32 & 224$\times$224 & 4M & 4.0M & 5.1M & 6.2M \\
\xvlml          & Swin-B/32 & 224$\times$224 & 14M + Objects365 + OpenImages & 17.4M & 16.2M & 12.4M \\
\bottomrule
\end{tabular}     }
    \caption{Overview of core evaluated models. All the models cross-attend to visual features, and use contrastive learning (CL) and a (masked) language modelling objective. Fine-grained models also predict object locations. Unsupervised pretraining data includes COCO~\cite{coco}, SBU~\cite{sbu}, VG~\citep{visual_genome}, CC$_\text{3M}$~\cite{cc3m}, CC$_\text{12M}$~\cite{cc12m} and LAION~\cite{laion}. Supervised data additionally includes RefCOCO and RefCOCO+~\cite{refcoco}, RefCOCOg~\cite{refcocog}, F30KE~\cite{f30k_entities}, GQA~\cite{gqa}, VCR~\cite{vcr}, Objects365~\cite{o365} and OpenImages~\cite{openimages}. In~\cref{tab:models}, we also list downstream performance on VQAv2~\cite{vqav2}, NLVR2~\cite{nlvr2} and RefCOCO+.}
    \label{tab:models_app}
\end{table*}

\subsection{Evaluated Models: Details} \label{app:models}
We provide more details on the models we use to evaluate progress in fine-grained \vl understanding.
See \cref{tab:models_app} for an overview.\footnote{Each model's text and multimodal layers were originally initialised with the weights of BERT$_\text{BASE}$~\cite{bert}.}

\paragraph{}
\noindent\textbf{ALBEF}~\cite{albef} is a recent VLM that has gained popularity due to its design choices, effectively combining core components in \vl learning, such as a contrastive objective and cross-attention, that result in strong downstream performance.
\albef is a dual-stream encoder~\cite{unmasked} that first encodes images and captions independently with a vision~(ViT;~\citealt{vit,deit}) and text~(BERT;~\citealt{bert}) Transformer, respectively; and then fuses them in a cross-modal Transformer.
The model is pretrained with three objectives: masked language modelling (MLM), unimodal image--text contrastive learning and cross-modal image--text matching.
We refer to the original work for more details.
While \albef does not explicitly train for fine-grained understanding, it serves as an important baseline since our three other models build on top of it.

\paragraph{}
\noindent \textbf{BLIP}~\cite{blip} is a unified \vl understanding and generation model, that can be applied to a wide range of downstream tasks.
A key component to \blip's success is \capfilt: a dataset boostrapping method which the authors use to generate synthetic captions and removing noisy pairs from large-scale Web data.
Moreover, unlike any other model we evaluate, \blip uses an autoregressive language modelling (LM) objective to convert visual information into coherent captions, allowing us to evaluate the potential benefits of this objective to learn fine-grained relationships.
\blip is not explicitly trained for fine-grained understanding, however, we believe it is important to assess whether generative language modelling and its data contributions that enhance downstream performance also lead to better fine-grained skills.

\paragraph{}
\noindent\textbf{PEVL}~\cite{pevl} explicitly connects image regions and text tokens through cross-modal position modelling.
Similar to Pix2Seq~\cite{pix2seq_mtl}, \pevl expresses visual positions in text by appending the bounding box coordinates corresponding to a given (annotated) entity in the caption, surrounded by two special tokens `\texttt{<}' and `\texttt{>}': ``\texttt{A cat < 10 73 206 175 > is napping.}''
The bounding box coordinates are discretised and added to the text vocabulary.
Starting from an \albefl checkpoint, \pevl is pretrained by recovering masked text and position tokens through a generalised MLM objective.
The model was trained on a diverse corpus of referring expressions, captions with visual coreferences, question answering, commonsense reasoning, object detection and region descriptions data~(\cref{tab:models}).
Unlike \albef, \pevl is explicitly trained to learn fine-grained, grounded representations of entities by predicting their coordinates in a unified MLM framework.
We evaluate three different models released by the authors, which differ in their pretraining and fine-tuning data: \pevlpre, underwent a second-stage pretraining on multiple supervised tasks (\cref{tab:models_app}); \pevlgrd, which was further fine-tuned for position-output tasks such as phrase grounding~\cite{f30k_entities}; and \pevlvrd, which was fine-tuned for the position-input task of visual relation detection~\cite{visual_genome}.

\paragraph{}
\noindent\textbf{X-VLM}~\cite{x-vlm} also aims at learning to locate visual concepts in the image given the associated texts.
Similar to the \albef architecture, the model consists of an image encoder, a text encoder, and a cross-modal encoder. 
However, unlike \pevl, \xvlm models visual position through an additional bounding box prediction head: given the visually grounded representation of an object label, the model is trained to regress the object's bounding box (bbox) coordinates.
The authors use both object detection labels and region descriptions to learn multi-grained alignments.
The pretraining objective is a linear combination of this bbox loss and the losses defined in \albef to align texts and visual concepts (for more details, see \cref{sec:x-vlm}).

\paragraph{}
In addition to the above models, which we extensively discuss, we also evaluate the following models, based on dual-encoder and frozen LLMs.

\paragraph{}
\noindent\textbf{CLIP}~\cite{clip} is a widely used dual-encoder network.
The model consists of two encoders, one for images and one for text, trained to represent both modalities in a joint space via an unsupervised contrastive objectives over more than 400M image--text pairs from the Web.
Due to its simplicity and wide adoption, we report its performance as a strong, representative baseline.

\paragraph{}
\noindent\textbf{ClipCap}~\cite{clipcap} is an autoregressive encoder--decoder network.
The image encoder is a pretrained CLIP model, while the text decoder is a pretrained GPT-2~\cite{gpt2} language model.
The authors propose to learn a lightweight Transformer-based network to map CLIP embeddings into a fixed length prefix.
The mapping network and the text decoder are fine-tuned to learn how to generate captions, while the CLIP image encoder is frozen.
At inference time, the model generates the caption word after word, starting from the CLIP-based prefix.
We report performance for the two released versions---one fine-tuned on COCO, the other on CC$_\text{3M}$---by ranking positive and negative samples on their likelihood.

\paragraph{}
\noindent\textbf{Flamingo}~\cite{alayrac2022flamingo} is a state-of-the-art VLM capable of tackling a wide range of vision and language tasks from a few input/output examples.
To achieve this, the model relies on a pretrained \clip-like image encoder and a strong pretrained LLM~\citep{chinchilla}, both kept frozen.
To ingest images and videos, the model learns a small fixed number of visual tokens~\citep{set_transformer,perceiver}.
The model is pretrained to generate text from a sequence of text tokens interleaved with images and/or videos.

\paragraph{}
\noindent\textbf{BLIP-2}~\cite{blip2} is the most recent, state-of-the-art VLM based on frozen large image encoders and frozen LLMs~\citep{opt,flan_t5}.
Like \clipcap, \bliptwo learns a mapping network, which in this case is a Transformer model initialised from BERT$_\text{BASE}$.
The mapping network learns visual query tokens to map the visual representations to the frozen LLM in two stages: a \vl representation stage, and a generative learning stage.
The model was pretrained with the same objectives and on the same 129M image--caption data as \blip.
Following the authors' setup for image--text retrieval and matching, we use the \bliptwo model after the first-state pretraining.

\begin{table*}[t!]
    \setlength{\tabcolsep}{2pt}
    \small
    \centering
    \resizebox{\linewidth}{!}{
        \hypersetup{citecolor=lightgray}
\begin{tabular}{rlr|cccccc|cccc}
\toprule
& \multicolumn{2}{c|}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{SVO-Probes}} & \multicolumn{1}{c}{\textbf{VALSE}} & \multicolumn{1}{c}{\textbf{VSR Random}} & \multicolumn{3}{c|}{\textbf{Winoground}} & \multicolumn{2}{c}{\textbf{Flickr30K}} & \multicolumn{2}{c}{\textbf{COCO}} \\
& Name & \multicolumn{1}{c|}{Size} & Avg. & Avg. & Test Avg. & Text & Image & Group &  TR@1    & IR@1   & TR@1   & IR@1  \\
\midrule
\multicolumn{1}{r|}{} & \gray{Random} & \gray{} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{25.0} & \gray{25.0} & \gray{12.5} & \gray{~~0.1} & \gray{~~0.1} & \gray{0.02} & \gray{0.02} \\\midrule
\multicolumn{1}{r|}{} & \gray{LXMERT} & \gray{263M} & \gray{-} & \gray{59.6} & \gray{72.5$^\dagger$} & \gray{19.2} & \gray{~~7.0} & \gray{~~4.0} & \gray{-} & \gray{-} & \gray{-} & \gray{-} \\
\multicolumn{1}{r|}{} & \gray{UNITER$_\text{Large}$} & \gray{303M} & \gray{-} & \gray{-} & \gray{-} & \gray{38.0} & \gray{14.0} & \gray{10.5} & \gray{80.7} & \gray{66.2} & \gray{64.1} & \gray{48.8} \\
\multicolumn{1}{r|}{} & \gray{12-in-1}  & \gray{270M}               & \gray{-} & \gray{75.1} & \gray{-} & \gray{-} & \gray{-} & \gray{-} & \gray{-} & \gray{67.8$^\dagger$} & \gray{-} & \gray{68.0$^\dagger$} \\
\multicolumn{1}{r|}{} & \gray{\clip (ViT-B/32)}  & \gray{151M}                   & \gray{81.6} & \gray{64.0} & \gray{N/A} & \gray{30.7} & \gray{10.5} & \gray{~~8.0} & \gray{88.0} & \gray{68.7} & \gray{58.4} & \gray{37.8} \\
\multicolumn{1}{r|}{} & \gray{\clipcap{}$_\text{CC3M}$} & \gray{295M} & \gray{83.1} & \gray{65.7} & \gray{N/A} & \gray{12.2} & \gray{14.7} & \gray{~~5.5} & \gray{26.4} & \gray{44.1} & \gray{~~6.7} & \gray{24.3} \\
\multicolumn{1}{r|}{} & \gray{\clipcap{}$_\text{COCO}$} & \gray{295M} & \gray{84.1} & \gray{68.5} & \gray{N/A} & \gray{12.2} & \gray{14.7} & \gray{~~5.5} & \gray{27.8} & \gray{52.2} & \gray{~~8.1} & \gray{38.4} \\
\multicolumn{1}{r|}{} & \gray{\flamingo} & \gray{80B} & \gray{88.4} & \gray{\bf 75.3} & \gray{N/A} & \gray{-} & \gray{-} & \gray{-} & \gray{-} & \gray{-} & \gray{-} & \gray{-} \\
\multicolumn{1}{r|}{} & \gray{\bliptwo} & \gray{1.2B} & \gray{86.5} & \gray{74.0} & \gray{61.5} & \gray{43.0} & \gray{22.0} & \gray{18.2} & \gray{\textbf{95.5}} & \gray{\textbf{86.7}} & \gray{\textbf{80.7}} & \gray{\textbf{64.2}} \\

\midrule
\multicolumn{1}{r|}{1} & \albefb  & 500M        & 87.6 & 69.1 & 57.3 & 29.2 & 15.5 & 11.0 & 85.2 & 69.4 & 69.7 & 51.1 \\
\multicolumn{1}{r|}{2} & \xvlmb{}$^\sharp$  & 239M        & 88.9 & 72.4 & 63.0 & 44.0 & \textbf{26.7} & \textbf{21.5} & 85.3 & 71.9 & 70.8 & 55.6 \\
\midrule
\multicolumn{1}{r|}{3} & \albefl  & 500M        & 88.6 & 69.4 & 58.3 & 32.5 & 16.2 & 12.7 & 90.9 & 75.9 & 73.2 & 54.8 \\
\multicolumn{1}{r|}{4} & \blipl  & 638M         & 48.7 & 67.8 & 49.7 & 36.5 & 18.5 & 14.5 & 82.6 & 78.4 & 70.4 & 57.3 \\
\multicolumn{1}{r|}{5} & \pevlpre{}$^\sharp$  & 500M       & 86.2 & 68.9 & 57.5 & 33.2 & 15.7 & 12.2 & 74.9 & 60.0 & 45.9 & 33.2 \\
\multicolumn{1}{r|}{6} & \pevlgrd{}$^\sharp$  & 502M       & 88.5 & 69.5 & 57.7 & 36.2 & 15.0 & 12.0 & 71.8 & 77.6 & 42.8 & 37.7 \\
\multicolumn{1}{r|}{7} & \pevlvrd{}$^\sharp$  & 502M       & 84.8 & 64.5 & 59.5 & 31.2 & 12.0 & ~~7.5 & 68.0 & 55.7 & 38.3 & 30.6 \\
\multicolumn{1}{r|}{8} & \xvlml{}$^\sharp$  & 239M         & \textbf{90.0} & {74.5} & \textbf{64.3} & \textbf{46.7} & 24.5 & 21.2 & 87.7 & 74.9 & 71.6 & 56.1\\
\midrule
\multicolumn{1}{r|}{9} & \blipxl  & 638M         & 51.4 & 68.8 & 46.9 & 35.5 & 15.0 & 11.7 & 90.2 & 79.5 & 71.9 & 58.6 \\
\multicolumn{1}{r|}{10} & \blipxlfilt  & 638M    & 51.2 & 68.2 & 48.7 & 34.7 & 15.2 & 12.2 & 89.1 & 79.7 & 72.2 & 57.8 \\
\multicolumn{1}{r|}{11} & \blipvitxl  & 1.1B     & 50.8 & 70.3 & 50.3 & 34.7 & 14.5 & 12.2 & 90.4 & 80.6 & 74.2 & 59.3 \\
\bottomrule
\end{tabular}     }
    \caption{Overall performance of our evaluated models on fine-grained benchmarks and zero-shot retrieval tasks. The overall best values for each task are marked in \textbf{bold}. $^\sharp$ marks the fine-grained models. $^\dagger$ denotes performance after task fine-tuning. \xvlm significantly outperforms the other models that we evaluate on fine-grained tasks.}
    \label{tab:overall_baselines_app}
\end{table*}

\begin{table*}[t!]
    \setlength{\tabcolsep}{3.5pt}
    \small
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ l c| c| ccc| c| cc| cc| c| c }
        \toprule
\multirow{2}{*}{\bf Model} &
        \multicolumn{1}{c}{\bf Existence} & \multicolumn{1}{c}{\bf Plurality} & \multicolumn{3}{c}{\bf Counting} & \multicolumn{1}{c}{\bf Sp.rel.$\ddagger$} & \multicolumn{2}{c}{\bf Action} & \multicolumn{2}{c}{\bf Coreference} & \multicolumn{1}{c}{\multirow{2}{*}{\bf Foil-it!}} & \multicolumn{1}{c}{\multirow{2}{*}{\bf Avg.}} \\
        & \multicolumn{1}{c|}{quantifiers} & \multicolumn{1}{c|}{number} & \multicolumn{1}{c}{balanced} & \multicolumn{1}{c}{sns.$\dagger$} & \multicolumn{1}{c|}{adv.$\dagger$} & \multicolumn{1}{c|}{relations} & \multicolumn{1}{c}{repl.$\dagger$} & \multicolumn{1}{c|}{actant swap} & \multicolumn{1}{c}{standard} & \multicolumn{1}{c|}{clean} &
        \multicolumn{1}{c|}{}\\ 
        \midrule
        \multicolumn{1}{l}{\gray{Random}} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} \\
        \midrule
        \multirow{1}{*}{\gray{GPT-2}} & \gray{58.0} & \gray{51.9} & \gray{51.6} & \gray{49.8} & \gray{45.3} & \gray{75.0} & \gray{66.8} & \gray{76.9} & \gray{54.5} & \gray{50.0} & \gray{80.7} & \gray{60.1} \\
        \midrule
        \multirow{1}{*}{\gray{CLIP}} & \gray{66.9} & \gray{56.2} & \gray{62.1} & \gray{62.5} & \gray{57.5} & \gray{64.3} & \gray{75.6} & \gray{68.6} & \gray{52.1} & \gray{49.7} & \gray{88.8} & \gray{64.0} \\
        \multirow{1}{*}{\gray{LXMERT}} & \gray{78.6} & \gray{64.4} & \gray{62.2} & \gray{69.2} & \gray{42.6} & \gray{60.2} & \gray{54.8} & \gray{45.8} & \gray{46.8} & \gray{44.2} & \gray{87.1} & \gray{59.6} \\
        \multirow{1}{*}{\gray{12-in-1}} & \gray{\bf 95.6} & \gray{72.4} & \gray{\bf 76.7} & \gray{\bf 80.2} & \gray{77.3} & \gray{67.7} & \gray{65.9} & \gray{58.9} & \gray{\bf 75.7} & \gray{\bf 69.2} & \gray{86.9} & \gray{75.1} \\
        \gray{\clipcap{}$_\text{CC3M}$} & \gray{66.3} & \gray{54.8} & \gray{49.4} & \gray{50.1} & \gray{51.5} & \gray{83.2} & \gray{75.5} & \gray{87.9} & \gray{45.1} & \gray{45.2} & \gray{94.7} & \gray{65.7} \\
        \gray{\clipcap{}$_\text{COCO}$} & \gray{74.9} & \gray{60.6} & \gray{55.0} & \gray{53.0} & \gray{53.0} & \gray{\textbf{89.7}} & \gray{71.0} & \gray{86.5} & \gray{47.5} & \gray{49.0} & \gray{\textbf{97.1}} & \gray{68.5} \\
        \gray{\flamingo}   & \gray{63.6} & \gray{59.8} & \gray{58.2} & \gray{55.2} & \gray{\bf 80.2} & \gray{\bf 89.7} & \gray{\bf 86.7} & \gray{\bf 92.8} & \gray{72.2} & \gray{65.4} & \gray{97.0} & \gray{\bf 75.3} \\
        \gray{\bliptwo}    & \gray{83.6} & \gray{\textbf{79.6}} & \gray{70.2} & \gray{68.7} & \gray{68.0} & \gray{65.6} & \gray{84.4} & \gray{63.2} & \gray{62.6} & \gray{58.7} & \gray{96.0} & \gray{74.0} \\
        \midrule
        \albefb         & 71.3 & 78.8 & 62.2 & 65.1 & 59.8 & 73.1 & 73.6 & 58.4 & 52.4 & 55.8 & 95.5 & 69.1 \\
        \xvlmb          & 80.0 & 77.8 & 69.0 & 68.4 & 72.5 & 74.8 & 77.3 & 65.0 & 50.1 & {48.1} & 92.5 & 72.4 \\
        \midrule
        \albefl         & 69.5 & 76.0 & 61.5 & 61.0 & 64.5 & 70.7 & 77.6 & 60.5 & 55.9 & 61.5 & 96.1 & 69.4 \\
        \blipl          & 82.4 & 73.8 & 61.8 & 62.6 & 63.7 & 65.2 & 74.7 & 55.2 & 52.3 & {42.3} & 92.3 & 67.8 \\
        \pevlpre        & 89.7 & 65.5 & 66.0 & 66.2 & 57.3 & 67.9 & 73.5 & 59.4 & 58.2 & 56.7 & 90.9 & 68.9 \\
        \pevlgrd        & 91.1 & 63.9 & 70.0 & 70.9 & 63.2 & 62.4 & 74.4 & 57.1 & 53.8 & {49.0} & 92.6 & 69.5 \\
        \pevlvrd        & 83.8 & 61.8 & 62.8 & 70.3 & {40.4} & 64.5 & 68.1 & 53.2 & {47.7} & {42.3} & 94.1 & 64.5 \\
        \xvlml          & 83.6 & 78.7 & 71.5 & 72.0 & 74.8 & 73.1 & 79.2 & 64.6 & 60.0 & {49.0} & 91.9 & 74.5 \\
        \midrule
        \blipxl         & 78.2 & 75.9 & 63.4 & 63.4 & 58.5 & 66.2 & 75.2 & 59.0 & 56.4 & 52.9 & 93.2 & 68.8 \\
        \blipxlfilt     & 75.4 & 75.0 & 64.7 & 68.8 & 53.0 & 66.7 & 73.0 & 60.6 & {48.2} & 51.0 & 93.8 & 68.2 \\
        \blipvitxl      & 73.3 & 77.7 & 68.2 & 67.6 & 61.2 & 71.8 & 75.3 & 60.8 & 51.1 & {45.2} & 96.1 & 70.3 \\
        \bottomrule
    \end{tabular}



         }
    \vspace{-2mm}
    \caption{Performance on the VALSE benchmark according to pairwise ranking accuracy. Best results are in \textbf{bold}.\\
    $\dagger${\bf sns.} Counting small numbers. {\bf adv.} Counting adversarial. {\bf repl.} Action replacement. $\ddagger$ {\bf Sp.rel.} Spatial relations.}
    \label{tab:valse}
\end{table*}


\begin{table*}[t!]
    \setlength{\tabcolsep}{3pt}
    \small
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lrrr|rrr|rrr|rrr|rrr}
\toprule
{\bf Model} & \multicolumn{3}{c}{{\bf Object}} & \multicolumn{3}{c}{{\bf Relation}} & \multicolumn{3}{c}{{\bf Both}} & \multicolumn{3}{c}{{\bf 1 Main Pred}} & \multicolumn{3}{c}{{\bf 2 Main Preds}} \\
& Text & Image & Group & Text & Image & Group & Text & Image & Group & Text & Image & Group & Text & Image & Group \\
\midrule
 \gray{Random}                  & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} \\
 \gray{MTurk Human}                  & \gray{92.20} & \gray{90.78} & \gray{88.65} & \gray{89.27} & \gray{90.56} & \gray{86.70} & \gray{76.92} & \gray{57.69} & \gray{57.69} & \gray{87.33} & \gray{85.62} & \gray{82.53} & \gray{95.37} & \gray{96.30} & \gray{93.52} \\
\midrule
 \gray{LXMERT}                       & \gray{22.70}          & \gray{9.22}           & \gray{6.38}           & \gray{17.60}          & \gray{5.58}           & \gray{2.58}           & \gray{15.38}          & \gray{7.69}           & \gray{3.85}           & \gray{19.18}          & \gray{8.56}           & \gray{5.14}           & \gray{19.44}          & \gray{2.78}           & \gray{0.93}           \\
 \gray{UNITER$_\text{Large}$}             & \gray{39.01} & \gray{12.77}          & \gray{9.93}           & \gray{36.05} & \gray{14.16}          & \gray{9.87}           & \gray{50.00} & \gray{19.23}          & \gray{19.23} & \gray{40.07} & \gray{16.44}          & \gray{13.36}          & \gray{32.41} & \gray{7.41}           & \gray{2.78}           \\
 \gray{\clip (ViT-B/32)}              & \gray{34.75} & \gray{7.80}           & \gray{6.38}           & \gray{22.75}          & \gray{8.58}           & \gray{5.58}           & \gray{\bf 80.77} & \gray{\bf 42.31} & \gray{\bf 38.46} & \gray{35.27} & \gray{13.01}          & \gray{10.27}          & \gray{18.52}          & \gray{3.70}           & \gray{1.85}           \\
 \gray{\clipcap{}$_\text{CC3M}$}               & \gray{14.18}          & \gray{17.02}          & \gray{7.80}           & \gray{11.16}          & \gray{12.02}          & \gray{3.43}           & \gray{11.54}          & \gray{26.92} & \gray{11.54}          & \gray{13.70}          & \gray{16.10}          & \gray{6.51}           & \gray{8.33}           & \gray{11.11}          & \gray{2.78}           \\
 \gray{\clipcap{}$_\text{COCO}$}             & \gray{12.77}          & \gray{17.02}          & \gray{5.67}           & \gray{12.88}          & \gray{9.87}           & \gray{3.86}           & \gray{23.08}          & \gray{34.62} & \gray{19.23} & \gray{14.73}          & \gray{16.44}          & \gray{6.85}           & \gray{10.19}          & \gray{7.41}           & \gray{1.85}           \\
 \gray{\bliptwo}               & \gray{47.52} & \gray{\bf 27.66} & \gray{\bf 21.99} & \gray{38.20} & \gray{17.60}          & \gray{14.59}          & \gray{61.54} & \gray{30.77} & \gray{30.77} & \gray{48.63} & \gray{26.37} & \gray{22.26} & \gray{27.78} & \gray{10.19}          & \gray{7.41}           \\
 \midrule
 \albefb                & {29.79} & 12.77          & 8.51           & {26.61} & 15.02          & 10.73          & {50.00} & {34.62} & {26.92} & {33.22} & 19.18          & 14.04          & 18.52          & 5.56           & 2.78           \\
 \xvlmb                 & {46.10} & {\bf 27.66} & {\bf 21.99} & {41.63} & {\bf 24.46}          & {19.31} & {53.85} & {\bf 42.31} & {\bf 38.46} & {47.60} & {\bf 30.48} & {25.68} & {34.26} & {\bf 16.67}          & {\bf 10.19}          \\
 \midrule
 \albefl                & {29.79} & 15.60          & 9.22           & {30.90} & 14.16          & 12.02          & {61.54} & {38.46} & {\bf 38.46} & {35.27} & 18.49          & 14.38          & 25.00          & 10.19          & 8.33           \\
 \blipl              & {41.13} & 24.11          & {17.73} & {32.19} & 14.16          & 11.16          & {50.00} & {26.92} & {26.92} & {42.12} & 21.92          & {18.15} & 21.30          & 9.26           & 4.63           \\
 \pevlpre  & {31.21} & 14.89          & 10.64          & {33.48} & 14.59          & 11.59          & {42.31} & {30.77} & {26.92} & {36.30} & 19.52          & 15.75          & 25.00          & 5.56           & 2.78           \\
 \pevlgrd & {39.01} & 14.89          & 12.77          & {33.91} & 13.73          & 10.30          & {42.31} & {26.92} & {23.08} & {37.67} & 17.47          & 15.07          & {32.41} & 8.33           & 3.70           \\
 \pevlvrd       & {26.95} & 10.64          & 7.09           & {32.19} & 12.45          & 6.87           & {46.15} & 15.38          & 15.38          & {31.85} & 11.64          & 8.22           & {29.63} & 12.96          & 5.56           \\

 \xvlml                & {\bf 48.23} & 23.40          & {19.86} & {\bf 44.21} & 23.18          & {\bf 20.17} & {61.54} & {\bf 42.31} & {\bf 38.46} & {\bf 51.03} & {29.11} & {\bf 26.03} & {\bf 35.19} & 12.04          & 8.33           \\
 \midrule
 \blipxl             & {37.59} & 17.02          & 10.64          & {34.76} & 12.02          & 10.73          & {30.77} & {30.77} & {26.92} & {40.07} & 18.84          & 14.73          & 23.15          & 4.63           & 3.70           \\
 \blipxlfilt & {34.04} & 16.31          & 11.35          & {33.48} & 13.30          & 11.16          & {50.00} & {26.92} & {26.92} & {38.70} & 19.18          & 15.41          & 24.07          & 4.63           & 3.70           \\
 \blipvitxl       & {35.46} & 16.31          & 13.48          & {32.62} & 12.88          & 11.59          & {50.00} & 19.23          & 11.54          & {39.04} & 17.81          & 15.07          & 23.15          & 5.56           & 4.63           \\
\bottomrule
\end{tabular}
     }
    \vspace{-2mm}
    \caption{Results on Winoground by linguistic tag. Best results are in \textbf{bold}.}
    \label{tab:winoground_ling}
    \vspace{4mm}

    \begin{minipage}{.6\linewidth}
        \scriptsize
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{lrrr|rrr|rrr}
\toprule
{\bf Model} & \multicolumn{3}{c}{{\bf Symbolic}} & \multicolumn{3}{c}{{\bf Pragmatics}} & \multicolumn{3}{c}{{\bf Same Image Series}} \\
& Text & Image & Group & Text & Image & Group & Text & Image & Group \\
\midrule
 \gray{Random}                  & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} & \gray{25.00} & \gray{25.00} & \gray{12.50} \\
 \gray{MTurk Human}                  & \gray{96.43} & \gray{92.86} & \gray{92.86} & \gray{58.82} & \gray{41.18} & \gray{41.18} & \gray{95.65} & \gray{91.30} & \gray{91.30} \\
 \midrule
 \gray{LXMERT}                       & \gray{28.57} & \gray{3.57}           & \gray{3.57}           & \gray{17.65}          & \gray{5.88}           & \gray{0.00}           & \gray{8.70}           & \gray{4.35}           & \gray{0.00}           \\
 \gray{UNITER$_\text{Large}$}             & \gray{39.29} & \gray{28.57} & \gray{17.86} & \gray{35.29} & \gray{0.00}           & \gray{0.00}           & \gray{4.35}           & \gray{8.70}           & \gray{0.00}           \\
 \gray{\clip (ViT-B/32)}              & \gray{39.29} & \gray{3.57}           & \gray{3.57}           & \gray{35.29} & \gray{5.88}           & \gray{5.88}           & \gray{8.70}           & \gray{0.00}           & \gray{0.00}           \\
 \gray{\clipcap{}$_\text{CC3M}$}               & \gray{21.43}          & \gray{21.43}          & \gray{10.71}          & \gray{5.88}           & \gray{5.88}           & \gray{0.00}           & \gray{0.00}           & \gray{8.70}           & \gray{0.00}           \\
 \gray{\clipcap{}$_\text{COCO}$}             & \gray{25.00}          & \gray{25.00}          & \gray{14.29}          & \gray{23.53}          & \gray{17.65}          & \gray{\bf 17.65} & \gray{13.04}          & \gray{13.04}          & \gray{0.00}           \\
 \gray{\bliptwo}               & \gray{42.86} & \gray{28.57} & \gray{25.00} & \gray{41.18} & \gray{\bf 23.53}          & \gray{\bf 17.65} & \gray{21.74}          & \gray{13.04}          & \gray{4.35}           \\
 \midrule
 \albefb                 & {42.86} & 25.00          & {17.86} & 17.65          & 17.65          & 5.88           & 8.70           & 0.00           & 0.00           \\
 \xvlmb                 & {50.00} & {\bf 32.14} & {\bf 32.14} & {41.18} & {\bf 23.53}          & {\bf 17.65} & {30.43} & {\bf 26.09} & {\bf 13.04}          \\
 \midrule
 \albefl                & {39.29} & 14.29          & 14.29          & 17.65          & 0.00           & 0.00           & {26.09} & 4.35           & 4.35           \\
 \blipl              & {39.29} & 25.00          & {17.86} & 23.53          & 17.65          & {\bf 17.65} & 8.70           & 4.35           & 0.00           \\
 \pevlpre  & {35.71} & 14.29          & 14.29          & {29.41} & 11.76          & 5.88           & 13.04          & 8.70           & 4.35           \\
 \pevlgrd & {35.71} & 7.14           & 7.14           & {29.41} & 11.76          & 11.76          & {26.09} & 8.70           & 4.35           \\
 \pevlvrd       & {42.86} & 10.71          & 7.14           & 23.53          & 5.88           & 0.00           & {\bf 34.78} & 17.39          & 8.70           \\
 \xvlml                & {42.86} & 21.43          & {17.86} & {\bf 47.06} & 11.76          & 5.88           & {26.09} & 4.35           & 4.35           \\
 \midrule
 \blipxl             & {\bf 57.14} & 14.29          & 14.29          & {35.29} & 11.76          & 11.76          & {26.09} & 0.00           & 0.00           \\
 \blipxlfilt & {50.00} & 14.29          & 14.29          & {35.29} & 5.88           & 5.88           & 21.74          & 0.00           & 0.00           \\
 \blipvitxl       & {39.29} & 14.29          & 14.29          & {29.41} & 0.00           & 0.00           & 13.04          & 0.00           & 0.00           \\
 \bottomrule
\end{tabular}

         }
        \vspace{-2mm}
        \caption{Results on Winoground by visual tag. Best results are in \textbf{bold}.}
        \label{tab:winoground_vis}

    \end{minipage}\hfill
    \begin{minipage}{.38\linewidth}
        \scriptsize
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{lrrrr}
\toprule
\textbf{Model}                & \textbf{ Subj.}  & \textbf{Verb} & \textbf{Obj.} & \textbf{Avg.} \\
\midrule
\gray{Random} & \gray{50.0} & \gray{50.0} & \gray{50.0} & \gray{50.0} \\
\midrule
\gray{\clip (ViT-B/32)} & \gray{83.6} & \gray{79.0} & \gray{88.1} & \gray{81.6} \\
\gray{\clipcap{}$_\text{CC3M}$} & \gray{84.2} & \gray{80.5} & \gray{90.2} & \gray{83.1} \\
\gray{\clipcap{}$_\text{COCO}$} & \gray{87.3} & \gray{81.5} & \gray{89.8} & \gray{84.1} \\
\gray{\flamingo} & \gray{90.1} & \gray{86.7} & \gray{92.3} & \gray{88.4} \\
\gray{\bliptwo} & \gray{87.6} & \gray{84.6} & \gray{91.7} & \gray{86.5} \\
\midrule
\albefb & 88.5 & 85.4 & 93.7 & 87.6 \\
\xvlmb & 89.3 & 87.1 & 94.5 & 88.9 \\
\midrule
\albefl & 89.4 & 86.4 & 94.7 & 88.6 \\
\blipl & 49.8 & 48.8 & 47.5 & 48.7 \\
\pevlpre & 89.4 & 82.9 & 93.9 & 86.2 \\
\pevlgrd & \textbf{91.2} & 85.9 & 94.6 & 88.5 \\
\pevlvrd & 90.1 & 81.1 & 92.3 & 84.8 \\
\xvlml & 90.3 & \textbf{88.4} & \textbf{94.6} & \textbf{90.0} \\
\midrule
\blipxl & 50.8 & 51.4 & 51.8 & 51.4 \\
\blipxlfilt & 49.4 & 51.3 & 52.5 & 51.2 \\
\blipvitxl & 50.0 & 50.9 & 50.9 & 50.8 \\
\bottomrule
\end{tabular}         }
        \vspace{-2mm}
        \caption{Performance on the SVO-Probes benchmark according to pairwise ranking accuracy. Best results are in \textbf{bold}.}
        \label{tab:svo}
        \vspace{4mm}
    \end{minipage}
    
    \small
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \textbf{Adjacency} & \textbf{Directional} & \textbf{Orientation} & \textbf{Projective} & \textbf{Proximity} & \textbf{Topological} & \textbf{Unallocated} & \textbf{Overall} \\
\midrule
\gray{Random} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} & \gray{50.0 / 50.0} \\
\midrule
\gray{\bliptwo}        & \gray{59.8 / 54.9} & \gray{50.0 / 43.3} & \gray{52.5 / 57.1} & \gray{59.8 / 63.6} & \gray{56.2 / 51.2} & \gray{66.4 / 67.0} & \gray{75.0 / 66.7} & \gray{61.2 / 61.5} \\
\midrule
\albefb         & 52.3 / 51.1 & 38.6 / 42.2 & 55.9 / \textbf{58.0} & 61.7 / 60.2 & 56.2 / \textbf{55.3} & 58.6 / 59.2 & 65.6 / 56.9 & 58.0 / 57.3 \\
\xvlmb          & 57.6 / 57.7 & 56.8 / 43.3 & 59.3 / 52.7 & \textbf{69.2} / 66.1 & 57.8 / 54.5 & \textbf{71.2} / 68.4 & 75.0 / 62.7 & 66.6 / 63.0 \\
\midrule
\albefl         & 52.3 / 54.2 & 59.1 / 40.0 & 55.9 / \textbf{58.0} & 59.8 / 62.6 & 46.9 / 52.0 & 66.8 / 58.9 & 71.9 / 58.8 & 60.2 / 58.3 \\
\blipl          & 56.8 / 49.3 & 56.8 / 50.0 & 57.6 / 47.3 & 42.5 / 49.3 & 51.6 / 48.0 & 45.1 / 51.8 & 50.0 / 41.2 & 47.4 / 49.7 \\
\pevlpre        & 47.0 / 55.3 & 56.8 / 48.9 & 57.6 / 56.2 & 61.9 / 60.8 & 51.6 / 48.8 & 62.4 / 57.4 & 71.9 / 58.8 & 59.3 / 57.5 \\
\pevlgrd        & 53.8 / 53.5 & \textbf{65.9} / 50.0 & 59.3 / 52.7 & 60.9 / 59.4 & 60.9 / 54.5 & 62.7 / 60.2 & 75.0 / 58.8 & 61.1 / 57.7 \\
\pevlvrd        & 54.5 / 55.6 & 59.1 / 52.2 & 61.0 / 53.6 & 59.8 / 60.4 & 59.4 / 54.5 & 64.1 / 63.1 & 68.8 / 64.7 & 60.7 / 59.5 \\
\xvlml          & \textbf{61.4} / \textbf{58.5} & \textbf{65.9} / 46.7 & \textbf{64.4} / \textbf{58.0} & 68.4 / \textbf{67.7} & \textbf{62.5} / 52.0 & 70.5 / \textbf{68.7} & \textbf{84.4} / \textbf{68.6} & \textbf{67.9} / \textbf{64.3} \\
\midrule
\blipxl         & 44.7 / 41.2 & 43.2 / 52.2 & 52.5 / 53.6 & 53.6 / 45.4 & 53.1 / 49.6 & 50.2 / 49.7 & 40.6 / 37.3 & 50.5 / 46.9 \\
\blipxlfilt     & 57.6 / 49.3 & 36.4 / 57.8 & 47.5 / 53.6 & 45.9 / 45.5 & 48.4 / 47.2 & 48.5 / 51.1 & 37.5 / 41.2 & 47.7 / 48.7 \\
\blipvitxl      & 56.1 / 51.8 & 29.5 / \textbf{58.9} & 49.2 / 52.7 & 46.9 / 48.5 & 53.1 / 43.9 & 49.8 / 51.8 & 46.9 / 47.1 & 48.7 / 50.3 \\
\bottomrule
\end{tabular}

     }
    \vspace{-2mm}
    \caption{Dev/Test results on the VSR Random dataset. Best results are in \textbf{bold}.}
    \label{tab:vsr}
\end{table*}



\subsection{Re-implementation Setup} \label{app:reimplement}
We re-implement ALBEF and X-VLM in JAX~\citep{dm_jax} to ensure full control of modelling, data, and initialisation decisions.\footnote{To verify our implementation, we compare an \albef model trained in our codebase with one trained in the original codebase. Specifically, we pretrain both models on COCO by initialising their visual encoder with a CLIP ViT-B/16 model, and their text encoder with a BERT$_\text{BASE}$ model. The two models perform similarly on both zero-shot Flickr30K and COCO retrieval tasks with a gap below 1pp Recall@1.}
We note \albef's vision encoder is initialised with a pretrained ViT-B/16 encoder~\citep{deit} with an input resolution of 256$\times$256 pixels, but \xvlm adopts a more efficient Swin-B/32~\cite{swin} encoder with input resolution of 224$\times$224 pixels.
In our re-implementation we initialise both models with a ViT-B/16 with a 224$\times$224 input resolution pretrained on ImageNet-21k~\cite{vit_augreg}, to ensure that different initialisation is not responsible for the results.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/svo_smoothed.pdf}
    \vspace{-7mm}
    \caption{Training dynamics on SVO-Probes subtasks. Random performance is 50\%.}\label{fig:svo_dynamics}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/valse_smoothed.pdf}
    \vspace{-7mm}
    \caption{Training dynamics on VALSE subtasks. Random performance is 50\%.}\label{fig:valse_dynamics}
\end{figure*}

We pretrain our models on the same 4M and 14M datasets that were originally used by the authors~(\cref{tab:models}), but note that only 1.8M and 11.2M data points were available for CC$_\text{3M}$ and CC$_\text{12M}$, respectively.
For object detection data, we use the data points released by the X-VLM authors, and interleave captioning and detection data with a 2:1 ratio following their official implementation.
Following \cite{x-vlm}, we pretrain our models for 200K steps using a batch size of 512 and 1024 samples for \albef and \xvlm, respectively. We pretrain once, using the same hyperparameters as the authors.\footnote{The \xvlm authors trained for 200K steps of image captioning data, not counting batches of detection datasets. We count each batch towards the final number of steps, hence effectively training for fewer steps than~\citet{x-vlm}.}
Training our models takes around 1.5 days on Cloud TPUv4 (a 2x2x2 slice).
We evaluate our models on both fine-grained benchmarks (SVO-Probes, VALSE and VSR) and on two zero-shot, coarse retrieval tasks (Flickr30K and COCO).
