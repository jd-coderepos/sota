\documentclass[runningheads]{llncs}

\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\usepackage{ae}
\usepackage[ruled]{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[breaklinks,bookmarks=false]{hyperref}
\DeclareMathOperator{\bord}{border}
\DeclareMathOperator{\period}{period}
\DeclareMathOperator{\prefix}{prefix}
\DeclareMathOperator{\suffix}{suffix}
\DeclareMathOperator{\nr}{nr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\post}{post}
\DeclareMathOperator{\pre}{pre}
\DeclareMathOperator{\prefixer}{prefixer}
\DeclareMathOperator{\detector}{detector}
\DeclareMathOperator{\LCP}{LCP}
\DeclareMathOperator{\LCPref}{LCPref}
\DeclareMathOperator{\LCSuf}{LCSuf}
\DeclareMathOperator{\child}{child}
\DeclareMathOperator{\prev}{prev}
\DeclareMathOperator{\next}{next}
\DeclareMathOperator{\failure}{failure}
\DeclareMathOperator{\RMQ}{RMQ}
\DeclareMathOperator{\heaviest}{heaviest}
\DeclareMathOperator{\ancestors}{path}
\DeclareMathOperator{\weight}{weight}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\head}{head}
\DeclareMathOperator{\depth}{depth}
\DeclareMathOperator{\led}{led}
\DeclareMathOperator{\ed}{ed}
\DeclareMathOperator{\leftval}{leftval}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\TotalLen}{TotalLen}
\DeclareMathOperator{\mism}{mism}

\newlength\mylen
\settowidth\mylen{\textbf{Case~5.}}
\newlist{mycases}{enumerate}{1}
\setlist[mycases,1]{label=\textbf{Case~\arabic*.}, 
  labelwidth=\dimexpr-\mylen-\labelsep\relax,leftmargin=0pt,align=right}
\newcommand\mynobreakpar{\par\nobreak\@afterheading} 

\newcommand{\twodots}{\mathinner{\ldotp\ldotp}}
\newcommand{\id}[1]{\ensuremath{\mathit{#1}}}
\newcommand{\proc}[1]{\textnormal{\scshape#1}}
\newcommand{\TODO}{{\LARGE\color{red}{TODO}}}

\begin{document}

\title{Beating  in approximate LZW-compressed pattern matching}

\author{Pawe\l{} Gawrychowski\inst{1} and Damian Straszak\inst{2}}
\authorrunning{}
\institute{
Max-Planck-Institut f\"{u}r Informatik, Saarbr\"ucken, Germany,\\ \email{gawry@cs.uni.wroc.pl}
\and
Institute of Computer Science, University of Wroc{\l}aw, Poland,\\ \email{damian.straszak@gmail.com}
}

\maketitle

\begin{abstract}
Given an LZW/LZ78 compressed text, we want to find an approximate occurrence of a given pattern of length . The goal is to achieve time complexity
depending on the size  of the compressed representation of the text instead of its length. We consider two specific definitions of approximate matching, namely the Hamming distance and the edit distance, and show how to achieve  and  running time, respectively, where  is the bound on the distance. Both
algorithms use just linear space.
Even for very small values of , the best previously known solutions required  time. Our main contribution is applying a periodicity-based argument in a way that is computationally effective even if we need to operate on a compressed representation of a string, while the previous solutions were either based on a dynamic programming, or a black-box application of tools developed for uncompressed strings.
\keywords{approximate pattern matching, edit distance, Lempel-Ziv compression}
\end{abstract}

\section{Introduction}

Pattern matching, which is the question of locating an occurrence of a given pattern in a text, is the most natural task as far as processing text data is concerned. Virtually any programming language contains a more or less efficient procedure for solving this problem, and any text processing application, including the widely available \texttt{grep} utility, gives users the means of solving it.

While exact pattern matching is well-understood, and in particular many linear time solutions are known~\cite{Jewels}, it seems that in its approximate version, where one asks for occurrences that are similar to a given pattern, there is still some room for improvement. Two most natural versions of the question are pattern matching with errors, where one ask for a substring of the text with small edit distance to the pattern, and pattern matching with mismatches, where one is interested in a substring with small Hamming distance to the pattern, which is simply the number of mismatched characters. It is known that if  is the length of the text and  is the number of allowed errors or mismatches, both problems can be solved in  time~\cite{LandauMismatches,Landau}, and in fact the complexity for the latter version can be improved to ~\cite{AmirMismatches}. Under the natural assumption that the value of  is small, one can do even better, and solve the problems in ~\cite{ColeHariharan} and ~\cite{AmirMismatches} time complexity, respectively, which might be linear in  if  is small enough. Unfortunately, in some cases even a linear time complexity might be not good enough. This is the case when we are talking about large collections of repetitive data stored in
a compressed form. Then the length of the text  might be substantially larger than the size  of its actual representation, and the goal is to achieve a running time depending on , not . Whether achieving such goal is possible clearly depends on the power of the compression method. In this paper we focus on the LZW/LZ78 compression~\cite{LZW,LZ78}, which is not as powerful as the more general LZ77 method, but still has some nice theoretical properties, and is used in real-world applications. It is known that exact LZW-compressed pattern matching can be solved very efficiently~\cite{Amir,GawrychowskiLZW}, even in the fully compressed version, where both the text and the pattern are LZW-compressed~\cite{GawrychowskiFully}. The obvious question is how efficiently can we solve approximate LZW-compressed pattern matching?

The best previously known solution by K{\"a}rkk{\"a}inen, Navarro, and Ukkonen~\cite{Juha}, locates all  occurrences with up to  errors using  time and  space. More precisely, it outputs all ending positions  such that there is  for which the edit distance between  and  is at most . In some cases, this time bound can be decreased using the idea of Bille, Fagerberg, and G{\o}rtz~\cite{Bille}, who presented a way to translate all uncompressed pattern matching bounds into the compressed setting. Their approach works for both the edit and Hamming distance, and by plugging the best known uncompressed pattern matching solutions, we can get:
\begin{enumerate}
\item  time and  space solution for the edit distance,
\item  time and  space solution for the edit distance,
\item  time and  space solution for the Hamming distance.
\end{enumerate}
While the space complexity of the resulting algorithms is small, even for constant values of  the time complexity is , and in fact this is an inherent shortcoming of the approach: the best we can hope for is  for sufficiently small values of , say, . In this paper we show that in fact this barrier can be broken. We prove that for the Hamming distance, running time of  is possible, which for  is . Then we show how to extend the algorithm by building on the ideas of Cole and Hariharan~\cite{ColeHariharan}, and achieve  for the edit distance. Both algorithms use  space. For the sake of making the description clear, we concentrate on the question of detecting just one occurrence, but our algorithms generalize to generating all of them in the left-to-right order.


Some of our methods are based on the concepts first used by Cole and Hariharan~\cite{ColeHariharan}, and later by Amir, Lewenstein, and Porat~\cite{AmirMismatches}. We would like to
stress out that applying them in the compressed setting is not just a trivial exercise, and creates new challenges. For instance, verifying whether a given position corresponds
to an occurrence with no more than  mismatches in  time is straightforward in the uncompressed setting using the suffix tree, but in our case requires some additional ideas.


We start with some basic tools in Sections~\ref{section:preliminaries} and~\ref{section:further}. Then we distinguish between two types of matches, called internal and crossing. Detecting the former is relatively straightforward in both versions. To detect the latter, we reduce the question to a problem that is easier to work with, which we call pattern matching in pc-strings, in Section~\ref{section:reduction}. To solve pattern matching with mismatches in pc-strings, we distinguish between
two cases depending on how periodic the pattern is. For this we apply the concept of breaks, heavily used in the previous papers on approximate pattern matching. In our case, though,
we are looking at -breaks for some value of  that will be specified in the very end. If there are many such breaks, or in other words the pattern is not very repetitive, we
can solve the problem by reducing to a generalization of (exact) compressed pattern matching with multiple patterns as shown in Section~\ref{section:weakly}. Otherwise, the pattern is {\it highly periodic},
and the situation is more complicated. In Section~\ref{section:highly} we show how to exploit the regular structure of such pattern to construct an efficient algorithm.
More precisely, we construct a small set of candidates for a potential occurrence, and verify them one by one. Then
in Section~\ref{section:faster}, which is the most technical part of the paper, we speed up the method using a new technique which considers all candidates in a more global manner instead of operating on them separately.
Finally, in Section~\ref{section:errors} we generalize the solution to solve the version with errors.

\section{Preliminaries}
\label{section:preliminaries}

We are given a text  and a pattern , both are strings over an integer alphabet . We assume that  and . The pattern is given explicitly, but the text is described implicitly using the LZW/LZ78 compression scheme. Such scheme is defined as follows: we partition the text into  disjoint fragments , where each fragment  is either a single letter, i.e., , or a word of the form , where . The fragments  are usually called the {\it codewords}, and because their set is closed under taking prefixes, we may represent it as a trie, which will be further denoted by . Depending on how we choose the partition and encode the codewords, we get different concrete compression methods, say LZW or LZ78. Our methods do not depend on such technicalities as long as we are given  and the text is described as a list of pointers to the nodes of  representing the successive fragments. From now on whenever we mention compression, we mean such representation.

The Hamming distance between two strings of the same length is simply the number of positions where their corresponding characters differ. The edit distance  is the minimal number of operations necessary to transform  into , where an operation is an insertion, replacement, or removal of a character.

The first problem we consider is {\it compressed pattern matching with mismatches}, where we are given a compressed representation of a text , a pattern , and a positive integer . We want to find  such that the Hamming distance between  and the pattern is at most . 
We also consider {\it compressed pattern matching with errors}, where the goal is to find  and  such that the edit distance between  and  is at most .
Our solutions generalize to generating all occurrences, where we are asked to report the ending positions of all matches (notice that in pattern matching with errors, there might
be multiple occurrences ending at the same position, and then we want to report such position just once), but we concentrate on the easier to describe version with just one
occurrence.

To efficiently operate on the compressed text and the pattern we need a number of data structures. We assume that the suffix array of the pattern is available. A description of this data structure, together with a simple linear time construction algorithm, can be found in~\cite{Karkkainen}. Notice that it assumes that the alphabet is of polynomial size, which is the case here as . By adding a constant time range minimum query structure~\cite{Bender}, we get the following useful primitive.
\begin{lemma}\label{lemma:lcapattern}
A string can be preprocessed in linear time so that given its any two fragments we can find their longest common prefix and longest common suffix in constant time.
\end{lemma}
It may seem surprising, but the problem of efficiently retrieving a single letter given a position in the text is rather nontrivial. By ``position in the text" we mean a pair 
which stands for ``-th letter of ". This problem can be reduced to accessing the -th ancestor of a node in a tree. A straightforward algorithm gives us  preprocessing time and  time per query, with  being the size of the tree as in our setting, and a much better solution is known~\cite{AlstrupAncestor}, giving the following tool.
\begin{lemma}\label{lemma:lettertext}
We can preprocess the text in linear time so that given any position there we can retrieve the corresponding letter in constant time.
\end{lemma}
The next two lemmas will provide methods for comparing substrings of the text with substrings of the pattern. The usual solution in such cases is to build suffix tree of the text concatenated with the pattern. However, this would require decompressing the text and result in a structure of size , which is unacceptable. Hence we need another method.

From now on, let  denote a trie representing the set of strings , in other words  is  extended by one path representing the whole pattern. The size of  is .

\begin{figure}[t]
\centering
\includegraphics[scale=0.7]{trie}
\caption{Constructing  from  and .}
\label{figure:trie_pattern}
\end{figure}


\begin{definition}
Let  be a trie of strings. We say that  is a chunk (in ) if  is a subword of some root-to-leaf path in . We represent a chunk as a pair consisting of a node in  corresponding to the last letter in  and the length .
\end{definition}

 is the longest common suffix and  is the longest prefix of given two strings. We are interested in computing them for any two chunks.

\begin{lemma}\label{lemma:LCSuf}
Given any trie  we can preprocess it in linear time so that the  of any two chunks can be computed in constant time.
\end{lemma}

\begin{proof}
Observe that we may concentrate only on the chunks beginning in the root. Let  be the set of all such chunks. Imagine now a compacted trie  representing all words in the set , where  stands for the reversed string . It may be seen that the size of the tree is  because each  adds at most one leaf.  is an interesting generalization of a suffix tree of a regular string, and as in the basic case it may be constructed in linear time assuming an integer alphabet of polynomial size~\cite{Shibuya99}. After building such a tree, we augment it with a linear size structure allowing constant time LCA queries~\cite{Bender}. This, along with pointers between nodes of  and the corresponding nodes of , gives the claimed result.
\qed
\end{proof}

\begin{lemma}\label{lemma:LCPref}
We can preprocess the text and the pattern in  time so that given a chunk in  and a subword of the pattern we can find their  in  time and  in  time.
\end{lemma}
\begin{proof}
We use the previous lemma for . We get immediately the result for . Now consider an  query: we use binary search together with  queries to get the answer in  time. Note that in such a procedure we need often to find the -th ancestor in the tree, but we already know by Lemma \ref{lemma:lettertext} that it can be done in constant time.
\qed
\end{proof}

We need also some basic concepts from combinatorics on words.  is a period of a string  if  holds for every , or in other words
we can write , where  and  is a prefix of . The smallest such 
is called {\bf the} period of . If the period of  is at most ,  is periodic, and otherwise we call it a break, or -break. A word is primitive if it cannot be
represented as a nontrivial power of some other word. For every word , there exists its unique cyclic shift  which is lexicographically smallest, and we call 
the cyclic representative of . For a periodic , the cyclic representative of  corresponding to the period of  is called the canonical period of . One of the basic
results concerning periods is the periodicity lemma, which says that if  and  are both periods of , and , so is .

\section{Further preprocessing}
\label{section:further}

From now on we fix  to be the number of allowed mismatches (errors) in our problem. We will say in short that the pattern matches at some position in the text if the Hamming distance (or the edit distance) between the pattern and the fragment of the text starting at this position is at most . It is natural to distinguish between two types of matches: {\it internal matches} (the pattern lies fully within a single codeword) and  {\it crossing matches} (the pattern crosses some boundary between two codewords). We are now going to show that one can easily find all internal matches. 


\begin{lemma}\label{lemma:internal_matches}
In case of pattern matching with mismatches we can find all internal matches in  time and if needed report all of them in  time per occurrence.
\end{lemma}
\begin{proof}
Each internal match can be seen as a chunk in  of length  (but of course one such chunk may correspond to many places in the text). For every such chunk we verify whether its Hamming distance with the pattern is at most . This can be done in  time using constant time  queries to jump over whole fragments with no mismatches, which is the standard method used in the uncompressed setting~\cite{LandauMismatches}. In total there are  chunks to verify, which gives  time. Reporting all occurrences is straightforward if we know which chunks match the pattern.
\qed
\end{proof}

\begin{lemma}\label{lemma:internal_matches_errors}
In case of pattern matching with errors we can find all internal matches in  time and if needed report all of them in  time per occurrence.
\end{lemma}
\begin{proof}
We use the same method as in the proof of Lemma~\ref{lemma:internal_matches}, but to verify a match we apply the Landau-Vishkin algorithm~\cite{Landau}, which computes the edit distance using a clever dynamic programming in  time (see Section 5 of~\cite{ColeHariharan}), assuming we can answer any  query in constant time (in the original formulation,  queries are used, but we can simply pretend that the text and the pattern are reversed). More precisely, given two strings, it can be used to compute all prefixes of the former whose edit distance to the latter is at most . This gives us, for each node of , the chunks ending there and corresponding to a match in  time. This can be modified to report all occurrences in a straightforward manner. 
\qed
\end{proof}

The situation with crossing matches is much more complicated. In this case the pattern crosses at least one boundary between two codewords, and it may cross a lot of them, which seems hard to deal with. Anyway, it suffices to iterate over all  boundaries and for each of them find all matches that cross it. After fixing such a boundary, we may concentrate only on a window of length  containing  characters to the left and  to the right. Problems arise when there are many very short codewords in some fragment of the text, because in such a case all boundaries in this fragment will create windows containing lots of codewords. This is one of the obstacles we need to tackle to construct an efficient algorithm.

We want to make now one technical assumption, which simplifies significantly some definitions and the description of the algorithm. Namely, we will assume that each letter appearing in text, appears also in the pattern. Our algorithms work in the general case after minor modifications.

The notion of a pc-string will play the main role in the rest of the paper. Note that the definition changes slightly when we want to move from mismatches to errors. Nevertheless, the change is very small, so we prefer to have just one common definition, and keep in mind that its meaning depends on the variant.

\begin{definition}
Let  be a pattern and  be a string. We say that  is a pattern-compressed-string, in short pc-string, if: 
\begin{enumerate}
\item  ( when we are dealing with errors) and ,
\item  is a factor of , for ,
\item  is not a factor of , for .
\end{enumerate}
We represent such string as a list , where .
\end{definition}

Pc-strings are very convenient to deal with. The fact that no  appears in  as a substring allows us to answer LCPref and LCSuf query between a subword of  and a subword of the pattern in constant time, as each result of such a query overlaps at most 3 's, so we need at most 3 queries between factors of . This also implies the
following proposition.



\begin{proposition}\label{proposition:verify_match_mismatches}
Given a position in a pc-string , we can verify whether the alignment of the pattern at this position results in a match in  time.
\end{proposition}
\begin{proof}
We already know that performing  queries takes constant time. Now the result follows, again, by using constant time  queries to jump over whole fragments with no mismatches. Now there is one additional detail, though. To answer such query in constant time, we need to maintain the corresponding current position in , or more precisely, the current  and the current letter there. This is easily done, as during the computation we only move to the right, and only to either  or , hence the current position can be updated in constant time.
\qed
\end{proof}




It turns out that finding matches crossing a fixed boundary can be reduced to one instance of pattern matching with mismatches or errors in a pc-string. This reduction is presented with details in the next section. Here we conclude it by the following theorem:

\begin{theorem}\label{theorem:reduce_to_PCstrings}
Suppose we have an algorithm solving pattern matching with  mismatches (errors) in pc-strings in  time. Then we can solve pattern matching with  mismatches (errors) in LZW-compressed text in  () time.
\end{theorem}

In order to solve pattern matching in a pc-string, we will extensively use a certain preprocessing of the pattern. This preprocessing takes  and is performed just once in the whole algorithm, not every time we get an instance of pattern matching in a pc-string. Hence we usually do not 
include this time in the statements of the lemmas.

\section{Reducing to pc-strings}
\label{section:reduction}

In this section we show a method for reducing the problem of finding matches crossing some boundary to pattern matching with mismatches in a pc-string. We will focus on the version with mismatches, as the version with errors requires just very minor modifications.

The main technical tool we are going to use in this section is a PH-decomposition.

\begin{definition}
Given a pattern  and a compressed text  we say that  is a -decomposition if the following conditions are met.
\begin{enumerate}
\item{Each block  is either a factor of the pattern (we say it is of type ) or is fully contained within a single codeword (we say it is a ``hole" or of type ).}
\item{Each codeword contains at most one hole.}
\item{For each two consecutive blocks ,  of type , their concatenation  does not appear as a subword in the pattern.}
\item{If  is a hole, then the whole  lies within a single codeword and all  are of type . }
\end{enumerate}
\end{definition}
Figure~\ref{figure:PH-decomp} demonstrates an important property of a PH-decomposition. Between a hole and the boundaries of the block it originates from there are always at least  full P-blocks.

\begin{figure}[t]
\includegraphics[width=\textwidth]{PH-decomp}
\caption{An important property of a PH-decomposition.}
\label{figure:PH-decomp}
\end{figure}

We will now give an algorithm for finding a -decomposition of the text. It is useful to first show the following lemma.

\begin{lemma}\label{lemma:longfactor}
Given a chunk  in  we can:
\begin{enumerate}
\item{find the longest prefix of  which appears as a subword in  (and locate this subword in p) in } time,
\item{find the longest suffix of  which appears as a subword in  (and locate this subword in p) in } time.
\end{enumerate}
\end{lemma}

\begin{proof}
Let us show the first part. We are able to compare lexicographically  with a suffix of the pattern in  time, since we can use one  query and compare the next letters. Imagine the suffix array of , and suppose we want to insert  into this array maintaining the lexicographical order. We can find the right place for  in  time by binary search. Then the immediate predecessor or successor of  will have the maximum possible  with , and this will correspond to the maximal prefix of  which is a subword of . 

The second part of the lemma holds because we can answer  queries in constant time.
\qed
\end{proof}

\begin{lemma}
A -decomposition of the text can be found in  time.
\end{lemma}

\begin{proof}
The idea is to first work with single codewords and decompose them partially, then take all these decompositions and merge them together into one -decomposition of the whole text.

Let us consider a codeword . We want to represent  in one of the following forms:
\begin{enumerate}
\item , where each  is a factor of the pattern, , and none of the words  appears in  as a subword,
\item  where each  and each  is a factor of the pattern, and no  nor  appears in  as a subword; here  can be seen as a ``hole".
\end{enumerate}
We go from left to right through z. First we use Lemma~\ref{lemma:longfactor} to find , the longest prefix of  which appears in  as a subword. We then erase  from the beginning of  and proceed similarly to find , and so on. If this procedure stops after at most  steps, meaning there is nothing left from , we are done, as we have got the first possible form of . In the opposite case, we stop and do the same starting from the end of  and going from right to left. We perform  such steps getting . Clearly, we can get the second form by taking the first  's, all 's, and setting  to be the remaining middle part of .

Now take all these representations and merge them together. We declare all 's and 's to be of type  and all 's to be of type . Note that we are almost done, as all conditions in the definition of a -decomposition except (maybe) the third are fulfilled. The violation of the third rule can occur in places where the representations were glued together. To fix this we repeat the following procedure until the third condition holds: take two consecutive blocks  and  of type  such that  appears as a subword in , replace  by one -block . In order to do this, we need a way to check whether a word of the form  appears as a subword in . This can be done by locating the position of  in the suffix array of  using binary search in  time, as in the proof of Lemma~\ref{lemma:longfactor}.

In total we produce at most  blocks, and spend  time per block. Thus the claimed time bound.
\qed
\end{proof}

Suppose now we have a -decomposition of the text . Thanks to this, we can simplify the problem of finding crossing matches. Fix a boundary between two codewords and suppose it is either inside or just before a -block . From the definition, all blocks  are of type .  Furthermore, we claim that every match crossing our fixed boundary lies within the fragment  of the text. This is because otherwise it either ends after  or starts before , so there are  consecutive -blocks such that their concatenation match with at most  mismatches with some subword of the pattern. But then some two consecutive blocks have to match exactly with some subword of the pattern, which contradicts the definition of the -decomposition.

Now a string  consisting of the at most  -blocks considered above is almost a pc-string. We only need to trim it so that its length does not exceed  ( to the left and  to the right from boundary). After such trimming the first or the last block might become shorter, and it might be necessary to merge such incomplete block with the neighbour, which can be done in  time as in the above proof. This concludes our reduction.

If we just want to find the first occurrence of the pattern, then we process the boundaries one by one and solve matching in the corresponding (at most) -length windows of the text. However, if our aim is to report all the occurrences of the pattern, we need to make sure no match is reported multiple times. For this just trim the considered windows so that they overlap on at most  positions. Note also that when performing the reduction carefully we only need to store  pattern factors at a time, so it won't affect the space complexity.

\section{Detecting matches in pc-strings}
\label{section:weakly}

In this section we concentrate on the version with mismatches and present an efficient algorithm for detecting matches in a pc-string (recall that by Theorem~\ref{theorem:reduce_to_PCstrings} this solves the problem of finding matches in a compressed text).
We distinguish two cases depending on the ``level of periodicity" of the pattern. Let  be a parameter to be fixed later. We find in  as many disjoint -breaks as possible. If there are just a few such breaks, the pattern can be seen as {\it highly periodic}.

For finding the maximum number of disjoint breaks in the pattern, we use the \proc{Find-breaks} procedure of Cole and Hariharan~\cite{ColeHariharan}. We prove its correctness for the sake of completeness.

\begin{algorithm}[t]
\caption{\proc{Find-breaks}(p)}
\begin{algorithmic}[1]
\State 
\While{}
\If{the period of  exceeds }
\State report -break 
\State 
\Else
\State 
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lemma:finding_breaks}
\proc{Find-breaks} finds the maximum possible number of disjoint -breaks in . Moreover, it can be implemented in  time.
\end{lemma}
\begin{proof}
To show the correctness of the procedure, we proceed by induction on .  \proc{Find-breaks} locates the leftmost -break, and then continues on 
its right, hence it is enough to argue that it is always possible to choose the maximum number of disjoint -breaks with the first break being the
leftmost break. But this is obvious, because we can always move the first break in the solution to the left.

Let us now concentrate on the time complexity. After the algorithm terminates, the pattern is of the form , where each  is a break and each  is -periodic for some .  We will show how to handle a portion  in  time. We start with the window containing the length- prefix of , we find its period  in  time. If  then stop. Otherwise we keep adding letters to the end, one by one, checking only whether the new character is the same as the one  positions earlier. First time this is not the case, we stop and mark the length- suffix of the considered portion as a break . This works because if  had period  (necessarily ) then, by the periodicity lemma,  with last character erased would have period at most , which is impossible.
\qed
\end{proof}

We consider now the case when  contains at least  disjoint -breaks. It turns out that in such a case we can discard most of the starting positions, and then verify all remaining candidates separately.

\begin{lemma}\label{lemma:sparsify_matches}
Let  be a text of length . Assume that the pattern  contains at least  disjoint -breaks. Then there are at most  matches (with  mismatches) of  in .
\end{lemma}

\begin{proof}
Choose  disjoint occurrences of breaks in the pattern. Let  be all pairwise different breaks among them, with  occurring  times, so . Consider one break , and denote the positions of the disjoint occurrences of  in  by . For each occurrence of  in the text, say at position , we add a mark to all positions  within the text. Since the distance between two different occurrences of  in the text is at least  (because if they were closer it would imply  has shorter period than ), there will be at most  marks caused by . So all in all, there will be at most  marks. Consider now a position in the text where  matches with at most  mismatches. At least  of the  breaks have to match exactly, so we have at least  marks there. But there are only at most  positions with at least  marks, so the lemma follows.
\qed
\end{proof}


This lemma is very useful, but it does not give a method to find all these  positions. For this need to locate all occurrences in  of up to  pattern breaks. We cannot simply use the usual multiple pattern matching algorithm, because it would cost  time, which is too much. However, we know that there are at most  occurrences of these breaks in . This fact, combined with an efficient algorithm for multiple pattern matching in a pc-string, which is an adaptation
of the method of Gawrychowski~\cite{GawrychowskiMultipleLZW}, gives a solution.


\begin{lemma}\label{lemma:break_matching}
Suppose  is a pattern and  is a fixed collection of its disjoint and pairwise different -breaks. We can preprocess the pattern and the collection of breaks
in  time, so that later given any pc-string  we can find all  occurrences of  in  in  time.
\end{lemma}

\begin{proof}
We assume that we have a sorted array of all suffixes and all prefixes of all breaks. We also assume that all occurrences of  in  have been found using the Aho-Corasick automaton~\cite{AhoCorasick}, and that we have an array storing for each  the leftmost occurrence of any break from the collection in .
As a byproduct of generating all occurrences, we also get an array storing for each  the longest prefix of  which is a prefix of some break.
By reversing the pattern and running the automaton again, we can also get an array storing for each  the longest suffix of  which is a suffix of some break. Additionally, we organize the prefixes of all breaks into a {\it prefix tree}, where the parent of  is the longest prefix of some break which is a proper suffix of . Such tree can be constructed in linear time using a single scan over the sorted array of all reversed prefixes of all breaks. The prefix tree is augmented with a constant time level ancestor structure.

The preprocessing is done in  time because the total length of all breaks and the total number of their occurrences is at most .
Now we will see how to adapt the algorithm given in~\cite{GawrychowskiMultipleLZW} to work in our case. One can see, that it is enough to show how to perform the following operations:
\begin{enumerate}
\item given some  find its longest suffix (prefix), which is a prefix (suffix) of some ,
\item given some  check whether it is a subword of some  (and locate this subword),
\item given some  find all occurrences of 's inside it.
\end{enumerate}
It is easy to see that the first two types of queries can be implemented in . Consider the first type: first we use the previously computed array to compute the longest prefix of some break which is a suffix of , where . Then, either such prefix is
fully within , and we return it, or we need to compute its sufficiently short suffix which is a prefix of some break. In other word, we need
to locate its lowest ancestor in the prefix tree which corresponds to a prefix of length at most . This can be done using binary search over all ancestors. Now consider
the second type: we binary search in the sorted array containing the suffixes of all 's to find the one sharing the longest prefix with . Finally, consider the third type of query. Because all breaks in our collection have the same length, we can find all their occurrences in  using the array storing the leftmost occurrence in any
suffix of  spending just  time per occurrence. This implies that our running time is .
\qed
\end{proof}

We now describe an algorithm for patterns with at least  disjoint -breaks.

\begin{theorem}\label{theorem:algorithm_nonperiodic}
Suppose the pattern contains at least  disjoint -breaks. Then pattern matching with  mismatches in pc-strings can be solved in  time.
\end{theorem}

\begin{proof}
First we find  disjoint -breaks in the pattern. We want now to detect the at most  positions in  where p can potentially match. Proceeding as in the proof of Lemma~\ref{lemma:sparsify_matches}, first choose some  disjoint -breaks and find all their matches in  using the algorithm from Lemma~\ref{lemma:break_matching}. This costs us  time. The marking phase can be done in  time. Now for each of the  positions verify whether  matches there in  time. So we can find all matches of  in  in  time.
\qed
\end{proof}

\begin{remark}
The marking phase uses an array of size . The array is reused whenever we apply the above lemma, so it adds just  to the final space complexity.
Observe that we cannot afford to initialize the whole array in every application, as  might actually be larger than .
We initialize it just once in the very beginning of the whole algorithm, and during each marking phase we prepare a list of modified entries in the array. Then we clean up
just the corresponding part of the array in  time.
\end{remark}

Note that taking big  makes our algorithm really fast. However, the larger is , the harder is for the pattern to contain many -breaks. Furthermore, we cannot expect each pattern to have many -breaks, even for small . Therefore, we need a different algorithm for the case when  has few breaks, or is {\it highly periodic}. The algorithm has to take advantage of the regular structure of the pattern.

\section{Basic algorithm for highly periodic patterns}

\label{section:highly}
In this section we assume the pattern is highly periodic. This means we can write it in the form , where , each  is a -break and each  is a (possibly empty) string with period at most . The fragments  are called {\it periodic stretches}.  As in the previous section we are interested in finding all matches (with at most  mismatches) of  in a pc-string . 

Below we describe how to reduce the general case to the one where the number of breaks in the text is small. A very similar reasoning will be also used later in matching with errors, the only change being increasing some constants.

\begin{lemma}\label{lemma:discarding_breaks}
Suppose  is a string of length at most  and  is a pattern containing at most  disjoint -breaks. There exists a subword  of  having at most  disjoint -breaks such that each match of  in  lies fully within .
\end{lemma}
\begin{proof}
Split  so that . Let  be the shortest suffix of  having exactly  disjoint -breaks (or the whole  in case there is no such suffix). Let  be the shortest prefix of  having exactly  disjoint -breaks (or the whole  in case there is no such prefix). We define . It is easy to verify that  has at most  disjoint -breaks. Assume for the sake of contradiction that some match of  doesn't lie within , for instance it ends beyond the right end of . In such a case  lies fully within this match, which means at least  out of its  breaks have to match exactly. Consequently,  has at least  disjoint -breaks, contradiction.
\qed
\end{proof}

\begin{proposition}\label{proposition:algorithm_text_breaks}
Suppose  is a pc-string, then we can find the corresponding  from Lemma~\ref{lemma:discarding_breaks} in  time.
\end{proposition}
\begin{proof}
Concentrate for example on finding  (using notation from the proof of Lemma~\ref{lemma:discarding_breaks}). We want to simulate the algorithm \proc{Find-breaks} until the -th break is found. The method from Lemma~\ref{lemma:finding_breaks} would give us  time, which is not good enough, but we can improve it since  is a pc-string. 

We will show how to find the first break (if any) in , the next ones are determined in the same way. We take the prefix  of  of length , and determine its period  in  time using the standard algorithm. If  exceeds  then  is a break and we are done. If not, the situation is as follows: we are given a string  represented as a concatenation of  factors of the pattern, and we want to find its unique prefix , such that its length- suffix is a -break and  has period .  We need to find out how long  is, or in other words we need to compute how far the period  extends. It turns out that there is a simple formula for this, namely . We can answer such a query in time proportional to the number of blocks in the result. 

Locating a single break using the above method can take even up to  time, but the total complexity amortizes to , because we always cut off the processed prefix of  and then work with the remaining part. So in total we spend  time. 
\qed
\end{proof}

By the discussion above we can restrict ourselves to pc-strings having at most  disjoint -breaks. We will give now an algorithm achieving  running time for pattern matching with  mismatches in such pc-strings. While this is not the best algorithm we have obtained, it serves well as an introduction to the more complicated  algorithm presented in the next section. 

Let us summarize the situation. We are given a pattern of the form  and a pc-string , where , 's denote -breaks and 's are strings with periods not exceeding . We will soon see that alignments of the pattern, for which the pattern breaks and text breaks are not too close from each other, are nice to work with. So we want to handle the remaining alignments separately.

\begin{proposition}\label{proposition:close_breaks}
There are at most  alignments of the pattern in the text such that some text break (or text endpoint) is within a distance of  from some pattern break (or pattern endpoint).
\end{proposition}

\begin{proof}
We treat pattern endpoints  and text endpoints as breaks for simplicity. There are  pairs (pattern break, text break). Each such pair can violate the rule on at most  positions. This gives us  positions in total.
\qed
\end{proof}

In this (simple) version of the algorithm we just verify all these  positions in  time per one. This results in  complexity and leaves us with the convenient case, where all distances between pattern and text breaks (or endpoints) are at least . We call such alignments {\it fine}, and we will
soon see that if such a fine alignment results in a match, it forces all periodic stretches involved in the match to have the same period.

Starting from now we assume that the distances between consecutive breaks in the text (pattern) are at least , and otherwise group some breaks together so that the groups meet this condition. Our argument will work also for such groups but it is simpler to describe it just for breaks. 
Similarly, we want to assume that  (and ) is either empty or has length at least , so if  we extend the first break to the left so that  becomes empty (and do the same with ). 

One can easily see that there are at most  intervals of consecutive fine alignments in the text. Within such an interval the order of appearance of the breaks does not change. Fix one interval and suppose we have at least one match there. We want to argue that in such a case all periodic stretches involved in this match are compatible, meaning
that their canonical periods are identical, and moreover start with the same offset modulo the period.

\begin{proposition}\label{proposition:periodic_mismatches}
Suppose  are periodic strings with periods not exceeding . If  and  then there are at least  mismatches between these two words.
\end{proposition}

\begin{proof}
We have two cases depending on whether the periods of  are equal or not. Let  be the period of  and  the period of . If  then in each fragment of length  we have at least one mismatch, so at least  mismatches in total. Assume now , then each fragment of length  contains a mismatch (if not then the periodicity lemma gives a contradiction). Since , there are at least  mismatches.
\qed
\end{proof}

\begin{figure}[t]
\includegraphics[width=\textwidth]{overlapping_stretches}
\caption{Long overlaps between stretches imply their canonical periods are the same.}
\label{figure:overlapping stretches}
\end{figure}

Suppose there is a match at some fine alignment. Between two consecutive breaks (we consider here all pattern and text breaks) there is always a periodic portion of length at least . From Proposition~\ref{proposition:periodic_mismatches}, there must be a perfect match between the corresponding fragments. So in particular, the periods of the corresponding pattern periodic stretch and text periodic stretch agree. Considering the way how the stretches overlap each other, see Figure~\ref{figure:overlapping stretches}, one can deduce by transitivity that all periodic stretches involved in this match have the same period (they even have the same canonical period). Surprisingly, it means that if some two periodic stretches in the pattern have different canonical periods then there is no hope for matches at fine positions. 

Suppose now all the periodic stretches in the pattern have the same canonical period . We consider an interval of consecutive fine alignments. Assume there is a match somewhere in this interval. One can see that each two alignments  and  from the interval have the same number of mismatches, because each break is aligned with a -periodic stretch, so 
the fragment we compare it to is the same.
So in order to find all matches within one interval, we only need to verify at most  alignments. Each verification takes  time, so the time taken over all intervals is .

\begin{theorem}\label{theorem:algorithm_highlyperiodic}
For highly periodic patterns, pattern matching with  mismatches in pc-strings can be solved in  time.
\end{theorem}


\section{Faster algorithm for highly periodic patterns}
\label{section:faster}


The purpose of this section is to show a faster algorithm for pattern matching with  mismatches in pc-strings, assuming the pattern is highly periodic. We will improve the time complexity from  to . We will make sure that the additional space required by the improved algorithm is just , which will
be crucial in achieving linear space usage of the whole solution.


In the previous section we showed that one can assume that the text has at most  disjoint -breaks. The idea of the basic algorithm was to first work with the ``bad" alignments. An alignment was considered ``bad" if there was a text break and a pattern break close to each other (within a distance of ). We took all such alignments and verified them in  time each. The fine alignments (meaning not ``bad") were analyzed in total time . This approach, although simple, seems to be very naive. Each time there is a single pair of close breaks, we waste  time to deal with such an alignment. It turns out that we can verify a ``bad" position in time proportional to the number of ``bad" breaks. In the following definitions and lemmas we make the idea formal.


\begin{definition}
In a fixed alignment of the pattern in the text, we call a pattern break black if there is some text break or text endpoint within distance  from it. Similarly, we call a text break black if there is some pattern break or pattern endpoint within distance  from it. Non-black breaks are called white.
\end{definition}

Note that one extreme case when a break is black is when it overlaps with some other break. It is convenient to deal with such situations separately. There are only  such alignments, so they can be all verified in  time, and from now on we consider only alignments where no two breaks overlap. Moreover, we can restrict our attention to alignments with at least one black break. The rest are among the fine alignments, which can be processed as shown in the previous section.

\begin{lemma}\label{lemma:fast_alignments}
After  time preprocessing, given an alignment with  black breaks we can test whether it corresponds to a match in  time.
\end{lemma}
We will prove the above lemma in the remaining part of this section. Suppose for a moment it holds, and consider all alignments with some black breaks. Call the number of black breaks in these alignments . Then by the above lemma, each single alignment can be processed in  time, so the total time is . Every specific break is black at most  times, so . So if we use this method to process the alignments, we will obtain an algorithm with  running time.

The main idea in the proof of the lemma is to partition the alignment into disjoint parts, such that in each of these parts we can count the number of mismatches easily. More precisely, if there are  black breaks in the considered alignment, we distinguish  intervals where the Hamming distance can be determined in  time. For this, we need some results memorized in arrays. We will give now the details by analyzing some cases of the relative arrangement of black and white breaks. Recall we have already reduced the situation to the case where no two breaks overlap.

Consider a periodic stretch  between two breaks in the pattern (text). It can be written in the form  where  is its canonical period (of length at most ), ,  is some suffix of  and  is some prefix of . Note also that the word  is primitive in such a case. It is easier to imagine the whole picture (and also to describe it) if , in other words when  is a power of its canonical period. We can achieve it by merging  ( respectively) to the neighbouring break on the left (on the right). After this operation the breaks have lengths between  and  and all periodic stretches, maybe except these at the start and at the end of the word, are powers of primitive words.

Let us fix an alignment with at least one black break, and take any black pattern break (the reasoning for text breaks is the same). We want to count the number of mismatches between it and the corresponding periodic stretch from the text. To answer such a query in constant time, we build in the preprocessing phase a table with all results. For each pattern break and periodic stretch  from the text we count mismatches between the break and the stretch for every possible shift smaller than . Each such count can be performed in  time, which results in  time preprocessing.

\begin{figure}[t]
\includegraphics[width=\textwidth]{black_breaks_1}
\caption{Two consecutive black breaks.}
\label{figure:black_breaks_1}
\end{figure}

Now take two consecutive black breaks . Consider the case, when there are no more breaks between them (of course there are no black ones, because we chose  to be consecutive, but some white breaks might be there). Two possible situations are depicted in Figure~\ref{figure:black_breaks_1}. 
Our aim is now to count the number of mismatches between  and , which are length- subwords of periodic stretches from the text and pattern, respectively. If  then by Proposition~\ref{proposition:periodic_mismatches} either there are no mismatches between  and , or there are at least  of them. It is easy to detect which case occurs: the strings agree if and only if their canonical periods are the same and they start with the same period offset, which can be determined in  time after some straightforward preprocessing. So we can assume . We consider the cases from the Figure~\ref{figure:black_breaks_1} separately.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{stretches_overlap}
\caption{Precomputation for a pair of periodic stretches.}
\label{figure:stretches_overlap}
\end{figure}

\begin{mycases}[listparindent=15pt]
\item In this case  is length- suffix of some text periodic stretch,  is length- prefix of some pattern periodic stretch. We want to precalculate all possible  results of such queries. Doing it as usually in  time per one is unfortunately too slow. Fix one pair of periodic stretches. We will calculate all the  required numbers in  total time. Let  be the canonical period of ,  and let  be the canonical period of . First calculate the answer for all overlaps of length at most  in  time. Now to process an overlap of length , we first use the result for , see Figure~\ref{figure:stretches_overlap}. Then we only need to take into account the prefix of length , or in other words add the number of mismatches between  and some factor of an infinite word . There are just  essentially different factors as far as counting mismatches is concerned, see Figure~\ref{figure:stretches_overlap}, so precomputing all these  numbers takes  time.

By the above discussion, we can precalculate all the required numbers in  time, which is fast enough, but unfortunately the space usage of  is 
too high for the purpose of achieving linear total space complexity. We will now explain how to decrease it to , i.e.,  per a pair of stretches,
while keeping the constant retrieval time. As previously, we compute and explicitly store the answers for all overlaps of length at most , which takes  space.
Now consider an overlap . The overlap can be partitioned into two pieces: the part of length
, and the remaining short part of length . The answer for the latter we have precomputed, so we only need how to compute the number of 
mismatches in the former, or in other words between a word of the form , and a word of the form , with some shift .
Denote by  the number of mismatches between  and  aligned with a shift . Then the sought number
is equal to

We need to evaluate such sum in constant time. For this we can arrange all  in a single array, and augment the array with all its prefix sums.
In more detail, we consider all  with the same remainder modulo  separately. If the remainder is , we put , ,
, and so on in the array, and then compute the prefix sums. Then, knowing where  appears in the array, we can compute the
sought sum in constant time. For each pair of stretches we need to store a constant number of arrays of size , so the total space complexity is as required.

\item In this case  is a complete periodic stretch, and  is a factor of a periodic stretch. Note that if  has period  then there are only  essentially different alignments of such form. Overall there are only  possible queries, so we precalculate all of them in  time.
\end{mycases}


Then we need to consider the more general situation when there are some white breaks between two consecutive black breaks , . Consider two cases.

\begin{figure}[t]
\includegraphics[width=\textwidth]{black_breaks_2}
\caption{Black breaks are represented as black boxes, white breaks are represented as grey boxes.}
\label{figure:black_breaks_2}
\vspace{0.2cm}
\includegraphics[width=\textwidth]{white_breaks_1}
\caption{Groups of white breaks are depicted as grey rectangles .}
\label{figure:white_breaks_1}
\vspace{0.2cm}
\includegraphics[width=\textwidth]{white_breaks_2}
\caption{Hamming distances that need to be precomputed. Note that  and  are multiples of , but the same is not necessarily true for  and .}
\label{figure:white_breaks_2}
\end{figure}

\begin{mycases}[listparindent=15pt]
\item Suppose among the white breaks between  and  there are both pattern breaks and text breaks. An example of such a situation is depicted in Figure~\ref{figure:black_breaks_2}, where one can see three regions .
The numbers of mismatches in regions  and  can be calculated in  time by the methods explained previously. Therefore, we concentrate on , which is the smallest region containing all white breaks between  and . The next step is to organize the white breaks into groups, where each group is a maximal set of consecutive white breaks of the same origin (meaning pattern break or text break),
see Figure~\ref{figure:white_breaks_1}.
Note that no two consecutive groups are of the same type, so the distances between them are at least . By Proposition~\ref{proposition:periodic_mismatches}, if the current alignment is a match, the overlap between  and  matches exactly, the same applies for  and . Hence, assuming there is a match at the current alignment, all  have the same canonical period . Moreover, because all periodic stretches are power of the same primitive word, the lengths of  and  must be multiplies of .
We can easily verify these conditions using some additional precomputed data. If the conditions are satisfied, then the total number of mismatches in  is equal to the Hamming distance between  and  plus the Hamming distance between  and , where  () denotes the -periodic word with the appropriate length starting (ending) with , see Figure~\ref{figure:white_breaks_2}. In the general case we need to know for any fragment of the pattern or the text starting and ending with a break its distance to  and . For each such fragment the only two interesting candidates for  are the canonical period of the stretch preceding and the canonical period of the stretch succeeding the first group in the fragment, so we can simply store both results and always use the appropriate one. The whole preprocessing costs us  time per fragment, so  time in total.

\item All white breaks between  and  are of the same type, say for example they are all text breaks. Similarly as in the previous case, we will argue that it is enough to precompute the Hamming distances of text fragments with some -periodic strings. We start with locating the rightmost black pattern break on the left of all our white breaks, and the leftmost black pattern break on the right. Call the region between them , and the region containing all white breaks , see Figure~\ref{figure:white_breaks_3}. As in the previous case, the nontrivial part is counting the number of mismatches within  in constant time. Let  be the fragment of the text corresponding to , and  the canonical period of the periodic stretch corresponding to  in the pattern. As previously, we want to precompute the number of mismatches between  and some -periodic string. As opposed to the previous case, now it is not trivial to find just a few interesting candidates for  in the preprocessing phase, and we cannot afford to perform the precomputation for all possible canonical periods. Nevertheless, it is possible to find for each fragment a unique potential candidate for  using a somewhat more complex reasoning, as shown below.

\begin{figure}[t]
\includegraphics[width=\textwidth]{white_breaks_3}
\caption{There may be many text breaks in  and .}
\label{figure:white_breaks_3}
\end{figure}

Define a new region  to be  extended by exactly  on both sides (note it is still fully contained inside ), and suppose the pattern matches at the current alignment. For this to happen, necessarily most of the periodic stretches in the text within  should have the same canonical period , which is formalized below.
\begin{proposition}
\label{proposition:most periods}
The total length of the periodic stretches in the text within , whose canonical period is not , doesn't exceed . 
\end{proposition}
\begin{proof}
A periodic fragment of length  whose canonical period is other than  generates at least  mismatches (because in every fragment of length  there is at least one mismatch, see the proof of Proposition~\ref{proposition:periodic_mismatches}).
Let the lengths of the stretches with a wrong canonical period be , and assume that . Then the number of mismatches is at least .  Because the whole text contains at most  breaks, , so there are at least  mismatches.
\qed
\end{proof}
\begin{definition}
 is the total length of the periodic stretches in the text within  having canonical period .
\end{definition} 
Note that the length of  exceeds . Thus, taking into account the at most  occurrences of breaks in region , each of length at most , and plugging in Proposition~\ref{proposition:most periods}, we obtain that: 

Now suppose that we choose  to be the canonical period of (any) periodic stretch in the text (within ), which maximizes , and use it instead of the (unknown in the preprocessing stage) . We take each text fragment starting and ending with a break, extend it on both sides by , find  maximizing the value of  (in case there is a tie, choose any of them). Then, we calculate the number of mismatches between this fragment and all  relevant factors of . There are  fragments of such a form, each is processed in  time, so the total preprocessing time is . Then to count the mismatches between  and the corresponding factor of , we first check what  we have performed the precomputation for. If , we can use the precomputed value.
If , we claim that the current alignment cannot correspond to a match. Indeed, otherwise we would have , so  would be the unique canonical period maximizing .
\end{mycases}

By the arguments given above, whenever we have an alignment with  black breaks, we may partition it into  regions and either count the number of mismatches in each of them, or report that it exceeds , in constant time, assuming a previous precomputation requiring  time. Thus the theorem.

\begin{theorem}\label{theorem:algorithm_highlyperiodic_fast}
For highly periodic patterns, pattern matching with  mismatches in pc-strings can be solved in  time using  additional space.
\end{theorem}

It is now a good moment to specify . Let . Using Theorem~\ref{theorem:algorithm_nonperiodic} and Theorem~\ref{theorem:algorithm_highlyperiodic_fast} we see that such a choice of  gives us a running time  for pattern matching with  mismatches in pc-strings. The additional space needed is .

\begin{remark}
Recall we assumed earlier that . Thus we should check, whether the choice  fulfils such requirement. Unfortunately, it doesn't whenever . But in such a case we can uncompress the pc-string and use the  algorithm~\cite{LandauMismatches}, also obtaining  running time.
\end{remark}

By Theorem~\ref{theorem:reduce_to_PCstrings} we obtain automatically that pattern matching with  mismatches in LZW-compressed text can be solved in , which is  because . The space complexity is . This is bounded by  whenever . In the opposite case we use the  algorithm~\cite{LandauMismatches} to process each pc-string using
 space and  total time.

\begin{theorem}
Pattern matching with  mismatches in LZW-compressed strings can be solved in  time and  space.
\end{theorem}

\section{Algorithm for pattern matching with errors}
\label{section:errors}
In this section we discuss the algorithm for pattern matching with  errors in pc-strings. It is obtained by combining our methods for compressed strings (applied for pattern matching with mismatches) with the ideas used by Cole and Hariharan~\cite{ColeHariharan}. In order to explain how to combine them, we need to restate most of their method. We skip the proofs whenever they don't require any modification to work in our setting.

In the following text, to simplify the description, we often say ``find the edit distance between  and ". By this we actually mean ``find the exact value of the edit distance between  and  if it does not exceed ,  or report that it is greater than  in the opposite case". We also often say ``matches at a given position'', which actually means ``there is a subword of the text ending at a given position such that the edit distance between it and the pattern is at most ''.

We start with a lemma which allows us to efficiently verify a potential match.

\begin{lemma}[see Section 5 of~\cite{ColeHariharan}]\label{lemma:verify_match}
Suppose  is a text and  is a pattern (given in some form), and assume that it is possible to answer  queries between their factors in constant time.
\begin{enumerate}
\item{We can verify whether  matches (with at most  errors) at a given position in  time.}
\item{We can find all matches ending in a given window of length  in  time.}
\item{Given  (a subword of ) and  (a subword of ), we can calculate the edit distances between all  longest suffixes of  and all  longest suffixes of  in  time.}
\end{enumerate}
\end{lemma}

In our setting, the text is given as a pc-string. In such a case it is not trivial to answer a  query in constant time, as this would require a predecessor search to locate the relevant block. Hence we need to maintain the current position in the text during the computation, or more precisely the current block and the current letter there. Inspecting the proof of the above lemma gives us that this can be easily done without sacrificing the time complexity, as long as all input positions (including the endpoints of a window or a subword) are given in such form.

Now we are ready to extend our methods for pattern matching with mismatches to the current setting. Recall that in this problem we are asked to find all ending positions of matches. Nevertheless, for convenience we will concentrate on the starting positions. This is justified, because in case of pc-strings the situation is symmetric. Let us start with the simpler case of non-periodic patterns.
\begin{theorem}\label{theorem:algorithm_errors_nonperiodic}
Suppose the pattern contains at least  disjoint -breaks. Then pattern matching with  errors in pc-strings can be solved in  time.
\end{theorem}
\begin{proof}
We modify the reasoning from Lemma~\ref{lemma:sparsify_matches} to show that there are at most  text windows of size  containing potential matches. Partition the text into disjoint fragments of length . For each -break  we do the marking in exactly the same way as in Lemma~\ref{lemma:sparsify_matches}. Consider now a fixed length- window in the text, it is easy to see that if there is some match ending there, then the total number of marks in the window and its two neighbouring windows must be at least . Since the total number of marks is only , we conclude that there are only  length- windows containing matches. 

Now we proceed as in the proof of Theorem~\ref{theorem:algorithm_nonperiodic}. First we use Lemma~\ref{lemma:break_matching} to simulate the marking as above. This gives us in  time at most  length- windows where some matches could potentially begin. We verify all these windows in  time per one using Lemma~\ref{lemma:verify_match}. The total running time is . 
\qed
\end{proof}

We are left with the case when the pattern has at most  disjoint -breaks. We argue (as in the case of mismatches) that the text (given as a pc-string) can be trimmed in  time to a string having at most  disjoint -breaks without losing any matches, see Lemma~\ref{lemma:discarding_breaks} and Proposition~\ref{proposition:algorithm_text_breaks}. Thus now the problem to solve is: given a pc-string  and a pattern , both containing at most  disjoint -breaks, find all starting positions of matches (with at most  errors) of  in . We will construct an algorithm solving this problem in  time.

The first step is to deal with the inconvenient situation when we have two or more breaks too close to each other in the text or pattern. For this, as in~\cite{ColeHariharan}, we use the notion of {\it intervals}. Consider a maximal group of (say, pattern) breaks such that the periodic stretch between neighbouring breaks is shorter than . We take the substring  of the pattern containing all breaks from the group and extend it on both sides so that it starts and ends with a suitable periodic fragment. More precisely, suppose on the left of  there is a periodic stretch with canonical period , then we extend  to the left, so that the length of the added part is between  and , and has prefix . Then the extended string starts with at least  repetitions of . Do the same on the right side and call the resulting string , then  is an interval. Note that there are situations when the extension to the left  or to the right is impossible, because the string  lies to close to some endpoint. In such a case we extend  till the left (right) endpoint and say that  is incomplete on the left (on the right). We process all breaks in the text in the same manner.

If the pattern is shorter than , it can happen that there is only one interval incomplete on both sides. We don't want to deal with such pathological situations, so we decompress the pc-string and apply the Landau-Vishkin algorithm~\cite{Landau}. This results in a  running time for this case. From now on, we assume that there are no intervals which are incomplete on both sides.

Consider now all alignments of the pattern in the text such that some text break (or some text endpoint) is within distance  from some pattern break (or some pattern endpoint). It is easy to see, that there are at most  windows of size  of such positions. In our algorithm, we verify all these windows in total time  using Lemma~\ref{lemma:verify_match} to process whole windows. So let us now concentrate on the remaining {\it fine} alignments. Observe that now no two intervals overlap. Moreover, if the current position corresponds to a match, all periodic stretches between the intervals in both the pattern and the text have the same canonical period , see Lemma 7.1 in~\cite{ColeHariharan}. This observation greatly simplifies the calculation of the number of errors at a fine alignment. To proceed, we need one more definition.

\begin{definition}
Let  be any interval and  some primitive string. We define the locked edit distance of  as follows.
\begin{enumerate}
\item{If  is complete on both sides, then: }
\item{If  is incomplete on the right, then: }
\item{If  is incomplete on the left, then: }
\end{enumerate}
\end{definition}
Note that we can perform one computation of  in  time. Assume for example that  is complete on both sides. We calculate the biggest  for which . To find the answer, it is enough to compute the edit distances between  and  longest suffixes of  (in fact we are interested only in suffixes being multiples of , but in the worst possible case , so it doesn't help much) in  time using Lemma~\ref{lemma:verify_match}. We calculate all the locked edit distances of all  intervals in the preprocessing stage in  time.

Our goal is now to present simple formulas for computing the number of errors at a given fine alignment. Let  be the sequence of intervals (both pattern and text) involved in this alignment in the same order as they appear. Observe that there are only  possible sequences of intervals we need to consider (each corresponding to some set of consecutive fine alignments). Let us analyze two cases separately: when the first interval  comes from the pattern, and when it comes from the text.

\begin{figure}[t]
\includegraphics[width=\textwidth]{leftvalp}
\caption{Fine alignment where the first interval comes from the pattern.}
\label{figure:leftvalp}
\end{figure}

\begin{figure}[t]
\includegraphics[width=\textwidth]{leftvalt}
\caption{Fine alignment where the first interval comes from the text.}
\label{figure:leftvalt}

\end{figure}

\begin{mycases}
\item{The situation is depicted in Figure~\ref{figure:leftvalp}. We denote by  the suffix of  which appears at the beginning of the alignment in the text. Also let us call  the prefix of the pattern ending at the last character of . Define . Cole and Hariharan showed that the number of errors at the considered alignment equals to: 
Thus we need to calculate , which can be done in  in a similar way as computing locked edit distances. Note also that the string  doesn't change at different alignments, so it is enough to precompute all 's for all  different values of . We do it in  time in the preprocessing phase, so all alignments with  coming from the pattern can be served in  total time. 
 
 }
\item{The situation is depicted in Figure~\ref{figure:leftvalt}. We denote by  the suffix of  which appears at the beginning of the pattern. Also let us call  the factor of the text starting at the alignment position and ending at the last character of . Define . Cole and Hariharan showed that the number of errors at the considered alignment equals to:   
Note that the situation now seems to be more complicated than in Case 1, because there are many possibilities for the string . However,  is always of the form  and one can see (or find in \cite{ColeHariharan}), that  (the main reason being that  starts with many copies of , so one of them have to match exactly). This allows us to precalculate only  for all possible suffixes  of . There are  candidates for , so the precomputation of all 's can be performed in  time. Hence all alignments with  coming from the text can be served in  time.

}
\end{mycases}

The above analysis shows that all fine alignments can be served in total time . So assuming the pattern is highly periodic, we have just obtained an algorithm for pattern matching with  errors in pc-strings with  running time. This allows us to prove:

\begin{theorem}\label{theorem:algorithm_errors_highlyperiodic}
For highly periodic patterns, pattern matching with  errors in pc-strings can be solved in  time using  additional space.
\end{theorem}

Now we can finally specify . Using Theorem~\ref{theorem:algorithm_errors_nonperiodic} and Theorem~\ref{theorem:algorithm_errors_highlyperiodic} and choosing  gives us a running time of . Then by Theorem~\ref{theorem:reduce_to_PCstrings} we obtain that pattern matching with  errors in LZW-compressed text can be solved in  time. The space complexity is , so if , it can be bounded by . If  is larger, we use the  algorithm~\cite{Landau} to process each pc-string using  space and  total time.

\begin{theorem}
Pattern matching with  errors in LZW-compressed strings can be solved in  time and  space.
\end{theorem}

\section{Conclusions}

We constructed efficient algorithms for pattern matching with  mismatches or errors in LZW-compressed strings. The main difference with the previously known solutions is that we used both a periodicity-based argument, and the repetitive structure of a compressed string, which allowed us to achieve a better running time for small values of , which seems to be the most natural setting. Our solutions achieve  running time, where  is some function depending only on the bound on the number of errors , while the complexity of the best previously known algorithms was of the form .

A natural question is whether our complexities can be improved. More concretely, in pattern matching with  mismatches instead of considering each non-fine alignment separately, we were able to analyze them in a more global manner in Section~\ref{section:faster}. Can a similar reasoning be applied to accelerate pattern matching with  errors?

Furthermore, is it possible to obtain an algorithm with running time , where , or maybe even ?

\bibliographystyle{splncs03}
\bibliography{biblio}

\end{document}
