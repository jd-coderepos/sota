\RequirePackage{fix-cm}
\documentclass[twocolumn,compsoc]{CVM}

\setcounter{page}{1}
\graphicspath{{figures/},{figures/cvm/},{figures/Imgs/},{figures/pictures/}}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{rotating}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{overpic}
\usepackage{colortbl}
 
\let\Letter\relax
\usepackage{marvosym}
\usepackage{pifont}

\definecolor{green}{RGB}{0,139,69} 
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue]{hyperref}

\usepackage{hyperref}
\makeatletter
\def\UrlAlphabet{\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X \do\Y\do\Z}
\def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\g@addto@macro{\UrlBreaks}{\UrlAlphabet}
\g@addto@macro{\UrlBreaks}{\UrlDigits}
\makeatother


\definecolor{mygray}{gray}{.90}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\jgp}[1]{{\textcolor{red}{#1}}}
\newcommand{\fdp}[1]{{\textcolor{cyan}{#1}}}

\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\myPara}[1]{\vspace{.05in}\noindent\textbf{#1~}}
\newcommand{\supp}[1]{\textcolor{magenta}{#1}}


\def\ourmodel{\textit{FSNet}}

\def\ManuscriptInfo{Manuscript received: 2021-08-31; accepted: 20xx-xx-xx.}

\def\PaperTitle{Full-Duplex Strategy for Video Object Segmentation}

\def\AuthorNames{
Ge-Peng~Ji, Deng-Ping Fan(\Letter), Keren Fu, Zhe Wu, Jianbing Shen, and Ling Shao}

\def\AuthorAdress{
* & College of Computer Science, Nankai University, Tianjin, China.\\
 & School of Computer Science, Wuhan University, Wuhan, China.\\
 & College of Computer Science, Sichuan University, China.\\ 
 & Peng Cheng Laboratory, Shenzhen, China.\\
 & School of Computer Science, Beijing Institute of Technology, Beijing, China. \\
 & IIAI, Abu Dhabi, UAE.\\ 
&Corresponding author: dengpingfan@mail.nankai.edu.cn\\
}

\def\firstheader{DOI 10.1007/s41095-xxx-xxxx-x\hfill Vol. x, No. x, month year, xx--xx\
     \mathcal{Q}_{k}^{X} = \mathcal{X}_k \otimes \sigma\left[\theta(\mathcal{V}_{k}^{Y};\mathbf{W}_\theta)\right],

     \mathcal{Q}_{k}^{Y} = \mathcal{Y}_k \otimes \sigma\left[\phi(\mathcal{V}_{k}^{X};\mathbf{W}_\phi)\right].
\label{equ:3}
  \mathcal{Z}_k = \mathcal{B}_k \left[ \mathcal{Q}^{X}_{k} \oplus \mathcal{Q}^{Y}_{k} \oplus \mathcal{Z}_{k-1} \right],
\label{equ:allocator}
    \mathbf{F}_{k}^{n} = \psi_{F}(\mathcal{Z}_k;\mathbf{W}_\psi^{F}),~
    \mathbf{G}_{k}^{n} = \psi_{G}(\mathcal{Y}_k;\mathbf{W}_\psi^{G}),
\label{equ:5}
\mathbf{F}_{k}^{n+1} = \mathbf{F}_{k}^{n} \oplus \bigcup_{i=k}^{K} \left[ \mathbf{F}_{k}^{n},\mathcal{P} ( \mathbf{G}_{i}^{n}) \right],

\mathbf{G}_{k}^{n+1} = \mathbf{G}_{k}^{n} \oplus \bigcap_{j=k}^{K}[ \mathbf{G}_{k}^{n},\mathcal{P}(\mathbf{F}_{j}^{n})],
\label{equ:ppm_1}
    \hat{\mathbf{F}}_{k}^{N} = \mathcal{C} [ \mathbf{F}_{k}^{N} \odot \mathcal{UP}(\hat{\mathbf{F}}_{k+1}^{N}) ],
\label{equ:ppm_2}
    \hat{\mathbf{G}}_{k}^{N} = \mathcal{C} [ \mathbf{G}_{k}^{N} \odot \mathcal{UP}(\hat{\mathbf{G}}_{k+1}^{N}) ].

\begin{aligned}
    \mathcal{L}_{bce}(\mathbf{S}^{t},\mathbf{G}^{t}) = 
    &-\sum_{(x, y)}[\mathbf{G}^{t}(x, y) \log (\mathbf{S}^{t}(x, y)) \\ 
    &+ (1 - \mathbf{G}^{t}(x, y)) \log (1-\mathbf{S}^{t}(x, y))],
\end{aligned}

    \mathcal{L}_{total} = \mathcal{L}_{bce}(\mathbf{S}_{A}^{t},\mathbf{G}^{t}) + \mathcal{L}_{bce}(\mathbf{S}_{M}^{t},\mathbf{G}^{t}).
\label{equ:region_sim}
      \mathcal{J} = \frac{|\mathbf{S}_{A}^{t} \cap \mathbf{G}^t|}{|\mathbf{S}_{A}^{t} \cup \mathbf{G}^t|},
    \label{equ:contour}
      \mathcal{F} = \frac{2 \times \textrm{Precision}_{c} \times \textrm{Recall}_{c}}{\textrm{Precision}_c + \textrm{Recall}_c},
    \label{equ:mae}
      \mathcal{M} = \frac{1}{W \times H} \sum_{x}^{W} \sum_{y}^{H} |\mathbf{S}_{A}^{t}(x,y) - \mathbf{G}^t(x,y)|,
    
        \textrm{Precision}=\frac{|\mathbf{S}_{A}^{t}(T) \cap \mathbf{G}^t|}{|\mathbf{S}_{A}^{t}(T)|},
    
        \textrm{Recall} =\frac{|\mathbf{S}_{A}^{t}(T)\cap \mathbf{G}^t|}{|\mathbf{G}^t|},
    \label{equ:max_f}
      F_\beta = \frac{(1+\beta^2) \textrm{Precision} \times \textrm{Recall}} {\beta^{2} \times \textrm{Precision} + \textrm{Recall}},
    \label{equ:e_m}
      E_{\xi} = \frac{1}{W \times H} \sum_{x}^{W} \sum_{y}^{H} \phi\left[\mathbf{S}_{A}^{t}(x, y), \mathbf{G}^t(x, y)\right],
    \label{equ:s_m}
      S_\alpha = (1-\alpha) \times S_o(\mathbf{S}_{A}^{t}, \mathbf{G}^t) + \alpha \times S_r(\mathbf{S}_{A}^{t}, \mathbf{G}^t),
    
where  balances the object-aware similarity  and region-aware similarity . We use the default setting () suggested in \cite{fan2017structure}.
    
    
\end{enumerate}




\begin{table*}[t!]
\centering
  \footnotesize
  \renewcommand{\arraystretch}{1.3}
  \setlength\tabcolsep{0.9pt}
  \caption{Video object segmentation (VOS) performance of our~\ourmodel, compared with 14 SOTA unsupervised models and seven semi-supervised models on DAVIS~\cite{perazzi2016benchmark} validation set. 
  `\textit{w/} Flow': the optical flow algorithm is used.
  `\textit{w/} CRF': conditional random field~\cite{krahenbuhl2011efficient} is used for post-processing.
  The best scores are marked in \textbf{bold}.}
  \label{tab:davis16}
  \begin{tabular}{cl|cc|cccccccccccccc|ccccccccccc}
  \toprule
   & & \multicolumn{16}{c|}{Unsupervised} & \multicolumn{7}{c}{Semi-supervised}\\
   \cline{3-25}
   \multirow{0}{*}{} & \multirow{0}{*}{Metrics}
       & \multicolumn{2}{c|}{\textbf{\ourmodel}}
& MAT    & AGNN      & AnDiff    & COS    & AGS & EpO+   & MOA       & LSMO      & ARP       & LVO & LMP    & SFL       & ELM       & FST
& CFBI & AGA  & RGM   & FEEL  & FA	 & OS & MSK \\  & & \multicolumn{2}{c|}{\textbf{(Ours)}}
& \cite{zhou2020motion_attentive}
       & \cite{Wang_2019_ICCV}
       & \cite{yang2019anchor} & \cite{lu2019see} & \cite{wang2019learning} & \cite{faisal2019exploiting} & \cite{siam2019video}
       & \cite{tokmakov2019learning}     & \cite{koh2017primary} & \cite{tokmakov2017learning1}
       & \cite{tokmakov2017learning2}    & \cite{cheng2017segflow}  & \cite{lao2018extending} & \cite{papazoglou2013fast}
& \cite{yang2020collaborative} & \cite{johnander2019generative}  & \cite{wug2018fast}   & \cite{voigtlaender2019feelvos}  & \cite{cheng2018fast}	 & \cite{caelles2017one}
       & \cite{perazzi2017learning} \\
  \hline
        & \textit{w/} Flow        
    & \checkmark  & \checkmark
    & \checkmark  &                 
    &                 &                 &                 & \checkmark  & \checkmark
    & \checkmark  & \checkmark  
    & \checkmark  & \checkmark  & \checkmark  & \checkmark  & \checkmark    & 
    &                 &                 &                 &                 &                   & \checkmark \\
        & \textit{w/} CRF         
    & \checkmark  &                 
    & \checkmark  & \checkmark  &                 & \checkmark
    & \checkmark  & \checkmark  & \checkmark  & \checkmark  & 
    & \checkmark  & \checkmark  & 
    &                 &                 &                &               & \checkmark
    &                 & \checkmark  &                 & \checkmark  \\
  \hline
        & Mean-
    & \cellcolor{mygray}{\textbf{83.4}} & 82.1
    & 82.4 & 80.7
    & 81.7 & 80.5 & 79.7 & 80.6 & 77.2
    & 78.2 & 76.2 & 75.9
    & 70.0 & 67.4 & 61.8 & 55.8
    & \cellcolor{mygray}{\textbf{85.3}} & 81.5 & 81.5 & 81.1 & 82.4 & 79.8 & 79.7 \\
        & Mean-
    & 83.1 & \cellcolor{mygray}{\textbf{83.3}}
    & 80.7 & 79.1
    & 80.5 & 79.5 & 77.4 & 75.5 & 77.4
    & 75.9 & 70.6 & 72.1
    & 65.9 & 66.7 & 61.2 & 51.1
    & \cellcolor{mygray}{\textbf{86.9}} & 82.2 & 82.0 & 82.2 & 79.5 & 80.6 & 75.4 \\
  \bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t!]
\centering
  \footnotesize
  \renewcommand{\arraystretch}{1.0}
  \setlength\tabcolsep{6pt}
  \caption{Video salient object detection (V-SOD) performance of our \ourmodel, compared with 13 SOTA models on three popular V-SOD datasets, including DAVIS~\cite{perazzi2016benchmark}, MCL~\cite{kim2015spatiotemporal}, and FBMS~\cite{ochs2013segmentation}.
`' denotes that we generate non-binary saliency maps without CRF~\cite{krahenbuhl2011efficient} for a fair comparison.
  `N/A' means the results are not available.}
  \label{tab:score_VOS}
  \begin{tabular}{rr|cccc|cccc|cccc}
\toprule
   &  &\multicolumn{4}{c|}{DAVIS~\cite{perazzi2016benchmark}}&\multicolumn{4}{c|}{MCL~\cite{kim2015spatiotemporal}} 
  & \multicolumn{4}{c}{FBMS~\cite{ochs2013segmentation}}
\\
   \cline{3-14}
   \multirow{-2}{*}{} & \multirow{1}{*}{Model}
       &  & 
       &  & 
       &  & 
       &  & 
       &  &  
       &  &  
\\
  \hline
  \multirow{5}{*}{\begin{sideways}2018\end{sideways}}
  & MBN~\cite{li2018unsupervisedECCV}  
    & 0.887   & 0.966   & 0.862   & 0.031
    & 0.755   & 0.858   & 0.698   & 0.119
    & 0.857   & 0.892   & 0.816   & 0.047
\\
  & FGRN~\cite{li2018flow} 
    & 0.838   & 0.917   & 0.783   & 0.043
    & 0.709   & 0.817   & 0.625   & 0.044
    & 0.809   & 0.863   & 0.767   & 0.088
\\
  & SCNN~\cite{tang2018weakly}
    & 0.761   & 0.843   & 0.679   & 0.077
    & 0.730   & 0.828   & 0.628   & 0.054
    & 0.794   & 0.865   & 0.762   & 0.095
\\
  & DLVS~\cite{wang2017videoTIP}
    & 0.802   & 0.895   & 0.721   & 0.055
    & 0.682   & 0.810   & 0.551   & 0.060
    & 0.794   & 0.861   & 0.759   & 0.091
\\
  & SCOM~\cite{chen2018scom}
    & 0.814   & 0.874   & 0.746   & 0.055
    & 0.569   & 0.704   & 0.422   & 0.204
    & 0.794   & 0.873   & 0.797   & 0.079
\\
  \hline
  \multirow{9}{*}{\begin{sideways}20192020\end{sideways}}
  & RSE~\cite{xu2019video}
    & 0.748   & 0.878   & 0.698   & 0.063
    & 0.682   & 0.657   & 0.576   & 0.073
    & 0.670   & 0.790   & 0.652   & 0.128
\\
  & SRP~\cite{cong2019video}
    & 0.662   & 0.843   & 0.660   & 0.070
    & 0.689   & 0.812   & 0.646   & 0.058
    & 0.648   & 0.773   & 0.671   & 0.134
\\
  & MESO~\cite{xu2019TMMvideo}
    & 0.718   & 0.853   & 0.660   & 0.070
    & 0.477   & 0.730   & 0.144   & 0.102
    & 0.635   & 0.767   & 0.618   & 0.134
\\
  & LTSI~\cite{chen2019improved}
    & 0.876   & 0.957   & 0.850   & 0.034
    & 0.768   & 0.872   & 0.667   & 0.044
    & 0.805   & 0.871   & 0.799   & 0.087
\\
  & SPD~\cite{li2019accurate}
    & 0.783   & 0.892   & 0.763   & 0.061
    & 0.685   & 0.794   & 0.601   & 0.069
    & 0.691   & 0.804   & 0.686   & 0.125
\\
  & SSAV\cite{fan2019shifting}
    & 0.893   & 0.948   & 0.861   & 0.028
    & 0.819   & 0.889   & 0.773   & 0.026
    & 0.879   & 0.926   & 0.865   & 0.040
\\	
  & RCR\cite{yan2019semi}
    & 0.886   & 0.947   & 0.848   & 0.027
    & 0.820   & 0.895   & 0.742   & 0.028
    & 0.872   & 0.905   & 0.859   & 0.053
\\
  & PCSA\cite{gu2020pyramid} 
& 0.902   & 0.961   & 0.880   & 0.022
    & N/A     &  N/A    & N/A     & N/A
    & 0.868   & 0.920   & 0.837   & \textbf{0.040}
\\
    \hline
  \rowcolor{mygray}
  &\textbf{\ourmodel} 
& \textbf{0.920}   & \textbf{0.970}   & \textbf{0.907}   & \textbf{0.020}
    & \textbf{0.864}   & \textbf{0.924}   & \textbf{0.821}   & \textbf{0.023}
    & \textbf{0.890}   & \textbf{0.935}   & \textbf{0.888}   & 0.041
\\	
\bottomrule
  \end{tabular}
\end{table*}

\subsection{U-VOS and V-SOD tasks}


\subsubsection{Evaluation on DAVIS dataset}\label{sec:evaluation_on_davis16}
As shown in~\tabref{tab:davis16}, we compare our \ourmodel~with 14 SOTA U-VOS models on the DAVIS public leaderboard.
We also compare it with seven recent semi-supervised approaches as reference. 
We use a threshold of 0.5 to generate the final binary maps for a fair comparison, as recommended by~\cite{yang2019anchor}. 
Our \ourmodel~outperforms the best model (AAAI'20-MAT~\cite{zhou2020motion_attentive}) by a margin of 2.4\% in Mean- and 1.0\% in Mean-, achieving the new SOTA performance. 
Notably, the proposed U-VOS model also outperforms the semi-supervised model (\eg, AGA~\cite{johnander2019generative}), even though it utilizes the first ground-truth mask to reference object location.

We also compare \ourmodel~against 13 SOTA V-SOD models. The non-binary saliency maps\footnote{Note that all compared maps in the V-SOD task, including ours, are non-binary.} are obtained from the standard benchmark~\cite{fan2019shifting}. 
This can be seen from~\tabref{tab:score_VOS}, our method consistently outperforms all other models since 2018 on all metrics. In particular, for the  and  metrics, our method improves the performance by 2.0\% compared with the best AAAI'20-PCAS~\cite{gu2020pyramid} model. 



\subsubsection{Evaluation on MCL dataset} 
This dataset has fuzzy object boundaries in the low-resolution frames due to fast object movements. 
Therefore, the overall performance is lower than on DAVIS. As shown in~\tabref{tab:score_VOS}, our method still stands out in these extreme circumstances, with a 3.08.0\% increase in all metrics compared with ICCV'19-RCR~\cite{yan2019semi} and CVPR'19-SSAV~\cite{fan2019shifting}. 

\subsubsection{Evaluation on FBMS dataset} 
This is one of the most popular VOS datasets with diverse attributes, such as interacting objects, dynamic backgrounds, and no per-frame annotation. As shown in \tabref{tab:score_VOS}, our model achieves competitive performance in terms of .
Further, compared to the previous best-performing SSAV~\cite{fan2019shifting}, it obtains improvements in other metrics, including  (0.890 \textit{vs.} SSAV=0.879) and  (0.935 \textit{vs.} SSAV=0.926), making it more suitable to the human visual system (HVS) as mentioned in~\cite{fan2017structure,fan2018enhanced}.








\begin{table*}[t!]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.0}
  \setlength\tabcolsep{6.0pt}
  \caption{Benchmarking results of 13 state-of-the-art V-SOD models on three subsets of DAVSOD~\cite{fan2019shifting}. `' denotes that we generate non-binary saliency maps without CRF~\cite{krahenbuhl2011efficient} for a fair comparison. `N/A' means the results are not available. 
}
  \label{tab:davsod_benchmark}
  \begin{tabular}{rr|cccc|cccc|cccc}
  \toprule
   & & \multicolumn{4}{c|}{DAVSOD-Easy35}
   &\multicolumn{4}{c|}{DAVSOD-Normal25}
   &\multicolumn{4}{c}{DAVSOD-Difficult20} \\
   \cline{3-14}
&Model
   & \multicolumn{1}{c}{}   &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c|}{}
   & \multicolumn{1}{c}{}   &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c|}{}
   & \multicolumn{1}{c}{}   &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
  \hline
\multirow{5}{*}{\begin{sideways}2018\end{sideways}}
&MBN~\cite{li2018unsupervisedECCV} 
& 0.646 & 0.694 & 0.506 &0.109 & 0.597 & 0.665 & 0.436 &0.127 & 0.561 & 0.635 & 0.352 &0.140 \\
&FGRN~\cite{li2018flow}            
& 0.701 & 0.765 & 0.589 &0.095 & 0.638 & 0.700 & 0.468 &0.126 & 0.608 & 0.698 & 0.390 &0.131 \\
&SCNN~\cite{tang2018weakly}        
& 0.680 & 0.745 & 0.541 &0.127 & 0.589 & 0.685 & 0.425 &0.193 & 0.533 & 0.677 & 0.345 &0.234 \\
&DLVS~\cite{wang2017videoTIP}         
& 0.664 & 0.737 & 0.541 &0.129 & 0.599 & 0.670 & 0.416 &0.147 & 0.571 & 0.687 & 0.336 &0.128 \\
&SCOM~\cite{chen2018scom}          
& 0.603 & 0.669 & 0.473 &0.219 & N/A & N/A & N/A & N/A & N/A & N/A & N/A & N/A \\
\hline
\multirow{9}{*}{\begin{sideways}20192020\end{sideways}}
&RSE~\cite{xu2019video} 
& 0.577 & 0.663 & 0.417 &0.146 & 0.549 & 0.590 & 0.360 &0.170 & 0.555 & 0.644 & 0.306 &0.130 \\
&SRP~\cite{cong2019video}
& 0.575 & 0.655 & 0.453 &0.146 & 0.545 & 0.601 & 0.387 &0.169 & 0.555 & 0.682 & 0.341 &0.123 \\
&MESO~\cite{xu2019TMMvideo}
& 0.549 & 0.673 & 0.360 &0.159 & 0.542 & 0.597 & 0.354 &0.165 & 0.556 & 0.661 & 0.310 &0.127 \\
&LTSI~\cite{chen2019improved}
& 0.695 & 0.769 & 0.585 &0.106 & 0.658 & 0.723 & 0.499 &0.128 & 0.618 & 0.718 & 0.406 &0.112 \\
&SPD~\cite{li2019accurate}
& 0.626 & 0.685 & 0.500 &0.138 & 0.596 & 0.633 & 0.443 &0.171 & 0.574 & 0.688 & 0.345 &0.137 \\
&SSAV\cite{fan2019shifting}
& 0.755 & 0.806 & 0.659 &0.084 & 0.661 & 0.723 & 0.509 &0.117 & 0.619 & 0.696 & 0.399 &0.114 \\	
&RCR\cite{yan2019semi}
& 0.741 & 0.803 & 0.653 &0.087 & 0.674 & 0.729 & 0.533 &0.118 & 0.644 & \textbf{0.768} & 0.444 & \textbf{0.094} \\
&PCSA\cite{gu2020pyramid}
& 0.741 & 0.793 & 0.656 &0.086 & N/A & N/A & N/A & N/A & N/A & N/A & N/A & N/A \\
\hline
\rowcolor{mygray}
&\textbf{\ourmodel}
& \textbf{0.773} & \textbf{0.825} & \textbf{0.685} & \textbf{0.072} 
& \textbf{0.707} & \textbf{0.764} & \textbf{0.597} & \textbf{0.104} 
& \textbf{0.662} & 0.752 & \textbf{0.487} & 0.099 \\
\bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Evaluation on SegTrack-V2 dataset} 
This is the earliest VOS dataset from the traditional era. Thus, only a limited number of deep U-VOS models have been tested on it. We only compare our \ourmodel~against the top-3 models: AAAI'20-PCAS~\cite{gu2020pyramid} (=0.866), ICCV'19-RCR~\cite{yan2019semi} (=0.842), and CVPR'19-SSAV~\cite{fan2019shifting} (=0.850). Our method achieves the best performance (=0.870).


\subsubsection{Evaluation on DAVSOD dataset}


Recently published, DAVSOD~\cite{fan2019shifting} is the most challenging visual attention consistent V-SOD dataset with high-quality annotations and diverse attributes.
It contains diversified challenging scenarios due to the video sequences containing shifts in attention.
DAVSOD is divided into three subsets, according to difficulty: DAVSOD-Easy-35 (35 clips), DAVSOD-Normal25 (25 clips), and DAVSOD-Difficult20 (20 clips). 
Note that, in the saliency field, non-binary maps are required for evaluation; thus, we only report the results of \ourmodel~without CRF post-processing in benchmarking the V-SOD task.
In this document, we adopt the four metrics mentioned in~\secref{sec:metric_vsod}, including , , , and .
For showing the robustness of \ourmodel, in~\tabref{tab:davsod_benchmark}, we also make the first effort to benchmark all 11 SOTA models since 2018, in terms of the three difficulty levels:




\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{./figures/pictures/PR-Curve-min}
  \caption{Precision-recall curves of SOTA V-SOD methods and the proposed \ourmodel~across six datasets. Zoom in for details and the best view in color for friendlier observation.}
  \label{fig:pr_curve}
\end{figure*}

\begin{itemize}
    \item \textbf{Easy35 subset:} Most of the video sequences are similar to those in the DAVIS dataset, which also consists of a large number of single video objects. We see that 
    \ourmodel~outperforms all the reported algorithms across all metrics. 
As shown in~\tabref{tab:davsod_benchmark}, compared with the recent method (PCSA), our model achieves large improvements of 3.2\% in terms of .
    
\item \textbf{Normal25 subset:} Different from previous subsets, this one includes multiple moving salient objects. Thus, it is more difficult than traditional V-SOD datasets due to the attention shift phenomena~\cite{fan2019shifting}. As expected, \ourmodel~still obtains the best performance, with significant improvement, \eg, 6.4\% for  metric.
    
\item \textbf{Difficult20 subset:} This is the most challenging subset in existing V-SOD datasets since it contains a large number of attention shift sequences under cluttered scenarios.
Therefore, from the results shown in~\tabref{tab:davsod_benchmark}, the performances of all the compared models decrease dramatically (\eg,  0.5). 
Even though our framework is not specifically designed for the V-SOD task, we still easily obtain the best performance in two metrics (\eg,  and ).
Different from the best two models, which utilize additional training data (\ie, RCR leverages pseudo-labels, SSAV utilizes the validation set), our model does not use any additional training data and still outperforms the SSAV model by 8.8\% (), and achieves comparable performance to the second-best RCR (ICCV'19) model.
These results are also supported by recent conclusions that ``human visual attention should be an underlying mechanism that drives U-VOS and V-SOD'' (TPAMI'20~\cite{wang2020paying}).
\end{itemize}



 
\subsubsection{PR Curve}\label{sec:pr_curve}
As shown in~\figref{fig:pr_curve}, we further investigate the precision-recall curves of different models on six V-SOD datasets, including DAVIS~\cite{perazzi2016benchmark}, MCL~\cite{kim2015spatiotemporal}, FBMS~\cite{ochs2013segmentation}, and DAVSOD~\cite{fan2019shifting} (\ie, Easy35, Normal25, and Difficult20).
Note that the higher and more to the right in the PR curve, the more accurate performance.
Even though existing SOTA methods have achieved significant progress in the V-SOD task on three typical benchmark datasets, we still obtain the best performance under all thresholds.
Besides, as a recent and challenging dataset, the overall performances on the three subsets of DAVSOD~\cite{fan2019shifting} are relatively poor. However, our \ourmodel~again achieves more satisfactory performance by large margins.


\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{./figures/pictures/performance-CVM-min}
  \caption{Qualitative results on five datasets, including  DAVIS~\cite{perazzi2016benchmark}, MCL~\cite{kim2015spatiotemporal},  FBMS~\cite{ochs2013segmentation}, SegTrack-V2~\cite{li2013video}, 
  and DAVSOD~\cite{fan2019shifting}. 
}
\label{Fig:performance1}
\end{figure*}


\subsubsection{Qualitative Results}
Some qualitative results on the five datasets are shown in~\figref{Fig:performance1}, validating that our method achieves high-quality U-VOS and V-SOD results. 
As can be seen in the 1 row, the behind camel did not move, so it does not get noticed.
Interestingly, as our full-duplex strategy model considers both appearance and motion bidirectionally, it can automatically predict the dominated camel in the centre of the video instead of the camel behind.
A similar phenomenon is also presented in the 5 row, our method successfully detects dynamic skiers with the video clip rather than the static man in the background.
Overall, for these challenging situations, \eg, 
dynamic background (1 \& 5 rows), 
fast-motion (4 row), 
out-of-view (6 \& 7 row),
occlusion (7 row), and 
deformation (8 row), 
our model is able to infer the real target object(s) with fine-grained details. 
From this point of view, we demonstrate that \ourmodel~is a general framework for both U-VOS and V-SOD tasks.









\subsection{Ablation Study}


In this section, we conduct ablation studies to analyse our \ourmodel, including stimulus selection (\secref{sec:stimulus_selection}), effectiveness of RCAM (\secref{sec:effectiveness_of_RCAM}) and BPM (\secref{sec:effectiveness_of_bpm}), number of cascaded BPMs (\secref{sec:number_of_bpms}), and effectiveness of full-duplex strategy (\secref{sec:effectivess_of_FS}).

\subsubsection{Stimulus Selection}\label{sec:stimulus_selection}
We explore the influence of different stimuli (appearance only \emph{vs.} motion only) in our framework. We use only video frames or motion maps (using~\cite{ilg2017flownet}) to train the ResNet-50~\cite{he2016deep} backbone together with the proposed decoder block (see~\secref{sec:decoder}). 
As shown in~\tabref{tab:ablation_icam},  performs slightly better than  in terms of  on DAVIS, which suggests that the ``optical flow'' setting can learn more visual cues than ``video frames''. 
Nevertheless,  outperforms  in  metric on MCL. 
This motivates us to explore how to use appearance and motion cues simultaneously effectively.

\subsubsection{Effectiveness of RCAM}\label{sec:effectiveness_of_RCAM}
To validate our RCAM (Rel.) effectiveness, we replace our fusion strategy with the vanilla fusion (Vanilla) using a concatenate operation followed by a convolutional layer to fuse two modalities. 
As expected (\tabref{tab:ablation_icam}), the proposed Rel. performs consistently better than the vanilla fusion strategy on both DAVIS and MCL datasets.
We would like to point out that our RCAM has two important properties: 
\begin{itemize}
\item It enables mutual correction and attention.

\item It can alleviate error propagation within a network to an extent due to the mutual correction and bidirectional interaction.
\end{itemize}


\begin{table}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \setlength\tabcolsep{1.8pt}
  \caption{Ablation studies (\secref{sec:stimulus_selection}, \secref{sec:effectiveness_of_RCAM}, \& \secref{sec:effectiveness_of_bpm}) for our components on DAVIS and MCL. We set  for BPM. 
  }\label{tab:ablation_icam}
  \begin{tabular}{r|cccc|cc|cc}
  \toprule
    &\multicolumn{4}{c|}{Component Settings} & \multicolumn{2}{c|}{DAVIS} &\multicolumn{2}{c}{MCL} \\
    \cline{2-9}
   & Appearance & Motion & RCAM & BPM 
   &  &
   & &  \\
  \hline
  & \checkmark& & &                                  & 0.834 & 0.047 & 0.754 & 0.038 \\
   & &\checkmark & &                                  & 0.858 & 0.039 & 0.763 & 0.053 \\
  Vanilla & \checkmark &\checkmark &  &                & 0.871 & 0.035 & 0.776 & 0.046 \\
  Rel.&\checkmark &\checkmark & \checkmark & & 0.900 & 0.025 & 0.833 & 0.031 \\
  Bi-Purf.& \checkmark &\checkmark &&\checkmark    & 0.904 & 0.026 & 0.855 & 0.023 \\
  \rowcolor{mygray}
  \hline
  \textbf{\ourmodel}&\checkmark &\checkmark &\checkmark &\checkmark & \textbf{0.920} & \textbf{0.020} & \textbf{0.864} & \textbf{0.023} \\
  \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \setlength\tabcolsep{4pt}
  \caption{Ablation study for the number () of BPMs on DAVIS~\cite{perazzi2016benchmark} and MCL~\cite{kim2015spatiotemporal}, focusing on parameter and FLOPs of BPMs, and runtime of~\ourmodel.
  }
  \label{tab:number_of_bpm}
  \begin{tabular}{r|c|c|c|cc|cc}
\toprule
   & Param. & FLOPs & Runtime & \multicolumn{2}{c|}{DAVIS} &\multicolumn{2}{c}{MCL} \\
  \cline{5-8}
   & (M) & (G) & (s/frame) 
   &  & 
   &  &  \\
  \hline
   & 0.000 & 0.000  & 0.03 & 0.900 &  0.025 & 0.833  & 0.031\\
 & 0.507 & 1.582 & 0.05 & 0.911 & 0.026 & 0.843 & 0.028 \\
\rowcolor{mygray}
  
  & 1.015 & 3.163 & 0.08 & \textbf{0.920} &  \textbf{0.020} & \textbf{0.864} & \textbf{0.023} \\
 & 1.522 & 4.745 & 0.10 & 0.918  & 0.023 & 0.863  & 0.023 \\
 & 2.030 & 6.327 & 0.13 & 0.920& 0.023 & 0.864 & 0.023 \\
\bottomrule \end{tabular}
\end{table}


\subsubsection{Effectiveness of BPM}\label{sec:effectiveness_of_bpm}

To illustrate the effectiveness of the BPM (with ), we derive two different models: Rel. and \ourmodel, referring to the framework \textit{without} or \textit{with} BPM. We observe that the model with BPM gains 2.03.0\% than the one without BPM, according to the statistics in \tabref{tab:ablation_icam}. We attribute this improvement to BPM's introduction of an interlaced decremental connection, enabling it to fuse the different signals effectively.
Similarly, we remove the RCAM and derive another pair of settings (Vanilla \& Bi-Purf.) to test the robustness of our BPM. The results show that even using the bidirectional vanilla fusion strategy (Bi-Purf.) can still enhance the stability and generalization of the model. This benefits from the purification forward process and re-calibration backward process in the whole network.



\begin{table}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \setlength\tabcolsep{1.2pt}
  \caption{Ablation study for the \textit{simplex} and \textit{full-duplex} strategies on DAVIS~\cite{perazzi2016benchmark} and MCL~\cite{kim2015spatiotemporal}. We set  for BPM. 
}
  \label{tab:ablation_bi_model}
  \begin{tabular}{c|c|c|cc|cc}
\toprule
   &\multicolumn{2}{c|}{Direction Setting} & \multicolumn{2}{c|}{DAVIS} &\multicolumn{2}{c}{MCL} \\
   \cline{2-7}
   & \multicolumn{1}{c|}{RCAM} & \multicolumn{1}{c|}{BPM} 
   &   & 
   &  & \\
  \hline
  \multirow{4}{*}{\begin{sideways} simplex \end{sideways}}
  &     & 
  & 0.896 & 0.026 & 0.816 & 0.038  \\
  &     &  
  & 0.902 & 0.025 & 0.832 & 0.031   \\
  &      & 
  & 0.891 & 0.029 & 0.806 & 0.039  \\
  &      & 
  & 0.897 & 0.028 & 0.840 & 0.028  \\
  \hline
  self-purf. & & & 0.899 & 0.026 & 0.854 & 0.023\\
  \hline
  \rowcolor{mygray}
  full-dup. &  &  
  & \textbf{0.920} & \textbf{0.020} & \textbf{0.864} & \textbf{0.023} \\
  \bottomrule
  \end{tabular}
\end{table}


\subsubsection{Number of Cascaded BPMs}\label{sec:number_of_bpms}
Naturally, more cascaded BPMs should lead to better boost performance. This is investigated and the evaluation results are shown in \tabref{tab:number_of_bpm}, where .
Note that  means that \textbf{NO} BPM is used.
Clearly, as can be seen from~\figref{fig:front_figure_2} (red star), we compare four variants of our~\ourmodel, including =0 (Mean-=76.4, Mean-=76.8), =2 (Mean-=80.4, Mean-=81.4), =4 (Mean-=82.1, Mean-=83.3), and =4, CRF (Mean-=83.4, Mean-=83.1). 
It demonstrates that more BPMs leads to better results, but the performance reaches saturation after . 
Further, too many BPMs (\ie, ) will cause high model-complexity and increase the over-fitting risk. As a trade-off, we use  throughout our experiments.

\subsubsection{Effectiveness of Full-Duplex Strategy}\label{sec:effectivess_of_FS}
To investigate the effectiveness of the RCAM and BPM modules with the full-duplex strategy, we study two unidirectional (\ie, simplex strategy in \figref{Fig:framework_RCAM} \& \figref{Fig:framework_bpm}) variants of our model. 
In \tabref{tab:ablation_bi_model}, the symbols , , and  indicate the feature transmission directions in the designed RCAM 
or BPM.
Specifically,  indicates that the attention vector in the optical flow branch weights the features in the appearance branch and vice versa.  indicates that motion cues are used to guide the fused features extracted from both appearance and motion. 
The comparison results show that our elaborately designed modules (RCAM and BPM) jointly cooperate in a full-duplex fashion and outperform all simplex (\textit{unidirectional}) settings.








\subsection{Further Discussion}\label{sec:discussion}
\subsubsection{Prediction Selection}\label{sec:prediction_selection}
Which is the final prediction,  or ?
As mentioned in~\secref{sec:training}, we choose  as our final segmentation result instead of . 
The major reasons for doing so can be summarized as follows:
\begin{itemize}
    \item We employ the auxiliary supervision for the motion-based branch to learn more motion patterns inspired by~\cite{tokmakov2017learning2}.
    \item More informative appearance and motion cues are contained in another branch at the phase of bidirectional purification.
\end{itemize}

\begin{table}[htbp]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \setlength\tabcolsep{8.5pt}
  \caption{Ablation study (\secref{sec:prediction_selection}) for the choice of final segmentation result on DAVIS~\cite{perazzi2016benchmark} and MCL~\cite{kim2015spatiotemporal} dataset.
  }
  \label{tab:choice_of_result}
  \begin{tabular}{l|cc|cc}
  \toprule
   & \multicolumn{2}{c|}{DAVIS} &\multicolumn{2}{c}{MCL} \\
   \cline{2-5}
   &  & 
   &  &  \\
  \hline
  \hline
  (a)  as result 
  & 0.920 & 0.022 & 0.862 & 0.024 \\
  (b)  as result 
  & 0.920 & 0.022 & 0.863 & 0.023\\
  \hline
  \rowcolor{mygray}
  (c)  as result (Ours) 
  & 0.920 & 0.022 & 0.864 & 0.023 \\
  \toprule
  \end{tabular}
\end{table}

As shown in~\tabref{tab:choice_of_result}, three experiments are conducted to verify our assumption: 
(a) choosing  as the final result, 
(b) choosing  as the final result, and 
(c) choosing  as the final result (Ours). 
As can be seen in~\tabref{tab:choice_of_result}, all three choices achieve very similar results, while  performs slightly better than the other two.
Besides, considering the reduction of unnecessary computational cost, we choose  as our final result for comparison with other methods.



\subsubsection{Effectiveness of CRF}\label{sec:crf_post_process}
From \figref{fig:front_figure_2} we can see that our~\ourmodel~without CRF post-processing technique, \ie, \ourmodel~(=4), still outperforms the best model AAAI'20-MAT in terms of Mean- metric.
This means that our initial method (\ie, \ourmodel~without CRF) can distinguish hard samples around the object boundaries without post-processing techniques.
When equipped with the CRF post-processing technique~\cite{krahenbuhl2011efficient}, our \ourmodel~(=4, CRF) achieves the best performance in terms of both Mean- and Mean- metrics.


\begin{figure}[htbp]
  \centering
  \begin{overpic}[width=0.8\linewidth]{./figures/pictures/teaser_figure-NoSemi}
    \put(74, 1){\small }
    \put(3, 64){\begin{rotate}{90}{\small }\end{rotate}}
  \end{overpic}
  \caption{
  Mean contour accuracy ()~\textit{vs.}~mean region similarity () scores on DAVIS dataset~\cite{perazzi2016benchmark}. 
  Circles indicate U-VOS methods.
  Four variants are shown in \textit{\textbf{bold-italic}}, in which `' indicates the number of bidirectional purification modules (BPM) and `CRF' means that using CRF~\cite{krahenbuhl2011efficient} post-processing technique.
  Compared with the best unsupervised VOS model (\ie, MAT~\cite{zhou2020motion_attentive} also with CRF),
  the proposed method \ourmodel~(=4, CRF) achieves the new SOTA by a large margin.
  }
  \label{fig:front_figure_2}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{./figures/pictures/Self-Purification-CVM.pdf}
  \caption{Illustration of self-purification strategy (a) and the proposed bidirectional purification strategy (b). Note that sub-figure (b) is same as the \figref{Fig:framework_bpm}~(c) for convenient comparison. Note that , , and  denote element-wise addition, multiplication, and concatenation, respectively.
}\label{fig:self_purf}
\end{figure}

\subsubsection{Training Effectiveness with Less Data}\label{sec:scale_of_dataset}
As shown in~\figref{fig:front_figure_2}, the proposed method, \ie, \ourmodel~(=4, CRF), surpasses the best U-VOS model MAT~\cite{zhou2020motion_attentive} (also with CRF), while our~\ourmodel~with less labelled data in the training phase (\ie, Ours-13K \textit{vs.} MAT-16K). 
Besides, we also observe that the recently proposed method 3DC-Seg~\cite{bmvc2020making}, based on a 3D convolutional network, can achieve the new state-of-the-art (Mean-=84.3, Mean-=84.7), while relies on a massive amount of labelled training samples as expert knowledge in the fine-tuning phase, including 158K images (\ie, COCO~\cite{lin2014microsoft} + YouTube-VOS~\cite{xu2018youtube} + DAVIS~\cite{perazzi2016benchmark}).
It requires about ten times more training data than the best model MAT~\cite{zhou2020motion_attentive} (16K images) in the fine-tuning phase. 
Thus, it demonstrates the efficient training process in our pipeline.



\subsubsection{Self-Purification Strategy in BPM}\label{sec:self_purf}
We provide more details on the different variants mentioned in~\secref{sec:effectivess_of_FS}
including ,  and  in BPM.
The implementation of  and  in RCAM are illustrated in~\figref{Fig:framework_RCAM}~(a)~\&~(b), whereas the structure implementation of  \&  in BPM are illustrated in~\figref{Fig:framework_bpm}~(a)~\&~(b).
Here, note that all of these variants indicate unidirectional refinement, in contrast to the proposed bi-directional schemes.

Last but not least, to validate that the gains of bi-directional schemes in practice \textbf{DO COME FROM} the bi-directional procedure and not more complex model structures, we implement another variant using the same complex structures but without any branch interactions before the decoding stage.
This is done by exchanging the places of  and  as illustrated in Fig.~\ref{fig:self_purf} (b), leading to a kind of ``self-purification'' strategy. Symbol ``'' in Fig.~\ref{fig:self_purf} (a) means that there is \textbf{NO} interaction between the two branches, \ie, there is only interaction within itself.
Comparisons of the uni-/bidirectional strategies are shown in~\tabref{tab:ablation_bi_model}. The comparison results show that
our elaborately designed modules (\ie, RCAM and BPM) jointly cooperate in a bidirectional manner and outperform all unidirectional settings.
Besides, our bidirectional purification scheme (\ie, `full-dup.'~in~\tabref{tab:ablation_bi_model}) also achieves very notable improvement (2.1\% and 1.0\% gains in  on DAVIS~\cite{perazzi2016benchmark} and MCL~\cite{kim2015spatiotemporal}, respectively) against the ``self-purification'' variant (\ie, `self-purf.' in~\tabref{tab:ablation_bi_model}), which has a similar complex structure, further validating the benefit of the bidirectional behavior claimed in this study.


\subsubsection{Relation Between RCAM and BPM}\label{sec:relation_RCAM_BPM}
The two introduced modules, \ie, RCAM and BPM, focus on using appearance and motion features while ensuring the information flow between them.
They can work collaboratively under the mutual restraint of our full-duplex strategy, but they cannot be substituted for one another.
This is due to the RCAM transmits the features at each level in a \textit{point-to-point} manner (\eg, ), and thus, it fits with the progressive feature extraction in the encoder. 
The BPM, on the other hand, broadcasts high-level features to low-level features via an interlaced decremental connection in a \textit{set-to-point} manner (\eg,  ), which is more suitable for the multi-level feature interaction in the decoder.







\section{Conclusion}
In this paper, we present a simple yet efficient framework, termed full-duplex strategy network (\ourmodel), that fully leverages the mutual constraints of appearance and motion cues to address the video object segmentation problem. 
It consists of two core modules: 
the relational cross-attention module (RCAM) in the encoding stage and the efficient bidirectional purification module (BPM) in the decoding stage.
The former one is used to abstract features from a dual-modality, while the latter is utilized to re-calibrate inconsistant features step-by-step.
We thoroughly validate functional modules of our architecture via extensive experiments, leading to several interesting findings.
Finally, \ourmodel~acts as a unified solution that significantly advances SOTA models for both U-VOS and V-SOD tasks.
In the future, we may extend our scheme to learn short-term and long-term information in an efficient Transformer-like framework \cite{wang2021pyramid,zhuge2021kaleido} to further boost the accurarcy.




\bibliographystyle{CVM}
{\normalsize  \bibliography{CVM2021-VOS}}

\end{document}
