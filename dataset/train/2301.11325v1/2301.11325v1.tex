\section{Results}
\label{results}

\looseness=-1
We evaluate {\model} by comparing it with two recent baselines for music generation from descriptive text, namely Mubert~\cite{Mubert} and Riffusion~\cite{riffusion}. 
In particular, we generate audio by querying the Mubert API,\footnote{\href{https://github.com/MubertAI}{github.com/MubertAI} (accessed in Dec 2022 and Jan 2023)} and by running inference on the Riffusion model.\footnote{\href{https://github.com/riffusion/riffusion-app}{github.com/riffusion/riffusion-app} (accessed on Dec 27, 2022)}
We perform our evaluations on {\dataset}, the evaluation dataset we publicly release together with this paper.

\paragraph{Comparison to baselines.}
\label{results/comparison}

\cref{table:quant-eval} reports the main quantitative and qualitative results of this paper. In terms of audio quality, as captured by the FAD metrics, on FAD {\model} achieves better scores than Mubert and Riffusion. On FAD, {\model} scores similarly to Mubert (0.44 vs. 0.45) and better than Riffusion (0.76).
We note that, according to these metrics, {\model} is capable of generating high-quality music comparable to Mubert, which relies on pre-recorded sounds prepared by musicians and sound designers.
In terms of faithfulness to the input text description, as captured by KLD and MCC, {\model} achieves the best scores, suggesting that it is able to capture more information from the text descriptions compared to the baselines.

We further supplement our evaluation of text faithfulness with a human listening test. 
Participants are presented with two 10-second clips and a text caption, and asked which clip is best described by the text of the caption on a 5-point Likert scale. 
We collect 1200 ratings, with each source involved in 600 pair-wise comparisons. Table~\ref{table:quant-eval} reports the total number of ``wins'', that is, counting how often the human raters preferred a model in a side-by-side comparison. 
{\model} is clearly preferred over both baselines, while there is still a measurable gap to the ground truth reference music. Full details of the listening study can be found in Appendix~\ref{sec:qualitative_appendix}.


Listening to examples in which the ground truth was preferred over \model{} reveals the following patterns:
(1)~captions are extremely detailed, referring to more than five instruments or describing non musical aspects such as \textit{``wind, people talking``}; (2)~captions describe temporal ordering of the audio being played; (3)~negations are used, which are not well captured by \mulan.

Overall, we conclude that: (1)~our approach is able to capture fine-grained information from the rich free-text captions of \dataset{}; (2)~the KLD and MCC metrics provide a quantitative measure of the faithfulness to the text description, which is in accordance with the human rating study.

\begin{table}
\setlength{\tabcolsep}{4pt}
\caption{Evaluation of generated samples using captions from the {\dataset} dataset. Models are compared in terms of audio quality, by means of Fréchet Audio Distance (FAD), and faithfulness to the text description, using Kullback–Leibler Divergence (KLD) and  MuLan Cycle Consistency (MCC), and counts of wins in pairwise human listening tests (Wins).}
\label{table:quant-eval}
\vskip 0.15in
\begin{center}
\scriptsize
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Model & FAD  & FAD  & KLD  & MCC  & Wins  \\
\midrule

Riffusion & 0.76 & 13.4 & 1.19 & 0.34 & 158 \\
Mubert & 0.45 & 9.6 & 1.58 & 0.32 & 97 \\
\model{} & 0.44 & 4.0 & 1.01 & 0.51 & 312 \\
\midrule
\datasetshort{} & - & - & - & - & 472 \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}



\paragraph{Importance of semantic tokens.}
\label{results/semantic-tokens}
To understand the usefulness of decoupling semantic modeling from acoustic modeling, we train a Transformer model which directly predicts coarse acoustic tokens from MuLan tokens, by modeling .
We observe that while the FAD metrics are comparable (0.42~FAD and 4.0~FAD), KLD and MCC scores worsen when removing the semantic modeling stage. In particular the KLD score increases from 1.01 to 1.05, and the MCC score decreases from 0.51 to 0.49, indicating that semantic tokens facilitate the adherence to the text description.
We also confirm this qualitatively by listening to the samples. In addition, we observe degradation in long term structure.



\paragraph{Information represented by audio tokens.}
\label{results/semantic-vs-acoustic}
We conduct additional experiments to study the information captured by the semantic and the acoustic tokens. In the first study, we fix the {\mulan} text tokens as well as the semantic tokens, running the acoustic modeling stage multiple times to generate several samples. In this case, by listening to the generated music, it is possible to observe that the samples are diverse, yet they tend to share the same genre, rhythmical properties (e.g., drums), and part of the main melody. They differ in terms of specific acoustic properties (e.g., level of reverb, distortion) and, in some cases, different instruments with a similar pitch range can be synthesized in different examples. In the second study, we fix only the {\mulan} text tokens and generate both the semantic and acoustic tokens. In this case, we observe a much higher level of diversity in terms of melodies and rhythmic properties, still coherent with the text description. We provide samples from this study in the accompanying material. 

\paragraph{Memorization analysis.}
\label{results/memorization}
\looseness=-1
Figure~\ref{fig:memorization} reports both exact and approximate matches when the length of the semantic token prompt is varied between 0~and 10~seconds. We observe that the fraction of exact matches always remains very small (), even when using a 10 second prompt to generate a continuation of 5 seconds. Figure~\ref{fig:memorization} also includes results for approximate matches, using . We can see a higher number of matches detected with this methodology, also when using only MuLan tokens as input (prompt length ) and the fraction of matching examples increases as the length of the prompt increases. We inspect these matches more closely and observe that those with the lowest matching score correspond to sequences characterized by a low level of token diversity. Namely, the average empirical entropy of a sample of 125 semantic tokens is 4.6 bits, while it drops to 1.0 bits when considering sequences detected as approximate matches with matching score less than~0.5. We include a sample of approximate matches obtained with  in the accompanying material. Note that acoustic modeling carried out by the second stage introduces further diversity in the generated samples, also when the semantic tokens match exactly. 

\begin{figure}[t]
\begin{center}
    \centerline{\includegraphics[clip, width=0.42\textwidth]{figures/memorization.pdf}}
    \vspace{-2mm}
    \caption{Memorization results for the semantic modeling stage. We compare the semantic tokens generated for 5~seconds of audio to corresponding tokens in the training set, considering exact and approximate matches.}
    \label{fig:memorization}
    \vspace{-6mm}
\end{center}
\end{figure}



\section{Extensions}
\paragraph{Melody conditioning.}
\label{melody-conditioning}
We extend {\model} in such a way that it can generate music based on both a text description and a melody, which is provided in the form of humming, singing, whistling, or playing an instrument. 
This requires extending the conditioning signal in a way that captures the target melody. To this end, we create a synthetic dataset composed of audio pairs with matching melodies but different acoustics. To create such pairs, we use different versions of the same music clip, such as covers, instrumentals, or vocals. Additionally, we acquire data pairs of people humming and singing. We then train a joint embedding model such that when two audio clips contain the same melody, the corresponding embeddings are close to each other. For implementation details we refer to Appendix~\ref{appendix:melody-conditioning}. 

To extract the melody conditioning for {\model}, we quantize the melody embeddings with RVQ, and concatenate the resulting token sequences with the {\mulan} audio tokens .
During inference, we compute melody tokens from the input audio clip and concatenate them with the {\mulan} text tokens .
Based on this conditioning, {\model} can successfully generate music which follows the melody contained in the input audio clip, while adhering to the text description.

\paragraph{Long generation and story mode.}
\label{story-mode}
\looseness=-1
In {\model}, generation is autoregressive in the temporal dimension which makes it possible to generate sequences longer than those used during training. In practice, the semantic modeling stage is trained on sequences of 30 seconds. To generate longer sequences, we advance with a stride of 15~seconds, using 15~seconds as prefix to generate an additional 15~seconds, always conditioning on the same text description. With this approach we can generate long audio sequences which are coherent over several minutes.

With a small modification, we can generate long audio sequences while changing the text description over time. Borrowing from ~\citet{phenaki} in the context of video generation, we refer to this approach as \emph{story mode}. Concretely, we compute  from multiple text descriptions and change the conditioning signal every 15 seconds. The model generates smooth transitions which are tempo consistent and semantically plausible, while changing music context according to the text description.
