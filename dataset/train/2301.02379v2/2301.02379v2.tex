

\appendix



\noindent{\Large \textbf{{Appendix}}}
\vspace{0.2mm}
\\

This supplemental document contains four sections: Section~\ref{sec:implementation} shows implementation details of our CodeTalker; Section~\ref{sec:more} presents more discussions on the proposed method; Section~\ref{sec:user} presents details of the user study; and Section~\ref{sec:video} presents short descriptions of the supplemental video. The source code and trained model will also be released upon publication.


\section{Implementation Details}
\label{sec:implementation}
\subsection{{Hyper-parameters of Codebook}}
We have explored and discussed the important hyper-parameters of our motion codebook in Section~\ref{subsec:ablation} ``Codebook construction" on the BIWI dataset in the main paper. Here we provide more specific parameters adopted for CodeTalker trained on the two datasets. For BIWI, we have the ground truth for quantitative evaluation on the testing set BIWI-Test-A to determine a group of parameters $P=1$ and $H=8$ for high-quality results (\ie, Section~\ref{subsec:ablation} ``Codebook construction" in the main paper). Additionally, we set the codebook item number $N=256$ and the dimension of items $C=128$. Although more codebook items and dimensions could ease reconstruction, the redundant elements may cause ambiguity in speech-driven motion synthesis. Hence, we did not heavily tune these parameters and just empirically set them for good visual quality. For VOCASET, since there is no ground truth for us to obtain the quantitative results, we empirically select a group of parameters (\ie, $N=256$, $P=1$, $H=16$, $C=64$), which could produce visually plausible facial animations in our experiments.

\subsection{Network Architecture}
To improve the reproducibility of our CodeTalker, we further illustrate the detailed network architectures for the facial motion space learning and the speech-driven motion synthesis (Section~\ref{subsec:motion_space} and~\ref{subsec:motion_prediction} in the main paper, respectively), which are shown Table~\ref{tab:network_arch}.


\section{More Discussions on CodeTalker}
\label{sec:more}
\begin{table*}[!t]
\centering
\caption{
Parameter illustration of network architectures. C(k,s,p,n) denotes a 1D Convolutional layer with kernel size k, stride size s, padding size p, and output channels of n. $\text{T}_\text{enc}$(d1,d2,h,l) denotes a transformer encoder layer with basic channel number of d1, forward channel number of d2, self-attention head number of h, and layer number l, while similarly, $\text{T}_\text{dec}$ represents a transformer decoder layer. L(n) denotes a linear layer with output channels of n. CA[$\cdot$] stands for the additional cross-attention input for transformer decoders. $\text{l}_\text{CM}=12$ for BIWI, while $\text{l}_\text{CM}=6$ for VOCASET. $n \cdot T$ stands for the interpolated audio feature length in order to align with visual frames, where $n=2$ for BIWI and $n=1$ for VOCASET. `+' denotes the channel-wise addition. ``Drop" means the dropout operation.
}
{
\resizebox{1\textwidth}{!}{
{\renewcommand{\arraystretch}{1.4}\begin{tabular}{c|c|l|l}
  \toprule[0.8pt]
  Stage & Module  & Input $\rightarrow$ Output& Layer Operation \\
  \hline
  \multirow{7}{*}{I}&\multirow{4}{*}{Encoder}  & $\mathbf{M}(T, V,3) \rightarrow \mathbf{M}(T, V \cdot 3)$    &  Reshape\\
  &&    $\mathbf{M}(T,V \cdot 3) \rightarrow \mathbf{Z}^1_e(T, 1024)$   & L(1024) $\rightarrow$ LReLU $\rightarrow$ C(5,1,2,1024) $\rightarrow$ LReLU $\rightarrow$ IN \\ 
  &&    $\mathbf{Z}^1_e(T, 1024) \rightarrow \mathbf{Z}^2_e(T,H \cdot C)$    & L(1024) $\rightarrow$ $\text{T}_\text{enc}$(1024,1536,8,6) $\rightarrow$ L($H \cdot C$)\\
  && $\mathbf{Z}^2_e(T,H \cdot C) \rightarrow \mathbf{Z}_\Quantize(T,H,C)$ & Reshape $\rightarrow$ Quantize \\
  \cline{2-4}
  &\multirow{3}{*}{Decoder}    & $\mathbf{Z}_\Quantize(T,H,C) \rightarrow \mathbf{Z}_\Quantize(T,H \cdot C)$     & Reshape\\
  &&$\mathbf{Z}_\Quantize(T,H \cdot C) \rightarrow \mathbf{Z}^1_d(T,1024)$ & L(1024) $\rightarrow$ C(5,1,2,1024) $\rightarrow$ LReLU $\rightarrow$ IN\\
  && $\mathbf{Z}^1_d(T,1024) \rightarrow \mathbf{\hat{M}}(T,V \cdot 3)$& L(1024) $\rightarrow$ $\text{T}_\text{enc}$(1024,1536,8,6) $\rightarrow$ L($V \cdot 3$)\\
  \hline
  \multirow{12}{*}{II}& & \multirow{4}{*}{$\mathbf{A}(T, d) \rightarrow \mathbf{F}_e^1(T^\prime, 512)$} &    C(10,5,0,512) $\rightarrow$ GN $\rightarrow$ GeLU $\rightarrow$ C(3,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU \\
  &&&  $\rightarrow$ C(3,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU $\rightarrow$ C(3,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU\\
  &Speech&&  $\rightarrow$ C(3,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU $\rightarrow$ C(3,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU\\
  &Encoder&&  $\rightarrow$ C(2,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU $\rightarrow$ C(2,2,0,512) $\rightarrow$ GN $\rightarrow$ GeLU\\
  &&   $\mathbf{F}_e^1(T^\prime, 512) \rightarrow \mathbf{F}_e^2(n \cdot T, 768)$  & Interpolate $\rightarrow$ LN $\rightarrow$ L(768) $\rightarrow$ Drop \\
  && $\mathbf{F}_e^2(n \cdot T, 768) \rightarrow \mathbf{F}_e^3(n \cdot T, 1024)$  &  $\text{T}_\text{enc}$(768,3072,12,12) $\rightarrow$ L(1024) \\
  \cline{2-4}
  &&  $\mathbf{\hat{M}}_\text{past}(T,V \cdot 3) \rightarrow \mathbf{F}_\text{emb}^\text{past}(T, 1024)$ & L(1024) $\rightarrow$ +StyleVector \\
  && $\mathbf{F}_\text{emb}^\text{past}(T, 1024) \rightarrow \mathbf{\hat{Z}}_d^1(T, 1024)$ &  $\text{T}_\text{dec}$(1024,2048,4,$\text{l}_\text{CM}$) with CA[$\mathbf{F}_e^3$] $\rightarrow$ L($H \cdot C$) \\
  &Cross-modal& $\mathbf{\hat{Z}}_d^1(T, H \cdot C) \rightarrow \mathbf{\hat{Z}}_\Quantize(T,H,C)$ &  Reshape $\rightarrow$ Quantize  \\
  & Decoder  & $\mathbf{\hat{Z}}_\Quantize(T,H,C) \rightarrow \mathbf{\hat{Z}}_\Quantize(T,H \cdot C)$     & Reshape\\
  &&$\mathbf{\hat{Z}}_\Quantize(T,H \cdot C) \rightarrow \mathbf{\hat{Z}}^2_d(T,1024)$ & L(1024) $\rightarrow$ C(5,1,2,1024) $\rightarrow$ LReLU $\rightarrow$ IN\\
  && $\mathbf{\hat{Z}}^2_d(T,1024) \rightarrow \mathbf{\hat{M}}(T,V \cdot 3)$& L(1024) $\rightarrow$ $\text{T}_\text{enc}$(1024,1536,8,6) $\rightarrow$ L($V \cdot 3$)\\
  \bottomrule[0.8pt]
  \end{tabular} 
    }
    }
  }
  \label{tab:network_arch}
\end{table*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{supp/fig/ablation_IN_lip_distance.pdf}
    \caption{Distance between lower and upper lip within a sampled sequence from VOCA-Test of (a) reconstruction and (b) speech-driven motion synthesis results produced by different variants.}
    \label{fig:ablation_IN}
\end{figure}

\begin{table}
\centering
\caption{
  Ablation study on the Instance Normalization (IN) for self-reconstruction learning. The performance is measured by the reconstruction error on VOCA-Test and BIWI-Test-A.
}\vspace{-0.5em}
\resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lccc}
  \toprule[0.8pt]
  \multirow{2}{*}{Variants} & \multicolumn{2}{c}{Reconstruction Error} \\ 
  \cmidrule{2-3}
     &  VOCA-Test ($\times10^{-5}$ mm) & BIWI-Test-A ($\times10^{-5}$ mm)\\
\hline \\[-2.2ex]
  Ours (w/o IN)  & 0.12     & 3.27  \\
  Ours           &  \bf0.08 &  \bf2.83  \\
  \bottomrule[0.8pt]
  \end{tabular}
  }
  \label{tab:ablation_IN}
\end{table}

\subsection{Instance Normalization in Self-reconstruction Learning}
Instance Normalization~\cite{ulyanov2016instance} (IN) has been widely used in the filed of style transfer~\cite{ulyanov2017improved,huang2017arbitrary}, which is defined as:
\begin{equation}
    \text{IN}(x) = \gamma (\frac{x-\mu(x)}{\sigma(x)})+\beta.
\end{equation}
Different from BN~\cite{ioffe2015batch} layers, here $\mu(x)$ and $\sigma(x)$ are computed across temporal dimensions independently for each channel within each sample:
\begin{equation}
    \mu_{nc}(x) = \frac{1}{T}\sum_{t=1}^{T}x_{nct}
\end{equation}
\begin{equation}
    \sigma_{nc}(x) = \sqrt{\frac{1}{T}\sum_{t=1}^{T}(x_{nct}-\mu_{nc}(x))^2+\epsilon}
\end{equation}
Interestingly, we empirically find that normalizing feature statistics (\ie, mean and variance) with IN (not BN due to small mini-batch size) can boost the performance of our CodeTalker in self-reconstruction learning, as shown in Table~\ref{tab:ablation_IN}. In addition, it can also make self-reconstruction training more stable. To better show the gain of normalization, we also visualize the lip distance of a sampled sequence of reconstruction results from VOCA-Test in Figure~\ref{fig:ablation_IN}(a). The visualization result indicates that the predicted lip amplitudes are closer to those of the ground truth by equipping with IN, while the ablated variant (\ie, Ours (w/o IN)) cannot reconstruct lip movements with accurate amplitudes. The speech-driven facial motion synthesis (stage two) can also benefit from the facial motion codebook learned in self-reconstruction with IN, as shown in Figure~\ref{fig:ablation_IN}(b). Note that we synthesize facial motions conditioned on a randomly sampled speaking style. We conjecture that facial motions with different magnitudes could be well encapsulated into the discrete motion prior by normalizing temporal elements within each channel. The rationality and effect of IN deserve further studies as our potential direction.











\begin{table}[!t]
\centering
\setlength\tabcolsep{7pt}
\caption{
Comparison of lip-sync errors. We compare different methods on BIWI-Test-A. Lower means better. $\lambda$ is the weighting factor.
}\vspace{-0.5em}
{
\resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lcc}
  \toprule[0.8pt]
  Method   & Lip Vertex Error ($\times10^{-4}$ mm)     \\ \hline\\[-2.2ex]
  Alter. ($\mathcal{L}_\text{ce}$)                            & 9.6356                      \\Alter. ($\lambda \mathcal{L}_\text{ce}$+$\mathcal{L}_\text{reg}$) & 5.1138                      \\Alter. ($\lambda \mathcal{L}_\text{ce}$+$\mathcal{L}_\text{reg}$+$\mathcal{L}_\text{motion}$) & 5.0254  \\ CodeTalker (Ours)                                             & \textbf{4.7914}       \\
  \bottomrule[0.8pt]
  \end{tabular}
  }
  }
  \label{tab:dataflow}
  \vspace{-2mm}
\end{table}


\subsection{Alternative Data Flow and Supervision}
As we have summarized in Section~\ref{subsec:discrete_prior_learning} of the main paper, recent works explore the power of discrete prior learning in a large variety of tasks, among which most existing Vector Quantization (VQ)-based works~\cite{ng2022learning,zhou2022towards} adopt categorical cross-entropy (CE) loss to supervise their token predictions. Hence, we also explore some alternative data flow and supervision frameworks as our cross-modal decoder, which is shown in Figure~\ref{fig:ablation_dataflow}. It is worth noting that the style vector and audio features are omitted for simplicity.

Different from our cross-modal decoder in the main paper, the alternative takes past motion code as input and then autoregressively predicts code sequences in form of n-way classification. The predicted code sequence then retrieves the respective code items from the learned codebook $\mathcal{Z}$, and further produces facial motion sequences through the fixed decoder $D$. A CE loss is adopted to penalize error between the predicted code sequence $\hat{\mathbf{c}}\in \{0,\dots,|N|-1\}^{T^\prime \cdot H}$ and the ground truth $\mathbf{c}$ generated by the pre-trained encoder $E$:
\begin{equation}
    \mathcal{L}_\text{ce}=\sum_{i=0}^{T^\prime \cdot H}-\mathbf{c}_i\log(\hat{\mathbf{c}}_i).
\end{equation}
We train the alternatives with the same settings as those in the main paper (Section~\ref{subsec:details}). The lip-sync evaluation result is tabulated in Table~\ref{tab:dataflow}. Alternative model with $\mathcal{L}_\text{ce}$ alone cannot converge well due to the difficult cross-modality mapping of token prediction. While adding more constraints (\ie, $\mathcal{L}_\text{reg}$ and $\mathcal{L}_\text{motion}$ in the main paper Eq.~\ref{eqn:final_loss}) can ease the difficulty of token prediction learning, the performance is still limited with this token prediction framework. Overall, the lower average lip error achieved by our CodeTalker suggests its framework superiority in terms of the accuracy of lip movements.



\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{supp/fig/ablation_dataflow.pdf}
    \caption{Alternative data flow and supervision framework of our cross-modal decoder. Note that we omit the style vector and audio features input for simplicity. Given the past motion code as input, the alternative cross-modal decoder first autoregressively predict motion code and then decode them into motions with the pre-trained codebook and decoder.}
    \label{fig:ablation_dataflow}
\end{figure}


\footnotetext[1]{~\url{https://doubiiu.github.io/projects/codetalker}}

\section{User Study}
\label{sec:user}
The designed user study interface is shown in Figure~\ref{fig:user_study}. A user study is expected to be completed with 5--10 minutes (24 video pairs $\times$ 5 seconds $\times$ 3 times watching). To remove the impact of random selection, we filter out those comparison results completed in less than two minutes. For each participant, the user study interface shows 24 video pairs and the participant is instructed to judge the videos twice with the following two questions, respectively: ``Comparing the lips of two faces, which one is more in sync with the audio?" and ``Comparing the two full faces, which one looks more realistic?".

\section{Video Comparison}
\label{sec:video}
To better evaluate the qualitative results produced by competitors~\cite{richard2021meshtalk,cudeiro2019capture,fan2022faceformer,karras2017audio} and our CodeTalker, we provide a supplemental video\footnotemark[1] for demonstration and comparison. Specifically, we test our model using various audio clips, including the audio clips extracted from TED and TEDx videos, audio sequences from the VOCASET and BIWI datasets, and the speech from supplementary videos of previous methods. The video shows that CodeTalker can synthesize natural and plausible facial animations with well-synchronized lip movements. It is worth noting that, compared to the competitors (\ie, VOCA, MeshTalk and FaceFormer) suffering from the over-smoothing problem, our CodeTalker can produce more vivid and realistic facial motions and better lip sync. Besides, we also show the talking style interpolation results and facial animations of talking in different languages.






\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth, trim=0 0 0 1.7mm, clip]{supp/fig/user_study_ui.png}
    \caption{Designed user study interface. Each participant need to answer 24 video pairs and here only one video pair is shown due to the page limit.}
    \label{fig:user_study}
\end{figure*}








