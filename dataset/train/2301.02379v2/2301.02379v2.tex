

\appendix



\noindent{\Large \textbf{{Appendix}}}
\vspace{0.2mm}
\\

This supplemental document contains four sections: Section~\ref{sec:implementation} shows implementation details of our CodeTalker; Section~\ref{sec:more} presents more discussions on the proposed method; Section~\ref{sec:user} presents details of the user study; and Section~\ref{sec:video} presents short descriptions of the supplemental video. The source code and trained model will also be released upon publication.


\section{Implementation Details}
\label{sec:implementation}
\subsection{{Hyper-parameters of Codebook}}
We have explored and discussed the important hyper-parameters of our motion codebook in Section~\ref{subsec:ablation} ``Codebook construction" on the BIWI dataset in the main paper. Here we provide more specific parameters adopted for CodeTalker trained on the two datasets. For BIWI, we have the ground truth for quantitative evaluation on the testing set BIWI-Test-A to determine a group of parameters  and  for high-quality results (\ie, Section~\ref{subsec:ablation} ``Codebook construction" in the main paper). Additionally, we set the codebook item number  and the dimension of items . Although more codebook items and dimensions could ease reconstruction, the redundant elements may cause ambiguity in speech-driven motion synthesis. Hence, we did not heavily tune these parameters and just empirically set them for good visual quality. For VOCASET, since there is no ground truth for us to obtain the quantitative results, we empirically select a group of parameters (\ie, , , , ), which could produce visually plausible facial animations in our experiments.

\subsection{Network Architecture}
To improve the reproducibility of our CodeTalker, we further illustrate the detailed network architectures for the facial motion space learning and the speech-driven motion synthesis (Section~\ref{subsec:motion_space} and~\ref{subsec:motion_prediction} in the main paper, respectively), which are shown Table~\ref{tab:network_arch}.


\section{More Discussions on CodeTalker}
\label{sec:more}
\begin{table*}[!t]
\centering
\caption{
Parameter illustration of network architectures. C(k,s,p,n) denotes a 1D Convolutional layer with kernel size k, stride size s, padding size p, and output channels of n. (d1,d2,h,l) denotes a transformer encoder layer with basic channel number of d1, forward channel number of d2, self-attention head number of h, and layer number l, while similarly,  represents a transformer decoder layer. L(n) denotes a linear layer with output channels of n. CA[] stands for the additional cross-attention input for transformer decoders.  for BIWI, while  for VOCASET.  stands for the interpolated audio feature length in order to align with visual frames, where  for BIWI and  for VOCASET. `+' denotes the channel-wise addition. ``Drop" means the dropout operation.
}
{
\resizebox{1\textwidth}{!}{
{\renewcommand{\arraystretch}{1.4}\begin{tabular}{c|c|l|l}
  \toprule[0.8pt]
  Stage & Module  & Input  Output& Layer Operation \\
  \hline
  \multirow{7}{*}{I}&\multirow{4}{*}{Encoder}  &     &  Reshape\\
  &&       & L(1024)  LReLU  C(5,1,2,1024)  LReLU  IN \\ 
  &&        & L(1024)  (1024,1536,8,6)  L()\\
  &&  & Reshape  Quantize \\
  \cline{2-4}
  &\multirow{3}{*}{Decoder}    &      & Reshape\\
  && & L(1024)  C(5,1,2,1024)  LReLU  IN\\
  && & L(1024)  (1024,1536,8,6)  L()\\
  \hline
  \multirow{12}{*}{II}& & \multirow{4}{*}{} &    C(10,5,0,512)  GN  GeLU  C(3,2,0,512)  GN  GeLU \\
  &&&   C(3,2,0,512)  GN  GeLU  C(3,2,0,512)  GN  GeLU\\
  &Speech&&   C(3,2,0,512)  GN  GeLU  C(3,2,0,512)  GN  GeLU\\
  &Encoder&&   C(2,2,0,512)  GN  GeLU  C(2,2,0,512)  GN  GeLU\\
  &&     & Interpolate  LN  L(768)  Drop \\
  &&   &  (768,3072,12,12)  L(1024) \\
  \cline{2-4}
  &&   & L(1024)  +StyleVector \\
  &&  &  (1024,2048,4,) with CA[]  L() \\
  &Cross-modal&  &  Reshape  Quantize  \\
  & Decoder  &      & Reshape\\
  && & L(1024)  C(5,1,2,1024)  LReLU  IN\\
  && & L(1024)  (1024,1536,8,6)  L()\\
  \bottomrule[0.8pt]
  \end{tabular} 
    }
    }
  }
  \label{tab:network_arch}
\end{table*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{supp/fig/ablation_IN_lip_distance.pdf}
    \caption{Distance between lower and upper lip within a sampled sequence from VOCA-Test of (a) reconstruction and (b) speech-driven motion synthesis results produced by different variants.}
    \label{fig:ablation_IN}
\end{figure}

\begin{table}
\centering
\caption{
  Ablation study on the Instance Normalization (IN) for self-reconstruction learning. The performance is measured by the reconstruction error on VOCA-Test and BIWI-Test-A.
}\vspace{-0.5em}
\resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lccc}
  \toprule[0.8pt]
  \multirow{2}{*}{Variants} & \multicolumn{2}{c}{Reconstruction Error} \\ 
  \cmidrule{2-3}
     &  VOCA-Test ( mm) & BIWI-Test-A ( mm)\\
\hline \
    \text{IN}(x) = \gamma (\frac{x-\mu(x)}{\sigma(x)})+\beta.

    \mu_{nc}(x) = \frac{1}{T}\sum_{t=1}^{T}x_{nct}

    \sigma_{nc}(x) = \sqrt{\frac{1}{T}\sum_{t=1}^{T}(x_{nct}-\mu_{nc}(x))^2+\epsilon}
-2.2ex]
  Alter. ()                            & 9.6356                      \\Alter. (+) & 5.1138                      \\Alter. (++) & 5.0254  \\ CodeTalker (Ours)                                             & \textbf{4.7914}       \\
  \bottomrule[0.8pt]
  \end{tabular}
  }
  }
  \label{tab:dataflow}
  \vspace{-2mm}
\end{table}


\subsection{Alternative Data Flow and Supervision}
As we have summarized in Section~\ref{subsec:discrete_prior_learning} of the main paper, recent works explore the power of discrete prior learning in a large variety of tasks, among which most existing Vector Quantization (VQ)-based works~\cite{ng2022learning,zhou2022towards} adopt categorical cross-entropy (CE) loss to supervise their token predictions. Hence, we also explore some alternative data flow and supervision frameworks as our cross-modal decoder, which is shown in Figure~\ref{fig:ablation_dataflow}. It is worth noting that the style vector and audio features are omitted for simplicity.

Different from our cross-modal decoder in the main paper, the alternative takes past motion code as input and then autoregressively predicts code sequences in form of n-way classification. The predicted code sequence then retrieves the respective code items from the learned codebook , and further produces facial motion sequences through the fixed decoder . A CE loss is adopted to penalize error between the predicted code sequence  and the ground truth  generated by the pre-trained encoder :

We train the alternatives with the same settings as those in the main paper (Section~\ref{subsec:details}). The lip-sync evaluation result is tabulated in Table~\ref{tab:dataflow}. Alternative model with  alone cannot converge well due to the difficult cross-modality mapping of token prediction. While adding more constraints (\ie,  and  in the main paper Eq.~\ref{eqn:final_loss}) can ease the difficulty of token prediction learning, the performance is still limited with this token prediction framework. Overall, the lower average lip error achieved by our CodeTalker suggests its framework superiority in terms of the accuracy of lip movements.



\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{supp/fig/ablation_dataflow.pdf}
    \caption{Alternative data flow and supervision framework of our cross-modal decoder. Note that we omit the style vector and audio features input for simplicity. Given the past motion code as input, the alternative cross-modal decoder first autoregressively predict motion code and then decode them into motions with the pre-trained codebook and decoder.}
    \label{fig:ablation_dataflow}
\end{figure}


\footnotetext[1]{~\url{https://doubiiu.github.io/projects/codetalker}}

\section{User Study}
\label{sec:user}
The designed user study interface is shown in Figure~\ref{fig:user_study}. A user study is expected to be completed with 5--10 minutes (24 video pairs  5 seconds  3 times watching). To remove the impact of random selection, we filter out those comparison results completed in less than two minutes. For each participant, the user study interface shows 24 video pairs and the participant is instructed to judge the videos twice with the following two questions, respectively: ``Comparing the lips of two faces, which one is more in sync with the audio?" and ``Comparing the two full faces, which one looks more realistic?".

\section{Video Comparison}
\label{sec:video}
To better evaluate the qualitative results produced by competitors~\cite{richard2021meshtalk,cudeiro2019capture,fan2022faceformer,karras2017audio} and our CodeTalker, we provide a supplemental video\footnotemark[1] for demonstration and comparison. Specifically, we test our model using various audio clips, including the audio clips extracted from TED and TEDx videos, audio sequences from the VOCASET and BIWI datasets, and the speech from supplementary videos of previous methods. The video shows that CodeTalker can synthesize natural and plausible facial animations with well-synchronized lip movements. It is worth noting that, compared to the competitors (\ie, VOCA, MeshTalk and FaceFormer) suffering from the over-smoothing problem, our CodeTalker can produce more vivid and realistic facial motions and better lip sync. Besides, we also show the talking style interpolation results and facial animations of talking in different languages.






\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth, trim=0 0 0 1.7mm, clip]{supp/fig/user_study_ui.png}
    \caption{Designed user study interface. Each participant need to answer 24 video pairs and here only one video pair is shown due to the page limit.}
    \label{fig:user_study}
\end{figure*}








