\documentclass[sigconf]{acmart}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{multirow}

\hypersetup{colorlinks=true,urlcolor=blue}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}


\fancyhead{}
\settopmatter{printacmref=false, printfolios=false}
\copyrightyear{2019}
\acmYear{2019} 
\setcopyright{none}
\acmConference[WWW 2019]{The 2019 Web Conference}{May 13--17, 2019}{San Francisco, USA}


\begin{document}

\title{Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation}


\author{Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo}
\authornote{M. Guo is the corresponding author. This work was partially sponsored by the National Basic Research 973 Program of China under Grant 2015CB352403.}
\affiliation{Shanghai Jiao Tong University, Shanghai, China}
\affiliation{Microsoft Research Asia, Beijing, China}
\affiliation{Meituan-Dianping Group, Beijing, China}
\affiliation{The Hong Kong Polytechnic University, Hong Kong, China}
\email{wanghongwei55@gmail.com, zhangfuzheng@meituan.com, {csmiaozhao, cswjli}@comp.polyu.edu.hk}
\email{xingx@microsoft.com, guo-my@cs.sjtu.edu.cn}


\begin{abstract}
	Collaborative filtering often suffers from sparsity and cold start problems in real recommendation scenarios, therefore, researchers and engineers usually use side information to address the issues and improve the performance of recommender systems.
	In this paper, we consider knowledge graphs as the source of side information.
 	We propose \textbf{MKR}, a \textbf{M}ulti-task feature learning approach for \textbf{K}nowledge graph enhanced \textbf{R}ecommendation.
 	MKR is a deep end-to-end framework that utilizes knowledge graph embedding task to assist recommendation task.
 	The two tasks are associated by crosscompress units, which automatically share latent features and learn high-order interactions between items in recommender systems and entities in the knowledge graph.
 	We prove that crosscompress units have sufficient capability of polynomial approximation, and show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning.
 	Through extensive experiments on real-world datasets, we demonstrate that MKR achieves substantial gains in movie, book, music, and news recommendation, over state-of-the-art baselines.
 	MKR is also shown to be able to maintain a decent performance even if user-item interactions are sparse.
\end{abstract}

\keywords{Recommender systems; knowledge graph; multi-task learning}

\maketitle

\subsection*{\small{ACM Reference Format:}}
\vspace{-0.05in}
	{\small
		Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo.
		2019.
		Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation
		In \textit{Proceedings of The 2019 Web Conference (WWW 2019)}.
		ACM, New York, NY, USA, 11 pages.
		https://doi.org/xxxxx
	}

\section{Introduction}
	Recommender systems (RS) aims to address the information explosion and meet users personalized interests.
	One of the most popular recommendation techniques is collaborative filtering (CF) \cite{koren2009matrix}, which utilizes users' historical interactions and makes recommendations based on their common preferences.
	However, CF-based methods usually suffer from the sparsity of user-item interactions and the cold start problem.
	Therefore, researchers propose using \textit{side information} in recommender systems, including social networks \cite{jamali2010matrix}, attributes \cite{wang2018shine}, and multimedia (e.g., texts \cite{wang2015collaborative}, images \cite{zhang2016collaborative}).
	\textit{Knowledge graphs} (KGs) are one type of side information for RS, which usually contain fruitful facts and connections about items.
	Recently, researchers have proposed several academic and commercial KGs, such as NELL\footnote{\url{http://rtw.ml.cmu.edu/rtw/}}, DBpedia\footnote{\url{http://wiki.dbpedia.org/}}, Google Knowledge Graph\footnote{\url{https://developers.google.com/knowledge-graph/}} and Microsoft Satori\footnote{\url{https://searchengineland.com/library/bing/bing-satori}}.
	Due to its high dimensionality and heterogeneity, a KG is usually pre-processed by \textit{knowledge graph embedding} (KGE) methods \cite{wang2018graphgan}, which embeds entities and relations into low-dimensional vector spaces while preserving its inherent structure.
	
	\vspace{0.5em}
	\noindent\textbf{Existing KG-aware methods}
	
	Inspired by the success of applying KG in a wide variety of tasks, researchers have recently tried to utilize KG to improve the performance of recommender systems \cite{yu2014personalized,zhao2017meta,wang2018dkn,wang2018ripple,zhang2016collaborative}.
	Personalized Entity Recommendation (PER) \cite{yu2014personalized} and Factorization Machine with Group lasso (FMG) \cite{zhao2017meta} treat KG as a heterogeneous information network, and extract meta-path/meta-graph based latent features to represent the connectivity between users and items along different types of relation paths/graphs.
	It should be noted that PER and FMG rely heavily on manually designed meta-paths/meta-graphs, which limits its application in generic recommendation scenarios.
	Deep Knowledge-aware Network (DKN) \cite{wang2018dkn} designs a CNN framework to combine entity embeddings with word embeddings for news recommendation.
	However, the entity embeddings are required in advance of using DKN, causing DKN to lack an end-to-end way of training.
	Another concern about DKN is that it can hardly incorporate side information other than texts.
	RippleNet \cite{wang2018ripple} is a memory-network-like model that propagates users' potential preferences in the KG and explores their hierarchical interests.
	But the importance of relations is weakly characterized in RippleNet, because the embedding matrix of a relation  can hardly be trained to capture the sense of importance in the quadratic form  ( and  are embedding vectors of two entities).
Collaborative Knowledge base Embedding (CKE) \cite{zhang2016collaborative} combines CF with structural knowledge, textual knowledge, and visual knowledge in a unified framework.
	However, the KGE module in CKE (i.e., TransR \cite{lin2015learning}) is more suitable for in-graph applications (such as KG completion and link prediction) rather than recommendation.
	In addition, the CF module and the KGE module are loosely coupled in CKE under a Bayesian framework, making the supervision from KG less obvious for recommender systems.
	
	\vspace{0.5em}
	\noindent\textbf{The proposed approach}	
	
	To address the limitations of previous work, we propose MKR, a multi-task learning (MTL) approach for knowledge graph enhanced recommendation.
	MKR is a generic, end-to-end deep recommendation framework, which aims to utilize KGE task to assist recommendation task\footnote{KGE task can also benefit from recommendation task empirically as shown in the experiments section.}.
	Note that the two tasks are not mutually independent, but are highly correlated since an item in RS may associate with one or more entities in KG.
	Therefore, an item and its corresponding entity are likely to have a similar proximity structure in RS and KG, and share similar features in low-level and non-task-specific latent feature spaces \cite{long2017learning}.
	We will further validate the similarity in the experiments section.
	To model the shared features between items and entities, we design a \textit{crosscompress unit} in MKR.
	The crosscompress unit explicitly models high-order interactions between item and entity features, and automatically control the cross knowledge transfer for both tasks.
	Through crosscompress units, representations of items and entities can complement each other, assisting both tasks in avoiding fitting noises and improving generalization.
	The whole framework can be trained by alternately optimizing the two tasks with different
frequencies, which endows MKR with high flexibility and adaptability in real recommendation scenarios.
	
	We probe the expressive capability of MKR and show, through theoretical analysis, that the crosscompress unit is capable of approximating sufficiently high order feature interactions between items and entities.
	We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning, including factorization machines \cite{rendle2010factorization, rendle2012factorization}, deepcross network \cite{wang2017deep}, and cross-stitch network \cite{misra2016cross}.
	Empirically, we evaluate our method in four recommendation scenarios, i.e., movie, book, music, and news recommendations.
	The results demonstrate that MKR achieves substantial gains over state-of-the-art baselines in both click-through rate (CTR) prediction (e.g.,   improvements on average for movies) and top- recommendation (e.g.,   improvements on average for books).
	MKR can also maintain a decent performance in sparse scenarios.
	
	\vspace{0.5em}
	\noindent\textbf{Contribution}		
	
	It is worth noticing that the problem studied in this paper can also be modelled as \textit{cross-domain recommendation} \cite{tang2012cross} or \textit{transfer learning} \cite{pan2010survey}, since we care more about the performance of recommendation task.
	However, the key observation is that though cross-domain recommendation and transfer learning have single objective for the target domain, their loss functions still contain constraint terms for measuring data distribution in the source domain or similarity between two domains.
	In our proposed MKR, the KGE task serves as the constraint term \textit{explicitly} to provide regularization for recommender systems.
	We would like to emphasize that the major contribution of this paper is exactly modeling the problem as multi-task learning: We go a step further than cross-domain recommendation and transfer learning by finding that the inter-task similarity is helpful to not only recommender systems but also knowledge graph embedding, as shown in theoretical analysis and experiment results.
	
	

\section{Our Approach}
	In this section, we first formulate the knowledge graph enhanced recommendation problem, then introduce the framework of MKR and present the design of the crosscompress unit, recommendation module and KGE module in detail.
	We lastly discuss the learning algorithm for MKR.
	
	\begin{figure*}[t]
		\centering
        \begin{subfigure}[b]{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/framework.pdf}
            \caption{Framework of MKR}
            \label{fig:framework}
        \end{subfigure}
        \hspace{0.06\textwidth}
        \begin{subfigure}[b]{0.36\textwidth}
            \includegraphics[width=\textwidth]{figures/cross_and_compress_unit.pdf}
            \caption{Crosscompress unit}
            \label{fig:cross_feature_sharing_unit}
        \end{subfigure}
        \caption{(a) The framework of MKR. The left and right part illustrate the recommendation module and the KGE module, respectively, which are bridged by the crosscompress units. (b) Illustration of a crosscompress unit. The crosscompress unit generates a cross feature matrix from item and entity vectors by \textit{cross} operation, and outputs their vectors for the next layer by \textit{compress} operation.}
        \label{fig:1}
    \end{figure*}
    
   \subsection{Problem Formulation}
		We formulate the knowledge graph enhanced recommendation problem in this paper as follows.
		In a typical recommendation scenario, we have a set of  users  and a set of  items .
		The user-item interaction matrix  is defined according to users' implicit feedback, where  indicates that user  engaged with item , such as behaviors of clicking, watching, browsing, or purchasing; otherwise .
		Additionally, we also have access to a knowledge graph , which is comprised of entity-relation-entity triples .
		Here , , and  denote the head, relation, and tail of a knowledge triple, respectively.
		For example, the triple (\textit{Quentin Tarantino}, \textit{film.director.film}, \textit{Pulp Fiction}) states the fact that Quentin Tarantino directs the film Pulp Fiction.
		In many recommendation scenarios, an item  may associate with one or more entities in .
		For example, in movie recommendation, the item "Pulp Fiction" is linked with its namesake in a KG, while in news recommendation, news with the title "Trump pledges aid to Silicon Valley during tech meeting" is linked with entities "Donald Trump" and "Silicon Valley" in a KG.
		
		Given the user-item interaction matrix  as well as the knowledge graph , we aim to predict whether user  has potential interest in item  with which he has had no interaction before.
		Our goal is to learn a prediction function , where  denotes the probability that user  will engage with item , and  is the model parameters of function .
	
	
	\subsection{Framework}		
		The framework of MKR is illustrated in Figure \ref{fig:framework}.
		MKR consists of three main components: recommendation module, KGE module, and crosscompress units.
		(1) The recommendation module on the left takes a user and an item as input, and uses a multi-layer perceptron (MLP) and crosscompress units to extract short and dense features for the user and the item, respectively.
		The extracted features are then fed into another MLP together to output the predicted probability.
		(2) Similar to the left part, the KGE module in the right part also uses multiple layers to extract features from the head and relation of a knowledge triple, and outputs the representation of the predicted tail under the supervision of a score function  and the real tail.
		(3) The recommendation module and the KGE module are bridged by specially designed crosscompress units.
		The proposed unit can automatically learn high-order feature interactions of items in recommender systems and entities in the KG.		
		
	
	\subsection{Crosscompress Unit}
To model feature interactions between items and entities, we design a crosscompress unit in MKR framework.
		As shown in Figure \ref{fig:cross_feature_sharing_unit}, for item  and one of its associated entities , we first construct  pairwise interactions of their latent feature  and  from layer :
		
		where  is the cross feature matrix of layer , and  is the dimension of hidden layers.
		This is called the \textit{cross} operation, since each possible feature interaction  between item  and its associated entity  is modeled explicitly in the cross feature matrix.
		We then output the feature vectors of items and entities for the next layer by projecting the cross feature matrix into their latent representation spaces:
		
		where  and  are trainable weight and bias vectors.
		This is called the \textit{compress} operation, since the weight vectors project the cross feature matrix from  space back to the feature spaces .
		Note that in Eq. (\ref{eq:compress}), the cross feature matrix is compressed along both horizontal and vertical directions (by operating on  and ) for the sake of symmetry, but we will provide more insights of the design in Section \ref{sec:unified_view}.
		For simplicity, the crosscompress unit is denoted as:
		
		and we use a suffix  or  to distinguish its two outputs in the following of this paper.
		Through crosscompress units, MKR can adaptively adjust the weights of knowledge transfer and learn the relevance between the two tasks.
		
		It should be noted that crosscompress units should only exist in low-level layers of MKR, as shown in Figure \ref{fig:framework}.
		This is because:
		(1) In deep architectures, features usually transform from general to specific along the network, and feature transferability drops significantly in higher layers with increasing task dissimilarity \cite{yosinski2014transferable}.
		Therefore, sharing high-level layers risks to possible negative transfer, especially for the heterogeneous tasks in MKR.
		(2) In high-level layers of MKR, item features are mixed with user features, and entity features are mixed with relation features.
		The mixed features are not suitable for sharing since they have no explicit association.
		
		
	\subsection{Recommendation Module}
	\label{section:rm}
		The input of the recommendation module in MKR consists of two raw feature vectors  and  that describe user  and item , respectively.
		 and  can be customized as one-hot ID \cite{he2017neural}, attributes \cite{wang2018shine}, bag-of-words \cite{wang2015collaborative}, or their combinations, based on the application scenario.
		Given user 's raw feature vector , we use an -layer MLP to extract his latent condensed feature\footnote{We use the exponent notation  in Eq. (\ref{eq:mlp}) and following equations in the rest of this paper for simplicity, but note that the parameters of  layers are actually different.}:
						
		where  is a fully-connected neural network layer\footnote{Exploring a more elaborate design of layers in the recommendation module is an important direction of future work.} with weight , bias , and nonlinear activation function .
		For item , we use  crosscompress units to extract its feature:
		
		where  is the set of associated entities of item .
		
		After having user 's latent feature  and item 's latent feature , we combine the two pathways by a predicting function , for example, inner product or an -layer MLP.
		The final predicted probability of user  engaging item  is:
				
	
	
	\subsection{Knowledge Graph Embedding Module}
	\label{section:kgem}
		Knowledge graph embedding is to embed entities and relations into continuous vector spaces while preserving their structure.
		Recently, researchers have proposed a great many KGE methods, including translational distance models \cite{bordes2013translating,lin2015learning} and semantic matching models \cite{nickel2016holographic,liu2017analogical}.
		In MKR, we propose a deep semantic matching architecture for KGE module.
		Similar to the recommendation module, for a given knowledge triple , we first utilize multiple crosscompress units and nonlinear layers to process the raw feature vectors of head  and relation  (including ID \cite{lin2015learning}, types \cite{xie2016representation}, textual description \cite{wang2014knowledge}, etc.), respectively.
		Their latent features are then concatenated together, followed by a -layer MLP for predicting tail :
		
		where  is the set of associated items of entity , and  is the predicted vector of tail .
		Finally, the score of the triple  is calculated using a score (similarity) function :
		
		where  is the real feature vector of .
		In this paper, we use the normalized inner product  as the choice of score function \cite{misra2016cross}, but other forms of (dis)similarity metrics can also be applied here such as Kullback–Leibler divergence.
	
	
	\subsection{Learning Algorithm}
		The complete loss function of MKR is as follows:
		
			
		In Eq. (\ref{eq:loss}), the first term measures loss in the recommendation module, where  and  traverse the set of users and the items, respectively, and  is the cross-entropy function.
		The second term calculates the loss in the KGE module, in which we aim to increase the score for all true triples while reducing the score for all false triples.
		The last item is the regularization term for preventing over-fitting,  and  are the balancing parameters.\footnote{ can be seen as the ratio of two learning rates for the two tasks.}
			
		Note that the loss function in Eq. (\ref{eq:loss}) traverses all possible user-item pairs and knowledge triples.
		To make computation more efficient, following \cite{mikolov2013distributed}, we use a negative sampling strategy during training.
		\begin{algorithm}[t]
  		\caption{Multi-Task Training for MKR}
			\begin{algorithmic}[1]
				\REQUIRE{Interaction matrix , knowledge graph }
				\ENSURE{Prediction function }
				\STATE Initialize all parameters
				\FOR{number of training iteration}
					\STATEx \quad // \textit{recommendation task}
					\FOR{ steps}
						\STATE Sample minibatch of positive and negative interactions from ;
						\STATE Sample  for each item  in the minibatch;
						\STATE Update parameters of  by gradient descent on Eq. (1)-(6), (9);
					\ENDFOR
					\STATEx \quad // \textit{knowledge graph embedding task}
					\STATE Sample minibatch of true and false triples from ;
					\STATE Sample  for each head  in the minibatch;
					\STATE Update parameters of  by gradient descent on Eq. (1)-(3), (7)-(9);
				\ENDFOR
			\end{algorithmic}
		\end{algorithm}		
		The learning algorithm of MKR is presented in Algorithm 1, in which a training epoch consists of two stages: recommendation task (line 3-7) and KGE task (line 8-10).
		In each iteration, we repeat training on recommendation task for  times ( is a hyper-parameter and normally ) before training on KGE task once in each epoch, since we are more focused on improving recommendation performance.
		We will discuss the choice of  in the experiments section.
		


\section{Theoretical Analysis}
	In this section, we prove that crosscompress units have sufficient capability of polynomial approximation.
	We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning.
	
	\subsection{Polynomial Approximation}
		According to the Weierstrass approximation theorem \cite{rudin1964principles}, any function under certain smoothness assumption can be approximated by a polynomial to an arbitrary accuracy.
		Therefore, we examine the ability of high-order interaction approximation of the crosscompress unit. 
		We show that crosscompress units can model the order of item-entity feature interaction up to exponential degree:
		
		\begin{theorem}
		\label{thm:1}
			Denote the input of item and entity in MKR network as  and , respectively.
			Then the cross terms about  and  in  and  (the L1-norm of  and ) with maximal degree is , where ,  for , , and  ().
		\end{theorem}
		
		In recommender systems,  is also called \textit{combinatorial} feature, as it measures the interactions of multiple original features.
		Theorem \ref{thm:1} states that crosscompress units can automatically model the combinatorial features of items and entities for sufficiently high order, which demonstrates the superior approximation capacity of MKR as compared with existing work such as WideDeep \cite{cheng2016wide}, factorization machines \cite{rendle2010factorization, rendle2012factorization} and DCN \cite{wang2017deep}.
		The proof of Theorem \ref{thm:1} is provided in the Appendix.
		Note that Theorem \ref{thm:1} gives a theoretical view of the polynomial approximation ability of the crosscompress unit rather than providing guarantees on its actual performance.
		We will empirically evaluate the crosscompress unit in the experiments section.
		
	
	\subsection{Unified View of Representative Methods}
	\label{sec:unified_view}
		In the following we provide a unified view of several representative models in recommender systems and multi-task learning, by showing that they are restricted versions of or theoretically related to MKR.
		This justifies the design of crosscompress unit and conceptually explains its strong empirical performance as compared to baselines.
		
		\subsubsection{Factorization machines}
		Factorization machines \cite{rendle2010factorization, rendle2012factorization} are a generic method for recommender systems.
		Given an input feature vector, FMs model all interactions between variables in the input vector using factorized parameters, thus being able to estimate interactions in problems with huge sparsity such as recommender systems.
		The model equation for a 2-degree factorization machine is defined as
		
		where  is the -th unit of input vector ,  is weight scalar,  is weight vector, and  is dot product of two vectors.
		We show that the essence of FM is conceptually similar to an 1-layer crosscompress unit:
		
		\begin{proposition}
		\label{prop:1}
			The L1-norm of  and  can be written as the following form:
			
			where  is the sum of two scalars.
		\end{proposition}
		
		It is interesting to notice that, instead of factorizing the weight parameter of  into the dot product of two vectors as in FM, the weight of term  is factorized into the sum of two scalars in crosscompress unit to reduce the number of parameters and increase robustness of the model.
		
		
		\subsubsection{DeepCross Network}
		DCN \cite{wang2017deep} learns explicit and high-order cross features by introducing the layers:
					
		where , , and  are representation, weight, and bias of the -th layer.
		We demonstrate the link between DCN and MKR by the following proposition:
		
		\begin{proposition}
		\label{prop:2}
			In the formula of  in Eq. (\ref{eq:compress}), if we restrict  in the first term to satisfy  and restrict  in the second term to be  (and impose similar restrictions on ), the crosscompress unit is then conceptually equivalent to DCN layer in the sense of multi-task learning:
			
		\end{proposition}
		
		It can be proven that the polynomial approximation ability of the above DCN-equivalent version (i.e., the maximal degree of cross terms in  and ) is , which is weaker than original crosscompress units with  approximation ability.
		
		\subsubsection{Cross-stitch Networks}
		Cross-stitch networks \cite{misra2016cross} is a multi-task learning model in convolutional networks, in which the designed cross-stitch unit can learn a combination of shared and task-specific representations between two tasks.
		Specifically, given two activation maps  and  from layer  for both the tasks, cross-stitch networks learn linear combinations  and  of both the input activations and feed these combinations as input to the next layers' filters.
		The formula at location  in the activation map is
		
		5pt]
   				\tilde x_B^{ij}
  			\end{bmatrix}
  			=
  			\begin{bmatrix}
   				\alpha_{AA} & \alpha_{AB}\\
   				\alpha_{BA} & \alpha_{BB}
  			\end{bmatrix}
  			\begin{bmatrix}
   				x_A^{ij}\
		where 's are trainable transfer weights of representations between task A and task B.
		We show that the cross-stitch unit in Eq. (\ref{eq:csn}) is a simplified version of our crosscompress unit by the following proposition:
		
		\begin{proposition}
		\label{prop:3}
			If we omit all biases in Eq. (\ref{eq:compress}), the crosscompress unit can be written as
			5pt]
   				{\bf e}_l^\top {\bf w}_l^{VE} & {\bf v}_l^\top {\bf w}_l^{EE}
  			\end{bmatrix}
  			\begin{bmatrix}
   				{\bf v}_l\\
   				{\bf e}_l
  			\end{bmatrix}.
			
		\begin{split}
			{\bf v}_1 =& {\bf v} {\bf e}^\top {\bf w}_0^{VV} + {\bf e} {\bf v}^\top {\bf w}_0^{EV} + {\bf b}_0^V\\
			=& \left[v_1 \sum_{i=1}^d e_i w_0^{VV(i)} \ \cdots \ v_d \sum_{i=1}^d e_i w_0^{VV(i)} \right]^\top\\
			&+ \left[e_1 \sum_{i=1}^d v_i w_0^{EV(i)} \ \cdots \ e_d \sum_{i=1}^d v_i w_0^{EV(i)} \right]^\top\\
			&+ \left[ b_0^{V(0)} \ \cdots \ b_0^{V(d)} \right]^\top.
		\end{split}
		
		\begin{split}
			\|{\bf v}_1\|_1 =& \left| \sum_{j=1}^d v_j \sum_{i=1}^d e_i w_0^{VV(i)} + \sum_{j=1}^d e_j \sum_{i=1}^d v_i w_0^{EV(i)} + \sum_{i=1}^d b_0^{V(d)} \right|\\
			=& \left| \sum_{i=1}^d \sum_{j=1}^d (w_0^{EV(i)} + w_0^{VV(j)}) v_i e_j + \sum_{i=1}^d b_0^{V(d)} \right|.
		\end{split}
		
			\|{\bf v}_{l+1}\|_1 = \sum_{i=1}^d \sum_{j=1}^d (w_l^{EV(i)} + w_l^{VV(j)}) v_l^{(i)} e_l^{(j)} + \sum_{i=1}^d b_l^{V(d)}.
		
			\|{\bf v}_1\|_1 = \left| \sum_{i=1}^d \sum_{j=1}^d (w_0^{EV(i)} + w_0^{VV(j)}) v_i e_j + \sum_{i=1}^d b_0^{V(d)} \right|.
		
		
		It is easy to see that , , and .
		The proof is similar for .
	\end{proof}
	
	\vspace{1em}
	
	We omit the proofs for Proposition \ref{prop:2} and Proposition \ref{prop:3} as they are straightforward.
	
	
	
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference} 


\end{document}
