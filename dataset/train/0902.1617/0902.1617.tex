\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\textheight 9.1in 
\textwidth 6.6in


\newcommand{\etal}{{\em et al.}}
\newcommand{\qed}{\mbox{}\hspace*{\fill}\nolinebreak\mbox{}
}
\newcommand{\expect}{{\bf \mbox{\bf E}}}
\newcommand{\prob}{{\bf \mbox{\bf Pr}}}
\newcommand{\opt}{{\rm OPT}}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\newcommand{\e}{{\epsilon}}

\newcommand{\comment}[1]{{\color{gray}[\textsf{#1}]}}
\newcommand{\grainofsalt}[1]{{\color{gray}[\textsf{#1}]}}
\newcommand{\redospace}{\small\renewcommand{\baselinestretch}{1.5}\normalsize}
\newcommand{\undospace}{\small\renewcommand{\baselinestretch}{1}\normalsize}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newenvironment{proof}{{\bf Proof:}}{\par}
\newenvironment{proofnoqed}{{\bf Proof:}}{}
\newenvironment{proofof}[1]{\noindent{\bf Proof of #1:}}{\par}
\newenvironment{proofsketch}{{\sc{Proof Outline:}}}{\par}
\usepackage{hyperref}
\hypersetup{
bookmarksnumbered
}

\renewcommand{\b}{{\mbox {\bf b}}}
\newcommand{\barb}{{\mbox {\bf \bar{b}}}}
\newcommand{\coll}{{\mathcal R}}
\newcommand{\colltwo}{{\mathcal T}}
\newcommand{\collpart}{{\mathcal S}}
\newcommand{\edgesets}{\mathcal X}



\begin{document}


\title{\Large Perfect Matchings in  Time in Regular Bipartite Graphs}
\author{Ashish Goel\thanks{
    Departments of Management Science and Engineering and (by courtesy)
    Computer Science, Stanford University.
    Email: {\tt ashishg@stanford.edu}.
    Research supported by NSF
    ITR grant 0428868, NSF CAREER award 0339262, and a grant from the
    Stanford-KAUST alliance for academic excellence.}\\
\and Michael Kapralov\thanks{
    Institute for Computational and Mathematical Engineering, Stanford University.
    Email: {\tt kapralov@stanford.edu}. Research supported by a Stanford Graduate Fellowship.}\\   
\and Sanjeev Khanna\thanks{Department of Computer and Information Science, University of Pennsylvania,
Philadelphia PA. Email: {\tt sanjeev@cis.upenn.edu}. Supported in
part by a Guggenheim Fellowship, an IBM Faculty Award, and by NSF Award CCF-0635084.}
}

\date{}
\maketitle
\setcounter{page}{0}
\thispagestyle{empty}
\pdfbookmark[1]{Title and abstract}{Myabstract}
\begin{abstract}

  We consider the well-studied problem of finding a perfect matching in
  -regular bipartite graphs with  vertices and  edges. While
  the best-known algorithm for general bipartite graphs (due to Hopcroft and
  Karp) takes  time, in regular bipartite graphs, a perfect
  matching is known to be computable in  time.  Very recently, the
   bound was improved to  expected
  time, an expression that is bounded by .  In this
  paper, we further improve this result by giving an  expected time algorithm for finding a perfect
  matching in regular bipartite graphs; as a function of  alone, the
  algorithm takes expected time .

  To obtain this result, we design and analyze a two-stage sampling scheme that reduces
  the problem of finding a perfect matching in a regular bipartite graph to the
  same problem on a subsampled bipartite graph with  edges. The
  first-stage is a sub-linear time uniform sampling that reduces the size of
  the input graph while maintaining certain structural properties of the
  original graph. The second-stage is a non-uniform sampling that takes
  linear-time (on the reduced graph) and outputs a graph with 
  edges, while preserving a matching with high probability.  This matching is
  then recovered using the Hopcroft-Karp algorithm. While the standard
  analysis of Hopcroft-Karp also gives us an  running
  time, we present a tighter analysis for our special case that results in the
  stronger  time mentioned earlier.


  Our proof of correctness of this sampling scheme uses
  a new correspondence theorem between cuts and Hall's theorem ``witnesses'' for a perfect matching in a bipartite
  graph that we prove. We believe this theorem may be of independent
  interest; as another example application, we show that a perfect matching in
  the support of an  doubly stochastic matrix with  non-zero
  entries can be found in expected time .
\end{abstract}
\newpage

\section{Introduction}

A bipartite graph  with vertex set  and edge set  is said to be regular if every vertex has the same
degree . We use  to denote the number of edges in  and  to
represent the number of vertices in  (as a consequence of regularity, 
and  have the same size). Regular bipartite graphs are a fundamental
combinatorial object, and arise, among other things, in expander
constructions, scheduling, routing in switch fabrics, and
task-assignment~\cite{mr:random,amsz:color2003,cos:regular2001}.

A regular bipartite graph of degree  can be decomposed into exactly 
perfect matchings, a fact that is an easy consequence of Hall's
theorem~\cite{b:graphtheory}, and is closely related to the Birkhoff-von
Neumann decomposition of a doubly stochastic matrix~\cite{b:bvn46,vn:bvn53}.
Finding a matching in a regular bipartite graph is a well-studied problem,
starting with the algorithm of K\"{o}nig in 1916~\cite{k:regular16}, which is
now known to run in time . The well-known bipartite matching algorithm
of Hopcroft and Karp~\cite{hk:match73} can be used to obtain a running time of
. In graphs where  is a power of 2, the following elegant
idea, due to Gabow and Kariv~\cite{gk:edge1982}, leads to an algorithm with
 running time. First, compute an Euler tour of the graph (in time
) and then follow this tour in an arbitrary direction. Exactly half the
edges will go from left to right; these form a regular bipartite graph of
degree . The total running time  thus follows the recurrence  which yields . Extending this idea to the
general case proved quite hard, and after a series of improvements (eg. by
Cole and Hopcroft~\cite{ch:color82}, and then by Schrijver~\cite{s:color99} to
), Cole, Ost, and Schirra~\cite{cos:regular2001} gave an 
algorithm for the case of general . Their main interest was in edge
coloring of general bipartite graphs, where finding perfect matchings in
regular bipartite graphs is an important
subroutine.
Very recently, Goel, Kapralov, and Khanna~\cite{gkk:rbp08}, gave a
sampling-based algorithm that computes a perfect matching in -regular
bipartite graphs in  expected time, an
expression that is bounded by .  The algorithm
of~\cite{gkk:rbp08} uses uniform sampling to reduce the number of edges in the
input graph while preserving a perfect matching, and then runs the
Hopcroft-Karp algorithm on the sampled graph.

\paragraph{Our Results and Techniques:}
We present a significantly faster algorithm for finding perfect matchings in
regular bipartite graphs.
\begin{theorem}
There is an  expected time
algorithm to find a perfect matching in a -regular bipartite graph .
\end{theorem}
As a function of  alone, the running time stated above is . Since the  running time is guaranteed by the algorithm of
Cole, Ost, and Schirra, we are only concerned with the case where  is
. For this regime, our algorithm reduces the perfect
matching problem on a regular bipartite graph  to the same problem on a
(not necessarily regular) sparse bipartite graph  with 
edges. This reduction takes time . We then use the
Hopcroft-Karp algorithm on  to recover a perfect matching. A black-box use
of the analysis of the Hopcroft-Karp algorithm would suggest a running time of
. However, we show that the final
sampled graph has some special structure that guarantees that the
Hopcroft-Karp algorithm would complete in time  whp.

For every pair , we define a {\em witness set}
 to be the set of all edges going from  to . Of
particular interest are what we call {\em Hall witness sets}, which correspond
to ; the well-known Hall's theorem~\cite{b:graphtheory} says that a
bipartite graph  contains a perfect matching iff  includes an
edge from each Hall witness set.
Thus any approach that reduces the size of the input bipartite graph by
sampling must ensure that some edge from every Hall witness set is included in
the sampled graph; otherwise the sampled graph no longer contains a perfect
matching.  Goel, Kapralov, and Khanna~\cite{gkk:rbp08} showed that no {\em
  uniform sampling} scheme on a -regular bipartite graph can reduce the
number of edges to  while preserving a perfect
matching, and hence their -time algorithm is the best
possible running time achievable via uniform sampling followed by a black-box
invocation of the Hopcroft-Karp analysis.

In order to get past this barrier, we use here a two-stage sampling
process. The first stage is a uniform sampling (along the lines
of~\cite{gkk:rbp08}) which generates a reduced-size graph  that
preserves not only a perfect matching but also a key relationship between the
sizes of ``relevant'' witness sets and cuts in the graph . The second stage
is to run the non-uniform Bencz\'{u}r-Karger sampling scheme
~\cite{benczurkarger96} on  to generate a graph  with 
edges while preserving a perfect matching w.h.p. Since this step requires
 time, we crucially rely on the fact that  does not
contain too many edges.

While our algorithm is easy to state and understand, the proof of correctness
is quite involved. The Bencz\'{u}r-Karger sampling was developed to generate, for
any graph, a weighted subgraph with  edges that approximately
preserves the size of all cuts in the original graph. The central idea
underlying our result is to show that there exists a collection of {\em core}
witness sets that can be identified in an almost one-one manner with cuts in
the graph such that the probability mass of edges in each witness set is
comparable to the probability mass of the edges in the cut identified with
it. Further, every witness set in the graph has a ``representative'' in this
collection of core witness sets. Informally, this allows us to employ
cut-preserving sampling schemes such as Bencz\'{u}r-Karger as
``witness-preserving'' schemes. We note here that the natural mapping which
assigns the witness set of a pair  to the cut edges associated with
this pair can map arbitrarily many witness sets to the same cut and is not
useful for our purposes. One of our contributions is an uncrossing
theorem for witness sets, that we refer to as the {\em proportionate
  uncrossing theorem}.  Informally speaking, it says that given any collection
of witness sets  such that the probability mass of each witness set is
comparable to that of its associated cut, there exists another collection
 of witness sets such that (i) the natural mapping to cuts as
defined above is {\em half-injective} for , that is, at most two
witness sets in  map to any given cut, (ii) the probability mass of
each witness set is comparable to the probability mass of its associated cut,
and (iii) any subset of edges that hits every witness set in  also
hits every witness set in .  The collection  is referred to
as a proportional uncrossing of . As shown in
Figure~\ref{fig:cross}(a), we can not achieve an injective mapping, and hence
the half-injectivity is unavoidable.

We believe the half-injective correspondence between witness sets and cuts, as
facilitated by the proportionate uncrossing theorem, is of independent
interest, and will perhaps have other applications in this space of
problems. We also emphasize here that the uncrossing theorem holds for all
bipartite graphs, and not only regular bipartite graphs. Indeed, the graph
 on which we invoke this theorem does not inherit the regularity property
of the original graph .  As another illustrative example, consider the
celebrated Birkhoff-von Neumann theorem~\cite{b:graphtheory,vn:bvn53} which
says that every doubly stochastic matrix can be expressed as a convex
combination of permutation matrices (i.e., perfect matchings).  In some
applications, it is of interest to do an iterative decomposition whereby a
single matching is recovered in each iteration. The best-known bound for this
problem, to our knowledge, is an  time algorithm that follows from the
work of Gabow and Kariv~\cite{gk:edge1982}; here  denotes the maximum
number of bits needed to express any entry in . The following theorem is an
easy consequence of our proportionate uncrossing result.

\begin{theorem} \label{thm:bvn}
  Given an  doubly-stochastic matrix  with  non-zero
  entries, one can find a perfect matching in the support of  in
   expected time.
\end{theorem}

The proof of this theorem and a discussion of known results about this problem are given in section ~\ref{sec:bvn}. Though this
result itself represents only a modest improvement over the earlier 
running time, it is an instructive illustration of the utility of the
proportionate uncrossing theorem.

It is worth noting that while the analysis of Goel, Kapralov, and Khanna was
along broadly similar lines (sample edges from the original graph, followed by
running the Hopcroft-Karp algorithm), the proportionate uncrossing theorem
developed in this paper requires significant new ideas and is crucial to
incorporating the non-uniform sampling stage into our algorithm. Further, the
running time of the Hopcroft-Karp algorithm is easily seen to be
 even for the 2-regular graph consisting of
 disjoint cycles of lengths 
respectively; the stronger analysis for our special case requires both our
uncrossing theorem as well as a stronger decomposition\footnote{It is known
  that the Hopcroft-Karp algorithm terminates quickly on bipartite
  expanders~\cite{motwani}, but those techniques don't help in our setting
  since we start with an arbitrary regular bipartite graph.}. As a step in
this analysis, we prove the independently interesting fact that after sampling
edges from a -regular bipartite graph with rate , for some
suitable constant , we obtain a graph that has a matching of size
 whp and such a matching can be found in  augmenting phases
of the Hopcroft-Karp algorithm whp.

\noindent
\paragraph{Organization:}
Section~\ref{sec:prelim} reviews and presents some useful corollaries of
relevant earlier work. In Section~\ref{sec:proportion}, we establish the
proportionate uncrossing theorem. In section~\ref{sec:algo}, we
present and analyze our two-stage sampling scheme, and
section~\ref{sec:improved-runtime} outlines the stronger analysis of the
Hopcroft-Karp algorithm for our special case. Section~\ref{sec:bvn} contains the proof of Theorem \ref{thm:bvn}
and a discussion of known results on finding perfect matchings in the support of double stochastic matrices.

\section{Preliminaries}
\label{sec:prelim}

In this section, we adapt and present recent results of Goel, Kapralov, and
Khanna~\cite{gkk:rbp08} as well as the Bencz\'{u}r-Karger sampling
theorem~\cite{benczurkarger96} for our purposes, and also prove a simple
technical lemma for later use.
\subsection{Bipartite Decompositions and Relevant Witness Pairs}
\label{sec:define}
Let  be a regular bipartite graph, with vertex set  and
edge set . Consider any partition of  into  sets
, and a partition of  into . Let  denote the (not necessarily regular) bipartite graph  where . We will call this a
``decomposition'' of . 

Given  and , define the witness set corresponding
to the pair , denoted , as the set of all edges between  and
, and define the cut  as the set of all edges between  and . The rest of the definitions
in this section are with respect to some arbitrary but fixed decomposition of
.

\begin{definition}
  An edge  is relevant if  for some .
\end{definition}

\begin{definition}
  Let  be the set of all relevant edges. A pair  is said to be
  relevant if
  \begin{enumerate}
  \item  and  for some ,
  \item , and
  \item There does not exist another , , such that
    , , and .
  \end{enumerate}
\end{definition}

Informally, a relevant pair is one which is contained completely within a
single piece in the decomposition, and is ``minimal'' with respect to that
piece. The following lemma is implicit in~\cite{gkk:rbp08} and is proved in
appendix~\ref{append:relevant-only} for completeness.
\begin{lemma}
\label{lem:relevant-only}
  Let  denote all relevant pairs  with respect to a
  decomposition of , and let  denote all relevant edges. Consider any
  graph . If for all , we have , then  has a perfect matching.
\end{lemma}

\subsection{A Corollary of Bencz\'{u}r-Karger Sampling Scheme}

The Bencz\'{u}r-Karger sampling theorem~\cite{benczurkarger96} shows that for any
graph, a relatively small {\em non-uniform} edge sampling rate suffices to
ensure that every cut in the graph is hit by the sampled edges (i.e. it has a non-empty intersection)
with high probability. The sampling rate used for each edge  inversely
depends on its strength, as defined below.

\label{sec:benczur-karger}
\begin{definition}~\cite{benczurkarger96} A -strong component of a graph
   is a maximal vertex-induced subgraph of  with edge-connectivity .  The
  strength of an edge  in a graph  is the maximum value of  such that
  a -strong component contains .
\end{definition}
\begin{definition}
  Given a graph , let  denote the subgraph of 
  restricted to edges of strength  or higher, where  is some integer in
  .
\end{definition}

It is easy to see that whenever a cut in a graph  contains an edge of strength ,
then the cut must contain at least  edges. Furthermore, for any , each connected
component of graph  is contained inside some connected component of .
The Bencz\'{u}r-Karger theorem utilizes these properties to show that it suffices to sample
each edge  with probability .

We now extend this sampling result to any collection of edge-sets for which there exists an injection
(one-one mapping) to cuts of comparable inverse strengths. The statement
of our theorem~\ref{thm:Benczur-Karger} closely mirrors the Bencz\'{u}r-Karger
sampling theorem, and the proof is also along the same general lines. However, the proof does not follow from the Bencz\'{u}r-Karger sampling theorem in a black-box fashion, so a proof is provided in appendix \ref{append:bk}
\begin{theorem}
\label{thm:Benczur-Karger}
Let  be any graph on  vertices, and let  denote the set
of all possible edge cuts in , and  be a constant. Let
 be a subgraph of  obtained by sampling each edge  in  with
probability

where  denotes the strength of edge , and  is a suitably large
constant. Further, let  be a collection of subset of edges, and let 
be a one-one (not necessarily onto) mapping from  to 
satisfying  for all . Then

\end{theorem}

The result below from~\cite{benczurkarger96} bounds the number of edges chosen
by the sampling in Theorem~\ref{thm:Benczur-Karger}.
\begin{theorem}
\label{thm:Benczur-Karger2}
Let  be any graph on  vertices, and let  be a subgraph of  obtained by
sampling each edge  in  with probability

where  denotes the strength of edge , and  is any constant. Then
with probability at least , the graph  contains at most
 edges, where  is another suitably large constant.
\end{theorem}

We conclude with a simple property of integer multisets that we will use
later. A similar statement was used in \cite{karger-levine} (lemma 4.5). A proof is provided in appendix~\ref{append:strength} for completeness.
\begin{lemma}
\label{lem:strength}
Let  and  be two arbitrary multisets of positive integers such that  for some .
Then there exists an integer  such that

\end{lemma}

\section{Proportionate Uncrossing of Witness Sets}
\label{sec:proportion}
Consider a bipartite graph , with a non-negative weight function
 defined on the edges. Assume further that we are given a set of ``relevant
edges'' . We can extend the definition of  to sets of
edges, so that , where .

\begin{definition}
\label{def:gamma_thick}
For any  and , the pair  is said to be
-thick with respect to  if , {\em i.e.}, the total weight of the relevant edges in  is {\em
  strictly} more than  times the total weight of . A set of
pairs  where each  and each  is said to be a -thick
collection with respect to  if every pair  is
-thick.
\end{definition}

The quantities  and  will be fixed for this section, and
for brevity, we will omit the phrase ``with respect to '' in the
rest of this section.

Before defining proportionate uncrossings of witness sets, we will informally
point out the motivation for doing so. If a pair  is -thick for
some constant , and if we know that a sampling process where edge 
is chosen with probability  chooses some edge from  with high
probability, then increasing the sampling probability by a factor of
 should result in some relevant edge from  being chosen with
high probability as well, a fact that would be very useful in the rest of this
paper. The sampling sub-routines that we employ in the rest of this paper are
analyzed by using union-bound over all cuts, and in order to apply the same
union bound, it would be useful if each witness set were to correspond to a
unique cut. However, in figure~\ref{fig:cross}(a), we show two pairs 
and  which are both -thick but correspond to the same cut; we
call this a ``crossing'' of the pairs  and , drawing intuition
from the figure. In general, we can have many witness sets that map to the
same cut. We would like to ``uncross'' these witness sets by finding subsets
of each witness set that map to unique cuts, but there is no way to uncross
figure~\ref{fig:cross}(a) in this fashion. Fortunately, and somewhat
surprisingly, this is the worst case: any collection of -thick pairs
can be uncrossed into another collection such that all the pairs in the new
collection are also -thick (hence the term proportionate uncrossing),
every original witness set has a representative in this new collection, and no
more than two new pairs have the same cut. Figure~\ref{fig:cross}(b) shows two
-thick pairs that can be uncrossed using a single -thick
representative, . We will spend the rest of this section
formalizing the notion of proportionate uncrossings and proving their
existence. The uncrossing process is algorithmically inefficient, but we only
need to demonstrate existence for the purpose of this paper. The arguments in
this section represent the primary technical contribution of this paper; these
arguments apply to bipartite graphs in general (not necessarily regular), and
may be independently interesting. \vspace{-0.1in}
\begin{figure}[htbp]
  \centering
  \includegraphics[height=1.8in]{uncross_fig.pdf}
  \vspace{-0.15in}
  \caption{ Both (a) and (b) depict two -thick pairs  and
     that have different witness sets but the same cut (i.e.  but ). The pairs in
    (a) can not be uncrossed, whereas the pairs in (b) can be uncrossed by
    choosing the single pair  as a representative.}
\label{fig:cross}
\end{figure}
\subsection{Proportionate Uncrossings: Definitions and Properties}
\label{sec:define-proportionate}
\begin{definition}
\label{define:uncross}
A -uncrossing of a -thick collection  is another
-thick collection of pairs  that satisfies the three
properties below:
\begin{description}
\item[P1:] For every pair  there exists a pair  such that , and . We will refer to  as a representative of .
\item[P2] For every , there exists  such
    that .
  \item[P3:] {\em (Half-injectivity):} There can not be three distinct pairs
     and  in  such that .
\end{description}
\end{definition}

Since  has the same (or larger) thickness as the thickness guarantee
that we had for , it seems appropriate to refer to  as a
proportionate uncrossing of .

\begin{definition}
  \label{define:partial}
  A -partial-uncrossing of a -thick collection  is
  another -thick collection of pairs  which satisfies
  properties P1,P2 above but not necessarily P3.
\end{definition}
The following three lemmas follow immediately from the two definitions above,
and it will be useful to state them explicitly. Informally, the first says
that every collection is its own partial uncrossing, the second says that
uncrossings can be composed, and the third says that the union of the partial
uncrossings of two collections is a partial uncrossing of the union of the
collections.
\begin{lemma}
  \label{lem:self-uncross}
  If  is a -thick collection, then  is a
  -partial uncrossing of itself.
\end{lemma}
\begin{lemma}
\label{lem:compose}
If  is a -partial uncrossing of a -thick collection
, and  is a -uncrossing of , then
 is also a -uncrossing of .
\end{lemma}
\begin{lemma}
  \label{lem:union}
  If  and  are two -thick connections, 
  is a -partial-uncrossing of , and  is a
  -partial-uncrossing of , then 
  is a -partial-uncrossing of .
\end{lemma}

\subsection{Proportionate Uncrossings: An Existence Theorem}
\label{sec:exists-proportionate}
The main technical result of this section is the following:
\begin{theorem}
\label{thm:uncross}
  For every -thick collection , there exists a
  -uncrossing of . 
\end{theorem}
The proof is via induction over the ``largest cut'' corresponding to any pair
in the collection ; each inductive step ``uncrosses'' the witness sets
which corresponds to this largest cut. Before proving this theorem, we need to
provide several useful definitions and also establish a key lemma.

Define some total ordering  over all subsets of  which respects set
cardinality, so that if  then . Overload
notation to use  to denote the set of cuts \}. Analogously, use  to denote the set of witness sets
corresponding to pairs in . Since  may be equal to 
for , it is possible that  may be smaller than
. In fact, if  and  are equal, then  is its
own -uncrossing and the theorem is trivially true. Similarly, it is
possible that  is equal to  for two different pairs 
and  in . However, suppose  and  for two different pairs  and  in . In this
case, we can remove one of the two pairs from the collection to obtain a new
collection ; it is easy to see that a -uncrossing of 
is also a -uncrossing of . So we will assume without loss of
generality that for any two pairs  and  in , either
 or ; we will call this the {\em
  non-redundancy} assumption.

We will now prove a key lemma which contains the meat of the uncrossing
argument. When we use this lemma later in the proof of
theorem~\ref{thm:uncross}, we will only use the fact that there exists a
-partial-uncrossing of , where  satisfies the
preconditions of the lemma. However, the stronger claim of existence of a
-uncrossing does not require much additional work and appears to be an
interesting graph theoretic argument in its own right, so we prove this
stronger claim.

\begin{lemma}
  \label{lem:uncross}
  If  is a -thick collection such that , 
  satisfies the non-redundancy assumption, and  contains a single
  set , then there exists a -uncrossing  of
  . Further, for every pair , we have .
\end{lemma}
\begin{proofnoqed}
  Let . Since  for all , we know by the non-redundancy assumption that  for . We break the proof down into multiple
  stages.

  \begin{enumerate}
  \item {\em Definition of Venn witnesses and Venn cuts. } For any
    -dimensional bit-vector , define
    

    We overload notation and use  to denote the witness set
     and  to denote the cut set
    . A node  belongs to  if it is in every
    set  such that  and not in any of the sets  for which
    . Thus, each  corresponds to one of the regions in the
    Venn diagram of the sets , and the analogous
    statement holds for each . Hence, we will refer to the sets
     and  as the Venn-witness and the Venn-cut for ,
    respectively, and refer to the pair  as a Venn
    pair. Also, we will use  to refer to a vector which differs
    from  in every bit.

  \item {\em The special structure of Venn witnesses and Venn cuts.} Consider
    an edge  that goes out of . Suppose that edge goes to
     where  and . Then there must exist
     such that  and . Since
    , either  (if ) or  (if ). In either case the edge 
    does not belong to the cut , and since all pairs in 
    have the same cut , we conclude that . On the other
    hand, since , either 
    (if ) or  (if
    ). In either case the edge  belongs to the
    cut  and hence to , which is a contradiction. Thus,
    {\em any edge from  goes to either  or
      }.

    If the edge  goes to  then it does not belong to any
    witness set in , any Venn witness set, any Venn cut, or . If
     goes to  then it belongs to , to the Venn
    witness set , to the Venn cuts  and ,
    and to no other Venn witness set or Venn cut. This edge also belongs to
     for all  such that . These observations, and the
    definitions of Venn witnesses, cuts, and pairs easily lead to the
    following consequences:
    
    
    
    
    
 and finally,
 

\item{\em The collection .} Define  to consist of all
  -thick Venn pairs  where  is not the all zero
  vector.

\item{\em Proving that  is a -uncrossing of .}  {\bf
    (P1):} Fix some . Since  is a -thick
  collection, it follows from the definition that  must be a
  -thick pair. From equations~\ref{eq:venn-witness-union}
  and~\ref{eq:venn-witness-disjoint}, we know that . We also know, from
  equations~\ref{eq:venn-cut-disjoint} and~\ref{eq:venn-cut-union}, that . Hence, there must be some  such that  and  is -thick,
  which in turn implies that  is in . This is the
  representative of  and hence  satisfies P1.  {\bf
    (P2):} This follows trivially from equation~\ref{eq:venn-cut-union}.  {\bf
    (P3):} From equation~\ref{eq:venn-cut-disjoint} we know that there are
  only two possible Venn pairs (specifically,  and
  ) that have the same non-empty cut
  . Observe that our definition of -thickness involves
  ``strict inequality'', and hence Venn pairs where the Venn witness set and
  the Venn cut are both empty can't be -thick and can't be in
  .

\item {\em Proving that  for all pairs .}
  Any cut  is of the form  for some
  -dimensional bit vector . Each , from
  equation~\ref{eq:venn-cut-union}. We will now show that this containment is
  strict. Suppose not, {\em i.e.}, there exists some . By
  equation~\ref{eq:venn-cut-symmetry},  as well. Since
  , either  or  must have two bits that are set to 1;
  without loss of generality, assume that . From
  equations~\ref{eq:venn-witness-disjoint} and~\ref{eq:venn-witness-cut}, we
  know that  (and hence ) is the disjoint union of  and
  . Any edge in  must belong to both 
  and , whereas any edge in  can not belong to
  either  or . Hence,  which contradicts the non-redundancy assumption on
  . Therefore, we must have . 
\end{enumerate}
\end{proofnoqed}


\begin{proofof}{Theorem~\ref{thm:uncross}}
  The proof will be by induction over the largest set in  according
  to the ordering . Let  denote this largest set.

  For the base case, suppose  is the smallest set  under the
  ordering . Then  must be singleton,  must have just a
  single set , and  must also have a single witness set, which
  must be the same as  since  is -thick. By the
  non-redundancy assumption,  must have at most one pair, and is its
  own -uncrossing.

  For the inductive step, consider any possible cut  and assume that the
  theorem is true when . We will show that the theorem is
  also true when , which will complete the inductive proof.

  Suppose there is a unique  such that . Intuitively, one would expect this to be the easy case, since there is
  no ``uncrossing'' to be done for , and indeed, this case is quite
  straightforward. Define . Let  denote a
  -uncrossing of , which is guaranteed to exist by the
  inductive hypothesis. Since  is -thick, so is . The pair  clearly has a representative in
   (itself), and any  has a
  representative in  and hence also in . Thus, 
  satisfies property P1 for being a -uncrossing of .  Every set
  in  is a subset of some cut in  (by property P2)
  and  is also in , and hence  satisfies property
  P2 for being a -uncrossing of . Every set in  is
  smaller than  according to  and  satisfies
  property P3. Hence,  also satisfies property P3. Thus, 
  is a -uncrossing of . If there are exactly two distinct pairs
   and  in  such that , then the
  same argument works again, except that  and .

  We now need to tackle the most interesting case of the inductive step, where
  there are more than two pairs in  that correspond to the same cut
  . Write  where  for all
   and  for all . Recall
  that for two different pairs  and  in , we must
  have  by the non-redundancy assumption. From
  lemma~\ref{lem:uncross}, there exists a -partial-uncrossing, say
  , of  with the property that for every set , we have , and hence . By
  lemma~\ref{lem:self-uncross}, we know that  is its own
  -partial-uncrossing. Further, by definition of , every set
   must satisfy . Define . By lemma~\ref{lem:union},  is a
  -partial-uncrossing of , {\em i.e.}, of
  . Further, for every cut , we have . Hence, by our inductive hypothesis, there exists a -uncrossing
  of ; let  be a -uncrossing of . By
  lemma~\ref{lem:compose},  is also a -uncrossing of
  , which completes the inductive proof.
\end{proofof}

\begin{remark} \label{rm:other-approach}
An alternate approach to relating cuts and witness sets is to suitably
modify the proof of the
Bencz\'{u}r-Karger sampling theorem, circumventing the need for the
proportionate uncrossing
theorem. The idea is based on the observation that Karger's sampling
theorem also holds for
vertex cuts in graphs. Since Bencz\'{u}r-Karger sampling theorem is
proved using multiple
invocations of Karger's sampling theorem, it is possible to set up a
correspondence between
cuts and witness sets using a vertex-cut version of the
Bencz\'{u}r-Karger sampling theorem.
However, we prefer to use here the approach based on the proportionate
uncrossing theorem
as it is an interesting combinatorial statement in its own right.
\end{remark}


\section{An  Time Algorithm for Finding a Perfect Matching}
\label{sec:algo}

We present here an  time randomized algorithm to find a
perfect matching in a given -regular bipartite graph  on 
vertices.  Throughout this section, we follow the convention that for any pair
, the sets  and  are defined with respect to the graph
.  Our starting point is the following theorem, established by Goel,
Kapralov, and Khanna~\cite{gkk:rbp08}\footnote{Part 1 of
  theorem~\ref{thm:GKK09} corresponds to theorem 2.3 in~\cite{gkk:rbp08}, part
  2 is proved as part of the proof of theorem 2.1 in~\cite{gkk:rbp08}, and
  part 3 combines remark 2.5 in~\cite{gkk:rbp08} with Karger's sampling
  theorem~\cite{kar94a}.}

\begin{theorem}
\label{thm:GKK09}
Let  be a -regular bipartite graph,  any number in
, and  a suitably large constant that depends on
. There {\em exists} a decomposition of  into 
vertex-disjoint bipartite graphs, say , such that

\begin{enumerate}
\item
Each  contains at least  perfect matchings, and the minimum cut in each  is .

\item Let  denote the set of relevant pairs with respect to this
  decomposition, and  denote the set of relevant edges.  Then for each
   in , we have .

\item Let  be a random graph generated by sampling the edges of
   {\em uniformly} at random with probability .
  Then with probability at least , for every pair ,

\end{enumerate}
\end{theorem}


The last condition above says that in addition to all cuts, all relevant
witness edge sets are also preserved to within  of their
expected value in , with high probability. We emphasize here that the
decomposition highlighted in Theorem~\ref{thm:GKK09} will be used only in the
analysis of our algorithm; the algorithm itself is oblivious to this
decomposition.

Our algorithm consists of the following three steps.

\begin{description}

\item[(S1)] Generate a random graph  by sampling edges of 
  {\em uniformly} at random with probability 
  where  is a constant as in Theorem~\ref{thm:GKK09}\footnote{The time
    required for this sampling is proportional to the number of edges chosen,
    assuming the graph is presented in an adjacency list representation with
    each list stored in an array.}.  We choose  to be any fixed
  constant not larger than .

\item[(S2)] The graph  contains  edges w.h.p. We
  now run the Bencz\'{u}r-Karger sampling algorithm~\cite{benczurkarger96} that
  takes  time to compute the strength  of every edge ,
  and samples each edge  with probability ;\footnote{In fact, this
    sampling algorithm computes an upper bound on , but this only affects
    the running time and the number of edges sampled by a constant factor.}
  here  is as given by Theorem~\ref{thm:Benczur-Karger} with . We show below that w.h.p. the graph  obtained
  from this sampling contains a perfect matching.

\item[(S3)] Finally, we run the Hopcroft-Karp algorithm to obtain a maximum
  cardinality matching in  in  time since by
  Theorem~\ref{thm:Benczur-Karger2},  contains  edges w.h.p.
\end{description}

\paragraph{Running time:} With high probability, the running time of this
algorithm is bounded by . Since we can
always use the algorithm of Cole, Ost, and Schirra~\cite{cos:regular2001}
instead, the final running time is . This reduces to  if ; to
 when ; and to at most  in the narrow range .

\paragraph{Correctness:} To prove correctness, we need to show that 
contains a perfect matching w.h.p.
\begin{theorem}\label{thm:correctness}
  The graph  contains a perfect matching with probability .
\end{theorem}
\begin{proof}
  Consider the decomposition defined in Theorem~\ref{thm:GKK09}.  Let 
  denote the set of relevant pairs with respect to this decomposition, and let
   denote the set of all relevant edges with respect to this
  decomposition.  We will now focus on proving that, with high probability,
  for every , ; by
  Lemma~\ref{lem:relevant-only}, this is sufficient to prove the theorem.

  For convenience, define  and . Assume for now that the low-probability event in
  Theorem~\ref{thm:GKK09} does not occur. Thus, by choosing , we know that for , every relevant pair 
  satisfies
  
  
  Let  denote the strength of  in . Recall that
   is the graph with the same vertex set as  but
  consisting of only those edges in  which have strength at least
  . Define  to be the set of all edges in ;
  define  analogously. Define . Since , by
  Lemma~\ref{lem:strength}, there must exist a  such that
  
  which implies that  is -thick with
  respect to , as defined in Definition~\ref{def:gamma_thick}.
  Partition  into ,
  , such that if  then
   is -thick with respect to , breaking
  ties arbitrarily if  can belong to multiple . Consider
  an arbitrary non-empty . Let  represent a
  -uncrossing of , as guaranteed by
  Theorem~\ref{thm:uncross}. By property P3, no three pairs in a
  -uncrossing can have the same cut; partition  into
   and  such that every pair 
  has a unique cut  and the same holds for . We
  focus on  for now. For any , define . Define . For any , define  for
  some arbitrary  such that . The function 
  is one-one by construction, and since  is -thick, we know
  that . Thus,
   satisfies the preconditions of
  Theorem~\ref{thm:Benczur-Karger}. Further, the sampling probability  in
  step {\bf(S2)} of the algorithm is chosen to correspond to
  . Thus, with probability at least ,  is
  non-empty for all , {\em i.e.},  for all . Since  is a
  subgraph of , we can conclude that  for all  with probability at least
  .

  Since the analogous argument holds for , we obtain  for all  with probability
  at least . Since  is a -uncrossing of
  , we use property P1 to conclude that  for all , again with
  probability at least . Applying the union bound over all , we
  further conclude that  for all
   with probability at least . As mentioned before,
  this suffices to prove that  has a perfect matching with probability
  at least , by Lemma~\ref{lem:relevant-only}.  We assumed that
  condition 3 in theorem~\ref{thm:GKK09} is satisfied; this is violated with
  probability at most , which proves that  has a perfect
  matching with probability at least .
\end{proof}


As presented above, the algorithm takes time  with high probability, and outputs a perfect matching with probability
. We conclude with two simple observations. First.  it is easy to
convert this into a Monte Carlo algorithm with a worst case running-time of
, or a Las Vegas algorithm with an expected
running-time of .  If either the sampling
process in steps {\bf (S1)} or {\bf (S2)} returns too many edges, or step {\bf
  (S3)} does not produce a perfect matching, then (a) abort the computation to
get a Monte Carlo algorithm, or (b) run the  time algorithm of Cole,
Ost, and Schirra~\cite{cos:regular2001} to get a Las Vegas algorithm. Second,
by choosing larger constants during steps {\bf (S1)} and {\bf (S2)}, it is
easy to amplify the success probability to be at least  for
any fixed .

\section{An Improved  Bound on the
  Runtime} \label{sec:improved-runtime} 
 In this section we give an improved analysis of the runtime of the Hopcroft-Karp algorithm on the subsampled graph, ultimately leading to a bound of 
  for our algorithm. The main ingredients of our analysis are (1) a decomposition of the graph  into  vertex-disjoint -edge-connected subgraphs,  (2) a modification of the uncrossing argument that reveals properties of sufficiently unbalanced witness sets in the sampled graph obtained in step \textbf{S2}, and (3) an upper bound on 	length of the shortest augmentating path in the sampled graph relative to any matching of size smaller than . 

\subsection{Combinatorial uncrossings}  
Theorem \ref{thm:ss} below, which we state for general bipartite graphs, requires a variant of the uncrossing theorem that we formulate now. We introduce the definition of combinatorial uncrossings:
\begin{definition}
\label{define:comp-uncross}
Let  be any collection of pairs . A combinatorial uncrossing of  is a tuple , where  is another collection and  is a mapping from  to subsets of , such that the following properties are satisfied: 
\begin{description}
\item[Q1:] For all 
\begin{enumerate}
\item  are disjoint;
\item  are disjoint;
\item  are disjoint;
\item  for all ;
\item 

\end{enumerate}
\item[Q2:](Half-injectivity) There cannot be three distinct pairs  in  such that .
\end{description}
\end{definition}
The proof of existence of combinatorial uncrossings is along the lines of the proof of existence of -thick uncrossings, so we omit it here.

For a graph  we denote  and , and omit the subscript when the
underlying graph is fixed.
\begin{theorem} \label{thm:ss}
Let  be a graph obtained by sampling edges uniformly at random with
probability  from a bipartite graph  on  vertices with a minimum cut of size
. Then there
exists a constant  such that for all  if 
then w.h.p. for all , and , we have

\end{theorem}
\begin{proof}
Define  as the set of pairs . Denote a combinatorial uncrossing of  by .
We first prove the statement for pairs from , and then extend it to pairs from  to obtain the desired result.

Consider a pair .  Denote . We shall write  and  instead of  and  in what follows for brevity. We have by Chernoff bounds that for a given pair  

since . Since  satisfies Q2, we get that

for  by Corollary 2.4 in \cite{kar94a}. This implies that for  we have with probability  for all 
 

Now consider any pair . Summing \eqref{ab-bound} over all  and using properties Q1.1-5, we get
 
for all  as required.
\end{proof}

 
\subsection{Decomposition of the graph } \label{subsec:decomp}
Corollary \ref{cor:unbalanced-ws}, which relates
the size of sufficiently unbalanced witness sets in the sampled graph to the
size of the corresponding cuts is the main result of this subsection. It follows from theorem~\ref{thm:ss} and a stronger
(than~\cite{gkk:rbp08}) decomposition of bipartite -regular graphs that we outline now.
\begin{theorem} \label{thm:decomposition}
Any -regular graph  with  vertices can be decomposed into
 vertex-disjoint induced subgraphs , where , that satisfy the following properties:

\begin{enumerate}
 \item The minimum cut in each  is at least .



 \item .
\end{enumerate}
\end{theorem}

To prove Theorem \ref{thm:decomposition}, we give a procedure that decomposes the graph  into vertex-disjoint induced subgraphs , ,  such that the min-cut in  is at least  and at most  edges run between pieces of the decomposition. 


The procedure is as follows. Initialize , and set .
\begin{enumerate}
\item Find a smallest proper subset  such that . If no such set exists, define  to be the graph  and terminate. 
\item Define  to be the subgraph of  induced by vertices in , i.e. . Also, define  to be the graph  with vertices from  removed.
\item Increment  and go to step 1.	
\end{enumerate}

We now prove that the output of the decomposition procedure satisfies the properties claimed above.
\begin{lemma}\label{lm:mincut}
The min-cut in  is greater than .
\end{lemma}
\begin{proof}
If  contains a single vertex the min-cut is infinite by definition, so we assume wlog that  contains at least two vertices.
The proof is essentially the same as the proof of property \textbf{P1} of the decomposition procedure in \cite{gkk:rbp08} (see Theorem 2.4).

Suppose that there exists a cut  in  where  and
, such that  (note that it is possible that  and ).  We have  by the choice of  in (1). Suppose without loss of generality that .
Then  and , which contradicts the choice of  as the smallest cut of value at most  in step (1) of the procedure.
\end{proof}


\begin{lemma} \label{lm:edges-removed}
The number of steps in the decomposition procedure is , and at most  edges are removed in the process.
\end{lemma}
\begin{proof}
We call a vertex  {\em bad} if its degree in  is smaller than . Note that for each  either  contains a bad vertex or . 

Note that since strictly fewer than  edges are removed in each iteration, the number of bad vertices created in the first  iterations is strictly less than .  Hence, during at least half of the  iterations at least  vertices were removed from the graph, i.e.

This implies that the process terminates in at most  steps, and the number of edges removed is at most .
\end{proof}
\begin{proofof}{Theorem \ref{thm:decomposition}}
The proof follows by putting together lemmas \ref{lm:mincut} and \ref{lm:edges-removed}.
\end{proofof}

We overload notation here by denoting  for . The main result of this subsection is
\begin{corollary} \label{cor:unbalanced-ws}
Let  be a graph obtained by sampling the edges of a -regular bipartite graph  on  vertices independently with probability . There exists a constant  such that if  then whp for all pairs ,  one has that 
 for all . In particular,  contains a matching of size at least  whp.
\end{corollary}
\begin{proof}
Set , where  are the pieces of the decomposition obtained in Section \ref{subsec:decomp}. For each  such that  is not an isolated vertex we have by Lemma \ref{lm:mincut} and Theorem \ref{thm:ss}

If  is an isolated vertex, we have . Since the latter estimate is stronger than the former, we shall not consider the isolated vertices separately in what follows.

Adding these inequalities over all  we get

Denote the set of edges removed  during the decomposition process by . Denote  and . Since  and , this implies

Likewise, since , we have

Since , we have , so 

By similar arguments , i.e. . Hence, we have

which implies

for . This completes the proof.
\end{proof}

\begin{remark} \label{rmk:logd}
The result in corollary \ref{cor:unbalanced-ws} is tight up to an  factor for . 
\end{remark}
\begin{proof}
The following construction gives a lower bound of . Denote by  the graph from Theorem 4.1 in \cite{gkk:rbp08} and denote by  a graph obtained by sampling edges of  at the rate of  for a constant . Define the graph  as  disjoint copies of , and denote the sampled graph by . Note that by Theorem 4.1 the maximum matching in each copy of  has size at most  whp, and since the number of vertices in  is , the maximum matching in  has size at most  whp.
\end{proof}

\subsection{Runtime analysis of the Hopcroft-Karp algorithm}
In this section we derive a bound on the runtime of the Hopcroft-Karp algorithm on the subsampled graph obtained in step \textbf{S2} of our algorithm.
The main object of our analysis is the alternating level graph, which we now define. Given a partial matching of a graph , the alternating level graph is defined inductively. Define sets  and ,  as follows. Let  be the set of unmatched vertices in  and let . Then let , where   is the set of neighbours of vertices in , and let  be the set of vertices matched to vertices from . The construction terminates when either  contains an unmatched vertex or when , and then we set . We use the notation . 
We now give an outline of the Hopcroft-Karp algorithm for convenience of the reader. Given a non-maximum matching, the algorithm starts by constructing the alternating level graph described above and stops when an unmatched vertex is found. Then the algorithm finds a maximal set of vertex-disjoint augmenting paths of length  (this can be done by depth-first search in  time) and performs the augmentations, thus completing one augmentation phase. It can be shown that each augmentation phase increases the length of the shortest augmenting path. Standard analysis of the run-time for general bipartite graphs is based on the observation that once  augmentations have been performed, the constructed matching necessarily has size at most  smaller than the maximum matching.  

We denote the graph obtained by sampling edges of  independently with probability  for a constant  by . Note that  is obtained from  by \textit{uniform} sampling. We will make the connection to non-uniform sampling in Theorem \ref{thm:runtime}. For  denote the set of edges in the cut  in  by  and the set of edges in the same cut in  by .
Similarly, we denote the vertex neighbourhood of  in  by  and the vertex neighbourhood in  by .
 We consider the alternating level graph in  and prove that whp for any partial matching of size smaller than  for
each  either  or  expands by at least a factor of  in
either forward or backward direction ( or ). This implies that , thus yielding the same bound on the length of
the shortest augmenting path by virtue of corollary \ref{cor:unbalanced-ws}. The main technical result of this subsection is

\begin{lemma} \label{lm:main} Let the graph  be obtained from the
  bipartite -regular graph  on  vertices by uniform sampling with
  probability . There exist constants  such that if , then whp for any partial matching in  of size
  smaller than  there exists an augmenting path of length
  .
\end{lemma}


The following expansion property of the graph  will be used to prove lemma \ref{lm:main}:
\begin{lemma} \label{lm:expansion}
Define . For all  there exists a constant  that depends on  and  such that if  is obtained by sampling the edges of  independently with probability , then whp for every set ,  (resp. , ) 

\end{lemma}
\begin{proof}
Consider a set , . For  denote the indicator variable corresponding to the event that at least one edge incident on  and going to  is sampled by , i.e. . Denote the number of edges between  and vertices of  by . We have 

since  and  for .

Hence, 


There are at most  subsets  of  of size  and  for all , so we obtain using Chernoff bounds and the union bound

which can be made  by choosing  for any .
\end{proof}

\begin{proofof}{Lemma \ref{lm:main}}
First note that since the partial matching is of size strictly less than , by Corollary \ref{cor:unbalanced-ws} there exists an augmenting path with
 respect to the partial matching.

In order to upperbound the length of the shortest augmenting path, we will show that for each , at least one of the following is true:  

\begin{enumerate}
\item ;
\item ;
\item ;
\item ; 
\item .
\end{enumerate}

It then follows that for each   there exists  such that  and . Hence, there cannot be more than  levels in the alternating level graph, so there always exists an augmenting path of length .

For each , where  is the number of levels in the alternating level graph, we classify the edges leaving  into three classes:
(1)  contains edges that go to , (2)  contains edges that go to , and (3)  contains edges that go to .
At least one of  has at least  edges by Lemma \ref{lm:expansion}. We now consider each of these possibilities.

\textbf{Case (A):}
First suppose that  contains at least
  edges. Note that since the partial matching has size smaller than  by assumption, we have that . Hence, by Corollary \ref{cor:unbalanced-ws} the number of edges going from  to  is at least 


Suppose first that . Then by Lemma \ref{lm:expansion} one has that . Let . Observe that since one edge going out of  yields at most one neighbor, at most  neighbours of vertices of  are outside .  Setting , we get that  contains at least  neighbours of , i.e.  (this corresponds to case 3 above). Now if , one can find  such that  and at least  edges going out of  go to , which implies by the same argument that  (this corresponds to case 2 above).

\textbf{Case (B):} Suppose that  contains at least  edges. Then by the same argument as in the previous paragraph (after first weakening our estimate to ) we have that  if . This is impossible when  since . Hence,  by same argument as above, and hence  (this corresponds to case 1 above). 

\textbf{Case (C):} Suppose that  contains at least  edges.  By the same argument as above we have that either  (this corresponds to case 5 above) or  (this corresponds to case 4 above).

This completes the proof.
\end{proofof}

We can now prove the main result of this section:
\begin{theorem}\label{thm:runtime}
  Let the graph  be obtained from  using steps \textbf{S1} and
  \textbf{S2} in the algorithm of section \ref{sec:algo}. Then step
  \textbf{S3} takes  time
  whp, giving a time of  for the entire
  algorithm.
\end{theorem}
\begin{proof}
We analyze the runtime of step \textbf{S3} in two stages: (1) finding a matching of size , and (2) extending the matching of size  to a perfect matching.

Note that the strength of edges in  obtained after \textbf{S1} does not exceed , the maximum degree, with high probability, for a constant . Hence, the combination of sampling uniformly in \textbf{S1} and non-uniformly in \textbf{S2} dominates sampling each edge with probability , so we write , where  is obtained from  by sampling uniformly with probability  for a sufficiently large . The constant  can be made sufficiently large so that Lemma \ref{lm:main} applies by adjusting the constant in the sampling in steps \textbf{S1} and \textbf{S2}. Denote  and note that the proof of Lemma \ref{lm:main} only uses lower bounds on the number of edges incident to vertices in a given set, as well as the number of vertex neighbours of a set of vertices. Hence, since all bounds apply to , the conclusion of the lemma is valid for  whp as well, and we conclude that the maximum number of layers in  an alternating level graph, and hence the length of the shortest augmenting path, is . As each augmentation phase takes time proportional to the number of edges in the graph, this implies that the first stage takes .

Finally, note that each augmentation phase increases the size of the matching by at least , and thus  augmentation suffice to extend the matching constructed in the first stage to a perfect matching. This takes  time, so the runtime is  for step \textbf{S3}, and  overall.
\end{proof}

\begin{remark}
Theorem \ref{thm:runtime} as well as lemma \ref{lm:main} can be slightly altered to show that the runtime of the Hopcroft-Karp algorithm on the subsampled graph from \cite{gkk:rbp08} is . This shows that the approach in \cite{gkk:rbp08} yields an  algorithm, which is better than  stated in \cite{gkk:rbp08}.
\end{remark}

\begin{theorem}\label{thm:lowerbound}
For any function  there exists an infinite family of -regular graphs with  vertices such that whp the algorithm in section \ref{sec:algo} performs  augmentations in the worst case. 
\end{theorem}
\begin{proof}
 In what follows we omit the dependence of  on  for brevity. Define , , to be a -regular bipartite graph with .  The graph  consists of  copies of , which we denote by , where , and  vertices  and . Each of  is connected to all  vertices in the -part of , and 
 for  , the vertex   is connected to all vertices in the -part of . The remaining connections are established by adding  edge-disjoint perfect matchings between the  part of  and the  part of  for all .
 
 Set . Note that the strength of edges in  is at least , so whp there exists a perfect matching in subgraph of  generated 
 by the sampling steps \textbf{S1} and \textbf{S2}, for . Suppose that at the first iteration of the Hopcroft-Karp algorithm a perfect matching is found in each , thus leaving unmatched the vertices   and .
 Then from this point on, the shortest augmenting path for each pair pair  has length , and each augmentation phase of the Hopcroft-Karp algorithm
 will increase the size of the matching by . Hence, it takes  augmentations to find a perfect matching. The number of vertices is . 
\end{proof}




\section{Perfect Matchings in Doubly Stochastic Matrices}
\label{sec:bvn}
An  matrix  is said to be {\em doubly stochastic} if every element is
non-negative, and every row-sum and every column-sum is 1. The celebrated
Birkhoff-von Neumann theorem says that every doubly stochastic matrix is a
convex combination of permutation matrices ({\em i.e.}, matchings). Surprisingly, the
running time of computing this convex combination (known as a Birkhoff-von
Neumann decomposition) is typically reported as , even though
much better algorithms can be easily obtained using existing techniques or
very simple modifications. We list these running times here since there does
not seem to be any published record\footnote{This list was compiled by
  Bhattacharjee and Goel and is presented here to provide some context rather
  than as original work.}. After listing the running times that can be obtained
using existing techniques, we will show how proportionate uncrossings can be
applied to this problem to obtain a slight improvement.

\begin{enumerate}
\item An -time algorithm for finding a Birkhoff-von Neumann
  decomposition can be obtained by finding a perfect matching in the existing
  graph using augmenting paths (in time ), assigning this matching a
  weight which is the weight of the smallest edge in the matching, subtracting
  this weight from every edge in the matching (causing one or more edges to be
  removed from the support of ), and continuing the augmenting path
  algorithm without restarting. When a matching is found, if we remove 
  edges then we need to find only  augmenting paths (finding each
  augmenting path takes time ) to find another matching, which leads to
  a total time of .
\item Let  be the maximum number of significant bits in any entry of
  . An -time algorithm for finding a single perfect matching in the
  support of a doubly stochastic matrix can be easily obtained using the
  technique of Gabow and Kariv~\cite{gk:edge1982}: repeatedly find Euler tours
  in edges where the lowest order bit (say bit ) is 1, and then increase
  the weight of all edges going from left to right by  and decrease
  the weight of all edges going from right to left by the same amount, where
  the directionality of edges corresponds to an arbitrary orientation of the
  Euler tour; this eliminates bit  while preserving the doubly stochastic
  property and without increasing the support.
\item An -time algorithm to compute the Birkhoff-von Neumann
  decomposition can be obtained using the edge coloring algorithm of Gabow and
  Kariv~\cite{gk:edge1982}.
\end{enumerate}

We now show how our techniques lead to an -time
algorithm for finding a single perfect matching in the support of a doubly
stochastic matrix. In realistic scenarios, this is unlikely to be better than
 above, and we present this primarily to illustrate another application
of our proportionate uncrossing technique. First, define a weighted bipartite
graph , where  corresponds to rows of
,  corresponds to columns of , and
 iff . Define a weight function  on edges,
with . Let  be the collection of all pairs
. Since  is doubly
stochastic, the collection  is -thick with respect to . Let  be a -uncrossing of . Performing a
Bencz\'{u}r-Karger sampling on  will guarantee (with high probability) that at
least one edge is sampled from every witness set in , and hence
running the Hopcroft-Karp algorithm on the sampled graph will yield a perfect
matching with high probability. The running time of  is just the sum of the running times of Bencz\'{u}r-Karger sampling for
weighted graphs~\cite{benczurkarger96} and the Hopcroft-Karp matching
algorithm~\cite{hk:match73}.

\newpage
\subsection*{Acknowledgments:} We would like to thank Rajat Bhattacharjee for many useful discussions on precursors to this work, and Michel Goemans for suggesting an alternate proof mentioned in remark \ref{rm:other-approach}.

\pdfbookmark[1]{\refname}{My\refname} 
\begin{thebibliography}{10}

\bibitem{amsz:color2003}
G.~Aggarwal, R.~Motwani, D.~Shah, and A.~Zhu.
\newblock Switch scheduling via randomized edge coloring.
\newblock {\em FOCS}, 2003.

\bibitem{benczurkarger96}
Andr{\'a}s~A. Bencz{\'u}r and David~R. Karger.
\newblock Approximating {\it s-t} minimum cuts in  time.
\newblock {\em Proceedings of the 28th annual ACM symposium on Theory of
  computing}, pages 47--55, 1996.

\bibitem{b:bvn46}
G.~Birkhoff.
\newblock Tres observaciones sobre el algebra lineal.
\newblock {\em Univ. Nac. Tucum\'{a}n Rev. Ser. A}, 5:147--151, 1946.

\bibitem{b:graphtheory}
B.~Bollobas.
\newblock {\em Modern graph theory}.
\newblock Springer, 1998.

\bibitem{ch:color82}
R.~Cole and J.E. Hopcroft.
\newblock On edge coloring bipartite graphs.
\newblock {\em SIAM J. Comput.}, 11(3):540--546, 1982.

\bibitem{cos:regular2001}
R.~Cole, K.~Ost, and S.~Schirra.
\newblock Edge-coloring bipartite multigraphs in {} time.
\newblock {\em Combinatorica}, 21(1):5--12, 2001.

\bibitem{gk:edge1982}
H.N. Gabow and O.~Kariv.
\newblock Algorithms for edge coloring bipartite graphs and multigraphs.
\newblock {\em SIAM J. Comput.}, 11(1):117--129, 1982.

\bibitem{gkk:rbp08}
A.~Goel, M.~Kapralov, and S.~Khanna.
\newblock Perfect matchings via uniform sampling in regular bipartite graphs.
\newblock {\em Proceedings of the Nineteenth Annual ACM -SIAM Symposium on
  Discrete Algorithms}, 2009.

\bibitem{hk:match73}
J.E. Hopcroft and R.M. Karp.
\newblock An  algorithm for maximum matchings in bipartite
  graphs.
\newblock {\em SIAM J. Comput.}, 2(4):225--231, 1973.

\bibitem{kar94a}
D.~Karger.
\newblock Random sampling in cut, flow, and network design problems.
\newblock {\em Mathematics of Operations Research (Preliminary version appeared
  in the Proceedings of the 26th annual ACM symposium on Theory of computing)},
  24(2):383--413, 1999.

\bibitem{karger-levine}
D.~Karger and M.~Levine.
\newblock Random sampling in residual graphs.
\newblock {\em Proceedings of the thiry-fourth annual ACM symposium on Theory
  of computing}, 2002.

\bibitem{kargerstein96}
David~R. Karger and Clifford Stein.
\newblock A new approach to the minimum cut problem.
\newblock {\em J. ACM}, 43(4):601--640, 1996.

\bibitem{k:regular16}
D.~K\"{o}nig.
\newblock Uber graphen und ihre anwendung auf determinententheorie und
  mengenlehre.
\newblock {\em Math. Annalen}, 77:453--465, 1916.

\bibitem{motwani}
R.~Motwani.
\newblock Average-case analysis of algorithms for matchings and related
  problems.
\newblock {\em Journal of the ACM(JACM)}, 41:1329--1356, 1994.

\bibitem{mr:random}
R.~Motwani and P.~Raghavan.
\newblock {\em Randomized Algorithms}.
\newblock Cambridge University Press, 1995.

\bibitem{s:color99}
A.~Schrijver.
\newblock Bipartite edge coloring in {} time.
\newblock {\em SIAM J. on Comput.}, 28:841--846, 1999.

\bibitem{vn:bvn53}
J.~von Neumann.
\newblock A certain zero-sum two-person game equivalent to the optimal
  assignment problem.
\newblock {\em Contributions to the optimal assignment problem to the Theory of
  Games}, 2:5--12, 1953.

\end{thebibliography}

\appendix

\section{Proof of Lemma~\ref{lem:relevant-only}}
\label{append:relevant-only}
Consider any  where .  Define
 and . Fix an  such that ; such an  is guaranteed to exist. By the definition of relevance,
there exists a pair  such that , and . By the assumption in the theorem,
there exists an edge . Since , it follows that . This edge is in , and goes from  to , {\em i.e.}, from  to , and hence, from  to
. Since the only assumption on  was that , we
can now invoke Hall's theorem to claim that  has a perfect
matching.


\section{Proof of Theorem~\ref{thm:Benczur-Karger}}
\label{append:bk}
As mentioned before, the proof is along very similar lines to that of the
Bencz\'{u}r-Karger sampling theorem, but does not follow in a black-box fashion
and is presented here for completeness. The proof relies on the following
result due to Karger and Stein~\cite{kargerstein96}:
\begin{lemma}
\label{lem:karger}
Let  be an undirected graph on  vertices such that each edge  has an associated
non-negative weight . Let  be the value of minimum cut in 
under the weight function . Then for any , the number of cuts in 
of weight at most  is less than .
\end{lemma}
\begin{proofof}{Theorem~\ref{thm:Benczur-Karger}}
  We will choose . The first part of the proof shows that it is
  sufficient to bound a certain expression that involves only cuts. The second
  part then bounds this expression.

  For the first part, let  denote the expected
  number of edges chosen from  by the sampling process. If a set  contains an edge  with , then that edge will
  definitely be chosen, and that set does not contribute to  and can be
  removed from . Hence, assume without loss of generality that  for every edge in . Define . Now for any set ,

  

  where
   Since  is
  a one-one function, it is sufficient to provide an upper-bound on
  .

  For the second part, let  be a non-decreasing sorted
  sequence corresponding to the multi-set . Define . Consider an arbitrary cut
  . Any edge in  can have strength at most , and hence
  , and therefore, . So the sum
  of  for the first  cuts in the sequence is bounded by
  . We now focus on the remaining cuts.  By Lemma~\ref{lem:karger},
  we know that for any , we have .  Hence
  
  which in turn implies that . Thus

  
  giving us the desired result when we choose .
\end{proofof}

\section{Proof of Lemma~\ref{lem:strength}}
\label{append:strength}
Assume by way of contradiction that no such integer  exists for some pair
of multisets  and .  Let  be the largest integer in , and let  and  denote the number of occurrences of 
in the multisets  and  respectively.  Then for all , we
have  Summing the above inequality for all , we get  which is a contradiction since  by assumption.

\end{document} 
