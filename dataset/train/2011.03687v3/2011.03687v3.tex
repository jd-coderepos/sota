\section{Experiments}\label{sec:exp}
In this section, we validate our analysis of  measures' robustness via a set of empirical evaluations on 5 datasets: MNIST (\cite{lecun1998gradient}), Fashion-MNIST (\cite{xiao2017fashionmnist}), CIFAR-10 and CIFAR-100 (\cite{krizhevsky2009learning}), and Clothing1M (\cite{xiao2015learning}). Omitted experiment details are available in the appendix.

\vspace{-0.1in}
\paragraph{Baselines} We compare our approach with five baseline methods: \textbf{Cross-Entropy (CE)}, \textbf{Backward (BLC) and Forward Loss Correction (FLC)} methods as introduced in \citep{patrini2017making}, the \textbf{determinant-based mutual information (DMI)} method introduced in \citep{xu2019l_dmi} and \textbf{Peer-Loss (PL)} functions in \citep{liu2019peer}. BLC and FLC methods require estimating the noise transition matrix. DMI  and  PL are approaches that do not require such estimation.

\vspace{-0.1in}
\paragraph{Noise model} We test three types of noise transition models: uniform noise, sparse noise, and random noise. All details of the noise are in the Appendix. Here we briefly overview them. The uniform and sparse noise are as specified at the end of Section 3 for which our theoretical analyses mainly focus on. The noise rates of low-level uniform noise and sparse noise are both approximately 0.2 (the average probability of a label being wrong). 
The high-levels are about 0.55 and 0.4 respectively. 
In the random noise setting, each class randomly flips to one of 10 classes with probability  (Random ). For CIFAR-100, the noise rate of uniform noise is about 0.25. The sparse label noise is generated by randomly dividing 100 classes into 50 pairs, and the noise rate is about 0.4. 

\vspace{-0.02in}
\paragraph{Optimizing  using noisy samples}
With the noisy training dataset , we optimize  using gradient ascent of its variational form. Sketch is given in Algorithm \ref{alg:main1}. For the bias correction version of our algorithm, the gradient will simply include the . The variational function  can be updated progressively or can be fixed beforehand using an approximate activation function for each  (see e.g., \citep{nowozin2016f}). 

\begin{algorithm}
\caption{Maximizing  measures: one step gradient}\label{alg:main1}
\begin{algorithmic}[1]
\STATE \textbf{Inputs}: Training data , , variational function , conjugate , classifier . 
\STATE Randomly sample three mini-batches , ,  from . : simulate samples ;   
 to simulate . 
\STATE Use  to denote model prediction on  for label ,   to denote the empirical sample mean calculated using the mini-batch data.
\STATE At step , update  by ascending its stochastic gradient with learning rate : 
 
Tips: In practice, we suggest (also implemented in our experiments) using the fixed form of  which appears as  in Table \ref{table:f_div-full} (appendix).
\end{algorithmic}
\end{algorithm}


\subsection{How good is  on clean data}
As a supplementary of Section \ref{sec:hf}, we validate the quality of  on clean dataset of MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100. In experiments, since the estimation of product noisy distribution are unstable when trained on CIFAR-100 training dataset, we use CE as a warm-up (120 epochs) and then switch to train with  measures. For other datasets, we train with  measures without the warm-up stage. Results in Table \ref{Tab:hg} demonstrate that optimizing divergence on clean dataset returns a high-quality  by referring to the performance of CE. Even though  measures can't outperform CE on clean dataset, we do observe that the gap between CE and  measures are negligible, for example, the largest gap of Total-Variation (TV) is only  among four datasets.  


\begin{table*}[!ht]
\scriptsize
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
Dataset &  CE &  \textbf{TV} & Gap & \textbf{J-S} & Gap & \textbf{KL} & Gap \\ \hline\hline
\multirow{1}{*}{MNIST}
& 99.39(99.380.01) & 99.37(99.340.02) &{\color{blue}\textbf{-0.04}}  & 99.35(99.310.04) & {\color{blue}\textbf{-0.07}} & 99.31(99.210.06) & {\color{blue}\textbf{-0.17}}
\\ 
\hline
\multirow{1}{*}{Fashion MNIST}
& 90.44(90.340.12) & 89.98(89.940.06) & {\color{blue}\textbf{-0.40 }}& 90.40(90.170.24)  & {\color{blue}\textbf{-0.17}} & 90.19(89.960.14) & {\color{blue}\textbf{-0.38}}
\\ 
\hline
\multirow{1}{*}{CIFAR-10}
& 93.58(93.470.08) & 92.80(92.660.13) &{\color{blue}\textbf{-0.81}} & 92.35(92.230.07) & -1.24 & 90.55(90.380.15) & -3.09
\\ 
\hline
\multirow{1}{*}{CIFAR-100}
& 73.47(73.390.05) & 73.43(73.390.06) & {\color{blue}\textbf{0.00}} & 73.47(73.260.17)& {\color{blue}\textbf{-0.13}} & 73.33(73.160.10) & {\color{blue}\textbf{-0.23}}
\\ 
\hline
\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison on clean datasets: We report the maximum accuracy of CE and each  measures along with (mean  standard deviation); Gap: mean performance comparison w.r.t. CE. Numbers highlighted in \color{blue}\textbf{{blue}} \color{black} indicate the gap is less than 1\%.
}
\label{Tab:hg}
\end{table*}

\subsection{Robustness of  measures}
\begin{figure}[ht]
\vspace{-0.2in}
    \centering
    {\includegraphics[width=.6\textwidth]{figures/CIFAR_fig1.pdf}
    }
        \vspace{-5pt}
        \caption{Robustness of TV, JS, PS divergences. 
        \vspace{-15pt}
    }
    \label{fig: robust_f_div}
\end{figure}
As a demonstration, we apply the uniform noise model to CIFAR-10 dataset to test the robustness of three  measures: Total-Variation (TV), Jensen-Shannon (JS) and Pearson (PS). We trained models with  measures using Algorithm \ref{alg:main1} on 10 noise settings with an increasing noise rate from 0\% to approximately 81\%. The visualization of the  values and accuracy w.r.t. noise rates are shown in Figure \ref{fig: robust_f_div}. Both the  values and test accuracy are calculated on the reserved clean test data. We observe that almost all  measures are robust to noisy labels, especially when the percentage of noisy labels is not overwhelmingly large, e.g., . Note that the curves for other -divergences are almost the same as the curve of total variation (TV), which is proved to be robust theoretically. This partially validates the analytical evidences we provided for the robustness of other -divergences in Section \ref{sec:bias} and \ref{sec:robust}. 



\subsection{Performance Evaluation and Comparison}

From Table \ref{Tab:Experiment_Results_no_bias}, several  measures arise as competitive solutions in a variety of noise scenarios. Among the proposed -divergences, Total Variation (TV) has been consistently ranked as one of the top performing method. This aligns also with our analyses that TV is inherently robust. For most settings, the presented -divergences outperformed the baselines we compare to, while they fell short to DMI (once) and Peer Loss (5 times) on several cases, particularly when the noise is sparse and high. 
The sparse high noise setting tends to be a challenging setting for all methods. We conjecture this is because sparse high noise setting creates a highly imbalanced dataset, model training is more likely to converge to a ``sub-optimal" early in the training process. It is also possible that with sparse noise, the impact of  terms becomes non-negligible.  We do observe better performances with very careful and intensive hyper-parameter tuning, but the results are not confident and we chose to not report it. Fully understanding the limitation of our approach in this setting remains an interesting on-going investigation. 


In Table \ref{Tab:Experiment_Results_bias} (full details on MNIST and Fashion MNIST can be found in Appendix), we use noise transition estimation method in (\cite{patrini2017making}) to estimate the noise rate. The estimates help us define the bias term and perform bias correction for  measures. We observe that while adding bias correction can further improve the performance of several divergence functions (Gap being positive), the improvement or difference is not significant. This partially justified our analysis of the bias term, especially when the noise is dense and high (uniform and random high).

\begin{table*}[!ht]
\centering
\tiny
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
Dataset & Noise & CE & BLC & FLC & DMI& PL & \textbf{TV} & \textbf{J-S} & \textbf{KL} \\ \hline\hline

\multirow{5}{*}{MNIST}             
& Sparse, Low   & 97.21  & 95.23 & 97.37 & 97.76  & 98.59 & {\color{blue}\textbf{99.23(99.110.08)}} & {\color{blue}\textbf{99.15(99.030.09)}} & {\color{blue}\textbf{99.21(99.150.05)}}  \\ \cline{2-10} 
& Sparse, High  & 48.55 & 55.86 & 49.67 &  49.61 & {\color{blue}\textbf{60.27}} & 58.27(54.724.36) & 58.93(55.801.93) & 49.24(49.170.06)  \\ \cline{2-10} 
& Uniform, Low  & 97.14 & 94.27 & 95.51 & 97.72  & 99.06 & {\color{blue}\textbf{99.23(99.170.05)}} & {\color{blue}\textbf{99.1(99.080.04)}}& {\color{blue}\textbf{99.13(99.060.07)}}    \\ \cline{2-10} 
& Uniform, High & 93.25 & 85.92 & 87.75 & 95.50  & 97.77& {\color{blue}\textbf{98.09(97.960.13)}} & {\color{black}\text{97.86(97.710.10)}} &{\color{blue}\textbf{98.14(97.880.18)}} \\\cline{2-10}  
& Random (0.2) & 98.26 & 97.46 & 97.61 & 98.82  & 99.25 & 99.26(99.190.05) & {\color{blue}\textbf{99.29(99.270.02)}} & 99.26(99.190.06) \\ \cline{2-10} 
& Random (0.7) & 97.00 & 93.52 & 87.74 & 95.47  & 98.52 & {\color{blue}\textbf{98.81(98.730.06)}} & {\color{blue}\textbf{98.72(98.630.08)}} & {\color{blue}\textbf{98.76(98.650.10)}} \\ \cline{2-10} 
\hline\hline

\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Fashion\\ MNIST\end{tabular}} 
& Sparse, Low   & 84.36 & 86.02 & 88.15 & 85.65  & 88.32 & {\color{blue}\textbf{89.74(89.340.33)}} & {\color{blue}\textbf{88.80(88.790.01)}} &  {\color{blue}\textbf{89.77(89.420.34)}}\\ \cline{2-10} 
& Sparse, High  & 43.33 & 46.97 & 47.63 & 47.16 & {\color{blue}\textbf{51.92}} & 45.66(45.220.26) & 47.46(46.390.70) & 38.96(38.900.06)\\ \cline{2-10} 
& Uniform, Low  & 82.98 & 84.48 & 86.58 &  83.69  & {\color{blue}\textbf{89.31}} & 89.00(88.750.16) & 88.58(88.460.18) &   88.32(88.160.11)  \\ \cline{2-10} 
& Uniform, High & 79.52 & 78.10 & 82.41 & 77.94 & 84.69 & {\color{blue}\textbf{85.58(85.070.31)}} & {\color{blue}\textbf{85.62(85.39 0.33)}} &  {\color{blue}\textbf{85.69(85.430.30)}} \\\cline{2-10}  
  
& Random (0.2) & 85.47 & 83.40 & 77.61 & 86.21  & 89.78 & {\color{blue}\textbf{90.22(90.090.19)}} & 89.73(89.430.24) & 89.24(89.050.14) \\ \cline{2-10} 
& Random (0.7) & 82.05 & 78.41 & 73.42 & 80.89  & 87.22 & 86.69(86.490.16) & {\color{blue}\textbf{87.79(87.330.29)}} & 87.06(87.000.06) \\ \cline{2-10} 


\hline\hline

\multirow{5}{*}{CIFAR-10}             
& Sparse, Low   & 87.20    & 72.96    & 76.17   & {\color{blue}\textbf{92.32}}  &   91.35   &  91.81(91.560.16) & 91.49 (91.430.08)  &   91.62(91.320.31) \\ \cline{2-10} 
& Sparse, High  & 61.81  & 56.30    & 66.12   & 27.94 & {\color{blue} \textbf{69.70}}        & 63.96(62.251.00) & 67.33(65.271.34) & 46.55(46.430.08)  \\\cline{2-10}
& Uniform, Low  & 85.68 & 72.73 & 77.12 & 90.39 & 91.70 & {\color{blue}\textbf{92.10(92.010.09)}} & 91.52(91.470.08) & {\color{blue}\textbf{92.26(92.080.12)}}   \\ \cline{2-10} 
& Uniform, High & 71.38  & 54.41 & 64.22 & 82.68 & 83.42 & {\color{blue}\textbf{85.56(85.440.08)}}  & {\color{blue}\textbf{84.49(84.350.13)}} &  {\color{blue}\textbf{84.36(84.190.13)}}  \\ \cline{2-10}  
& Random (0.5) & 78.40 & 59.31 & 68.97 & 85.06 & 86.47  & {\color{blue}\textbf{87.28(87.030.17)}} & {\color{blue}\textbf{86.92 (86.800.10)}}  &   {\color{blue}\textbf{86.93(86.850.11)}}  \\ 
\cline{2-10}  
& Random (0.7) & 68.26 & 38.59  &  54.39 &  77.91  & 57.81 &  {\color{blue}\textbf{80.59(80.450.10)}} & {\color{blue}\textbf{80.50(80.270.15)}} &  {\color{blue}\textbf{78.93(78.590.30)}}   \\ \hline\hline
\multirow{5}{*}{CIFAR-100}
& Uniform & 63.87 &  51.40& 60.04 & 64.39  & 67.94 & {\color{blue}\textbf{69.15(68.900.17)}} & {\color{blue}\textbf{69.13(68.800.21)}} &   {\color{blue}\textbf{68.79(68.600.11)}}\\ \cline{2-10}
& Sparse & 40.45  & 36.57 & 43.39 & 40.53  & {\color{blue}\textbf{44.25}} & 42.45(38.062.82) & 38.09(38.000.08) & 37.74(37.630.08) \\ \cline{2-10}
& Random (0.2) & 65.84  & 61.21 & 61.52 & 66.23  & 62.92 & {\color{blue}\textbf{70.43(70.220.13)}} & {\color{blue}\textbf{70.40(70.120.21)}} &  {\color{blue}\textbf{70.28(70.060.14)}}  \\ \cline{2-10}
& Random (0.5) & 56.92  & 22.21 & 55.88 & 56.06  & 49.62  & {\color{blue}\textbf{62.14(61.890.18)}} & {\color{blue}\textbf{61.58(61.150.27)}} &   {\color{blue}\textbf{61.68(61.490.13)}}  \\ \hline

\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison (w/o bias correction): The best performance in each setting is highlighted in {\color{blue}\textbf{{blue}}}. \color{black}We report the maximum accuracy of each  measures along with (mean  standard deviation). All -divergences will be highlighted if their mean performances are better (or no worse) than all baselines we compare to. A supplementary table including Pearson  and Jeffrey (JF) is attached in Table \ref{Tab:Experiment_Results_no_bias_full} (Appendix).
}
\label{Tab:Experiment_Results_no_bias}
\end{table*}


\begin{table*}[!ht]
\tiny
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Noise &  \textbf{J-S} & Gap & \textbf{PS} & Gap & \textbf{KL} & Gap & \textbf{JF} & Gap \\ \hline\hline
Sparse, Low   & 91.23(90.930.34)  & -0.26& {\color{black} \text{91.48(91.120.42)}} & {\color{red}\textbf{+0.08}}&  {\color{black} \text{91.73(91.570.18)}}& {\color{red}\textbf{+0.11}}& {\color{black} \text{91.45(91.180.21)}}  & -0.10\\ \cline{1-9} 
Sparse, High   & 46.45(46.310.14) &-20.88 & 46.31(45.900.44) & -0.05 & 46.59(46.520.05) & {\color{red}\textbf{+0.04}} & 46.25(45.770.50) & {\color{red}\textbf{+0.04}}\\ \cline{1-9}
Uniform, Low    & {\color{blue} \textbf{92.16(92.090.09)}} & {\color{red}\textbf{+0.64}}& {\color{blue} \textbf{92.25(92.130.09)}} & -0.12&  {\color{black} \text{90.92(90.840.10)}} & -1.34& {\color{blue} \textbf{92.19(92.100.08)}} & {\color{red}\textbf{+0.02}}\\ \cline{1-9}
Uniform, High   & {\color{blue} \textbf{84.31(84.130.10)}} & -0.18& {\color{blue} \textbf{83.79(83.610.12)}}  & {\color{red}\textbf{+0.18}}& {\color{blue} \textbf{83.98(83.790.12)}} & -0.38& {\color{blue} \textbf{83.93(83.620.22)}} & {\color{red}\textbf{+0.13}}\\ \cline{1-9}
 \hline
\end{tabular}
\end{threeparttable}
\caption{ measures with bias correction on CIFAR-10: Numbers highlighted in \color{blue}\textbf{{blue}} \color{black} indicate better than all baseline methods; Gap: relative performance w.r.t. their version w/o bias correction (Table \ref{Tab:Experiment_Results_no_bias}); those in \color{red}\textbf{{red}} \color{black} indicate better than w/o bias correction.  
}
\label{Tab:Experiment_Results_bias}
\end{table*}



\paragraph{Clothing1M}
Clothing1M is a large-scale clothes dataset with comprehensive annotations and can be categorized as a feature-dependent human-level noise dataset. Although this noise setting does not exactly follow our assumption, we are interested in testing the robustness of our -divergence approaches. Experiment results in Table \ref{Tab:Experiment_Results_C1M} demonstrate the robustness of the  measures. TV and KL divergences have outperformed other baseline methods.


\begin{table*}[!ht]
\scriptsize
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Dataset & Noise & CE & BLC & FLC & DMI& PL & T-V & J-S & Pear  & KL & Jeffrey \\ \hline\hline
\multirow{1}{*}{Clothing1M}
& Human Noise & 68.94 & 69.13 & 69.84 & 72.46  & 72.60  & {\color{blue}\textbf{73.09}} & 72.32 &  72.22 & {\color{blue}\textbf{72.65}}  & 72.46
\\ \cline{2-12}
\hline
\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison on Clothing1M dataset.
}
\label{Tab:Experiment_Results_C1M}
\end{table*}

