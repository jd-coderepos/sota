

\documentclass[sigconf]{acmart}


\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs} 
\usepackage{tabularx}
\usepackage{subfigure}

\settopmatter{printacmref=true}


\fancyhead{}


\usepackage{balance}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    


\copyrightyear{2019} 
\acmYear{2019} 
\acmConference[CIKM '19]{The 28th ACM International Conference on Information and Knowledge Management}{November 3--7, 2019}{Beijing, China}
\acmBooktitle{The 28th ACM International Conference on Information and Knowledge Management (CIKM '19), November 3--7, 2019, Beijing, China}
\acmPrice{15.00}
\acmDOI{10.1145/3357384.3357951}
\acmISBN{978-1-4503-6976-3/19/11}





\settopmatter{printacmref=true}
\begin{document}

\fancyhead{}


\title{Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction}




\author{Zekun Li}
\affiliation{Institute of Information Engineering, Chinese Academy of Sciences}
\affiliation{University of Chinese Academy of Sciences}
\email{lizekunlee@gmail.com}
\author{Zeyu Cui}
\affiliation{Institute of Automation, Chinese Academy of Sciences}
\affiliation{University of Chinese Academy of Sciences}
\email{zeyu.cui@nlpr.ia.ac.cn}
\author{Shu Wu}
\affiliation{Institute of Automation and \\Artificial Intelligence Research, Chinese Academy of Sciences}
\email{shu.wu@nlpr.ia.ac.cn}
\author{Xiaoyu Zhang}
\affiliation{Institute of Information Engineering, Chinese Academy of Sciences}
\email{zhangxiaoyu@iie.ac.cn}
\author{Liang Wang}
\affiliation{Institute of Automation, Chinese Academy of Sciences}
\affiliation{University of Chinese Academy of Sciences}
\email{wangliang@nlpr.ia.ac.cn}
\thanks{The first two authors Zekun Li and Zeyu Cui contribute to this work equally.
Shu Wu and Xiaoyu Zhang are both corresponding authors.}


\renewcommand{\shortauthors}{Li and Cui, et al.}

\begin{abstract}
Click-through rate (CTR) prediction is an essential task in web applications such as online advertising and recommender systems, whose features are usually in multi-field form.
The key of this task is to model feature interactions among different feature fields.
Recently proposed deep learning based models follow a general paradigm: raw sparse input multi-filed features are first mapped into dense field embedding vectors, and then simply concatenated together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. 
However, the simple \emph{unstructured combination} of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.

In this work, we propose to represent the multi-field features in a graph structure intuitively, where each node corresponds to a feature field and different fields can interact through edges.  
The task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph.
To this end, we design a novel model Feature Interaction Graph Neural Networks (Fi-GNN).
Taking advantage of the strong representative power of graphs, our proposed model can not only model sophisticated feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction.
Experimental results on two real-world datasets show its superiority over the state-of-the-arts.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003347.10011712</concept_id>
<concept_desc>Information systems~Business intelligence</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010405.10003550.10003555</concept_id>
<concept_desc>Applied computing~Online shopping</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[300]{Information systems~Business intelligence}
\ccsdesc[300]{Applied computing~Online shopping}



\keywords{Feature interactions, Graph neural networks, CTR prediction, Recommender system}

\maketitle

\section{Introduction}
The goal of click-through rate prediction is to predict the probabilities of users clicking ads or items, which is critical to many web applications such as online advertising and recommender systems.
Modeling sophisticated feature interactions plays a central role in the success of CTR prediction.
Distinct from continuous features which can be naturally found in images and audios, the features for web applications are mostly in multi-field categorical form.
For example, the four-fields categorical features for movies may be: (1) \textsf{Language = \{English, Chinese, Japanese, ... \}}, (2) \textsf{Genre = \{action, fiction, ... \}}, (3) \textsf{Director = \{Ang Lee, Christopher Nolan, ... \}}, and (4) \textsf{Starring = \{Bruce Lee, Leonardo DiCaprio, ... \}} (noted that there are much more feature fields in real applications).
These multi-field categorical features are usually converted to sparse one-hot encoding vectors, and then embedded to dense real-value vectors, which can be used to model feature interactions.           


Factorization machine (FM) \cite{rendle2010factorization} is a well-known model proposed to learn second-order feature interactions from vector inner products. 
Field-aware factorization machine (FFM) \cite{juan2016field} further considers the field information and introduces field-aware embedding.
Regrettably, these FM-based models can only model second-order interaction and the linearity modeling limits its representative power.      
Recently, many deep learning based models have been proposed to learn high-order feature interactions, which follow a general paradigm: simply concatenate the field embedding vectors together and feed them into DNN or other specifically designed models to learn interactions.
For example, Factorisation-machine supported Neural Networks (FNN) \cite{zhang2016deep}, Neural Factorization Machine (NFM) \cite{he2017neural}, Wide\&Deep \cite{cheng2016wide} and DeepFM \cite{guo2017deepfm} utilize DNN to model interactions.
However, these model based on DNN learn high-order feature interactions in a bit-wise, implicit fashion, which lacks good model explanations.
Some models try to learn high order interactions explicitly by introducing specifically designed networks.
For example, Deep\&Cross \cite{wang2017deep} introduces Cross Network (CrossNet) and xDeepFM \cite{lian2018xdeepfm} introduces Compressed Interaction Network (CIN).
Nevertheless, we argue that they are still not sufficiently effective and explicit, since they still follow the general paradigm of combining feature fields together to model their interactions.
The simple \emph{unstructured combination} will inevitably limit the capability to model sophisticated interactions among different feature fields in a flexible and explicit fashion.
 

In this work, we take the structure of multi-field features into consideration.
Specifically, we represent the multi-field features in a graph structure named \emph{feature graph}.
Intuitively, each node in the graph corresponds to a feature field and different fields can interact through edges.
The task of modeling sophisticated interactions among feature fields can be thus converted to modeling node interactions on the feature graph.
To this end, we design a novel model Feature interaction Graph Neural Networks (Fi-GNN) based on Graph Neural Networks (GNN), which is able to model sophisticated node (feature) interactions in a flexible and explicit fashion.
In Fi-GNN, the nodes will interact by communicating the node states with neighbors and update themselves in a recurrent fashion.
At every time step, the model interact with neighbors at one hop deeper.  
Therefore, the number of interaction steps equals to the order of feature interactions.
Moreover, the edge weights reflecting importances of different feature interactions and node weights reflecting importances of each feature field on the final CTR prediction can be learnt by Fi-GNN, which can provide good explanations. 
Overall, our proposed model can model sophisticated feature interactions in an explicit, flexible fashion and also provide good model explanations.









Our contributions can be summarized in threefold:
\begin{itemize}
\item 
We point out the limitation of the existing works which consider multi-field features as an unstructured combination of feature fields.
To this end, we propose to represent the multi-field features in a graph structure for the first time.  
\item We design a novel model Feature Interaction Graph Neural Networks (Fi-GNN) to model sophisticated interactions among feature fields on the graph-structured features in a more flexible and explicit fashion.
\item Extensive experiments on two real-world datasets show that our proposed method can not only outperform the state-of-the-arts but also provide good model explanations. 
\end{itemize}

The rest of this paper is organized as follows. 
Section 2 summarizes the related work.
Section 3 provides an elaborative description of our proposed method.
The extensive experiments and detailed analysis are presented in Section 4, followed by the conclusion in Section 5.


\section{Related Work}
In this section, we briefly review the existing models that model feature interactions for CTR prediction and graph neural networks.
\subsection{Feature Interaction in CTR Prediction} \label{sect:related}
Modeling feature interactions is the key to success of CTR prediction and therefore extensively studied in the literature.
LR is a linear approach, which can only models the first-order interaction on the linear combination of raw individual features.
FM \cite{rendle2010factorization} learns second-order feature interactions from vector inner products. 
Afterwards, different variants of FM have been proposed.
Field-aware factorization machine (FFM) \cite{juan2016field} considers the field information and introduces field-aware embedding.
AFM \cite{xiao2017attentional} considers the weight of different second-order feature interactions.
However, these approaches can only model second-order interaction which is not sufficient.


With the success of DNN in various fields, researchers start to use it to learn high-order feature interactions due to its deeper structures and nonlinear activation functions.
The general paradigm is to concatenate the field embedding vectors together and feed them into DNN to learn the high-order feature interactions.
\cite{liu2015convolutional} utilizes convolutional networks to model feature interactions. 
Factorisation-machine supported Neural Networks (FNNs) \cite{zhang2016deep} uses the pre-trained factorization machines for field embedding before applying DNN.
Product-based Neural Network (PNN) \cite{qu2016product} models both second-order and high-order interactions by introducing a product layer between field embedding layer and DNN layer.
Similarly, Neural Factorization Machine (NFM) \cite{he2017neural} has a Bi-Interaction Pooling layer between embedding layer and DNN layer to model second-order interactions, but the followed operation is summation instead of concatenation as in PNN. 
Some works on another line try to model the second-order and high-order interactions jointly via a hybrid architectures.
The Wide\&Deep \cite{cheng2016wide} and DeepFM \cite{guo2017deepfm} contain a wide part to model the low-order interaction and a deep part to model the high-order interaction.
However, all these approaches leveraging DNN learn the high-order feature interactions in an implicit, bit-wise way and therefore lack good model explainability. 
Recently, some work try to learn feature interactions in an explicit fashion via specifically designed networks. 
Deep\&Cross \cite{wang2017deep} introduces a CrossNet which takes outer product of features at the bit level.
On the contrary, xDeepFM \cite{lian2018xdeepfm} introduces a CIN to take outer product at the vector level.
Nevertheless, they still don't solve the most fundamental problem, that is to concatenate the field embedding vectors together.
The simple unstructured combination of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a flexible and explicit fashion.
To this end, we proposed to represent the multi-field features in a graph structure, where each node represents a field and different feature fields can interact through the edges.
Accordingly, we can model the flexible interactions among different feature fields on the graphs. 
 



\subsection{Graph Neural Networks}
Graph is a kind of data structure which models a set of objects (nodes) and their relationships (edges). 
Recently, researches of analyzing graphs with machine learning have been receiving more and more attention because of the great representative power of graphs.
Early works usually convert graph-structured data into sequence-structured data to deal with. 
Inspired by word2vec \cite{mikolov2013distributed},
the work \cite{perozzi2014deepwalk} proposed an unsupervised DeepWalk algorithm to learn node embedding in graph based on random walks. 
After that, \cite{tang2015line} proposed a network embedding algorithm LINE, which preserve the first- and second-order structural information.
\cite{Grover2016node2vec} proposed node2vec which introduces a biased random walk.
However, these methods can be computationally expensive and non-optimal for large graphs. 

Graph neural networks (GNN) are designed to tackle these problems, which are deep learning based methods that operate on the graph domain.
The concept of GNN is first proposed by \cite{scarselli2009graph}.
Generally, nodes in GNNs interact with neighbors by aggregating information from neighborhoods and updating their hidden states.
There have been many variants of GNN with various kinds of aggregators and updaters proposed these days.
Here we only present some representative and classical methods.
Gated Graph Neural Networks (GGNN) \cite{li2015gated} uses GRU \cite{cho2014learning} as updater.
Graph Convolutional Networks (GCN) \cite{kipf2016semi} considers the spectral structure of graphs and utilizes the convolutional aggregator.
GraphSAGE \cite{hamilton2017inductive} considers the spatial information. It introduces three kinds of aggregators:
mean aggregator, LSTM aggregator and Pooling aggregator.
Graph attention network (GAT) \cite{velivckovic2017graph} incorporates the attention mechanism into the propagation step.
There are some surveys \cite{wu2019comprehensive,zhou2018graph} which provide more elaborative introduction of various kinds of GNN models.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./pic/overview2.pdf}
\caption{
Overview of our proposed method.
The input raw multi-field feature vector is first converted to field embedding vectors via an embedding layer and represented as a feature graph, which is then feed into Fi-GNN to model feature interactions.
An attention layer is applied on the output of Fi-GNN to predict the click through rate .
Details of embedding layer and Fi-GNN are illustrated in Figure 2 and Figure 3 respectively.}
\vspace{-4mm}
\label{fig:overview}
\end{figure}

Due to convincing performance and high interpretability, GNN has been a widely applied graph analysis method.
Recently, there are many application of GNN like neural machine translation \cite{beck2018graph}, semantic segmentation \cite{qi20173d}, image classification \cite{marino2017more}, situation recognition \cite{li2017situation}, recommendation \cite{Wu2018Session}, script event prediction \cite{Zhongyang2018Constructing}, fashion analysis \cite{cui2019dressing,li2019semi}.
GNN is suitable for modeling node interactions on graph-structured features intrinsically.
In this work, we proposed a model Fi-GNN based on GGNN to model feature interactions on the graph-structured features for CTR prediction. 

\section{Our Proposed Method}
We first formulate the problem and then introduce the overview of our proposed method, followed by the elaborate detail of each component.
 
\subsection{Problem Formulation}
Suppose the training dataset consists of -fields categorical features ( is the number of feature fields) and the associated labels  which indicate user click behaviors.
The task of CTR prediction is to predict  for the input -fields features, which estimates the probability of a user clicking.
The key of the task is to model the sophisticated interactions among different feature fields.

\subsection{Overview}
Figure \ref{fig:overview} is the overview of our proposed method (=4).   
The input sparse -field feature vector is first mapped into sparse one-hot embedding vectors and then embedded to dense field embedding vectors via the embedding layer and the multi-head self-attention layer. 
The field embedding vectors are then represented as a feature graph, where each node corresponds to a feature field and different feature fields can interact through edges.
The task of modeling interaction can be thus converted to modeling node interactions on the feature graph.
Therefore, the feature graph is feed into our proposed Fi-GNN to model node interactions.  
An attention scoring layer is applied on the output of Fi-GNN to estimate the click-through rate . 
In the following, we will introduce the details of our proposed method.
    


\subsection{Embedding Layer} \label{sect:graph}
The multi-field categorical feature   is usually sparse and of huge dimension.
Following previous works \cite{zhang2016deep,qu2016product,wang2017deep,guo2017deepfm,qu2018product}, we represent each field as a one-hot encoding vector and then embed it to a dense vector, noted as field embedding vector.
Let us consider the example in Section 1, 
a movie \textsf{\{Language: English, Genre: fiction, Director: Christopher Nolan, Starring: Leonardo DiCaprio \}} is first transformed into a high-dimensional sparse features via one-hot encoding:
\begin{center}	

 \end{center}	
A field-aware embedding layer is then applied upon the one-hot vectors to embed them to low dimensional, dense real-value field embedding vectors as shown in Figure \ref{fig:embedding}.
Likewise, the field embedding vectors of -field feature can be obtained: 
\begin{center}	

\end{center}
where  denotes the embedding vector of field  and  denotes the dimension of field embedding vectors.

\subsection{Multi-head Self-attention Layer} 
Transformer~\cite{vaswani2017attention} is prevalent in NLP and has achieved great success in many tasks.
At the core of Transformer, the multi-head self-attention mechanism is able to model complicated dependencies between word pairs in multiple semantic subspaces.
In the literature of CTR prediction, we take advantage of the multi-head self-attention mechanism to capture the complex dependencies between feature field pairs, i.e, pairwise feature interactions, in different semantic subspaces.

Following~\cite{song2018autoint}, given the feature embeddings , we obtain the feature representation of features that cover the pairwise interactions of an attention head  via scaled dot-product:


The matrices , ,  are three weight parameters for attention head ,  is the dimension size of head , and .

Then we combine the learnt feature representations of each head to preserve the pairwise feature interactions in each semantic subspace:

where  denotes the concatenation operation and  denotes the number of attention heads.
The learnt feature representations  are used for the initial node states of the graph neural network, where .

\subsection{Feature Graph} 
Distinguished from the previous works which simply concatenate the field embedding vectors together and feed them into designed models to learn feature interactions, we represent them in a graph structure.
In particular, We represent each input multi-field feature as a \emph{feature graph} ,
where each node  corresponds to a feature field  and different fields can interact through the edges, so that . 
Since each two fields ought to interact, it is a weighted fully connected graph while the edge weights reflect importances of different feature interactions. 
Accordingly, the task of modeling feature interactions can be converted to modeling node interactions on the feature graph. 





\subsection{Feature Interaction Graph Neural Network}\label{sect:model} 
Fi-GNN is designed to model node interactions on the feature graph, which is based on GGNN \cite{li2015gated}.
It is able to model the interactions in a flexible and explicit fashion.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{./pic/framework16.png}
\caption{Framework of Fi-GNN. 
The nodes interact with neighbors and update their states in a recurrent fashion.
At each interaction step, each node will first aggregate transformed state information from neighbors and then update its state according to the aggregated information and history via GRU and residual connection.}
\label{fig:framework}
\end{figure}

\noindent \textbf{Preliminaries.}
In Fi-GNN, each node  is associated with a hidden state vector  and the state of graph is composed of these node states
\begin{center}	

\end{center}
where  denote the interaction step.
The learnt feature representations by the multi-head self-attention layer are used for their initial node states .
As shown in Figure \ref{fig:framework}, the nodes interact and update their states in a recurrent fashion. 
At each interaction step, the nodes aggregate the transformed state information with neighbors, and then update their node states according to the aggregated information and history via GRU and residual connection.
Next, we will introduce the details of Fi-GNN elaborately.




\noindent \textbf{State Aggregation.}
At interaction step , each node will aggregate the state information from neighbors.
Formally, the aggregated information of node  is sum of its neighbors' transformed state information,

where  is the transformation function.
 is the adjacency matrix containing the edge weights.
For example,  is the weight of edge from node  to , which can reflect the importance of their interaction.
Apparently, the transformation function and adjacency matrix decide on the node interactions.
Since the interaction on each edge ought to differ, we aim to achieve edge-wise interaction, which requires a unique weight and transformation function for each edge.        


(1) \textit{\textbf{Attentional Edge Weights.}}
The adjacency matrix in the conventional GNN models is usually in the binary form, i.e., only contains 0 and 1.
It can only reflect the connected relation of nodes but fails to reflect the importances of their relations.
In order to infer the importances of interactions between different nodes, we propose to learn the edge weights via an attention mechanism.
In particular, the weight of edge from node  to node  is calculated with their initial node states, i.e., the corresponding field embedding vectors.
Formally,

where  is a weight matrix,  is the concatenation operation.
The softmax function is utilized to make weights easily comparable across different nodes.
Therefore, the adjacency matrix is,



Since the edge weights reflects the importances of different interaction, Fi-GNN can provide good explanations on the relation of different feature fields of input instance, which will be further discussed in Section \ref{sect:explannation}.


       


(2) \textit{\textbf{Edge-wise Transformation.}}
As discussed before, a fixed transformed function on all the edges is unable to model the flexible interactions and a unique transformation for each edge is essential. 
Nevertheless, our graph is complete graph with a huge number of edges.
Simply assigning a unique transformation weight to each edge will consuming too much parameter space and running time. 
To reduce the time and space complexity and also achieve edge-wise transformation, we assign an output matrix  and an input matrix  to each node  similar with \cite{cui2019dressing}.
As shown in Figure \ref{fig:framework}, when node  sends its state information to node , the state information will first be transformed by its output matrix  and then transformed by node 's input matrix  before  receives it. 
The transformation function of edge  from node  to node  thus could be written as,

Likewise, the transformation function of edge  from node  to node  is

Accordingly, the Equation \ref{ggnn_original} could be rewritten as, 
In this way, the number of parameters is proportional to the number of nodes rather than numerous edges, which greatly reduces the space and time complexity and meanwhile achieves edge-wise interaction.

\noindent \textbf{State Update.}
After aggregating state information, the nodes will update the state vectors via GRU and residual connections.

(1) \textit{\textbf{State update via GRU.}}
In traditional GGNN, the state vector of node  is updated via GRU based on the aggregated state information  and its state at last step.
Formally, 

It can be formalized in detail as:

where, , , , , ,  are weights and biases of the updating function Gated Recurrent Unit (GRU) \cite{li2015gated}. 
 and  are update gate vector and reset gate vector, respectively.


(2) \textit{\textbf{State update via Residual Connections.}}
Previous works \cite{shan2016deep,song2018autoint,cheng2016wide} have proved that it's effective to combine the low-order and high-order interactions together.
We thus introduce extra residual connections to update note states along with GRU, which can facilitate low-order feature reuse and gradients back-propagation.
Therefore, the Eq. (\ref{ggnn_1}) can be rewritten as, 




\subsection{Attentional Scoring Layer}
After  propagation steps, we can obtain the node states
\begin{center}

\end{center}
Since the nodes have interacted with their -order neighbors, the -order feature interactions is modeled.
We need a graph-level output to predict CTR.

\noindent{\textbf{Attentional Node Weights}}
The final state of each field node has captured the global information. In other words, these field nodes are neighborhood-aware. 
Here we predict a score on the final state of each field respectively and sum them up with an attention mechanism which measures their influences on the overall prediction.
Formally, the prediction score of each node  and its attentional node weight can be estimated via two multiple layers perceptions respectively as,


The overall prediction is a summation of all nodes:

Note that it is actually same as the work \cite{li2015gated}.
Intuitively,  is used to model the prediction score of each field aware of the global information and  is used to model the weights of each field (i.e., importance of fields' influence on the overall prediction).




\subsection{Training}
Our loss function is Log loss, which is defined as follows:
 \label{eqa:logloss}
where  is the total number of training samples and  indexes the training samples.
The parameters are updated via minimizing the Log Loss using RMSProp \cite{tieleman2012lecture}.
Most CTR datasets have unbalanced proportion of positive and negative samples, which will mislead the predictions.
To balance the proportion, we randomly select
 equal number of positive and negative samples in each batch during training process.
 
\subsubsection{\textbf{Parameter Space.}}
The parameter needed to be learnt mainly consists of the parameters correlated to nodes and the perception networks in attention mechanism.
For each node , we have an input matrix  and an output matrix  to transform state information. 
Totally we have  matrices, which are proportional to the number of nodes . 
Besides, the multi-head self-attention layer contains the following weight matrices  for each head, and the number of parameters of the entire layer is .  
In addition, we have two matrices of perception networks in the self-attention mechanism and also parameters in GRU. 
Overall, there are  matrices. 



 
 
\subsection{Model Analysis}
\subsubsection{\textbf{Comparison with Previous CTR Models.}}
As discussed before, the previous deep learning based CTR models model high-order interactions in a general paradigm:
raw sparse input multi-filed features are first mapped into dense field embedding vectors, then simply concatenated together and feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. 
The simple unstructured combination of feature fields inevitably limits the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.
In this way, the interaction between different fields is conducted in a fixed fashion, no matter how sophisticated the used network is.
In addition, they lack good model explanation.

Since we represent the multi-field features in a graph structure, our proposed model Fi-GNN is able to model interactions among different fields in the form of node interactions.
Compared with the previous CTR models, Fi-GNN can model the sophisticated feature interaction via flexible edge-wise interaction function, which is more effective and explicit.
Moreover, the edge weights reflecting importance of different interactions can be learnt in Fi-GNN, which provides good model explanations for CTR prediction.   
In fact, if the edge weight is all 1 and the transformation matrix on each edge is same, our model Fi-GNN collapses into FM.
Taking advantage of the great power of GNN, we can apply flexible interactions on different feature fields.

\subsubsection{\textbf{Comparison with Previous GNN Models.}}
Our proposed model Fi-GNN is designed based on GGNN, upon which we mainly make two improvements:
(1) we achieve edge-wise interaction via attentional edge weights and edge-wise transformation;
(2) we introduce an extra residual connection along with GRU to update states, which can help regain the low-order information.

As discussed before, the node interaction on each edge in GNN depends on the edge weight and the transformation function on the edge.
The conventional GGNN uses binary edge weights which fails to reflect the importance of the relations, and a fixed transformation function on all the edges. 
In contrast, our proposed Fi-GNN can model edge-wise interactions via attention edge weights and edge-wise transformation functions.
When the interaction order is high, the node states tend to be smooth, i.e., the states of all the nodes tend to be similar.
The residual connections can help identity the nodes by adding initial node states.

\begin{table}[h]
\centering\caption{Statistics of evaluation datasets.}
\begin{tabular}{cccc} 
\hline
Dataset & \#Instances & \#Fields & \#Features (sparse)  \\
\hline
Criteo & 45,840,617 & 39 & 998,960 \\
Avazu & 40,428,967 & 23 & 1,544,488 \\
\hline
\end{tabular}\label{tab::dataset}
\end{table}
  
 
\begin{table*}
\centering\caption{Performance Comparison of Different methods. The best performance on each dataset and metric are highlighted. Further analysis is provided in Section \ref{sect:result}.}
\begin{tabular}{llcccccccc} 
\hline
\multirow{2}{*}{Model Type} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{Criteo} & \multicolumn{4}{c}{Avazu}\\
 &  & AUC & RI-AUC & Logloss & RI-Logloss & AUC & RI-AUC & Logloss & RI-Logloss \\
\hline
\multirow{1}{*}{First-order} & LR & 0.7820 & 3.00\% & 0.4695 & 5.43\% & 0.7560 & 2.60\% & 0.3964 & 3.63\% \\
 \hline
\multirow{2}{*}{Second-order} & FM~\cite{rendle2010factorization} & 0.7836 & 2.80\% & 0.4700 & 5.55\% & 0.7706 & 0.72\% & 0.3856 & 0.76\% \\ 
& AFM\cite{xiao2017attentional} & 0.7938 & 1.54\% & 0.4584 & 2.94\% & 0.7718 & 0.57\% & 0.3854 & 0.81\% \\
\midrule
\multirow{5}{*}{High-order} 
& DeepCrossing~\cite{shan2016deep} & 0.8009 & 0.66\% &0.4513 & 1.35\% & 0.7643 & 1.53\% & 0.3889 & 1.67\% \\
& NFM~\cite{he2017neural} & 0.7957 & 1.57\% & 0.4562 & 2.45\% & 0.7708 & 0.70\% & 0.3864 & 1.02\% \\
& CrossNet~\cite{wang2017deep} & 0.7907 & 1.92\% & 0.4591 & 3.10\% & 0.7667 & 1.22\% & 0.3868 & 1.12\% \\
& CIN~\cite{lian2018xdeepfm} & 0.8009 & 0.63\% & 0.4517 & 1.44\% & 0.7758 & 0.05\% & 0.3829 & 0.10\% \\
& Fi-GNN (ours)  & \textbf{0.8062} & 0.00\% & \textbf{0.4453} & 0.00\% & \textbf{0.7762} & 0.00\% & \textbf{0.3825} & 0.00\% \\
\bottomrule
\end{tabular}
\label{tab::results}
\end{table*}

\section{Experiments}
In this section, we conduct extensive experiments to answer the following questions:
\begin{itemize}
\item[\textbf{RQ1}]
 How does our proposed Fi-GNN perform in modeling high-order feature interactions compared with the state-of-the-art models?
\item[\textbf{RQ2}]
 Does our proposed Fi-GNN perform better than original GGNN in modeling high-order feature interactions?
\item[\textbf{RQ3}]
 What are the influences of different model configurations?
 \item[\textbf{RQ4}]
What are the relations between features of different fields? 
Is our proposed model explainable?
\end{itemize}
We first present some fundamental experimental settings before answering these questions.


\subsection{Experiment Setup}

\subsubsection{Datasets}
We evaluate our proposed models on the following two datasets, whose statistics are summarized in Table~\ref{tab::dataset}.

\textbf{1. Criteo\footnote{https://www.kaggle.com/c/criteo-display-ad-challenge}.} This is a famous industry benchmark dataset for CTR prediction, which has 45 million users' click records in 39 anonymous feature fields on displayed ads.
Given a user and the page he is visiting, the goal
is to predict the probability that he will click on a given ad.

\textbf{2. Avazu\footnote{https://www.kaggle.com/c/avazu-ctr-prediction}.} This dataset contains users' click behaviors on displayed mobile ads. 
There are 23 feature fields including user/device features and ad attributes.  
The fields are partial anonymous.

For the two datasets, we remove the infrequent features appearing in less than 10, 5 times respectively and treat them as a single feature ``<unknown>''.
Since the numerical features may have large variance, we normalize numerical values by transforming a value  to  if , which is proposed by the winner of Criteo Competition\footnote{\url{https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf}}. 
The instances are randomly split in 8:1:1 for training, validation and testing. 


\subsubsection{Evaluation Metrics}
We use the following two metrics for model evaluation: AUC (Area Under the ROC curve) and Logloss (cross entropy).

\textbf{AUC} measures the probability that a positive instance will be ranked higher than a randomly chosen negative one.  
A higher AUC indicates a better performance.

\textbf{Logloss} measures the distance between the predicted score and the true label for each instance.
A lower Logloss indicates a better performance.

\textbf{Relative Improvement (RI)}. It should be noted that a small improvement with respect to AUC is regarded significant for real-world CTR tasks \cite{cheng2016wide,guo2017deepfm,wang2017deep,lian2018xdeepfm}. 
In order to estimate the relative improvement of our model achieves over the compared models, we here measure \textbf{RI-AUC} and \textbf{RI-Logloss}, which can be formulated as,

where  returns the absolute value of x,  can be either AUC or Logloss,  refers to our proposed model and  refers to the compared model.




\subsubsection{Baselines} 
As described in Section \ref{sect:related}, 
the early approaches can be categorized into three types: 
(A) Logistic Regression (LR) which models first-order interaction;
(B) Factorization Machine (FM) based linear models which model second-order interactions; 
(C) Deep learning based models which model high-order interactions on the concatenated field embedding vectors. 

We select the following representative methods of three types to compare with ours.

\textbf{LR} (A) models first-order interaction on the linear combination of raw individual features.  

\textbf{FM}~\cite{rendle2010factorization} (B) models second-order feature interactions from vector inner products. 

\textbf{AFM}~\cite{xiao2017attentional} (B) is a extent of FM, which considers the weight of different second-order feature interactions by using attention mechanism.
It is one of the state-of-the-art models that model second-order feature interactions. 

\textbf{DeepCrossing}~\cite{shan2016deep} (C) utilizes DNN with residual connections to learn high-order feature interactions in an implicit fashion.


\textbf{NFM}~\cite{he2017neural} (C) utilizes a Bi-Interaction Pooling layer to model the second-order interactions, and then feeds the concatenated second-order combinatorial features into DNNs to model high-order interactions.

\textbf{CrossNet (Deep\&Cross) }~\cite{wang2017deep} (C) is the core of Deep\&Cross model, which tries to model feature interactions explicitly by taking outer product of concatenated feature vector at the bit-wise level.

\textbf{CIN (xDeepFM)}~\cite{lian2018xdeepfm} (C) is the core of xDeepFM model, which takes outer product of stacked feature matrix at vector-wise level.





\subsubsection{Implementation Details}
We implement our method using Tensorflow\footnote{The code is released at \url{https://github.com/CRIPAC-DIG/Fi_GNN}}. The optimal hyper-parameters are determined by the grid search strategy. 
Implementation of baselines follows \cite{song2018autoint}.
Dimension of field embedding vectors is 16 and batch size is 1024 for all methods. 
DeepCrossing has four feed-forward layers, each with 100 hidden units.  
NFM has one hidden layer of size 200 on top of Bi-Interaction layer as recommended in the paper \cite{he2017neural}. 
There are three interaction layers for both CrossNet and CIN. 
All the experiments were conducted over a sever equipped with 8 NVIDIA Titan X GPUs.


\begin{figure*}[hbtp]
\subfigure[edge-wise interaction (E) and residual connections (R)]{
\begin{minipage}[b]{0.5\textwidth}
\label{fig:ablation_er} \includegraphics[width=1\textwidth]{./pic/er.pdf}

\end{minipage}}\subfigure[attentional edge weight (W) and edge-wise transformation (T)]{
\begin{minipage}[b]{0.5\textwidth}
\label{fig:ablation_wt} \includegraphics[width=1\textwidth]{./pic/wt.pdf}
\end{minipage}}\caption{Two groups of ablation studies on Fi-GNN.}
\label{fig:performance}
\end{figure*}


\subsection{Model Comparison (RQ1)}\label{sect:result}
The performance of different methods is summarized in Table \ref{tab::results}, from which we can obtain the following observations:
 
\begin{itemize}
\item[(1)]
LR achieves the worst performance among these baselines, which proves that the individual features is insufficient in CTR prediction.
\item[(2)]
FM and AFM, which model second-order feature interactions, outperform LR on all datasets, indicating that it's effective to model pair-wise interaction between feature fields. 
In addition, AFM achieves better performance than FM, which proves the effectiveness of attention on different interactions.
\item[(3)]
The methods modeling high-order interaction mostly outperform the methods that model second-order interactions.
This indicates the second-order feature interactions is not sufficient. 
\item[(4)]
DeepCrossing outperforms NFM, proving the effectiveness of residual connections in CTR prediction.
\item[(5)]
Our proposed Fi-GNN achieves best performance among all these methods on two datasets.
Considering the fact that previous improvements with respect to AUC at \textbf{0.001-level} are regarded significant for CTR prediction task, our proposed method shows great superiority over these state-of-the-arts especially on Criteo dataset, owing to the great representative power of graph structure and the effectiveness of GNN on modeling node interactions.
\item[(6)] Compared with these baselines, the relative improvement of our model achieves on Criteo dataset is higher than that on Avazu dataset. This might be attributed to that there are more feature fields in Criteo dataset, which can take more advantage of the representative power of graph structure.
\end{itemize}



\subsection{Ablation Study (RQ2)}\label{sect:comp_gnn}

Our proposed model Fi-GNN is based on GGNN, upon which we mainly make two improvements:
(1) we achieve edge-wise node interactions via attentional edge weights and edge-wise transformation;
(2) we introduce extra residual connections to update state along with GRU.
To evaluate the effectiveness of the two improvements on modeling node interactions, we conduct ablation study and compare the following three variants of Fi-GNN:

\textbf{Fi-GNN(-E/R)}:
Fi-GNN without the two above mentioned improvements: edge-wise node interactions (\textbf{E}) and residual connections (\textbf{R}).

\textbf{Fi-GNN(-E)}:
Fi-GNN without edge-wise interactions (\textbf{E}).

\textbf{Fi-GNN(-R)}:
Fi-GNN without residual connections (\textbf{R}), which is also GGNN with edge-wise interactions. 

The performance comparison is shown in Figure \ref{fig:ablation_er}, from which we can obtain the following observations:
\begin{itemize}
\item[(1)]
Compared with FiGNNï¼Œthe performance of Fi-GNN(-E) drops by a large margin, suggesting that it's crucial to model the edge-wise interaction.
Fi-GNN(-E) achieves better performance than Fi-GNN(-E/R), proving that the residual connections can indeed provide useful information. 
\item[(2)]
The full model Fi-GNN outperforms the three variants, indicating that the two improvements we make, i.e., residual connections and edge-wise interactions, can jointly boost the performance.
\end{itemize}




We take two measures to achieve edge-wise node interactions in Fi-GNN: attentional edge weight (\textbf{W}) and edge-wise transformation (\textbf{T}).
To further investigate where dose the great improvement come from, we conduct another ablation study and compare the following three variants of Fi-GNN:

\textbf{Fi-GNN(-W/T)}: Fi-GNN without self-adaptive adjacency matrix (\textbf{W}) and edge-wise transformation (\textbf{T}), i.e., uses binary adjacency matrix (all the edge weights are 1) and a shared transformation matrix on all the edges.
It is also \textbf{Fi-GNN-(E)},

\textbf{Fi-GNN(-W)}: FI-GNN without attentional edge weights, i.e., uses binary adjacency matrix.

\textbf{Fi-GNN(-T)}: FI-GNN without edge-wise transformation,
i.e., uses a shared transformation on all the edges. 

The performance comparison is shown in Figure \ref{fig:ablation_er}.
We can see that Fi-GNN(-T) and Fi-GNN(-W) both outperform Fi-GNN(-W/T), which proves their effectiveness.
Nevertheless, Fi-GNN(-W) achieves greater improvements than Fi-GNN(-T), suggesting that the edge-wise transformation is more effective than attentional edge weights in modeling edge-wise interaction.
This is quite reasonable since the transformation matrix oughts to have stronger influence on interactions than a scalar attentional edge weight.
In addition, Fi-GNN achieves the best performance demonstrates that it's crucial to take both the two measures to model edge-wise interaction.






\begin{figure}[t]
\centering
\subfigure[State Dimensionality]{
\begin{minipage}[b]{0.24\textwidth}
\label{fig:hidden} \includegraphics[width=1\textwidth]{./pic/hidden.pdf}

\end{minipage}}\subfigure[Interaction Step]{
\begin{minipage}[b]{0.24\textwidth}
\label{fig:order} \includegraphics[width=1\textwidth]{./pic/order.pdf}
\end{minipage}}\caption{AUC performance with different state dimensionality  (left) and interaction step  (right) on Criteo and Avazu dataset.}
\label{fig:performance}
\vspace{-4mm}
\end{figure}


\subsection{Hyper-Parameter Study (RQ3)}

\subsubsection{\textbf{Influence of different state dimensionality.}}
We first investigate how the performance changes w.r.t. the dimension of the node states , which is also the output size of the initial multi-head self-attention layer. 
The results on Criteo and Avazu datasets are shown in Figure \ref{fig:hidden}.
On Avazu dataset, the performance first increases and then begins to decrease when the dimension size reaches 32, which indicates that state size of 32 has been represented enough information and the model is overfitted when too many parameters are used. 
Nevertheless, on Criteo dataset, the performance peaks with the dimension size of 64, which is reasonable since the dataset is more complexed which needs larger dimension size to carry out enough information.



\subsubsection{\textbf{Influence of different interaction steps.}}
We are interested in what the optimal highest order of feature interactions is.  
Our proposed Fi-GNN can answer the question, since the interaction step  equals to the highest order of feature interaction.
Therefore, we conduct experiments on how the performance changes w.r.t. the highest order of feature interaction, i.e., the interaction step .
The results on Criteo and Avazu datasets are shown in Figure \ref{fig:order}.
On Avazu datasets, we can see that the performance increases along with the increasing of  until it reaches 2, after that the performance starts to decrease.
By contrast, the performance peaks when  on Criteo dataset.
This finding suggests 2-order and 3-order interactions are enough for Avazu and Criteo dataset, respectively.
It is reasonable since the Avazu and Criteo datasets have 23 and 39 feature fields, respectively.
Thus the Criteo dataset needs more interaction steps for the field nodes to fully interact with other nodes in the feature graphs.



\subsection{Model Explanation (RQ4)} \label{sect:explannation}
In this section, we will answer the question that can Fi-GNN provide explanations.
We apply attention mechanisms on the edges and nodes in the feature graphs and obtain attentional edge weights and attentional node weights respectively, which can provide explanations from different aspects.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{./pic/attention_edge.png}
\caption{Heat map of attentional edge weights at the global-level on Avazu, which reflects the importance of relations between different feature fields.}
\label{fig:heatmap_edge}
\end{figure}

\subsubsection{\textbf{Attentional Edge weights.}}
The attentional edge weight reflects the importance of interaction between the two connected field nodes, which can also reflect the relation of the two feature fields.
Higher the weight is, stronger the relation is.
Figure \ref{fig:heatmap_edge} presents the heat map of the globally averaged adjacency matrix of all the samples in Avazu dataset, which can reflect the relations between different fields in a global level. 
Since they are some anonymous feature fields, we only show the remaining 13 feature fields with real meanings.

As can be seen, some feature fields tend to have a strong relations with others, such as \textsf{site\_category} and \textsf{site\_id}.
This makes sense since the two feature field both corresponds to the website where the impressions are put on. They contain the main contextual information of impressions. 
\textsf{Hour} is another feature which have close relations with others. It is reasonable since Avazu focuses on mobile scene, where user surfing online at any time of a day.  
The surfing time has strong influence on other advertising features.  
On the other hand, \textsf{device\_ip} and \textsf{device\_id} seem to have weak relations with other feature fields.
This may due to that they nearly equal to user identity, which is relatively fixed and hard to be influenced by other features. 

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{./pic/node_attention.pdf}
\caption{Heat map of attentional node weights at both global- and case-level on Avazu, which reflects the importance of different feature fields on the final prediction.}
\label{fig:heatmap_node}
\vspace{-5mm}
\end{figure}

\subsubsection{\textbf{Attentional Node weights.}}
The attentional node weights reflect the importances of feature fields' influence on the overall prediction score.
Figure \ref{fig:heatmap_node} presents the heat map of global-level and case-level attentional node weights.
The leftmost is an globally averaged one of all the samples in Avazu dataset.
The left four are randomly selected, whose predicted scores are , and labels are  respectively. 
At the global level, we can see that the feature field \textsf{app\_category} have the strongest influence on the clicking behaviors. 
It is reasonable since Avazu focuses on mobile scene, where the app is the most important factor. 
At the case level, we observe that the final clicking behavior mainly depends on one critical feature field in most cases.

\section{Conclusions}
In this paper, we point out the limitations of the previous CTR models which consider multi-field features as an unstructured combination of feature fields.
To overcome these limitations, we propose to represent the multi-field features in a graph structure for the first time, where each node corresponds to a feature field and different fields can interact through edges.
Therefore, modeling feature interactions can be converted to modeling node interaction on the graph.  
To this end, we design a novel model Fi-GNN which is able to model sophisticated interactions among feature fields in a flexible and explicit fashion.
Overall, we propose a new paradigm of CTR prediction: represent multi-field features in a graph structure and convert the task of modeling feature interactions to modeling node interactions on graphs, which may motivate the future work in this line.




\begin{acks}
This work is supported by National Natural Science Foundation of China (61772528, 61871378) and National Key Research and Development Program (2016YFB1001000, 2018YFB1402600).
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}



\end{document}
