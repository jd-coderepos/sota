

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{xspace}



\usepackage{threeparttable} 
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{float}
\usepackage{multirow}

\definecolor{Gray}{gray}{0.9}
\definecolor{Red}{RGB}{230, 57, 70}
\definecolor{Blue}{RGB}{0, 100, 148}

\newcommand{\TODO}[1]{\textcolor{red}{(TODO: #1)}}
\newcommand{\etal}{\textit{et al}.\@ }
\newcommand{\name}{\emph{DeciWatch}\xspace}

\usepackage{mmstyle}
\usepackage{wrapfig}
\setcounter{figure}{0}
\setcounter{table}{0}
 

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{1845}  

\title{---Supplementary Materials---\\DeciWatch: A Simple Baseline for $10\times$ Efficient 2D and 3D Pose Estimation} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{DeciWatch}
\author{Ailing Zeng$^{1}$  \and
Xuan Ju$^{1}$  \and
Lei Yang$^{2}$  \and
Ruiyuan Gao$^{1}$  \and
Xizhou Zhu$^{2}$  \and
Bo Dai$^{3}$  \and
Qiang Xu$^{1}$ 
}
\authorrunning{A. Zeng et al.}
\institute{$^{1}$The Chinese University of Hong Kong, $^{2}$Sensetime Group Limited, \\$^{3}$Shanghai AI Laboratory\\
\email{\{alzeng, qxu\}@cse.cuhk.edu.hk}
}
\maketitle

In Sec.~\ref{sec:supp_exp}, we present dataset descriptions. Next, we present results of efficient labeling in Sec.~\ref{sec:supp_app} and the generalization ability of \name in Sec.~\ref{sec:supp_general}. Then, we show more ablation studies on different sampling ratios, model designs of \textit{DenoiseNet} and \textit{RecoverNet}, and hyper-parameters in Sec.~\ref{sec:supp_ablation}. Moreover, we show qualitative comparison results in Sec.~\ref{sec:supp_viz} to demonstrate why \name works. Last, in Sec.~\ref{sec:supp_fail}, we discuss some failure cases in this method to motivate further research.


\section{Dataset Descriptions}
\label{sec:supp_exp}

\noindent -- \textbf{Sub-JHMDB}  JHMDB\cite{jhuang2013towards} is a video-based dataset for 2D human pose estimation. For a fair comparison, we only conduct our experiments on a subset of JHMDB called sub-JHMDB. It contains $316$ videos and the average duration is $35$ frames. For each frame, it provides $15$ annotated body keypoints. We use the bounding box calculated from the puppet mask provided by \cite{luo2018lstm}. Following the settings \cite{zhang2020key,fan2021motion}, we mix $3$ original splitting schemes for training and testing together in 2D pose estimation experiments.

\noindent -- \textbf{Human3.6M}~\cite{ionescu2013human3} Human3.6M is a large-scale indoor video dataset with $15$ actions from $4$ camera viewpoints. It has $3.6$ million frames and a frame rate of $50$ fps. 3D human joint positions are captured accurately from a high-speed motion capture system. Following previous works~\cite{zeng2020srnet,martinez2017simple,pavllo20193d,zeng2021smoothnet}, we use the standard protocol with $5$ actors (S$1$, S$5$, S$6$, S$7$, S$8$) as the training set and another $2$ actors (S$9$, S$11$) as the testing set. 

\noindent -- \textbf{3DPW} \cite{von2018recovering} 3DPW is a challenging in-the-wild dataset consisting of more than $51$k frames with accurate 3D poses and shapes annotation. The sequences are $30$fps. This dataset is usually used to validate the performance of model-based body recovery methods~\cite{kanazawa2018hmr,kolotouros2019spin,joo2020eft,kocabas2021pare}.


\noindent -- \textbf{AIST++} \cite{li2021aist} is a challenging dataset with diverse and fast-moving dances that comes from the AIST Dance Video DB~\cite{tsuchida2019aist}. It contains 3D human motion annotations of $1,408$ video sequences at $60$ fps, which is $10.1$M frames in total. The 3D human keypoint annotations and SMPL parameters it provides cover $30$ different actors in $9$ views. We follow the original settings to split the training and testing sets based on actors and actions.

\section{An application: Efficient Pose Labeling in Videos}
\label{sec:supp_app}
Due to the efficiency and smoothness of the pose sequences recovered by \name, reducing the need for dense labeling could be a potential application. We verify the effectiveness of this application on the Human3.6M and AIST++ dataset by directly inputting the sparse ground-truth 3D positions into the \emph{RecoverNet} of \name. In Table~\ref{tab:label}, we compare \name with the most used spline interpolation, linear interpolation, and quadratic interpolation. Our method has a slower error growth as the interval $N$ gets larger. 
To be specific, it is possible to label one frame every $10$ frames with only $2.89$mm position errors in slow movement videos (e.g., in Human3.6M~\cite{ionescu2013human3}) and label one frame every $5$ frames with only $4.03$mm position errors in fast-moving videos (e.g., in AIST++~\cite{li2021aist}). This application can improve annotation efficiency by more than $10\times$.
\begin{table}[h]
\small
	\centering
    \caption{\textbf{Comparison of \emph{MPJPE} on efficient pose labeling that labels one frame in every $N$ frames on Human3.6M~\cite{ionescu2013human3} and AIST++~\cite{li2021aist} dataset.}}
	{\begin{tabular}{l|ccccc||ccccc}

			\specialrule{.1em}{.05em}{.05em}
			&\multicolumn{5}{c||}{\cellcolor{Gray}\textbf{Human3.6M}}&\multicolumn{5}{c}{\cellcolor{Gray}\textbf{AIST++}}\\
			\cmidrule{2-11}
			Interval N&2 &  5 &10&15 &20&2 & 5 &10&15 &20\\
			\midrule
			Linear&2.21&6.55&10.81&24.15&35.20&7.21&21.31&27.72&73.69&99.04 \\
			Quadratic&1.26&4.31&10.05&\underline{17.22}&\underline{22.85}&2.04&8.33&23.59&\underline{43.13}&\underline{61.16}\\
			Cubic-Spline&\textbf{0.18}&\textbf{0.99}&\underline{5.36}&18.42&29.21&\underline{0.89}&\underline{5.12}&\underline{18.31}&45.32&77.39\\
		    \textbf{\name}& \underline{0.25}&\underline{1.33
		    }&\textbf{2.89}&\textbf{6.21}&\textbf{10.59}&\textbf{0.83}&\textbf{4.03}&\textbf{11.25}&\textbf{20.12}&\textbf{41.25}\\
        \midrule
        \end{tabular}}
	\label{tab:label}
\end{table}

\section{Additional Evaluation Metrics for 2D Pose Estimation}

As is shown in Tab. 1 in the main paper, the results of \name have achieved nearly 99\% accuracy on PCK@0.2. However, qualitative visualization shows that an awful lot of errors still exist in the recovery results. We attribute it to the fact that PCK@0.2 is quite loose for accuracy measurement, which only requires the detected keypoints to be within 20\% of the bounding box size under pixel level. As a result, we use two additional evaluation metrics, PCK@0.1, and PCK@0.05, for better localization evaluation. More specifically, PCK@0.1 and PCK@0.05 restrict the matching threshold to 10\% and 5\% of the bounding box size. Tab.~\ref{table:pck0050102} shows the results of \name and SimplePose\cite{xiao2018simple} on these three metrics. In future work, we recommend using PCK@0.05 as the main metrics for 2D pose estimation.
 
\begin{table}[H]
\centering
\caption{\textbf{Comparison of \name and SimplePose\cite{xiao2018simple} on PCK@0.2, PCK@0.1, and PCK@0.05}. In future work, we recommend using PCK@0.05 as the main metrics for 2D pose estimation.}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{c|c|ccc}
\hline
\multicolumn{5}{l}{\cellcolor{Gray}\textbf{Sub-JHMDB dataset - 2D Pose Estimation}}                                         \\ \hline
\multicolumn{1}{l}{Sampling Ratio} & \multicolumn{1}{l}{Evaluation Metric} & PCK@0.2 $\uparrow$          & PCK@0.1 $\uparrow$          & PCK@0.05 $\uparrow$         \\ \hline
                                   & SimplePose                            & 93.92\%          & 81.25\%          & 56.88\%          \\ \cline{2-5} 
\multirow{-2}{*}{20\%}             & \textbf{DeciWatch}                    & \textbf{99.11\%} & \textbf{95.43\%} & \textbf{82.66\%} \\ \hline
                                   & SimplePose                            & 93.94\%          & 81.61\%          & 57.30\%          \\ \cline{2-5} 
\multirow{-2}{*}{10\%}             & \textbf{DeciWatch}                    & \textbf{98.75\%} & \textbf{94.05\%} & \textbf{79.44\%} \\ \hline
                                   & SimplePose                            & 92.38\%          & 82.79\%          & 58.95\%          \\ \cline{2-5} 
\multirow{-2}{*}{5\%}              & \textbf{DeciWatch}                    & \textbf{97.50\%} & \textbf{91.76\%} & \textbf{73.02\%} \\ \hline
\end{tabular}
}
\label{table:pck0050102}

\end{table}


\section{Generalization Ability}
\label{sec:supp_general}
\name learns the patterns of noisy human motions since motion distribution could be overlapped among some datasets, making it has potential generalization ability. 
We further test \name trained on 3DPW-PARE across various backbones and datasets in Tab.~\ref{table:generalize}, where \name still achieves competitive pose estimation results with 10x efficiency. We attribute it to the fact that \name effectively learns the continuity of motions, which is applicable for different sorts of motions. 


\begin{table}[H]
	\centering
    \scriptsize
    \caption{\textbf{Cross-backbone and cross-dataset results from \name checkpoints trained on 3DPW-PARE.}}
	{
		\begin{tabular}{l|cccc}
		\midrule[0.25pt]
        Dataset/Estimator            &           & MPJPE$\downarrow$ & Accel.$\downarrow$  \\
        \midrule[0.25pt]
        \multirow{2}{*}{3DPW/EFT}    & Estimator (100\%) & 90.3  & 32.8      \\
                                     &\name (\textbf{10\%})   & \textbf{87.2}{\color{Red}$\downarrow_{3.1(3.4\%)}$}  & \textbf{7.2}{\color{Red}$\downarrow_{25.6(77.9\%)}$}     \\
        \midrule[0.15pt]
        \multirow{2}{*}{3DPW/SPIN}   & Estimator (100\%) & \textbf{96.9}  & 34.6      \\
                                     & \name (\textbf{10\%})   & 98.3{\color{Blue}$\uparrow_{1.4(1.4\%)}$}  & \textbf{7.1}{\color{Red}$\downarrow_{27.5(79.5\%)}$}       \\
        \midrule[0.15pt]
        \multirow{2}{*}{AIST++/SPIN} & Estimator (100\%) & 107.7 & 33.8   \\
                                     & \name (\textbf{10\%})  & \textbf{101.8}{\color{Red}$\downarrow_{5.9(5.5\%)}$} & \textbf{6.2}{\color{Red}$\downarrow_{27.6(81.7\%)}$}   \\
        \midrule[0.25pt]
        \end{tabular}
	}

	\label{table:generalize}
\end{table}



\section{Ablation Study}
\label{sec:supp_ablation}

\noindent \textbf{Impact of sampling ratio and input window size.}
Both input window size and sampling ratio will affect inference efficiency and performance of \name. With the same input window size, the lower the sampling ratio is, the more efficient the inference process will be. We present the comparison of original pose estimator (Ori.)and \name with sampling ratio from $100\%$ to $5\%$ (sampling interval $N$ changes from $1$ to $20$) in Fig.~\ref{fig:supp_ablation_size}(a). When the sampling ratio is $100\%$, \name can be regarded as a denoise model. As shown in Fig.~\ref{fig:supp_ablation_size}(a), the changing trends of \emph{MPJPE} are similar for all three estimation methods (PARE~\cite{kocabas2021pare}, EFT~\cite{joo2020eft}, SPIN~\cite{kolotouros2019spin}). Surprisingly, we find that \emph{MPJPEs} first drop before rising, and they are smallest when the sampling ratio is about $20\%$, with improvements of $4.9\%$, $3.4\%$, and $4.9\%$ for PARE, EFT, and SPIN respectively. This gives us a new perspective that in pose estimation, \emph{not every frame has to be watched to achieve better performance}. The reason behind this is the different degrees of noise in estimated poses. It may be harder to eliminate these diverse degrees of errors in all frames than only denoise some of the frames and recover the rest by temporal continuity.
Besides, the \emph{MPJPEs} of \name is worse than that of the original pose estimator when the sampling ratio is larger than $8\%$ due to too limited input information.



With the sampling ratio fixed at $10\%$, we further explore the influence of window size. 
In Fig.~\ref{fig:supp_ablation_size}(b), we test window sizes from $11$ to $201$. Results indicate that our framework is robust to different window sizes. 

\begin{figure}[h]	
\centering
    \subfigure[Effect of the sampling ratio] 
	{
		\begin{minipage}[t]{0.46\linewidth}
			\centering      
			\includegraphics[width=2.3in]{figures/sample_ratio_1.pdf}
		\end{minipage}
	}
    	\label{fig:sample_ratio}  
\subfigure[Effect of the window size] 
	{
		\begin{minipage}[t]{0.46\linewidth}
			\centering         
			\includegraphics[width=2.2in]{figures/window_size.pdf}   
		\end{minipage}
	} 
    	\label{fig:window_size} 
\caption{\textbf{Comparing effects of different (a) sampling ratios and (b) window sizes.} Sampling interval $N$ is from $1$ to $20$. We compare \emph{MPJPEs} of the three original (Ori.) pose estimators~\cite{kocabas2021pare,joo2020eft,kolotouros2019spin} to our framework on the 3DPW dataset.}
\label{fig:supp_ablation_size} 
\end{figure}


To serve for future research, we report results, including \emph{MPJPEs} and \emph{Accels}, of 3D pose and body estimation on 3DPW, Human3.6M, and AIST++ datasets in Table~\ref{tab:supp_sample_ratio}. All results show similar trends in the change of precision (\emph{MPJPEs}) and smoothness (\emph{Accels}). In addition, \name utilizes the natural smoothness of human motions to recover the detected poses. As a result, the \textit{Accels} decreases steadily when the interval $N$ increases, indicating \name can enhance the smoothness of the existing backbone methods. 

\begin{table}[h]
\centering
\caption{\textbf{Results of original (Ori.) estimators~\cite{kocabas2021pare,joo2020eft,kolotouros2019spin,martinez2017simple} and \name under different sampling ratios.} Ori. is the watch-every-frame pose estimator. Sampling interval $N$ is set from $1$ to $20$. The best results are in bold.}
\label{tab:supp_sample_ratio}
\tiny
\setlength{\tabcolsep}{0.7mm}{\begin{tabular}{l|ccccccccccccccccccc}
\toprule
Metrics/$N$&  Ori.&1 & 3 & 5&6 &7&8 &9&10&11&12&13&14&15&16&17&18&19&20\\\midrule
\multicolumn{20}{l}{\cellcolor{Gray}\textbf{PARE~\cite{kocabas2021pare} Backbone on 3DPW dataset}}   \\\midrule[0.25pt]                                                              
\emph{MPJPE}&78.9&75.7&\textbf{75.0}&75.1&75.2&75.4&76.0&76.4&77.2&77.7&78.4&79.4&80.3&80.7&81.7&82.5&83.5&84.4& 85.3 \\
\emph{Accel}&25.7&25.2 &9.2&7.7&7.4&7.2&7.1&7.0&6.9&6.9&6.8&6.8&6.7&6.7&6.7&6.7&\textbf{6.6}&\textbf{6.6}&\textbf{6.6}\\\hline
  \multicolumn{20}{l}{\cellcolor{Gray}\textbf{EFT~\cite{joo2020eft} Backbone on 3DPW dataset}} \\\midrule[0.25pt]             
\emph{MPJPE} & 90.3&88.0 &\textbf{87.2} & \textbf{87.2}&87.3 &87.6 & 88.3& 88.4& 89.0& 89.8& 90.3& 91.5&92.3 &92.3 &93.1 &94.0 &94.6 & 95.4&96.1 \\
\emph{Accel} &32.8 &32.7 &10.2 &8.0 &7.5 &7.3 &7.1 &6.9 & 6.8& 6.8& 6.7&6.7 &6.6 &6.6 &6.5 &6.5 &6.5&\textbf{6.4}&\textbf{6.4}
 \\\hline
 \multicolumn{20}{l}{\cellcolor{Gray}\textbf{SPIN~\cite{kolotouros2019spin} Backbone on 3DPW dataset} } \\\midrule[0.25pt]             
\emph{MPJPE} &96.9&93.8&92.9&\textbf{92.2}&92.7&92.6&93.2&93.4&93.3&94.2&95.0&95.3&96.6&96.7&97.4&97.6&98.8&99.2&100.2\\
\emph{Accel}  &34.6 &33.5 &10.5 &8.2 &7.7 & 7.5& 7.3& 7.2& 7.1&7.0 &6.9 &6.9 &6.9 &6.9 &\textbf{6.8} &6.9 &\textbf{6.8}&\textbf{6.8} &\textbf{6.8}
 \\\toprule \hline
 \multicolumn{20}{l}{\cellcolor{Gray}\textbf{FCN~\cite{martinez2017simple} Backbone on Human3.6M dataset} } \\\midrule[0.25pt]         
\emph{MPJPE}  &54.6 &53.3 & 53.0& 52.8& 52.6&\textbf{52.3} &52.5 &52.6 &52.8 &53.0 & 53.2& 53.2& 53.4& 53.5&53.9 &53.8 &54.0 &54.2 &54.4 \\
\emph{Accel}  &19.2 &15.4 &3.1 &2.0 &1.8 &1.6 &1.6 &1.5 &1.5 &\textbf{1.4} &\textbf{1.4} &\textbf{1.4} &\textbf{1.4} &\textbf{1.4} & \textbf{1.4}&\textbf{1.4} &\textbf{1.4} &\textbf{1.4} &\textbf{1.4}
 \\\toprule \hline
  \multicolumn{20}{l}{\cellcolor{Gray}\textbf{SPIN~\cite{kolotouros2019spin} Backbone on AIST++ dataset} }  \\\midrule[0.25pt]
\emph{MPJPE}  & 107.7&67.2 &\textbf{66.6} &67.6 &68.4 &69.7 &71.2 &71.6 &71.3 &76.1 &77.1 &79.0 &80.2 &82.3 &84.3 &85.2 &87.0 &88.9 &90.8 \\
\emph{Accel}  & 33.8& 7.6& 7.6&6.6 &6.3 &6.1 &6.0 &5.9 &5.7 &5.7 &5.6 &5.6 &5.5 &5.5 &5.5 &5.5 &5.4 &\textbf{5.3} &\textbf{5.3} 
 \\\bottomrule
\end{tabular}}
\end{table}

\noindent \textbf{Analyses on the phenomenon: fewer samples with better performance.}
When the inputs of \name are \underline{ground-truth poses}, the performance deteriorates with decreased sampling ratio (see Table~\ref{tab:label} above). However, in practice, the inputs of \name are \underline{noisy detected poses}, and some of them have high errors (e.g., due to occlusion). 
Consequently, considering the detected poses' errors, two factors affect the recovered poses.

1) On the one hand, not considering/aggregating the highly noisy poses can improve performance by reducing the impact of noisy poses on both \emph{DenoiseNet} and \emph{RecoverNet}. 2) On the other hand, dropping too many frames would lead to performance degradation due to information insufficiency.

Generally speaking, when the sampling ratio is high (e.g., \textgreater 20\%), we could easily recover intermediate poses thanks to the continuity of motions.
And for those dropped intermediate poses,
the denoised and recovered poses via \name obtain lower error compared to their original noisy estimation.
Consequently, the overall MPJPEs would drop with the increase of intervals (i.e., the decrease of sampling ratio) in the beginning. However, when the sampling ratio becomes too low (e.g., \textless 5\%), the highly sparse poses do not provide sufficient information for motion recovery, and the MPJPEs would go up under such circumstances. In other words, there would be a ``sweet spot'' for the sampling ratio with the minimum MPJPEs.  
This phenomenon is also present in the traditional interpolation method, where \emph{MPJPE} first drops (from 107.7mm to 105.8mm) before rising.


\begin{table}[H]
\centering
\tabcolsep=8pt
 
    \caption{Comparison results with different denoise network designs on 3DPW dataset with the state-of-the-art pose estimator Pare~\cite{kocabas2021pare} (\emph{MPJPE} is $78.9$mm).}
	{\begin{tabular}{l|c|c|c|c}
			\specialrule{.1em}{.05em}{.05em}
			
			Metrics& No \emph{DenoiseNet} & TCNs~\cite{pavllo20193d} & MLPs~\cite{zeng2021smoothnet} & Ours\\
			\midrule
		    \emph{MPJPE}&79.8&80.5&79.5&\textbf{77.2}\\
        \midrule
        \end{tabular}}
	\label{tab:denoise}
\end{table}


\noindent \textbf{Study on different denoise networks.}
As a baseline framework, we try to explore the performance of different network designs of the two subnets, \textit{DenoiseNet} and \textit{RecoverNet}. In the second step, we use \emph{DenoiseNet} with Transformer architecture to relieve noises from single-frame estimators. We first remove this network to validate its effectiveness. Table~\ref{tab:denoise} shows a $2.1$mm reduction of \emph{MPJPE} without \emph{DenoiseNet}, indicating this step is essential to the recovery of more precise poses. Then, we try to simply replace the Transformer with TCNs~\cite{pavllo20193d}, with zero paddings to make the input and output length the same, and MLPs~\cite{zeng2021smoothnet} along temporal axes. Results show these models are incapable of handling the discrete diverse noises, making the final recovery results worse than the original result.



\begin{table}[H]
\tabcolsep=8pt
	\centering
    \caption{\textbf{Comparison results with different recovery network designs} on 3DPW dataset with the state-of-the-art pose estimator PARE~\cite{kocabas2021pare}(\emph{MPJPE} is $78.9$mm).}
	{\begin{tabular}{l|c|c|c|c|c}

			\specialrule{.1em}{.05em}{.05em}
			
			Metrics&Linear&TCNs~\cite{pavllo20193d} &TCNs w/MLPs& MLPs~\cite{zeng2021smoothnet} &Ours\\
			\midrule
		    \emph{MPJPE}&79.8& 172.3&99.5&78.0&\textbf{77.2}\\
        \midrule
        \end{tabular}}
	\label{tab:recovery}
\end{table}



\noindent \textbf{Study on different recovery methods.}
Lastly, we analyze possible designs of the recovery process in the third step. First, we try the simple Linear interpolation, which shows more significant errors compared with the original PARE since it loses the non-linear motion dynamics. Then, we adopt TCNs~\cite{pavllo20193d}, which have local temporal receptive fields (e.g., $3$) in each layer to recover the missing values with the interval $N$ as $10$, and it leads to the worst results. After adding MLPs~\cite{zeng2021smoothnet} at the last layer to enhance long-term temporal coherence, the error reduces from $172.3$mm to $99.5$mm (by $42.3$\% improvement), but the error is still far from satisfactory. MLPs~\cite{zeng2021smoothnet} can utilize the continuity of temporal dimension to learn non-linear fitting curves from sampled points. Still, they do not aggregate spatial information, which makes them get a slightly worse result. 





\begin{table}[H]
\tabcolsep=8pt
\centering
    \caption{\textbf{Comparison results of different loss weight $\lambda$} on 3DPW dataset with the state-of-the-art pose estimator Pare~\cite{kocabas2021pare}(MPJPE is $78.9$mm).}
	{\begin{tabular}{l|c|c|c|c}

			\specialrule{.1em}{.05em}{.05em}
			
			$\lambda$& 1 & 2 & 5 & 10\\
			\midrule
		    \emph{MPJPE}&78.0&77.6&\textbf{77.2}&77.5\\
        \midrule
        \end{tabular}}
	\label{tab:supp_loss}
\end{table}


\noindent \textbf{Study on different hyper-parameters.}
We also show the effects of hyper-parameters in \name. 
$\lambda$ is used in the loss function to balance the losses between \emph{RecoverNet} and \emph{DenoiseNet}. Results in Table~\ref{tab:supp_loss} show that \emph{MPJPEs} are robust to diverse loss values. Therefore, we set it to $5$ by default.
Moreover, we use the same embedding dimension $C$ and block number $M$ in transformer blocks. We show the results of different $C$ in Table~\ref{tab:supp_c} and $M$ in Table~\ref{tab:supp_m}. Fewer parameters, such as $C=12$ and $M=1$, will lead to performance degradation. As the model becomes deeper (larger $M$) and wider (larger $C$), the performance will meet saturation. By default, we set $C=64$ and $M=5$ for all experiments.
 
 
\begin{table}[H]
\tabcolsep=8pt
    \centering
    \caption{\textbf{Comparison results of different embedding dimension $C$} on 3DPW dataset with the state-of-the-art  pose estimator Pare~\cite{kocabas2021pare}.}
	{\begin{tabular}{l|c|c|c|c|c}

			\specialrule{.1em}{.05em}{.05em}
			
			$C$& 12 & 32 & 64& 128&256\\
			\midrule
		    \emph{MPJPE}&78.0&\textbf{77.2}&77.4&77.6&77.4\\
        \midrule
        \end{tabular}}
	\label{tab:supp_c}
	\centering
\end{table}


\begin{table}[H]
\centering
\tabcolsep=8pt
    \caption{\textbf{Comparison results of different block number $M$} on 3DPW dataset with the state-of-the-art pose estimator Pare~\cite{kocabas2021pare}.}
	{\begin{tabular}{l|c|c|c|c}

			\specialrule{.1em}{.05em}{.05em}
			
			$\lambda$& 1 & 3 & 5 & 10\\
			\midrule
		    \emph{MPJPE}&79.3&77.5&\textbf{77.2}&77.6\\
        \midrule
        \end{tabular}}
	\label{tab:supp_m}
\end{table}





\section{Qualitative Results}
\label{sec:supp_viz}
We demonstrate three typical successful cases of \name to understand why \name uses fewer frames with higher efficiency but gets better performance than existing single-frame methods.

First, cases in Fig.~\ref{fig:supp_viz_1} show that \name can improve not only efficiency but also effectiveness on the 3D body recovery task. The estimated body in the yellow boxes are inputs of \name, where the interval $N$ is set to $10$. Existing SOTA models, like PARE~\cite{kocabas2021pare}, will fail (illustrated in red boxes) when the frames have heavy body occlusions, human interactions, or poor image quality. Interestingly, \name skips some frames (inputs are in yellow boxes) to avoid the negative effect. Therefore, compared with the watch-every-frame model~\cite{kocabas2021pare}, \name may reduce the effects of unreliable and noisy estimated poses by a temporal recovery scheme to obtain the rest of the results. 

\begin{figure}[h]	
\centering
 	
 		\begin{minipage}[t]{0.98\linewidth}
 			\centering         
 			\includegraphics[width=4.5in]{figures/visual_3dpw.pdf}   
 		\end{minipage}
     	
    	
\caption{Visualization results of estimated body recovery from two video sequences with eleven frames in (a) and (e) rows. (b) and (f) are estimated poses from the existing SOTA model PARE~\cite{kocabas2021pare}. We highlight the input poses of \name in the yellow boxes and the high-error poses in the red boxes. (c) and (g) are output poses of our proposed \name, the sampling ratio is 10\% in this framework. (d) and (h) show the ground truth of the corresponding poses. }
\label{fig:supp_viz_1} 
\end{figure} 




\emph{What happens if there are mistakes in the visible frames?} In Fig.~\ref{fig:supp_viz2}, we show the impact of denoising scheme in \name on 2D pose estimation. Given a sliding window of $31$ frames, we mainly demonstrate the visible four frames (highlighted in yellow boxes) with their detected 2D poses by SimplePose~\cite{xiao2018simple}. We observe that there are left-right flipped keypoint detection in the $1_{th}$ and $21_{th}$ frames of Fig.~\ref{fig:supp_viz2}(a), which sometimes happens when the input image is the back of the person. In the $31_{th}$ frame of Fig.~\ref{fig:supp_viz2}(d), high errors occur due to heavy self-occlusion. Our method utilizes long temporal effective receptive fields to denoise the noisy input poses and then recover the clean sparse poses to get the final sequence poses, making the output poses smooth and precise in an efficient way.


\begin{figure}[h]	
\centering
 	
 		\begin{minipage}[t]{0.98\linewidth}
 			\centering         
 			\includegraphics[width=4.5in]{figures/viz_2d.pdf}   
 		\end{minipage}     	
    	
\caption{Visualization the impact of denoising scheme in \name on calibrating the wrong detected poses on four visible frames from the single-frame backbone. We demonstrate the cases via two video sequences and simply ignore the invisible frames. (a) and (d) rows show estimated poses from the popular model SimplePose~\cite{xiao2018simple}, where the sampling interval is 10. Inputs of \name are highlighted in the yellow boxes. (b) and (e) are  output poses of our proposed \name, which can denoise and smooth the input poses by the proposed \emph{DenoiseNet} and \emph{RecoverNet}. (c) and (f) show the ground truth of the corresponding poses. }
\label{fig:supp_viz2} 
\end{figure} 

\emph{In addition to being able to do better motion sequence recovery, can \name still learn motion prior?}
In some cases, even if all visible frames are inaccurate, \name can still recover accurate poses by learning motion prior. As shown in Fig.~\ref{fig:all_wrong_visible}, (a) shows the original video frames of AIST++ with an interval of $10$, which is all the visible frames in one slide window (a sliding window with the length of $101$ has $11$ visible frames). (b) is the corresponding SMPL pose detected by SPIN~\cite{kolotouros2019spin}. Large errors occur in the actor's occluded right arm and hand. In Fig.~\ref{fig:all_wrong_visible}(c), \name can successfully correct the errors and outputs smooth poses leveraging dancing action prior and human motion continuity, which are hard for existing single-frame estimators to estimate occluded body parts. (d) shows the ground truth poses of the video frames.


\begin{figure}[h]	
\centering
 	
 		\begin{minipage}[t]{0.98\linewidth}
 			\centering         
 			\includegraphics[width=4.5in]{figures/wrong_pose_demo.pdf}   
 		\end{minipage}
    	
\caption{Visualization of the recovery results on high-error estimated poses from AIST++ dataset. Only visible frames are shown, which are sampled with an interval of $10$. Images in the row (a) are the original input frames at the $1_{th}$, $11_{th}$, $21_{th}$,...,$101_{th}$ frame. (b), (c), (d) show the poses detected by SPIN~\cite{kolotouros2019spin}, poses recovered by \name, and the corresponding ground truth. }
\label{fig:all_wrong_visible} 
\end{figure} 

For more visualization of 2D pose estimation, 3D pose estimation as well as body recovery, please refer to our website\footnote{Website: \url{https://ailingzeng.site/deciwatch}}.



\section{Failure Case Analyses}
\label{sec:supp_fail}

There are two types of failure cases in \name, which motivates the two corresponding future directions. 
\begin{itemize}

\item \emph{When the sampling rate is lower than the motion frequency of some body parts, it will be difficult to supplement the actual motion.} Human body is articulated. Thus different body parts have different movement frequencies and distribution. For example, the moving frequency and amplitude of hands and feet will be greater than that of the trunk. Our method adopts the same sampling rate for the whole body without considering that the motion distribution of different keypoints is different. In some actions, such as playing the guitar, only the hand will move at high frequency, but most other joints will not move, so the detailed information recovery of hand movement will be lost. Therefore, adaptive sampling strategies, especially on different body parts or keypoints, will be beneficial.

\item \emph{If the estimated poses of most visible frames in the sliding window are in large errors, it is hard for \name to recover the correct poses.} As shown in Fig.~\ref{fig:supp_viz2}, although our method can correct the noisy poses to some extent, this is the advantage of learnable methods. That is, the traditional interpolation method can not fix them. However, if most of the visible poses are noisy, our output may also tend to have similar (but smooth) errors. Thus, it is still essential to continuously improve the performance and robustness of pose estimation methods, especially in extreme scenes. At the same time, we can also consider using additional lightweight information, such as IMUs, to help improve performance.


\end{itemize}








    	





\clearpage
\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{burke2016estimating}
Burke, M., Lasenby, J.: Estimating missing marker positions using low
  dimensional kalman smoothing. Journal of biomechanics  \textbf{49}(9),
  1854--1858 (2016)

\bibitem{cai2021unified}
Cai, Y., Wang, Y., Zhu, Y., Cham, T.J., Cai, J., Yuan, J., Liu, J., Zheng, C.,
  Yan, S., Ding, H., et~al.: A unified 3d human motion synthesis model via
  conditional variational auto-encoder. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 11645--11655 (2021)

\bibitem{cao2019openpose}
Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: realtime
  multi-person 2d pose estimation using part affinity fields. IEEE transactions
  on pattern analysis and machine intelligence  \textbf{43}(1),  172--186
  (2019)

\bibitem{choi2021mobilehumanpose}
Choi, S., Choi, S., Kim, C.: Mobilehumanpose: Toward real-time 3d human pose
  estimation in mobile devices. In: Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition. pp. 2328--2338 (2021)

\bibitem{Chu_2021_CVPR}
Chu, H., Lee, J.H., Lee, Y.C., Hsu, C.H., Li, J.D., Chen, C.S.: Part-aware
  measurement for robust multi-view multi-human 3d pose estimation and
  tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops. pp. 1472--1481 (June 2021)

\bibitem{dai2021fasterpose}
Dai, H., Shi, H., Liu, W., Wang, L., Liu, Y., Mei, T.: Fasterpose: A faster
  simple baseline for human pose estimation. arXiv preprint arXiv:2107.03215
  (2021)

\bibitem{desmarais2021review}
Desmarais, Y., Mottet, D., Slangen, P., Montesinos, P.: A review of 3d human
  pose estimation algorithms for markerless motion capture. Computer Vision and
  Image Understanding  \textbf{212},  103275 (2021)

\bibitem{duan2021single}
Duan, Y., Shi, T., Zou, Z., Lin, Y., Qian, Z., Zhang, B., Yuan, Y.: Single-shot
  motion completion with transformer. arXiv preprint arXiv:2103.00776  (2021)

\bibitem{fan2020adaptive}
Fan, Z., Liu, J., Wang, Y.: Adaptive computationally efficient network for
  monocular 3d hand pose estimation. In: European Conference on Computer
  Vision. pp. 127--144. Springer (2020)

\bibitem{fan2021motion}
Fan, Z., Liu, J., Wang, Y.: Motion adaptive pose estimation from compressed
  videos. In: Proceedings of the IEEE/CVF International Conference on Computer
  Vision. pp. 11719--11728 (2021)

\bibitem{fragkiadaki2015recurrent}
Fragkiadaki, K., Levine, S., Felsen, P., Malik, J.: Recurrent network models
  for human dynamics. In: Proceedings of the IEEE International Conference on
  Computer Vision. pp. 4346--4354 (2015)

\bibitem{gloersen2016predicting}
Gl{\o}ersen, {\O}., Federolf, P.: Predicting missing marker trajectories in
  human motion data using marker intercorrelations. PloS one  \textbf{11}(3),
  e0152616 (2016)

\bibitem{gundavarapu2019structured}
Gundavarapu, N.B., Srivastava, D., Mitra, R., Sharma, A., Jain, A.: Structured
  aleatoric uncertainty in human pose estimation. In: CVPR Workshops. vol.~2,
  p.~2 (2019)

\bibitem{harvey2018recurrent}
Harvey, F.G., Pal, C.: Recurrent transition networks for character locomotion.
  In: SIGGRAPH Asia 2018 Technical Briefs. pp.~1--4 (2018)

\bibitem{harvey2020robust}
Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion
  in-betweening. ACM Transactions on Graphics (TOG)  \textbf{39}(4),  60--1
  (2020)

\bibitem{he2016deep}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
  recognition. In: Proceedings of the IEEE conference on computer vision and
  pattern recognition. pp. 770--778 (2016)

\bibitem{hernandez2019human}
Hernandez, A., Gall, J., Moreno-Noguer, F.: Human motion prediction via
  spatio-temporal inpainting. In: Proceedings of the IEEE/CVF International
  Conference on Computer Vision. pp. 7134--7143 (2019)

\bibitem{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural
  network. arXiv preprint arXiv:1503.02531  (2015)

\bibitem{ho2021render}
Ho, H.I., Chen, X., Song, J., Hilliges, O.: Render in-between: Motion guided
  video synthesis for action interpolation. arXiv preprint arXiv:2111.01029
  (2021)

\bibitem{howarth2010quantitative}
Howarth, S.J., Callaghan, J.P.: Quantitative assessment of the accuracy for
  three interpolation techniques in kinematic analysis of human movement.
  Computer methods in biomechanics and biomedical engineering  \textbf{13}(6),
  847--855 (2010)

\bibitem{hwang2020lightweight}
Hwang, D.H., Kim, S., Monet, N., Koike, H., Bae, S.: Lightweight 3d human pose
  estimation network training using teacher-student learning. In: Proceedings
  of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp.
  479--488 (2020)

\bibitem{ionescu2013human3}
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale
  datasets and predictive methods for 3d human sensing in natural environments.
  IEEE transactions on pattern analysis and machine intelligence
  \textbf{36}(7),  1325--1339 (2013)

\bibitem{jhuang2013towards}
Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.: Towards understanding
  action recognition. In: Proceedings of the IEEE international conference on
  computer vision. pp. 3192--3199 (2013)

\bibitem{ji2020missing}
Ji, L., Liu, R., Zhou, D., Zhang, Q., Wei, X.: Missing data recovery for human
  mocap data based on a-lstm and ls constraint. In: 2020 IEEE 5th International
  Conference on Signal and Image Processing (ICSIP). pp. 729--734. IEEE (2020)

\bibitem{joo2020eft}
Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model
  fitting towards in-the-wild 3d human pose estimation. In: 2021 International
  Conference on 3D Vision (3DV). pp. 42--52. IEEE (2021)

\bibitem{kanazawa2018hmr}
Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of
  human shape and pose. In: Proceedings of the IEEE conference on computer
  vision and pattern recognition. pp. 7122--7131 (2018)

\bibitem{karras2019style}
Karras, T., Laine, S., Aila, T.: A style-based generator architecture for
  generative adversarial networks. In: Proceedings of the IEEE/CVF conference
  on computer vision and pattern recognition. pp. 4401--4410 (2019)

\bibitem{kaufmann2020convolutional}
Kaufmann, M., Aksan, E., Song, J., Pece, F., Ziegler, R., Hilliges, O.:
  Convolutional autoencoders for human motion infilling. In: 2020 International
  Conference on 3D Vision (3DV). pp. 918--927. IEEE (2020)

\bibitem{kocabas2021pare}
Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention
  regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 11127--11137 (2021)

\bibitem{kolotouros2019spin}
Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to
  reconstruct 3d human pose and shape via model-fitting in the loop. In:
  Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
  2252--2261 (2019)

\bibitem{kucherenko2018neural}
Kucherenko, T., Beskow, J., Kjellstr{\"o}m, H.: A neural network approach to
  missing marker reconstruction in human motion capture. arXiv preprint
  arXiv:1803.02665  (2018)

\bibitem{lai2011motion}
Lai, R.Y., Yuen, P.C., Lee, K.K.: Motion capture data completion and denoising
  by singular value thresholding. In: Eurographics (Short Papers). pp. 45--48
  (2011)

\bibitem{li2021rle}
Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu, C.: Human pose
  regression with residual log-likelihood estimation. In: ICCV (2021)

\bibitem{li2021aist}
Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned
  3d dance generation with aist++. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV). pp. 13401--13412 (October
  2021)

\bibitem{li2021online}
Li, Z., Ye, J., Song, M., Huang, Y., Pan, Z.: Online knowledge distillation for
  efficient pose estimation. In: Proceedings of the IEEE/CVF International
  Conference on Computer Vision. pp. 11740--11750 (2021)

\bibitem{liu2021recent}
Liu, W., Bao, Q., Sun, Y., Mei, T.: Recent advances in monocular 2d and 3d
  human pose estimation: A deep learning perspective. arXiv preprint
  arXiv:2104.11536  (2021)

\bibitem{luo2018lstm}
Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang, J., Lin, L.: Lstm
  pose machines. In: Proceedings of the IEEE conference on computer vision and
  pattern recognition. pp. 5207--5215 (2018)

\bibitem{von2018recovering}
von Marcard, T., Henschel, R., Black, M.J., Rosenhahn, B., Pons-Moll, G.:
  Recovering accurate 3d human pose in the wild using imus and a moving camera.
  In: Proceedings of the European Conference on Computer Vision (ECCV). pp.
  601--617 (2018)

\bibitem{martinez2017simple}
Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective
  baseline for 3d human pose estimation. In: Proceedings of the IEEE
  International Conference on Computer Vision. pp. 2640--2649 (2017)

\bibitem{newell2016stacked}
Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose
  estimation. In: European conference on computer vision. pp. 483--499.
  Springer (2016)

\bibitem{nie2019dynamic}
Nie, X., Li, Y., Luo, L., Zhang, N., Feng, J.: Dynamic kernel distillation for
  efficient pose estimation in videos. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 6942--6950 (2019)

\bibitem{osokin2018real}
Osokin, D.: Real-time 2d multi-person pose estimation on cpu: Lightweight
  openpose. arXiv preprint arXiv:1811.12004  (2018)

\bibitem{pavllo20193d}
Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation
  in video with temporal convolutions and semi-supervised training. In:
  Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition. pp. 7753--7762 (2019)

\bibitem{reda2018mocap}
Reda, H.E.A., Benaoumeur, I., Kamel, B., Zoubir, A.F.: Mocap systems and hand
  movement reconstruction using cubic spline. In: 2018 5th International
  Conference on Control, Decision and Information Technologies (CoDIT).
  pp.~1--5. IEEE (2018)

\bibitem{shuai2021adaptively}
Shuai, H., Wu, L., Liu, Q.: Adaptively multi-view and temporal fusing
  transformer for 3d human pose estimation. arXiv preprint arXiv:2110.05092
  (2021)

\bibitem{skurowski2021gap}
Skurowski, P., Pawlyta, M.: Gap reconstruction in optical motion capture
  sequences using neural networks. Sensors  \textbf{21}(18), ~6115 (2021)

\bibitem{SovrasovFlopsCounterConvolutional2022}
Sovrasov, V.: Flops counter for convolutional networks in pytorch framework
  (Mar 2022), \url{https://github.com/sovrasov/flops-counter.pytorch},
  original-date: 2018-08-17T09:54:59Z

\bibitem{sun2019deep}
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation
  learning for human pose estimation. In: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition. pp. 5693--5703 (2019)

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, {\L}., Polosukhin, I.: Attention is all you need. Advances in neural
  information processing systems  \textbf{30} (2017)

\bibitem{wu2011real}
Wu, Q., Boulanger, P.: Real-time estimation of missing markers for
  reconstruction of human motion. In: 2011 XIII Symposium on Virtual Reality.
  pp. 161--168. IEEE (2011)

\bibitem{xiao2018simple}
Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and
  tracking. In: Proceedings of the European conference on computer vision
  (ECCV). pp. 466--481 (2018)

\bibitem{xu2021exploring}
Xu, J., Wang, M., Gong, J., Liu, W., Qian, C., Xie, Y., Ma, L.: Exploring
  versatile prior for human motion via motion frequency guidance. In: 2021
  International Conference on 3D Vision (3DV). pp. 606--616. IEEE (2021)

\bibitem{yan2019convolutional}
Yan, S., Li, Z., Xiong, Y., Yan, H., Lin, D.: Convolutional sequence generation
  for skeleton-based action synthesis. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 4394--4402 (2019)

\bibitem{yu2021lite}
Yu, C., Xiao, B., Gao, C., Yuan, L., Zhang, L., Sang, N., Wang, J.: Lite-hrnet:
  A lightweight high-resolution network. In: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition. pp. 10440--10450
  (2021)

\bibitem{yuan2021glamr}
Yuan, Y., Iqbal, U., Molchanov, P., Kitani, K., Kautz, J.: Glamr: Global
  occlusion-aware human mesh recovery with dynamic cameras. arXiv preprint
  arXiv:2112.01524  (2021)

\bibitem{zeng2020srnet}
Zeng, A., Sun, X., Huang, F., Liu, M., Xu, Q., Lin, S.C.F.: Srnet: Improving
  generalization in 3d human pose estimation with a split-and-recombine
  approach. In: ECCV (2020)

\bibitem{zeng2021learning}
Zeng, A., Sun, X., Yang, L., Zhao, N., Liu, M., Xu, Q.: Learning skeletal graph
  neural networks for hard 3d pose estimation. In: Proceedings of the IEEE
  International Conference on Computer Vision (2021)

\bibitem{zeng2021smoothnet}
Zeng, A., Yang, L., Ju, X., Li, J., Wang, J., Xu, Q.: Smoothnet: A
  plug-and-play network for refining human poses in videos. arXiv preprint
  arXiv:2112.13715  (2021)

\bibitem{zhang2020key}
Zhang, Y., Wang, Y., Camps, O., Sznaier, M.: Key frame proposal network for
  efficient pose estimation in videos. In: European Conference on Computer
  Vision. pp. 609--625. Springer (2020)

\bibitem{zhang2019simple}
Zhang, Z., Tang, J., Wu, G.: Simple and lightweight human pose estimation.
  arXiv preprint arXiv:1911.10346  (2019)

\bibitem{zhao2021estimating}
Zhao, L., Wang, N., Gong, C., Yang, J., Gao, X.: Estimating human pose
  efficiently by parallel pyramid networks. IEEE Transactions on Image
  Processing  \textbf{30},  6785--6800 (2021)

\bibitem{zheng2021lightweight}
Zheng, C., Mendieta, M., Wang, P., Lu, A., Chen, C.: A lightweight graph
  transformer network for human mesh reconstruction from 2d human pose. arXiv
  preprint arXiv:2111.12696  (2021)

\bibitem{zheng2020deep}
Zheng, C., Wu, W., Yang, T., Zhu, S., Chen, C., Liu, R., Shen, J., Kehtarnavaz,
  N., Shah, M.: Deep learning-based human pose estimation: A survey. arXiv
  preprint arXiv:2012.13392  (2020)

\bibitem{zheng20213d}
Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding, Z.: 3d human pose
  estimation with spatial and temporal transformers. In: Proceedings of the
  IEEE/CVF International Conference on Computer Vision. pp. 11656--11665 (2021)

\end{thebibliography}
 \bibliographystyle{splncs04}

\end{document}