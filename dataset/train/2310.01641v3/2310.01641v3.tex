\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}

\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}




\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{caption}
\captionsetup[figure]{justification=raggedright, singlelinecheck=false}

\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{subcaption}
\usepackage{hyperref}

\usepackage{rotating}

\usepackage{color} 

\newcommand{\INPUT}{\item[\textbf{Input:}]}
\newcommand{\OUTPUT}{\item[\textbf{Output:}]}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\begin{document}

\title{You Only Look at Once for Real-Time and Generic Multi-Task}

\author{Jiayuan Wang, \IEEEmembership{Graduate Student Member,~IEEE}, Q. M. Jonathan Wu,~\IEEEmembership{Senior Member,~IEEE},\\ and Ning Zhang,~\IEEEmembership{Senior Member,~IEEE}
\thanks{Manuscript received June 28, 2022; revised August 26, 2015.\\
Corresponding author: Q. M. Jonathan Wu }
\thanks{Jiayuan Wang, Q. M. Jonathan Wu and Ning Zhang are with the Centre for Computer Vision and Deep Learning, Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON N9B 3P4, Canada (e-mails: wang621@uwindsor.ca, jwu@uwindsor.ca and ning.zhang@uwindsor.ca)}}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\\mathcal{L}_{\text {det}}\mathcal{L}_{\text {segda}}\mathcal{L}_{\text {segll}}\mathcal{L}_{\text {dec}}\lambda_{\text {BCE }}\lambda_{\text {DFL }}\lambda_{\text {CIoU }}x_ny_nyy_{i+1}yy_{i}ybb^{g t}\rhocv\alphahw^{g t}h^{g t}\mathcal{L}_{\text {segda}}\mathcal{L}_{\text {segll}}\mathcal{L}_{\text {seg}}\mathcal{L}_{\text {FL }}\mathcal{L}_{\text {TL }}\lambda_{\text {FL }}\lambda_{\text {TL }}p_ty\alpha_t\gamma\alpha\betaF\Theta\taux_{val}\phiyF(x; \Theta)\Thetaxy\tau\hat{y}\text{Sensitivity} = \frac{TP}{TP + FN}\text{Specificity} = \frac{TN}{TN + FP}lr\lambda_{\text {FL}}=24.0\lambda_{\text {TL}}=8.0\lambda_{\text {DFL}}=1.5\lambda_{\text {CIoU}}=7.5\lambda_{\text {BCE}}=0.5\alpha=0.7\beta=0.3\mathcal{L}_{\text {TL }}\alpha_t=0.25\gamma=2\mathcal{L}_{\text {FL }}\mathcal{L}_{det}$. This means our model is more conservative and focuses on achieving a higher mAP at the expense of recall performance. We believe that mAP50 better reflects the comprehensive performance in the detection task. Besides, our model achieves real-time performance, whereas YOLOP cannot when the batch size is set to 1 according to Table~\ref{tab:inference}. 

\begin{table}[h]
    \centering
    \caption{Traffic object detection results}
    \label{tab:object_detect}
    \begin{tabularx}{\linewidth}{YYY}
        \toprule
        Model & Recall (\%) & mAP50 (\%) \\
        \midrule
        MultiNet & 81.3 & 60.2 \\
        DLT-Net & \textbf{89.4} & 68.4 \\
        Faster R-CNN & 81.2 & 64.9 \\
        YOLOv5s & 86.8 & 77.2 \\
        YOLOv8n(det) & 82.2 & 75.1 \\
        YOLOP & 88.6 & 76.5 \\
        A-YOLOM(n) & 85.3 & 78.0\\
        A-YOLOM(s) & 86.9 & \textbf{81.1} \\
        \bottomrule
    \end{tabularx}
\end{table}


For the drivable area segmentation task, Table~\ref{tab:da_results} provides the quantitative results. Our model achieved the second and third best performances in terms of mIoU. YOLOP outperforms our model because they customized the loss function for the segmentation task. In terms of mIoU, A-YOLOM(n) and A-YOLOM(s) trail YOLOP by 1\% and 0.5\% respectively. We believe this result is acceptable. While our model might exhibit a slight loss in performance, it is more flexible and faster. Additionally, our model performs much better than other models, such as YOLOv8n(seg), MultiNet, and PSPNet. It's worth noting that while YOLOv8n(seg) is faster than our model, its deployment cost is higher, and its performance is significantly inferior to ours.


\begin{table}[h]
    \centering
    \caption{Drivable area segmentation results}
    \label{tab:da_results}
    \begin{tabularx}{\linewidth}{YY}
        \toprule
        Model & mIoU (\%) \\
        \midrule
        MultiNet & 71.6 \\
        DLT-Net & 72.1 \\
        PSPNet & 89.6 \\
        YOLOv8n(seg) & 78.1 \\
        YOLOP & \textbf{91.6} \\
        A-YOLOM(n) & 90.5\\
        A-YOLOM(s) & 91.0 \\
        \bottomrule
    \end{tabularx}
\end{table}

For the lane line segmentation task, Table~\ref{tab:ll_results} provides the quantitative results. A-YOLOM(s) achieves the best performance in terms of both accuracy and IoU. Specifically, it delivers competitive results in accuracy and is 2.3\% higher in IoU compared to YOLOP. It's worth noting that our model maintains the same structure and loss function across all segmentation tasks, eliminating the need for adjustments when encountering a new segmentation task. Additionally, YOLOv8 (seg) is slightly inferior to ours. Based on results in Table~\ref{tab:da_results} and Table~\ref{tab:ll_results}, we believe that our proposed neck and head are better suited for the segmentation task compared to YOLOv8 (seg). Since we use balanced accuracy for evaluation, Enet, SCNN, and ENet-SAD employ different accuracy calculation approaches. Therefore, we cannot directly compare our accuracy with theirs. However, our results in IoU metrics are significantly better than theirs. This alone sufficiently demonstrates the superior performance of our model compared to theirs.

\begin{table}[h]
    \centering
    \caption{Lane line segmentation results}
    \label{tab:ll_results}
    \begin{tabularx}{\linewidth}{YYY}
        \toprule
        Model & Accuracy (\%) & IoU (\%) \\
        \midrule
        Enet & N/A & 14.64 \\
        SCNN & N/A & 15.84 \\
        ENet-SAD & N/A & 16.02 \\
        YOLOv8n(seg) & 80.5 & 22.9 \\
        YOLOP & 84.8 & 26.5 \\
        A-YOLOM(n) & 81.3 & 28.2 \\
        A-YOLOM(s) & \textbf{84.9} & \textbf{28.8} \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Visualization}
\label{subsubsec: Visualization}
\begin{figure*}[!h]
    \centering

\begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{YOLOP}
        \vspace{0.6cm} \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/sunny/b394dd7a-fe45748a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/sunny/b3f0cdab-d5954c9a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/sunny/b97b9ee8-ea15f29a.jpg}
    \end{subfigure}

    \medskip

\begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{A-YOLOM(n)}
        \vspace{0.2cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/sunny/b394dd7a-fe45748a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/sunny/b3f0cdab-d5954c9a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/sunny/b97b9ee8-ea15f29a.jpg}
    \end{subfigure}

    \medskip

\begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{ A-YOLOM(s)}
        \vspace{0.1cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/sunny/b394dd7a-fe45748a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/sunny/b3f0cdab-d5954c9a.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/sunny/b97b9ee8-ea15f29a.jpg}
    \end{subfigure}

    \caption{Visual Comparison of Results on a Sunny Day}
    \label{fig:sunny_day_comparison}
\end{figure*}
This section presents a visual comparison between YOLOP and our model. We evaluate performance not only in favourable weather conditions but also under adverse conditions, including strong sunlight, nighttime, rain, and snow. We will analyze each scene in turn. 

Figure~\ref{fig:sunny_day_comparison} displays the results from a sunny day. As we know, strong sunlight can affect a driver's vision. Similarly, it impacts the model's performance. This poses challenges for the drivable area and lane line segmentation performance. In this challenging scenario, our model outshines YOLOP. Specifically, under the conditions of intense sunlight (as seen in the left and right images), our model delivers accurate lane line predictions and provides smoother indications of drivable areas. Additionally, our model exhibits greater accuracy in detecting smaller and farther vehicles. As observed in the middle and right images, our model successfully detects vehicles located far in the distance, both on the road in opposite driving directions and near the house. We believe that our model outperformed YOLOP in this scene according to the visualization results.

\begin{figure*}[!h]
    \centering

    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{YOLOP}
        \vspace{0.6cm} 
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/night/b3cd4bfe-93377084.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/night/b3d46fca-a8bd9b5c.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/night/b3dd5345-15bbf8ed.jpg}
    \end{subfigure}
    
    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{A-YOLOM(n)}
        \vspace{0.2cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/night/b3cd4bfe-93377084.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/night/b3d46fca-a8bd9b5c.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/night/b3dd5345-15bbf8ed.jpg}
    \end{subfigure}

    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{ A-YOLOM(s)}
        \vspace{0.1cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/night/b3cd4bfe-93377084.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/night/b3d46fca-a8bd9b5c.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/night/b3dd5345-15bbf8ed.jpg}
    \end{subfigure}
    \caption{Visual Comparison of Results at Night}
    \label{fig:night_comparison}
\end{figure*}

Figure~\ref{fig:night_comparison} displays the results from night scenes. In nighttime scenes, the image quality decreases due to limited lighting and glare from oncoming vehicles. This poses a challenge for the model to make accurate predictions. Under this challenge, our model consistently produces more accurate and smoother predictions for both lane lines and drivable areas. A-YOLOM(s) excels not only in the segmentation task but also in the detection task. Specifically, it surpasses A-YOLOM(n) by accurately detecting vehicles travelling in the opposite direction, even amidst glare, and holds its own against YOLOP. Based on the detection results from the right image, YOLOP slightly outperforms our model in detecting distant vehicles at night. Nonetheless, our model produces significantly better segmentation results compared to YOLOP. Especially in the middle image, YOLOP mistakenly predicts the opposite lane as a drivable area. Such mispredictions are extremely hazardous for autonomous driving tasks. 

\begin{figure*}[!h]
    \centering
    
    \begin{subfigure}[b]{0.05\textwidth}
            \centering
            \rotatebox{90}{YOLOP}
            \vspace{0.6cm} 
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/rain/b1cac6a7-04e33135.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/rain/b79d7247-6750fc90.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/rain/b8afaffa-5ec67bbb.jpg}
    \end{subfigure}
    
    \medskip
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{A-YOLOM(n)}
        \vspace{0.2cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/rain/b1cac6a7-04e33135.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/rain/b79d7247-6750fc90.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/rain/b8afaffa-5ec67bbb.jpg}
    \end{subfigure}

    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{ A-YOLOM(s)}
        \vspace{0.1cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/rain/b1cac6a7-04e33135.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/rain/b79d7247-6750fc90.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/rain/b8afaffa-5ec67bbb.jpg}
    \end{subfigure}
    \caption{Visual Comparison of Results on Rainy Day}
    \label{fig:rainy_day_comparison}
\end{figure*}


\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{YOLOP}
        \vspace{0.6cm} 
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/snow/b3a84a0f-61844c69.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/snow/b7bb64a4-885f7346.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/YOLOP/snow/b936e35e-e46a87e6.jpg}
    \end{subfigure}
    
    \medskip

    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{A-YOLOM(n)}
        \vspace{0.2cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/snow/b3a84a0f-61844c69.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/snow/b7bb64a4-885f7346.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMn/snow/b936e35e-e46a87e6.jpg}
    \end{subfigure}

    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{ A-YOLOM(s)}
        \vspace{0.1cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/snow/b3a84a0f-61844c69.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/snow/b7bb64a4-885f7346.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/visualization/A-YOLOMs/snow/b936e35e-e46a87e6.jpg}
    \end{subfigure}
    \caption{Visual Comparison of Results on Snow Day}
    \label{fig:snow_day_comparison}
\end{figure*}

Figure~\ref{fig:rainy_day_comparison} displays the results from a rainy day. Rain will increase road surface reflection, thereby impacting a driver's judgment. This issue also manifests in deep learning models. Moreover, raindrops on the windshield, due to their refractive and scattering effects, can blur the entire image, creating considerable challenges for the model's predictions. In the results from the left image, we can observe that YOLOP cannot accurately segment the drivable area due to reflections on the road surface. A similar situation is evident in the middle scene. YOLOP even struggles to distinguish between the car hood and the road. However, our model still delivers outstanding performance in this scenario. Especially for A-YOLOM(s), it is less affected by the reflections on the road surface. Our model also demonstrates superior lane line detection compared to YOLOP, as evidenced by the right image. However, in this scene, YOLOP outperforms us in detection tasks. As observed in the left image, YOLOP can detect more vehicles at a farther distance.


Figure~\ref{fig:snow_day_comparison} displays the results from a snow day. In snowy conditions, accumulated snow can obscure the road or lanes, posing additional challenges for models. Some accumulated snow is cleared into snow mounds on the side of the road, which poses challenges for the model's vehicle prediction. For instance, on the left in the middle image, YOLOP mistakenly detects a snow mound as a vehicle. If there's a small snow mound on the road, vehicles can typically pass over it. Misidentifying such little snow mounds as stationary vehicles could be dangerous. Therefore, a higher recall is not always better. It's crucial to strike a balance between recall and precision. In all three results presented, our model significantly outperforms YOLOP in the segmentation task, with A-YOLOM(s) being particularly noteworthy. However, based on the results from the right image, our model lags behind YOLOP in detecting distant vehicles, particularly when using the A-YOLOM(n).

In various weather conditions, our model consistently delivers more accurate and smoother segmentation results. Our model's ability to detect distant and small targets falls slightly short compared to YOLOP in adverse weather conditions.

\subsection{Ablation Studies}
\label{subsec: Ablation Studies}
\begin{figure*}[!h]
    \centering
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{YOLOP}
        \vspace{0.6cm} 
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/YOLOP_night_frame_0590.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/YOLOP_frame_0182_daytime.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/YOLOP_highway_frame_0377.jpg}
    \end{subfigure}
    
    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{A-YOLOM(n)}
        \vspace{0.2cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4_nightframe_0590.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4_frame_0182_daytime.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4_highway_frame_0377.jpg}
    \end{subfigure}

    \medskip
    
    \begin{subfigure}[b]{0.05\textwidth}
        \centering
        \rotatebox{90}{ A-YOLOM(s)}
        \vspace{0.1cm}
    \end{subfigure}\begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4s_night_frame_0590.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4s_frame_0182_daytime.jpg}
    \end{subfigure}\hspace{0.5cm}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/real_data/v4s_highway_frame_0377.jpg}
    \end{subfigure}
    \caption{Real Road Results}
    \label{fig:real road}
\end{figure*}

\begin{table*}[!h]
\centering
\caption{The ablation study for the adaptive concatenation module}
\label{tab:training_methods_comparison}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X}
\toprule
Training method & Recall (\%) & mAP50 (\%) & mIoU (\%) & Accuracy (\%) & IoU (\%) \\
\midrule
YOLOM(n) & 85.2 & 77.7 & 90.6 & 80.8 & 26.7 \\
A-YOLOM(n) & 85.3 & 78 & 90.5 & 81.3 & 28.2 \\
YOLOM(s) & 86.9 & 81.1 & 90.9 & 83.9 & 28.2 \\
A-YOLOM(s) & 86.9 & 81.1 & 91 & 84.9 & 28.8 \\
\bottomrule
\end{tabularx}
\end{table*}

In this subsection, we present two ablation studies to elucidate the effects of our proposed adaptive concatenation, as well as the impact of the segmentation neck and head on the overall model.

\subsubsection{Adaptive concatenation module}
\label{subsubsec: Adaptive concatenation module}
To assess the impact of our proposed adaptive concatenation module, we compare performance with and without this module. Specifically, YOLOM(n) and YOLOM(s) act as baselines, representing two distinct experiment groups with different backbones. We've carefully designed the segmentation neck structure for them to address the demands of particular segmentation tasks. However, both A-YOLOM(n) and A-YOLOM(s) include the adaptive concatenation module. Their segmentation neck structure is entirely identical and without well-design. The results are presented in Table~\ref{tab:training_methods_comparison}. When comparing A-YOLOM(n) with YOLOM(n), we found comparable performance in detection and drivable area segmentation. However, both lane line accuracy and IoU showed significant improvements. Similarly, when comparing A-YOLOM(s) to YOLOM(s), we observe the same trend of improvement. These results indicated that our adaptive concatenation module could adaptively concatenate features without manual design and achieve a similar or better performance than well-designed segmentation heads, further enhancing the modelâ€™s generality.



\subsubsection{Multi-task model and segmentation structure}
\label{subsubsec: Multi-task model and segmentation structure}
To evaluate the influence of the multi-task approach for each individual task, as well as our proposed segmentation neck and head structures, we compare performance and head parameters in YOLOv8(segda), YOLOv8(segll), YOLOv8(multi), and YOLOM(n) within the domain of segmentation tasks. YOLOM(n) have a similar backbone network with YOLOv8, making this comparison fair and make sense. The results are presented in Table~\ref{tab:segmentation_comparison}. YOLOv8(segda) and YOLOv8(segll) implement the drivable area and lane line segmentation tasks separately. YOLOv8 (multi) is an integrated multi-task learning model that combines YOLOv8(segda), YOLOv8(segll), and YOLOv8(det) neck and head structure into one shared backbone. We observed there's a significant improvement in performance. This indicated that multi-task learning can reciprocally enhance the performance of individual tasks. 

\begin{table}[!h]
    \centering
    \caption{Results of different Multi-task model and segmentation structure}
    \label{tab:segmentation_comparison}
    \begin{tabularx}{\linewidth}{YYYYY}
    \toprule
    Model & Parameters & mIoU (\%) & \mbox{Accuracy (\%)} & IoU (\%) \\
    \midrule
    YOLOv8(segda) & 1004275 & 78.1 & - & - \\
    YOLOv8(segll) & 1004275 & - & 80.5 & 22.9 \\
    YOLOv8(multi) & 2008550 & 84.2 & 81.7 & 24.3 \\
    YOLOM(n) & 15880 & 90.6 & 80.8 & 26.7 \\
    \bottomrule
    \end{tabularx}
\end{table}

Compared to YOLOv8(multi), YOLOM(n) has our carefully designed neck structure tailored for segmentation tasks. Additionally, it boasts a significantly more lightweight head structure, having only 0.008 times the complexity. This marked improvement stems from our unique head design, which only relies on a series of convolutional layers, directly outputting a mask without the need for additional protos information. YOLOM(n) achieves impressive results in both mIoU and IoU for the drivable area and lane line tasks, respectively. Furthermore, we deliver competitive accuracy in the lane line task. These results evident our proposed neck and head innovations have achieved commendable results with minimal parameters and performance overhead.


\subsection{Real roads experiments}
\label{subsec: Real roads experiments}

This section primarily discusses the experiments conducted in real road datasets. Specifically, we capture several videos using a dash camera and then convert them frame by frame into images to make predictions using our model and YOLOP. Each converted image dataset consists of 1428 images with a resolution of 1280x720. These images encompass three scenarios: highway, nighttime, and daytime. Figure~\ref{fig:real road} displays the results from the real road dataset. However, our model consistently maintains relatively stable performance, particularly for the A-YOLOM(s). In all tasks, A-YOLOM(s) outperforms YOLOP. Meanwhile, A-YOLOM(n) is slightly inferior only in detection tasks compared to YOLOP. ADS must be capable of operating smoothly in unfamiliar scenarios. This is crucial. The results on real roads demonstrate that our model can meet the actual needs of autonomous vehicles on real roads.





\section{CONCLUSION}
\label{sec: CONCLUSION}

In this study, we primarily introduced an end-to-end lightweight multi-task model designed for real-time autonomous driving applications. The strength of a multi-task model lies in its capacity for tasks to implicitly benefit from and enhance each other, culminating in holistic performance improvements. To further enhance the model's capabilities, we integrated an adaptive concatenation module and proposed the unified loss functions for each type of task, improving our model's flexibility. When compared to other state-of-the-art real-time multi-task methods using the BDD100k dataset, our model not only demonstrated superior visualization results but also showcased a higher FPS. The higher FPS is attributed to our streamlined segmentation head design, which optimizes both the number of parameters and inference time. Moreover, real-road dataset evaluations underline our model's robustness in novel environments, positioning it ahead of the competition. In the future, we aspire to encapsulate more autonomous driving tasks within our model and optimize it for edge device deployments.

\bibliographystyle{IEEEtran}

\bibliography{ref}




\end{document}
