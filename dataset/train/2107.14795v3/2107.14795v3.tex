
\documentclass{article} \usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage{wrapfig}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{xspace}
\usepackage{siunitx}
\usepackage[normalem]{ulem}

\usepackage{romannum}
\usepackage{selectp}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\newcommand{\carl}[1]{\textcolor{blue}{{[\textbf{Carl}: #1]}}}
\newcommand{\drew}[1]{\textcolor{orange}{{[\textbf{Drew}: #1]}}}

\newcommand{\ourmodel}{Perceiver IO\xspace}

\title{\ourmodel: A General Architecture \\for Structured Inputs \& Outputs}

\author{Andrew Jaegle, 
  Sebastian Borgeaud, 
  Jean-Baptiste Alayrac, 
  Carl Doersch,
  Catalin Ionescu,
  \AND
  David Ding, 
  Skanda Koppula,
  Daniel Zoran,
  Andrew Brock, 
  Evan Shelhamer, 
  Olivier H\'{e}naff, 
  \AND
  Matthew M. Botvinick, 
  Andrew Zisserman, 
  Oriol Vinyals, 
  Jo\~{a}o Carreira \\ \\
  DeepMind\\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose \ourmodel{}, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.
The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft \Romannum{2}. As highlights, \ourmodel{} outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.
\end{abstract}


\section{Introduction}

Humans have a remarkable ability to take in data from many sources, integrate it seamlessly, and deploy it in the service of a range of goals. Most machine learning research focuses on building bespoke systems to handle the stereotyped inputs and outputs associated with a single task. This is true even for models that handle multiple modalities. A typical approach independently processes each input with a modality specific architecture (for example using a 2D ResNet~\citep{he2016deep} for vision and a Transformer~\citep{vaswani2017attention} for language), integrates them afterwards using a third fusion network, and reads out the result in a task-specific manner. The complexity of systems like this can grow dramatically as the inputs or outputs grow more diverse (e.g.~\citealt{abramson2020imitating,vinyals2019grandmaster,ramesh2021zero}), and the structure of a task's inputs and outputs may place strong constraints on how data is processed, making adaptation to new settings difficult.

Is the development of problem-specific models for each new set of inputs and outputs unavoidable? Life would be drastically simpler if a single neural network architecture could handle a wide variety of both input modalities and output tasks. In this work, we propose such an architecture, with the ultimate goal of building a network that can easily integrate and transform arbitrary information for arbitrary tasks. Our starting point is the Perceiver~\citep{jaegle2021perceiver}, an architecture which has demonstrated a remarkable ability to handle data from many modalities with no changes to the network architecture. The Perceiver uses attention to map inputs of a wide range of modalities to a fixed-size latent space that is further processed by a deep, fully attentional network. This process decouples the bulk of the network's processing from the size and modality-specific details of the input, allowing it to scale to large and multimodal data.

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/domain_overview.pdf}
    \caption{The \ourmodel{} architecture can be used on domains with a wide variety of input and output spaces, including multi-task language understanding, dense visual tasks like optical flow, hybrid dense/sparse multimodal tasks such as video+audio+class autoencoding, and tasks with symbolic outputs like StarCraft \Romannum{2}. See Tables~\ref{tab:input_output_details} and~\ref{tab:pos_encodings_queries} for details of all domains considered here.}
    \label{fig:domain_overview}
    \vspace{-12pt}
\end{figure*}

But the Perceiver can only handle simple output spaces like classification. Much of the complexity of real-world tasks comes from the variety, size, and structure of their \textit{outputs}, and in this regard the original Perceiver can't be considered general purpose. In this work, we develop a mechanism for decoding structured outputs -- language, optical flow fields, audiovisual sequences, symbolic unordered sets, etc. -- directly from the Perceiver latent space, which allows the model to handle a host of new domains without sacrificing the benefits of deep, domain-agnostic processing. To do this, we produce each output by attending to the latent array using an \textit{output query} that specifies the semantics of that particular output. For example if we wanted the model to predict optical flow on one particular pixel we could compose a query from the pixel's xy coordinates plus an optical flow task embedding: the model would then attend using the query and produce a single flow vector. As a result, our architecture can produce many outputs, each with arbitrary shape and structure, and yet the latent features in our architecture remain agnostic to the shape and structure of the outputs.
 
\ourmodel{} does this using a fully attentional read-process-write architecture: inputs are encoded (read) to a latent space, the latent representation is refined (process) via many layers of processing, and the latent space is decoded (write) to produce outputs. This approach inherits the best features of both Transformers -- which leverage domain agnostic primitives for nonlocal processing of inputs -- and the encoder-decoder architectures (e.g.~\citealt{ronnenberger2015convolutional, newell2016stacked}) that are in widespread use in high-bandwidth domains such as computer vision or multimodal processing. This approach allows us to decouple the size of elements used for the bulk of the computation (the latent) from the size of the input and output spaces, while making minimal assumptions about the spatial or locality structure of the input and output. 

\ourmodel{}'s decoding procedure uses an attention mechanism to map from latents to arbitrarily sized and structured outputs using a querying system that can flexibly specify the semantics needed for outputs on a wide range of domains, including dense and multitask settings. This decoder allows \ourmodel{} to serve as a drop-in replacement for a wide range of specialist networks currently in use on a set of challenging domains, while improving performance on tasks like classification that could be handled by the Perceiver.

The proposed architecture can be applied with unprecedented levels of generality. \ourmodel{} can replace the Transformers used in BERT~\citep{devlin2019bert} and AlphaStar~\citep{vinyals2019grandmaster}. At the same time, \ourmodel{} produces state-of-the-art results on the Sintel optical flow benchmark~\citep{butler2012naturalistic} and good results on ImageNet image classification~\citep{deng2009imagenet}. \ourmodel{} produces compelling results even when handling highly diverse multimodal data, such as on joint \{video, audio, label\} autoencoding in Kinetics~\citep{smaira2020short} and joint audio-video classification on AudioSet~\citep{gemmeke2017audio}. \ourmodel{} allows us to simplify pipelines and remove domain-specific assumptions: we process language without tokenizers without a performance or speed hit, fine-tune on multiple classification tasks simultaneously and without the need for  tokens (Sec.~\ref{sec:language}), estimate optical flow without relying on explicit architectural features for multiscale correspondence (Sec.~\ref{sec:flow}), learn joint representations of video, audio, and labels without separate network trunks (Sec.~\ref{sec:multimodal}), and perform image classification with no information about the 2D structure of images (Sec.~\ref{sec:imagenet}).

\section{Related Work}

Neural network research has long sought architectures that can handle large, arbitrarily structured inputs and outputs. Autoencoding~\citep{hinton1994autoencoders} was among the first attempts to build representation which could encode and reproduce high-dimensional inputs like images. As hardware grew more powerful, neural nets led to breakthroughs in image understanding~\citep{krizhevsky2012imagenet, zeiler2014visualizing, szegedy2015going} and interest intensified: autoregressive models that could process and complete samples of handwriting were developed~\citep{graves2013generating}, and new convolutional network designs led to good results in structured output spaces like semantic segmentation~\citep{farabet2012learning,long2015fully,ronnenberger2015convolutional}, pose estimation~\citep{toshev2014deep}, detection~\citep{sermanet2014overfeat}, captioning~\citep{you2016image}, and optical flow~\citep{fischer2015flownet}. At the same time, natural language applications research has made extensive progressive in capturing the structured nature of language, typically via autoregressive models~\citep{collobert2011natural,sutskever2014sequence,vaswani2017attention,radford2019language, brown2020language} or context prediction~\citep{mikolov2013distributed, pennington2014glove, devlin2019bert}.

Similar to our work, several groups have proposed to solve tasks in multiple domains (e.g.~\citealt{kaiser2017one, alayrac2020self,akbari2021vatt}), but typically across a fixed and predefined set of modalities by means of domain-specific networks. Although single-task specialist networks remain dominant in vision, multi-task learning has become popular~\citep{misra2016cross,doersch2017multi,kokkinos2017ubernet,zamir2018taskonomy} and individual models achieve generality in a restricted domain: e.g. Mask-RCNN~\citep{he2017mask} handles object detection, segmentation, and pose estimation. In language, training or evaluation on multiple tasks has also become common~\citep{collobert2008unified,luong2015multi,devlin2019bert,liu2019roberta, raffell2020exploring}. Several groups have demonstrated that Transformers (originally designed for language) can be used or adapted to non-language tasks (e.g.~\citealt{chen2020generative, lu2021pretrained}), but the limited scalability of Transformers limits their usefulness as general-purpose architectures. 

Several groups have proposed to use attention to manipulate the size of arrays or to introduce bottlenecks in processing. Set Transformers and related work~\citep{lee2019set,goyal2021coordination} use a learned query (``inducing points'') to induce local bottlenecks by mapping a set back and forth from a set with fewer elements and learned decoder queries (``seed vectors'') to map to outputs (``pooling by multiheaded attention''). Each layer of these networks has complexity linear in the input size, while Perceivers use a deep latent network with complexity independent of the input and output. Our work uses attention over inputs and outputs of different sizes in part to produce an efficient attention architecture, and several other efficient attention architectures have been proposed, largely for language or small-scale problems (e.g.~\citealt{xiong2021nystromformer, wang2020linformer, tay2021synthesizer, beltagy2020longformer} and see~\citealt{tay2020long}). The focus of our work is developing an architecture that is efficient and also performs well in many settings with a wide range of inputs and outputs. Several works use attention to process latent spaces that interface with input/output data using task- or domain-specific architectures~\citep{carion2020endtoend, locatello2020object, wang2021maxdeeplab}, and cross-attention itself is widely used to produce outputs in of a different size or structure from inputs~\citep{dai2019transformerxl,desai2021virtex,miech2021thinking, vaswani2017attention, raffell2020exploring,santoro2018relational, hudson2021generative, ma2021luna}. \ourmodel{} builds on this body of work to produce a general purpose architecture that can be easily and widely applied.

\section{The \ourmodel{} architecture}

The \ourmodel{} architecture builds on the Perceiver~\citep{jaegle2021perceiver}, which achieved its cross-domain generality by assuming that its input is a simple 2D byte array: a set of elements (which might be pixels or patches in vision, characters or words in language, or some form of embedding, learned or otherwise), each described by a feature vector.  The model then encodes information about the input array using a smaller number of latent feature vectors, using Transformer-style attention, followed by iterative processing and a final aggregation down to a category label.

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=\linewidth]{figures/arch_fig.pdf}
    \caption{The \ourmodel{} architecture. \ourmodel{} maps arbitrary input arrays to arbitrary output arrays in a domain agnostic process. The bulk of the computation happens in a latent space whose size is typically smaller than the inputs and outputs, which makes the process computationally tractable even for very large inputs \& outputs. See Fig.~\ref{fig:encode_process_decode} for a more detailed look at encode, process, and decode attention.}
    \label{fig:perceiver_module}
    \vspace{-20pt}
\end{figure*}

Rather than output a single category, \ourmodel{} aims to have the same level of generality with respect to its \emph{outputs} as the Perceiver has with respect to its \emph{inputs}: that is, it should produce arbitrary output arrays. We can predict each element of the output array using another attention module by \emph{querying} the latent array using a query feature vector unique to the desired output element.  
In other words, we define a query array with the same number of elements as the desired output. The queries may be hand-designed, learned embeddings, or a simple function of the input. They attend to the latents to yield an output  array of the desired shape.

\subsection{Encoding, processing, and decoding}
Fig.~\ref{fig:perceiver_module} illustrates the \ourmodel{}. We first \textbf{encode} by applying an attention module that maps input arrays  to arrays in a latent space . We next \textbf{process} the latents  by applying a series of modules that take in and return arrays in this latent space. 
Finally, we \textbf{decode} by applying an attention module that maps latent arrays to output arrays . , , , and  are properties of the task data and can be very large (Tab.~\ref{tab:input_output_details}), while  and  are hyperparameters and can be chosen to make model computation tractable. Following the design of the Perceiver, we implement each of the architecture's components using Transformer-style attention modules. 

Each of these modules applies a global query-key-value (QKV) attention operation followed by a multi-layer perceptron (MLP). As usual in Transformer-style architectures, we apply the MLP independently to each element of the index dimension. Both encoder and decoder take in two input arrays, the first used as input to the module's key and value networks, and the second used as input to the module's query network. The module's output has the same index dimension (the same number of elements) as the query input.

The \ourmodel{} architecture builds on primitives similar to those in Transformers. Why aren't Transformers all you need? Transformers scale very poorly in both compute and memory~\citep{tay2020efficient}. Because Transformers deploy attention modules homogeneously throughout its architecture, using its full input to generate queries and keys at every layer. This means each layer scales quadratically in compute and memory, which makes it impossible to apply Transformers on high-dimensional data like images without some form of preprocessing. Even on domains like language where Transformers shine, preprocessing (e.g. tokenization) is often needed to scale beyond short input sequences. \ourmodel{} uses attention non-homogeneously by mapping inputs to a latent space, processing in that latent space, and decoding to an output space. \ourmodel{} has no quadratic dependence on the input or output size: encoder and decoder attention modules depend linearly on the input and output size (respectively), while latent attention is independent of both input and output sizes (Sec.~\ref{sec:complexity}). Because of the corresponding reduction in compute and memory requirements, \ourmodel{} scales to much larger inputs and outputs. While Transformers are typically used in settings with data preprocessed to contain at most a few thousand dimensions~\citep{brown2020language, raffell2020exploring}, we show good results on domains with hundreds of thousands of dimensions.

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=0.95\linewidth]{figures/query_construction.pdf}
    \vspace{-5pt}
    \caption{We construct queries with output-specific features to produce outputs with different semantics. For settings where each output point differs only in its position, like language, a position embedding can be used. Input features for the target output can also be used to query, either alone (as for StarCraft \Romannum{2}) or alongside position features (as for flow). For multi-\{task, modal\} settings we use one embedding for each \{task, modality\} instead of each position. A single learned embedding suffices for simple classification tasks, like ImageNet. For tasks with heterogeneous outputs like multimodal autoencoding, features that are specific to some queries (like xy position) can be combined with modality embeddings, which also pad embeddings to fixed length.}
    \label{fig:queries}
    \vspace{-15pt}
\end{figure*}


This architecture can be applied to inputs of any shape or spatial layout including inputs or outputs with different spatial structure (e.g. sound and video). In contrast to latent spaces typically used in vision (e.g.~\citealt{ronnenberger2015convolutional}) the latent does not explicitly share the structure (spatial or otherwise) of the inputs. To decode this information, we query for it using cross-attention.

\subsection{Decoding the latent representation with a query array}
\label{sec:decode}

Our goal is to produce a final output array of size , given a latent representation of size . We produce an output of this size by querying the decoder with an array of index dimension . To capture the structure of the output space, we use queries containing the appropriate information for each output point, e.g. its spatial position or its modality.

We construct queries by combining (concatenating or adding) a set of vectors into a query vector containing all of the information relevant for one of the  desired outputs. This process is analogous to the way that positional information is used to query implicit functions like NeRF~\citep{mildenhall2020nerf}. We illustrate the query structure for the tasks we consider here in Fig.~\ref{fig:queries}. For tasks with simple outputs, such as classification, these queries can be reused for every example and can be learned from scratch. For outputs with a spatial or sequence structure, we include a position encoding (e.g. a learned positional encoding or a Fourier feature) representing the position to be decoded in the output. For outputs with a multi-task or multimodal structure, we learn a single query for each task or for each modality: this information allows the network to distinguish one task or modality query from the others, much as positional encodings allow attention to distnguish one position from another. For other tasks, the output should reflect the content of the input at the query location: for instance, for flow we find it helpful to include the input feature at the point being queried, and for StarCraft \Romannum{2} we use the unit information to associate the model's output with the corresponding unit. We find that even very simple query features can produce good results, suggesting that the latent attention process is able to learn to organize the relevant information in a way that's easy to query.

Each output point depends only on its query and the latent array, allowing us to decode outputs in parallel. This property allows us to amortize model training on datasets of very large output size. For example, Kinetics consists of labels, video voxels, and audio samples which together come to over 800,000 points (Tab.~\ref{tab:input_output_details}), which is prohibitively expensive to decode at once, even with linear scaling. Instead, we subsample the output array at training time and compute the loss on an affordable subset of points. At test time, we generate outputs in batches to produce the full output array.

\vspace{-8pt}
\section{Experiments}

To probe the generality of \ourmodel{}, we evaluate it on several domains including language understanding (Wikipedia+C4 masked language modeling), visual understanding (Sintel/KITTI optical flow and ImageNet classification), multi-modal (Kinetics autoencoding and AudioSet classification) \& multi-task settings (multi-task GLUE), and symbolic representations for games (StarCraft \Romannum{2}). All experiments were conducted using JAX~\citep{jax2018github} and the DeepMind JAX ecosystem~\citep{deepmind2020jax}.

\subsection{Language}
\label{sec:language}

We first compare \ourmodel{} to standard Transformers for language. Although Transformers were originally developed for language, their quadratic complexity makes them difficult to use on language inputs without tokenization, which typically shortens the length of input sequences by a factor of 4. But unlike Transformer-based models such as BERT~\citep{devlin2019bert} or XLNet~\citep{yang2019xlnet}, \ourmodel{} scales linearly with input length. Our experiments focus on showing that \ourmodel{} performs as well as or better than Transformers for masked language modeling (MLM) while removing tokenization (which is hard to maintain, introduces engineering overhead, and adds needless complexity to language models~\citep{bostrom2020bytes-bad, clark2021canine}).

We compare results for a given FLOPs budget rather than a given parameter budget as the former grows quadratically with sequence length but the latter is independent (except for positional encodings). From a practioner's perspective, FLOPs matter more than parameters since FLOPs directly relate to training time. We evaluate the quality of the learned representation on the GLUE benchmark~\citep{wang2018glue} and report our results in Tab.~\ref{tab:language_res}. We find that at a given FLOPs budget, \ourmodel{} trained without tokenization matches the performance of a strong Transformer-based model trained with SentencePiece tokenization~\citep{sennrich2016bpe,kudo2018sentencepiece}.

\begin{table}[]
\resizebox{\linewidth}{!}{  
\begin{tabular}{@{}llccccccc@{}}
\toprule
\textbf{Model}                    & \textbf{Tokenization}         & \textbf{} & \textbf{} & \textbf{Depth} & \textbf{Params} & \textbf{FLOPs} & \textbf{SPS} & \textbf{Avg.}          \\ \midrule
BERT Base (test) & SentencePiece & 512          & 512         & 12  & 110M  & 109B    & - & 81.0          \\ 
BERT Base (ours)         & SentencePiece & 512          & 512         & 12  & 110M   & 109B    & 7.3 & 81.1          \\
\ourmodel{} Base           & SentencePiece & 512          & 256         & 26   & 223M  & 119B    & 7.4 & \textbf{81.2} \\ \midrule
BERT (matching FLOPs)         & UTF-8 bytes         & 2048         & 2048        & 6    & 20M  & 130B    & 2.9 & 71.5          \\
\ourmodel{}           & UTF-8 bytes          & 2048         & 256         & 26  & 201M   & 113B    & 7.6 & 81.0          \\
\ourmodel{}++         & UTF-8 bytes          & 2048         & 256         & 40   & 425M  & 241B    & 4.2 & \textbf{81.8} \\ \bottomrule
\end{tabular}}
\vspace*{0.1cm}
\caption{\small \label{tab:language_res} \textbf{\ourmodel{} on language}: results on the GLUE benchmark (Avg. = average performance, higher is better). Following~\cite{devlin2019bert} we exclude the WNLI task. We use Pearson correlation on STS-B, Matthews correlation on CoLa and accuracy on the remaining tasks. BERT Base (test) performance is reported from~\cite{devlin2019bert}. SPS = train-time steps per second.  = \# inputs and  = \# latents.} 
\vspace{-12pt}
\end{table}

\noindent
\textbf{Pretraining.}
We pretrain on the Masked Language Modeling (MLM) task proposed in~\cite{devlin2019bert} using a large text corpus obtained by combining English Wikipedia and C4~\citep{raffell2020exploring}. For both the SentencePiece and the byte-level models, we mask  of the words, where a word is defined as a space-delimited sequence of characters. 
As a token contains many bytes on average, we need to increase the sequence length to input a similar amount of text: we use input sequence lengths of  SentencePiece tokens or  UTF-8 bytes. 
For the SentencePiece models we use a vocabulary size of  following~\cite{devlin2019bert}. 
For the byte-level models, the vocabulary size is much smaller: 256 bytes and 4 special tokens (, , , ).
\ourmodel{} produces one output vector per masked input by using learnable position-dependent vectors to query the output of the final latent processing layer.
We then apply a position-wise linear layer on top of these output vectors and train the model using a softmax cross-entropy loss to predict the original non-masked input as target.
The full details of the architecture are given in Sec.~\ref{ref:language_arch_appendix}. See Appendix Fig.~\ref{fig:bert_vis} for analysis and visualization of the learnt features.


\noindent
\textbf{Finetuning.}
We finetune \ourmodel{} on the GLUE Benchmark~\cite{wang2018glue}, reporting the best performance on the dev set for a fixed size sweep of finetuning hyperparameters. Individual task results and hyperparameters are given in Sec.~\ref{sec:glue_finetuning_appendix}. 

\noindent
\textbf{\ourmodel{} on SentencePiece tokens.}
We first observe that \ourmodel{} applied on SentencePiece tokenized input sequences slightly outperforms a strong BERT baseline applied on the same inputs (81.2 vs 81.1).
As a result of the reduced latent size of  we can train a much deeper network with 26 processing layers compared to BERT Base (12 layers) while maintaining a similar FLOPs budget.

\noindent
\textbf{\ourmodel{} on UTF-8 bytes.}
Next, we show that we can leverage \ourmodel{} to run on much longer sequences than a regular Transformer. Rather than using a fixed, handcrafted vocabulary, our model works directly with the raw byte inputs: we simply feed in and predict the UTF-8 bytes of the input string.
\ourmodel{} significantly outperforms a byte-level BERT baseline at the same FLOPs budget, demonstrating the real advantage of \ourmodel{} architecture for language.\footnote{Despite its greater depth, \ourmodel is also faster than the Transformer-based BERT baselines in real wall-clock terms -- by over a factor of 2 for the byte-based models -- as shown in Tab.~\ref{tab:language_res}.}
Remarkably, the bytes \ourmodel{} is on par with BERT running on SentencePiece tokens, showing that \ourmodel{} is also competitive against strong baselines relying on handcrafted tokenizers. The performance of \ourmodel{} on bytes scales well with more FLOPs where we obtain 81.8 on the GLUE benchmark.

The byte-level \ourmodel{} shares some similarities with the concurrent CANINE work~\citep{clark2021canine}.
While~\cite{clark2021canine} rely on a relatively sophisticated pipeline that maps Unicode codepoints to hash embeddings~\citep{svenstrup17hash}, we embed raw UTF-8 bytes directly. \cite{clark2021canine} also uses a bottleneck architecture to scale to longer text inputs, but their upsampling strategy differs from ours: they concatenate raw inputs with their aligned downsampled latent representation, apply a 1D convolution and then run a shallow transformer stack on the resulting upsampled sequence. Their approach scales quadratically with respect to the original input length while \ourmodel{}'s decoder scales \emph{linearly} with respect to the target output size. Our work scales to byte-level inputs without making any assumptions about the structure of the input, which allows it to be used beyond language as shown in the following sections.

\noindent
\textbf{Multitask \ourmodel{}.}
We use multitask queries as described in Sec.~\ref{sec:decode} 
\begin{wraptable}{l}{0.35\linewidth}
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{lc}
\toprule
\textbf{Method}         & \textbf{Avg.} \\ \midrule
Single-task query & 81.0 \\ \midrule
\textit{Multitask} & \\
Shared input token    & 81.5  \\
Task-specific input tokens    & \textbf{81.8} \\
Multitask query     & \textbf{81.8} \\ \bottomrule
\end{tabular}
}
        \caption{\small Multitask \ourmodel{}. Results use the same metric as Tab.~\ref{tab:language_res} (higher is better).}
        \label{tab:multitask_berceiver}
\end{wraptable}
to finetune on all 8 GLUE tasks simultaneously using the UTF-8 byte model (results in Tab.~\ref{tab:multitask_berceiver}). We compare to results from the single task regime where the model is trained independently on each task. 
We also compare to an approach analogous to BERT's  token that prepends a special token to the input and uses the position corresponding to this token to query the task logits.
We do this either by sharing a single token among tasks (\emph{Shared input token}) or using task-specific tokens (\emph{Task-specific input token}).
In both cases, we use a 2-layer task-specific MLP head to generate output logits for each task. 
We observe that our multitask approach outperforms single-task approaches and matches the approach that uses 8 task-specific input tokens. 
Our approach is more generic as it decouples the output array from the input array by not relying on tokens. 
This is especially appealing when the tasks are many or inhomogeneous, as we show in Sec.~\ref{sec:multimodal}.

\subsection{Optical flow}
\label{sec:flow}

Optical flow is a decades-old open problem in computer vision~\citep{lucas1981iterative,horn1981determining}. Given two images of the same scene (e.g. two consecutive frames of a video), the task is to estimate the 2D displacement for each pixel in the first image.  This has many broader applications, such as navigation and visual odometry in robots~\citep{campbell2004techniques}, estimation of 3D geometry~\citep{ranftl2020towards}, and even to aid transfer of more complex, learned inference such as 3D human pose estimation from synthetic to real images~\citep{doersch2019sim2real}.
Optical flow is challenging for neural networks for two reasons.  First, optical flow relies on finding correspondence: a single frame provides no information about flow, and images with extremely different appearance can produce the same flow.  Second, flow is extremely difficult to annotate, and the few datasets with realistic images and high-quality ground truth are small and biased.  While it is straightforward to generate large synthetic datasets as training data, e.g. AutoFlow~\citep{sun2021autoflow}, there is still a large domain gap.

Algorithms for optical flow thus must learn to accomplish several steps in a way that transfers from synthetic to real data. First, the algorithm must find correspondence between points. Then it must compute their relative offsets. Finally it must propagate flow across large regions of space, including to parts of the image which have no texture for correspondence. To generalize to real data, the learned procedure needs to work for objects and textures that weren't seen in the training data.

These difficulties have led flow researchers to develop some of the most involved architectures in the computer vision literature. State of the art algorithms, such as PWCNet~\citep{sun2018pwc}, RAFT~\citep{teed2020raft} or GMA~\citep{jiang2021learning}, use explicit machinery to ensure each of these steps is performed correctly even on out-of-domain data.  Expensive global correlation volumes explicitly compare features within a spatiotemporal neighborhood across images to find correspondences. Flows are computed iteratively and hierarchically in 2D space using explicit lookup operators to verify correctness, leading to slow performance on TPUs~\citep{jouppi2017datacenter}.

\paragraph{\ourmodel{} on Flow} In contrast, we apply \ourmodel{} to flow in a straightforward manner.  We concatenate the frames along the channel dimension and extract a  patch around each pixel (leading to  values for each pixel).  We concatenate a fixed position encoding to these features and then apply \ourmodel{}.  To decode, we query the latent representation using the input encoding.  See Sec.~\ref{sec:flow_appendix} for training details and results with various forms of pre- and post-processing, which typically perform similarly.  We also test a version with convolutional downsampling and RAFT-style upsampling, which performs only slightly worse while improving computation time.

It may seem counter-intuitive to append the images along the channel dimension, as large motions might result in pixels on entirely different objects being concatenated.  However, this kind of operation isn't unprecedented: one of the earliest optical flow algorithms, Lucas-Kanade~\citep{lucas1981iterative}, makes explicit use of the \emph{temporal} image gradient, which is approximated by the difference in intensities at a given pixel across two frames.  The algorithm uses the fact that the temporal gradient of the image approximates the spatial gradient times the spatial velocity, if lighting effects are ignored.  The approximation is even better for image regions with very little texture.  Such regions are challenging for algorithms that attempt to find explicit correspondence in feature space, especially if feature encoding involves any normalization operations, which may destroy intensity information.

\begin{wraptable}{l}{0.55\linewidth}
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{lccc}
\toprule
\textbf{Network}         & \textbf{Sintel.clean} & \textbf{Sintel.final} & \textbf{KITTI} \\ 
\midrule
PWCNet~\citep{sun2018pwc} & 2.17 & 2.91 & 5.76 \\
RAFT~\citep{teed2020raft} & 1.95 & 2.57 & \textbf{4.23} \\
\ourmodel{} & \textbf{1.81} & \textbf{2.42} & 4.98 \\
\bottomrule
\end{tabular}
}
        \caption{\small Optical Flow evaluated on Sintel~\citep{butler2012naturalistic} and KITTI with average end-point error (EPE) (lower is better).  Baselines are reported from~\cite{sun2021autoflow}.}
        \label{tab:optflow}
\end{wraptable}

\paragraph{Results} Tab.~\ref{tab:optflow} shows our results, following the standard protocol for training on AutoFlow~\citep{sun2021autoflow}. We compare to PWCNet and RAFT baselines trained by the AutoFlow authors.  On Sintel~\citep{butler2012naturalistic}, our results are slightly better than RAFT on Sintel and outperform PWCNet on KITTI~\citep{menze2015object}.  As far as we are aware, this result is state of the art on Sintel.final (GMA~\cite{jiang2021learning} produces slightly better numbers on the somewhat easier Sintel.clean evaluation set using different training data). This is surprising considering how different our architecture is from PWCNet and RAFT and how little tuning for flow \ourmodel{} required.  We use no cost volumes or explicit warping, our model is not explicitly hierarchical, and the latent representation doesn't even maintain the 2D layout of the inputs.  Also note that we reuse RAFT's AutoFlow augmentation parameters, which were tuned specifically for RAFT using population-based training~\citep{sun2021autoflow}.  As shown in Appendix Fig.~\ref{fig:flow_qualitative}, qualitatively \ourmodel{} is good at following object boundaries, and can easily propagate motion across image regions with little texture.  

\subsection{Multimodal autoencoding}
\label{sec:multimodal}
We explore using \ourmodel{} for audio-video-label multimodal autoencoding on the Kinetics-700-2020 dataset~\citep{smaira2020short}. The goal of multimodal autoencoding is to learn a model that can accurately reconstruct multimodal inputs in the the presence of a bottleneck induced by an architecture. This problem has been previously studied using techniques such as Restricted Boltzmann Machines~\citep{ngiam2011multimodal}, but on much more stereotyped and smaller scale data.

Kinetics-700-2020 has video, audio, and class labels. We wish to train a model to reconstruct all modalities simultaneously. With traditional autoencoding models like convolutional encoder-decoders, it is not obvious how to combine these modalities, because each uses data of different dimensions -- 3D (video), 1D (raw audio), and 0D (class labels) -- and with wildly different numbers of elements. With \ourmodel{}, we pad the inputs with modality-specific embeddings, serialize them into a single 2D input array and query outputs using queries containing position encodings (for video and audio) and modality embeddings.

We train on 16 frames at  resolution, preprocessed into 50k 4x4 patches as well as 30k raw audio samples, producing a total of 1920 16-d vectors and one 700-d one-hot class label. We decode directly into pixels, raw audio, and the one-hot label without any post-processing.
To prevent the model from encoding the label directly into one of the latent variables, we mask the class label 50\% of the time in training. Due to the scale of inputs and outputs in this task we subsample decoding in training, while fully decoding in testing: we sampled 512 audio samples and 512 pixels and the class label for every training example. This allows us to directly decode to a video-sized array, which would otherwise be infeasible given memory constraints. We used a latent array with 512 channels and 784, 392, and 196 latents, resulting in compression ratios of 88x, 176x, and 352x respectively. 

\begin{wraptable}{l}{0.45\linewidth}
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{cccc}
\toprule
\textbf{Compression}         & \textbf{Audio} & \textbf{Video} & \textbf{Top-1} \\ 
\textbf{Ratio}               & \textbf{PSNR}  & \textbf{PSNR}  & \textbf{Accuracy} \\ 
\midrule
88x & 26.97 & 24.37 & 10.2\% \\
176x & 25.33 & 24.27 & 8.6\% \\
352x & 14.15 &  23.21 & 11.5\% \\
\bottomrule
\end{tabular}
}
    \caption{\small Multimodal autoencoding results. Higher is better for accuracy and PSNR.}
    \label{tab:psnr}
    \vspace{-10pt}
\end{wraptable}

We show results in Tab.~\ref{tab:psnr} and reconstructions in Fig.~\ref{fig:autoencoding_viz}.
By masking the classification label during evaluation, our autoencoding model becomes a Kinetics 700 classifier. Latent variables are shared across modalities, so the quality of reconstructions for each modality is sensitive to the weight of its loss term and other training hyperparameters.
Tab.~\ref{tab:psnr} shows one tradeoff, where we emphasized video and audio PSNR
at the expense of classification accuracy.
By putting stronger weight on the class loss,
we can reach 45\% top-1 accuracy while maintaining 20.7 PSNR for video (Sec.~\ref{sec:multimodal_supp}). This strongly suggests that \ourmodel{} can jointly represent modalities with very different properties. 

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=0.31\linewidth]{figures/perceiver_autoencoding_2.png}
    \includegraphics[keepaspectratio,width=0.31\linewidth]{figures/perceiver_video_autoencoding.png}
    \includegraphics[keepaspectratio,width=0.33\linewidth]{figures/perceiver_audio_autoencoding.png}
    \caption{Multimodal audio-video-label autoencoding with 88x compression. Side-by-side: inputs on left, reconstructions right. See the supplemental material for example output video and audio.}
    \label{fig:autoencoding_viz}
    \vspace{-8pt}
\end{figure*}

\subsection{ImageNet, StarCraft II, and AudioSet}
\textbf{Please read the Appendix for results on ImageNet (Sec.~\ref{sec:imagenet}), StarCraft \Romannum{2} (Sec.~\ref{sec:starcraft_appendix}), and AudioSet (Sec.~\ref{sec:audioset_appendix})}. We have omitted these results from the main paper to make the exposition as clear as possible within 9 pages (the ICLR camera ready page limit). As highlights of these experiments: (1) on ImageNet, \ourmodel surpasses 80\% top-1 accuracy (84.5\% top-1) without using 2D convolutions after pretraining on JFT. (2) When used to replace AlphaStar's entity Transformer, \ourmodel obtains a  reduction in FLOPs while preserving StarCraft \Romannum{2} 87 \% win rate and parameter count, after only 3 experimental runs. (3) On AudioSet, \ourmodel consistently outperforms the original Perceiver when using the same training protocol on multimodal video + audio classification. The Appendix includes additional details of the experimental domains included in the main paper.

\section{Conclusion}

In this work we introduce \ourmodel{}, an architecture capable of handling general purpose inputs and outputs while scaling linearly in both input and output sizes. As we show, this architecture achieves good results in a wide variety of settings, making it a promising candidate for a \textit{general purpose} neural network architecture.  \ourmodel{} leverages the expressive power of latent attention and uses learned queries to expose a simple and unified interface that can handle multimodal and multitask settings. Overall, \ourmodel{} offers a promising way to simplify the construction of sophisticated neural pipelines and facilitate progress on multimodal and multiask problems.

\subsubsection*{Acknowledgments}
We are grateful to Ankush Gupta and Adri\`{a} Recasens Continente for reviewing drafts of this paper and to Deqing Sun for sharing code and helpful advice on the optical flow experiments.


\bibliographystyle{iclr2022_conference}
\bibliography{perceiver}

\newpage

\appendix
\section*{Appendix}

\begin{table}[h]
\resizebox{\linewidth}{!}{
 \begin{tabular}{cccccc}
 \toprule
 \textbf{Modalities} & \textbf{Tasks} & \textbf{Preprocessing} & \textbf{Postprocessing} & \textbf{\# Inputs} & \textbf{\# Outputs}   \\
 \midrule
 Text & Token-level pred. & Tokenization + Embed.  & Linear projection &  &  \\
 Text & Byte-level pred. & Embed.      & None &  &  \\
 Text & Multi-task (8 tasks) & Embed.   & None   &  &  \\ \midrule
 Video & Flow prediction & None  & None &  &  \\
 Video & Flow prediction & Concat  & None &  &  \\
 Video & Flow prediction & Conv+maxpool  & RAFT upsampling &  &  \\
 Video & Flow prediction & Conv+maxpool+concat  & RAFT upsampling &  &  \\ \midrule
 Video+Audio+Label & Autoencoding & Patch: 1x4x4 Vid, 16 Aud & None &  &  \\ \midrule
 Image & Classification & None      & None &  &  \\
 Image & Classification & Linear projection      & None &  &  \\
 Image & Classification & Conv+maxpool      & None &  &  \\ \midrule
 StarCraft Unit Set & Encoding and Classification & Tokenization & Pointer network &  &  \\ \midrule
 Video+Audio & Classification & Patch:  Vid, 128 Aud & None &  &  \\
 Video+Audio & Classification & Patch:  Vid. Aud  mel-spectrogram      & None &  &  \\ 
 \bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\small \label{tab:input_output_details} Details of each of the tasks we use to evaluate \ourmodel{} here. The positional and task embeddings appended to inputs for each case are listed in Tab.~\ref{tab:pos_encodings_queries}.}
\end{table}

In the following sections, we describe experiments on three additional domains (ImageNet, StarCraft \Romannum{2}, and AudioSet) and provide additional details for the methods and experiments described in the paper. For ease of reference and comparison across domains, we describe the input and output size and processing used in all experiments in Tab.~\ref{tab:input_output_details} and provide details of input key/value, position encoding, and output queries used in all experiments in Tab.~\ref{tab:pos_encodings_queries}. 

On all domains but StarCraft \Romannum{2}, we include experiments with several input configurations, ranging from no domain adaptation (e.g. tokenizer-free language, flow from raw pixels, ImageNet with no convolutional or patch-based preprocessing and fully learned position encodings) to moderate domain adaptation (e.g. SentencePiece language understanding, flow from conv+maxpool-processed images and with RAFT upsampling, ImageNet with conv+maxpool-preprocessing and 2D Fourier features). These results demonstrate the unprecedented generality of \ourmodel{}, the simplicity that this architecture unlocks in handling a range of tasks, and its flexibility to work as part of a domain-adapted system. 

\begin{table}[h]
\resizebox{\linewidth}{!}{
 \begin{tabular}{c  c  c  c  c  c}
 \toprule
 \textbf{Domain} & \textbf{Input Modality} & \textbf{Encoder KV input} & \textbf{Encoder KV channels} & \textbf{Decoder query input} & \textbf{Decoder query channels} \\
 \midrule
Language (MLM) & Text & byte/token encoding + learned pos & 768 & learned pos & 1280 \\ \midrule
Language \\ (\ourmodel{}++ MLM) & Text & byte/token encoding + learned pos & 768 & learned pos & 1536 \\ \midrule
Language (GLUE) & Text & byte/token encoding + learned pos & 768 & Class query (per-task) & 1280 \\ \midrule
Language \\ (\ourmodel{}++ GLUE) & Text & byte/token encoding + learned pos & 768 & Class query (per-task) & 1536 \\ \midrule
Optical Flow & Video (concat. frames) & [conv or Linear(concat RGB), 2D FFs] & 322 & [Linear(RGB), 2D FFs] & 322 \\ \midrule
Optical Flow & Video & [conv or Linear(RGB), 3D FFs] & 451 & [conv features, 3D FFs] & 451 \\ \midrule
Kinetics & Video, & [patched RGB, 3D FFs,  learned modality feat.] &  704 & [3D FFs,  learned modality feat.] & 1026 \\
 & Audio, & [patched sound pressure, 1D FF,  learned modality feat.] &  704 & [1D FF,  learned modality feat.] & 1026 \\
 & Label & [one-hot label,  learned modality feat.] &  704 & [learned modality feat.] & 1026 \\ \midrule
ImageNet (2D FFs) & Image & [RGB, 2D FFs] & 261 & Class query (single) & 1024 \\ \midrule
ImageNet (learned pos) & Image & [Linear(RGB), learned pos] & 512      & Class query (single) & 1024 \\ \midrule
ImageNet (conv) & Image & [Conv features, 2D FFs] & 322      & Class query (single) & 1024 \\ \midrule
StarCraft \Romannum{2} & SC2 entities & Entity features & 128 & Entity features & 128 \\ \midrule
AudioSet & Video, & [patched RGB, 3D FFs, learned modality feature] & 487 & Class query (single) & 1024 \\
 & Audio & [patched sound pressure, 1D FFs, learned modality feature] & 487 &  &  \\ \midrule
AudioSet & Video, & [patched RGB, 3D FFs, learned modality feature] & 487 & Class query (single) & 1024 \\
 & Mel-spectrogram & [mel-spectrogram features, 1D FFs, learned modality feature] & 487 &  &  \\
 \bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\small \label{tab:pos_encodings_queries} \textbf{Table best viewed on a screen.} The structure and size of the positional and task embeddings used to construct \ourmodel{}'s encoder key-value inputs and decoder query inputs, for each domain described in the main text. ``[x, y]'' indicates that x's and y's features are concatenated, while ``x + y'' indicates that x's and y's features are added to produce the full featurization. ``FF'' = Fourier features as in~\cite{jaegle2021perceiver}.}
\end{table}

\section{Image classification}
\label{sec:imagenet}

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model}           & \textbf{Pretrained?}  & \textbf{Accuracy} & \textbf{FLOPs} & \textbf{Params}         \\ \midrule
\textbf{ConvNet baselines} & & & & \\
ResNet-50~\citep{he2016deep}       & N &78.6 & 4.1B & 26M         \\
NFNet-F6+SAM~\citep{brock2021high} & N & 86.5 & 377.3B & 438.4M         \\
Meta Pseudo Labels~\citep{pham2021meta} & Y & 90.2 & - & 480M  \\ \midrule
\textbf{ViT baselines} & & & & \\
ViT-B/16~\citep{dosovitskiy2020image} & N & 77.9 & 55.4B & 86M          \\
ViT-H/14~\citep{dosovitskiy2020image} & Y & 88.6 & - & 632M          \\
DeiT 1000 epochs~\citep{touvron2020training} & N & 85.2 & - & 87M          \\
CaiT-M48 448~\citep{touvron2021going} & N & 86.5 & 329.6B & 356M          \\ \midrule
\textbf{w/ 2D Fourier features} & & & & \\
Perceiver & N & 78.6 & 404B & 42.1M           \\ 
\ourmodel{}, config A & N & 79.0 & 407B & 48.4M          \\ 
\ourmodel{}, config B (pretrained) & Y & 84.5 & 213B & 212M          \\ \midrule
\textbf{w/ learned position features} & & & & \\
Perceiver (learned pos) & N & 67.6 & 404B & 55.9M          \\
\ourmodel{}, config A (learned pos) & N & 72.7 & 407B & 62.3M          \\ \midrule
\textbf{w/ 2D conv + maxpool preprocessing} & & & & \\
Perceiver (conv) & N & 77.4 & 367B & 42.1M           \\ 
\ourmodel{}, config A (conv) & N & 82.1 & 369B & 48.6M          \\
\ourmodel{}, config B (conv) (pretrained) & Y & 86.4 & 176B & 212M          \\ \bottomrule
\end{tabular}}
\caption{\label{tab:imagenet_flops_params} \small Results on ImageNet image classification (top-1 accuracy, higher is better). ``-'' indicates a value we could not find reported in the literature. We did not extensively tune our models for efficiency on image classification -- the primary focus of this work is generality, rather than speed on images -- \ourmodel{} uses comparable FLOPs to attention-based image classification models, especially for the more compact configuration B pretrained on JFT. The positional encoding does not significantly change model FLOPs. 
}
\vspace{-15pt}
\end{table}

Perceiver did well on ImageNet~\citep{deng2009imagenet} classification without using 2D structure in the design of the architecture, but generated class scores using a simple average + project decoder (see Sec.~\ref{sec:decoding} and Fig.~\ref{fig:decoder_pooling} for a diagram illustrating the difference between the two forms of decoder). We now evaluate the effect of this more general decoder. See Sec.~\ref{sec:audioset_appendix} for similar validation on AudioSet.

\paragraph{Results} Tab.~\ref{tab:imagenet_flops_params} shows our results alongside representative numbers from the literature. Perceiver and \ourmodel{} differ in their decoder, and neither model uses convolutional preprocessing by default. \ourmodel{} consistently outperforms the original architecture. After pretraining on JFT~\citep{sun2017revisiting}, \ourmodel{} performs in the ballpark of models designed primarily for image classification. \ourmodel{} is competitive with members of the Vision Transformer (ViT)~\citep{dosovitskiy2020image} family even without relying on 2D convolutions. \ourmodel{} is also compatible with convolutional preprocessing: adding a 2D conv+maxpool preprocessing stage leads to a moderate increase in efficiency and bump in performance.

While neither the Perceiver and \ourmodel{} incorporate any 2D spatial structure architecturally, they use positional features that inject 2D spatial information (Sec. 3.2 and Appendix sec. D of~\citealt{jaegle2021perceiver}). By replacing these 2D position features with a fully learned position encoding as used on language, we can learn an image classification model that is given no privileged information about the structure of images. This positional encoding is an array of shape 50,176  256, which is randomly initialized using a truncated Gaussian distribution with scale 0.02. ImageNet networks that use this positional encoding are given no information about 2D image structure. For these experiments, we additionally use a 1D convolutional network to project the RGB at each point to 256 before concatenating it with the learned positional encoding. The results of this experiment are shown in Tab.~\ref{tab:imagenet_flops_params} (\textbf{w/ learned position features}). To our knowledge, this is the best result by any model on ImageNet without 2D architectural or feature information.

\subsection{Details of ImageNet training}
\label{sec:imagenet_appendix}

For ImageNet experiments, we use CutMix~\citep{yun2019cutmix} and MixUp~\citep{zhang2018mixup} regularization, in addition to RandAugment~\citep{cubuk2020randaugment} as used in~\cite{jaegle2021perceiver}. We observed only marginal improvements in performance from this change, but it brings the augmentation strategy more in line with the strategy used elsewhere in the literature~\citep{brock2021high, touvron2020training}. In all experiments, we use RandAugment with 4 layers at magnitude 5 (as in~\citealt{jaegle2021perceiver}) and CutMix with a ratio of 0.2. In early experiments, we found that higher weight decay and moderate gradient clipping contributed to better generalization: we use a weight decay of \num{0.1} and clip to a maximum global gradient norm of \num{10}. We use no dropout. We use an architecture with weight sharing in depth: the latent (processing) component of the architecture includes 8 blocks of 6 attention modules each, and weights are shared between the corresponding modules in each block. We omit the repeated encoder cross-attends used in~\cite{jaegle2021perceiver} as we found these to lead to relatively small performance improvements but to significantly slow down training: using 8 encoder cross-attention instead of 1 adds an additional 303 billion FLOPs. The FLOPs for all ImageNet models presented here are given in Tab.~\ref{tab:imagenet_flops_params} and the model training step time on 64 TPUv3 are given in Tab.~\ref{tab:imagenet_train_time}.

\begin{table}
        \centering
\resizebox{0.5\linewidth}{!}{  

\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Model}           & \textbf{Train steps/sec}          \\ \midrule
Perceiver (2D FF) & 4.73          \\ 
\ourmodel{} (2D FF) & 4.85          \\ \midrule
Perceiver (learned pos) & 4.16          \\ 
\ourmodel{} (learned pos) & 4.14          \\ \midrule
Perceiver (conv) & 4.73          \\ 
\ourmodel{} (conv) & 5.58          \\ \midrule
\ourmodel{} (pretrained) & 6.41          \\ \bottomrule
\end{tabular}}
\caption{\label{tab:imagenet_train_time} \small ImageNet model training speed. The model used for pretraining is faster because it uses only 16 process modules. We did not reimplement baselines, so we report only the training speed of Perceiver and \ourmodel{} models.
} 
\vspace{-5pt}
\end{table}

For all ImageNet experiments, we train for 110 epochs, using a batch size of 1024 and 64 TPUs. We use LAMB with a simple learning rate schedule consisting of a flat learning rate of \num{2e-3} for 55 epochs, after which the learning rate is decayed to 0 over the final 55 epochs following a cosine decay schedule~\citep{loshchilov2017sgdr}. We found a cosine learning rate decay schedule simpler to tune than the step decay schedule used in \cite{jaegle2021perceiver} and that beginning the decay process halfway through training generally led to good performance without introducing instability. We found it important to omit an initial learning rate warm-up period, as this often prevented models from training when using LAMB.

\subsection{Large-scale pretraining}

As reported in~\cite{jaegle2021perceiver}, Perceiver models are able to easily overfit ImageNet-scale datasets without regularization. For this reason, we explored pretraining a model on JFT, a large-scale, multi-labeled internal dataset with 300 million images spanning approximately 18,000 classes~\citep{sun2017revisiting}. We pretrain on this dataset at the same resolution used on ImageNet (224  224) using a base learning rate of \num{3e-4} and a cosine decay schedule, decaying to 0 over 14 epochs. We omit all augmentation except basic cropping, resizing, and left-right flipping. We use a weight decay of 0.1. We use a larger batch size of 8192 and train on 256 TPUs. Images in this dataset come with a variable number of labels, so we use a cross-entropy loss with a multi-one-hot representation of the targets. Unlike in the other ImageNet experiments, we do not share weights in the latent self-attention process modules, but use a 16-layer latent network with no weight sharing in depth. Unlike the other ImageNet experiments, the process-module MLPs use a hidden layer with 4 the number of channels (rather than 1 as on other ImageNet experiments). When pretraining the 2D FF model, we use a 1D convolutional network to project input RGB at each point to 256 before concatenating it with the positional encoding (a 2D Fourier frequency positional encoding). When pretraining the conv+maxpool model, we instead use the initial convolutional preprocessing described in Sec.~\ref{sec:2d_conv} below.

To evaluate transfer, we fine-tune our pre-trained model on ImageNet. We replace only the final linear layer of the decoder to produce the required 18,000 classes. For 2D FF fine-tuning, we used similar optimizer and augmentation settings as with our from-scratch ImageNet training: 1024 batch size on 64 TPUs, 131K steps with LAMB using a flat base LR of 0.002 for the first 70K steps and a cosine learning rate decay for the last 61K steps. We use identical settings for conv+maxpool fine-tuning with the exception of the base learning rate, which we set to 0.0002, as training with the higher 0.002 rate was unstable. 

\subsection{2D convolutional preprocessing on ImageNet}
\label{sec:2d_conv}

In other image settings discussed here, we optionally use simple pre- and post-processing steps to reduce the size of very large inputs and outputs. Because ImageNet data points are relatively small (Tab.~\ref{tab:input_output_details}), we are able to process full images without convolutional pre- and post-processing. Consequently, we can use this dataset to probe the sensitivity of the model to convolutional pre-processing. Incorporating a single convolution + max pooling leads to a moderate improvement in the performance of the architecture: this is perhaps unsurprising, as convolutional pre-processing injects information about the 2D structure of images into the architecture. By comparison ViT first processes images by applying a 2D convolution with matched kernel and stride to downsample its inputs (referred to as a ``linear projection of flattened patches'' in that work and throughout the ViT literature). As in other experiments, we find that incorporating an attention-based decoder (\ourmodel{}) leads to better results than averaging and pooling the output (Perceiver). Using convolutional preprocessing leads to a moderate reduction in the number of FLOPs used by the model (Tab.~\ref{tab:imagenet_flops_params}) and training speed in some configurations (Tab.~\ref{tab:imagenet_train_time}). The input to the network after preprocessing is 56  56 instead of 224  224 as in the experiments directly on pixels.

\section{StarCraft II }
\label{sec:starcraft_appendix}

\begin{table}[]
\resizebox{\linewidth}{!}{  
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Entity encoder}                    & \textbf{Win rate}         & \textbf{Params (M)} & \textbf{FLOPs} & \textbf{Train steps/sec}          \\ \midrule
Transformer~\citep{vinyals2019grandmaster} & 0.87 & 144    & 3.3B  & 2.9  \\ 
\ourmodel{}                    & 0.87 & 140    & 0.93B  & 2.9 \\
\bottomrule
\end{tabular}}
\vspace*{0.1cm}
\caption{\small \label{tab:starcraft_res} We evaluate \ourmodel{} on StarCraft \Romannum{2} by using it to replace the well-tuned Transformer entity encoder. \ourmodel{} matches the performance of the original Transformer despite using fewer FLOPs and parameters and requiring essentially no tuning. Note that the training steps/sec of the overall system does not change because the entity encoder is not the speed bottleneck.}
\vspace{-12pt}
\end{table}

To further demonstrate \ourmodel{}'s capabilities on discrete modalities and as a drop-in replacement for Transformers, we plug in \ourmodel{} in place of AlphaStar's Transformer. AlphaStar~\citep{vinyals2019grandmaster} is the state-of-the-art system for the challenging real-time strategy game of StarCraft \Romannum{2}. 

At its core, AlphaStar represents the units in the game as a discrete, unordered set of symbols (the ``units''). These units are represented by a vector of properties including unit type, position, and health. At each timestep, the architecture encodes units with an entity encoder, which in the original model was parameterized using a vanilla Transformer.

The entity encoder takes as input a set of 512 entities (referred to as  in~\cite{vinyals2019grandmaster}) and produces as output an embedding for each entity () and a 1D embedding reduced over entities (). These 512 entities represent the units and other entities that are present in the game: unused entity slots are masked.  is produced by passing the outputs of the entity encoder through a ReLU and a 1D convolution with 256 channels.  is produced by averaging the (unmasked) entity encoder outputs and passing it through a linear layer with 256 units and a ReLU.

In the original AlphaStar system, the entity encoder consisted of a Transformer with 3 attention layers, each of which used 2 heads and a feature dimension of 128. The output of each attention layer is projected to 256 and followed by an 2-layer MLP with hidden size 1024 and output size 256. This architecture was arrived by an extensive tuning process as reported in~\cite{vinyals2019grandmaster}.

The representation produced by the entity encoder is used both as a summary of the state (after pooling) and as a rich representation of the units. This representation is used by a pointer network \citep{vinyals2015pointer} to assign a probability to each possible unit selection, in the process parameterizing the agent's unit selection policy. For this reason, we view AlphaStar as an important test case for \ourmodel{}'s ability to function as a general-purpose tool for processing symbolic or set-valued data. If the question is ``can \ourmodel{} serve as a replacement for a well-tuned Transformer as a symbolic processing engine?'' then the answer is yes:

We obtained StarCraft \Romannum{2} results by using \ourmodel{} instead of a Transformer for the AlphaStar entity encoder. We replaced the Transformer with a \ourmodel{} with a latent of index dimension 32, keeping the input and output size of 512 units. We performed \textbf{no} tuning beyond sweeping the size of the latent index dimension (we tried values of 32 and 64): 
\ourmodel{} works out of the box. We observed that the resulting agent reached the same level of performance as the original AlphaStar agent, reaching an 87\% win-rate versus the Elite bot after behavioral cloning~\citep{pomerleau1989alvinn} on human data, while also leading to a 3 decrease in FLOPs (Tab.~\ref{tab:starcraft_res}).

We replaced this Transformer with a 3-layer \ourmodel{} with a latent of index dimension 32. We tuned only the size of the index dimension (sweeping values of 32 and 64), but otherwise used the same hyperparameters as ImageNet.

\section{AudioSet}
\label{sec:audioset_appendix}

We seek to confirm that the the attention-based decoder helps even on classification, where the original Perceiver's decoder could be used. We show that the trend identified on ImageNet holds more generally, by revisiting the multimodal AudioSet classification domain. AudioSet is a large-scale event classification dataset containing 1.7 million training examples, each consisting of 10s long video and audio. Each example is labeled with several labels drawn from 527 classes. 

We perform experiments using the protocol described in~\cite{jaegle2021perceiver}, training models for 100 epochs using 32-frame clips at train time and 16 overlapping 32-frame clips at test time. As in the ImageNet experiments, We compare the performance of Perceiver and \ourmodel{} using models that are matched except for the decoder (we use an average + project decoder for Perceiver and a query-based attention decoder for \ourmodel{}, see Sec.~\ref{sec:decoding} and Fig.~\ref{fig:decoder_pooling}). All models use an architecture with 12 processor modules and a latent index dimension  of 512 (we omit the repeated cross-attends used in \cite{jaegle2021perceiver}). We compare models taking video and either raw audio or mel-spectrogram (pre-processed audio) as input. For all four model settings, we swept the number of latent channels (using  \{512, 1024\}) and report the best value for each setting. We performed no additional tuning.

Results of this experiment are shown in Tab.~\ref{tab:audioset_res}. We find that as in the ImageNet experiments, using the attention-based decoder leads to small but consistent improvements over the less generally applicable average + project decoder. Because \ourmodel{} introduces no domain assumptions not present in the original Perceiver, this is evidence that \ourmodel{} is a strictly more general model.

\begin{table}[]
\resizebox{\linewidth}{!}{  
\begin{tabular}{@{}lcccccc@{}}
\toprule
Model & Input & mAP         & Latent channels (\textit{D}) & Params (M) & FLOPs & Train steps/sec          \\ \midrule
Perceiver & Raw audio + video & 42.4 & 512 & 21.0  & 52.3B & 3.8  \\
\ourmodel{} & Raw audio + video & 43.3 & 512 & 25.0  & 52.9B & 3.8  \\ \midrule
Perceiver & mel-spectrogram + video & 43.6 & 512 & 21.0  & 60.7B & 3.8  \\ 
\ourmodel{} & mel-spectrogram + video & 44.9 & 1024 & 88.2  & 129.5B & 3.8  \\ \midrule
\end{tabular}}
\vspace*{0.1cm}
\caption{\small \label{tab:audioset_res} \ourmodel{} on multimodal (audio + video) AudioSet classification (mAP = mean average precision, higher is better). All models have similar runtimes despite FLOPs differences because the bottleneck is data loading and preprocessing rather than model forward/backward passes.}
\vspace{-12pt}
\end{table}

\section{FLOPs calculation}

In all cases, we report theoretical FLOPs with multiplies and accumulates counted as separate operations. This is the strategy used in~\cite{kaplan2020scaling} and elsewhere in the literature. We use this strategy consistently here to allow comparisons between the models we propose and develop (including our BERT reimplementation). Note that some papers in the literature report FLOPs using fused multiply-accumulates: using this strategy will cut the figures reported here in half.

\section{Architectural details}

\begin{figure}[t]
    \centering
    \includegraphics[keepaspectratio,width=0.6\linewidth]{figures/encode_process_decode.pdf}
    \caption{Schematic depiction of encode, process, and decode attention. Each attention module uses the same operations, but differs in which inputs are used to generate key/values or queries and in the output shape. Encode attention can be viewed as mapping an input to a latent space, typically with a smaller index dimension (fewer elements). Decode attention can be viewed as mapping a latent to an output space, often with a larger index dimension (more elements). Both of these are forms of cross-attention. Process attention (self-attention) preserves the input index dimension (same elements). Red and blue dashed lines are used to highlight the two matrix multiplications used in QKV attention, as described in the text.}
    \label{fig:encode_process_decode}
    \vspace{-8pt}
\end{figure}

\ourmodel{} is constructed from GPT-2-style~\citep{radford2019language} Transformer attention modules, which consist of QKV attention followed by an MLP, along with linear projection layers to ensure inputs to and outputs from the QKV attention and MLP take on desired sizes. Using the array sizes of the encoder attention, the QKV attention takes in two two-dimensional arrays, a key-value input array  and a query input array , and maps them to an array  sharing the shape of the query input (after projection).  is used as input to an MLP, which is applied independently to each element of the index dimension (i.e. convolving the MLP with its input along the first dimension), producing a final array .

While we describe attention as taking two inputs, in standard Transformers it is typically described as mapping one input to an output of the same size. This is because all modules of a standard Transformer use \textit{self}-attention, where the same input is used for both key-value inputs and query inputs. The view of attention that we describe encompasses both cross-attention and self-attention, both of which are specific ways of using QKV-attention. \ourmodel{} uses cross-attention for encoder and decoder attention modules and uses self-attention for the latent processing modules. These modules differ primarily in terms of what shape data they ingest and produce (Fig.~\ref{fig:encode_process_decode}). 

We now describe the structure of QKV attention and the MLP in more detail.

\subsection{Attention module internals}

QKV attention takes in two two-dimensional arrays, a query input  and a key-value input . The output of QKV attention is an array with the same index (first) dimension as the query input and a channel (second) dimension determined by an output projection:



where  is an array of attention maps , and  is an array . The functions  are linear layers mapping each input to a shared feature dimension  and  is a linear layer projecting the output to a target channel dimension, which is often the same size as 's. All linear layers are applied convolutionally over the index dimension (the first dimension of their inputs). We have omitted batch and head dimensions (in the case of multi-headed attention) for readability. 
QKV attention is followed by a two-layer MLP with a GELU~\citep{hendrycks2016gelu} nonlinearity following the first layer. The full module has the following structure:



slightly abusing notation for simplicity and to emphasize the residual structure. ``Attn'' refers to QKV as described above. 

In the context of decoder attention, we sometimes find it helpful to omit the second step (), as it involves adding the model output with a query. Queries sometimes include features inherited from the input space (Tab.~\ref{tab:pos_encodings_queries}), and this residual connection may make learning unnecessarily difficult. For example, for optical flow, including this residual connection forces the network to produce optical flow output by adding RGB and Fourier features to the model's output.

\subsection{Computational complexity}
\label{sec:complexity}
The computational complexity of each attention module is dominated by the two matrix multiplications in QKV attention. Still using the shapes of the encoder attention, these two matrix multiplies involve matrices of shape  and  and  and , giving overall time and memory complexity of .
Let , , and  be the index dimensions for the input, latent, and output arrays, and
to simplify the analysis let  be the feature size for all layers.
The KV and Q sizes for the 
encoder, latent transformer,
and decoder will then be
 and 
(for the encoder),
 and 
(for the latent transformer),
and  and 
(for the decoder).
A model with  latent attention blocks has complexity . 
In other words, \ourmodel{} has complexity linear in the size of the input and output arrays and it decouples the depth of the latent transformer from the input and output sizes. Both of these properties contribute to \ourmodel{}'s efficiency: while many proposals for efficient attention modules or architectures include linear or sub-quadratic scaling with input/output size, \ourmodel{} is unusual in also decoupling depth from input/output size (without requiring domain-specific strategies like 2D convolution). For further discussion of these points, see Sec. 2 and Sec. A of~\cite{jaegle2021perceiver}.

\subsection{Using the decoder for classification / regression}
\label{sec:decoding}

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/pooling.pdf}
    \caption{Single-query attention decoder (left), as used in \ourmodel for classification tasks and a standard average + project decoder (right), as used in \cite{jaegle2021perceiver}. Both modules can be seen as first \textcolor{red}{aggregating latents by weighted averaging} (learned, data-dependent weighting for the attention decoder; uniform weights for the average + project decoder) and then \textcolor{blue}{projecting to an output channel dimension} (linear value projection + MLP for the attention decoder; simple linear projection by the average + project decoder). Attentional decoding is more expressive than average + project decoding and follows the same architectural template as encoder and processor modules.}
    \label{fig:decoder_pooling}
    \vspace{-8pt}
\end{figure*}

As we show in ImageNet and AudioSet experiments, the attentional decoder used here can be used in settings where standard average + project decoders are applicable. We find that the attentional decoder typically produces somewhat better results than the standard decoder. This likely occurs because attentional decoding is more expressive than average + project decoding. To make this clear, we illustrate the two pooling schemes in Fig.~\ref{fig:decoder_pooling}. Both decoders can be viewed as first averaging the latents and then projecting them to a target shape, but decoder attention uses more expressive modules for each of these operations. Instead of uniformly weighting each input in the averaging operation, decoder attention uses the attention scores as data-dependent weights for each input point. Instead of projecting the raw averaged input to a target dimensionality, decoder attention first projects inputs via a value layer and then processes them with an MLP. In addition to its greater expressivity, decoder attention has the advantage of being easily generalizable to dense outputs (by increasing the number of queries) and of reusing the same architectural pattern used for the encoder and processor modules.

\section{Language: additional details}

\subsection{Other Tokenizer-free models}
One application of \ourmodel{} is byte-level language processing, which has concurrently been addressed by several other groups. \cite{clark2021canine} trains models on Unicode code points and shows results competitive with subword-based models on a multilingual question answering dataset. \cite{tay2021charformer} trains on UTF-8 bytes directly by introducing a hand-designed module that is trained end-to-end to perform subword tokenization and produces results on-par with and sometimes better than subword-based models. \cite{xue2021byt5} trains encoder-decoder T5 models on UTF-8 bytes directly and shows that making the encoder 3x deeper than the decoder leads to comparable performance with subword baselines.

\subsection{Architecture details}
\label{ref:language_arch_appendix}

The architecture hyperparameters and the training speed for the \ourmodel{} used in the language experiments are given in Tab.~\ref{tab:language_perceiver_arch}.

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
 \begin{tabular}{l c  c  c  c  c}
 \toprule
  \textbf{Model} & \textbf{BERT Base} & \textbf{BERT matching FLOPs} & \textbf{\ourmodel{} Base} & \textbf{\ourmodel{}} & \textbf{\ourmodel{}++}  \\
 \midrule
Tokenizer & SentencePiece & UTF-8 bytes & SentencePiece & UTF-8 bytes & UTF-8 bytes \\
\midrule
Number of inputs () & 512 & 2048 & 512 & 2048 & 2048 \\
Input embedding size () & 768 & 768 & 768 & 768 & 768 \\
\midrule
Number of Process layers & 12 & 6 & 26 & 26 & 40 \\
Number of latents () & - & - & 256 & 256 & 256 \\
Latent size () & - & - & 1280 & 1280 & 1536 \\
FFW hidden dimension for latents & - & - & 1280 & 1280 & 1536 \\
\midrule
Number of output queries during pretraining () & - & - & 512 & 2048 & 2048 \\
Dimension of learned queries () & - & - & 768 & 768 & 768 \\
FFW hidden dimension for outputs  & - & - & 768 & 768 & 768 \\
\midrule
Training steps/second & 7.3 & 2.9 & 7.4 & 7.6 & 4.2 \\
 \bottomrule
\end{tabular}
}
\vspace{1mm}
\caption{\small \label{tab:language_perceiver_arch} \ourmodel{} architecture details for language experiments.}
\end{table}

\subsection{MLM pretraining}
We pretrain all models on a mixture of the C4 dataset~\citep{raffell2020exploring} and English Wikipedia, where  of the training tokens are sampled from the C4 dataset and the remaining  from Wikipedia. We concatenate 10 documents before splitting into crops to reduce wasteful computation on padding tokens. 
We use the same masking strategy for SentencePiece and byte-level experiments: each word is masked independently with probability  where word boundaries are defined using white-space boundaries.

The pretraining hyperparameters are given in Tab.~\ref{tab:language_pretrain_hypers}. For the BERT (matching FLOPs) model trained on bytes, we reduce the model width from  to , the feed-forward hidden size from  to , the number of layers from  to  and the number of attention heads from  to . Given the longer sequence length of  bytes, this model has about the same number of inference FLOPs as a BERT Base model on a sequence length of  tokens.

In order to decode, we use learned queries of the same dimension of the input array (Tab.~\ref{tab:language_perceiver_arch}). 
We have as many output queries as inputs to be able to predict the masked token at all positions in the sentence (=).

To get an insight into the learnt queries we visualize the attention weights in the first cross attention layer on a small paragraph (Fig.~\ref{fig:bert_vis}). We discover that the model has learnt both position and content based look-ups. The position-based look-ups can be either very sparse and precise or more distributed and periodic. This second mode appears somewhat less often and is more efficient because more data is being attended to at the same time, but also more distributed, since the values are subsequently averaged: this acts as a learned pooling. The content based retrievals focus mostly on syntactic elements like capital letters and punctuation (colon, exclamation marks, quotation marks, etc). This is probably because these are good word delimiters and can help the model reduce prediction uncertainty.
\begin{table}[h]
\centering
 \begin{tabular}{l c}
 \toprule
Training steps & 500,000 \\
Batch size & 2048 \\
Masking strategy & Words \\
\midrule
Optimizer & LAMB~\citep{you2019lamb} \\
Learning rate & 0.00125  \\
Linear warmup steps & 1,000 \\
Cosine cycle decay & 500,000 \\
Weight decay & 0.01 \\
 \bottomrule
\end{tabular}
\vspace{1mm}
\caption{\small \label{tab:language_pretrain_hypers} Hyperparameters for masked language modelling (MLM) pre-training experiments}
\end{table}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{1.\textwidth}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_local_2.png}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_local_1.png}
        \subcaption{Very sharp location based attention.}
    \end{subfigure}
    \begin{subfigure}[b]{1.\textwidth}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_periodic_2.png}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_periodic_1.png}
        \subcaption{A more efficient and more distributed ``periodic'' location based attention.}
    \end{subfigure}
    \begin{subfigure}[b]{1.\textwidth}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_syntax_1.png}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_syntax_2.png}
        \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/bertceiver_syntax_3.png}
        \subcaption{Content based attention for syntactic elements like punctuation and capital letters.}
    \end{subfigure}
    \caption{Visualization of attention weights for a few queries in the initial cross-attention layer. We use the color to convey the weight of the attention and normalize by the maximum weight to make them easier to visualize. Best viewed in color.}
    \label{fig:bert_vis}
    \vspace{-8pt}
\end{figure}

\subsection{GLUE Finetuning}
\label{sec:glue_finetuning_appendix}
Following~\cite{devlin2019bert}, we specify a fixed-size hyperparameter grid and select the best dev performance across that grid for each task independently (Tab.~\ref{tab:language_pretrain_hypers}). The full GLUE results are shown in Tab.~\ref{tab:full_glue_results}. Following~\cite{devlin2019bert} we exclude the WNLI task. We use accuracy for all tasks expect STS-B and CoLA where we use Pearson correlation and Matthews correlation respectively. The average is computed by first averaging the results of MNLI-matched and MNLI-mismatched, which is then counted as a single task in the overall average.

For single-task experiments, we do not require a  token as we use a single decoding query vector. In both single-task and multi-task experiments an extra 2-layer MLP with a hidden size of  and a  activation is used to map the the \ourmodel{} outputs to the class logits (or regression target for STS-B).


\begin{table}[h]
\centering
 \begin{tabular}{l c}
 \toprule
    Training epochs &  \\
    Batch size & \{, , \} \\ 
 \midrule
    Optimizer & LAMB \\
    Learning rate & \{, , ,  \} \\
    Linear warmup steps &  \\
    Weight decay &  \\
 \bottomrule
\end{tabular}
\vspace{1mm}
\caption{\small Hyperparameters for GLUE finetuning experiments. We sweep over the values in brackets.}
\end{table}

\begin{table}[h]
\resizebox{\linewidth}{!}{
\centering
 \begin{tabular}{l  c  c  c c c c c c c c  c}
 \toprule
  \textbf{Model} & \textbf{Tokenizer} & \textbf{Multi-task} & \textbf{CoLA} & \textbf{MNLI-m/mm} & \textbf{MRPC} & \textbf{QNLI} & \textbf{QQP} & \textbf{RTE} & \textbf{SST-2} & \textbf{STS-B} & \textbf{Average} \\
 \midrule
  Bert Base (test)~\citep{devlin2019bert} & SentencePiece & No  & 52.10 & 84.60/83.40 & 84.80 & 90.50 & 89.20 & 66.40 & 93.50 & 87.10 & 80.95 \\
  Bert Base (ours) & SentencePiece & No & 50.28 & 85.56/85.68 & 85.75 & 92.67 & 91.05 & 61.72 & 93.98 & 88.04 & 81.14 \\
  \ourmodel{} Base & SentencePiece & No  & 47.11 & 84.53/85.03 & 87.25 & 92.12 & 90.22 & 65.23 & 94.38 & 88.18 & 81.16\\
  \midrule
  BERT (matching FLOPs) & UTF-8 Bytes & No & 20.06 & 74.11/75.55 & 77.00 & 85.75 & 88.23 & 53.91 & 89.00 & 82.84 & 71.45\\
  \ourmodel{} & UTF-8 Bytes & No & 50.19 & 83.22/83.89 & 87.24 & 91.71 & 90.12 & 64.84 & 93.17 & 86.81 & 80.95 \\
  \ourmodel{}++ & UTF-8 Bytes & No & 52.54 & 84.13/84.91 & 86.03 & 92.06 & 90.46 & 66.54 & 93.98 & 87.93 & 81.76\\
\midrule
  \ourmodel{} (Shared input token) & UTF-8 Bytes & Yes & 47.43 & 82.03/82.65 & 89.58 & 90.18 & 89.20 & 82.03 & 93.17 & 77.95 & 81.49\\
   \ourmodel{} (Task specific input token) & UTF-8 Bytes & Yes & 49.06 & 82.14/82.64 & 89.84 & 90.53 & 89.40 & 79.69 & 93.17 & 80.02 & 81.76\\
    \ourmodel{} (Multitask query) & UTF-8 Bytes & Yes & 47.88 & 82.05/82.77 & 90.36 & 90.37 & 89.49 & 80.08 & 93.75 & 79.95 & 81.79\\
 \bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\small \label{tab:full_glue_results} Full GLUE results (higher is better). The first 3 models use SentencePiece tokens, the latter 3 use UTF-8 bytes directly.}
\end{table}

\subsection{Ablation on the number of latents}

For a given FLOPs budget, there is a trade off between the number of latents  and the width  of the latents. We ablate this in Tab.~\ref{tab:appendix_language_ablation} by varying the number of latents between 128, 256 (best), and 512. We adapt the latent dimension accordingly to match the FLOPs budget.

\begin{table}[h]
\centering
\resizebox{0.8\linewidth}{!}{
 \begin{tabular}{c c c c c}
 \toprule
  \textbf{Number of latents ()} & \textbf{Latent width ()} & \textbf{FLOPs} & \textbf{Average GLUE score}  \\
 \midrule
  128 &	1920 & 120B & 75.84 \\
  256 & 1280 & 113B & 80.95 \\
  512 &	896 & 125B & 80.92 \\
 \bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\small \label{tab:appendix_language_ablation} Ablation on the UTF-8 Bytes \ourmodel{} latent width versus depth.}
\end{table}


\section{Positional encodings for image and audio experiments}
\label{sec:fourier_pos_enc}

For all image experiments (with the exception of the ImageNet experiment that uses learned positions, Sec.~\ref{sec:imagenet_appendix}), we use a 2D Fourier feature positional encoding~\citep{vaswani2017attention, stanley2007compositional, mildenhall2020nerf, tancik2020fourier} using a sine and cosine bands with frequencies spaced linearly from a minimum frequency to a maximum frequency. We use 64 sine/cosine bands per dimension in all settings. The minimum frequency is always set to the minimum frequency of the input signal, corresponding to a single full oscillation over the input dimension. The maximum frequency is typically set to the input's Nyquist frequency (e.g. 112 cycles for an image with 224 pixels per dimension). The input position used to construct the Fourier frequencies is scaled to [-1, 1] for each input dimension. For example, the upper left corner of an image is at position [-1, -1] while the bottom right corner is at position [1, 1]. We follow the same strategy using 1D and 3D Fourier feature positional encoding for audio's time and video's spatiotemporal inputs, respectively.

\section{Optical Flow: additional details and results}
\label{sec:flow_appendix}

Pre- and post-processing can provide non-trivial inductive biases when processing image data and also change computation time.  In this section, we ablate these choices.  The network in the main paper concatenates the two frames frames before extracting 3D patches around each pixel, each of size .  Tab.~\ref{tab:optical_flow_appendix} shows a few alternative designs for patch extraction.   means that only a single pixel (or pair of pixels) is used for each input element.  `Separate frames' means that the frames are not concatenated, but rather, input array elements are extracted independently from the two frames (thereby doubling the number of input elements).  In the case of separate frames,  means essentially no preprocessing: each pixel becomes its own element with no spatio-temporal context whatsoever.

We also performed experiments with a less expensive input model which uses a  convolution to 64 channels, followed by a max pool, similar to the one used in our ImageNet experiments.  After feeding this through the \ourmodel{} architecture (including querying with the same convolutional features used as input), we have an output a feature grid with stride 4 and 64 channels, on top of which we apply a RAFT upsampling layer.  This involves a linear projection from 64 dimensions to 2, which is the coarse-resolution optical flow estimate.  We then upsample this flow for a given pixel in the high-resolution flow map by applying attention over a neighboring 3x3 block of the low-resolution flow map, following the uppsampling approach in RAFT~\citep{teed2020raft}. 

\begin{table}[t]
\centering
\resizebox{0.98\linewidth}{!}{

 \begin{tabular}{l c c c c c c c c}
 \toprule
  \textbf{Method} & \textbf{Patch size} & \textbf{Concat. frames} & \textbf{Downsample} & \textbf{Depth} & \textbf{Latents} & \textbf{Sintel.clean} & \textbf{Sintel.final} & \textbf{KITTI} \\
 \midrule
  PWCNet~\citep{sun2018pwc} & - & - & - & - & - & 2.17 & 2.91 & 5.76 \\
  RAFT~\citep{teed2020raft} & - & - & - & - & - & 1.95 & 2.57 & 4.23 \\
  \hline
  \ourmodel{} &  & Yes & No & 24 & 2048 & 1.81 & 2.42 & 4.98 \\
  \ourmodel{} &  & No & No & 24 & 2048 &1.78 & 2.70 & 6.19\\
  \ourmodel{} &  & Yes & No & 24 & 2048 &1.91 & 2.56 & 5.39\\
  \ourmodel{} &  & No & No & 24 & 2048 &1.72 & 2.63 & 5.93\\
  \hline
  \ourmodel{} & N/A & Yes & Yes & 24 & 2048 & 1.84 & 2.52 & 4.83\\
  \ourmodel{} & N/A & No & Yes & 24 & 2048 & 1.90 & 2.53 & 6.66\\  
  \hline
  \ourmodel{}& N/A & Yes & Yes & 16 & 1024 & 2.06 & 2.67 & 6.12\\
 \bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\small \label{tab:optical_flow_appendix} Ablated Optical Flow results (end-point error, lower is better). The top \ourmodel{} results show the configuration from the main paper. We ablate 1) patch size for the context surrounding each pixel, 2) whether the two frames are concatenated or input separately to the Perceiver, 3) whether the inputs and queries are downsampled by a factor of 4 using a convolution, and then subsequently upsampled with RAFT, and finally a the number of self-attention modules (depth) and number of elements in the latent array, resulting in a bottom-row network which is substantially less expensive than the original model. }
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio,width=1.0\linewidth]{figures/optical_flow_qualitative.pdf}
    \caption{Qualitative examples of optical flow. For each image pair, we show the two frames (top), and then the estimated flow (bottom left) and the ground-truth flow (bottom right).  In the left example, we see one person under heavy occlusion where the correct flow is propagated into a region with few details. Another person in the foreground has clothes with little texture and substantial blur, and yet the algorithm can propagate the flow across the entire region.  In the center example, we see very large motions from both the dragon and the person, yet many fine structures are preserved like the pole.  On the right, we see a forest scene with a few extremely small objects with very subtle motions (circled) which our algorithm is able to detect and segment correctly.}
    \label{fig:flow_qualitative}
    \vspace{-8pt}
\end{figure*}

We found that concatenating frames led to a non-trivial performance improvement across the more difficult Sintel.final and KITTI Flow 2015~\citep{menze2015object} datasets. Spatial context helps, and the impact of frame concatenation is larger when more context is available, suggesting that the algorithm is comparing spatial and temporal gradients. Convolutional downsampling and RAFT upsampling provide even more spatial context for both the input features and the queries, but this doesn't make up for the loss of resolution and overall performs slightly worse than using the full resolution.

\ourmodel{} is somewhat slower on traditional GPUs than our baseline RAFT model, but we find that the trend reverses on TPUs, which is the target architecture for our work. For ease of comparison, we report inference speed on  images, using a tiled inference setup.  Our most expensive model achieves approximately 0.8 frames/sec on a 2017 TITAN Xp, and our lightweight model (with conv downsampling and RAFT-style upsampling) achieves 3.3 frames/sec, which is not far from the 10 frames per second reported for RAFT~\citep{teed2020raft}. On the publicly-available TPU v3, however, our most expensive model achieves 4.4 frames/sec on a single TPU core, and 17.8 frames/sec for the lightweight model. An efficient Tensorflow implementation of RAFT~\citep{sun2020tf} (received courtesy of the authors) achieves only 1.6 frames/sec on the same hardware.  We suspect that the difference is due to the gather operations required for RAFT but not for Perceivers, which are slow on TPU due to their poor memory locality properties.

Fig.~\ref{fig:flow_qualitative} shows some results on example image pairs from the Sintel.final dataset.  We see that the algorithm is capable of dealing with heavy occlusion, and can propagate optical flow across large regions with very little texture.  The network can also deal with very large motions and very small objects.
 
Finally, to verify that \ourmodel{} performs well on real-world data despite being trained only on synthetic imagery, we applied it to a small number (roughly 10) real videos taken from Getty images (\texttt{www.gettyimages.com}).  \ourmodel typically performs very well out-of-domain, although some failure cases remain: for instance, shadows tend to be interpreted as objects (Autoflow contains no shadows), and large regions with compression artifacts but no other texture may result in hallucinated flow (Autoflow contains no video compression artifacts).  We include three challenging examples in the supplementary zip file, each of which depict complex motion and small objects. Perceiver IO can pick up on remarkably small objects such as the water droplets thrown by the girl's shoe in \texttt{pigeon.mp4} or the confetti in \texttt{thai\_dance.mp4}.  

\paragraph{Implementation details:} Our experiments with pixels and patches use a sine and cosine position encoding with 64 bands for both  and , plus the raw  and  values resulting in 258 extra features concatenated to the pixel or patch values. For experiments without concatenated frames, we have an additional time dimension which must be encoded with positional encoding, and for this we also use 64 sine and cosine bands (which are highly redundant, as there's only two frames).  For this version, only the elements associated with the first frame are included as queries for the decoder.  For both input and query, we project these concatenated features to 64 dimensions before inputting them into the transformer. We use a latent array with 2048 elements and 512 channels and 24 self-attention modules, each with 16 self-attention heads, unless otherwise noted.  Our experiments with convolutional downsampling and RAFT upsampling use settings that are mostly similar, although we use no additional projection as the output of the convolutional network is already 64 channels. For these experiments, the output of the perceiver decoder's cross attend is 64 channels, which is fed into a RAFT-style upsampling operation.  For the pixel- and patch-based models, total computational complexity for a forward pass on a  image is roughly 987 billion FLOPs, and there are roughly 27.9 million parameters. 

In all cases, we train on the AutoFlow dataset~\citep{sun2021autoflow}, which consists of  image pairs, for 480 epochs using a cosine learning rate schedule which starts at a learning rate of 4e-4.  We use a batch size of 512.  We use the LAMB~\citep{you2019lamb} optimizer. We also use the default curriculum for AutoFlow, which gradually increases the severity of the augmentations over time. We find that na\"{i}ve training on AutoFlow does not train, so we use an additional phase in this curriculum, where we completely disable all augmentations.  Furthermore, for this phase, we feed every image pair twice in a batch: once forward, and once reversed.  As the inverse flow is not currently available for AutoFlow, this inverse flow was computed via an approximation which averages all the flows terminating at a given pixel.

The evaluation datasets have a different resolution, so we evaluated in a tiled manner, using six evenly-spaced tiles.  For pixels that are covered by multiple tiles, we average the predictions, weighted proportional the distance to the nearest edge of the respective tile (as we expect predictions nearer to the tile edges to be less accurate).  We leave the possibility of making \ourmodel{} invariant to input shape to future work.

\section{Multimodal autoencoding: additional details}
\label{sec:multimodal_supp}

For the multimodal autoencoding experiments, we patch preprocessing for both images and audio,
and we embed the labels as one-hot labels. The patch size is  for video
and 16 for audio. The audio is sampled at 48kHz, or 1920 samples per frame.
The decoder outputs  vectors with 512 channels,
that is, one element for each pixel in the video, one element for each audio patch, and one element for the classification label.
These are then linearly projected to the appropriate channel size for each modality:
3 for videos, 16 for audio and 700 for classification (the logits for each of the 700 classes in Kinetics700).
Finally, we un-patch the audio to arrive at the output audio.
We note that we read and generate the audio waveform directly in the time domain;
we do not transform first to a spectrogram.

\begin{table}
        \centering
\resizebox{0.7\linewidth}{!}{  
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Params} & \textbf{FLOPs (train)} & \textbf{FLOPs (eval)} & \textbf{Train steps/sec} \\ \midrule
20.0M & 310B & 6.85T & 4.4  \\ \bottomrule
\end{tabular}}
\vspace*{0.1cm}
\caption{\small \label{tab:autoencoding_supp} Additional details of the model used for Multimodal autoencoding.}
\vspace{-5pt}
\end{table}

We use a 387 dimensional 3D Fourier position embedding for each input video patch and a 385 dimensional 1D Fourier position embedding for each audio patch (385 to ensure the input dimensions to \ourmodel{} match for all elements).
In addition, we pad all input elements with a learned vector
representing the modality;
inputs from the same modality share the same token.
In particular,
we add a 317 dimensional modality embedding to video elements,
a 319 dimensional modality embedding to audio elements,
and a 4 dimensional modality embedding to the label,
so that all elements have 704 features.

The decoder queries are also constructed from Fourier position embeddings for video and audio and a learned positional embedding for label:
387 features for video,
385 features for audio,
and 1024 learned features for the label.
We pad the queries for each modality with a different learned vector for each modality,
so that the final feature size for the queries is 1026.

We train on Kinetics 700~\citep{smaira2020short}. We use batch size of 1024, and learning rate of 1e-3. The training loss is a weighted sum of the L1 loss for video, the L1 loss for audio, and the cross entropy loss for the label. The weightings are 0.03 for video, 1 for audio, and 0.0001 for the label; the loss weights are imbalanced in favor of audio because it is more difficult to obtain
audio of high perceptual quality by directly outputting the waveform.
We also tried a different weighting (0.03 for video, 1 for audio, and 1 for the label) to obtain higher classification accuracy. Additional model details are given in Tab.~\ref{tab:autoencoding_supp}.

To help verify the quality of \ourmodel{}'s outputs on real-world data, we applied it a small number of real videos (10) with audio taken from Getty Images. \ourmodel{} is able to capture the structure of both video and audio inputs, despite encoding both jointly with a single network. The model introduces blurriness to both video and audio: this may be partially attributable to the preprocessing, which included coarse patching (Tab.~\ref{tab:input_output_details}) for both modalities due to the very high computational cost of processing raw video and audio inputs (which amount to over 2 million raw points). Although decoding can be done in parallel, allowing us to decode very large output arrays in sequential batches, \ourmodel{} requires all points are encoded simultaneously. Addressing this limitation and scaling to even larger inputs is an important direction for future work.

\end{document}
