\documentclass{article}
\usepackage{amsmath, amsthm, amssymb,graphicx,subfig,multirow}
\tolerance=1000

\bibliographystyle{plain}

\newcommand{\cond}[1]{\mathop{\rm{cond}}(#1)}
\newcommand{\diam}{\mathop{\rm{diam}}}
\newcommand{\icond}[1]{\mathop{\rm{icond}}(#1)}
\newcommand{\condp}[1]{\mathop{\rm{cond}}\mbox{}'(#1)}
\newcommand{\norm}[1]{\left\|#1\right\|} \newcommand{\norma}[1]{\left\|#1\right\|}   \newcommand{\inv}[1]{#1^{-1}}
\newcommand{\mpinv}[1]{#1^{\dagger}}  \newcommand\eps{\epsilon}
\newcommand\eref[1]{}
\newcommand{\nchoosek}[2]{\left(\begin{array}{c} #1\\ #2 \end{array} \right)}
\newcommand\R{\mathbb{R}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\rank}[1]{\mathop{\rm{rank}}(#1)}

\title{A condition number analysis of an algorithm for solving a system of polynomial equations with one degree of freedom\thanks{Supported in part by
NSF DMS 0434338, NSF CCF 0085969, and a grant from NSERC
(Canada).}}

\author{Gun Srijuntongsiri\thanks{Sirindhorn International Institute of Technology, Thammasat University, 131 Moo 5, Tiwanont Road, Bangkadi,
Muang, Pathum Thani, 12000, Thailand. Email: gun@siit.tu.ac.th.} \and Stephen A. Vavasis\thanks{Department of Combinatorics and Optimization,
University of Waterloo, 200 University Avenue W., Waterloo, ON N2L
3G1, Canada. Email: vavasis@math.uwaterloo.ca.}}

\begin{document}

\maketitle

\begin{abstract}
This article considers the problem of solving a 
system of  real polynomial equations in  variables.  We propose an algorithm
based on Newton's
method and subdivision for this problem.  Our algorithm is intended only for
nondegenerate cases, in which case the solution is a 1-dimensional curve.
Our first main contribution is a definition of a condition number 
measuring reciprocal distance to degeneracy
that can distinguish poor and well conditioned instances of this problem.  
(Degenerate problems would be infinitely
ill conditioned in our framework.)
Our second contribution, which is the main
novelty of our algorithm, is an analysis showing that its running
time is bounded in terms of the condition number of the
problem instance as well as  and the polynomial degrees.  
\end{abstract}

\section{Introduction}
We consider the problem of finding all zeros of a polynomial function
.  The zero-set of such a function will, in
the generic case, be a 1-dimensional algebraic set.  Our algorithm is
enumerative in nature, and therefore is feasible only in the case of small
values of . 
We refer to this problem as the {\em single-degree-of-freedom polynomial 
system problem} (SDPS).

Perhaps the most common application of the SDPS problem is finding the intersection
of two rational or polynomial surfaces, the so-called surface/surface intersection
(SSI) problem.  In this case, one is given two polynomials  both mapping
 to  that each parametrize a surfaces.  
The problem is to find their intersection,
i.e., all points  in  such that .
Other applications arise in robotics and motion planning.  A final application
is global optimization in which one finds all the local minimizers by constructing
a network of paths that connect local minimizers (see, e.g., \cite{Leary}).


Our proposed algorithm is a hybrid between subdivision and iterative
methods.  This hybrid idea has been used to solve surface/surface
intersection by Koparkar \cite{koparkar} and to solve line/surface
intersection by Toth \cite{toth}.  The approach is to subdivide the
domain recursively, discard the subdomains found to contain no
solutions, and invoke an iterative method to locate a solution once it
is certain that the iterative method converges.  The convergence tests
used by Toth and Koparkar are both based on contraction mapping and
evaluating ranges of functions.


Our first main contribution, detailed in Section~\ref{section_cond}, is the
definition of a condition number for SDPS problems.
Intuitively, a problem instance is ill conditioned if it 
is close to a degenerate instance.
A degenerate instance is one in which the Jacobian fails to have full rank at 
a root.  Our condition number is the reciprocal of a quantity
related to nearness to
degeneracy.  It is natural to expect that algorithms would have poorer behavior
as the condition number grows larger.

Our algorithm, which is presented in Sections~\ref{sec:algo}--\ref{sec:impdetails},
is similar to Koparkar's in that it subdivides the
parametric domains of the problem until the subdomains pass
certain tests. It uses a bounding volume of a subdomain to exclude
any that cannot have a solution.  Our convergence test is
based on the Kantorovich theorem, which tells us if Newton's
method converges quadratically for the initial point in question
in addition to whether it converges at all.  
For this reason, we
can choose to hold off Newton's method until quadratic convergence
is assured. Kantorovich's theorem is presented in Section~\ref{sec:kanto}.


The main feature of our algorithm is that there is a lower bound on
the size of the smallest hypercube occurring during the course of the
algorithm that depends on the reciprocal condition number of the
problem instance and on the polynomial degrees.  Because our interest is
in the low-dimensional and moderate degree case, we regard the factors
depending on the degrees as `constants' and the dependence on the
condition number as the interesting feature.
This analysis is presented in
Section~\ref{section_analysis}.  A lower bound on the smallest hypercube
size consequently implies an upper bound on the overall running time.  

As mentioned above, the SSI problem is a special case of SDPS with .  Since there are many algorithms for SSI proposed in literature, we discuss here the main advantage of our algorithm as compared to other SSI algorithms.  To the best of our knowledge, there is no previous algorithm for SSI in this class whose running time has been bounded in terms of the condition of
the underlying problem instance, and we are not sure whether such an
analysis is possible for previous algorithms.  Indeed, we do not know
of any SSI algorithm in the literature that has any {\em a priori}
bound on the running time.  We do know, however, that some algorithms
do not have this property---their running time can be arbitrarily
large even if the input instance is well conditioned.  This is because
these algorithms can sometimes create degenerate or nearly degenerate
subproblems even though the original input instance is well
conditioned.  Section \ref{section_other} shows in details how
marching methods based on collinear normal points and Koparkar's
algorithm in particular have the capacity to create bad subproblems
from a good instance.  

The notion of bounding the running time of an iterative method in
terms of the condition number of the instance is an old one, with
the most notable example being the condition-number bound of
conjugate gradient (see Chapter 10 of \cite{gvl}). This approach
has also been used in interior-point methods for linear
programming \cite{freund}, Krylov-space eigenvalue computation
\cite{toh}, and the line/surface intersection problem
\cite{srijuntongsiri_lsi}.  We note that a related problem of computing convex hull of points on a plane is shown to always be well-conditioned \cite{jiang}.

An additional motivation, not pursued further herein, for defining a
condition number and condition-aware algorithms like ours is that this
creates the possibility of preconditioning.  {\em Preconditioning},
which has been very successfully applied in numerical linear algebra
(see, e.g., \cite{TrefethenBau}), means improving the condition number
of an instance via some kind of transformation prior to solving it.

We now define the problem under consideration more precisely by
specifying a representation for the input polynomial system.
Let  denote the \emph{Bernstein polynomials}

We are interested in finding \emph{all} points  satisfying

where   denote the coefficients, also known as the \emph{control points}.  Therefore, the
problem is specified by the  control points.  
(See a further remark on this matter in Section~\ref{section_conclusion}).
This form of a multivariate polynomial is sometimes called {\em tensor product} B\'ezier
representation: it presumes 
that the maximum degree of variable  (separately) is 
for each .  
Note that  is a function that maps  to .  
Note that this representation would be intractable
for a large value of , but, as mentioned earlier, our algorithm is
intended for small values such as .

The Bernstein basis is known to 
have better numerical stability for polynomials on the unit interval
than the power basis \cite{farouki_rajan, farouki_stable_bernstein},  
and computation using parametric representation is often
much more efficient than other types of surface representations.  
Furthermore,
our algorithm makes direct use of the Bernstein-B\'ezier representation.
In particular, our exclusion test is based
on B\'ezier control points.  It
should be noted that the algorithm proposed in this article can be
generalized to use with parametric surfaces represented by other
polynomial bases provided that an appropriate exclusion test is
available, and a few other properties hold for the basis.  Refer to
\cite{srijuntongsiri_lsi} for a related algorithm for line/surface
intersection that can operate on parametric surfaces represented by
other polynomial bases.


\section{The theorem of Kantorovich}
\label{sec:kanto}

Denote the closed ball centered at  with radius  by

and let  denote the interior of .
Kantorovich's theorem in affinely invariant form, which is valid for
any norm, is as follows.

\begin{theorem}[Kantorovich, affinely invariant form \cite{deuflhard,kantorovich}]
\label{standardkantorovich}
Let  be differentiable in
the open convex set . Assume that for some point , the Jacobian 
is invertible with

Let there be a Lipschitz constant  for  such that

If  and , where

then  has a zero  in . Moreover, this zero is the unique zero
of  in  where

and the Newton iterates  defined by

are well-defined, remain in , and converge to . In addition,

\end{theorem}
We call  a \emph{fast starting point} if the quantity 
defined above satisfies  and .  In this case, quadratic convergence of the iterates
starting from  is implied.


\section{A condition number of a polynomial system with one degree of freedom}
\label{section_cond}

In this section we propose a definition for a condition number of the SDPS problem and prove that our condition number is related to the distance
from degeneracy.  Let  denote the maximum -norm among the control points of  for some .  
Later on, we will specialize to the infinity norm.
It is easy to show that this quantity satisfies
the axioms of a norm, so we will write  also as .
Define the {\em condition number} of  to be

Here, the notation 
means .  In the case that
the rank of  is , this corresponds to the Moore-Penrose
pseudo-inverse of , typically denoted as .  In general,
the Moore-Penrose pseudo-inverse is defined for matrices of all ranks.
In this paper, however, we need  only in the case that
; we will 
take the second factor  of \eref{cond_def}
to be  if the rank of  is
less than .
Similarly, the first factor is taken to be  if .
Note that small condition number means the problem is
well-conditioned.
Indeed, it follows
from \eref{eq:fMbound} and \eref{eq:Mpinvineq} below
that both terms in the min have a lower bound of  .

The rationale for this definition is that, as mentioned earlier,
a degenerate instance has a point  such that  and
.  For such a point, both terms occurring in the min
of \eref{cond_def}
are infinity, i.e., the condition number is infinite.  Thus, the
problem is ill-conditioned
if  is close to zero and  is close
to rank-deficiency at the same point .


The inclusion of the factor
of  makes the definition scale-invariant.  Note that  depends
on the choice of basis, namely, tensor-product Bernstein-B\'ezier basis,
whereas the other factors in the condition number
are basis-independent.  The dependence on
basis, however, is only up to a scalar factor depending
on degree.  This is because polynomials are a finite-dimensional vector
space, hence all norms are equivalent.  We could redefine  in a 
basis-independent manner as follows:

It follows from \eref{eq:fMbound} and \eref{theta_eq}
below that  and  differ by a scalar
that is bounded in terms of the degrees.
The definition , however, 
would be harder to compute
in practice.

This condition number is similar to the definition of
  of Cucker et al.\ \cite{Cuckeretal} 
for the problem of computing isolated roots of polynomial
systems .
Our definition is somewhat simpler than theirs, however,
because
of the assumption we have imposed
that the domain of interest
is  rather than all of .  This simplifying
assumption obviates the need for introducing projective
space and scaling of coefficients as in \cite{Cuckeretal}.

The classical Turing Theorem \cite{Chaitin} states that the condition
number of a matrix, which bounds the iteration count of the conjugate
gradient algorithm, is exactly the reciprocal of the relative distance
of the matrix to singularity.  Similarly, Shub and Smale show that
their condition number for a homogeneous polynomial system is equal to
the distance of the system to singularity \cite{shub}.  We now derive
a result showing that our proposed condition number is also related
to
the distance of an SDPS instance to degeneracy.

Before stating and proving the theorem, we
require two well known bounds concerning polynomials 
in Bernstein-B\'ezier form:

for all .
This follows because every value of  over the
parametric domain is a convex combination
of control points \cite{Farin}.
Next, 

for all .
This follows because
the control points of a column of  (i.e., a partial derivative of )
are finite differences of control points
of  multiplied by the degree in the direction of differentiation
as in \eref{eq:bezderiv1}.  A
factor of  comes from the taking of finite differences, and a further
factor of  arises from the fact that the infinity norm is a sum
over rows (not columns) of the derivative.  Furthermore, applying the equivalence of norms to \eref{eq:fpMbound} yields

for any arbitrary -norm, where  is a scalar depending on  and the choice of norm.

Now, finally, we come to the main theorem of this
section.
\begin{theorem}
Let  be a polynomial function of degrees 
 in its  variables, and assume that it
is nondegenerate, i.e., there is no  such that
 and .
Then any  satisfying

is nondegenerate, where  is a scalar depending on the degrees and the choice of norm.

Conversely,
there exists a degenerate 
polynomial  such that

where
 is another scalar depending on the degrees.
\label{thm:disttodegen}
\end{theorem}


\noindent
{\bf Remark 1.} 
As mentioned above, the vector and matrix norms appearing in 
\eref{eq:hatfminusf1}, \eref{eq:hatfminusf2}
may be any of the standard -norms, although later we will specialize
to the infinity norm.
Inequalities \eref{eq:hatfminusf1} and \eref{eq:hatfminusf2} involve
the norm of the polynomial function.  
As mentioned above, we take this to mean the maximum norm control point
when written in Bernstein-B\'ezier form.  

\noindent
{\bf Remark 2.}  Let  denote the set of 
degenerate polynomials, that is, those polynomials 
such that .
This theorem shows that 
our condition number is, up to constant factors,
the reciprocal distance of
 to  scaled by the norm of .
Consider a larger set 
of degenerate polynomials defined as follows.
Polynomial  if
there is any point  in  (not merely )
such that  and  is rank deficient. This set 
 is an algebraic variety, i.e., the coefficients of such 's are
the roots of a polynomial system.  This means that we can apply
Demmel's theorem \cite{Demmel} to conclude that the 
expected logarithm of
the condition number of a random instance is modest. (Clearly, the distance of 
a polynomial  to
 is bounded below by the distance of  to .)
 We can also apply the more
recent analysis of B\"urgisser et al.\ \cite{burgisser}
to show that the `smoothed'
condition number \cite{spielman} is modest, i.e., for any polynomial 
(even a degenerate one), if we select a random small perturbation
of it, then the resulting polynomial is expected to have
a modest logarithmic condition number.



\begin{proof}
Let  satisfy \eref{eq:hatfminusf1} and
let  (i.e., a polynomial), so that
.
Choose an
.  By definition of ,

We take two cases depending on which term achieves the min.
First, suppose .  In this case,
. 
 Assume  is sufficiently small so that .
Then

In particular, .

For the other case, the hypothesis is
.
We recall that 

(see \cite[(2.3.11) and \S5.5.4]{gvl}),
where  is notation for the th
singular value of a matrix.  By the equivalence of norms, \eref{eq:fromgvl} is equivalent to

for any arbitrary -norm, where  and  are positive constants that depend on  and the choice of norm.



A second fact from numerical linear algebra is that

(see \cite[(2.3.11) and Cor.~8.6.2]{gvl}).  
Combining these facts together with \eref{eq:ebound} yields

If we select , then we are assured in this case
that , i.e., the rank of  is
.

Thus, combining the cases, we have shown that for all , 
either
 or the rank of  is at least , 
thus proving that  is not degenerate.

Next, let us turn to \eref{eq:hatfminusf2}.   
In this proof, we will drop the prime from 
  and indeed will allow  to denote a constant
depending on the degrees that may change from line to line.


Let  be the point where the max in \eref{cond_def} is
achieved, and let  be the value of this max, i.e., the
value of . This means, first, that
.  Second, it
means that either (i)
,  or (ii) .
Let . 

If case (i) is true, then by definition of the matrix
-norm there exists a unit vector (in the norm under consideration) 

such that .  Let ,
so that  and .  
Let  (a 
matrix) and define .  
In case (ii) (when ), define
, i.e., take , and we do not need
 and .

For either case,
we now must establish that  is a degenerate instance
and that inequality \eref{eq:hatfminusf2} holds.

First, let us establish the degeneracy of .  Clearly 
is a root of .  In case (ii), , a matrix
whose rank is less than .
In case (i),
.
We claim that  has at least two independent vectors in its null space,
which implies that its rank is at most .  Observe first that , as 
an  matrix, must have a nonzero vector  in its null space.
Then  is also in the null space of  since 
and .  Also,  has  in
its null space as  evaluates to .  Finally,  and
 are independent since  whereas , which
is not zero.  This concludes the argument that  is
degenerate.  

Next, consider  appearing in
\eref{eq:hatfminusf2}.  
Let us introduce the following notation for this
argument:  denotes the polynomial  and
 denotes the constant real-valued polynomial 1. 
With these definitions, .
Note that  since .
Also, .
We have the following chain of inequalities,
which applies to case (i).
 The first line involves
norms of polynomials, whereas the remaining lines are norms of vectors
and matrices.  

The first line follows from the definition of . The second uses the
fact that  and 
are bounded by constants.  The third line uses the inequality established
earlier that  and also
expands the definition of .  The fifth line uses the fact that
, and the 2-norm and the -norm under
consideration
are related by constants  depending on .  
The last line uses the assumption that 
is a unit vector.

For case (ii), \eref{eq:hatfminusf_numer} also holds since , so
the result is already established by the third line in the above chain
of inequalities.
Since the denominator occurring in the left-hand side of
\eref{eq:hatfminusf2} is ,
and
recalling that , we see that
\eref{eq:hatfminusf_numer} proves the theorem.
\end{proof}

Although the condition number defined by \eref{cond_def} is scale invariant
(i.e.,  for ), it is not
affinely invariant.
In other words, if  is a nonsingular  matrix, then
in general .  On the other hand, our algorithm
is affinely invariant as we shall see in Section~\ref{sec:algo}.  Therefore,
we can define a new condition number that is indeed affinely invariant,
which is as follows:

Here,  denotes the set of all  nonsingular
matrices.
Obviously, \eref{eq:invarcond} is affinely invariant, and also,
it is clear that for any instance  of SDPS, .
Furthermore, if we are able to show that our affinely invariant algorithm
has running time bounded in terms of , then it will follow
automatically that it is also bounded in terms of .

The difficulty with \eref{eq:invarcond} is that there is no obvious
way to compute this quantity other than the exhaustive
method of trying out all choices of 
in a dense grid lying in .  (If the matrix 2-norm is
used, then it suffices to try a dense sampling of upper triangular matrices,
a smaller search space,
since the  in a  factorization of  does not affect the norm.)
Unless a better method
can be found,  
definition \eref{eq:invarcond}
would be useful in practice
mainly in cases where there is {\em a priori} information
about a linear transformation that improves the condition number.



\section{Performance of other algorithms for  case on well conditioned instances}
\label{section_other}

Recall that SSI is a special case of SDPS with .  Due to the 
abundance of SSI algorithms in literature, we compare our algorithm to well-known SSI algorithms.  Many previously published SSI algorithms work
well in practice and are widely used in computer-aided geometric design software.  Nonetheless, we
suspect that most of these algorithms can behave nonrobustly in the sense
that, given a well conditioned problem instance, they can sometimes internally
generate an arbitrarily ill conditioned subproblem that they then
must solve.  If an algorithm
is capable of this behavior, then it is not possible to bound
its running time in terms of the instance's condition number as we shall do
for our algorithm.
Indeed, as far as we know, there is no \emph{a priori} running time upper bound for any SSI algorithm in the literature.
In this section, we consider how two well-known SSI
algorithms can generate bad subproblems given good problem instances.


\subsection{Marching methods based on collinear normal points}
\label{section_sederberg}
Collinear normal points are points on the two surfaces whose normals
are collinear.  Marching methods based on collinear normal points
split the parametric domains in at least one direction at these
collinear normal points.  The consequence is all solution curves
have one point on the boundaries of the resulting subdomains provided
that the dot product of any two normal vectors of either surfaces is
never zero.  These points are located by a curve/surface intersection
algorithm and used as starting points for the marching step.

Consider applying collinear normal points marching methods to find
intersections between the two B\'{e}zier surfaces  and  whose
control points are defined in Table \ref{table_sed1} and Table
\ref{table_sed2}, respectively.  This problem is equivalent to solving SDPS with control points

where 's  are control points of  and 's  are control points of .
The condition number of this instance
is , which is reasonably well-conditioned.  The instance
is also intuitively well-conditioned as there are neither complicated
nor almost singular intersections.  The two surfaces and their
intersection in object space are shown in Figure \ref{fig_sed}.

\begin{table}
\begin{center}
{\scriptsize{
\begin{tabular}{|c|c|c|c|}
\hline 
 &  &&\\
\hline
&& &  \\
& && \\
& &&    \\
\hline
\end{tabular}
}}\end{center}
\caption{The control points of B\'{e}zier surfaces .  The entry in the th row and the th column is the control point  of .\label{table_sed1}}
\end{table}
\begin{table}
\begin{center}
{\scriptsize{
\begin{tabular}{|c|c|c|c|c|}
\hline 
 &  &&&\\
\hline
&&&&\\
&&&&\\
&&&&\\
&&&&\\
\hline
\end{tabular}
}}
\end{center}
\caption{The control points of B\'{e}zier surfaces .  The entry in the th row and the th column is the control point  of .\label{table_sed2}}
\end{table}

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{sederberg_ex}
\caption{An example of a well-conditioned instance of SSI 
problem where collinear normal point methods and Koparkar's algorithm 
require long computation time to solve.}
\label{fig_sed}
\end{figure}

Figure \ref{fig_sed_para} shows a pair of collinear normal points in
parametric space and the subdivision of domain at these points, as
well as the intersection between the surfaces (Other pairs of
collinear normal points are not shown).  Parts of the intersection
curves lie almost parallel to the boundaries of the created
subdomains.  In other words, the cuts made by the algorithm happen to
intersect degenerately or nearly degenerately with the problem data.
Thus, after the algorithm has made a subdivision of the domain
based on the collinear normals,
it must now recursively solve
arbitrarily ill conditioned subproblems since the
intersection point between the surface and the boundary curve is
almost singular. The running time of algorithms for finding these
intersection points typically depends on their conditioning
(see, e.g., the RIA algorithm of \cite{patrikalakis}).
Thus, the running time of the algorithm cannot be
bounded in terms of the condition number of the input.  
Furthermore, ill-conditioning of the
subproblems may cause unexpected inaccuracy of the solution of the
original well-conditioned instance.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{sederberg_ex_para_thick}
\caption{Intersection of the surfaces in Figure \ref{fig_sed} in parametric space and splitting of the domains at a pair of collinear normal points showing that marching methods based on collinear normal points create ill-conditioned subproblem instances from a well-conditioned original instance.  Solid lines are the intersections. Asterisks are the pair of collinear normal points.  Dotted lines are the splitting lines at those points. To the left is the parametric domain of .  To the right is that of .}
\label{fig_sed_para}
\end{figure}

\subsection{Koparkar's algorithm}
\label{section_koparkar_cond}

Koparkar's algorithm uses a test based on the contraction mapping theorem
to determine if, in a given domain, a Newton-like method converges or
the two surfaces do not intersect at all.  If the Newton-like method
is guaranteed to converge, the method is used to find part of the
solution curves inside the domain.  If the two surfaces are known 
not to intersect, the domain is discarded.  Otherwise, each surface is
subdivided by splitting their parametric domains into four, 
and the test is repeated on the created subdomains.
This process continues until the entire domain is examined.

The test in Koparkar's algorithm requires the ability to evaluate
ranges of functions, which is typically accomplished by variety of
interval arithmetics.  None of these techniques can give the exact
ranges, however; they yield supersets of the ranges.  For this reason,
the convergence test is likely to fail when part of the
solution lies very close or directly on the border of a subcube in
both -space and -space at the same time, which is
not necessarily on or near the border of the original domain
.  The same problem instance discussed in Section
\ref{section_sederberg}, which is shown in Figure \ref{fig_sed}, has
such problem.  The domain  does not pass the test, and the
domain is subdivided at the midpoints as shown in Figure
\ref{fig_koparkar}.  The subdomains now have solutions directly on a
boundary.  Koparkar's algorithm needs to subdivide these subdomains to
very small ones before the convergence test is satisfied.  This
example demonstrates that Koparkar's algorithm is inefficient at
solving certain well-conditioned instances.  The algorithm may solve
other instances with higher condition numbers but without any parts of
the intersections near any boundaries of the subdivided subdomains
faster than this well-conditioned instance.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{koparkar_ex_thick}
\caption{Intersection of the surfaces in Figure \ref{fig_sed} in parametric space and subdomains created after the original domain  fails Koparkar's test.  Due to part of the intersections lying on a boundary of a subdomain, Koparkar's algorithm requires many subdivisions before those parts of the intersections can be located.  Solid lines are the intersections.  Dashed lines are the subdivision lines. To the left is the parametric domain of .  To the right is that of .}
\label{fig_koparkar}
\end{figure}


\section{The Kantorovich-Test Subdivision algorithm}
\label{sec:algo}

This section describes our algorithm for the SDPS problem. 
Some details are postponed to the next section.
Since we are interested in solutions of  within the hypercube , and the closed ball  defined in the infinity norm is a hypercube, our algorithm uses the infinity norm for all of its norm computation.  Therefore, for
the rest of this article, the notation  is used to
refer specifically to infinity norm.

During the computation, our algorithm maintains a list of
\emph{explored regions} defined as parts of the domain 
guaranteed by Kantorovich's Theorem to contain only
the solutions that have already been found.  This list is used
in addition to another test to determine whether to subdivide a
hypercube.  We define the \emph{Kantorovich test} on a hypercube
 as the application of Kantorovich's Theorem
on the point  to the function  for each  and any .  The hypercube  is used as the domain
 in the statement of the theorem, and

is used as . For , we instead use , where  is defined by (\ref{computedlip})
below, as the minimal  is too expensive to compute. The
hypercube  passes the Kantorovich test if there exists an  such that for every ,
 and .

The choice of  mentioned in the previous paragraph is used in
the analysis of the algorithm in Section~\ref{section_analysis}, but
in practice a smaller  such that 
may be used.  The advantage of using a smaller  is that the Lipschitz
constant  will be smaller, so the 
inequality 
may be satisfied
more easily (i.e., for larger hypercubes) than using the full .
The disadvantage, however, is that if  is chosen too small,
then
the condition  of the theorem may be hard
to satisfy. In particular, the choice  would be
unacceptable for this reason.

If  passes the Kantorovich test, then three important consequences
follow.  First,  is a fast starting point for
 for the particular  that satisfies the condition of
the Kantorovich test and any . Second,
the segment of the solution curve of  that contains the root
guaranteed by the conclusion of Kantorovich's theorem is not a loop in -space inside  (although it may be part of a loop in the
original domain).  Third, an explored region for this segment of the
solution curve can be derived. The explored region is

where  and  are  and 
in the statement of Kantorovich's theorem with respect to
.  Observe that  is a hyperrectangle in
 and can be stored and computed succinctly as
detailed in Section \ref{section_impkan}.  Note also that the
explored region provides an effective way to prevent the points on
different but nearby solution curves from being incorrectly
joined into the same curve.

The other test our algorithm uses is the exclusion test. For a
given hypercube , let  be the Bernstein polynomial
that reparametrizes with  the function defined by  over
.  In other words, , where
 is a composition of a dilation and translation
(uniquely determined) such that  is
bijective.  (See Section~\ref{sec:reparam} for information how to
efficiently compute .)
The hypercube  passes the \emph{exclusion test} if the
convex hull of the control points of  excludes the
origin.  It is a well-known property of the Bernstein-B\'ezier representation
of a polynomial  that 
lies in the convex hull of its control
points, where  represents its natural parametric domain.
Thus, if the hull of the coefficients of a polynomial system
excludes the origin, this system has no solutions in the parametric domain.
We can check whether the hull excludes the origin by solving a low-dimensional
linear programming problem.  Megiddo \cite{Megiddo} showed that low-dimensional linear
programming problems can be solved in linear time (i.e., linear in the number
of control points), although we have not used Megiddo's algorithm.
Note that some other polynomial bases also have a similar exclusion test;
refer to \cite{srijuntongsiri_basis}.

We now proceed to describe our algorithm, the
\emph{Kantorovich-Test Subdivision algorithm} or KTS in short.

\begin{flushleft}
\textbf{Algorithm KTS}:
\end{flushleft}
\begin{itemize}
\item Let  be a queue with  as its only entry. Set . \item Repeat until 
\begin{enumerate}
\item Let  be the hypercube at the front of .  Remove 
from . \item If  for all ,
\begin{itemize}
\item Perform the exclusion test on 
\item If  fails the exclusion test,
\begin{enumerate}
\item Perform the Kantorovich test on 
\item If  passes the Kantorovich test,
\begin{enumerate}
\item Perform Newton's method on , where  is the
index that satisfies the condition of the Kantorovich test and , starting from  to find a zero .

\item Trace the segment of the solution curve using  as
the starting point and going toward  direction until the
 boundary is reached.

\item If the newly found segment is contained in any  (i.e. the segment has been found before), discard the segment.

\item Otherwise, compute the new explored region  according
to (\ref{explored_region}).  Set .
\end{enumerate}
\item If either  fails the Kantorovich test or  passes the
test with , subdivide  along all 
parametric axes into  equal smaller hypercubes. Add these hypercubes to
the end of .
\end{enumerate}
\end{itemize}
\end{enumerate}
\item Check if any two segments of solution curves overlap. If
so, remove the overlapping part from one of the segments.

\item Join any two segments sharing an endpoint into one
continuous curve. Repeat until there are no two curves sharing an
endpoint.
\end{itemize}
A few remarks are needed regarding the description of the KTS
algorithm.
\begin{itemize}
\item The subdivision in step 2.c is performed in the case that
 passes the Kantorovich test but  because,
in general, passing the Kantorovich test does not imply that there
is only one solution curve in .

\item The check in step 2.b.iii is necessary since the segment detected
by the Kantorovich test may be outside of .

\item For the same reason as above, certain parts of a solution curve may be traced twice and hence must be removed
from one of the segments before the segments are joined.  The
overlapping segments can be detected by checking if an endpoint of
a segment is inside an explored region of another segment.  The
segments sharing an endpoint can also be detected from explored
regions in a similar manner.  Note that there is no ambiguity in this
step because an explored region, having passed the Kantorovich test,
cannot contain more than one connected component of the solution.

\item If the Kantorovich test is not applicable for a certain
hypercube due to the Jacobian of the midpoint being singular, the
hypercube is treated as if it fails the Kantorovich test and is then subdivided by step 2.c.
\end{itemize}

One property of KTS is that it is affinely invariant.  In other
words, left-multiplying  with an  matrix  prior to
executing KTS does not change its behavior. This is the main
reason that we introduced  earlier.




\section{Implementation details}
\label{sec:impdetails}

The implementations of certain steps of KTS are not apparent and thus
are explained in detail in this section.  While this
section focuses only on the B\'{e}zier surface case,
all the results herein can be generalized to certain other polynomial
bases as mentioned in the introduction.



\subsection{Computation of Lipschitz constant}
\label{section_implip}

For simplicity, denote  as  when the choice of 
is clear from context.  The Lipschitz constant for , which is required for the Kantorovich test, is obtained
from an upper bound over all  of the derivative of 

where  denotes the th entry of
. Let  be
the Bernstein polynomial that reparametrizes with  the
surface defined by  over  (Refer to Section~\ref{sec:reparam}).
We have

Note that each entry of  can be written as a Bernstein
polynomial efficiently because

where , which can be used to compute
the control points of the derivatives in Bernstein basis from a
given Bernstein polynomial directly. Hence, the maximum absolute
value of the control points of  when written in
Bernstein basis is an upper bound of . Let  denote the Lipschitz
constant computed in this manner, that is,

 are the control points
of .


\subsection{The Kantorovich test and solution curve tracing}
\label{section_impkan}

Recall that for a hypercube  to pass the Kantorovich test,
there must exist an  satisfying  and  for
\emph{all} functions 's where . The algorithm, however, need not explicitly check the
conditions for all values of .  Notice that  and
 are independent of  and  is an increasing function
of .  For these reasons, KTS only needs to check the
conditions for the value of  that maximizes . Similarly,
the explored region  can be computed solely from the
maximizer .  But note also that  is linear in , which
means that the value of  maximizing  is either 
or .

After a hypercube passes the Kantorovich test, the segment of the
solution curve detected by the test must be traced.  Since
the Kantorovich test guarantees that performing Newton's
method on  starting on  converges for any , we can trace the segment by
repeating Newton's method starting on  for many different
values of  to locate the points on the segment of the
solution curve.  Alternatively, we can perform Newton's method on  starting on  to find a point  on the segment, use  as the starting point for Newton's method on , ,
to find the next point  on the segment, use  as the starting point on 
 to find the next point on the segment, and so on.


\subsection{Reparametrization}
\label{sec:reparam}

There are two steps of KTS involving reparametrization of
polynomials in Bernstein basis, namely the exclusion test and the
computation of the Lipschitz constant for the Kantorovich test.
``Reparametrization'' in this context means the computation of new
control points that describe the same function with respect
to the new parameter domain.
Both steps require the reparametrization with  of
Bernstein polynomials with  variables, which is a
straightforward extension of reparametrization with  of
bivariate Bernstein polynomials.  An example of efficient 
algorithms for reparametrizing bivariate Bernstein polynomials with  is discussed in \cite{srijuntongsiri_basis}.  Alternatively, the polynomials can be reparametrized by two applications of the \emph{de Casteljau algorithm} \cite{patrikalakis}; one to reparametrize the right endpoints with  and another to reparametrize the left endpoints with .

\section{Time complexity analysis}
\label{section_analysis}

In this section, we prove a number of theorems leading to the
theorem regarding the running time of the KTS algorithm. Since
both the exclusion test and the computation of the Lipschitz
constant in the Kantorovich test use the control points in their
computations, it is useful to find the relationship between the
control points and the function values of the polynomial defined
by them. Recall that  is defined as the maximum norm among
control points of  and was denoted  earlier.

We have already shown that  and  are bounded
by  in 
\eref{eq:fMbound} and \eref{eq:fpMbound}.
Using the same logic, we can derive
a Lipschitz bound on , i.e., an upper bound on
, as follows:


The use of the Kantorovich theorem requires a Lipschitz bound
for a slightly larger region.  If we require a Lipschitz bound for
 over , then we can argue based on the
deCasteljau algorithm for evaluating B\'ezier polynomials that


Another useful inequality is that for any ,

Equation \eref{eq:Mpinvineq} can be established
by multiplying both sides of \eref{eq:fpMbound}
by   and then using
the fact that
 for any
matrix -norm, and for the infinity-norm in particular.


Now, we establish a bound that reverses \eref{eq:fMbound},
namely,

for any polynomial , where  is as
defined as follows.

This value of   is specific to the choice of the
Bernstein-B\'ezier
basis; see \cite{srijuntongsiri_basis} for a discussion of other bases.


Our proof of \eref{theta_eq} is based on establishing a similar
result for univariate polynomials as shown by the following
lemmas.
\begin{theorem}[Srijuntongsiri and Vavasis \cite{srijuntongsiri_basis}]
\label{bibound} Let  be a polynomial system

where . The norm of the coefficients can
be bounded by

where

\end{theorem}

\begin{lemma}
\label{theta_to_gen} 
Let  and  be constants satisfying .  Let  be a given positive integer.  Suppose there exists a function  satisfying

for any   and any univariate polynomial
, where  denotes the
polynomial basis.  Suppose also there exists a function  satisfying

for any   and any polynomial in  variables 

Then

for any  , where
 is the 
polynomial in  variables defined by 

\end{lemma}
\begin{proof}
Let  be an arbitrary polynomial defined as in (\ref{definef}).  For any -tuple  , define 

Applying (\ref{eq:bound_1d}) to  yields

for any  .  Let  be a point where  is achieved.  Define 

Applying (\ref{eq:bound_nd}) to  yields

for any  ().  Consequently, by combining (\ref{eq:g1}) and (\ref{eq:g2}),

\end{proof}

Hence, (\ref{theta_eq}) holds by induction using Theorem \ref{bibound} as the basis and Lemma \ref{theta_to_gen} as the inductive step.

Recall that the Lipschitz constant  given by
(\ref{computedlip}) is not the smallest Lipschitz constant for
 over . However, we
can show that , where 
denotes the smallest Lipschitz constant for  over
. Since  is computed from the absolute values of
the control points of , by (\ref{theta_eq}),

With this bound on , we can now analyze the behavior of the Kantorovich test.  


The following is the main technical result of this article.
\begin{theorem}
\label{thm2} Let  be a Bernstein polynomial system in  dimensions such that .
Let  be a point in .
Let  be such that . If

where

then either
\begin{enumerate}
\item The hypercube  passes the Kantorovich test
and the associated explored region  contains , or 
\item
The hypercube  passes the exclusion test.
\end{enumerate}
\end{theorem}
\begin{proof}
Let  denote the hypercube . 
Let us introduce additional notation for frequently used quantities.
Let  stand for , and
let  
stand for .
The proof is
divided into two cases by the relative value of the two
terms in the definition of condition number \eref{cond_def} 
evaluated at .
Let 


\begin{flushleft} \textbf{Case 1: } .
\end{flushleft}
Note that this case encompasses the possibility that , i.e.,
that  lies on a solution curve.  On the other hand, this
case requires , i.e., .
Note that  is the second term of \eref{cond_def} evaluated
at  while  is the first term.  Therefore,

Since  and 
(combine \eref{eq:Mpinvineq} and \eref{eq:k1def}), it follows
that the second term dominates the first, hence 

By the hypothesis for this case, , i.e.,


Let  be the unit-length null vector of , i.e.,
, .  Let  be such that
. Define 

for an arbitrary . By using the facts that
, , and

where  denotes the th column of the identity matrix, it is
seen that

for any . 
(Note that in the infinity norm, , 
where  is the index such that .)
The first parenthesized
term on the right-hand side of \eref{eta_ineq} is clearly bounded
by , which in turn is bounded by  by
\eref{eq:case1phibd}.

It follows from \eref{eq:case1condf}, \eref{eq:Mpinvineq}, and
\eref{eq:k0def} that , hence
.  Combining this with
\eref{deltahatcond} 
and \eref{eq:case1condf}
yields

Thus, proceeding from \eref{eta_ineq},



Note that  is a Lipschitz
constant of  on  by \eref{eq:exLip}.  Consequently,  is a Lipschitz
constant of   on . From this bound,
we derive a Lipschitz constant
 for
  over  as follows.

For the last line, we used
\eref{eq:exLip}.
But from \eref{hodef},

where  is the Lipschitz constant for  over

and  is the \emph{computed} Lipschitz constant for
 over  as defined in (\ref{computedlip}).

Combining \eref{eq:etabound} with
\eref{eq:omegahat} yields

By choice of  and  in \eref{eq:k0def} and \eref{eq:k1def}
respectively, we see that 
for any , which is one of the conditions
for  to pass the Kantorovich test.

For the other condition, note that  for . Therefore,

By choice of  and , we conclude that  and
therefore , the domain for which  is
a Lipschitz constant.
This proves that  satisfies the Kantorovich conditions.

Finally, the associated explored region  contains  because

for any .
The last line follows from \eref{eq:k0def} and \eref{eq:rbdcase1}.

\begin{flushleft} \textbf{Case 2: } .
\end{flushleft}
Note that this case encompasses the possibility that ,
i.e., .  On the other hand, this case requires
, i.e.,  is not a root.
Since , we conclude

which implies by \eref{deltahatcond} that

Select an arbitrary .  
We now derive a bound on  by
applying the fundamental theorem
of calculus.

Thus, by definition of ,

Define  such that

In other word,  is a polynomial that
reparametrizes with  the surface defined by  over .
In terms of , (\ref{before_define_fhat}) is equivalent to

for an arbitrary , where  is the rescaled
, and  is the rescaled  according to
(\ref{rescale}). In particular,

Let . 
By (\ref{theta_eq}),

for any control point  of , which is
equivalent to

for any control point  of  (since a constant
additive term to a polynomial corresponds to a translation of all of
its control points). Substituting
(\ref{shifted_theta}) into the left-hand side of (\ref{beforebi})
yields

which implies that the convex hull of the control points of
 does not contain the origin. 
Therefore,  passes the exclusion test.
\end{proof}


\section{Computational results}
\label{section_comp}
The KTS algorithm is implemented in Matlab and is tested against a
number of problem instances with three equations and four variables of varying condition numbers.  Higher dimension problems, especially the ill-conditioned instances, require too much computation time due to the large number of hypercubes that must be considered.  We estimate the condition number by evaluating  at the center point  of every square considered by KTS during its execution and also at uniformly sampled points in .  

Table \ref{table_res} compares the efficiency of KTS for each test
problem with its condition number.  The total number of hypercubes
examined by KTS during the entire computation, the width of the
smallest hypercube among those examined, and the maximum number of
Newton iterations to converge are reported.  Note that the high
number of Newton iterations of some test cases (the th and th rows of Table \ref{table_res}) is because the Jacobians of the zeros are ill-conditioned causing large roundoff error in the computation of the Newton iterations.


\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|r|}
\hline \multirow{2}{*}{} &
\multicolumn{1}{|c|}{\multirow{2}{*}{}} & \multicolumn{1}{|c|}{Number of} & \multicolumn{1}{|c|}{Smallest} &
Max. Newton \\
& & hypercubes & \multicolumn{1}{|c|}{width} & \multicolumn{1}{|c|}{iterations} \\
\hline 2 & 6.60 & 641 & .03125 & 3 \\   2 & 11.5 & 3089 & .01563 & 3 \\ 2 & 15.5 & 673 & .03125 & 3 \\  3 & 24.0 & 145 & .06250 & - \\ 3 & 50.0 & 4273 & .00781 & 3 \\ 3 & 120 & 1009 & .00391 & 3 \\ 3 &  & 18177 & .00049 & 3 \\ 3 &  & 15841 & .00195 & 4 \\ 3 &  &  28881 & .00098 & 7 \\ 3&  & 29649 & .00098 & 7 \\ \hline
\end{tabular}
\end{center}
\caption{Efficiency of KTS algorithm on problems of different
condition numbers.\label{table_res}}
\end{table}


\section{Conclusion and future directions}
\label{section_conclusion}

We present the KTS algorithm for solving systems of polynomial equations with one more unknowns than the number of polynomials.  By using the combination of subdivision and Kantorovich's theorem, our algorithm can take advantage of the
quadratic convergence of Newton's method without the problems of
divergence and missing some solutions that commonly occur with
Newton's method. We also show that the efficiency of KTS has an
upper bound that depends solely on the condition number of the
problem instance.  Nevertheless, there are a number of questions left
unanswered by this article such as
\begin{itemize}
\item \textbf{Tighter bound on .}
Some of the bounds in Section~\ref{section_analysis} appear
loose and could potentially underestimate the performance
of the algorithm.  For example, the scalars may be loose,
and one step in the argument preceding \eref{eq:rbdcase1}
uses the weak bound that  since .  Thus, it
seems like there is room for tightening the analysis.  

A second
limitation of our analysis is that we establish a lower bound on
the smallest hypercube size, which indirectly places an upper bound on
the total number of hypercubes explored by the KTS algorithm (and hence its
running time).  This
upper bound, however, is usually far from tight as illustrated by
our computational experiments.  Thus, a different analysis that
addresses the number of hypercubes more directly would be useful.

\item \textbf{Using KTS in floating point arithmetic}. In the
presence of roundoff error, we may need to make adjustments for
KTS to be able to guarantee that the computed intersections are
accurate and that all of the solutions are found.

\item \textbf{Handling singular solutions and degenerate instances}.  Instances containing singular solutions or degeneracy are ill-posed, and our proposed KTS algorithm does not aim at handling such instances.  Certain applications, however, look for singular solutions or solutions to degenerate instances.  Further investigation on extending KTS to handle these situations would be beneficial.

\item \textbf{Other representations of }.  As mentioned in the introduction, we
assume that  is specified  by its  
Bernstein-B\'ezier control
points.  In many applications, however, there may be a more parsimonious representation.
For example, in the SSI problem, two surfaces of bi-degree 
are separately each represented by
 control points, hence  is fully described
by  control points rather than the
 control points needed for the general case.
It would be useful if the KTS algorithm could work 
directly on a more concise representation.


\item \textbf{Extension to general underdetermined polynomial systems}.  Polynomial systems with  equations and  unknowns generally contain higher dimension solutions.  The subdivision and exclusion test ideas still hold for the general case, but a different technique is needed to trace an approximation to the intersection surface. 


\end{itemize}

\section{Acknowledgements}
We benefited from a helpful discussion with F.~Cucker about
condition numbers.

\bibliography{allbib}

\end{document}
