\section{Experimental results}
\label{Exp_results}
\subsection{Few-shot action recognition}



\textbf{Implementation details.}  In \cite{zhu_18_compound}, Zhu and Yang introduced a dataset for few-shot video classification, that is a modification of the original Kinetics dataset \cite{kay_17_kinetics}. In this work, we follow \cite{zhu_18_compound} and use their dataset and evaluation protocol. The dataset videos are sliced into fixed length segments of 16 frames each, and then these segments are fed to a C3D network pre-trained on Sports-1M \cite{karpathy_14_sports1m}. Visual features are extracted from the last FC layer (i.e. FC7) of the C3D network, and used as input to the embedding bidirectional GRU. The dimension of the C3D features is 4096, and the GRU has a hidden state of size 256. In the relation module, the output of the comparison layer is used as input to a unidirectional GRU layer of size 256, and the GRU output at the last time step is fed to an FC layer with a single neuron for predicting the final relation score. We use  episodes for training,  for validation, and  for testing. Each episode has a sample set of 5 classes. We evaluate the performance of our architecture on the validation set every  training episodes. The best-performing model on the validation set is used for testing. We train our architecture using Stochastic Gradient Descent (SGD) with momentum , and learning rate equal to  for the first  training episodes, and  for the last  episodes.







\textbf{Results.} In our first experiment, we investigate the impact of the various functions that can be used as distance/similarity measure in the comparison layer, on the TARN performance. Following \cite{wang_17_match}, we compare five different distance measures (Mult, Subt, NN, SubMultNN, EucCos). Table~\ref{FSL_Metrics} shows the accuracy obtained by the TARN model over the different measures. EucCos leads to the best accuracy over all shots. Although EucCos is a fixed measure with no learnable parameters, the following layers in the relation module are trainable and non-linear.





\begin{table}[!b]
\centering
\begin{adjustbox}{width=0.73 \textwidth,center}
\begin{tabular}{|P{1.0cm}|P{2.0cm}?{0.4mm}P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|}\hline
Method     & Measure    & 1-shot & 2-shot & 3-shot & 4-shot & 5-shot  \\
\Xhline{3\arrayrulewidth}
\textbf{TARN}        & Mult\color{white}{*} & 63.10 & 71.16 & 74.08 & 76.36 & 75.64  \\
\hline
\textbf{TARN}        & Subt\color{white}{*} & 64.82 & 70.70 & 73.90 & 76.26 & 77.54  \\
\hline
\textbf{TARN}        & NN\color{white}{*} & 63.26 & 70.46 & 72.70 & 75.62 & 75.58  \\
\hline
\textbf{TARN}        & SubMultNN\color{white}{*} & 66.10 & 73.74 & 75.44 & 77.08 & 78.20  \\
\hline
\textbf{TARN}        & EucCos\color{white}{*} & \textbf{66.55}  & \textbf{74.56}  & \textbf{77.33}  & \textbf{78.89}  & \textbf{80.66}  \\
\hline
\end{tabular}
\end{adjustbox}
\caption{TARN model accuracy when using different similarity/distance measures in the comparison layer.
}
\label{FSL_Metrics}
\end{table}





In the next experiment, we investigate the benefits of using segment-by-segment attention and comparing segment-wise representations in our architecture. To do so, we compare the TARN model to another model that has no attention layer and performs a single comparison for each video pair. Specifically, we modify the embedding module by replacing the bidirectional GRU with a unidirectional GRU of size 256, and the relation module by replacing the unidirectional GRU with a FC layer of size 256 for deep metric learning. In this case, the embedding GRU output at the last time step summarizes the entire video into a single vector. We call this model ``TARN-single''. Table ~\ref{FSL_SOA} shows the obtained accuracies by both the TARN and TARN-single models. Aligning video segments through attention and comparing segment-wise embeddings leads to consistent gains over the different shots. Furthermore, TARN model achieves better accuracy than the state-of-the-art method \cite{zhu_18_compound}. The accuracy gains hold for all shots, with significant boosts in the more difficult one-shot case, showing that even with a single sample per class in the sample set, the TARN model can still perform well.








In the last experiment, we compare the TARN model to the architecture proposed in \cite{zhu_18_compound} when using the same visual features. In the comparison, we use features extracted from a ResNet-50 \cite{he_16_resnet} model pretrained on ImageNet \cite{russakovsky_15_imagenet}. ResNet features are extracted every 3 frames. We modify the TARN model to use the frame-level ResNet-50 features instead of the C3D ones, by replacing the bidirectional GRU in the embedding module by a unidirectional GRU, that is applied over video segments and gives an output at the last time step of each segment. The rest of the network remains the same to that of the original TARN model. This model is called  ``TARN-f''. In this comparison, we also show the effect of not using attention and segment-level comparisons. To do so, the unidirectional GRU is applied over the entire video, and then the GRU output at the last time step is fed to the comparison layer. This model is called ``TARN-f-single''. Table ~\ref{FSL_SOA} shows the results obtained by both the TARN-f and TARN-f-single models. First, we can see that TARN-f performs better than CMN \cite{zhu_18_compound} when using the same visual features (ResNet-50). Second, using attention and segment-wise comparisons improves the performance of our architecture. Finally, the C3D features perform better than the ResNet features, when trying to compare video segments in a few-shot scenario. This is probably due to the fact that C3D features are spatiotemporal, while ResNet features are static. 







\begin{table}[!t]
\centering
\begin{adjustbox}{width=0.86 \textwidth,center}
\begin{tabular}{|l|P{2.0cm}?{0.4mm}P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|}
\hline

Method     & Features    & 1-shot & 2-shot & 3-shot & 4-shot & 5-shot  \\
\Xhline{3\arrayrulewidth}

\textbf{CMN~\cite{zhu_18_compound} - ECCV 2018} & \multirow{3}{*}{ResNet-50} & 60.5   & 70.0   & 75.6   & 77.3   & 78.9   \\ \cline{1-1} \cline{3-7} 

\textbf{TARN-f-single} &  &  57.92  &  63.96  &  65.90  &  68.52  &  70.64  \\ \cline{1-1} \cline{3-7} 

\textbf{TARN-f} &  & 64.83  & 72.94  & 76.22  & 78.02  & 78.52  \\ \hline

\textbf{TARN-single} & \multirow{2}{*}{C3D} & 62.84 & 69.80 & 73.96 & 74.88 & 76.88 \\  \cline{1-1} \cline{3-7} 

\textbf{TARN}        &   & \textbf{66.55}  & \textbf{74.56}  & \textbf{77.33}  & \textbf{78.89}  & \textbf{80.66}  \\  \hline


\end{tabular}
\end{adjustbox}
\caption{Accuracies of the the state-of-the-art method \cite{zhu_18_compound}, as well as the TARN model at different settings.}
\label{FSL_SOA}
\end{table}



\subsection{Zero-shot action recognition}
\textbf{Datasets and settings:} We use two action recognition datasets to evaluate our architecture, namely UCF-101 \cite{soomro_12_ucf101} and HMDB51 \cite{kuehne_11_hmdb51}. UCF-101 has 13320 video clips from 101 classes, while HMDB51 has 6766 clips from 51 classes. Following \cite{wang_17_bidilel}, we divide the 101 actions in UCF-101 into 51/50 and 81/20 (seen/unseen) classes. For HMDB51, we divide the 51 actions into 26/25 classes. In the literature, different numbers of splits (ranging between 5-50) are randomly generated for the seen/unseen classes, and the mean accuracy and standard deviation are reported over them. In this work, we follow \cite{wang_17_bidilel} and use 30 random splits used by \cite{xu2015semantic} for the 51/50 and 81/20 cases in UCF-101, and for the 26/25 case in HMDB51.



\textbf{Video/class representations:} In our experiments, we use two types of semantic representations, both of which are widely used in the literature. First, we use the 115 binary semantic attributes (denoted as ``Attr'') that are manually annotated by \cite{THUMOS14} for UCF-101. To the best of our knowledge, there are no semantic attributes available for HMDB51. Second, we use 300-dimensional Word Vectors (mentioned as ``WV'') generated by the skip-gram model of \cite{mikolov2013distributed}, that is trained on the Google News dataset. While there are other ways of representing semantic information, an extensive analysis of their influence is beyond the scope of this work. For example, \cite{qin2017zero} showed better performance using Error-Correcting Output Codes (ECOC). Here, we follow the majority of the works and use attributes and/or word vectors. Similar to FSL, we use C3D for extracting visual features.



\textbf{Implementation details:} The C3D features are embedded using a bidirectional GRU layer with a hidden state of size . Hence, the output dimension of the GRU at each time step (i.e. video segment) is of size . The semantic information is embedded using two FC layers with  and  nodes. The deep network used for metric learning has two FC layers of size  and . We train our architecture in an end-to-end fashion using episode-based training strategy \cite{vinyals_16_oneshot, snell_17_prototypical}. Adam optimizer \cite{kingma2014adam} with an initial learning rate set to  and gradient clipping to  is used in the training. The architecture is trained for  episodes and tested for  episodes. During training, episodes/batches of size  for UCF-101 and  for HMDB51 are used, while in testing episodes are formed from all the unseen classes in the target split. We evaluate our architecture every  training episodes.



\textbf{Ablation studies:} In Table~\ref{TARN_ZSL} we show the results obtained by our architecture on ZSL using different settings. The first and second settings (first two rows in Table~\ref{TARN_ZSL}) show the performance of our architecture when having a single representation for an entire video, instead of multiple segment representations. In the first setting, we use a unidirectional GRU to summarize the video segments into a single vector, obtained from the last time step of the GRU, and perform a single comparison between the semantic and the video vectors. In the second setting, we use a bidirectional GRU that gives an output at each time step. The attention mechanism is applied to the visual and semantic embedding, so as to map the multiple segment representations of the query video into a single representation. The single aligned query representation is then compared to the semantic embedding. The third setting (third row in Table~\ref{TARN_ZSL}) is a network that performs per-segment comparisons between the visual and semantic features. The attention mechanism maps the single semantic embedding of each class into multiple representations, as many, as the number of segments in the query video. In this way, each segment of the query video is compared to a semantic representation, allowing us to find relations between class-related semantic attributes and the visual features of each segment. Regarding the single comparison cases, the results show that the attention mechanism (second row in Table~\ref{TARN_ZSL}) effectively encodes multiple segment features into a single representation. The best performance is achieved when performing multiple segment-to-attribute comparisons.




\begin{table}[!b] \centering
\begin{adjustbox}{width=0.8 \textwidth,center}
    \begin{tabular}{ | c ?{0.4mm} c | c ?{0.4mm} c | c | c |}
    \hline
  \multirow{2}{*}{\textbf{Method}}  & \textbf{UCF-101}  & \textbf{UCF-101} & \textbf{HMDB51} \\

&  \textbf{(51/50)}  & \textbf{(81/20)} & \textbf{(26/25)} \\
\Xhline{3\arrayrulewidth}
 \textbf{TARN (w/o attention, single comparison)} &  16.74.0  & 35.85.9 &  16.63.4 \\ \hline
 
 \textbf{TARN (with attention, single comparison)} & 20.02.5  & 38.15.6 &  17.42.8 \\ \hline
 
 \textbf{TARN (with attention, multi comparison)} &  \textbf{23.2}\textbf{2.9}  & \textbf{42.7}\textbf{5.4} &  \textbf{19.5}\textbf{4.2} \\ \hline
 




    \end{tabular}
\end{adjustbox}
\caption{Accuracies of the TARN model at different settings on zero-shot action recognition on the UCF-101 and HMDB51 datasets.}
\label{TARN_ZSL}
\end{table}






\textbf{Comparison to state of the art in ZSL:} Methods proposed in the literature for ZSL have used a wide range of testing settings. In order to have a common setting across the majority of works, we do not compare with works that: (1) use auxiliary data to augment the training set \cite{xu2015semantic, xu2016multi, zhu_2018_cvpr}; (2) fuse different semantic or visual features \cite{kodirov2015unsupervised, wang_17_bidilel}; or (3) require access to the testing (unseen) classes during training (also known as ``transductive'' setting) \cite{kodirov2015unsupervised, xu2015semantic, xu2016multi, wang_17_bidilel}. This allows us to have a clear and model-based comparison to the literature. Table \ref{SOA_ZSL} summarizes the comparison results over the UCF-101 (51/50), UCF-101 (81/20), and HMDB51 (26/25) splits. We state in Table \ref{SOA_ZSL} the type of semantic and visual representation used in each method. Our architecture achieves the best results over the UCF-101 51/50 and 81/20 splits with almost 0.5\% and 3\%, respectively, in comparison to the second best performing methods. For HMDB51, we only get lower results than works that use either Improved Dense Trajectories (IDT) as visual features~\cite{zhu_2018_cvpr,qin2017zero}, or ECOC as semantic representation~\cite{qin2017zero}.




\begin{table}[!t] \centering
\begin{adjustbox}{width=0.8 \textwidth,center}
    \begin{tabular}{ | c ?{0.4mm} c | c ?{0.4mm} c | c | c |}
    \hline
  \multirow{2}{*}{\textbf{Method}}  & \textbf{Visual} & \textbf{Semantic} & \textbf{UCF-101}  & \textbf{UCF-101} & \textbf{HMDB51} \\
  
           & \textbf{Repr.} & \textbf{Repr.} & \textbf{(51/50)}  & \textbf{(81/20)} & \textbf{(26/25)} \\  
\Xhline{3\arrayrulewidth}


 \textbf{ESZSL~\cite{romera2015embarrassingly} - ICML 2015 } & IDT  & WV & 15.01.3  & - & 18.52.0 \\
 \hline
 \multirow{2}{*}{\textbf{SJE~\cite{akata2015evaluation} - CVPR 2015}} & \multirow{2}{*}{IDT} & Attr & 12.0  1.2  & - & - \\ \cline{3-6}
                                          &     & WV & 9.9  1.4  & -  & 13.3  2.4 \\ \hline
 
  \textbf{UDICA \cite{GanYG16} - CVPR 2016} & \multirow{2}{*}{C3D} & \multirow{2}{*}{Attr} & -  &  29.61.2 & - \\ \cline{1-1} \cline{4-6} 
  \textbf{KDICA \cite{GanYG16} - CVPR 2016} &  &  & -  &  31.10.8 & - \\ \hline
 
 
 \multirow{2}{*}{\textbf{MTE \cite{xu2016multi} - ECCV 2016}} & \multirow{2}{*}{IDT} & Attr & 18.31.7  & - & - \\  \cline{3-6}
                                           &  & WV & 15.81.3 & - & 19.71.6 \\ \hline
  


 \multirow{3}{*}{\textbf{ZSECOC \cite{qin2017zero} - CVPR 2017}} & \multirow{3}{*}{IDT} & Attr & 3.20.7  & - & - \\  \cline{3-6}
                                              &  & WV & 13.70.5  & - & 16.53.9 \\ \cline{3-6}
                                              &  & ECOC & 15.11.7  & - & 22.61.2 \\ \hline
                                              
 \multirow{2}{*}{\textbf{BiDiLEL \cite{wang_17_bidilel} - IJCV 2017}} & \multirow{2}{*}{C3D} & Attr & 20.50.5  & 39.21.0 & - \\  \cline{3-6}
                                               &  & WV & 18.90.4  & 38.31.2 & 18.60.7 \\ \hline
 
 \multirow{2}{*}{\textbf{GMM \cite{mishra_18_generative} - WACV 2018}} & \multirow{2}{*}{C3D} & Attr & 22.71.2  & - & - \\  \cline{3-6}
                                           &  & WV & 17.31.1  & - & 19.32.1 \\ \hline
 
 \textbf{UAR \cite{zhu_2018_cvpr} - CVPR 2018} & IDT & WV &  17.51.6 & - & \textbf{24.4}\textbf{1.6} \\ \hline
\multirow{2}{*}{\textbf{TARN}} & \multirow{2}{*}{C3D} & Attr & \textbf{23.2}\textbf{2.9}  & \textbf{42.7}\textbf{5.4}  & - \\  \cline{3-6}
                                           &  & WV & 19.02.3  & 36.05.3 & 19.54.2  \\ \hline

    \end{tabular}
\end{adjustbox}
\caption{Accuracies of the TARN model as well as other state-of-the-art methods on zero-shot action recognition on the UCF-101 and HMDB51 datasets. Results marked with ``'' are reported as reproduced by~\cite{xu2016multi}.
}
\label{SOA_ZSL}
\end{table}
