\clearpage
\begin{center}
    \Large{\textbf{Non-parametric Outlier Synthesis\\(Appendix)}}
\end{center}

\section{Details of datasets}
\label{sec:app_dataset}
\textbf{ImageNet-100.} We randomly sample 100 classes from ImageNet-1k~\citep{deng2009imagenet} to create ImageNet-100. The dataset contains the following categories: {\scriptsize n01986214, n04200800, n03680355, n03208938, n02963159, n03874293, n02058221, n04612504, n02841315, n02099712, n02093754, n03649909, n02114712, n03733281, n02319095, n01978455, n04127249, n07614500, n03595614, n04542943, n02391049, n04540053, n03483316, n03146219, n02091134, n02870880, n04479046, n03347037, n02090379, n10148035, n07717556, n04487081, n04192698, n02268853, n02883205, n02002556, n04273569, n02443114, n03544143, n03697007, n04557648, n02510455, n03633091, n02174001, n02077923, n03085013, n03888605, n02279972, n04311174, n01748264, n02837789, n07613480, n02113712, n02137549, n02111129, n01689811, n02099601, n02085620, n03786901, n04476259, n12998815, n04371774, n02814533, n02009229, n02500267, n04592741, n02119789, n02090622, n02132136, n02797295, n01740131, n02951358, n04141975, n02169497, n01774750, n02128757, n02097298, n02085782, n03476684, n03095699, n04326547, n02107142, n02641379, n04081281, n06596364, n03444034, n07745940, n03876231, n09421951, n02672831, n03467068, n01530575, n03388043, n03991062, n02777292, n03710193, n09256479, n02443484, n01728572, n03903868}.

\vspace{-0.2cm}

\paragraph{OOD datasets.} \citeauthor{huang2021mos} curated a diverse collection of subsets from iNaturalist~\citep{van2018inaturalist}, SUN~\citep{xiao2010sun}, Places~\citep{zhou2017places}, and Texture~\citep{cimpoi2014describing} as large-scale OOD datasets for ImageNet-1k, where the classes of the test sets do not overlap with ImageNet-1k. We provide a brief introduction for each dataset as follows.
 
\textbf{iNaturalist} contains images of natural world~\citep{van2018inaturalist}. It has 13 super-categories and 5,089 sub-categories covering plants, insects, birds, mammals, and so on. We use the subset that contains 110 plant classes which are not overlapping with ImageNet-1k.

\textbf{SUN} stands for the Scene UNderstanding Dataset~\citep{xiao2010sun}. SUN contains 899 categories that cover more than indoor, urban, and natural places with or without human beings appearing in them. We use the subset which contains 50 natural objects not in ImageNet-1k.

\textbf{Places} is a large scene photographs dataset~\citep{zhou2017places}. It contains photos that are labeled with scene semantic categories from three macro-classes: Indoor, Nature, and Urban. The subset we use contains 50 categories that are not present in ImageNet-1k.

\textbf{Texture} stands for the Describable Textures Dataset~\citep{cimpoi2014describing}. It contains images of textures and abstracted patterns. As no categories overlap with ImageNet-1k, we use the entire dataset as in~\cite{huang2021mos}.





\section{Baselines}
\label{sec:baseline_app}
To evaluate the baselines, we follow the original definition in  MSP \citep{hendrycks2016baseline, fort2021exploring}, ODIN score \citep{liang2018enhancing}, Energy score \citep{liu2020energy}, GradNorm score~\citep{huang2021importance}, ViM score~\citep{haoqi2022vim}, KNN distance~\citep{sun2022out} and VOS~\citep{du2022unknown}. 
\begin{itemize}
    \item For ODIN, we follow the original setting in the work and set 
the temperature $T$ as 1000. 
\item For both Energy and GradNorm scores, the temperature is set to be $T = 1$. 
\item For ViM, we follow the original implementation according to the released code. 
\item For VOS, we ensure that the number of negative samples is consistent with our method --- for each class, we sample $60$k points after estimating the distribution and select six outliers with the lowest likelihood. For the OOD score, we adopt the uncertainty proposed in the original method. 
\item For VOS+, we use the same loss function as defined in Section~\ref{sec:method}, but only replace the sampling method to be parametric. The way VOS+ synthesizes outliers is the same as VOS (first modeling the feature embedding as a mixture of multivariate Gaussian distribution, and then sample virtual outliers from the low-likelihood region in the embedding space). For a fair comparison, we also use the textual embedding extracted from CLIP as the prototype for VOS+. Note that VOS+ and NPOS \emph{only} differs in how outliers are synthesized. 
\end{itemize}





\section{Algorithm of \model}
\label{sec:algorithm_block}
We summarize our algorithm in implementation. Following~\citep{du2022towards}, we construct a class-conditional in-distribution sample queue $\{Q_c\}_{c=1}^C$, which is periodically updated as new batches of training samples arrive. 
\begin{algorithm}[h]
\SetAlgoLined
\textbf{Input:} ID training data $\mathcal{D}_\text{in}=\left\{\left(\*x_{i}, {\*y}_{i}\right)\right\}_{i=1}^{n}$, initial model parameters $\theta$ for backbone, nonlinear MLP layer $\phi$ and class-conditional prototypes $\boldsymbol{\mu}$.
\\
\textbf{Output:} Learned classifier $f(\*x)$, and OOD detector $G(\*x)$.\\
\While{train}{
        1.  Update class-conditional queue $\{Q_c\}_{c=1}^C$ with the feature embeddings  $h(\*x)$ of training samples in the current batch.\\
    2.  Select a set of boundary samples $\mathbb{B}_c$ consisting of top-$m$ embeddings with the largest $k$-NN distances using Equation~\ref{eq:knn_distance}. \\
    3. Synthesize a set of outliers $V_i$ around each boundary sample $\*x_i \in \mathbb{B}_1\cup\mathbb{B}_2 \cup...\mathbb{B}_C$ using Equation~\ref{eq:sample}. \\
    4. Accept the outliers in each $V_i$ with large  $k$-NN distances. \\
    5. Calculate level-set estimation loss $R_{\text{open}}$ and ID embedding optimization loss $R_{\text{closed}}$ using Equations~\ref{eq:r_open} and~\ref{eq:r_closed}, respectively, update the parameters $\theta, \phi$ based on loss in Equation~\ref{eq:obj}.\\
    6. Update prototypes using ${\boldsymbol{\mu}}_c := \text{Normalize}( \gamma{\boldsymbol{\mu}}_c + (1-\gamma)\*z), 
     \; \forall c\in\{1, 2, \ldots, C\}$.
  }
  \While{eval}{
  1.  Calculate the OOD score defined in Section~\ref{sec:inference_score}.\\
  2.  Perform OOD detection by thresholding comparison.

 }
 \caption{\model: Non-parametric Outlier Synthesis }
 \label{alg:algo}
\end{algorithm}
\vspace{-2em}




\section{Experimental Details and Results on training from scratch}
\label{sec:scratch_app}
In this section, we provide the implementation details and the experimental results for \model trained from scratch. We evaluate on three datasets: CIFAR-10, CIFAR-100, and ImageNet-100. We summarize the training configurations of NPOS in Table~\ref{tab:train_from_scratch_hyper}.

\paragraph{CIFAR-10 and CIFAR-100.}
The results on CIFAR-10 are shown in Table~\ref{tab:train_from_scratch_results_c10}. All methods are trained on ResNet-18. We consider the same set of baselines as in the main paper. For the post-hoc OOD detection methods (MSP, ODIN, Energy score, GradNorm, ViM, KNN), we report the results by training the model with the cross-entropy loss for 100 epochs using stochastic gradient descent with momentum 0.9. The start learning rate is 0.1 and decays by a factor of 10 at epochs
50, 75, and 90 respectively. The batch size is set to 256. The average FPR95 of NPOS is 10.16\%, significantly outperforming the best baseline VOS (27.88\%). The results on CIFAR-100 are shown in Table~\ref{tab:train_from_scratch_results_c100}, where the strong performance of NPOS holds.




\begin{table}[h]
  \centering
  \vspace{-1em}
  \caption{OOD detection performance on CIFAR-10 as ID. All methods are trained on ResNet-18. Values are percentages. \textbf{Bold} numbers are superior results.}
  \scalebox{0.65}{
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multirow{3}[4]{*}{Methods} & \multicolumn{12}{c}{OOD Datasets}                                                             & \multirow{3}[4]{*}{ID ACC} \\
    \cmidrule{2-13}
          & \multicolumn{2}{c}{SVHN} & \multicolumn{2}{c}{LSUN} & \multicolumn{2}{c}{iSUN} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{Places365} & \multicolumn{2}{c}{Average} &  \\
\cmidrule{2-13}          & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC &  \\
    \midrule
    MSP   & 59.66	&91.25	&45.21	&93.80	&54.57	&92.12	&66.45	&88.50	&62.46	&88.64&	57.67	&90.86	&94.21 \\
        ODIN  &20.93	&95.55	&7.26&	\textbf{98.53}	&33.17&	94.65&	56.40&	86.21	&63.04	&86.57	&36.16	&92.30	&94.21\\
    Energy & 	54.41	&91.22&	10.19	&98.05	&27.52&	\textbf{95.59}	&55.23&	89.37	&42.77&	91.02	&38.02&	93.05&	94.21 \\
                GradNorm &80.86	&81.41&	53.87	&88.39&	60.32	&88.00	&71.66	&80.79&	80.71&	72.57&	69.49	&82.23	&94.21\\
    ViM   &24.95&	95.36&	18.80	&96.63	&29.25	&95.10	&24.35	&\textbf{95.20}&	44.70	&90.71	&28.41&	94.60&	94.21 \\
    KNN   & 24.53&	95.96&	25.29	&95.69&	25.55	&95.26&	27.57	&94.71&	50.90	&89.14&	30.77	&94.15&	94.21\\
    VOS   & 15.69	&96.37&	27.64	&93.82&	30.42&	94.87	&32.68	&93.68	&37.95	&\textbf{91.78}&	27.88	&94.10&	93.96\\
   \rowcolor{Gray} NPOS  & \textbf{5.61}&	\textbf{97.64}&\textbf{4.08}&	97.52	&\textbf{14.13}&	94.92	&\textbf{8.39}&	94.67	&\textbf{18.57}&	91.35&	\textbf{10.16}&	\textbf{95.22}	&93.86 \\
    \bottomrule
    \end{tabular}}
  \label{tab:train_from_scratch_results_c10}\end{table}




\begin{table}[h]
  \centering
  \vspace{-1em}
  \caption{OOD detection performance on CIFAR-100 as ID. All methods are trained on ResNet-34. Values are percentages. \textbf{Bold} numbers are superior results.}
  \scalebox{0.65}{
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multirow{3}[4]{*}{Methods} & \multicolumn{12}{c}{OOD Datasets}                                                             & \multirow{3}[4]{*}{ID ACC} \\
    \cmidrule{2-13}
          & \multicolumn{2}{c}{SVHN} & \multicolumn{2}{c}{Places365} & \multicolumn{2}{c}{LSUN} & \multicolumn{2}{c}{iSUN} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{Average} &  \\
\cmidrule{2-13}          & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC &  \\
    \midrule
    MSP   & 85.30	&72.41	&73.40	&81.09	&85.55&	74.00&	88.55&	68.59	&86.45	&71.32	&83.85&	73.48&	73.12 \\
        ODIN  &89.50	&76.13	&41.50&	91.60	&74.70	&83.93	&90.20	&68.27&	85.75&	73.17	&76.33	&78.62	&73.12 \\
    Energy &  89.15	&78.16	&44.15&	90.85&	81.85&	80.57	&90.35	&68.18&	84.30	&73.86	&77.96&	78.32	&73.12\\
                GradNorm & 91.05&	67.13	&55.72&	86.09	&97.80&	44.21&	89.71&	58.23&	96.20&	52.17	&86.10&	61.57&	73.12 \\
    ViM   & 54.30	&88.85&	84.70&	74.64	&57.15&	88.17&	56.65&	87.13	&86.00	&71.95	&67.76	&82.15&	73.12 \\
    KNN   &66.38	&83.76&	79.17&	71.91&	70.96&	83.71&	77.83&	78.85&	88.00	&67.19	&76.47	&77.08&	73.12  \\
    VOS   &  76.55	&75.68&\textbf{29.95}&	\textbf{94.02}	&75.61&	76.84&	83.64	&71.46	&76.94&	76.23	&68.18	&78.95&	73.69 \\
    \rowcolor{Gray} NPOS  &\textbf{17.98} &\textbf{96.43} & 80.41 & 73.74& \textbf{28.90} & \textbf{92.99} & \textbf{43.50} & \textbf{89.56} & \textbf{33.07} & \textbf{92.86} & \textbf{40.77} & \textbf{89.12} & 73.78 \\
    \bottomrule
    \end{tabular}}
  \label{tab:train_from_scratch_results_c100}\end{table}





\paragraph{ImageNet-100.} The results are shown in Table~\ref{tab:train_from_scratch_results_in100}. All methods are trained on ResNet-101 using the ImageNet-100 dataset. We use a slightly larger model capacity  to accommodate for the larger-scale dataset with high-solution images. NPOS significantly outperforms the best baseline KNN by 18.96\% in FPR95. For the post-hoc OOD detection methods, we report the results by training the model with the cross-entropy loss for 100 epochs using stochastic gradient descent with momentum 0.9. The start learning rate is 0.1 and decays by a factor of 10 at epochs 50, 75, and 90 respectively. The batch size is set to 512.




Our results above demonstrate that NPOS can achieve strong OOD detection performance without necessarily relying on the pre-trained models. Thus, our framework  provides strong generality across both scenarios: training from scratch  or fine-tuning on pre-trained models. 
\begin{table}[t]
  \centering

  \caption{OOD detection performance on ImageNet-100 as ID. All methods are trained on ResNet-101. Values are percentages. \textbf{Bold} numbers are superior results.}
  \scalebox{0.75}{
\begin{tabular}{cccccccccccc}
    \toprule
    \multirow{3}[4]{*}{Methods} & \multicolumn{10}{c}{OOD Datasets}                                                             & \multirow{3}[4]{*}{ID ACC} \\
    \cmidrule{2-11}
          & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Textures} &\multicolumn{2}{c}{Average} &  \\
\cmidrule{2-11}          & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC &  \\
    \midrule
    MSP   &76.30&	82.20	&81.90&	77.54	&82.70&	78.35	&75.30&	80.01&	79.05	&79.52	&84.16  \\
        ODIN  &53.00&	89.52&	70.40	&82.77	&66.90	&85.01&	48.40&	89.19&	59.67&	86.62&	84.16\\
    Energy & 	72.60& 	85.31	& 69.80	& 83.15	& 74.40& 	83.96	& 63.40	& 84.80	& 70.05	& 84.31	& 84.16 \\
                GradNorm &50.82	&84.86	&68.27&	74.46&	65.77&	77.11&40.48	&88.17&	56.33&	81.15&	84.16\\
    ViM   &72.40&		84.88&		76.20&		81.54&		73.80&		83.99	&	22.20	&	95.63	&	61.15	&	86.51	&	84.16\\
    KNN   & 56.96&	86.98&	64.54&	83.68&	63.04&	85.37&	15.83&	96.24&	50.09	&88.07&	84.16\\
    VOS   &54.68	&89.74	&63.71	&\textbf{88.97}&	\textbf{41.67}&	91.54	&67.92&	84.34	&56.99	&88.65 &	84.39\\
       \rowcolor{Gray} NPOS  &   \textbf{19.49} & \textbf{96.43 }&\textbf{56.17} & 88.47 & 44.91 &\textbf{91.62 }& \textbf{3.97} & \textbf{99.18} &\textbf{31.13} & \textbf{93.93}    &84.23  \\
      
    \bottomrule
    \end{tabular}}
  \label{tab:train_from_scratch_results_in100}\end{table}



\begin{table}[h]
  \centering
  \small
  \caption{Configurations of NPOS: training from scratch}
  \begin{tabular}{lccc}
    \toprule
     & CIFAR-10 & CIFAR-100 & ImageNet-100 \\
    \midrule
    Training epochs & 500   & 500   & 500 \\
    Momentum & 0.9   & 0.9   & 0.9 \\
    Batch size & 256   & 256   & 512 \\
    Weight decay & 0.0001 & 0.0001 & 0.0001 \\
    Classification branch initial LR & 0.5   & 0.5   & 0.1 \\
    Initial LR for $R_\text{open}(g)$ & 0.05  & 0.05  & 0.01 \\
    LR schedule & cosine & cosine & cosine \\
    Prototype update factor $\gamma$ & 0.95  & 0.95  & 0.95 \\
    Regularization weight $\alpha$ & 0.1   & 0.1   & 0.1 \\
    starting epoch of regularization & 200   & 200   & 200 \\
    Queue size (per class) $|Q_c|$ & 600   & 600   & 1000 \\
     $k$ in nearest neighbor distance & 300   & 300   & 400 \\
    Number of boundary samples (per class) $m$ & 200   & 200   & 300 \\
    $\sigma^2$Â  & 0.1   & 0.1   & 0.1 \\
    Temperature $\tau$ & 0.1   & 0.1   & 0.1 \\
    \bottomrule
    \end{tabular}\label{tab:train_from_scratch_hyper}\end{table}


\section{Additional experiments on model calibration and data-shift robustness}
\paragraph{Data-shift robustness.} We evaluate the NPOS-trained model (from scratch with 5 random seeds) on different test data with distribution shifts in Table~\ref{tab:distri_shift}. Specifically, we report the mean classification accuracy and the standard deviation for measuring data-shift robustness. The number in the bracket of the first column indicates clean accuracy on the in-distribution test data. The results demonstrate that compared to the vanilla classifier trained with the cross-entropy loss only, NPOS does not incur substantial change in distributional robustness.
\begin{table}[h]
    \centering
    \small 
    \caption{Evaluations on data-shift robustness (numbers are in \%).}
      \scalebox{0.8}{\begin{tabular}{c|cccc}
      \toprule
     ID data& Original test ACC&	shifted dataset	&CE-OOD ACC ($R_{\text{closed}}(g)$)&	NPOS-OOD ACC ($R_{\text{closed}}(g)+R_{\text{open}}(g)$) \\
      \midrule
    CIFAR-10	&94.06	&CIFAR-10-C&	72.97 $\pm$ 0.36 &	72.63 $\pm$ 0.52   \\
    CIFAR-100	&74.86	&CIFAR-100-C&	46.68 $\pm$ 0.24	&46.93 $\pm$ 0.64\\
    \hline
    ImageNet-100&	84.22&	ImageNet-C	&33.49 $\pm$ 0.34&	34.29 $\pm$ 0.76\\
    ImageNet-100&	84.22	&ImageNet-R&	28.98 $\pm$ 0.62&	29.50 $\pm$ 1.43\\
    ImageNet-100&	84.22&	ImageNet-A	& 13.92 $\pm$ 1.46	&12.16 $\pm$ 2.24\\
ImageNet-100&	84.22&	ImageNet-v2&	72.17 $\pm$ 0.95&	71.67 $\pm$ 1.54\\
    ImageNet-100&	84.22&	ImageNet-Sketch	&18.83 $\pm$ 0.24&	17.94 $\pm$ 1.54\\
      \bottomrule
      \end{tabular}}\label{tab:distri_shift}\vspace{-1em}
  \end{table}


\vspace{-0.3cm}
\paragraph{Model calibration.} In Table~\ref{tab:calibration}, we also measure the calibration error of NPOS (with 5 random seeds) on different datasets using Expected Calibration Error (ECE, in \%)~\citep{guo2017calibration}. In the implementation, we adopt the codebase\footnote{\url{https://github.com/gpleiss/temperature_scaling}} for metric calculation. The results suggest that NPOS maintains an overall comparable (in some cases even better) calibration performance, while achieving a much stronger performance of OOD uncertainty estimation.

\begin{table}[h]
    \centering
    \caption{Calibration performance (numbers are in \%).}
      \scalebox{0.9}{\begin{tabular}{c|ccc}
      \toprule
    Model & Dataset&	Method	&ECE \\
      \midrule
\multirow{6}{*}{training from scratch}&\multirow{2}{*}{CIFAR-10} & CE($R_{\text{closed}}(g)$)& 0.62$\pm$ 0.04  \\
& & NPOS ($R_{\text{closed}}(g)+R_{\text{open}}(g)$)& 0.27$\pm$ 0.06 \\
& \multirow{2}{*}{CIFAR-100}& CE&3.19$\pm$ 0.05\\
& & NPOS &4.02$\pm$ 0.13 \\ 

& \multirow{2}{*}{ImageNet-100}& CE& 7.17$\pm$ 0.07\\
& &  NPOS&7.52$\pm$ 0.29\\
\hline
\multirow{4}{*}{w/ pre-trained model} &\multirow{2}{*}{ImageNet-100} &CE& 2.34$\pm$ 0.01  \\
& & NPOS& 1.06$\pm$ 0.03  \\
 &\multirow{2}{*}{ImageNet-1K} &CE& 3.42$\pm$ 0.02 \\
& & NPOS&2.66$\pm$ 0.07  \\
      \bottomrule
      \end{tabular}}\label{tab:calibration}\vspace{-1em}
  \end{table}






\section{Additional ablations on hyperparameters and designs}
\label{sec:abaltion_app}
In this section, we provide additional analysis of the hyperparameters and designs of \model. For all the ablations, we use the ImageNet-100 dataset as the in-distribution training data, and fine-tune on ViT-B/16.




\vspace{-0.2cm}
\paragraph{Ablation on the number of boundary samples.} We show in Table~\ref{tab:m_ablation} the effect of $m$ --- the number of boundary samples selected per class. We vary $m\in\{100, 150, 200, 250, 300, 350, 400\}$. We observe that NPOS is not sensitive to this hyperparameter.
\begin{table}[h]
    \centering
    \small 
    \caption{Ablation study on the number of boundary samples (per class).}
      \begin{tabular}{c|cccc}
      \toprule
      $m$& FPR95 & AUROC & AUPR  & ID ACC  \\
      \midrule
      100   & 10.63  & 98.14  & 97.56  & 93.97  \\
      150   & 9.94  & 98.21  & 97.75  & 94.02  \\
      200   & 8.52  & 98.34  & 97.93  & 93.78  \\
      250   & 7.41  & 98.49  & 98.25  & 94.42  \\
      \textbf{300}   & \textbf{6.12} & \textbf{98.70} & \textbf{98.49} & 94.46 \\
      350   & 8.77  & 98.15  & 97.75  & 94.00  \\
      400   & 7.43  & 98.51  & 98.21  & 94.52  \\
      \bottomrule
      \end{tabular}\label{tab:m_ablation}\vspace{-1em}
  \end{table}

\vspace{-0.2cm}
\paragraph{Ablation on the number of samples in the class-conditional queue.}  In Table~\ref{tab:q_ablation}, we investigate the effect of ID queue size $|Q_c| \in \{1000, 1500, 2000, 2500, 3000\}$. Overall,  the OOD detection performance of \model is not sensitive to the size of the class-conditional queue.  A sufficiently large $|Q_c|$ is desirable since the non-parametric density estimation can be more accurate.
\begin{table}[htbp]
  \centering
      \small 
   \vspace{-1em}
  \caption{Ablation study on the size of ID queue (per class).}
    \begin{tabular}{c|cccc}
    \toprule
   $|Q_c|$ & FPR95 & AUROC & AUPR  & ID  ACC  \\
    \midrule
    1000  & 7.18  & 98.51  & 98.24  & 94.40  \\
    \textbf{1500}  & \textbf{5.76}  & \textbf{98.74}   & \textbf{98.73}  &  94.76  \\
    2000  & 6.76  & 98.64  & 98.35  & 94.76 \\
    2500  & 8.75  & 98.28  & 97.89  & 94.36  \\
    3000  & 7.57  & 98.45  & 98.19  & 94.28  \\
    \bottomrule
    \end{tabular}\label{tab:q_ablation}\end{table}



\vspace{-0.2cm}
  \paragraph{Ablation on the number of candidate outliers sampled from the Gaussian kernel (per boundary ID sample).} As shown in Table~\ref{tab:c_ablation}, we analyze the effect of $p$ --- the number of synthesized candidate outliers using Equation~\ref{eq:sample} around each ID boundary sample. We vary $p \in \{600,800,1000,1200,1400\}$. A reasonably large $p$ helps provide a meaningful set of candidate outliers to be selected.


\begin{table}[h]
  \centering
      \small 
  \vspace{-1em}
  \caption{Ablation on the number of candidate outliers drawn from the Gaussian kernel.}
    \begin{tabular}{c|cccc}
    \toprule
   $p$& FPR95 & AUROC & AUPR  & ID  ACC  \\
    \midrule
    600   & 19.26  & 96.25  & 94.66  & 94.20  \\
    800   & 10.75  & 97.96  & 97.44  & 94.24  \\
    \textbf{1000}  & \textbf{5.76}  & \textbf{98.74}   & \textbf{98.73}  &  94.76  \\
    1200  & 8.72  & 98.28  & 97.90  & 94.34  \\
    1400  & 10.59  & 98.12 & 97.61 & 94.38  \\
    \bottomrule
    \end{tabular}\label{tab:c_ablation}\vspace{-1em}
\end{table}\vspace{-0.2cm}
\paragraph{Ablation on the temperature for ID embedding optimization.}  In Table~\ref{tab:tau}, we ablate the effect of temperature $\tau$ used for the ID embedding optimization loss (\emph{cf}. Equation~\ref{eq:r_closed}).  



 \begin{table}[h]
    \centering
    \small 
    \vspace{-1em}
    \caption{Ablation study on the temperature $\tau$.}
      \begin{tabular}{c|cccc}
      \toprule
      $\tau$     & FPR95 & AUROC & AUPR  & ID  ACC  \\
      \midrule
      5     & 13.83  & 97.33  & 96.65  & 94.80\\
      6     & 10.32  & 98.12  & 97.32  & 94.64  \\
      7     & 6.26  & 98.61  & 98.40  & 94.58  \\
      \textbf{8}     &\textbf{5.76}  & 98.74   & \textbf{98.73}  &  94.76 \\
      9     & 9.99  & 98.09  & 97.59  & 93.58  \\
      10    & 8.92  & \textbf{98.96} & 97.96 & 93.30  \\
      \bottomrule
      \end{tabular}\label{tab:tau}\end{table}


\vspace{-0.7cm}
\paragraph{Ablation on the starting epoch of  adding $R_\text{open}(g)$.}  In Table~\ref{tab:beginning_epoch}, we ablate on the effect of the starting epoch of adding $R_{\text{open}}(g)$ in training. The table shows that adding $R_{\text{open}}(g)$ at the beginning of the training yields a slightly worse OOD detection performance. The reason might be that the representations are still not well-formed at the early stage of training. Instead, adding regularization in the middle of training yields more desirable performance. 

\begin{table}[!h]
    \centering
    \small 
    \vspace{-0.3cm}
    \caption{Ablation study on the starting epoch of adding $R_{\text{open}}(g)$.}
     \begin{tabular}{c|ccc}
      \toprule
     epoch& FPR95 & AUROC & ID ACC  \\
      \midrule
     0& 9.36	&98.03&	94.06   \\
      5	& 5.79	& 98.61& 	94.21 \\
     \textbf{10}	&\textbf{5.76}  & \textbf{98.74}   & 94.76\\
   15	&16.21&	97.34	&94.39\\
     20	&32.68	&94.62	&94.16\\
      \bottomrule
      \end{tabular}\label{tab:beginning_epoch}\vspace{-1em}
  \end{table}



\paragraph{Ablation on the density estimation implementation.} \model adopts a class-conditional approach for outlier synthesis. For instance, it identifies the boundary ID samples by calculating the $k$-NN distance between sample pairs holding the same class label. After synthesizing the outliers in the feature space, it rejects synthesized outliers that have lower $k$-NN distance, which is also implemented in a class-conditional way. In this ablation, we contrast with an alternative  class-agnostic implementation, \emph{i.e.}, we calculate the $k$-NN distance between samples across all classes in the training set. Under the same training and inference setting, the class-agnostic \model gives a similar OOD detection performance compared to the class-conditional \model (Table~\ref{tab:sampling_strategy}). 

\begin{table}[h]
  \centering
  \caption{Ablation on different implementations of the non-parametric density estimation. 
  }
  \scalebox{0.7}{
    \begin{tabular}{cccccccccccc}
    \toprule
    \multirow{3}[4]{*}{Methods} & \multicolumn{10}{c}{OOD Datasets}                                             & \multirow{3}[4]{*}{ID ACC} \\
    \cmidrule{2-11}
          & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Textures} & \multicolumn{2}{c}{Average} &  \\
\cmidrule{2-11}          & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC &  \\
    \midrule
   Class-conditional &  \textbf{0.70}  & 99.14  & \textbf{9.22}  & \textbf{98.48}  & \textbf{5.12}  & \textbf{98.86}  & \textbf{8.01}  & \textbf{98.47}  & \textbf{5.76}  & \textbf{98.74}   & 94.76 \\

    
    Class-agnostic & 2.46  & \textbf{99.32} & 11.63 & 98.06 & 8.43 & 97.69 & 9.43  & 98.45 & 7.99  & 98.38  & 94.61 \\
    \bottomrule
    \end{tabular}}
  \label{tab:sampling_strategy}\end{table}


\section{Additional results on the mean and standard deviations}
We repeat the training of our method on ImageNet-100 with pre-trained ViT-B/16 for 5 different times. We report the mean and standard deviations for both NPOS (ours) and the most relevant baseline VOS in Table~\ref{tab:mean_std}. NPOS is relatively stable, and outperforms VOS by a significant margin. 

  
\begin{table}[!h]
    \centering
    \small 
    \caption{Results on the mean and standard deviations after 5 runs.}
     \begin{tabular}{c|ccc}
      \toprule
    Method	&FPR95	&AUROC&	ID ACC  \\
      \midrule
    VOS	&18.26\(\pm\)1.1&95.76\(\pm\)0.7&94.51\(\pm\)0.3 \\
NPOS&\textbf{7.43}\(\pm\)\textbf{0.8}&\textbf{98.34}\(\pm\)\textbf{0.6}&94.38\(\pm\)0.2\\
      \bottomrule
      \end{tabular}\label{tab:mean_std}\vspace{-1em}
  \end{table}






\section{Software and Hardware}
\label{sec:hardware}
We use Python 3.8.5 and PyTorch 1.11.0, and 8 NVIDIA GeForce RTX 2080Ti GPUs.

