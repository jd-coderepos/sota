Given  MPI processes and the distributed tree provided by \ASKIT{}, 
distributed \IASKIT{} is invoked from tree level- to the
root level. Without loss of generality, we assume  and  are powers 
of two, and each MPI process owns  points corresponding to a 
subtree in level-.
Each subtree is factorized and solved locally, but computing the tree nodes
above requires collaboration and communication.
We first illustrate the distributed algorithms, and later we analyze the
complexity. 

\textbf{Data Distribution.}
In \figref{fig:tree}, we show a 4-process distributed tree provided
by \ASKIT{}. In level-1, the root has two children  and .
In level-,  has two children  and 
as well as .
Each MPI process is labeled with different colors, and we label the
data they own including , , skeletons and projection matrices.
Processes in the same distributed tree node are grouped together
by the purple box to form a subcommunicator, and we use 
\{0, ..., \} to denote the local MPI rank. 
In each non-root treenode, only \{0\} possesses the skeletons
and the projection matrix.

In each level, each MPI process only maintain a portion of 
 and  of the distributed node.
Thus, communication is required within and between each node in the 
same level during both factorization and solving phase.
\figref{fig:matrix} shows how  and  are distributed in the
tree we show in \figref{fig:tree}.
Each process is designed to own the  (later overwritten by ) 
and  portion with the same color, and the computation may require 
skeletons and the projection matrices from other processes.
Take the yellow process as an example.
In level-1, evaluating  requires \{0\} (green) to broadcast the 
projection matrix . 
Evaluating  requires skeletons  from 
its sibling node (orange).
Factorizing  requires \{0\} to reduce all 
parts in the same distributed node.
Solving \eqref{e:smw} requires both reducing  and broadcasting
 in the same node.  

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.35]{figures/distributed_tree_v2.pdf}
  \caption{Distributed binary tree with 4 processes. The leafs of the 
  tree are the roots of each local tree.
  Each process possesses the data of its colored rectangle 
  region, and processes in the same treenode are group into
  a subcommunicator.}
  \label{fig:tree}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.35]{figures/distributed_tree_v3.pdf}
  \caption{Distributed  and  factors. Each process
  only store the  and  portion of the same color in each
  level.}
  \label{fig:matrix}
\end{figure}

{\bf Factorization.}
In \algref{a:distributedfactor}, we present a recursive distributed 
factorization that traverses from level- to level-0.
In each level, \{0, ..., \} will traverse the left child,
and \{, ..., \} will traverse the right child. 
We use \texttt{c} to denote the child node of .
Whether \texttt{c} is  or  depends on its local MPI rank.
To satisfy the data dependency, \{0\} \texttt{Bcast} 
 to all processes in the node
and \texttt{Bcast} the skeleton  to all processes
in the sibling node.
Each process then computes the  and  portion it owns.
Once the  portion is computed, \algref{a:distributedsolve} is called
to apply the inverse of  to get .
Finally, every process compute the its  portion, and 
\{0\} \texttt{Reduce} all  portion to factorize .






\begin{algorithm}[!htp]
\begin{algorithmic}
  \IF { is at level-}
    \STATE \texttt{Factorize()}.
  \ELSE
    \STATE \{\} \texttt{DistributedFactorize}(, ).
    \STATE \{\} \texttt{DistributedFactorize}(, ).
    \STATE \{0\} \texttt{Bcast}  inside the distributed node.
    \STATE \{\} \texttt{SendRecv} .
    \STATE \{0\} \texttt{Bcast} inside  using the  communicator.
    \STATE \{\} \texttt{Bcast} inside  using the  communicator.
    \STATE \{\} \texttt{SendRecv} .
    \STATE Compute \eqref{e:innerid} to get the  and  portions.
    \STATE \{\}.
    \STATE \{\}.
    \STATE \{0, \} \texttt{Reduce} .
    \STATE \{ \texttt{SendRecv} .
    \STATE \{0\} computes and LU factorizes .
  \ENDIF
\end{algorithmic}
\caption{{} \texttt{DistributedFactorize}(,)}
\label{a:distributedfactor}
\end{algorithm}


{\bf Solver.}
The distributed solver is also implemented using recursive bottom-up
traversal. \algref{a:solve} is called to solve the local tree while 
reaching level-.
\{0\} reduces all portions of  and compute 
where  was previously factorized in \algref{a:distributedfactor}.
 is \texttt{Bcast} back to all processes by \{0\}, 
and each process updates its own 
by  to complete the solving phase.


\begin{algorithm}[!htp]
\begin{algorithmic}
  \IF { is at level-}
    \STATE \texttt{Solve}(). \ELSE
    \STATE \texttt{DistributedSolve}(, , ). \STATE Compute . \STATE \{0\} \texttt{Reduce} . \STATE \{0\}  Compute  using \eqref{e:blocksmw}. \STATE \{0\} \texttt{Bcast} . \STATE Compute . \ENDIF
\end{algorithmic}
\caption{{} }
\label{a:distributedsolve}
\end{algorithm}

{\bf Complexity.}
We use a network model to derive the parallel complexity.
For simplicity, we assume that the network topology provides 
 \texttt{Bcast} and \texttt{Reduce}, and
we only use the order of the 
overall message size to simplify the the communication model
without discussing the latency and the bandwidth.

Let  be the parallel complexity of the solving phase 
with  right hand side and  processes. The base case 
 is the complexity of \algref{a:solve}.
We can derive the complexity recursively as:

Here  is the computation cost of \eqref{e:blocksmw}, and
 is the communication cost.
The overall complexity is 
 including
both computation and communication.
Observe that the communication cost doesn't scale with the problem size
, instead it's only related to ,  and .


We derive  as the 
parallel complexity of the factorization phase with a fixed
skeleton size  similarly. The base case 
 is the complexity of \algref{a:factor}.
We derive the complexity recursively as:

Here  is the cost spent in \algref{a:distributedsolve}, 
 includes the cost of \eqref{e:innerid}, and
 is the communication cost spent on broadcasting the
skeleton coordinates and the projection matrix.
Overall, the complexity is 
 
including both computation and communication.
Observe that the sequential complexity is  times the parallel
computation complexity, which indicates that the algorithm should
be able to achieve almost linear scalability. 
Overall the communication cost is minor, since it doesn't scale with .












{\bf Discussion.}
Despite that the communication cost is not a bottleneck, \IASKIT{}
shares the common problem of all treecodes: (1) the parallelism shrinks
exponentially when traversing to the root, and (2) the possible global 
synchronization during the level by level traversal. 
Solving \eqref{e:smw} in a distributed node reduces the  product
to one process in the subcommunicator. This allows us to take the 
advantage of the collective \texttt{Bcast} and \texttt{Reduce}, but
the parallelism shrinks during the solving phase.
This efficiency degradation is not significant when  and  are
small, but we will observe this effect in the scaling experiments.
The load-balancing problem happens when the adaptive skeletonization
strategy is used to dynamically decide the rank of  and .
Processes with a smaller rank will finish the factorization earlier
and wait at the global barrier until other process to finish.
This situation requires a more sophisticate schedule to prevent
the process from idling.














