Given $p$ MPI processes and the distributed tree provided by \ASKIT{}, 
distributed \IASKIT{} is invoked from tree level-$\log{p}$ to the
root level. Without loss of generality, we assume $N$ and $p$ are powers 
of two, and each MPI process owns $n=N/p$ points corresponding to a 
subtree in level-$\log{p}$.
Each subtree is factorized and solved locally, but computing the tree nodes
above requires collaboration and communication.
We first illustrate the distributed algorithms, and later we analyze the
complexity. 

\textbf{Data Distribution.}
In \figref{fig:tree}, we show a 4-process distributed tree provided
by \ASKIT{}. In level-1, the root has two children $\alpha$ and $\beta$.
In level-$\log{4}$, $\alpha$ has two children $\lc(\alpha)$ and $\rc(\alpha)$
as well as $\beta$.
Each MPI process is labeled with different colors, and we label the
data they own including $\XX$, $\bu$, skeletons and projection matrices.
Processes in the same distributed tree node are grouped together
by the purple box to form a subcommunicator, and we use 
\{0, ..., $q-1$\} to denote the local MPI rank. 
In each non-root treenode, only \{0\} possesses the skeletons
and the projection matrix.

In each level, each MPI process only maintain a portion of 
$U$ and $V$ of the distributed node.
Thus, communication is required within and between each node in the 
same level during both factorization and solving phase.
\figref{fig:matrix} shows how $U$ and $V$ are distributed in the
tree we show in \figref{fig:tree}.
Each process is designed to own the $U$ (later overwritten by $W$) 
and $V$ portion with the same color, and the computation may require 
skeletons and the projection matrices from other processes.
Take the yellow process as an example.
In level-1, evaluating $V^{(1)}$ requires \{0\} (green) to broadcast the 
projection matrix $P_{\alpha[\tilde{\lc}\tilde{\rc}]}$. 
Evaluating $U^{(1)}$ requires skeletons $\tilde{\beta}$ from 
its sibling node (orange).
Factorizing $Z$ requires \{0\} to reduce all $VW$
parts in the same distributed node.
Solving \eqref{e:smw} requires both reducing $V\bu$ and broadcasting
$ZV\bu$ in the same node.  

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.35]{figures/distributed_tree_v2.pdf}
  \caption{Distributed binary tree with 4 processes. The leafs of the 
  tree are the roots of each local tree.
  Each process possesses the data of its colored rectangle 
  region, and processes in the same treenode are group into
  a subcommunicator.}
  \label{fig:tree}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[scale=.35]{figures/distributed_tree_v3.pdf}
  \caption{Distributed $U$ and $V$ factors. Each process
  only store the $U$ and $V$ portion of the same color in each
  level.}
  \label{fig:matrix}
\end{figure}

{\bf Factorization.}
In \algref{a:distributedfactor}, we present a recursive distributed 
factorization that traverses from level-$\log{p}$ to level-0.
In each level, \{0, ..., $\frac{q}{2}-1$\} will traverse the left child,
and \{$\frac{q}{2}$, ..., $q-1$\} will traverse the right child. 
We use \texttt{c} to denote the child node of $\alpha$.
Whether \texttt{c} is $\lc$ or $\rc$ depends on its local MPI rank.
To satisfy the data dependency, \{0\} \texttt{Bcast} 
$P_{\tilde{\alpha}[\tilde{l}\tilde{r}]}$ to all processes in the node
and \texttt{Bcast} the skeleton $\tilde{\alpha}$ to all processes
in the sibling node.
Each process then computes the $U$ and $V$ portion it owns.
Once the $U$ portion is computed, \algref{a:distributedsolve} is called
to apply the inverse of $\tilde{K}^{-1}_{\alpha\alpha}$ to get $W$.
Finally, every process compute the its $VW$ portion, and 
\{0\} \texttt{Reduce} all $VW$ portion to factorize $Z$.






\begin{algorithm}[!htp]
\begin{algorithmic}
  \IF {$\alpha$ is at level-$\log{p}$}
    \STATE \texttt{Factorize($\alpha$)}.
  \ELSE
    \STATE \{$i<\frac{q}{2}$\} \texttt{DistributedFactorize}($\lc$, $\frac{q}{2}$).
    \STATE \{$i\geq\frac{q}{2}$\} \texttt{DistributedFactorize}($\rc$, $\frac{q}{2}$).
    \STATE \{0\} \texttt{Bcast} $P_{\tilde{\alpha}[\tilde{l}\tilde{r}]}$ inside the distributed node.
    \STATE \{$0\rightarrow\frac{q}{2}$\} \texttt{SendRecv} $\sk{\lc}$.
    \STATE \{0\} \texttt{Bcast} inside $\lc$ using the $\lc$ communicator.
    \STATE \{$\frac{q}{2}$\} \texttt{Bcast} inside $\rc$ using the $\rc$ communicator.
    \STATE \{$\frac{q}{2}\rightarrow0$\} \texttt{SendRecv} $\sk{\rc}$.
    \STATE Compute \eqref{e:innerid} to get the $U$ and $V$ portions.
    \STATE \{$i<\frac{q}{2}$\}$W=\texttt{DistributedSolve}(\lc, U, \frac{q}{2})$.
    \STATE \{$i\geq\frac{q}{2}$\}$W=\texttt{DistributedSolve}(\rc, U, \frac{q}{2})$.
    \STATE \{0, $\frac{q}{2}$\} \texttt{Reduce} $VW$.
    \STATE \{$\frac{q}{2}\rightarrow0$ \texttt{SendRecv} $VW$.
    \STATE \{0\} computes and LU factorizes $Z$.
  \ENDIF
\end{algorithmic}
\caption{{} \texttt{DistributedFactorize}($\alpha$,$q$)}
\label{a:distributedfactor}
\end{algorithm}


{\bf Solver.}
The distributed solver is also implemented using recursive bottom-up
traversal. \algref{a:solve} is called to solve the local tree while 
reaching level-$\log{p}$.
\{0\} reduces all portions of $V\bu$ and compute $ZV\bu$
where $Z$ was previously factorized in \algref{a:distributedfactor}.
$Z^{-1}V\bu$ is \texttt{Bcast} back to all processes by \{0\}, 
and each process updates its own$\bw$ 
by $\bu-WZ^{-1}V\bu$ to complete the solving phase.


\begin{algorithm}[!htp]
\begin{algorithmic}
  \IF {$\alpha$ is at level-$\log{p}$}
    \STATE \texttt{Solve}($\bu$). \ELSE
    \STATE \texttt{DistributedSolve}($c(\alpha)$, $\bu$, $\frac{q}{2}$). \STATE Compute $V\bu$. \STATE \{0\} \texttt{Reduce} $V\bu$. \STATE \{0\}  Compute $ZV\bu$ using \eqref{e:blocksmw}. \STATE \{0\} \texttt{Bcast} $ZV\bu$. \STATE Compute $\bw=\bu-WZV\bu$. \ENDIF
\end{algorithmic}
\caption{{} $\bw=\texttt{DistributedSolve}(\alpha, \bu, q)$}
\label{a:distributedsolve}
\end{algorithm}

{\bf Complexity.}
We use a network model to derive the parallel complexity.
For simplicity, we assume that the network topology provides 
$\MA{O}(\log{p})$ \texttt{Bcast} and \texttt{Reduce}, and
we only use the order of the 
overall message size to simplify the the communication model
without discussing the latency and the bandwidth.

Let $T^{s}(r,q)$ be the parallel complexity of the solving phase 
with $r$ right hand side and $q$ processes. The base case 
$T^{s}(r,1)=\MA{O}(rn\log{n})$ is the complexity of \algref{a:solve}.
We can derive the complexity recursively as:
\begin{equation}
  T^{s}(r,q) = T^{s}(r,\frac{q}{2}) + \MA{O}(rns+rs^2) + \MA{O}(sr\log{q}).
\end{equation}
Here $\MA{O}{(rns+rs^2)}$ is the computation cost of \eqref{e:blocksmw}, and
$\MA{O}(sr\log{q})$ is the communication cost.
The overall complexity is 
$\MA{O}(rn\log{n} + rn\log{p}) + \MA{O}(sr\log^{2}{p})$ including
both computation and communication.
Observe that the communication cost doesn't scale with the problem size
$N$, instead it's only related to $r$, $s$ and $p$.


We derive $T^{f}(q)$ as the 
parallel complexity of the factorization phase with a fixed
skeleton size $s$ similarly. The base case 
$T^f(1) = \MA{O}(n\log^{2}{n})$ is the complexity of \algref{a:factor}.
We derive the complexity recursively as:
\begin{equation}
  T^f(q) = T^f(\frac{q}{2}) + T^s(s,\frac{q}{2}) + \MA{O}(s^3+ ns) + \MA{O}(sd+s^2).
\end{equation}
Here $T^s(s,\frac{q}{2})$ is the cost spent in \algref{a:distributedsolve}, 
$\MA{O}(s^3+ ns)$ includes the cost of \eqref{e:innerid}, and
$\MA{O}(sd+s^2)$ is the communication cost spent on broadcasting the
skeleton coordinates and the projection matrix.
Overall, the complexity is 
$\MA{O}(n\log^{2}{N})+\MA{O}(s^2\log^{3}{p}+d\log^{2}{p})$ 
including both computation and communication.
Observe that the sequential complexity is $p$ times the parallel
computation complexity, which indicates that the algorithm should
be able to achieve almost linear scalability. 
Overall the communication cost is minor, since it doesn't scale with $N$.












{\bf Discussion.}
Despite that the communication cost is not a bottleneck, \IASKIT{}
shares the common problem of all treecodes: (1) the parallelism shrinks
exponentially when traversing to the root, and (2) the possible global 
synchronization during the level by level traversal. 
Solving \eqref{e:smw} in a distributed node reduces the $VW$ product
to one process in the subcommunicator. This allows us to take the 
advantage of the collective \texttt{Bcast} and \texttt{Reduce}, but
the parallelism shrinks during the solving phase.
This efficiency degradation is not significant when $p$ and $s$ are
small, but we will observe this effect in the scaling experiments.
The load-balancing problem happens when the adaptive skeletonization
strategy is used to dynamically decide the rank of $U$ and $V$.
Processes with a smaller rank will finish the factorization earlier
and wait at the global barrier until other process to finish.
This situation requires a more sophisticate schedule to prevent
the process from idling.














