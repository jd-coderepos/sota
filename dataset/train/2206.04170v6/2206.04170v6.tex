\section{Results and Discussion}


\subsection{Compute and Time analysis Analysis}

We ran all the experiments on a single NVIDIA
RTX8000 GPU with 48GB video memory. In Table \ref{computetime}, we compare the cumulative training times for self-supervised training of a CNN and Transformer with DINO and CASS. We observed that CASS took an average of 69\% less time compared to DINO. Another point to note is that, CASS trained two architectures at the same time or in a single pass. While to train a CNN and Transformer with DINO it would take two separate passes.

\begin{table}[!htb]
\centering
\begin{tabular}{lll}
\hline
Dataset    & DINO              & CASS                    \\
\hline
Autoimmune & 1 Hour 13 Mins    & \textbf{21 Mins}          \\
Dermofit & 3 Hours 9 mins & \textbf{1 Hour 11 Mins} \\
Brain MRI  & 26 Hours 21 Mins  & \textbf{7 Hours 11 Mins}  \\
ISIC-2019  & 109 Hours 21 Mins & \textbf{29 Hours 58 Mins} \\
\hline
\end{tabular}
\caption{Self-supervised training time comparison for 100 epochs on a single RTX8000 GPU.}
\label{computetime}
\end{table}

\subsection{Autoimmune Diseases Biopsy Slides Dataset}
We did not perform 1\% training for the autoimmune diseases biopsy slides of 198 images because using 1\% images would be too small number to learn anything meaningful and the results would be highly randomized.

Following the self-supervised training and fine tuning procedure as described in Section 4.1 and 4.2, we observed that using CASS with the ViT B/16 backbone and ResNet50 improved upon existing state of the art results from DINO trained CNN and Transformers. 



\begin{table}[!htb]
\centering
\begin{tabular}{llll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Techniques}} & \multicolumn{1}{c}{\multirow{2}{*}{Backbone}} & \multicolumn{2}{l}{Testing F1 score} \\
\multicolumn{1}{c}{}                            & \multicolumn{1}{c}{}                           & 10\%       & 100\%         \\
\hline

DINO                                            & Resnet-50                                      & \textbf{0.8237±0.001}            &0.84252±0.008           \\
CASS                                           & Resnet-50                                   & 0.8158±0.0055           & \textbf{0.8650±0.0001}           \\
Supervised                                      & Resnet-50                                 & 0.819±0.0216          &  0.83895±0.007            \\
\hline

DINO                                            & ViT B/16                             & 0.8445±0.0008           &0.8639± 0.002             \\
CASS                                           & ViT B/16                                          & \textbf{0.8717±0.005}           & \textbf{0.8894±0.005}            \\
Supervised                                      & ViT B/16                                          &0.8356±0.007           &0.8420±0.009          \\
\hline
\end{tabular}
\caption{Results for autoimmune biopsy slides dataset. In this table we compare the F1 score on test set. We observed that CASS outperformed the existing state-of-art self-supervised method using 100\% labels for CNN as well as for Transformers. Although DINO outperforms CASS for CNN with 10\% labeled fraction. Overall CASS outperforms DINO by ~2.2\% for 100\% labeled training for CNN and Transformer. For Transformers in 10\% labeled training CASS' performance was ~2.7\% better than DINO.}
\end{table}

\subsection{Dermofit Dataset}
We did not perform 1\% training for this dataset as the training set was too small to draw meaningful results with just 10 samples. We observe that CASS outperforms both supervised and existing state of the art self-supervised methods for all label fractions. We present the F1 score for different label fractions in Table \ref{dermofitperformance}. 
\begin{table}[!htb]
\centering
\begin{tabular}{lll}

\hline
\multicolumn{1}{c}{\multirow{2}{*}{Techniques}} & \multicolumn{2}{l}{Testing F1 score} \\
\multicolumn{1}{c}{}                                         & 10\%       & 100\%         \\
\hline

DINO                                             (Resnet-50)                                     &0.3749±0.0011          &0.6775±0.0005           \\
CASS                                            (Resnet-50)                                     & \textbf{0.4367±0.0002}      & \textbf{0.7132±0.0003}           \\
Supervised                                       (Resnet-50)                                   & 0.33±0.0001           &  0.6341±0.0077           \\
\hline

DINO                        (ViT B/16)                             &0.332± 0.0002             &0.4810±0.0012            \\
CASS    (ViT B/16)                                  & \textbf{0.3896±0.0013}           & \textbf{0.6667±0.0002}            \\
Supervised         (ViT B/16)                               &0.299±0.002          &0.456±0.0077          \\
\hline
\end{tabular}
\caption{Results for the dermofit dataset. Parenthesis next to the techniques represent the architecture used, for example DINO(ViT B/16) represents ViT B/16 trianed with DINO. In this table we compare the F1 score on test set. We observed that CASS outperformed the existing state-of-art self-supervised method using for all label fractions and for both the architectures.}
\label{dermofitperformance}
\end{table}


\subsection{Brain tumor MRI dataset}

We observed that supervised CNNs performed better than Transformers on this dataset. Similarly, for DINO, CNNs performed better than Transformers by a margin. We observed that this trend is followed by CASS as well. The difference between CNN and Transformer performance is smaller for CASS as compared to the difference in performance for CNN and Transformer with supervised and DINO training. We report these results in Table \ref{brainMRIperformance}.
\begin{table*}[t]
\centering
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Techniques}} & \multicolumn{1}{c}{\multirow{2}{*}{Backbone}} & \multicolumn{3}{l}{Testing F1 score} \\
\multicolumn{1}{c}{}                            & \multicolumn{1}{c}{}                           & 1\%       & 10\%       & 100\%       \\
\hline
DINO                                            & Resnet-50                                      &\textbf{0.63405±0.09}          &\textbf{0.92325±0.02819}            & 0.9900±0.0058          \\
CASS                                           & Resnet-50                                      & 0.40816±0.13          & 0.8925±0.0254          &\textbf{0.9909±
0.0032}
             \\
Supervised                                      & Resnet-50                                      &0.52±0.018          &0.9022±0.011            & 0.9899± 0.003            \\
\hline
DINO                                            & ViT B/16                                          &0.3211±0.071      &0.7529±0.044           &0.8841±
0.0052
           \\
CASS                                           & ViT B/16                                           & \textbf{0.3345±0.11}          & \textbf{0.7833±0.0259}           &\textbf{0.9279± 
0.0213}
             \\
Supervised                                      & ViT B/16                                           & 0.3017 ± 0.077         & 0.747±0.0245           & 0.8719± 0.017           \\
\hline
\end{tabular}
\caption{While DINO outperformed CASS for 1\% and 10\% labeled training for CNN, CASS maintained its superiority for 100\% labeled training, albeit by just 0.09\%. Similarly, CASS outperformed DINO for all data regimes for Transformers, incrementally 1.34\% in for 1\%, 3.04\% for 10\%, and 4.38\% for 100\% labeled training. We observe that this margin is more significant than for biopsy images. Such results could be ascribed to the increase in dataset size and increasing learnable information.}
\label{brainMRIperformance}
\end{table*}


\subsection {ISIC 2019 Dataset}

The ISIC-2019 dataset is an incredibly challenging dataset, not only because of the class imbalance issue but because it is made of partially processed and inconsistent images with hard-to-classify classes. From Table \ref{ISICperformance} it is clear that CASS outperforms DINO for all label fractions for both CNN and Transformer by a margin.
\begin{table*}[t]
\centering
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Techniques}} & \multicolumn{1}{c}{\multirow{2}{*}{Backbone}} & \multicolumn{3}{l}{Testing Balanced multi-class accuracy} \\
\multicolumn{1}{c}{}                            & \multicolumn{1}{c}{}                           & 1\%       & 10\%       & 100\%       \\
\hline
DINO                                            & Resnet-50                                      &0.328±0.0016         &0.3797±0.0027            &0.493±3.9e-05            \\
CASS                                           & Resnet-50                                      &\textbf{0.3617±0.0047}            &\textbf{0.41±0.0019}            &  \textbf{0.543±2.85e-05}           \\
Supervised                                      & Resnet-50                                      &0.2640±0.031           &0.3070±0.0121            &0.35±0.006           \\ 
\hline
DINO                                            & ViT B/16                                           & 0.3676± 0.012           &0.3998±0.056          &0.5408±0.001            \\
CASS                                           & ViT B/16                                           &\textbf{0.3973± 0.0465}           &\textbf{0.4395±0.0179}            &  \textbf{0.5819±0.0015}          \\
Supervised                                      & ViT B/16                                           &0.3074±0.0005           & 0.3586±0.0314           &  0.42±0.007  \\
\hline
\end{tabular}

\caption{Results for the ISIC-2019  dataset. Comparable to the official metrics used in the challenge \url{https://challenge.isic-archive.com/landing/2019/}. We use balanced multi-class accuracy as our metric, which is semantically equal to recall value. We observed that CASS consistently outperforms DINO by approximately 4\% for all label fractions with CNN and Transformer.}
\label{ISICperformance}
\end{table*}

\subsection{Ablation Studies}

\subsubsection{Change in batch size}
To gauge the change in performance with the change in batch size, we ran experiments with CASS with a batch size of 8, 16, and 32. We present these results in Appendix B.1 and C.1. Based on them we concluded that CASS is robust to change in batch size. Interestingly, instead of dropping performance like existing methods, CASS-trained Transformers with smaller batch sizes performed better. 

\subsubsection{Training Epochs} We report the performance of CASS trained for 50, 100, 200 and 300 epochs in Appendix B.2 and C.2. On reducing the epochs to 50, a performance drop of 2\% was observed for CNN, while the performance of Transformer remained almost constant. Similarly, there is a ~2\% gain when we double the number of self-supervised training epochs. However, after that, the gain is minimal on changing epochs from 200 to 300. 

\subsubsection{Effect of augmentation change} CASS does not use hard augmentations like DINO or BYOL.
We study the effect of adding BYOL/DINO-like augmentations in Appendix B.3. Although, Gaussian blur helps in converging the CNN and Transformer for CASS. We find that adding BYOL-like hard augmentations costs performance. CASS has global-local cropping inbuilt due to difference in the receptive field of CNN and Transformer, unlike DINO, where it was added with augmentation. 

\subsubsection{Change in architecture} We provide intuition for changing the architecture of CNNs and Transformers in Appendix B.5 and C.3. As a baseline we started with ResNet-50 and ViT Base/16 Transformer for our experiments. But we also expand these results to other CNN and Transformer families. Furthermore, we also report results of using two CNNs or two Transformers on the brain MRI classification dataset in Appendix C.3.

\subsubsection{Optimization} For CASS, we used Adam optimizer for both CNN and Transformer. Traditionally, CNNs and more specifically ResNets have been used with a SGD optimizer, but in our case we use Adam optimiser. This choice is fairly unconventional and we further expand upon this in Appendix B.



\subsubsection{Studying the attention and feature maps}Our motivation to combine CNN and Transformer was to help the two architectures learn meaningful representations from each other. As mentioned already in Section 3, the two architectures focus on different parts of the image and hence create positive pairs without differential augmentation. In this section we study the feature maps and attention maps of CNN and Transformer respectively to see this gain qualitatively. Quantitative gains have been summarized in Section 5. We present the attention and feature maps for a given input image \ref{fig:main_sample_image} in this section and expand the study of feature and attention maps in Appendix D.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{Images/sample_image.png}
\caption{Sample image used from the test set of the autoimmune dataset.}
    \label{fig:main_sample_image}
\end{figure}
\begin{figure}
    \centering
\includegraphics[width=0.45\linewidth]{Images/autoimmune_sup.png}
    \includegraphics[width=0.45\linewidth]{Images/autoiimune_cass.png}
    \caption{Overall attention maps from supervised Transformer (on the left) and CASS trained Transformer (on the right). We pass the same image as input through both of them; the image used is shown in Figure \ref{fig:main_sample_image}. We observed that the CASS-trained Transformer's attention is more spread as compared to supervised Transformer. This can be easily inferred from the right-hand side bottom portion of both the attention maps.}
    
    \label{fig:attnmapssingle}
\end{figure}
\begin{figure}[!ht]
    \centering
\includegraphics[width=1\linewidth]{Images/cass2.drawio.png}
    \caption{This figure shows the feature map extracted after the first layer of ResNet-50 for CASS (on the left) and supervised CNN (on the right). From the four circles, it is clear that CASS-trained CNN can retain much more intricate detail about the input image (Figure \ref{fig:main_sample_image}) that the supervised CNN misses.}
    
    \label{fig:fmapssingle}
\end{figure}

\begin{itemize}

\item \textbf{Feature maps.} We studied the feature maps after Conv2d layer of ResNet-50. In this section, we focus on the features extracted after the first layer of ResNet-50 trained with CASS and supervised technique in Appendix D.2. We observed that CASS trained Resnet-50 was able to retain a lot more detail/information about the input as compared to the supervised ResNet-50. We present the extracted features in Figure \ref{fig:fmapssingle}. Furthermore, we expand this study to include feature maps from the first five layers of CASS and supervised ResNet-50 in Appendix D.

\item \textbf{Class attention maps.} Similar to feature maps of CNNs, for Transformers we study their class attention maps. We extracted the attention maps after the first attention block. For the sample image in \ref{fig:main_sample_image}, we present the attnetion maps resized to the image size in \ref{fig:attnmapssingle}. We observed that CASS trained Transformer is able to pay more attention to the intricate details in the input image as compared to the supervised Transformer. Furthermore, It is able to pay attention to areas that are missed by a supervised Transformer. We also expand this study to study attention maps averaged over 30 samples on different datasets in Appendix D. 

\end{itemize}
