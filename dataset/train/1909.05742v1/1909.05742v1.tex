\documentclass{article}







\PassOptionsToPackage{numbers,sort&compress}{natbib}
\usepackage[final]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[justification=centering]{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}


\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\DeclareMathOperator{\argmin}{arg\,min}

\title{Rethinking the CSC Model for Natural Images}



\author{
  Dror Simon \\
  Department of Computer Science\\
  Technion, Israel \\
  \texttt{dror.simon@cs.technion.ac.il}
  \And
  Michael Elad\\
  Department of Computer Science\\
  Technion, Israel \\
  \texttt{elad@cs.technion.ac.il}
}

\begin{document}

\maketitle

\begin{abstract}
  Sparse representation with respect to an overcomplete dictionary is often used when regularizing inverse problems in signal and image processing. In recent years, the Convolutional Sparse Coding (CSC) model, in which the dictionary consists of shift invariant filters, has gained renewed interest. While this model has been successfully used in some image processing problems, it still falls behind traditional patch-based methods on simple tasks such as denoising.
  In this work we provide new insights regarding the CSC model and its capability to represent natural images, and suggest a Bayesian connection between this model and its patch-based ancestor. Armed with these observations, we suggest a novel feed-forward network that follows an MMSE approximation process to the CSC model, using strided convolutions. The performance of this supervised architecture is shown to be on par with state of the art methods while using much fewer parameters.
\end{abstract}



\section{Introduction}
\label{sec:introduction}
The field of image restoration deals with the recovery of degraded images. Popular forms of degradation include an additive noise, a blurring kernel, missing pixels, and more. Retrieving an image from its degraded version is typically an ill-posed problem. Therefore, to enable the inversion task, it is necessary to include prior information on the original signal. An image prior, also referred to as an image model, relates to a mathematical description of the image true distribution. In the past 2-3 decades, many such models have been suggested and deployed. Some of these include reliance on spatial smoothness, self-similarity, and sparse representation \cite{rudin1992nonlinear,dabov2007image,elad2006image,buades2005non}. The later is the focus of this work.

The sparse representation model has been successfully incorporated in various signal and image processing applications \cite{dong2011image,dabov2007image,salmon2014poisson,dong2012nonlocally,ravishankar2011mr,plumbley2009sparse}. This model assumes that a signal  is formed by a linear combination of only a few , taken from the \emph{dictionary} , i.e. , where  is sparse. When a noisy signal  is at hand ( is a bounded energy noise: ), seeking for its sparse representation , leads to an estimation of the original signal via . Finding  is commonly referred to as \emph{sparse-coding} or a \emph{pursuit}, formulated as 

where the  pseudo-norm counts the number of non-zeros in the vector.\footnote{ is not formally a norm since it does not satisfy the homogeneity property.}  Sparse coding is NP-hard in general \cite{natarajan1995sparse}, hence approximation methods are used. A common approach replaces the  pseudo-norm with the , leading to a convex problem termed Basis-Pursuit (BP) \cite{chen1994basis}. The BP method has been theoretically analyzed \cite{elad2010sparse}, shown to successfully recover a solution close to the original sparse representation, depending on properties of the dictionary and the cardinality of the sought solution.



The dictionary is an important ingredient in the formation of this prior, as its atoms characterize the signals that this model can represent sparsely. Learning the dictionary from the corrupted signal itself, or from an external dataset, has been shown to be quite effective, leading to the development of various dictionary learning algorithms and their use \cite{engan1999method,aharon2006k, mairal2009online, tosic2011dictionary}. Unfortunately, due to the curse of dimensionality, these algorithms are applicable only for reasonably sized signals. Many algorithms overcome this limitation by dividing the complete signal (e.g. a complete image) into fully overlapping small \emph{patches}, treating each independently \cite{aharon2006k, mairal2009online, dabov2007image, zoran2011learning}. This treatment consists of imposing the sparse representation model on the patches using a local dictionary ,

where  extracts the -th patch from  , and its representation  is assumed to be sparse. Once clean estimates of the patches are found, this process proceeds by \emph{Patch Averaging} (PA), i.e. merging all the refined patches together to form a final global estimate of the clean image: 

where  places  in the -th location in the constructed image. Intuitively, operating independently on patches must be sub-optimal, since the dependencies between the patches are falsely neglected \cite{batenkov2017global}. To overcome this flaw, past work suggested enforcing the local prior on the patches of the merged image \cite{zoran2011learning,sulam2015expected}, leveraging the self-similarity between different patches \cite{mairal2009non}, and more.

Recently, there has been a renewed interest in global models that may overcome this local-global dichotomy. The Convolutional Sparse Coding (CSC) prior \cite{Grosse2007,szlam2010convolutional} replaces the traditional patch-based model with a global shift-invariant one. Instead of operating on patches, it suggests a global dictionary constrained by a specific structure -- a concatenation of banded circulant matrices\footnote{These represent convolutions with small support filters.}, limiting the degrees of freedom introduced by the general sparsity-based model. Various algorithms have been suggested to efficiently handle the global pursuit \cite{heide2015fast, silva2017fast, plaut2019greedy, zisselman2018local, wohlberg-2016-efficient}. These methods have been augmented by efficient dictionary learning algorithms \cite{papyan2017convolutional, zisselman2018local, garcia2018convolutional,chun2018convolutional}. A recent work provided theoretical guarantees for the CSC model and its corresponding global pursuit results \cite{papyan2017working}.

The CSC model has shown great success in several natural image processing tasks such as image separation, image fusion, and super-resolution, matching or outperforming local-based methods \cite{gu2015convolutional,papyan2017convolutional,liu2016image,rey2018variations,zisselman2018local}. Interestingly, one can find two common properties to all these success stories. The first is the fact that the CSC is merely used as a complementary component, modeling only the texture part of the image, after stripping its low-frequencies. Second, these applications assume noiseless images, and thus the CSC cannot fail in over-fitting the data. Indeed, when brought to other classical tasks, such as image denoising or other inverse problems that involve an additive noise, the CSC has been shown to fail utterly. 

The main contribution of this work is in providing novel insights regarding the CSC for modeling natural images, and extending the applicability of this prior while tying it to deep-learning. We propose an explanation for the incompetence of this model in representing natural images reliably, and show that PA can be perceived as a Minimum Mean Square Error (MMSE) approximation to the CSC. Building on this, we suggest to improve this approximation and obtain a CSC estimation process that operates directly on an image without any pre-processing steps. Finally, we leverage these observations to implement a feed-forward Convolutional Neural Network (CNN) whose layers strictly correspond to each step in the processing flow of sparse-coding based image denoising. Our results are on par with current state of the art supervised methods while drastically reducing the number of parameters.



\section{Background: Convolutional Sparse Coding}
\label{sec:background}

\subsection{The CSC Model}
\label{sec:csc}

The CSC model considers a shift-invariant property in the signal, by assuming that\footnote{For simplicity of the description, and without loss of generality, we describe the CSC throughout this paper as operating on 1D signals.}  is constructed by a sum of  convolutions of sparse feature maps  by filters  of length . CSC then refers to solving the following optimization problem:

Equivalently, we define a single global sparse representation vector , constructed by interlacing the sparse feature maps . A global dictionary is then composed as follows: Let  represent a local dictionary whose columns are the filters ; then  contains  shifts of this local dictionary -- see Figure \ref{fig:cscDefinition}. Under this description, \eqref{eq:cscFeatures} is equivalent to

When noisy measurements  are at hand, \eqref{eq:cscEqual} is modified to allow for variations in the signal, leading to the problem defined in Equation \eqref{eq:sc}.

We introduce additional definitions, taken from \cite{papyan2017working}, which will aid in our exposition. The sparse representation vector  can be thought of  concatenated vectors , termed \emph{needles}. Each  describes the contribution of the  filters when aligned to the -th element in , i.e.

where we have defined the \emph{slice} . Observe the resemblance between this and the patch-averaging in Equation \eqref{eqn:PA}. This may suggest that the CSC is in-fact a global model extending PA. If this was true, one could have expected the CSC to be at least as good as the local model on any image processing task. Is this the case? Keep reading. 

Similarly,  is a patch of size  extracted from . Equivalently, one may write , where a \emph{stripe}  concatenates  needles and  is termed the \emph{stripe dictionary}. Figure \ref{fig:cscDefinition} demonstrates these definitions.
An analysis of the convolutional sparse coding problem is proposed in \cite{papyan2017working}, showing that when all the stripes  are sparse,\footnote{This is measured via an  pseudo-norm, defined as 
.} the solution to \eqref{eq:cscEqual} is unique. Moreover, under the same conditions, BP is guaranteed to retrieve this solution. An analysis was also given for the noisy case, showing that under similar conditions, the solution attained by BP is stable.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth,trim={0 0 0 0},clip]{figures/CSC_structure.pdf}
    \caption{The CSC model and its components.}
    \label{fig:cscDefinition}
\end{figure}



\subsection{CSC in Practice}
\label{sec:uses}



The first application we mention is cartoon-texture separation, where the goal is to blindly decompose an image into its texture and cartoon parts. Recent papers have achieved successful results by incorporating the CSC model \cite{papyan2017convolutional,rey2018variations}. Curiously, these algorithms model the cartoon image via the Total-Variation smoothness assumption, while using the CSC to model only the texture.

A second application where the CSC achieves satisfactory results is image fusion \cite{liu2016image,zisselman2018local}. Here the goal is to integrate complementary information from multiple source images of the same scene. The results obtained by integrating the CSC model surpass those achieved by patch-based methods on various metrics \cite{liu2016image}. In such an algorithm, each image is first decomposed to smooth and detail layers. The fusion itself is obtained by computing the convolutional sparse representation of the detailed layers, and merging these by a pixel-wise max-pooling strategy.

Another application where CSC has been demonstrated to perform very well is single-image-super-resolution. Here, the objective is a high resolution (HR) image, obtained from a low resolution (LR) one. In \cite{gu2015convolutional} a CSC scheme is suggested, leading to superior results over patch-based methods. As in the image fusion task, the algorithm separates the LR image into a smooth and a residual image. While the smooth part is simply interpolated, the residual is coded using CSC, and the final details image is recovered by applying a set of filters on the obtained sparse representations.

In all these successful applications, the input image is first separated into smooth and non-smooth images. Then, without a formal reasoning, the CSC only models the detail-rich content of the image, hinting to its limitations. Why does the CSC perform well only on non-smooth signals? We  answer this question in the following sections. Another common feature to these successful applications is the fact that the data is assumed to be noiseless. Is this a coincidence? Can the CSC be of benefit on noisy natural images? In order to answer these questions let us refer to an unsuccessful use of the CSC: image denoising. Applying the CSC model directly on the noisy image leads to disappointing results, falling far behind PA \cite{plaut2019greedy,carrera2017sparse,wohlberg2017convolutional}. We emphasize that the same is true for other applications where noise cannot be neglected, such as deblurring and other inverse problems. 

To date, no CSC denoising algorithm competes favorably with the PA method on natural images, with the exception of \cite{sreter2018learned}. This work extends the concept of Learned Iterative Soft Thresholding (LISTA) \cite{gregor2010learning,rolfe2013discriminative,wang2015deep} to CSC, unfolding the pursuit algorithm into a recurrent network. The results obtained are on par with the K-SVD algorithm \cite{elad2006image}.\footnote{Improved performance is reported in their follow-up thesis.} The last part of our work is closely related to~\cite{sreter2018learned}. By adopting our insights on the CSC model and its MMSE approximation, we offer a CSC deployment that leads to enhanced denoising performance that are on par with the most recent supervised methods.



\section{Why Does the CSC Model Denoise Natural Images Poorly?}
\label{sec:whyCsc}

\subsection{Poor Coherence}
The work in \cite{papyan2017working} has show that the theoretical uniqueness and stability guarantees for the convolutional sparse coding problem are conditioned on the maximum number of local non-zero elements in  

where  is the mutual coherence of , i.e. the max absolute normalized cross-correlation between its columns.\footnote{The  is defined as 
.}
Moreover, under the same conditions, BP is guaranteed to recover this solution. This bound implies that to allow for a large number of active filters in the signal while keeping the solution accessible, the filters and all their shifts must have low cross-correlations. Specifically, the auto-correlation of the filters should be low as well. Unfortunately, this property does not align with the characteristics of natural images. For the most part, these consist of piece-wise smooth regions and occasional textures. This structure is key in many denoising and compression methods \cite{rudin1992nonlinear,lewis1992image}. Hence, to allow for a sparse representation, a convolutional dictionary must contain smooth or piece-wise smooth filters. That said, the auto-correlation of a these filters decay slowly, leading to highly correlated atoms in the global dictionary, restricting the number of non-zero elements allowed in each stripe  while satisfaying the above bound. For example, if a dictionary contains the constant (DC) filter, the maximum number of non-zeros allowed in a stripe to assure uniqueness is , enforcing unpainted pixels in the signal. 

Generally, a CSC representation of a natural image imposes a contradiction between the cardinality of the sparse representation  and the coherence of the global dictionary. To assure a sparse representation, the former requires piecewise smooth filters, whereas the latter demands low global mutual-coherence, which counters these slow-changing filters. In its current form, the CSC cannot satisfy the two demands simultaneously, making it unsuitable for natural images. Note that the successful applications that were mentioned in Section \ref{sec:uses}, apply the CSC model only on the texture rich part of the image, leading to Gabor-like non-smooth filters, thus avoiding the described conflict.



\subsection{A Bayesian Standpoint}
\label{sec:bayes}
From a Bayesian point of view, the solution to the problem posed in Equation \eqref{eq:sc} (or its Lagrangian form) corresponds to the Maximum A-posteriori Probability (MAP) estimator under a sparse prior~\cite{larsson2007linear,schniter2008fast,eladweighted,simon2018mmse}. Clearly, this solution is inferior to the Minimum MSE (MMSE) estimator in terms of MSE when the two differ. As we show next, as opposed to a convolutional pursuit (being MAP approximation), PA performs a restrained approximation to the CSC MMSE estimator w.r.t. the entire image, explaining its superiority. To do so, we first formalize the PA approach, then we present the CSC MMSE estimator and finally, we show their connection.

PA obtains a clean estimate for each patch and averages overlapping estimates together. Specifically, under a (local) sparse prior with a dictionary , each patch participates in a local pursuit independently, leading to  independent optimization problems,\footnote{we assume the use of the BP in its Lagrangian form}

Once  are found, the clean patches are synthesized by , and these are placed in the signal while averaging overlapping elements from different patches -- see Equation \eqref{eqn:PA}.

We move now to discuss the MMSE estimation under a sparsity-promoting prior \cite{eladweighted,simon2018mmse}. Using marginalization, MMSE of the global convolutional sparse representation vector can be written as 
where  stands for the support of ,  is the prior probability of such a support (assumed to promote sparse vectors) and  is the set of all possible supports. Furthermore,  is the MMSE estimator of the sparse representation vector given the support and the noisy measurements, known as the \emph{oracle estimator} \cite{eladweighted}. Equation \eqref{eq:cscMmse} suggests that the MMSE estimator is actually a dense vector consisting of a weighted average of all the possible oracle estimators, where the weight of each is its prior probability, .  Note that computing the MMSE is an exhaustive task that sweeps through all the possible supports, and therefore approximation methods are needed. A natural strategy in this context is to sample a sufficient number of supports from , and replace the expectation with a sample mean over these. 

Indeed, consider the case where the sampled supports are such that  results in non-overlapping tangent slices. Overall, there are  different slice arrangements that uphold this assumption differing only in the location of the first slice on the image. Equivalently, the -th arrangement can be described by a convolutional strided dictionary, where the stride equals the size of the filter , and the first non-zero needle in  is located in the -th index . We shall denote this strided dictionary by  and its corresponding representation as . Sparse coding the -th shift can be done using BP, which under these constraints can be written as

This can be solved for each needle separately,

while zeroing all the other needles.
Thus, under the constraint of non-overlapping slices, estimating the CSC representation  is equivalent to  independent local pursuits. Clearly, this estimation results with a ``blockified'' image, due to the lack of overlaps.

Repeating this estimation process  times, each time forcing a different shift , leads to a set of estimates , where  denotes the estimate obtained using the -th shift. Looking back into the MMSE estimator in Equation \eqref{eq:cscMmse}, the MMSE can be approximated as 

if we further assume that all the estimates are a-priori equally likely.
Since each  is obtained by a local non-overlapping pursuit \eqref{eq:localCsc}, the result in \eqref{eq:gammaMmse} is exactly the above outlined PA procedure. Hence, PA can be perceived as an MMSE approximation of the CSC model, explaining its superior MSE performance when compared to a single global CSC pursuit.\footnote{The term approximation refers to considering only a small subset of supports in the averaging process.} Armed with this insight, can we propose better CSC MMSE estimates? This takes us to the next section. 



\section{The Proposed Approach}
\subsection{Generalizing the MMSE Approximation Using Strided Convolutions}
\label{sec:strided_csc}
We suggest to generalize the non-overlapping slices assumption and allow for a smaller constant stride. Formally, in the non-overlapping case, the stride between adjacent slices was of the same size as the filters themselves, leading to  such estimates. We suggest to use a stride , where , leading to  estimates in a 1D signal or  in a 2D one -- each originating from a different initial shift in the signal. Finally, we average these together, as suggested in Equation~\eqref{eq:cscMmse}. Note that when , each estimate allows for overlapping slices, implying that the pursuit must be done globally on all the involved slices together. This necessarily leads to a global agreement between these slices, as opposed to PA where each patch (slice) is estimated separately. Furthermore, when  is sufficiently large, the mutual coherence of the global dictionary can be preserved even for smooth filters, in contrast to the standard CSC pursuit (), since the filters only partially overlap.

For a preliminary evaluation of our approach, we perform a denoising experiment on images from the Set12 dataset contaminated with white Gaussian noise with standard deviation . We use both the standard PA algorithm, and the proposed strided CSC using various strides , followed by an averaging operation. BP in its error-bounded from followed by a debiasing step is used to sparse code the signals both in the convolutional and the PA cases. The twice over-redundant DCT dictionary of size  is chosen as the local dictionary . Note that in the strided case, pixels may now have a different number of slices (filters) overlapping them, depending on their position in the image and the stride. To compensate for this, we normalize the filters appropriately for each stride.

A summary of the results of this experiment are presented in Table~\ref{table:csc_vs_pa} (per-image results can be found in the supplementary material). As expected, when using CSC with a stride of , i.e. standard CSC, the denoising performance is poor and the PA method is substantially better. We attribute this to the high coherence of the global dictionary, making the estimated image overfit the noise. However, the best results are achieved when the stride is large but smaller than the size of the filters, restraining the coherence, while allowing the filters to overlap, leading to a global consensus in each of the estimates. Interestingly, the CSC achieved better results even though its error constraint () is global, as opposed to the much more detailed local constraint used by PA.

\begin{table}[t]
\centering
\caption{Average Set12 denoising results (PSNR) using PA and CSC with various strides (). CSC Results that surpass PA are marked in blue. Best results are bold.}
\label{table:csc_vs_pa}
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
         & \multicolumn{10}{c}{CSC - stride size }                                                               &       \\ \cmidrule(lr){2-11}
 &  &  &  &  &  &  &  &  &  &          & PA    \\ \midrule
15       & 28.99 & 29.27 & 30.01 & 30.66 & 31.06 & 31.21 & {\color{blue}31.31} & {\color{blue}31.39} & {\color{blue}31.45} & {\color{blue}\textbf{31.46}} & 31.23 \\
25       & 25.78 & 26.11 & 26.94 & 27.72 & 28.26 & 28.50 & 28.64 & {\color{blue}28.75} & {\color{blue}28.84} & {\color{blue}\textbf{28.88}} & 28.73 \\
50       & 21.49 & 22.11 & 23.17 & 23.83 & 24.52 & 24.86 & 25.05 & 25.29 & {\color{blue}25.47} & {\color{blue}\textbf{25.56}} & 25.32 \\
75       & 18.83 & 19.58 & 20.95 & 21.81 & 22.43 & 22.75 & 22.97 & 23.25 & {\color{blue}23.51} & {\color{blue}\textbf{23.66}} & 23.28 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{CSCNet -- a Supervised Denoising Model}
A popular method to solve BP, i.e. ,
is the ISTA algorithm, which operates iteratively as follows:

where , and  is the soft-thresholding operator extended to operate in an elementwise fashion.\footnote{ represent the largest eigenvalue, and  is defined as }
Often times, convergence requires a large number of iterations, making this process inefficient. To overcome this burden, the LISTA algorithm~\cite{gregor2010learning} has been proposed to approximate the sparse coding process, by learning the parameters of a non-linear recurrent encoder that strictly follows  iterations of the iterative process described in Equation \eqref{eq:ista}. This concept has been extended to the convolutional setting in \cite{sreter2018learned} as follows:

where  stands for a convolution operator and  a transposed-convolution one.
Once the sparse vector is at hand, the estimated clean image is then obtained by a linear transposed-convolutional decoder, i.e. . The matrices  and  are structured as a set of support bounded shift invariant filters, and together with the thresholds vector , are learned in a supervised manner. Note that the number of parameters does not grow with , the number of unrolled iterations.

Following the CSC MMSE approximation introduced in Section \ref{sec:strided_csc}, we propose to use a strided convolutional structure on the learned matrices using a constant stride . To obtain an estimate for each possible shift, we duplicate the input image  times, where each duplicate is a shifted version of the original image. Following Equation \eqref{eq:gammaMmse} the estimated image is a simple average of the estimates of all the shifts. A diagram of the proposed architecture is presented in Figure \ref{fig:architecture}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 0 0 0},clip]{figures/network.pdf}
    \caption{The CSCNET architecture.}
    \label{fig:architecture}
    \vspace{-2mm}
\end{figure}

\subsection{Experiments}
To train the proposed model, we prepare a training set of input-output pairs. The clean images are taken from the Waterloo Exploration Dataset~\cite{ma2017waterloo} and 432 images from BSD~\cite{amfm_pami2011}. The noisy inputs are obtained by adding white Gaussian noise with a constant standard deviation . In each iteration, a random patch of size 128 is cropped from an image and a random realization of noise is sampled.

We train 4 models, one for each noise level . For each model we learn  filters of size , use a stride  and set . To learn the parameters of the model, we employ the ADAM optimizer~\cite{kingma2014adam} and minimize the  loss, i.e. . We use a learning rate of  and decrease it by a factor of 0.7 every 50 epochs and iterate over 250 epochs. To avoid divergence, we set the  parameter of the optimizer to . We evaluate the performance of the models using the BSD68 dataset that was excluded from the training set. Additional experiments and information can be found in the supplementary material and in \url{https://github.com/drorsimon/CSCNet}.

Table \ref{table:csc-net_results} presents the results of our models compared to other leading methods, and Figure \ref{fig:filters} shows some of the learned filters, taken from . The proposed model outperforms BM3D~\cite{dabov2007image}, TNRD~\cite{chen2017trainable}, WNNM~\cite{gu2014weighted} and MLP~\cite{burger2012image}, while being on par with DnCNN~\cite{zhang2017beyond} and FFDNet~\cite{zhang2018ffdnet}. That said, we mention two differences between the proposed model and the other two leading methods:
\begin{enumerate}
	\item Number of parameters -- The number of parameters in the proposed approach does not grow with the depth of the model. Hence, it uses much fewer parameters compared to other modern methods, as demonstrated in Table~\ref{table:params}.
	\item Batch Normalization (BN) -- The other two leading denoising methods are based on general deep learning techniques, and therefore employ BN which is known to improve the performance and convergence rate of the trained model~\cite{ioffe2015batch}. As our presented method relies only on the CSC prior, we did not include such operators.
\end{enumerate}

To study the effect of the stride-size, we have trained 6 models, each one with a different stride. The results in Figure \ref{fig:stride}, referring to noise level , show the same tendency as in Table \ref{table:csc_vs_pa}, namely, setting the stride too high () results in independent patch based processing with weaker performance (dB); setting it to  leads to a regular (non-MMSE) deployment of the CSC with highly correlated atoms and the weakest estimate (dB), which is still better than BM3D. The best results are obtained for  or  (dB).

\begin{table}[t]
\centering
\caption{Denoising performance (PSNR) on the BSD68 dataset.\label{table:csc-net_results}}
\begin{tabular}{@{}cccccccc@{}}
\toprule
 & BM3D  & WNNM  & TNRD & MLP & DnCNN & FFDNet & CSCNet \\ \midrule
15       & 31.07 & 31.37 & 31.42 & --    & 31.72 & 31.63  & 31.57   \\
25       & 28.57 & 28.83 & 28.92 & 28.96 & 29.22 & 29.19  & 29.11   \\
50       & 25.62 & 25.87 & 25.97 & 26.03 & 26.23 & 26.29  & 26.24   \\
75       & 24.21 & 24.40 & --    & 24.59 & 24.64 & 24.79  & 24.77   \\ \bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.83\linewidth,trim={0 -15 0 -15},clip]{figures/filters.pdf}
  \captionof{figure}{CSCNet filters.}
  \label{fig:filters}
\end{minipage}\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/lista_strides_results.pdf}
  \captionof{figure}{CSCNet test error for various strides.}
  \label{fig:stride}
\end{minipage}
\vspace{-3mm}
\end{figure}

\begin{table}[h]
\vspace{-2mm}
\centering
\caption{Comparison of number of parameters in leading denoising architectures.\label{table:params}}
\begin{tabular}{@{}llllll@{}}
\toprule
       Model & First layer                  & Last layer                   & Mid layers                                                                     & Total   \\ \midrule
DnCNN  &  &  &                                            & 556,032 \\
FFDNet &  &  &                                            & 486,080 \\
CSCNet & \multicolumn{1}{c}{--}                           &       &  & \multicolumn{1}{r}{~~63,700}  \\ \bottomrule
\end{tabular}
\end{table}

We further test our approach on color image denoising. We perform similar experiments to those described earlier, where this time the input image and each filter have 3 channels (RGB). The denoising performance of our architecture is presented in Table \ref{table:color}. As before, our results are on par with other leading methods (DnCNN, FFDNet) while using much fewer parameters.


\begin{table}[t]
\centering
\caption{Denoising performance (PSNR) on the \\ color-BSD68 dataset.\label{table:color}}
\begin{tabular}{@{}ccccc@{}}
\toprule
 & CBM3D & CDnCNN & FFDNet & CSCNet \\ \midrule
15       & 33.52 & 33.89  & 33.87  & 33.83  \\
25       & 30.71 & 31.23  & 31.21  & 31.18  \\
50       & 27.38 & 27.92  & 27.96  & 28.00  \\
75       & 25.74 & 24.47  & 26.24  & 26.32  \\ \bottomrule
\end{tabular}
\end{table}

\section{Conclusions}
This work exposed the limitations of the CSC model in representing natural images in the presence of noise. Investigation of the patch-averaging scheme and the origins for its success has lead us to offer an MMSE approximation pursuit that overcomes these limitations effectively. A feed-forward architecture based on our insights was shown to be on par with the best supervised denoising algorithms in the literature. Our future work will focus on further improvements of this scheme by considering (i) the addition of  batch-normalization; (ii) a multi-scale architecture, as proposed in recent leading methods; (iii) adoption of a local error constraint as in PA; and (iv) exploiting self-similarity, as practiced in \cite{mairal2009non} and more recently in \cite{liu2018non}. In all these directions, our prime goal is to incorporate these ideas while maintaining the purity of CSC model, so as to preserve intact the theoretical justification of the proposed architecture. 

\section*{Acknowledgement}
The research leading to these results has received funding from the  Technion Hiroshi Fujiwara Cyber Security Research Center and the Israel Cyber Directorate.

\small
\bibliographystyle{ieeetr}
\bibliography{cites.bib}
\end{document}
