
\newpage
\begin{center}
	\textbf{\Large Appendix}
\end{center}

In this document, we provide details on the experimental setting, more ablation studies, more quantitative results on semantic correspondence benchmarks, including SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal}, and PF-WILLOW~\cite{ham2016proposal}, and more qualitative results on all the benchmarks we used.\vspace{-10pt}

\section*{Appendix A. Experimental Setting for Semantic Correspondence}



\subsubsection{Datasets.}
For the datasets we used, we follow the common protocol~\cite{min2019hyperpixel,min2020learning,Rocco18b,zhao2021multi,huang2019dynamic,min2021convolutional,cho2021semantic} and use standard benchmarks~\cite{ham2016proposal,ham2017proposal,min2019spair}. Specifically, we consider SPair-71k~\cite{min2019spair}, which provides a total of 70,958 image pairs with extreme and diverse viewpoints, scale variations, and rich annotations for each image pair. We also consider relatively small-scale datasets, which include PF-PASCAL~\cite{ham2017proposal} containing 1,351 image pairs from 20 categories and PF-WILLOW~\cite{ham2016proposal} containing 900 image pairs from 4 categories, where each dataset provides corresponding ground-truth annotations.\vspace{-10pt}
\subsubsection{Evaluation metric.}
For evaluation on SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal}, and PF-WILLOW~\cite{ham2016proposal}, we employ the percentage of correct keypoints (PCK). It is computed as the ratio of estimated keypoints within the threshold from ground-truths to the total number of keypoints. Concretely, given predicted keypoint  and ground-truth keypoint , we count the number of predicted keypoints that satisfy the following condition: , where  denotes Euclidean distance;  denotes a threshold value;  and  denote height and width of the object bounding box or the entire image, respectively. We evaluate on PF-PASCAL with , and SPair-71k, and PF-WILLOW with  following the common protocol. \vspace{-10pt}

\subsubsection{Implementation Details.}
We use ResNet-101~\cite{he2016deep} pre-trained on ImageNet~\cite{deng2009imagenet} for the backbone feature extraction networks. We leave all the components in VAT unchanged. However, we build a different objective function. As in~\cite{min2019hyperpixel,min2020learning,min2021convolutional}, we assume  ground-truth keypoints are provided. We utilize Average End-Point Error (AEPE)~\cite{truong2020glu} and compute it by averaging the Euclidean distance between the ground-truth and estimated flow. Specifically, we compute the loss as , where  is the ground-truth flow field and  is the predicted flow field. Note that we achieve this without making any modification to the network architecture. To report the results for different  thresholds, we employ the pre-trained weights released by authors, and simply evaluate without making any changes to their architectures. We use the same data augmentation used in CATs~\cite{cho2021semantic}. For the learning rate, we use the AdamW~\cite{loshchilov2017decoupled} optimizer with  for VAT and  for the backbone feature networks. Finally, we use appearance embedding from conv, conv and conv as done for FSS-1000~\cite{li2020fss}.

\vspace{-10pt}


\section*{Appendix B. Additional Ablation Study} 

\subsubsection{Ablation study for feature backbone.}
\begin{wraptable}{r}{5.5cm}
\begin{center}
\scalebox{1}{
\begin{tabular}{l|cc}
\toprule
     \multirow{3}{*}{Backbones feature} &\multicolumn{2}{c}{FSS-1000~\cite{li2020fss}} \\
     &\multicolumn{2}{c}{mIoU ()}\\
&1-shot &5-shot \\  \midrule
ResNet50~\cite{he2016deep} &\underline{90.1} &\underline{90.7}\\
ResNet101~\cite{he2016deep}&\textbf{90.3} &\textbf{90.8}\\
PVT~\cite{wang2021pyramid} &{90.0}&90.6\\
Swin transformer~\cite{liu2021swin} &89.8 &90.2 \\\bottomrule
    \end{tabular}
}
\end{center}
\vspace{-10pt}\caption{\textbf{Ablation study of different feature backbone. }}\label{tab:backbone}
\end{wraptable}
Conventional few-shot segmentation methods only utilized CNN-based feature backbones~\cite{he2016deep} for extracting features.~\cite{zhang2019canet} observed that high-level features contain semantics of objects which could lead to overfitting and is not suitable to use for the task of few-shot segmentation. Then the question naturally arises, what about other networks? As addressed in many works~\cite{raghu2021vision,dosovitskiy2020image}, CNN and transformers see images differently, which means that the kinds of backbone networks may affect the performance significantly, but this has never been explored for this task. We thus exploit several well-known vision transformer architectures to explore the potential differences that may exist. 

The results are summarized in Table~\ref{tab:backbone}. We find that both convolution- and transformer-based backbone networks attain similar performance. We conjecture that although it has been widely studied that convolutions and transformers see differently~\cite{raghu2021vision}, as they are pre-trained on the same dataset~\cite{deng2009imagenet}, the representations learned by models are almost alike.  Note that we only utilized backbones with a pyramidal structure, and the results may differ if other backbone networks are used, which we leave for future exploration. \vspace{-10pt}




\subsubsection{Effectiveness of Data Augmentation. }
We explore the effectiveness of data augmentation for few-shot segmentation. In this experiment, we employ two types of data augmentation, which are introduced either in PFE-Net~\cite{tian2020prior} or CATs~\cite{cho2021semantic}. We summarize the augmentation types in Table~\ref{pfeaug} and Table~\ref{catsaug}. For this ablation study, we use two datasets, PASCAL-5~\cite{shaban2017one} and FSS-1000~\cite{li2020fss}.  The results are summarized in Table~\ref{ablation}. Note that we use the same augmentation types and probability as theirs. For a fair comparison, we keep all the other experimental settings the same, \textit{e.g.,} number of iterations and learning rate.  

\begin{table}[]
    \centering
    \scalebox{1}{
   \begin{tabular}{c|c|ccccc|c}
\toprule
     \multirow{3}{*}{PFE-Net Aug.~\cite{tian2020prior}} &\multirow{3}{*}{CATs Aug.~\cite{cho2021semantic}} &\multicolumn{5}{c|}{PASCAL-5~\cite{shaban2017one}} &{FSS-1000~\cite{li2020fss}} \\
     & &\multicolumn{5}{c|}{mIoU ()}&{mIoU ()}\\
     &&  &  &  &  &mean &\\
   \midrule
   \xmark &\xmark &\textbf{70.0} &\textbf{72.5} &\textbf{64.8} &\underline{64.2} &\textbf{67.9} &\textbf{90.3}\\
   \cmark &\xmark &\underline{68.4} &\underline{72.3} &64.4 &63.9 &\underline{67.3} &90.0\\
   \xmark &\cmark &65.7 &72.2 &62.3 &\textbf{64.3} &66.1 &90.1\\
   \cmark &\cmark &65.2 &71.1 &63.2 &63.4 &65.7 &\underline{90.2}\\\bottomrule


    \end{tabular}}
        \caption{\textbf{Ablation study of Data Augmentation. }}\label{ablation}
\end{table}

As PFE-Net~\cite{tian2020prior} does not address the effectiveness of data augmentation and CATs~\cite{cho2021semantic} is designed for the semantic correspondence task, we are the first to analyze the effectiveness of data augmentation in the few-shot segmentation setting. Overall, we observe that using the data augmentation techniques severely affects the overall performance. Interestingly, although the augmentation technique introduced by CATs~\cite{cho2021semantic} showed a significant performance boost in the semantic correspondence task, it attains the lowest mIoU when evaluated on PASCAL-5~\cite{shaban2017one} and the second lowest for FSS-1000~\cite{li2020fss}. The severe performance drop in PASCAL-5~\cite{shaban2017one} indicates a detrimental influence of using CATs~\cite{cho2021semantic} data augmentation. However, given the small difference to the best performance (0.3) for FSS-1000~\cite{li2020fss}, the results may differ in a retrial. For PFE-Net~\cite{tian2020prior} data augmentation, we observe results to be on par with the best reported results. However, at fold 0, there is a large gap between them, which indicates the detrimental effects of data augmentation on performance. Using both augmentations results in a large performance drop for PASCAL-5~\cite{shaban2017one}, arguably due to the detrimental effects of both augmentations, but for FSS-1000~\cite{li2020fss}, we observe only a small difference. 

\begin{table*}
    \parbox{.5\linewidth}{
    \centering
         \begin{tabular}{cl|c}
       \toprule
        &Augmentation type &Probability \\
        \midrule
        \textbf{(I)} &ToGray  &0.2 \\
        \textbf{(II)} &Posterize &0.2 \\
        \textbf{(III)} &Equalize &0.2 \\
        \textbf{(IV)} &Sharpen &0.2 \\
        \textbf{(V)} &RandomBrightnessContrast &0.2 \\
        \textbf{(VI)} &Solarize &0.2 \\
        \textbf{(VII)} &ColorJitter &0.2 \\\bottomrule
        
 
\end{tabular}
    \caption{\textbf{CATs~\cite{cho2021semantic} Aug. Type. }}
    \label{catsaug}
       
    }
    \hfill
    \parbox{.45\linewidth}{
    \centering
        
       \begin{tabular}{cl|c}
       \toprule
        &Strong Aug. type &Probability \\
        \midrule
        \textbf{(I)} &RandScale  &1 \\
        \textbf{(II)} &Crop &1 \\
        \textbf{(III)} &Gaussian Blur &0.5 \\
        \textbf{(IV)} &Horizontal Flip &0.5 \\
         \textbf{(V)} &Rotate &0.5 \\
        \bottomrule
        

\end{tabular}
    \caption{\textbf{PFE-Net~\cite{tian2020prior} Aug. Type.}}
    \label{pfeaug}
   
    }
\end{table*}
Consequently, we conjecture that the detrimental effects on PASCAL-5~\cite{shaban2017one} and seemingly trivial effects on FSS-1000~\cite{li2020fss} could be attributed to a few reasons: First, as shown in Table~\ref{ablation}, since the difference between the results of the non-data augmentation approach and the PFE-Net~\cite{tian2020prior} augmentation approach is only 0.6 for PASCAL-5, this may be due to the implementation details. For the training, we followed HSNet~\cite{min2021hypercorrelation} to force randomness for diverse episode combinations, which may have made such a gap. Second, although the data augmentation may help transformers by providing inductive bias and addressing the heavy need for data, for few-shot setting, where the objective is to predict labels of unseen classes, the results may be different to that of semantic correspondence. It was demonstrated~\cite{cho2021semantic} that for semantic correspondence, data augmentation indeed helps to boost the performance, but a different problem formulation for few-shot segmentation may result in detrimental effects. Third, since we act on correlation maps, applying data augmentation may significantly affect the matching distribution at each pixel. Unlike those works directly working on feature refinement~\cite{tian2020prior,zhang2021few}, where adopting data augmentation has a direct influence on feature maps, VAT aggregates the correlation maps computed between the features extracted from augmented images, which may result in different effects (performance drop) when the objective is to predict unseen classes. Lastly, combining both augmentations may increase the difficulty of learning, which in turn impacts accuracy.\vspace{-10pt}







\subsubsection{Ablation study for ATD.}
In this ablation study, we show a quantitative comparison between the proposed ATD and a decoder without transformers~\cite{vaswani2017attention,wang2020linformer,wu2021fastformer,liu2021swin} to find out whether the model benefits from the use of transformers for further cost aggregation and filtering with the aid of the appearance embedding. For convenience, we call this Appearance-aware Decoder (AD). To implement this, we only exclude the transformers within ATD and leave all the other components and training settings unchanged, \textit{e.g.,} network architecture, hyperparameters, learning rate and number of iterations.\vspace{-10pt}
\begin{table}[]
    \centering
   \begin{tabular}{l|cc|cc|cc}
\toprule
     \multirow{3}{*}{Components} &\multicolumn{6}{c}{FSS-1000~\cite{li2020fss}} \\
     &\multicolumn{2}{c}{mIoU ()}&\multicolumn{2}{c}{FB-IoU ()}&\multicolumn{2}{c}{mBA ()}\\
 &1-shot &5-shot &1-shot &5-shot &1-shot &5-shot\\  \midrule
Convolutions& 87.3&88.8&92.2&93.2&66.8&67.2\\
Transformers&90.3&90.8&94.0&94.4&68.0&68.6\\\bottomrule
    \end{tabular}
        \caption{\textbf{Ablation study for ATD. }}\label{tab:decoder}
\end{table}
As shown in Table~\ref{tab:decoder}, we observe a large performance gap between AD and ATD, which demonstrates that using transformer allows for more effective aggregation, filtering and integration of correlation maps and appearance embedding. More specifically, we observe a 3 mIoU difference and find similar differences for FB-IoU and mBA. Without using transformers, where only convolutions are used, we observe that the results are equal  to that of (\textbf{VI}) in the ablation study for VAT. This indicates that meaningful aggregation may not have occurred. It should be noted that we observe highly competitive results for mBA for both approaches, confirming a positive effect from the high-resolution spatial structure of the appearance embedding.\vspace{-10pt}  

\subsubsection{Ablation study for VCM.}
For this ablation study, we aim to further support our claims that the VCM (overlapping convolutions) compensates for the lack of inductive bias and alleviates the detrimental effects caused at window boundaries. To this end, we use Linear transformer~\cite{katharopoulos2020transformers}, Fastformer~\cite{wu2021fastformer} and Swin Transformer~\cite{liu2021swin} to validate the effectiveness. Note that we already reported the results for the ones with VCM, but we additionally provide FB-IoU and mBA results. For the implementation of VEM, we refer the readers to Algorithm 1. 




\begin{table}[]
    \centering
   \begin{tabular}{l|cc|cc|cc}
\toprule
     \multirow{3}{*}{Components} &\multicolumn{6}{c}{FSS-1000~\cite{li2020fss}} \\
     &\multicolumn{2}{c}{mIoU ()}&\multicolumn{2}{c}{FB-IoU ()}&\multicolumn{2}{c}{mBA ()}\\
 &1-shot &5-shot &1-shot &5-shot &1-shot &5-shot\\  \midrule
VEM + Linear Transformer~\cite{katharopoulos2020transformers}&87.0 &87.4&90.7&91.0&65.0&64.9\\
VEM + Fastformer~\cite{wu2021fastformer}&87.1&87.6&90.9&91.2&65.3&65.2\\
VEM + Swin Transformer~\cite{liu2021swin}&89.9&90.5&92.9&94.0&67.8&68.2\\\midrule
VCM + Linear Transformer~\cite{katharopoulos2020transformers}& 87.7&88.3&92.3&92.2&66.5&66.7\\
VCM + Fastformer~\cite{wu2021fastformer}& 87.8&88.2&91.8&91.9&66.4&66.4\\
VCM + Swin Transformer~\cite{liu2021swin}&90.3&90.8&94.0&94.4&68.0&68.6\\

\bottomrule
    \end{tabular}
        \caption{\textbf{Ablation study for VCM. }}\label{vcm}
\end{table}



As shown in Table~\ref{vcm}, we find a similar pattern to the results for VCM. Swin Transformer attained the best results, while Linear Transformer~\cite{katharopoulos2020transformers} and Fastformer~\cite{wu2021fastformer} show similar results. Interestingly, when VCM is replaced with VEM, the performance difference for Swin Transformer and the other two differ substantially. Specifically, for Swin Transformer, the mIoU is 89.9 when equipped with VEM, which is a 0.4 performance drop and is a relatively lower drop compared to those of Linear Transformer and Fastformer. This could be due to the relative position bias that Swin Transformer provides, which the other two transformers lack. Furthermore, we suspect that the lower mIoU results could be explained by one of the following factors: simplified self-attention computation, local smoothness property of a correlation map, and consideration of spatial structure.  \vspace{-10pt}




\section*{Appendix C. Limitations}
An apparent limitation is that since our approach acts on correlation maps, we need to explicitly compute the global correlation maps and store them. This is indeed memory expensive, and increases with the spatial resolution of the correlation maps. Although we utilize a coarse-to-fine architecture, this does not make the training feasible when resolutions are high. Specifically, given a spatial resolution of feature maps at size 128128, the resultant size of correlation maps is at least 128, and counting the level dimensions as well as other pyramidal levels , it is difficult to train with a sufficient batch size even with NVIDIA GeForce RTX-3090 GPUs. This might limit the accessibility of this approach. We also visualize failure cases in Fig.~\ref{failure}. \vspace{-10pt}


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{latex/fig/failure_final.pdf}
\vspace{-10pt}\caption{\textbf{Failure cases.}   }
\label{failure}
\end{figure}

\clearpage





\section*{Appendix D. More Results}

\vspace{-10pt}
\subsubsection{Quantitative Results for Semantic Correspondence.}
As shown in Table~\ref{tab:spair}, we provide per-class quantitative results on  SPair-71k~\cite{min2019spair} in comparison to other semantic correspondence methods, including CNNGeo~\cite{rocco2017convolutional}, WeakAlign~\cite{rocco2018end}, NC-Net~\cite{Rocco18b}, HPF~\cite{min2019hyperpixel}, SFNet~\cite{lee2019sfnet}, DCC-Net~\cite{huang2019dynamic}, GSF~\cite{jeon2020guided}, SCOT~\cite{liu2020semantic}, DHPF~\cite{min2020learning}, CHM~\cite{min2021convolutional}, MMNet~\cite{zhao2021multi}, PMNC~\cite{Lee_2021_CVPR} and CATs~\cite{cho2021semantic}. \vspace{-10pt}

\begin{table*}[]
    \begin{center}
        \scalebox{0.7}{
        \begin{tabular}{l|cccccccccccccccccc|c}
        \toprule
         Methods & aero. & bike & bird & boat & bott. & bus & car & cat & chai. & cow & dog & hors. & mbik. & pers. & plan. & shee. & trai. & tv & all\\
        \midrule
        
       
        CNNGeo~\cite{rocco2017convolutional} &  23.4 & 16.7 & 40.2 & 14.3 & 36.4 & 27.7 & 26.0 & 32.7 & 12.7 & 27.4 & 22.8 & 13.7 & 20.9 & 21.0 & 17.5 & 10.2 & 30.8 & 34.1 & 20.6  \\
      
       WeakAlign~\cite{rocco2018end} &  22.2 & 17.6 & 41.9 & {15.1} & {38.1} & {27.4} & {27.2} & 31.8 & 12.8 & 26.8 & 22.6 & 14.2 & 20.0 & 22.2 & 17.9 & 10.4 & {32.2} & 35.1 & 20.9 \\
    
       NC-Net~\cite{Rocco18b} & 17.9 & 12.2 & 32.1 & 11.7 & 29.0 & 19.9 & 16.1 & 39.2 & 9.9 & 23.9 & 18.8 & 15.7 & 17.4 & 15.9 & 14.8 & 9.6 & 24.2 & 31.1 & 20.1   \\\-2.3ex]
        
        {SCOT}~\cite{liu2020semantic} & {34.9} & {20.7} & {63.8} & {21.1} & {43.5} & {27.3} & {21.3} & {63.1} & {20.0} & {42.9} & {42.5} & {31.1} & {29.8} & {35.0} & {27.7} & {24.4} & {48.4} & {40.8} & {35.6} \\
       
       {DHPF}~\cite{min2020learning} & {38.4} & {23.8} & {68.3} & {18.9} & {42.6} & {27.9} & {20.1} & {61.6} & {22.0} & {46.9} & {46.1} & {33.5} & {27.6} & {40.1} & {27.6} & {28.1} & {49.5} & {46.5} & {37.3} \\
        
        
        {CHM}~\cite{min2021convolutional} & {49.1} & {33.6} & {64.5} & {32.7} & {44.6} & {47.5} & {43.5} & {57.8} & {21.0} & {61.3} & {54.6} & {43.8} & {35.1} & {43.7} & {38.1} & {33.5} & {70.6} & {55.9} & {46.3} \\
        MMNet~\cite{zhao2021multi} &43.5&27.0&62.4&27.3&40.1&50.1&37.5&60.0&21.0&56.3&50.3&41.3&30.9&19.2&30.1&33.2&642&43.6&40.9  \\
        PMNC~\cite{Lee_2021_CVPR}  &\underline{54.1}  &35.9  &\underline{74.9}  &\underline{36.5} &42.1  &48.8  &40.0 &\textbf{72.6}  &21.1  &{67.6}  &\underline{58.1}  &50.5  &40.1  &\textbf{54.1}  &\underline{43.3}  &35.7  &{74.5}  &59.9  &\underline{50.4}  \\
        CATs~\cite{cho2021semantic}  & {52.0} & {34.7} & {72.2} & {34.3} &\underline{49.9} & \underline{57.5} & \underline{43.6} & {66.5} & \textbf{24.4} & {63.2} & {56.5} & \underline{52.0} & \underline{42.6} & {41.7} & {43.0} & {33.6} & {72.6} & {58.0} & {49.9} \\\midrule
        VAT (ours)  &{49.8}& \underline{36.8}& {70.1}& {33.5} &{46.1} &{46.0}   &{31.1} &\underline{69.9}  &{15.7}  &\underline{69.9}  &{57.2}  &{47.2}  &{38.5}  &{41.8}  &{43.0}  &\underline{35.5}  &\underline{75.0} &\underline{61.8} &{48.4}  \\
        VAT (ours)  &\textbf{58.8}& \textbf{40.0}& \textbf{75.3}& \textbf{40.1} &\textbf{52.1} &\textbf{59.7}   &\underline{44.2} &69.1  &\underline{23.3}  &\textbf{75.1}  &\textbf{61.9}  &\textbf{57.1}  &\underline{46.4}  &\underline{49.1}  &\textbf{51.8}  &\textbf{41.8}  &\textbf{80.9}&\textbf{70.1}  &\textbf{55.5}  \\
       \bottomrule
        \end{tabular}} 
    \caption{\label{tab:spair} \textbf{Per-class quantitative evaluation on SPair-71k~\cite{min2019spair} benchmark.}}\vspace{-10pt}
    
    \end{center}\vspace{-10pt}
\end{table*}
\begin{table}[t]
    \begin{center}
    \scalebox{1}{
    \begin{tabular}{cl|ccccc|ccccc}
            \toprule
            \multirow{2}{*}{\shortstack{Backbone\\network}} & \multirow{2}{*}{Methods} & \multicolumn{5}{c|}{1-shot} & \multicolumn{5}{c}{5-shot}  \\ 
            
            & &  &  &  &  &mean&  &  &  &  &mean  \\
            \midrule
             \multirow{3}{*}{ResNet50~\cite{he2016deep}} 
                & RePRI~\cite{boudiaf2021few}        &45.8 &53.7  &46.6   &50.0   &49.0&45.4  &46.9  &41.8  &41.0   &43.8\\    
                
                & HSNet~\cite{min2021hypercorrelation}     &53.9  &54.7  &53.3 &53.6 &53.9 &54.6&55.1  &54.0  &54.2  &54.5\\
            






                




              &VAT (ours) &55.1 &55.1  &53.8 &53.6 &54.4  &55.4 &55.3 &54.5 &53.9 &54.8\\
       
           
            
            \midrule
            
             \multirow{3}{*}{ResNet101~\cite{he2016deep}} 
                & RePRI~\cite{boudiaf2021few}        &47.6 &47.6  &41.9   &43.3   &45.1&46.4  &44.4  &38.4  &38.7   &42.0\\    
                
                & HSNet~\cite{min2021hypercorrelation}     & 53.9 & 54.4 &53.5 &53.9 &53.9 &54.3&54.7  &54.2  &54.2  &54.4\\
            






                




              &VAT (ours) &54.7 &54.6  &53.9 &55.5 &54.7  &55.0 &55.0 &54.5 &54.8 &54.8\\
          
           
            
            \bottomrule
    \end{tabular}
    }
    \end{center}\vspace{-10pt}
    \caption{\textbf{mBA comparison on PASCAL-5~\cite{shaban2017one}.} }\label{tab:pascalmba}\vspace{-20pt}
\end{table}


\begin{table}[t]
    \begin{center}
    \scalebox{1}{
    \begin{tabular}{clcccccccccccc}
                \toprule
                \multirow{2}{*}{\shortstack{Backbone\\feature}} & \multirow{2}{*}{Methods} & \multicolumn{5}{c}{1-shot} & \multicolumn{5}{c}{5-shot} \\ 
                
                & &  &  &  &  & mean &  &  &  &  & mean \\

                \midrule
                
                \multirow{3}{*}{ResNet50~\cite{he2016deep}} 
                & RePRI~\cite{boudiaf2021few}        &6.84 &6.16  &5.76   &6.46   &6.31&5.44  &4.45  &3.49  &3.47   &4.21\\    
                
                & HSNet~\cite{min2021hypercorrelation}     &53.1  &52.9  & 53.0&53.0 &53.0 &53.6&53.8  &54.1  &53.7  &53.8\\
            






                




              &VAT (ours) &54.1 &54.0  &54.5 &54.0 &54.2  &54.6 &54.8 &55.4 &54.7 &54.9\\
               
                
                \bottomrule
        \end{tabular}
        }
    \end{center}\vspace{-10pt}
        \caption{\textbf{mBA comparison on COCO-20~\cite{lin2014microsoft}.}}\label{tab:cocomba}\vspace{-10pt}
\end{table}


\subsubsection{More results for mBA comparison.}
In Table~\ref{tab:pascalmba} and Table~\ref{tab:cocomba}, we provide per fold quantitative results for mBA. Note that we obtained the mBA results for HSNet~\cite{min2021hypercorrelation} and RePRI~\cite{boudiaf2021few} using the pre-trained weights and code released by the authors. We omit the results for   CyCTR~\cite{zhang2021few} as the official code and weights by the authors are not publicly available. 

\subsubsection{Qualitative Results.}
As shown in Figure~\ref{PASCAL}, Figure~\ref{COCO}, Figure~\ref{FSS}, Figure~\ref{WILLOW} and Figure~\ref{SPAIR}, we provide qualitative results on all the benchmarks, which includes PASCAL-5~\cite{shaban2017one}, COCO-20~\cite{lin2014microsoft}, FSS-1000~\cite{li2020fss}, PF-PASCAL~\cite{ham2017proposal}, PF-WILLOW~\cite{ham2016proposal} and SPair-71k~\cite{min2019spair}.
\newpage



\clearpage



\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig/pascal_final.pdf}\vspace{-10pt}
\caption{\textbf{Qualitative results on PASCAL-5~\cite{shaban2017one}.}   }
\label{PASCAL}
\end{figure*}
\clearpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig/coco_final.pdf}\vspace{-10pt}
\caption{\textbf{Qualitative results on COCO-20~\cite{lin2014microsoft}.}   }
\label{COCO}
\end{figure*}
\clearpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig/fss_final.pdf}\vspace{-10pt}
\caption{\textbf{Qualitative results on FSS-1000~\cite{li2020fss}.}   }
\label{FSS}
\end{figure*}
\clearpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig/pf_final.pdf}\vspace{-10pt}
\caption{\textbf{Qualitative results on PF-PASCAL~\cite{ham2017proposal} (left) and PF-WILLOW~\cite{ham2016proposal} (right). }   }
\label{WILLOW}
\end{figure*}
\clearpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{latex/fig/spair_final.pdf}\vspace{-10pt}
\caption{\textbf{Qualitative results on SPair-71k~\cite{min2019spair}.}   }
\label{SPAIR}
\end{figure*}
\clearpage
\clearpage
