

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage[usenames, dvipsnames]{color}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\usepackage{listings}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\definecolor{orange}{RGB}{255, 125, 125}
\newcommand{\todo}[1]{{\color{orange}{TODO: #1}}}

\newcommand{\muzero}{\emph{MuZero}}
\newcommand{\reanalyse}{\emph{Reanalyse}}
\newcommand{\mzunplugged}{\emph{MuZero Unplugged}}



\usepackage[accepted]{icml2020}

\icmltitlerunning{Online and Offline Reinforcement Learning by Planning with a Learned Model}

\begin{document}

\twocolumn[
\icmltitle{Online and Offline Reinforcement Learning by Planning with a Learned Model}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Julian Schrittwieser}{equal,dm}
\icmlauthor{Thomas Hubert}{equal,dm}
\icmlauthor{Amol Mandhane}{dm}
\icmlauthor{Mohammadamin Barekatain}{dm}
\icmlauthor{Ioannis Antonoglou}{dm}
\icmlauthor{David Silver}{dm}
\end{icmlauthorlist}

\icmlaffiliation{dm}{DeepMind, London, UK}

\icmlcorrespondingauthor{Julian Schrittwieser}{swj@google.com}

\icmlkeywords{Machine Learning, ICML, RL, MCTS, Planning, Offline-RL}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results in both settings.
In this work, we describe the \reanalyse{} algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efficient learning for data budgets varying by several orders of magnitude. We further show that \reanalyse{} can also be used to learn entirely from demonstrations without any environment interactions, as in the case of offline Reinforcement Learning (offline RL). Combining \reanalyse{} with the \muzero{} algorithm, we introduce \mzunplugged{}, a single unified algorithm for any data budget, including offline RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or offline RL settings. \mzunplugged{} sets new state-of-the-art results in the RL Unplugged offline RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting.
\end{abstract}

\section{Introduction}

Offline reinforcement learning holds the promise of learning useful policies from many existing real-world datasets in a wide range of important problems such as robotics, healthcare or education \cite{levine2020offline}. Learning effectively from offline data is crucial for such tasks where interaction with the environment is costly or comes with safety concerns, but a large amount of logged and other offline data is often available.

A wide variety of effective reinforcement learning (RL) algorithms for the online case have been described in the literature, achieving impressive results in video games \cite{dqn}, robotic control \cite{akkaya2019solving} and many other problems. However, applying these online RL algorithms to offline datasets often remains challenging due to off-policy issues, with the best results in offline RL so far obtained by specialised offline algorithms \cite{kumar2020conservative,wang2020critic,agarwal2020optimistic}.

At the same time, model-based reinforcement learning (RL) has long focused on learning efficiently from little data, even going as far as learning completely within a model of the environment \cite{hafner:planet} - an approach ideally suited for offline RL.

So far, these developments have been relatively independent, with no unified algorithm that could achieve state-of-the art results in both the online and offline settings.

In this paper, we describe the \reanalyse{} algorithm, a simple yet effective technique for policy and value improvement at any data budget, including the fully offline case. A preliminary version of \reanalyse{} was briefly introduced in the context of MuZero \cite{muzero}, but limited to data efficiency improvements in the discrete action case. Here, we delve deeper into the algorithm and push its capabilities much further -- ultimately to the point where most or all of the data is reanalysed.

Starting with the possible uses of \reanalyse{}, we show how it can be used for data efficient learning and offline RL, leading to \mzunplugged{}. We demonstrate its effectiveness for the online case through results on Atari and for the offline case through results on the RL Unplugged benchmark for Atari and DM Control.

\section{Related Work}

Recent work by \cite{levine2020offline} provides a thorough review of offline RL literature and presents an excellent introduction to the subject.

A lot of recent work has focused on regularising the value or policy learning to counteract off-policy issues and learn only from high quality data. Critic-Regularized Regression (CRR) uses a critic to filter out bad actions and uses only good actions to train the policy \cite{wang2020critic}. Random Ensemble Mixture (REM) regularises q-value estimation by using random convex combinations of ensemble members during training, and the ensemble mean during evaluation \cite{agarwal2020optimistic}. Conservative Q-Learning (CQL) learns a conservative Q-function, used to lower bound the value of the current policy \cite{kumar2020conservative}. Pessimistic Offline Policy Optimization (POPO) also uses a pessimistic value function for policy learning \cite{he2021popo}.

Existing work has also demonstrated the promise of model-based RL for offline learning \cite{matsushima2020deploymentefficient,argenson2020modelbased}, but has often been restricted to tasks with low-dimensional action or state spaces, and has not been applied to visually more complex tasks such as Atari \cite{ALE}.

Model-Based Offline Reinforcement Learning (MOReL) implements a two-step procedure, first learning a pessimistic MDP from offline data using Gaussian dynamics models, then a policy within this learned MDP \cite{kidambi2020morel}. Results are presented for state-based control tasks.

Model-based Offline Policy Optimization (MOPO) penalises rewards by the uncertainty of the model dynamics to avoid distributional shift issues \cite{yu2020mopo}.

Offline Reinforcement Learning from Images with Latent Space Models (LOMPO) extends MOPO to image based tasks \cite{rafailov2020offline}. Results are reported on newly introduced datasets with image observations, which the authors aim to open-source in the near future.

All these approaches have in common that they primarily use the learned model for uncertainty estimation and to train a policy; they do not directly use the learned model for planning over action sequences.

In contrast, our method focuses on using the learned model directly for policy and value improvement through planning both offline (when learning from data) and online (when interacting with an environment). It requires no regularisation of the value or policy function either in the online or offline case, works well even in very high dimensional state spaces and is equally applicable to both discrete and continuous action spaces.

A combination of \mzunplugged{} with regularisation approaches such as introduced in the previous work discussed above \cite{kidambi2020morel,yu2020mopo,rafailov2020offline} is possible; we leave such investigations for future work.

\section{Reanalyse}

\reanalyse{} takes advantage of model-based value and policy improvement operators to generate new value and policy training targets for a given state (Algorithm \ref{alg:reanalyse}). In this work, we will use \muzero{}'s Monte Carlo Tree Search (MCTS) planning algorithm combined with its learned model of the environment dynamics as the improvement operator.\footnote{Other model-based algorithms can be used as improvement operators as well.}

As the learned model and its predictions are updated and improved throughout training, \reanalyse{} can be repeatedly applied to the same state to generate better and better training targets. The improved training targets in turn are used to improve the model and predictions, leading to a virtuous cycle of improvement.

\begin{algorithm}[h]
\begin{algorithmic}
   \FOR{step }
    \STATE 
    \STATE 
    \FOR{i }
      \STATE 
      \STATE 
      \STATE 
    \ENDFOR
    \STATE 
    \STATE 
  \ENDFOR

\end{algorithmic}
\caption[]{
\label{alg:reanalyse}
\textbf{The \reanalyse{} algorithm}.
}
\end{algorithm}

To run MCTS and compute new targets for a training point, the representation function of \muzero{} maps the input observations into an embedding. The search over possible future action sequences then takes place entirely in this embedding space, by rolling the dynamics forward and applying prediction functions at every step. These predictions output the key quantities required by planning: the policy, value function and reward. The resulting MCTS statistics at the root of the search tree - visit counts for the actions and value estimate averaged over the tree - are then used as new training targets. During reanalysis, no actions are selected -- instead the agent updates its model and prediction parameters based on the data it has already experienced.

Specifically, \muzero{} \reanalyse{} jointly adjusts its parameters  to repeatedly optimise the following loss at every time-step , applied to a model that is unrolled  steps into the future,


where , , and  are respectively the policy, value and reward prediction produced by the -step unrolled model. The respective targets for these predictions are drawn from the corresponding time-step  of the real trajectory:  is the improved policy generated by the search tree,  is an -step return, and  is the true reward.

The policy and value predictions are then updated towards the new training targets, in the same way they would be for targets computed based on environment interactions - through minimising losses ,  and . In other words, \reanalyse{} requires no changes on the part of the learner and can be implemented purely in terms of adapting the actors to generate improved targets based on stored data instead of environment interactions.


Since the actual MCTS procedure used to \reanalyse{} a state is the same as the one used to choose an action when interacting with an environment, it is straightforward to perform a mix of both. We refer to this ratio between targets computed from direct interactions with the environment, and targets computed by reanalysing existing data points as the \reanalyse{} fraction. A \reanalyse{} fraction of 0\% refers to training by only interacting with the environment, no \reanalyse{} of stored data, whereas a fraction of 100\% refers to the fully offline case with no environment interaction at all.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{generated/pacman_reanalyse_scaling}
\vspace*{-7mm}
\caption[]{
\label{fig:reanalyse-scaling}
\textbf{Final scores in Ms. Pac-Man for different \reanalyse{} fractions}. By scaling the \reanalyse{} fraction, \muzero{} can be trained at any desired data budget. All other parameters are held constant. Note the logarithmic x-axis: Linear improvements in score require exponentially more data, matching scaling laws such as described by \cite{kaplan2020scaling} for language models.
}
\end{figure}

Since \reanalyse{} only uses stored data points and the learned model to compute improved targets, it can be employed flexibly for many different purposes:

\begin{itemize}
\setlength\itemsep{0.1em}
\item \textbf{Data Efficiency}. The simplest use of \reanalyse{} is to improve data efficiency by repeatedly computing updated targets on previously collected data throughout training. By scaling the \reanalyse{} fraction as described in Section \ref{sec:data-efficiency}, learning can be optimised for any data budget. For this purpose, the data to be reanalysed is sampled from the  most recent environment interactions; in the limit this includes all interactions throughout training.
\item \textbf{Offline RL}. When increasing the \reanalyse{} fraction to 100\%, learning takes place entirely from stored offline data as described in Section \ref{sec:offline}, without any interaction with the environment. Offline data may be obtained from a variety of sources, such as other agents, logged data from a heuristic control system or human examples.
\item Learning from \textbf{Demonstrations}. \reanalyse{} can also be used to quickly bootstrap learning from demonstrations containing good or desirable behaviour that might otherwise be hard to discover - collected for instance from humans - while still interacting with the environment, learning from both sources of data at the same time. This is useful to skip past what might otherwise be hard exploration problems while still improving beyond the quality of the initial demonstration data.
\item \textbf{Exploitation} of good episodes. When using \reanalyse{} to improve data efficiency, \reanalyse{} is applied to the most recently collected data. If instead data is ordered by some other metric, such as episode reward, \reanalyse{} can be used to quickly learn from rare events, such as rewards observed in hard-exploration tasks. This variant is most useful in deterministic environments, as it could otherwise bias the value estimates in stochastic environments.
\end{itemize}

In this paper, we will focus on the data efficiency and offline RL cases. Remaining cases require no adjustments to the algorithm and only differ in the source of data to be reanalysed. Further combinations of the cases above are also possible, such as a mix of exploitation and data efficiency \reanalyse{} which we leave for future work.

The \reanalyse{} algorithm has some similarities to experience replay \cite{replay}. Whereas replay performs multiple gradient descent updates for the same data point and target, \reanalyse{} uses model-based improvement operators to generate multiple training targets for the same data point. \reanalyse{} and replay have independent effects and can be combined to further improve data efficiency of learning; in fact we do so for all experiments in this paper.

\section{Reanalyse for Data Efficiency}
\label{sec:data-efficiency}

\begin{table}[b]
\begin{center}
\begin{tabularx}{0.8\columnwidth}{r@{\hspace{5pt}}rrrr}
\toprule
\reanalyse{} & Median &     Mean & \# Frames \\
\midrule
50.0\% & 1331.7\% & 4094.4\% & 2000M \\
95.0\% & \input{generated/atari_muzero_sticky_res2_adam_median}\hspace{-0.3em} & 2856.2\%
 \hspace{-0.3em} & 200M \\
99.5\% & 126.6\% & 450.6\% & 20M \\
\bottomrule
\end{tabularx}
\end{center}

\caption{
\label{tab:reanalyse-scaling}
\textbf{\reanalyse{} scaling in Atari}. Mean and median human normalised scores over 57 Atari games, at different \reanalyse{} fractions. All other parameters are held constant. Varying the \reanalyse{} fraction alone is enough to learn efficiently at data budgets differing by orders of magnitude.
}
\end{table}


By adjusting the ratio between targets computed from interactions with the environment and from stored trajectories (\reanalyse{} fraction), \reanalyse{} can be used to train \muzero{} at any desired data budget, as shown in Figure \ref{fig:reanalyse-scaling} and Table \ref{tab:reanalyse-scaling}. For both figures, the total amount of computation for each training run (number of updates on the learner and number of searches on the actors) is held constant.

As training progresses, the policy produced by MCTS with the latest network weights will increasingly differ from the policy originally used to generate the trajectories that are being reanalysed. This can bias both the state distribution used for training as well as the computation of value targets along those trajectories when using n-step temporal-difference (TD).

The policy prediction  for a state  is always updated towards the MCTS statistics  for that same state. In this way, the policy can be learned completely independently from the trajectory; no off-policy issues can arise.

The reward prediction only depends on the state and the action that was taken from this state and is not affected by off-policy issues as such. However, if the state distribution is very biased - in the extreme an action may never be observed - the reward function will be unable to learn the correct reward prediction for these cases, limiting the maximum policy improvement step.

The situation for the value function depends on the choice of training target; when using an n-step TD return such as in Atari (), the value function will depend on the trajectory and off-policy issues can potentially arise. Whether this is an issue depends on how different the data distribution is from the policy that is being learned. Empirically, we observed that the gain from bootstrapping with the actually observed environment rewards seems to outweigh any harm from being off-policy. We speculate that the bias introduced by early bootstrapping may be larger than the bias introduced by off-policy targets, as also seen in prior work \cite{alphastar}.


\section{\mzunplugged{}: Offline RL with Reanalyse}
\label{sec:offline}

\begin{table}[t]
\begin{center}\begin{tabularx}{0.8\columnwidth}{@{}l@{\hspace{2pt}}X|rr@{}}
\toprule
 & Loss & Median & Mean \\
\midrule
a & BC & 53.3 \% & 48.5 \%\\
 & DQN & 86.2 \% & 89.5 \%\\
 & IQN & 100.8 \% & 96.1 \%\\
 & BCQ & 107.5 \% & 120.0 \%\\
 & REM & 107.9 \% & 113.5 \%\\
 & CRR (ours) & 155.6 \% & 271.2 \%\\
\midrule
b & \muzero{} BC & 54.0 \% & 46.9 \%\\
 & \mzunplugged{} & \textbf{265.3 \%} & \textbf{595.5 \%}\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-results}
\textbf{Overall results for the RL Unplugged Atari benchmark}. Mean and median normalised scores over the 46 Atari games from the RL Unplugged benchmark. a) Results for offline RL baseline algorithms. CRR results are for our own reimplementation, other results are from \cite{rl_unplugged}. b) Results using the \muzero{} network architecture.\\
Behaviour cloning (BC) with the \muzero{} network replicated the baseline BC results from a), confirming correct import of the dataset and evaluation settings. Critic Regularized Regression (CRR) \cite{wang2020critic} significantly improved performance of the policy. \mzunplugged{} training with \reanalyse{} loss and MCTS for action selection led to overall best performance.
}
\end{table}


\begin{figure*}
\includegraphics[width=\textwidth]{generated/rl_unplugged_improvements}
\vspace*{-7mm}
\caption[]{
\label{fig:rl-unplugged-atari-improvements}
\textbf{Atari performance improvement.} Improvement of performance with respect to the online DQN version that was used to generate the training data for the 46 Atari games from RL Unplugged, calculated as .  is random performance,  is the same performance as the training data, and larger than  represents an improvement. \mzunplugged{} shows very robust performance, reaching the same or higher score in 44 games, with a small decrease in only 2 games.
}
\end{figure*}


\begin{table}
\begin{center}
\begin{tabularx}{0.95\columnwidth}{l@{\hspace{-5pt}}rrrr}
\toprule
Game & QR-DQN & REM & CQL(H) & MZ \\

\midrule
asterix (1\%) & 166.3 & 386.5 & 592.4 & \textbf{27220.5}\\
breakout & 7.9 & 11.0 & 61.1 & \textbf{251.9}\\
pong & -13.8 & -6.9 & \textbf{19.3} & -16.2\\
qbert & 383.6 & 343.4 & \textbf{14012.0} & 6953.2\\
seaquest & 672.9 & 499.8 & 779.4 & \textbf{4964.0}\\
\midrule
asterix (10\%) & 189.2 & 75.1 & 156.3 & \textbf{40554.0}\\
breakout & 151.2 & 86.7 & 269.3 & \textbf{485.8}\\
pong & 15.1 & 8.9 & \textbf{18.5} & 15.6\\
qbert & 7091.3 & 8624.3 & 13855.6 & \textbf{16817.9}\\
seaquest & 2984.8 & 3936.6 & 3674.1 & \textbf{8556.3}\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:atari-low-data}
\textbf{Low-data Atari setting}. QR-DQN \cite{qr-dqn}, REM \cite{agarwal2020optimistic}, CQL(H) \cite{kumar2020conservative} and \mzunplugged{} results when trained on only 1\% (top, 2 million frames) or 10\% (bottom, 20 million frames) of Atari data. \mzunplugged{} performance improves consistently when trained on more data.
}
\end{table}



We obtain \mzunplugged{}, an offline version of \muzero{}, by adjusting the \reanalyse{} fraction to 100\% - learning without any environment interactions, purely from stored trajectories. In contrast to previous work, we perform no off-policy corrections or adjustments to the value and policy learning: the exact same algorithm applies to both the online and offline case.

We used the RL Unplugged \cite{rl_unplugged} benchmark dataset for all offline RL experiments in this paper. To demonstrate the generality of the approach, we report results for both discrete and continuous action spaces as well as state and pixel based data, specifically:
\begin{itemize}
\setlength\itemsep{0em}
\item \textbf{DM Control Suite}, 9 different tasks, number of frames varies by task (Table \ref{tab:rl-unplugged-mujoco}). Continuous action space with 1 to 21 dimensions, state observations.
\item \textbf{Atari}, 46 games with 200M frames each. Discrete action space, pixel observations, stochasticity through sticky actions \cite{Marlos2017Atari}.
\end{itemize}

\mzunplugged{} was highly effective in either setting, outperforming baseline algorithms in Atari (Table \ref{tab:rl-unplugged-results}) as well as the DM Control Suite (Table \ref{tab:rl-unplugged-mujoco}). We performed no tuning of hyperparameters for these experiments, instead using the same hyperparameter values as for the online RL case \cite{muzero, muzero_sampled}.

To add another strong baseline for the Atari benchmark, we also implemented Critic Regularized Regression (CRR), a recent offline RL algorithm \cite{wang2020critic}. For the critic value required by CRR we used the value head of \muzero{} model, trained by 5-step TD with respect to a target network, as in previous work \cite{muzero} and the same as used for \mzunplugged{}. Using CRR to train the policy head led to improved results in Atari (Table \ref{tab:rl-unplugged-results}a, CRR), matching results reported for continuous action tasks, but did not reach the same performance as \mzunplugged{}.

Performance of \mzunplugged{} was robust across the whole range of 46 Atari games in the RL Unplugged benchmark, reaching the same or better performance as the DQN policy used to generate the data in 44 games, and slightly worse performance in only 2 games (Figure \ref{fig:rl-unplugged-atari-improvements}). Improvements in performance with respect to the training data were considerable, exceeding a 20 times increase in score in several games.

\begin{table}[t]
\begin{center}\begin{tabularx}{\columnwidth}{l|rrrcc}
\toprule
Loss &  \multicolumn{3}{c}{supervised} & CRR & \reanalyse{} \\
Unroll & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1}  & \multicolumn{1}{c}{5} & 5 & 5 \\
\midrule
policy & 60.6 & 61.4 & 54.0 & 155.6 & 203.2\\
value & 92.2 & 105.0 & 159.2 & 153.0 & 239.9\\
MCTS &  \multicolumn{1}{c}{-}  & 137.3 & 169.7 & 172.5 & 265.3\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-ablations}
\textbf{Median score in RL Unplugged Atari: ablations of action selection and training loss}. Median normalized scores over the 46 Atari games from RL Unplugged. \\
Rows of the table correspond to different action selection methods: sampling according to the policy probabilities, selecting the action with the highest value or selecting according to MCTS visit counts.\\
Columns correspond to different number of unroll steps of the \muzero{} learned model and different losses. The leftmost three columns use the action from the training data as a supervised policy target, the rightmost two columns use the CRR and the \reanalyse{} loss respectively. For the case of 0 unroll steps, an action-value head is used to predict action values, instead of the state-value predicted by the normal model. All columns use a 5-step TD bootstrap towards a target network as the value target.\\
For all action selection methods, \reanalyse{} loss led to the best performance; for all losses, MCTS action selection also led to the best performance. Overall, the combination of MCTS action selection and \reanalyse{} loss - the \mzunplugged{} algorithm - led to the best results.
}
\end{table}


To examine the performance of \mzunplugged{} in detail and ascertain the contributions of action selection methods and training losses, we also performed a set of ablations (Tables \ref{tab:rl-unplugged-ablations} and \ref{tab:rl-unplugged-ablations-mean}) based on the Atari dataset. We chose Atari because the large number of diverse levels enables robust performance estimates and its discrete action space allows us to cleanly disentangle the contributions of value and policy predictions as well as planning with MCTS. In contrast, for continuous action spaces such as in the DM Control suite, the contributions of policy and value are entangled, as the value function can only evaluate actions already sampled from the policy.

For our ablations, we considered three possible action selection methods: Sampling actions according to the policy network probabilities, selecting the action with the maximum value, or selecting actions based on the MCTS visit count distribution (rows of Table \ref{tab:rl-unplugged-ablations}). We also considered different losses and network architectures: the leftmost three columns use variants of the \muzero{} learned model with 0 (no model at all), 1 or 5 steps of model unroll, all trained using the supervised behaviour cloning policy target and a 5-step TD value target based on a target network for the. The next column used CRR to train the policy. The last column used the the MCTS visit count distribution from the \reanalyse{} loss. These ablations allow us to separately measure the contribution of MCTS at training time (rightmost column) and evaluation time (bottom row), with the combination of MCTS at evaluation time and \reanalyse{} loss (bottom right cell) corresponding to \mzunplugged{}.

As expected, the policy prediction was insensitive to the choice of model depth, but benefited from an improved training target: the CRR loss significantly improved results. Best results were obtained when using the rich MCTS visit count distribution from the \reanalyse{} loss as a training target (top row of Table \ref{tab:rl-unplugged-ablations}).

When selecting actions according to the value estimate for each action (middle row of Table \ref{tab:rl-unplugged-ablations}), the depth of the learned model was surprisingly important. The difference between estimating q-values (0-step model) and state-values (1-step model) was small, with both attaining results similar to the IQN baseline (Table \ref{tab:rl-unplugged-ablations}a) - expected, since all of these results use a distributional value prediction. However, learning a full 5-step model led to a big improvement even though only 1-step value predictions were used for evaluation. We speculate that learning a full 5-step model is beneficial because it regularises the network representation and acts as a useful auxiliary loss.\footnote{These results suggest that an n-step model can also be used as an auxiliary loss to improve the performance of otherwise model-free algorithms. For value-based algorithms without an explicit policy prediction, the distribution used for action selection can be used as the target for the policy loss of the model.}

\begin{table*}[h]
\begin{center}\begin{tabularx}{0.9\textwidth}{l@{\hspace{0pt}}rr|rrrr|rr}
  \toprule
   & & & \multicolumn{4}{c|}{Baselines} & \multicolumn{2}{c}{\muzero{}} \\
  Task & \# dims & \# episodes & BC & D4PG & BRAC & RABM & BC & \emph{Unplugged} \\
\midrule
cartpole.swingup & 1 & 40  & 386.0 & 856.0 & \textbf{869.0} & 798.0 & 143.7 & 343.3 \\
finger.turn\_hard & 2 & 500  & 238.0 & \textbf{714.0} & 227.0 & 433.0 & 308.8 & 405.0 \\
fish.swim & 5 & 200  & 444.0 & 180.0 & 222.0 & 504.0 & 542.8 & \textbf{585.4} \\
manipulator.insert\_ball & 5 & 1500  & 385.0 & 154.0 & 55.6 & 409.0 & 412.7 & \textbf{557.0} \\
manipulator.insert\_peg & 5 & 1500  & 279.0 & 50.4 & 49.5 & 290.0 & 309.9 & \textbf{432.7} \\
walker.stand & 6 & 200  & 386.0 & \textbf{930.0} & 829.0 & 689.0 & 444.4 & 759.8 \\
walker.walk & 6 & 200  & 380.0 & 549.0 & 786.0 & 651.0 & 496.3 & \textbf{901.5} \\
cheetah.run & 6 & 300  & 408.0 & 308.0 & 539.0 & 304.0 & 592.9 & \textbf{798.9} \\
humanoid.run & 21 & 3000  & 382.0 & 1.7 & 9.6 & 303.0 & 408.5 & \textbf{633.4} \\
\midrule
mean & &  & 365.3 & 415.9 & 398.5 & 486.8 & 406.7 & \textbf{601.9}\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-mujoco}
\textbf{Results for DM Control benchmark from RL Unplugged}. Mean final score on 9 DM Control tasks, as well as mean score across all tasks. First three columns indicate task, action dimensonality and dataset size, subsequent four columns reproduce baseline results from \cite{rl_unplugged}. Final columns show performance of Behaviour Cloning (BC) with the \muzero{} network and results for \mzunplugged{}. As the data sets for the DM Control tasks are very small and vary a hundredfold between tasks, to keep the number of model parameters per datapoint constant and prevent memorisation, we scaled the neural network according to .
}
\end{table*}


Keeping the 5-step model but changing the loss for the policy head, we observed that CRR had no effect on the quality of the value prediction for action selection, while the richer MCTS visit count distribution from the \reanalyse{} loss led to another big improvement. Even though the policy head is not used when selecting actions according to the maximum 1-step value, we hypothesise that the auxiliary loss has a strong regularising effect and further improved the internal representation of the model. This matches the results of \cite{Silver17AG0} that training a single combined network to estimate both policy and value led to improved value prediction accuracy.

Finally, using MCTS to select actions at evaluation time (bottom row of Table \ref{tab:rl-unplugged-ablations}) improved results no matter which loss was used at training time, with best results obtained when using MCTS for both training and evaluation - the full \mzunplugged{} algorithm.

We also verified that our training setup correctly interpreted the offline data\footnote{We spent a surprisingly large amount of time tracking down action space mismatches, data discrepancies and compression artefacts. We recommend that any offline RL paper should first reproduce baseline results for the chosen dataset before attempting modifications and improvements to the algorithms.} and reproduced the baseline performance when using the same loss: Using the actions played in the training data as a supervised policy target to train a policy head using cross-entropy loss and sampling from it for evaluation (Table \ref{tab:rl-unplugged-results}, policy BC a and b) reproduced the behaviour cloning (BC) baseline results.

\section{Offline RL and Continuous Action Spaces}

\begin{table}[t]
\begin{center}\begin{tabularx}{\columnwidth}{l|r|rr}
  \toprule
   &  & \multicolumn{2}{c}{\muzero{}} \\
  Task & CRR & BC & \emph{Unplugged} \\
\midrule
cartpole.swingup & \textbf{664.0} & 501.8 & 594.3 \\
finger.turn\_hard & 714.0 & 333.8 & \textbf{759.0} \\
fish.swim & 517.0 & 556.8 & \textbf{681.6} \\
manipulator.insert\_ball & 625.0 & 465.6 & \textbf{659.2} \\
manipulator.insert\_peg & 387.0 & 325.9 & \textbf{556.0} \\
walker.stand & 797.0 & 473.3 & \textbf{887.2} \\
walker.walk & 901.0 & 637.9 & \textbf{949.5} \\
cheetah.run & 577.0 & 765.3 & \textbf{869.9} \\
humanoid.run & 586.0 & 416.5 & \textbf{643.1} \\
\midrule
mean & 640.9 & 497.4 & \textbf{733.3}\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-mujoco-crr}
\textbf{Comparison of \mzunplugged{} and CRR}. Results for CRR \cite{wang2020critic} were reported by selecting the checkpoint with the highest mean reward from each training run. Since this does not follow the offline policy selection guidelines from RL Unplugged and is therefore not directly comparable to the baseline results, we compared to it separately. The same highest mean reward evaluation scheme as used in CRR was used for \mzunplugged{} results in this table as well. All other tables report results at the end of training.
}
\end{table}


An important motivation for offline RL is the application to real-world systems such as robotics, which often have continuous and high-dimensional action spaces. To investigate the applicability of \mzunplugged{} to this setting, we used the DM Control Suite dataset from the RL Unplugged dataset. DM Control is a collection of physics based benchmark tasks \cite{tassa2018deepmind} with a variety of robotic bodies of different action and state dimensionalities (Table \ref{tab:rl-unplugged-mujoco}).

In order to use planning and \reanalyse{} with continuous action spaces, we used the sample based search extension of \muzero{} introduced by \cite{muzero_sampled}. This extension uses a policy head to produce a set of candidate actions to search over, where the MCTS considers only the sampled actions instead of fully enumerating the action space. Finally, the policy is updated towards the search distribution only at the sampled actions.

When applying \reanalyse{} for data efficiency improvements to data generated by the agent itself, no modifications are required to use sample based search and \reanalyse{} together. In offline RL or when reanalysing demonstrations from a source other than the agent itself, the policy that generated the actions making up the dataset is often quite different from the one learned by \mzunplugged{}, and unlikely to sample the same actions, at least at the beginning of training. Since in this case the MCTS (and by extension, \reanalyse{}) can only consider actions that have been sampled from the policy, it would be unlikely to learn about the actions contained in the dataset, and thus unable to sample them from the policy in the future. This effect is most pronounced in very high dimensional action spaces.

To prevent this issue, we explicitly included the action from the trajectory being reanalysed in the sample of actions searched over at the root of the MCTS tree. This serves the same purpose as the Dirichlet exploration noise used in standard \muzero{} - encouraging the MCTS to explore actions it would not otherwise consider. For the prior of the injected action we therefore use the same value as for the Dirichlet probability mass, 25\%, though the algorithm is not sensitive to the exact value. This step is redundant for discrete action spaces (such as in Atari) where the policy already always produces a prior for all possible actions.

We compared the performance of \mzunplugged{} to offline RL algorithms from the literature such as D4PG \cite{d4pg}, BRAC \cite{wu2019brac} and RABM \cite{siegel2020keep,rl_unplugged} (Table \ref{tab:rl-unplugged-mujoco}), as well as the recent Critic Regularized Regression (CRR) \cite{wang2020critic} algorithm (Table \ref{tab:rl-unplugged-mujoco-crr}, shown separately as CRR was evaluated by selecting the maximum performance throughout training and results are thus not comparable to the other baselines).

We first measured the performance of Behaviour Cloning (BC) when implemented using the \muzero{} network to ensure we used the offline dataset correctly and that it matches the evaluation environment. Overall performance indeed approximately matches the BC baseline (Table \ref{tab:rl-unplugged-mujoco}).

\mzunplugged{} outperformed baseline algorithms both in individual tasks and for the mean return\footnote{Using the mean is appropriate in DM Control as the return for all tasks is in , with the return for the optimal policy close to . Therefore no outlier can dominate the mean; this is unlike the situation in Atari where scores of wildly varying magnitude require usage of the median.} averaged across all tasks. It did best in difficult high-dimensional tasks such as humanoid.run or the manipulator tasks, classified as "hard" by \cite{wang2020critic}, compared to "easy" for the other tasks. Performance in the simplest tasks, especially cartpole, was somewhat lower - primarily due to the very small datasets\footnote{The amount of information contained in the dataset is the product of the number of episodes and the size of the state and action space.} leading to overfitting of the learned model and value function throughout training: in cartpole, performance of the best checkpoint (Figure \ref{tab:rl-unplugged-mujoco-crr}) was much better than performance at the end of training (Figure \ref{tab:rl-unplugged-mujoco}). Additional regularisation techniques such as dropout \cite{hinton2012dropout} could be employed to prevent this. We leave this for future work since we are primarily interested in performance on complex tasks that we consider most representative of real-world problems.

\section{MuZero Implementation}

\begin{table}[t]
\begin{tabularx}{\columnwidth}{l c c c}
\toprule
Algorithm &   Median &     Mean & Frames \\
\midrule
IMPALA\textsuperscript{1}  & 191.8\% & 957.6\% & 200M \\
Rainbow\textsuperscript{2} &  231.1\% & -- & 200M \\
UNREAL\textsuperscript{3} \textsuperscript{a}  & 250.0\%\textsuperscript{a} & 880\%\textsuperscript{a} & 250M \\
LASER\textsuperscript{4} & 431.0\% & -- & 200M \\
\muzero{}\textsuperscript{5} & \input{generated/atari_muzero_median} & 2183.6\%
  & 200M \\
\midrule
\muzero{} sticky & \input{generated/atari_muzero_sticky_median} & 2188.4\%
  & 200M \\
\muzero{} Res2 Adam & \textbf{\input{generated/atari_muzero_sticky_res2_adam_median}} & \textbf{2856.2\%
 } & 200M \\
\bottomrule
\end{tabularx}

\caption{
\label{tab:atari-comparison}
\textbf{\muzero{} improvements in Atari}. Mean and median human normalized scores over 57 Atari games, best results are highlighted in \textbf{bold}. For per game scores, see Table \ref{tab:atari-per-level}. Top of the table shows results for previously published work, bottom row show results for our experiments. Using sticky actions does not significantly hurt performance, and switching to ResNet v2 style pre-activation residual blocks with Layer Normalisation and training with Adam significantly improves performance.\\
\textsuperscript{1} \cite{impala},  \textsuperscript{2} \cite{rainbow}, \textsuperscript{3} \cite{unreal}, \textsuperscript{4} \cite{laser}, \textsuperscript{5} \cite{muzero}\\
\textsuperscript{a}Hyper-parameters were tuned per game.
}
\end{table}

All experiments in this paper are based on a JAX \cite{jax2018github} implementation of  \muzero{}, closely following the description in \cite{muzero}. For experiments in environments with continuous actions, we used the extension to  \muzero{} proposed in \cite{muzero_sampled}. To facilitate a more direct comparison with other algorithms, we used the same Gaussian policy representation as used for data generation and baselines in RL Unplugged \cite{rl_unplugged}.

The original \muzero{} did not use \emph{sticky actions} \cite{Marlos2017Atari} (a 25\% chance that the selected action is ignored and that instead the previous action is repeated) for Atari experiments. To make comparisons against other algorithms easier and match the data from RL Unplugged, our implementation did use sticky actions. As shown in Table \ref{tab:atari-comparison} (\muzero{} vs \muzero{} sticky), this did not affect performance despite introducing slight stochasticity into the environment.

Additionally, we updated the network architecture of the \muzero{} learned model to use ResNet v2 style pre-activation residual blocks \cite{resv2} coupled with Layer Normalisation \cite{ba2016layer} and use the Adam optimiser \cite{adam} with decoupled weight decay \cite{adam_weight_decay} for training. This significantly improved both mean and median normalized performance, setting a new state of the art for Atari at the 200 million frame budget - see Table \ref{tab:atari-comparison} for details.

\section{Conclusions}

In this paper we have investigated the \reanalyse{} algorithm and its applications to both data efficient online RL at any data budget and completely offline RL. We combined \reanalyse{} with \muzero{} to obtain \mzunplugged{}, a unified model-based RL algorithm that achieved a new state of the art in both online and offline reinforcement learning. Specifically, \mzunplugged{} outperformed prior baselines in the Atari Learning Environment both using a standard online budget of 200 million frames and other data budgets spanning multiple orders of magnitude. Furthermore, \mzunplugged{} also outperformed offline baselines in the RL Unplugged benchmark for Atari and continuous control. Unlike previous approaches, \mzunplugged{} uses the same algorithm for multiple regimes without any special treatment for off-policy or offline data.

This work represents a further step towards the vision of a single algorithm that can address a wide range of reinforcement learning applications, extending the capabilities of model-based planning algorithms to encompass new dimensions such as online and offline learning, using discrete and continuous action spaces, across pixel and state-based observation spaces, in addition to the wide array of challenging planning tasks addressed by prior work \cite{Silver18AZ}.

\section*{Acknowledgements}
We would like to thank Caglar Gulcehre for providing very detailed feedback and helpful suggestions to improve the paper.

\bibliography{main}
\bibliographystyle{icml2020}

\clearpage

\appendix

\begin{table}[t]
\begin{tabularx}{\columnwidth}{r@{\hspace{5pt}}rrrr}
\toprule
\reanalyse{} & TD steps &   Median &     Mean & \# Frames \\
\midrule
99.5\% & 5 & 126.6\% & 450.6\% & 20M \\
99.5\% & 0 & 115.3\% & 385.8\% & 20M \\
\bottomrule
\end{tabularx}

\caption{
\label{tab:atari-td}
\textbf{Comparison of value targets in Atari}. Mean and median human normalized scores over 57 Atari games, comparing a 5-step TD update towards a target network compared with direct regression against the search value for a state ("0-step TD"). All other parameters are held constant. Even in the 20M frame setting, where learning is almost entirely off-policy from reanalysed data and a trajectory independent value target might be expected to give better results, bootstrapping along the trajectory (TD 5) was better.
}
\end{table}

\section{Network Architecture}

For all experiments in this work we used a network architecture based on the one introduced by \muzero{} \cite{muzero}, but updated to use use ResNet v2 style pre-activation residual blocks \cite{resv2} coupled with Layer Normalisation \cite{ba2016layer}.

Both the representation function and the dynamics function were implemented by a ResNet with 10 blocks, each block containing 2 layers. For image inputs each layer was convolutional with a kernel size of 3x3 and 256 planes; for state based inputs each layer was fully-connected with a hidden size of 512.

To implement the network, we used the modules provided by the Haiku neural network library \cite{haiku2020github}.

\section{Policy Representation}

For domains with discrete action spaces, i.e. Atari, we used the same categorical policy representation as \muzero{}, trained by minimising the KL-divergence.

In continuous action spaces, we used the Gaussian policy representation used for the data generation policies in RL Unplugged. We did not observe any benefit from using a Gaussian mixture, so instead in all our experiments we used a single Gaussian with diagonal covariance. We trained the Gaussian policy by maximising the log-likelihood of the training target: for behaviour cloning the log-likelihood of the action from the trajectory; for \reanalyse{} the log-likelihood of each searched action sample, weighted by its normalized visit count.

\section{Value Learning}

We follow the approach described in previous work for value learning as well.

In Atari, we followed the \muzero{} training and use Temporal Difference (TD) learning with a TD step size of 5 towards a target network that is updated every 100 training steps by copying the weights of the network being trained \cite{muzero}.

For DM Control, we followed \cite{muzero_sampled} and directly regressed the value prediction for a state against the MCTS value estimate for that state. This allows the value to be learned independently from the trajectory and is helpful to prevent overfitting, useful when only very little training data is available. When more training data is available, TD 5 training with a target network often led to better results (Table \ref{tab:atari-td}).

\begin{figure*}
\includegraphics[width=\textwidth]{generated/rl_unplugged_control_results}
\vspace*{-7mm}
\caption[]{
\label{fig:rl-unplugged-control-plots}
\textbf{Performance throughout training in RL Unplugged Control Suite.} The x-axis shows thousands of training batches, the y-axis mean reward. Performance of \mzunplugged{} was better than Behaviour Cloning (BC) throughout training. For some tasks with a small dataset such as cartpole, walker or finger.turn\_hard, \mzunplugged{} performance peaked at the beginning of training and subsequently declined due to overfitting.
}
\end{figure*}

\begin{table}[t]
\begin{center}\begin{tabularx}{\columnwidth}{l|rrrcc}
\toprule
Loss &  \multicolumn{3}{c}{supervised} & CRR & \reanalyse{} \\
Unroll & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1}  & \multicolumn{1}{c}{5} & 5 & 5 \\
\midrule
policy & 50.7 & 49.6 & 46.9 & 271.2 & 433.0\\
value & 143.2 & 151.2 & 359.4 & 346.7 & 549.7\\
MCTS &  \multicolumn{1}{c}{-}  & 248.4 & 394.8 & 408.3 & 595.5\\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-ablations-mean}
\textbf{Mean score in RL Unplugged Atari: ablations of action selection and training loss}. As Table \ref{tab:rl-unplugged-ablations}, but showing mean normalized score instead of median normalized score.
}
\end{table}


\section{Optimization}

All experiments used the Adam optimiser \cite{adam} with decoupled weight decay \cite{adam_weight_decay} for training. We used a weight decay scale of  and an initial learning rate of , decayed to 0 over 1 million training batches using a cosine schedule:



where  and .

The batch size was  for all experiments.

\section{Evaluation}

Unless otherwise noted, all results were obtained by evaluating the final network checkpoint, at the end of training for 1 million mini-batches. Results are reported as the mean for 300 evaluation episodes.



In domains with continuous action spaces, we followed previous work \cite{rl_unplugged} in reducing the scale of the Gaussian policy close to 0 to obtain the performance of the policy mode. However, since setting the scale to 0 would not allow us to sample a set of different actions for MCTS to consider, we instead use a softer approach: .

\section{Other Hyperparameters}

Our hyperparameters follow previous work \cite{muzero, muzero_sampled}, but we reproduce them here for convenience.

We used a discount of  for Atari and  for DM Control experiments.

For replay, we kept a buffer of the most recent  subsequences in Atari and  in DM Control, splitting episodes into subsequences of length up to . Samples were drawn from the replay buffer according to prioritised replay \cite{Schaul2016}, with priority , where ,  is the search value and  the observed n-step return. To correct for sampling bias introduced by the prioritised sampling, we scaled the loss using the importance sampling ratio . In all our experiments, we set .


\begin{table}[h]
\begin{center}\begin{tabularx}{\columnwidth}{lr|rr}
\toprule
Task & \# dims & no inject & inject \\
\midrule
fish.swim & 5  & 79.7 & 585.4 \\
manipulator.insert\_ball & 5  & 21.2 & 557.0 \\
manipulator.insert\_peg & 5  & 90.1 & 432.7 \\
walker.stand & 6  & 735.6 & 759.8 \\
cheetah.run & 6  & 101.3 & 798.9 \\
humanoid.run & 21  & 3.3 & 633.4 \\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-mujoco-inject}
\textbf{Impact of injecting the trajectory action when reanalysing}. When reanalysing continuous action offline data or demonstrations from other agents, the learned policy is unlikely to sample the same actions as occur in the data, preventing the MCTS from considering those actions. This effect is especially pronounced in high dimensional tasks. Injecting the trajectory actions as one of the actions for MCTS to consider avoids this issue.
}
\end{table}


\begin{table*}[h]

  \begin{center}\begin{tabularx}{0.65\textwidth}{l@{\hspace{0pt}}rr|rrrrr}
  \toprule
   & & & \multicolumn{5}{c}{\# hidden size} \\
  Task & \# dims & \# episodes & 32 & 64 & 128 & 256 & 512 \\
\midrule
cartpole.swingup & 1 & 40  & 605.0 & 343.3 & 293.2 & 302.6 & 259.3 \\
fish.swim & 5 & 200  & 511.9 & 566.0 & 579.1 & 614.7 & 458.2 \\
walker.stand & 6 & 200  & 820.2 & 889.4 & 868.4 & 782.6 & 676.1 \\
walker.walk & 6 & 200  & 922.8 & 954.0 & 920.6 & 919.9 & 904.5 \\
\bottomrule
\end{tabularx}
\end{center}
 \caption{
\label{tab:rl-unplugged-mujoco-network-size}
\textbf{Varying network size in DM Control benchmark from RL Unplugged}. In tasks with very small amounts of training data, networks with a large number of parameters overfit easily.
}
\end{table*}


\onecolumn

\begin{table}
\scriptsize

\begin{center}\begin{tabularx}{\textwidth}{X| r r r r r r r r}
\toprule
\multirow{2}{*}{Game}  & \multirow{2}{*}{Random}  & \multirow{2}{*}{Human}  & \multicolumn{2}{c}{\muzero{} no-op starts} & \multicolumn{2}{c}{\muzero{} sticky actions} & \multicolumn{2}{c}{\muzero{} Res2 Adam}\\
 & & & mean & normalized & mean & normalized & mean & normalized\\
\midrule
alien &227.75 & 7,127.80 & \textbf{84,300.22} & 1,218.4 \% & 56,834.58 & 820.4 \% & 70,192.35 & 1,014.0 \%\\
amidar &5.77 & 1,719.53 & \textbf{13,011.88} & 758.9 \% & 1,516.53 & 88.2 \% & 1,197.38 & 69.5 \%\\
assault &222.39 & 742.00 & 38,745.37 & 7,413.8 \% & \textbf{42,742.00} & 8,183.0 \% & 33,292.22 & 6,364.4 \%\\
asterix &210.00 & 8,503.33 & 860,812.50 & 10,377.0 \% & \textbf{879,375.00} & 10,600.9 \% & 862,406.65 & 10,396.3 \%\\
asteroids &719.10 & 47,388.67 & 265,336.41 & 567.0 \% & 374,146.38 & 800.2 \% & \textbf{476,412.00} & 1,019.3 \%\\
atlantis &12,850.00 & 29,028.13 & 1,055,658.62 & 6,445.8 \% & \textbf{1,353,616.62} & 8,287.5 \% & 1,137,475.12 & 6,951.5 \%\\
bank heist &14.20 & 753.13 & 1,429.19 & 191.5 \% & 1,077.31 & 143.9 \% & \textbf{27,219.80} & 3,681.8 \%\\
battle zone &2,360.00 & 37,187.50 & 148,691.02 & 420.2 \% & 167,411.77 & 473.9 \% & \textbf{178,716.90} & 506.4 \%\\
beam rider &363.88 & 16,926.53 & 125,085.20 & 753.0 \% & 201,154.00 & 1,212.3 \% & \textbf{333,077.44} & 2,008.8 \%\\
berzerk &123.65 & 2,630.42 & \textbf{5,821.18} & 227.3 \% & 1,698.21 & 62.8 \% & 2,705.82 & 103.0 \%\\
bowling &23.11 & \textbf{160.73} & 138.33 & 83.7 \% & 133.23 & 80.0 \% & 131.65 & 78.9 \%\\
boxing &0.05 & 12.06 & 99.99 & 832.1 \% & 99.96 & 831.9 \% & \textbf{100.00} & 832.2 \%\\
breakout &1.72 & 30.47 & \textbf{851.92} & 2,957.2 \% & 798.75 & 2,772.3 \% & 758.04 & 2,630.7 \%\\
centipede &2,090.87 & 12,017.04 & 660,348.19 & 6,631.5 \% & 774,420.88 & 7,780.7 \% & \textbf{874,301.64} & 8,787.0 \%\\
chopper command &811.00 & 7,387.80 & \textbf{961,926.94} & 14,613.7 \% & 8,945.45 & 123.7 \% & 5,989.55 & 78.7 \%\\
crazy climber &10,780.50 & 35,829.41 & \textbf{196,566.92} & 741.7 \% & 184,394.12 & 693.1 \% & 158,541.58 & 589.9 \%\\
defender &2,874.50 & 18,688.89 & 519,002.88 & 3,263.7 \% & 554,491.69 & 3,488.1 \% & \textbf{557,200.75} & 3,505.2 \%\\
demon attack &152.07 & 1,971.00 & 141,508.56 & 7,771.4 \% & 142,509.17 & 7,826.4 \% & \textbf{143,838.04} & 7,899.5 \%\\
double dunk &-18.55 & -16.40 & \textbf{23.92} & 1,975.3 \% & 23.46 & 1,954.0 \% & 23.91 & 1,974.9 \%\\
enduro &0.00 & 860.53 & 0.00 & 0.0 \% & \textbf{2,369.00} & 275.3 \% & 2,365.81 & 274.9 \%\\
fishing derby &-91.71 & -38.80 & 62.54 & 291.5 \% & 57.70 & 282.4 \% & \textbf{73.94} & 313.1 \%\\
freeway &0.01 & 29.60 & 30.57 & 103.3 \% & 0.00 & -0.0 \% & \textbf{33.87} & 114.4 \%\\
frostbite &65.20 & 4,334.67 & 30,225.09 & 706.4 \% & 17,087.18 & 398.7 \% & \textbf{374,769.76} & 8,776.4 \%\\
gopher &257.60 & 2,412.50 & 78,132.26 & 3,613.8 \% & 122,025.00 & 5,650.7 \% & \textbf{122,882.50} & 5,690.5 \%\\
gravitar &173.00 & 3,351.43 & \textbf{12,709.44} & 394.4 \% & 10,301.37 & 318.7 \% & 8,006.93 & 246.5 \%\\
hero &1,026.97 & 30,826.38 & 36,592.23 & 119.3 \% & 36,062.90 & 117.6 \% & \textbf{37,234.31} & 121.5 \%\\
ice hockey &-11.15 & 0.88 & 14.62 & 214.2 \% & 26.26 & 311.0 \% & \textbf{41.66} & 439.0 \%\\
jamesbond &29.00 & 302.80 & \textbf{33,799.58} & 12,334.0 \% & 14,871.88 & 5,421.1 \% & 28,626.23 & 10,444.6 \%\\
kangaroo &52.00 & 3,035.00 & \textbf{14,556.96} & 486.3 \% & 14,380.00 & 480.3 \% & 13,838.00 & 462.2 \%\\
krull &1,598.05 & 2,665.53 & 12,537.17 & 1,024.8 \% & 11,476.19 & 925.4 \% & \textbf{72,570.50} & 6,648.6 \%\\
kung fu master &258.50 & 22,736.25 & \textbf{164,599.47} & 731.1 \% & 148,935.72 & 661.4 \% & 116,726.96 & 518.1 \%\\
montezuma revenge &0.00 & \textbf{4,753.33} & 0.00 & 0.0 \% & 0.00 & 0.0 \% & 2,500.00 & 52.6 \%\\
ms pacman &307.30 & 6,951.60 & 59,421.84 & 889.7 \% & 51,310.00 & 767.6 \% & \textbf{70,659.76} & 1,058.8 \%\\
name this game &2,292.35 & 8,049.00 & \textbf{101,463.46} & 1,722.7 \% & 85,331.43 & 1,442.5 \% & 101,197.71 & 1,718.1 \%\\
phoenix &761.40 & 7,242.60 & 121,017.59 & 1,855.5 \% & 105,592.66 & 1,617.5 \% & \textbf{815,728.70} & 12,574.3 \%\\
pitfall &-229.44 & \textbf{6,463.69} & 0.00 & 3.4 \% & 0.00 & 3.4 \% & 0.00 & 3.4 \%\\
pong &-20.71 & 14.59 & 20.93 & 118.0 \% & 20.93 & 118.0 \% & \textbf{20.95} & 118.0 \%\\
private eye &24.94 & \textbf{69,571.27} & 200.00 & 0.3 \% & 100.00 & 0.1 \% & 100.00 & 0.1 \%\\
qbert &163.88 & 13,455.00 & 62,678.37 & 470.3 \% & \textbf{102,129.41} & 767.2 \% & 94,906.25 & 712.8 \%\\
riverraid &1,338.50 & 17,118.00 & \textbf{173,779.34} & 1,092.8 \% & 137,983.33 & 866.0 \% & 171,673.78 & 1,079.5 \%\\
road runner &11.50 & 7,845.00 & 380,115.75 & 4,852.3 \% & \textbf{604,083.31} & 7,711.4 \% & 531,097.00 & 6,779.7 \%\\
robotank &2.16 & 11.94 & 66.46 & 657.5 \% & 69.93 & 692.9 \% & \textbf{100.59} & 1,006.4 \%\\
seaquest &68.40 & 42,054.71 & 352,721.81 & 839.9 \% & 399,763.62 & 952.0 \% & \textbf{999,659.18} & 2,380.8 \%\\
skiing &-17,098.09 & \textbf{-4,336.93} & -28,449.98 & -89.0 \% & -30,000.00 & -101.1 \% & -30,000.00 & -101.1 \%\\
solaris &1,236.30 & \textbf{12,326.67} & 3,507.99 & 20.5 \% & 5,860.00 & 41.7 \% & 5,132.95 & 35.1 \%\\
space invaders &148.03 & 1,668.67 & \textbf{3,663.32} & 231.2 \% & 3,639.04 & 229.6 \% & 3,645.63 & 230.0 \%\\
star gunner &664.00 & 10,250.00 & \textbf{156,559.81} & 1,626.3 \% & 127,416.66 & 1,322.3 \% & 154,548.26 & 1,605.3 \%\\
surround &-9.99 & 6.53 & 8.82 & 113.9 \% & 8.62 & 112.7 \% & \textbf{9.90} & 120.4 \%\\
tennis &-23.84 & -8.27 & -0.23 & 151.6 \% & \textbf{0.00} & 153.1 \% & -0.00 & 153.1 \%\\
time pilot &3,568.00 & 5,229.10 & 183,259.78 & 10,817.6 \% & \textbf{427,209.09} & 25,503.6 \% & 424,011.16 & 25,311.1 \%\\
tutankham &11.43 & 167.59 & 290.98 & 179.0 \% & 235.00 & 143.2 \% & \textbf{347.99} & 215.5 \%\\
up n down &533.40 & 11,693.23 & 441,300.81 & 3,949.6 \% & 522,961.53 & 4,681.3 \% & \textbf{634,898.18} & 5,684.4 \%\\
venture &0.00 & 1,187.50 & 0.00 & 0.0 \% & 0.00 & 0.0 \% & \textbf{1,731.47} & 145.8 \%\\
video pinball &0.00 & 17,667.90 & 638,373.31 & 3,613.2 \% & 775,303.81 & 4,388.2 \% & \textbf{865,543.44} & 4,899.0 \%\\
wizard of wor &563.50 & 4,756.52 & \textbf{104,527.07} & 2,479.4 \% & 0.00 & -13.4 \% & 100,096.60 & 2,373.8 \%\\
yars revenge &3,092.91 & 54,576.93 & 829,662.62 & 1,605.5 \% & \textbf{846,060.69} & 1,637.3 \% & 219,838.09 & 421.0 \%\\
zaxxon &32.50 & 9,173.30 & 0.53 & -0.3 \% & 58,115.00 & 635.4 \% & \textbf{154,131.86} & 1,685.8 \%\\
\midrule
\# best & 0 & 6 & 16 &  & 9 &  & 26\\
median&  &  & & 741.7 \% & & 692.9 \% & & 1,006.4 \% \\
mean&  &  & & 2,183.6 \% & & 2,188.4 \% & & 2,856.2 \% \\
\bottomrule
\end{tabularx}
\end{center}
 
\caption{
\label{tab:atari-per-level}
\textbf{Evaluation of \muzero{} in Atari for individual games.} Best result for each game highlighted in \textbf{bold}. Each episode is limited to a maximum of 30 minutes of game time (108k frames). Human normalized score is calculated as . The original \muzero{} was trained without sticky actions and evaluated with 30 random no-op starts, we reproduce the results from \cite{muzero} here for easy reference. Our version of \muzero{} was trained and evaluated with \emph{sticky actions} \cite{Marlos2017Atari}. As shown in the table, this does not negatively impact performance, mean and median score are essentially unchanged. Finally, our version of \muzero{} with using Res v2 \cite{resv2} and trained with the Adam optimiser shows clear improvements.
}
\end{table}

\begin{table}
\tiny

\begin{center}\begin{tabularx}{1.05\textwidth}{X|rr|rrrrr|rrrr|r}
\toprule
 &  &  &  &  &  &  &  & \multicolumn{4}{c|}{supervised} & \muzero{} \\
Game & Random & Online DQN & BC & DQN & IQN & BCQ & REM & policy & CRR & max-v & MCTS & Unplugged \\
\midrule
alien  & 199.8 & 2766.8 & 2670.0 & 1690.0 & 2860.0 & 2090.0 & 1730.0 & 1621.0 & 3658.0 & 3414.7 & 3699.7 & \textbf{6335.4} \\
amidar  & 3.2 & \textbf{1557.0} & 256.0 & 224.0 & 351.0 & 254.0 & 214.0 & 157.7 & 499.3 & 399.4 & 540.7 & 962.7 \\
assault  & 235.2 & 1946.1 & 1810.0 & 1940.0 & 2180.0 & 2260.0 & 3070.0 & 1461.3 & 8537.5 & 11063.5 & \textbf{13517.6} & 13406.9 \\
asterix  & 279.1 & 4131.8 & 2960.0 & 1520.0 & 5710.0 & 1930.0 & 4890.0 & 2742.2 & 10484.5 & 28217.5 & 24016.0 & \textbf{34674.0} \\
atlantis  & 16973.0 & 944228.0 & 2390000.0 & 3020000.0 & 2710000.0 & 3200000.0 & \textbf{3360000.0} & 175194.0 & 888496.5 & 399428.5 & 893635.5 & 873651.5 \\
bank heist  & 13.7 & 907.7 & 1050.0 & 50.0 & 1110.0 & 270.0 & 160.0 & 619.0 & 1071.7 & 1057.2 & \textbf{1210.3} & 1168.3 \\
battle zone  & 2786.8 & 26459.0 & 4800.0 & 25600.0 & 16500.0 & 25400.0 & 26200.0 & 15180.0 & 34455.0 & 33290.0 & 43375.0 & \textbf{65815.0} \\
beam rider  & 362.1 & 6453.3 & 1480.0 & 1810.0 & 3020.0 & 1990.0 & 2200.0 & 2418.0 & 13965.5 & 15268.1 & 18499.7 & \textbf{33150.9} \\
boxing  & 0.8 & 84.1 & 83.9 & 96.3 & 95.8 & 97.2 & 97.3 & 74.9 & 95.6 & 97.7 & 95.8 & \textbf{99.4} \\
breakout  & 1.3 & 157.9 & 235.0 & 324.0 & 314.0 & 375.0 & 362.0 & 97.8 & 435.1 & 458.6 & 503.8 & \textbf{582.2} \\
carnival  & 669.5 & 5339.5 & 3920.0 & 1450.0 & 4820.0 & 4310.0 & 2080.0 & 5160.7 & 5588.8 & 4945.8 & 5525.2 & \textbf{5609.5} \\
centipede  & 2181.7 & 3972.5 & 1070.0 & 1250.0 & 1830.0 & 1430.0 & 810.0 & 2304.8 & 3953.9 & 4460.7 & 4397.6 & \textbf{10764.0} \\
chopper command  & 823.1 & 3678.2 & 660.0 & 2250.0 & 830.0 & 3950.0 & 3610.0 & 1409.0 & 10315.0 & 15332.0 & 16929.5 & \textbf{41567.0} \\
crazy climber  & 8173.6 & 118080.2 & 123000.0 & 23000.0 & 126000.0 & 28000.0 & 42000.0 & 94845.5 & 135524.5 & 140857.0 & 150201.5 & \textbf{167372.5} \\
demon attack  & 166.0 & 6517.0 & 7600.0 & 11000.0 & 15500.0 & 19300.0 & 17000.0 & 2984.0 & 101626.9 & 101440.5 & 118643.1 & \textbf{212935.6} \\
double dunk  & -18.4 & -1.2 & -16.4 & -17.9 & -16.7 & -12.9 & -17.9 & -16.9 & 0.5 & -3.0 & -0.8 & \textbf{20.9} \\
enduro  & 0.0 & 1016.3 & 720.0 & 1210.0 & 1700.0 & 1390.0 & \textbf{3650.0} & 534.2 & 2334.6 & 1830.1 & 2193.8 & 2334.2 \\
fishing derby  & -93.2 & 18.6 & -7.4 & 17.0 & 20.8 & 28.9 & 29.3 & -6.1 & 24.8 & 14.1 & 11.7 & \textbf{46.0} \\
freeway  & 0.0 & 26.8 & 21.8 & 15.4 & 24.7 & 16.9 & 7.2 & 28.0 & \textbf{33.7} & 27.6 & 33.1 & 32.4 \\
frostbite  & 72.0 & 1643.7 & 780.0 & 3230.0 & 2630.0 & 3520.0 & 3070.0 & 828.6 & 3812.0 & 4518.3 & 4377.4 & \textbf{5094.9} \\
gopher  & 282.6 & 8241.0 & 4900.0 & 2400.0 & 11300.0 & 8700.0 & 3700.0 & 3854.7 & 34731.1 & 93322.6 & 83799.2 & \textbf{101380.3} \\
gravitar  & 213.7 & 310.6 & 20.0 & 500.0 & 235.0 & 580.0 & 424.0 & 229.8 & 479.5 & 648.2 & 709.2 & \textbf{1891.8} \\
hero  & 719.1 & 16233.5 & 13900.0 & 5200.0 & 16200.0 & 13200.0 & 14000.0 & 11251.6 & 20615.4 & 20485.1 & 20755.3 & \textbf{20757.1} \\
ice hockey  & -9.8 & -4.0 & -5.6 & -2.9 & -4.7 & -2.5 & -1.2 & -6.3 & -1.5 & -4.3 & -4.0 & \textbf{20.7} \\
jamesbond  & 27.6 & 777.7 & 237.0 & 490.0 & 699.0 & 438.0 & 369.0 & 393.0 & 670.8 & 649.2 & 753.5 & \textbf{945.2} \\
kangaroo  & 41.3 & 14125.1 & 5690.0 & 820.0 & 9120.0 & 1300.0 & 1210.0 & 4701.0 & 14463.0 & 5817.0 & 12045.5 & \textbf{14736.0} \\
krull  & 1556.9 & 7238.5 & 8500.0 & 7480.0 & 8470.0 & 7780.0 & 7980.0 & 5835.9 & 8624.2 & 8803.3 & 9347.3 & \textbf{9815.3} \\
kung fu master  & 556.3 & 26637.9 & 5100.0 & 16100.0 & 19500.0 & 16900.0 & 19400.0 & 8998.0 & 28020.0 & 24526.0 & 39628.5 & \textbf{61354.0} \\
ms pacman  & 248.0 & 4171.5 & 4040.0 & 2470.0 & 4390.0 & 3080.0 & 3150.0 & 2946.8 & 6240.7 & 6745.7 & 6839.1 & \textbf{10934.1} \\
name this game  & 2401.1 & 8645.1 & 4100.0 & 11500.0 & 9900.0 & 12600.0 & 13000.0 & 6043.2 & 17094.8 & 19084.8 & 19090.2 & \textbf{27429.3} \\
phoenix  & 873.0 & 5122.3 & 2940.0 & 6410.0 & 4940.0 & 6620.0 & 7480.0 & 4376.6 & 16698.7 & 37443.7 & \textbf{38667.5} & 32532.8 \\
pong  & -20.3 & 18.2 & 18.9 & 12.9 & 19.2 & 16.5 & 16.5 & 15.0 & 19.7 & 2.7 & 14.5 & \textbf{20.6} \\
pooyan  & 411.4 & 4135.3 & 3850.0 & 3180.0 & 5000.0 & 4200.0 & 4470.0 & 2703.8 & 8082.8 & 7098.6 & 8916.2 & \textbf{13630.0} \\
qbert  & 155.0 & 12275.1 & 12600.0 & 10600.0 & 13400.0 & 12600.0 & 13100.0 & 8296.6 & 14900.6 & 12469.9 & 13428.0 & \textbf{14943.2} \\
riverraid  & 1504.2 & 12798.9 & 6000.0 & 9100.0 & 13000.0 & 14200.0 & 14200.0 & 8476.6 & 21091.2 & 20142.1 & 21509.6 & \textbf{22245.7} \\
road runner  & 15.5 & 47880.5 & 19000.0 & 31700.0 & 44700.0 & 57400.0 & 56500.0 & 31321.5 & 59337.5 & 59514.0 & \textbf{60856.0} & 60465.0 \\
robotank  & 2.0 & 63.4 & 15.7 & 55.7 & 42.7 & 60.7 & 60.5 & 27.6 & 69.4 & 74.5 & 75.2 & \textbf{91.1} \\
seaquest  & 81.8 & 3233.5 & 150.0 & 2870.0 & 1670.0 & 5410.0 & 5910.0 & 802.7 & 7514.2 & 7315.1 & 7714.5 & \textbf{8411.8} \\
space invaders  & 149.5 & 2044.6 & 790.0 & 2710.0 & 2840.0 & 2920.0 & 2810.0 & 1235.5 & 35375.9 & 45017.3 & 46391.2 & \textbf{55704.7} \\
star gunner  & 677.2 & 55103.8 & 3000.0 & 1600.0 & 39400.0 & 2500.0 & 7500.0 & 11661.0 & 86955.0 & 98493.0 & 101165.0 & \textbf{145711.5} \\
time pilot  & 3450.9 & 4160.5 & 1950.0 & 5310.0 & 3140.0 & 5180.0 & 4490.0 & 2157.5 & 5282.5 & 6572.5 & 7786.0 & \textbf{9427.5} \\
up n down  & 513.9 & 15677.9 & 16300.0 & 14600.0 & 32300.0 & 32500.0 & 27600.0 & 6127.2 & 124783.7 & 209816.5 & 244367.5 & \textbf{251927.9} \\
video pinball  & 26024.4 & 335055.7 & 27000.0 & 82000.0 & 102000.0 & 103000.0 & 313000.0 & 100873.0 & \textbf{537485.5} & 500181.8 & 465922.8 & 462317.5 \\
wizard of wor  & 686.6 & 1787.8 & 730.0 & 2300.0 & 1400.0 & 4680.0 & 2730.0 & 846.0 & 7531.0 & 15541.0 & 18634.5 & \textbf{40651.0} \\
yars revenge  & 3147.7 & 26763.0 & 19100.0 & 24900.0 & 28400.0 & 29100.0 & 23100.0 & 20572.5 & 56552.6 & 76380.4 & 77773.8 & \textbf{141317.3} \\
zaxxon  & 10.6 & 4681.9 & 10.0 & 6050.0 & 870.0 & 9430.0 & 8300.0 & 2457.0 & 9397.0 & 11864.0 & 12102.5 & \textbf{16143.5} \\
\bottomrule
\end{tabularx}
\end{center}
 
\caption{
\label{tab:rl-unplugged-per-level}
\textbf{Evaluation of \muzero{} in RL Unplugged for individual games.} Best result for each game highlighted in \textbf{bold}. Each episode is limited to a maximum of 30 minutes of game time (108k frames).
}
\end{table}

\end{document}
