\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{multirow}
\usepackage[ruled,linesnumbered]{algorithm2e}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1271} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning}

\author{
Haohang Xu\textsuperscript{1,2}\quad  Xiaopeng Zhang\textsuperscript{2}\quad Hao Li\textsuperscript{1,2}\quad Lingxi Xie\textsuperscript{2}\quad Hongkai Xiong \textsuperscript{1}\quad Qi Tian\textsuperscript{2}\\
\textsuperscript{1}Shanghai Jiao Tong University,\quad\textsuperscript{2}Huawei Inc.\\
\small\texttt{\{xuhaohang,lihao0374,xionghongkai\}@sjtu.edu.cn}\\\small\texttt{\{zxphistory,198808xc\}@gmail.com}\quad\small\texttt{tian.qi1@huawei.com}
}

\maketitle


\begin{abstract}
   Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, pushing away two images that are de facto similar is suboptimal for general representation. In this paper, we propose a hierarchical semantic alignment strategy via expanding the views generated by a single image to \textbf{Cross-samples and Multi-level} representation, and models the invariance to semantically similar images in a hierarchical way. This is achieved by extending the contrastive loss to allow for multiple positives per anchor, and explicitly pulling semantically similar images/patches together at different layers of the network. Our method, termed as CsMl, has the ability to integrate multi-level visual representations across samples in a robust way. CsMl is applicable to current contrastive learning based methods and consistently improves the performance. Notably, using the moco as an instantiation, CsMl achieves a \textbf{76.6\% }top-1 accuracy with linear evaluation using ResNet-50 as backbone, and \textbf{66.7\%} and \textbf{75.1\%} top-1 accuracy with only 1\% and 10\% labels, respectively. \textbf{All these numbers set the new state-of-the-art.}
\end{abstract}

\section{Introduction}
As a fundamental task in machine learning, representation learning targets at extracting compact features from the raw data, and has been dominated by the fully supervised paradigm over the past decades \cite{he2016deep}, \cite{russakovsky2015imagenet}, \cite{yun2019cutmix}, \cite{zhang2017mixup}. Recent progress on representation learning has witnessed a remarkable success over self-supervised learning \cite{doersch2015unsupervised}, \cite{gidaris2018unsupervised}, \cite{noroozi2016unsupervised}, \cite{pathak2016context}, \cite{wu2018unsupervised}, which facilitates feature learning without human annotated labels. In self-supervised learning, a network is trained based on a series of predefined tasks according to the intrinsic distribution priors of images, such as image colorization \cite{zhang2016colorful}, rotation prediction \cite{gidaris2018unsupervised}, context completion \cite{pathak2016context}, \emph{etc}. More recently, the contrastive learning method \cite{he2020momentum}, which is based on instance discrimination as a pretext task, has taken-off as it has been demonstrated to outperform the supervised counterparts on several downstream tasks like classification and detection.

\begin{figure}[t!]
  \begin{center}
        \includegraphics[width=0.98\linewidth,height=5cm]{fig/Acc_overview.pdf}
  \end{center}
  \vspace{-0.2cm}
     \caption{An overall performance comparison of our proposed pre-trained features on several widely evaluated unsupervised benchmarks. Here, a standard ResNet-50 is used as backbone. CsMl significantly improves the performance on various tasks.}
  \label{overall_results}
\end{figure}



In contrastive learning, each image as well its augmentations is treated as a separate class, and the views generated by a single image are pulled closer together, while all other images are treated as negatives and pushed away. In this setting, the invariance is only encoded from low-level image transformations such as cropping, blurring, and color jittering, \emph{etc}. While the invariance to semantically similar images is not explicitly modeled but on the contrary, they are treated as negatives and pushed away as in MoCo \cite{he2020momentum}. This is contradictory with the alignment principle \cite{wang2020understanding} of feature representation, which favors the encoders to assign similar features to similar samples. As a result, the optimization is contradictory with the intrinsic distribution of images and is not optimal for feature representation. 

In this paper, we introduce a hierarchical semantic alignment strategy via seed the views that are constrained within a single image and a single level representation to \textbf{Cross-samples and Multi-levels}. The idea behind our strategy is to align semantically similar samples in different latent space, and thus enable better representation throughout the network. Specifically, the cross-sample views are achieved by simply searching the feature representation in the embedding space, and selecting the nearest neighbors that are similar with the anchor for contrastive learning. While the multi-level views are expanded at the intermediate layers of a network, which enables hierarchical representation of the same image/patch. As a result, the views are seeded from a single image, and expanded across different samples at different levels, these views are pulled together for more general and discriminative representation.

For \textbf{cross-sample views}, it is a dilemma to select appropriate nearest samples as positives, pulling samples that are \emph{de facto} very similar in the feature space brings about limited performance gain since current representation handles these invariance well, while enlarging the searching space would inevitably introduce noisy samples. To solve this issue, we rely on data mixing to generate extra positive samples, which can be treated as a smoothing regularization of the anchor. In this way, similar samples are extended in a smoother and robust way, and we are able to better model the intra-class similarity for compact representation. 




For \textbf{multi-level views}, we find that although the linear classification accuracy of the last layers is approaching the supervised baseline \cite{chen2020simple}, \cite{chen2020big}, the middle-level representation of current contrastive based methods suffers much lower discrimination capacity, which is harmful for downstream tasks such as detection that require intermediate discrimination ability. Towards this goal, we extend the views to intermediate layers of a network and propose a hierarchical training strategy that enables the feature representation to be more discriminative in the intermediate layers. However, it suffers from optimization contradiction when directly adding a loss layer on the intermediate representation due to the gradient competition issue. To solve this issue, we add a bottleneck layer for each intermediate loss, which we find is applicable for robust optimization. In this way, the features are deeply supervised throughout the network, which also benefits for transferability.






The proposed hierarchical semantic alignment strategy via CsMl significantly boosts the feature representation of contrastive learning when evaluated on several self-supervised learning benchmarks. As shown in Fig. \ref{overall_results}, using MoCo as an instantiation  \cite{he2020momentum}, we achieve  top-1 accuracy with a standard ResNet-50 on ImageNet linear evaluation, and  and  top-1 accuracy with  and  labels, respectively. We also validate its transferring ability on several downstream tasks covering detection and segmentation, and achieve better results comparing with previous self-supervised methods.

\section{Related Work}
Self-supervised representation learning has attracted more and more attentions over the past few years since it is free of labels and is easy to scale up. Self-supervised learning aims at exploring the intrinsic distribution of data samples via constructing a series of pretext tasks, which varies in utilizing different priors of images. Traditional self-supervised learning has sought to learn a compressed code which can effectively reconstruct the input. Among them, a typical strategy is to take advantage of the spatial properties of images, like predicting the relative spatial positions of images patches \cite{doersch2015unsupervised}, \cite{noroozi2016unsupervised}, or inferring the missing parts of images by inpainting \cite{pathak2016context}, colorization \cite{zhang2016colorful}, or rotation prediction \cite{gidaris2018unsupervised} \emph{etc.}. Recent progress in self-supervised learning is mainly based on instance discrimination \cite{wu2018unsupervised}, in which each image as well as its augmentations is treated as a separate class. The motivation behind these works is the InfoMax principle, which aims at maximizing mutual information \cite{tian2019contrastive}, \cite{wu2018unsupervised} across different augmentations of the same image \cite{chen2020simple}, \cite{he2020momentum}, \cite{tian2019contrastive}. The design choices of the InfoMax principle, such as the number of negatives and how to sample them, hyper-parameter settings, and data augmentations all play a critical role for a good representation.



Data augmentation plays a key role for contrastive learning based representation. Since the invariance is only encoded by different transformations of an image. According to \cite{chen2020simple}, \cite{tian2020makes}, the performance of contrastive learning based approaches strongly relies on the types and strength of augmentations, \emph{i.e.}, image transformation priors that do not change object identity. In this way, the network is encouraged to hold invariance in the local vicinities of each sample, and usually more augmentations benefit for feature representation. However, current widely used data augmentation methods are mostly operated within a single sample. One exception is the method in \cite{shen2020rethinking}, which makes use of mixup mixture for flattened contrastive predictions. However, such a mixture strategy is conducted among all the images, which destroys the local similarity when contrasting mixed samples that are semantically dissimilar.

Our method is reminiscent of the recent proposed Supervised Contrastive Learning \cite{khosla2020supervised}, which pulls multiple positive samples together. The differences are that, first, SCL is designed for fully supervised paradigm, where the positive samples are simply selected from the ground truth labels, while our method does not rely on these labels, and deliberately design a positive sample selection strategy to facilitate semantic alignment in a robust way. Second, we extend the contrastive loss to the intermediate hidden layers to enhance its discriminative power of the earlier layers, which is beneficial for discriminative representation and with better transferability, especially for semi-supervised learning.



\begin{figure*}[t!]
  \begin{center}
        \includegraphics[width=0.9\linewidth,height=7.6cm]{fig/HSA_overview_ori.pdf}
  \end{center}
  \vspace{-0.3cm}
     \caption{An overview of our proposed hierarchical semantic alignment framework. The baseline is based on MoCo \cite{he2020momentum}, which requires a query encoder  and an asynchronously updated key encoder . Given an anchor image , we randomly select a positive sample  from the nearest neighborhood set , and generate the mixed sample . The hierarchical semantic alignment is enforced by pulling the positive samples , , and  in the intermediate layers as well as the last embedding space.}
  \label{framework}
\end{figure*}

\section{Methodology}
In this section, we start by reviewing contrastive loss for self-supervised learning, and investigate its drawbacks for general feature representation. Then we present the proposed hierarchical training strategy that pulls semantically similar images at different layers of a network. As shown in Fig. \ref{framework}, the core idea includes two modules, first, we deliberately design a positive sample selection strategy to expand the neighborhood of a single image, and adjust the contrastive loss to allow for multiple positives during each forward propagation. Furthermore, we propagate the semantic alignment to the earlier layers to encourage class separability. In this way, the features are trained in a hierarchical way for more compact representation. Each module would be elaborated in the following.

\subsection{An Overview of Contrastive Learning}










Contrastive learning targets at learning an encoder that is able to map positive pairs to similar representations while pushing away those negative samples in the embedding space. It can be efficiently addressed via momentum contrast \cite{he2020momentum}, which substantially increases the number of negative samples. Given a reference image with two augmented views  and , MoCo aims to learn feature representation  by a query encoder , that can distinguish  from all other images , where  and all the negatives  are encoded by an asynchronously updated key encoder , with  and , the contrastive loss can be defined as:



\noindent where  is the temperature parameter scaling the distribution of distances. However, the positive samples are constrained within a single image with different transformations, and only support one positive sample for each query , which is low efficient and hard for modeling invariance to semantically similar images.


\subsection{Contrastive Learning with Cross-Sample Views}
In this section, we extend the views generated from a single image to cross-sample views, and describe the proposed positive sample selection strategy and adjust contrastive loss to allow for multiple positives to explicitly model the invariance among similar images.


\textbf{Positive Sample Selection.} For positive samples, we simply make use of  nearest neighbors to search semantically similar images in the embedding space. Specially, given unlabeled training set  and a query encoder , we obtain the corresponding embedding representation  where . For a typical Res-50 network, the embedding is obtained from the last average pooled features with dimension 2048. Given an anchor sample , we compute the cosine similarity with all other images, and  select the top  samples with the highest similarity as positives .


\textbf{Loss Function.} We simply adjust the contrastive loss in Eq. \ref{moco} to allow for multiple positives per anchor. Given an anchor sample  and its nearest neighborhood set , we randomly select a positive sample , and the loss term  for  can be reformulated as:


where each anchor sample  encoded with , is pulled with two samples  and  encoded with , and pushed away with all other samples in the key encoder . Symmetrically, the loss term  for positive sample  can be obtained accordingly. The overall loss is the combination of the two losses, which is equipped with two positive samples in the query encoder , and the corresponding two positive samples in the key encoder . Each sample is accompanied with a random data augmentation as described in \cite{chen2020improved}, and is pulled together with all positive samples (also undergo a random data augmentation) from the other encoder.



\textbf{Expanding the Neighborhood.} It is a dilemma to define an appropriate  for nearest sample selection, setting it too small, the objective pulls samples that are already very similar in the feature space, and brings about limited performance gain since current representation handles these invariance well, while setting it too large, it would inevitably introduce noisy samples, and pulling these samples would destroy the local similarity constraint and is harmful for general representation. To solve this issue, we rely on data mixture to expand the neighborhood space of an anchor based on the selected positive samples. The assumption is that the mixed samples act as an interpolation between two samples, and lies in the local neighborhood of the two samples. In this way, the generated mixed samples expand the neighbors in the embedding space that current model cannot handle well, and pulling these samples is beneficial for better generalization.


In particular, we apply CutMix \cite{yun2019cutmix} augmentation, which is widely used as a regularization strategy to train neural networks in fully supervised paradigm. Given an anchor sample , and its positive neighbor , the mixed sample  is generated as follows:

where  is a binary mask that has the same size as , and indicates where to drop out the region in  and replaced with a randomly selected patch from , and  denotes the width and height of an image, respectively.  is a binary mask filled with ones, and  is the element-wise multiplication operation. For mask  generation, we simply follow the setting in \cite{yun2019cutmix}, and do not carefully tune the parameters. Note that different from CutMix used in fully supervised learning that randomly selects two images for mixing and changes the corresponding labels accordingly, we only sample those similar samples and ensure that the generated samples lie in the local neighborhood of the anchor.


The mixed samples  is treated as a new positive sample, and pulled together with  and  accordingly:


where  is a combination ratio that determines the cropped area for CutMix operation, and is sampled from beta distribution Beta with parameter  (). The final loss function can be formulated as :



We simply set the balancing factors of the three terms as 1 in all our experiments.





\subsection{Contrastive Learning with Multi-level Views}
Following training a customized network, the contrastive loss is only penalized at the last embedding layers, while the optimization of the intermediate hidden layers is implicitly penalized by back propagating the gradients to the earlier layers. However, due to the lack of labels, the optimization objective is more challenging and suffers slow convergence, and the intermediate layers are especially under fitted and with limited discriminative power comparing with fully supervised learning (c.f. Fig. \ref{fig:inter_layer}). Inspired by \cite{lee2015deeply}, we extend the proposed contrastive loss in Eq. \ref{final_loss} to the intermediate hidden layers, which targets at explicitly modeling the similarities among image/patches for better discrimination. Specifically, we introduce a companied objective at the end of each stage for a typical ResNet network, which acts as an additional constraint during the optimization procedure.

As demonstrated in \cite{romero2014fitnets}, it is too aggressive to directly add a loss layer as side branch of the intermediate layers due to the gradient competition issue. The optimization objective from one side branch would probably inconsistent with that from other branches since they undergoes extremely different levels of layers during back propagation. To solve this issue, for each companied loss at stage , we add another embedding layer  which consists of bottleneck layers before the contrastive loss as side branch. In practice, we find that a single bottleneck layer is applicable to alleviate the gradient competition and enable efficient optimization. \emph{Note that These companied branches are removed after training and hence do not increase complexity of the network after pretraining}.

Specifically, given encoder  and , we define feature map at stage  as  and , which is truncated at stage . Each encoder is passed through another embedding layer , which consists of a bottleneck layer and 2 MLP layers to encode the feature into a 128-dim vector. The loss function of  specific to stage  is defined as:


where  and . Similarly,  the loss term  and  can be obtained according to Eq. \ref{eq: mix_loss_term} and Eq. \ref{final_loss}, respectively. When there are  losses corresponding to  intermediate stages, the final losses of the whole network can be computed as:



\begin{table}[]
\caption{Top-1 accuracy under linear classification on ImageNet with ResNet-50 as backbone.}
\vspace{0.05in}
\fontsize{10}{12}\selectfont
\centering
\setlength{\tabcolsep}{3mm}
\begin{tabular}{lc}
\toprule
Method     & Accuracy(\%) \\
\midrule
BigBiGAN\cite{donahue2019large} &56.6 \\
Local aggregation \cite{zhuang2019local} &58.8 \\
SeLa \cite{asano2020self} &61.5 \\
PIRL \cite{misra2020self} &63.6 \\
CPCv2 \cite{henaff2019data} &63.8 \\
PCL \cite{li2020prototypical} &65.9 \\
SimCLRv2 \cite{chen2020big} &71.7 \\
MoCo v2 \cite{chen2020improved} &71.1 \\
BYOL \cite{grill2020bootstrap} &74.3 \\
SwAV \cite{caron2020unsupervised} &75.3 \\
\midrule
CsMl 200 Epochs (w/o multi-crop) & 71.6 \\
CsMl 800 Epochs (w/o multi-crop) &74.4 \\
\midrule
CsMl 200 Epochs(w/ multi-crop) & 74.6 \\
CsMl 800 Epochs(w/ multi-crop) & \textbf{76.6} \\
\bottomrule
\end{tabular}
\label{tab:lincls_imagenet1K}
\end{table}

\section{Experiments}
In this section, we access our proposed feature representation on several widely used unsupervised benchmarks. We first evaluate the classification performance on ImageNet under linear evaluation and semi-supervised protocols \cite{chen2020simple}, \cite{he2020momentum}. Then we transfer the representation to several downstream tasks including detection and instance segmentation. We also analyze the performance of our feature representation with detailed ablation studies.

\begin{table}[t]
\caption{KNN classification accuracy on ImageNet. We report top-1 accuracy with 20 and 200 nearest neighbors, respectively.}
\vspace{0.05in}
\renewcommand\arraystretch{1.2}
\centering
\setlength{\tabcolsep}{5mm}
\begin{tabular}{lcc}
\toprule
Method       & 20-NN    & 200-NN               \\
\midrule
Supervised   & 75.0     & 73.2                \\
\midrule
NPID \cite{wu2018unsupervised}        & -        & 46.5                 \\
LA\cite{zhuang2019local}           & -        & 49.4                 \\
PCL\cite{li2020prototypical}          & 54.5     & -                    \\
MoCo v2\cite{chen2020improved}      & 62.0     & 59.0                 \\
SwAV\cite{caron2020unsupervised}         & 65.7     & 62.7                 \\
\midrule
CsMl          & \textbf{72.4}     & \textbf{70.7} \\
\bottomrule
\end{tabular}
\label{tab: knn_acc}
\end{table}

\subsection{Pre-training Details}
The feature representation is trained based on a standard ResNet-50 \cite{he2016deep} network, using ImageNet 2012 training dataset \cite{deng2009imagenet}. We follow the settings MoCo as in \cite{he2020momentum} \footnote{Our method is also applicable for other contrastive based method, see the appendix for the results of BYOL \cite{grill2020bootstrap}}, which employs an asynchronously updated key encoder to enlarge the capacity of negative samples, and add a 2-layer MLP on top of the last average pooling layer to form a 128-d embedding vector \cite{chen2020improved}. The model is trained using SGD optimizer with momentum 0.9 and weight decay 0.0001. The batch size and learning rate are set to 1024 and 0.12 for 32 GPUs, according to the parameters recommended by \cite{chen2020improved}, \cite{goyal2017accurate}. The learning rate is decayed to  by cosine scheduler \cite{loshchilov2016sgdr} during the whole training process.

For companied loss in the intermediate stages, each branch is added with a bottleneck with three layers (, , and  convolutions, and the number of channels follows the setting of the corresponding stage) and 2-MLP layers. We add the companied loss at both stage 2 and stage 3, and simply set the balance factor of each layer as 1.  For positive sample selection, we perform knn every 5 epochs and select top-10 nearest neighbors for each anchor. We find that the update frequency does not affect the performance too much when it ranges from 1 to 20. For efficiency, we only pull one positive sample per anchor as well as another mixed sample during each forward propagation, which we find is sufficient (see appendix for pulling more positive samples at once). For data augmentation, except for those used in \cite{he2020momentum}, we also report the results of multi-crop augmentation \cite{caron2020unsupervised}, which has been demonstrated to be effective for further performance gain. The final model is trained for 800 epochs for evaluation.

\subsection{Experiments on ImageNet}
\paragraph{Classification with Linear Evaluation.} We first evaluate our pretrained features by training a linear classifier on top of the frozen representation, following a common protocol in \cite{he2020momentum}. The classifier is trained on global average pooled features of ResNet-50 for 100 epochs, and we report the center crop, top-1 classification accuracy on ImageNet validation set. As shown in Table \ref{tab:lincls_imagenet1K}, for fair comparison, all results are based on the same network structure with the same amount of parameters. CsMl achieves  top-1 accuracy under 800 epochs pretraining, which outperforms the MoCo v2 baseline \cite{chen2020improved} by , and the performance can be further boosted to  when adding multi-crop data augmentations, which is better than previous best performed result SwAV by . 


\begin{table}[t]
\renewcommand\arraystretch{1.2}
\caption{Semi-supervised learning by fine-tuning  and  labeled images on ImageNet. The last row reports results of using a simple data mining procedure (averaged over 5 trials).}
\vspace{0.05in}
\centering
\setlength{\tabcolsep}{2.2mm}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{ labels} & \multicolumn{2}{c}{ labels} \\
\multicolumn{1}{c}{}            & Top-1  & Top-5    & Top-1  & Top-5 \\\toprule
Supervised                      & 25.4   & 48.4     & 56.4   & 80.4  \\ \midrule
UDA\cite{xie2019unsupervised}   & -      & -        & 68.8   & 88.5  \\
FixMatch\cite{sohn2020fixmatch} & -      & -        & 71.5   & 89.1  \\
PIRL\cite{misra2020self}        & 30.7   & 57.2     & 60.4   & 83.8  \\
PCL\cite{li2020prototypical}    & -      & 75.6     & -      & 86.2 \\
SimCLR\cite{chen2020simple}     & 48.3   & 75.5     & 65.6   & 87.8 \\
MoCo v2\cite{chen2020improved}  & 52.4   & 78.4     & 65.3   & 86.6 \\
SwAV\cite{caron2020unsupervised}        & 53.9   & 78.5     & 70.2   & 89.9  \\
SimCLRv2\cite{chen2020big}      & 57.9   & 82.5     & 68.4   & 89.2 \\
\midrule
CsMl                            & \textbf{62.2}   & \textbf{83.0}     & \textbf{72.9}   &\textbf{90.7}   \\
CsMl+data mining                            & \textbf{66.7}   & \textbf{87.7}     & \textbf{75.1}   &\textbf{92.1}   \\
\bottomrule
\end{tabular}
\label{tab:semi_imagenet}
\end{table}

\paragraph{Classification with KNN Classifier.} We also evaluate our representation with KNN classifier, which is able to evaluate the pre-trained features more directly. Following \cite{caron2020unsupervised}, we center crop the images to obtain features from the last average pooled layers, and report the accuracy with 20 and 200 NN (we choose the result of 800 epochs with multi-crop augmentation) in Table \ref{tab: knn_acc}. For convenient comparison, we also list the KNN classification results of fully supervised model, which achieves accuracy of . CsMl is only  lower than the supervised baseline, and significantly outperforms previous methods, which validates the effectiveness of explicitly modeling similarities cross samples.



\paragraph{Semi-supervised Settings.} We also evaluate the representations by fine-tuning the whole network with few shot labels. Following the evaluation protocol in \cite{chen2020simple}, \cite{chen2020big}, we fine-tune all layers with only  and  labeled data. For fair comparison, we use the same splits of training data as in \cite{chen2020simple}, using SGD optimizer with momentum 0.9 to fine-tune all layers for 60 epochs. The initial learning rate is set to  for backbone and 10 for randomly initialized fc layer. During fine-tuning, only random cropping and horizontal flipping are applied for fair comparisons. Note that our method does not apply any special design like \cite{chen2020big}, which makes use of more MLP layers and has shown improved results when fine-tuning with few labels. As shown in Table \ref{tab:semi_imagenet}, our method achieves  top-1 accuracy with only  labels and  top-1 accuracy with  labels. In both two settings, our method consistently outperforms other semi-supervised and self-supervised methods, especially when  labeled samples are available.

\begin{table*}[]
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{2.0mm}
\caption{Transfer learning accuracy (\%) on COCO detection and COCO instance segmentation (averaged over 5 trials).}
\vspace{0.05in}
\centering
\begin{tabular}{l|cccccc|cccccc}
\hline
\multirow{3}{*}{Method} & \multicolumn{6}{c|}{Mask R-CNN, R50-FPN, Detection}  &\multicolumn{6}{c}{Mask R-CNN, R50-FPN, Segmentation} \\ \cline{2-13}
& AP  & AP & AP & AP  & AP &AP &AP  &AP  &AP  &AP  &AP &AP \\ \hline

Supervised                  &38.9  &59.6  &42.0  &23.0  &42.9  &49.9    &35.4  &56.5  &38.1  &17.5  &38.2 &51.3     \\ \hline
MoCo v2 \cite{chen2020improved} &39.2  &59.9 &42.7 &23.8 &42.7 &50.0    &35.7  &56.8  &38.1  &17.8  &38.1  &50.5   \\
BYOL \cite{grill2020bootstrap} &39.9 &60.2 &43.2 &23.3 &43.2 &\textbf{52.8}   &- &-	&- &- &- &-\\
DenseCL\cite{wang2020dense}  &\textbf{40.3} &59.9 &\textbf{44.3} &- &- &-    &36.4 &57.0 &\textbf{39.2} &- &- &- \\ \hline

CsMl(w/o multi-level) &39.5 &60.1 &43.1 &24.1 &42.8 &50.9  &35.9 &56.9 &38.6 &18.3 &38.1 &\textbf{51.4} \\
CsMl(w/ multi-level) &\textbf{40.3}  &\textbf{61.1} &43.8 &\textbf{25.0} &\textbf{43.6} &50.8 &\textbf{36.6}  &\textbf{58.1}  &39.1  &\textbf{18.8}  &\textbf{39.0}  &51.2 \\ \hline
\end{tabular}
\label{tab: coco_detec}
\end{table*}

Following semi-supervised learning setting that the unlabeled samples are used for training via assigning pseudo labels, we further conduct a simple data mining procedure to access how the pre-trained models improve data mining. The data mining procedure is conducted as follows: 1) Using the fine-tuned model to infer on all unlabeled samples, and obtaining a confidence distribution for each image. 2) The information entropy is calculated to measure the confidence degree of each image, and we filter those samples with entropy higher than a threshold. 3) Convert the soft labels into one-hot pseudo label via only retaining the highest scored dimension, and train the model together with the ground truth labeled data.
We simply set the entropy threshold as 1 and the model is trained following a standard fully supervised learning procedure \cite{he2016deep}. Specifically, the model is trained for 90 epoch with initial learning rate set as  for backbone and  for fc layer, and they are both decayed by 0.1 after every 30 epochs. The results are shown in the last row of Table \ref{tab:semi_imagenet}, the performance can be further boosted via a simple data mining procedure, and we achieve  top-1 accuracy with  labeled samples, and  top-1 accuracy with  labeled samples. 

\begin{table}[]
\caption{Transfer Learning results on PASCAL VOC detection (averaged over 5 trials).}
\vspace{0.05in}
\centering
\setlength{\tabcolsep}{5mm}
\begin{tabular}{lcccc}
\toprule
Methods & AP50 & AP75   \\
\midrule
Supervised                  & 81.4  & 58.8  \\
MoCo v2 \cite{chen2020improved}                    & 82.5  & 64.0  \\
CsMl                        & \textbf{82.7}   & \textbf{64.1}        \\
\bottomrule
\end{tabular}
\label{tab: voc_detec}
\end{table}










\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/inter_layer_acc.pdf}
    \vspace{0.05in}
    \caption{Top-1 linear classification accuracy of different methods at different stages using ResNet-50 as backbone.}
    \label{fig:inter_layer}
    \vspace{-0.2in}
\end{figure}



\subsection{Downstream Tasks}
We also test the generalization of our unsupervised learned representations on more downstream tasks, including object detection and instance segmentation. All experiments follow MoCo \cite{he2020momentum} settings for fair comparisons.

\vspace{-0.1in}
\paragraph{PASCAL VOC}
Following evaluation protocol in \cite{he2020momentum}, we use Faster-RCNN \cite{ren2015faster} detector with R50-C4 backbone. All layers are fine-tuned end-to-end on the union set of VOC07+12 for  schedule, and we evaluate the performance on VOC test07. As shown in Table \ref{tab: voc_detec}, CsMl achieves  and  mAP under AP50 and AP75 metric, which is slightly better than the results of MoCo v2.


\vspace{-0.15in}
\paragraph{MS COCO} We also evaluate the representation learned on a large scale COCO dataset. Following \cite{he2020momentum}, we use mask R-CNN \cite{he2017mask} detector with FPN, fine-tune all the layers end-to-end over the train2017 set, and evaluate the performance on val2017.  As shown in Table \ref{tab: coco_detec}, which compare the detection and segmentation results under default  learning schedule. Our method consistently outperforms the supervised and MoCo v2 baseline.
Notably, we achieve much higher performance on small and medium objects. This is mainly due to the hierarchical alignment strategy that increases the discrimination power of the intermediate layers. Note that our method is comparable with recently proposed DenseCL \cite{wang2020dense}  which is particularly designed for detection task. 


\begin{table}[]
\centering
\caption{Results of adding different modules.}

\vspace{0.05in}
\begin{tabular}{lc}
\toprule
Method                                  & Accuracy (\%)  \\
\midrule
MoCo v2                                 & 67.5         \\
 +                      & 70.4        \\
 +  +                    & 71.6        \\
 +  +  + multi-crop      & 74.6        \\
\bottomrule
\end{tabular}
\label{tab: diff_modules}
\end{table}


\subsection{Ablation Studies}
In this section, we conduct extensive ablation studies to better understand how each component affects the performance. Unless specified, \textbf{all results are compared with models trained for 200 epochs for efficiency}, and we report the top-1 accuracy under linear evaluation protocol.
\vspace{-0.1in}
\paragraph{Effects of Different Modules.} We first diagnose how each component affects the performance, as shown in Table \ref{tab: diff_modules}, simply add one positive sample  boost the baseline MoCo v2 by , and introducing mixed samples for pulling further improve the performance by another . Following \cite{caron2020unsupervised}, we also adopt multi-crop augmentations, and the performance can be further improved by .

\vspace{-0.15in}
\paragraph{Effects of  in KNN Positive Sample Selection}









\begin{figure*}
\centering
\subfigure[]{
    \begin{minipage}{0.32\linewidth}
    \centering
        \includegraphics[width=0.90\linewidth]{fig/diff_topk_acc.pdf}
\label{fig:diff_K}
        \vspace{0.07in}
    \end{minipage}
    }
    \subfigure[]{
    \begin{minipage}{0.32\linewidth}
    \centering
    \vspace{0.01in}
        \includegraphics[width=0.95\linewidth]{fig/gpu_hours.pdf}
\label{fig:gpu_hours}
    \vspace{-0.15in}
    \end{minipage}
    }
    \subfigure[]{
    \begin{minipage}{0.32\linewidth}
    \centering
        \includegraphics[width=0.90\linewidth]{fig/diff_epoch_acc.pdf}
\label{fig:diff_epoch}
        \vspace{0.07in}
    \end{minipage}
    }
    \caption{(a) Top-1 accuracy with different  in knn. (b) Top-1 accuracy and computational complexity comparisons for different models. (c) Top-1 accuracy comparisons for different training epochs of CsMl and MoCo v2.}

\end{figure*}

We then inspect the influence when selecting different number of samples as positive candidates. Fig. \ref{fig:diff_K} shows the top-1 accuracy with respect to different  in knn. It can be shown that when no mixed sample is included, the performance is relatively sensitive to , and the results are relatively robust for a range of  () when mixed samples are introduced. The reason is that mixed samples act as a strong smoothing regularization, which alleviates the noise introduced by knn selection. We also consider a special case, \emph{i.e.,} always select the same sample, \emph{i.e.,}, , noted as . In this setting, we always select the correct positive sample for pulling, but sacrifice the diversity of using cross-samples. The performance gain is limited (), which validate the effectiveness of selecting positives via cross samples.

\vspace{-0.15in}
\paragraph{Effects of Pulling Multi-level Views.}
\begin{table}[]
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{1.8mm}
\caption{Classification accuracy with features of different stages and fine-tuning results with  labels.}
\vspace{0.05in}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{} & Stage2 & Stage3 & Stage4 & 1\% label \\
\midrule
MoCo v2 \cite{chen2020improved}             & 44.5   & 60.2   & 71.1   & 52.4      \\
Pull stage4          & 45.1   & 60.7   & \textbf{71.6}   & 53.4      \\
Pull stage3\&4       & 45.6   & \textbf{64.0}   & 71.5   & 54.5      \\
Pull stage2\&3\&4    & \textbf{46.0}   & \textbf{64.0}   & \textbf{71.6}   & \textbf{54.8}      \\
\bottomrule
\end{tabular}
\label{tab: mulhead}
\end{table}

We analyze the performance of the intermediate layers and validate how our proposed hierarchical alignment strategy benefits the representation. Except for the last layers of stage 4, we also explicitly pull similar samples in earlier layers such as stage 2 and stage 3. Table \ref{tab: mulhead} shows the linear classification accuracy of each stage, it can be shown that pulling similar samples in the shallow layers consistently increases its discrimination power, especially for stage 3, \emph{e.g.,} the accuracy increased by , from  to . The performance gain is relatively small in stage 2, partially because the representation in this stage is too low-level, and is hard for global representation. We also note that the performance of the last layers is not significantly affected by penalizing the shallow layer. However, We find that better separability in the shallow layers is beneficial for fine-tuning  labels, the accuracy increased by  when introducing shallow pulling. The advantages of increased representation of shallow layers can be also validated by the downstream detection and segmentation tasks, as shown in Table \ref{tab: coco_detec}. We also compare the linear classification accuracy of different methods at intermediate layers, as shown in Fig. \ref{fig:inter_layer} 






\subsection{Analysis and Discussions}



\paragraph{Analysis of epoch\& batch size} In our implementation, the batch size is measured based on the number of anchors, and the actual samples are doubled during each forward propagation when we select one positive sample for pulling. In order to get rid of the effects of batch size, we compare our methods with MoCo v2 that trained for double epochs. The results are 68.5\% and 70.4\% for 100 and 200 epochs, which is better than MoCo v2 with accuracies of 67.5\% and 69.5\% for 200 and 400 epochs, respectively. Note that, Moco v2 achieves marginal improvement when extending the training epochs from 800 to 1600 (), while the proposed method achieves much better (74.4\% for 800 epochs) performance. In addition, we double the batch size of MoCo v2 and train it for 200 epochs, the actual batch size of MoCo is same as our method in such setting. As shown in Fig. \ref{fig:diff_epoch}, the performance gain of enlarging batch size is  (()), which is lower than that of CsMl under same epochs (70.4\%). The results verify that the improvement of CsMl is not simply caused by extending the training epochs.

\vspace{-0.15in}
\paragraph{Computational Complexity} We compare the computational complexity with MoCo v2 under 200 epochs pretraining and different variants of CsMl under 100 epochs pretraining with 8 v100 GPUs. As shown in Figure \ref{fig:gpu_hours}, when mixed samples are not involved, the cost time of MoCo v2 and CsMl is almost the same: 53h v.s. 57h. The increased 4 hours is mainly from the cost of nearest sample selection. When the mixed sample is included in the query, the computation cost increased by roughly . Finally, we find that the computation cost of multi-level is marginal because the additional bottleneck's computation is very small.





































\section{Conclusion}
This paper proposes a hierarchical training strategy that pulls semantically similar images  for contrastive learning. The main contributions are two folds, first, in order to select similar samples without labels, we deliberately design a sample selection strategy relying on data mixing, which generates new samples that current model does not perform well. The highlight is that samples are only mixed from those similar samples and does not destroy the local similarity structure. Second, we extend the semantic alignment to intermediate hidden layers and enforces the feature representation to be discriminative throughout the network. In this way, the network can be optimized in a more robust way and we find it is beneficial for general representation. We conduct extensive experiments on widely used self-supervised benchmarks, and consistently outperforms previous self-supervised learning methods.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\section{Appendix}

\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0} 
\setcounter{table}{0} 

\begin{table}[]
\centering
\caption{Top-1 accuracy comparisons based on BYOL}

\vspace{0.01in}
\begin{tabular}{lc}
\toprule
Method  & Accuracy(\%)  \\
\midrule
BYOL 300 epochs & 72.5        \\
BYOL 1000 epochs & 74.3        \\
CsMl 300 epochs & 75.3 \\

\bottomrule
\end{tabular}
\label{tab: HSA_byol}
\end{table}

\begin{table}[]
\centering
\caption{Top-1 accuracy comparisons with more positive pairs}

\vspace{0.01in}
\begin{tabular}{lc}
\toprule
Method  & Accuracy(\%)  \\
\midrule
 & 70.4        \\
 & 70.2  \\
 & 71.6 \\

\bottomrule
\end{tabular}
\label{tab: more_postive}
\end{table}


\subsection{{Results on BYOL}}
{Our method is applicable to other contrastive based methods to further improve the performance. As shown in \ref{tab: HSA_byol}, we applied CsMl to BYOL [10], which does not need negative samples during contrastive learning. Following the hyper-parameters used in [10], we pretrain the model with CsMl for 300 epochs. CsMl achieves an accuracy of  linear classification accuracy, which is even better than BYOL under 1000 epochs pre-training. }

\subsection{{Results of more positive pairs}}
{Based on Eq. (2), CsMl can also be extended to support pulling multiple positive samples during each forward propagation. 
As shown in Table \ref{tab: more_postive}, using 2 positive pairs achieves similar result as using 1 positive pairs. As comparison, introducing mixed samples as positive pairs is more effective. Considering  that adding more positive samples would inevitably increasing the computational complexity, we simply choose one positive sample for each anchor. 
}

\subsection{MixUp v.s. CutMix augmentations}
\begin{table}[]
\centering
\caption{Comparisons of Mixup with CutMix augmentations.}

\vspace{0.01in}
\begin{tabular}{lc}
\toprule
Method                                            & Accuracy(\%)  \\
\midrule
MoCo v2                                             & 67.5         \\
 +                                        & 70.4        \\
 +  +           & 70.1        \\
 +  +          & 71.6        \\

\bottomrule
\end{tabular}
\label{tab: cutmix_mixup}
\end{table}

Besides CutMix, We also consider another cross-sample augmentation strategy Mixup [ref] to expand the neighborhood of an anchor. As shown in Table \ref{tab: cutmix_mixup}, Mixup is worse than Cutmix, and even slightly worse than baseline method that do not involve any mixed sample in query, \emph{i.e.}  setting, the reason may be that Mixup augmentations destroy the naturality of the pixel distribution. 

\subsection{Visualization of Feature Representation} 
We visualize the last embedding feature to better understand the semantic alignment properties of the proposed method. Specifically, we randomly choose 10 classes from the validation set and provide the \textit{t-sne} visualization of feature representation, supervised training and MoCo v2. As shown in Fig.~\ref{t-sne}, the same color denotes features with the same label. It can be shown that CsMl presents higher alignment property comparing with MoCo, and the fully supervised learned representation reveals the highest alignment due to the available of image labels.




\begin{figure}[t!]
  \begin{center}
        \includegraphics[width=0.98\linewidth]{fig/tsne_visual.pdf}
  \end{center}
  \vspace{-0.3cm}
     \caption{\textit{t-sne} visualization of representation learned by MoCo v2, CsMl, and fully supervised learning.}
  \label{t-sne}
\end{figure}






\subsection{Performance comparisons of CsMl and supervised model}





\begin{figure*}
    \centering
    
    \includegraphics[width=0.98\linewidth,height=9cm]{fig/sup_unsup_compare.pdf}
    \caption{Top-1 accuracy comparisons on different categories of CsMl and fully supervised model. Here we compare most successful categories of each method.}
    \label{fig:diff_sup_unsup}
\end{figure*}


\begin{figure*}
    \centering
    
    \includegraphics[width=1.0\linewidth]{fig/pos_visual_unsup_sup_1.pdf}
    \caption{An illustration of the selected knn samples using CsMl and the supervised model.}
    \label{fig:diff_sup_unsup}
\end{figure*}

Here we diagnose the performance difference of CsMl and fully supervised baseline to uncover the advantages of each model. The performance evaluation is under the linear classification protocol and we report per-category accuracy. As shown in Fig. \ref{fig:diff_sup_unsup}, we illustrate the most successful categories of each model, and find that the advantage of supervised model lies in discriminating fine-grained subcategories, \emph{e.g.}, on many sub-classes of dogs (Border collie, Siberian husky, bull mastiff \emph{etc.}). It is intuitive since feature representation among fine-grained sub-categories is very similar, and it is hard to discriminate them without ground truth labels. While for unsupervised model CsMl, the most successful categories roughly around categories that require contour information for discrimination, \emph{e.g. shopping basket and plate rack}, while supervised model usually focuses on discriminative details such as texture. Following the evaluation in Table 2 in the original paper, we also show some example images that use knn for similar samples selection, \emph{i.e.,} given an image in the validation set, and find its top-k nearest neighbors in the training set. The most successful categories of CsMl results from capturing the global contour information.

%
 
\end{document} 