\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{multirow}

\usepackage{xcolor}
\definecolor{darkgreen}{RGB}{18,178,26}
\newcommand\todo[1]{\textcolor{red}{\textit{#1}}}
\newcommand{\RN}[1]{\textup{\uppercase\expandafter{\romannumeral#1}}}


\title{SiamixFormer: a fully-transformer Siamese network with temporal Fusion for accurate building detection and change detection in bi-temporal remote sensing images}


\author{
 Amir mohammadian, Foad Ghaderi \\
  Human-Computer interaction lab, Electrical and Computer Engineering Department\\
  Tarbiat Modares University \\
  Tehran, Iran \\
}

\begin{document}
\maketitle
\begin{abstract}
Building detection and change detection using remote sensing images can help urban and rescue planning. Moreover, they can be used for building damage assessment after natural disasters. Currently, most of the existing models for building detection use only one image (pre-disaster image) to detect buildings. This is based on the idea that post-disaster images reduce the model's performance because of presence of destroyed buildings. In this paper, we propose a siamese model, called SiamixFormer, which uses pre- and post-disaster images as input. Our model has two encoders and has a hierarchical transformer architecture. The output of each stage in both encoders is given to a temporal transformer for feature fusion in a way that query is generated from pre-disaster images and (key, value) is generated from post-disaster images. To this end, temporal features are also considered in feature fusion. Another advantage of using temporal transformers in feature fusion is that they can better maintain large receptive fields generated by transformer encoders compared with CNNs. Finally, the output of the temporal transformer is given to a simple MLP decoder at each stage. The SiamixFormer model is evaluated on xBD, and WHU datasets, for building detection and on LEVIR-CD and CDD datasets for change detection and could outperform the state-of-the-art.
\end{abstract}





\section{Introduction}
\label{sec1}
Natural disasters such as earthquakes, floods, and tsunamis have always been a human concern that take lives of many people every year and impose many financial losses on different countries. Highly affected by the changes in global climate, natural disasters have become stronger and more frequent in recent years \cite{rolnick2022tackling}.One of the most important tasks in response phase of disaster management is assessing damages to buildings and roads from satellite images as fast as possible. To this end, it is essential to detect buildings and identify the level of changes in them. Detecting building and their changes can also be used for urban planning, rescue planning, and preparation before disasters. Traditionally, these analyses are performed by experts, which is a time-consuming task \cite{shen2021bdanet}.

With recent advances in computer vision, it is possible to analyze satellite images automatically with high accuracy and speed. In traditional methods, hand-crafted features such as color, shadow, edge, and roof texture were extracted from images, and a feature vector was generated for each sample. Then classification or clustering was done using classic machine learning algorithms \cite{sirmacek2008building, ferraioli2009multichannel, awrangjeb2011improved, gong2013fuzzy}. In more recent techniques, like convolutional neural networks (CNNs), features are extracted in a data-driven manner. Much effort has been done on utilizing CNN models and fusing deep and shallow features, in this problem. For example, a dual-stream network (DS-Net) that adaptively captures local and long-range information is proposed in \cite{zhang2020local}. The authors of \cite{liu2020multiscale} introduced a multi-scale fusion network to tackle the problem of different scales of buildings in remote sensing images. A Siamese fully connected network is proposed in \cite{ji2018fully} and it is shown that using two inputs can help the model obtain better segmentation accuracy. 

Using bi-temporal images is a common approach in problems such as change detection or building damage assessment. In this approach, images that are taken at two different times are compared in order to assess the difference between them. In general, combining the features extracted from bi-temporal images is performed at different levels as follows: 
\begin{enumerate}
	\item 
	\textbf{Segmentation map level}: Two separate encoder-decoder pairs and a differentiating  module are used. Each encoder-decoder pair generates a segmentation map for each bi-temporal image, and the differentiating module is used to compare them and generate the final change map \cite{ji2019building, liu2019temporal}.
	\item 
	\textbf{Feature level}: Two separate encoders, a common decoder and a fusion module are used. The extracted features from each encoder are compared and fused by the fusion module, and the decoder generates the final segmentation map \cite{chen2020spatial,peng2020optical}.
	\item 
	\textbf{Input image level}: Bi-temporal images are combined. An encoder-decoder pair extracts the features from combination of the two images and produces the final change map \cite{de2020change,zhao2020using}. 
\end{enumerate} 

Recently, many researchers from different domains used transformer architectures. These models were first introduced for natural language processing \cite{vaswani2017attention}, however, their application expanded quickly to other domains, e.g., computer vision applications \cite{dosovitskiy2020image, xie2021segformer}. Due to their robust feature presentation, large and global receptive field, and the capability of modeling long-range dependency between pixels, transformer models can be used in remote sensing problems such as building detection and change detection as well \cite{chen2021building}. Xiao \textit{et al.} \cite{xiao2022swin} presented STEB-UNET model, which is a combination of swin transformer and U-Net architectures. Because each building occupies a tiny part of the remote sensing images, Chen \textit{et al.} represented buildings as a set of sparse feature vectors and introduced the sparse token transformer and could reduce computational complexity \cite{chen2021building}. Bandara \textit{et al.} used a hierarchical structure for the transformer encoder and used two encoders to extract features of bi-temporal images \cite{bandara2022transformer}. They concatenated each stage’s output and fed it to an decoder. Chen \textit{et al.} introduced the BIT-CD model by combining CNN and transformer architectures \cite{chen2021remote}.

The authors of \cite{xie2021segformer}, extended the idea of transformers and proposed SegFormer architecture, which is a model for semantic segmentation problems. Although this model has fewer parameters compared with the other transformer-based models, it performs better in benchmark datasets \cite{xie2021segformer}. Considering the success of Segformer, recently, different models based on this architecture have been introduced, e.g., the Damformer \cite{chen2022dual} and the Changeformer \cite{bandara2022transformer} models that have been used for building damage assessment and change detection problems, respectively. In both models, CNNs are used in the feature fusion section.

In this paper, we propose the SiamixFormer model that uses bi-temporal images for building detection. Experimental results confirm that this approach outperforms the existing methods for building and change detection problems. Our proposed model differs from other siamese models in its approach to feature fusion. While both models use a semantic segmentation network for feature extraction, SiamixFormer incorporates a temporal transformer for feature fusion, which takes into account the temporal relationship between features in  and  images. Unlike other models that use CNNs for feature fusion, this allows Siamixformer to maintain a large receptive field and improve the accuracy of change detection and building detection tasks. The contributions of our work are as follows:
\begin{itemize}
	\item We introduced a novel siamese model, SiamixFormer, that leverages a fully-transformer architecture for semantic segmentation tasks involving bi-temporal input images, such as building detection and change detection from remote sensing data. Our model adopts transformer-based feature extraction and incorporates a temporal transformer for feature fusion, which explicitly accounts for the temporal relationship between features in  and  images. By doing so, SiamixFormer achieves superior performance in terms of accuracy compared to other state-of-the-art methods, thanks to its ability to maintain a large receptive field and capture both spatial and temporal dependencies in the data.
	\item We utilized bi-temporal images instead of mono-temporal images used by other existing models. Bi-temporal images offer additional information about buildings in the target area over time, enabling our model to capture and analyze pre- and post-disaster images more accurately. The use of bi-temporal images in our model enhances its ability to identify and segment buildings.
	\item Our model is superior to other state-of-the-art methods in terms of F1-score and IoU for building detection and change detection.
\end{itemize}

The structure of the paper is as follows. In section 2, we introduce our proposed model and its details. In section 3, we present the experimental results and compare the performance of our model with that of the state-of-the-art models. Finally, we conclude the paper in section 4.

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{images/siamix-2.jpg}
		\caption{Overview of the proposed SiamixFormer model that consists of four main modules: two encoders that extract features of  and  input images taken from the same place at two different times; a feature fusion module using a temporal transformer to fuse extracted features considering the temporal feature and their context; and a light-weight decoder. 
			Output of the model for building detection is a segmentation map that indicates the location of buildings in the  input image, and the output for change detection is a change map that highlights the differences between the  and  images.}
		\label{modelfig}
	\end{center}
\end{figure*} 

\section{Methodology}           
\label{sec2}
In this section, we introduce our proposed model, named SiamixFormer, which stands for Siamese mix transformer. First, we explain the data pipeline and the overall architecture of the model. After that, encoder, temporal transformer, and decoder architectures are described. Finally, the loss functions that we used for different datasets are explained.

\subsection{Overall architecture and data pipeline}
As shown in Figure \ref{modelfig}, two input images taken from the same place at two different times are given to the model. For building detection task, the output of the model is a segmentation map corresponding to the  image. The output of the model in change detection task is a change map indicating the changes happened between the two images.  
The SiamixFormer model consists of four main modules:
\begin{itemize}
	\item Encoder-\RN{1} dedicated to processing the  image.
	\item Encoder-\RN{2} dedicated to processing the  image.
	\item Feature fusion module.
	\item Decoder module.
\end{itemize} 
In our model, we use encoders that are consisting of four transformer stages with a hierarchical structure. As depicted in Figure~\ref{modelfig}, height and width of the output of subsequent transformer blocks decrease, while the number of channels increase. The output of the transformer blocks are fed to the feature fusion module as well.

Using bi-temporal images and the two parallel encoders helps us to obtain diverse features from the same place. Taking advantage of the temporal changes between the two input images and their context, the extracted features are fused in the feature fusion module using the temporal transformer. 
Each temporal transformer's output of the feature fusion module is given directly to the decoder, and a segmentation map is generated using a multilayer perceptron (MLP) network. In the building detection problem, the segmentation map highlights the buildings in  image, and in the change detection problem, it detects the changes between  and  images.


\subsection{Encoders}
We used the architecture of SegFormer encoders in our proposed method. The SegFormer architecture is an encoder-decoder model that has a hierarchical transformer structure. Six types of SegFormers exist according to the number of layers in its encoder. SegFormer-B0 is the smallest model for fast inference, and SegFormer-B5 is the largest model for the best performance \cite{xie2021segformer}. In a similar approach, our proposed model has six implementation types, i.e., SiamixFormer-0 to SiamixFormer-5.

The proposed encoder architecture has four transformer blocks and the output of each transformer block has the the dimensionality of  where \textit{H} and \textit{W} are respectively the height and width of the input image,   is the transformer block number, and  is number of channel in transformer block- while . This provides high-resolution coarse features and low-resolution fine-grained features that help semantic segmentation performance.

In each transformer block, CNN splits the input data into the desired patches with the specified patch size, stride, and padding, which helps the patches to overlap and maintain local continuity around the patches. Then these patches are flattened, and query, key, and value are generated. We use the method suggested in \cite{wang2021pyramid} to reduce computational complexity and generate the new query, key, and value. The formulas used for reducing the length of the sequences are given below:

 where \textit{R} is reduction ratio and \textbf{X} denotes the sequence to be reduced, \textit{i.e.}, \textbf{Q}, \textbf{K}, and \textbf{V}.  denotes reshape \textbf{X} to the one with the shape of .  denotes a linear layer that takes input with  channel and generates output with  channel. So we  have new \textbf{Q}, \textbf{K}, and \textbf{V} with size . This process reduces the computational complexity from  to , where . In the SiamixFormer, \textit{R} is set to [8,4,2,1] from transformer block-1 to transformer block-4. 

Each transformer block consists of many layers, and each layer consists of some heads. Table~\ref{table0} shows the number of layers and heads for each model in different transformer blocks. The new values of  \textbf{Q}, \textbf{K}, and \textbf{V} are given in parallel to each layer's heads. Heads use the self-attention module, which can be considered as follow:

where  is the head's dimensionality.

The outputs of the parallel heads are concatenated, and multi-head self-attention(MHA) module's output is generated. To consider positional encoding, a 3×3 convolution is used in two layers of MLP, and finally, the output of each layer is generated. Each layer's output is given to the next layer, and this procedure continues until the last layer in transformer block. These steps can be formulated as follows:

where  is layer's output, and GELU denotes Gaussian Error Linear Unit activation function \cite{hendrycks2016gaussian}, \textit{L} is number of layer in transformer block, \textit{MHA} is  multi-head self-attention module and MLP is fully connected layer.

\begin{table}[b!]
	\begin{center}
		\caption{Detailed settings of SiamixFormer's encoder. \textit{H}, \textit{L}, and \textit{C} denote the number of heads, the number of layers, and the number of channels in different transformer blocks, respectively.}
		\label{table0}
		\begin{tabular}{c|c|c|c|c}
			\hline
			\hline
			\textbf{model} & \textbf{block-1} & \textbf{block-2} & \textbf{block-3} & \textbf{block-4} \\
			\hline
			
			SiamixFormer-0 &
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			
			SiamixFormer-1 &
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			
			SiamixFormer-2&
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			
			SiamixFormer-3 &
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			
			SiamixFormer-4 &
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			
			SiamixFormer-5 &
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ } & 
			\makecell{ \\  \\ }   \\
			\hline
			\hline			
			
		\end{tabular}  
	\end{center}
\end{table}


\subsection{Temporal Transformer}
The temporal transformer is a critical component of the SiamixFormer model that enables accurate building and change detection in bi-temporal remote sensing images, as inspired by \cite{liu2022siamtrans}. The key innovation of this approach lies in its ability to fuse features while considering their temporal relationship, which is essential for bi-temporal tasks. In building detection, the  data stream helps the  data stream by providing contextual information from the second image, leading to more accurate building detection. In change detection, the temporal transformer acts as a differentiation module that focuses on the differences between the features extracted from  and  data streams, enabling the identification of changes between the two input images. 

Furthermore, the temporal transformer also preserves the large effective receptive field provided by the transformer encoder, which is crucial for capturing the spatial context of remote sensing images. This allows the model to consider a larger area of the image when making predictions, improving its ability to detect changes and buildings accurately. Additionally, the attention mechanism of transformers is leveraged in the temporal transformer to enable the model to selectively focus on the most informative features from both  and  data streams, leading to further improvements in bi-temporal tasks. Overall, the temporal transformer is a novel and powerful feature fusion technique that enhances the performance of the SiamixFormer model in bi-temporal remote sensing image analysis. The process of the temporal transformer can be formulated as:

where ,  denote output of  and  data stream transformer block-, respectively. Also  denotes output of the  temporal transformer.

\subsection{Decoder}\label{decsec}
Using a hierarchical transformer allows us to have large receptive fields, and hence we can use a lightweight decoder. In our method, first, the output of each temporal transformer in each stage is entered to one MLP module in order to unify the number of channels. Next, the outputs are upsampled to the same size of , and concatenated to fuse features from different stages. 
Finally, the concatenated feature maps are fed to the last MLP to generate the final output with the size of , where  denotes number of the classes. The decoder can be formulated as follows:

where MLP is fully connected layer, and Upsample, Concat denote upsampling and concatenating operators, respectively.

\subsection{Loss function}
SiamixFormer model was utilized for building detection and change detection problems. The datasets we used were imbalanced to varying degrees, resulted in models favoring majority classes in some cases. To overcome this issue, we employed various loss functions and weights for different datasets to ensure that our model learned from both minority and majority classes, resulting in more accurate and balanced predictions. The details are as follows:

\subsubsection{Weighted Cross-entropy}
To explain weighted cross-entropy, assume that the vectorized segmentation map is 
which can be represented as:

where  represents a pixel in the image. Weighted cross-entropy can be formulated as follows.

where  indicates class weights \cite{pihur2007weighted}.
\subsubsection{Focal Loss}
Focal loss is used for datasets that are imbalanced. It applies a modulating term to cross-entropy loss, and the loss value will increase for misclassified data \cite{lin2017focal}. Focal loss can be formulated as follows:

where  specifies the ground-truth class and  is the model’s estimated probability.   0 is a hyperparameter and selecting bigger  values, yields to reduced relative loss for well-classified samples.
\subsubsection{Dice Loss}
Dice coefficient is a metric widely used to calculate the similarity between two images. In segmentation problems, it can be adjusted as a loss function and formulated as follows: \cite{sudre2017generalised}:


We tested several loss functions and weights that were customized for each dataset. In our experiments, we found that the sum of dice and focal loss functions were the most effective for the LEVIR-CD and WHU datasets. For the CDD dataset, we employed a weighted cross-entropy loss, while for the xBD dataset, we used a standard cross-entropy loss. By selecting the optimal loss function for each dataset, we were able to improve the overall performance of our SiamixFormer model. It is worth noting that we evaluated multiple loss functions and only present the most successful ones here.

\section{Experimental Results}           
\label{sec3}
\subsection{Datasets and Evaluation}
In order to investigate the effectiveness of our proposed model on building detection and change detection problems, we conducted the experiments on two different datasets for each problem. The datasets are introduced in the sequel:
\subsubsection{xBD}
xBD is the largest publicly available dataset for building segmentation and disaster damage assessment. In this dataset, satellite images of 19 types of disasters such as earthquakes, floods, wildfires, and hurricanes with the size of 10241024 and a resolution of 0.8 m/pixel are collected. The dataset also includes more than 850,000 buildings with an area of more than 450,000  annotated. There are 18,336/1,866/1,866 images and 632,228/109,724/108,784 buildings in this dataset for train, validation, and test, respectively \cite{gupta2019creating}.
\subsubsection{WHU}
The WHU dataset includes aerial images from 2012 and 2016 of areas where a 6.3-magnitude earthquake occurred in 2011. This dataset is labeled for building detection and change detection. It contains 1260 pairs of images for train and 690 pairs of images for test in the size of 512512. There are also 12,796 buildings in 2012 and 16,077 buildings in 2016 in an area of 20.5  in this dataset. We have used this dataset for building detection by predicting the buildings in 2016 images \cite{ji2018fully} and Due to the GPU memory capacity limitation, we split the images to 256256 without overlap.


\begin{figure*}[t!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{images/compare_label.jpg}
		\caption{Labels of the CDD dataset, before and after pre-processing.}
		\label{compare_label}
	\end{center}
\end{figure*}

\subsubsection{LEVIR-CD}
This dataset contains 637 pairs of images in the size of  and a resolution of 0.5 m/pixel, collected from 20 different regions. It focuses on building-related changes, including building growth and building decline. LEVIR-CD covers various buildings such as villa residences, tall apartments, small garages, and large warehouses. This dataset contains 31,333 building changes, with an average of approximately 50 building changes per image and 987 pixels per image change. Due to the GPU memory capacity limitation, we split the images to 256256 without overlap, as suggested in \cite{chen2021remote}. Finally, we obtained 7120 images for train, 1024 images for validation, and 2048 images for test \cite{chen2020spatial}.

\subsubsection{CDD}
This dataset contains 11 pairs of satellite images with a resolution of 0.03 m/pixel to 1 m/pixel, collected in different seasons. Seven images have the size of 4,7252,200, and four images with the size of 1,9001,000, which have been clipped with the size of 256256. Therefore, we obtain 10,000 images for train, 3000 for validation, and 3000 test images \cite{lebedev2018change}. In this dataset, labels are images in jpg format, and their values are in the range of [0,255] instead of \{0,1\}. To handle this issue, we must consider a threshold and cluster the values into two classes, 0 and 1. By choosing any value for the threshold, some noise is produced in the label images, which makes the learning process challenging. We first considered all values greater than 0 as class 1, which creates much noise; then, we removed these noises by using erosion and dilation. Figure~\ref{compare_label} shows some examples of these images before and after this pre-processing.

\subsubsection{Evaluation metrics}
The experimental results of building detection are evaluated using the F1-score () and intersection over union () for only building class, which are defined as:

where TP, FP, and FN are the number of true-positive, false-positive, and false-negative pixel of segmentation result, respectively. Moreover, the experimental results of change detection are assessed using the mean of F1-score and IoU for all classes.

\begin{figure*}[b!]
	\begin{center}
		\includegraphics[width=\linewidth]{images/compare_xbd.jpg}
		\caption{Performance of different SiamixFormer models on the xBD dataset in building detection. In a) red rectangles show some buildings that even though were not labeled in the ground truth (GT) image, the SiamixFormer models succeeded to detect them correctly.}
		\label{compare_xbd}
	\end{center}
\end{figure*}

\subsection{Implementation Details}
We used MMSegmentation \cite{mmseg2020} codebase and trained the models on a single NVIDIA Geforce 1080Ti GPU. For the encoders, we used the pre-trained weights of the SegFormer trained on the ImageNet-1k dataset. The temporal transformer and decoder were randomly initialized. During training, we applied data augmentation through random resize with a ratio of 0.5-2.0 and random horizontal flipping. We also used random cropping with 512512 for the xBD dataset. The model was trained using an AdamW optimizer for 1M iteration. Due to memory capacity limitations, we used a batch size of 1 for all datasets. Learning rate was set to an initial value of  and a poly LR schedule with the default factor of 1.0 was used.
\begin{table}[b]
	\begin{center}
		\caption{Performance comparison of different method in building detection problem on xBD datasets.}
		\label{table1}
		\begin{tabular}{c|ccccc}
			
			\hline
			\hline
			Method & Input shape & Params(M) & FLOPs(G) &  &  \\
			\hline
			RAPNet  & 512512 & - & - &- &73.26  \\
			BDANet & 512512 & 34.4& 155.4&86.40 & -  \\
			DamFormer  & 512512 & - & - &86.86 & - \\
			
			\hline
			SiamixFormer-0 & 512512 & \textbf{10.04} & \textbf{10.53} & 86.46&76.43 \\
			SiamixFormer-1 & 512512 & 38.57& 27.71& 87.23&77.35  \\
			SiamixFormer-2 & 512512 & 60.65&  40.23& 88.05&78.66 \\
			SiamixFormer-3 & 512512 & 100.41&  63.34& 88.30&79.06 \\
			SiamixFormer-4 & 512512 & 133.95& 85.59& 88.35& 79.14  \\
			SiamixFormer-5 & 512512 & 175.15&  108.03& \textbf{88.43}& \textbf{79.26} \\
			\hline
			\hline			
			
		\end{tabular}  
	\end{center}
\end{table}

\begin{figure*}[t!]
	\begin{center}
		\includegraphics[width=\linewidth]{images/compare_whu.jpg}
		\caption{Performance of different SiamixFormer models on the WHU dataset in building detection problem.}
		\label{compare_whu}
	\end{center}
\end{figure*}

\begin{table}[t!]
	\begin{center}
		\caption{Performance comparison of different method in building detection problem on  WHU datasets.}
		\label{table2}
		\begin{tabular}{c|ccccc}
			
			\hline
			\hline
			Method & Input shape & Params(M) & FLOPs(G) &  &  \\
			\hline
			DTCDSCN & 256256 & -& -&- & 88.86\\
			FSAU-Net & 256256 & -& -&- & 91.73\\
			
			\hline
			SiamixFormer-0 & 256256 & \textbf{10.04} & \textbf{2.62} & 95.53 &91.43 \\
			SiamixFormer-1 & 256256 & 38.57&6.91 & 96.01& 92.31 \\
			SiamixFormer-2 & 256256 & 60.65& 10.06& 96.22& 92.70\\
			SiamixFormer-3 & 256256 & 100.41& 15.82& 96.32& 92.90\\
			SiamixFormer-4 & 256256 & 133.95&21.4 & 96.59&93.40 \\
			SiamixFormer-5 & 256256 & 175.15& 27.01&  \textbf{96.69}&\textbf{93.58} \\
			\hline
			\hline			
			
		\end{tabular}  
	\end{center}
\end{table}

\subsection{Comparison and Analysis}
In this section, we compare the performance of our SiamixFormer model in building detection and change detection with other existing deep learning (CNN-based or transformer-based) models.

\subsubsection{Building Detection}
We considered the following models for building detection:
\begin{itemize}
	\item \textbf{BDANet} \cite{shen2021bdanet}: introduced for building damage assessment, uses a UNet-based model and only pre-disaster images for building detection.
	\item \textbf{RAPNet} \cite{tian2021multiscale}: uses a combination of atrous convolution (AC), deformable convolution (DC), pyramid pooling module (PPM), FPN, and attention mechanism (AM) for building detection.                                                                                                                                                        
	\item \textbf{DamFormer} \cite{chen2022dual}: is based on the SegFormer architecture. It uses CBAM \cite{woo2018cbam} for feature fusion and both pre-disaster and post-disaster images for building detection and building damage assessment.
	\item \textbf{DTCDSCN} \cite{liu2020building}: proposed for change detection. It consists of three parts: two parts perform semantic segmentation, and one part performs change detection. This model has been used for building detection using two inputs on the WHU dataset.
	\item \textbf{FSAU-Net} \cite{hu2023fsau}: proposes a features self-attention U-block network (FSAU-Net) that focuses on the target feature self-attention in the coding stage and introduces spatial attention in the decoder stage to highlight building information areas.
\end{itemize}

Unlike BDANet and RAPNet models, our SiamixFormer model uses two inputs for building detection. As shown in Table \ref{table1}, using pre-disaster and post-disaster images can improve the model's performance in terms of  and  metrics. This is because the number of destroyed and major-damaged buildings in post-disaster images is small compared to no-damaged and minor-damaged buildings, so using post-disaster images can help detect buildings better. On the other hand, using two inputs can help train the models with large receptive fields better. This way, we can maintain the large receptive field and properly fuse the extracted features. Table \ref{table1} shows that the SiamixFormer model using the temporal transformer in the feature fusion section obtained better results compared with the DamFormer model, which uses CBAM for feature fusion. In Figure \ref{compare_xbd}, the qualitative results of different SimixFormer models on the holdout images of the xBD dataset are presented.

\begin{figure*}[b!]
	\begin{center}
		\includegraphics[width=\linewidth]{images/compare_levir.jpg}
		\caption{Performance of different SiamixFormer models on the Levir-CD dataset in change detection problem.}
		\label{compare_levir}
	\end{center}
\end{figure*}


\begin{table}[b]
	\begin{center}
		\caption{Performance comparison in change detection task on LEVIR-CD dataset.}
		\label{table3}
		\begin{tabular}{c|ccccc}
			\hline
			\hline
			
			Method & Input shape & Params(M) & FLOPs(G) &  &  \\
			\hline
			UVACD  & 256256& -& -& 91.30 & 83.98 \\
			FDOR-Net  & 256256& -& -& 90.85 & 91.13 \\
			FTN  & 256256& - & 45& 91.01 & 83.51 \\
			SNUNet & 256256 & 27.06 & 250 &88.16 &78.83 \\
			BIT  & 256256 & \textbf{3.55}&4.35 &89.31 &80.68  \\
			BIT(IMP-ViTAEv2-S) & 256256& -& -& 91.26&-\\
			ChangeFormer  & 256256& 29.75 & 21.19& 90.40 &82.48 \\
			\hline
			SiamixFormer-0 &256256 & 10.04 &  \textbf{2.62}  &89.47&82.29  \\
			SiamixFormer-1 &256256 & 38.57&6.91 &   89.29&82.03  \\
			SiamixFormer-2 &256256 & 60.65& 10.06&   90.57&83.88 \\
			SiamixFormer-3 &256256 & 100.41& 15.82&   90.54&83.84 \\
			SiamixFormer-4 &256256 & 133.95&21.4 &   90.70 & 84.05   \\
			SiamixFormer-5 &256256 & 175.15& 27.01&    \textbf{91.58} & \textbf{85.38}\\
			\hline
			\hline			
		\end{tabular}  
	\end{center}
\end{table}

\begin{table}[t]
	\begin{center}
		\caption{Performance comparison in change detection task and CDD dataset.}
		\label{table4}
		\begin{tabular}{c|ccccc}
			\hline
			\hline
			Method & Input shape & Params(M) & FLOPs(G) &  &  \\
			\hline
			DSAMNET &256256 & -&- &93.69 & 88.13 \\
			SNUNet & 256256& 27.06& 250 &96.2 & - \\
			BIT  & 256256 & \textbf{3.55}&4.35 &90.73 &83.03  \\
			BIT(IMP-ViTAEv2-S) &256256 &- &- &97.02&-\\
			ChangeFormer  & 256256& 29.75 & 21.19& 89.38 &80.79 \\
			\hline
			SiamixFormer-0 &256256 &10.04 &  \textbf{2.62} &92.05&85.81 \\
			SiamixFormer-1 &256256 & 38.57&6.91  &92.78& 87.00 \\
			SiamixFormer-2 &256256 & 60.65& 10.06 &95.52& 91.62\\
			SiamixFormer-3 &256256 & 100.41& 15.82 &96.48&93.33\\
			SiamixFormer-4 &256256 & 133.95&21.4  &96.85 &94.00 \\
			SiamixFormer-5 &256256 & 175.15& 27.01 &\textbf{97.13}& \textbf{94.51}\\
			\hline
			\hline			
		\end{tabular}  
	\end{center}
\end{table}

\begin{figure*}[t!]
	\begin{center}
		\includegraphics[width=\linewidth]{images/compare_cdd.jpg}
		\caption{Performance of different SiamixFormer models on the CCD dataset in change detection problem.}
		\label{compare_cdd}
	\end{center}
\end{figure*}



In order to show the robustness of our proposed model in building detection, we also evaluate model on the WHU dataset. This dataset is usually used for change detection, but the DTCDSCN and FSAU-Net models used this dataset for building detection and change detection. As can be seen in Table \ref{table2}, the SiamixFormer model has also achieved outstanding results on this dataset. Figure \ref{compare_whu} shows the qualitative results of different SimixFormer models on some sample images from the test set of the WHU dataset.

\subsubsection{Change Detection}
We considered the following models to compare with our proposed model in change detection problem:
\begin{itemize}
	\item \textbf{DSAMNET} \cite{shi2021deeply}: fuses different levels of extracted features using metric module and CBAM.
	\item \textbf{SNUNet} \cite{fang2021snunet}: based on NestedUNet, fuses the extracted features using the ensemble channel attention module.
	\item \textbf{BIT} \cite{chen2021remote}: uses the CNN backbone and converts the extracted features into semantic tokens. Then, passed the tokens to transformer encoder and generates a change map with transformer decoder. Feature differencing module is the last stage.
	\item \textbf{BIT(IMP-ViTAEv2-S)} \cite{wang2022empirical} : used BIT structure with ViTAEv2-S backbone.
	\item \textbf{ChangeFormer} \cite{bandara2022transformer}: is based on SegFormer and concatenates the features extracted from each stage for feature fusion.
	\item \textbf{UVACD} \cite{wang2022network}: fuses the features extracted from the CNN backbone using a visual transformer.
	\item \textbf{FDOR-Net} \cite{ye2022feature}: The method is based on a Feature Decomposition-Optimization-Reorganization Network (FDOR-Net), which decomposes the input image into different feature maps and optimizes them for building change detection. The network then reorganizes the feature maps to generate a change detection map.
	\item \textbf{FTN} \cite{yan2022fully}: The paper proposes a method that utilizes the Swin transformer network as an encoder and a pyramid structure grafted with a Progressive Attention Module (PAM) as a CNN based feature fusion module.
\end{itemize}

\begin{figure*}[b!]
	\begin{center}
		\includegraphics[width=\linewidth]{images/compare_with_other.jpg}
		\caption{Compare result of SiamixFomer-5 with BIT and ChangeFormer on LEVIR-CD dataset.}
		\label{compare_images}
	\end{center}
\end{figure*}
Most of the models introduced for change detection problem use two decoders to generate a segmentation map for each image and afterwards use another module to detect the changes. These modules analyze the difference between the two segmentation maps. However, we used our SiamixFormer model for change detection without any alteration to the model that was designed for building detection. As shown in Table \ref{table3} and \ref{table4}, the SiamixFormer model achieved promising results on both LEVIR-CD and CDD datasets compared with the other methods. Figures \ref{compare_levir} and \ref{compare_cdd} show the qualitative results of different SimixFormer models on some samples of the test sets of these datasets. To compare the qualitative results of our model with other models, we compared the results of SiamixFormer-5 with models for which model weights are available, such as BIT and ChangeFormer. The comparative results on the LEVIR-CD dataset are shown in the Figure \ref{compare_images}.



A notable point about the SiamixFormer model is that the temporal transformer, which is used for feature fusion in the model, fuses the features in building detection tasks in a way that the  stream outputs assist the  stream in correctly detecting buildings. On the other hand, in change detection tasks, this module fuses features of the two streams in such a way that the difference between them is given to the decoder.

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{images/erf.jpg}
		\caption{Comparison effective receptive field of each stage's output between SiamixFormer-2 and ChangeFormer.}
		\label{erf}
	\end{center}
\end{figure*}

\subsection{Discussion}
In order to analyze the usefulness of different parts of the proposed architecture, we investigated the effects of using bi-temporal images and using the temporal transformers at different stages of the feature fusion module in experiments conducted on the xBD dataset. As shown in Table \ref{table5}, the SiamixFormer-5 can improve  the  metric compared with the SegFormer-B5. This confirms the effect of using pre-disaster and post-disaster images in improved building detection. In another attempt, we used a CNN model to extract features, a temporal transformer (TT) for feature fusion and SegFormer-B5 architecture. The SiamixFormer-5 could achieve  better  than this model. This shows that using bi-temporal inputs can help the model if the encoder module can produce features with large receptive fields and the feature fusion module can maintain large receptive fields.

Figure \ref{erf} compares the effective receptive field resulting from the output of each stage between Changeformer and SiamixFormer-2 models on the test set of the LEVIR-CD dataset. Both models have almost used segformer as an encoder, and their difference is in feature fusion. Nevertheless, the SiamixFormer-2 model has preserved a larger effective receptive field than ChangeFormer.


\begin{table}[h!]
	\begin{center}
		\caption{Ablation study on the effects of using pre-disaster and post-disaster images and using the temporal transformers (TT) at stages of the feature fusion module. Experiments were conducted on the xBD dataset.} 
		\label{table5}
		\begin{tabular}{c|cc|c}
			\hline
			\hline
			Method & pre-disaster & post-disaster &  \\
			\hline
			SegFormer-B5 & \checkmark &  & 86.19 \\
			CNN + TT + SegFormer-B5 & \checkmark & \checkmark & 82.82 \\
			SiamixFormer-5 & \checkmark & \checkmark & \textbf{88.43} \\
			\hline
			\hline			
		\end{tabular}  
	\end{center}
\end{table}
   

\section{Conclusions}           
\label{sec4}
In this paper, we proposed a fully-transformer model with a hierarchical architecture named SiamixFormer. The model was used for building detection and change detection on four different datasets. The SiamixFormer model uses two SegFormer-based encoders with four stages, each receiving one of the bi-temporal images and extracting features with large receptive fields. In each stage, the output of the encoders, in addition to being given to the next stage, is also given to the temporal transformer of the same stage. The temporal transformer is used for feature fusion and achieves this target by exploiting temporal features. Moreover, the temporal transformer can maintain large receptive fields. Experimental results show that using bi-temporal images and temporal transformers in feature fusion can improve the model's performance in both building detection and change detection. One limitation of our proposed method is the number of model parameters, particularly in the SiamixFormer-5 architecture, which may require a larger GPU memory. In future work, we plan to focus on optimizing the model from this perspective, while also exploring how the proposed model can be applied to building classification in building damage assessment problems.


\bibliographystyle{unsrt}  
\bibliography{references}  





\end{document}
