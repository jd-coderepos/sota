\documentclass[a4paper,10pt,reqno]{amsart}
\usepackage{amsopn}
\usepackage{color}
\usepackage[square,numbers,comma,sort&compress]{natbib}
\usepackage{enumerate}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{ifpdf}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{mathrsfs}  
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[breaklinks=true,colorlinks,citecolor=blue]{hyperref}
\usepackage{threeparttable}
\usepackage{morefloats}

\usepackage{natbib}



\newcommand{\assign}[1]{\textcolor{red}{\textsf{---[#1]}}}

\textwidth = 6in
\textheight = 9in
\hoffset = -0.5in
\voffset = -0.25in

\def\ve#1{\mathchoice{\mbox{\boldmath}}
{\mbox{\boldmath}}
{\mbox{\boldmath}}
{\mbox{\boldmath}}}
\newcommand\vealpha{{\boldsymbol{\alpha}}}
\newcommand\vebeta{{\boldsymbol{\beta}}}
\newcommand\velambda{{\boldsymbol{\lambda}}}
\newcommand\vehatlambda{{\boldsymbol{\hat\lambda}}}
\newcommand\vetildelambda{{\boldsymbol{\tilde\lambda}}}
\newcommand\vemu{{\boldsymbol{\mu}}}
\newcommand\Z{\mathbb Z}  
\newcommand\N{\mathbb N}  
\newcommand\R{\mathbb R}  
\newcommand\Q{\mathbb Q}  
\newcommand\Po{\mathcal P}
\newcommand\Co{\mathcal C}
\newcommand\Tr{\mathcal T}
\newcommand\In{\mathcal I}
\newcommand\V{\mathcal V} 
\DeclareMathOperator{\ind}{ind}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\td}{td}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\interior}{relint}
\DeclareMathOperator{\closure}{cl}
\DeclareMathOperator{\diagonal}{diag}
\DeclareMathOperator{\conv}{conv}  
\DeclareMathOperator{\vol}{vol}    
\DeclareMathOperator{\midpoint}{midpoint}
\DeclareMathOperator{\supp}{supp}        
\DeclareMathOperator{\affine}{aff}       
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\cl}{cl}      
\DeclareMathOperator{\lin}{lin}    
\DeclareMathOperator{\argmax}{argmax}    
\DeclareMathOperator{\argmin}{argmin}    
\DeclareMathOperator{\vertices}{vert}    
\DeclareMathOperator{\IDval}{IDval}    
\DeclareMathOperator{\IDvalInit}{IDvalInit}    
\DeclareMathOperator{\IDtoIDmap}{IDtoIDmap}    
\DeclareMathOperator{\curMaxID}{curMaxID}    
\DeclareMathOperator{\oldCurMaxID}{oldCurMaxID}    
\DeclareMathOperator{\orderedID}{orderedID}    
\DeclareMathOperator{\insertPointOrderedID}{insertPointOrderedID}    
\DeclareMathOperator{\IDmap}{IDmap}    
\let\boundary=\partial
\let\epsilon=\varepsilon
\usepackage{ifthen}
\makeatletter
\newcommand{\DeclareBracket}[3]{
  \newcommand{#1}[2][]{\ifthenelse {\equal{##1}{}}{\left#2##2\right#3}{\csname ##1l\endcsname#2##2\csname ##1r\endcsname#3}}}    
\makeatother
\DeclareBracket\abs||           \DeclareBracket\norm\|\|        \DeclareBracket\floor\lfloor\rfloor
\DeclareBracket\ceil\lceil\rceil
\DeclareBracket\set\{\}
\DeclareBracket\paren()
\DeclareBracket\inner\langle\rangle
\DeclareBracket\fractional\{\}
\definecolor{purple}{RGB}{160,32,240}       \definecolor{darkyellow}{RGB}{190,190,0}       \definecolor{darkgreen}{RGB}{0,120,0}       \definecolor{darkblue}{RGB}{0,0,180}       \newcommand{\davecomment}[1]{ {\textcolor{darkgreen}{ #1  --David}}} \newcommand{\cractional}[1]{\left\{\!\left\{#1\right\}\!\right\}}
\newcommand{\normall}{\mathopen} \newcommand{\normalr}{\mathclose}
\newcommand\C{\mathbb C}
\newcommand\inputfig[1]{\ifpdf
    \input{#1.pdf_t}
    \else
    \input{#1.pstex_t}
    \fi}
\newenvironment{notes}{\ttfamily\raggedright}{\par}
\renewcommand{\thetable}{\Roman{table}}
 \newtheorem{theorem}{Theorem}\makeatletter
 \newtheorem{lemma}{Lemma}
 \renewcommand*{\c@lemma}{\c@theorem}
 \renewcommand*{\p@lemma}{\p@theorem}
 \renewcommand*{\thelemma}{\thetheorem}
 \newcommand{\lemmaname}{Lemma}
\newtheorem{conjecture}{Conjecture}
 \renewcommand*{\c@conjecture}{\c@theorem}
 \renewcommand*{\p@conjecture}{\p@theorem}
 \renewcommand*{\theconjecture}{\thetheorem}
 \newcommand{\conjecturename}{Conjecture}
\newtheorem{proposition}{Proposition}
 \renewcommand*{\c@proposition}{\c@theorem}
 \renewcommand*{\p@proposition}{\p@theorem}
 \renewcommand*{\theproposition}{\thetheorem}
 \newcommand{\propositionname}{Proposition}
\newtheorem{corollary}{Corollary}
 \renewcommand*{\c@corollary}{\c@theorem}
 \renewcommand*{\p@corollary}{\p@theorem}
 \renewcommand*{\thecorollary}{\thetheorem}
 \newcommand{\corollaryname}{Corollary}
\newtheorem{observation}{Observation}
 \renewcommand*{\c@observation}{\c@theorem}
 \renewcommand*{\p@observation}{\p@theorem}
 \renewcommand*{\theobservation}{\thetheorem}
 \newcommand{\observationname}{Observation}
 \theoremstyle{definition}
\newtheorem{problem}{Problem}
 \renewcommand*{\c@problem}{\c@theorem}
 \renewcommand*{\p@problem}{\p@theorem}
 \renewcommand*{\theproblem}{\thetheorem}
 \newcommand{\problemname}{Problem}
\newtheorem{definition}{Definition}
 \renewcommand*{\c@definition}{\c@theorem}
 \renewcommand*{\p@definition}{\p@theorem}
 \renewcommand*{\thedefinition}{\thetheorem}
 \newcommand{\definitionname}{Definition}
\newtheorem{remark}{Remark}
 \renewcommand*{\c@remark}{\c@theorem}
 \renewcommand*{\p@remark}{\p@theorem}
 \renewcommand*{\theremark}{\thetheorem}
 \newcommand{\remarkname}{Remark}
\newtheorem{example}{Example}
 \renewcommand*{\c@example}{\c@theorem}
 \renewcommand*{\p@example}{\p@theorem}
 \renewcommand*{\theexample}{\thetheorem}
 \newcommand{\examplename}{Example}
\renewcommand*{\c@algorithm}{\c@theorem}
\renewcommand*{\p@algorithm}{\p@theorem}
\renewcommand*{\thealgorithm}{\thetheorem}
\newcommand{\algorithmname}{Algorithm}
\title[QuickLexSort]{QuickLexSort: An efficient algorithm for lexicographically sorting nested restrictions of a database}
\author[]{David Haws\\IBM T.J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights, NY 10598, USA}
\email{dhaws@us.ibm.com, dchaws@gmail.com}
\date{}
\begin{document}



\begin{abstract}
Lexicographical sorting is a fundamental problem with applications to
contingency tables, databases, Bayesian networks, and more. A standard method
to lexicographically sort general data is to iteratively use a stable sort -- a
sort which preserves existing orders. Here we present a new method of
lexicographical sorting called QuickLexSort.  Whereas a stable sort based
lexicographical sorting algorithm operates from the least important to most
important features, in contrast, QuickLexSort sorts from the most important to
least important features, refining the sort as it goes.  QuickLexSort first
requires a one-time modest pre-processing step where each feature of the data set is
sorted independently.  When lexicographically sorting a database, QuickLexSort
(including pre-processing) has comparable running time to using a stable sort
based approach.  For a data base with  rows and  columns, and a sorting
algorithm running in time , a stable sort based lexicographical
sort and QuickLexSort will both take time .  However in many
applications one has the need to lexicographically sort nested data, e.g.\ all
possible sub-matrices up to a certain cardinality of columns. In such cases we
show QuickLexSort gives a performance improvement of a log factor
of the database length (rows in matrix) over using a standard stable sort based
approach. E.g.\ to sort all sub-matrices up to cardinality , QuickLexSort
has running time  whereas a stable sort based lexicographical sort
will take time .  After the pre-processing step that is run only
once for the entire matrix, QuickLexSort has a running time linear in the
number of nested sub-matrices to sort. We conclude with an application to
Bayesian network scoring to detect epistasis using SNP marker data.

\end{abstract}

\maketitle
\footnotetext{\today}
\footnotetext{dhaws@us.ibm.com, dchaws@gmail.com}


\section{Introduction}
\label{sec:introduction}
Lexicographical ordering is a method to sort a list of elements where each
element has multiple features, such as a vector, provided one has an order for
each feature. Lexicographic ordering is also known as dictionary or
alphabetical ordering. Put simply, the lexicographical ordering places the
elements in a sequence such that: elements are ordered according to the first
feature, any ties are broken by the second feature, any ties are broken by the
third feature, etc. Lexicographical sorting is a fundamental problem with
applications to contingency tables, Bayesian networks,
databases\cite{poess2003data,lemire2010sorting}, and more. A contingency table
lists the frequency of each element present in the data. For example, given a
matrix, one can form a contingency table which for each unique row, counts the
number of equal rows in the data. A naive approach would loop through each row,
then again loop through the rows and count the number of equal rows. A better
approach would be to sort all the rows of the matrix lexicographically, then
loop through the matrix one last time forming the counts for the contingency
table. Contingency tables formed from a matrix are used in the learning of
Bayesian networks, as well as other applications.

Traditionally, one sorts the rows of a matrix lexicographically by iteratively
applying a stable sort -- a sorting algorithm which preserves the original
order of elements that are equal. The rows of the matrix  are stable sorted by
the least important feature, the next to least important feature, etc. If  and the stable sorting algorithm runs in time , then
the time to sort the matrix is . If the stable sort is a comparison
base sort then  is bounded below by
\cite{Cormen:2001fk}. In many applications, such as learning
Bayesian networks, one not only wants to sort the rows of a data matrix ,
but also sort the rows of  restricted to a sequence of columns. 


We present \emph{QuickLexSort} which can efficiently sort a database (rows of a
matrix) restricted to any sequence of features (columns of a matrix). Moreover,
QuickLexSort is designed to quickly sort a nested set of restrictions by
features. For example, one may wish to lexicographically sort all possible
sub-matrices given by all non-empty sets of columns up to a specific
cardinality. QuickLexSort first requires a modest pre-processing step where
each feature of the data set is sorted independently.  When lexicographically
sorting the rows of a single matrix, QuickLexSort (including pre-processing)
has comparable running time to using a stable sort based approach.  However,
when sorting a set of nested sets of features we show QuickLexSort gives a
performance improvement of a multiple of a log factor of the database length
(rows in matrix) over using a standard stable sort based approach. That is,
after the pre-processing step, QuickLexSort has a running time linear in the
number of nested sub-matrices to sort. The pre-processing step need only be
computed once for the entire matrix and not for each sub-matrix. 

The article is organized as follows. In \autoref{sec:background} we
give background details on lexicographical sorting and the stable sorting
approach. In \autoref{sec:qls} we present QuickLexSort and prove the validity and space and
running time. In \autoref{sec:sortingsubmat} we show how
QuickLexSort can be used to efficiently sort nested restrictions of databases.
In \autoref{sec:experiments} we present a small experiment verifying
computationally the advantage of QuickLexSort over a stable sort based approach
to lexicographically sorting a nested set of data base restrictions. In
\autoref{sec:adtrees} we briefly compare QuickLexSort with AD-trees. In
\autoref{sec:bn} we give details on applications of QuickLexSort to scoring and
learning Bayesian networks.  Moreover, we perform experiments showing the
validity to using QuickLexSort and a Bayesian scoring approach to detecting
epistasis in biological SNP data.  Finally, in the Appendix in \autoref{sec:appendix} we
present an augmented version of QuickLexSort which provides a more natural
encoding of the lexicographical ordering.





\section{Background}
\label{sec:background}
Sorting is the method of rearranging a sequence of items such that they are
placed with respect to some order. Here we consider total or linear orderings.
A set follows a \emph{total ordering}, given by the symbol '', if the following 
three conditions hold:
\begin{enumerate}
    \item If  and  then , (antisymmetry)
    \item If  and  then , (transitivity)
    \item  or  (totality).
\end{enumerate}
Here we primarily focus on comparison sorting, a class of sorting
algorithms which only uses a binary comparison operation. That is, the only
information used to sort is the given comparison operation. It is known that the
optimal running time of comparison sort is bounded below by 
\cite{Cormen:2001fk}. Some non-comparison sorting algorithms, such as bucket
sort, may run in linear or near linear time, depending on the data.
The exposition here will focus on real valued data and the standard ordering.
However, the results extend naturally to any data with some type of comparison
operation.

Let  be a real matrix where we index rows and columns by
 and  respectively. 

\begin{definition}[\bf Lexicographic Order]
We say  is less
than  \emph{lexicographically}, denoted , if 
\begin{enumerate}
    \item  or 
    \item  and  for some . 
\end{enumerate}
\end{definition}
For example .  If , lexicographical
order is equivalent to the normal ordering.  The task here is to sort
the rows of sub-matrices of  (obtained by taking subsets of the columns and
all rows of ) using lexicographical order. Here we point out the distinction
of a \emph{set} and a \emph{sequence}, both are groups of objects where in the
former the ordering of the object is irrelevant and in the latter the ordering
matters. That is  and  are distinguishable as sequences but
not as sets.  Typically we will use ``\{\}'' to denote sets and ``()'' to
denote sequences.

By lexicographical sorting of  we mean the
lexicographical sorting of the rows (vectors) of  assuming the original
ordering of columns of . By lexicographical sorting of  restricted to the
set of columns  we mean the lexicographical sorting of the sub-matrix of 
given by the set of columns  where the original order of the columns is preserved.
By lexicographical sorting of  restricted to the sequence of columns  we
mean the lexicographical sorting of the sub-matrix of  given by the sequence
of columns  where the order of the columns is take from .

\begin{example}
Consider a matrix , the lexicographical sorting () of ,
the lexicographical sorting () of the sub-matrix of  given by the set
of columns , and the lexicographical sorting () of the
sub-matrix of  given by the sequence of columns . In the following,
we write the row and column indices of the original matrix  on the 
left and top of the matrix respectively.

\end{example}

We note that all algorithms presented here do not modify the original
data matrices, and simply represent the ordering via certain vectors
which we define below. Note though, that in the examples we do reorder
the rows to better illustrate certain concepts, although we are careful
to preserve the original matrix row indices on the left.

\begin{definition}[\bf Ranking Vector]
Let  be a sub-matrix of . 
The unique \emph{ranking vector}  of some 
ordering  of rows of  is a vector such that
\begin{enumerate}
    \item ,
    \item if row  of  is equal to row  in the ordering , then ,
    \item if row  of  is less than row  in the ordering , then ,
    \item  is minimal.
\end{enumerate}
\end{definition}
The last item guarantees the ranking vector  is unique. We say ranking
vector  is a \emph{refinement} of ranking vector  if 
implies  for all .
Intuitively, the  rows of  are sorted lexicographically and thus form
 blocks, where , every row in a block is lexicographically equal,
and the  blocks are in increasing lexicographic order. In this sense, 
is the block index in which row  resides.

In what follows we consider a ranking vector sufficient information to describe
an ordering. However, it may be desirable to have an alternative data structure
to describe the ordering, such as an ordered list of row indices giving the
smallest to largest row vectors. We describe such a case and the appropriate 
modifications to our algorithms in the Appendix in \autoref{sec:appendix}.
Our modified algorithm has the same time and space complexity.






\begin{example}

A matrix  with row indices  and column indices
. The matrix  gives the rows of  sorted
lexicographically,  gives the rows of  restricted to columns
 sorted lexicographically, and  gives the rows of 
restricted to columns  sorted lexicographically.  The ranking vectors
of the lexicographic orderings shown in , , and  are  Note the ranking vectors refer to the original
row indices of the matrix .  Note that  is a refinement of .
\label{ex:D}
\end{example}
    
\subsection{Stable Sort}
\begin{definition}[\bf Stable Sort]
A sorting algorithm is \emph{stable} if it maintains the relative order of
items with equal value. That is, if  comes before  in the original input
and , then a stable sorting algorithm orders  before .
\end{definition}

\begin{example}
Suppose we performed a stable sorting of the rows of  where we use only the values in
column  to perform the sort.  We preserve the order of all rows which have
the same value in column , and get  below.

Although rows  and  have repeated values in column , a stable sorting
algorithm places row  before row , preserving the original
ordering.
\end{example}

A stable sorting algorithm can be used iteratively to perform
lexicographical sorting. When a stable sorting algorithm is used to do
lexicographical sorting we will refer to it as \emph{StableLexSort}.
See \autoref{alg:stablelex} below.

\begin{algorithm}
\begin{algorithmic}[1]
    \REQUIRE ,  where  ,  a stable sorting algorithm.
    \ENSURE  a lexicographic sorting of rows of matrix .
    \STATE Let .
    \FOR{}
        \STATE Sort rows of  using the stable sorting algorithm  and values in column .
    \ENDFOR
    \RETURN 
\end{algorithmic}
\caption{StableLexSort: Lexicographic sort using stable sort.}
\label{alg:stablelex}
\end{algorithm}

\begin{example}
We give an example of \autoref{alg:stablelex} with input  and .
First stable sort the rows  by the values in column . Further stable sort
the rows by values in column .  Repeat stable sort of the rows by values in
column , then , and finally
.

\end{example}

\begin{proposition}
If , and the running time of the stable sort algorithm
 is , then \autoref{alg:stablelex} sorts in time .
\end{proposition}

Thus if the stable sort algorithm  is a comparison sort, then the running
time of \autoref{alg:stablelex} is bounded below by .
For example, if Merge sort \cite{Knuth:1998fk} was used, which has running time
of , then the running time of StableLexSort on  would be
.

\section{QuickLexSort}
\label{sec:qls}
Here we present a new algorithm for lexicographical sorting called
\emph{QuickLexSort}.  We will show that the running time of QuickLexSort is
comparable to StableLexSort when sorting a single matrix. Moreover, we will
demonstrate that QuickLexSort is considerably faster than StableLexSort when
performing multiple lexicographic sorts of related sub-matrices.

The proposed algorithm QuickLexSort first requires each column of  to be
independently sorted and stored. The results are stored in the matrix  where the th column  of  stores the row indices
 after sorting the th column  of .  


\begin{example}

A matrix  and the matrix  storing the sort of the columns of 
described above. E.g., reading down the th column of Q, for column  of , the smallest
entry is in row , followed by row , followed by row , etc.
\end{example}

The QuickLexSort algorithm sorts (conceptually) by iteratively appending 
columns to the current matrix and sorting, until the desired sequence of columns is reached. That is,
\autoref{alg:qlsrefine} refines the current sort with respect to the sequence of columns
 to give a sort with respect to the sequence of columns
. In some sense this is opposite of StableLexSort.  In
StableLexSort one stable sorts from the least important column to the most
important. In QuickLexSort one sorts from the most important column to the
least important, refining the ranking vector as it goes.

\autoref{alg:qlsrefine} (QuickLexSortRefine), is the core of the methods described here.
\autoref{alg:qlsrefine} takes as input the matrix  [], the sorting of the
columns of  [], the column to refine  by [], and the
current ranking vector []. It returns the refined ranking vector . That
is, if the input ranking vector  represents the sorting of the rows of 
(restricted to some sequence of columns), the returned ranking vector 
represents the refined sorting where we consider appending the  column of
. Again, if the input ranking vector  represents the lexicographical
sorting of a matrix

the ranking vector  output from \autoref{alg:qlsrefine} represents the lexicographical sorting of the matrix


\begin{algorithm}[!h]
\begin{algorithmic}[1]
    \REQUIRE , , , .
    \ENSURE .
    \STATE . 
    \STATE . \# Records most recent value in  w.r.t.\ ID.
    \STATE .
    \STATE . \# Records subID of .
    \STATE . \# Records count of refinements of each input ID of L.
    \FOR{  }
        \IF{ } \STATE .
            \STATE .
        \ELSE
            \IF{ }
                \STATE .
                \STATE .
            \ENDIF
        \ENDIF
        \STATE .
    \ENDFOR
    \STATE .
    \STATE .
    \FOR{  }
        \STATE .
    \ENDFOR
    \FOR{ }
        \STATE .
    \ENDFOR
    \RETURN .
\end{algorithmic}
\caption{QuickLexSortRefine}
\label{alg:qlsrefine}
\end{algorithm}

Intuitively, the task of \autoref{alg:qlsrefine} is to 1) preserve the current
ordering, i.e.\ if row  was lexicographically smaller than row , then
this is true in the new order, 2) all previous rows that were lexicographically
equal should be sorted given the newly appended column . The novelty of
\autoref{alg:qlsrefine} is that it performs the second item above in linear
time using the pre-computed ordering of the newly appended column . 

\begin{example}
Consider matrix  in \autoref{ex:D}. Suppose  is the ranking vector of the
lexicographical ordering of the sub-matrix given by columns  of 
and we then perform \autoref{alg:qlsrefine} with . Thus, part of the input would be 


The progression of the vectors , , and  are shown from
left to right as the for loop on line 6 goes from  to , Note, ``''
signifies unassigned values.



\end{example}

Next we present \autoref{alg:qls} (QuickLexSort) which, we will prove,
lexicographically sorts a sub-matrix restricted to a sequence of columns of
, requiring  an initial sorting of the columns of .  


\begin{algorithm}[!h]
\begin{algorithmic}[1]
    \REQUIRE , ,  where  .
    \ENSURE .
    \STATE .
    \FORALL{ }
        \STATE .
    \ENDFOR
    \RETURN .
\end{algorithmic}
\caption{QuickLexSort}
\label{alg:qls}
\end{algorithm}

We now prove the validity and running times of \autoref{alg:qlsrefine}
and \autoref{alg:qls}. 

\begin{lemma} 
\label{lem:qlsrefine}
If ,  where the th column of
 stores the sorting of the th column of
,  is the ranking vector of the lexicographical sorting of the
sub-matrix of  determined the sequence of columns
, and , then 
\autoref{alg:qlsrefine} returns the ranking vector  of the lexicographical
sorting of the sub-matrix of  determined by the sequence of columns
.
\end{lemma}
\begin{proof}
We need to prove 
\begin{enumerate}
    \item if  then , and
    \item when  we have  if and only if .
\end{enumerate}
First, we note the for
loop on line 6 visits the elements of  in increasing order by using the
data structure . Thus, for all ranks  in L, all elements of  of rank
 will be visited in increasing order. This is the crux of the validity of
\autoref{alg:qlsrefine} and is worthwhile to repeat. Given the current
lexicographical order given by , every row of , restricted to the
sequence of columns  has some rank . Because \autoref{alg:qlsrefine}
uses , the algorithm will visit all the rows of current rank  
in the order given by the new column  of interest.

We first claim that  is equal to the number of unique elements
of  of rank , with respect to . The data structure  records
the most recently observed value of  of rank . The data structure
 simply denotes if nothing has been observed yet. Thus, when
we observe an element of  of rank  that differs from ,
we update  (line 9 and line 12) and increase  by
one (line 13).

Second, we claim that  restricted to all elements of rank 
is a ranking vector over the elements of  restricted to elements of rank .
More specifically,  is equal to the number of unique entries in 
of rank  strictly less than . The variable  is initialized 
to be zero and is set to the current value of  (line 16). That is,
 is set to the current number of unique elements of  of
rank .

The vector  is simply a partial sum (offset by one index) of the vector
. We can now prove the two important properties required to complete
the proof.  Suppose  and consider



and note .
Thus we have 

and therefore .

Lastly, if  then considering the definition of  and 
(line 24) we see the only variable is . We have already shown
that  is a ranking vector of items of the same rank. Thus 
if and only if  and the claim is proved.
\end{proof}

\begin{lemma} 
\label{lem:qls}
If ,  where the th column of
 stores the sorting of the th column of
,  where  , then
\autoref{alg:qls} returns the ranking vector  of the lexicographical
sorting  of the sub-matrix of  determined by the sequence of columns
.
\end{lemma}
\begin{proof}
Since \autoref{alg:qlsrefine} refines the ranking vector for each newly
appended column, the result follows.
\end{proof}

\begin{lemma} 
\autoref{alg:qlsrefine} runs in time  and space .
\label{lem:qlsrefineruntime}
\end{lemma}
\begin{proof}
There are only three loops in \autoref{alg:qlsrefine}, each of them
repeated  times. Each inner operation is constant time.  The only space
requirements are determined by column vectors of the  input
matrices and the vectors of length .
\end{proof}

\begin{lemma} 
\autoref{alg:qls} runs in time  and space .
\end{lemma}
\begin{proof}
There are  calls made to \autoref{alg:qlsrefine} which by
\autoref{lem:qlsrefineruntime} imply the total running time is .
The only space requirments are determined by the input matrices
and the vectors of length .
\end{proof}

Recall that both \autoref{alg:qlsrefine} and \autoref{alg:qls} require the
columns of  to be sorted and recorded in the input . Thus to
lexicographically sort a matrix  using
\autoref{alg:qls} requires , where we use an
 comparison sort to find .


\section{Sorting Sub-Matrices}
\label{sec:sortingsubmat}

Consider the problem of sorting all sub-matrices of  given by every possible
sequence of columns. 

\begin{problem}[\bf Sort All Sub-Matrices Given By Column Sequences]  \\
Let . 
\begin{itemize}
    \item For every sequence of columns  where, , ,  :
    \begin{itemize}
        \item Lexicographically sort the sub-matrix of  determined by the sequence of columns .
    \end{itemize}
    \label{prob:allseq}
\end{itemize}
\end{problem}

Also consider the sub-problem of sorting all sub-matrices of 
 given by every possible subset of columns.

\begin{problem}[\bf Sort All Sub-Matrices Given By Column Sets]  \\
Let . 
\begin{itemize}
    \item For every non-empty subset of columns :
    \begin{itemize}
        \item Lexicographically sort the sub-matrix of  determined by the set of columns .
    \end{itemize}
    \label{prob:allsubset}
\end{itemize}
\end{problem}

In \autoref{prob:allseq} there are  non-empty
sub-matrices to consider.  In \autoref{prob:allsubset} there are 
non-empty sub-matrices to consider. Both StableLexSort (\autoref{alg:stablelex})
and QuickLexSort (\autoref{alg:qls}) can be used to solve \autoref{prob:allseq}
and \autoref{prob:allsubset}. One simply enumerates the set of sub-matrices and
applies either algorithm.

We now present how the core of the QuickLexSort Algorithm
(\autoref{alg:qlsrefine}) lends itself ideally to \autoref{prob:allseq} and
\autoref{prob:allsubset}. That is, we can use \autoref{alg:qlsrefine} to
efficiently sort all the nested sub-matrices. The new \autoref{alg:qlsallperms}
for \autoref{prob:allseq} enumerates all sequences of columns in a
depth-first-search (DFS) manner.  It then exploits the fact that
\autoref{alg:qlsrefine} will take a current ranking vector and refine it by
considering appending an additional column. In this way we save the current
ranking vector and refine it based on all possible ways to append a column to
the current sub-matrix. 

\begin{algorithm}
\begin{algorithmic}[1]
    \REQUIRE , ,  where  .
\FOR{ }
        \STATE QuickLexSortRefine(,,,).
        \STATE Print .
        \STATE QuickLexSortAllSeq(,,,)
    \ENDFOR
\end{algorithmic}
\caption{QuickLexSortAllSeq}
\label{alg:qlsallperms}
\end{algorithm}

\autoref{alg:qlsallperms} is initially called with QuickLexSortAllSeq(,,,).

\begin{lemma}
    \autoref{alg:qlsallperms} has running time  and space requirements .
\end{lemma}
\begin{proof}
    Exactly  calls are made to \autoref{alg:qlsrefine}, which itself has running time and space .
\end{proof}

As it stands, StableLexSort could be used  
inside \autoref{alg:qlsallperms} but would not achieve the same running time.
If we replaced QuickLexSort (\autoref{alg:qlsallperms}) with StableLexSort (\autoref{alg:stablelex}) on
line 2 of \autoref{alg:qlsallperms} then the running time would
increase to .

This highlights the distinct advantage of QuickLexSort: It is linear time to
refine the lexicographical sorting when appending a column, provided the
columns of the data matrix have been pre-sorted.

Naively one may think to use a stable sort algorithm and append the columns in
the opposite order (since it has to work from least to most important columns),
and proceed in a DFS manner to explore all possible sorting. However, the
stable sort can not take advantage of the information contained in  and
would still need to do a comparison sort on each new column. 

With minor alteration of \autoref{alg:qlsallperms} we can handle \autoref{prob:allsubset}.

\begin{algorithm}
\begin{algorithmic}[1]
    \REQUIRE , ,  where  .
\FOR{  such that }
        \STATE QuickLexSortRefine(,,,).
        \STATE Print .
        \STATE QuickLexSortAllSubsets(,,,)
    \ENDFOR
\end{algorithmic}
\caption{QuickLexSortAllSubsets}
\label{alg:qlsallsubset}
\end{algorithm}

\autoref{alg:qlsallsubset} is initially called with QuickLexSortAllSubsets(,,,).

\begin{lemma}
    \autoref{alg:qlsallsubset} has running time  and space requirements .
\end{lemma}
\begin{proof}
    Exactly  calls are made to \autoref{alg:qlsrefine}, which itself has running time and space .
\end{proof}

Again, attempting to use StableLexSort on line 2 of \autoref{alg:qlsallsubset}
would increase the running time to . In both cases, this gain
may seem modest given the dominating terms involving . However, we note that
in many applications one may not in fact enumerate all sub-matrices but will
instead enumerate all nested sub-matrices up to a certain cardinality.  For
example if one wishes to enumerate all sub-matrices with up to two columns
then the running time of using QuickLexSort is  compared to  
for StableLexSort.

In general consider a set of nested sub-matrices indexed by their sequence of
columns , and let  denote the size of . Nested in
the sense that if  then either  is a singleton or there
exist  such that  and  differ by one element. Then if
one can efficiently (linear in ) enumerate the sub-matrices given
by  then the running time to sort all  sub-matries
using QuickLexSort is .  Extending the previous example,
if one wishes to sort all sub-matrices with up to  columns, then the
running time of QuickLexSort is .



\section{Experiments}
\label{sec:experiments}
As a verification of the running times of \autoref{alg:qlsrefine} and
\autoref{alg:qls} claimed in \autoref{lem:qlsrefine} and \autoref{lem:qls}, we
performed a short experiment using the {\tt Poker Hand} data set from the
University of California, Irvine's Machine Learning Repository
\cite{Bache+Lichman:2013}. The data set consists of a matrix with  rows
and  columns with discrete numerical values. Ten data sets were created for
the experiments, consisting of the first , , , 
rows. For each of the ten data sets, QuickLexSort (using merge sort for the
preliminary sorting of data columns) and StableLexSort (using merge sort) were
run to sort all possible non-empty subsets of  columns. Note, the running
times for QuickLexSort includes the pre-sorting step.
\autoref{fig:SLSvsQLS} shows the time to lexicographically sort using both
methods. It is fairly easy to see the linear growth in running time of
QuickLexSort compared to the linear times log factor running time of StableLexSort with
respect to the number of rows.

\begin{figure}
    \begin{center}
    \includegraphics[width=12cm]{SLS_vs_QLS}
    \end{center}
    \caption{Running times of QuickLexSort and StableLexSort to sort all
    possible subsets of seven columns on ten truncations of the {\tt Poker
Hand} data set from the UCI Machine Learning data base.}
\label{fig:SLSvsQLS}
\end{figure}



\section{Comparison to AD-trees}
\label{sec:adtrees}
A popular method which specifically computes contingency tables (and sorts) is
ADtrees \cite{Moore:1998fk}. Although retrieving a contingency table (or
sorting) can be very fast -- faster than QuickLexSort -- the time and space
requirements to compute and store the required data structures can be enormous. Assuming
binary features (features only take two values), the cost to build an AD
tree is bounded above by 
 

where . If all possible combinations
of the binary features appear in , the space requirement would be .
Even with a reasonable number of rows the space requirement would be bound
above by 

 
The time
and space requirements can become practically prohibitive as  and  grow.
For example, constructing and storing the ADtree for a dataset  would be infeasible.  By contrast, QuickLexSort only requires
linear space and time for each sort.


\section{Applications to Bayesian Networks}
\label{sec:bn}
Bayesian networks (BN) are graphical models that have applications in a
plethora of areas including machine learning, statistical inference, finance,
biology, artificial intelligence, etc \cite{kollar2009probabilistic,
studeny2005probabilistic}. Bayesian networks represent the conditional
independences in some given data and are modeled through directed acyclic
graphs (DAGs). In a naive sense, the task of learning the BN structure is to
explore all possible DAGs and choose the DAG which best fits the data. Note
that learning the BN structure is NP-hard \cite{chickering1996learning,chickering2004large}. The fit
of a proposed DAG to the data is evaluated by a scoring function such as
Bayesian information criteria (BIC) or Bayesian Dirichlet equivalence (BDE). 
The BIC and BDE graph scoring functions evaluate a graph  by looking
at each node  and its parents  (the nodes which have edges directed
to  in ). For every pair , BIC and BDE compute local scores (a
score depending only on ). In the end, the score of the proposed
graph  is a sum over all local scores .  At
a low level, the BIC or BDE local score of  simply requires two
contingency tables: 1) a contingency table of the input data matrix restricted
to columns indexed by , 2) a contingency table of the input data matrix
restricted to columns indexed by .

In many current methods to learn a BN, the task is roughly broken into two
steps. In step one, all local scores are precomputed. Often this is prohibitive
and in practice only the local scores up to a certain cardinality are computed,
or steps are taken to theoretically exclude certain local scores
\cite{de2011efficient,de2009structure}.  In step two, the structure is
learned by some intelligent method (Integer programming, Dynamic Programming,
Heuristically, and more)\cite{de2011efficient,chickering2003optimal,de2009structure,de2000new,jaakkola2010learning,barlettadvances,cussens2012bayesian,silander2012simple,singh2005finding}. Research has focused mainly on the
second step. However, the first step of local scoring merits exploration. For
example the condition that parent sets are limited in cardinality can be quite
artificial. But, if  and  become large, it may be prohibitive to compute
all local scores using an approach such as StableLexSort. However, QuickLexSort
is fast and requires small space.  Moreover, approaches can be taken in which
the local scores are done on-the-fly, in which case QuickLexSort can be of use.

One approach to learning BN is to perform a heuristic search of the solution
space by iteratively changing the current graph
structure\cite{moore2003optimal,madigan1994model,madigan1995bayesian,giudici1999decomposable}. Again, in many cases it may be
infeasible to store all contingency tables and it would be better to score each
new graph. The proposed new graph can be chosen such that previous contingency
tables can be updated efficiently by QuickLexSort. For example if the graphical
moves are restricted to simply adding or removing a single edge of the current
graph.  As a new approach, we are currently developing a method which explores
the solution space using characteristic imsets\cite{hemmecke2012characteristic,studeny2010characteristic,studeny2011polyhedral,StudenyHaws:2013} -- a more natural encoding of unique probability models forming BNs --
and QuickLexSort in order to efficiently move through the solution space.

A biological example of an application of Bayesian networks is the modeling of epistasis --
the interaction of multiple genes to produce a phenotype. Using Bayesian
networks (and related measures dependent on contingency tables) has
proven useful in detecting epistasis
\cite{sucheston2010comparison,jiang2011learning,shang2011performance}.  In this
case, one does not need to consider the full class of DAGs, and the problem
reduces to simply scoring.  Suppose we are given the genotypes for 
individuals each with  single nucleotide polymorphisms (SNPs) and some
phenotype (disease/no-disease). Then in this case we have a matrix .  The task of detecting k-way epistasis using Bayesian
networks reduces to computing contingency tables (sorting) all possible choices
of  subsets columns of .  Considering \autoref{eq:ad1} and
\autoref{eq:ad2}, it would be impractical to use ADtrees.  For most , it
would be infeasible to store all contingency tables for all 
SNP -tuples. However, QuickLexSort requires linear time and space, to check
each choice of k columns of .

As a preliminary experiment, QuickLexSort was used for Bayesian network scoring
and the detection of epistasis on Maize genotype and phenotype data
\cite{rincent2012maximizing} used for the European CornFed program. The data
used consisted of  inbred \emph{dent} maize plant lines (rows) and
 SNPs (columns). The phenotype used was male flowering time. The
Bayesian Dirichlet equivalent (BDE) \cite{heckerman1995learning} score was used
to detect up to two-way epistasis. In general, to compute the BDE score of any
 SNPs with respect to the phenotype, one needs two contingency tables: the
contingency table given by the sub-matrix over those  SNPs as well as the
contingency table given by the sub-matrix over the  SNPs with the additional
column of phenotypes. QuickLexSort lends itself naturally to this process
since we iterate through the sets of SNPs in depth-first-search manner. Moreover,
to compute the latter contingency table we simply append the phenotype column
and do one call to \autoref{alg:qlsrefine}. For the experiment, the BDE score was computed
for all singleton and pairs of SNPs. The QuickLexSort based approach took
approximately  hours to perform  BDE scores on the  data matrix, which required
twice as many contingency table computations. 






\bibliographystyle{plainnat}
\bibliography{references}


\section{Appendix}
\label{sec:appendix}
\autoref{alg:qlsrefine} can be easily modified to handle extra input and output 
of more natural data structure to store the current sorting. Moreover, the modification
does not change the running time or space constraints. 

\begin{definition}
Let  be a sub-matrix of . 
The unique \emph{order vector}  of some 
ordering  of rows of  is a vector such that
 is the row index of  of the th item in the ordering .
\end{definition}

If we take the order  and ranking vector  of the same ordering together
we define another useful data structure.

\begin{definition}
Let  be a sub-matrix of 
with some ordering  of rows of  and its unique order
vector  and ranking vector .
The \emph{partitioning vector}  is the vector such that
 is the index into the order vector  where the rows of rank 
begin.
\end{definition}

The ranking and order vectors store the same information, but it not
necessarily linear time to transform from one to the other. The benefit to the
modified algorithm is that it updates , , and  simultaneously. In many
cases it is easier to work with the ordering vector. Moreover, the partitioning
vector with the order vector gives all the necessary information to form a
contingency table.

\begin{example}
Consider matrix  and the sub-matrix  in \autoref{ex:D}. The ranking,
ordering, and partitioning vectors are ,
, and . Immediately, we can read off
from  that there are six rank blocks, if we want to traverse the rows
of the matrix  in the order we simply use , and  tells us how many rows
of each rank are present.
\end{example}


We now give \autoref{alg:qlsrefinenew} which takes the same input as
\autoref{alg:qlsrefine} as well as the ordering and partitioning vectors. It
outputs the new ranking, ordering, and partitioning vector with respect to the
lexicographical ordering one gets by appending column .


\begin{algorithm}[!h]
\begin{algorithmic}[1]
    \REQUIRE , , , , .
    \ENSURE , , .
    \STATE . 
    \STATE . 
    \STATE . 
    \STATE . \# Records most recent value in  w.r.t.\ ID.
    \STATE .
    \STATE . \# Records subID of .
    \STATE . \# Records count of refinements of each input ID of L.
    \FOR{  }
        \STATE .
        \STATE .
        \IF{ } \STATE .
            \STATE .
        \ELSE
            \IF{ }
                \STATE .
                \STATE .
            \ENDIF
        \ENDIF
        \STATE .
    \ENDFOR
    \STATE .
    \STATE .
    \FOR{  }
        \STATE .
    \ENDFOR
    \STATE .
    \FOR{ }
        \STATE .
        \IF{ }
            \STATE .
            \STATE .
        \ENDIF
    \ENDFOR
    \RETURN .
\end{algorithmic}
\caption{QuickLexSortRefine\dag: Handles order vector.}
\label{alg:qlsrefinenew}
\end{algorithm}


\begin{lemma} 
\label{lem:qlsrefinenew}
If ,  where the th column of
 stores the sorting of the th column of ,  is the ranking vector,
 is the ordering vector, and  is the partitioning vector of the
lexicographical sorting of the sub-matrix of  determined the sequence of
columns , and , then
\autoref{alg:qlsrefinenew} returns the ranking vector , the ordering vector
, and the partitioning vector  of the lexicographical sorting of the
sub-matrix of  determined by the sequence of columns .
\end{lemma}
\begin{proof}
The data structures , , , and  all are
initialized and updated the same in \autoref{alg:qlsrefinenew} as they were in
\autoref{alg:qlsrefine}. The new order vector  is initialized to be zeros
and the new partitioning vector  is initialized to equal the input
partitioning vector . Recall the  is the index into the ordering vector
 where the rows of rank  begin. The goal of lines  are to create
the new ordering vector . To do this we must reorder all rows that have the
same previous ranking according to . Thus, the vector  is temporarily
used to point to the next available index into  were the rows all have the
same rank according to . In line  we fill in the entries of  as we
traverse the new column  according to the pre-computed sorting given in
. Specifically we look at the current rank of  which is
given in . Then  points to the next available
position in  with rank equal to . Since we have filled this
position  we increment  in line . In the end,  will
be the ordering vector with respect to the new order given by the sequence
of columns .

In line  we initialize the data structure  which will store the
previously observed rank in the following for loop.  In \autoref{alg:qlsrefine}
and lines  we filled in entries of  by traversing .
We note that we could have traversed  in any particular order. Thus, in
\autoref{alg:qlsrefinenew} we traverse  in the order given by the new
ordering vector .  Therefore, by the arguments in the proof
\autoref{lem:qlsrefine},  is the unique ranking vector given by the
sequence of columns . 

Lastly in lines  whenever we observe a row with a new rank, we set 
to point to the appropriate index into . Therefore,  is the 
partitioning vector given by the sequence of columns .

\end{proof}

\begin{lemma} 
\autoref{alg:qlsrefinenew} runs in time  and space .
\label{lem:qlsrefinenewruntime}
\end{lemma}
\begin{proof}
There are only three loops in \autoref{alg:qlsrefinenew}, each of them
repeated  times. Each inner operation is constant time. The only space
requirements are determined by column vectors of the  input
matrices and the vectors of length .
\end{proof}

\end{document}
