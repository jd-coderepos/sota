\documentclass[twocolumn]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,pgf,tikz,subfig} \usepackage{amsmath,amssymb,amsfonts,amsthm} \usepackage{multirow,booktabs} \usepackage{textcomp} \usepackage{enumitem} \usepackage{appendix} \usepackage{lipsum,mwe} \usepackage{algorithm,algpseudocode} \usepackage{hyperref,url}



\newcommand{\tlts}[1]{{\color{blue}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}


\newcommand{\opn}{\operatorname}
\newcommand{\concat}[2]{\ensuremath{#1\|#2}}
\DeclareMathOperator*{\Concat}{\text{\Large } }

\newcommand{\canonicsty}{\mathbb}
\newcommand{\setsty}{\mathcal}
\newcommand{\vecsty}{\mathbf}
\newcommand{\binset}{\{\bot,\top\}}

\newcommand{\reals}{\mathbb{R}}

\usepackage[symbol]{footmisc}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\title{Superpixel Image Classification with Graph Attention Networks}
\author{
    Pedro H. C. Avelar\textsuperscript{1,2,*}~\href{https://orcid.org/0000-0002-0347-7002}{\includegraphics[height=10pt]{orcid.pdf}}
    \and Anderson R. Tavares\textsuperscript{1}~\href{https://orcid.org/0000-0002-8530-6468}{\includegraphics[height=10pt]{orcid.pdf}}
    \and Thiago L. T. da Silveira\textsuperscript{3}~\href{https://orcid.org/0000-0001-6788-2667}{\includegraphics[height=10pt]{orcid.pdf}}
    \and Cláudio R. Jung\textsuperscript{1}~\href{https://orcid.org/0000-0002-4711-5783}{\includegraphics[height=10pt]{orcid.pdf}} \quad\quad\noexpand
    \and Luís C. Lamb\textsuperscript{1}~\href{https://orcid.org/0000-0003-1571-165X}{\includegraphics[height=10pt]{orcid.pdf}}\\label{eq:attention}
    \alpha(s,t) = \frac{e^{a(\concat{x(s)}{x(t)})}} {\sum_{s' \in \mathcal{T}(t)}{ e^{a(\concat{x(s')}{x(t)}) }} }
\label{eq:layer}
    o(t) = \sum_{s \in \mathcal{T}(t)} \alpha(s,t)f(\concat{x(s)}{x(t}))
\label{eq:multihead}
    o(t) = \sum_{s \in \mathcal{T}(t)} \Concat_{i = 1}^k  \alpha_i(s,t)f_i(\concat{x(s)}{x(t)})


The output of the final GAT layer can then be sum-pooled, having all the values added, and then passed through a MultiLayer Perceptron (MLP) for the final prediction. The Python/Pytorch implementation in its fullest can be seen in the provided GitHub repository\footnote{\url{https://github.com/machine-reasoning-ufrgs/spixel-gat}} as well. Most operations have been parallelized as much as the authors could fathom, with some operations done in a preprocessing phase to avoid overload. 

\section{Experiments}\label{sec:experiments}

In this section, we show 
the potential of our technique on
four datasets: MNIST~\cite{lecun1998mnist}, FashionMNIST~\cite{xiao2017fashionmnist}, SVHN~\cite{netzer2011svhn} and CIFAR-10~\cite{krizhevsky2009cifar}. The superpixel algorithm has a target of 75 regions, so that the analyzed graphs present approximately 75 nodes each. 

All experiments were ran either in a computer with a NVIDIA Quadro P6000 or one with a NVIDIA GTX 1070 Mobile. Both computers have 32GB of RAM. For development, we adopted the Pytorch library, version 1.X, using CUDA.

For all experiments we set a budget of 100 epochs for optimisation, with a batch size of 32 images, using a  split for training and validation in the dataset's original training data. We use Adam as the optimiser, with a learning rate of , , and , using the model with the best validation accuracy on the test dataset. If the model failed to leave a baseline accuracy for the first 10 epochs, we restarted the training procedure from scratch.

\subsection{MNIST}\label{ssec:mnist}

We trained two versions of the GAT model: A single-headed GAT with  layers, with  and  neurons, and a two-headed GAT, where each head has the same amount of neurons as the single-headed model. Both models used sum-pooling and a MLP with two layers of  and  neurons for the final classification, where  is the number of classes in  MNIST. All neurons use ReLU activations, except for those at the last layer of the classification MLP, which use softmax activations. We did not use any regularisation technique.

All dataset images are converted to a corresponding RAG, using SLICO~\cite{slico}, a zero-parameter variant the SLIC algorithm. We set the target number of 
superpixels as 75, but the generated RAGs are not guaranteed to have exactly 75 nodes 
due to how the SLIC algorithm works.

Table~\ref{tab:testresults} shows that both 
GAT models performed better than the MoNET model \cite{monti2017monet}, showing that a learned representation of the geometric distance can lead to better performance than the fixed one of \cite{monti2017monet}. Note that our model has also to deal with graphs that are sparser than the ones used in the baselines~\cite{monti2017monet,fey2018splinecnn,spurek2019geogcn}, since in their graph edges are formed through K-nearest neighbours and ours use only directly adjacent nodes. Although one could expect worse accuracy, the results suggest that our approach is able to learn relevant geometric information relating the features from all neighbouring superpixels. We also report the performance of the graph-based approaches  SplineCNN \cite{fey2018splinecnn} and Geo-GCN \cite{spurek2019geogcn}, as well as recent alternative approaches such as the Generative Tensor Network Classifier (GTNC)~\cite{sun2020generative} and the best performing method based on Support Vector Classification (SVC) as reported in the dataset homepage\footnote{\url{https://github.com/zalandoresearch/fashion-mnist}}. Although our approach does not reach state-of-the-art results, it is competitive and the best among graph-based methods.

\subsection{FashionMNIST}\label{ssec:fashionmnist}

We trained the single-headed and multi-headed GAT models with the same configuration of Section~\ref{ssec:mnist}. Since the FashionMNIST dataset also has  classes, the number of output neurons is  as well. 
The RAGs were also generated as in Section~\ref{ssec:mnist}.

Since none of the found graph-based papers present results for both MNIST and FashionMNIST,
we provide 
a 
performance comparison
of
our models
to GTNC~\cite{sun2020generative} and the two best classifiers from the FashionMNIST benchmark \cite{xiao2017fashionmnist} in Table~\ref{tab:testresults}. The gap on the performance between the traditional ML models that  used the full features of the dataset and our models, which use a reduced representation based on the oversegmented image, is higher on FashionMNIST. This shows how much harder the FashionMNIST dataset is for oversegmented images, where the information loss is greater with the aggregation of the features of the pixels in the superpixel.

\begin{table}[tpb]
    \centering
 \caption{Test accuracy for the tested models on MNIST and FashionMNIST, processed as RAGs with approximately 75 nodes (called MNIST-75 and FashionMNIST-75), compared to the baseline models. Bold values show the best of the Graph-based models. We also present the mean accuracies of the two best classifiers for the non-oversegmented MNIST and FashionMNIST datasets, available in the FashionMNIST benchmark.}
       \begin{tabular}{ccc}
    \toprule
                    & MNIST-75  & FashionMNIST-75 \\
    \midrule
        MoNET \cite{monti2017monet}                      &  & - \\
        SplineCNN \cite{fey2018splinecnn}                  &  & - \\
        GeoGCN \cite{spurek2019geogcn}                      &  & - \\
    \midrule
        GAT-1Head                   &  &  \\
        GAT-2Head                   &  &  \\
    \midrule
                    & MNIST     & FashionMNIST \\
    \midrule
        GTNC~\cite{sun2020generative} &  &   \\
        SVC(10,poly)                & * & * \\
        SVC(100,poly)               & * & * \\
    \bottomrule
    \end{tabular}
    \label{tab:testresults}
\end{table}

Another interesting fact to note is that the multi-headed model 
performed worse
than the single-headed one. This was further confirmed when we tried learning with a 4-headed model, which performed slightly worse than the 2-headed one.

\subsection{Street View House Numbers}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/svhn-training.pdf}
    \caption{Training Curve for the Street View House Numbers  dataset}
    \label{fig:svhn-training}
\end{figure}

To check the performance of the model with multi-channel data, we trained and tested the two-headed model on the Street View House Numbers (SVHN) dataset~\cite{netzer2011svhn}, using the  cropped version and the same parameters for 
oversegmentation
as in the previous sections. We achieved a similar performance with the two-headed GAT model on the FashionMNIST dataset, with  test accuracy. This comes to show that our model works even with full colour data, as well as the confounders present in the SVHN dataset. 

That is, the classifier has learned both to prioritise the centermost superpixels and to identify which structure it contains by comparing changes in colour tone, instead of simple changes in luminosity, proving that, although it does not reach state-of-the-art performance, the model can achieve relatively good accuracy even 
working with
less expressive data.




\subsection{CIFAR-10}

The CIFAR-10 dataset~\cite{krizhevsky2009cifar} contains 50,000  colour images distributed in 10 classes. We used the same parameters for 
oversegmentation
as in the previous sections.
The results we achieved through a network with the same architecture as GAT-2Head was 45.93\% accuracy on the test set -- very distant from what we achieved on the MNIST and FashionMNIST datasets.
The training and validation accuracies were not impressive either, being 58.61\% and 53.40\% respectively. 

As a baseline, we 
considered
the VGG11 \cite{simonyan2015very} architecture, with and without batch normalisation. We were unable to train the model without
batch normalisation,
whereas with batch normalisation we achieved 62.86\% validation accuracy, which shows the heavy information loss during the RAG transformation procedure. However the comparison of the GAT with the VGG model is still unfair, in terms both of information available to the model and the number of parameters.

While the VGG model has access to the oversegmented
image, with each segment's pixel having the averaged RGB values for each superpixel -- approaching also \emph{geometry} information --
the GAT model only has access to the average colour and the centroid position, 
not knowing anything about the superpixel's shape.
More precisely, while the VGG model has access to the middle images in Fig.~\ref{fig:degradation}, the GNN model has only access to the graph on the right images, with each node containing the average pixel value and position.

As for the model size, the VGG11 network has 132,868,840 parameters, while the GAT 
has only 55,364. The VGG network also consumed almost twice as much VRAM as the GAT-2Head architecture on the CIFAR images. 
VGG11 consumed 4,109MiB for training and 1,055MiB for testing, while the GAT model expended 1,067MiB for training and 485MiB for testing. For the sake of illustration, the current state-of-the-art result on CIFAR-10 \cite{huang2019gpipe} is reached with an  AmoebaNet-B (550M parameters) pre-trained on ImageNet and fine-tuned on CIFAR-10. 

\begin{figure}
    \centering
    \begin{tabular}{cccccc}
        \includegraphics[width=.12\textwidth]{figures/cifar/0i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/0s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/0n.png}
        \\
        \includegraphics[width=.12\textwidth]{figures/cifar/1i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/1s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/1n.png}
        \\
        \includegraphics[width=.12\textwidth]{figures/cifar/2i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/2s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/2n.png}
        \\
        \includegraphics[width=.12\textwidth]{figures/cifar/3i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/3s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/3n.png}
        \\
        \includegraphics[width=.12\textwidth]{figures/cifar/4i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/4s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/4n.png}
        \\
        \includegraphics[width=.12\textwidth]{figures/cifar/5i.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/5s.png}
        & \includegraphics[width=.12\textwidth]{figures/cifar/5n.png}
    \end{tabular}
    
    \caption{Examples showing the loss of information in the RAG procedure when using only the average of each channel, which negatively affects the performance of the network.}
    \label{fig:degradation}
\end{figure}

\section{Conclusion}\label{sec:conclusion}

In this paper, we have investigated the application and interplay between Graph Attention Networks (GATs) \cite{velickovic2018gat} and  image classification problems. In order to do so, we have used  Region Adjacency Graphs (RAGs) computed from an image segmented using a superpixel algorithm, SLICO.
We showed that using attention-based graph neural networks on a feature space that contains the geometric information can be improved by weighting the edges of a superpixel graph
using a learned function which operates solely on the geometric information.

However, this approach to image classification has its shortcomings. The information loss in the pixel aggregation for more complex images can result in significant performance degradation when compared to using the full image. Also, graph-based approaches may come with the same limitations intrinsic to the models they use, and in our case the GNN-based architecture imposes some limitations in terms of memory usage for larger graphs due to the batching procedure (and thus finer segmentations), despite the smaller number of parameters the model itself had. Training in small batches lead to an unreliable training pattern, further aggravating such issues.

These limitations are, however, venues for future work. It has been shown that architectures based on graph convolutional networks, such as the GAT, suffer from an oversmoothing of node-level information, thus acting like low-pass filters \cite{nt2019lowpassfilters}.
While GATs might not be subject to the same limitation, this could be investigated to allow deeper GATs with potentially better performance in this domain. 
Another venue is helping scaling Graph Neural Networks, of which GAT is a representative, to larger graphs (and thus larger images) or to make them work in an online manner, or with smaller batches.

Also, our models used no regularization whatsoever, and investigating regularization techniques for these models could incur in better performance. Lastly, investigating different node feature vectors could provide the network with richer information and lesser the information loss due to the RAG procedure, possibly with information to recustruct the superpixel's components.

These graph-based approaches to image classification are also a prime example of application to non-euclidean images, such as omnidirectional images \cite{lee2019spherephd}. The flexibility of a graph-based approach could be more invariant to the domain of the image, possibly allowing pre-training on planar images and transfer to spherical images.

\section*{Acknowledgements}We would like to thank NVIDIA Corporation for the Quadro GPU granted to our research group. This work is partly supported by Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) -- Finance Code 001 and by the Brazilian Research Council CNPq. We would also like to thank the Pytorch developers for their library.

\bibliographystyle{plain}
\bibliography{superpixelgat}

\end{document}