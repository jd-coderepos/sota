\documentclass[1p,authoryear,letterpaper]{elsarticle}







\usepackage{ragged2e}
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{xcolor}
\graphicspath{{./}{./figures/}}

\renewcommand{\cite}{\citep}

\usepackage{ushort}
\usepackage{mathdots}

\providecolor{TufteRed}{rgb}{0.8,0,0}
\providecolor{tuftered}{rgb}{0.8,0,0}
\providecolor{subtleblue}{rgb}{0.8,0,0}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newdefinition{remark}{Remark}
\newdefinition{result}{Result}
\newdefinition{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}


\DeclareMathOperator{\tcirc}{\texttt{circ}}
\DeclareMathOperator{\tspan}{\texttt{span}}
\DeclareMathOperator{\tabs}{\texttt{abs}}
\DeclareMathOperator{\tvec}{\texttt{vec}}
\DeclareMathOperator{\rank}{rank}\DeclareMathOperator{\diag}{diag}\DeclareMathOperator{\Diag}{Diag}\DeclareMathOperator{\supp}{supp}

\providecommand{\ii}{\imath}
\providecommand{\eps}{\varepsilon}\providecommand{\kron}{\otimes}
\providecommand{\eqdef}{\equiv}
\providecommand{\mathdef}{\equiv}

\providecommand{\RR}{\mathbb{R}}
\providecommand{\KK}{\mathbb{K}}
\providecommand{\CC}{\mathbb{C}}
\providecommand{\FF}{\mathbb{F}}
\providecommand{\DD}{\mathbb{D}}

\newcommand{\indof}[1]{\!\left\llbracket{#1}\right\rrbracket}
\providecommand{\normof}[2][]{\left\| #2 \right\|_{#1}}\providecommand{\nnormof}[2][]{\| #2 \|_{#1}}\providecommand{\itr}[2]{#1^{(#2)}}
\providecommand{\itn}[1]{^{(#1)}}\providecommand{\cardof}[1]{\left| #1 \right|}
\providecommand{\absof}[1]{\left| #1 \right|}
\newcommand{\niprod}[2]{\langle {#1}, {#2} \rangle}
\newcommand{\siprod}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\liprod}[2]{\bigl\langle {#1}, {#2} \bigr\rangle}
\newcommand{\Liprod}[2]{\biggl\langle {#1}, {#2} \biggr\rangle}
\newcommand{\hiprod}[2]{\Bigl\langle {#1}, {#2} \biggr\rangle}
\newcommand{\Hiprod}[2]{\Biggl\langle {#1}, {#2} \Biggr\rangle}
\newcommand{\iprod}{\niprod}
\newcommand{\bigOof}[1]{\bigO(#1)}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\conj}[1]{\overline{#1}}


\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\providecommand{\subjectto}{\ensuremath{\text{subject to}}}
\providecommand{\MINof}[1][]{{\displaystyle \minimize_{#1}}}
\providecommand{\MIN}[2]{\begin{array}{ll} \MINof[#1] & #2 \end{array}}
\providecommand{\MINone}[3]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \end{array}}
\providecommand{\MINtwo}[4]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \end{array}}
\providecommand{\MINthree}[5]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \\ & #5 \end{array}}
\providecommand{\MINfour}[6]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \\ & #5 \\ & #6 \end{array}}
\providecommand{\MAXof}[1][]{{\displaystyle \maximize_{#1}}}
\providecommand{\MAX}[2]{\begin{array}{ll} \MAXof[#1] & #2 \end{array}}
\providecommand{\MAXone}[3]{\begin{array}{ll} \MAXof[#1] & #2 \\ \subjectto & #3 \end{array}}
\providecommand{\MAXtwo}[4]{\begin{array}{ll} \MAXof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \end{array}}
\providecommand{\MAXthree}[5]{\begin{array}{ll} \MAXof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \\ & #5 \end{array}}
\providecommand{\MAXfour}[6]{\begin{array}{ll} \MAXof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \\ & #5 \\ & #6 \end{array}}

\providecommand{\pmat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\providecommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\providecommand{\spmat}[1]{\left(\begin{smallmatrix} #1 \end{smallmatrix}\right)}
\providecommand{\sbmat}[1]{\left[\begin{smallmatrix} #1 \end{smallmatrix}\right]}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\p}{\mathbb{P}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Std}{Std}
\providecommand{\Eof}[1]{\E\left[#1\right]}
\providecommand{\Stdof}[1]{\Std\left[#1\right]}
\providecommand{\Covof}[2]{\Cov\left[#1,#2\right]}
\providecommand{\prob}[2][]{\p_{#1}\left[ #2 \right]}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Unif}{Unif}



\providecommand{\set}{\mathcal}
\providecommand{\graph}{\mathcal}
\providecommand{\mat}{\boldsymbol}
\renewcommand{\vec}{\mathbf}


\providecommand{\eye}{\mat{I}}
\providecommand{\mA}{\ensuremath{\mat{A}}}
\providecommand{\mB}{\ensuremath{\mat{B}}}
\providecommand{\mC}{\ensuremath{\mat{C}}}
\providecommand{\mD}{\ensuremath{\mat{D}}}
\providecommand{\mE}{\ensuremath{\mat{E}}}
\providecommand{\mF}{\ensuremath{\mat{F}}}
\providecommand{\mG}{\ensuremath{\mat{G}}}
\providecommand{\mH}{\ensuremath{\mat{H}}}
\providecommand{\mI}{\ensuremath{\mat{I}}}
\providecommand{\mJ}{\ensuremath{\mat{J}}}
\providecommand{\mK}{\ensuremath{\mat{K}}}
\providecommand{\mL}{\ensuremath{\mat{L}}}
\providecommand{\mM}{\ensuremath{\mat{M}}}
\providecommand{\mN}{\ensuremath{\mat{N}}}
\providecommand{\mO}{\ensuremath{\mat{O}}}
\providecommand{\mP}{\ensuremath{\mat{P}}}
\providecommand{\mQ}{\ensuremath{\mat{Q}}}
\providecommand{\mR}{\ensuremath{\mat{R}}}
\providecommand{\mS}{\ensuremath{\mat{S}}}
\providecommand{\mT}{\ensuremath{\mat{T}}}
\providecommand{\mU}{\ensuremath{\mat{U}}}
\providecommand{\mV}{\ensuremath{\mat{V}}}
\providecommand{\mW}{\ensuremath{\mat{W}}}
\providecommand{\mX}{\ensuremath{\mat{X}}}
\providecommand{\mY}{\ensuremath{\mat{Y}}}
\providecommand{\mZ}{\ensuremath{\mat{Z}}}
\providecommand{\mzero}{\ensuremath{\mat{0}}}
\providecommand{\mLambda}{\ensuremath{\mat{\Lambda}}}
\providecommand{\mSigma}{\ensuremath{\mat{\Sigma}}}

\providecommand{\mhat}[1]{\ensuremath{\mat{\hat{#1}}}}
\providecommand{\vhat}[1]{\ensuremath{\vec{\hat{#1}}}}
\providecommand{\mAhat}{\mhat{A}}
\providecommand{\vxhat}{\vhat{x}}

\providecommand{\ones}{\vec{e}}
\providecommand{\va}{\ensuremath{\vec{a}}}
\providecommand{\vb}{\ensuremath{\vec{b}}}
\providecommand{\vc}{\ensuremath{\vec{c}}}
\providecommand{\vd}{\ensuremath{\vec{d}}}
\providecommand{\ve}{\ensuremath{\vec{e}}}
\providecommand{\vf}{\ensuremath{\vec{f}}}
\providecommand{\vg}{\ensuremath{\vec{g}}}
\providecommand{\vh}{\ensuremath{\vec{h}}}
\providecommand{\vi}{\ensuremath{\vec{i}}}
\providecommand{\vj}{\ensuremath{\vec{j}}}
\providecommand{\vk}{\ensuremath{\vec{k}}}
\providecommand{\vl}{\ensuremath{\vec{l}}}
\providecommand{\vm}{\ensuremath{\vec{l}}}
\providecommand{\vn}{\ensuremath{\vec{n}}}
\providecommand{\vo}{\ensuremath{\vec{o}}}
\providecommand{\vp}{\ensuremath{\vec{p}}}
\providecommand{\vq}{\ensuremath{\vec{q}}}
\providecommand{\vr}{\ensuremath{\vec{r}}}
\providecommand{\vs}{\ensuremath{\vec{s}}}
\providecommand{\vt}{\ensuremath{\vec{t}}}
\providecommand{\vu}{\ensuremath{\vec{u}}}
\providecommand{\vv}{\ensuremath{\vec{v}}}
\providecommand{\vw}{\ensuremath{\vec{w}}}
\providecommand{\vx}{\ensuremath{\vec{x}}}
\providecommand{\vy}{\ensuremath{\vec{y}}}
\providecommand{\vz}{\ensuremath{\vec{z}}}
\providecommand{\vzero}{\ensuremath{\vec{0}}}
\providecommand{\vpi}{\ensuremath{\vecalt{\pi}}} 



 


\newcommand{\fftm}{\mF}
\newcommand{\ifftm}{{\mF^*}}

\newcommand{\im}{\imath}
\DeclareMathOperator{\fft}{\texttt{cft}}
\DeclareMathOperator{\ifft}{\texttt{icft}}
\DeclareMathOperator{\icft}{\texttt{icft}}
\DeclareMathOperator{\cft}{\texttt{cft}}

\newcommand{\todo}[2][TODO~]{\marginpar{\footnotesize{\textsc{\lowercase{#1}}}\RaggedRight\textcolor{TufteRed}{#2}}}

\newcommand{\ceilof}[1]{\lceil #1 \rceil}

\renewcommand{\circeq}{\leftrightarrow}
\newcommand{\cel}[1]{\ushort{#1}}
\newcommand{\cbmat}[1]{\left\{ \begin{matrix} #1 \end{matrix} \right\}}
\newcommand{\csbmat}[1]{\left\{\! \begin{smallmatrix} #1
\end{smallmatrix} \! \right\}}
\newcommand{\smallmath}[1]{\begin{smallmatrix}#1\end{smallmatrix}}
\newcommand{\celm}[1]{\cel{\mat{#1}}}
\newcommand{\celv}[1]{\cel{\vec{#1}}}

\newcommand{\calpha}{\ensuremath{\cel{\alpha}}}
\newcommand{\cbeta}{\ensuremath{\cel{\beta}}}
\newcommand{\clambda}{\ensuremath{\cel{\lambda}}}

\newcommand{\cone}{\ensuremath{\cel{1}}}
\newcommand{\ca}{\ensuremath{\cel{a}}}
\newcommand{\cb}{\ensuremath{\cel{b}}}
\newcommand{\cc}{\ensuremath{\cel{c}}}
\newcommand{\cd}{\ensuremath{\cel{d}}}
\newcommand{\ce}{\ensuremath{\cel{e}}}
\newcommand{\cf}{\ensuremath{\cel{f}}}
\newcommand{\cg}{\ensuremath{\cel{g}}}
\newcommand{\ch}{\ensuremath{\cel{h}}}
\newcommand{\ci}{\ensuremath{\cel{i}}}
\newcommand{\cj}{\ensuremath{\cel{j}}}
\newcommand{\ck}{\ensuremath{\cel{k}}}
\newcommand{\cl}{\ensuremath{\cel{l}}}
\newcommand{\cm}{\ensuremath{\cel{m}}}
\newcommand{\cn}{\ensuremath{\cel{n}}}
\newcommand{\co}{\ensuremath{\cel{o}}}
\newcommand{\cp}{\ensuremath{\cel{p}}}
\newcommand{\cq}{\ensuremath{\cel{q}}}
\newcommand{\cs}{\ensuremath{\cel{s}}}
\newcommand{\ct}{\ensuremath{\cel{t}}}
\newcommand{\cu}{\ensuremath{\cel{u}}}
\newcommand{\cv}{\ensuremath{\cel{v}}}
\newcommand{\cw}{\ensuremath{\cel{w}}}
\newcommand{\cx}{\ensuremath{\cel{x}}}
\newcommand{\cy}{\ensuremath{\cel{y}}}
\newcommand{\cz}{\ensuremath{\cel{z}}}

\newcommand{\cA}{\ensuremath{\cel{A}}}
\newcommand{\cB}{\ensuremath{\cel{B}}}
\newcommand{\cC}{\ensuremath{\cel{C}}}
\newcommand{\cD}{\ensuremath{\cel{D}}}
\newcommand{\cE}{\ensuremath{\cel{E}}}
\newcommand{\cF}{\ensuremath{\cel{F}}}
\newcommand{\cG}{\ensuremath{\cel{G}}}
\newcommand{\cH}{\ensuremath{\cel{H}}}
\newcommand{\cI}{\ensuremath{\cel{I}}}
\newcommand{\cJ}{\ensuremath{\cel{J}}}
\newcommand{\cK}{\ensuremath{\cel{K}}}
\newcommand{\cL}{\ensuremath{\cel{L}}}
\newcommand{\cM}{\ensuremath{\cel{M}}}
\newcommand{\cN}{\ensuremath{\cel{N}}}
\newcommand{\cO}{\ensuremath{\cel{O}}}
\newcommand{\cP}{\ensuremath{\cel{P}}}
\newcommand{\cQ}{\ensuremath{\cel{Q}}}
\newcommand{\cR}{\ensuremath{\cel{R}}}
\newcommand{\cS}{\ensuremath{\cel{S}}}
\newcommand{\cT}{\ensuremath{\cel{T}}}
\newcommand{\cU}{\ensuremath{\cel{U}}}
\newcommand{\cV}{\ensuremath{\cel{V}}}
\newcommand{\cW}{\ensuremath{\cel{W}}}
\newcommand{\cX}{\ensuremath{\cel{X}}}
\newcommand{\cY}{\ensuremath{\cel{Y}}}
\newcommand{\cZ}{\ensuremath{\cel{Z}}}

\newcommand{\cva}{\ensuremath{\celv{a}}}
\newcommand{\cvb}{\ensuremath{\celv{b}}}
\newcommand{\cvc}{\ensuremath{\celv{c}}}
\newcommand{\cvd}{\ensuremath{\celv{d}}}
\newcommand{\cve}{\ensuremath{\celv{e}}}
\newcommand{\cvf}{\ensuremath{\celv{f}}}
\newcommand{\cvg}{\ensuremath{\celv{g}}}
\newcommand{\cvh}{\ensuremath{\celv{h}}}
\newcommand{\cvi}{\ensuremath{\celv{i}}}
\newcommand{\cvj}{\ensuremath{\celv{j}}}
\newcommand{\cvk}{\ensuremath{\celv{k}}}
\newcommand{\cvl}{\ensuremath{\celv{l}}}
\newcommand{\cvm}{\ensuremath{\celv{m}}}
\newcommand{\cvn}{\ensuremath{\celv{n}}}
\newcommand{\cvo}{\ensuremath{\celv{o}}}
\newcommand{\cvp}{\ensuremath{\celv{p}}}
\newcommand{\cvq}{\ensuremath{\celv{q}}}
\newcommand{\cvr}{\ensuremath{\celv{r}}}
\newcommand{\cvs}{\ensuremath{\celv{s}}}
\newcommand{\cvt}{\ensuremath{\celv{t}}}
\newcommand{\cvu}{\ensuremath{\celv{u}}}
\newcommand{\cvv}{\ensuremath{\celv{v}}}
\newcommand{\cvw}{\ensuremath{\celv{w}}}
\newcommand{\cvx}{\ensuremath{\celv{x}}}
\newcommand{\cvy}{\ensuremath{\celv{y}}}
\newcommand{\cvz}{\ensuremath{\celv{z}}}

\providecommand{\cmA}{\ensuremath{\celm{A}}}
\providecommand{\cmB}{\ensuremath{\celm{B}}}
\providecommand{\cmC}{\ensuremath{\celm{C}}}
\providecommand{\cmD}{\ensuremath{\celm{D}}}
\providecommand{\cmE}{\ensuremath{\celm{E}}}
\providecommand{\cmF}{\ensuremath{\celm{F}}}
\providecommand{\cmG}{\ensuremath{\celm{G}}}
\providecommand{\cmH}{\ensuremath{\celm{H}}}
\providecommand{\cmI}{\ensuremath{\celm{I}}}
\providecommand{\cmJ}{\ensuremath{\celm{J}}}
\providecommand{\cmK}{\ensuremath{\celm{K}}}
\providecommand{\cmL}{\ensuremath{\celm{L}}}
\providecommand{\cmM}{\ensuremath{\celm{M}}}
\providecommand{\cmN}{\ensuremath{\celm{N}}}
\providecommand{\cmO}{\ensuremath{\celm{O}}}
\providecommand{\cmP}{\ensuremath{\celm{P}}}
\providecommand{\cmQ}{\ensuremath{\celm{Q}}}
\providecommand{\cmR}{\ensuremath{\celm{R}}}
\providecommand{\cmS}{\ensuremath{\celm{S}}}
\providecommand{\cmT}{\ensuremath{\celm{T}}}
\providecommand{\cmU}{\ensuremath{\celm{U}}}
\providecommand{\cmV}{\ensuremath{\celm{V}}}
\providecommand{\cmW}{\ensuremath{\celm{W}}}
\providecommand{\cmX}{\ensuremath{\celm{X}}}
\providecommand{\cmY}{\ensuremath{\celm{Y}}}
\providecommand{\cmZ}{\ensuremath{\celm{Z}}}
 \usepackage{url}

\newcommand{\mGamma}{\mat{\Gamma}}
\newcommand{\mGammahat}{\mat{\hat{\Gamma}}}
\newcommand{\mLambdahat}{\mat{\hat{\Lambda}}}
\newcommand{\mXhat}{\mat{\hat{X}}}
\newcommand{\mRhat}{\mat{\hat{R}}}
\newcommand{\mYhat}{\mat{\hat{Y}}}
\newcommand{\mZhat}{\mat{\hat{Z}}}
\newcommand{\mQhat}{\mat{\hat{Q}}}
\newcommand{\mHhat}{\mat{\hat{H}}}
\newcommand{\mBhat}{\mat{\hat{B}}}
\newcommand{\vyhat}{\vhat{y}}
\newcommand{\vahat}{\vhat{a}}
\providecommand{\cmLambda}{\ensuremath{\celm{\Lambda}}}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\matlab}{\Matlab}
\newcommand{\MATLAB}{\Matlab}

\DeclareMathSymbol{\minus}{\mathord}{operators}{"2D}
\DeclareMathOperator{\tangle}{\texttt{angle}}

\usepackage{geometry}
 \geometry{twoside,
  letterpaper,
  textheight=562pt,
  textwidth=384pt,
  lmargin=60pt,
marginpar=120pt,
  headheight=50pt,
  headsep=12pt,
  footskip=36pt,
  footnotesep=24pt plus 2pt minus 12pt,
 }

\usepackage{algorithmic}
\usepackage{epstopdf}


\begin{document}

\begin{frontmatter}

\title{The power and Arnoldi methods in an algebra of circulants}
\author{David F.~Gleich\footnotemark[1]\footnotemark[2]}
\address{Sandia National Labs\footnotemark[3], Livermore, CA, United States}
\ead{dfgleic@sandia.gov}


\author{Chen Greif\footnotemark[2]}
\ead{greif@cs.ubc.ca}


\author{James M.~Varah\footnotemark[2]}
\address{The University of British Columbia, Vancouver, BC, Canada}
\ead{varah@cs.ubc.ca}


\begin{abstract}
Circulant matrices play a central role in a recently proposed
formulation of three-way data computations.
In this
setting, a three-way table corresponds to a matrix where each
``scalar'' is a vector of parameters defining a circulant.
This interpretation
provides many generalizations of results from matrix or
vector-space algebra. We derive the power and Arnoldi methods
in this algebra.
In the course of our derivation, we define inner products, norms, and other notions.
These extensions are straightforward in an
algebraic sense, but the implications are dramatically different
from the standard matrix case. For example, a matrix of
circulants has a polynomial number of eigenvalues in its
dimension; although, these can all be represented by a
carefully chosen canonical set of eigenvalues and vectors.
These results and algorithms are closely related to standard decoupling
techniques on block-circulant matrices using the fast Fourier transform.
\end{abstract}


\begin{keyword}
 block-circulant \sep circulant module \sep tensor
 \sep FIR matrix algebra \sep power method \sep Arnoldi process
\end{keyword}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{\scriptsize Corresponding author.  
Half of this author's work was conducted at the University
of British Columbia.}
\footnotetext[2]{\scriptsize The work of this author was
 supported in part by the Natural Sciences and Engineering Research
 Council of Canada}
\footnotetext[3]{\scriptsize Sandia National Laboratories is a multi-program laboratory 
managed and operated by Sandia Corporation, a wholly owned subsidiary of 
Lockheed Martin Corporation, for the U.S. Department of Energy's National 
Nuclear Security Administration under contract DE-AC04-94AL85000.}


\end{frontmatter}

\section{Introduction}



We study iterative algorithms in a circulant algebra, which is
a recent proposal for a set of operations that generalize matrix
algebra to three-way data \cite{kilmer2008-circ-tensor-svd}.
In particular, we extend this algebra
with the ingredients required for iterative methods such as the
power method and Arnoldi method, and study the behavior of
these two algorithms.



Given an  table of data, we view this data
as an  matrix where each ``scalar'' is a vector of
length .  We denote the space of length- scalars as .
These scalars interact like circulant matrices.
Circulant matrices are a commutative, closed class under the standard matrix operations.
Indeed,  is the ring of circulant matrices, where we
identify each circulant
matrix with the  parameters defining it.

Formally, let .
Elements in the circulant algebra are denoted by an underline to distinguish
them from regular scalars.  When an element is written with an explicit
parameter set, it is denoted by braces, for example  In what follows, we will use the notation  to provide an equivalent matrix-based notation for an operation involving .
We define the operation 
as the ``circulant matrix representation'' of a scalar:
				
Let  be as above, and also let  .
The basic addition  and multiplication operations between scalars are then

We use here a special symbol, the  operation, to denote the product between these scalars, highlighting the difference from the standard matrix product.
Note that the element
 is
the multiplicative identity.

Operations between vectors and matrices have similar, matricized, expressions. 
We use  to denote the space of length- vectors where each
component is a -vector in , and   to denote the space of  
matrices of these -vectors.  Thus, we identify each  table 
with an element of .  Let  and .  Their product is:

Thus, we extend the operation  to matrices and vectors of  scalars so that

The definition of the product can now be compactly written as

Of course this notation also holds for the special case of scalar-vector multiplication.  Let .  Then



The above operations define the basic computational routines to treat 
  arrays as  matrices of .  
They are equivalent to those proposed by \citet{kilmer2008-circ-tensor-svd}, 
and they constitute a module over vectors composed of circulants, as 
shown recently in~\citet{braman201x-tensor-eigenvalues}.  Based on this 
analysis, we term the set of operations the \emph{circulant algebra}.  
We note that these operations have more efficient implementations,
which will be discussed in Sections~\ref{sec:fft}~and~\ref{sec:computations}.

The circulant algebra analyzed in this paper is closely related to the
\emph{FIR matrix algebra} due to
\citet[Chapter 3]{lambert1996-thesis}.
Lambert proposes an algebra
of circulants; but his circulants are
padded with additional zeros to better
approximation a finite impulse response
operator.  He uses it to study
blind deconvolution problems~\cite{lambert2001-polynomials-svd}.
As he observed, the relationship with matrices implies that many
standard decompositions and techniques from real or complex
valued matrix algebra carry over to the circulant algebra.

The circulant algebra in this
manuscript is a particular instance of a matrix-over-a-ring,
a long studied generalization of linear algebra
\cite{McDonald1984-ring-algebra, brewer1986-linear-systems}.
Prior work focuses on
Roth theorems for the
equation  \cite{Gustafson1979-Roth-theorems}; generalized inverses
\cite{Prasad1994-generalized}; completion and
controllability problems \cite{Gurvits1992-controllability};
matrices over the ring of integers for
computer algebra systems \cite{Hafner1991-matrices-over-rings};
and transfer functions and linear dynamic systems \cite{Sontag1976-ring-systems}.
Finally, see \citet{Gustafson1991-modules-and-matrices} for some interesting relationships between vectors space
theory and module theory.
A recent proposal extends many of
the operations in \citet{kilmer2008-circ-tensor-svd}
to more general algebraic structures \cite{Navasca2010-modules}.

Let us provide some further context on related work.
Multi-way arrays, tensors, and hypermatrices
are a burgeoning area of research;
see \citet{kolda2009-tensor-decompositions}
for a recent  comprehensive survey.  Some
of the major themes are multi-linear
operations, fitting multi-linear
models, and multi-linear generalizations
of eigenvalues \cite{Qi2007-tensor-eigenvalues}.
The formulation in this paper gives rise to stronger
relationships with the literature
on block-circulant matrices,
which have
been studied for quite some time.
See \citet{tee2005-block-circulant} and the references therein 
for further historical and mathematical context on circulant matrices.
In particular, \citet{baker1989-block-circulant-svd} gives a procedure
for the SVD of a block circulant that involves using
the fast Fourier transform to decouple
the problem into independent sub-problems, just
as we shall do throughout this manuscript.
Other work in this vein includes solving
block-circulant systems that arise in
the theory of antenna arrays: \cite{sinott1973-antenna-arrays,mazancourt1983-block-circulant,vescovo1997-block-circulant}.

The remainder of this paper is structured as follows.
We first derive a few necessary operations in Section~\ref{sec:ops}, 
including an inner product and norm. We then continue this discussion by studying these same operations 
using the Fourier transform of the underlying circulant matrices 
(Section~\ref{sec:fft}).  A few theoretical properties of eigenvalues 
in the circulant algebra are analyzed in Section~\ref{sec:eigen}.  
That section is a necessary prelude to the subsequent discussion of 
how the power method~\cite{vonMises1929-power} and the Arnoldi 
method~\cite{Krylov1931-equations,lanczos1950-iteration,Arnoldi1951-minimized}
 generalize to this algebra, 
which comes in Section~\ref{sec:algs}.
We next explain how we implemented these operations in a \Matlab 
package (Section~\ref{sec:computations}); and we provide a numerical 
example of the algorithms (Section~\ref{sec:example}).  
Section~\ref{sec:conclusion} concludes the manuscript with some 
ideas for future work.






\section{Operations with the power method}
\label{sec:ops}

In the introduction, we provided the basic set of operations in the circulant algebra 
(eqs.~\eqref{eq:circ-op-start}-\eqref{eq:circ-op-end}).  We begin this section by stating the standard power method, and then follow by deriving the operations it requires.

Let  and let  be an arbitrary starting vector.  Then the power method proceeds by repeated applications of ; see Figure~\ref{fig:power-standard} for a standard algorithmic description. (Line \ref{alg:power:conv} checks for convergence and is one of several possible stopping criteria.) Under mild and well-known conditions (see \citet{Stewart2001-eigensystems}), this iteration converges to the eigenvector with the largest magnitude eigenvalue.

\begin{figure}
\caption{The power method for a matrix .
}
\label{fig:power-standard}
\begin{algorithmic}[1]
\REQUIRE  
\STATE 
\FOR { until convergence}
  \STATE 
  \STATE 
  \STATE 
  \IF {} \label{alg:power:conv}
    \RETURN 
  \ENDIF
\ENDFOR
\end{algorithmic}
\end{figure}

Not all of the operations in Figure~\ref{fig:power-standard} are defined for the
circulant algebra.  In the
first line, we use the norm  that returns
a scalar in .  We also use the scalar inverse .
The next operation is the  function for a scalar.
Let us define these operations, in order of
their complexity.  In the next section, we will reinterpret
these operations in light of the relationships between
the fast Fourier transform and circulant matrices.
This will help illuminate a few additional properties
of these operations and will let us state an ordering for elements.

\subsection{The scalar inverse}
We begin with the scalar inverse.  Recall that
all operations between scalars behave like circulant
matrices.  Thus, the inverse of  is

The matrix  is also
circulant~\cite{davis1979-circulant}.

\subsection{Scalar functions and the angle function}
Other scalar functions are also functions of a
matrix (see
\citet{Higham2008-functions-of-matrices}).
Let 
be a function, then

where the right hand side is the same function
applied to a matrix.  (Note that it is not the function applied
to the matrix element-wise.)  



The sign function for a matrix is a special case.
As explained in~\citet{Higham2008-functions-of-matrices},
the sign function applied to a complex value is
the sign of the real-valued part.  We wish to use
a related concept that generalizes the real-valued
sign that we term ``angle.''  Given a complex value
, then
.
For real or complex numbers , we then have

Thus, we define




\subsection{Inner products, norms, and conjugates}
\label{sec:norm}

We now proceed  to define a norm.  The norm of
a vector in  produces a scalar in :

For a standard vector , the norm
.  This definition, in turn,
follows from the standard inner product attached
to the vector space .  As we shall see, our definition
has a similar interpretation.  The inner product implied by our
definition is

Additionally, this definition implies that that the \emph{conjugate}
operation in the circulant algebra corresponds to the transpose
of the circulant matrix

With this conjugate, our inner product
satisfies two of the standard properties: conjugate symmetry
 and
linearity .
The notion of positive definiteness is more intricate and we
delay that discussion until after introducing
a decoupling technique using the fast Fourier transform
in the following section.   Then, in Section~\ref{sec:triangle},
we use positive definiteness to
demonstrate a Cauchy-Schwarz inequality, which in turn
provides a triangle inequality for the norm.




\section{Operations with the fast Fourier transform}
\label{sec:fft}
In Section~\ref{sec:ops}, we explained the basic operations
of the circulant algebra as operations between matrices.
All of these matrices consisted of circulant blocks.
In this section, we show how to accelerate these operations
by exploiting the relationship between the fast Fourier transform and circulant matrices.

Let  be a  circulant
matrix. Then the
eigenvector matrix of  is given by the 
discrete Fourier transform matrix , where

and .
This matrix is complex symmetric, , and
unitary, .  Thus, ,
.
Recall that multiplying a vector by  or  can be
accomplished via the fast Fourier transform in
 time instead of 
for the typical matrix-vector product algorithm.  Also,
computing the matrix  can be done in time 
as well.

To express our operations, we define a new transformation,
the ``Circulant Fourier Transform'' or .  Formally,
 and its
inverse  as follows:

where  are the eigenvalues of  as produced
in the Fourier transform order.  These transformations satisfy
 and provide a convenient way of moving
between operations in  to the more familiar environment
of diagonal matrices in .

The  and  transformations are extended to matrices and
vectors over 
differently than the  operation we saw before.   Observe
that  applied ``element-wise'' to the  matrix produces a matrix
of diagonal blocks.  In our extension of the  routine,
we perform an additional permutation to expose
block-diagonal structure from these diagonal blocks.  This permutation
 
transforms an  matrix of 
diagonal blocks into a block diagonal  with  size
blocks.
It is also known as a
\emph{stride permutation matrix}~\cite{Granata1992-tensor}.
The construction of , expressed in \textsc{Matlab} code is
\begin{quote}
\begin{verbatim}
p = reshape(1:m*k,k,m)';
Pm = sparse(1:m*k,p(:),1,m*k,m*k);
\end{verbatim}
\end{quote}
The construction for  is identical.
In Figure~\ref{fig:circ-fft}, we illustrate the overall
transformation process that extends  to matrices
and vectors.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{fft-fig}

\caption{The sequence of transformations in our  operation.
Given a circulant , we convert it into a matrix by .
The color of the circles in the figure is emphasizing the circulant
structure, and not equality between blocks.  In the third
figure, we diagonalize each
circulant using the Fourier transform.  The pattern of
eigenvalues is represented by squares.  Here, we are coloring
the squares to show the reordering induced by the permutation
at the final step of the  operation.}
\label{fig:circ-fft}
\end{figure}

Algebraically, the  operation for a matrix  is

where  and  are the permutation matrices introduced
above.
We can equivalently write this directly in terms of the
eigenvalues of each of the circulant blocks of :

where  are the diagonal
elements of .
The inverse operation , takes a block diagonal matrix and returns
the matrix in :


Let us close this discussion by providing a concrete example of this operation.
\begin{example} \label{ex:cft}
Let  .
The result of the  and  operations, as illustrated in Figure~\ref{fig:circ-fft}, are:

\end{example}


\subsection{Operations}
We now briefly illustrate how the  accelerates
and simplifies many operations.  Let .
Note that

In the Fourier space -- the output of the  operation --
these operations are both  time because they occur between diagonal
matrices.
Due to the linearity of the  operation, arbitrary sequences of
operations in the Fourier space
transform back seamlessly, for instance

But even more importantly, these simplifications generalize to matrix-based
operations too.  For example,

In fact, in the Fourier space, this system is a series of
independent matrix vector products:

Here, we have again used  and  to denote the
blocks of Fourier coefficients, or equivalently, circulant eigenvalues.
\emph{The rest of the paper frequently uses this convention and
shorthand where it is clear from context.}
This formulation takes

operations instead of  using the  formulation
in the previous section.

More operations are simplified in the Fourier space too.
Let .
Because the  values are the eigenvalues of , the following functions simplify:


\paragraph{Complex values in the CFT}

A small concern with the  operation is that it may produce
complex-valued elements in .
It suffices to note that when the output of a sequence of
circulant operations produces a real-valued circulant,
then the output of  is also real-valued.  In other
words, there is no problem working in Fourier space instead
of the real-valued circulant space.  This
fact can be formally verified by first formally stating the conditions
under which  produces real-valued circulants
( is real if and only if ,
see \citet{davis1979-circulant}),
and then
checking that the operations in the Fourier space do not
alter this condition.


\subsection{Properties}
Representations in Fourier space are convenient for illustrating
some properties of these operations. 

\begin{proposition}
The matrix  is orthogonal.
\end{proposition}
\begin{proof} We have

\end{proof}
Additionally, the Fourier space is an easy place to understand
spanning sets and bases in , as the following proposition shows.
\begin{proposition} \label{thm:basis}
Let . Then
 spans  \emph{if and only if} 
and  have rank .  Also 
is a basis \emph{if and only if} 
and  are invertible.
\end{proposition}
\begin{proof}
First note that 
because  is a similarity transformation applied to .
It suffices to show this result for , then.  Now consider
:

Thus, if there is a 
that is feasible, then all  must be rank .
Conversely, if  has rank 
then each  must have rank , and any 
is feasible.  The result about the basis follows
from an analogous argument.
\end{proof}

\subsection{Inner products, norms, and ordering}
\label{sec:triangle}

We now return to our inner product and norm to elaborate on the
positive-definiteness and the triangle inequality.
In terms of the Fourier transform,

If we write this in terms of the blocks of Fourier coefficients then

For , each diagonal term has the form .
Consequently, we do consider this a positive semi-definite inner product because
the output  is a matrix with non-negative
eigenvalues.  This idea motivates the following definition of element ordering.
\begin{definition}[Ordering] \label{def:ordering}
 Let .  We write
 
\end{definition}


We now show that our inner product satisfies the Cauchy-Schwarz
inequality:

In Fourier space, this fact holds because
 follows from the
standard Cauchy-Schwarz inequality.  Using this inequality,
we find that our norm satisfies the triangle inequality:

In this expression, the constant  is twice the multiplicative identify, that is .



\section{Eigenvalues and Eigenvectors}
\label{sec:eigen}

With the results of the previous few sections, we can
now state and analyze an eigenvalue problem in
circulant algebra.  \citet{braman201x-tensor-eigenvalues} investigated
these already and proposed a decomposition approach to compute them.
We offer an extended analysis that addresses a few additional aspects.
Specifically, we focus on a {\em canonical} set of eigenpairs.


Recall that eigenvalues of matrices are the roots of the
characteristic polynomial 
Now let 
and .
The eigenvalue problem does not change:

(As an aside, note that the standard properties of the determinant hold for any
matrix over a commutative ring with identity; in particular,
the Cayley-Hamilton theorem holds in this algebra.)
The existence of an eigenvalue implies
the existence of a corresponding eigenvector .
Thus, an eigenvalue and eigenvector pair in this algebra is



Just like the matrix case, these eigenvectors can
be rescaled by any constant :

In terms of normalization, note that

if  is an orthogonal circulant.  This follows
most easily by noting that

because circulant matrices commute and  is orthogonal by construction.
For this reason, we consider orthogonal circulant matrices the
analogues of \emph{angles} or \emph{signs}, and normalized eigenvectors in the circulant algebra
can be rescaled by them. (Recall that we showed
that  is an orthogonal circulant in
Section~\ref{sec:fft}.)




The Fourier transform offers a
convenient decoupling procedure to compute eigenvalues and eigenvectors,
as observed by \citet{braman201x-tensor-eigenvalues}.
 Let  and let
 and  be an eigenvalue
and eigenvector pair: 
and .
Then it is straightforward to show that the Fourier transforms , ,
and  decouple as follows:

where  and .
The last equation follows because


The decoupling procedure we just described shows that \emph{any}
eigenvalue or eigenvector of  must
decompose into individual eigenvalues or eigenvectors of the
-transformed problem.
This illustrates a fundamental difference from the
standard matrix algebra. For standard matrices, requiring
 and finding a nonzero solution
 for  are equivalent. In contrast,
the determinant and the eigenvector
equations are not  equivalent in the circulant algebra:
 actually has an
infinite number of solutions .  For instance,
set 
to be an eigenpair of  and  for , then any
value for  solves .
However, only a few of these solutions also satisfy
.


Eigenvalues of matrices in  have some
interesting properties. Most notably, a matrix may have more than 
eigenvalues. As a special case, the diagonal elements of a
matrix are not necessarily the only eigenvalues.  We demonstrate
these properties with an example.
\begin{example} \label{ex:diag-evals}
 For the diagonal matrix
  
  we have
  





Thus,

 The corresponding eigenvectors are
  3ex]
       \cvx_3 = \bmat{\csbmat{1 & 0 & 0} \\ \csbmat{0 & 0 & 0}} ;
     &
      \cvx_4 = \bmat{\csbmat{0 & 0 & 0} \\ \csbmat{1 & 0 & 0}}.
  \end{array}
  
\begin{aligned}
\clambda_5 & = \ifft(\diag\sbmat{6 & \minus\ii\sqrt{3} & 2}) &
 \qquad
\clambda_6 & = \ifft(\diag\sbmat{6 & 2 & \ii\sqrt{3}}) \\
\clambda_7 & = \ifft(\diag\sbmat{5 & \minus\ii \sqrt{3} & 2}) &
\qquad
\clambda_8 & = \ifft(\diag\sbmat{5 & 2 & \ii \sqrt{3}}). \\
\end{aligned}
  \mAhat_1 = \bmat{6 & 6\\0 & 5}, \quad
   \mAhat_2 = \bmat{\minus \sqrt{3} & \minus9 + \ii\sqrt{3} \\ \minus3 + \ii\sqrt{3} & 2}, \quad
   \mAhat_3 = \bmat{\ii \sqrt{3} & \minus9 - \ii\sqrt{3} \\ \minus3 + \ii\sqrt{3} & 2}. 
\begin{aligned}
\clambda_1 & = \csbmat{1.9401 & \minus1.6814 & 5.7413} &
\clambda_2 & = \csbmat{3.0599 & 3.6814 & \minus1.7413}  \\
\clambda_3 & = \csbmat{3.3933 & 4.0147 & \minus1.4080} &
\clambda_4 & = \csbmat{1.6067 & \minus2.0147 & 5.4080} .
\end{aligned}

\begin{aligned}
\clambda_5 & = \csbmat{4.6966 - 1.5654\ii, & \minus0.7040 + 1.9114\ii, & 2.0073 - 0.3461\ii} \\
\clambda_6 & = \csbmat{3.6367 + 2.1427\ii, & 3.0373 + 0.3980\ii, & \minus0.6740 - 2.5407\ii}  \\
\clambda_7 & = \csbmat{4.3633 - 1.5654\ii, & \minus1.0373 + 1.9114\ii, & 1.6740 - 0.3461\ii } \\
\clambda_8 & = \csbmat{3.3034 + 2.1427\ii, & 2.7040 + 0.3980\ii, & \minus1.0073 - 2.5407\ii} .
\end{aligned}
 \cmA = \cmX \circ \cmLambda \circ \cmX^{-1} .  \begin{aligned}
& \fft( \cmA \circ \cvx \circ (\normof{\cmA \circ \cvx})^{-1} ) \\
& \qquad  = \fft(\cmA) \fft(\cvx) (\fft(\cvx)^* \fft(\cvx))^{-1/2}  \\
& \qquad = \sbmat{\mAhat_1\vxhat_1 \\ & \ddots \\ & & \mAhat_k \vxhat_k}
  \left( \sbmat{\mAhat_1\vxhat_1 \\ & \ddots \\ & & \mAhat_k \vxhat_k}^*
   \sbmat{\mAhat_1\vxhat_1 \\ & \ddots \\ & & \mAhat_k \vxhat_k} \right)^{-1/2}.\\
  \end{aligned} 
\begin{aligned}
\left(
  \sbmat{\mAhat_1\vxhat_1 \\ & \ddots \\ & & \mAhat_k \vxhat_k}^*
  \sbmat{\mAhat_1\vxhat_1 \\ & \ddots \\ & & \mAhat_k \vxhat_k}
\right)^{-1/2}
& =
\sbmat{ \vxhat_1^* \mAhat_1^* \mAhat_1 \vxhat_1 \\
        & \ddots \\ & & \vxhat_k^* \mAhat_k^* \mAhat_k \vxhat_k }^{-1/2}
\\ & =
\sbmat{ \normof{\mAhat_1 \vxhat_1}^{-1} \\
        & \ddots \\ & & \normof{\mAhat_k \vxhat_k}^{-1} }.
\end{aligned}
 \fft( \cmA \circ \cvx \circ (\normof{\cmA \circ \cvx})^{-1} )
= \sbmat{\mAhat_1 \vxhat_1 / \normof{\mAhat_1 \vxhat_1} \\
         & \ddots \\
         & & \mAhat_k \vxhat_k / \normof{\mAhat_1 \vxhat_1}}.
 \label{eq:power-convergence}
 \normof{ \tangle(\cvx_1\itn{k})^{-1} \circ \cvx\itn{k}
     - \tangle(\cvx\itn{k-1}_1)^{-1} \circ \cvx\itn{k-1} } < \cel{\tau}.
 \mY \leftarrow \mA \mX\itn{k}, \qquad
     \mX\itn{k+1}, \mR\itn{k+1} = \text{\texttt{qr}}(\mY).

 \mathcal{K}_t(\mA,\vv) = \tspan\{ \vv, \mA \vv, \ldots, \mA^{t-1} \vv \} ,
 \mA \mQ_t = \mQ_{t+1} \mH_{t+1,t}  \cmA \circ \cmQ_{t} = \cmQ_{t+1} \circ \cmH_{t+1,t}.  \tvec(\calpha) \eqdef \sbmat{ \alpha_1 \\ \vdots \\ \alpha_k} = \tcirc(\calpha) \ve_1 ,  -\Delta u(x,y) = f(x,y) \qquad u(x,0) = u(x,1), u(0,y) = y(1,y) = 0 \qquad (x,y) \in [0,1] \times [0,1].  -\Delta u(x_i,y_j) \approx -u(x_{i-1},y_j) -u(x_i,y_{j-1}) + 4 u(x_i,y_j) - u(x_{i+1},y_j) - u(x_i,y_{j+1}).  \underbrace{\bmat{
    \mC & -\mI \\
    -\mI & \mC & \ddots \\
    & \ddots & \ddots & -\mI \\
    & & -\mI & \mC \\
   }}_{\mA} \underbrace{\bmat {
    \vu(x_1, \cdot) \\
    \vu(x_2, \cdot) \\
    \vdots \\
    \vu(x_{N-1}, \cdot)
  }}_{\vu} = \underbrace{\bmat{
    \vf(x_1, \cdot) \\
    \vf(x_2, \cdot) \\
    \vdots \\
    \vf(x_{N-1}, \cdot)
  }}_{\vf}, \qquad
  \mC = \underbrace{\bmat{
    4 & -1 & & -1\\
    -1 & 4 & \ddots \\
    & \ddots & \ddots & -1 \\
    -1 & & -1 & 4 \\
   }}_{N \times N} ,
   \cmA \circ \cvu = \cvf  \mA =\tcirc(\cmA) \qquad  \vu =  \tvec(\cvu)  \qquad \vf = \tvec(\cvf).  \clambda_j = \csbmat{ 4 + 2\cos(j \pi / {N}), -1, 0, \ldots, 0, -1 }.  (\cmA - \clambda(\mu) \circ \cmI)
 = \bmat{ (4-\mu) \circ \cel{1} & -1 \circ \cel{1} \\
          -1 \circ \cel{1} & (4-\mu) \circ \cel{1} & \ddots \\
          & \ddots & \ddots & -1 \circ \cel{1} \\
          & & -1 \circ \cel{1} & (4-\mu) \circ \cel{1} }.

\begin{aligned}
\cft(\clambda_1) & =
  \diag\sbmat{ \gamma_1 + \delta_1, & \ldots, \gamma_1 + \delta_N} \\
\cft(\clambda_2) & =
  \diag\sbmat{ \gamma_2 + \delta_1, & \ldots, \gamma_2 + \delta_N}	
\end{aligned}
 \begin{aligned}
\min_j \frac{\lambda_2(\mAhat_j)}{\lambda_1(\mAhat_j)} & = \frac{\gamma_2 + \delta_1}{\gamma_1 + \delta_1} = \frac{2 + 2\cos(2\pi/N)}{2 + 2\cos(\pi/N)} && \qquad \text{(fastest)} \\
\max_j \frac{\lambda_2(\mAhat_j)}{\lambda_1(\mAhat_j)} & = \frac{\gamma_2 + \delta_{N/2+1}}{\gamma_1 + \delta_{N/2+1}} = \frac{6 + 2\cos(2\pi/N)}{6 + 2\cos(\pi/N)} && \qquad \text{(slowest)}. \\
\end{aligned}
 \cvu\itn{t} \approx
 \cmQ_t \circ \mathop{\mathrm{arg\,min}}_{\cvy \in\KK_k} \normof{\cmH_{t+1,t} \circ \cvy - \cbeta \circ \cve_1},
 \absof{\calpha} \eqdef \normof[2]{\tcirc(\calpha)} = \normof[1]{\cft(\calpha)}.  \tabs(\calpha) \le \tabs(\cbeta) \qquad  \Rightarrow \qquad \absof{\calpha} \le \absof{\cbeta}. 
We implement this operation as the \texttt{mag} function
in our \Matlab package.


\end{document}
