\section{Experiments}

\subsection{Experiment Setup}

\subsubsection{Datasets.}
We evaluate models on two VLN tasks with high-level instructions, REVERIE~\cite{qi2020reverie} and SOON~\cite{zhu2021soon}.
Each dataset is divided into training, val seen, val unseen and a hidden test unseen split. 
The statistics of training splits are presented in Table~\ref{tab:hm3d_autovln_stats}.
The REVERIE dataset provides groundtruth object bounding boxes at each location and only requires an agent to select one of the bounding boxes. The shortest path from the agent's initial location to the target location is between 4 to 7 steps.
The SOON dataset instead does not give groundtruth bounding boxes, and asks an agent to directly predict the orientation of the object's center. The path lengths are also longer than REVERIE with 9.5 steps on average.
Due to the increased task difficulty and smaller training dataset size, it is more challenging to achieve high performance on SOON  than on REVERIE. 

\subsubsection{Evaluation metrics.}
We measure two types of metrics for the VLN task, navigation-only and remote object grounding.
The navigation-only metrics focus solely on whether an agent arrives at the target location. We use standard navigation metrics~\cite{anderson2018vision} including
\textbf{Success Rate (SR)}, the percentage of paths with the \emph{final} location near any target locations within 3 meters; 
\textbf{Oracle Success Rate (OSR)}, the percentage of paths with \emph{any} location close to target within 3 meters; and
\textbf{SR weighted by Path Length (SPL)} which multiplies SR with the ratio between the length of shortest path and the agent's predicted path.
As SPL takes into account navigation accuracy and efficiency, it is the primary metric in navigation.
The remote object grounding metrics~\cite{qi2020reverie} consider both navigation and object grounding performance.
The standard metrics are \textbf{Remote Grounding Success (RGS)} and \textbf{RGS weighted by Path Length (RGSPL)}.
RGS in REVERIE is defined as correctly selecting the object among groundtruth bounding boxes, while in SOON as predicting a center point that is inside of the groundtruth polygon.
RGSPL penalizes RGS by path length similar to SPL.
For all these metrics, higher is better.

\subsubsection{Implementation details.}
We adopt ViT-B/16~\cite{dosovitskiy2020image} pretrained on ImageNet to extract view and object features.
For SOON dataset, we use the Mask2Former trained on ADE20K~\cite{cheng2021mask2former} to extract object bounding boxes and transfer the prediction in the same way as in REVERIE.
We use the same hyper-parameters in modeling as the DUET model \cite{chen2022duet}.
All experiments were run on a single Nvidia RTX8000 GPU.
The best epoch is selected based on SPL on the val unseen split.

\subsection{Ablation Studies}
The main objective of our work is to explore how much VLN agents can benefit from large-scale synthesized dataset.
In this section, we carry out extensive ablations on the REVERIE dataset to study the effectiveness of our HM3D-AutoVLN dataset and key design choices.


\subsubsection{Contributions from HM3D-AutoVLN dataset.}
In Table~\ref{tab:rvr_instr_vln}, we compare the impact of VLN data generated from different sources for training DUET. 
Row 1 (R1) only relies on manually annotated instructions on 60 buildings in MP3D. Row 2 (R2) utilizes groundtruth room and object annotations to generate instructions for all objects in the 60 seen buildings. Such data is of high-quality and significantly improves VLN performance,~\eg~3\% on SPL.
In R3, we only use our generated HM3D-AutoVLN for pretraining, while still finetuning on the REVERIE train split.
Due to the visual diversity of the 900 additional buildings, it brings a larger boost than R2.
Compared to R1, the improvement of the SPL metric is 8.6\%, whereas other metrics like SR improve by 6.5\%.
This suggests that the pretraining on large-scale environments enables the model to learn efficient exploration, even without any additional manual annotations.
Moreover, when the model is fine-tuned jointly on the HM3D-AutoVLN and REVERIE datasets~(R4), the SR metric is further improved by 5\% over R3. This indicates that fine-tuning on the downstream dataset alone may suffer from forgetting and lead to worse generalization performance. 
Compared to the navigation metrics, the remote object grounding metrics see modest improvements.
We hypothesize that though our generated instructions often describe the object and scene accurately, they are not discriminative enough to refer to a specific object in the environment (\eg~unable to discriminate between multiple pillows placed on a sofa) or confuse predicting relationships between objects (\eg~second pillow from the left).
We see improving RGS and RGSPL as a promising future direction that would need to take into account relations between objects. 


\begin{table}[t]
\centering
\tabcolsep=0.13cm
\caption{DUET performance on REVERIE (RVR) val unseen split using different training data. $^{\dagger}$ denotes manual object annotations are used to synthesize data}
\label{tab:rvr_instr_vln}
\begin{tabular}{lccccccc} \toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{Training Data} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} \\
 & Pretrain & Finetune & OSR & SR & SPL & RGS & RGSPL \\ \midrule
R1 & RVR & RVR & 48.74 & 44.36 & 30.79 & 30.30 & 21.08 \\
R2 & RVR+Speaker$^{\dagger}$ & RVR & 51.07 & 46.98 & 33.73 & 32.15 & 23.03 \\
R3 & HM3D & RVR & 54.81 & 50.87 & 39.36 & 34.65 & \textbf{26.79} \\
R4 & HM3D & RVR+HM3D & \textbf{62.14} & \textbf{55.89} & \textbf{40.85} & \textbf{36.58} & 26.76 \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/rvr_vln_num_hm3d_houses}
        \caption{Navigation performance with respect to the number of training environments in HM3D-AutoVLN dataset.}
        \label{fig:rvr_vln_num_hm3d_environments}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rvr_houses_vs_instrs}
        \caption{Given the same number of training examples, collecting data from more environments (blue) performs better than fewer environments.}
        \label{fig:rvr_environments_vs_instrs}
    \end{subfigure}
    \caption{Influence of the number of environments on DUET performance.}
    \label{fig:three graphs}
\end{figure}


\subsubsection{Impact of the number of training environments.}
Here, we evaluate the impact of the number of training environments. 
As pretraining on HM3D-AutoVLN mostly improves navigation performance, we mainly show navigation metrics SR and SPL. The full result table is provided in the supplementary material.
As shown in Figure~\ref{fig:rvr_vln_num_hm3d_environments}, more environments continuously improve the navigation performance.
We observe that even with the full 900 environments in HM3D, the gains are not saturated but increase gradually.
We also evaluate the impact of the number of environments given a fixed budget of VLN training examples.
Figure~\ref{fig:rvr_environments_vs_instrs} shows that using more environments improves the performance. 
On the \textbf{left}, the red bar uses 200 environments with 51,018 instructions, and the blue bar use the same amount of instructions but covers all 900 environments in HM3D. The \textbf{right} part is the same, but uses 400 environments for the red bar. 
The results suggest that it is beneficial to increase the number of environments rather than just increasing the trajectories for a limited number of environments. 



\subsubsection{Finetuning with a small amount of supervised data.}
A large-scale automatically generated dataset improves pretraining and hence can reduce the supervised data requirement in downstream tasks.
We verify the effectiveness of HM3D-AutoVLN on a few-shot learning setting~\cite{guhur2021airbert} that compares the impact of finetuning on a variable number of REVERIE environments, see Table~\ref{tab:rvr_few_shot}.
Even without finetuning on any supervised instructions from REVERIE, our pretrained model achieves fair performance in comparison to a baseline that is only finetuned on all supervised data.
On the SPL metric, we outperform the baseline DUET model when finetuning on 10 environments (1/6 of the supervised data).
Moreover, finetuning with half the original data (30 environments) achieves significant boosts on all metrics compared to the baseline model.
A similar trend can be observed on the SOON dataset, see supplementary material.

\begin{table}[t]
\centering
\tabcolsep=0.13cm
\caption{DUET performance when using a fraction of the supervised data}
\label{tab:rvr_few_shot}
\begin{tabular}{cccccccc} \toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}HM3D\\ Pretrain\end{tabular}} & \multirow{2}{*}{\#environments} & \multirow{2}{*}{\#instructions} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} \\ 
 &  &  & OSR & SR & SPL & RGS & RGSPL \\ \midrule
$\times$ & 60 & 10,466 & 48.74 & 44.36 & 30.79 & 30.30 & 21.08  \\ \midrule
\checkmark & 0 & 0 & 43.08 & 36.81 & 25.28 & 20.82 & 13.74 \\
\checkmark & 1 & 449 & 50.78 & 42.12 & 29.55 & 25.02 & 17.26 \\
\checkmark & 10 & 1,404 & 50.47 & 43.79 & 33.61 & 26.30 & 20.24 \\
\checkmark & 30 & 5,244 & 60.81 & 53.71 & 39.26 & 34.42 & 25.11 \\
\checkmark & 60 & 10,466 & 62.14 & 55.89 & 40.85 & 36.58 & 26.76 \\ \bottomrule
\end{tabular}
\end{table}


\subsubsection{Comparing distance to objects.}
As mentioned in Sec.~\ref{sec:hm3d_vln_instr}, we select some of the visible objects that are close to the agent, within $d_o$ meters, to generate instructions.
In Table~\ref{tab:rvr_depth_obj_filter}, we compare the influence of different distances to the objects on the VLN performance.
Larger $d_o$ allows us to generate more instructions.
We can see that while including additional remote objects increases the number of instructions, it leads to a small drop in performance as the model struggles to identify objects that are small and far away.

\begin{minipage}[t]{\linewidth}
\begin{minipage}[t]{0.45\linewidth}
\makeatletter\def\@captype{table}
\tabcolsep=0.05cm
\scriptsize
\caption{DUET performance on instructions where the visible objects are at a different distance $d_o$ from the agent location}
\label{tab:rvr_depth_obj_filter}
\begin{tabular}{ccccccc} \toprule
\multirow{2}{*}{$d_o$} & \multirow{2}{*}{\#instrs} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} \\
&  & OSR & SR & SPL & RGS & RGSPL \\ \midrule
    2 & 217,703 &\textbf{62.14} & \textbf{55.89} & \textbf{40.85} & \textbf{36.58} & \textbf{26.76} \\
    3 & 396,401 & 57.37 & 53.25 & 40.38 & 34.28 & 25.65 \\ 
    $\infty$ & 544,606 & 59.98 & 53.37 & 38.03 & 35.70 & 25.50 \\ \bottomrule
\end{tabular}
\end{minipage}
\quad
\begin{minipage}[t]{0.48\linewidth}
\makeatletter\def\@captype{table}
\tabcolsep=0.05cm
\scriptsize
\centering
\caption{Comparison of different speaker models in terms of manual captioning evaluation and the followup navigation performance}
\label{tab:instr_eval}
\begin{tabular}{cccccc} \toprule
 & \multicolumn{3}{c}{Captioning} & \multicolumn{2}{c}{Navigation} \\
 & Room & Obj & Rel & SR & RGS \\ \midrule
Template & 0.13 & 0.76 & 0.05 & 52.20 & 32.75 \\
LSTM & 0.58 & 0.65 & 0.27 & 49.59 & 32.29 \\
GPT2 (Ours) & \textbf{0.73} & \textbf{0.78} & \textbf{0.35} & \textbf{55.89} & \textbf{36.58} \\ \bottomrule
\end{tabular}
\end{minipage}
\end{minipage}



\subsubsection{Evaluating quality of the HM3D-AutoVLN dataset.}
We validate each annotation procedure in our automatic dataset construction.
For \emph{navigation graph generation}, we measure whether the graph covers the whole building. Assuming each navigation node covers a circle with a radius of 2 meters, the graph achieves a high coverage rate of 93.4\% on average.
For \emph{3D object labeling}, we randomly select 300 bounding boxes and manually annotate semantic labels for them. We observe that 37.4\% of them are correctly predicted with 2D predictions, whereas 58.3\% of them were correctly predicted when applying cross-view consistency, showing an absolute improvement of 21\%.
Examples in Figure~\ref{fig:quality_pseudolabels} highlight the advantages of using cross-view consistency.
Due to the distorted view, it's reasonable that the 2D models wrongly predict the \texttt{mirror} and \texttt{wardrobe} to be a \texttt{window} and \texttt{door} respectively.
Cross-view consistency also helps to integrate scene context, swapping a \texttt{table} to a \texttt{desk} and a normal \texttt{chair} to a \texttt{swivel chair}.
For \emph{instruction generation}, we further compare our GPT2-based speaker model with template-based methods and a LSTM baseline~\cite{chen2022duet}. 
We manually evaluate 100 randomly selected instructions by measuring whether the instruction correctly mentions the target room, object class and object instance (with correct relations). We also measure the VLN performance using the generated instructions. Our model performs best on manual evaluation and on downstream VLN tasks as shown in Table~\ref{tab:instr_eval}. 





\begin{figure}[t]
    \scriptsize
    \begin{minipage}{.25\linewidth}
      \centering
                \includegraphics[width=\linewidth]{figures/pseudo_label2}
                2D \texttt{window}, 3D \texttt{mirror}
    \end{minipage}\begin{minipage}{.25\linewidth}
      \centering
                \includegraphics[width=\linewidth]{figures/pseudo_label3}
                2D \texttt{door}, 3D \texttt{wardrobe}
    \end{minipage}
    \begin{minipage}{.25\linewidth}
      \centering
                \includegraphics[width=\linewidth]{figures/pseudo_label1}
                2D \texttt{table}, 3D \texttt{desk}
    \end{minipage}\begin{minipage}{.3\linewidth}
      \centering
                \includegraphics[width=0.83\linewidth]{figures/pseudo_label4}\\
                2D \texttt{chair}, 3D \texttt{swivel chair}
    \end{minipage}
        \caption{Qualitative examples of pseudo labelling}
        \label{fig:quality_pseudolabels}
\end{figure}


\subsection{Comparison with State of the Art}
In Table~\ref{tab:reverie_sota_cmpr}, we compare with the state of the art on the REVERIE dataset.
To demonstrate the contributions of our automatically constructed dataset, we further train additional VLN agents with the augmentation of the HM3D-AutoVLN dataset, including EnvDrop~\cite{tan2019learning}, RecBert~\cite{hong2020recurrent} and HAMT~\cite{chen2021hamt}.
Our proposed dataset improves results of all methods and gives a particularly large boost to high-capacity models.
When pretraining DUET on HM3D-AutoVLN, the increase in performance over DUET with pretraining is significant for all metrics on both the val unseen and the test unseen splits. For example, the SPL measure increases by 7.1\% and 2.8\% on val unseen and test unseen splits of REVERIE.
Table~\ref{tab:soon_sota_cmpr} provides results for the SOON dataset.
Note that instructions from the SOON dataset are not used to train our speaker model (\cf~Sec.~\ref{sec:hm3d_vln_instr}) and are somewhat different from the REVERIE instructions.
Nevertheless, the large performance increase demonstrates cross-domain benefits of pretraining on an automatically collected large-scale dataset. Finally, we can also observe that the gain for object grounding is less significant, as discussed before this can be explained by the confusion between objects of the same categories and erroneous spatial relations. 




\begin{table*}[t]
\scriptsize
\centering
\tabcolsep=0.09cm
\caption{Comparison with the state-of-the-art methods on REVERIE dataset}
\label{tab:reverie_sota_cmpr}
\begin{tabular}{l|ccccc|ccccc} \toprule
\multirow{3}{*}{Methods} & \multicolumn{5}{c|}{Val Unseen} & \multicolumn{5}{c}{Test Unseen} \\
& \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c|}{Grounding} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding}  \\ 
& OSR & SR & SPL & RGS & RGSPL & OSR & SR & SPL & RGS & RGSPL\\ \midrule
Human & - & - & - & - & - & 86.83 & 81.51 & 53.66 & 77.84 & 51.44 \\ \midrule
Seq2Seq \cite{anderson2018vision} & 8.07 & 4.20 & 2.84 & 2.16 & 1.63 & 6.88 & 3.99 & 3.09 & 2.00 & 1.58 \\
RCM \cite{wang2019reinforced} & 14.23 & 9.29 & 6.97 & 4.89 & 3.89 & 11.68 & 7.84 & 6.67 & 3.67 & 3.14 \\
SMNA \cite{ma2019self} & 11.28 & 8.15 & 6.44 & 4.54 & 3.61& 8.39 & 5.80 & 4.53 & 3.10 & 2.39 \\
\scriptsize{FAST-MAttNet} \cite{qi2020reverie} & 28.20 & 14.40 & 7.19 & 7.84 & 4.67 & 30.63 & 19.88 & 11.61 & 11.28 & 6.08 \\
SIA \cite{lin2021scene} & 44.67 & 31.53 & 16.28 & 22.41 & 11.56 & 44.56 & 30.80 & 14.85 & 19.02 & 9.20 \\
Airbert \cite{guhur2021airbert} & 34.51 & 27.89 & 21.88 & 18.23 & 14.18 & 34.20 & 30.28 & 23.61  & 16.83 & 13.28 \\ \midrule
EnvDrop~\cite{tan2019learning} &  26.3 & 23.0 & 19.9  & - & - & - & - & - & - & -\\
\quad \scriptsize{+HM3D-AutoVLN} &  29.3 & 25.2 & 20.6 & - & - & - & - & - & - & - \\ 
RecBERT (oscar) \cite{hong2020recurrent} & 27.66 & 25.53 & 21.06 & 14.20 & 12.00 & 26.67 & 24.62 & 19.48 & 12.65 & 10.00 \\ 
\quad \scriptsize{+HM3D-AutoVLN} & 33.23 & 29.20 & 23.12 & 18.12 & 14.18 & - & - & - & - & - \\ 
HAMT \cite{chen2021hamt} & 36.84 & 32.95 & 30.20 & 18.92 & 17.28 & 33.41 & 30.40 & 26.67 & 14.88 & 13.08 \\ 
\quad \scriptsize{+HM3D-AutoVLN}& 42.09 & 37.80 & 31.35 & 23.03 & 18.88 & - & - & - & - & - \\ 
DUET \cite{chen2022duet} & 51.07 & 46.98 & 33.73 & 32.15 & 23.03 & 56.91 & 52.51 & 36.06 & 31.88 & 22.06 \\  
\quad \scriptsize{+HM3D-AutoVLN} & \textbf{62.14} & \textbf{55.89} & \textbf{40.85} & \textbf{36.58} & \textbf{26.76} & \textbf{62.3} & \textbf{55.17} & \textbf{38.88} & \textbf{32.23} & \textbf{22.68} \\  \bottomrule

\end{tabular}
\end{table*}

 
\begin{table}[t]
\centering
\scriptsize
\tabcolsep=0.1cm
\caption{Comparison with the state-of-the-art methods on the SOON dataset}
\label{tab:soon_sota_cmpr}
\begin{tabular}{l|cccc|cccc} \toprule
\multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Val Unseen} & \multicolumn{4}{c}{Test Unseen} \\
 & OSR & SR & SPL & RGSPL & OSR & SR & SPL & RGSPL \\ \midrule
GBE \cite{zhu2021soon} & 28.54 & 19.52 & 13.34 & 1.16 & 21.45 & 12.90 & 9.23 & 0.45 \\
DUET \cite{chen2022duet} & 50.91 & 36.28 & 22.58 & 3.75 & 43.00 & 33.44 & 21.42 & 4.17 \\  \midrule
DUET (+HM3D) & \textbf{53.19} & \textbf{41.00} & \textbf{30.69} & \textbf{4.06}  & \textbf{48.74} & \textbf{40.36} & \textbf{27.83} & \textbf{5.11} \\ \bottomrule
\end{tabular}
\end{table}

