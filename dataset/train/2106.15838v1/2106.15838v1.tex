

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{float}
\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{3212} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\usepackage{wrapfig}
\usepackage{CJK,algorithm,algorithmic,amssymb,amsmath,array,epsfig,graphics,multirow,array,float,subfigure,verbatim,epstopdf}
\usepackage{enumitem}
\usepackage{color,soul}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{seqsplit}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage[ruled,vlined,algo2e]{algorithm2e}
\usepackage{xparse}


\NewDocumentCommand{\heng}{ mO{} }{\textcolor{red}{\textsuperscript{\textit{Heng}}\textsf{\textbf{\small[#1]}}}}

\NewDocumentCommand{\julia}{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{Julia}}\textsf{\textbf{\small[#1]}}}}


\NewDocumentCommand{\chenkai}{ mO{} }{\textcolor{orange}{\textsuperscript{\textit{Chenkai}}\textsf{\textbf{\small[#1]}}}}

\NewDocumentCommand{\liliang}{ mO{} }{\textcolor{brown}{\textsuperscript{\textit{liliang}}\textsf{\textbf{\small[#1]}}}}

\title{HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction}

\author{Liliang Ren, Chenkai Sun,  Heng Ji, Julia Hockenmaier\\ 
  University of Illinois, Urbana Champaign\\
  Department of Computer Science \\
  \texttt{\{liliang3,chenkai5,hengji,juliahmr\}@illinois.edu}
  }


\date{}

\begin{document}
\maketitle
\begin{abstract}
Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a \textbf{Hy}brid \textbf{SP}an Gener\textbf{A}tor (\textbf{HySPA}) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.\footnote{Our code is publicly available at \url{https://github.com/renll/HySPA}
}
\end{abstract}
\section{Introduction}

Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph \cite{li-etal-2014-constructing,hgraph} consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) \cite{florian-etal-2006-factorizing,florian-etal-2010-improving} and Relation Extraction (RE) \cite{sun-etal-2011-semi,jiang-zhai-2007-systematic}, and either perform them separately \cite{chan-roth-2011-exploiting} or jointly \cite{li-ji-2014-incremental,eberts2019span}.

Recent joint IE models \cite{dygie,tse,lin-etal-2020-joint}  have shown impressive performance on various IE tasks, since they can mitigate  error propagation  and leverage  inter-dependencies between the tasks. 
Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a \emph{null} value for most of the cases due to the sparsity of relations between entities. Also,  pairwise scoring techniques evaluate each relation type independently and thus fail to capture  interrelations between relation types for different pairs of mentions.

\begin{figure}[t]
\centering
\includegraphics[width=.9\columnwidth]{1.png}
\caption{We represent directed multigraphs  as \emph{alternating} sequences of nodes (blue) and edges (orange). 
Here, the graph is traversed by Breadth First Search (BFS) with an ascending ordering of nodes and edge types. ``[s]'' or [SEP] is a virtual edge type,  representing the end of each BFS level.}\label{f1}
\end{figure}


Another approach is to treat the joint information extraction task as a table filling problem \cite{zhang-etal-2017-end,tse}, and generate two-dimensional tables with a Multi-Dimensional Recurrent Neural Network \cite{graves2007multi}. This  can capture interrelations among entities and relations, but the space complexity grows quadratically with respect to the length of the input text, making this approach impractical for long sequences. 


Some attempts, such as Seq2RDF \cite{liu2018seq2rdf} and IMoJIE \cite{kolluru2020imojie},  leverage the power of Seq2seq models \cite{seq2seq} to capture the interrelations among  mentions and  types with  first-order complexity, but they all use a pre-defined vocabulary for mention prediction, which largely depends on the distribution of the target words and will not be able to handle unseen out-of-vocabulary words. 

To solve these problems, we propose a first-order approach that invertibly maps the target graph to an alternating sequence of nodes and edges, and applies a hybrid span generator that directly learns to generate such alternating sequences. Our main contributions are three-fold:

\begin{itemize}
    \item We propose a general technique to invertibly map between an information graph and an alternating sequence (assuming a given  graph traversal algorithm). Generating an alternating sequence is equivalent to generating the original information graph. 
    \item We propose a novel neural decoder that is enforced to only generate alternating sequences by decoding spans and types in a hybrid manner. For each decoding step, our decoder only has linear space and time complexity with respect to the length of the input sequence, and it can capture inter-dependencies among mentions and types due to its nature as a sequential decision process.
    \item We conduct extensive experiments on the Automatic Content Extraction (ACE) dataset which show that our model achieves state-of-the-art performance on the joint entity and relation extraction task which aims to extract a knowledge graph from a piece of unstructured text. \end{itemize}

\section{Modeling Information Graphs as Alternating Sequences}


An \textbf{information graph} can be viewed as a heterogeneous multigraph \cite{li-etal-2014-constructing,hgraph} , where  is a set of nodes (typically representing spans  in the input document) and  is a multiset of edges with a node type mapping function  and an edge type mapping function . Node and edge types are  assumed to be drawn from a finite vocabulary. Node types can be used e.g. to represent entity types (PER, ORG, etc.), while edge types may represent relations (PHYS, ORG-AFF, etc.) between the nodes.
In this work, we represent node types as separate nodes that are connected to their node  by a special edge type, [TYPE]. \footnote{ includes a [NULL] node type for the case when the input text does not have an information graph.}


\paragraph{Representing information graphs as sequences}
Instead of directly modeling the  space of heterogeneous multigraphs, , we  build a mapping   from  , to a sequence space .   depends on a (given) ordering  of nodes and their edges in , constructed by a graph traversal algorithm like Breadth First Search (BFS) or Depth First Search (DFS), and an internal ordering of nodes and edge types.
We assume that the elements  of the resultant sequences  are drawn from finite sets of node representations  (defined below), node types , edge types  (incl. [TYPE]), and  ``virtual" edge types : .  Virtual edge types   do not represent edges in , but serve to control the generation of the sequence, indicating the start/end of sequences and the separation of levels in the graph.


We furthermore assume that   that represent graphs have an \textbf{alternating} structure, where  represent nodes , and   represent actual or virtual edges.  In the case of BFS, we exploit the fact that it visits nodes level by level, \emph{i.e.}, in the order  (where  is the -th child of parent , connected by edge , and   may or may not be equal to one of the children of ), which we turn into a sequence,

where we use the special edge type [SEP] to delineate the levels in the graph. This representation allows us to unambiguously recover the original graph, if we know which type of graph traversal is assumed (BFS or DFS).\footnote{In the case of DFS, [SEP] tokens appear after leaf nodes. Parents appear once for each child.} 
Algorithm 1 (which we use to translate graphs in the training data to sequences) shows how an alternating sequence for a given graph can be constructed with BFS traversal.  \Cref{f1} shows the alternating sequence for an information multigraph. 
 The length  is bounded linearly by the size of the graph  (which is also the complexity of typical graph traversal algorithms like BFS/DFS).


 \begin{algorithm}[H]
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Ordered adjacency dictionary of an information graph , positions of nodes in the input text , frequency of edge types in the training set }


 \Output{An alternating sequence }
 
\BlankLine

Sort the nodes in  according to 

For each node  in , sort the neighbors and the edges of  according to  and  respectively

Instantiate  as an empty list

 \For{ in }{
\If{ is not visited}{
    Initialize an empty queue \;
    
    Mark  as visited and enqueue  to  \;
    
   \While{ is not empty}{
   Dequeue a node  from \;
   
   \If{ in }{
    Append  and all the neighbors of  with their edge types to \;
    
    Append the separation edge type, [SEP], to \;
    
    Mark all unvisited neighbors of  as visited and enqueue them to \;
   }
   }
}
}
 \textbf{Return} 
 \caption{Alternating sequence construction algorithm with BFS}
\end{algorithm}




\paragraph{Node and Edge Representations}
Our node and edge representations (explained below) rely on the observation that there are only two kinds of objects in an information graph: spans (as addresses to pieces of input texts) and types (as representations of abstract concepts). Since we can view types as special spans of length 1 grounded on the vocabulary of all types, , we only need  number of indices to unambiguously represent the spans grounded on a concatenated representation of the type vocabulary and the input text, where  is the maximum input length,  is the maximum span length, and  . We denote these indices as \emph{hybrid spans} because they consist of both the spans of texts and the length-1 spans of types. These indices can be invertibly mapped back to types or text spans depending on their magnitudes (details of this mapping are explained in Section \ref{map}). With this joint indexing of spans and types, the task of generating an information graph is thus converted to generating an alternating sequence of \emph{hybrid spans}.











\paragraph{Generating sequences}
We model the distribution  by a sequence generator  with parameters  ( is the length of the ):

We will address in the following sections how to enforce the sequence generator, , to only generate  sequences in the space  , since we do not want  to assign non-zero probabilities to  arbitrary sequences that do not have a corresponding graph. 












 






\begin{figure*}[htb]
\centering
\includegraphics[width=13cm]{2.png}
\caption{The encoder architecture of our model, where the  symbol is the concatenation operator,  is the index of the word vectors in , and . The colored table on the right indicates the assignment of the meta-types for different blocks of the concatenated word vectors from . }\label{f2}
\end{figure*}





\section{HySPA: Hybrid Span Generation for Alternating Sequences}










In order to directly generate a target  sequence that alternates between  nodes that represent spans in the input and a set of node/edge types that depend on our extraction task, we first build a hybrid representation  that is a concatenation of the hidden representations from edge types, node types and the input text. This representation functions as both the context space and the output space for our decoder. Then we invertibly map both the spans of input text and the indices of the types to the \emph{hybrid spans} grounded on the representation .
Finally, hybrid spans are generated auto-regressively through a hybrid span decoder to form the alternating sequence . By translating the graph extraction task to a sequence generation task, we can easily use beam-search decoding to reduce possible exposure bias \cite{beam} of the sequential decision process and thus find globally better graph representation.


\paragraph{High-level overview of HySPA:} The HySPA model takes a piece of text (e.g. a sentence or passage), and the pre-defined node and edge types as input, and outputs an alternating sequence representation of an information graph. We enforce the generation of this sequence to be alternated by applying an alternating mask to the output probabilities. The detailed architecture is described in the following subsections.

\subsection{Text and Types Encoder}

\Cref{f2} shows the encoder architecture of our proposed model. For the set of node types, , and the set of edge types, , and the virtual edge types, , we arrange the type list,  as a concatenation of the label names of the edge types, virtual edge types and node types, \emph{i.e.}, 

where  means the concatenation operator between two lists, and  are the lists of the type names in the sets , respectively (e.g. ). Note that the concatenation order between the lists of type names can be arbitrary as long as it is kept consistent throughout the whole model.
Then, as in the embedding part of the table-sequence encoder~\cite{tse}, for each type, , we embed the label tokens of the types with the contextualized word embedding from a pre-trained language model, the GloVe embedding \cite{glove} and the character embedding,


 
where  is the number of all kinds of types,  is the weight matrix of the linear projection layer,  is the total embedding dimension and  is the hidden size of our model. After we obtain the contextualized embedding of the tokens of each type , we take the average of these token vectors as the representation of  and freeze its update during training. More details of the embedding pipeline can be found in Appendix \ref{hyper}.

This embedding pipeline is also used to embed the words in the input text, . Unlike the pipeline for the type embedding, we represent the word as the contextualized embedding of its first sub-token from the pre-trained Language Model (LM, e.g. BERT \cite{bert}), and finetune the LM in an end-to-end fashion.

After obtaining the type embedding , and the text embedding  respectively, we concatenate them along the sequence length dimension to form the hybrid representation . Since  is a concatenation of word vectors from four different types of tokens, \emph{i.e.}, edge types, virtual edge types, node types and text, a meta-type embedding is applied
 to indicate this type difference between the blocks of vectors from the representation , as shown in \Cref{f2}. The final context representation  is obtained by element-wise addition of the meta-type embedding and ,

where  is the height of our hybrid representation matrix .




 \subsection{Invertible Mapping between Spans \& Types and Hybrid Spans}\label{map}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{3.png}
\caption{An example of the alternating sequence representation (in the middle) of a knowledge graph (at bottom) from the ACE05 training set, where  means the Algorithm 1. We take  and  for this example. ``19" in the alternating sequence is the index for the span (0,1) of ``He", ``83" is the index for the span (4,5) of ``Baghdad", and  ``10" is the index of the virtual edge type, [SEP]. The input text (on top) for this graph is ``He was captured in Baghdad late Monday night".}\label{f4}
\end{figure}

Given a span in the text, , we convert the span  to an index , , in the representation  via the mapping ,

where  is the maximum length of spans, and . We keep the type indices in the graph unchanged because they are smaller than  and . Since, for an information graph, the maximum span length, , of a mention is often far smaller than the length of the text, \emph{i.e.}, , we can then reduce the bound of the maximum magnitude of  from  to  by only considering spans of length smaller than , and thus maintain  linear space complexity for our decoder with respect to the length of the input text, . \Cref{f4} shows a concrete example of our alternating sequence for a knowledge graph in the ACE05 dataset.




Since  are all natural numbers, we can construct an inverse mapping  that converts the index  in  back to ,

where  is the integer floor function and  is the modulus operator.  Note that  can be directly applied to the indices from the types segment of  and remain their values unchanged, \emph{i.e.},  

With this property, we can easily incorporate the mapping  into our decoder to map the alternating sequence  back to the spans in the hybrid representation .



\subsection{Hybrid Span Decoder}


\begin{figure*}[htb]
\centering
\includegraphics[width=14cm]{4.png}
\caption{The  architecture of our hybrid span decoder.  is the number of the decoder layers.  before the softmax function means the concatenation operator.  is the hidden representation of the sequence  from the last decoder layer. Our hybrid span decoder can be understood as an auto-regressive model that operates in a closed context space and output space defined by .}\label{f3}
\end{figure*} 


\Cref{f3} shows the general model architecture of our hybrid span decoder. Our decoder takes the context representation  as input, and recurrently decodes the alternating sequence  given a start-of-sequence token.







\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{trav.png}
\caption{An example of BFS traversal embedding for an alternating sequence, [``He'', Type, PER, [SEP], ``Baghdad'', Type, GPE, PHYS, ``He'']. Our traversal embedding is the sum of the level embedding, the parent-child embedding and the tree embedding.}\label{ft}
\end{figure}

\paragraph{Hybrid Span Encoding via Attention}
Given the alternating sequence , and the mapping  (section \ref{map}), our decoder first maps each index in  to a span, , grounded on the representation  and then converts the span to an attention mask, , to allow the model to learn to represent a span as a weighted sum of a segment of the contextualized word representations referred by the span,

where  is the -times repeated hidden representation of the start of the sequence token, [CLS], from the text segment of , and  is our final representation of the hybrid spans in .  are learnable parameters, and  are the start and the end position of the span thatwe are encoding. Note that for the type spans whose length is 1, the result of the \emph{softmax} calculation will always be 1, which leads to its span representation to be exactly its embedding vector as we desired. 


\paragraph{Traversal Embedding}
In order to distinguish the hybrid spans at different position in , a naive way is to
add a sinusoidal position embedding \cite{tran} to . However, this approach treats the alternating sequence as an ordinary sequence and ignores the underlying graph structure it encodes. To alleviate this issue, we propose a novel traversal embedding approach which captures the traversal level information, the parent-child information and the intra-level connection information as a substitution of the naive position embedding. Our traversal embedding can either encode the BFS or DFS traversal pattern. As an example, we assume BFS traversal here and leave the details of DFS traversal embedding in Appendix \ref{dfst}.

Our BFS traversal embedding is a pointwise sum of the level embedding, , the parent-child embedding, , and the tree embedding,  of a given alternating sequence, ,

 where the level embedding assigns the same embedding vector  for each position at the BFS traversal level , and the value of the embedding vector is filled according to the non-parametric sinusoidal position embedding since we want our embedding to extrapolate to the sequence that is longer than any sequences in the training set. The parent-child embedding assigns different random initialized embedding vectors at the positions of the parent nodes and the child nodes in the BFS traversal levels to help model distinguish between these two kinds of nodes. For encoding the intra-level connection information, our insight is that the connection between each nodes in a BFS level can be viewed as a depth-3 tree, where the first depth takes the parent node, the second depth is filled with the edge types and the third depth consists of the corresponding child nodes for each of the edge types. Our tree embedding is then formed by encoding the position information of the depth-3 tree with a tree positional embedding \cite{treepos} for each BFS level. \Cref{ft} shows a concrete example of how these embeddings function for a given alternating sequence. The obtained traversal embedding is then pointwisely added to the hidden representation of the alternating sequence  for injecting the traversal information of the graph structure.




\paragraph{Inner blocks}

With the input text representation  sliced from the hybrid representation  and the target sequence representation , we apply an -layer transformer structure with mixed-attention \cite{layerwise} to allow our model to utilize features from different attention layers when decoding the edges or the nodes of an alternating sequence. Note that our hybrid span decoder is perpendicular to the actual choice of the neural structures of the inner blocks, and we choose the design of mixed-attention transformer \cite{layerwise} because its layerwise coordination property is empirically more suitable for our heterogeneous decoding of two different kinds of sequence elements. The detailed structure of the inner blocks is explained in Appendix \ref{mix}.


\paragraph{Hybrid span decoding}

For the hybrid span decoding module, we first slice off the hidden representation of the alternating sequence  from the output of the -layer inner blocks and denote it as .
Then for each hidden representation , we apply two different linear layers to obtain the start position representation, , and the end position representation, ,

where  and  are learnable parameters.
Then we  calculate the scores of the target spans separately for the types segment and the text segment of , and concatenate them together before the final softmax operator for a joint estimation of the probabilities of text spans and type spans,
 
where 
 is the score vector of possible spans in the type segment of , and  is the score vector of possible spans in the text segment of . Since the type spans always have a span length 1, we only need an element-wise addition between the start position scores,  and the end position scores  to calculate . The entries of  contain the scores for the text spans, , which are calculated with the help of an \emph{unfold} function which converts the vector  to a stack of  sliding windows of size , the maximum span length, with stride 1. The alternating masks  are defined as:

 where  is the total number of  edge types.  In this way, while we have a joint model of  nodes and edge types, the output distribution is enforced by the alternating masks to produce an alternating decoding of  nodes and  edge types, and this is the main reason why we call this decoder a hybrid span decoder.






 







\begin{table*}
\centering
\begin{tabular}{lllll}
\hline
\textbf{IE Models} & \textbf{Space Complexity} & \textbf{Time Complexity}  &\textbf{NER} & \textbf{RE} \\
\hline
PointerNet \cite{ptr} &  & & 82.6 & 55.9 \\
SpanRE \cite{dixit-al-onaizan-2019-span} &  &   & 86.0 & 62.8\\
Dygie++ \cite{dygie} &  & & 88.6 & 63.4 \\
OneIE \cite{lin-etal-2020-joint} &  & &88.8 & 67.5 \\
TabSeq \cite{tse} &  & & 89.5 & 67.6 \\
\hline
HySPA (ours) \quad w/ RoBERTa  & \multirow{2}{4em}{} &\multirow{2}{4em}{}& 88.9 & \textbf{68.2} \\
\quad \quad \quad \quad\quad \quad\quad w/ ALBERT &  & & \textbf{89.9} & \textbf{68.0} \\
\hline
\end{tabular}
\caption{\label{res}
Joint NER and RE F1 scores of the IE models on the ACE05 test set.  Complexities are calculated for the entity and relation decoding part of the models ( is the length of the input text). The performance of the TabSeq model reported here is based on the same ALBERT-xxlarge \cite{albert} pretrained language model as ours.
}
\end{table*}


\section{Experiments}


\subsection{Experimental Setting}


We test our model on the ACE 2005 dataset distributed by LDC\footnote{\url{https://catalog.ldc.upenn.edu/LDC2006T06}},  which includes 14.5k sentences, 38.3k entities (with 7 types), and 7.1k relations (with 6 types), derived from the general news domain. More details can be found in Appendix \ref{data}.

Following previous work, we use F1 as an evaluation metric for both NER and RE. For the NER task, a prediction is marked correct when both the type and the boundary span match those of the gold entity. For the RE task, a prediction is correct when both the relation type and the boundaries of the two entities are correct.  





\subsection{Implementation Details}
When training our model, we apply the cross-entropy loss with a label smoothing factor of 0.1. The model is trained with 2048 tokens per batch (roughly a batch size of 28) for 25000 steps using an AdamW optimizer \cite{adamw} with a learning rate of , a weight decay of 0.01, and an inverse square root scheduler with 2000 warm-up steps. Following the TabSeq model \cite{tse}, we use RoBERTa-large \cite{roberta} or ALBERT-xxlarge-v1 \cite{albert} for the pretrained language model and slow its learning rate by a factor of 0.1 during training. A hidden state dropout rate of 0.2 is applied to RoBERTa-large while the rate of 0.1 for  ALBERT-xxlarge-v1. A dropout rate of 0.1 is also applied to our hybrid span decoder during training. We set the maximum span length, , the hidden size of our model, , and the number of the decoder blocks, . Even though theoretically the beam-search should help us reduce the exposure bias, we do not observe any performance gain during grid search of the beam size and the length penalty on the validation set (detailed grid search setting is in Appendix \ref{hyper}). Thus we set a vanilla beam size of 1 and the length penalty of 1, and leave this theory-experiment contradiction for future research. Our model is built with the FAIRSEQ toolkit \cite{ott2019fairseq} for efficient distributed training and all the experiments are conducted on two NVIDIA TITAN X GPUs.


\subsection{Results}

\Cref{res} compares our model with the previous state-of-the-art results on the ACE05 test set. Compared with the previous SOTA, TabSeq \cite{tse} with ALBERT pretrained language model, our model with ALBERT has significantly better performance for both NER score and RE score, while maintaining a linear space complexity which is an order smaller than TabSeq. Our model is the first joint model that has both linear space and time complexities compared with all previous joint IE models, and thus has the best scalability for large-scale real world applications.

\subsection{Ablation Study}


\begin{table}[t!]
\begin{center}
\begin{tabular}{lcc} 
\toprule \bf Model & \bf NER F1 & \bf RE F1 \\ \midrule
HySPA  w/ RoBERTa &88.9 &68.2  \\
~ -- Traversal-embedding  &88.9 &66.7 \\
\quad -- Masking  &88.1 &64.8 \\
\quad -- BFS  &88.7 &66.2  \\
\quad -- Mixed-attention  &88.6 &64.7  \\
\quad -- Span-attention  &88.5 &66.1 \\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{abm} Ablation study on the ACE05 test set. ``-- Traversal-embedding'': we remove the traversal embedding and instead use sinusoidal position embedding, and the following ablations are based on the model after this ablation. ``-- Masking'':  we remove the alternating mask from the  hybrid span decoder. ``-- BFS": we use DFS instead of BFS as traversal. ``-- Mixed-attention":  we remove the mixed-attention layer and use a standard transformer encoder decoder structure. ``-- Span-attention":  we remove the span attention in the span encoding module and instead average the words in the span.}
\end{table} 


To prove the effectiveness of our approach, we conduct ablation experiments on the ACE05 dataset. As shown in \Cref{abm}, after we remove the traversal embedding the RE F1 scores drop significantly, which indicates that our traversal embedding can help encode the graph structure and improve relation predictions. Also if the alternating masking is dropped, the NER F1 and RE F1 scores both drop significantly, which proves the importance of enforcing the alternating pattern. 
We can observe that the mixed-attention layer contributes significantly for relation extraction. This is because the layer-wise coordination can help the decoder to disentangle the source features and utilize different layer features between the entity and the relation prediction. We can also observe that the DFS traversal has worse performance than BFS. We suspect that this is because the resultant alternating sequence from DFS is often longer than the one from BFS due to the nature of the knowledge graphs, and thus increases the learning difficulty.




\subsection{Error Analysis}
\label{section:analysis}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{case_dist.png}
\caption{Distribution of remaining errors on the ACE05 test set.}
\label{edist}
\end{figure}

After analyzing 80 remaining errors, we categorize and discuss common cases 
below (Figure~\ref{edist} plots the distribution of  error types). These  may require additional features and strategies to address. 



\noindent \textbf{Insufficient context}. In many examples, the answer entity is  a  pronoun that cannot be accurately typed given the limited  context: in \textit{``We notice they said they did not want to 
use the word destroyed, in fact, they said let others do that''}, it's 
difficult to correctly classify  \textit{We} as an 
organization. 
This  could be mitigated by using entire documents as input, leveraging cross-sentence context. 





\noindent \textbf{Rare words}. The rare word issue is when the word in test 
set rarely 
appeared in the training set and often not termed in the dictionary. In the sentence \textit{``There are 
also Marine FA-18s and 
Marine Heriers at this base''}, the term \textit{Heriers} (a vehicle incorrectly classified as person by the model) 
neither
appeared in the training set, nor understood well by pre-trained language 
model; the model, in this case, can only rely on subword-level representation.










\noindent \textbf{Background knowledge required} Often the sentence mentions entities that are difficult to infer from the context, but are easily identified by consulting a knowledge base: in \textit{``but critics say Airbus should have sounded a stronger alarm after a similar incident occurred in 1997''}, our model incorrectly predicts the \textit{Airbus} to be a vehicle while the \textit{Airbus} here refers to the European aerospace corporation. Our system also separated \textit{United Nations Security Council} into two entities \textit{United Nations} and \textit{Security Council}, generating a non-existing relation triple (\textit{Security Council} part-of \textit{United Nations}). Such mistakes could be avoided by consulting a knowledge base such as DBpedia~\cite{dbpedia} or by performing entity linking. 







\noindent \textbf{Inherent ambiguity} Many examples have inherent ambiguity, e.g.   \textit{European Union} can be typed as organization or 
political entity, while some entities (e.g., military bases) can be both locations and organizations, or facilities.

































\section{Related Work}












NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem~\cite{tse_prior, tse_prior2, tse}, where the -th column or row represents the -th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by~\citet{bilstm_joint}, the authors used BiLSTM~\cite{bilstm_original} for NER and consequently a Tree-LSTM~\cite{treelstm} based on dependency graph for RE. \citet{dygie} and \citet{dygie_original}, on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on \citet{dygie}, \citet{lin-etal-2020-joint} introduced \textsc{One}IE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a  graph. Note that our model differs from \textsc{One}IE~\cite{lin-etal-2020-joint} in that our model captures global relationships automatically through autoregressive generation while \textsc{One}IE uses feature engineered templates; 
Moreover, \textsc{One}IE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. 

While several Seq2Seq-based models~\cite{seq2umtree, seq2seq_re, seq2seq_re2, seq2seq_re3,zhang-etal-2019-broad} have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in 
that: (1) it is
generating a BFS/DFS traversal of the target graph, which captures 
dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information.








\section{Conclusion}
In this work, we propose the Hybrid Span Generation (HySPA) model, the first end-to-end text-to-graph extraction model that has a linear space and time complexity at the graph decoding stage. Besides its scalability, the model also achieves  state-of-the-art performance on the ACE05 joint entity and relation extraction task. Given the flexibility of the structure of our hybrid span generator, abundant future research directions remain, e.g. incorporating the external knowledge for hybrid span generation, applying more efficient sparse self-attention, and developing better search methods to find more globally plausible graphs represented by the alternating sequence.

\section*{Acknowledgments}
This work is supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.
\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021}

\appendix

\section{Hyperparameters}\label{hyper}
We use 100-dimensional GloVe word embeddings trained on 6B tokens as intialization \footnote{\url{https://nlp.stanford.edu/projects/glove/}}, and freeze its update during training. The character embedding has 30-dimension with LSTM encoding \footnote{\url{https://github.com/LorrinWWW/two-are-better-than-one/blob/master/layers/encodings/embeddings.py}} and the Glove Embeddings for the out of vocabulary tokens are replaced with randomly initialized vectors following \citet{tse}. We use gradient clipping of 0.25 during training. The number of heads for our mixed attention is set to 8. The beam size and length penalty is decided by a grid-search on the validation set of the ACE05 dataset, and the range for the beam size is from 1 to 7 with a step size of 1 and the length penalty is from 0.7 to 1.2 with a step size of 0.1. We choose the best beam size and length penalty based on the metric of relation extraction F1 score. 



\section{Training Details}\label{training}

Our model has 236 million parameters with the ALBERT-xxlarge pretrained language model. On average, our best performing model with ALBERT-xxlarge can be trained distributedly on two NVIDIA TITAN X GPUs for 20 hours.

\section{Data} \label{data}
The Automatic Content Extraction (ACE) 2005~\footnote{\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}} dataset contains English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation, providing entity, relation, and event annotations. We follow~\citet{dygie}~\footnote{\url{https://github.com/dwadden/dygiepp/tree/master/scripts/data/ace05/preprocess}} for preprocessing and data splits. The preprocessed data contains 7.1k relations, 38k entities, and 14.5k sentences. The split contains 10051 samples for training, 2424 samples for development, and 2050 for testing.

\section{DFS Traversal Embedding}\label{dfst}

Since the parent-child information is already contained in the intra-level connections of DFS traversal, we only have the sum of the level embedding and the connection embedding for DFS traversal embedding. Similar to BFS embedding, the DFS level embedding assigns the same embedding vector  for each position at the DFS traversal level , but the value of the embedding vector is randomly initialized instead of filled with the non-parametric sinusoidal position embedding, since the proximity information does not exist between the traversal levels of DFS. However, we do have clear distance information for the elements in a DFS level, \emph{i,e.}, for a DFS level , the distance from A to the elements [A, B, C, ..., [sep]] is . We encode this distance information with the sinusoidal position embedding which becomes our connection embedding that captures the intra-level connection information.

\section{Transformer with Mixed-attention}\label{mix}



\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{inner.png}
\caption{The general model architecture of the mixed-attention transformer.}\label{fm}
\end{figure}

 We first slice off the hidden representation of the input text from the hybrid representation , and denote it as , then the input text representation  and the output from the Hybrid Span Encoding  gets fed into a stack of  mixed-attention/feedforward blocks that have the following structure (as shown in \Cref{fm}):

Since generating the node and  edge types may need  features from different layers, we use mixed attention \cite{layerwise}, which allows our model to utilize the features from different attention layers when encoding the text  segment, , and the target features, ,

where  is the length of the input text,  is the total length of the source and the target features. Denoting the concatenation of the source features, , and the target features, , as , a source/target embedding \cite{layerwise} is also added to  before the first layer of the mixed attention to allow the model to distinguish the features from the source and the target sequences. The mixed-attention layer is  combined with a feed-forward layer to form a decoder block:

where  are the learnable parameters, and LayerNorm is the Layer Normalization layer \cite{LayerN}. The decoder block is stacked  times to obtain the final hidden representation , and output the final representation of the target sequence, . The mixed-attention has a time complexity of  when encoding the source features, but we can cache the hidden representation of this part when generating the target tokens due to the causal masking of the target features, and thus maintain a time complexity of  for each decoding step.

\end{document}
