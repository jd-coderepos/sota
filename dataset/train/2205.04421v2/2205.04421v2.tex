\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}





\usepackage[preprint]{neurips_2022}







\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\Tabref#1{Table~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\mathbf{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\mathbf{x}}}
\def\vy{{\mathbf{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\mathbf{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\mathbf{X}}}
\def\mY{{\mathbf{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tipa}
\usepackage{multirow}
\usepackage{longtable}

\usepackage{subcaption}
\usepackage{color}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{threeparttable}
\usepackage{diagbox}
\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage{hyperref}
\usepackage{url}            \usepackage{xurl}
\usepackage{breakurl}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}



\usepackage[edges]{forest}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{fadings}
\usetikzlibrary{mindmap,trees}
\usetikzlibrary{arrows,automata,shapes,positioning,shadows,trees}
\usetikzlibrary{shapes,snakes,shadows}
\usetikzlibrary{shapes.arrows}
\usetikzlibrary{calc,shapes, positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usetikzlibrary{bayesnet}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes.geometric}


\tikzstyle{edge}=[-latex',draw=black!90,shorten <=1pt,shorten >=1pt]
\tikzstyle{redge}=[latex'-,draw=black!90,shorten <=1pt,shorten >=1pt]
\tikzstyle{dedge}=[latex'-latex',draw=black!90,shorten <=1pt,shorten >=1pt]
\tikzstyle{block}=[draw, text width=5em,align=center,shape=rectangle, rounded corners, , align=center]
\tikzstyle{nobox}=[align=center]

\definecolor{emb}{RGB}{209,228,252}
\definecolor{hidden-blue}{RGB}{194,232,247}
\definecolor{hidden-orange}{RGB}{243,202,120}
\definecolor{hidden-yellow}{RGB}{242,244,193}
\definecolor{output-purple}{RGB}{219,203,231}
\definecolor{output-green}{RGB}{204,231,207}
\definecolor{hiddendraw}{RGB}{205, 44, 36}


\tikzstyle{mybox}=[
    rectangle,
    draw=hiddendraw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.2,
    ]

\tikzstyle{emb-purple}=[
    rectangle,
    draw=output-purple!50!purple,
    fill=output-purple,
text opacity=1,
    minimum height=1.5em,
    minimum width=1.5em,
    inner sep=0pt,
    align=center,
    fill opacity=.9,
    ]

\tikzstyle{emb-blue}=[
    rectangle,
    draw=emb!50!blue,
    fill=emb,
text opacity=1,
    minimum height=1.5em,
    minimum width=1.5em,
    inner sep=0pt,
    align=center,
    fill opacity=.9,
]



\definecolor{colone}{RGB}{178, 34, 34}
\definecolor{coltwo}{RGB}{106, 90, 205}
\definecolor{colthree}{RGB}{255, 250, 205}
\definecolor{colfour}{RGB}{0, 139, 69}
\definecolor{colfive}{RGB}{245,238,197}
\definecolor{colsix}{RGB}{243,235,179}
\definecolor{colseven}{RGB}{241,231,163}
\setlength{\marginparwidth}{2cm}


\def\myname{NaturalSpeech}



\title{\textit{\myname{}}: End-to-End Text to Speech Synthesis with Human-Level Quality}



\author{
Xu Tan\thanks{Equal contribution. Corresponding author: Xu Tan, \texttt{xuta@microsoft.com}}, ~Jiawei Chen\footnotemark[1], ~Haohe Liu\footnotemark[1], ~Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang  \\
\textbf{Yichong Leng, Yuanhao Yi, Lei He, Frank Soong} \\  
\textbf{Tao Qin, Sheng Zhao, Tie-Yan Liu}  \\
\\
Microsoft Research Asia \& Microsoft Azure Speech \\
}

\begin{document}

\maketitle

\begin{abstract}
Text to speech (TTS) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a TTS system can achieve human-level quality, how to define/judge that quality and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a TTS system called \myname{} that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder (VAE) for end-to-end text to waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in VAE. Experiment evaluations on popular LJSpeech dataset show that our proposed \myname{} achieves  CMOS (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed rank test at p-level , which demonstrates no statistically significant difference from human recordings for the first time on this dataset. 

\end{abstract}


\section{Introduction}

Text to speech (TTS) aims at synthesizing intelligible and natural speech from text~\cite{tan2021survey}, and has made rapid progress in recent years due to the development of deep learning. Neural network based TTS has evolved from CNN/RNN-based models~\cite{oord2016wavenet,shen2018natural,wang2017tacotron,arik2017deep,gibiansky2017deep,ping2018deep,tachibana2018efficiently} to Transformer-based models~\cite{li2019neural,ren2019fastspeech,liu2021delightfultts}, from basic generative models (autoregressive)~\cite{oord2016wavenet,shen2018natural,li2019neural} to more powerful models (VAE, GAN, flow, diffusion)~\cite{prenger2019waveglow,kim2020glow,popov2021grad,kim2021conditional}, from cascaded acoustic models/vocoders~\cite{oord2016wavenet,wang2017tacotron,shen2018natural,ren2019fastspeech,kalchbrenner2018efficient,kong2020hifi} to fully end-to-end models~\cite{ren2021fastspeech,donahue2020end,kim2021conditional}.

Building TTS systems with human-level quality has always been the dream of the practitioners in speech synthesis. While current TTS systems achieve high voice quality, they still have quality gap compared with human recordings. To pursue this goal, several questions need to be answered: 1) how to define human-level quality in text to speech synthesis? 2) how to judge whether a TTS system has achieved human-level quality or not? 3) how to build a TTS system to achieve human-level quality? In this paper, we conduct a comprehensive study on these problems in TTS. We first give a formal definition on human-level quality in TTS based on a statistical and measurable way (see Definition~\ref{def_onpar}). Then we introduce some guidelines to judge whether a TTS system has achieved human-level quality with a hypothesis test. Using this judge method, we found several previous TTS systems have not achieved it (see Table~\ref{tab_mos_cmos_judge_onpar}). 



In this paper, we further develop a fully end-to-end text to waveform generation system called \myname{} to bridge the quality gap to recordings and achieve human-level quality. Specifically, inspired by image/video/waveform generation~\cite{van2017neural,ramesh2021zero,kim2021conditional}, we leverage variational autoencoder (VAE)~\cite{kingma2013auto} to compress the high-dimensional speech () into continuous frame-level representations (denoted as posterior ), which are used to reconstruct the waveform (denoted as ). The corresponding prior (denoted as ) is obtained from the text sequence . Considering the posterior from speech is more complicated than the prior from text, we design several modules (see Figure~\ref{fig_model_overall}) to match the posterior and prior as close to each other as possible, to enable text to speech synthesis through :

\begin{itemize}[leftmargin=*]
\item We leverage large-scale pre-training on the phoneme encoder to extract better representations from phoneme sequence (Section~\ref{sec_phoneme_pretrain}).  

\item We leverage a fully differentiable durator\footnote{Since duration is very important in TTS, especially in non-autoregressive TTS, we name the module related to duration modeling as \textit{durator}, including but not limited to the functionalities of duration prediction and hidden expansion. It is common to come up with new term to revolutionize the concept in speech community, such as \textit{vocoder}, \textit{cepstrum}.} that consists of a duration predictor and an upsampling layer to improve the duration modeling (Section~\ref{sec_diff_durator}). 

\item We design a bidirectional prior/posterior module based on flow models~\cite{dinh2014nice,kingma2016improved,kingma2018glow} to further enhance the prior  and reduce the complexity of posterior  (Section~\ref{sec_bidirect_flow}).

\item We propose a memory based VAE to reduce the complexity of the posterior needed to reconstruct waveform (Section~\ref{sec_memory_vae}). 

\end{itemize}

Compared to previous TTS systems, \myname{} has several advantages: 1) Reduce training-inference mismatch. In previous cascaded acoustic model/vocoder pipeline~\citep{kim2020glow,ren2021fastspeech,popov2021grad} and explicit duration prediction~\citep{kim2020glow,kim2021conditional,ren2021fastspeech}, both mel-spectrogram and duration suffer from training-inference mismatch since ground-truth values are used in training the vocoder and mel-spectrogram decoder while predicted values are used in inference. Our fully end-to-end text to waveform generation and differentiable durator can avoid the training-inference mismatch. 2) Alleviate one-to-many mapping problem. One text sequence can correspond to multiple speech utterances with different variation information (e.g., pitch, duration, speed, pause, prosody, etc). Previous works only using variance adaptor~\citep{ren2021fastspeech,liu2021delightfultts} to predict pitch/duration cannot well handle the one-to-many mapping problem. Our memory based VAE and bidirectional prior/posterior can reduce the complexity of posterior and enhance the prior, which helps relieve the one-to-many mapping problem. 3) Improve representation capacity. Previous models are not powerful enough to extract good representations from phoneme sequence~\citep{kim2020glow,kim2021conditional,popov2021grad} and learn complicated data distribution in speech~\citep{ren2021fastspeech}. Our large-scale phoneme pre-training and powerful generative models such as flow and VAE can learn better text representations and speech data distributions. 


We conduct experimental evaluations on the widely adopted LJSpeech dataset~\cite{ljspeech17} to measure the voice quality of our \myname{} system. Based on the proposed judgement guidelines, \myname{} achieves similar quality with human recordings in terms of MOS (mean opinion score) and CMOS (comparative MOS). Specifically, the speech generated by \myname{} achieves  CMOS compared to recordings, with p-level  under Wilcoxon signed rank test, which demonstrates that \myname{} can generate speech with no statistically significant difference from recordings.





\section{Definition and Judgement of Human-Level Quality in TTS}
\label{sec_def_judge}

In this section, we introduce the formal definition of human-level quality in text to speech synthesis, and describe how to judge whether a TTS system achieves human-level quality or not. 

\subsection{Definition of Human-Level Quality}
We define human-level quality in a statistical and measurable way.
\begin{definition}
\textit{If there is no statistically significant difference between the quality scores of the speech generated by a TTS system and the quality scores of the corresponding human recordings on a test set, then this TTS system achieves human-level quality on this test set.}
\label{def_onpar}
\end{definition}

Note that by claiming a TTS system achieves human-level quality on a test set, we do not mean that a TTS system can surpass or replace human, but the quality of this TTS system is statistically indistinguishable from human recordings on this test set.


\subsection{Judgement of Human-Level Quality}

\paragraph{Judgement Guideline} While there are some objective metrics to measure the quality gap between the generated speech and human recordings, such as PESQ~\citep{rix2001perceptual}, STOI~\citep{taal2011algorithm}, SI-SDR~\citep{le2019sdr}, they are not reliable to measure the perception quality in TTS. Therefore, we use subjective evaluation to measure the voice quality. Previous works usually use mean opinion score (MOS) with  points (from  to ) to compare the generated speech with recordings. However, MOS is not sensitive enough to the difference in voice quality since the judge simply rates the quality of each sentence alone from the two systems with no paired comparison. Thus, we choose comparative mean opinion score (CMOS) with 7 points (from  to ) as the evaluation metric, where each judge measures the voice quality by comparing samples from two systems head by head. We further conduct Wilcoxon signed rank test~\citep{wilcoxon1992individual} to measure whether the two systems are significantly different or not in terms of CMOS evaluation. 

Therefore, we list the judgement guidelines of human-level quality as follows: 1) Each utterance from TTS system and human recordings should be listened and compared side-by-side by more than 20 judges, who should be native language speakers. At least 50 test utterances from each system should be used in the judgement.
2) The speech generated by TTS system has no statistically significant difference from human recordings, if and only if the average CMOS is close to  and the p-level of Wilcoxon signed rank test satisfies . 

\paragraph{Judgement of Previous TTS Systems} 
Based on these guidelines, we test whether current TTS systems can achieve human-level quality or not on the LJSpeech dataset. The systems we study include: 1) FastSpeech 2~\citep{ren2021fastspeech} + HiFiGAN~\citep{kong2020hifi}, 2) Glow-TTS~\citep{kim2020glow} + HiFiGAN~\citep{kong2020hifi}, 3) Grad-TTS~\citep{popov2021grad} + HiFiGAN~\citep{kong2020hifi}, 4) VITS~\citep{kim2021conditional}. We re-produce the results of all these systems by our own, which can match or even beat the quality in their original papers (note that the HiFiGAN vocoder is fine-tuned on the predicted mel-spectrograms for better synthesis quality). We use 50 test utterances, each with 20 judges for MOS and CMOS evaluation. As shown in Table~\ref{tab_mos_cmos_judge_onpar}, although the current TTS systems can achieve close MOS with recordings, they have a large CMOS gap to recordings, with Wilcoxon signed rank test at p-level , which shows statistically significant difference from human recordings. We further study where the quality gap comes from by analyzing each component in one of the above TTS systems in Appendix~\ref{appendix_study}.  


\begin{table}[h!]
\small
\caption{The MOS and CMOS comparisons between previous TTS systems and human recordings. Note that the Wilcoxon p-value in MOS is conducted using Wilcoxon rank sum test~\citep{wilcoxon1992individual}, instead of the Wilcoxon signed rank test in CMOS, due to no paired comparison in MOS evaluation. For Grad-TTS, we use 1000 steps for inference.}
\centering
\begin{tabular}{ l | c c | c c}
    \toprule
	    System & MOS & Wilcoxon p-value & CMOS & Wilcoxon p-value \\
	    \midrule
	    Human Recordings  &   & - &  & - \\
	    \midrule
	    FastSpeech 2~\citep{ren2021fastspeech} + HiFiGAN~\citep{kong2020hifi} &  &  &  &  \\
	    Glow-TTS~\citep{kim2020glow} + HiFiGAN~\citep{kong2020hifi} &  &  &  & \\
Grad-TTS~\citep{popov2021grad} + HiFiGAN~\citep{kong2020hifi}  &   &  &  &  \\
	    VITS~\citep{kim2021conditional} &   &  &  &  \\
	    \bottomrule
	\end{tabular}
	\vspace{0.3cm}
	\label{tab_mos_cmos_judge_onpar}
\end{table}






\section{Description of \myname{} System}
\label{sec_method}


To bridge the quality gap to human recordings, we develop \myname{}, a fully end-to-end text to waveform generation model. We first describe the design principle of our system (Section~\ref{sec_deign_principle}), and then introduce each module of this system (Section~\ref{sec_phoneme_pretrain}-\ref{sec_memory_vae}) and training/inference pipeline (Section~\ref{sec_train_infer}), and finally explain why our system can bridge the quality gap to human recordings (Section~\ref{sec_why_can_close}). 

\subsection{Design Principle}
\label{sec_deign_principle}

Inspired by image/video generation~\cite{ramesh2021zero,ding2021cogview,wu2021nuwa,yan2021videogpt,rakhimov2020latent} that uses VQ-VAE~\cite{van2017neural,razavi2019generating,esser2021taming} to compress high-dimensional image into low-dimensional representations to ease the generation, we leverage VAE~\cite{kingma2013auto} to compress high-dimensional speech  into frame-level representations  (i.e.,  is sampled from posterior distribution ), which are used to reconstruct the waveform (denoted as ). In general formulation of VAE, the prior  is chosen to be standard isotropic multivariate Gaussian. To enable conditional waveform generation from input text in TTS, we predict  from phoneme sequence , i.e.,  is sampled from predicted prior distribution . We jointly optimize the VAE and the prior prediction with gradients propogating to both  and . Derived from the evidence lower bound~\cite{kingma2013auto}, the loss function consists of a waveform reconstruction loss  and a Kullback-Leibler divergence loss between the posterior  and the prior , i.e., . 


Considering the posterior from speech is more complicated than the prior from text, to match them as close as possible to enable text to waveform generation, we design several modules to simplify the posterior and to enhance the prior, as shown in Figure~\ref{fig_model_overall}. First, to learn a good representations of phoneme sequence for better prior prediction, we pre-train a phoneme encoder on a large-scale text corpus using masked language modeling on phoneme sequence (Section~\ref{sec_phoneme_pretrain}). Second, since the posterior is at the frame level while the phoneme prior is at the phoneme level, we need to expand the phoneme prior according to its duration to bridge the length difference. We leverage a differentiable durator to improve duration modeling (Section~\ref{sec_diff_durator}). Third, we design a bidirectional prior/posterior module to enhance the prior or simplify the posterior (Section~\ref{sec_bidirect_flow}). Fourth, we propose a memory based VAE that leverages a memory bank through Q-K-V attention~\citep{vaswani2017attention} to reduce the complexity of posterior needed to reconstruct the waveform (Section~\ref{sec_memory_vae}). 



\begin{figure} [t!]
\centering
    \includegraphics[page=1,width=0.65\columnwidth,trim=4.5cm 1.9cm 4.6cm 1.9cm,clip=true]{fig/fig.pdf}
    \caption{System overview of \myname{}.}
  \label{fig_model_overall}
\vspace{-0.3cm}
\end{figure}




\subsection{Phoneme Encoder}
\label{sec_phoneme_pretrain}
The phoneme encoder  takes a phoneme sequence  as input and outputs a phoneme hidden sequence. To enhance the representation capability of the phoneme encoder, we conduct large-scale phoneme pre-training. Previous works~\citep{xiao2020improving} conduct pre-training in character/word level and apply the pre-trained model to phoneme encoder, which will cause inconsistency, and the works~\citep{jia2021png} directly using phoneme pre-training will suffer from limited capacity due to too small size of phoneme vocabulary. To avoid these issues, we leverage mixed-phoneme pre-training~\citep{zhang2022mixed}, which uses both phoneme and sup-phoneme (adjacent phonemes merged together) as the input of the model, as shown in Figure~\ref{fig_model_pho}. When using masked language modeling~\cite{devlin2018bert}, we randomly mask some sup-phoneme tokens and their corresponding phoneme tokens and predict the masked phoneme and sup-phoneme at the same time. After mixed phoneme pre-training, we use the pre-trained model to initialize the phoneme encoder of our TTS system.

\subsection{Differentiable Durator}
\label{sec_diff_durator}

The differentiable durator  takes a phoneme hidden sequence as input, and outputs a sequence of prior distribution at the frame level, as shown in Figure~\ref{fig_model_dur}. We denote the prior distribution as , where . The differentiable durator  consists of several modules: 1) a duration predictor that builds upon the phoneme encoder to predict the duration for each phoneme, 2) a learnable upsampling layer that leverages the predicted duration to learn a projection matrix to extend the phoneme hidden sequence from phoneme level to frame level in a differentiable way~\citep{elias2021parallel}, and 3) two additional linear layers on the expanded hidden sequence to calculate the mean and variance of the prior distribution . The detailed formulation of differentiable durator is in Appendix~\ref{appendix_durator}.
We optimize the duration prediction, learnable upsampling layer, and mean/variance linear layers together with the TTS model in a fully differentiable way, which can reduce the training-inference mismatch in previous duration prediction (ground-truth duration is used in training while predicted duration is used in inference)~\citep{kim2020glow,kim2021conditional,ren2021fastspeech} and better use duration in a soft and flexible way instead of a hard expansion, hence the side-effect of inaccurate duration prediction is mitigated. 


\begin{figure} [t!]
\centering
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[page=3,width=1.0\columnwidth,trim=6cm 3.0cm 6.5cm 3.1cm,clip=true]{fig/fig.pdf} 
    \caption{Differentiable durator.}
    \label{fig_model_dur}
    \end{subfigure}
    \begin{subfigure}[b]{0.67\textwidth}
    \includegraphics[page=2,width=0.8\columnwidth,trim=4.5cm 3.3cm 5.0cm 3.3cm,clip=true]{fig/fig.pdf}
    \caption{Bidirectional prior/posterior.}
    \label{fig_model_bpp}
    \end{subfigure}\hspace{-0.7cm}
	\begin{subfigure}[b]{0.54\textwidth}
    \includegraphics[page=6,width=1\columnwidth,trim=4.5cm 2.7cm 5cm 2.55cm,clip=true]{fig/fig.pdf}
    \caption{Phoneme pre-training.}
    \label{fig_model_pho}
    \end{subfigure}\hspace{0.0cm}
    \begin{subfigure}[b]{0.39\textwidth}
    \includegraphics[page=5,width=1\columnwidth,trim=6cm 2.5cm 5.5cm 2.3cm,clip=true]{fig/fig.pdf} 
    \caption{Memory mechanism in VAE.}
    \label{fig_model_mem}
    \end{subfigure}
\caption{The designed modules in \myname{}.}
\label{fig_each_design}
\vspace{-0.2cm}
\end{figure}



\subsection{Bidirectional Prior/Posterior}
\label{sec_bidirect_flow}

As shown in Figure~\ref{fig_model_bpp}, we design a bidirectional prior/posterior module to enhance the capacity of the prior  or to reduce the complexity of the posterior  where  is the posterior encoder, since there is information gap between the posterior obtained from speech sequence and the prior obtained from phoneme sequence. We choose a flow model~\cite{dinh2014nice,rezende2015variational,kingma2016improved,kingma2018glow} as the bidirectional prior/posterior module (denoted as ) since it is easy to optimize and has a nice property of invertibility. 

\paragraph{Reduce Posterior  with Backward Mapping }
The bidirectional prior/posterior module can reduce the complexity of posterior from  to  through the backward mapping , i.e., for , . The objective is to match the simplified posterior  to the prior  by using the KL divergence loss as follows:

where the third equality (the second line) in Equation~\ref{eqa_kl_backward} is obtained via the change of variables: , and  according to inverse function theorem.


\paragraph{Enhance Prior  with Forward Mapping }
The bidirectional prior/posterior module can enhance the capacity of prior from  to  through the forward mapping , i.e., for , . The objective is to match the enhanced prior  to the posterior  using the KL divergence loss as follows:

where the third equality (the second line) in Equation~\ref{eqa_kl_forward} is obtained via the change of variables: , and  according to inverse function theorem, similar to that in Equation~\ref{eqa_kl_backward}. 

By using backward and forward loss functions, both directions of the flow model are considered in training, which can reduce the training-inference mismatch in the previous flow models that train in backward direction but infer in forward direction. We also provide another formulation of the bidirectional prior/posterior in Appendix~\ref{appendix_bpp}.

\subsection{VAE with Memory}
\label{sec_memory_vae}

The posterior  in the original VAE model is used to reconstruct the speech waveform, and thus is more complicated than the prior from the phoneme sequence. To further relieve the burden of prior prediction, we simplify the posterior by designing a memory based VAE model. The high-level idea of this design is that instead of directly using  for waveform reconstruction, we just use  as a query to attend to a memory bank, and use the attention result for waveform reconstruction, as shown in Figure~\ref{fig_model_mem}. In this way, the posterior  is only used to determine the attention weights in the memory bank, and thus is largely simplified. The waveform reconstruction loss based on memory VAE can be formulated as 

where  denotes the waveform decoder, which covers not only the original waveform decoder but also the model parameters related to the memory mechanism, including the memory bank  and the attention parameters , , , and , where  and ,  is the size of the memory bank and  is the hidden dimension. 




\subsection{Training and Inference Pipeline}
\label{sec_train_infer}

Besides the waveform reconstruction loss and bidirectional prior/posterior loss, we additionally conduct a fully end-to-end optimization to take the whole inference procedure in training for better voice quality. The loss function is formulated as follows. 



Based on Equation~\ref{eqa_kl_backward},~\ref{eqa_kl_forward},~\ref{eqa_vae_rec}, and ~\ref{eqa_fully_e2e}, the total loss function is

where .


\begin{wrapfigure}{r}{4cm}
  \centering
\includegraphics[page=4,width=0.3\columnwidth,trim=6.3cm 1.9cm 6.7cm 2.4cm,clip=true]{fig/fig.pdf}
\caption{Gradient flows.}
\label{fig_gradient_flow}
\vspace{-2mm}
\end{wrapfigure}
Note that there are some special explanations of the above loss functions: 1) Since the frame-level prior distribution  cannot well align with the ground-truth speech frames due to the intrinsically inaccurate duration prediction in durator, we leverage a soft dynamic time warping (DTW) version of KL loss for  and . See Appendix~\ref{appendix_soft_dtw} for the detailed formulation of the soft-DTW loss. 2) We write the waveform loss in   and  as negative log-likelihood loss for simplicity. Actually following~\cite{kong2020hifi},  consists of GAN loss, feature mapping loss and mel-spectrogram loss, while  consists of only GAN loss. We do not use soft-DTW in  since we found GAN loss can still perform well with mismatched lengths. See Appendix~\ref{appendix_wav_dec_loss} for the details of the waveform loss. 


There are several different gradient flows in training the model, as shown in Figure~\ref{fig_gradient_flow}: 1) ; 2) ; 3) ; 4) ; 5)  ; 6) . After training, we discard the posterior encoder  and only use  and  for inference. The training and inference pipeline is summarized in Algorithm~\ref{alg_train_infer}.

\begin{algorithm}[h]
\caption{Training and inference of \myname{}}
\label{alg}
\begin{algorithmic}[1]
\State \textbf{Training:}
\State ~~~~ Pre-train the phoneme encoder . 
\State ~~~~ Train the whole model  using loss  defined in Equation~\ref{eqa_total_loss}.
\State \textbf{Inference:} 
\State ~~~~ Sample prior .
\State ~~~~ Get enhanced prior .
\State ~~~~ Generate waveform sample . 
\end{algorithmic}
\label{alg_train_infer}
\end{algorithm}


\subsection{Advantages of \myname{}}
\label{sec_why_can_close}

We explain how the designs in our \myname{} system can close the quality gap to recordings.

\begin{itemize}[leftmargin=*]
\item \textit{Reduce training-inference mismatch}. We directly generate waveform from text and leverage a differentiable durator to ensure a fully end-to-end optimization, which can reduce the training-inference mismatch in the cascaded acoustic model/vocoder~\citep{kim2020glow,ren2021fastspeech,popov2021grad,elias2021parallel} and explicit duration prediction~\citep{kim2021conditional,kim2020glow,ren2021fastspeech}. Note that although VAE and flow can have training-inference mismatch inherently (waveform is reconstructed from the posterior in training while predicted from the prior in inference for VAE, and flow is trained in backward direction and infered in forward direction), we design the backward/forward loss in Equation~\ref{eqa_kl_backward} and~\ref{eqa_kl_forward} and the end-to-end loss in Equation~\ref{eqa_fully_e2e} to alleviate this problem.  

\item \textit{Alleviate one-to-many mapping problem}. Compared to previous methods using reference encoder~\citep{wang2018style,chen2021adaspeech,wu2022adaspeech,liu2021delightfultts} or pitch/energy extraction~\citep{ren2021fastspeech} for variation information modeling, our posterior encoder  in VAE acts like a reference encoder that can extract all the necessary variance information in posterior distribution . We do not predict pitch explicitly since it can be learned implicitly in the posterior encoder and the memory bank of VAE. To ensure the prior and posterior can match with each other, on the one hand, we simplify the posterior with memory VAE and backward mapping in the bidirectional prior/posterior module, and on the other hand, we enhance the prior with phoneme pre-training, differentiable durator, and forward mapping in the bidirectional prior/posterior module. Thus, we can alleviate the one-to-mapping problem to a large extent. 

\item \textit{Increase representation capacity}. We leverage large-scale phoneme pre-training to extract better representation from the phoneme sequence, and leverage the advanced generative models (flow, VAE, GAN) to capture the speech data distributions better, which can enhance the representation capacity of the TTS models for better voice quality. 

\end{itemize}

We further list the difference between our \myname{} and previous TTS systems as follows: 1) Compared to previous autoregressive TTS models such as Tacotron 1/2~\citep{wang2017tacotron,shen2018natural}, WaveNet~\citep{oord2016wavenet}, TransformerTTS~\citep{li2019neural}, and Wave-Tacotron~\cite{weiss2021wave}, our \myname{} is non-autoregressive in nature with a fast inference speed. 2) Compared to the previous systems with cascaded acoustic model and vocoder, such as Tacotron 1/2~\citep{wang2017tacotron,shen2018natural}, FastSpeech 1/2~\citep{ren2019fastspeech,ren2021fastspeech}, ParallelTacotron 2~\citep{elias2021parallel}, Glow-TTS~\citep{kim2020glow}, and Grad-TTS~\citep{popov2021grad}, we are fully end-to-end with no cascaded errors. 3) Compared to previous systems with various reference encoders and pitch/duration prediction, such as FastSpeech 2~\citep{ren2021fastspeech}, AdaSpeech~\citep{chen2021adaspeech}, and DelightfulTTS~\citep{liu2021delightfultts}, we unify all the variance information with a posterior encoder and model the duration in a fully differentiable way. 4) Compared to previous fully end-to-end TTS systems such as EATS~\citep{donahue2020end}, FastSpeech 2s~\citep{ren2021fastspeech}, and VITS~\citep{kim2021conditional}, we bridge the quality gap to recordings with advanced model designs to closely match the prior and posterior in the VAE framework. 






\section{Experiments and Results}

\subsection{Experimental Settings}
\label{sec_exp_setting}

\paragraph{Datasets}
We evaluate our proposed \myname{} on the LJSpeech dataset~\citep{ljspeech17}, which is widely used for benchmarking TTS. LJSpeech is a single speaker English corpus and consists of  audios and text transcripts, with a total length of nearly 24 hours at a sampling rate of kHz. We randomly split the dataset into training set with  samples, validation set with  samples, and test set with  samples. For phoneme pre-training on phoneme encoder, we collect a large-scale text corpus with  million sentences from the news-crawl dataset~\citep{newscrawl22}. Note that we do not use any extra paired text and speech data except for LJSpeech dataset. We conduct several preprocessings on the speech and text sequences: 1) We convert the text/character sequence into phoneme sequence~\cite{sun2019token} using a grapheme-to-phoneme tool~\citep{Bernard2021}. 2) We use linear-spectrograms as the input of the posterior encoder~\citep{kim2021conditional}, instead of original waveform sequence for simplicity. The linear-spectrograms are obtained by short-time Fourier transform (STFT) with FFT size, window size, and hop size of 1024, 1024, and 256, respectively. 3) For the mel-spectrogram loss on the waveform decoder, we obtain the mel-spectrograms by applying -dimension mel-filterbanks on the linear-spectrograms of the speech waveform. 

\paragraph{Model Configurations} 
Our phoneme encoder is a stack of  Feed-Forward Transformer~(FFT) blocks~\cite{ren2019fastspeech}, where each block consists of a multi-head attention layer and a 1D convolution feed-forward layer, with hidden size of . In the differentiable durator, the duration predictor consists of -layer convolution. We use 4 consecutive affine coupling layers~\cite{dinh2016density} in our bidirectional prior/posterior module following~\citep{kim2021conditional}. We discard the scaling operation in the affine transform to stabilize the bidirectional training. The shifting in the affine transform is estimated by a -layer WaveNet~\cite{oord2016wavenet} with a dilation rate of . The posterior encoder is based on a -layer WaveNet with a kernel size of  and a dilation rate of . The waveform decoder consists of  residual convolution blocks following~\citep{kong2020hifi}, where each block has  layers of 1D convolution. We perform transpose convolution for upsampling at every convolution block at a rate of . The hyperparameters of \myname{} are listed in Appendix~\ref{appendix_hyperpara}.


\paragraph{Training Details}
\label{sec:training-details}
We train our proposed system on  NVIDIA V100 GPUs with 32G memory, with a dynamic batch size of  speech frames (under hop size of ) per GPU, and a total k training epochs. We use AdamW optimizer~\citep{loshchilov2018decoupled} with  = ,  = . The initial learning rate is , with a learning rate decay factor  in each epoch, i.e., the learning rate is multiplied by  in every epoch. We find it is helpful to stabilize the training of our system and achieve better results through a warmup stage with k epochs at the beginning of the training, and a tuning stage with k epochs at the end of the training. More details about these training stages can be found in Appendix~\ref{appendix_training_details}.



\subsection{Comparison with Human Recordings}
We first compare the speech generated by \myname{} with human recordings in terms of both MOS and CMOS evaluation. As described in Section~\ref{sec_def_judge}, we use 50 test utterances, each with 20 judges for evaluation. As shown in Table~\ref{tab_mos_recording} and ~\ref{tab_cmos_recording}, our system achieves similar quality scores with human recordings in both MOS and CMOS. Importantly, our system achieves  CMOS compared to recordings, with a Wilcoxon p-value~\citep{wilcoxon1992individual} , which demonstrates the speech generated by our system has no statistically significant difference from human recordings\footnote{Audio samples can be found in \url{https://speechresearch.github.io/naturalspeech/}}~\footnote{Note that some human recordings in LJSpeech dataset may contain strange rhythm ups and downs that affect the rating score. To ensure the human recordings used for evaluation are of good quality, we let judges to exclude the recordings with strange rhythms from evaluation. Otherwise, our \myname{} will achieve better CMOS than human recordings. In a CMOS test without excluding bad recordings, \myname{} achieves  CMOS better than recordings.}. Thus, our \myname{} achieves human-level quality according to the definition and judgement in Section~\ref{sec_def_judge}.



\begin{table}[h!]
\caption{MOS comparison between \myname{} and human recordings. Wilcoxon rank sum test is used to measure the p-value in MOS evaluation.}
\centering
\begin{tabular}{ccc}
	\toprule
	Human Recordings &\myname{} & Wilcoxon p-value \\
	\midrule
	 &  &  \\
	\bottomrule
\end{tabular}
\label{tab_mos_recording}
\end{table}

\begin{table}[h!]
\caption{CMOS comparison between \myname{} and human recordings. Wilcoxon signed rank test is used to measure the p-value in CMOS evaluation.}
\centering
\begin{tabular}{ccc}
	\toprule
	Human Recordings &\myname{} & Wilcoxon p-value \\
	\midrule
	0 &  &  \\
	\bottomrule
\end{tabular}
\label{tab_cmos_recording}
\end{table}


\subsection{Comparison with Previous TTS Systems}
We compare our \myname{} with previous TTS systems, including: 1) FastSpeech 2~\citep{ren2021fastspeech} + HiFiGAN~\citep{kong2020hifi}, 2) Glow-TTS~\citep{kim2020glow} + HiFiGAN~\citep{kong2020hifi}, 3) Grad-TTS~\citep{popov2021grad} + HiFiGAN~\citep{kong2020hifi}, and 4) VITS~\citep{kim2021conditional}. We re-produce the results of all these systems by our own, which can match or even beat the quality in their original papers (note that the HiFiGAN vocoder is fine-tuned on the predicted mel-spectrograms for better synthesis quality). Both the MOS and CMOS results are shown in Table~\ref{tab_mos_previous_system}.  It can be seen that our \myname{} achieves better voice quality than these systems in terms of both MOS and CMOS. 


\begin{table}[h!]
\caption{MOS and CMOS comparisons between \myname{} and previous TTS systems.}
\centering
\begin{tabular}{ l | l | c }
    \toprule
	    System & MOS & CMOS \\
	    \midrule
	    FastSpeech 2~\citep{ren2021fastspeech} + HiFiGAN~\citep{kong2020hifi} &  &  \\
	    Glow-TTS~\citep{kim2020glow} + HiFiGAN~\citep{kong2020hifi} &  &  \\
	    Grad-TTS~\citep{popov2021grad} + HiFiGAN~\citep{kong2020hifi}  &   &  \\
	    VITS~\citep{kim2021conditional} &   &  \\
	    \midrule
	    \myname{} &  &  \\
		\bottomrule
	\end{tabular}
	\label{tab_mos_previous_system}
\end{table}






\subsection{Ablation Studies and Method Analyses}
\paragraph{Ablation Studies} We further conduct ablation studies to verify the effectiveness of each module in our system, as shown in Table~\ref{tab_cmos_ablation}. We describe the ablation studies as follows: 1) By removing phoneme pre-training, we do not initialize the phoneme encoder from pre-trained weights but just random initialization, which brings  CMOS drop, demonstrating the effectiveness of phoneme pre-training. 2) By removing differentiable durator, we do not use learnable upsampling layer and end-to-end duration optimization, but just use duration predictor for hard expansion. In this way, we use monotonic alignment search~\citep{kim2020glow} to provide the duration label to train the duration predictor through the whole training process. Removing differentiable durator causes  CMOS drop, demonstrates the importance of end-to-end optimization in duration modeling. 3) By removing bidirectional prior/posterior module, we only use  in training and do not use . It brings  CMOS drop, showing the gain by leveraging bidirectional training to bridge the gap between posterior and prior. 4) By removing memory mechanism in VAE, we use original VAE for waveform reconstruction, which causes  CMOS drop, showing the effectiveness of memory in VAE to simplify the posterior. 



\begin{table}[h!]
	\caption{Ablation studies on each design in \myname{} .}
	\centering
	\begin{tabular}{ l c}
		\toprule
	    Setting & CMOS \\
	    \midrule
		\myname{}  &  \\
		 Phoneme Pre-training &  \\
		 Differentiable Durator & \\
		 Bidirectional Prior/Posterior &  \\
		 Memory in VAE &  \\
		\bottomrule
	\end{tabular}
	\label{tab_cmos_ablation}
\end{table}


\paragraph{Inference Latency}
We compare the inference speed of our \myname{} with previous TTS systems. We measure the latency by using an NVIDIA V100 GPU with a batch size of  sentence and averaging the latency over the sentences in the test set. The results are shown in Table~\ref{tab_exp_latency}. The model components , , , and  in \myname{} are used in inference, with M model parameters. Our \myname{} achieves faster or comparable inference speed when compared with the previous systems, and achieves better voice quality.

\begin{table}[h!]
	\caption{Inference speed comparison. RTF (real-time factor) means the time (in seconds) to synthesize a -second waveform.  Grad-TTS (1000) and  Grad-TTS (10) mean using 1000 and 10 steps in inference respectively.}
	\centering
	\begin{tabular}{ l c}
		\toprule
	    System & RTF \\
	    \midrule
	    FastSpeech 2~\citep{ren2021fastspeech} + HiFiGAN~\citep{kong2020hifi}  &  \\
	    Glow-TTS~\citep{kim2020glow} + HiFiGAN~\citep{kong2020hifi} & \\
	    Grad-TTS~\citep{popov2021grad} (1000) + HiFiGAN~\citep{kong2020hifi} & \\
	    Grad-TTS~\citep{popov2021grad} (10) + HiFiGAN~\citep{kong2020hifi} & \\
	    VITS~\citep{kim2021conditional} & \\
	    \midrule
		\myname{}  &  \\
		\bottomrule
	\end{tabular}
	\label{tab_exp_latency}
\end{table}




\section{Conclusions and Discussions}
In this paper, we conduct a systematic study on the problems related to human-level quality in TTS. We first give a formal definition of human-level quality and describe the guidelines to judge it, and further build a TTS system called \myname{} to achieve human-level quality. Specifically, after analyzing the quality gap on several competitive TTS systems, we develop a fully end-to-end text to waveform generation system, with several designs to close the gap to human recordings, including phoneme pre-training, differentiable durator, bidirectional prior/posterior module, and memory mechanism in VAE. Evaluations on the popular LJSpeech dataset demonstrate that our \myname{} achieves human-level quality with CMOS evaluations, with no statistically significant difference from human recordings for the first time on this dataset. 

Note that by claiming our \myname{} system achieves human-level quality on LJSpeech dataset, we do not mean that we can surpass or replace human, but the quality of \myname{} is statistically indistinguishable from human recordings on this dataset. Meanwhile, although our evaluations are conducted on LJSpeech dataset, we believe the technologies in \myname{} can be applied to other languages, speakers, and styles to improve the general synthesis quality. We will further try to achieve human-level quality in more challenging datasets or scenarios, such as expressive voices, longform audiobook voices, and singing voices that have more dynamic, diverse, and contextual prosody in our future work. 


\bibliography{main}
\bibliographystyle{unsrt}





\appendix





\section{Study of the Quality Gap of Previous TTS System}
\label{appendix_study}
To understand where and how the quality gap to recordings comes from, we conduct a systematic study on the current TTS systems, which can help us to find the problems, and is equally important (if not more) than solving the problems. Specifically, we choose a state-of-the-art TTS system using FastSpeech 2~\cite{ren2021fastspeech} as the acoustic model and HiFiGAN~\cite{kong2020hifi} as the vocoder, which consists of four components: phoneme encoder, variance adaptor, mel-spectrogram decoder, and vocoder. We design a series of comparison experiments to measure the quality gap (in terms of CMOS) of each component to its corresponding upper bound. We conduct analyses from this order (from the closest to waveform to the farest): vocoder, mel-spectrogram decoder, variance adaptor, and phoneme encoder. 

\begin{table}[h!]
\small
	\caption{The CMOS of each component to its upper bound. Negative CMOS means this component setting is worse than its upper bound.}
	\centering
	\begin{tabular}{ l l l l}
		\toprule
	    Component & Setting & Upper Bound & CMOS   \\
		\midrule
		Vocoder  & GT MelVocoder & Human Recordings &  \\
		Mel Decoder & GT Pitch/DurationMel Decoder & GT Mel &   \\
		Variance Adaptor & Predicted Pitch/Duration & GT Pitch/Duration &  \\
		Phoneme Encoder & Phoneme Encoder & Phoneme Encoder + Pre-training &  \\
		\bottomrule
	\end{tabular}
	\vspace{0.3cm}
	\label{tab_study_on_quality_drop}
\end{table}


\begin{itemize}[leftmargin=*]

\item \textit{Vocoder.} We study the quality drop on the vocoder by comparing the two settings: 1) waveform generated by vocoder with ground-truth mel-spectrograms as input; 2) ground-truth waveform (human recordings). The CMOS is shown in Table~\ref{tab_study_on_quality_drop}. It can be seen that when taking ground-truth mel-spectrograms as input, the waveform generated by vocoder has some but not huge gap to human recordings. However, we need to pay attention to the training-inference mismatch in vocoder: in training, vocoder takes ground-truth mel-spectrograms as input, while in inference, it takes predicted mel-spectrograms as input. 

\item \textit{Mel-spectrogram Decoder.} We study the quality drop on the mel-spectrogram decoder by comparing the two settings: 1) mel-spectrograms generated by mel-spectrogram decoder with ground-truth pitch and duration as input\footnote{Ideally, we should also use ground-truth phoneme hidden sequence as input. However, ground-truth hidden sequence cannot be obtained. Thus, this comparison setting is just a approximation.}; 2) ground-truth mel-spectrograms (extracted from human recordings). We use the vocoder to convert the mel-sepctrograms in the two settings into waveform for evaluation. As shown in Table~\ref{tab_study_on_quality_drop}, the predicted mel-spectrograms have  CMOS drop compared to the ground-truth mel-spectrograms.

\item \textit{Variance Adaptor.} We study the quality drop on the variance adaptor by comparing the predicted pitch/duration with the ground-truth pitch/duration. We need the mel-spectrogram decoder and vocoder to generate the waveform for evaluation in the two settings. As shown in Table~\ref{tab_study_on_quality_drop}, the predicted pitch/duration have  CMOS drop compared to the ground-truth pitch/duration. 

\item \textit{Phoneme Encoder.} Since it is not straightforward to construct the upper bound of the phoneme encoder, we analyze the approximate quality drop through backward verification, by improving phoneme encoder for better voice quality. We conduct large-scale phoneme pre-training on the phoneme encoder, and fine-tune it with the FastSpeech 2 training pipeline, and achieves a  CMOS gain, as shown in Table~\ref{tab_study_on_quality_drop}, which demonstrates the phoneme encoder has improvement space.

\end{itemize}

According to the above experimental studies, we analyze several reasons causing the quality drop in each component: 1) Training-inference mismatch. Ground-truth mel-spectrogram, pitch, and duration are used in training, while predicted values are used in inference, which causes mismatch in the input of vocoder and mel-spectrogram decoder. Fully end-to-end text to waveform optimization is helpful to eliminate this mismatch. 2) One-to-many mapping problem. Text to speech mapping is one-to-many, where a text sequence can correspond to multiple speech utterances with different variation information (e.g., pitch, duration, speed, pause, prosody, etc). Current systems usually use a variance adaptor to predict variance information (e.g., pitch, duration) to alleviate this problem, which is not enough to well handle this problem. We should rethink previous methods on variance information and come up with some thorough and elegant solutions. 3) Lack of representation capacity. Current models are not powerful enough to extract good representations from phoneme sequence and learn complicated data distribution in speech. More advanced methods such as large-scale pre-training and powerful generative models are critical to enhance the learning capacity.  




\section{Differentiable Durator}
\label{appendix_durator}

To enable end-to-end duration optimization, we design a durator that can upsample a phoneme hidden sequence  into a frame-level hidden sequence  in a differentiable way, where  is the hidden dimension size, phoneme sequence length and frame sequence length, respectively.
The differentiable durator consists of a duration predictor  for phoneme duration prediction and a learnable upsampling layer  for sequence expansion from phoneme level to frame level. 


\paragraph{Duration Predictor}
The input to the duration predictor  is phoneme hidden sequence  and the output is the estimated phoneme duration . The duration predictor  consists of  layers of one-dimensional convolution, with ReLU activation, layer normalization, and dropout between each layer. 


\paragraph{Learnable Upsampling Layer}

The learnable upsampling layer  takes phoneme duration  as input and upsamples phoneme hidden sequence  to frame-level sequence ~\citep{elias2021parallel}. First, we calculate the duration start and end matrices  and  by 

where  indexes the -th element in the matrix. We calculate the primary attention matrix  and auxiliary context matrix  following ~\cite{elias2021parallel}:


where  represents one linear layer with input and output dimensions of .  is one-dimensional convolution operation with layer normalization and Swish activation~\cite{ramachandran2017searching}. The input and output dimensions of  are  and .  means adding an extra dimension by repeating the input matrix by  times. 
 stands for matrix concatenation along the hidden dimension, and gets a hidden dimension of .
 is a two-layer full-connected network with Swish activations. The numbers underneath  denote the input and output hidden dimensions. We set  and . The  operation is performed on the sequence time dimension. We calculate the frame-level hidden sequence output  with the following equation:


where  represents the einsum operation . We first permute  from  to  for computation, and after we get  with shape  and  with shape , we reshape them to  and  respectively for final projection to dimension . Finally, we map  with a mean and variance linear layer to get the frame-level prior distribution parameter  and , and get the prior distribution . 


Compared to simply repeating each phoneme hidden sequence with the predicted duration in a hard way, the learnable upsampling layer enables more flexible duration adjustment for each phoneme. Also, the learnable upsampling layer makes the phoneme to frame expansion differentiable, and thus can be jointly optimized with other modules in the TTS system. 




\section{Alternative Formulation of Bidirectional Prior/Posterior}
\label{appendix_bpp}
We provide another formulation of the backward loss  in Equation~\ref{eqa_kl_backward} and forward loss  in Equation~\ref{eqa_kl_forward} by directly using KL loss to match two distributions. 

For the backward loss, we directly match the posterior  to the prior :

where , and  according to the change of variable rule. 

For the forward loss, we directly match the prior  to the posterior :

where , and  according to the change of variable rule.





\section{Soft Dynamic Time Warping in KL loss}
\label{appendix_soft_dtw}

Since the frame-level prior distribution  usually has different lengths from the ground-truth speech frames, the standard KL loss cannot be applied. Therefore, we use a soft dynamic time warping (Soft-DTW) of KL loss for  and  to circumvent this mismatch.

The Soft-DTW version of the KL loss for  can be obtained by recursive calculation:

where  is the KL divergence loss between the simplified posterior  from frame  to frame  and the prior  from frame  to frame  with the best alignment.  is defined in Equation~\ref{eqa_kl_backward}.  a soft-min operator, which is defined as  and .  is a warp penalty for not choosing the diagonal path and is set as .  is the -th frame of the simplified posterior, and  is the -th frame of the prior.

The Soft-DTW version of KL loss for  is similar to that of , which can be defined as:

where  is the KL divergence loss between the enhanced prior  from frame  to frame  and the posterior  from frame  to frame  with the best alignment.  is defined in Equation~\ref{eqa_kl_forward}.  is the -th frame of the enhanced prior, and  is the j-th frame of the posterior.





\section{Waveform Decoder Loss}
\label{appendix_wav_dec_loss}

Instead of using negative log-likelihood loss in waveform reconstruction and prediction in Equation~\ref{eqa_vae_rec} and~\ref{eqa_fully_e2e}, we use GAN loss, feature mapping loss, and mel-spectrogram loss as used in ~\citep{kong2020hifi}.

\paragraph{GAN Loss}
The GAN loss follows LS-GAN~\citep{mao2017least}, which is defined as follows. The generator is trained to minimize the loss function while the discriminator is train to maximize it:

where is  the ground-truth waveform and  is the input of waveform decoder. We follow~\citep{kim2021conditional} for the design of discriminators.

\paragraph{Feature Mapping Loss}
The feature mapping loss consists of the L1 distance between real samples and fake samples in terms of the intermediate feature in each layer of the discriminator, which can be formulated as:

where  is the layer index in discriminator,  and  are the features and the number of features in the -th layer of the discriminator, respectively.

\paragraph{Mel-Spectrogram Loss}
The mel-spectrogram loss is L1 distance between the mel-spectrogram of ground-truth waveform and that of generated waveform, which can be defined as:

where  is the function that converts the waveform into corresponding mel-spectrogram.





\section{Training Details of \myname{}}
\label{appendix_training_details}




\paragraph{Phoneme Pre-training}
We pre-train our phoneme encoder on M phoneme sequences, which is converted from text with grapheme-to-phoneme conversion. The size of the phoneme dictionary is . We learn the sup-phoneme using Byte-Pair Encoding (BPE)~\citep{sennrich2015neural} with a sup-phoneme dictionary size of . We conduct the pre-training on  NVIDIA A100 GPUs with 80G memory (we only use A100 for phoneme pre-training, and use V100 for the remaining training of \myname{}), with a total batch size of  sentences for k training steps. The mask ratio for sup-phoneme is .  

\paragraph{Duration Predictor}
In the warmup stage (the first k epochs), we obtain the duration label to train the duration predictor to speed up the convergence of differentiable durator. We can choose any tools to provide duration label, such as Montreal forced alignment~\citep{mcauliffe2017montreal}. Here we choose monotonic alignment search~(MAS)~\cite{kim2020glow}, which estimates the optimal alignment between the phoneme prior distribution  and simplified frame-level posterior , where  are the mean and variance parameters obtained from the phoneme hidden sequence by two linear layers. The monotonic and non-skipping constraints of MAS provide the inductive bias that human read words in orders without skipping. The optimal alignment search result  can be formulated as

where  denotes the aligned phoneme index of the -th frame  from . 
We search the alignment result using dynamic programming. Let  denote the probability of  belongs to the prior distribution of the -th phoneme, then we can formulate  recursively with  and  with the following equation:


We calculate all the  from  to . Since the best alignment path is determined by the highest  value, we utilize all the cached  value to backtrack from  to  for the most probable alignment .

Note that in the warmup training stage, the duration  comes from MAS. After the warmup stage, the input duration comes from the duration predictor . During the whole training process, we apply gradient stop operation on the input of duration predictor.

\paragraph{Bidirectional Prior/Posterior}
For the two loss terms  and  in bidirectional prior/posterior module, we only use  during the warmup stage to learn a reasonable prior distribution, and then add  to the loss function for bidirectional optimization after the warmup stage.

\paragraph{VAE with Memory}
In the warmup stage, we do not use the memory bank in VAE training, i.e.,  is directly taken as the input of the waveform decoder. After the warmup stage, we initialize the memory banks  as follows: we first get the posterior  of each frame of the utterances in the training set, and then conduct K-means clustering on these  to get K clusters, and use the cluster center to initialize the memory bank . After the initialization, we jointly train the memory mechanism with the whole TTS system.

In the tuning stage (the last k epochs), we only use  to tune the model. We freeze the parameters of posterior encoder, waveform decoder, phoneme encoder, and bidirectional prior/posterior, and only update the durator for fully end-to-end duration optimization.


\section{Hyper-Parameters of \myname{}}
\label{appendix_hyperpara}
The hyper-parameters of \myname{} are listed in Table~\ref{tab_hyperpara}.


\begin{table}[h!]
\small
    \centering
    \caption{Hyper-parameters of \myname{}.}
    \begin{tabular}{llc}
    \toprule
    Module & Hyper-Parameter     &   Value  \\
    \midrule
    \multirow{7}{*}{Phoneme Encoder } & Phoneme Encoder Embedding Dimension &       \\
    &Phoneme Encoder Blocks  &  \\
    &Phoneme Encoder Multi-Head Attention Hidden Dimension &  \\
    &Phoneme Encoder Multi-Head Attention Heads &  \\
    &Phoneme Encoder Conv Kernel Size &  \\
    &Phoneme Encoder Conv Filter Size &  \\
    &Phoneme Encoder Dropout &  \\
    \midrule
    \multirow{5}{*}{Durator }&Duration Predictor Kernel Size &  \\
    &Duration Predictor Filter Size &  \\
    &Duration Predictor Dropout &  \\
    &Upsampling Layer Kernel Size &  \\
    &Upsampling Layer Filter Size &  \\
    \midrule
    \multirow{5}{*}{Prior/Posterior }&Flow Model Affine Coupling Layers &  \\
    &Flow Model Affine Coupling Dilation &  \\
    &Flow Model Affine Coupling Kernel Size &  \\
    &Flow Model Affine Coupling Filter Size &  \\
    &Flow Model Affine Coupling WaveNet Layers &  \\
    \midrule
    \multirow{6}{*}{Waveform Decoder }&Waveform Decoder ConvBlocks &  \\
    &Waveform Decoder ConvBlock Hidden &  \\
    &Waveform Decoder ConvBlock Upsampling Ratio &  \\
    &Waveform Decoder ConvLayers &  \\
    &Waveform Decoder ConvLayer Kernel Size &  \\
    &Waveform Decoder Conv Dilation &  \\
    &Memory Banks Size &  \\
    &Memory Banks Hidden Dimension &  \\
    &Memory Banks Attention Heads &  \\
    \midrule
    \midrule
    \multirow{5}{*}{Posterior Encoder }&Posterior Encoder WaveNet Layers &  \\
    &Posterior Encoder Dilation &   \\
    &Posterior Encoder Conv Kernel Size &  \\
    &Posterior Encoder Conv Filter Size &  \\
    \midrule
    Discriminator  & Multi-Period Discriminator Periods &  \\
    \bottomrule
    \end{tabular}
    \label{tab_hyperpara}
\end{table}


The number of model parameters for , , , and  is M, for the posterior encoder  is M, and for the discriminators is M. Note that only , , , and  with M model parameters are used in inference. 


\end{document}
