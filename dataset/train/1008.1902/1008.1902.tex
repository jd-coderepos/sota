\documentclass[final]{svjour3}
\usepackage[draft]{fixme}


\usepackage{graphicx}

\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{bbding}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{url}

\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{stackrel}
\usepackage{nicefrac}

\input xy
\xyoption{all}

\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}

\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb C}
\newcommand{\HH}{\mathbb H}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\Gr}{\mathrm{Gr}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\Irr}{\mathrm{Irr}}
\newcommand{\E}{\mathscr{E}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Span}{\mathrm{span}\,}
\newcommand{\Ker}{\mathrm{Ker\ }}
\newcommand{\Ima}{\mathrm{Im\ }}

\newcommand{\SOtwo}{\mathrm{SO}(2)}

\newcommand{\SSS}{\mathbb{S}}
\renewcommand{\TT}{\mathbb{T}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\codim}{\mathrm{codim\,}}
\newcommand{\pr}[2]{\left<#1\,|\,#2\right>}
\newcommand{\Int}{\mathrm{Int}\,}
\newcommand{\cyc}[1]{\left<#1\right>}
\newcommand{\tr}{\mathrm{tr}\,}
\renewcommand{\phi}{\varphi}
\newcommand{\inclusion}{\hookrightarrow}
\newcommand{\pf}[2]{\partial_{#2} #1}
\newcommand{\pfb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pfc}[2]{\tfrac{\partial}{\partial #2}#1}
\newcommand{\pfcc}[3]{\tfrac{\partial^2}{\partial #2\partial #3}#1}
\newcommand{\pfa}[1]{\partial_{#1}}
\newcommand{\df}[2]{\tfrac{d}{d#2} #1}
\newcommand{\sdf}[2]{\scriptscriptstyle\frac{d}{d#2} #1}
\newcommand{\Df}[2]{\tfrac{D}{d#2} #1}
\newcommand{\tm}{d}
\newcommand{\D}[1]{D_{#1}}
\newcommand{\grad}{\mathrm{grad}}

\newcommand{\closure}[1]{\mathrm{cl}\left(#1\right)}
\newcommand{\interior}[1]{\mathrm{int}\left(#1\right)}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\cyclic}[1]{\left<#1\right>}
\newcommand{\rvec}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\rdvec}[3]{\begin{bmatrix}#1_#2,\ldots,#1_#3\end{bmatrix}}
\newcommand{\cvec}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\cdvec}[3]{\begin{bmatrix}#1_#2\\ \vdots\\#1_#3\end{bmatrix}}
\newcommand{\ip}[1]{\left<#1\right>}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\set}[2]{\left\{ #1\,|\,#2 \right\}}
\newcommand{\len}{l}

\newcommand{\cut}{\tau_c}
\newcommand{\distx}[1]{d_{#1}}
\newcommand{\dist}{d}
\newcommand{\lenc}[1]{L\left(#1\right)}
\newcommand{\g}[1]{g\left(#1\right)}
\newcommand{\tg}[1]{\tilde{g}\left(#1\right)}
\newcommand{\mindex}{\mathrm{index}\,}
\newcommand{\diam}{\mathrm{diam}\,}
\newcommand{\Vs}{\mathbf{V}}
\newcommand{\Gras}[2]{\mathrm{Gr}_{#1}\left(#2\right)}
\newcommand{\tensor}{\otimes}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Log}{\mathrm{Log}}

\newcommand{\conv}[1]{\stackrel{#1}{\rightarrow}}
\newcommand{\weakconv}[1]{\stackrel{#1}{\rightharpoondown}}
\newcommand{\strongconv}[1]{\stackrel{#1}{\longrightarrow}}


\newcommand{\lemref}[1]{\text{Lemma~\ref{lem:#1}}}
\newcommand{\slemref}[1]{\text{Lem.~\ref{lem:#1}}}
\newcommand{\lemmataref}[2]{\text{Lemmata~\ref{lem:#1} and \ref{lem:#2}}}
\newcommand{\scorref}[1]{\text{Cor.~\ref{cor:#1}}}
\newcommand{\corollariesref}[2]{\text{Corollaries~\ref{cor:#1} and \ref{cor:#2}}}
\newcommand{\thmref}[1]{\text{Theorem~\ref{thm:#1}}}
\newcommand{\sthmref}[1]{\text{Thm.~\ref{thm:#1}}}
\newcommand{\thmsref}[2]{\text{Theorems~\ref{thm:#1} and \ref{thm:#2}}}
\newcommand{\propref}[1]{\text{Proposition~\ref{prop:#1}}}
\newcommand{\spropref}[1]{\text{Prop.~\ref{prop:#1}}}
\newcommand{\propsref}[2]{\text{Propositions~\ref{prop:#1} and \ref{prop:#2}}}
\newcommand{\remref}[1]{\text{Remark~\ref{rem:#1}}}
\newcommand{\sremref}[1]{\text{Rem.~\ref{rem:#1}}}
\newcommand{\remsref}[2]{\text{Remarks~\ref{rem:#1} and \ref{rem:#2}}}

\newcommand{\stacktwo}[2]{\stackrel{#1}{#2}}
\newcommand{\stackthree}[3]{\stackrel{#1}{\stackrel{#2}{#3}}}

\newcommand{\define}[1]{\emph{#1}}
 
\newcommand{\dfp}{d^v}
\newcommand{\tmp}[1]{\tm^{#1}}
\newcommand{\Hess}{H}
\newcommand{\param}[1]{\mathbf{#1}}


\begin{document}

\title{Optimization over Geodesics for Exact Principal Geodesic Analysis
}
\titlerunning{Optimization over Geodesics for Exact Principal Geodesic Analysis}

\author{S. Sommer
\and
F. Lauze
\and
M. Nielsen
}

\institute{S. Sommer (\Envelope) \at
              Dept. of Computer Science, Univ. of Copenhagen, Copenhagen, Denmark\\
              \email{sommer@diku.dk}, 
              Tel.: +4535321400
           \and
F. Lauze \at
              Dept. of Computer Science, Univ. of Copenhagen, Copenhagen, Denmark\\
              \email{francois@diku.dk}
           \and
M. Nielsen \at
              Dept. of Computer Science, Univ. of Copenhagen, Copenhagen, Denmark\\
              Biomediq, Copenhagen, Denmark, 
              \email{madsn@diku.dk}
}

\maketitle

\begin{abstract}
In fields ranging from computer vision to signal processing and statistics, increasing computational power allows a move from classical linear models to models that incorporate non-linear phenomena. This shift has created interest in computational aspects of differential geometry, and solving optimization problems that incorporate non-linear geometry constitutes an important computational task. In this paper, we develop methods for numerically solving optimization problems over spaces of geodesics using numerical integration of Jacobi fields and second order derivatives of geodesic families. As an important application of this optimization strategy, we compute exact Principal Geodesic Analysis (PGA), a non-linear version of the PCA dimensionality reduction procedure. By applying the exact PGA algorithm to synthetic data, we exemplify the differences between the linearized and exact algorithms caused by the non-linear geometry. In addition, we use the numerically integrated Jacobi fields to determine sectional curvatures and provide upper bounds for injectivity radii. 

\keywords{
    geometric optimization,
    principal geodesic analysis, 
    manifold statistics,
    differential geometry, 
    Riemannian metrics
    }
    \subclass{65K10 \and 57R99}
\end{abstract}



\section{Introduction}
\label{introduction}
Manifolds, sets locally modeled by Euclidean spaces, have a long and intriguing
history in mathematics, and topological, differential geometric, and Riemannian
geometric properties of manifolds have been studied extensively. The introduction of
high performance computing in applied fields has widened the use of differential
geometry,
and Riemannian manifolds, in particular, are now used for modeling a range
of problems possessing non-linear structure. Applications include shape modeling 
(complex projective shape spaces
\cite{kendall_shape_1984} and medial representations of surfaces
\cite{blum_transformation_1967,joshi_multiscale_2002}),
imaging (tensor manifolds in diffusion tensor imaging 
\cite{fletcher_principal_2004,fletcher_riemannian_2007,pennec_riemannian_2006} and
image segmentation and registration \cite{caselles_geodesic_1995,pennec_feature-based_1998}),
and several other fields (forestry \cite{huckemann_intrinsic_2010}, human motion
modeling \cite{sminchisescu_generative_2004,urtasun_priors_2005},
information geometry and signal processing \cite{yang_means_2011}).

To fully utilize the power of manifolds in applied modeling, it is essential to develop fast and
robust algorithms for performing computations on manifolds, and, in particular,
availability of methods for solving optimization problems is paramount. In this paper, 
we develop methods for numerically solving optimization problems over spaces of
geodesics using numerical integration of Jacobi fields and second order derivatives of
geodesic families.
In addition, the approach allows numerical approximation of sectional curvatures
and bounds on injectivity
radii \cite{huckemann_intrinsic_2010}. The methods apply to
manifolds represented both parametrically and implicitly without 
preconditions such as knowledge of explicit formulas for geodesics. This fact
makes the approach applicable to a range of applications, and it allows
exploration of the effect of curvature on non-linear statistical
methods.

To exemplify this, we consider the problem of capturing the variation of a set
of manifold valued data with the 
Principal Geodesic Analysis (PGA, \cite{fletcher_principal_2004-1}) non-linear
generalization of Principal Component Analysis (PCA). Until now, there has been no
method for numerically computing PGA for general manifolds without linearizing
the problem. Because PGA can be formulated as an optimization
problem over geodesics, the tools developed here apply to computing
it without discarding the non-linear structure. As a result, the paper 
provides an algorithm for
computing exact PGA for a wide range of manifolds.

\subsection{Related Work}
\label{sec:related}
A vast body of mathematical literature describes manifolds and Riemannian
structures; 
\cite{do_carmo_riemannian_1992,lee_riemannian_1997} provide
excellent introductions to the field. From an applied point of view, the papers
\cite{dedieu_symplectic_2005,herbert_bishop_keller_numerical_1968,noakes_global_1998,klassen_geodesics_2006,schmidt_shape_2006,sommer_bicycle_2009}
address first-order problems such as computing geodesics and solving the
exponential map inverse problem, the logarithm map.
Certain second-order problems including computing Jacobi fields on
diffeomorphism groups \cite{younes_evolutions_2009,ferreira_newton_2008}
have been considered but mainly on limited classes of manifolds. 
Different aspects of numerical computation on implicitly defined manifolds are covered in
\cite{zhang_curvature_2007,rheinboldt_manpak:_1996,rabier_computational_1990},
and generalizing linear statistics to manifolds has been the focus of the papers
\cite{karcher_riemannian_1977,pennec_intrinsic_2006,fletcher_robust_2008,fletcher_principal_2004-1,huckemann_intrinsic_2010}.

Optimization problems can be posed on a manifold in the sense that the domain of
the cost function is restricted to the manifold.
Such problems are extensively covered in the literature 
(e.g. \cite{luenberger_gradient_1972,yang_globally_2007}). In contrast, this
paper concerns
optimization problems over geodesics with the complexity residing in 
the cost functions and not the optimization
domains.

The manifold generalization of linear PCA, PGA, was first introduced in
\cite{fletcher_statistics_2003} but it was formulated in the form most widely used in
\cite{fletcher_principal_2004-1}. It has subsequently been used for several
applications. To mention a few, the authors in 
\cite{fletcher_principal_2004-1,fletcher_principal_2004} study
variations of medial atoms, \cite{wu_weighted_2008} uses a variation
of PGA for facial classification, \cite{said_exact_2007} presents examples on
motion capture data, and \cite{sommer_bicycle_2009} applies PGA to vertebrae
outlines. 
The algorithm presented in \cite{fletcher_principal_2004-1} for computing PGA with tangent space
linearization is most widely used. 
In contrast, \cite{said_exact_2007} computes PGA as
defined in \cite{fletcher_statistics_2003} without approximations, exact PGA, on
a particular 
manifold, the Lie group . The paper
\cite{sommer_manifold_2010} uses the methods presented here to experimentally
assess the effect of tangent space linearization
on high dimensional manifolds modeling real-life
data.

A recent wave of interest in manifold valued statistics 
has lead to the development of Geodesic PCA (GPCA, 
\cite{huckemann_intrinsic_2010})
and Horizontal Component Analysis (HCA, \cite{sommer_horizontal_2013}).
GPCA is in many respects close to PGA but
GPCA optimizes for the placement of the center point
and minimizes projection residuals along geodesics. HCA builds low-dimensional orthogonal
decompositions in the frame bundle of the manifold that project back to
approximating subspaces in the manifold.

\subsection{Content and Outline}
The paper presents two main contributions: (1) how numerical integration of 
Jacobi fields and second order derivatives can be used to solve optimization problems over
geodesics; and (2) how the approach allows numerical computation of exact PGA. In addition,
we use the computed Jacobi fields to numerically approximate 
geometric properties such as sectional curvatures.
After a brief discussion of the geometric background, explicit differential equations
for computing Jacobi fields and second derivatives of geodesic families are
presented in Section~\ref{sec:differentials}.
The actual derivations are performed in the appendices due to
their notational complexity. In Section~\ref{sec:pga}, the
exact PGA algorithm is developed. We end the paper with experiments that
illustrate the effect of curvature on the non-linear statistical method 
and with estimation of sectional curvatures and injectivity radii bounds.

The importance of curvature computations is noted in
\cite{huckemann_intrinsic_2010}, which lists the ability to compute sectional
curvature as a high importance open problem. The paper presents a partial
solution to this problem: we discuss how sectional curvatures can be determined
numerically when either a parametrization or implicit representation is available.

In the experiments, we evaluate how the differences between the exact
and linearized PGA depend on
the curvature of the manifold. This experiment, which to the best of our
knowledge has not been made before, is made possible by the
generality of the optimization approach that makes the algorithm 
applicable to a wide range of manifolds with varying curvature.

\section{Background}
\label{sec:geom-notation}
This section will include brief discussions of relevant aspects 
of differential and Riemannian geometry. We keep the notation close to the
notation used in the book \cite{do_carmo_riemannian_1992}; 
see in addition Appendix~\ref{app:notation}.

\subsection{Manifolds and Their Representations}
\label{sec:rep}
In the sequel,  will denote a Riemannian manifold of finite dimension . We will need  
to be sufficiently smooth, i.e. of class  for  or  depending on the
application. For concrete computational applications, we will represent 
 either using \emph{parametrizations} or \emph{implicitly}.
A local parametrization is a map  from an open subset
 to . With an implicit representation,
 is represented as a level set of a differentiable map , e.g.
. If the Jacobian matrix 
 has full rank  everywhere on ,  will be an -dimensional
manifold. The space  is called the embedding space.
When dealing with implicitly defined manifolds, we let  and  
denote the dimension of the domain and codomain of , respectively, so that
the dimension  of the manifold equals .
Examples of applications using implicit representations include
shape and human poses models \cite{sommer_bicycle_2009,hauberg_natural_2012}, and
several shape models use parametric representations
\cite{joshi_multiscale_2002,klassen_analysis_2004}.\footnote{Other 
representations include discrete triangulations used for surfaces and
quotients  of a larger manifold  by a group . The
latter is for example the case for Kendall's shape-spaces 
\cite{kendall_shape_1984}. Kendall's shape-spaces for planar points are actually
complex projective spaces  for which parameterizations are
available, and, for points in -dimensional space and higher, the shape-spaces
are anomalous and not manifolds. The spaces studied in
\cite{huckemann_intrinsic_2010} belong to this class.}

\subsection{Geodesic Systems}
\label{sec:geodesic-systems}
Given a local parametrization , a curve  on
 is a geodesic if the curve  in  representing , i.e.
, satisfies the ODE

Here  denotes the Christoffel symbols of the Riemannian metric.
Conversely, geodesics can be found by solving the ODE with a starting point
 and initial velocity  as initial conditions. 
The exponential map  maps the initial point  and velocity
 to 
, the
point on the geodesic at time .
When defined, the logarithm map  is the inverse of
, i.e. .
For implicitly represented manifolds, the classical ODE describing geodesics is 
not directly usable because neither parametrizations nor Christoffel symbols 
are directly available. Instead, the geodesic with initial point  and
initial velocity  can be found as the -part of the solution 
of the IVP

see \cite{dedieu_symplectic_2005}. Note that  is a curve in the embedding
space  but since  is a subset of the embedding space and the starting
point  is in ,  will stay in  for all .
Recall that  is the map defining the manifold by
e.g.  and that  denotes the Hessian of the th
component of .  is map between Euclidean spaces and the Hessian is
therefore the ordinary Euclidean Hessian matrix. 
The map  is defined by
 where
the symbol  denotes the generalized inverse or pseudo-inverse
of the non-square matrix . Since  has full-rank ,  equals
. Numerical stability of the geodesic
system is treated in \cite{dedieu_symplectic_2005}.

\subsection{Geodesic Families and Variations of Geodesics}
In the next sections, we will treat optimization problems over geodesics of which 
the PGA problem \eqref{eq:pga-cost} constitute a concrete example;
in addition, problems such as geodesic regression \cite{fletcher_geodesic_2011} and
manifold total least squares belong to this class.
For this purpose, we here recall the close connection between variations of geodesics,
Jacobi fields, and the differential . 
Let  be a family of geodesics parametrized by , i.e. for each
, the curve  is a geodesic. 
By varying the parameter , a vector field  is obtained.\footnote{
Recall that  is a shorthand for
, see Appendix~\ref{app:notation}.}
These \emph{Jacobi fields} are uniquely determined by 
the initial conditions  and , the variation of the initial
points  and the covariant derivative of the field at , respectively. Define 
, , and
. If  and  then 
 equals  \cite[Chap.
5]{do_carmo_riemannian_1992}. When 
is constant, i.e. , we have the following connection between  and
the differential :

Jacobi fields can equivalently be defined as solutions to an ODE that involves
the curvature endomorphism of the manifold \cite[Chap.
5]{do_carmo_riemannian_1992}.  However, the curvature
endomorphism is not easily computed when the manifold is represented implicitly,
and, therefore, the ODE is hard to use for computational applications in this
case. In the next section, we
numerically compute Jacobi fields by integrating the
differential of the system \eqref{sys:impl-geo}.
\begin{figure}[h]
    \begin{center}
      \includegraphics[width=0.50\columnwidth,trim=30 20 10 10,clip=true]{figures/sphere3}
    \end{center}
    \caption{The sphere  with a Jacobi field along a geodesic connecting
    the poles. Each pole is a conjugate point to the other since the non-zero Jacobi
    field vanishes. The injectivity radius is equal to the length of the
    geodesic, .}
    \label{fig:sphere-jacobi}
\end{figure}

Jacobi fields can be used to retrieve various geometric information e.g.
sectional curvature. Let  denote a Jacobi field along the geodesic  with
 and derivative . Assume the vectors  and  
are orthonormal. These vectors define
a plane  in , and  denotes the sectional
curvature of the plane . Because  occurs in a Taylor expansion of the length ,
the sectional curvature can be estimated by

for small .
Furthermore, if  is a non-zero Jacobi field with  along a geodesic
 and, for some
, also  then  is called a
conjugate point to . This
can provide an upper bound on the injectivity radius of , that, in
general terms, specifies the minimum length of non-minimizing
geodesics. Figure~\ref{fig:sphere-jacobi} illustrates the situation on the
sphere . We will explore both these points in the experiments section.

\subsection{Principal Geodesic Analysis}
\label{sec:pga-def}
Principal Component Analysis (PCA) is widely used to model the
variability of data in Euclidean spaces. The procedure provides linear
dimensionality reduction by
defining a sequence of linear subspaces maximizing the variance of the projection
of the data to the subspaces or, equivalently, minimizing the reconstruction errors. The th 
subspace is spanned by an orthogonal
basis  of principal components , and
the th principal component is defined recursively by

when formulated as to maximize the variance of the projection of the dataset
 to the subspaces .

PCA is dependent on the vector space structure of the Euclidean space and hence cannot be performed 
on manifold valued datasets. Principal Geodesic
Analysis was developed to overcome this limitation. 
PGA finds geodesic subspaces centered at point  with  usually being
an intrinsic mean\footnote{
The notion of intrinsic mean goes back to Fr\'echet \cite{frechet_les_1948} 
and Karcher \cite{karcher_riemannian_1977}.
As in \cite{fletcher_principal_2004-1}, an intrinsic mean is here a minimizer of
. Uniqueness issues are treated in 
\cite{karcher_riemannian_1977}.
}
of the dataset , .
The th geodesic subspace  of  is defined as
 with  
being the span of the principal directions  defined recursively by

The projection
 of a point 
onto a geodesic subspace  is

The term being maximized in \eqref{eq:pga-cost} is the sample variance of the projected data, the expected value of the
squared distance to , and PGA therefore extends PCA by finding \emph{geodesic subspaces} in 
which variance is maximized.\footnote{
A slightly different definition that uses 
one-dimensional subspaces and Lie group structure was introduced
in \cite{fletcher_statistics_2003}.
}

Since both optimization problems \eqref{eq:pga-cost} and
\eqref{eq:proj-cost} are difficult to optimize, PGA has traditionally
been computed using the orthogonal projection in the tangent space of 
to approximate the true projection. With this approximation,
equation \eqref{eq:pga-cost} simplifies to

which is equivalent to \eqref{eq:pca-cost}, and, therefore,
the procedure amounts to performing regular PCA on the vectors .
We will refer to PGA with the approximation
as \emph{linearized} PGA, and, following \cite{said_exact_2007}, PGA as 
defined by \eqref{eq:pga-cost} will be referred to
as \emph{exact} PGA.\footnote{In \cite{said_exact_2007},
the fact that  has a closed form
solution on the sphere  when  is a one-dimensional geodesic subspace is used to 
iteratively compute
PGA with the \cite{fletcher_statistics_2003} definition.}
The ability to iteratively solve optimization problems over
geodesics that we will develop in the next sections will allow us to
optimize \eqref{eq:pga-cost} and hence numerically compute exact PGA.

In general, PGA might not be well-defined as the intrinsic mean might not be
unique and both existence and uniqueness may fail for the 
projections \eqref{eq:proj-cost} and the optimization problem \eqref{eq:pga-cost}. The convexity
bounds of Karcher \cite{karcher_riemannian_1977} ensures uniqueness of the mean
for sufficiently local data but setting up sufficient conditions to ensure
well-posedness of \eqref{eq:proj-cost} and \eqref{eq:pga-cost}
for general manifolds is difficult
because they depend on the global geometry of the manifold.

There is ongoing discussion of when principal components should be constrained to
pass the intrinsic mean as in PGA or if other types of means should be used, see 
\cite{huckemann_intrinsic_2010} with discussions. 
In Geodesic PCA \cite{huckemann_intrinsic_2010}, the principal geodesics do not necessarily pass the intrinsic
mean, and similar optimization that allows the PGA base point to move away from the
intrinsic mean can be carried
out with the optimization approach used in this paper.
PGA can also be modified by replacing maximization of sample variance by 
minimization of reconstruction error.
This alternate definition is not equivalent to the definition above, a fact that
again underlines the difference between the Euclidean and the curved
situation. We will illustrate differences between the formulations in the
experiments section but we mainly use the variance formulation \eqref{eq:pga-cost}.

\section{Optimization over Geodesics}
\label{sec:differentials}
Equation \eqref{eq:pga-cost} and \eqref{eq:proj-cost} defining 
PGA are examples of optimization problems over geodesics that in those cases are
represented by their starting point  and initial velocity .
More generally, we here consider problems

where  is a function defining the cost of the geodesic
 here at time .\footnote{Even more generally, 
can be a function of the entire curve ,  instead of just
for the point ,  Note that for PGA, the initial velocity is in addition
constrained to subspaces of .}. In order to iteratively solve
optimization problems of the form \eqref{eq:geo-optim}, we will need 
derivatives of  since

with . Thus, we wish to compute the differential of 
with respect to initial point  and initial velocity .
Since \eqref{eq:pga-cost} is a function of the
projection  given by \eqref{eq:proj-cost}, we will later see that we need the second 
order differential of  as well.

Only in specific cases where explicit expressions for geodesics are available
can the above mention differentials be derived in closed form. Instead, for general
manifolds, the ODEs \eqref{eq:param-geo} and \eqref{sys:impl-geo} describing geodesics 
can be differentiated giving systems that can be numerically integrated to
provide the differentials.
This approach relies on the fact that sufficiently smooth initial value problems
(IVPs) are
differentiable with respect to their initial values, see e.g. \cite[Chap.
I.14]{hairer_solving_2008}.

We will here derive explicit expressions for IVPs describing the 
differential of the exponential map and Jacobi fields. In addition, we 
will differentiate the IVPs a second time. The concrete expressions will allow
the IVPs to be used for iterative optimization of problems on the form
\eqref{eq:geo-optim}. In particular, they will be used for the
exact PGA algorithm presented in the next section. 
The basic strategy is simple: we differentiate the geodesic systems of
Section~\ref{sec:geodesic-systems}. Though the resulting equations are
notationally complex, 
their derivation is in principle just repeated application of the
chain and product rules for differentiation.
MATLAB code for numerical integration of the systems is available at
\url{http://image.diku.dk/sommer}.

Since the geodesic equations \eqref{sys:impl-geo} contain the
generalized inverse of the Jacobian matrix , we will use the following
formula for derivatives of generalized inverses. When an  matrix  depends
on a parameter  and has full rank , and if
its generalized inverse  is differentiable,
then the derivative  is given by 

This result was derived in \cite{decell_derivative_1974,golub_differentiation_1973}
and \cite{hanson_extensions_1969} for the full-rank case.
We will apply \eqref{eq:decell-expr} with  when  is an  dependent
family of curves in the embedding space  that are geodesics on  and
when  is fixed. To see 
that  is
differentiable with respect to  when  depends smoothly on , take a 
frame of the normal space to  in a neighborhood of , and note that 
 is a composition of a invertible map onto the frame 
depending smoothly on  and the frame itself. 

The explicit expressions for the differential equations are notationally
heavy. Therefore, we only
state the results here and postpone the actual derivation 
to Appendix~\ref{app:A}.
\begin{em}
  \vspace{0.5em}
  \\
    Let  be defined as a regular zero
    level set of a  map . Using the embedding, we
    identify curves in  and vectors in  with curves and vectors in
    . Let  be a geodesic with  and .
    The Jacobi field  along  with  and
     can then be found as the -part of the solution of the IVP
    
    with  the map given in explicit form in Appendix~\ref{app:A}.
\end{em}\vspace{0.5em}\\
As previously noted, Jacobi fields can be described using an ODE incorporating
the curvature endomorphism in the parameterized case. We can, however, apply a
procedure similar to the implicit case and derive and IVP by differentiating
the geodesic system \eqref{eq:param-geo}. We will use the resulting IVP
\eqref{sys:JP} when working
with variations of geodesics in the parameterized case, see
Appendix~\ref{app:A}.

The systems \eqref{sys:JI} and \eqref{sys:JP} are both linear in the initial values  
as expected of systems describing differentials. They are non-autonomous due to the
dependence on the position on the curve .

Recall the equivalence \eqref{eq:jacobi-dexp} between Jacobi fields 
and : if  satisfy \eqref{sys:JI}
    (or \eqref{sys:JP}) with initial values  then  is
    equal to .
Therefore, we can compute the differential  with respect to  by
numerically integrating the system using a
basis  for the tangent space  at . With
initial conditions  instead, we can similarly compute the derivative
with respect to the initial point . Note 
that 
implies that , a fact that allows the
computation of  as well.

Assuming the manifold is sufficiently smooth, we can differentiate the systems 
\eqref{sys:JI} and \eqref{sys:JP} once more and thereby obtain second order
information that we will need later. The main difficulty is
performing the algebra of the already complicated expressions for the systems,
and, for the implicit case, we will need second
order derivatives of the generalized inverses .
For simplicity, we consider a families of geodesics with stationary initial
point. The derivations are
again postponed to Appendix~\ref{app:A}.
\begin{em}
  \vspace{0.5em}
  \\
    Let  be of class , and let  be a family of 
    geodesics. Assume  is a local parametrization
    containing , and let 
    be the curve in  representing , i.e.
    . Let  with 
    and . Define , and let
    .
    Then, in coordinates defined by ,  can be found as the 
    -part of the solution of the IVP
    
    with  the map given in explicit form in Appendix~\ref{app:A}.

    Now, let instead  be defined as a regular zero
    level set of a  map .
    Then  can be found as the -part of the solution of the IVP
    
    with  the map given in explicit form in Appendix~\ref{app:A}.
\end{em}\vspace{0.5em}\\
We note that solutions to \eqref{sys:JP2} and \eqref{sys:JI2} depend linearly on  even
though the systems themselves are not linear.

\subsection{Numerical Considerations}
The geodesic systems \eqref{eq:param-geo} and \eqref{sys:impl-geo} can in both 
the parametrized and implicit case be expressed
in Hamiltonian forms. In \cite{dedieu_symplectic_2005}, the authors use this
property along with symplectic numerical integrators to ensure 
the computed curves will be close to actual geodesics. This is possible since the Hamiltonian encodes
the Riemannian metric. The usefulness of pursuing a similar approach of 
expressing the differential systems in
Hamiltonian form and using symplectic integrators to preserve the Hamiltonians
is limited since there is no
direct interpretation of such Hamiltonians in contrast to the case for 
geodesic systems.

Along the same lines, we would like to use the preservation of quadratic
forms for symplectic integrators \cite{hairer_geometric_2002} to preserve
quadratic properties of the differential of the exponential map, e.g. the Gauss
Lemma \cite{do_carmo_riemannian_1992}. We are currently
investigating numerical schemes that could possibly ensure such stability.

\section{Exact Principal Geodesic Analysis}
\label{sec:pga}
As an example of how the IVPs describing differentials allow
optimizing over geodesics, we will provide algorithms that allow iterative
optimization of \eqref{eq:pga-cost} and that thus allow PGA as defined in
\cite{fletcher_principal_2004-1} to be computed without the traditional linear 
approximation.

Solving the optimization problem \eqref{eq:pga-cost} 
requires the ability to compute the projection .
We start with the gradient needed for iteratively computing the projection before deriving
the gradient of the cost function of 
\eqref{eq:pga-cost}. Computing these gradients will
require the differentials over geodesic families
derived in Section~\ref{sec:differentials}.
Thereafter, we present the actual algorithms for solving the problems before
discussing convergence issues.

The optimization problems \eqref{eq:pga-cost} and \eqref{eq:proj-cost} are posed
in the tangent space of the manifold at the sample mean and the unit sphere
of that tangent space, respectively. These domains have relatively simple
geometry, and, therefore, the complexity of the problems is contained in the
cost functions. Because of this, we will not need optimizing
algorithms that are specialized for domains with complicated geometry.
For simplicity, we compute
gradients and present steepest descent algorithms but it is straightforward to
compute Jacobians instead and use more advanced optimization algorithms such as
Gauss-Newton or Levenberg-Marquardt.

The overall approach is similar 
to the approach used for computing exact PGA in \cite{said_exact_2007}. Our solution differs in that we are
able to compute  and its differential without restricting to
the manifold SO(3) and in that we optimize the functional \eqref{eq:pga-cost} instead of
the cost function used in \cite{fletcher_statistics_2003} that involves one-dimensional 
subspaces.

\subsection{The Geodesic Subspace Projection}
We consider the projection  of a point  on a geodesic subspace . Assume  is centered at , 
let  be a -dimensional subspace of 
such that , and define a residual function  
by  that measures squared distances between  and points
in . Computing  by solving \eqref{eq:proj-cost} is then equivalent to
finding  minimizing . To find the gradient of , 
choose an orthonormal basis for  
and extend it to a basis for . Furthermore, let
 and choose an orthonormal basis for the tangent space . 
Karcher showed in \cite{karcher_riemannian_1977} that the gradient
 equals , and, using this, we get the gradient of the
residual function as

with  denoting the first  columns of
the matrix  expressed using the chosen bases.\footnote{In
coordinates of the bases, the differential  becomes a matrix
that we write .
The notation  denotes differentiation along the basis
elements of . See Appendix~\ref{app:notation} for additional notation.}
This matrix can be computed using the IVPs \eqref{sys:JI} or \eqref{sys:JP}.

\subsection{The Differential of the Subspace Projection}
\label{sec:grad-proj}
In order to optimize \eqref{eq:pga-cost}, we will need to compute gradients of
the form

with ,  and .\footnote{
Since  in \eqref{eq:pga-cost} is restricted to the unit sphere, we will not
need the gradient in the direction of , and, therefore, we find the gradient
in the subspace  instead of in the larger space
.
As noted in Section~\ref{sec:pga-def}, the optimization approach presented here can be extended to
include optimization of the base point  as well. Here, we use a fixed base
point that for PGA is an intrinsic mean of a data set.
}
This will involve the differential of  with respect to . 
Since  is defined as a minimizer of \eqref{eq:proj-cost}, 
its differential cannot be obtained just by applying the chain and product rules.
Instead, we use the implicit function theorem to define a map  that equals 
 around a neighborhood of  in . We then derive the
differential of .

For the result below, we extend the domain of residual function 
defined above from  to the entire tangent space .
We will a choose basis for , and we let 
denote the Hessian matrix of  with respect to the basis. Similarly,
we will choose a basis for , and
we let  denote the Hessian matrix of 
restricted to  with respect to this basis. Using this notation, we 
get the following result for the derivative of the projection :
\begin{proposition}
    Let  be an orthonormal basis for a subspace . 
    For each , let  be the subspace , and
    let  be the corresponding geodesic subspace. 
    Fix  and define  for an
    . Suppose the matrix  has full rank . Extend the
    orthonormal basis 
    for  to an orthonormal basis for 
    . Then
    
    The coordinates of the vector 
    in the basis for  are contained in the st column of 
    the matrix , the scalar 
    is the st coordinate of  in the basis, and
     is the matrix
    
    with  the last  columns of 
    the matrix
    
    and
    
    the identity matrix.
    \label{prop:grad-proj}
\end{proposition}
Before proving the result, we discuss its use for computing the gradient
\eqref{eq:grad-1}. 
The assumption that the Hessian of the restricted residual  
must have full rank is discussed below.

Because , 
we have

which, combined with \eqref{eq:grad-proj}, gives \eqref{eq:grad-1}.
In order to compute the right hand side of \eqref{eq:grad-proj}, it is necessary to
compute parts of the Hessian of the non-restricted residual .
For doing this, we will use the alternative formulation 
 for the residual function.
With  let .
Working in the chosen orthonormal basis, we have

and hence

Note that

Using that  for a time dependent,
invertible matrix \footnote{
See \cite[Eq. (2)]{decell_derivative_1974}.
} and the fact that
 for all , we get

The middle term of this product and the term 
 in \eqref{eq:hessian}
can be computed using the IVPs \eqref{sys:JP2},\eqref{sys:JI2} discussed in
Section~\ref{sec:differentials}.

\begin{proof}[Proposition~\ref{prop:grad-proj}]
Extend the basis
 for  to an orthonormal basis for 
. The argument is not dependent on this choice of basis, but it will 
make the reasoning and notation easier. Let  be an open neighborhood of  and
define the map  by

with the vectors  constituting an orthonormal basis for
 for each  and with  and  denoting the matrices having 
and  in the columns, respectively. 
Since  for all
 because  is a minimizer for  among
vectors in in , we see that 
 vanishes. Therefore, if  is 
non-singular, the implicit function theorem asserts the existence of a map  
from a neighborhood 
of  to  with the property that  for all  in
the neighborhood. We then compute

and hence

For the differentials on the right hand side of \eqref{eq:dpsi}, we have

 and

With the choice of basis, the above matrix is block triangular,

with  equal to .  The
requirement that  is non-singular is 
fulfilled, because  has rank  by assumption
and  has rank .

Since the first  rows of  are zero, we need only 
the last  columns of  in order to
compute \eqref{eq:dpsi}. The vector  as defined in
the statement of the theorem is equal to the st column. 
Let  be the matrix consisting of the remaining  
columns. Using the form \eqref{eq:blockt}, we have


Assume  is chosen such that 
equals the previously chosen basis for . With this assumption,
 is the identity matrix .
In addition, let  denote the st component of , 
that is, the projection of
 onto . Since  and
by choice of , \lemref{orth-deriv} (see Appendix~\ref{app:B}) gives

Therefore,

Note, in particular, that  is independent
on the actual choice of bases . Combining \eqref{eq:dpsi}, \eqref{eq:Fv},
and the fact that
 and  constitute the needed
columns of , we get 

Because , this provides \eqref{eq:grad-proj}.

\end{proof}

\subsection{Exact PGA Algorithm}
The gradients of the cost functions enable us to 
iteratively solve the optimization problems
\eqref{eq:pga-cost} and \eqref{eq:proj-cost}.
We let  be an intrinsic mean of a dataset , 
.
The algorithms listed below are essentially steepest descent methods but,
as previously noted, Jacobian-based optimization algorithms can be employed as well.

Algorithm~\ref{alg:proj} for computing  updates 
instead of the actual point  that we are interested in. The vector  is
related to  by .
\begin{algorithm}
\caption{Calculate }
\label{alg:proj}
\begin{algorithmic}
    \REQUIRE ,  geodesic subspace.
    \STATE 
    \COMMENT{initial guess} 
  \REPEAT 
  \STATE 
    \COMMENT{vector to point}
  \STATE  
    \COMMENT{gradient} 
  \STATE  
    \COMMENT{previous }
  \STATE  
    \COMMENT{update }
  \UNTIL{ is sufficiently small}.
\end{algorithmic}
\end{algorithm}

The algorithm for solving \eqref{eq:pga-cost}
is listed in Algorithm~\ref{alg:pga}. 
Since  in \eqref{eq:pga-cost} is required to be on the unit sphere, 
the optimization will take place on a manifold,
and a natural approach to compute iteration updates will use the exponential
map of the sphere. Yet, because of the symmetric geometry of the sphere, we 
approximate this using the simpler method
of adding the gradient to the previous guess and normalizing.
When computing the  principal direction, we 
choose the initial guess as the first regular PCA vector of the data projected
to  in . See Figure~\ref{fig:alg} for
an illustration of an iteration of the algorithm.
\begin{algorithm}
    \caption{Calculate the  principal direction of \eqref{eq:pga-cost}.}
\label{alg:pga}
\begin{algorithmic}
    \REQUIRE ,  orthogonal basis for
    .
  \STATE  first PCA vector of  projected first to
  \\\hspace{2.1em}using  and then to 
    \COMMENT{initial guess} 
  \REPEAT 
  \STATE 
    \COMMENT{for each  using \eqref{eq:full-expr}}
    \STATE  
    \COMMENT{gradient} 
  \STATE  
    \COMMENT{previous }
  \STATE  
    \COMMENT{update }
  \STATE  
    \COMMENT{normalize}
  \UNTIL{ is sufficiently small}.
\end{algorithmic}
\end{algorithm}
\begin{figure}[h]
    \begin{center}
      \includegraphics[width=0.5\columnwidth,trim=0 40 0 0,clip=true]{figures/alg-annotated3}
    \end{center}
    \caption{An iteration of Algorithm~\ref{alg:pga}. The figure shows data points  and
     (red points) with projections (blue points) to the geodesic subspace  (green
    line). The vector  defining  is updated to the new guess by adding the
  gradient  and normalizing.}
    \label{fig:alg}
\end{figure}

\subsection{Assumptions and Convergence}
As discussed in section \ref{sec:pga-def}, because a uniqueness and existence of
both the intrinsic mean and optima for \eqref{eq:pga-cost} may fail, the PGA
problem may not be well defined in itself. The uniqueness of the mean can be
obtained by assuming the data is sufficiently concentrated depending on the
curvature, see \cite{karcher_riemannian_1977}.

The curvature of the manifold may make the optimization problems non-convex,
and convergence to a global optimum is therefore only
ensured under the assumption that the problems
\eqref{eq:pga-cost} and \eqref{eq:proj-cost} are convex or that no local minima
exist.  Giving criteria for convexity or non-existence of local optima for
general manifolds and data sets is 
difficult because of the dependence on the global geometry of the manifolds.

The rank assumption on the Hessian used in Proposition~\ref{prop:grad-proj} is
equivalent to the residual  having only non-degenerate critical points when
restricted to . 
It is shown in \cite{karcher_riemannian_1977} that  is convex at points sufficiently
close to  and the assumption is therefore satisfied in such cases. In
particular, this is satisfied if Algorithm~\ref{alg:pga} is initialized with
subspaces that provide a good approximation to the data.


\section{Experiments}
\label{sec:experiments}
We will use the optimization strategy and the developed algorithm for exact PGA
to illustrate the
differences between exact and linearized PGA. Furthermore, we will estimate sectional
curvatures and compute injectivity radius bounds.
Even though the algorithms are not limited to low dimensional data, we aim at visualizing the
results and we will therefore provide examples with synthetic data on low
dimensional manifolds. The setup allows exploring the connection between
the geometry and curvature of the manifolds and the exact PGA result, and we
will show how the variance and residual formulation can provide fundamentally
different results.
For a comparison between the methods on high dimensional 
manifolds modeling real-life data, we refer the reader to
\cite{sommer_manifold_2010} where datasets of
human vertebrae X-rays and motion capture data are treated.

The PGA algorithm is implemented in Matlab using 
Runge-Kutta ODE solvers. For the logarithm map, we use the shooting algorithm developed
in \cite{sommer_bicycle_2009}. All tolerances used for the integration and
logarithm calculations are set at or lower than an order of magnitude of the 
precision used for the displayed results. Intrinsic means are computed by
iteratively minimizing variance using the gradient  (see
\cite{karcher_riemannian_1977}). The code used for the experiments
is available at \url{http://image.diku.dk/sommer}.

\ \\
We first consider surfaces embedded in  and defined by the equation

for different values of the scalar . For ,  is an ellipsoid
and it is equal to  in the case . The surface  is a cylinder and, for
,  is hyperboloid. Consider the point  and note that  for all . 
The curvature of  at  is equal to . Note in particular that for the
cylinder case the curvature is zero; the cylinder locally has the geometry
of the plane  even though it informally seems to curve.

We evenly distribute 20 points along two straight lines
through the origin of the tangent space , project the points 
from  to the surface , and perform linearized and exact PGA. 
Figure~\ref{fig:low} illustrates the
situation in  and on  embedded in , respectively.
The lines are chosen in order to ensure the points are spread over areas of the
surface with different geometry. This choice is made to illustrate the influence
of the curvature;
a more realistic example with points sampled from a Gaussian will be provided
below.
\begin{figure}[h]
    \begin{center}
      \subfigure[ with sampled points and first principal components
      (blue exact PGA, green linearized PGA).\label{fig:low1}]{\includegraphics[width=0.45\columnwidth,trim=30 20 10 10,clip=true]{figures/low1}}
      \subfigure[ with projected points and first principal components (blue
      exact PGA \eqref{eq:pga-cost}, green linearized PGA).\label{fig:low2}]{\includegraphics[width=0.5\columnwidth,trim=40 30 70 50,clip=true]{figures/low2}}
    \end{center}
    \caption{The tangent space  and the manifold  with sample
  points.}
    \label{fig:low}
\end{figure}

Since linearized PCA amounts to Euclidean PCA in , the first principal
direction found using linearized PGA divides the angle between the lines for all . 
In contrast to this, the variance and the
first principal direction found using exact PGA are dependent on .
Table~\ref{table:low1} shows the angle between the principal directions found
using the two methods, the variances and variance differences for different
values of . 
\begin{table}[ht]
    \scriptsize
    \setlength{\tabcolsep}{5pt}
\begin{center}
\begin{tabular}{lrrrrrrrrrrr}
  \hline
  \bf{c}: & 
  \multicolumn{1}{c}{\bf{1}} & 
  \multicolumn{1}{c}{\bf{0.5}} &
  \multicolumn{1}{c}{\bf{0}} &
  \multicolumn{1}{c}{\bf{-0.5}} &
  \multicolumn{1}{c}{\bf{-1}} &
  \multicolumn{1}{c}{\bf{-1.5}} &
  \multicolumn{1}{c}{\bf{-2}} & 
  \multicolumn{1}{c}{\bf{-3}} &
  \multicolumn{1}{c}{\bf{-4}} &
  \multicolumn{1}{c}{\bf{-5}}\\
  \hline
  angle :     & 0.0   & 0.1   & 0.0   & 22.3  & 29.2  & 31.5  & 32.6 & 33.8  & 34.2   & 34.5 \\
  linearized var.:         & 0.899 & 0.785 & 0.601 & 0.504 & 0.459 & 0.435 & 0.423 & 0.413 & 0.413 & 0.417 \\
  exact var.:           & 0.899 & 0.785 & 0.601 & 0.525 & 0.517 & 0.512 & 0.510 & 0.508 & 0.507 & 0.506 \\
  difference:           & 0.000 & 0.000 & 0.000 & 0.012 & 0.058 & 0.077 & 0.087 & 0.095 & 0.094 & 0.089 \\
  difference (\%):      & 0.0   & 0.0   & 0.0   & 4.2   & 12.5  & 17.6  & 20.6 & 23.0  & 22.7   & 21.4 \\
   \hline
\end{tabular}
\caption{Differences between methods for different values of .}
\label{table:low1}
\end{center}
\end{table}

Let us give a brief explanation of the result. The symmetry of the sphere and the
dataset cause the effect of curvature to even out in the spherical case .
The cylinder  has local geometry equal to  which causes the equality
between the methods in the  case. The hyperboloids with  that 
can be constructed by revolving a hyperbola around its semi-minor axis
are non-symmetric causing an increase in variance as the first 
principal direction approaches the
hyperbolic axis. The effect increases with the curvature causing 
the first principal direction to align with the hyperbolic axis for large negative 
values of . That the non-linearity is quite complex can be seen from the
decreasing differences for , a consequence of the increasing variance captured
using linearized PGA. This is caused by geodesics close to the semi-minor axis
being curved upwards towards the hyperbolic axis for large negative . This
results in increased captured variance that dominates the otherwise decreasing trend as  drops
below .
For all negative values of , exact PGA is able to capture 
more variance in the subspace spanned by the first principal direction than
linearized PGA.

Differences between the maximal variance PGA formulation
\eqref{eq:pga-cost} and the formulation that minimizes residual errors can be
exemplified on simple geometries when the spread of the data is large.
Similar examples for Geodesic PCA with variance formulation is reported \cite{huckemann_intrinsic_2010}.
In Figure~\ref{fig:sphere-orth}, points are sampled along a great circle through
the north pole on a sphere (). In order to illustrate the result of
maximizing projection variance, we start with the PGA center point fixed to
the north pole. In this case, each iteration of the optimization procedure
pushes the first principal component  \emph{away} from the direction of the great
circle. In fact, the optimal direction is \emph{orthogonal} to the direction of
the great circle. This very counter-intuitive effect is caused by the projection of the
points on the southern hemisphere moving closer to the south pole as the principal
subspaces moves away from the great circle thus causing the measured variance to
increase. In fact, the cost function \eqref{eq:pga-cost}
is non-differentiable at the optimal direction and the projections become
discontinuous functions of . If we instead choose the formulation that
minimizes residuals, the first principal component will align with the direction
of the great circle.
\begin{figure}[h]
    \begin{center}
      \subfigure[Top view of the sphere .\label{fig:sphere-orth1}]{\includegraphics[width=0.45\columnwidth,trim=30 10 10 00,clip=true]{figures/sphere-orth1}}
      \subfigure[Southern hemisphere of .\label{fig:sphere-orth2}]{\includegraphics[width=0.45\columnwidth,trim=30 10 10 00,clip=true]{figures/sphere-orth2}}
    \end{center}
    \caption{The sphere  with points (red) sampled along a great circle
    (black dotted circle) with tangent vector (red arrow). The optimization for the first principal 
    component  is stopped before it reaches its optimum (25
    iterations). The actual optimum (black arrow) is orthogonal to the
    great circle containing the data points. The variance is measured for the points (blue) projected to
    the current guess for the first principal subspace (black circle). 
    As the guess for  moves away from being tangent to the circle containing the data points,
    points on
    the southern hemisphere move southwards 
    causing the measured variance to increase.
}
    \label{fig:sphere-orth}
\end{figure}
To show that this effect persists under permutations of the data,
we sample points uniformly along a geodesic on an ellipsoid ()
adding Gaussian noise on the component orthogonal to the geodesic (std. dev.
). This time, we optimize for the mean. The ellipsoidal geometry forces the mean
to be close to the geodesic which is the reason for sampling on an ellipsoid; on a
sphere, the mean is unstable under permutations of the data when the data lies
close to a great circle. In
Figure~\ref{fig:ellipsoid-orth}, we show the first principal component as
computed with the variance and residual formulation, respectively.
As for the example on the sphere, the optimization
converges to a first principal component orthogonal to the geodesic with the
variance formulation. We again stop the optimization after a number of iterations
before it reaches its non-differentiable optimum.
With the residual formulation, the first principal
component aligns with the geodesic along which the points are sampled.
\begin{figure}[h]
    \begin{center}
      \subfigure[First principal component (black arrow), variance formulation.\label{fig:ellipsoid-orth1}]{\includegraphics[width=0.45\columnwidth,trim=100 70 100 50,clip=true]{figures/ellipsoid-var}}
      \subfigure[First principal component (black arrow), residual formulation.\label{fig:ellipsoid-orth2}]{\includegraphics[width=0.45\columnwidth,trim=100 70 100 50,clip=true]{figures/ellipsoid-res}}
    \end{center}
    \caption{An ellipsoid  with points (red) sampled uniformly along a geodesic
    (black dotted circle). Gaussian noise
    (std. dev. 0.1) is added to displace the points orthogonally to the
    geodesic.
    The black arrows show
    the result of the optimization with (a) the variance formulation and (b) the
    residual formulation. 
    With the variance formulation, the optimization is again
    stopped before it reaches its non-differential optimum orthogonal to
    the geodesic along which the points are sampled.
    The results show that the orthogonality of the first principal component
    observed in Figure~\ref{fig:sphere-orth} also occurs with perturbed data.}
    \label{fig:ellipsoid-orth}
\end{figure}
See also \cite{huckemann_intrinsic_2010} for futher
discussions on variance vs. residual formulations.

To investigate the difference between exact and linearized PGA with more than one principal direction, we consider
a four dimensional manifold embedded in  and defined by

We make the situation more realistic than in the previous experiment by 
sampling 32 random points in the tangent space , . Since  is
an affine subspace of  orthogonal to the  axis, we can identify it with
 by the map . We use this
identification when sampling by defining a normal distribution in ,
sampling the 32 points from the distribution, and mapping the results to .
The covariance is set to
 to get non-spherical distribution and to
increase the probability of data spreading over high-curvature parts of the
manifold. Table~\ref{table:low2} lists the variances and variance differences for 
the four principal directions for both methods along with angular differences. 
The lower variance for exact PGA compared to the linearized method for the
2nd principal direction is due to the greedy definition of PGA; when maximizing variance
for the 2nd principal direction, we keep the first principal direction
fixed. Hence we may get lower variance than what is obtainable if we were to maximize
for both principal directions together.
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrr}
  \hline
  \bf{Princ. comp.}: & \multicolumn{1}{c}{\bf{1}} & \multicolumn{1}{c}{\bf{2}} &
  \multicolumn{1}{c}{\bf{3}}& \multicolumn{1}{c}{\bf{4}} \\
  \hline
  angle :     & 10.1 & 10.6 & 12.0 & 12.2 \\
  linearized var.:         & 1.58 & 3.86 & 4.13 & 4.35 \\
  exact var.:           & 1.93 & 3.85 & 4.24 & 4.35 \\
  difference:           & 0.35 & -0.01 & 0.11 & 0.00 \\
  difference (\%):      & 21.9 & -0.3 & 2.6 & 0.0 \\
   \hline
\end{tabular}
\caption{Differences between the methods on . The variances 
of the data projected to the subspaces spanned by the first  principal 
directions and the percentage and angular differences are shown for .}
\label{table:low2}
\end{center}
\end{table}

We clearly see angular differences between the principal directions. In
addition, there is significant difference in accumulated variance in the 
first and third principal
direction. We note that the percentage difference is calculated from 
what corresponds to the accumulated eigenspectrum in PCA. The percentage difference of the increase between the
second and third principal direction, corresponding to the squared length of the third eigenvalue in
PCA, is greater.

\subsection{Curvature and Conjugate Points}
Again considering the surfaces ,
we can approximate the sectional curvature  of
 at  using \eqref{eq:sec-curve}.
The approximation is dependent on the value of the positive scalar  with 
increasing precision as  decreases to zero.
Table~\ref{table:Ks} shows the result of the sectional curvature approximation for 
two values of  compared to the real sectional curvature.
\begin{table}[ht]
    \scriptsize
    \setlength{\tabcolsep}{5pt}
\begin{center}
\begin{tabular}{lrrrrrr}
  \hline
  \bf{c}: & 
\multicolumn{1}{c}{\bf{1}} &
  \multicolumn{1}{c}{\bf{0}} &
  \multicolumn{1}{c}{\bf{-1}} & 
  \multicolumn{1}{c}{\bf{-2}} &
  \multicolumn{1}{c}{\bf{-3}}\\
  \hline
  :        
& 1   & 0  & -1  & -2  & -3  \\
   est., :  
& 1.000   & 0.000  & -1.000  & -2.000  & -3.000  \\
   est., :  
& 1.000   & 0.000  & -1.001  & -2.002  & -3.005  \\
   \hline
\end{tabular}
\caption{Sectional curvature at  for different values of .}
\label{table:Ks}
\end{center}
\end{table}

Now let  be the Jacobi field with  and  along the geodesic
. Figure~\ref{fig:conjugate} shows  for different values of .
We see that  for the spherical case  showing that  is a conjugate point and
hence giving the upper bound  on the injectivity radius.
The situation is illustrated in Figure~\ref{fig:sphere-jacobi}.
The local geometric equivalence between the cylinder  and  causes the
straight line for . 
For all , the injectivity radius of  is , but for , the point  
is not a conjugate point\footnote{
For ,  is a \emph{cut} point 
\cite[Chap.  13]{do_carmo_riemannian_1992}.
}. By looking at , we are only able to detect
conjugate points and hence, with this experiment, we only get the bound on the injectivity radius for
. For  the injectivity radius decreases below  as seen in the
case  with  for .
\begin{figure}[t]
    \begin{center}
      \includegraphics[width=0.50\columnwidth,trim=30 20 10 10,clip=true]{figures/conjugate}
    \end{center}
    \caption{ for  when , , and .}
    \label{fig:conjugate}
\end{figure}

\section{Conclusion and Outlook}
Optimization problems over geodesics can be solved by constructing 
IVPs for numerical computation of Jacobi fields and second order differentials.
We use this to develop an algorithm for numerically computing exact Principal 
Geodesic Analysis and thereby eliminating the need for the traditionally
used linear approximations. In addition, the numerically computed Jacobi fields
allow injectivity radii bounds and estimation of sectional curvatures partially
solving an open problem stated in \cite{huckemann_intrinsic_2010}.

We use the developed algorithm to explore examples of manifold valued datasets where 
the principal subspaces computed by exact PGA differs from linearized PGA, and we show 
how the differences 
depend on the curvature of the manifolds and which 
formulation of PGA is used. In addition, we 
approximate sectional curvatures and bound injectivity radii and evaluate the
computed results.

We are currently extending the methods to work for quotient manifolds
 and thereby allowing the similar computations to be performed on practically all
commonly occurring non-triangulated manifolds.
We expect this would allow Geodesic PCA to be computed on general 
quotient manifolds as well.
In addition, we are working on giving a theoretical treatment of the differences
between the variance and residual formulations of PGA. Finally, we
expect to use the automatic computation of sectional curvatures to investigate
further the effect of curvature on exact PGA and other 
statistical methods for manifold valued data.


\section*{Acknowledgements}
The authors would like to thank P. Thomas Fletcher for fruitful discussions on
the computation of exact PGA and Nicolas Courty for important remarks on
problems regarding data locality. 

\appendix

\section{Notation}
\label{app:notation}
In general, the paper follows the notation in \cite{do_carmo_riemannian_1992}.
Subscripts are used for curves on  dependent on a parameter,
e.g. the curve  is a map .
The subscript notation should 
not be confused with differentiation with respect to the parameter .
When a local parametrization  is
available, it is often used to represent a
curve  so that  is a curve in  satisfying .

The derivative  of the curve 
evaluated at  belongs to the tangent space .
The shorthand  will be used for such vectors, i.e.
. In addition, when differentiating curves with
respect to , we often use the shorthand . With these conventions,
, the initial velocity of the curve , will be written
.

Let  denote the differential of a map  and write  for
the differential evaluated at . When bases 
for  and  are specified, or when  and  are Euclidean spaces, 
 is used instead of .
For maps on product manifolds, e.g. , we will need to distinguish differentiation with respect
to one of the variables only. Letting one of the parameters have a fixed value
, the differential of the restricted function  from 
to 
evaluated at  is denoted . Similarly, if  is a submanifold of , the differential
of  will be denoted  and its evaluation at  will
be written .

When defined, the inverse of the exponential map  is the
logarithm map denoted . Subsets  of  with
 being a ball in  and with the radius  sufficiently small
are examples of neighborhoods of  in which 
 is defined. Whenever the -map is used, 
we will restrict to such neighborhoods without explicitly mentioning it.

When  is a real valued function,the
gradient of  with respect to the metric is denoted , i.e. 
 satisfies  for all . Whenever a basis of  is specified, or when  is Euclidean, we switch 
to the usual notation . Similarly, the Hessian of
 is defined by the relation  for all
vector fields  using the covariant derivative . Again, when a basis of  is specified, or when  is
Euclidean, the usual notation  will be used. 


\section{Expressions for the Derivative ODEs}
\label{app:A}
Because we will work with curves on manifolds that are either embedded in a
Euclidean space or where local parametrizations are available, we can perform
the derivations needed for the differential systems in Euclidean
spaces: the embedding space  for the implicit case, and the parameter
space  when a parametrization  is
available. The tensors we construct below will be tensors on the Euclidean
spaces  or
; they will be used as a compact notational representations,
and we do not attempt to give them intrinsic geometric
interpretations. The tensors will be embedding or coordinate \emph{dependent}; this is
by construction, and the tensors are thereby inherently different from
intrinsic and coordinate independent tensors such as the curvature endomorphism.

The notation will as far as possible follow the tensor notation used in
\cite{do_carmo_riemannian_1992}; however, we again note that
we use the embedding or parametrization to define the tensors 
on Euclidean domains. We will use the common identification 
between tensors and multilinear maps, i.e.
the tensor  defines a map multilinear map
 by
. We will not
distinguish between a tensor and its corresponding multilinear map, and hence,
in the above case, write  for both maps. 

For -dependent vector fields  and tensor field ,
we will use the equality

for the derivative with respect to .
If  is a composition of an -dependent tensor field  and an
-dependent curve , the derivative  equals the
covariant tensor derivative  \cite[Chap.  4]{do_carmo_riemannian_1992}.
Since we will only use tensors on Euclidean spaces, such tensor derivatives 
will consist of component-wise derivatives.

In the following, when a parametrization  is available, we let  be the -dependent -tensor on
 defined by

such that the th component of 
equals the right hand side of \eqref{eq:param-geo}. Note that  is symmetric
in the first two components since the Christoffel symbols are symmetric in 
and . Similarly, in the implicit case, we let
the -dependent -tensor  and -tensor  equal the right hand side of the  and
 parts of \eqref{sys:impl-geo}, respectively:


The derivation below of \eqref{sys:JI} concerns the implicit case.
\\\ \\
    To derive , we let  be a family of geodesics with
    , and define
     and . Assuming  and ,
    the Jacobi field  equals , 
    and, therefore, we can obtain  by differentiating the 
    geodesic system \eqref{sys:impl-geo}. Since  is embedded in , we
    consider all curves and vectors to be elements of .

    We use the map  of section~\ref{sec:geodesic-systems}
    to define the tensors
    
    Note, in particular, that .
    In addition, we will use the
    notation  for the right hand side of equation
    \eqref{eq:decell-expr}
    so that the derivative of a generalized inverse can be written
    .
    We claim that  equals the -part of the solution of
    \eqref{sys:JI} with
    
    Here  where  are the -parts of the solutions to \eqref{sys:impl-geo} 
    with initial conditions  and . To justify the claim, we differentiate the system \eqref{sys:impl-geo}. 
    Using \eqref{eq:t-deriv}, we get
    
    and
    
    Note that the tensor derivative 
    consists of derivatives of . Both the derivatives
     and 
     involve derivatives of generalized
    inverses. Therefore, we apply \eqref{eq:decell-expr} to differentiate 
    and get that
    
    The tensor derivative  consists of derivatives of .
    Similarly,
    
    By differentiating the initial conditions, we get
    \eqref{sys:JI} with , , and
     as defined in \eqref{eq:FI}.

As noted, we can obtain an IVP in the parametrized
case using a similar procedure. Let  be a geodesic in the  manifold . We assume 
 is a local parametrization containing , and
we let  be the curve in  representing , i.e. . 
Let  and , and let  be vectors in . We
associate  with  using .
The Jacobi field  along  with  and
 can then be found as the -part of the solution of the IVP
    
    with  the map constructed below.

    To derive , we let  be a family of geodesics with
    , and define
     and . Let  represent
     using .
    Again assuming  and ,
    we can obtain  by differentiating the 
    geodesic system \eqref{eq:param-geo}. Using \eqref{eq:t-deriv} and symmetry of
    , we have
    
    because  are solutions to \eqref{eq:param-geo} with initial conditions
     and . Therefore, setting 
    
    and 
    ,
    we get \eqref{sys:JP} with
    
    As noted above, the derivative  consists of 
    just the component-wise
    derivatives of , i.e. the derivatives of the Christoffel symbols.

\ \\
For deriving the second order differentials, we will 
need second order derivatives of generalized inverses.
Let  be an - and -dependent matrix of full rank.
From repeated application of the product rule and
\eqref{eq:decell-expr}, we see that 
when the - and -dependent matrices  and  are 
differentiable with respect to both variables
and the mixed partial derivative  exists then

where


We start the derivation with the parameterized
case. We will use the tensors introduced in the beginning of this section
and for the first order differentials.
\\\ \\
    We compute the  and 
    parts of  separately; denote them  and 
    , respectively. Let  be solutions to 
    \eqref{sys:JP} with IV's
     and along the curves  that represents the geodesics
    . In addition, let  and 
    denote  and , respectively. Let also  be
    solutions to  with IV's  along .
    Differentiating system \eqref{sys:JP}, we get
        
    and, using symmetry of the tensors,
    
    Therefore, letting  and , we get
     
    as the right hand side of \eqref{eq:GPq}
    and  equal to .
    The initial values are both  since  and
     equal  and , respectively, and, therefore, are not -dependent.
    
    For the implicit case, we will again compute the  and 
    parts of  separately. Let now  
    be solutions to \eqref{sys:JI} along the geodesics  and with IV's
    , and let  be
    solutions to  along  and with IV's .
    Let also  denote the -parts of the solutions to \eqref{sys:impl-geo} with
    initial conditions  and , and write , ,
    and . Recall that all curves and vectors are considered
    elements of the embedding space .

    Differentiating system \eqref{sys:JI}, we get
    
    Using the map  defined in \eqref{eq:second-deriv}, we have
    
    Combining the equations, we get
    
    Substituting  with  and  with , we get
     as the right hand side of the equation.
    Likewise,
    
    Again, after substituting  with  as above, we get  as the
    right hand side of the equation. As for the parametric case, both initial
    values are zero.
    

\section{The Projection Differential}
\label{app:B}
For the proof of Proposition~\ref{prop:grad-proj}, we will need the following result 
to show that equation \eqref{eq:grad-proj} is independent of the chosen basis.
\begin{lemma}
    Let  be an open subset of  and  a  map with the property that for any
    , the columns of the matrix  constitute an orthonormal basis
    for . Let  denote the th column of . Then for any
     and ,
    . As consequence of
    this, if  denotes the map 
    then
    
    in the basis  for
    .
    
    \label{lem:orth-deriv}
\end{lemma}



\bibliographystyle{myamsplain}
\bibliography{bibliography.bib}

\end{document}
