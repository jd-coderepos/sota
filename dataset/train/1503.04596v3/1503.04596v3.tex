\documentclass[conference]{IEEEtran}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
\fi


\usepackage[cmex10]{amsmath}
\usepackage{amssymb}

\usepackage[lined,algonl,boxed]{algorithm2e}



\usepackage{stfloats}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{citesort}

\begin{document}
\title{Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network}


\author{\IEEEauthorblockN{Mark D. McDonnell and Tony Vladusich}
\IEEEauthorblockA{Computational and Theoretical Neuroscience Laboratory, Institute for Telecommunications Research,\\School of Information Technology and Mathematical Sciences,\\
University of South Australia\\
Mawson Lakes, SA 5095, Australia\\
Email: mark.mcdonnell@unisa.edu.au}
}




\maketitle


\begin{abstract}
We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few  tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units.  We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37\% error) and NORB-small (2.2\% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4\% error) database is  also competitive with state-of-the art methods.

\end{abstract}

\IEEEpeerreviewmaketitle


\section{Introduction}

State-of-the-art performance on many image classification databases has been achieved recently using multilayered (i.e., {\em deep}) neural networks~\cite{Schmidhuber.15}.
Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale~\cite{LeCun.98,Coates.11,Coates.11a,Le.10}. Training of deep networks, however, often requires significant resources, in terms of time, memory and computing power (e.g. in the order of hours on GPU clusters). Tasks that require online learning, or periodic replacement of all network weights based on fresh data may thus not be able to benefit from deep learning techniques. It is desirable, therefore, to seek very rapid training methods, even if this is potentially at the expense of a small performance decrease. 

Recent work has shown that good performance on image classification tasks can be achieved in `shallow' convolutional networks---neural architectures containing a single training layer---provided sufficiently many features are extracted~\cite{Coates.11}. Perhaps surprisingly, such performance arises even with the use of entirely random convolutional filters or filters based on randomly selected patches from training images~\cite{Coates.11a}. Although application of a relatively large numbers of filters is common (followed by spatial image smoothing and downsampling), good classification performance can also be obtained with a sparse feature representation (i.e. relatively few filters and minimal downsampling)~\cite{Le.10}.

Based on these insights and the goal of devising a fast training method, we introduce a method for combining several existing general techniques into what is equivalent to a five layer neural network (see Figure~\ref{fig_sim}) with only a single trained layer (the output layer), and show that the method:
\begin{enumerate}
\item produces state-of-the-art results on  well known image classification databases;
\item is trainable in times in the order of minutes (up to several hours for large training sets) on standard desktop/laptop  computers;
\item is sufficiently versatile that the same hyper-parameter sets can be applied to different datasets and still produce results comparable to dataset-specific optimisation of hyper-parameters.
\end{enumerate}

The fast training method  we use has been developed independently several times~\cite{Schmidt.92,Chen.96,Eliasmith,Huang.04} and has gained increasing recognition in recent years---see~\cite{Eliasmith.12,Stewart.14a,Huang.12,Huang.14} for recent reviews of the different contexts and applications. The network architecture  in the classification stage is that of a three layer neural network comprised from an input layer, a hidden layer of nonlinear units, and a linear output layer. The input weights are randomly chosen and untrained, and the output weights are trained in a single batch using least squares regression. Due to the convexity of the objective function, this method ensures the output weights are optimally chosen for a given set of random input weights. The rapid speed of training is due to the fact that the least squares optimisation problem an be solved using an O() algorithm, where  is the number of hidden units and  the number of training points~\cite{McDonnell.15PLOS}. 

When applied  to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases~\cite{vanSchaik.14,Tapson.14,Yu.12,Zhu.14,McDonnell.15PLOS,Zhu.15} but poor performance on more difficult ones.  To our knowledge, however, the method has not yet been applied to  convolutional features.

Therefore, we  have devised a network architecture (see Figure~\ref{fig_sim}) that consists of three key elements that work together to ensure fast learning and good classification performance: namely, the use of (a) convolutional feature extraction, (b) random-valued input weights for classification, (c) least squares training of output weights that feed in to (d) linear output units. We apply our network to several  image classification databases, including MNIST~\cite{MNIST}, CIFAR-10~\cite{Krizhevsky}, Google Street View House Numbers (SVHN)~\cite{SVHN} and NORB~\cite{LeCun.04}. The network produces state-of-the-art classification results on MNIST and NORB-small databases and near state-of-the-art performance on SVHN. 

These promising results are presented in this paper to demonstrate the potential benefits of the method; clearly further innovations within the method are required if it is to be competitive on harder datasets like CIFAR-10, or Imagenet. We expect that the most likely avenues for improving our presented results for CIFAR-10, whilst retaining the method's core attributes, are (1) to introduce limited training of the Stage 1 filters by generalizing the method of~\cite{Yu.12}; (2)  introduction of training data augmentation. We aim to pursuing these directions in our future work.

The remainder of the paper is organized as follows. Section~\ref{S:2} contains a generic description of the network architecture and the algorithms we use for obtaining convolutional features and classifying inputs based on them. Section~\ref{S:3} describes how the generic architecture and training algorithms are {\em specifically} applied to four well-known benchmark image classification datasets. Next, Section~\ref{S:4} describes the results we obtained for these datasets, and finally the paper concludes with discussion and remarks in Section~\ref{S:5}.







\section{Network architecture and training algorithms}\label{S:2}

\begin{figure*}[!ht]
\centering
\includegraphics[width=2\columnwidth]{IJCNN2015_network_v4}
 \caption{Overall network architecture. In total there are three hidden layers, plus an input layer and a linear output layer. There are two main stages: a convolutional filtering and pooling stage, and a classification stage. Only the final layer of weights,  is learnt, and this is achieved in a single batch using least squares regression. Of the remaining weights matrices,  is specified and remains fixed, e.g. taken from Overfeat~\cite{Sermanet.14_overfeat};  describes standard average pooling and downsampling; and  is set randomly or by using the method of~\cite{Zhu.15} that specifies the weights by sampling examples of the training distribution, as described in the text. Other variables shown are as follows:  is the number of pixels in an image,  is the number of features extracted per image,  is a downsampling factor,   is the number of hidden units in the classifier stage and  is the number of classes. }
\label{fig_sim}
\end{figure*}

The overall network is shown in Figure~\ref{fig_sim}. There are three hidden layers  with nonlinear units, and four layers of weights. The first layer of weights is the convolutional filter layer. The second layer is a pooling (low pass filtering) and downsampling layer. The third layer is a random projection layer. The fourth layer is the only trained layer. The output layer has linear units. 



The network can be conceptually divided into two stages and two algorithms, that to our knowledge have not previously been combined. The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification~\cite{Coates.11,Coates.11a,Le.10,Sermanet.12}. The second stage is the classifier stage, and largely follows the approach of~\cite{Zhu.15,McDonnell.15PLOS}.  We now describe the two stages in detail.







\subsection{Stage 1 Architecture: Convolutional filtering and pooling}\label{S:Stage1}


The algorithm we apply to extract features from images (including those with multiple channels) is summarised in {\bf Algorithm~\ref{algorithm1}}. Note that the details of the filters  and  described in  {\bf Algorithm~\ref{algorithm1}} are given in Section~\ref{S:filters}, but here we introduce the size of these two-dimensional filters as  and . The functions  and  are nonlinear transformations applied termwise to matrix inputs to produce matrix outputs of the same size. The symbol * represents two-dimensional convolution.


\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{Convolutional feature detection.}
\Input{Set of  images, , each with  channels}
\Output{Feature vectors, , }
\BlankLine
\ForEach{}{
split  into channels, , \\
\ForEach{ {\rm filters}}{
Apply filter to each channel: \\
Apply termwise nonlinearity: \\
Apply lowpass filter: \\
Apply termwise nonlinearity: \\
Downsample: \\Concatenate  channels:  \\
Normalize: 
}
Concatenate over filters:  \\
}
\label{algorithm1}
\end{algorithm}


This sequence of steps in  {\bf Algorithm~\ref{algorithm1}} suggest looping over all images and channels sequentially. However, the following mathematical formulation of the  algorithm indicates a standard layered neural network formulation of this algorithm is applicable, as shown in Figure~\ref{fig_sim}, and therefore that computation of all features ( ) can be obtained in one shot from a -column matrix containing a batch of  training points.

The key to this formulation is to note  that since convolution is a linear operator, a matrix can be constructed that when multiplied by a data matrix produces the same result as convolution applied to one instance of the data. Hence, for a total of  features per image, we introduce the following matrices. 

Let 
\begin{itemize}
\item  be a feature matrix of size ;
\item  be a data matrix with  columns;
\item  be a concatenation of the  convolution matrices corresponding to ;
\item  be a convolution matrix corresponding to , that also down samples by a factor of ;
\item  be a block diagonal matrix containing  copies of  on the diagonals.
\end{itemize}
The entire flow described in Algorithm~\ref{algorithm1} can  be written mathematically as

where   and  are applied term by term to all elements of their arguments. The matrices   and  are sparse Toeplitz matrices. In practice we would not form them directly, but instead form one pooling matrix, and one filtering matrix for each filter, and sequential apply each filter to the entire data matrix, .

We use a particular  form for the nonlinear hidden-unit functions  and   inspired by {\em LP-pooling}~\cite{Sermanet.12}, which is of the form  and
. For example, with  we have


An intuitive explanation for the use of LP-pooling is as follows. First, note that each hidden unit receives as input a linear combination of a patch of the input data, i.e.  in  has the form . Hence, squaring  results in a sum that contains  terms proportional to  and terms proportional to products of each . Thus, squaring is a simple way to produce hidden layer responses that depend on the product of pairs of input data elements, i.e. {\em interaction terms}, and this is important for discriminability.  Second, the square root transforms the distribution of the hidden-unit responses; we have observed that in practice, the result of the square root operation is often a distribution that is closer to Gaussian than without it, which helps to regularise the least squares regression method of training the output weights.

However, as will be described shortly, the classifier of Stage 2 also has a square nonlinearity. Using this nonlinearity, we have found that classification performance is generally optimised by taking the square root of the input to the random projection layer.  Based on this observation, we do not strictly use LP-pooling, and instead set 

and

This effectively combines the implementation of L2-pooling, and the subsequent square root operation.


\subsection{Stage 2 Architecture: Classifier}

The following descriptions are applicable whether or not  raw pixels are treated as features or the input is the features extracted in stage 1.
First, we introduce notation. Let:
\begin{itemize}
\item , of size , contain each length  feature vector;
\item  be an indicator matrix of size , which numerically represents the labels of each training vector, where there are  classes---we set each column to have a  in a single row, corresponding to the label class for each training vector, and all other entries to be zero;
\item , of size  be the real-valued input weights matrix for the classifier stage;
\item , of size  be the real-valued output weights matrix for the classifier stage;
\item the function  be the activation function of each hidden-unit; for example,  may be the logistic sigmoid, , or a squarer, ;
\item , of size , contain the hidden-unit activations that occur due to each feature vector;  is applied termwise to each element in the matrix  .
\end{itemize}

\subsection{Stage 1 Training: Filters and Pooling}

In this paper we do not employ any form of training for the filters and pooling matrices. The details of the filter weights and form of pooling used for the example classification problems presented in this paper are given Section~\ref{S:3}.

\subsection{Stage 2 Training: Classifier Weights}

The  training approach for the classifier is that described by  e.g.~\cite{Schmidt.92,Chen.96,Eliasmith,Huang.14}. The default situation for these methods is that the input weights, , are generated randomly from a specific distribution, e.g. standard Gaussian, uniform, or bipolar. However, it is known that setting these weights non-randomly based on the training data leads to superior performance~\cite{McDonnell.15PLOS,Tapson.14,Zhu.15}. In this paper, we use the method of~\cite{Zhu.15}.  The input weights can also be trained iteratively, if desired, using single-batch  backpropagation~\cite{Yu.12}.

Given a choice of , the output weights matrix is determined according to

where  is the size  Moore-Penrose pseudo inverse corresponding to . This solution is equivalent to least squares regression applied to an overcomplete set of linear equations, with an -dimensional target.  It is known to often be useful to regularise such problems, and instead solve the following {\em ridge regression} problem~\cite{Huang.12,Huang.14}:

where  is a hyper-parameter and  is the  identity matrix. In practice, it is efficient to avoid explicit calculation of the inverse in Equation~(\ref{Q})~\cite{McDonnell.15PLOS} and instead use QR factorisation to solve the following set of  linear equations for the  unknown variables in :

Above we mentioned two algorithms, and {\bf Algorithm 2} is simply to form  and solve Eqn.~(\ref{Final}), followed by optimisation of  using ridge regression. For large  and  (which is typically valid) the runtime bottleneck for this method is typically the O matrix multiplication required to obtain the Gram matrix, .

\subsection{Application to Test Data}

For a total of  test images contained in a matrix , we first obtain a matrix  , of size , by following {\bf Algorithm~\ref{algorithm1}}. The output of the classifier is then the  matrix

Note that we can write the response to all test images in terms of the training data:

Thus, since the pseudo-inverse, , can be obtained from Equation~(\ref{Q}), Equations~(\ref{Y_test1a}),~(\ref{Y_test1b}) and~(\ref{Y_test1c}) constitute a closed-form solution for the entire test-data classification output, given specified matrices, ,  and , and hidden-unit activation functions, , and .

The final classification decision for each image is obtained by taking the index of the maximum value of each column of .



\section{Image Classification Experiments: Specific Design}\label{S:3}

We  examined the method's performance  when used as a classifier of images. Table~\ref{Table1} lists the attributes of  four well known databases we used. For the two databases comprised from RGB images, we used  channels, namely the  raw RGB channels, and a conversion to greyscale. This approach was shown to be effective for SVHN in~\cite{Sermanet}. 

\begin{table}[!ht]
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Database & Classes & Training & Test  & Channels & Pixels\\
\hline
MNIST~\cite{MNIST} & 10 & 60000 & 10000 & 1 & 2828\\
NORB-small~\cite{LeCun.04}\ & 5 & 24300 & 24300 & 2 (stereo) & 3232\\
SVHN~\cite{SVHN}  & 10 & 604308 & 26032 &  3 (RGB) & 3232\\
CIFAR-10~\cite{Krizhevsky} & 10 & 50000 & 10000 & 3 (RGB) & 3232\\
\hline
\end{tabular}
~\\
\caption{\bf{Image Databases. Note that the NORB-small database consists of images of size  pixels, but we first downsampled all training and test images to  pixels, as in~\cite{Le.10}.}}\label{Table1}
}
\end{table}

\subsection{Preprocessing}

All raw image pixel values were scaled to the interval . Due to the use of quadratic nonlinearities and LP-pooling, this scaling does not affect performance. The only other preprocessing done was as follows:
\begin{enumerate}
\item MNIST: None;
\item NORB-small: downsample from 9696 to 3232, for implementation efficiency reasons (this is consistent with some previous work on NORB-small, e.g.~\cite{Le.10});
\item SVHN: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB. We found that local and/or global contrast enhancement only diminished performance;
\item CIFAR-10: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB; apply ZCA whitening to each channel of each image, as in~\cite{Coates.11}.
\end{enumerate}


\subsection{Stage 1 Design: Filters and Pooling}\label{S:s1design}\label{S:filters}

Since our objective here was to train only a single layer of the network, we did not seek to train the network to find filters optimised for the training set. Instead, for the size  two-dimension filters, , we considered the following options:
\begin{enumerate}
\item simple rotated bar and corner filters, and square uniform centre-surround filters;
\item filters trained on Imagenet and made available in Overfeat~\cite{Sermanet.14_overfeat}; we used only the 96 stage-1 `accurate' 77 filters;
\item patches obtained  from the central  region of randomly selected training images, with  training images from each class.
\end{enumerate}
The  filters from Overfeat\footnote{Available from http://cilvr.nyu.edu/doku.php?id=software:overfeat:start} are RGB filters. Hence, for the databases with RGB images, we applied each channel of the filter to the corresponding channel of each image. When applied to greyscale channels, we converted the Overfeat filter to greyscale. For NORB, we applied the same filter to both stereo channels. For all filters, we subtract the mean value over all  dimensions in each channel, in order to ensure a mean of zero in each channel.

In implementing the two-dimensional convolution operation required for filtering the raw images using , we obtained only the central `valid' region, i.e.~for images of size , the total dimension of the valid region is . Consequently, the total number of features per image obtained prior to pooling, from  filters, and images with  channels is .


In previous work, e.g.~\cite{Sermanet.12}, the form of the  two-dimension filter,  is a normalised Gaussian. Instead, we used a simple summing filter, equivalent to a kernel with all entries equal to the same value, i.e.

In implementing the two-dimensional convolution operation required for filtering using , we obtained the `full' convolutional region, which for images of size  is , given the `valid' convolution first applied using , as described above.

The remaining part of the pooling step is to downsample each image dimension by a factor of , resulting in a total of  features per image. In choosing , we experimented with a variety of scales before settling on the value  shown in Table~\ref{Table2}. We note there exists an interesting tradeoff between the number of filters , and the downsampling factor, . For example, in~\cite{Coates.11}, , whereas in~\cite{Le.10} .  We found that, up to a point, smaller  enables a smaller number of filters, , for comparable performance.

The hyper-parameters we used for each dataset are shown in Table~\ref{Table2}.

\begin{table}[!ht]
{\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Hyper-parameter & MNIST & NORB & SVHN & CIFAR-10\\
\hline
Filter size,  & 7 & 7 & 7 & 7\\
Pooling size,  & 8 & 10 & 7 & 7\\
Downsample factor,   & 2 & 2 & 5 & 3\\
\hline
\end{tabular}
~\\
\caption{\bf{Stage 1 Hyper-parameters (Convolutional Feature Extraction).  }}\label{Table2}
}
\end{table}


\subsection{Stage 2 Design: Classifier projection weights}

To construct the  matrix  we use the method proposed by~\cite{Zhu.14}. In this method, each row of the matrix  is chosen to be a normalized difference between the data vectors corresponding to randomly chosen examples from distinct classes of the training set. This method has previously been shown to be superior to setting the weights to values chosen from random distributions~\cite{Zhu.14,McDonnell.15PLOS}.

For the nonlinearity in the classifier stage hidden units, , the typical choice in other work~\cite{Huang.14}  is  a sigmoid. However, we found it sufficient (and much faster in an implementation) to use the quadratic nonlinearity. This suggests that good image classification is strongly dependent on  the presence of interaction terms---see the discussion about this in Section~\ref{S:Stage1}.

\subsection{Stage 2 Design: Ridge Regression parameter}

With these choices, there remains only two hyper-parameters for the Classifier stage: the regression parameter, , and the number of hidden-units, . In our experiments, we examined classification error rates as a function of varying . For each , we can optimize  using cross-validation. However, we also found that a good generic heuristic for setting  was
 
and this reduces the number of hyper-parameters for the classification stage to just one: the number of hidden-units, .

\subsection{Stage 1 and 2 Design: Nonlinearities}

For the hidden-layer nonlinearities, to reiterate, we use:




\section{Results}\label{S:4}

We examined the performance of the network on classifying the test images in the four chosen databases, as a function of the number of filters, , the downsampling rate , and the number of hidden units in the classifier stage, . We use the maximum number of channels, , available in each dataset (recall from above that we convert RGB images to greyscale, as a fourth channel). 

We considered the three kinds of untuned filters described in Section~\ref{S:s1design}, as well as combinations of them. We did not exhaustively consider all options, but settled on the Overfeat filters as being marginally superior for NORB, SVHN and CIFAR-10 (in the order of 1\% in comparison with other options), while hand-designed filters were superior for MNIST, but only marginally compared to randomly selected patches from the training data. There is clearly more that can be investigated to determine whether hand-designed filters can match trained filters when using the method of this paper.

\subsection{Summary of best performance attained}

The best performance we achieved is summarised in Table~\ref{Table4}. 

\begin{table}[!ht]
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Database  &  &  & & Our best & State-of-the-art\\
\hline
MNIST & 1& 12000  & 60 & 0.37\% & 0.39\%~\cite{Mairal.14,Lee.14}\\
NORB-small & 2&  3200 &  & 2.21\% & 2.53\%~\cite{Ciresan.11}\\
SVHN  &  4& 40000 &  & 3.96\%& 1.92\%~\cite{Lee.14}\\
CIFAR-10 &   4& 40000 &  & 24.14\% & 9.78\%~\cite{Lee.14}\\ 
\hline
\end{tabular}
~\\
\caption{\bf{Results for various databases. The state-of-the-art result listed for MNIST and CIFAR-10 can be improved by augmenting the training set with distortions and other methods~\cite{Simard.03,Ciresan.10,Ciresan.12}; we have not done so here, and report state-of-the-art only for methods not doing so.}}\label{Table4}
}
\end{table}

\subsection{Trend with increasing }

We now use MNIST as an example to indicate how classification performance scales with the number of hidden units in the classifier stage, . The remain parameters were ,  and , which included hand-designed filters comprised from 20 rotated bars (width of one pixel), 20 rotated corners (dimension 4 pixels) and 3 centred squares (dimensions 3, 4 and 5 pixels), all with  zero mean. The rotations were of binary filters and used standard pixel value interpolation. Figure~\ref{fig_MNIST} shows a power law-like decrease in error rate as  increases, with a linear trend on the log-log axes. The best error rate shown on this figure is 0.40\%. As shown in Table~\ref{Table4}, we have attained a best repeatable rate of \% using 60 filters and . When we combined Overfeat filters with hand-designed filters and randomly selected patches from the training data, we obtained up to \% error on MNIST, but this was an outlier since it was not repeatedly obtained by different samples of .


\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\columnwidth]{IJCNN_MNIST}
 \caption{Example set of error percentage value on the 10000 MNIST test images, for ten repetitions of the selection . The best result shown is 40 errors out of 10000. Increasing  above 6400 saturates in performance.}
\label{fig_MNIST}
\end{figure}

\subsection{Indicative training times}

For an implementation in Matlab on a PC with 4 cores and 32 GB of RAM, for MNIST (60000 training points) the total time required to generate all features for all 60000 training images from one filter is approximately 2 seconds. The largest number of filters we used to date was 384 (96 RGB+greyscale), and when applied to SVHN (600000 training points), the total run time for feature extraction is then about two hours (in this case we used batches of size 100000 images). 

The runtime we achieve for feature generation benefits from carrying out convolutions using matrix multiplication applied to large batches simultaneously; if instead we iterate over all training images individually, but still carry out convolutions using matrix multiplication, the time for generating features approximately doubles.   Note also that we employ Matlab's sparse matrix data structure functionality to represent  and , which also provides a speed boost when multiplying these matrices to carry out the convolutions.  If we do not use the matrix-multiplication method for convolution, and instead apply two-dimensional convolutions to each individual image, the feature generation is slowed even more.

For the classifier stage, on MNIST with , the runtime is approximately 150 seconds for  (there is a small time penalty for smaller , due to the larger dimension of the input to the classifier stage). Hence, the total run time for MNIST with 40 filters and  is in the order of 4 minutes to achieve a correct classification rate above  99.5\%. With fewer filters and smaller , it is simple to achieve over 99.2\% in a minute or less.

For SVHN and CIFAR-10 where we scaled up to , the run time bottleneck is the classifier, due to the  runtime complexity. We found it necessary to use a PC with more RAM (peak usage was approximately 70 GB) for . In the case of , the network was trained in under an hour on CIFAR-10, while SVHN took about 8-9 hours.  Results within a few percent of our best, however, can be obtained in far less time.


\section{Discussion and Conclusions}\label{S:5}

As stated in the introduction, the purpose of this paper is to highlight the potential benefits of the method presented, namely that it can attain excellent results with a rapid training speed  and low implementation complexity, whilst only suffering from reduced performance relative to state-of-the-art on particularly hard problems.

In terms of efficacy on classification tasks, as shown in Table~\ref{Table4}, our best result (0.37\% error rate) surpasses the best ever reported performance for classification of the MNIST test set when no  augmentation of the training set is done. We have also achieved, to our knowledge, the best performance reported in the literature for the NORB-small database, surpassing the previous best~\cite{Ciresan.11} by about 0.3\%.

For SVHN, our best result is within \% of state-of-the-art. It is highly likely that using  filters trained on the SVHN database rather than on Imagenet would reduce this gap, given the structured nature of digits, as opposed to the more complex nature of Imagenet images. Another avenue for closing the gap on state-of-the-art using the same filters would be to increase  and decrease , thus resulting in more features and more classifier hidden units. Although we increased  to 40000, we did not observe saturation in the error rate as we increased  to this point.

For CIFAR-10, it is less clear what is lacking in our method in comparison with the gap of about 14\% to state-of-the-art methods. We note that CIFAR-10 has relatively few training points, and we observed that the gap between classification performance on the actual training set, in comparison with the test set, can be up to 20\%. This suggests that designing enhanced methods of regularisation (e.g. methods similar to dropout in the convolutional stage, or data augmentation) are necessary to ensure our method can achieve good performance on  CIFAR-10.  Another possibility is to use a nonlinearity in the classifier stage that ensures the hidden-layer responses reflect higher order correlations than possible from the squaring function we used. However, we expect that training the convolutional filters in Stage 1 so that they extract features that are more discriminative for the specific dataset will be the most likely enhancement for improving results on CIFAR-10.

Finally, we note that there exist iterative approaches for training the classifier component of Stage 2 using least squares regression, and without training the input weights---see, e.g.,~\cite{Tapson.13,Widrow.13,McDonnell.15PLOS}. These methods can be easily adapted for use with the convolutional front-end, if, for example,  additional batches of training data become available, or if the problem involves online learning. 

In closing, following acceptance of this paper, we became aware of a newly published paper that combines convolutional feature extraction with least squares regression training of classifier weights to obtain good results for the NORB dataset~\cite{Huang.15}. The three main differences between the method of the current paper and the method of~\cite{Huang.15} are as follows. First, we used a hidden layer in our classifier stage, whereas~\cite{Huang.15} solves for output weights using least squares regression  applied to  the output of the pooling stage. Second, we used a variety of methods for the convolutional filter weights, whereas~\cite{Huang.15} uses orthogonalised random weights only.  Third, we downsample following pooling, whereas~\cite{Huang.15} does not do so.  


\section*{Acknowledgment}


Mark D. McDonnell's contribution was by supported by an Australian Research Fellowship from the Australian Research Council (project number DP1093425).  We gratefully acknowledge Prof David Kearney and Dr Victor  Stamatescu from University of South Australia and Dr Sebastien Wong of DSTO, Australia, for useful discussions and provision of computing resources. We also acknowledge discussions with Prof Philip De Chazal of University of Sydney. 



\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Schmidhuber.15}
J.~Schmidhuber, ``Deep learning in neural networks: {A}n overview,''
  \emph{Neural Networks}, vol.~61, pp. 85--117, 2015.

\bibitem{LeCun.98}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' \emph{Proceedings of the IEEE}, vol.~86,
  pp. 2278--2324, 1998.

\bibitem{Coates.11}
A.~Coates, H.~Lee, and A.~Y. Ng, ``An analysis of single-layer networks in
  unsupervised feature learning,'' in \emph{Proc.14th International Conference
  on Artificial Intelligence and Statistics (AISTATS), 2011, Fort Lauderdale,
  FL, USA. Volume 15 of JMLR:W\&CP 15}, 2011.

\bibitem{Coates.11a}
A.~Coates and A.~Y. Ng, ``The importance of encoding versus training with
  sparse coding and vector quantization,'' in \emph{Proceedings of the 28th
  International Conference on Machine Learning (ICML), Bellevue, WA, USA},
  2011.

\bibitem{Le.10}
Q.~V. Le, J.~Ngiam, Z.~Chen, D.~Chia, P.~W. Koh, and A.~Y. Ng, ``Tiled
  convolutional neural networks,'' in \emph{Advances in Neural Information
  Processing Systems 23}, J.~Lafferty, C.~Williams, J.~Shawe-Taylor, R.~Zemel,
  and A.~Culotta, Eds., 2010, pp. 1279--1287.

\bibitem{Schmidt.92}
P.~F. Schmidt, M.~A. Kraaijveld, and R.~P.~W. Duin, ``Feed forward neural
  networks with random weights,'' in \emph{Proc. 11th IAPR Int. Conf. on
  Pattern Recognition, Volume II, Conf. B: Pattern Recognition Methodology and
  Systems (ICPR11, The Hague, Aug.30 - Sep.3), IEEE Computer Society Press, Los
  Alamitos, CA, 1992, 1-4}, 1992.

\bibitem{Chen.96}
C.~L.~P. Chen, ``A rapid supervised learning neural network for function
  interpolation and approximation,'' \emph{IEEE Transactions on Neural
  Networks}, vol.~7, pp. 1220--1230, 1996.

\bibitem{Eliasmith}
C.~Eliasmith and C.~H. Anderson, \emph{Neural Engineering: Computation,
  Representation, and Dynamics in Neurobiological Systems}.\hskip 1em plus
  0.5em minus 0.4em\relax MIT Press, Cambridge, MA, 2003.

\bibitem{Huang.04}
G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, ``Extreme learning machine: {A} new
  learning scheme of feedforward neural networks,'' in \emph{In Proc.
  International Joint Conference on Neural Networks (IJCNN'2004), (Budapest,
  Hungary), July 25-29}, 2004.

\bibitem{Eliasmith.12}
C.~Eliasmith, T.~C. Stewart, X.~Choo, T.~Bekolay, T.~{DeWolf}, C.~Tang, and
  D.~Rasmussen, ``A large-scale model of the functioning brain,''
  \emph{Science}, vol. 338, pp. 1202--1205, 2012.

\bibitem{Stewart.14a}
T.~C. Stewart and C.~Eliasmith, ``Large-scale synthesis of functional spiking
  neural circuits,'' \emph{Proceedings of the IEEE}, vol. 102, pp. 881--898,
  2014.

\bibitem{Huang.12}
G.-B. Huang, H.~Zhou, X.~Ding, and R.~Zhang, ``Extreme learning machine for
  regression and multiclass classification,'' \emph{IEEE Transactions on
  Systems, Man, and Cybernetics---Part B: Cybernetics}, vol.~42, pp. 513--529,
  2012.

\bibitem{Huang.14}
G.-B. Huang, ``An insight into extreme learning machines: {R}andom neurons,
  random features and kernels,'' \emph{Cognitive Computation}, vol.~6, pp.
  376--390, 2014.

\bibitem{McDonnell.15PLOS}
M.~D. McDonnell, M.~D. Tissera, T.~Vladusich, A.~{van Schaik}, and J.~Tapson,
  ``Fast, simple and accurate handwritten digit classification by training
  shallow neural network classifiers with the `extreme learning machine'
  algorithm,'' \emph{PLOS One}, vol.~10, {e0134254~(1--20)}, 2015.

\bibitem{vanSchaik.14}
A.~{van Schaik} and J.Tapson, ``Online and adaptive pseudoinverse solutions for
  {ELM} weights,'' \emph{Neurocomputing}, vol. 149, pp. 233--238, 2015.

\bibitem{Tapson.14}
J.Tapson, P.~{de Chazal}, and A.~{van Schaik}, ``Explicit computation of input
  weights in extreme learning machines,'' in \emph{Proc. ELM2014 conference},
  2014, arXiv:1406.2889.

\bibitem{Yu.12}
D.~Yu and L.~Ding, ``Efficient and effective algorithms for training
  single-hidden-layer neural networks,'' \emph{Pattern Recognition Letters},
  vol.~33, pp. 554--558, 2012.

\bibitem{Zhu.14}
W.~Zhu, J.~Miao, and L.~Qing, ``Constrained extreme learning machine: a novel
  highly discriminative random feedforward neural network,'' in \emph{Proc.
  IJCNN}, 2014, p. XXX.

\bibitem{Zhu.15}
------, ``Constrained extreme learning machines: {A} study on classification
  cases,'' 2015, arXiv:1501.06115.

\bibitem{MNIST}
Y.~LeCun, C.~Cortes, and C.~J.~C. Burges, ``The {MNIST} database of handwritten
  digits,'' Accessed July 2015, {\tt http://yann.lecun.com/exdb/mnist/}.

\bibitem{Krizhevsky}
A.~Krizhevsky, ``Learning multiple layers of features from tiny images,''
  Master's thesis, Dept of CS, University of Toronto. See {\tt
  {http://www.cs.toronto.edu/~kriz/cifar.html}}), 2009.

\bibitem{SVHN}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng, ``Reading
  digits in natural images with unsupervised feature learning,'' 2011, nIPS
  Workshop on Deep Learning and Unsupervised Feature Learning. See {\tt
  http://ufldl.stanford.edu/housenumbers}.

\bibitem{LeCun.04}
Y.~LeCun, F.~J. Huang, and L.~Bottou, ``Learning methods for generic object
  recognition with invariance to pose and lighting,'' in \emph{Proceedings IEEE
  Computer Society Conference on Computer Vision and Pattern Recognition
  (CVPR)}, vol.~2, 2004, pp. 97--104.

\bibitem{Sermanet.14_overfeat}
P.~Sermanet, D.~Eigen, X.~Zhang, M.~Mathieu, R.~Fergus, and Y.~LeCun,
  ``Overfeat: Integrated recognition, localization and detection using
  convolutional networks,'' in \emph{International Conference on Learning
  Representations (ICLR 2014)}.\hskip 1em plus 0.5em minus 0.4em\relax CBLS,
  2014.

\bibitem{Sermanet.12}
P.~Sermanet, S.~Chintala, and Y.~LeCun, ``Convolutional neural networks applied
  to house numbers digit classification,'' in \emph{International Conference on
  Pattern Recognition (ICPR)}, 2012.

\bibitem{Sermanet}
P.~Sermanet, ``A deep learning pipeline for image understanding and acoustic
  modeling,'' Ph.D. dissertation, Department of Computer Science, New York
  University., 2014.

\bibitem{Mairal.14}
J.~Mairal, P.~Koniusz, Z.~Harchaoui, and C.~Schmid, ``Convolutional kernel
  networks,'' in \emph{Advances in Neural Information Processing Systems},
  Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.~Weinberger, Eds.,
  vol.~27, 2014, pp. 2627--2635.

\bibitem{Lee.14}
C.-Y. Lee, S.~Xie, P.~Gallagher, Z.~Zhang, and Z.~Tu, ``Deeply-supervised
  nets,'' in \emph{Deep Learning and Representation Learning Workshop, NIPS},
  2014.

\bibitem{Ciresan.11}
D.~C.~D. Cire{\c{s}}an, U.~Meier, J.~Masci, L.~M. Gambardella, and
  J.~Schmidhuber, ``Flexible, high performance convolutional neural networks
  for image classification,'' in \emph{Proceedings of the Twenty-Second
  International Joint Conference on Artificial Intelligence}, 2011, pp.
  1237--1242.

\bibitem{Simard.03}
P.~Y. Simard, D.~Steinkraus, and J.~C. Platt, ``Best practices for
  convolutional neural networks applied to visual document analysis,'' in
  \emph{Proceedings of the Seventh International Conference on Document
  Analysis and Recognition (ICDAR 2003)}, 2003.

\bibitem{Ciresan.10}
D.~Cire{\c{s}}an, U.~Meier, L.~M. Gambardella, and J.~Schmidhuber, ``Deep, big,
  simple neural nets for handwritten digit recognition,'' \emph{Neural
  Computation}, vol.~22, pp. 3207--3220, 2010.

\bibitem{Ciresan.12}
D.~Cire{\c{s}}an, U.~Meier, and J.~Schmidhuber, ``Multi-column deep neural
  networks for image classification,'' in \emph{Proc. CVPR}, 2012, pp.
  3642--3649.

\bibitem{Tapson.13}
J.~Tapson and A.~{van Schaik}, ``Learning the pseudoinverse solution to network
  weights,'' \emph{Neural Networks}, vol.~45, pp. 94--100, 2013.

\bibitem{Widrow.13}
B.~Widrow, A.~Greenblatt, Y.~Kim, and D.~Park, ``The {No-Prop} algorithm: {A}
  new learning algorithm for multilayer neural networks,'' \emph{Neural
  Networks}, vol.~37, pp. 182--188, 2013.

\bibitem{Huang.15}
G.-B. Huang, Z.~Bai, L.~L.~C. Kasun, and C.~M. Vong, ``Local receptive fields
  based extreme learning machine,'' \emph{IEEE Computational Intelligence
  Magazine}, vol.~10, pp. 18--29, 2015.

\end{thebibliography}




\end{document}
