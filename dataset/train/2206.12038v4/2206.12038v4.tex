

Humans are able to learn, memorize, and distinguish various auditory patterns from limited data by projecting low-level audio inputs to high-level representations in the brain~\citep{griffiths1999human}. Inspired by human capabilities, a substantial body of research has been dedicated over the past few decades to build models capable of extracting and representing auditory information. Historically, handcrafted feature sets, based on digital signal processing (DSP), have been employed to extract audio representations~\citep{liu1998audio, eyben2010opensmile}. However, the recent success of deep learning in computer vision and natural language processing has propelled the development of data-driven frameworks, where deep neural networks (DNNs) are trained on large audio corpora to capture crucial features~\citep{hershey2017cnn,van2018representation}.

Two main methods currently coexist to build deep-learning-based audio representations: supervised~\citep{hershey2017cnn, beckmann2021word} and
self-supervised learning~\citep{baevski2020,niizumi2021byol}. While supervised methods have been at the center of most initial breakthroughs in vision, audio, and language understanding, they are inevitably limited by their reliance on well-defined labels for each training data input. Conversely, self-supervised learning aims at leveraging relations within input data to generate pseudo-labels, thus creating proxy supervised tasks \citep{liu2021self, murphy2022}. Recently, several self-supervised models have been proposed as robust general-purpose audio representations \citep{van2018representation, shor2020towards, saeed2021contrastive, niizumi2021byol, shor2021universal}. Most models are trained using contrastive learning setups, where an encoder network learns to produce a latent space representation by assessing the degree of similarity between input examples \citep{van2018representation, shor2020towards, saeed2021contrastive}. In this framework, similar inputs should be mapped closer in the latent space, whereas unrelated examples should appear more distant. In the context of audio, \textit{similarity} can be measured in terms of temporal proximity \citep{shor2020towards} or, more simply, whether two audio segments originate from the same source or not \citep{saeed2021contrastive}.

However, \citet{niizumi2021byol} argue that contrastive learning frameworks may potentially suffer from several limitations when it comes to audio representation learning. For example, similar rhythmic patterns can be found in different audio sources. Alternatively, short impulsive sounds, such as glass breaking, may be found only in a single example of an audio clip and thus appear as “dissimilar” to other inputs from the same clip. Consequently, \citet{niizumi2021byol} proposed Bootstrap Your Own Latent for Audio (BYOL-A) to learn audio representations by comparing augmented views of a single audio segment. Inspired by the success of BYOL for self-supervised image representation \citep{grill2020}, BYOL-A achieved competitive results in various tasks, including speaker identification, language identification, speech commands, and musical instrument classification.

That being said, handcrafted DSP-based feature sets remain widely used in various speech- and music-related applications. For instance, extensive feature sets such as openSMILE~\citep{eyben2010opensmile} often constitute a strong baseline in paralinguistic tasks and challenges \citep{schuller2013interspeech, schuller2016interspeech, schuller2020interspeech}. Unlike DNNs, such frameworks are completely transparent and interpretable. Moreover, they do not require any training, thus reducing both computational costs and risks of model overfitting (omitting any bias introduced by the handcrafted feature set designer). While the above-outlined properties make the DSP-based features remain relevant for many applications, most recent pre-trained DNNs significantly outperform handcrafted feature sets \citep{shor2020towards, scheidwasser2021serab}.
Accordingly, in this paper, we propose new extensions of BYOL-A for speech representation learning. Whereas the original BYOL-A model was trained on the entire AudioSet \citep{gemmeke2017audio}, a large audio dataset with more than 5800 hours of audio, here we retrained different models on a speech-specific subset from AudioSet creating BYOL for speech (BYOL-S). Originally, the BYOL-S model was developed for speech emotion recognition (SER) and outperformed BYOL-A and other pre-trained models in the context of a speech emotion recognition adaptation benchmark (SERAB) \citep{scheidwasser2021serab}. Here, we extend our previous work to assess BYOL-S, as a general-purpose audio representation and to thoroughly study the impact of different hyperparameters and training protocols on the model performance. In particular, we introduce different encoder architectures (Section~\ref{sec:encoder}) than the default one used in \citet{niizumi2021byol}. With the aim of leveraging the best of both DSP- and DNN-based approaches, we finally assessed the impact of incorporating DSP-based features to the BYOL training paradigm. This led to the development of a novel pre-training protocol for BYOL-S that combines learned and fixed DSP-based handcrafted features (Section~\ref{sec:hybrid}). Such a \textit{hybrid} approach can facilitate the pre-training of the model by grounding the complex DNN features with the considerably simpler DSP-based ones.


All models were evaluated in the context of the Holistic Evaluation of Audio Representations (HEAR) 2021 challenge\footnote{\url{https://neuralaudio.ai/}}, a challenge aimed at designing general-purpose audio embeddings. Launched at NeurIPS 2021, the HEAR challenge featured a new 16-task benchmark suite to compare models, including scene-based (i.e., audio classification) and timestamp-based tasks (i.e., sound event detection). Importantly, the benchmark comprises data from a variety of sources, e.g., human speech, environment sounds, and music.









%
