





\documentclass[11pt,a4paper]{article}
\usepackage{authblk} \usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{graphicx} \usepackage{amsfonts} \usepackage{mathtools}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor} \usepackage{booktabs}
\usepackage{subfigure}

\usepackage{url}

\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\newcommand{\af}[1]{{\color{red}#1}}
\newcommand{\afr}[1]{{\color{red}#1}}
\newcommand{\mc}[1]{{\color{red}#1}}
\newcommand{\nk}[1]{{\color{red}#1}}

\title{Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes}

\renewcommand\Affilfont{\small}
\renewcommand*{\Authfont}{\normalfont\normalsize}
\setlength{\affilsep}{.9em}
\renewcommand\Authsep{ \hspace{0.4em} }
\renewcommand\Authands{\hspace{0.4em} }

\author[]{No{\'e}mien Kocher}
\author[]{Christian Scuito}
\author[]{Lorenzo Tarantino}
\author[]{Alexandros Lazaridis}
\author[,]{\\Andreas Fischer}
\author[]{Claudiu Musat}
\affil[]{School of Engineering and Architecture of Fribourg, Switzerland, HES-SO, iCoSys Institute}

\makeatletter
\renewcommand\AB@affilsepx{\hspace{.02\linewidth} \protect\Affilfont}
\makeatother

\affil[]{Ecole Polytechnique F{\'e}d{\'e}rale de Lausanne (EPFL)}
\affil[]{Swisscom}

\makeatletter
\renewcommand\AB@affilsepx{\\
\Affilfont}
\makeatother

\affil[]{University of Fribourg}

\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother

\affil[ ]{\scriptsize \tt \{noemien.kocher|sciutochristian|lorenzotara7\}@gmail.com\protect\
(P-1) / P
\frac{P}{K} \cdot q = n,\ \ \ q < K, q,n \in \mathbb{N}
independent of the number of data points . A repetition thus occurs iff the greatest common divisor (GCD) of  and  is larger than 1. Otherwise, for GCD a data point repeats only after period , i.e. there is no repetition within the same batch.

Table~\ref{finite period} lists exemplary periods for a batch size of  and different values of  for the Alleviated TOI (P). The worst case is  with  repetitions of the same data point within the same batch and the best case is , which avoids any redundancy because the GCD of  and  is 1. Figure~\ref{fig:datapoints as pixels} illustrates the repetition with grayscale values, where similar grayscale values indicate that two data points are close within the original data points sequence.

In general, while we aim for large values of  for reducing the TOI, a simple solution for avoiding redundancy within batches is to choose a prime number for the batch size .

\section{Experimental Setup}
\label{sec-exp}
To validate the generalization capability of the proposed technique, we apply it on both text and speech related tasks. We thus run the Alleviated TOI~(P) with language modeling (text) and emotion recognition (speech). The text datasets used are Penn-Tree-Bank (PTB) \cite{Marcus:1993} as preprocessed in \citet{Mikolov:11}, Wikitext-2 (WT2), and Wikitext-103 (WT103) \cite{Merity:16}.
The speech dataset is the IEMOCAP database \cite{Busso2008IEMOCAPIE}, a collection of more than 12 hours of recorded emotional speech of 10 native-English speakers, men and women. The audio data is filtered down to 5.5 hours containing only \textit{angry}, \textit{happy}, \textit{neutral} and \textit{sad} utterances.

\subsection{TOI in Language Modelling}

For language modeling, we use three different methods:
\begin{itemize}
    \item A simple LSTM that does not benefit from extensive hyper-parameter optimization.
    \item An Average Stochastic Gradient Descent Weight-Dropped LSTM (AWD-LSTM) as described in \citet{Merity:17}, with the same hyper-parameters.
    \item The latest State-of-the-Art model: Mixture of Softmaxes (MoS) \cite{Zhilin:17}.
\end{itemize} 
We compare our results against the original process of building data points, i.e. Standard TOI, and use the same computation load allocated for each experiment. We use the same set of hyper-parameters as described in the base papers, except for the batch size with Alleviated TOI~(P), where we use a prime batch size in order to prevent any repetitions in batches, as described in Section \ref{section: prime}. 
That is, on the PTB dataset, we use a sequence length of 70 for all the models. For the Simple LSTM and AWD-LSTM, we use a batch size of 20 and a hidden size of 400.
AWD-LSTM and MoS are trained on 1000 epochs, and the Simple LSTM on 100 epochs. For the MoS model, embedding size used is 280, batch size 12, and hidden size 980.
All the models use SGD as the optimizer.

We set up experiments to compare 4 different token order imbalance setups: Extreme TOI, Inter-batch TOI, Standard TOI, and Alleviated TOI~(P).

\paragraph{Extreme TOI} The Extreme TOI setup builds batches using a random sequence of data points. This removes any order inside the batches (i.e. among data points within a batch), and among batches. 

\paragraph{Inter-batch TOI} In the Inter-batch TOI setup, batches are built using an ordered sequence of data points, but the sequence of batches is shuffled. This keeps the order inside batches, but removes it among batches. Looking at the 2D matrix of batches, in Figure~\ref{fig:datapoints as pixels}, this results in shuffling the rows of the matrix.

\paragraph{Standard TOI} In the Standard TOI setup, the process of building batches is untouched, as described in section \ref{overlapping method}. This keeps the order inside, and among batches. 

\paragraph{Alleviated TOI~(P)} In the Alleviated TOI~(P) setup, we apply our proposed TOI reduction by creating  overlapped data point sequences (see Sections~\ref{overlapping method} and~\ref{section: prime}). This strategy not only keeps the order inside and among batches, but it also restores the full token order information in the dataset.

\subsection{TOI in Speech Emotion Recognition}

For Speech Emotion Recognition (SER) we use two different models: the encoder of the Transformer \cite{vaswani:2017} followed by convolutional layers, and the simple LSTM used in text domain case. Since the Transformer is stateless and uses self-attention instead, we are able to investigate the effect of Alleviated TOI~(P) independently of LSTM cells.

As with language modeling, we set up experiments to compare the 4 different token order imbalance strategies: Extreme TOI, Inter-batch TOI, Standard TOI, and Alleviated TOI~(P). 

We apply the methodology used in text on the SER task, using the simple LSTM and a window size of 300 frames. In this case, a data point, instead of being a sequence of words, is a sequence of frames coming from the same utterance. Each frame is described by a 384-dimensional features vector. OpenSMILE \cite{Eyben:2013:RDO:2502081.2502224} is used for extracting the features. We opt for the IS09 features set \cite{Schuller2009Interspeech} as proposed by \citet{Ramet2018ContextAwareAM} and commonly used for SER.

Finally, to investigate the effect of the Alleviated TOI~(P) strategy independently of LSTM cells, we design a final experiment in the SER task. We investigate whether or not we have improved results as we increase , the number of overlapped data point sequences in a stateless scenario. For this reason, we use the Transformer model described above.

\section{Experimental Results}
\label{sec-results}
\subsection{Language Modelling}

Table~\ref{table: orders with awd model} compares the 4 token order imbalance strategies using the AWD model and three text datasets. We use the test perplexity after the same equivalent number of epochs. The different Alleviated TOI (P) experiments use a different number of overlapped sequence: An Alleviated TOI (P) means building and concatenating  overlapped sequences. Our results indicate that an Alleviated TOI (P)
is better than the Standard TOI, which is better than an Extreme or Inter-batch TOI. We note a tendency that higher values of  lead to better results, which is in accordance with our hypothesis that a higher TOI ratio  improves the results.

\begin{table}[t]
\centering

\begin{tabular}{@{}llll@{}}
\toprule
  Experiment & PTB & WT2 & WT103\\
  \midrule
  Extreme TOI       & 63.49          & 73.52          & 36.19 \\
  Inter-batch TOI   & 64.20          & 72.61          & 36.39 \\
  Standard TOI      & \textbf{58.94}         & \textbf{65.86} & \textbf{32.94} \\
  Alleviated TOI 2  & 57.97          & 65.14          & 32.98 \\
  Alleviated TOI 5  & 57.14        & 65.11          & 33.07 \\
  Alleviated TOI 7  & 57.16         & 64.79          & 32.89 \\
  Alleviated TOI 10 & \textbf{56.46} & \textbf{64.73} & \textbf{32.85} \\
\bottomrule
\end{tabular}
\caption{
    \label{table: orders with awd model}
    Perplexity score (PPL) comparison of the AWD model, on the three datasets, with batch sizes ~(PTB), ~(WT2) and ~(WT103), with different levels of Token Order Imbalance (TOI). With Alleviated TOI~(P), we use a prime batch size of ~(PTB), ~(WT2) and ~(WT103).
  }
\end{table}

\paragraph{Comparison with State of the Art and Simple LSTM.}

With the MoS model and an Alleviated TOI, we improve the current state of the art without fine tuning for the PTB dataset with 54.58 perplexity on the test set. Table~\ref{table: sota with mos} demonstrates how models can be improved by applying our Alleviated TOI method on 2 latest state-of-the-art models: AWD-LSTM \cite{Merity:17} and AWD-LSTM-MoS \cite{Zhilin:17}, and the Simple LSTM model. We compare the results with the same hyper-parameters used on the original papers with the only exception of the batch size, that must be prime. To ensure fairness, we allocate the same computational resources for the base model as well the model with Alleviated TOI, i.e. we train with the equivalent number of epochs.

\begin{table}[t]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
  Model & test ppl\\
  \midrule
  AWD-LSTM \cite{Merity:17} & 58.8 \\
  AWD-LSTM + Alleviated TOI & 56.46 \\
  AWD-LSTM-MoS \cite{Zhilin:17} & 55.97 \\
  AWD-LSTM-MoS + Alleviated TOI & 54.58 \\
  Simple-LSTM & 75.36 \\
  Simple-LSTM + Alleviated TOI & 74.44 \\
\bottomrule
\end{tabular}
\caption{
    \label{table: sota with mos}
    Comparison between state-of-the-art models \cite{Merity:17, Zhilin:17} and a Simple LSTM, and the same models with Alleviated TOI. The comparison highlights how the addition of Alleviated TOI is able to improve state-of-the-art models, as well as a simple model that does not benefit from extensive hyper-parameter optimization.
}
\end{table}

\paragraph{Comparison without prime batch size.}

In Table~\ref{table: prime comparison} we demonstrate how using a prime batch size with Alleviated TOI (P) actually impacts the scores. We compare the scores of a prime batch size  with the scores of the original batch size  for the AWD model with Alleviated TOI (P). When using a prime batch size, we observe consistent and increasing results as  increases. This is due to the good distribution of data points in the batches regardless of the value of , which is visible in Figure~\ref{fig: prime matrix of pixel} where each row contains a high diversity of grayscale values. With the original batch size , we observe a strong performance for , but a low performance for . Again, this effect is related to the distribution of data points in the batches, which is visible in Figure~\ref{fig:non prime matrix of pixel}. The matrix with  shows a good distribution---corresponding to the strong performance---and the matrix with  shows that each row contains a low diversity of data points.

\begin{table}[t]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
  Experiment & K=20 & K=19\\
  \midrule
  Alleviated TOI 2  & 59.37 & 57.97 \\
  Alleviated TOI 5  & 60.50 & 57.14 \\
  Alleviated TOI 7  & \textbf{56.70} & 57.16 \\
  Alleviated TOI 10 & 65.88 & \textbf{56.46} \\
\bottomrule
\end{tabular}
\caption{
    \label{table: prime comparison}
    Perplexity score (PPL) comparison on the PTB dataset and the AWD model. We use two different values for the batch size  --- the original one with , and a prime one with . The results directly corroborate the observation portrayed in Figure~\ref{fig:datapoints as pixels}, where the obtained score is related to the diversity of grayscale values in each row.
  }
\end{table}



\subsection{Speech Emotion Recognition Results}

The results on the IEMOCAP database are evaluated in terms of weighted (WA) and unweighted accuracy (UA). The first metric is the accuracy on the entire evaluation dataset, while the second is the average of the accuracies of each class of the evaluation set. UA is often used when the database is unbalanced, which is true in our case, since the \textit{happy} class has a total duration that is half of the second smallest class in speech duration.

Table~\ref{table: lstm on iemocap} shows that our proposed method brings value in the speech related task as well. When choosing the Extreme TOI instead of the Standard TOI approach we observe a smaller effect than in text related task: this is due to the different nature of the text datasets (large "continuous" corpuses) and the IEMOCAP one (composed of shorter utterances). The fact that we can still observe improvements on a dataset with short utterances is a proof of the robustness of the method.

\begin{table}[t]
\centering

\begin{tabular}{@{}lll@{}}
\toprule
  Experiment & WA & UA \\
  \midrule
  Extreme TOI (15k steps) & 0.475 & 0.377 \\
  Inter-batch TOI (15k steps) & 0.478 & 0.386 \\
  Standard TOI (15k steps) & 0.486 & 0.404 \\
  Alleviated TOI (15k steps) & \textbf{0.553} & \textbf{0.489} \\
  Alleviated TOI (60 epochs) & \textbf{0.591} & \textbf{0.523} \\
\bottomrule
\end{tabular}
\caption{
    \label{table: lstm on iemocap}
    Token order imbalance (TOI) comparison for the IEMOCAP dataset on a SER task using \textit{angry, happy, neutral} and \textit{sad} classes with a simple LSTM model. }
\end{table}

A greater effect is obtained when we increase the size of the dataset with the proposed Alleviated TOI~(P) approach: Due to the increasing offset at each overlapped sequence, the data fed into the model contains utterances where the emotions are expressed in slightly different ways. For this reason, the performance notably increases.

Table~\ref{table: transformer overlapping on iemocap - epochs} reports the result of a final experiment that aims to investigate the effect of Alleviated TOI~(P) independently of LSTM cells. For each Alleviated TOI (P) setup and Standard TOI described in Table~\ref{table: transformer overlapping on iemocap - epochs}, we repeat the training and evaluation for each of the following window sizes: 100, 200, 300, 400 and 500 frames. The previously described Transformer model is used in these experiments.
The results reported in Table~\ref{table: transformer overlapping on iemocap - epochs} are the mean  the standard deviation computed for different P-values of Alleviated TOI~(P).

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}lll@{}}
\toprule
  Experiment & WA (60 epochs) & UA (60 epochs)\\
  \midrule
  Alleviated TOI 1 & 0.5910.012 & 0.5430.021 \\
  Alleviated TOI 2 & 0.5940.007 & 0.5490.016 \\
  Alleviated TOI 3 & 0.6050.018 & 0.5630.024 \\
  Alleviated TOI 5 & 0.6080.015 & 0.5620.028 \\
  Alleviated TOI 10 & \textbf{0.6170.015} & \textbf{0.5710.024} \\
  \midrule
  Local attention & \textbf{0.635} & \textbf{0.588} \\
\bottomrule
\end{tabular}}
\caption{
    \label{table: transformer overlapping on iemocap - epochs}
    Token order imbalance (TOI) comparison for the IEMOCAP dataset on a SER task using \textit{angry, happy, neutral and sad} classes for 60 epochs using the Transformer model.
  }
\end{table}

The last line of Table~\ref{table: transformer overlapping on iemocap - epochs} refers to \citet{Mirsamadi2017AutomaticSE} results. We want to highlight the fact that the goal of these experiments is to show the direct contribution of the Alleviated TOI technique for a different model. For this reason we use a smaller version of the Transformer in order to reduce the computational cost. We believe that with a more expressive model and more repetitions, the proposed method may further improve the results.

The results from Table~\ref{table: transformer overlapping on iemocap - epochs} demonstrate that, as we increase the value of , more significant improvements are achieved. This is in accordance with our hypothesis that a higher TOI ratio  improves the results.


\section{Conclusions}
In this work, the importance of overlapping and token order in sequence modelling tasks were investigated.
Series discretization is an essential step in machine learning processes which nonetheless can be responsible for the loss of the continuation of the tokens, through the token order imbalance (TOI) phenomenon.
The proposed method, Alleviated TOI, has managed to overcome this drawback and ensures that all token sequences are taken into account.
The proposed method was validated in sequence modelling tasks both in the text and speech domain outperforming the state of the art techniques.


\bibliography{emnlp-ijcnlp-2019}
\bibliographystyle{acl_natbib}

\end{document}
