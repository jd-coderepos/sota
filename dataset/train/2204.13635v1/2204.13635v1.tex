\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}


\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}

\usepackage{times}
\usepackage{epsfig}


\usepackage{blindtext}
\usepackage{subfigure}
\usepackage{caption}

\usepackage{color,soul}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bbm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}

\title{SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion}
\author{\uppercase{Danish Nazir}\authorrefmark{1},
\uppercase{Marcus Liwicki \authorrefmark{2}, Didier Stricker \authorrefmark{1}, Muhammad Zeshan Afzal \authorrefmark{1}
}
}
\address[1]{Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI) Trippstadter Str. 122, 67663 Kaiserslautern (e-mail: first.last@dfki.de)}
\address[2]{Luleå University of Technology (e-mail: first.last@ltu.se)}



\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}



\begin{abstract}
Depth completion involves recovering a dense depth map from a sparse map and an RGB image.~Recent approaches focus on utilizing color images as guidance images to recover depth at invalid pixels.~However, color images alone are not enough to provide the necessary semantic understanding of the scene.~Consequently, the depth completion task suffers from sudden illumination changes in RGB images (e.g., shadows).~In this paper, we propose a novel three-branch backbone comprising color-guided, semantic-guided, and depth-guided branches.~Specifically, the color-guided branch takes a sparse depth map and RGB image as an input and generates color depth which includes color cues (e.g., object boundaries) of the scene.~The predicted dense depth map of color-guided branch along-with
semantic image and sparse depth map is passed as input to semantic-guided branch for estimating semantic depth.~The depth-guided branch takes sparse, color, and semantic depths to generate the dense depth map.~The color depth, semantic depth, and guided depth are adaptively fused to produce the output of our proposed three-branch backbone.~In addition, we also propose to apply semantic-aware multi-modal attention-based fusion block (SAMMAFB) to fuse features between all three branches.~We further use CSPN++ with Atrous convolutions to refine the dense depth map produced by our three-branch backbone.~Extensive experiments show that our model achieves state-of-the-art performance in the KITTI depth completion benchmark at the time of submission.
\end{abstract}

\begin{keywords}
State-of-the-art Depth Completion
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
\begin{figure}
\begin{center}
    \includegraphics[width=0.45\textwidth,keepaspectratio]{Images/1st_image.jpg} 
\end{center}
\caption{Block diagram of SemAttNet. In the first stage, the multi-modal data consisting of RGB, semantic, and LiDAR sparse depth maps are passed to our novel three-branch backbone, which outputs a fused dense depth map. Our proposed backbone utilize Semantic-aware multi-model attention-based fusion (SAMMAFB) to adaptively fuse features between RGB, semantic and sparse depth branches.~In the second stage, the output of the three-branch backbone is further refined through CSPN++ with Atrous convolutions. }
\label{figure1}
\end{figure}
Estimation of dense depth measurements is crucial in various 3D vision and robotics applications such as augmented and mixed reality  \cite{holynski2018fast, augmented_app},  scene reconstruction  \cite{3DReconstruction, park2020non}, autonomous driving \cite{DepthNet, fisheyeautonomous}, and obstacle avoidance \cite{obstacle_avoid, tang2020learning}.~For obtaining a reliable prediction of depth in outdoor scenes, measurements from various sensors are used. Most commonly used sensors include RGB cameras in a stereo setting, light detection and ranging (LiDAR), and time of flight (ToF) cameras\cite{tof}.~Among all the sensors, LiDAR is considered the most reliable and efficient, and it produces accurate depth measurements in an outdoor setting \cite{tang2020learning}.~However, the density of LiDAR depth measurements is sparse, with a considerable amount of missing depth data\cite{KITTI, kitti_2}.~For example, the LiDAR sensor for mobile applications \cite{tang2020learning}, Velodyne HDL-64e, which is also used in the KITTI \cite{KITTI, kitti_2} dataset produces depth maps consisting of valid depth values only on  pixels.~Such sparse depth maps cannot be utilized directly in the application areas mentioned above.~Therefore, dense depth maps estimation from sparse measurements is of utmost importance. It is considered a challenging problem since the measured depth values only form  of the complete depth map.


Recent methods \cite{Qiu_2019_CVPR,hu2020PENet,yan2021rignet,lee2021depth,Multi-TaskGan,zhao2021adaptive,liu2021fcfr,cheng2020cspn++,chen2019learning}  employ the deep learning-based approaches to dense depth completion.~These methods utilize convolutional neural networks and combine sparse LiDAR data with different modalities, e.g. RGB images \cite{hu2020PENet, yan2021rignet}, semantic maps \cite{Multi-TaskGan,schneider2016semantically,chen2019towards}, and surface normal's \cite{Qiu_2019_CVPR}.~These modalities act as guidance and significantly help in the recovery of missing depth values in the sparse maps.~The idea is to actively fuse features between different modalities. Most of the current methods adopt two-branch networks architectures for feature fusion.~For instance, DeepLiDAR \cite{Qiu_2019_CVPR}, FusionNet \cite{vangansbeke2019sparse} and PENet \cite{hu2020PENet} utilize an encoder-decoder architecture to perform both early and late fusion between color images and LiDAR sparse depth maps.~The combination of early and late fusion between the modalities enhances the depth completion performance.~Recently, RigNet \cite{yan2021rignet} proposed a repetitive architecture, which repeats Hourglass network architecture for color images at multiple levels to extract features and fuse them with depth generation branch to create structure-detailed dense depth maps.~These methods rely heavily only on color images to extract color dominant information such as object boundaries to complete sparse maps.~However, color images truly do not provide this information.~It is because color images suffer from shadows and reflections, which causes irregularities in pixel values \cite{chen2019towards}.

To solve the problems in the methods mentioned earlier, we propose a novel three-branch network inspired by PENet \cite{hu2020PENet}, FusionNet \cite{vangansbeke2019sparse} and DeepLiDAR \cite{Qiu_2019_CVPR}.~It consists of color-guided (CG), semantic-guided (SG), and dense depth-guided (DG) branches.~In contrast to earlier methods \cite{Qiu_2019_CVPR,hu2020PENet,vangansbeke2019sparse}, we add SG branch to decrease the variance of depth values around object boundaries.~The CG branch produces a noisy depth estimate through the color image and LiDAR sparse depth map.~The output of the CG branch, LiDAR sparse depth and semantic map of the color image is passed to the SG branch, which actively refines the color depth and produces a depth based on semantic information. For ease of understanding, we name it semantic depth in this paper.~The semantic depth is much more reliable around object boundaries than color depth.~It is because the pixel values of semantic maps are uniform and have fewer irregularities.~Furthermore, we also use the DG branch to refine further the depth produced by the SG branch.~It takes LiDAR sparse depth map, CG and SG depth as input and produces a dense depth maps.~Similar to earlier methods \cite{hu2020PENet, zhao2021adaptive}, we also combine depth information between CG, SG and DG depths with learned confidence maps.~The confidence map weights enables us to compare the reliability of the predictions at each branch.~In the end, we perform multi-modal depth fusion between CG, SG and DG predicted dense depths to generate the final dense depth map.~The whole backbone model is trained in an end-to-end manner.~Furthermore, we apply CSPN++ with Atrous convolutions \cite{chen2017deeplab,hu2020PENet,cheng2020cspn++} on the final dense depth map predicted by the three-branch backbone for further refinement.



Since we have branches dealing with multi-modal data, we perform both early and late fusion between them for effective guidance of dense depth map generation.~Early fusion is performed by the depth-wise concatenation of color, semantic or LiDAR sparse depth maps.~The concatenation of the input is fed to the respective branch.~Late fusion is performed at the feature level and involves the fusion of multi-modal data.~The naive strategy of performing late fusion is to apply simply concatenation or addition of features maps of different modalities and it is used in many earlier works such as e.g.  \cite{tang2020learning, hu2020PENet, ma2019self}.~However, this is not an optimal way because each branch contains different information.~Therefore, we propose to apply semantic-aware multi-modal attention-based fusion block (SAMMAFB) inspired by CBAM \cite{woo2018cbam} and AFB \cite{fooladgar2019multi} to depth completion problem.~SAMMAFB applies both channel and spatial wise attention maps of the input feature maps and produces refined feature maps.~As demonstrated in Figure \ref{figure1}, we actively fuse features at multiple stages through SAMMAFB in all of our branches.

The main contributions of our work is summarized as follows:
\begin{itemize}
  \item We propose a novel three-branch backbone for sparse depth completion, which counters the sensitivity of image-guided methods to optical changes (e.g., shadows and reflections).
  \item We present a novel SAMMAFB block to actively fuse the color, semantic, and depth modalities at multiple stages in our three-branch backbone.
  \item Extensive experimental results show that our model achieves state-of-the-art results on the outdoor KITTI depth completion dataset.
  
\end{itemize}


\section{Related Work}

\subsection{Sparse Depth Based Approaches}
Earlier approaches \cite{KITTI, chodosh2018deep} based on convolutional neural networks (CNN) utilized only sparse depth maps to generate dense depth maps.~To counter the sparsity of data in sparse depth maps, Depth-Net \cite{bai2020depthnet} performed nearest neighbor interpolation in the sparse maps to fill out the holes (points with no depth values).~Later on, uncertainty aware CNN's \cite{eldesokey2020uncertainty} proposed probabilistic normalized convolutions to model the uncertainty in the sparse depth maps.~However, the obvious drawback of these approaches is that without color or semantic image guidance, the predicted depth maps lack clear object boundaries and also these methods are not suitable for real-time applications.
 

 
\subsection{Image-Guided Methods}

Image-Guided methods~\cite{park2020non,tang2020learning,hu2020PENet,yan2021rignet,zhao2021adaptive,liu2021fcfr,cheng2020cspn++,dspn,gu2021denselidar,cheng2018depth,liu2017learning} utilize multi-modal information to facilitate dense depth completion.~The multi-modal data includes RGB images, semantic images, and surface normal's, which act as reference images for generating dense depth maps.

~Methods such as SPN\cite{liu2017learning}, CSPN\cite{cheng2018depth}, CSPN++\cite{cheng2020cspn++}, DSPN\cite{dspn} and NLSPN  \cite{park2020non} focus on learning affinity matrix for high-level vision tasks including semantic segmentation and depth completion.~SPN \cite{liu2017learning} serially propagates the affinity matrix, which is inefficient for real-time systems.~CSPN \cite{cheng2018depth} improved SPN \cite{liu2017learning} by predicting affinity values of local neighbors and updating pixel values simultaneously.~Both of the methods suffer with the problem of fixed local neighborhoods, which often have irrelevant information.~To solve this problem, CSPN++ \cite{cheng2020cspn++}, DSPN \cite{dspn} and NLSPN \cite{park2020non} methods are introduced.~CSPN++ \cite{cheng2020cspn++} proposed adaptive learning of kernel sizes and the number of iterations of propagation, which helped in reducing the computation time of CSPN \cite{cheng2018depth}.~DSPN \cite{dspn} and NLSPN \cite{park2020non} learn deformable convolutional kernels to relax the fixed local neighborhood of pixels, which enabled them to focus only on relevant pixel neighbors for depth completion.~All of the methods mentioned above utilizes a single branch AutoEncoder (AE) \cite{liou2014autoencoder} network architecture.~The input of the AE is the concatenation of RGB image and sparse depth map, which outputs a dense depth map.




The two branch network architectures  \cite{Qiu_2019_CVPR,hu2020PENet,yan2021rignet,zhao2021adaptive,liu2021fcfr} consist of RGB and sparse depth map branches. The RGB branch extracts color dominant information, e.g., object boundaries, which is actively fused with a sparse depth map branch at multiple stages.
ACMNet \cite{zhao2021adaptive} introduced a symmetric gated fusion strategy for performing the fusion between two branches.~FCFR-Net proposed a channel shuffling and energy-based fusion between the two modalities.~PENET \cite{hu2020PENet}, utilized concatenation and addition operations to perform fusion at multiple levels.~Recently, RigNet \cite{yan2021rignet} proposed an efficient guidance algorithm to fuse and guide the sparse depth branch.~In addition to RGB image and sparse depth map, DeepLiDAR \cite{Qiu_2019_CVPR} introduced learning pixel-wise surface normal's of the scene, which is fused through addition with RGB branch to generate dense depth maps.~Similarly, Multi-Task GAN's \cite{Multi-TaskGan} generated semantic maps of the RGB images and concatenated them with sparse and RGB images to guide dense depth map completion.~Compared to earlier approaches, the performance of two branch methods are much better. 

Unlike existing Image-guided methods, our approach consists of a semantic-guided branch, which given the color and sparse dense depths, learns to understand the semantic meaning of the scene.~Consequently, it enables our approach to perform consistently under different lighting conditions, which is a challenging aspect of outdoor depth completion.

 


\subsection{Attention-based Multi-Modal Data Fusion}

Attention-based multi-modal information fusion has been studied in various computer vision applications, such as video description \cite{hori2017attention}, human motion estimation \cite{li2020attention}, emotion and sentiment classification \cite{huddar2021attention} and 
many more.~In the context of dense depth completion, the attention mechanism enables the model to focus on meaningful regions of multi-modal information, generating depths with clear and sharp structural details.~Moreover, given the sparsity of the LiDAR depth data, the attention module is important for depth completion task.~Recently, ACM Net \cite{zhao2021adaptive} proposed a Co-attention Guided Graph Propagation (CGPM), which exploits the attention mechanism in the spatial domain to propagate graphs at multiple scales.~However, spatial attention only capture local contexts within a fixed neighborhood.~Therefore, to capture both local and global attention, we propose to apply SAMMAFB between RGB, semantic, and LiDAR sparse depth modalities.~It produces both channel-wise and spatial-wise attention weights, which are applied to multi-modal information to generate refined fused feature maps.
\begin{figure*}
\begin{center}
    \includegraphics[width=\textwidth,keepaspectratio]{Images/Full_Architecture.jpg} 
\end{center}
\caption{The overview of proposed SemAttNet.~It consists of a novel three-branch backbone and a CSPN++ module with Atrous convolutions.~Unlike earlier image-guided methods, we design a separate branch for learning the semantic information of the scene.~Furthermore, we propose to apply attention based fusion block (ABF) to perform semantic-aware fusion between RGB, depth, and semantic modalities.~Each branch outputs a depth map and a confidence map, which are adaptively fused to produce a fused depth map.~In the end, the fused depth map are sent to CSPN++ module with Atrous convolutions for refinement.~Note, due to shortage of space, we use AFB to represent SAMMAFB block. }


\label{SemAttNet_Architecture}
\end{figure*}
\section{Methodology}


~We formulate an image-guided depth completion problem as a two-stage task.~Figure \ref{SemAttNet_Architecture} shows the pipeline of our approach.~In the first stage, we design a three-branch backbone network to produce a dense depth map, whereas, in the second stage, we utilize CSPN++ \cite{hu2020PENet, cheng2020cspn++} with Atrous convolutions for further refinement.~The three-branch backbone consists of CG, SG and DG branches.~The purpose of the CG branch is to focus on color-dominant information, whereas the SG branch aims to learn the semantics information of the scene.~The predicted dense depths of CG and SG branches are noisy depth estimates but they contain important color and semantic cues of the scene.~The DG branch takes predicted dense depth maps of SG and CG branch along with aligned sparse map as input to produce a dense depth map.~DG branch focuses on learning depth dominant features.~All of the branches are complimentary to each other.~Therefore, we adaptively fuse them with learned confidence weights.


\subsection{Semantic-aware Multi-Modal Attention-based Fusion Block}

In order to perform fusion between RGB, semantic, and depth features, we propose to apply Semantic-aware Multi-Modal Attention-based Fusion Block (SAMMAFB) on RGB, semantic, and depth features in our three-branch backbone.~SAMMAFB helps us refine the concatenated feature maps of modalities by capturing salient feature maps while suppressing the unnecessary ones \cite{fooladgar2019multi}.~We apply SAMMAFB in two different settings, first we apply it to fuse the intermediate feature maps of RGB and semantic guided branches.~Then, for depth-guided branch, we perform the fusion between the intermediate feature maps of all branches.

 Figure \ref{SAMMAFB_Architecture} depicts the visual representation of SAMMAFB.~It consists of a channel and spatial attention sub-modules, which enables it to capture channel-wise and spatial-wise attention.~Specifically, channel-wise attention aims to learn the important channels by assigning a weight to each channel relative to their contribution towards increasing the overall performance.~Similarly, the spatial-wise attention module focuses on learning the location of the important channels produced by the channel-wise attention sub-module.~Both of the modules complement each other and vital to producing refined fused feature maps.
\begin{figure}
\begin{center}
    \includegraphics[width=0.45\textwidth,keepaspectratio]{Images/SAMMABF.jpg} 
\end{center}
\caption{Architecture of SAMMAFB.~The input to SAMMAFB is the concatenation of RGB, semantic and sparse depth modalities.~It applies channel-wise and spatial-wise attention to the concatenated features to produce the refined fused feature maps. }


\label{SAMMAFB_Architecture}
\end{figure}

Let ,  and  represent the intermediate feature maps of RGB, semantic and depth guided branches at same level, respectively and  denotes their depth-wise concatenation.~Channel-wise attention weights are computed with Equation \ref{equation1}. 


Where  represents channel-wise attention weights of ,  denotes sigmoid function, ,  represents the weights matrices for multi-layer perceptron (MLP) layers.~The parameter  controls the number of learnable parameters in MLP layer.~ and   denotes the max-pooled and average-pooled features respectively. 


The channel-wise attentions weights are applied to  to obtain  such that .~The product  is then sent to spatial-wise attention module, which is given in Equation \ref{equation_2}.

Where  represents spatial-wise attention weights of ,  denotes sigmoid function.~ and   denotes the max-pooled and average-pooled features respectively. 

The spatial-wise attentions weights are applied to  to obtain final output of SAMMAFB i.e. refined fused feature maps  such that .


\subsection{The Three-branch Backbone}
The earlier two-branch methods \cite{hu2020PENet,yan2021rignet,zhao2021adaptive,liu2021fcfr} relied heavily on the color image to extract both semantic and color-dominant information.~However, color images alone might not be able to provide this information.~Therefore, to overcome this limitation, we propose a three-branch backbone consisting of CG, SG and DG branches.~The addition of the SG branch enables better learning of semantic cues of the scene, which help predict depth maps with accurate and sharp structure in the DG branch.~The dataset comprises of color and semantic images that are represented by  and , whereas  and  denotes sparse and ground truth depth maps, and  denotes the number of samples.~Note, that we don't have any supervision of confidence maps due the unavailability of ground truth.~Therefore, confidence maps are trained indirectly based on the  loss of each branch. 


\subsubsection{Color-guided Branch}
The CG branch aims to learn important color cues for dense depth completion.~It takes a color image concatenated with an aligned sparse depth map as input and outputs a dense depth map.~The concatenation of an aligned sparse depth map with a color image helps in the prediction of a dense depth map \cite{hu2020PENet}.~The CG branch follows an encoder-decoder network architecture with skip connections.~The encoder consists of ten ResNet \cite{he2016deep} blocks, whereas the decoder consists of one convolution and five transpose convolution layers for upsampling.~The output of this branch consist of dense depth map and a confidence map.~The resultant dense depth map of this branch is a noisy depth estimate, but it provides a baseline for learning structural information of the scene in other branches.

Let  represent the concatenation of batch of size  of color images  and sparse depth maps .~The CG branch takes  as input, and in the first step, it is encoded to a hidden representation  based on Equation \ref{equation_CD_1}.~Then,  is decoded into a confidence map   and a color depth map  based on Equation \ref{equation_CD_2}. 



Where  and  represents encoder and decoder weight matrices.~The variables   and  denote encoding and decoding bias values.~ and  represent activation functions.

Equation \ref{loss_CD} defines the  loss for CG branch.~Since the ground truth contains invalid depth values, we only consider pixels with valid depth values for computing loss.

\subsubsection{Semantic-guided Branch}
Semantic cues help to understand the scene and are essential for depth completion task \cite{semantic}.~However, CG branch alone is not enough to learn semantic information.~Therefore, we propose an SG branch to encourage learning effective semantic cues and to complement RGB images for depth completion.~The SG branch takes the concatenation of color depth, semantic image and sparse depth map as an input and outputs a dense depth map, which consist of both color and semantic cues.~The KITTI depth completion dataset does not provide aligned semantic maps of RGB images.~Therefore, we transform the RGB images to semantic maps through a pre-trained WideResNet38 \cite{wu2016wider} model on KITTI semantic segmentation benchmark \cite{Alhaija2018IJCV}.~Furthermore, Inspired by earlier works \cite{tang2020learning,hu2020PENet}, we fuse the decoder features from the CG branch into the corresponding encoder features of the SG branch.~The features from CG and SG are sent to SAMMAFB, which outputs refined fused depth feature maps.





Similar to CG branch, we define  to represent the concatenation of batch size  of color depths , color images  and sparse depth maps .~The color depth consists of important color cues e.g. object boundaries and its concatenation will complement in learning semantic cues.~The matrix  is passed as an input to SG branch.~In the first step, the input is encoded to a hidden representation  based on Equation \ref{equation_SD_1}.~In the second step, the encoded representation  is decoded into a confidence map  and a semantic depth map  based on Equation \ref{equation_SD_2}. 



Where  and  represents encoder and decoder weight matrices.~The variables  and  represents encoding and decoding bias values.~ and  denotes activation functions.

~Equation \ref{loss_SG} defines the  loss for SG branch.~we only consider pixels with valid depth values for loss calculation because the ground truth contains invalid depth values.

\subsubsection{Depth-guided Branch}

The DG branch focuses on learning depth-dominant features, which help generate accurate dense depth maps.~It takes the concatenation of CG, SG, and sparse depth as input and outputs a dense depth map.~Similar to the feature fusion strategy in the CG and SG branch, we fuse decoder features from CG and SG branches into the corresponding encoder features of the DG branch.~However, in DG branch, SAMMAFB take feature maps from three modalities as input and outputs refined fused feature maps.~The refined fused feature maps consists of useful information from CG and SG branches, which guides the DG branch in learning effective depth feature representations.~The network architecture is similar to CG and SG branches, but we add two more layers in encoder and decoder architectures to accommodate CG and SG branch decoder features.




~We define  to represent the concatenation of batch of size  of color depths , semantic depths  and sparse depth maps .~The DD branch takes  as an input and encodes it to a hidden representation  based on Equation \ref{equation_DD_1}.~Afterwards, the encoded representation  is decoded into a confidence map  and a dense depth map  based on Equation \ref{equation_DD_2}. 



Where  and  represents encoder and decoder weight matrices.~The variables  and  represents encoding and decoding bias values.~ and  denotes activation functions.

~Equation \ref{loss_SD} defines the  loss for DD branch.~Similar to SG and CG branches, we only consider pixels with valid depth values in ground truth for loss calculation.


\subsubsection{Multi-modal Depth Fusion}
~Similar to earlier approaches \cite{hu2020PENet, vangansbeke2019sparse}, we fuse the predicted depth maps of each branch by using learned confidence weights.~Equation \ref{equation_fusion} shows the mathematical representation of our depth map fusion strategy.
 

Where  denotes the fused depth map, ,  and  represents the depth maps predicted by CG, SG and DG branches and , ,  denotes the learned confidence maps of CG, SG and DG predicted depth maps.

Following the CG, SG and DG branches, we also calculate the loss on fused depth map by taking  norm between the fused depth map and ground truth as depicted in Equation \ref{loss_DF}.~We only consider pixels with valid depth values for loss calculation of fused depth map. 


\subsubsection{Loss Function}
The training loss of our three-branch backbone consists of the sum of CD, SD, DD and fused depth losses.~Equation \ref{loss_sum} defines the training loss of our three-branch backbone.


Where  denotes the weights for color-guided, semantic-guided and depth-guided branches.




\subsection{CSPN++ with Atrous Convolutions}

The reason for applying CSPN++ for refining the dense depth maps produced by our three-branch backbone is two folds.~First, it recovers depth values at valid pixels.~Second, enabling a smooth transition between the neighbours of our completed dense depth map.~Furthermore, inspired by PENet \cite{hu2020PENet}, we use Atrous convolutions \cite{yu2016multiscale} to enlarge the propagation neighborhood of pixels \cite{xu2020deformable}.

Given a fused depth map , it is embedded into a hidden representation , the one step propagation of CSPN++ with Atrous convolution is given in Equation \ref{cspn++_1}.



Where  denotes the neighborhood pixels of ,  represents the affinity values produced by the network.~During propagation, pixel  receives information from enlarged neighbourhood . 


\section{Experiments}

    \begin{table*}
        \centering
        \begin{tabular}{cccc}
\includegraphics[width=11em]{Images/rgb_full.jpg} & 
            \includegraphics[width=11em]{Images/penet_full.jpg} & 
            \includegraphics[width=11em]{Images/rignet_full.jpg} &             \includegraphics[width=11em]{Images/ours_full.jpg} \\

            
\\
            (a) RGB Image & (b) PENet & (c) RigNet & (d) SemAttNet(ours)\\
\end{tabular}
        \captionof{figure} {Qualitative results on KITTI depth completion test set.~The results are obtained by online KITTI depth completion leaderboard.~From left-to-right (a) RGB Image, (b) PENet \cite{hu2020PENet}, (c) RigNet \cite{yan2021rignet}, (d) SemAttNet(ours).~The correct estimates in errors maps are given in blue color, whereas wrong estimate are depicted by warmer colors.  } \label{qualitative_results}
\end{table*}

\begin{table}


\begin{tabular}{l|cccc}
\hline
Method & \makecell{RMSE \\ mm} & \makecell{MAE \\ mm} & \makecell{iRMSE \\ 1/km} & \makecell{iMAE \\ 1/km}\\
\hline
TWISE \cite{imran2021depth} & 840.20 & \textbf{195.58} & 2.08 & \textbf{0.82}  \\
DSPN \cite{dspn} & 766.74 & 220.36 & 2.47 & 1.03  \\
DLiDAR \cite{Qiu_2019_CVPR} & 758.38 & 226.50 & 2.56 & 1.15  \\
FuseNet \cite{chen2019learning} & 752.88 & 221.19 & 2.34 & 1.14  \\
ACMNet \cite{zhao2021adaptive} & 744.91 & 206.09 & 2.08 & 0.90  \\
CSPN++ \cite{cheng2020cspn++} & 743.69 & 209.28 & 2.07 & 0.90  \\
NLSPN \cite{park2020non} & 741.68 & 199.59 & \textbf{1.99} & 0.84  \\
GuideNet \cite{tang2020learning} & 736.24 & 218.83 & 2.25 & 0.99  \\
FCFRNet \cite{liu2021fcfr} & 735.81 & 217.15 & 2.20 & 0.98  \\
PENet \cite{hu2020PENet} & 730.08 & 210.55 & 2.17 & 0.94  \\
RigNet \cite{yan2021rignet} & 713.44 & 204.55 & 2.16 & 0.92  \\
\hline
\textbf{SemAttNet} & \textbf{709.41} & 205.49 & 2.03 & 0.90  \\
\hline
\end{tabular}
\caption{Quantitative comparison with state-of-the-art results on KITTI depth completion benchmark, ranked by \textbf{RMSE}.~The results of other methods are obtained from KITTI leaderboard.~Our method i.e. SemAttNet achieves state-of-the-art performance at the time of submission.}
\label{table1}
\vspace{-11pt}
\end{table}
\subsection{Dataset}

We evaluate the performance of our method against different state-of-the-art (SoTA) methods on KITTI depth completion benchmark \cite{KITTI, kitti_2}.~The KITTI dataset is a large outdoor dataset for autonomous vehicles.~It provides 85K sparse depth maps with corresponding RGB images for training, 7K for validation, and 1K for testing.~The sparse depth map in the KITTI dataset is generated by using LiDAR HDL-64, which provides valid depth values on only  of all pixels \cite{KITTI, kitti_2}.~However, the ground-truth contains valid depth values on  of all the pixels.~The ground-truth is generated by accumulating LiDAR and stereo estimation of the scenes \cite{KITTI, kitti_2}.~For all of our experiments, we use the official validation set, which consists of 1K frames  \cite{KITTI, kitti_2}.~During training, we bottom crop the semantic images, RGB images, and sparse depth map from  to . 

\subsection{Evaluation Metrics}
We use the official evaluation metrics of the KITTI depth completion benchmark \cite{KITTI, kitti_2} to evaluate the performance of our approach.~The official evaluation measures consist of root mean squared error (RMSE), mean absolute error (MAE), root mean squared error of inverse depth (iRMSE), and mean absolute error of the inverse depth (iMAE).~Among the evaluation metrics, RMSE is chosen to rank the submissions on the KITTI leaderboard. 
\begin{table*}

\begin{center}

\begin{tabular}{c|cc|ccc|c|cccc}

\hline
\multirow{2}{*}{Methods} & 
\multicolumn{2}{c|}{ Branches} & 
\multicolumn{3}{c|}{ Fusion} &
\multirow{2}{*}{\makecell{CSPN++ \\ Refinement}} & 
\multirow{2}{*}{ \makecell{RMSE \\ mm}} 
\\
\cline{2-6}&  CG + DG &  CG + SG + DG  &  add & concat & SAMMAFB & & \\
\hline

(a)  & \checkmark & & \checkmark &  &     &   &  782.89    \\
(b)  & \checkmark & & & \checkmark &     &   &  781.66     \\
(c)  & \checkmark & & & \checkmark &     &  \checkmark &  762.84    \\
\hline
(d)  &  & \checkmark &   & \checkmark &     &   &  755.16     \\
(e)  &  & \checkmark &   & \checkmark &     &  \checkmark & 750.06     \\

(f)  &  & \checkmark &   &  & \checkmark    &   &  753.02    \\
(g)  &  & \checkmark &   &  & \checkmark    &  \checkmark & \textbf{738.13}     \\
\hline

\end{tabular}

\end{center}

\caption{Ablation study on KITTI depth completion selected validation dataset.~CG, SG and DG stands for color-guided, semantic-guided and depth-guided branches and concat, add refers to simple concatenation and addition of multi-modal features.}
\label{table2}
\end{table*}

\subsection{Implementation Details}


We used PyTorch \cite{PyTorch} framework to implement the three-branch backbone and CSPN++ with Atrous convolutions.~The three-branch backbone is trained on two NVIDIA A100 GPUs, whereas the full model is trained on three NVIDIA RTXA6000 GPUs.~Both models use same setting of ADAM optimizer \cite{kingma2017adam} with  of  0.9,  of 0.99 and weight decay of .~For efficient learning, we performed data prepossessing methods including random crop, flip and color jitter \cite{ma2018sparse} on the dataset.~The random crops are performed at  resolution. 

 The three-branch backbone is trained with a batch size of 8 and an initial learning rate of 0.00128. The initial learning rate is decayed using ReduceLROnPlateu scheduler.~Furthermore, we assign an initial weight of 0.2 to  ,  and  coefficients, but we decay the weight to 0 after few epochs. We train the backbone for 60 epochs. To train the CSPN++ with Atrous convolutions, similar to PENet \cite{hu2020PENet}, we first perform the initialization step and then fully train the models for 95 epochs.

\subsection{Evaluation on KITTI dataset}
Table \ref{table1} shows the quantitative comparison of our method with other state-of-the-art methods.~Our proposed method, SemAttNet, outperforms the previous state-of-the-art methods under the primary evaluation metric RMSE at the time of submission, and presents comparable performance on other evaluation metrics.~Specifically, in contrast to previous state-of-the-arts, i.e., RigNet and PENet, we observe a difference of  and  to the performance of SemAttNet.~Moreover from the qualitative comparison of our method given in Figure \ref{qualitative_results}, it's evident that our method produces much more precise results than other methods in the areas affected by optical changes in RGB images.~Please refer to supplementary section for additional qualitative results.

\subsection{Ablation Studies}
In this section, we investigate the impact of each component proposed by our approach on final performance.~All of the ablation studies are performed on selected KITTI validation dataset.~Table \ref{table2} depicts the results of our experiments with different settings.~Note, that we consider RMSE as our primary metric to rank the performance of methods.

\subsubsection{Effect of exploiting Semantic information.}


From methods (b) and (d) listed in Table \ref{table2}, we can observe that by using the proposed semantic-guided branch even with naive concatenation method for fusion, there is a significant improvement of  in the RMSE metric.~This shows the promise and importance of our proposed SG branch. 

\subsubsection{Effect of SAMMAFB}


The methods (d) and (e) in Table \ref{table2} list the performance of the naive fusion method, and SAMMAFB.~It is quite evident that SAMMAFB, i.e., method (d), outperforms the naive fusion method by achieving  less error than method (f).~Moreover, if we consider methods (f) and (g) which employs CSPN++ refinement along with naive and SAMMAFB fusion methods, we see a significant improvement of  of RMSE in the method utilizing the SAMMAFB for fusion.~These improvement backs our claim that SAMMAFB is a better fusion strategy than naive fusion methods.
\subsubsection{Effect of CSPN++ refinement}



Inspired by original CSPN++ \cite{cheng2020cspn++}, we also use 12 iterations for neighborhood propagation.~However, similar to PENet \cite{hu2020PENet}, we apply a dilation strategy to the original CSPN++ in which for the first 6 iterations, we use a dilation rate of 2, and then we decrease it to 1 for remaining iterations to propagate the neighborhood of each pixel.~From Table \ref{table2}, the method (g) that employs CSPN++ refinement with the dilation strategy, outperforms method (f) with a significant margin of  in RMSE metric.~This proves that CSPN++ is an effective method for dense depth refinement.

\section{Conclusion}
In this paper, we presented a novel three-branch backbone consisting of color-guided, semantic-guided, and depth-guided branches for dense depth completion.~Existing image-guided methods focus only on RGB images to extract both color and semantic cues of the scene. However, RGB images suffer from artifacts such as varying contrast and shadows. To tackle this problem, in our three-branch backbone, we introduced a novel semantic-guided branch, which, irrespective of any optical changes in RGB images, learns to understand the semantics of the scene and reduces the reliance on RGB images.~Moreover, instead of fusing features naively through addition or concatenation, we also utilized semantic-aware multi-model fusion block (SAMMAFB) for feature fusion between all branches.~In addition, we also implement CSPN++ with Atrous convolutions to further refine the dense depth maps.~Extensive experiments show that our results are superior both quantitatively and qualitatively compared to previous state-of-the-art methods.


{\small
\bibliographystyle{ieeetr}
\bibliography{access}
}


\EOD

\end{document}
