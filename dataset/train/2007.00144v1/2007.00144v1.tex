


\documentclass{article}

\usepackage{microtype, subfigure, verbatim}
\usepackage{booktabs, hyperref}
\usepackage{amssymb, amsthm, graphicx, amsmath,multirow}
\usepackage{algorithm, algorithmic, wrapfig}
\usepackage[inline]{enumitem}
\usepackage[small]{caption}

\usepackage[accepted]{icml2020}

\def\x{{\mathbf x}} \def\X{{\mathbf X}} 
\def\y{{\mathbf y}} \def\z{{\mathbf z}} \def\by{{\bar{\mathbf y}}} \def\bT{{\bar{T}}} 
\def\ty{{\mathbf y^*}} \def\Y{{\mathbf Y}} \def\tt{{\tilde t}}
\def\L{{\cal L}} \def\D{{\cal D}}
\def\p{{\mathbf p}} \def\hp{{\hat{\mathbf p}}} \def\l{{\ell}}
\def\N{{\mathcal N}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}



\icmltitlerunning{Sequential Self Teaching for Learning Sounds}

\begin{document}

\twocolumn[
\icmltitle{A Sequential Self Teaching Approach for \\Improving Generalization in Sound Event Recognition}

\begin{icmlauthorlist}
\icmlauthor{Anurag Kumar}{frl}
\icmlauthor{Vamsi  Krishna Ithapu}{frl}
\end{icmlauthorlist}

\icmlaffiliation{frl}{Facebook Reality Labs, Redmond, USA}
\icmlcorrespondingauthor{Anurag Kumar}{anuragkr@fb.com}


\icmlkeywords{Sequential Learning, Noisy Labels, Audio Event Detection}

\vskip 0.3in
]
\printAffiliationsAndNotice{}



\begin{abstract}
An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learning sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data,  and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9\% improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks. 
\end{abstract}



\section{Introduction} 
\label{sec:intro}

Human interaction with the environment is driven by multi-sensory perception. 
Sounds and sound events, natural or otherwise, play a vital role in this first person interaction. 
To that end, it is  imperative that we build acoustically intelligent devices and systems which can {\it recognize } and {\it understand} sounds. 
Although this aspect has been identified to an extent, 
and the field of Sound Event Recognition and detection (SER) is at least a couple of decades old~\cite{xiong2003audio,atrey2006audio}, 
much of the progress has been in the last few years~\cite{virtanen2018computational}. 
Similar to related research domains in machine perception, like speech recognition, 
most of the early works in SER were fully supervised and driven by \emph{strongly labeled data}. 
Here, audio recordings were carefully (and meticulously) annotated with time stamps of sound events to produce exemplars.
These exemplars then drive the training modules in supervised learning methods. 
Clearly, obtaining well annotated strongly labeled data is prohibitively expensive and cannot be scaled in practice. 
Hence, much of the recent progress on SER has focused on efficiently leveraging \emph{weakly labeled} data \cite{kumar2016audio}. 

Weakly labeled audio recordings are only tagged with presence or absence of sounds (i.e.,  a binary label), 
and no temporal information about the event is provided. 
Although this has played a crucial role in scaling SER, 
large scale learning of sounds remains a challenging and open problem. 
This is mainly because, even in the presence of strong labels, large scale SER brings adverse learning conditions into the picture, 
either implicitly by design or explicitly because of the sheer number and variety of classes. 
This becomes more critical when we replace strong labels with weak labels.  
Tagging ({\it a.k.a.} weak labeling) very large number of sound categories in large number of recordings, 
often leads to considerable label noise in the training data. This is expected. 
Implicit noise via human annotation errors is clearly one of the primary factors contributing to this.
\emph{Audioset}~\cite{gemmeke2017audio}, currently the largest sound event dataset, suffers from this implicit label noise issue.
Correcting for this implicit noise is naturally very expensive (one has to perform multiple re-labeling of the same dataset). 
Beyond this, there are more nuanced noise inducing attributes, which are outcomes of the number and variance of the classes themselves. 
For instance, real world sound events often overlap and as we increase the sound vocabulary and audio data size, 
the ``mass'' of overlapping audio in the training data can become large enough to start affecting the learning process. 
This is trickier to address in weakly labeled data where temporal locations of events are not available. 

Lastly, when working with large real world datasets, one cannot avoid the noise in the inputs themselves.
For SER these manifest either via signal corruption in the audio snippets themselves (i.e., acoustic noise), 
or signals from non-target sound events, both of which will interfere in the learning process. 
In weakly labeled setting, by definition, this noise level would be high, presenting harsher learning space for networks. 
We need efficient SER methods that are sufficiently robust to the above three adverse learning conditions. 
In this work, we present an interesting take on large scale weakly supervised learning for sound events.
Although we focus on SER in this work, we expect that 
the proposed framework is applicable for any supervised learning task. 


The main idea behind our proposed framework is motivated by the attributes of human learning, 
and how humans adapt and learn when solving new tasks. 
An important characteristic of human's ability to learn is that it is {\it not} a one-shot learning process, 
i.e., in general, we do not learn to solve a task in the first attempt. 
Our learning typically involves multiple stages of development where past experiences, and past failures or successes, 
``guide'' the learning process at any given time. 
This idea of sequential learning in humans wherein each stage of learning is guided by previous stage(s) was referred to as \emph{sequence of teaching selves} in \cite{minsky1994society}. 
Our proposal follows this meta principle of sequential learning, and at the core, it involves the concept of learning over time. 
Observe that this learning over time is rather different from, for instance, 
learning over iterations or epochs in stochastic gradients, and we make this distinction clear as we present our model. 
We also note that the notion of lifelong learning in humans, which has inspired \emph{lifelong machine learning}~\cite{silver2013lifelong,parisi2019continual}, is also, in principle, related to our framework. 

Our proposed framework is called SeqUential Self TeAchINg (SUSTAIN). 
We train a sequence of neural networks (designed for weakly labeled audio data) wherein 
the network at the current stage is guided by trained network(s) from the previous stage(s). 
The guidance from networks in previous stages comes in the form of ``co-supervision''; 
i.e., the current stage network is trained using a \emph{convex combination} of ground truth labels and the outputs from one or more networks from the previous stages. 
Clearly, this leads to a cascade of {\it teacher-student} networks. 
The student network trained in the current stage will become a teacher in the future stages. 
We note that this is also related to the recent work on knowledge distillation through teacher-student frameworks \cite{hinton2015distilling, ba2014deep, bucilua2006model}. 
However, unlike these, our aim is not to construct a smaller, compressed, model that emulates the performance of high capacity teacher. 
Instead, our SUSTAIN framework's goal is to simply utilize the teacher`s knowledge better. 

Specifically, the student network tries to correct the mistakes of the teachers, 
and this happens over multiple sequential stages of training and co-supervision with the aim of building better models as time progresses. 
We show that one can quantify the performance improvement, 
by explicitly controlling the transfer of knowledge from teacher to student over successive stages.

The {\bf contributions} of this work include: 
\begin{enumerate*}[label=\textbf{(\alph*)}]
\item A sequential self-teaching framework based on co-supervision for improving learning over time, 
including few technical results characterizing the limits of this improved learnability;  
\item A novel CNN for large scale weakly labeled SER, and
\item Extensive evaluations of the framework showing up to  performance 
improvement on Audioset,  significantly outperforming existing procedures, and applicability to knowledge transfer. 
\end{enumerate*}

The rest of the paper is organized as follows. We discuss some related work in Section \ref{sec:rwork}.  In Section \ref{sec:sustain}, we introduce the sequential self-teaching framework and then discuss few technical results. 
In Section \ref{sec:weakcnn}, we describe our novel CNN architecture for SER which learns from weakly labeled audio data. 
Sections \ref{sec:expt} and \ref{sec:transfer} show our experimental results, and we conclude in Section \ref{sec:conc}. 



\section{Related Work} \label{sec:rwork}

While earlier works on SER were primarily small scale~\cite{couvreur1998automatic}, large scale SER has received considerable attention in the last few years. 
The possibility of learning from weakly labeled data~\cite{kumar2016audio, su2017weakly} is the primary driver here, 
including availability of large scale weakly labeled datasets later on, like Audioset ~\cite{gemmeke2017audio}. 
Several methods have been proposed for weakly labeled SER; \cite{kumar2017knowledge,kong2019weakly,chou2018learning,mcfee2018adaptive,yu2018multi,wang2018comparing,adavanne2017sound} to name a few. 
Most of these works employ deep convolutional neural networks (CNN). 
The inputs to CNNs are often time-frequency representations such as spectrograms, logmel spectrograms, constant-q spectrograms~\cite{zhang2015robust,kumar2017knowledge,ye2015acoustic}. 
Specifically, with respect to Audioset, some prior works, for example ~\cite{kong2019weakly}, have used features from a pre-trained network, trained on a massive amount of YouTube data~\cite{hershey2017cnn} for instance. 


The weak label component of the learning process was earlier handled via  or  global pooling~\cite{su2017weakly,kumar2017knowledge}. 
Recently, several authors proposed to use attention~\cite{kong2019weakly,wang2018comparing,chen2018class}, recurrent neural networks~\cite{adavanne2017sound}, adaptive pooling~\cite{mcfee2018adaptive}. 
Some works have tried to understand adverse learning conditions in weakly supervised learning of sounds~\cite{shah2018closer, kumar2019learning}, although it still is an open problem. 
Recently, problems related to learning from noisy labels have been included in the annual DCASE challenge on sound event classification~\cite{fonseca2019audio} \footnote{http://dcase.community/challenge2019/}. 

Sequential learning, and more generally, learning over time, 
is being actively studied recently~\cite{parisi2019continual}, 
starting from the seminal work~\cite{minsky1994society}. 
Building cascades of models has also been tied to lifelong learning~\cite{silver2013lifelong,ruvolo2013ella}.
Further, several authors have looked at the teacher-student paradigm in a variety of contexts including 
knowledge distillation~\cite{hinton2015distilling,furlanello2018born,chen2017learning,mirzadeh2019improved}, compression~\cite{polino2018model} and transfer learning~\cite{Yim_2017_CVPR,weinshall2018curriculum}. \cite{furlanello2018born} in particular show that it is possible to sequentially distill knowledge from neural networks and improve performance. Our work builds on top of \cite{kumar2020secost}, and proposes to learn a sequence of self-teacher(s) to improve generalizability  in adverse learning conditions. This is done by co-supervising the network training along with available labels and controlling the knowledge transfer from the teacher to the student. 



\section{Sequential Self-Teaching (SUSTAIN)} \label{sec:sustain}

\subsection{SUSTAIN Framework} \label{sec:framework}

\paragraph{Notation} \label{para:notation}: 
Let  () denote the dataset we want to learn with  training pairs.  are the inputs to the learning algorithms and   are the desired outputs.  is the number of classes.
 indicates the presence of  class in the input . Note that  are the observed labels and may have noise. 

For the rest of the paper, we restrict ourselves to the binary cross-entropy loss function. However, in general, the method is applicable to other loss functions as well, such as mean squared error loss. 

If  is the predicted output, then the loss is




With this notation, we will now formalize the ideas motivated in Section \ref{sec:intro}.
The learning process entails  stages indexed by . 
The goal is to train a cascade of learning models denoted by  at each stage. 
The final model of interest is . 
Zeroth stage serves as an {\it initialization} for this cascade. 
It is the default teacher that learns from the available labels . Once  is trained, we can get the predictions  (note the  here). 

The learning in each of the later stages is co-supervised by the already trained network(s) from previous stages, 
i.e., at  stage,  guide the training of . 
This guidance is done via replacing the original labels () with a convex combination of the predictions from the teacher network(s) and , which will be the new targets for training . 
In the most general case, if all networks from previous stages are used for teaching, the new target at  stage is, 



More practically, the network from only last stage will be used, in which case, 

or the students from previous  stages will co-supervise the learning at stage , which will lead to ,  . 

Algorithm \ref{alg:secost} summarizes this self teaching approach driven by co-supervision with single teacher per stage. It is easy to extend it to  teachers per stage, driven by appropriately chosen `s. 

\setlength{\textfloatsep}{5pt}
\begin{algorithm}[t!]
	\caption{\textbf{SUSTAIN}: Single Teacher Per Stage}
	\begin{algorithmic}[1]
		\REQUIRE: , stages , \{,  \} \\
		\ENSURE: Trained Network  after  stages
		\STATE Train default teacher  using   
		\FOR{}
		\STATE Compute new target  () using Eq. \ref{eq:new-labels-1-teacher}\STATE Train  using new target   
		\ENDFOR
		\STATE Return 
	\end{algorithmic}
	\label{alg:secost}
\end{algorithm}



\subsection{Analyzing SUSTAIN w.r.t to label noise} \label{sec:analysis}

In this section, we provide some insights into our SUSTAIN method with respect to label noise, a common problem in large scale learning of sound events.  denote our noisy observed labels. Let  be the corresponding true label parameterized as follows,
  
Within the context of learning sounds, in the simplest case,  characterizes the per-class noise in labeling process. Nevertheless, depending on the nature of the labels themselves, it may represent something more general like sensor noise, overlapping speakers and sounds etc. 

To analyze our approach and to derive some technical guarantees on performance, we assume a trained default teacher  and a new student to be learned (i.e., ). The new training targets in this case are given by


Recall from Eq. \ref{eq:true-labels} that  parameterizes the error in  vs. the unknown truth .
Similarly, we define  to parameterize the error in  vs.  i.e., noise in teacher's predictions w.r.t the true unobserved labels. 

  
The interplay between  and  in tandem with the performance accuracy of  will help us evaluate the gain in performance for  versus . 
To theoretically assess this performance gain, we consider the case of uniform noise  followed by a commentary on class-dependent noise. 
Further, we explicitly focus the technical results on high noise setting and revisit the low-to-medium noise setup in evaluations in Section \ref{sec:expt}. 

\subsubsection{Uniform Noise: } \label{sec:same-noise}

This is the simpler setting where the apriori noise in classes is uniform across all categories with . We have the following result. 

 
\begin{proposition} \label{prop:delta-bar}
  Let  be trained using  using binary cross-entropy loss, and let  denote the average accuracy of  for class .
  Then, we have
  
  and whenever ,  improves performance over . The per class performance gain is  
\end{proposition}
\begin{proof}
Recall the entropy loss from Eq. \ref{eq:crossent}, for a given  and . 
Using the definition of the new label from Eq. \ref{eq:new-label-1-stage}, we get the following 

Now, Eq. \ref{eq:true-labels} says that {\it w.p.}  (recall  here), , else . 
Hence, using Eq. \ref{eq:true-labels} and Eq. \ref{eq:one-teacher-delta-bar-def}, and using the resulting equations in Eq. \ref{eq:prop-1} we have the following

If  then we can ensure that using  as targets is better than using . 
Now given the accuracy of  denoted by , combining Eq. \ref{eq:true-labels} and Eq. \ref{eq:one-teacher-delta-bar-def}, we can see that . Using this, for  to be better than ,  we need

which requires . And the gain is simply  which reduces to . 
\end{proof}

\subsubsection{Remarks} \label{sec:remarks}

The above proposition is fairly intuitive and summarizes a core aspect of the proposed framework. 
Observe that, Proposition \ref{prop:delta-bar} is rather conservative in the sense that we are claiming  is better than  only if Eq. \ref{eq:prop-2} holds for all classes, 
i.e., performance improves for all classes. 
This may be relaxed, and we may care more about some specific classes. 
We discuss this below, for the high and low noise scenarios separately. 

\noindent \paragraph{High noise } \label{sec:high-noise}
The given labels  are wrong more than half of the time, 
and with such high noise, we expect  to have high error i.e.,  and  do not match.
Putting these together, as Proposition \ref{prop:delta-bar} suggests, 
the probability that  matches the truth  is implicitly large, 
leading to . 
Note that we cannot just flip {\bf all} predictions i.e.,  would be infeasible, 
and there is some trade-off between 's predictions and given labels. 
Thereby, the choice of  then becomes critical (which we discuss further in Section \ref{sec:alpha-vs-T}).
Beyond this interpretation, we show extensive results in Section \ref{sec:expt} supporting this. 

\paragraph{Low-to-medium noise }  \label{sec:low-noise}
When ,  is expected to perform well, 
and  matches , which in turn matches  since the noise is low. 
Hence, 's role of combining 's output with  becomes rather moot, because on average, for most cases, they are same. 
For medium noise settings with , proposition \ref{prop:delta-bar} does not infer anything specific. 
Nevertheless, via extensive set of experiments, we show in section \ref{sec:expt} that  still improves over  in some cases. 

\noindent \paragraph{Class-Specific Noise: } \label{sec:different-noise}
It is reasonable to assume that in practice there are specific classes of interest that we desire to be 
more accurately predictable than others, including the fact that annotation is more carefully done for such classes. 
One can generalize Proposition \ref{prop:delta-bar} for this class-dependent s, 
by putting some reasonable lower bound on loss of accuracy for undesired classes s. 
We leave such technical details to a follow-up work, and now address the issue of choosing s for learning.  

\subsubsection{Interplay of  and } \label{sec:alpha-vs-T}

Recall that the main hyperparameters of SUSTAIN are the weights  and , 
and the main unknowns are the noise levels in the dataset ().
We now suggest that Algorithm \ref{alg:secost} is implicitly robust to these unknowns and provides an empirical strategy to choose the hyperparameters as well. 
We have the following result focusing on a given class .  and  denote the optimal number of stages per class  and across all classes respectively. The proof is in supplement. 

\begin{corollary} \label{cor:optimal-T}
Let  denote the accuracy of  for class . 
Given some , there exists an optimal 
such that . 
\end{corollary}

\noindent \paragraph{Remarks.} 
The main observation here is that  might be very different for each , 
and it may be possible that  in certain cases, i.e., the teacher is already better than any student. 
In principle, there may exist an optimal  that is class independent for the given dataset, 
but it is rather hard to comment about its behaviour in general without explicitly accounting for the individual class-specific accuracies s.
This is simply because correcting for noisy labels in one class may have the outcome of corrupting another class. 
Lastly, it should be apparent that the gain in performance per class  has diminishing returns as  increases. 

\noindent \paragraph{Choosing  and s:} \label{sec:choose-bt-alphas}
Corollary \ref{cor:optimal-T} is an existence result and does not give us a procedure to compute s (or ) and the corresponding s. In practice, there is a simple strategy one can follow. 
At stage , we train  and we record its average across-class performance .
At stage , we empirically select the best  that results in maximal . 
If , then we stop and declare  i.e., no student needed. 
On the other hand, if , then we continue to stage . And repeat this process until the accuracy  saturates or starts to decrease. 
This averages out the per-class influence on . 




\section{CNN for Weakly Labeled SER} \label{sec:weakcnn}

We now evaluate Algorithm \ref{alg:secost} for weakly labeled SER, as motivated in Section \ref{sec:intro}. 
We first propose a novel architecture for the problem and then study SUSTAIN using this network. 
Observe that most of the existing approaches to SER are variants of Multiple Instance Learning (MIL)~\cite{dietterich1997solving}, 
the first proposed framework being \cite{kumar2016audio}. 

Our key novelty is to include a class-specific ``attention'' learning mechanism within the MIL framework. 
We introduce some brief notation followed by presenting the model.  
In MIL, the training data  is made available via \emph{Bags} , 
with each bag corresponding to a collection of  training instances . 
Each  has one label vector . 
 for class  if {\it at least} one of the  instances is positive, otherwise . 

The key idea in MIL is that the learner first predicts on instances, 
and then maps (accumulates) these instance-level predictions to a bag-level prediction. 
For instance, a widely used SVM based MIL~\cite{andrews2002support} uses this principle, 
using  operator as the mapping function. 
Based on similar principle, we formulate the learning process as follows: 

, parameterized by , is the learner and does the instance level prediction of outputs, 
and  maps these  to bag level predictions. 
For weakly labeled SER, s are full audio recordings and instances are short duration segments of the recordings. 
We design a CNN which takes in Log-scaled Melfilter-bank feature representations of the entire audio recording, 
produces instance (i.e., segment) level predictions which are then mapped to recording level predictions. 

The inputs are computed as follows:  Mel-filter-bank is obtained for each ms window of audio, and the window moves by ms, 
leading to  Logmel frames per second of audio (with a sampling rate of KHz for the audio recordings). 

\begin{table}[t]
  \centering
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
    \textbf{Stage}              & \textbf{Layers}                                                           &  \textbf{Output Size}   \\
    \midrule
    Input                       & Unless specified -- (S)tride = 1, (P)adding = 1 &  \\
    \midrule
    \multirow{3}{*}{Block B1}   & Conv: 64,       &  \\
                                & Conv: 64,      & \\
                                & Pool:  (S:4)                                                  &  \\
    \midrule
	\multirow{3}{*}{Block B2}   & Conv: 128,      &  \\
                                & Conv: 128,      &  \\
                                & Pool:  (S:2)                                                  &  \\ 
    \midrule
	\multirow{3}{*}{Block B3}   & Conv: 256,     &  \\
                                & Conv: 256,      &  \\
                                & Pool:  (S:2)                                                  &  \\       
    \midrule
    \multirow{3}{*}{Block B4}   & Conv: 512,      &  \\
                                & Conv: 512,      &  \\
                                & Pool:  (S:2)                                                  &  \\ 
    \midrule
    Block B5                    & Conv: 2048,  (P:0)   &  \\
	\midrule 
	Block B6                    & Conv: 1024,    &  \\
	\midrule
	Block B7                   & Conv: 1024,  &  \\
	\midrule
	Block B8                    & Conv: C,   &  \\
	\midrule
		                        &  ()       &   \\
	\bottomrule      
    \end{tabular}
    }
  \caption{WEANET: All convolutional layers (except B8) are followed by batch norm and ReLU; Sigmoid activation follows B8.}\label{tab:cnnarch}
\end{table}

The proposed architecture, referred to as WEakly labeled Attention NETwork (WEANET), is shown in Table \ref{tab:cnnarch}. 
Example output sizes for an input with  Logmel frames (approx. 10 seconds long audio) is shown in Table \ref{tab:cnnarch}. 
The first few convolutional layers (B1 to B5) produce  dimensional bag representations for the input at Block B5. 
B6-B8 are  convolutional layers that produce instance level predictions of size ,  is number of classes and  is number of segments obtained for a given input. 
The network is designed such that the receptive field of each segment (i.e., instance) is  second ( frames), and the segments themselves move by  seconds ( frames). 
The instance level predictions are then used to produce bag (i.e., recording) level predictions using . 
An easy parameter free way of doing this is to use  (or ) functions which will simply take average (or maximum) over segment level predictions from B8. 

Instead, we propose an attention mechanism here which aims to appropriately weigh each segment's contribution in the final recording level prediction. 
Moreover, this is done in a class-specific manner as different sounds might be located at different places in the recording. 
More formally,  is parameterized as follows: 

where .  denotes the segment level predictions. 
 is the softmax function applied across segments, 
and  gives us the attention weights for each segment and class.
 is element wise multiplication and  is  column of , 
which represents the {\it weighted} predictions for each class in  segment. 
All these are then pooled into , which represents the recording level prediction for the input.  
 is learned along with rest of the parameters of the WEANET. Note that, the size of attention parameter  is independent of the number of segments obtained for an input or in other words the duration of the input. It depends on the number of classes in the dataset. 



\section{Experiments and Results} \label{sec:expt}

\subsection{Datasets and Experimental Setup}

\noindent {\bf Audioset:}~\cite{gemmeke2017audio} \label{sec:audioset} is very challenging dataset in terms of adverse learning conditions outlined in Section \ref{sec:intro}. It is the largest dataset for sound events with weakly labeled YouTube clips for  sound classes. 
Each recording is  seconds long and on an average, there are  labels per recording. 
The training and evaluation sets consist of  million and  recordings respectively. 
The dataset is highly unbalanced with  the number of training examples varying from close to  million for classes such as \emph{Music} and \emph{Speech} to  for classes such as \emph{Screech} and \emph{Toothbrush}. 
The evaluation set has at least  examples for each class. 
A sample of  videos from the training set are sampled out for validation. 

An analysis of label noise was done by the authors by sampling 10 examples for each class and sending them for expert label reviewing. This puts label noise at broad range of 0 to 80-90\%  across classes. Note however that this is an extremely rough estimate for a dataset of this size. 

\noindent {\bf FSDKaggle:}\label{sec:fsdkaggle} ~\cite{fonseca2019audio} is a dataset of 80 sound events. It has  training sets: a \emph{Curated} set with  recordings and a \emph{Noisy} set with  audio recordings. 
The \emph{Curated} set is a clean training set which has been carefully annotated by humans to ensure minimal to no label noise. 
The \emph{Noisy} training set is obtained from Flickr videos and not labeled by humans. They contain considerable amount of label noise. The evaluation set has  recordings. 
We use the \emph{Public} test set with  recordings for validation. 

~\cite{fonseca2019audio} does a more thorough examination of label noise. The estimated per-class label noise roughly ranges from 20\% to 80\% and overall around  60\% of the labels show some type of label noise. While the \emph{Curated}, validation and test sets are sourced from freesounds.org (and then labeled by humans), 
the \emph{Noisy} training set recordings are sourced from Flickr. This heavy mismatch in domain adds on to the already difficult learning conditions for the \emph{Noisy} training set and leads to considerable impact on performance.  

\noindent {\bf ESC-50:}~\cite{piczak2015esc} \label{sec:esc50}
This dataset consists of  recordings from  sound classes. 
Each sound class has 40 audio recordings and all recordings are  seconds long. 
We use this dataset primarily in our transfer learning experiments in Section \ref{sec:transfer}. 
It comes with  pre-defined sets and we follow the same setup in our experiments as in prior works such as \cite{kumar2017knowledge}. 

\noindent {\bf Experimental Setup:} \label{sec:exp-setup}
All of our experiments uses Pytorch~\cite{paszke2017automatic} for neural network implementations. Adam optimizer is used, and networks are trained for 20 epochs. Minibatch size is set to 144. Hyperparameters such as learning rates and the best model during training is selected using the validation set. The attention weight parameter  is initialized with 's such that the initial attention weights come out to be equal for all segments for all classes, , . The updates for attention weight parameter is turned on from fifth epoch. For Audioset, given its highly unbalanced nature, we use a weighted loss for each class. 
This weight for class  is given by , 
where  is the inverse of the class prior in the training set. The training set up is consistent across all stages of SUSTAIN and only the teacher(s) and the parameter  changes. 

Similar to prior works on Audioset, Average Precision (AP) and Area under ROC curves (AUC) are used to measure performance. 
Mean AP (mAP) and mean AUC over all classes are used as overall metrics for performance assessment. For FSDkaggle dataset, the metric used is a label-weighted label-ranking average precision (lwlrap)~\cite{fonseca2019audio}. 
Given the smaller size of this dataset, we use a lighter version of WEANET. 
The details of this lighter WEANET are provided in the supplementary material. For ESC-50 dataset accuracy is used as the performance metric. 


\subsection{WEANET Model} \label{sec:weanet-eval}

\begin{table}[t!]
  \centering
\begin{tabular}{c|c|c}
    \toprule
	Method & mAP & mAUC \\ \midrule
    \cite{kong2019weakly}-1 & 0.361 & 0.969 \\
	\cite{wang2019comparison}-1  & 0.354 & 0.963 \\ 
	\midrule
	\cite{kong2019weakly}-2   & 0.369 &  0.969 \\
	\cite{wang2019comparison}-2 & 0.362  & 0.965 \\
	\midrule
	WEANET () & 0.352 & 0.970 \\
	WEANET ()  & 0.366  & 0.958 \\
	\bottomrule
    \end{tabular}\caption{Comparison of WEANET with other attention architectures on Audioset dataset.}
  \label{tab:weanet}\end{table}

\begin{figure}[t]
      \centering
      \includegraphics[width=\linewidth]{figures/10379+PlMt7NoIYDU_seg_out.png}
      \includegraphics[width=\linewidth]{figures/10379+PlMt7NoIYDU_atten_wt1.png}
      \includegraphics[width=\linewidth]{figures/10379+PlMt7NoIYDU_atten_wt2.png}
    \caption{An example of WEANET outputs on a recording from test set. \textbf{Top}: Segment level probability outputs for three classes present in the recording. \textbf{Mid}: Segment wise attention weights () for the Breaking Sound. \textbf{Bottom}: Segment wise attention weights () for the Speech Sound. Red line denotes if all segments were given equal weights (1/30).}
    \label{fig:example_out}
\end{figure}

We first provide some results on WEANET. 
Table \ref{tab:weanet} shows performance of WEANET framework and compares it with respect to some other attention frameworks for weakly labeled SER. 
Note that, \cite{kong2019weakly} uses embeddings for audio recordings from a network trained on a very large database (YouTube-70M) \cite{hershey2017cnn}. 
These pre-trained representations lead to enhanced performance on Audioset. 
We (and also \cite{wang2018comparing}) work with the actual audio recordings and use logmel feature representations.  
In summary, WEANET performs better than other attention frameworks. 
\cite{kong2019weakly}-2 performs slightly better but uses pre-trained embeddings as just mentioned. 
WEANET with class-specific attention is  better than WEANET with  as simple average pooling. 

The major advantage of having class-specific attention learning is for localization of events. 
Figure \ref{fig:example_out} shows segment level outputs for  sounds present in a specific recording from the test set. 
Note that we matched with the location of the events in the actual recording. 
The lower two figures show attention weights for the two events (\emph{Breaking} sound and \emph{Speech} sound) that are highly localized in the recording. 
Observe that the weights are much higher than average for segments where the event is actually located. 
For speech in particular, segments  show high probability of presence even though actually speech is not present. 
However, the class-specific attention framework is capable of flagging this false positive and assigns very low weights to them.   

\subsection{SUSTAIN Framework} \label{sec:sustain-eval}

\paragraph{Comparison with state-of-the-art:}
\begin{table}[t!]
  \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
	Method & mAP & mAUC \\
    \toprule
    \cite{kong2019weakly} - Small & 0.361 & 0.969 \\
    \cite{kong2019weakly} - Large   & 0.369 &  0.969 \\
	\midrule 
	\cite{wang2019comparison} - TALNet (exp. pooling)  & 0.362   & 0.965 \\ 
	\cite{wang2019comparison} - TALNeT (Attention) & 0.354  & 0.963 \\
	\midrule
	\cite{ford2019deep} - ResNet-34 (Attention) & 0.360 & 0.966 \\
	\cite{ford2019deep} - ResNet-101 (Attention) & 0.380 & 0.970 \\
	\midrule
	WEANET & 0.366 & 0.958 \\
	\textbf{SUSTAIN - Single Teacher} & \textbf{0.394} & \textbf{0.972} \\
	\textbf{SUSTAIN - 2 Teachers} & \textbf{0.398} & \textbf{0.972} \\
	
	\bottomrule
    \end{tabular}}
    \caption{Comparison with state-of-the-art methods on Audioset}
  \label{tab:sota}\end{table}

Table \ref{tab:sota} compares performance of our SUSTAIN framework with state-of-the-art methods on Audioset. 
``SUSTAIN-Single Teacher" uses  teacher at each stage, specifically the network trained in the previous stage (as in Eq. \ref{eq:new-labels-1-teacher}). 
``SUSTAIN-2 Teachers" uses  networks learned in the previous  consecutive stages as teachers.
SUSTAIN learning leads to superior performance over all prior methods. 
The ResNet based architectures in Table \ref{tab:sota} are much 
larger compared to our WEANET and have several times more parameters. 
Our method outperforms the previous best method (ResNet-50) by . Note that, \cite{ford2019deep} also reports a performance of  but that is obtained through ensemble of models, by averaging outputs of multiple models.  


\begin{figure}[t] 
\centering
 \includegraphics[width=0.8\columnwidth]{figures/1teacher_1stage_vs_alphas.png}
 \caption{\label{fig:vs_alphas} Single teacher:  vs. }
\end{figure}

Focusing primarily on the SUSTAIN learning, we notice that it can lead up to  improvement in results for the WEANET model. Thus, the same architecture WEANET, generalizes much better after a few stages of SUSTAIN learning as opposed to just training it on the available labels. 


\noindent {\bf Single Stage () vs. varying :}
Figure \ref{fig:vs_alphas} shows that  influences mAP, as suggested by in Section \ref{sec:remarks}. The two extremes of  (only using 's predicted labels) and  (only using provided labels  for learning) perform worse than learning using a combination of the two. 
This asserts our primary claim in Proposition \ref{prop:delta-bar}. Depending on the weight () given to the teacher, even a single stage of SUSTAIN can lead to up to 5.7\% improvement in performance. 

\noindent {\bf Multiple Stages () vs. :}
For a fixed , Figure \ref{fig:Nt_alpha}(a) shows that as  increases mAP starts to increase and then quickly saturates, showing evidence for Corollary \ref{cor:optimal-T}. 
Note that this setup corresponds to using the last trained network as teacher (i.e.,  uses  as the teacher).
It is reasonable to expect that  is better than , and so, 
one can put more confidence in the predicted labels of latest teachers than teachers from earlier stages. 
This is validated in Figure \ref{fig:Nt_alpha}(b) where  uses only predicted labels () and as  increases, we reduce , 
putting more confidence on teacher's predictions and achieve better mAP.  Unlike the fixed alpha case, considerable improvement is obtained from stage 1 to 2 and then stage 2 to 3 by increasing the weight given to teacher's predictions. 

\begin{figure}[t!]
      \centering
      \includegraphics[width=0.51\linewidth]{figures/1teacher_multiplestages.png} \hspace{-5mm}
      \includegraphics[width=0.51\linewidth]{figures/1teacher_multiplestages_changingalpha.png}
    \caption{Performance of students as  increases: (a) , (b) decreasing  as  increases.}\label{fig:Nt_alpha}
\end{figure}

\begin{table}[t!]
  \centering
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c}
    \toprule
    Stage(T) & Teach.  & ,  &   & Stud. & Stud. Perf.  \\
    \midrule
    0 & - & - & 1.0 &  & 0.366 \\
    \midrule
    1 & , - & 0.7, - & 0.3 &  & 0.387 \\ 
    \midrule
    2 & ,  & 0.3, 0.5 & 0.2 &  & 0.393 \\
    \midrule
    3 & ,  & 0.4, 0.5 & 0.1 &  & 0.396 \\
   \midrule
   4 & ,  & 0.45, 0.5 & 0.05 &  & 0.398 \\
   \midrule
   5 & ,  & 0.45, 0.53 & 0.03 &  & 0.398 \\
   \bottomrule
    \end{tabular}}
	\caption{ teachers at each stage (weights:  and ).} \label{tab:multipleteach}\end{table}

\noindent {\bf Multiple Stages and Multiple Teachers:}
Table \ref{tab:multipleteach} shows the performance as  increases with  teachers. 
Each row corresponds to one stage and  is as usual the default teacher. 
As expected, the mAP improves as  increases. 
In particular, observe that after  stages we reach  mAP here, compared to the single teacher setup where we get  after  stages (refer to Figure \ref{fig:Nt_alpha}(b)), a  relative improvement. 
We also see the saturation of performance as  increases, further supporting our results from Section \ref{sec:analysis}.

\begin{table}[t!]
  \centering
\begin{tabular}{c|c|c}
    \toprule
	Method & lwlrap & Remarks\\
    \toprule
    \cite{fonseca2019audio} & 0.312 & -\\
	\midrule 
	WEANET - (T = 0) & 0.436 & -\\ 
	\midrule
	SUSTAIN - (T = 1) & 0.454 &  \\
	SUSTAIN - (T = 2) & 0.456 &  \\
	SUSTAIN - (T = 3) & 0.462 &  \\
	SUSTAIN - (T = 4) & 0.470 &  \\
	SUSTAIN - (T = 5) & 0.472 &  \\
	\textbf{SUSTAIN - (T = 6)} & \textbf{0.472} &  \\
	
	\bottomrule
    \end{tabular}\caption{Performance when trained on FSDKaggle-Noisy set. First row is baseline. Last column shows  for each stage. Single Teacher ()  at each  stage of training. }
  \label{tab:fsdnoisy}\end{table}

\subsection{Noisy label vs Clean Label Conditions:}

We now try to specifically look into clean and noisy label learning conditions using FSDKaggle2019 dataset. 
The \emph{Noisy} and \emph{Curated} training sets of this dataset (refer to their descriptions from Section \ref{sec:exp-setup}) are used as noisy (i.e., hard) and clean (i.e., easy) learning conditions. 
The test set remains same for the two cases. 
We use a lighter version of WEANET model (WEANET) for these experiments, the details of which are available in the supplementary material. For all these experiments, only one teacher is used per stage; the network trained in the previous stage. Most prior works on FSDKaggle have relied heavily on different forms of data augmentation on the \emph{Curated} set for improved performance. We do not do any data augmentation and instead focus  on easy and hard conditions. We use the performance reported by the dataset paper, \cite{fonseca2019learning}, on each training set as the baseline. 

Table \ref{tab:fsdnoisy} summarizes the results for \emph{Noisy} training set. We observed that for the \emph{Noisy} training set, the trends of results (for different parameters such as s, stages etc.) are similar to those observed for Audioset.  Overall SUSTAIN leads to around  improvement. Even just  stage of SUSTAIN, leads to almost  improvement in performance over base WEANET (T = 0). The performance saturates after 5 stages of SUSTAIN. 


The \emph{Curated} set presents a different picture though. While one stage of SUSTAIN still leads to small improvement, any further co-supervision leads to deterioration in performance. This is the expected behavior our technical results claimed in Section \ref{sec:analysis} i.e., SUSTAIN learning primarily helps in adverse learning conditions. 



\begin{table}[t!]
  \centering
    \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
	Method & lwlrap & Remarks\\
    \toprule
    Baseline - \cite{fonseca2019audio} & 0.542 & -\\
	\midrule 
	WEANET - (T = 0) & 0.619 & -\\ 
	\midrule
	SUSTAIN - (T = 1) & 0.619 &  \\
	SUSTAIN - (T = 1) & 0.625 &  \\
	SUSTAIN - (T = 1) & 0.632 &  \\
	SUSTAIN - (T = 1) & 0.622 &  \\
	SUSTAIN - (T = 1) & 0.622 &  \\
	\midrule
	SUSTAIN - (T = 2) & 0.624 &  \\
	SUSTAIN - (T = 2) & 0.623 &  \\
	SUSTAIN - (T = 2) & 0.627 &  \\
	SUSTAIN - (T = 2) & 0.624 &  \\
	SUSTAIN - (T = 2) & 0.625 &  \\
	
	\bottomrule
    \end{tabular}}
    \caption{Performance when trained on FSDKaggle-Curated set. Last row last column shows  for each stage. }
  \label{tab:fsdclean}\end{table}


\subsection{Class-specific Performance Gains}
On the Audioset dataset, we observed that for almost  of all the classes (527 total), the performance improved with SUSTAIN learned model  (from Table \ref{tab:multipleteach}), compared to base WEANET model ( from Table \ref{tab:multipleteach}). 
Most classes have under  relative improvement, and  of the classes get  improvement, and this reaches up to  for classes like \emph{Squeal} and \emph{Rattle}. 
Maximum drop in performance (down by ) is observed for \emph{Gurgling} class. 
We also see that low performing classes (AP ) have more improvements in relative sense. 
On average, AP of these classes ( of them) improve by , while classes with high AP (,  in number), we see  gain in performance. Class-specific performance plots are shown in supplementary material. Overall, \emph{Bagpipes} sounds are easiest to recognize and we achieve an AP of 0.931 for it. \emph{Squish} on the other hand is hardest to recognize with an AP of 0.02.


\section{Knowledge Transfer using SUSTAIN} \label{sec:transfer}

In the preceding sections, we showed that the generalizability of a model can be improved through the proposed SUSTAIN learning. We now ask, are the models obtained from SUSTAIN learning more suitable for transfer learning? 
We study whether WEANET obtained after  stages of training () is more suited for transfer learning compared to the one just trained on the available labels.
Since SUSTAIN is not explicitly designed to handle this, the transfer learning question reveals the learnability power of the proposed framework. 

We pick  and  WEANET models from Table \ref{tab:multipleteach} for this analysis, with   being the base model trained only on available labels and  being a SUSTAIN trained model. These WEANET models trained on Audioset are used to obtain representations for the audio recordings in the given target tasks. 
Outputs after Block B5 (refer to WEANET model from Table \ref{tab:weanet}) are used as feature representations for the audio recordings. 
Recall that, Block B5 produces representations for  second long audio every  sec. These segment level representations are simply max-pooled across all segments to get a fixed -dimensional vector for all audio recordings. 

We study these transfer learning tasks on FSDKaggle and ESC-50 datasets. A simple linear classifier is trained on the feature representations obtained for the audio recordings. 

Table \ref{tab:transfer} shows the results for these transfer learning tasks. We see that the representations from SUSTAIN framework leads to significantly improved feature learning for all datasets. For the clean conditions (ESC-50 and FSDKaggle-Curated), we see 1.5-2.2\% improvement whereas for the noisy learning conditions we see up to 3.5\% improvement in performance.  For the ESC-50 dataset, this transfer learning also outperforms previous state-of-the-art results by a considerable margin (2.8\% relative). 

\begin{table}[t!]
  \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cc|c|c}
    \toprule
    \multicolumn{2}{c|}{ESC-50} & \multicolumn{2}{c}{FSDKaggle}\\
    \midrule
    Method & Acc. (\%) & Method & lwlrap \\
    \midrule
    \cite{sailor2017unsupervised} & 86.5 & Noisy, WEANET () & 0.486\\
    \cite{guzhov2020esresnet} & 91.5 & Noisy, WEANET () & 0.503\\
    \cmidrule{3-4}
    WEANET () & 92.6 & Curated, WEANET () & 0.712 \\
    WEANET () & 94.1 & Curated, WEANET () & 0.728\\
	\bottomrule
    \end{tabular}}
    \caption{Transfer Learning from SUSTAIN Models trained on Audioset. Results on ESC-50 and FSDKaggle dataset.}
  \label{tab:transfer}\end{table}



\section{Conclusions} \label{sec:conc}


Designing robust learning models for weakly labelled datasets while also scaling them to large scale and ensuring good generalization is a hard problem, and is an open question. 
We addressed this problem in this paper. We proposed a sequential self-teaching framework that utilizes co-supervision across trained models to improve generalization. 
We specifically show promising results on sound event recognition and detection, in particular in large scale weakly labelled settings. We also proposed a novel architecture for learning sounds which incorporates class-specific attention learning. A better theoretical understanding of the role  and  play in different adverse learning conditions can lead to an enhanced understanding of SUSTAIN. We will explore these directions in future works. 

\bibliography{references}
\bibliographystyle{icml2020}



\clearpage
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\bibnumfmt}[1]{[S#1]}
\renewcommand{\citenumfont}[1]{S#1}
\renewcommand{\thetable}{S\arabic{table}}

\begin{center}
\textbf{\large Supplementary Materials}
\end{center}

\section{Technical Results}











  

\begin{proposition} \label{sprop:delta-bar}
  Let  be trained using  using binary cross-entropy loss, and let  denote the average accuracy of  for class .
  Then, we have
  
  and whenever ,  improves performance over . The per class performance gain is  
\end{proposition}
\begin{proof}
Recall the entropy loss from Eq. \ref{seq:crossent}, for a given  and . 
Using the definition of the new label from Eq. \ref{seq:new-label-1-stage}, we get the following 

Now, Eq. \ref{seq:true-labels} says that {\it w.p.}  (recall  here), , else . 
Hence, using Eq. \ref{seq:true-labels} and Eq. \ref{seq:one-teacher-delta-bar-def}, and using the resulting equations in Eq. \ref{seq:prop-1} we have the following

If  then we can ensure that using  as targets is better than using . 
Now given the accuracy of  denoted by , combining Eq. \ref{seq:true-labels} and Eq. \ref{seq:one-teacher-delta-bar-def}, we can see that . Using this, for  to be better than ,  we need

which requires . And the gain is simply  which reduces to . 
\end{proof}

\begin{corollary} \label{scor:optimal-T}
Let  denote the accuracy of  for class . 
Given some , there exists an optimal 
such that . 
\end{corollary}
\begin{proof}
When , Eq. \ref{seq:prop-2} will not hold, and Proposition \ref{sprop:delta-bar} says that  is worse than . Hence . 
On the other hand, if , then , and the performance improves. 
For the given , one can repeat the analysis for next stages with different values of . 
 is the stage  where the corresponding  increases over . 
\end{proof}

\section{WEANET for FSDKaggle-2019}
\begin{table}[t]
  \centering
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
    \textbf{Stage}              & \textbf{Layers}                                                           &  \textbf{Output Size}   \\
    \midrule
    Input                       & Unless specified -- (S)tride = 1, (P)adding = 1 &  \\
    \midrule
    \multirow{3}{*}{Block B1}   & Conv: 64,       &  \\
                                & Conv: 64,      & \\
                                & Pool:  (S:4)                                                  &  \\
    \midrule
	\multirow{3}{*}{Block B2}   & Conv: 128,      &  \\
                                & Conv: 128,      &  \\
                                & Pool:  (S:2)                                                  &  \\ 
    \midrule
	\multirow{3}{*}{Block B3}   & Conv: 256,     &  \\
                                & Conv: 256,      &  \\
                                & Pool:  (S:2)                                                  &  \\       
    \midrule
    \multirow{3}{*}{Block B4}   & Conv: 256,      &  \\
                                & Conv: 256,      &  \\
                                & Pool:  (S:2)                                                  &  \\ 
    \midrule
    Block B5                    & Conv: 512,  (P:0)   &  \\
	\midrule
	Block B6                    & Conv: C,   &  \\
	\midrule
		                        & Global Average Pooling       &   \\
	\bottomrule      
    \end{tabular}
    }
  \caption{Model architecture for  for FSDKaggle-2019 dataset: All convolutional layers (except B6) are followed by batch norm and ReLU; B6 is followed by sigmoid activation.}\label{tab:cnnarchfsd}
\end{table}

Table \ref{tab:cnnarchfsd} shows the  architecture used for experiments on FSDKaggle-2019 dataset.  is just a lighter version of the one shown in Table 1 in the main paper. To keep things simple, we also use a simpler parameter-free mapping function . We use global average pooling as , which takes an average of segment level outputs to produce recording level output. 


\section{Class-wise performance for Audioset}
\textbf{Figure \ref{fig:classaud}} shows class-wise performance for different sound classes and the improvement obtained from the sequential self-teaching approach.  The blue bar shows performance obtained from base-model (a.k.a default teacher ). The green or red bar shows the change in performance from SUSTAIN model (corresponding to ) in Table 4 from main text. The classes have been sorted by change in performance, with maximum improvement for first bar in top plot and maximum reduction in \emph{Vibraphone} class in right most bar of bottommost plot. 

We see that classes such as \emph{Zing, Moo, Cattle, Owl, Yodeling} (first 5 bars in topmost plot), get an absolute improvement of up to  to  in MAP, leading to  improvement in relative sense. As mentioned in the main text, there are few classes such as \emph{Mouse, Squeal, Rattle} for which performance improves by more than 100\%. Overall, \emph{Bagpipes} sounds are easiest to recognize and we achieve an AP of 0.931 for it. \emph{Squish} on the other hand is hardest to recognize with an AP of 0.02.











\begin{figure*}[t]
      \centering
      \includegraphics[width=0.9\linewidth]{figures/improved_aps1.pdf}
      \includegraphics[width=0.9\linewidth]{figures/improved_aps2.pdf}
      \includegraphics[width=0.9\linewidth]{figures/improved_aps3.pdf}
      \includegraphics[width=0.9\linewidth]{figures/improved_aps4.pdf}
      \includegraphics[width=0.9\linewidth]{figures/reduced_aps.pdf}
    \caption{\textbf{Audioset} Class-wise AP and improvement in AP from SUSTAIN. The blue bar shows performance of , i.e. model trained only on available labels. The bar on top of each blue bar shows improvement (green) or deterioration (red) in performance from sequential teaching. Several classes (along with \textbf{absolute change} in performance ) have been annotated to bring out noteworthy observations. }\label{fig:classaud}
\end{figure*}


\end{document}
