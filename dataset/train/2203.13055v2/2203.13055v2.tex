

\begin{table*}


\caption{\textbf{Quantitative results on AIST++ test set.} The best and runner-up values are bold and underlined, respectively.
Among compared methods, ``Li \etal'', DanceNet and FACT are multiplexing the same results of AIST++ benchmark~\cite{Li2021LearnTD},  while DanceRevolution~\cite{Huang2021DanceRL} is reproduced using officially released code with the optimal settings.
$\dag$ FID$_k$ and DIV$_k$ are fetched from \cite{Li2021LearnTD} while FID$_g$ and DIV$_g$ are recomputed using the officially updated evaluation code.
*The generated dances of ``Li \etal'' are highly jittery making its velocity variation extremely high, which is also reported in \cite{Li2021LearnTD}.
}
\vspace{-8pt}
  
  \centering
  \small
{
  \begin{tabular}{l  c c c c c c   }
    \toprule
    &  \multicolumn{2}{c}{ Motion Quality} & \multicolumn{2}{c}{ Motion Diversity } &  & \multicolumn{1}{c}{ User Study}             \\

     \cmidrule(r){2-3} \cmidrule(r){4-5}  \cmidrule(r){7-7} 
   Method & FID$_k\downarrow$ & FID$_g^{\dag}\downarrow$ &  Div$_k\uparrow$ &   Div$_g^{\dag}\uparrow$ &  Beat Align Score $\uparrow$  & Our Method Wins \\
      \midrule

Ground Truth & 17.10 & 10.60 & 8.19 & 7.45 & 0.2374 & 40.0\%$\pm$25.2\%\\
\midrule
Li \etal~\cite{Li2020LearningTG} & 86.43 & 43.46 & 6.85$^*$ & 3.32 & 0.1607 & 100.0\%$\pm$0.0\%$\,\,\,\,\,\,$\\
DanceNet~\cite{Zhuang2020Music2DanceMD} & 69.18 & 25.49  & 2.86 &  2.85 & 0.1430 &  92.7\%$\pm$12.1\%\\
DanceRevolution~\cite{Huang2021DanceRL} & 73.42 & 25.92 & 3.52 & 4.87 & 0.1950 & 84.5\%$\pm$10.8\% \\
FACT~\cite{Li2021LearnTD} & \underline{35.35} & \underline{22.11} & \underline{5.94} & \underline{6.18} & \underline{0.2209} & 98.2\%$\pm$3.9\%$\,\,\,$\\



\rowcolor{mygray}
{{\textit{Bailando}}} (Ours) & \bf 28.16 & \bf 9.62 & \bf 7.83 & \bf 6.34 & \bf{0.2332} & -- \\


   
    \bottomrule
  \end{tabular}
  }
  \label{table:quantitative_eval}
   \vspace{-15pt}
\end{table*}



\noindent{\textbf{Dataset}.}
We perform the training and evaluation on the AIST++ dataset proposed in \cite{Li2021LearnTD}, which to our best knowledge is the largest public available dataset for paired music and motions. 
This dataset contains 992 pieces of high-quality $60$-FPS 3D pose sequence in SMPL format \cite{SMPL:2015}, where 952 are kept for training and 40 are used for evaluation.


\noindent{\textbf{Implementation Details}.}
In this work, the choreographic memory codebook size $N$ for both upper and lower bodies is set to $512$, while the channel dimension $C$ of encoded features is $512$ and the temporal downsampling rate $d$ of encoders is $8$.
The structures of the convolutional encoder and decoders are provided in the supplementary file.
While training the VQ-VAEs,  dance data are cropped to length of $T=240$ ($4$ seconds)  and sampled in batch size of $32$.
The commit loss trade-off $\beta$ in $\mathcal L_{VQ}$ is $0.1$, while  $\alpha_1$ and $\alpha_2$ in $\mathcal L_{rec}$ are both set to be $1$.
We adopt Adam optimizer \cite{kingma2014adam} with $\beta_1=0.9$ and $\beta_2=0.99$ to train both pose VQ-VAEs for $400$ epochs with learning rate $3\times10^{-5}$.
As to the motion GPT, we comply a structure mirroring \cite{mingpt}, where the channel dimension is $768$, and the attention layer is implemented in $12$ heads with  dropoout probability $0.1$.
The music features are extracted by the public audio processing toolbox Librosa \cite{jin2017towards}, including \emph{mel frequency cepstral
coefficients (MFCC)}, \emph{MFCC delta}, \emph{constant-Q chromagram}, \emph{tempogram} and \emph{onset strength}, which are $438$-dim in total, and are mapped to the same dimension of GPT via a learned linear transform.
The block size $T'$ of GPT is set to be $29$. 
While training, the dance sequences are first encoded to pose codes $\bm p$ and sampled to length of $30$, where $\bm p_{0:28}$ are used as input and $\bm p_{1:29}$ are supervision labels.
The motion GPT is optimized using Adam optimizer with $\beta_1=0.5$ and $\beta_2=0.99$ for $400$ epochs, where the learning rate is initialized as $3\times10^{-4}$ and decayed after $200$ epochs with factor $0.1$.
In the actor-critic finetuning process, we adopt a small learning rate of $1\times 10^{-5}$ to learn $f_{\bf a}$ and $f_{\bf v}$ for $10$ epochs.
The reward trade-offs $\gamma_b$ and $\gamma_c$ are $5$ and $1$, respectively.
In our experiment, the pose VQ-VAEs and the motion GPT are trained sequentially, and the weights of VQ-VAEs are fixed during the learning process of GPT.
The whole framework is learned in three days on one Tesla V100 GPU.
During test, the motion GPT takes a pair of starting pose codes, which can be either manually indicated or randomly sampled, as input and autoregressively generates the motion sequence as long as the target music.

\noindent{\bf Evaluation Metrics.}
For quantitative evaluations, we measure the generated dance from three perspectives: the quality of generated dances, the diversity of motions and the alignment between the rhythms of music and generated movements.
In concrete, for the dance quality, we calculate the Fr√©chet Inception Distances (FID) \cite{heusel2017gans} between the generated dance and all motion sequences (including training and test data) of the AIST++ dataset on kinetic features \cite{onuma2008fmdistance} (denoted as `$k$') and geometric features \cite{muller2005efficient} (denoted as `$g$'), which are both extracted using the toolbox of \cite{gopinath2020fairmotion}.
As to the diversity, we compute the average feature distance of generated movements following \cite{Li2021LearnTD}.
Regarding to the alignment between music and generated motions, we calculate the average temporal distance between each music
beat and its closest dance beat as the Beat Align Score:
\begin{equation}
    \frac{1}{|B^m|}\sum_{t^m \in B^m}\exp{\left\{-\frac{\min_{ t^d \in B^d} {\|t^d - t^m\|^2}}{2\sigma^2}\right\}},
\end{equation}
where $B^d$ and $B^m$ record the time of beats in dance and music, respectively, while $\sigma$ is normalized parameter which is set to be $3$ in our experiment.

\vspace{-3pt}
\subsection{Comparison to Existing Methods}
\vspace{-3pt}

We compare our proposed model
to several state-of-the-art methods including Li \etal \cite{Li2020LearningTG}, DanceNet \cite{Zhuang2020Music2DanceMD}, DanceRevolution \cite{Huang2021DanceRL} and FACT \cite{Li2021LearnTD}.
For each method, we generate $40$ pieces of dances in AIST++ test set,
and sample the generated dance sequence with length of $20$ seconds to compute the evaluation metrics mentioned above.
We also calculate the quantitative scores for ground truth data in AIST++ test set and compare it to the generated dances.


The quantitative results are shown in Table~\ref{table:quantitative_eval}.
According to the comparison, our proposed model consistently performs favorably against all the other existing methods on all evaluations.
Specifically, our method improves $7.19$ ($20\%$) and $12.49$ ($56\%$) than the best compared baseline model FACT on FID$_k$ and FID$_g$, respectively, and even achieves a better FID$_g$ score than the ground truth ($9.62$ v.s. $10.60$).
If look closely to the metrics on these two kinds of features, the kinetic feature is defined on motion velocities and energies, which reflects the physical characteristics of dance, while the geometric feature is defined based on multiple man-made templates of movements, which reflects the quality of choreography.
The superiority of our method on both dance quality metrics reveals that {{\textit{Bailando}}} not only synthesizes more real-like motions than the compared baseline methods, but also achieves outstanding performance on organizing the movements to dance via the proposed  actor-critic GPT  scheme with learned choreographic memory.
Meanwhile, {{\textit{Bailando}}} can generate dance with high choreographic diversity instead of converging to few templates, and also achieves improvement on the correlation between music and motion.



\noindent{\bf User Study.}
To further understand the real visual performance of our method, we conduct a user study among the dance sequences generated by each compared method and the ground truth data in AIST++ test set.
The experiment is conducted with 11 participants separately.
For each participant, we randomly play 50 pairs of  comparison videos with a length of around 10 seconds, where each pair contains our result and one competitor's in the same music, and ask the participant to indicate ``\textit{which one is dancing better to the music}''.
The statistics are shown in Table~\ref{table:quantitative_eval}.
Notably, our method significantly surpasses the compared state-of-art methods with at least $84.5\%$ winning rate.
Even in comparison to the ground truth, $40\%$ of our generated dance is voted as the better in average.
According to the feedback from participants, our generated dance is more ``stable to the rythm'' with ``higher diversity'', while the reason why our method is still not as good as real dances is mainly due to ``lacking of long-term regularity  and subjective beauty''.
A detailed winning rate distribution on styles of dance can be referred to the supplementary file.








\vspace{-4pt}
\subsection{Ablation Studies}
\label{sec:ablation}
\vspace{-3pt}

We conduct ablation studies on the pose VQ-VAEs and the motion GPT, respectively.
The quantitative scores are shown in Table \ref{table:ablation}.
The visual comparisons of this study can be also referred to the supplementary video.





\noindent{\bf Pose VQ-VAE.}
We explore the effectiveness of the following components: (1) the up-lower half body separation, (2) the global velocity prediction branch, and (3) the velocity-and-acceleration loss used in $\mathcal L_{rec}$.
We train three variant models without each of the three components, respectively.
The motion quality measured for VQ-VAEs is on reconstructed results of ground truth of AIST++ test set.
As shown in Table~\ref{table:ablation}, the FID$_k$ and FID$_g$ values for variant ``w/o. upper/lower'' become worse by $12.98$ ($46\%$) and $3.22$ ($25\%$), respectively.
The VQ-VAE trained on whole body cannot reconstruct the dancing pose of test set effectively.
Therefore, the separate representations of upper-lower half bodies are necessary to enlarge the range of poses that the choreographic memory can cover.
As to the global velocity branch, the motion quality scores of ``w/o. global vel.'' sharply drops $42.72$ ($151\%$) and $5.89$ ($47\%$), respectively, which shows the isolated velocity prediction is critical for representing the dance movement.
For ``w/o. vel./acc. loss'' variant, the FID$_k$ is worsened by $2.68$.
Although the FID$_g$ value of ``w/o. vel./acc. loss'' is slightly improved by $0.76$, 
the model produces strong motion jitters if without adopting vel./acc loss for training in the supplementary video.



\begin{table}
\centering
    \small{
\caption{\small{\textbf{Ablation study on AIST++ test set} {Experiments are conducted on pose VQ-VAE and  GPT, respectively.} }}
\vspace{-10pt}
  
  \centering{
  \begin{tabular}{l l  c c c}
\toprule

 &  Method & FID$_k\downarrow$ & FID$_g\downarrow$ & BAS $\uparrow$\\
\midrule
\multirow{5}[0]*{\rotatebox{90}{Pose VQ-VAE}} & {Ground Truth} &  17.10 & 10.60 & -- \\
 
 & w/o. upper/lower  & 41.21  & 15.85  & --\\
 & w/o. global vel. & 70.95 & 18.52 & --\\  
& w/o. {vel./acc. loss}  & 30.91 & 11.87 & -- \\
&
\cellcolor{mygray}
 full pose VQ-VAE &  \cellcolor{mygray} 28.23 &  \cellcolor{mygray} 12.63 & \cellcolor{mygray} --\\

\midrule
\multirow{4}[0]*{\rotatebox{90}{GPT}} 
& w/o. quantization & 42.71 & 147.28 & --\\
& w/o. cross-cond. att.  & 37.41 & 15.52 & --\\
& w/o. actor critic & 28.75 & 11.82 & 0.2245\\
&
\cellcolor{mygray}
 full actor-critic GPT &  \cellcolor{mygray} 28.16 & \cellcolor{mygray} 9.62 & \cellcolor{mygray} {0.2332}\\
    \bottomrule
  \end{tabular}
  }
  \label{table:ablation}
  \vspace{-18pt}
  }
\end{table}


\noindent{\bf Motion GPT.}
For the proposed actor-critic GPT, first, we explore the effect of quantized choreography memory
by training a variant GPT 
directly 
regress to the encoding features of 3D joint sequence via an $L_2$ Loss.
As shown in Table \ref{table:ablation}, the FID$_g$ drops $135.41$ for variant ``w/o. quantization'' (compared to ``w/o. actor critic'', same below), while the generated dance sequences contain frequent jitters in vision, which shows the quantization of dancing positions is essential to our proposed framework.
Second, to test the effectiveness of the proposed cross-conditional causal attention, we substitute it to causal attention, and train two motion GPTs for upper and lower half bodies separately. 
The motion quality scores of ``w/o. cross-cond. att.'' drop $8.66$ ($30\%$) and $3.70$ ($31\%$), respectively. 
The main reason for the poor performance is that the generated dances of  contain frequent asynchronization of upper and lower half bodies, while the proposed cross-conditional attention layer can effectively prevent such situations via the interaction of information between the half bodies. 
At last, we compare the motion quality and music-motion consistency between the model with (denoted as ``full actor-critic GPT'') and without actor critic finetuning (denoted as ``w/o. actor critic'').
After the actor-critic learning, the beat-align score (BAS) of motion GPT increases from $0.2245$ to $0.2332$, proving the effectiveness of reinforcement learning scheme with proposed beat-align reward.
Meanwhile, by constraining the consistency with music, the actor-critic finetuning process can also enhance the motion quanlity on choreography and saliently improves the FID$_g$ score by $2.20$ ($19\%$).




\begin{figure}[t]
    \small{
    \scriptsize
    \setlength{\tabcolsep}{1.5pt}
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.9\linewidth]{figures/fig_interpretation2.pdf}\\
    \vspace{-3pt}
\small\caption{\small{\textbf{Interpretability of choreographic memory code.} The sequence of single code is decoded to static pose, while the sequence of two various codes is decoded to smooth transition between two poses, which means each code represents a dancing-style pose and the decoder links poses of different codes to movements.}}
    \label{fig:ablation}}
    \vspace{-14pt}
\end{figure}

\vspace{-5pt}
\subsection{Interpretability of Choreographic Memory}
\label{subsec:interpretable}
\vspace{-2pt}
In this work, we propose to  summarize meaningful dancing units into the codebook via pose VQ-VAE in an unsupervised manner.
To understand what kind of dance unit is learned in the choreographic memory, we visualize the latent codes and find each code represents a unique 3D dancing-style pose.
As revealed in Figure \ref{fig:ablation}, the first and the second rows are 3D poses decoded from $\bm p_0 = [4, 4]$ and $\bm p_1 = [5, 5]$, respectively, where the former one is doing right leg lifting and the latter is right bicep curl.
The decoded pose will keep static for repeating codes, and will make smooth transition between postures of different codes.
As shown in the third row of Figure \ref{fig:ablation}, the decoded 3D poses of $[\bm p_0, \bm p_1]$ starts with the posture of $\bm p_0$, while gradually putting down the leg and blending the arm towards the pose of $\bm p_1$.
Furthermore, for arbitrary combination of learned choreographic memory codes, the decoders can synthesize fluent movement based on the represented dance positions, which can be referred to the supplementary video.
With such characteristic, the choreography process becomes interpretable in proposed  {\textit{Bailando}} as a process of selecting and sorting the quantized dance positions from the learned choreographic memories, instead of a black box as most previous works. 
%
