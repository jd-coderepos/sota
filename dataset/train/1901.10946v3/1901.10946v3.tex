\documentclass{article}





\PassOptionsToPackage{numbers}{natbib}
\usepackage[final]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage[pdftex]{graphicx}
\usepackage{subfiles}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{color}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\loss} {\mathcal{L}}
\newcommand{\W} {\mathcal{W}}

\newcommand\smallO{
	\mathchoice
	{{\scriptstyle\mathcal{O}}}{{\scriptstyle\mathcal{O}}}{{\scriptscriptstyle\mathcal{O}}}{\scalebox{.7}{}}}

\newcommand{\R}{{\mathbb R}}   \newcommand{\E}{{\mathbb E}}   

\newcommand{\skt}{{\mathcal{S}}}
\newcommand{\tr}{{\text{tr}}}
\newcommand{\vt}{{\text{vec}}}


\newcommand{\eat}[1]{}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}


\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}



\newcommand{\real}{\mathbb{R}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pq}[1]{\left( #1 \right)}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\M}[1]{{\mathbf{#1}}} \newcommand{\T}[1]{{\mathcal{#1}}} \newcommand{\V}[1]{{\mathbf{#1}}} \newcommand{\SM}[1]{\mathbf{\hat{#1}}}
\newcommand{\TM}[1]{\mathbf{\bar{#1}}}
\newcommand{\krp}{\odot}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\MG}[2]{\mathcal{N}(#1,#2)}
\newcommand{\OO}[1]{\mathcal{O}(#1)}
\newcommand{\OOt}[1]{\tilde{\mathcal{O}}(#1)}
\newcommand{\NI} [1]{ \N{ #1}_{1}}
\newcommand{\Nsp}[1]{ \N{ #1}_{2}}
\newcommand{\N}[1]{ {\left\| #1 \right\|}}
\newcommand{\Nf} [1]{ \N{ #1}_{\textsc{F}}}
\newcommand{\Mij}[3]{#1_{{#2},{#3}}}
\newcommand{\Mcij}[3]{{\{#1\}}_{{#2},{#3}}}
\newcommand{\SV}[2]{\sigma_{#1}(#2)}
\newcommand{\EV}[2]{\lambda_{#1}(#2)}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\abs}[1]{ {\left| #1 \right|}}

\newcommand{\ryedit}[1]{{\color{magenta} #1}}
\newcommand{\ry}[1]{\ryedit{[RY: #1]}}
\newcommand{\hsedit}[1]{{\color{red} #1}}
\newcommand{\hscomment}[1]{\hsedit{[HS: #1]}}
\newcommand{\yledit}[1]{{\color{blue} #1}}
\newcommand{\yl}[1]{\yledit{[YL: #1]}}
\newcommand{\ezedit}[1]{{\color{cyan} #1}}
\newcommand{\ez}[1]{\ezedit{[EZ: #1]}}

\newcommand{\C}[1]{\mathcal{#1}} \newcommand{\edges}{\C{E}}
\newcommand{\graph}{\C{G}}
\newcommand{\vertices}{\C{V}}
\newcommand{\gconv}{{\,\star_{\mathcal{G}}\,}}
\newcommand{\ppr}{{\C{P}}}  \newcommand{\kp}{\otimes}
\newcommand{\hp}{*}
\newcommand{\op}{\circ }
\newcommand{\cpd}[1]{{\left\llbracket {#1} \right\rrbracket}}



\newcommand{\algMMT}{\texttt{MSMRL}}
\newcommand{\algSGD}{\texttt{SGD}}
\newcommand{\brck}[1]{\left(#1\right)}
\newcommand{\brcksq}[1]{\left[#1\right]}
\newcommand{\brckcur}[1]{\left\{#1\right\}}
\newcommand{\brckc}[1]{\left[#1\right\}}
\newcommand{\brcka}[1]{\langle #1\rangle}
\newcommand{\refn}[1]{(\ref{#1})}

\newcommand{\tsc}[1]{\textsc{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}

\newcommand{\half}{\frac{1}{2}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\cvec}{\mathbf{\boldsymbol{c}}}
\newcommand{\uvec}{\mathbf{\boldsymbol{u}}}
\newcommand{\vvec}{\mathbf{\boldsymbol{v}}}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\Scal}{{\mathcal{S}}}
 
\graphicspath{{figures/}{../figures/}}
\newcommand{\ours}{\texttt{NAOMI}}
\def\biblio{\bibliographystyle{unsrt}\bibliography{./references}}

\title{NAOMI: Non-Autoregressive Multiresolution Sequence Imputation}



\author{Yukai Liu \\
  Caltech \\
  \texttt{yukai@caltech.edu} \\
  \And
  Rose Yu \\
  Northeastern University \\
  \texttt{roseyu@northeastern.edu} \\
  \And
  Stephan Zheng\thanks{This work was done while the author was at Caltech.} \\
  Caltech, Salesforce \\
  \texttt{stephan.zheng@salesforce.com} \\
  \And
  Eric Zhan \\
  Caltech \\
  \texttt{ezhan@caltech.edu} \\
  \And
  Yisong Yue \\
  Caltech \\
  \texttt{yyue@caltech.edu}
}

\begin{document}

\maketitle

\begin{abstract}
Missing value imputation is a fundamental problem in  spatiotemporal modeling, from motion tracking to the dynamics of physical systems.
Deep autoregressive models suffer from  error propagation which becomes catastrophic for imputing long-range sequences. 
In this paper, we take a \textit{non-autoregressive} approach and propose a novel deep generative model: \textbf{N}on-\textbf{A}ut\textbf{O}regressive \textbf{M}ultiresolution \textbf{I}mputation (\ours{}) to impute long-range sequences given arbitrary missing patterns.
\ours{} exploits the multiresolution structure of spatiotemporal data and decodes recursively from coarse to fine-grained resolutions using a divide-and-conquer strategy.
We further enhance our model with adversarial training.
When evaluated extensively on  benchmark  datasets from systems of both deterministic and stochastic dynamics. In our experiments, \ours{} demonstrates significant improvement in imputation accuracy (reducing average error by 60\% compared to autoregressive counterparts) and generalization  for long-range sequences.
\end{abstract}

\subfile{sections/1_introduction}

\subfile{sections/2_related}

\subfile{sections/3_approach}

\subfile{sections/4_experiments}

\subfile{sections/5_conclusion}

\begin{thebibliography}{10}

\bibitem{urtasun20063d}
Raquel Urtasun, David~J Fleet, and Pascal Fua.
\newblock 3d people tracking with gaussian process dynamical models.
\newblock In {\em Computer Vision and Pattern Recognition, 2006 IEEE Computer
  Society Conference on}, volume~1, pages 238--245. IEEE, 2006.

\bibitem{ansley1984estimation}
Craig~F Ansley and Robert Kohn.
\newblock On the estimation of arima models with missing values.
\newblock In {\em Time series analysis of irregularly observed data}, pages
  9--37. Springer, 1984.

\bibitem{rubin2004multiple}
Donald~B Rubin.
\newblock {\em Multiple imputation for nonresponse in surveys}, volume~81.
\newblock John Wiley \& Sons, 2004.

\bibitem{acuna2004treatment}
Edgar Acuna and Caroline Rodriguez.
\newblock The treatment of missing values and its effect on classifier
  accuracy.
\newblock In {\em Classification, clustering, and data mining applications},
  pages 639--647. Springer, 2004.

\bibitem{little2019statistical}
Roderick~JA Little and Donald~B Rubin.
\newblock {\em Statistical analysis with missing data}, volume 793.
\newblock Wiley, 2019.

\bibitem{che2018recurrent}
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock {\em Scientific reports}, 8(1):6085, 2018.

\bibitem{maskgan}
William Fedus, Ian Goodfellow, and Andrew Dai.
\newblock Maskgan: Better text generation via filling in the (blank).
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{yoon2018gain}
Jinsung Yoon, James Jordon, and Mihaela van~der Schaar.
\newblock Gain: Missing data imputation using generative adversarial nets.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{luo2018multivariate}
Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et~al.
\newblock Multivariate time series imputation with generative adversarial
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1603--1614, 2018.

\bibitem{buuren2010mice}
S~van Buuren and Karin Groothuis-Oudshoorn.
\newblock mice: Multivariate imputation by chained equations in r.
\newblock {\em Journal of statistical software}, pages 1--68, 2010.

\bibitem{friedman2001elements}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock {\em The elements of statistical learning}, volume~1.
\newblock Springer series in statistics New York, NY, USA:, 2001.

\bibitem{nelwamondo2007missing}
Fulufhelo~V Nelwamondo, Shakir Mohamed, and Tshilidzi Marwala.
\newblock Missing data: A comparison of neural network and expectation
  maximization techniques.
\newblock {\em Current Science}, pages 1514--1521, 2007.

\bibitem{yoon2018estimating}
Jinsung Yoon, William~R Zame, and Mihaela van~der Schaar.
\newblock Estimating missing data in temporal data streams using
  multi-directional recurrent neural networks.
\newblock {\em IEEE Transactions on Biomedical Engineering}, 2018.

\bibitem{cao2018brits}
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li.
\newblock Brits: Bidirectional recurrent imputation for time series.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  6776--6786, 2018.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{lee2018deterministic}
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, 2018.

\bibitem{libovicky2018end}
Jind{\v{r}}ich Libovick{\`y} and Jind{\v{r}}ich Helcl.
\newblock End-to-end non-autoregressive neural machine translation with
  connectionist temporal classification.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, 2018.

\bibitem{oord2017parallel}
Aaron van~den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals,
  Koray Kavukcuoglu, George van~den Driessche, Edward Lockhart, Luis~C Cobo,
  Florian Stimberg, et~al.
\newblock Parallel wavenet: Fast high-fidelity speech synthesis.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{kingma2016improved}
Durk~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and Max
  Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em Advances in neural information processing systems}, pages
  4743--4751, 2016.

\bibitem{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4565--4573, 2016.

\bibitem{yu2017seqgan}
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Seqgan: Sequence generative adversarial nets with policy gradient.
\newblock In {\em AAAI}, pages 2852--2858, 2017.

\bibitem{karras2017progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{reed2017parallel}
Scott Reed, A{\"a}ron van~den Oord, Nal Kalchbrenner, Sergio~G{\'o}mez
  Colmenarejo, Ziyu Wang, Dan Belov, and Nando de~Freitas.
\newblock Parallel multiscale autoregressive density estimation.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{serban2017multiresolution}
Iulian~Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen
  Zhou, Yoshua Bengio, and Aaron~C Courville.
\newblock Multiresolution recurrent neural networks: An application to dialogue
  response generation.
\newblock In {\em AAAI}, pages 3288--3294, 2017.

\bibitem{roberts2018hierarchical}
Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck.
\newblock A hierarchical latent vector model for learning long-term structure
  in music.
\newblock {\em International Conference on Machine Learning}, 2018.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2014.

\bibitem{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{fragkiadaki2015learning}
Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik.
\newblock Learning visual predictive models of physics for playing billiards.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2016.

\bibitem{zheng2016generating}
Stephan Zheng, Yisong Yue, and Jennifer Hobbs.
\newblock Generating long-term trajectories using deep hierarchical networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1543--1551, 2016.

\bibitem{zhan2018generative}
Eric Zhan, Stephan Zheng, Yisong Yue, and Patrick Lucey.
\newblock Generating multi-agent trajectories using programmatic weak
  supervision.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{mallat1989theory}
Stephane~G Mallat.
\newblock A theory for multiresolution signal decomposition: the wavelet
  representation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  11(7):674--693, 1989.

\end{thebibliography}





\end{document}