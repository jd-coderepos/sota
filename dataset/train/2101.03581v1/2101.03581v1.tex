\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{amsmath,epstopdf,flushend,amssymb} 
\usepackage[figuresright]{rotating}
\usepackage{multirow,multicol,mathtools,comment}
\usepackage{xcolor}
\usepackage{pifont}
\newcommand{\cmark}{\ding{52}}\newcommand{\xmark}{\ding{56}}\newcommand{\greencmark}{{\color{green}\cmark}}
\newcommand{\redxmark}{{\color{red}\xmark}}

\newcommand{\zzie}{\emph{i.e.}~}
\newcommand{\zzeg}{\emph{e.g.}~}
\newcommand{\zzaka}{\emph{a.k.a.}~}

\newcommand*{\1}{\textcolor{red}} 
\usepackage{subfigure}



\title{Curvature-based Feature Selection with Application in Classifying Electronic Health Records}


\author{
 Zheming Zuo\thanks{Equal contribution.} \\
  Department of Computer Science\\
  Durham University\\
  Durham DH1 3LE, UK\\
  \texttt{zheming.zuo@durham.ac.uk} \\
\And
 Jie Li \\
  School of Computing, Engineering \& Digital Technologies\\
  Teesside University\\
  Middlesbrough TS3 6DR, UK\\
  \texttt{jie.li@tees.ac.uk} \\
  \And
 Noura Al Moubayed\thanks{Corresponding author.} \\
  Department of Computer Science\\
  Durham University\\
  Durham DH1 3LE, UK\\
  \texttt{noura.al-moubayed@durham.ac.uk} \\
}

\begin{document}
\maketitle
\begin{abstract}
Electronic Health Records (EHRs) are widely applied in healthcare facilities nowadays. Due to the inherent heterogeneity, unbalanced, incompleteness, and high-dimensional nature of EHRs, it is a challenging task to employ machine learning algorithms to analyse such EHRs for prediction and diagnostics within the scope of precision medicine. Dimensionality reduction is an efficient data preprocessing technique for the analysis of high dimensional data that reduces the number of features while improving the performance of the data analysis, \zzeg classification. In this paper, we propose an efficient curvature-based feature selection method for supporting more precise diagnosis. The proposed method is a filter-based feature selection method, which directly utilises the Menger Curvature for ranking all the attributes in the given data set. We evaluate the performance of our method against conventional PCA and recent ones including BPCM, GSAM, WCNN, BLS II, VIBES, 2L-MJFA, RFGA, and VAF. Our method achieves state-of-the-art performance on four benchmark healthcare data sets including CCRFDS, BCCDS, BTDS, and DRDDS with impressive 24.73\% and 13.93\% improvements respectively on BTDS and CCRFDS, 7.97\% improvement on BCCDS, and 3.63\% improvement on DRDDS. Our CFS source code is publicly available at \href{https://github.com/zhemingzuo/CFS}{\texttt{\1{https://github.com/zhemingzuo/CFS}}}. 
\end{abstract}


\keywords{feature selection \and precision medicine  \and healthcare \and electronic health records \and classification}


\section{Introduction}
Due to the era of big data, large amounts of high-dimensional data becomes available in a variety of domains, especially within the realm of digital healthcare~\cite{8370732}. The dramatically increased data volume has become a challenge for effective and efficient data analysis, as it significantly increases the memory storage requirement and computational costs~\cite{li2017feature}. To improve the quality of patient care more efficiently, Electronic Health Records (EHRs) are widely employed in the healthcare facilities for analysis. Currently, maintenance of such EHRs has become a crucial task in the medical sector. The patients' digital healthcare data is usually highly unstructured and consists of various features and diagnostics related information. In addition, EHR data may include missing values and a certain degree of redundancy. Due to the inherent heterogeneity, unbalanced, incompleteness, and high dimensional nature of EHRs, it is essential to develop appropriate data preparation and data reduction mechanisms to explore such healthcare data for data mining and data analytics. 

Dimensionality reduction is an efficient data preprocessing technique for the analysis of high dimensional data, which aims at reducing the number of features while improving the classification performance (\zzeg treatment planning \cite{duanmu2020prediction}, survival analysis \cite{rietschel2018feature}, and risk prediction \cite{denaxas2018prediction}) and reducing the related computational cost~\cite{8361067}. It is important to identify the most significant factors that related to the disease, which helps in removing unnecessary and redundant data from the given datasets, thus, to increase the data analysis performance. The selection is usually achieved by either projecting the original data into lower feature space, \zzaka feature extraction~\cite{han2018unified}, or selecting a subset of features from the original data, \zzie feature selection~\cite{liu2014feature}. For the latter, the least relevant features required to be removed can be identified by two criteria: a) features that are not correlated with any other features (\zzie redundancy); b) features do not contribute to the classification decision (\zzie noise).



It is well-known that the dimensionality reduction-based feature extraction approaches, such as Principal Component Analysis (PCA), reduces the number of features by mapping the original data set into a new feature space with lower dimensions, which changes or removes the physical meanings of original features. In contrast, methods of selecting a subset of features keep the physical meanings of the original features and enable models with better interpretability, underlying complexity of the subset evaluation metric may lead to unnecessary computational cost~\cite{JENSEN20151}. This motivates us to design an efficient selection-based feature selection method that could meet the requirement of the real-time system in the era of big data. 

Recently, pervasive healthcare becomes the central topic which attracts intensive attentions and interests from academia, industry, as well as healthcare sectors~\cite{li2019bayesian,lu2020machine,livieris2019improving,tang2019construction,aydin2019construction,yang2018new,elyan2017genetic,apicella2019simple}. In this problem domain, highly class-imbalanced data set with a large number of missing values are common problems~\cite{CervicalCancerDS2017}. It has been proved that the selected features may be with a higher degree of usefulness in comparison to the projected features, due to preservation of the original semantics of the dimensions~\cite{8361067,liu2014feature}. Thus, we focus on selecting a sub-set of features, even use the anonymised data set (\zzeg one possible reason of having the missing attribute values could be the participants or patients are reluctant to share personal identifiable information to the public \cite{CervicalCancerDS2017}), for efficient medical data classification.

Based on the aforementioned two motivations, we address the issues of the time-complexity and efficiency in an intuitive explainable fashion in this work. Our contribution is two folds:



\begin{enumerate}
  \item we propose a simple yet efficient feature selection method, namely Curvature-based Feature Selection (CFS), to select discriminative attributes in line with the ranked and averaged curvature values for each dimension in the given EHR data set;
  
  \item we further embed the CFS approach into the TSK+~\cite{Jie2017} fuzzy inference system, termed as CFS-TSK+, for supporting better decision-making of clinical diagnosis, \zzie improving the performance of classifying digital healthcare data.
\end{enumerate}

The rest of the paper is organised as follows. Section~\ref{sec:bg} revisits the related work. Section~\ref{sec:appr} presents our CFS approach and CFS-TSK+ classifier. Section~\ref{sec:exp} details the experimental results for comparison and validation. Section~\ref{sec:concl} concludes the paper.

\section{Background}\label{sec:bg}


In this section, the most recent developments of machine learning techniques in classifying medical data will be showcased first. This is followed by revisiting dimensionality reductions techniques for EHR data from the perspectives of feature extraction and feature selection, respectively.

\begin{comment}
\subsection{TSK+ Fuzzy Inference}\label{sparseFRI}
The Takagi-Sugeno-Kang (TSK) inference system generates a crisp inference result by utilising the weighted average of the sub-consequences based on determined firing strength for all the fired rules~\cite{TSK1985}. Notably, no rule will be fired if a given input does not overlap with any rule antecedent. Thereby, the TSK inference cannot be adopted. TSK+ fuzzy inference~\cite{li2018extended} was proposed to address such an issue which generates a consequence by considering all the rules in the rule base~\cite{Jie2017}. Suppose that a sparse TSK rule base is comprised of  rules:

where  and  represents a normal and convex polygonal fuzzy set that is denoted as (),  is the number of odd points of the fuzzy set. Given an input  in the input domain, a crisp inference result can be generated by the following three steps: \\
\textbf{Step 1}: Identify the matching degrees  between the given input  and rule antecedents () for each rule  by:

where  is a distance factor defined to measure the distance between the given two fuzzy sets, and is computed as:

where  denotes the sensitivity factor, and  represents the Euclidean distance between the two fuzzy sets.  is a positive-real number s.t. . Smaller value of  yields a similarity degree which is more sensitive to the distance of two fuzzy sets, and vice versa. \\
\textbf{Step 2}: Determine the firing degree  of each rule by aggregating the matching degrees between a given input and its antecedent terms via:

where  is a t-norm operator and is implemented as a minimum operator. \\
\textbf{Step 3}: Calculate the final output  by integrating the sub-consequences from all the rules by:

\end{comment}

\subsection{Machine Learning for Digital Healthcare}
In the past few decades, machine learning and deep learning algorithms have been widely proposed for solving healthcare problems, such as diagnose prediction of various diseases including cervical cancer~\cite{ghoneim2020cervical}, breast cancer~\cite{devarriya2020unbalanced}, and thoracic disease~\cite{li2018thoracic}, which usually taken in the form of classification.

Due to privacy considerations, there is a large number of healthcare data sets contain missing values. For coping with this common issue, the Bayesian Possibilistic C-means (BPCM) \cite{li2019bayesian} was devised for interpolating the missing values by extending the Fuzzy C-Means clustering algorithm (to model the noise and uncertainty) with the support of Bayesian theory (to calculate cluster centroids). The Gene Sequence-based Auxiliary Model (GSAM)~\cite{lu2020machine}, as an ensemble learner, was proposed for predicting the missing values via data correction and classifying testing data samples via a combination of multiple weak learners within a gene auxiliary module.

For enhancing the classification performance in terms of accuracy, the Weight Constrained Neural Network (WCNN)~\cite{livieris2019improving} was proposed. WCNN utilises network training to solve a constraint optimisation problem. The work of~\cite{tang2019construction} devised the extension of the Broad Learning System (BLS) by adding label-based autoencoder (BLS II), for learning robust feature representations in an ensemble way, and also to for tuning the hyper-parameters in Support Vector Machine (SVM), namely BLS II-SVM. Another ensemble learner VIBES was presented in~\cite{aydin2019construction} for detecting the dependency between attributes in the given data set and speeding the forward search for base learners.

In addition, the Genetic Algorithm is adopted for optimising the performance of Random Forest (RFGA)~\cite{elyan2017genetic}. Work towards the enhancement of activation functions in the neural networks was also proposed, such as Variable Activation Function (VAF)~\cite{apicella2019simple} and Adaptive Takagi-Sugeno-Kang (AdaTSK)~\cite{8858838}. Apart from those adaptive action functions,~\cite{yang2018new} presented a proposition of two-layer mixture of factor analysers with joint factor loading (2L-MJFA) for conducting the dimensionality reduction and classification together. This is done by utilising two mixtures nested with each other, and each of which contains a number of components, where each class of the data set is represented in a specific mixture of factor analysers (MFA). Such an approach has been proven to be suitable for small-scale data set, particularly, for the data set that contains a smaller number of data instances but includes a larger number of data attributes.

\subsection{Dimensionality Reduction for EHRs}\label{sec:relatedworks}
The EHR data is usually with high dimensions, thereby contains a large number of input features. It is noteworthy that some of input features may not relevant with the problem to be resolved. To effectively deal with such high-dimensional data, a typical solution is to apply specific techniques to reduce the dimensions of the original data set. Fundamentally, the dimensionality reduction techniques are typically divided into two aspects: 1) feature extraction, which combines the original features and creating a set of new feature representation, 2) feature selection that selects a subset of the original features~\cite{remeseiro2019review}. Fig.~\ref{fig:fe-fs} depicts the major difference between those two types of techniques, and both technologies are described below.  



\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{./Fig/FeatureExtractionAndFeatureSelection.png}
	\caption{Illustration of feature extraction and feature selection.}\label{fig:fe-fs}
\end{figure}


\subsubsection{Feature Extraction}\label{sec:fe}
Feature extraction (FE), also termed as Feature Construction, is a substitute of feature selection that transforms the original data from a high-dimensional space into a low-dimensional one, as illustrated in the upper pathway of Fig~\ref{fig:fe-fs}. By adopting this type of techniques, the problem is represented in a more discriminating (\zzie informative) space, thus, lead to more efficient analysis process. Such techniques have typically applied in the fields of medical image analysis, such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT) Scan, Ultrasound and X-Rays~\cite{remeseiro2019review}. The common feature extraction techniques can be grouped into two main types: linear and non-linear. Linear feature extraction approaches, such as PCA, adopt the matrix factorisation method to transform the original data into a lower dimensional subspace. For instance, PCA looks for ``principal components'' in the given data that are uncorrelated eigenvectors by considering the covariance matrix and its eigenvalues and eigenvectors~\cite{zuo2018gaze}. Although unsupervised PCA is highly effective in identifying important features of the data, it cannot easily determine the non-linear relationship among the features, which commonly exists in the complex EHRs, especially, the electrocardiogram (ECG), electroencephalography (EEG)~\cite{al2017enhanced}, and biological data~\cite{remeseiro2019review}. 

Compared with linear feature extraction methods, which linearly maps the original data into a low-dimensional subspace, non-linear feature extraction approaches works in different ways to represent the non-linear distribution, such as Kernel PCA~\cite{aziz2017dimension}, Locally Linear Embedding (LLE)~\cite{aziz2017dimension}, and Self-Organising Maps (SOM)~\cite{li2019machine}. Such approaches worked based on the hypothesis which the data lies on an embedded non-linear manifold that has a lower dimension than the raw data space and lies within it~\cite{aziz2017dimension}. 

Although the extracted features have the higher discriminating power that not only reduces the computational cost but also increases the classification accuracy, the combinations of the newly created set of features may have no physical meaning, therefore, feature extraction may not be a good approach with respect to readability, explainabibility and transparency~\cite{remeseiro2019review}. 

\subsubsection{Feature Selection}\label{sec:fs}
Feature selection (FS), \zzaka Variable Selection or Attribute Selection, is a process of selecting a subset of the most relevant attributes in the given data set for use of model construction (\zzie data modelling). Similar to FE, the aim of FS is also to aid in the task of generating accurate predictive models, however, is achieved by identifying and removing unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may, in fact, decrease the accuracy of the model~\cite{zuo2018grooming}, as depicted in the lower pathway of Fig.~\ref{fig:fe-fs}. Thereby, it is perfect when interpretability and knowledge extraction are crucial, \zzeg in medicine. Essentially, FS methods assess and evaluate the individual feature in the original data set to determine the relevance of each feature for the given problem, so as to select the most relevant features. In general, based on the relationship with the different learning methods, the process of feature selection can be categorised into three types, filter method, wrapper method, and embedded method. 

\begin{itemize}
	\item \textit{Filter}: Filter method focuses on the general characteristics of the each feature itself, which ranks features based on a certain evaluation criteria. This is followed by a threshold value selection process in order to eliminate the features that less than the selected crisp value. This method is computationally efficient and learning invariant, as it independent of any learning algorithm. The limitation of such approaches is that there is no interaction between the classifiers, class labels (outputs), and dependency of one feature over others. Consequently, those approaches may fail to determine the most ``useful'' features. 
	\item \textit{Wrapper}: Unlike the filter method, the wrapper method depends on the performance of learning algorithm to select features. In this method, candidate subsets of features are evaluated by an induction algorithm. The learning algorithms are employed to analyse the relationship between input features and the outputs (\zzie class labels), thus, to identify the most useful/relevant features. Compared with filter methods, which are not computationally intensive, the wrapper approaches usually have a complex progress and more computationally costly than filter methods. In addition, this method is more prone to over-fitting on small training data sets. 
	\item \textit{Embedded}: Though embedded method-based approaches still interact with the learning algorithms for selection relevant features, it conducts a different procedure from the filter and wrapper methods. In general, the embedded approaches can be described as a combination of the filter method and the wrapper method. It not only measures the relations between one input feature and its output feature (\zzie class labels) but also considers the each feature's general characteristic itself locally for better local discrimination~\cite{ang2015supervised}. In particular, the embedded approaches firstly use the independent criteria to determine the optimal feature subsets from the given data set, and then, the learning algorithm is applied to finalise the final optimal feature subsets from the previous results. Compared with the wrapper method, the embedded approaches are computationally inexpensive and less prone to over-fitting~\cite{ang2015supervised}. 
\end{itemize}

Recently, a hybrid method is also widely employed to preprocess the EHRs, in order to increase the model prediction capability. This method aggregates one or more feature selection methods together, \zzeg filter and wrapper methods, to take the advantages of different methods, hence, to generate optimal results. The hybrid method usually can achieve a better performance, \zzeg higher prediction accuracy, however, it also requires a higher computational cost~\cite{jain2018feature}.

\section{Proposed System} \label{sec:appr}
In this section, a novel filter method feature selection approach, called Curvature-based Feature Selection (CFS), is proposed and detailed. The system pipeline is outlined in Fig.~\ref{fig:pip}, which comprises of three main components: two-dimensional (2-D) data re-construction, feature weight calculation by Menger Curvature (depicted in Fig.~\ref{fig:mengercurvature}), and feature ranking.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\textwidth]{./Fig/CFS_pipeline_20200121.pdf}
	\caption{Curvature-based Feature Selection method.}\label{fig:pip}
\end{figure}

\subsection{Menger Curvature}\label{sec:RoC}
The Menger Curvature ()~\cite{leger1999menger} measures the curvature of triple data points within the -dimensional Euclidean space  represented by the reciprocal of the radius of the circle that passes through the three points , , and  in Fig.~\ref{fig:mengercurvature}.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.4\textwidth]{./Fig/MengerCurvature.pdf}
	\caption{The Menger Curvature of a triple of data points on a 2-D space.}\label{fig:mengercurvature}
\end{figure}

In this work, only two-dimensional plane curves problems are considered. Given that  are the three points in a 2-D space  and  are not collinear, as depicted in Fig.~\ref{fig:mengercurvature},  on  is calculated as:


where  represents the radius,  denotes the Euclidean distance between  and , and   is the angle of -corner of the triangle spanned by , which can be calculated in line with the Law of Cosines:

 on points  and  is not calculable, as these points are boundary points. The efficacy of  is confirmed in constructing Mamdani fuzzy rule base~\cite{zuo2020CSRBG}.

\subsection{Curvature-based Feature Selection}\label{sec:CFS}
Assume that a high-dimensional raw data set, denoted as , contains  data instances,  inputs attributes, and a single output feature . In real-world problem domain, a data cleaning process (\zzeg removing attributes with missing values) and data normalisation phase (\zzeg bounding all the values within the interval of ) may be applied on  to obtain  s.t. . In this work, we adopt the Min-Max (MM) normalisation technique:


This operation helps to cancel out the influence of possible large variations in the raw data set and guarantees that our CFS is able to compare the curvatures for each attribute in an equitable manner. In other words, all the attribute values are normalised to the same frame of reference to ensure the correct rankings generated by CFS. The proposed curvature-based feature selection method is described as follows:


\textbf{Step 1 -- \textit{2-D Data Re-construction}}: The first step of the proposed CFS is to break down the cleaned high-dimensional data set  into  2-D planes, which is implemented by combining all input attributes  () and the output . Thus,  can be decomposed to  2-D planes, represented as . 

\textbf{Step 2 -- \textit{Feature Weighting}}: For each decomposed 2-D plane , the Menger Curvature method, introduced in Section~\ref{sec:RoC}, is adopted to obtain the averaged curvature value of the feature . Given that a decomposed 2-D panel () contains  data instances, the Menger Curvature value () of data point   can be determined by Eq.~(\ref{eq:MCvalue}). To this end, the mean of  for , denoted as , is computed as in:

where  represents the curvature value of the  data point in feature .  indicates the corresponding weight of the feature , the greater value of  signifies a higher degree of importance of the corresponding feature  for the data set , and vice versa. 

\textbf{Step 3 -- \textit{Feature Ranking and Feature Selection}}: A conventional ordinal ranking method is used to rank the features, based on the obtained . Thereby, the features of  are ranked. This is followed by selecting the corresponding features from the raw data set . Given a threshold , the features with  greater than the given threshold  will be selected. Equivalently, Top method can be employed:

such that . To this end, we have reduced the dimensionality of  to  while preserving the statistical nature of the original data set. Then, in line with the rest parts shown in Fig.~\ref{fig:pip}, obtained  will be further normalised and classified.

\subsection{Feature Normalisation} \label{sec:featNorm}
For improving the performance of classification and ensuring the degree of membership in the TSK+ calculable,
the selected features in  are further normalised using a total number of eight normalisation techniques~\cite{8858838} in this work including the Min-Max (MM) normalisation, 1-normalisation, 2-normalisation, Power Normalisation (PN), and its variants (\zzie 1PN, 2PN, PN1, and PN2).

\subsection{Classification} \label{sec:featClasf}
For classifying the selected and normalised features, nine classifiers~\cite{zuo2018grooming,8858838} are used, namely Gaussian Na\"ive Bayes (GNB), Random Forest (RF), AdaBoost (AB), Logistic Regression (LR), Linear Support Vector Machine (Linear SVM), Quadratic Discriminant Analysis (QDA), Decision Tree (DT),  Nearest Neighbours (NN), Back-Propagation Neural Network (BPNN). Additionally, we also combine the proposed CFS method with TSK+ (CFS-TSK+) and evaluate its performance for the classification of four benchmark medical data sets.

\section{Experiments} \label{sec:exp}
In this section, we evaluate CFS performance  and compare the results against PCA, one of the most popular  dimensionality-reduction techniques, on four benchmark clinical data sets. Following we describe the data sets and the experimental setup we used to examine both techniques.

\subsection{Data Sets}
\begin{table}[!h]
	\centering
	\caption{Summary of the four clinical data sets.  denotes the exclusion of class label.  represents the number of dimensions contain the missing values.}
	\begin{tabular}{l|ccccccc}
		\hline
		\rotatebox{90}{Data Set}
		& \rotatebox{90}{\texttt{\#} of instances} & \rotatebox{90}{\texttt{\#} of dim.}
		& \rotatebox{90}{\texttt{\#} of classes}
		& \rotatebox{90}{Missing values?}
		& \rotatebox{90}{\texttt{\#} of instances. }
		& \rotatebox{90}{\texttt{\#} of dim. } 
		& \rotatebox{90}{Year published}\\
		\hline\hline

		CCRFDS~\cite{CervicalCancerDS2017} & 858 & 35 & 2 & \greencmark & 799 & 26 & 2017\\ 
		\hline
		BCCDS~\cite{BreastCancerCoimbraDS2018} & 116 & 9 & 2 & \redxmark & N/A & N/A & 2018\\ 
		\hline
		BTDS~\cite{BeastTissueDS2010} & 106 & 9 & 6 & \redxmark & N/A & N/A & 2010\\ 
		\hline
		DRDDS~\cite{DiabeticRetinopathyDebrecenDS2014} & 1,151 & 19 & 2 & \redxmark & N/A & N/A & 2014\\ 
		\hline
	\end{tabular}
	\label{tbl:ppmlComparison}
\end{table}
\textbf{Cervical Cancer (Risk Factors) Data Set}~\cite{CervicalCancerDS2017} (CCRFDS) comprises demographic information, habits, and historic medical records of 858 patients with some missing attribute values due to consents of the participated patients. The data set is categorised by the Boolean value of the biopsy. CCRFDS is also highly class-imbalanced, \zzie only 18 out of 858 participants have cancer, demonstrated in Fig.~\ref{fig:DS}(a). In addition, there are 799 out of 858 data instances, with 26 populated out of 35 attributes, due to missing values `?'.

\textbf{Breast Cancer Coimbra Data Set}~\cite{BreastCancerCoimbraDS2018} (BCCDS) is composed of 116 data instances (each of which contains 9 attributes) that can be grouped into 2 categories, \zzie healthy controls, and patients.

\textbf{Breast Tissue Data Set}~\cite{BeastTissueDS2010} (BTDS) contains 106 data instances and each of which is with 9 feature dimensions that can be classified into 6 categories including carcinoma, fibro-adenoma, mastopathy, glandular, connective, and adipose.

\textbf{Diabetic Retinopathy Debrecen Data Set}~\cite{DiabeticRetinopathyDebrecenDS2014} (DRDDS) includes 1,151 data instances that categorised into 2 classes which respectively indicating having Diabetic Retinopathy (DR) and not having DR.
\begin{figure}[!ht]
	\setlength{\tabcolsep}{-8pt}
	\centering
	\begin{tabular}{cc}
		\hspace{-0.3em}\includegraphics[width=0.4\linewidth]{Fig/DataSet_CCRFDS.pdf} & \includegraphics[width=0.4\linewidth]{Fig/DataSet_BCCDS.pdf}\\
		(a) CCRFDS & (b) BCCDS\\
		\hspace{-0.3em}\includegraphics[width=0.4\linewidth]{Fig/DataSet_BTDS.pdf} & \includegraphics[width=0.4\linewidth]{Fig/DataSet_DRDDS.pdf}\\
		(c) BTDS & (d) DRDDS
	\end{tabular}
	\caption{Data instances percentage for each data set. Best viewed in colour.}
	\label{fig:DS}
\end{figure}

\subsection{Dealing with Missing Data}\label{sec:missingvalues}
Missing data, \zzaka missing values, is a common issue in the digital healthcare domain. As introduced above, missing data could reduce the statistical power of the predictive model, as well as lead to incorrect or invalid results. Thereby, an extra stage may be required for handling the issue of missing data. There are two major methods that could be adopted to cope with the missing values, \zzie data imputation and case deletion~\cite{kang2013prevention}. Concretely, the imputation is the process of inserting the missed values with substituting components. Several approaches have been well-discussed in the literature, such as mean, Multiple Imputation with Chained Equations-Full (MICE-full), and missForset~\cite{luo2016using}. Among those methods, mean imputation approach imputes missing values as the mean of the available values for each variable, MICE-full and missForset are then use machine learning algorithms, \zzeg random forest, to predict the missing values based on the observed values of a data matrix. For the latter, entire data instances with the missing data are simply omitted/removed, and then only the remaining data is to be used for the analysis. In this work, we merely apply the case deletion method on CCRFDS.

\subsection{Experiment Setup}
All the experiments were implemented in Python\textsuperscript{\texttrademark} 3.6.9 in conjunction with MATLAB\textsuperscript{\textregistered} 2019b, and conducted using a workstation equipped with INTEL\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i9-9900K (8-core) CPU @ 3.60 GHz and 64GB RAM.

Among all the four selected data sets, we perform data cleaning on CCRFDS, \zzie the attributes, which contain missing value, are eliminated. Thereby, the CCRFDS used in this work contains 858 data instances and each of which has 9 attributes (not including the class label). For all the rest three data sets, we use the originally published data.

For selecting features, we compare the proposed CFS using the Top method defined in Eq.~(\ref{eq:topK}), with the well-known PCA by varying the number of the selected features. That is, we select 7 out of 9 attributes (in CCRFDS, BCCDS, and BTDS) and 15 out of 19 attributes (in DRDDS). For normalising the selected features, 8 normalisation methods introduced in Section~\ref{sec:featNorm} are employed, in which the power coefficient in PN and its variants were set to 0.1. For classification, 10 classifiers (introduced in Section~\ref{sec:featClasf}) are employed with the configuration information: the maximum number of estimators is 100 at which the boosting is terminated in AB; the 1 regularisation is adopted as the penalty function in LR; the Gini index is employed as the criterion in DT and the maximum depth of the tree is valued as 5; the number of neurons in a single hidden layer of BPNN is set to 20; the  is valued as 3 in NN. The mean accuracy is reported for the ten employed classifiers for performance comparisons via the 10-Fold cross-validation.

\subsection{The Efficacy of CFS}
We verify the efficacy of the proposed CFS method by comparing it with PCA on 4 clinical data sets using 8 feature normalisation techniques and 10 classifiers.

Based on the summary visualised in the second row of Fig.~\ref{fig:CFS}, CFS outperforms PCA for the following data sets: CCRFDS, BCCDS, and DRDDS, and slightly less competitive for DRDDS. For the first three data sets, CFS yields an average mean accuracy of 95.00\%, 61.30\%, and 69.79\%, versus 94.73\%, 60.17\%, and 53.73\% resulted from PCA, respectively. For the last data set DRDDS, CFS achieved average mean accuracy of 65.02\% in contrast to 65.20\% generated by PCA. This observation indicates that the CFS is generally more competitive than PCA.



For the CCRFDS, the best performance yielded by PCA-based and CFS-based are 96.27\% and 97.09\%, using the MM and PN1 normalisation approaches, respectively. Concretely, in Fig. \ref{fig:CFS}(a), the PCA-based classifiers tend to generate better performance using conventional data normalisation methods (\zzie MM, 1, and 2) where the CFS-based ones yield more competitive acccuracies when using PN and its variants (\zzie 1PN, 2PN, PN1, and PN2).

\begin{figure*}
	\centering
	\includegraphics[width=0.98\textwidth]{Fig/Results/Fig5.pdf}
	\caption{Classification accuracies of PCA and CFS feature selection methods on four EHR data sets by varying both feature normalisation methods and classifiers. Leftmost column shows the detailed classification performance, second column further summarises the corresponding statistics, and rightmost columns lists the feature rankings provided by CFS in a descending order over each data set for intuitive explainability. Best viewed in colour.}
	\label{fig:CFS}
\end{figure*}

For the BCCDS, the best performance obtained by PCA-based and CFS-based are 76.67\% and 79.17\%, all using the MM normalisation approach. Concretely, in Fig. \ref{fig:CFS}(b), the PCA-based classifiers tend to generate better performance using conventional data normalisation methods (\zzie MM, 1, and 2) where the CFS-based classifiers (except the CFS-TSK+) yield more competitive acccuracies when using PN and its variants.

For the BTDS, we show that CFS is capable of help differentiating between certain categories of healthy or pathological breast tissue. In general, CFS-based classifiers outperforms their PCA counterparts over all the data normalisation approaches used. Concretely, CFS-based classifiers achieved several times of 100\% accuracy , while the peak performance of PCA-based ones is 69.73\%.

For the DRDDS, the peak performance of CFS and PCA are 76.98\% and 74.72\%. In constrast to BTDS, PCA-based classifiers showed to be more accurate than CFS-based ones via all the data normalisation techniques, though averagely close with each other.

For better explain the reason that CFS is a more competitive candidate of feature selection in comparison to PCA, we visualised the ranking of all the attributes generated by CFS in the two bottom rows of Fig. \ref{fig:CFS}. 
























\subsection{The Efficiency of CFS-TSK+}
We detail here the peak performances of PCA-TSK+ and CFS-TSK+ in Table~\ref{tbl:csktsk+}. Notably, in conjunction with Fig.~\ref{fig:CFS}, CFS-TSK+ achieved the best performance in the data sets of CCRFDS and BCCDS. This observation confirmed the practicability and efficiency of combing the CFS with TSK+. 

However, the best performance of CFS-TSK+ and PCA-TSK+ on BTDS help us to identify the possible drawback of the TSK+ in coping with classification task. That is, the TSK+ is not sensitive to formulate the class boundary when the given data samples are sparsely distributed in the feature space. Alternatively, the rule base are not generalised well in the step of clustering where each cluster is corresponding to a fuzzy rule. Based on the time consumption, we did not perform rule base optimisation in this work as this is bit beyond our scope. For the last data set DRDDS, owing greatly to the lack of expert knowledge, it is not explainable to show how reasonable the ranked results of CFS in comparison to the rest data sets, which are more common sensible. The part of designing a self-explainable component could be treated as an active future work.

\begin{table}[]
	\centering
	\caption{Optimal performance comparisons of PCA-TSK+ and CFS-TSK+. Rule base employed in PCA-TSK+ and CFS-TSK+ were not optimised. () denotes multiple combinations achieved the same performance. Best performance is marked in bold.}
	\begin{tabular}{l|c|c}
		\hline
		Data Set
		& PCA-TSK+ (\%) & CFS-TSK+ (\%)\\
		\hline
		\hline
		CCRFDS & 95.81 (MM) & \textbf{97.09} (PN1)\\ 
		\hline
		BCCDS & 76.67 (MM) & \textbf{79.17} (MM)\\ 
		\hline
		BTDS & \textbf{46.36} (1PN) & 33.65 ()\\ 
		\hline
		DRDDS & \textbf{61.03} (PN1) & 60.95 (PN1)\%\\ 
		\hline
	\end{tabular}
	\label{tbl:csktsk+}
\end{table}

\begin{table}[ht!]
	\centering
	\caption{Performance summary and comparisons. DS denotes data set. Abbreviations of all the experimented data sets are summarised in Table \ref{tbl:csktsk+}.  represents multiple combinations achieved the same performance (see Fig. \ref{fig:CFS}(c). Best performance is marked in bold.}
	\begin{tabular}{c|c|c|c}
		\hline
		DS
		& Method & \texttt{Dim.} & Top \texttt{Mean Acc.} (\%)\\
		\hline
		\hline
		\multirow{4}{*}{\rotatebox{90}{CCRFDS}} & BPCM+NN \cite{li2019bayesian} & 9 & 80.00\\ 
		& GSAM \cite{lu2020machine} & 9 & 83.16\\
		& PCA-RF & 7 & 96.27\\ 
		& CFS-TSK+ & 7 & \textbf{97.09}\\ 
		
		\hline
		\multirow{4}{*}{\rotatebox{90}{BCCDS}} & WCNN \cite{livieris2019improving} & 9 & 62.00\\ 
		& BLS II-SVM \cite{tang2019construction} & 9 & 71.20\\
		& PCA-TSK+ & 7 & 76.67\\
		& CFS-TSK+ & 7 & \textbf{79.17}\\
		
		\hline
		\multirow{4}{*}{\rotatebox{90}{BTDS}} & VIBES \cite{aydin2019construction} & 9 & 65.09\\
		& PCA-QDA & 7 & 69.73\\
		& 2L-MJFA \cite{yang2018new} & 9 & 75.27\\
		& CFS- & 7 & \textbf{100.00}\\
		
		\hline
		\multirow{4}{*}{\rotatebox{90}{DRDDS}} & RFGA \cite{elyan2017genetic} & 19 & 68.26\\
		& VAF \cite{apicella2019simple} & 19 & 73.35\\
		& CFS-BPNN & 15 & 74.72\\ 
		& PCA-BPNN & 15 & \textbf{76.98}\\ 
		\hline
	\end{tabular}
	\label{tbl:csktsk+}
\end{table}

\subsection{Discussions}
To summarise the proposed approach, we compare our CFS with PCA and other recent competitive works in Table~\ref{tbl:csktsk+}. Though CFS achieved three best performance among the total four medical data sets, and CFS-TSK+ yielded two highest mean accuracies on two data sets, we identified that possible drawback of the proposed CFS is the lack of better explainability when the domain (\zzeg clinical science) knowledge is not available. This might be mitigated by predicting the missing values on the anonymised data set and training a self-explainable component. Another piece of active future work could be the enhancement of sparsity awareness of the CFS-TSK+ in the scenario of classification.




\section{Conclusion} \label{sec:concl}
In this work we propose the Curvature-based Feature Selection method for contributing the classification of clinical (EHR) data sets. We achieve state-of-the-art performance on four benchmark clinical data sets. Though lack of expert knowledge of clinical science, we visualise the results of feature ranking of the proposed CFS to support better explainability. It is noteworthy that self-explainability of CFS and sparsity awareness of class boundaries of CFS-TSK+ are observed as possible future directions.


\bibliographystyle{unsrt}  
\bibliography{ref} 


\end{document}
