\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage[switch]{lineno}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\usepackage{multirow}


\pdfinfo{
/Title (Scene Graph Transformer  with Neural-based Decision Tree Loss for Scene Graph Generation)
/Author ()
}



\setcounter{secnumdepth}{2} 

\setlength\titlebox{2.5in} \title{CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation}














\author {
Jing Yu,\textsuperscript{\rm 1,\rm 3}\footnote{Corresponding author.}\textsuperscript{}
    Yuan Chai, \textsuperscript{\rm 1,\rm 2}\footnote{Equal contribution. This work is done when Yuan Chai is an intern in IIE, CAS.}
    Yue Hu, \textsuperscript{\rm 1,\rm 3}
    Qi Wu \textsuperscript{\rm 4} \\
}

\affiliations {
\textsuperscript{\rm 1} Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China \\
    \textsuperscript{\rm 2} Intelligent Computing and Machine Learning Lab, School of ASEE, Beihang University, Beijing, China \\
    \textsuperscript{\rm 3} School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China \\
    \textsuperscript{\rm 4} University of Adelaide, Australia \\
    \{yujing02, huyue\}@iie.ac.cn, chaiyuan@buaa.edu.cn, qi.wu01@adelaide.edu.au
}


\begin{document}


\maketitle
\begin{abstract}

Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of data representation, e.g. balancing data distribution or learning unbiased models and representations, ignoring the mechanism that how humans accomplish this task. Inspired by the role of the prefrontal cortex (PFC) in hierarchical reasoning, we analyze this problem from a novel cognition perspective: learning a hierarchical cognitive structure of the highly-biased relationships and navigating that hierarchy to locate the classes, making the tail classes receive more attention in a coarse-to-fine mode. To this end, we propose a novel Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small  portion  of  easily  confused  ones. Then, we propose a hierarchical loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships while progressively eliminating the interference of irrelevant ones. The loss is model-independent and  can  be  applied  to  various  SGG  models without extra supervision. The proposed CogTree loss consistently boosts the performance of several state-of-the-art models on the Visual Genome benchmark. 
\end{abstract}

\iffalse
\begin{figure}[h]
    \centering
    \subfigure[Distribution of predicates.]{\includegraphics[width=0.9\columnwidth]{figure/sampling_fraction.png}}
    \subfigure[1]{\includegraphics[width=0.9\columnwidth]{figure/bias1.png}}
    \subfigure[2]{\includegraphics[width=0.9\columnwidth]{figure/bias2.png}}
    \caption{Examples of scene graph.}
    \label{fig:problem}
\end{figure}
\fi

\begin{figure}[t]
    \centering
    \subfigure[Flat thinking.]{\includegraphics[width=\columnwidth]{Figure1_a.pdf}}
    \subfigure[Cognition-based hierarchical thinking.]{\includegraphics[width=\columnwidth]{Figure1_b.pdf}}
    \caption{(a) SGG model with conventional flat loss. (b) SGG model with our proposed cognition tree loss. The word size in the top-left box is proportional to the word frequency.}
    \label{fig:comparison}
\end{figure}


\section{Introduction}
\label{sec:intro}
Making abstraction from an image into high-level semantics is one of the most remarkable capabilities of humans. Scene Graph Generation (SGG) \cite{krishna2017visual}  a task of extracting objects and their semantic relationships in an image to form a graphical representation  aims to achieve the abstraction capability and bridge the gap between vision and language. SGG has greatly benefited the down-stream tasks in the domain of question answering \cite{norcliffe2018learning,Zhu2020Mucko} and visual understanding \cite{shi2019explainable,Jiang2020DualVD}. Thereinto, some works \cite{Zhu2020Mucko,Jiang2020DualVD} feed the scene graphs into graph neural networks for relation-aware object representation, and some others \cite{hudson2019learning} perform sequential reasoning by traversing the relational graphs. Compared with independent objects, the rich relationships in SGG play the predominant role, which improve the performance of down-stream tasks and enable explainable reasoning. 




Although much effort has been made in SGG with high accuracy in object detection, most of the detected relationships are far from satisfaction due to the long-tailed data distribution. Only a small portion of the collected relationship classes have abundant samples (head) while most classes contain just a few (tail). This heavily biased training data causes biased relationship prediction. Fine-grained relationships (tail) will be mostly predicted into head classes, which are not accurate or discriminative  enough for high-level reasoning tasks, such as falsely predicting {\ttfamily on} instead of {\ttfamily looking at}, and coarsely predicting {\ttfamily on} instead of {\ttfamily walking on}.  Besides, some fine-grained relationships, like {\ttfamily standing on}, {\ttfamily sitting on} and {\ttfamily lying on} can be even harder to distinguish from each other due to their visual similarity and scarce training data.

To tackle this problem, most research focuses on learning unbiased models by re-weighting losses \cite{zareian2020bridging} or disentangling unbiased representations from the biased \cite{Tang2020Unbiased}. However, humans can effectively infer the correct relationships even when some relationships appear more frequently than others. The essential difference between human and AI systems that has been ignored lies not in the learning strategy or feature representation, but in the way that the concepts are organized. To illustrate this discrepancy, we show an example {\ttfamily trunk parked on street} in Figure \ref{fig:comparison}. Existing models treat all the relationships independently for flat classification (Figure \ref{fig:comparison}(a)). Because of the data bias, the head relationships, \textit{e.g.} {\ttfamily on} and {\ttfamily near}, obtain high predicted probability. In contrast, the recent proposed cognition theory \cite{Sarafyazd2019Hierarchical} supports that humans process information hierarchically due to the  role  of  the  prefrontal  cortex  (PFC). In the SGG task, we generally start from making a rough distinction between remarkably different relationships. As shown in Figure \ref{fig:comparison}(b), relationships belonging to the concept  ``on'',
\textit{e.g.} {\ttfamily on} and {\ttfamily parked on}, will firstly be distinguished from the ones about ``near'' concept, \textit{e.g.} {\ttfamily near} and {\ttfamily behind}; then we move to distinguish the slight discrepancy among some easily confused ones in one concept, \textit{e.g.} {\ttfamily parked on}, {\ttfamily standing on} and {\ttfamily walking on}, regardless of most irrelevant relationships. 

Inspired by the hierarchical reasoning mechanism in PFC, we propose a  novel loss function, \textbf{Cognition Tree} (\textbf{CogTree}) loss, for unbiased scene graph generation. We first propose to build a hierarchy of the relationships, imitating the knowledge structure built from the independent relationships in human mind. The CogTree is derived from the prediction of a biased SGG model that satisfies the aforementioned  thinking principles: distinguishing remarkably different relationships at first and then 
focusing on a small portion of easily confused ones. Then we design a CogTree-based loss to train the SGG network from scratch. This loss enables the network to surpass the noises from inter-concept relationships and then intra-concept relationships progressively. In this way, the SGG models are no longer required to distinguish each detailed discrepancy among all the relationships as flat thinking, resulting in 
more accurate prediction due to the coarse-to-fine thinking strategy.  



The main contributions are summarized as follows: (1) We exploit the possibility of cognition in SGG by building the cognitive structure of the relationships, which reveals the hierarchy that the relationships are organized after preliminary learning; 
(2) We propose a hierarchical loss specially for the above cognitive structure,  which supports coarse-to-fine distinction for the correct relationships while progressively eliminating the interference of irrelevant ones. It is model-independent and  can be applied to various SGG models without extra supervision; (3) We perform extensive evaluation on state-of-the-art models and a stronger transformer baseline. Results show that the CogTree loss consistently boosts their performance with remarkable improvement. 











\section{Related Work}
\label{sec:relatedWork}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figure2.pdf}
    \caption{The overview of CogTree loss applied to SGG models. It contains three parts: Scene Graph Generation Network summarizes the framework of biased SGG models; Bias-Adaptive Cognition Tree Building organizes relationships by a coarse-to-fine tree based on biased prediction; Learning with CogTree Loss supports network to distinguish relationships hierarchically. }
    \label{fig:framwork}
\end{figure*}

\textbf{Scene Graph Generation.}
SGG \cite{xu2017scene} products graphical abstraction of an image and encourages visual relational reasoning and understanding in various down-stream tasks \cite{Zhu2020Mucko,Jiang2020DualVD}. Early works focus on object detection and relationship detection via independent networks \cite{lu2016visual,Zhang2017Relationship}, ignoring the rich contextual information. To incorporate the global visual context, recent works leverage message passing mechanism \cite{xu2017scene,Li2017scene,yang2018graph,li2018factorizable,qi2019attentive,chen2019counterfactual} and recurrent sequential architectures \cite{zellers2018neural,woo2018linknet,tang2019learning} for more discriminative object and relationship representations. Although the accuracy is high in object detection, the relationship detection is far from satisfaction due to the heavily biased data. \cite{chen2019knowledge,tang2019learning} consider the biased SGG problem and propose mean Recall as the unbiased metric without corresponding debiasing solutions. The recent work \cite{Liang2019VRR} prunes the predominant spacial relationships and keeps the tail but informative ones in the dataset. \cite{Tang2020Unbiased} proposes the first solution for unbiased SGG  by counterfactual surgeries on causal graphs. We rethink SGG task from the cognition perspective and novelly  solve the bias problem based on the coarse-to-fine structure of the relationships, imitating human's hierarchical thinking mechanism. 
\\
\textbf{Biased Classification.}
Classification on highly-biased training data has been extensively studied in previous work, which can be divided into three categories: (1) balancing data distribution by data augmentation or re-sampling \cite{burnaev2015influence,Li2018resound,li2019repair}; (2) debiasing learning strategies by re-weighting losses or training curriculums \cite{mikolov2013distributed,huang2016learning,lin2017focal,Cui2019Class}; (3) separating biased representations from the unbiased for prediction \cite{misra2016seeing,cadene2019rubi,Tang2020Unbiased}. Our CogTree loss belongs to the second category but differs from existing methods in that we first leverage the hierarchical structure inherent in the relationships, which enables more discriminative representation learning by a coarse-to-fine mode. 














\section{Methodology}
\label{sec:method}

The central goal of our CogTree loss is to enable the existing SGG models to generate  unbiased scene graphs with highly biased data. Since the CogTree loss is model-independent, in this section, we start by summarizing the framework of conventional biased SGG models. Based on this framework, we propose a novel transformer-based SGG network (SG-Transformer) for better considering contextual information as a stronger baseline. Then we introduce the CogTree building process to adaptively construct the coarse-to-fine structure among the independent relationships from the biased SGG models. A CogTree loss is then proposed to train the network by suppressing inter-concept and intra-concept noises in a hierarchical way, resulting in unbiased scene graphs without  requiring extra  supervision. 
The framework is illustrated in Figure \ref{fig:framwork}.   

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure3.pdf}
    \caption{The encoder structure in SG-Transformer.}
    \label{fig:SGTransformer}
\end{figure}

\subsection{The Framework of Biased SGG Models}
\label{ssec:biasedModel}

We summarize the SGG framework from three typical models, i.e. the state-of-the-art MOTIFS \cite{zellers2018neural}, VCTree \cite{tang2019learning} and our proposed Transformer-based SGG network (SG-Transformer). As shown in Figure \ref{fig:framwork}, it mainly contains two processes: object classification (bottom) and relationship classification (top).
\\
\textbf{Object classification} aims to detect the object locations and object classes in an image. Typically, a pre-trained Faster R-CNN \cite{ren2015faster} is applied to extract  objects  and describe object  by the visual feature , the initial object class probabilities , and the spatial feature . The above three kinds of contextual features are concatenated, linear transformed and fed into  to obtain the object embedding , which is then decoded by   to predict its fine-tuned object label .

It is a beneficial prior that some objects always co-occur in different scenes, which is an informative clue to enrich object semantics. In MOTIFS and VCTree, LSTMs/TreeLSTMs are used in  and  to capture the  co-occurrence among objects. In this work, we propose a transformer-based  to adaptively gather multi-view contextual information for a certain object without the limitation of sequential inputs. As shown in the bottom of Figure \ref{fig:SGTransformer}, the encoder consists of  object-to-object (O2O) blocks 
and each block contains a multi-head self-attention layer followed  by  a  fully-connected  layer,  both  succeeded with residual connection and layer normalization \cite{vaswani2017attention}. Our  is a fully connected layer followed by a softmax layer. \\
\textbf{Relationship classification} predicts the relationship  between objects  and   by typically taking three inputs: object embeddings  and , word embeddings of the object predictions  and , and the visual features of the union region . MOTIFS and VCTree utilize sum fusion as  and a fully connected layer as .

Unlike previous works that only encode subject and object features,we observe that the global objects are also beneficial for relationship representations. For example,  \texttt{wave}, \texttt{beach} and their spatial information will help to distinguish whether the relationship is \texttt{man-riding-surfboard} or \texttt{man}\texttt{-carrying}\texttt{-surfboard}. Therefore, we propose a transformer-based  to adaptively capture such contextual semantics by  relation-to-object (R2O) blocks. R2O  differs from O2O only in that the self-attention layer is replaced by a cross-attention layer to gather relevant information from global objects . The input relationship embedding  is computed by a linear transformation of the concatenated feature . The output of the last R2O block will be concatenated with subject  and object  orderly, and fed into the , a fully connected layer followed by a softmax layer, to predict .
\\
\textbf{Training loss} is the softmax cross-entropy loss \cite{zellers2018neural} for both object and relationship classification. It can be regarded as  flat thinking that treats all the classes independently and makes prediction at one time. 

\subsection{Bias-Adaptive Cognition Tree Building}
\label{ssec: buildTree}

Once the above SGG models have been trained on the imbalanced data, the biased models are most likely to predict fine-grained relationships (tail) into inaccurate but reasonable head ones. The models can make rough distinction between remarkably different kinds of relationships. Once predicted into the same class, different relationships mostly share similar properties on either visual appearance (\textit{e.g.} {\ttfamily walking on} and {\ttfamily standing on}) or high-level semantics (\textit{e.g.} {\ttfamily has} and {\ttfamily with}).
We term each distinct kind of relationships sharing similar properties as a \textit{concept}.  
Next, we induce concepts from all the relationships and represent their hierarchical structure by a cognition tree. As shown on the top of Figure \ref{fig:framwork}, this process includes the following three steps (see Appendix 1 for the pseudo code): 
\\
\textbf{Step 1: Bias-Adaptive Concept Induction.} We first induct each relationship into a certain concept based on the biased predictions. Specifically, for all the samples with the same ground-truth class , we predict their relationships via a biased model and calculate the distribution of predicted label frequency, denoted as . The most frequently predicted class for the ground-truth class  is regarded as its \textit{concept relationship}. As shown in Figure \ref{fig:framwork}, {\ttfamily on} is the concept relationship of itself and {\ttfamily standing on}. The above operation induces all the relationships into  concepts with corresponding concept relationships  .  
\\
\textbf{Step 2: Concept-Centered Subtree Building.} We represent the containment structure of relationships in each concept by a \textit{Concept-Centered Subtree}. For the  subtree, the root is  while the leaves are the relationships induced in this concept. Let the edges pointing from the root to the leaves, indicating the containment relations. Note that, if a subtree only contains the root  without leaves, \textit{e.g.} {\ttfamily parked on} in Figure \ref{fig:framwork}, we directly link it to the concept relationship , which has the second highest frequency in  and has leaves in its subtree. This operation supports to induce some tail but isolated relationships 
into the most approximate concepts. This process outputs  subtrees.    
\\
\textbf{Step 3: Cognition-Based Subtree Aggregation.} At last, we aggregate all the  subtrees into one CogTree by a coarse-to-fine approach: we construct 4 layers, including \textit{root layer} , \textit{concept layer} ,  \textit{coarse-fine layer}  and \textit{fine-grained layer} , one at a time progressively. The goal of each layer is to induce relationships into coarser groups than the layer that comes after it. As illustrated in Figure \ref{fig:treeloss},   
contains  virtual nodes, each representing a subtree induced in Step 2. The following  layer distinguishes the coarse and fine-grained relationships in that subtree by splitting each node in  into two nodes: one leaf node indicating the concept relationship while the other virtual node representing the cluster of fine-grained relationships. It is worth mentioning that  only focuses on distinguishing whether the input can be described by a coarse or fine-grained  relationship, without the burden of discriminating slight discrepancy  among  the  fine-grained  ones. The virtual node in  links to its fine-grained relationships of this concept in , which only contains easily confused ones, \textit{e.g.} {\ttfamily standing on} and {\ttfamily walking on}, regardless of most irrelevant relationships.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{Figure4.pdf}
    \caption{Illustration of calculating the TCB loss.}
    \label{fig:treeloss}
\end{figure}


























\iffalse
\begin{itemize}
	\item[\textbf{1.}] For each relation class , we can get a ranked list  by counting the predictions of the biased model in the validation set.  stands for the number one ranked class for the number of times that samples with  as ground-truth were classified as.
	\item[\textbf{2.}] With these ranked lists, next we divide all the classes into several sets, each of which contains the abstract class and the specific classes subordinate to it. The division follows the following rules:
	\begin{itemize}
	    \item[\textbf{a.}]  should be divided into the set containing .
	    \item[\textbf{b.}]  when  and this set contains only , it should be divided into the set containing .
	    \item[\textbf{c.}] the abstract class should be the most frequently used as  () of the other classes in each set.
	\end{itemize}
	
	\item[\textbf{3.}] With these sets and corresponding abstract classes, we build tree structure follows the following   assumption:
	\begin{itemize}
	    \item[\textbf{a.}] different sets should in different sub-trees.
	    \item[\textbf{b.}] the abstract class should be at the parent level of the specific classes instead of the same level,  unless there are only two classes in this sub-tree.
	\end{itemize}
	
\end{itemize}
\fi



\begin{table*}[t]
\centering
\caption{State-of-the-art comparison on Visual Genome dataset.}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
                                               & \multicolumn{3}{c}{Scene Graph Detection}                         & \multicolumn{3}{c}{Scene Graph Classification}                     & \multicolumn{3}{c}{Predicate Classification}  \\ \hline
\multicolumn{1}{l|}{Model}                     & mR@20        & mR@50         & \multicolumn{1}{c|}{mR@100}        & mR@20         & mR@50         & \multicolumn{1}{c|}{mR@100}        & mR@20         & mR@50         & mR@100        \\ \hline
\multicolumn{1}{l|}{IMP+}                       & -            & 3.8           & \multicolumn{1}{c|}{4.8}           & -             & 5.8           & \multicolumn{1}{c|}{6.0}           & -             & 9.8           & 10.5          \\
\multicolumn{1}{l|}{FREQ}                      & 4.5          & 6.1           & \multicolumn{1}{c|}{7.1}           & 5.1           & 7.2           & \multicolumn{1}{c|}{8.5}           & 8.3           & 13.0          & 16.0          \\
\multicolumn{1}{l|}{KERN}                      & -            & 6.4           & \multicolumn{1}{c|}{7.3}           & -             & 9.4           & \multicolumn{1}{c|}{10.0}          & -             & 17.7          & 19.2          \\
\multicolumn{1}{l|}{MOTIFS}                    & 4.2          & 5.7           & \multicolumn{1}{c|}{6.6}           & 6.3           & 7.7           & \multicolumn{1}{c|}{8.2}           & 10.8          & 14.0          & 15.3          \\
\multicolumn{1}{l|}{VCTree}                    & 5.2          & 6.9           & \multicolumn{1}{c|}{8.0}           & 8.2           & 10.1          & \multicolumn{1}{c|}{10.8}          & 14.0          & 17.9          & 19.4          \\ \hline
\multicolumn{1}{l|}{MOTIFS (baseline)}         & 4.1          & 5.5           & \multicolumn{1}{c|}{6.8}           & 6.5           & 8.0           & \multicolumn{1}{c|}{8.5}           & 11.5          & 14.6          & 15.8          \\
\multicolumn{1}{l|}{MOTIFS + Focal}            & 3.9          & 5.3           & \multicolumn{1}{c|}{6.6}           & 6.3           & 8.0           & \multicolumn{1}{c|}{8.5}           & 11.5          & 14.6          & 15.8          \\
\multicolumn{1}{l|}{MOTIFS + Reweight}         & 6.5          & 8.4           & \multicolumn{1}{c|}{9.8}           & 8.4           & 10.1          & \multicolumn{1}{c|}{10.9}          & 16.0          & 20.0          & 21.9          \\
\multicolumn{1}{l|}{MOTIFS + Resample}         & 5.9          & 8.2           & \multicolumn{1}{c|}{9.7}           & 9.1           & 11.0          & \multicolumn{1}{c|}{11.8}          & 14.7          & 18.5          & 20.0          \\
\multicolumn{1}{l|}{MOTIFS + TDE}              & 5.8          & 8.2           & \multicolumn{1}{c|}{9.8}           & 9.8           & 13.1          & \multicolumn{1}{c|}{14.9}          & 18.5          & 25.5          & \textbf{29.1} \\
\multicolumn{1}{l|}{MOTIFS + CogTree}          & \textbf{7.9} & \textbf{10.4} & \multicolumn{1}{c|}{\textbf{11.8}} & \textbf{12.1} & \textbf{14.9} & \multicolumn{1}{c|}{\textbf{16.1}} & \textbf{20.9} & \textbf{26.4} & 29.0          \\ \hline
\multicolumn{1}{l|}{VCTree (baseline)}         & 4.2          & 5.7           & \multicolumn{1}{c|}{6.9}           & 6.2           & 7.5           & \multicolumn{1}{c|}{7.9}           & 11.7          & 14.9          & 16.1          \\
\multicolumn{1}{l|}{VCTree + TDE}              & 6.9          & 9.3           & \multicolumn{1}{c|}{11.1}          & 8.9           & 12.2          & \multicolumn{1}{c|}{14.0}          & 18.4          & 25.4          & 28.7          \\
\multicolumn{1}{l|}{VCTree + CogTree}          & \textbf{7.8}           & \textbf{10.4}          & \multicolumn{1}{c|}{\textbf{12.1}}           & \textbf{15.4} & \textbf{18.8} & \multicolumn{1}{c|}{\textbf{19.9}} &    \textbf{22.0}       &    \textbf{27.6}       &    \textbf{29.7}       \\ \hline
\multicolumn{1}{l|}{SG-transformer (baseline)} & 5.6          & 7.7           & \multicolumn{1}{c|}{9.0}           & 8.6           & 11.5          & \multicolumn{1}{c|}{12.3}          & 14.4          & 18.5          & 20.2          \\
\multicolumn{1}{l|}{SG-transformer + CogTree}  & \textbf{7.9} & \textbf{11.1} & \multicolumn{1}{c|}{\textbf{12.7}} & \textbf{13.0} & \textbf{15.7} & \multicolumn{1}{c|}{\textbf{16.7}} & \textbf{22.9} & \textbf{28.4} & \textbf{31.0} \\ \hline
\end{tabular}}
\label{tab:sota}
\end{table*}

\subsection{Learning with Cognition Tree Loss}
\label{sec:TreeLoss}

Even though the SGG models are encouraged to separate relationships by ``flat'' cross-entropy loss, it is non-trivial to clearly separate  both coarse and fine-grained relationships at one time, especially when similarities existed among them. In fact, it is not a one-shot deal in human mind. Throughout this section, we make unbiased predictions by training the SGG models from scratch based on the above induced CogTree, imitating humans' hierarchical thinking. The basic idea is illustrated in the bottom right of Figure \ref{fig:framwork}. We add loss terms that further encourage the models to separate relationships from coarse to fine progressively. As shown in Figure \ref{fig:treeloss}, we define the loss terms  as follows:
\\
\textbf{Ground-Truth Labels.} Given the ground-truth label of a sample, we track the path from the root to the ground-truth leaf node in  CogTree and denote the path as , where the  node  in the path denotes the ground-truth node at layer . =2 or 3 in our CogTree.
\\
\textbf{Predicted Probability.} Given a sample, the predicted probabilities over all the classes  from a biased model is denoted as =. The probability of each leaf node in CogTree corresponds to the value in  with the same class. The probability of each internal node is the average value of its children. For the internal node , we denote  as its own probability and  as the set of probabilities of its children. 
\\
\textbf{Class-Balanced Weight.} Since the success of re-weighting strategy for debiasing, we adopt a well performed weighting factor   \cite{Cui2019Class} as the balance weight of a leaf node, with the hyper-parameter  and the sample number  of the leaf class. For the weight of each internal node, we compute the average value of its children in the same way as probability computation. We denote  as the weight of node .
\\
\textbf{Cognition Tree Loss.} It contains two parts: the  class-balanced (CB) softmax cross-entropy loss for ``flat'' thinking among independent relationships and the tree-based class-balanced (TCB) softmax cross-entropy loss for ``coarse-to-fine'' thinking along the ground-truth path in CogTree. Given a sample with the ground-truth label , the  CB loss is computed between predicted probabilities  and label  as: 



For this sample, if the ground-truth path in CogTree has  nodes, except the leaf node, we would have  different terms in our TCB loss. For the root and each internal node in the  ground-truth path, we compute the class-balanced softmax cross-entropy across the children and average over all the terms to obtain the TCB loss as:



TCB loss  enables  the  network  to  surpass  the  noises from inter-concept relationships to learn concept-independent representations first, and then surpass  the  noises from intra-concept relationships to refine relationship-independent representations,  resulting in more accurate and discriminative representations. The CogTree loss is defined as a weighted sum of  and  to leverage both of their advantages:

where  is a hyper-parameter. The SGG model is trained from scratch by the CogTree loss without any extra supervision. In the test stage, the models predict via original decoders without any modification. 










\section{Experiments}
\label{sec:experiments}




\iffalse
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.18]{figure/induced-tree.png}
    \caption{Cognition tree structure.}
    \label{fig:trees}
\end{figure}
\fi

\textbf{Dataset:} We evaluate the CogTree loss on the Visual Genome  dataset \cite{krishna2017visual}, which contains 108k images, 75k object classes and 37k predicate (\textit{i.e.} relationship) classes. Since 92\% predicate classes have less than 10 samples,  we adopt the widely used VG split \cite{xu2017scene,zellers2018neural,tang2019learning,chen2019counterfactual,Tang2020Unbiased}, with the 150 most frequent object classes and 50 predicate classes. The VG split only contains training set and test set and we follow previous work \cite{zellers2018neural} to sample a 5K validation set from the training set.
\\
\textbf{Tasks and Evaluation:} Following previous work \cite{zellers2018neural}, the SGG task can be divided into three sub-tasks: (1) Predicate Classification (\textbf{PredCls}) takes the ground-truth object labels and bounding boxes for relationship prediction; (2) Scene Graph Classification (\textbf{SGCls}) takes ground-truth bounding boxes for object label prediction; (3) Scene Graph Detection (\textbf{SGDet}) predict SGs from scratch. 
To evaluate the unbiased SGG, we follow \cite{chen2019knowledge,tang2019learning} to use the unbiased metric mean Recall@K (\textbf{mR@K}), which calculates R@K for each class separately and average R@K for all the classes.
\\
\textbf{Implementation:} We use a pre-trained Faster R-CNN \cite{ren2015faster} with ResNeXt-101-FPN \cite{Lin2017Feature} as the object detector. SG-transformer contains 3 O2O blocks, 2 R2O blocks and 12 attention heads. The balanced weight  in CogTee loss is set to 1.  in the re-weighting factor is set to 0.999. Our models are trained by SGD optimizer with 5 epochs, where the mini-batch size is 12 and the learning rate is .  All the experiments are implemented with PyTorch and conducted with NVIDIA Tesla V100 GPUs.   






\iffalse
\begin{table*}[t]
\centering
\caption{Ablation study of key components of our method.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccccc}
\hline
                       &                                                          & \multicolumn{3}{c}{Scene Graph Detection}                            & \multicolumn{3}{c}{Scene Graph Classification}                        & \multicolumn{3}{c}{Predicate Classification}    \\ \hline
\multicolumn{2}{l|}{Method}                                                       & mR@20         & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20          & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20         & mR@50          & mR@100         \\ \hline
\multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{O2O}                                 & 5.03          & 6.85           & \multicolumn{1}{c|}{7.87}           & 7.21           & 9.03           & \multicolumn{1}{c|}{9.66}           & 12.31         & 15.98          & 17.62          \\
\multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{R2O}                                 & 4.88          & 6.77           & \multicolumn{1}{c|}{7.78}           & 7.58           & 9.54           & \multicolumn{1}{c|}{10.19}          & 12.59         & 16.28          & 17.80           \\
\multicolumn{1}{l|}{3} & \multicolumn{1}{l|}{O2O + R2O}                           & 5.55          & 7.74           & \multicolumn{1}{c|}{8.98}           & 8.57           & 11.46          & \multicolumn{1}{c|}{12.27}          & 14.35         & 18.48          & 20.21          \\ \hline
\multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{O2O + R2O + }                      & 6.74          & 9.56           & \multicolumn{1}{c|}{11.29}          & 10.76          & 13.13          & \multicolumn{1}{c|}{13.88}          & 18.02         & 23.40          & 25.25           \\
\multicolumn{1}{l|}{5} & \multicolumn{1}{l|}{O2O + R2O + }                     & 7.57          & 10.53          & \multicolumn{1}{c|}{11.86}          & 12.14          & 14.42          & \multicolumn{1}{c|}{15.29}          & 21.16     & 26.14     & 28.32               \\
\multicolumn{1}{l|}{6} & \multicolumn{1}{l|}{O2O + R2O + }                     & 7.70          & 10.39          & \multicolumn{1}{c|}{12.07}          & 12.15          & 15.07          & \multicolumn{1}{c|}{16.15}          & 21.08     & 27.08     & 29.41               \\ \hline
\multicolumn{1}{l|}{7} & \multicolumn{1}{l|}{O2O + R2O + } & \textbf{7.92}          & \textbf{11.05}          & \multicolumn{1}{c|}{\textbf{12.7}}           & \textbf{12.96}          & \textbf{15.68}          & \multicolumn{1}{c|}{\textbf{16.72}}          & \textbf{22.89}         & \textbf{28.38}          & \textbf{30.97}          \\ \hline
\end{tabular}}
\label{tab:ablation}
\end{table*}
\fi




\subsection{State-of-the-Art Comparison}




\begin{table*}[t]
\centering
\caption{Ablation study of key components in our CogTree loss.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccccc}
\hline
                       &                                                          & \multicolumn{3}{c}{Scene Graph Detection}                            & \multicolumn{3}{c}{Scene Graph Classification}                        & \multicolumn{3}{c}{Predicate Classification}   
\\ \hline
\multicolumn{2}{l|}{Method}                                                       & mR@20         & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20          & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20         & mR@50          & mR@100         
\\ \hline
\multicolumn{2}{l|}{\textbf{CogTree +  (full model)}} & \textbf{7.92}          & \textbf{11.05}          & \multicolumn{1}{c|}{\textbf{12.70}}           & \textbf{12.96}          & \textbf{15.68}          & \multicolumn{1}{c|}{\textbf{16.72}}          & \textbf{22.89}         & \textbf{28.38}          & \textbf{30.97}          \\ \hline
\multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{CogTree + }                     & 7.70          & 10.39          & \multicolumn{1}{c|}{12.07}          & 12.15          & 15.07          & \multicolumn{1}{c|}{16.15}          & 21.08     & 27.08     & 29.41               
\\ 
\multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{CogTree + }                     & 7.57          & 10.53          & \multicolumn{1}{c|}{11.86}          & 12.14          & 14.42          & \multicolumn{1}{c|}{15.29}          & 21.16     & 26.14     & 28.32               
\\
\multicolumn{1}{l|}{3} & \multicolumn{1}{l|}{}                      & 6.74          & 9.56           & \multicolumn{1}{c|}{11.29}          & 10.76          & 13.13          & \multicolumn{1}{c|}{13.88}          & 18.02         & 23.40          & 25.25           \\
\multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{ }                           & 5.55          & 7.74           & \multicolumn{1}{c|}{8.98}           & 8.57           & 11.46          & \multicolumn{1}{c|}{12.27}          & 14.35         & 18.48          & 20.21         
\\ 
\hline
\multicolumn{1}{l|}{5} & \multicolumn{1}{l|}{Fuse-layer + } & 5.86  & 8.02 & \multicolumn{1}{c|}{9.05}    & 8.17   & 10.39  & \multicolumn{1}{c|}{11.32}   & 13.77         & 18.87        & 20.77          
\\ 
\multicolumn{1}{l|}{6} & \multicolumn{1}{l|}{Fuse-subtree + }  & 5.36  & 7.19  & \multicolumn{1}{c|}{8.28}   & 8.71  & 10.66  & \multicolumn{1}{c|}{11.61}   & 16.20         & 20.17        & 22.12         \\
\multicolumn{1}{l|}{7} & \multicolumn{1}{l|}{Cluster-tree + }  & 5.84  & 8.10 & \multicolumn{1}{c|}{9.12}   & 8.86  & 10.88  & \multicolumn{1}{c|}{11.52}   & 15.12         & 19.20        & 20.81         \\
\hline
\multicolumn{1}{l|}{8} & \multicolumn{1}{l|}{CogTree + (MAX)}  & 5.38  & 7.16 & \multicolumn{1}{c|}{8.16}   & 8.97  & 10.85  & \multicolumn{1}{c|}{11.83}   & 15.48         & 19.93        & 21.87         \\
\multicolumn{1}{l|}{9} & \multicolumn{1}{l|}{CogTree + (SUM)}  & 1.86  & 3.09  & \multicolumn{1}{c|}{3.68}   & 6.58  & 8.82  & \multicolumn{1}{c|}{9.86}   & 11.31         & 15.67        & 17.98   \\      
\hline
\end{tabular}}
\label{tab:ablation}
\end{table*}

We evaluate the CogTree loss on three baseline models: MOTIFS, VCTree, and SG-Transformer, and compare the performance with the state-of-the-art debiasing approach TDE \cite{Tang2020Unbiased}. All the above models share the same pre-trained Faster R-CNN detector and sum fusion decoder (except SG-Transformer). We also compare the performance with existing biased models, including IMP+ \cite{xu2017scene}, FREQ \cite{zellers2018neural}, KERN \cite{chen2019knowledge}, MOTIFS \cite{zellers2018neural}, and VCTree \cite{tang2019learning,Tang2020Unbiased}. 





In Table \ref{tab:sota}, we have the following observations: (1) CogTree loss is a stable method that remarkably improves all the baselines on all the metrics. SGCls and PredCls achieve more significant improvements compared with SGDet. (2) CogTree loss consistently outperforms conventional debiasing methods, including focal, reweight and resample, which achieve limited performance increase on MOTIFS. (3) Compared with the state-of-the-art debiasing method TDE, CogTree loss has obvious advantages on all the baselines. Moreover, the results of R@K in Appendix 2.1 show a performance increase from TDE to CogTree, which indicates that our method generates better unbiased SGs while keeping more correct head predicates compared with TDE. (4) SG-Transformer consistently outperforms all the existing SGG models and baselines, which verifies that the contextual information gathered by transformer structure 
is of great benefit for discriminative object and relationship representations. Notably, SG-Transformer achieves new state-of-the-art performance on SGG tasks among the biased models.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure5.pdf}
    \caption{R@100 of the most frequent 35 classes on PredCls.}
    \label{fig:recall}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure6.pdf}
    \caption{t-SNE visualization of relationship embeddings. A-C are from SG-Transformer while D-F are from SG-Transformer+CogTree.}
    \label{fig:tsne}
\end{figure}


We also report the R@100 performance of each class in Figure \ref{fig:recall}. SG-Transformer+CogTree obviously improves from SG-Transformer at most tail classes but drops from SG-Transformer at a few head classes, further indicating that the increase on mR@K is mainly due to the improvement of the tail classes instead of the head. All the improvement should owe to the discriminative relationship representations leaned by the CogTree loss. As the t-SNE visualization shown in Figure \ref{fig:tsne}, samples of different relationships, including both the concept relationships (A, D) and the fine-grained ones (B, C, E, F) are separated more obviously while samples of each relationship are clustered more densely. 




\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figure7.pdf}
    \caption{Visualization of scene graphs generated by SG-Transformer (blue) and SG-Transformer+CogTree (green). Compared with the ground-truth, the quality of predicted relationships are marked in three colors: red (false), blue (correct), purple (better).}
    \label{fig:cases}
\end{figure*}










\subsection{Ablation Study}
In Table \ref{tab:ablation}, we show ablation results to verify the contribution of each component in our CogTree loss on the SG-Transformer baseline. In models `1-4’, we assess the \textbf{effectiveness of each part in the loss}. Compared with the full model, the performance of both `1’ and `3’ decreases when removing the other loss term, which indicates that both hierarchical and flat losses are beneficial to provide complementary information for SGG tasks. Thereinto, the hierarchical loss has greater influence than the flat one. When removing the class-balanced weight from both `1’ and `3’, `2' and `4' result in a further decrease. It proves the benefits of re-weighting in model debiasing, though it is obviously less influential on the  hierarchical loss. Model `4' utilizes the softmax cross-entropy loss and obtains the worst performance. 

In models `5-7', we evaluate the \textbf{influence of the tree structure} on the performance (see Appendix 3 for tree structure visualization). The CogTree is built based on two principles: a) Relationships belonging to the same concept are organized in one subtree; b) Relationships in one subtree are organized in different layers from coarse to fine.  We manually adjust the induced tree that violates the above principles and evaluate the performance with the same loss . In `5', we violate the second principle and place the fine-grained relationships in the same coarse-fine layer as the concept relationship. In `6',  we disregard the first principle and mix relationships belonging to different concepts in one subtree. Specifically, we simply link all the concept relationships flatly to the root while placing all the fine-grained relationships flatly in a subtree linked to the root. We can observe a significant decrease in performance when the tree structure  does not satisfy either of the two principles. In `7', we build the tree structure by the conventional hierarchical clustering on relationship representations extracted from the last fully-connected layer weights \cite{wan2020nbdt}. This tree is indicative of the visual similarity of relationships, which results in an obvious decrease compared to the full model. It indicates that the relationship hierarchy is better based on semantic correlation instead of visual similarity. 

In models `8' and `9', we test the \textbf{influence of different functions: MAX and SUM, for calculating the predicted probabilities and the class-balanced weights} of internal nodes. Compared with the full model using AVERAGE, both `8' and `9' have an obvious performance drop. Further analysis implies that both MAX and SUM increase the predicted probabilities of the internal node in the coarse-fine layer, thus decreasing the 
incorrect prediction penalty  of fine-grained relationships in Eq. \ref{eq:TCB}. In comparison, the increased values of balancing weights by MAX and SUM have less influence on the final performance. 

\iffalse
\begin{table}[h]
\caption{Influence of Tree Hierarchies analysis.}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
                                   & \multicolumn{3}{c}{Scene Graph Detection}   & \multicolumn{3}{c}{Scene Graph Classification} & \multicolumn{3}{c}{Predicate Classification} \\ \hline
\multicolumn{1}{l|}{Model}         & mR@20 & mR@50 & \multicolumn{1}{c|}{mR@100} & mR@20  & mR@50  & \multicolumn{1}{c|}{mR@100}  & mR@20         & mR@50        & mR@100        \\ \hline
\multicolumn{1}{l|}{} & 6.74  & 9.56  & \multicolumn{1}{c|}{11.29} & 10.76 & 13.13 & \multicolumn{1}{c|}{13.88}  & 18.02         & 23.40          & 25.25           \\ \hline
\multicolumn{1}{l|}{(clustering)}  & 5.84  & 8.10 & \multicolumn{1}{c|}{9.12}   & 8.86  & 10.88  & \multicolumn{1}{c|}{11.52}   & 15.12         & 19.20        & 20.81         \\
\multicolumn{1}{l|}{(fuse-tree)}  & 5.36  & 7.19  & \multicolumn{1}{c|}{8.28}   & 12.15  & 14.90  & \multicolumn{1}{c|}{15.91}   & 21.54         & 27.19        & 29.60         \\
\multicolumn{1}{l|}{(fuse-level)} & 5.86  & 8.02 & \multicolumn{1}{c|}{9.05}    & 8.17   & 10.39  & \multicolumn{1}{c|}{11.32}   & 13.77         & 18.87        & 20.77          \\ 
\multicolumn{1}{l|}{(induced)}        & 7.92          & 11.05          & \multicolumn{1}{c|}{12.7}           & 12.96          & 15.68          & \multicolumn{1}{c|}{16.72}          & 22.89         & 28.38          & 30.97         \\ \hline
\end{tabular}}
\label{tab:tree-influence}
\end{table}
\fi



\subsection{Qualitative Analysis}
We visualize several PredCls samples that generated by SG-Transformer (blue) and SG-Transformer+CogTree (green) in Figure \ref{fig:cases}. Even though SG-Transformer is the strongest baseline among the three, it still achieves remarkable improvement when equipped with the CogTree loss: (1) CogTree loss encourages the model to predict more accurate and discriminative relationships compared with the baseline. As shown in the first two rows, the baseline prefers to predict the reasonable but trivial head classes  {\ttfamily on} and  {\ttfamily near} due to the biased training, while our model precisely predicts the fine-grained and informative relationships like  {\ttfamily parked on} 
and {\ttfamily in front of}. It mainly because that the CogTree loss effectively separates the concept relationships apart from the fine-grained ones in the coarse-fine layer. (2) CogTree loss enables the model to distinguish visually and semantically similar relationships, which is quite difficult for the baseline as shown in the last row. The baseline falsely predicts  {\ttfamily walking on} as {\ttfamily standing on} since it can not capture the detailed difference between the two actions. Our model succeeds in such prediction 
by surpassing noises from all the irrelevant relationships and just focusing on semantically similar ones.






\begin{table}[t]
\centering
\caption{Performance with different balancing weights.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\hline
& \multicolumn{3}{c}{Scene Graph Detection}   & \multicolumn{3}{c}{Scene Graph Classification} & \multicolumn{3}{c}{Predicate Classification} \\ \hline
\multicolumn{1}{c|}{}    & mR@20 & mR@50 & \multicolumn{1}{c|}{mR@100} & mR@20  & mR@50  & \multicolumn{1}{c|}{mR@100}  & mR@20         & mR@50        & mR@100        \\ \hline
\multicolumn{1}{c|}{0.4}  & 7.22   & 9.72 & \multicolumn{1}{c|}{11.30}  & 12,09  & 15.03  & \multicolumn{1}{c|}{16.12}   & 21.17     & 27.20        & 29.59         \\
\multicolumn{1}{c|}{0.7}  & \textbf{8.62}   & \textbf{11.30} & \multicolumn{1}{c|}{12.70}  & 12.22  & 14.91  & \multicolumn{1}{c|}{15.88}   & 21.28         & 26.39        & 28.69         \\
\multicolumn{1}{c|}{1}    & 7.92  & 11.05 & \multicolumn{1}{c|}{\textbf{12.70}}  & \textbf{12.96}   & \textbf{15.68}  & \multicolumn{1}{c|}{\textbf{16.72}}   & \textbf{22.89}          & \textbf{28.38}        & \textbf{30.97}         \\
\multicolumn{1}{c|}{1.3}  & 7.84  & 10.48 & \multicolumn{1}{c|}{12.19}  & 12.35   & 15.04  & \multicolumn{1}{c|}{15.87}   & 22.06     & 27.11        & 29.21         \\
\multicolumn{1}{c|}{1.6}  & 6.78  & 9.04  & \multicolumn{1}{c|}{10.22}  & 12.20   & 14.70  & \multicolumn{1}{c|}{15.82}   & 21.34        & 26.80     & 29.18          \\\hline
\end{tabular}}
\label{tab:parameter-loss}
\end{table}

\iffalse
\begin{table}[t]
\centering
\caption{Performance with different transformer blocks.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llccccccccc}
\hline
    &                          & \multicolumn{3}{c}{Scene Graph Detection}                            & \multicolumn{3}{c}{Scene Graph Classification}                        & \multicolumn{3}{c}{Predicate Classification}    \\ \hline
O2O & \multicolumn{1}{l|}{R2O} & mR@20         & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20          & mR@50          & \multicolumn{1}{c|}{mR@100}         & mR@20         & mR@50          & mR@100         \\ \hline
2   & \multicolumn{1}{l|}{2}   & 7.82          & 10.39          & \multicolumn{1}{c|}{11.97}          & 13.53          & 16.29          & \multicolumn{1}{c|}{17.50}     & 23.30         & 29.18               & 31.70                \\
3   & \multicolumn{1}{l|}{2}   & \textbf{9.72} & \textbf{12.64} & \multicolumn{1}{c|}{\textbf{14.22}} & \textbf{13.56} & \textbf{16.53} & \multicolumn{1}{c|}{\textbf{17.56}} & \textbf{23.9} & \textbf{29.74} & \textbf{32.39} \\
3   & \multicolumn{1}{l|}{3}   & 8.89          & 11.63          & \multicolumn{1}{c|}{13.54}          & \textbf{13.65} & \textbf{16.77} & \multicolumn{1}{c|}{\textbf{17.84}} & 19.34         & 24.97          & 27.54          \\
4   & \multicolumn{1}{l|}{2}   & 8.16          & 10.67          & \multicolumn{1}{c|}{12.14}          & 13.42          & 16.06          & \multicolumn{1}{c|}{17.01}          & 13.64         & 20.42          & 24.72           \\ \hline
\end{tabular}}
\label{tab:parameter-trf}
\end{table}
\fi

\subsection{Parameter Analysis}
In Table \ref{tab:parameter-loss}, we assess the influence of different values of  on the performance.  achieves the best performance on most metrics. The performance drops slightly in the range of . We also vary the number of O2O and R2O blocks from 2 to 4 in SG-Transformer (see Appendix 2.2) and obtain the highest mR@K with 3 O2O blocks and 2 R2O blocks. We use the above settings in our full model.



\section{Conclusion}
In this paper, we propose a CogTree loss to generate  unbiased scene graphs with highly biased data, which focuses on hierarchical relationship distinction from the cognition perspective.  We novelly leverage the biased prediction from SGG models to organize the independent relationships by a tree structure, which contains multiple layers corresponding to the relationships from coarse to fine. We propose a CogTree loss specially for the above tree structure that supports hierarchical distinction for the correct relationships while progressively eliminating the irrelevant ones. The loss is model-independent and consistently boosting the performance of various SGG models with remarkable improvement. How to incorporate commonsense knowledge to optimize the CogTree structure will be our future work.


\bibliography{aaai2021}



\end{document}
