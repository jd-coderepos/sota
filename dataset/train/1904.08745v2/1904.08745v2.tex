\documentclass{article} \usepackage{iclr2019_conference,times}
\usepackage{mathrsfs,amsmath}
\usepackage{amsthm}
\usepackage{paralist}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{svg}
\usepackage{booktabs}       \usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]

\usepackage{xcolor}

\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}., }

\title{edGNN: A Simple and Powerful GNN for Directed Labeled Graphs}


\author{Guillaume Jaume\thanks{equal contribution} \\
IBM Research, Zurich \\
EPFL, Lausanne \\
\texttt{gja@zurich.ibm.com} \\
\And
An-phi Nguyen\footnotemark[1] \\
IBM Research, Zurich \\
ETH, Zurich \\
\texttt{uye@zurich.ibm.com} \\
\And 
Mar\'{i}a Rodr\'{i}guez Mart\'{i}nez \\
IBM Research, Zurich \\
\texttt{mrm@zurich.ibm.com}
\And 
Jean-Philippe Thiran \\
EPFL, Lausanne \\
\texttt{jean-philippe.thiran@epfl.ch} \\
\And
Maria Gabrani \\
IBM Research, Zurich \\
\texttt{mga@zurich.ibm.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}

The ability of a graph neural network (GNN) to leverage both the graph topology and graph labels is fundamental to building discriminative node and graph embeddings. Building on previous work, we theoretically show that edGNN, our model for directed labeled graphs, is as powerful as the Weisfeiler--Lehman algorithm for graph isomorphism. Our experiments support our theoretical findings, confirming that graph neural networks can be  used effectively for inference problems on directed graphs with both node and edge labels. Code available at \url{https://github.com/guillaumejaume/edGNN}.

\end{abstract}













\section{Introduction}



In recent years, much work has been devoted to extending deep-learning models to graphs,~\eg~\cite{Scarselli2009, Bruna, Li2015b, Defferrard, Kipf2017, Hamilton2017, Hamilton2017a, Velickovic2017a, Ying2018a}. \cite{Gilmer2017a} formulated numerous such models within their proposed Message Passing Neural Network (MPNN) framework. In this model, the state of a vertex is represented by a feature vector. The state is then  updated iteratively via a two-step strategy: For each vertex \begin{inparaenum}[(i)]
\item in the aggregation step, the feature vectors of the neighboring vertices are combined into a single vector via a differentiable operator, \eg a sum. Then, 
\item in the update step, a new state is computed by applying another differentiable operator, \eg a one-layer perceptron, to the current state of the vertex and to the aggregate vector from the previous step.
\end{inparaenum} 

In two recent works, \cite{Xu2018} and \cite{Morris2018}, independently proved that certain formulations of MPNNs are \emph{as powerful as} the Weisfeiler--Lehman (WL) algorithm for graph isomorphism~(\cite{Weisfeiler1968}). 
In practice, this means that there exist MPNNs able to learn \emph{unique} representations for 
(\emph{almost})
\emph{all}  undirected node-labeled graphs, which is desirable for such tasks as node or graph classification. 
Note that previous work has drawn a parallel between the WL test and the MPNN, \eg ~\cite{Hamilton2017, Jin2017, Lei2017}.

In this paper, we extend the above-mentioned results to \emph{directed} graphs with labels for both nodes and \emph{edges}. In particular, by extending the theoretical framework provided by \cite{Morris2018}, we show that there exist MPNNs as powerful as the (one-dimensional) WL algorithm for directed labeled graphs. Although this problem has already been addressed,~\eg~\cite{Li2015b, Niepert2016, Simonovsky2017, Beck2018, Schlichtkrull2018}, we present a theoretically-grounded GNN formulation for directed labeled graphs. We experimentally corroborate our theoretical results by comparing our model, edGNN, against state-of-the-art models for node and graph classification.












\section{Theoretical framework}
\subsection{Notation and setup}

As our work is an extension of~\cite{Morris2018}, we maintain a notation consistent to theirs in order to more easily reference their results. 

A graph $G$ is a pair $(V,E)$, where $V$ is the set of vertices, $E$ is the set of edges and the directed edge $(u,v) \in E$ for $u, v \in V$ is an edge starting in $u$ and ending in $v$. We will denote the vertex and the edge sets of $G$ as $V_G$ and $E_G$, respectively.

We are interested in graphs with both node and edge labels. We therefore assume that, given a graph $G$, there exists a vertex-labeling function $l_V: V_G \rightarrow \mathcal{X}$ and an edge-labeling function $l_E: E_G \rightarrow \mathcal{Z}$ that assign to each vertex and edge of $G$ a label from \emph{countable} sets $\mathcal{X}$ and $\mathcal{Z}$. For the rest of this paper, we will refer to graphs with node and edge labels simply as \emph{labeled graphs}.


For each vertex $v \in V$, we can define the neighborhood $\mathcal{N}(v) := \{u \in V \; | \; (v,u) \in E \; \vee \; (u,v) \in E\}$. As we are dealing with directed graphs, we distinguish between incoming neighbors $\mathcal{N}^I(v) := \{u \in V \; | \; (u,v) \in E\}$ and outgoing neighbors $\mathcal{N}^O(v) := \{u \in V \; | \; (v,u) \in E\}$. Naturally, $\mathcal{N}(v) = \mathcal{N}^I(v) \cup \mathcal{N}^O(v)$. The cardinalities $|\mathcal{N}(v)|, \; |\mathcal{N}^I(v)| \; \mathrm{and} \; |\mathcal{N}^O(v)|$ of the neighborhoods are referred to as the \emph{degree}, the \emph{in-degree} and the \emph{out-degree} of vertex $v$, respectively.

\begin{definition}
Two labeled directed graphs $G$ and $H$ are \emph{isomorphic} if there exists a bijection $f: V_G \rightarrow V_H$ such that $(u, v) \in E_G$ if and only if $(f(u), f(v)) \in E_H$ with $l_{V_G}(v) = l_{V_H}(f(v))$ and $l_{E_G}(u, v) = l_{E_H}(f(u), f(v))$. 
\end{definition} 

\subsubsection{The Weisfeiler--Lehman algorithm}\label{sec:wl_test}

The Weisfeiler--Lehman (WL) test~(\cite{Weisfeiler1968}) is an algorithm to distinguish whether two graphs are non-isomorphic. We present the test in its one-dimensional variant, also known as the \emph{naive vertex refinement}. We will start by presenting the WL test on node-labeled graphs, and later discuss its extension to directed labeled graphs. 

At initialization, the vertices are labeled consistently with the vertex-labeling function $l_V$. We call this the \emph{initial coloring} of the graph and we denote it as $c_l^{(0)}(v) := l_V(v), \; \forall v \in V$. The algorithm then proceeds in a recursive fashion. At iteration $t$, new labels are computed for each vertex from the current labels of the vertex itself and its neighbors,~\ie
\begin{align}\label{eq:wl_iteration}
    c_l^{(t)}(v) = g\Big(c_l^{(t-1)}(v), \big\{\big\{ c_l^{(t-1)}(u): u \in \mathcal{N}(v) \big\}\big\}\Big),
\end{align}
where $g$ is an injective hashing function, and $\big\{\big\{\cdot\big\}\big\}$ denotes a \emph{multiset},~\ie a generalization of a set that allows elements to be repeated.
Each iteration is performed in parallel for the two graphs to be tested, $G$ and $H$. If at some iteration $t$, the number of vertices assigned to a label $l \in \mathcal{X}$ differs for the two graphs, then the algorithm stops, concluding that the two graphs are not isomorphic. Otherwise, the algorithm will stop whenever a \emph{stable coloring} is achieved,~\ie whenever $c_l^{(t)}(v_G) = c_l^{(t)}(v_H)$ for all $t \geq T$ and for any pair $(v_G, v_H)$ with $v_G \in V_G$, $v_H \in V_H$, and $c_l^{(T)}(v_G) = c_l^{(T)}(v_H)$.
This is guaranteed to happen at most after $T=\mathrm{max}\{|V_G|,|V_H|\}$ iterations.
In this case, $G$ and $H$ are considered isomorphic. 

Despite a few corner cases, the WL test is able to distinguish a wide class of graphs~(\cite{Cai1992}). Moreover, the most efficient implementation of the algorithm has a runtime complexity which is quasi-linear in the number of vertices~(\cite{Grohe2017a}).


The extension of the WL test to a directed graph with edge labels is straightforward~(\cite{Grohe2017a, Orsini2015}). During the recursive step, for each vertex $v$, we need to include  the in-degrees and  out-degrees of $v$ separately in the hashing function \emph{with respect to each} edge label.

Let us denote an edge label as $e \in \mathcal{Z}$. For each vertex $v$, we then define $n^I_v(e) := |\{u \in \mathcal{N}^I(v) \; | \; l_E(u,v) = e\}|$ as the number of edges incoming to $v$ with label $e$. Similarly, $n^O_v(e)$ is defined for outgoing edges. Then,  Eq.~(\ref{eq:wl_iteration}) can be adapted for labeled directed graphs in the following way:
\begin{equation}\label{eq:wl_iteration_dir_edgelabel}
\begin{split}
    c_l^{(t)}(v) = g\Big(c_l^{(t-1)}(v), &\big\{\big\{ c_l^{(t-1)}(u): u \in \mathcal{N}(v) \big\} \big\}, \\ &\big\{ \big(n^I_v(e), e\big): \exists (u,v) \in E\;\mathrm{with}\;l_E(u,v) = e \big\},\\ &\big\{ \big(n^O_v(e), e\big): \exists (v,u) \in E\;\mathrm{with}\;l_E(v,u) = e \big\}\Big).
\end{split}
\end{equation}


\subsection{Graph neural networks}

Graph neural networks architectures implement a neighborhood aggregation strategy. Similar to~\cite{Morris2018}, our work focuses on the GNN model presented by \cite{Hamilton2017}, which implements this strategy with the node update function 
\begin{align}\label{eq:1gnn}
    f^{(t)}(v) = \sigma\Big(f^{(t-1)}(v)W_1^{(t)} + \sum_{u \in \mathcal{N}(v)}f^{(t-1)}(u)W_2^{(t)}\Big),
\end{align}

where $f^{(t)}(v) \in \mathbb{R}^{1 \times d^{(t)}}$ is the $d^{(t)}$-dimensional node representation, or node embedding, at time step $t$ of the vertex $v$, and $W_1^{(t)}, W_2^{(t)} \in \mathbb{R}^{d^{(t-1)}\times d^{(t)}}$ are weight matrices. The initial representation $f^{(0)}(v)$ is \emph{consistent} with the vertex-labeling function $l_V$, \ie~$f^{(0)}(v) = f^{(0)}(u)$ if and only if $l_V(v) = l_V(u)$ for all $v,u \in V$. \cite{Morris2018} showed that there exists a sequence $(W_1^{(t)},W_2^{(t)})$ such that the GNN model in Eq.~(\ref{eq:1gnn}) is as powerful as the WL test.


\subsubsection{Extension to directed labeled graphs}

The extension of Eq.~(\ref{eq:1gnn}) to directed labeled graphs follows the WL test extension. We simply need to augment the equation with embeddings for the labeled edges with incoming and outgoing edges considered separately,~\ie
\begin{equation}\label{eq:1gnn_edges}
\begin{split}
    f^{(t)}(v) = \sigma\Big(&f^{(t-1)}(v)W_1^{(t)} + \sum_{u \in \mathcal{N}(v)}f^{(t-1)}(u)W_2^{(t)} + \\ &+ \sum_{(u,v) \in E}f_E\big(u,v, l_E(u,v)\big)W_3^{(t)} + \sum_{(v,u) \in E}f_E\big(v,u, l_E(v,u)\big)W_4^{(t)}\Big),
\end{split}
\end{equation}

where $f_E\big(v,u,l_E(v,u)\big) \in \mathbb{R}^{1 \times d_E}$ is the $d_E$-dimensional embedding of the edge $(v,u)$ with label $l_E(v,u)$. The embeddings $f_E$ should be defined such that $\sum_{(u,v) \in E}f_E\big(u,v, l_E(u,v)\big) = \sum_{(u,v') \in E}f_E\big(u,v', l_E(u,v')\big)$ if and only if $\big\{ \big(n^I_v(e), e\big): \exists (u,v) \in E\;\mathrm{with}\;l_E(u,v) = e \big\} = \big\{ \big(n^I_{v'}(e), e\big): \exists (u,v') \in E\;\mathrm{with}\;l_E(u,v') = e \big\}$. The same should hold for outgoing edges. For practical applications, this can be achieved by using one-hot encodings of the edge labels.\\
We can now directly extend the theorems presented in~\cite{Morris2018}.
\begin{theorem}[Theorem 1 in \cite{Morris2018}]\label{thm:at_most} Let G be a directed labeled graph. Then for all $t\geq0$ and for all choices of initial colorings $f^{(0)}$ consistent with $l_V$ and of edge embeddings $f_E$ consistent with $l_E$, and weights $W_1^{(t)}, W_2^{(t)},W_3^{(t)},W_4^{(t)}$
\begin{align}\label{eq:refines}
    c_l^{(t)}(v) = c_l^{(t)}(u) \Rightarrow f^{(t)}(v) = f^{(t)}(u) \;\; \forall u,v \in V
\end{align}
with $c_l^{(t)}$ and $f^{(t)}$ defined in Eqs.~(\ref{eq:wl_iteration_dir_edgelabel}) and (\ref{eq:1gnn_edges}),  respectively. 
\end{theorem}
\cite{Morris2018} prove this theorem by induction. The proof is essentially the same for our extended case. In fact, as neither the labels $l_E$ nor the embeddings $f_E$ change over the iterations, there is no need to include them in the induction step.
\begin{theorem}[Theorem 2 in \cite{Morris2018}]\label{thm:exists} Let G be a directed labeled graph with finite vertex degree. Then there exists a sequence $(W_1^{(t)}, W_2^{(t)},W_3^{(t)},W_4^{(t)})$ with $t\geq0$ such that
\begin{align}\label{eq:equiv}
     c_l^{(t)}(v) = c_l^{(t)}(u)  \Leftrightarrow f^{(t)}(v) = f^{(t)}(u) \;\; \forall u,v \in V
\end{align}
\end{theorem}

The proof is provided in the supplemental material. Note that we specifically require the graph to have finite vertex degree. However, this is not a strong assumption in real-world applications.

\subsubsection{Graph classification}

For graph classification tasks, we need a representation $f_G$ of the graph. We build it from the node representations $f^{(t)}(v)$ following the formulation of \cite{Xu2018}:  \begin{align}
    f_G = \mathrm{Concat}\Big( \Big\{\sum_{v \in V_G} f^{(t)}(v) \;\Big|\;t = 0, \dots , T\Big\}\Big).
\end{align}
Note that, although  $T$ should theoretically be at least $|V_G|$ (Section~\ref{sec:wl_test}), only a few layers (\ie iterations) are used in practice to update the node representations. Finally, a linear classifier is  applied to the graph representation to perform the classification.

\section{Experiments}

\subsection{Datasets and baselines}



We benchmark our algorithm edGNN on graph and node classification tasks.
For graph classification tasks, we benchmark our model against the Subgraph Matching Kernel (CSM)~(\cite{Kriege2012}), Weisfeiler--Lehman Shortest Path Kernel~(\cite{Shervashidze2011}) and R-GCN~(\cite{Schlichtkrull2018})
. As R-GCN only defines how to build node embeddings, we reuse the formulation of \cite{Xu2018} to build a graph-level representation. For node classification tasks, we compare our model against R-GCN~(\cite{Schlichtkrull2018}), RDF2Vec~(\cite{Ristoski2016a}) and WL~(\cite{Shervashidze2011, DeVries2015}) on the AIFB and MUTAG dataset~(\cite{Ristoski2016b}). 
Dataset statistics as well as training details are reported in the Appendix.



\subsection{Results and discussion}



In Table~\ref{table:result}, we report results for the graph (left-hand side) and node classification (right-hand side) tasks. Our provably powerful model, edGNN, reaches comparable performance with the state-of-the-art. We observe that the kernel-based and gradient-based methods (R-GCN and edGNN) perform similarly without being able to clearly identify better models. We conjecture that the relatively small size of the datasets (\eg only $188$ graphs in the MUTAG dataset) does not allow to fully explore the potential of the most expressive models.
This does not contradict our theoretical findings. In fact, the power of a learnable model does not guarantee its generalization nor that the best model can be learned. However, it is true that, \emph{in the best-case scenario}, a more powerful model should perform better than a less powerful one, as shown by the results regarding the best-learned edGNN model (max).
An interesting future research direction would be the study of all the proposed models fitting the MPNN framework in order to understand whether they can be as powerful as the WL test or whether, instead, they introduce a particular bias. 

\begin{table}[h]
\caption{
Graph (left) and node (right) classification results in accuracy averaged over ten runs. For the proposed edGNN approach, we report both the average (avg) and the maximum accuracy (max). Results are expressed as percentages.
For graph classification, following prior art, we performed 10-fold cross validation. For node classification, we used the split provided by~\cite{Ristoski2016a}.
}
\label{table:result}
\centering
\scalebox{0.8}{
\begin{tabular}{lccccc}
  \toprule
    Model & MUTAG & PTC FM & PTC FR & PTC MM & PTC MR \\
    \midrule
    CSM         & $85.4$          & $\mathbf{63.8}$ & $65.5$          & $63.3$          & $58.1$ \\
    WLSP        & $85.4$          & $60.4$          & ${65.7}$ & $\mathbf{66.6}$ & $\mathbf{59.7}$ \\
    R-GCN       & $81.5$          & $60.7$          & $\mathbf{65.8}$          & $64.7$          & $58.2$ \\
    \midrule
    edGNN (avg) & $\mathbf{86.9}$ & $59.8$          & ${65.7}$ & $64.4$          & $56.3$ \\
    edGNN (max) & $88.8$          & $62.2$          & $68.0$          & $66.1$          & $59.4$ \\
    \bottomrule
\end{tabular}
\quad
\begin{tabular}{lccc}
  \toprule
    Model & AIFB & MUTAG \\
    \midrule
    WL &   $80.5$  & $\mathbf{80.9}$  \\
    RDF2Vec & $88.9$ & $67.2$  \\
    R-GCN & $\mathbf{95.8}$ & $73.2$ \\
    \midrule
    edGNN (avg) & $91.1$ & $80.0$  \\
    edGNN (max) & $97.2$ & $85.3$ \\
\bottomrule
  \end{tabular}
}
\end{table}









\clearpage
\bibliography{GINe}
\bibliographystyle{iclr2019_conference}

\clearpage
\section{Appendix}

\subsection{Proof of Theorem~\ref{thm:exists}} 

For a given vertex $v$, we define $E^I_v := \{ l_E(u,v):  \exists (u,v) \in E\}$ as the set of labels of edges incoming into the vertex $v$. We then define $L_v^I := \mathrm{Concat}\Big(\big\{ \big(n^I_v(e), e\big): \forall e \in E^I_v \big\}\Big)$. That is, for each vertex $v$, we create a label by concatenating all the labels of the incoming edges together with their multiplicities $n^I_v(e)$. Similarly, we define $E^O_v$ and $L_v^O$ for the outgoing edges. Note that the pairs $\big(n^I_v(e), e\big)$ and $\big(n^O_v(e), e\big)$ take values in $\mathcal{L} := \mathbb{N} \times \mathcal{Z}$. Therefore, $L_v^I$ (or $L_v^O$, respectively) can take values in $\mathcal{L}^{|E^I_v|}$ (or $\mathcal{L}^{|E^O_v|}$, respectively), \ie~the Cartesian product of $|E^I_v|$ (or $|E^O_v|$, respectively) copies of the set $\mathcal{L}$.

For all vertices $v$, we can construct a function $h_v: \mathcal{X} \times \mathcal{L}^{|E^I_v|} \times \mathcal{L}^{|E^O_v|} \rightarrow \mathcal{Y}_v$ that \emph{bijectively} maps a tuple $\Big(c_l^{(t-1)}(v),L^I_v,L^O_v\Big)$ to a label $y \in \mathcal{Y}_v$. 

Note that, as we are considering graphs with finite vertex degree, $|\mathcal{N}^I(v)|$ and $|\mathcal{N}^O(v)|$ (and, consequently, $|E^I_v|$ and $|E^O_v|$) are finite. Therefore, $\mathcal{X} \times \mathcal{L}^{|E^I_v|} \times \mathcal{L}^{|E^O_v|}$ is a \emph{countable} set because the finite Cartesian product of countable sets is itself countable. Thus, as we built the function $h_v$ to be bijective, the sets $\mathcal{Y}_v$, and their countable union $\mathcal{Y} := \bigcup_{v \in V} \mathcal{Y}_v$, are also countable (for results on countable sets, refer for example to~\cite{Patterson1967}).

We can then construct an injective hash function $g'$ such that 
\begin{equation}\label{eq:reformulate_wl}
\begin{split}
    g'\Big(y, \big\{ c_l^{(t-1)}(u): u \in \mathcal{N}(v) \big\}\Big)
    = g\Big(c_l^{(t-1)}(v), &\big\{ c_l^{(t-1)}(u): u \in \mathcal{N}(v) \big\},L^I_v,L^O_v\Big).
\end{split}
\end{equation}
where the right-hand side is the relabeling function defined in Eq.~(\ref{eq:wl_iteration_dir_edgelabel}). 

These constructions highlight the fact that an iteration of the WL algorithm on a directed labeled graph is the same as performing an iteration of the WL algorithm on a undirected node-only-labeled graph, where node labels take values in an appropriately augmented label set $\mathcal{Y}$.

The same equivalence can be highlighted between the GNN update functions in Eqs.~(\ref{eq:1gnn}) and (\ref{eq:1gnn_edges}). In fact, Eq.~(\ref{eq:1gnn_edges}) can be rewritten as 
\begin{align}\label{eq:reformulate_1gnn}
    f^{(t)}(v) = \sigma\Big(f_\mathcal{Y}^{(t-1)}(v)W_{1,3,4}^{(t)} + \sum_{u \in \mathcal{N}(v)}f^{(t-1)}(u)W_2^{(t)}\Big),
\end{align}

where $f_\mathcal{Y}^{(t)}(v) \in \mathbb{R}^{1 \times (d^{(t)} + 2d_E)}$ is the embedding resulting from the (horizontal) concatenation of $f^{(t)}(v)$, $\sum_{(u,v) \in E}f_E\big(u,v, l_E(u,v)\big)$, and $\sum_{(v,u) \in E}f_E\big(v,u, l_E(v,u)\big)$, whereas $W_{1,3,4}^{(t)}$ is the (vertical) concatenation of $W_{1}^{(t)}$, $W_{3}^{(t)}$, and $W_{4}^{(t)}$.

The reformulations presented in Eqs.~(\ref{eq:reformulate_wl}) and  (\ref{eq:reformulate_1gnn}) allow us to treat our problem as one of undirected graphs with labels only for nodes. We can therefore prove this theorem by directly using the proof of Theorem 2 in \cite{Morris2018}. $\qed$

\subsection{Details of training and more results}

\subsection{Initialization}

We initialize the node and edge features with a one-hot encoding of their input label. For node classification, we use the in-degree as input label of the nodes. To model the outgoing edges of the node classification graphs, we create new artificial relations by reversing each directed edge. We also report results without reversing the edges (reg). and with learned embeddings (emb) instead of a one-hot encoding.

\subsubsection{Graph classification}

The tasks  predict the mutagenicity~(\cite{Debnath1991,Kriege2012}) (MUTAG) and the toxicity~(\cite{Helma2001}) of chemical compounds (PTC).

We ran our graph classification experiments with a batch size of $8$ and a learning rate of $10^{-4}$ with a weight decay of $5 \times 10^{-4}$. We then performed a parameter search over the number of layers and node embedding size. The best performance was reached by using two layers and $64$ hidden units. We used a ReLu activation at each layer. The system was trained for at most $40$ epochs with early stopping with respect to the validation set cross-entropy loss. 

R-GCN for graph classification was also trained with a batch size of $8$, a learning rate of $10^{-4}$ with a weight decay of $5 \times 10^{-4}$. We used three layers and $64$ hidden units with learned initial embeddings for the nodes (instead of a one-hot encoding for edGNN).
We used a basis decomposition with the number of basis set to the number of relations. Results for CSM~(\cite{Kriege2012}) and WLSP~(\cite{Shervashidze2011}) are based on the re-implementation of \cite{Kriege2012}. All our experiments were performed with 10-fold cross validation as in~\cite{Kriege2012}. 

Dataset statistics for the graph classification tasks are shown in Table~\ref{tab:graph_class_stat}, whereas Table~\ref{tab:graph_class} shows the accuracy results together with the standard deviation. 

\begin{table}[h]
  \caption{Graph classification dataset statistics with the number of graphs (Graphs), the number of classes (Classes), the average number of nodes per graph (Avg nodes), the average number of edges per graph (Avg edges), the number of node labels (Node labels) and the number of edge labels (Edge labels).}
  \label{tab:graph_class_stat}
  \centering
  \begin{tabular}{lcccccc}
  \toprule
    Dataset & Graphs & Classes & Avg nodes & Avg edges & Node labels & Edge labels   \\
    \midrule
    MUTAG  &   188  & 2 & 17.9 & 19.8  & 6 & 3   \\
    PTC FM &   349  & 2 & 14.1 & 14.5 & 18 & 4  \\
    PTC FR &   351  & 2 & 14.6 & 15.0 & 19 & 4  \\
    PTC MM &   336  & 2 & 14.0 & 14.3 & 20 & 4  \\
    PTC MR &   344  & 2 & 14.3 & 14.7 & 18 & 4  \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Graph classification results in accuracy obtained with 10-fold cross validation. Results are expressed as percentages. edGNN is compared with the Subgraph Matching Kernel (CSM)~(\cite{Kriege2012}), Weisfeiler--Lehman Shortest Path Kernel~(\cite{Shervashidze2011}) and R-GCN~(\cite{Schlichtkrull2018}).}
  \label{tab:graph_class}
  \centering
  \begin{tabular}{lccccc}
  \toprule
    Model & MUTAG & PTC FM & PTC FR & PTC MM & PTC MR \\
    \midrule
    CSM & ${85.4 \pm 1.2}$  & $\mathbf{63.8 \pm 1.0}$ & $65.5 \pm 1.4$  & $63.3 \pm 1.7$ & $58.1 \pm 1.6$ \\
    WLSP & ${85.4 \pm 1.2}$ & $60.4 \pm 1.32$ & ${65.7 \pm 1.3}$ & $\mathbf{66.6 \pm 1.1}$ & $\mathbf{59.7 \pm 1.6}$ \\
    R-GCN & $81.5 \pm 2.1$  & $60.7 \pm 1.7$  & $\mathbf{65.8 \pm 0.6}$ & $64.7 \pm 1.7$  & $58.2 \pm 1.7$ \\
    \midrule
    edGNN (avg) & $\mathbf{86.9 \pm 1.0}$ & $59.8 \pm 1.5$          & ${65.7 \pm 1.3}$ & $64.4 \pm 0.8$          & $56.3 \pm 1.9$ \\
    edGNN (max) & $88.8$          & $62.2$          & $68.0$          & $66.1$          & $59.4$ \\
    
    \bottomrule
  \end{tabular}
\end{table}


\subsubsection{Node classification}

The node classification experiments were run with a learning rate of $5 \times 10^{-3}$  without weight decay. We used dropout $0.5$ on each layer with a ReLu activation. The best performance was achieved by using $2$ layers and $64$ hidden units. The maximum number of epochs was set to $400$ with early stopping with respect to the validation set cross-entropy loss. Results with R-GCN~(\cite{Schlichtkrull2018}), RDF2Vec~(\cite{Ristoski2016a}) and WL~(\cite{Shervashidze2011}) are based on the re-implementation of \cite{Schlichtkrull2018}. 

\begin{table}[h]
  \caption{Node classification dataset statistics with the number of classes (Classes), the total number of nodes (Nodes), the total number of edges (Edges) and the number of edge labels (Edge labels).}
  \label{tab:node_class_stat}
  \centering
  \begin{tabular}{lccccc}
  \toprule
    Dataset & Classes & Nodes & Edges & Edge labels   \\
    \midrule
    AIFB  &   4 & 8,285  & 29,043  & 45  \\
    MUTAG &   2 & 23,644 & 74,227  & 23  \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Node classification results in accuracy averaged over ten runs. Results are expressed as percentages. edGNN is compared with WL~(\cite{DeVries2015}), RDF2Vec~(\cite{Ristoski2016a}) and R-GCN~(\cite{Schlichtkrull2018}).}
  \label{tab:node_class}
  \centering
  \begin{tabular}{lccc}
  \toprule
    Model & AIFB & MUTAG \\
    \midrule
    WL          & $80.5 \pm 0.0$          & $\mathbf{80.9 \pm 0.0}$  \\
    RDF2Vec     & $88.9 \pm 0.0$          & $67.2\pm 1.2$            \\
    R-GCN       & $\mathbf{95.8 \pm 0.6}$ & $73.2 \pm 0.5$           \\
    edGNN (avg) & $91.1 \pm 2.4$          & $80.0 \pm 3.2$           \\
    edGNN (max) & $\mathbf{97.2}$         & $\mathbf{85.3}$          \\
    \midrule
    edGNN (emb) & $91.1 \pm 1.7$          & $77.2 \pm 2.6$           \\
    edGNN (reg) & $89.4 \pm 1.7$          & $80.4 \pm 3.4$           \\
    \bottomrule
  \end{tabular}
\end{table}

\end{document}
