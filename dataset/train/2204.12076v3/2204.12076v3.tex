\section{Experiments}

\label{sec:result}
\begin{table*}[htb]
  \caption{Linear evaluation results of \textbf{Small} model w.r.t. different view creation strategies. "Average" is taken over the last four tasks.}
    \label{tab:segments}
   \centering
  \begin{tabular}{l|cc|ccccc|c}
    \toprule
   \textbf{Method} &\textbf{Segments} & \textbf{\makecell{length of\\segment (s)}}   &   \textbf{\makecell{AS-20K\\mAP}}    & \textbf{\makecell{SPCV2\\Acc (\%)}}    & \textbf{\makecell{VOX1\\Acc (\%)}}   
     & \textbf{\makecell{NSYNTH\\Acc (\%)}} & \textbf{\makecell{US8K\\Acc (\%)}} &\textbf{\makecell{Average\\Acc (\%)}}  \\ 
    \midrule
    \textbf{BYOL-A \cite{niizumi_byol_2021}} & single & 1 & - & 92.2 & 40.1& 74.1& 79.1&  71.4 \\
    \midrule
    \textbf{\multirow{4}{*}{Small (Ours)}}   &     single & 1 & 0.210  & 94.3 &  52.3  & 73.8 & 79.3  & 74.9     \\ 
    &two  & 1 &  0.191  & 91.3 &  50.0 & 74.3 & 76.6 &   73.1   \\ 
    &single  & 6 & 0.257 & \textbf{94.0} &    &      & 80.9  & 76.5                              \\
     & two & 6 & \textbf{0.279}  & 93.6 &  \textbf{61.9}  & \textbf{75.3} &  \textbf{82.0} &  \textbf{78.2}   \\   
    \bottomrule 

  \end{tabular}
  
\end{table*}




We evaluate the performance of our model under the protocol of linear evaluation or finetuning.  In linear evaluation, the pre-trained encoder is frozen as a feature extractor, on top of which a linear classifier is trained. Whereas in finetuning, the pre-trained encoder and linear classifier are finetuned together.  

\subsection{Implementation Details of Pre-training}

We use Audioset \cite{gemmeke_audio_2017} for pre-training. The full Audioset (\textbf{AS-2M}) contains 200 million audio clips with a length of 10 seconds captured from Youtube Videos. Using the full Audioset, a \textbf{Base} model is trained, which contains 12 blocks, and 12 heads for each block. The dimension and inner dimension are 768 and 3072 respectively. Besides, using a subset with 200 thousand audio clips (\textbf{AS-200K}) randomly sampled from \textbf{AS-2M}, we also trained a \textbf{Small} model, which contains 12 blocks, and 6 heads for each block. The dimension and inner dimension are 384 and 1536 respectively.

\label{sec:impdetail_pre}
 Audio is re-sampled to 16 kHz. Audio clips are transformed to the mel-spectrogram domain, with a Hamming window, a window length of 25 ms, a hop size of 10 ms, and 64 frequency bins ranging from 60 Hz to 7800 Hz. The mel-spectrogram feature is min-max normalized, where the minimum and maximum values are calculated globally on the pre-training dataset. We intentionally set the length of two segments (for creating two views) to 6 seconds, which will leads to a segment overlap of at least 1 second, considering that the length of audio clip is 10 seconds. The two randomly sampled segments are augmented by Mixup and RRC with the same configurations used in BYOL-A \cite{niizumi_byol_2021}. 
 


We pre-train our models with the ADAMW optimizer \cite{loshchilov2017fixing}. The learning rate  is warmed up for 10 epochs, and then annealed to 1e-6 at cosine rate. Following DINO \cite{caron_emerging_2021}, the weight decay of transformer is increased from 0.04 to 0.4 at cosine rate. The EMA decay rate  increases from an initial value  to 1 at cosine rate.  Batch size is set to 1536. The \textbf{Base} model is trained using \textbf{AS-2M} for 200 epochs, with  being 2e-4, and  being 0.9995.
The \textbf{Small} model is trained using \textbf{AS-200K} for 300 epochs, with  being 5e-4, and  being 0.99.


\subsection{Downstream Tasks}
\label{sec:dataset}

Evaluations are carried out on a variety of downstream tasks, which cover all the three types of audio signals, namely audio event, speech and music. Datasets and downstream tasks are described as follows.
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item \textbf{AS-20K} for multi-label sound event classification. We use the balanced subset of Audioset-2M, with 527 audio classes. It contains 20,886 audio clips for training. For test, we use the evaluation set of Audioset, with 18,886 audio clips. 
\item \textbf{US8K} for single-label audio scene classification. We use the Urbansound8k dataset \cite{salamon2014dataset} to classify audio clips (less than 4 seconds) into 10 classes. It contains 8,732 audio clips and has ten folds for cross-validation.
\item \textbf{SPCV2} for spoken command recognition. We use Speech Command V2 \cite{warden2018speech} to recognize 35 spoken commands for one second of audio. It contains 84,843, 9,981 and 11,005 audio clips for training, validation and evaluation, respectively.
\item \textbf{VOX1} for speaker identification. We use the Voxceleb1 dataset \cite{nagrani2017voxceleb}, with 1,251 speakers. It contains 13,8361, 6,904 and 8,251 for training, validation and evaluation, respectively.
\item \textbf{NSYNTH} for music instrument classification. We use the NSYNTH dataset \cite{engel2017neural}, to recognize 11 instrument family classes from 4-seconds audio clips.
\end{itemize}

The mel-spectrogram feature is computed in the same way as for the pre-training data.  For linear evaluation, from the pre-trained encoder, segment embedding is obtained by concatenating the class token  and the average of the embedding sequence of all blocks. For finetuning, segment embedding is obtained by concatenating the class token and the average of the embedding sequence of the last block. Audio clips that are longer than 12 seconds are centrally cropped with a maximum length of 12 seconds, and then split into 6-second long chunks without overlap. The chunks are independently processed by the pre-trained encoder, and their outputs are averaged to obtain the final segment embedding. Audio clips that are shorter than 6 seconds are directly processed by the pre-trained encoder to obtain the segment embedding.  

 For linear evaluation, we train the linear classifier for 100 epochs with the SGD optimizer. The learning rate is annealed to 1e-6 at cosine rate during training. The optimal initial learning rate is searched for each task separately. Batch size is set to 1024. Augmentation is not used. 
 
 For finetuning, we finetune all models with the SGD optimizer.  The learning rate  is warmed up for 5 epochs, and then annealed to 1e-6 at cosine rate. The optimal  is searched for each task separately. Batch size is set to 512. We trained \textbf{SPCV2} and \textbf{VOX1} for 50 epochs, and \textbf{AS-20K} for 200 epochs. For \textbf{SPCV2} and \textbf{AS-20K}, we use Mixup \cite{kong2020panns} and RRC for data augmentation. As for supervised downstream tasks, Mixup mixes both audio clips and labels. For \textbf{VOX1}, data augmentation is not applied. 


Classification accuracy (Acc) is taken as the performance metric for single-label tasks, including audio scene classification, spoken command recognition, speaker identification and music instrument classification, and mean average precision (mAP) for the task of multi-label sound event classification. For \textbf{US8K}, we conduct 10-fold cross-validation, and report the average accuracy of the 10 folds. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{length.eps}
\caption{Normalized Acc/mAP as a function of segment length, Acc/mAP of each task is normalized into the range of (0,1). "Avg" denotes averaging over normalized score of all tasks. }
  \label{fig:length}
\end{figure}

\begin{comment}
\begin{table}[t]
  \caption{Patch Shape}
    \label{tab:shape}
  \scalebox{0.7}{

  \centering
  \begin{tabular}{llllll}
    \toprule
    Patch Shape &   \textbf{Audioset-20K}   & \textbf{SPCV2}    & \textbf{Voxceleb1}  
     & \textbf{Nsynth} & \textbf{Urbansound8k}  \\
    \midrule
    (16,16) &  27.4 & 90.4 & 50.6  &  73.6  & \textbf{83.2}                  \\
     (32,8) & 27.5  & 91.6 &  52.9  & 74.6  & 82.3   &  -          
     \\   
      (64,4)(default) & \textbf{27.8}  & \textbf{93.3} &  \textbf{57.0}  & \textbf{75.7} &  82.4
      \\
     \bottomrule
                             

  \end{tabular}
  }
\end{table}

\begin{table}[t]
  \caption{length of segment}
  \label{tab:length}
    \scalebox{0.7}{
  \centering
  \begin{tabular}{lllllll}
    \toprule
    \textbf{length (second)} &   \textbf{Audioset-20K}   & \textbf{SPCV2}    & \textbf{Voxceleb1}  
     & \textbf{Nsynth} & \textbf{Urbansound8k}  \\
    \midrule
    1 &   &  &    &     &    78.2                                \\
    2 &   &  &    &      &     81.3                             \\     
    3 &   &  &    &       &     81.3                            \\ 
    4 &   &  &    &        &    81.3                            \\ 
    5 &   &  &    &         &   81.6                            \\ 
    6(default) &   & \textbf{93.3} &  \textbf{57.0}  & \textbf{75.7}     &    82.4         \\ 
    7 &   &  &    &  & 81.3                                     \\ 
    \bottomrule
  \end{tabular}
  }
\end{table}
\end{comment}



\subsection{Ablation study}
\label{sec:ablation}
 We separately evaluate the effectiveness of transformer encoder and the proposed view creation strategy. Ablation experiments are conducted using the \textbf{Small} model with the linear evaluation protocol, due to their low computational complexities. Table~\ref{tab:segments} shows the results. The result of BYOL-A is also given, which uses a CNN encoder and a single 1-second segment. Our models use a single or two segments, with a length of 1 second or 6 seconds. For a fair comparison, when the segment length is set to 1 second, we split audio clips into 1-second chunks for downstream tasks.
 
 \textbf{Transformer Encoder:} With the same view creation strategy, i.e. creating two views from a 1-second segment, our model (line 2 in Table \ref{tab:segments}) outperforms BYOL-A, especially for the two speech tasks ( \textbf{SPCV2} and \textbf{VOX1}). Speech involves more long-term semantic information, and transformer is more suitable than CNN for learning these long-term dependencies.
 
 \textbf{View Creation Strategy:} As shown in Table \ref{tab:segments}, when the segment length is set to 1 second, using one single segment is better than using two segments. This phenomenon is consistent with the claim made in BYOL-A \cite{niizumi_byol_2021} that the two segments may be too different to be identified as a positive pair. However, two views created from a single segment may share too much semantic content, thus leading our model to find an easy solution. 
When the segment length is increased to 6 seconds, the performance measures of \textbf{AS-20K}, \textbf{VOX1} and \textbf{US8K} are systematically increased, no matter whether using one or two segments. This is partially due to the capability of learning long-term dependencies of the transformer encoder.
In addition, for the 6-seconds case, using two segments exhibits superior performance over using one segment. The possible reasons are: the two segments can be rationally identified as a positive pair as they share a small portion of overlap, and meanwhile they are different enough to increases the task difficulty and thus leads the model to learn a more generalized representation.  




Fig. \ref{fig:length} shows the normalized performance of each task as a function of segment length, where two segments are used. We can see that the performance measures increase along with the increasing of segment length until 6 seconds. This further verifies our new findings: i) when transformer encoder is used, increasing the segment length helps to learn more information; ii) when two segments are used, the segment length should be set
to make the segments share a proper amount of overlap, and have a proper difficulty for matching them as a positive pair.





\begin{comment}
\subsubsection{Number of blocks used for evaluation}
Figure \ref{fig:blocks}  shows the linear evaluation result of using different number of blocks for evaluation. In general, using more blocks leads to better performance. this is agreement with findings in speech representation study using transformer\cite{yang_superb_2021}. 

\textbf{Impact of length of segment}:
We further study the impact of length of segment, as is shown in Table \ref{tab:length}. Here we use two segments and vary the length. 

\textbf{Impact of patch shape}:
We study the impact of different patch shapes, as is shown in Table \ref{tab:shape}.
\end{comment}



\subsection{Linear Evaluation Results}

Table \ref{tab:le_result} shows the linear evaluation results. For fair comparison, we compare with other methods that also use Audioset for pre-training, including TRILL \cite{shor2020towards},  COLA \cite{saeed_contrastive_2020} and BYOL-A \cite{niizumi_byol_2021}. 
The performance scores are directly quoted from their original papers. It can be seen that our \textbf{Small} model outperforms other methods on all tasks, even though it only uses 1/10 data of Audioset-2M, while BYOL-A and COLA use the full Audioset-2M dataset. In particular, on the speaker identification task, our \textbf{Small} model obtains an accuracy of 61.9\%, compared to the 40.1\% accuracy of BYOL-A. By increasing the network size and the amount of training data, the performance can be further systematically increased by our \textbf{Base} model. The superiority of the proposed model comes from the use of transformer encoder and the proposed view creation strategy, which both are critical modules for the success of contrastive learning based pretraining technique. 

\def\a{true}
\def\b{false}
\setlength\tabcolsep{3pt}
\begin{table}[tb]
  \caption{Linear evaluation results.}
  \label{tab:le_result}
  \centering
  \footnotesize
     \scalebox{1}{
  \begin{threeparttable}
  \begin{tabular}{l|ccccc}
    \toprule
    \textbf{Method}   & \textbf{\makecell{AS-20K\\mAP}}    & \textbf{\makecell{SPCV2\\Acc (\%)}}    & \textbf{\makecell{VOX1\\Acc (\%)}}   
     & \textbf{\makecell{NSYNTH\\Acc (\%)}} & \textbf{\makecell{US8K\\Acc (\%)}}\\
    \midrule
    \textbf{TRILL \cite{shor2020towards} }  & -&-&17.9 &- &- \\
    \textbf{COLA \cite{saeed_contrastive_2020}} & - &  &    &    & -                                    \\
    \textbf{BYOL-A \cite{niizumi_byol_2021}}   & - &  &    &       & 79.1                                 \\
        \midrule
    \textbf{Small (ours)}  & 0.279 & 93.6 &  61.9  & 75.3 &   82.0                                    \\
    \textbf{Base (ours)} & \textbf{0.338} & \textbf{95.1} &  \textbf{72.0}  & \textbf{75.6}& \textbf{84.1}
                                      \\
    
    \bottomrule
  \end{tabular}


  
  \end{threeparttable}
  }
\end{table}

\setlength\tabcolsep{3pt}
\begin{table}[tb]
  \caption{Finetuning results.}
  \label{tab:finetune_result}
  \centering
  \footnotesize
   \scalebox{1}{
  \begin{threeparttable}
  \begin{tabular}{l|c|cccc}
    \toprule
    \textbf{Method}    &  \textbf{\# Params} &\textbf{\makecell{AS-20K\\mAP}}    & \textbf{\makecell{SPCV2\\Acc (\%)}}    & \textbf{\makecell{VOX1\\Acc (\%)}}    
     \\
    \midrule
    \textbf{COLA \cite{saeed_contrastive_2020}}  &  & - &  &                                        \\
    \textbf{Small-SSAST \cite{gong_ssast_2022}} & 23M   & 0.308 &  &                                           \\
    \textbf{SSAST-PATCH \cite{gong_ssast_2022}}  &89M & 0.310 &  &                                          \\
    \textbf{SSAST-FRAME \cite{gong_ssast_2022}}  & 89M  & 0.292 & \textbf{98.1} &                                          \\
    \textbf{Conformer \cite{srivastava_conformer-based_2022}} & 88M  & 0.276 & - & - \\
    \midrule
    \textbf{Small (ours)} & 22M   & 0.315 &  &                                           \\
    \textbf{Base (ours)} & 86M  & \textbf{0.374} &  98.0 &  \textbf{94.3}  
                                      \\
    
    \bottomrule
  \end{tabular}


  
  \end{threeparttable}
  }
\end{table}


\subsection{Fine-tuning Results}
To evaluate to what extent our models can further achieve, finetuning  experiments are conducted on the tasks of multi-label audio event classification (\textbf{AS-20K}), Spoken command recognition (\textbf{SPCV2}) and speaker identification (\textbf{VOX1}). We compare with those methods that also report the finetuning results, including  COLA \cite{saeed_contrastive_2020}, SSAST \cite{srivastava_conformer-based_2022} and Conformer \cite{srivastava_conformer-based_2022}. 
Table \ref{tab:finetune_result} shows the results. Compared to the best results achieved by other methods, our \textbf{Base} model performs better on  \textbf{AS-20K} (0.374 versus 0.310) and \textbf{VOX1} (94.3\% versus 80.8\%) by a large margin, and perform comparably on \textbf{SPCV2}. Remarkably, our \textbf{Small} model performs even better than SSAST and Conformer on \textbf{AS-20K} and \textbf{VOX1}, by using a much smaller network (22 M versus about 88 M). It is worth mentioning that both SSAST and Conformer use a transformer encoder as the proposed model, and use a wav2vec2-style pre-training scheme. Better results achieved by the proposed model may indicate that the teacher-student scheme is superior to the wav2vec2-style scheme for (segment-level) general audio pre-training.  

