\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{adjustbox}
\usepackage{color}
\usepackage{bm}
\usepackage{multirow}
\usepackage{paralist}

\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\def\proposed{\texttt{D2-Net}{}}
\title{D2-Net: Weakly-Supervised Action Localization
via Discriminative Embeddings and Denoised Activations}


\author{
Sanath Narayan\affmark[1], 
Hisham Cholakkal\affmark[2],  
Munawar Hayat\affmark[3], 
Fahad Shahbaz Khan\affmark[2], \\
Ming-Hsuan Yang\affmark[4,5], 
Ling Shao\affmark[1] \vspace{0.01cm}\\
\affaddr{\affmark[1]Inception Institute of Artificial Intelligence, UAE} \hspace{0.1cm}
\affaddr{\affmark[2]Mohamed bin Zayed University of AI, UAE} \\
\affaddr{\affmark[3]Monash University, Australia} \hspace{0.1cm}
\affaddr{\affmark[4]University of California, Merced, USA} \hspace{0.1cm}
\affaddr{\affmark[5]Google Research}

}


\begin{document}

\maketitle

\begin{abstract}

   This work proposes a weakly-supervised temporal action localization framework, called \proposed, which strives to temporally localize actions using video-level supervision. 
Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. 
The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. 
The discriminative term incorporates a classification loss and utilizes a top-down attention mechanism to enhance the separability of latent foreground-background embeddings. 
The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions.
Comprehensive experiments are performed on two benchmarks: THUMOS14 and ActivityNet1.2. 
Our \proposed{} performs favorably in comparison to the existing methods on both datasets, achieving gains as high as  3.6\% in terms of mean average precision on THUMOS14.

      
\end{abstract}




\section{Introduction\label{sec:introduction}}

Temporal action localization is a challenging problem, which aims to jointly classify and localize the temporal boundaries of actions in videos.~Most existing approaches~\cite{gtad,talnet,rc3d,cdc,ssn,scnn} are based on strong supervision, requiring manually annotated temporal boundaries of actions during training.
In contrast to these strong frame-level supervision based methods, weakly-supervised action localization learns to localize actions in videos, leveraging only video-level supervision.
Weakly-supervised action localization is therefore of greater importance since the manual annotation of temporal boundaries in videos is laborious, expensive and prone to large variations~\cite{action-snippet,action-extent}. 







Existing methods~\cite{hideseek,untrimnets,stpn,wtalc,autoloc} for weakly-supervised action localization typically use video-level annotations in the form of action classes and learn a sequence of class-specific scores, called temporal class activation maps (TCAMs). In general, a classification loss is used to obtain the discriminative foreground regions in TCAMs.
Some approaches~\cite{stpn,wtalc,3cnet,bg-modeling} learn TCAMs using action labels and obtain temporal boundaries via a post-processing step, while others~\cite{autoloc,cleannet} use a TCAM-generating video classification branch along with an explicit localization branch to directly regress action boundaries. 
Nevertheless, the localization performance is heavily dependent on the quality of the TCAMs. 
The quality of TCAMs is likely to improve in fully-supervised settings where frame-level annotations are available. 
Such frame-level information (true foreground and background regions) are unavailable in the weakly-supervised paradigm. In such a paradigm, the predicted foreground regions often overlap with the ground-truth background regions, while predicted background regions are likely to overlap with the ground-truth foreground regions. 
This leads to noisy activations, \ie, false positives and false negatives, in the learned TCAMs. Most existing weakly-supervised action localization methods that learn TCAMs typically rely on separating foreground and background regions (foreground-background separation) and do not explicitly handle its noisy outputs.




In this work, we address the problem of foreground-background separation along with explicit tackling of noise in TCAMs for weakly-supervised action localization. 
We propose a unified loss formulation that is jointly optimized to classify and temporally localize action snippets (group of frames) in videos. 
Our loss formulation comprises a discriminative and a denoising loss term.
The discriminative loss seeks to maximally separate backgrounds from actions (foregrounds) via interlinked classification and localization learning objectives (Sec.~\ref{sec:integrated_loss}). 
The denoising loss (Sec.~\ref{sec:denoising_loss}) complements the discriminative term by explicitly addressing the foreground-background noise in activations, thereby producing robust TCAMs (see Fig.~\ref{fig:denoise_intuition}).


In our loss formulation, we learn distinct latent embeddings such that their foreground-background separation is maximized based upon the corresponding top-down attention generated from the output TCAMs. 
Furthermore, the embeddings are employed to generate pseudo-labels based on their foreground scores (bottom-up attention). These pseudo-labels are utilized to explicitly handle the noise by emphasizing the corresponding output activations in pseudo-foreground regions, while suppressing the activations in pseudo-background regions.~This pseudo-background suppression and pseudo-foreground enhancement is achieved by maximizing the mutual information (MI) between activations and generated pseudo-labels within an action video (intra-video). Maximizing MI between predicted activations and labels decreases the uncertainty of predictions, leading to more robust predictions. In addition to capturing intra-video MI, our formulation also strives to maximize MI between the action class predictions and video-level ground-truth labels, across videos in a mini-batch (inter-video).  



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/Intro_fig.pdf}
    \vspace{-0.1cm}
    \caption{\textbf{Impact of our proposed loss formulation} on the quality of the output TCAMs. Compared to the baseline (without our discriminative and denoising loss terms), the introduction of the discriminative loss term improves the separation between foreground and background activations (\eg, third and fourth ground-truth action instance from the left). Furthermore, our final \proposed{} comprising both the discriminative and the denoising loss terms reduces the noise in the TCAMs, leading to more robust TCAMs.  
    \vspace{-0.3cm}}
    \label{fig:denoise_intuition}
\end{figure}



\noindent\textbf{Contributions:} We introduce a weakly-supervised action localization framework, \proposed{}, which incorporates a novel loss formulation that jointly enhances the foreground-background separability and explicitly tackles the noise to robustify the output TCAMs. 
Our main contributions are: 
\begin{compactitem}
\item We introduce a discriminative loss term, which simultaneously aims at video categorization and enhanced foreground-background separation.

\item We introduce a denoising loss term to improve the robustness of TCAMs. Our denoising loss explicitly addresses noise in TCAMs by maximizing the MI between activations and labels within a video (intra-video) \textit{and} across videos (inter-video).  To the best of our knowledge, we are the first to introduce a loss term that simultaneously captures MI across multiple snippets within a video and across all videos in a batch for weakly-supervised action localization.



\item Comprehensive experiments are performed on the THUMOS14~\cite{thumos14} and ActivityNet1.2~\cite{activitynet} benchmarks. Our \proposed{} performs favorably against existing weakly-supervised methods on both datasets, achieving gains as high as 3.6\% in terms of mAP on THUMOS14. 

\end{compactitem}








\section{Related Work}

Several weak supervision strategies have been explored in the context of action localization, including category labels~\cite{hideseek,stpn,untrimnets,wtalc,autoloc}, sparse temporal points~\cite{point-supervision}, order of actions~\cite{order-constraints-kuehne,order-constraints-bojan} and instance count~\cite{3cnet,star}. 
Most existing weakly-supervised action localization methods employ category labels as weak supervision and typically utilize features extracted from backbone networks~\cite{tsn,kinetics} trained on the action recognition task. 
The work of \cite{untrimnets} proposes a selection module for detecting the relevant temporal segments and employs a classification loss for training. 
The Autoloc method~\cite{autoloc} extends~\cite{untrimnets} by adding an explicit localization branch and utilizes an outer-inner contrastive loss for its training.
While in~\cite{3cnet}, additional supervision of action instance count is used to delineate adjacent action instances, the training of~\cite{refineloc,zhai2020twostream} comprises several refinement passes, where the model is trained iteratively in each pass using the results from the previous pass as pseudo-supervision.
In contrast, the approach of~\cite{wtalc} utilizes classification and similarity-based losses to match similar segments of actions in paired videos, while \cite{dml} employs a deep metric learning approach towards the same end. Both these methods constrain the mini-batch to contain multiple videos of the same actions. Furthermore, existing approaches, including~\cite{wtalc,dml}, do not explicitly address the issue of a large number of easy negatives overwhelming a smaller number of hard positives. 

Different from aforementioned works~\cite{wtalc,untrimnets,3cnet,dml}, \textit{our approach} explicitly addresses the issue of large number of easy negatives overwhelming a smaller number of hard positives through sample re-weighting. Furthermore, our approach places no batch constraint and performs foreground-background separation by inter-linking classification and localization objectives.




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{images/TAL_overall.pdf}
     \vspace{-1mm}
    \caption{\textbf{Overall architecture} of our \proposed{}. The focus of our design is the introduction of a novel loss formulation that jointly enhances the discriminability of latent embeddings and explicitly addresses the foreground-background noise in the output class activations. 
The network comprises two identical parallel streams (RGB and flow) consisting of three temporal convolutional \texttt{TC} layers. The second \texttt{TC} layer activations from both streams are averaged to obtain latent embeddings . The final outputs of both streams are then averaged to obtain the temporal class activation maps (TCAMs)  of untrimmed input videos. 
A discriminative loss  (Sec.~\ref{sec:integrated_loss}) is introduced to enhance the foreground-background separability (\textcolor{blue}{}) of embeddings  by utlizing a top-down attention mechanism, 
    in addition to achieving video classification. 
    Furthermore, a denoising loss  (Sec.~\ref{sec:denoising_loss}) is introduced to explicitly address the foreground-background noise (\textcolor{red}{}) in the class activations of , by utilizing a bottom-up attention.
The network is trained jointly using both loss terms  and .\vspace{-0.27cm}
    }
    \label{fig:overall_arch}
\end{figure*}




The work of~\cite{bg-modeling} employs a background-aware loss along with a self-guided loss for modeling the background. The approach of~\cite{moniruzzaman2020action} additionally utilizes an iterative multi-pass erasing step for discovering different action segments in TCAMs.
Unlike these works, the training in~\cite{luo2020emmil} alternates between updating a key-instance assignment branch and a classification branch via Expectation Maximization.
However, all these approaches~\cite{bg-modeling,moniruzzaman2020action,luo2020emmil} aggregate per-snippet losses for training and do not explicitly capture the mutual information (MI) between the activations and labels, which is likely to be more beneficial due to the absence of snippet-level labels in a weakly-supervised setting.




Different from existing methods~\cite{bg-modeling,moniruzzaman2020action,luo2020emmil,3cnet,stpn,refineloc,dml}, \textit{our approach} addresses the problem of foreground-background noise by exploiting both inter-video \textit{and} intra-video MI between the class activations and their corresponding labels, resulting in robust TCAMs. To the best of our knowledge, we are the first to propose a weakly-supervised action localization approach that simultaneously
captures MI across multiple snippets within a video and
across all videos in a mini-batch (see also Fig.~\ref{fig:mi_concept}). 





\section{Proposed Method\label{sec:method}}
Our \proposed{} strives to improve the separation of foreground-background feature representations in videos, while jointly enhancing the robustness of output TCAMs \wrt foreground-background noise.
This leads to better differentiation between foreground actions and surrounding background regions, resulting in enhanced action localization in the challenging weakly-supervised setting.
Here, we first present our overall architecture, followed by a detailed description of our proposed losses for training \proposed. 

\noindent\textbf{Overall architecture} of \proposed{} is illustrated in Fig.~\ref{fig:overall_arch}. Given a video , we divide it into non-overlapping snippets of  frames each. Features are then extracted to encode appearance (RGB) and motion (optical flow) information. Similar to \cite{stpn,wtalc,3cnet}, we use the Inflated 3D (I3D)~\cite{kinetics} to obtain  dimensional features for each -frame snippet. Let  denote features for a video, where  is the number of snippets. The extracted features become the inputs to our \proposed{}, which comprises two parallel streams for RGB and optical flow. Each stream consists of three temporal convolutional (\texttt{TC}) layers. The first two layers learn latent discriminative embeddings  (with time ), from the input features . The output of the final \texttt{TC} layer is passed through a \textit{sigmoid} activation. Subsequently, the outputs from both streams are averaged to obtain TCAMs  representing a sequence of class-specific scores over time for  action classes. 
The main contribution of our work is the introduction of a novel loss formulation to train the proposed \proposed. 
Our training objective combines a discriminative () and a denoising term (), with a balancing weight ,

These two loss terms utilize foreground-background attention sequences computed in opposite directions: (\textit{i}) the discriminative loss  utilizes a top-down attention, which is computed from the output TCAMs (the top-most layer) and (\textit{ii}) the denoising loss  utilizes a bottom-up attention, which is derived from the foreground scores of the latent embeddings (intermediate layer features). 
We describe these losses in detail in Sec.~\ref{sec:integrated_loss} and~\ref{sec:denoising_loss}.




\subsection{\hspace{-0.07em}Foreground-Background Discriminability:\label{sec:integrated_loss}}
In this work, we introduce a discriminative loss () to learn separable class-agnostic foreground and action-free background feature representations, in terms of latent embeddings, using a top-down attention from the TCAMs. 
The embedding of a video with  snippets is defined by a weighted temporal pooling based on the class activations .
Let the top-down foreground attention  denote the maximum foreground activation across all action classes , where  and  is the number of classes. Then, the class-agnostic foreground and background embeddings are:

where  and  is the background attention.
Maximizing the distance between foreground and background embeddings enhances the separability of the corresponding output activations, leading to improved localization.
Furthermore, enhancing the separation between a foreground embedding of a video and background embeddings of different videos improves the generalizability.
In addition, minimizing the intra-class (foreground/background classes) distance at a coarse-level amongst embeddings of different videos ensures that embeddings of the same class are clustered together.
This intra-class compactness is achieved by using a weight  with low magnitude. 
Three weight terms,  and , are introduced in our , targeting foreground-background separation, foreground grouping and background grouping, respectively. They are defined as: 

where  and  denote embeddings from different videos in a mini-batch. 
Alongside robust localization, our other objective is the multi-label classification of action categories. 
A major challenge is introduced by the class-imbalance problem, where easy background snippets overwhelmingly outnumber the hard foregrounds. 
To address this, inspired by the focal loss for object detection~\cite{focal_loss}, we propose to include penalty terms based on the weights (Eq.~\ref{eqn:cosine_wts}), in our .
To this end, a video-level prediction  is obtained by performing a temporal \emph{top-k} pooling on . Our  term, which jointly addresses the class-imbalance and enhances foreground-background separation, is defined by

where  denotes the video-level label and  is the focusing parameter. The first term in Eq.~\ref{eqn:discriminative_loss} denotes the loss for a positive action class, while the second term incorporates the loss for a negative class. The weight term  (see Eq.~\ref{eqn:cosine_wts}) is added for both positive action classes and background classes since it represents the foreground-background separation. 
The terms  and  enhance intra-class compactness for the positive and background classes, respectively. 
The first term in Eq.~\ref{eqn:discriminative_loss} indicates that the loss due to a positive action class  is low only when (i) its predicted probability  is high, and (ii) the foreground grouping  and foreground-background separation  for the corresponding video are both simultaneously low. A similar observation holds in the second term for the negative class. Thus,  enhances the discriminability of embeddings  by encouraging foreground-background separation while simultaneously achieving classification. 



\subsection{Robust Temporal Class Activation Maps: \label{sec:denoising_loss}}
Our discriminative loss , introduced in Sec.~\ref{sec:integrated_loss}, aims to enhance the distinctiveness of latent embeddings, leading to improved action localization.
However, under weak supervision, the temporal locations of foreground and background regions are unknown. 
This results in a noisy top-down attention as it relies on noisy output temporal class activations learned from video-level labels. 
Consequently, noise is likely to be introduced in the foreground and background embeddings ( and ), which are learned from the top-down attention . 
Our goal is to explicitly reduce the noise caused by the absence of snippet-level labels under weak supervision, thereby improving the robustness of the output class activations with respect to the foreground-background noise. 
To this end, we introduce a denoising loss . Our  comprises a novel pseudo-Determinant based Mutual Information (pDMI) loss, which exploits both intra-video and inter-video mutual information (MI) between the class activations and corresponding labels.





Our pseudo-Determinant based Mutual Information (pDMI) loss is inspired by the Determinant based Mutual Information (DMI)~\cite{dmi_neurips19}. The original DMI, proposed for multi-class classification, is computed as the determinant of a joint distribution matrix, \ie, . Here,  is the joint distribution over the predicted posterior probabilities  and the ground-truth (noisy) labels . The matrices  and  are of sizes  and , where  denotes the mini-batch size and  the number of classes. The DMI loss  is defined as

where  denotes Expectation.
Note that  depends on the determinant of . To ensure a non-zero , the label matrix  must be full-rank, \ie, a mini-batch must contain instances from all classes. This is prohibitive for a large number of classes. Furthermore, such mini-batch sampling for action localization leads to memory issues in GPUs due to the long duration of untrimmed videos in the dataset, especially when capturing inter-video mutual information.

Our pDMI loss overcomes these limitations and ensures a non-degenerate value of DMI, since it avoids an explicit computation of the determinant.
To this end, we first observe that, when the DMI loss tends to zero, the determinant of the joint distribution tends to one.
Formally,

As a result, DMI is maximum when  with the identity matrix  as an optimal solution for the joint distribution matrix  of size  (since elements of ). Furthermore, the condition number  for the optimal solution  is minimum, \ie . Hence, instead of maximizing the determinant of the distribution , we can alternatively minimize its condition number. In effect,  becomes better-conditioned and this improves the robustness of the activations towards label noise. 
The proposed pseudo-DMI loss  is then given by

where  denotes the condition number of . Since the rank of  is ,  is computed as , where  are non-zero singular values of . Thus, our pDMI loss avoids an explicit computation of the determinant and overcomes the limitations of the standard DMI. 


\begin{figure}[t]
    \centering
    \includegraphics[clip=true, trim=0em 0em 0em 0em, width=1\columnwidth, keepaspectratio]{images/mi_concept.pdf}\vspace{-0.1cm}
    \caption{\label{fig:mi_concept}\textbf{A conceptual illustration of loss computation} with (on the right) and without (on the left) capturing mutual information (MI). Typically, existing methods compute the loss without MI (\eg, cross-entropy loss) by aggregating individual losses () between prediction  and labels  either at a per-video or per-snippet level. Instead, we compute a collective loss across (i) all snippets within a video (snippet-level) \textit{and} (ii) all videos in a batch (video-level), by capturing the MI between predictions () and labels ().\vspace{-0.25cm}}
\end{figure}


\subsubsection{Snippet-level and Video-level Noise Removal}
To enhance the robustness of TCAMs, we employ the pDMI loss  at two levels: (\textit{i}) a snippet-level to exploit intra-video MI, and (\textit{ii}) a video-level to exploit inter-video MI. 
The snippet-level denoising step incorporates a bottom-up attention to emphasize the foreground activations, while suppressing the background ones by capturing the MI between the temporal activations and their corresponding foreground labels within a video. 
On the other hand, the video-level denoising step exploits MI between the video representations and corresponding labels, across videos, to achieve the same objective. 
Fig.~\ref{fig:mi_concept} shows a conceptual illustration of loss computation with and without capturing MI. \\
\noindent\textbf{Snippet-level joint distribution:}
The snippet-level joint distribution captures the MI between the foreground-background activations and the snippet-level pseudo-labels within a video. For this, we utilize a bottom-up attention mechanism, which encodes the foreground scores of latent embeddings for the corresponding snippets. These foreground scores are computed \wrt a reference background feature embedding . 
The foreground score  of a latent embedding  at time  is then given by

where  is progressively computed as a running mean of the embeddings  over  iterations. Here,  denotes the mean of the background embeddings in a mini-batch at iteration . 
Let  and  represent the time instants for selecting the foreground and background activations with respect to . Using the set of pseudo-foreground temporal locations , a row matrix
 of width  is constructed using top-down attention values, . Similarly,  of width  is constructed for the pseudo-background snippets. Then, a prediction matrix  representing the top-down attention values at the corresponding foreground and background pseudo-locations obtained from the bottom-up attention and a pseudo-label matrix  are given by

where , , ,  and  are  dimensional column vectors of ones and zeros. The snippet-level foreground-background joint distribution is defined as .
\\
\textbf{Video-level joint distribution:}
Unlike the snippet-level joint distribution where the noise arises due to the absence of ground-truth (snippet-level) labels, the noise in the video-level joint distribution stems from the video-level prediction . 
This noise in  is predominantly caused by the temporal \textit{top-k} pooling operation. Under the weakly-supervised setting, there is no guarantee that all the \textit{top-k} activations for an action class will belong to that class. Furthermore, actions in untrimmed videos may not necessarily span  snippets. 
Hence, denoising the video-level prediction  eventually results in improved robustness of the class activations output at the snippet-level. In order to obtain a video-level joint distribution  that captures the MI between class activations and action classes across videos, we compute the prediction  and label  as

where  and  denote the video-level representation and associated label of the -th video in the mini-batch. The joint distribution is defined as .  Since the joint distribution is over the action classes ,  is a  matrix. We finally define our denoising loss as

where the pDMI loss is given by Eq.~\ref{eqn:pdmi_loss}. Here,  and  denote the snippet-level and video-level losses.~Thus, our denoising loss improves the TCAMs, at the snippet-level and video-level, by making them robust to the foreground-background noise under the weakly-supervised setting. 


\subsection{Inference: Action Localization from TCAMs}
At inference, given a video, \proposed{} outputs a bottom-up attention sequence  (Eq.~\ref{eqn:bottom_up_attention}) of length  and a class activation map  of size . We perform \textit{top-k} pooling to obtain the predicted class probabilities , which are then used to find the relevant action classes above a threshold . For every relevant class , its corresponding class activations  are multiplied element-wise with  to obtain a refined sequence . The snippets with activations above a threshold are retained and a 1-D connected component is used to obtain segment proposals. 
Multiple thresholds are used to obtain a larger pool of proposals. Each proposal is then scored using the contrast between the mean activation of the proposal itself and its surrounding areas~\cite{autoloc}, , where  and  respectively denote the mean activation of the proposal and its neighboring background. The neighboring background is obtained by inflating the proposal on either side by  of its width, as in~\cite{autoloc}.
Proposals with high overlap are removed using class-wise non-maximal suppression.~Finally, a threshold  on the proposal score is applied to retain the high-scoring detections.


\begin{table}[t]
\centering
\caption{\label{tab:sota_th14}\textbf{State-of-the-art comparison} on the THUMOS14 dataset. Methods with superscript `+' require strong frame-level supervision for training. Our \proposed{} performs favorably in comparison to existing weakly-supervised methods and achieves consistent improvements, in terms of mean average precision (mAP).}
\adjustbox{width=0.85\columnwidth}{
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Approach}}} & \multicolumn{5}{c}{\textbf{mAP @ IoU}} \\ 
\multicolumn{1}{c}{} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.4} & \textbf{0.5}  \\ 
 \midrule
\texttt{R-C3D}~\cite{rc3d}  & 54.5 & 51.5 & 44.8 & 35.6 & 28.9 \\
\texttt{GTAD}~\cite{gtad}  & - & - & \textbf{54.5} & 47.6 & 40.2 \\
\texttt{TAL-Net}~\cite{talnet}  & \textbf{59.8} & \textbf{57.1} & 53.2 & \textbf{48.5} & \textbf{42.8} \\ 
\midrule 
\texttt{Autoloc}~\cite{autoloc}  & - & - & 35.8 & 29.0 & 21.2 \\ 
\texttt{BM}~\cite{bg-modeling}  & 64.2 & 59.5 & 49.1 & 38.4 & 27.5 \\
\texttt{3C-Net}~\cite{3cnet}  & 59.1 & 53.5 & 44.2 & 34.1 & 26.6 \\
\texttt{BaS-Net}~\cite{basnet} & 58.2 & 52.3 & 44.6 & 36.0 & 27.0 \\
\texttt{DGAM}~\cite{dgam} & 60.0 & 54.2 & 46.8 & 38.2 & 28.8  \\
\texttt{DML}~\cite{dml} & 62.3 & - & 46.8 & - & 29.6 \\
\texttt{A2CL-PT}~\cite{min2020adversarial} & 61.2 & 56.1 & 48.1 & 39.0 & 30.1 \\
\texttt{EM-MIL}~\cite{luo2020emmil} & 59.1 & 52.7 & 45.5 & 36.8 & 30.5 \\
\texttt{ACM-BANet}~\cite{moniruzzaman2020action} & 64.6 & 57.7 & 48.9 & 40.9 & 32.3 \\
\textbf{Ours: \proposed} & \textbf{65.6} & \textbf{60.0} & \textbf{52.1} & \textbf{43.3} & \textbf{35.9} \\ \bottomrule
\end{tabular}
}
\vspace{-0.1cm}
\end{table}


\section{Experiments}
\noindent\textbf{Datasets}: We evaluate \proposed{} on two challenging temporal action localization benchmark datasets, containing untrimmed videos with varying degrees of activity duration. 
The \textbf{THUMOS14}~\cite{thumos14} dataset contains temporal annotations for  validation and  test videos from  action categories. The dataset is challenging since each video contains  action instances on average. Similar to~\cite{wtalc,autoloc,refineloc}, the validation and test set are used for training and evaluating our \proposed, respectively. 
The \textbf{ActivityNet1.2}~\cite{activitynet} dataset has temporal annotations of  action categories with  training and  validation videos, with each video having  activity instances on average. As in~\cite{autoloc,wtalc}, we use the training and validation sets to respectively train and evaluate. \\
\textbf{Implementation details}: The optical flow frames of a video are generated using the TV-L1 flow~\cite{tvl1-flow}. For each -frame snippet, - features are extracted by concatenating the activations of \emph{Mixed\_5c} after average pooling from the RGB and Flow I3D models pre-trained on Kinetics~\cite{kinetics}. The kernel size and dilation rate of the temporal convolutional layers are: (, ) for THUMOS14 and (, ) for ActivityNet1.2. The first two convolutions in each stream are followed by a leaky ReLU with  negative slope. Our \proposed{} is trained with a mini-batch size of  for K iterations, using the Adam~\cite{adam} optimizer with a  learning rate and  weight decay.
The  for - is set to , as in~\cite{wtalc,3cnet}.
All the hyperparameters are chosen via cross-validation. The balancing parameter  is set to  and  for THUMOS14 and ActivityNet1.2, respectively. The intra-class compactness weight  and focusing parameter  are set to  and , respectively for both datasets. Multiple thresholds from  to  with increments of  are used for proposal generation. The overlap threshold for NMS is set to  and the score threshold  for retaining detections in a video is set to  of the maximum proposal score in that video.




\begin{table}[t]
\centering
\caption{\label{tab:sota_acn}\textbf{State-of-the-art comparison} on the ActivityNet1.2 dataset. Our \proposed{} performs favorably compared to existing weakly-supervised approaches. Furthermore, our \proposed{} performs comparably to \texttt{SSN}~\cite{ssn}, which is trained with strong supervision (denoted with superscript `+'). AVG denotes the mean of the mAP values for IoU in  with steps of .}
\adjustbox{width=0.75\columnwidth}{
\begin{tabular}{lccc|c}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Approach}}} & \multicolumn{3}{c}{\textbf{mAP @ IoU}} & \multicolumn{1}{|c}{\multirow{2}{*}{\textbf{AVG}}} \\ 
\multicolumn{1}{c}{} & \textbf{0.5} & \textbf{0.75} & \textbf{0.95} &    \\ 
 \midrule
\texttt{SSN}~\cite{ssn} & \textbf{41.3} & \textbf{27.0} & \textbf{6.1} & \textbf{26.6} \\ \midrule
\texttt{DML}~\cite{dml} & 35.2 & - & - & -  \\
\texttt{EM-MIL}~\cite{luo2020emmil} & 37.4 & - & - & 20.3  \\
\texttt{CMCS}~\cite{cmcs-modeling} & 36.8 & 22.0 & 5.6 & 22.4  \\
\texttt{3C-Net}~\cite{3cnet} & 37.2 & - & - & 21.7  \\
\texttt{BaS-Net}~\cite{basnet} & 38.5 & 24.2 & 5.6 & 24.3 \\
\texttt{DGAM}~\cite{dgam} & 41.0 & 23.5 & 5.3 & 24.4 \\
 \textbf{Ours: \proposed} &\textbf{42.3} & \textbf{25.5} & \textbf{5.8} & \textbf{26.0} \\ \bottomrule
\end{tabular}
}
\vspace{-0.1cm}
\end{table}



\subsection{State-of-the-art Comparison \label{sec:sota_compare}}
Tab.~\ref{tab:sota_th14} and \ref{tab:sota_acn} compare \proposed{} with state-of-the-art methods on THUMOS14 and ActivityNet1.2, respectively. Methods with '' require strong supervision for training.\\
\noindent\textbf{THUMOS14:} 
Similar to ours, all weakly-supervised methods in Tab.~\ref{tab:sota_th14} use an I3D backbone, except \texttt{Autoloc}~\cite{autoloc}, which uses TSN~\cite{tsn}. While \texttt{BM}~\cite{bg-modeling} and \texttt{BaS-Net}~\cite{basnet} consider an additional background class and train using a cross-entropy loss term, \texttt{DGAM}~\cite{dgam} extends~\cite{bg-modeling} using a VAE~\cite{kingma13iclr}. Although \texttt{DML}~\cite{dml} and \texttt{EM-MIL}~\cite{luo2020emmil} achieve a promising mAP of  and  at IoU=0.5, they do not generalize well to ActivityNet1.2 (see Tab.~\ref{tab:sota_acn}). Our \proposed{} performs well against existing weakly-supervised approaches and achieves an absolute gain of  at IoU= over the best existing method~\cite{moniruzzaman2020action}. Moreover, a consistent improvement in performance is obtained at other IoU thresholds.\\
\noindent\textbf{ActivityNet1.2:} 
Similar to our approach, all weakly-supervised methods in Tab.~\ref{tab:sota_acn} use an I3D backbone. Following standard evaluation protocol~\cite{activitynet}, we report the mean of the mAP scores (denoted as \texttt{AVG}) at different IoU thresholds ( in steps of ). The generative modeling based approach \texttt{DGAM}~\cite{dgam} and background suppression based \texttt{BaS-Net}~\cite{basnet} perform comparably, achieving mean mAP scores of  and , respectively. Our proposed \proposed{} performs favorably in comparison to these and achieves an absolute gain of  mean mAP over~\cite{dgam}. 








\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth,height=0.2\textwidth]{images/Diving.pdf}
    \includegraphics[width=0.95\textwidth,height=0.2\textwidth]{images/ThrowDiscus.pdf}
    \includegraphics[width=0.95\textwidth,height=0.2\textwidth]{images/MowingLawn.pdf}
    \caption{\textbf{Qualitative temporal action localization results} of our proposed \proposed{} on example test videos, with \textit{Diving}, \textit{Throw Discus} actions from THUMOS14, and \textit{Mowing Lawn} activity from ActivityNet1.2. For each video, example frames (top row), ground-truth GT segments (green), baseline detections (red) and \proposed{} detections (blue) are shown. The height of a detection is indicative of its score. The \texttt{Baseline} incorrectly merges multiple GT instances, has false positives in background regions and falsely detects the presence of the activity over the entire video length. Our \proposed{} correctly detects multiple instances (\eg,  to  GT in \textit{Diving},  to  in \textit{Throw Discus}) and suppresses most false positives in the background regions, achieving promising localization performance.
    \vspace{-0.2cm}}
    \label{fig:qual_res_acn}
\end{figure*}






\subsection{Ablation Study\label{sec:ablation}}
As discussed earlier, our \proposed{} comprises a discriminative  and a denoising loss . Here, we perform comparisons by replacing the two proposed loss terms ( and ) in our framework with either the standard cross-entropy loss  or the focal loss . In addition, we also show the performance of our \proposed{} with only . Tab.~\ref{tab:ablation} presents these performance comparisons, in terms of mAP and F1, on THUMOS14. Employing a standard cross-entropy loss ( in Tab.~\ref{tab:ablation}) in our framework results in an mAP score of  at IoU=. We observe that training with the standard focal loss (obtained by zeroing the weights  in Eq.~\ref{eqn:discriminative_loss}) helps alleviate the issue of a large number of easy samples overwhelming hard samples. This setting,  in Tab.~\ref{tab:ablation}, gains  mAP at IoU= over , thereby  highlighting the need to tackle imbalance between easy backgrounds and hard foregrounds. To the best of our knowledge, we are the first to evaluate the standard focal loss, , in weakly-supervised action localization setting.
Our \proposed{} with the discriminative loss term , which jointly addresses class-imbalance and enhances background-foreground separation, provides consistent improvements over  and achieves  mAP at IoU=. An absolute gain of  in terms of mAP at IoU= is obtained by the introduction of our proposed  in place of . Furthermore, our \proposed{} comprising both  and  obtains the best results with an mAP score of  at IoU=. Our  \proposed{} achieves absolute gains of  and  in terms of mAP at IoU=, over  and , respectively. It is noteworthy that our final \proposed{}, containing both   and , obtains a significant gain of 5.9\% in terms of F1 score over  alone. 
This improvement over  alone is obtained due to explicitly addressing the noise in TCAMs by our , leading to a substantial reduction (28\%) in the number of false positives without affecting the recall.   
\\
\noindent\textbf{Impact of MI-based denoising:} We also perform an experiment by replacing the proposed pDMI loss in our  with the standard \texttt{L1} and \texttt{BCE} losses for denoising the snippet-level activations. On THUMOS14, the \texttt{L1} and \texttt{BCE} losses, which do not explicitly capture MI, achieve mAP scores of  and  at IoU=, respectively. Our proposed \proposed{}, which employs MI-based pDMI loss in , achieves improved results with an mAP score at IoU= of . These results suggest that our MI-based denoising is able to robustify the TCAMs in a weakly-supervised setting. 

\begin{table}[t]
\centering
\caption{\label{tab:ablation}\textbf{Performance comparison} by replacing our two loss terms ( and ) in the proposed \proposed{} with either the standard cross-entropy loss () or the focal loss (). In addition, we also show the performance of our \proposed{} with only .
Results are shown in terms of mAP and F1 score at IoU=, on THUMOS14. Replacing the proposed loss terms in our framework with  and  results in mAP scores at IoU= of 23.0 and 26.7, respectively. 
Our \proposed{} with the discriminative loss term  achieves consistent improvement in performance over  with an absolute gain of 5.5\% in terms of mAP at IoU=. Furthermore, our final \proposed{} comprising both loss terms ( and ) achieves the best performance with absolute gains of 12.9\% and 9.2\% in terms of mAP at IoU= over  and , respectively.
}
\adjustbox{width=\columnwidth}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Loss term}} &  \multicolumn{5}{c}{\textbf{mAP @ IoU}} & \textbf{F1} \\ 
 &  \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.4} & \textbf{0.5}  & \textbf{} \\ \midrule
  & 55.0 & 47.6 & 38.7 & 30.7 & 23.0 & 23.5 \\
  &  58.8 & 52.4 & 44.3 & 35.7 & 26.7 & 27.2 \\ \midrule
  &  65.4 & 59.7 & 50.1 & 40.4 & 32.2 & 30.7  \\
 \textbf{\proposed:}  +  &  \textbf{65.6} & \textbf{60.0} & \textbf{52.1} & \textbf{43.3} & \textbf{35.9} & \textbf{36.6}  \\
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}

\end{table}



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/tsne_bg_fg.pdf}
    \caption{\textbf{Illustration of foreground-background separability} obtained in the latent embedding space of (a) the baseline using the standard focal loss and (b) our \proposed{} via t-SNE scatter plots on the THUMOS14 test set. 
     In both cases, foreground and background embeddings per video are obtained as the mean of latent embeddings at their respective ground-truth locations.
Our \proposed{} better separates the foreground and background, compared to the baseline. \vspace{-0.6cm}}
    \label{fig:tsne}
\end{figure}


\noindent\textbf{Qualitative results:}
Fig.~\ref{fig:qual_res_acn} shows a qualitative comparison between the baseline (red) and \proposed{} (blue), along with the ground-truth (GT) action segments (green). Here, the baseline employs . Example test videos with \textit{Diving} and \textit{Throw Discus} actions from THUMOS14 are shown in the first two rows. The baseline incorrectly merges multiple GT instances (\eg,  to  GT in \textit{Diving}) and produces false positives in background regions (\eg, towards the beginning of \textit{Diving} video). Our \proposed{} correctly detects these multiple action instances and suppresses most false positives in the background regions. The third row shows an example test video with \textit{Mowing Lawn} activity from ActivityNet1.2. The baseline incorrectly detects the presence of the activity over the entire video length. In contrast, our \proposed{} improves the detection of multiple activity instances, leading to promising localization performance. 
Fig.~\ref{fig:tsne} shows the foreground-background separability comparison, in terms of t-SNE scatter plots, between the baseline and our \proposed{}. 






\section{Conclusion}
We propose a weakly-supervised action localization approach, called \proposed{}, that comprises a discriminative and a denoising loss. 
The discriminative loss term strives for improved foreground-background separability through interlinked classification and localization objectives. The denoising loss term complements the discriminative term by tackling the foreground-background noise in the activations. This is achieved by maximizing the mutual information between activations and labels within a video (intra-video) and across videos (inter-video). We evaluate our \proposed{} on two benchmarks. Our results show that \proposed{} performs favorably against existing methods on both datasets. 






\bibliographystyle{ieee_fullname}
\bibliography{egbib}

\end{document}
