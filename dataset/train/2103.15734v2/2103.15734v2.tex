\section{Experiment}


\begin{table*}[t]
	\begin{minipage}[!t]{\linewidth}
	    \centering
		\begin{minipage}{.45\linewidth}
		\subfloat[Effect of each component.]{
		\resizebox{1.0\textwidth}{!}{\centering
		\begin{tabular}{c c c c l l l l}
					\hline
					 Baseline & +RDM & +cascade & +PGM & mIoU $\uparrow$ & mBER $\downarrow$ & mAE $\downarrow$  \\  
					\hline
				    Deeplabv3+	& - & - & - & 85.4 & 6.73 & 0.075 \\
					& \checkmark & - & - & 89.2 & 4.63 & 0.054 \\
					&\checkmark &\checkmark & - &  90.1 & 4.15 & 0.049 \\
					&\checkmark &\checkmark &\checkmark & \bf{90.7} & \bf{3.97} & \bf{0.047} \\
					\hline
				\end{tabular}}}\hspace{3mm}
	    \end{minipage}
	    \hspace{3mm}
	    \begin{minipage}{.45\linewidth}
		\subfloat[Effect of loss function design.]{
		
		\resizebox{0.95\textwidth}{!}{\centering
		\begin{tabular}{c c c c l l l l}
					\hline
					 $L_{merge}$ & $L_{edge-bce}$ & $L_{edge-dice}$ & $L_{residual}$ &  mIoU $\uparrow$& mBER $\downarrow$ & mAE $\downarrow$  \\  
					\hline
					\checkmark & - & - & - & 88.3 & 5.02  & 0.060 \\
					\checkmark & \checkmark & - & & 89.4 & 4.48 & 0.053 \\
					\hline
				    \checkmark &\checkmark & -  &\checkmark & 90.4 & 4.12 & 0.049\\
					\checkmark & - &\checkmark & \checkmark & \bf{90.7} & \bf{3.97} & \bf{0.047} \\
					\hline
				\end{tabular}	
				
				}}\hspace{3mm}
		\end{minipage}
		
	\end{minipage}
    \vspace{3pt}
	\begin{minipage}[!t]{\linewidth}
	   \centering
		\begin{minipage}{.275\linewidth}
		\subfloat[Effect of each component in RDM. \label{expr:ablation_norm}]{
			\footnotesize
			\resizebox{0.95\textwidth}{!}{\begin{tabular}{l|c c c}
				\hline
				Settings  & mIoU $\uparrow$ & mBER $\downarrow$ & mAE $\downarrow$ \\
				\hline
			    All operators & \bf{89.2} & \bf{4.63} & \bf{0.054} \\
				\hline
				w/o $F_{low}$ & 88.8 & 4.92 & 0.059 \\
				\hline
				w/o $F_{high}$& 88.4 & 5.01 & 0.058 \\
				\hline
				w/o Both &  88.2  & 4.98 & 0.058 \\
				\hline
				w/o residual learning & 86.4 & 6.17 & 0.070 \\
				\hline
		\end{tabular}}}
		\end{minipage}
        \begin{minipage}{.35\linewidth}
		\subfloat[Effectiveness on boundary refinement.]{
			\footnotesize
			\resizebox{0.95\textwidth}{!}{\begin{tabular}{l c c c c c c }
				\toprule[0.15em]
				Method    & mIoU $\uparrow$  & F1(12px) $\uparrow$ & F(9px) $\uparrow$ & F1(5px) $\uparrow$ & F1(3px) $\uparrow$ \\
				\toprule[0.15em]
        baseline  & 85.4 & 77.6 & 73.5 & 66.6 & 56.7 \\
         +RDM &  89.2 & 84.7 & 81.6 & 77.6 & 68.5 \\
         +RDM \& PGM & 89.5 & 85.8 & 83.1 & 79.2 & 73.3 \\
	\bottomrule[0.1em]
	\end{tabular}}}
	    \end{minipage}
	    \begin{minipage}{.25\linewidth}
		\subfloat[Application on Other Architectures. \label{expr:ablation_placement}]{
			\footnotesize
			\resizebox{0.85\textwidth}{!}{\begin{tabular}{l|c c c}
				\hline
				Network  & mIoU $\uparrow$ & mBER $\downarrow$ & mAE$\downarrow$ \\
				\hline
				FCN  & 83.2 & 7.94 & 0.094 \\
				\hline
			    +our modules &  87.9 & 5.04 & 0.062 \\
				\hline
				PSPNet~\cite{pspnet} & 83.5 & 8.34 & 0.089 \\
				\hline
				+our modules & 88.6 & 4.93 &  0.058 \\
				\hline
				DANet~\cite{DAnet} & 84.1 & 7.03 & 0.084 \\
				\hline
				+our modules & 87.8 & 5.14 & 0.062\\
				\hline
		\end{tabular}}}
		\end{minipage}
	\end{minipage}
	\vspace{-3mm}
	\caption{\small \textbf{Ablation studies.} We first verify the effect of each module and loss function in (a) and (b) in terms of mIoU, mBER and mAE. Then we give a detailed component analysis in RDM in (c), and use boundary metrics to verify each module's effect in (d). Finally, verify the various architectures in (e) to show the generality of our proposed modules. All the results are reported on the validation set of Trans10k. Best view it on screen and zoom in.}\label{tab:ablations}
	\vspace{-3mm}
\end{table*}



\subsection{Datasets and evaluation metrics}

\noindent \textbf{Datasets:} \textbf{(a) Trans10k}~\cite{trans10k_xieenze} dataset contains 10,428 images with two categories of transparent objects, including things and stuff. For this dataset, 5000, 1000, and 4428 images are used for train, validation, and test, respectively. 
\textbf{(b) GDD}~\cite{tranparent_gdnet} is another large-scale glass object segmentation dataset covering diverse daily-life scenes~(such as office, street). For dataset split, 2980 images are randomly selected for training and the remaining 936 images are used for testing.
\textbf{(c) MSD}~\cite{Mirror_net} is a large-scale mirror dataset. For dataset split, 3063 images are used for training while 955 images are used for testing.
We use the Trans10k for ablation studies and analyses and also report results on the GDD and MSD.

\noindent \textbf{Evaluation metrics:} Following the previous work~\cite{trans10k_xieenze, tranparent_gdnet, Mirror_net} , for the Trans10k dataset, we adopt four metrics for quantitatively evaluating the model performance unless otherwise specified. Specifically, we use the~(mean) intersection over union~(IoU/mIoU) and pixel accuracy~(ACC) metrics from the semantic segmentation field. Mean absolute error~(mAE) from the salient object detection field is also used. On top of that, we use the balance error rate~(BER/mBER), which considers the unbalanced areas of transparent~(mirror) and non-transparent~(non-mirror) regions. Following~\cite{tranparent_gdnet, Mirror_net}, F-measure is also used to evaluate the model performance of GDD and MSD datasets. When evaluating the accuracy of predicted boundaries, following~\cite{gated-scnn}, we use F1-score as the metric.

\subsection{Experiment on Trans10k}

\noindent \textbf{Overview:} We firstly perform ablation studies on the validation set. Then visualization analysis is given to show the accurate segmentation results. Finally, we compare our model with the state-of-the-art models on the test set.

\noindent \textbf{Trans10k implementation details:} \iffalse We implement our method using  Pytorch~\cite{pytorch} \fi We adopt the same training setting as the original Trans10k~\cite{trans10k_xieenze} paper for fair comparison. The input images are resized to the resolution of 512$\times$512. We use the pre-trained ResNet50 as the backbone and refine it along with other modules. We use 8 GPUs for all experiments and batch size is set to 4 per GPU. The learning rate is initialized to 0.01 and decayed by the poly strategy with the power of 0.9 for 16 epochs. We report mIoU, mBER, mAE in ablation study and also report ACC when comparing with other methods on test set.

\noindent \textbf{Ablation on Each Components:} We first verify the effectiveness of each module with DeeplabV3+~\cite{deeplabv3p} as the baseline in Tab.~\ref{tab:ablations}(a). After using the proposed RDM, there exists a significant gain over the baseline by 3.8\% mIoU, 2.1 in mBER and 0.021 in mAE. These results indicate the importance of precise boundaries generation for glass object segmentation. After appending more RDM modules in the cascade framework, the performance further gains 0.9\% mIoU. Finally, based on the precise boundary, applying our proposed PGM, our method gain another 0.6\% mIoU on the cascade model. Although PGM does not provide as much numerical improvement as RDM because of the small number of pixels at the boundaries, it does provide a significant visual improvement, as shown in Fig.~\ref{fig:pointGCN}(c). 

\noindent \textbf{Ablation on Loss function design:} Then we explore the impact of loss function by fixing each component where the number of sampled points is set to 96 in PGM, and the number of cascade stages is set to 3. Results are shown in Tab.~\ref{tab:ablations}(b), after using binary Cross-Entropy loss~($L_{edge-bce}$) as $L_{edge}$, we find 1.1\% mIoU gain. With an additional  $L_{residual}$, we find another 1.0\% mIoU gain which indicates that dual supervision is conducive to sharpen edge generation then improve the final prediction. Compared to using binary Cross-Entropy loss as $L_{edge}$, Dice loss~($L_{edge-dice}$) leads to 0.3\% mIoU gain since dice loss can better handle the imbalance problems for foreground objects. 



\noindent \textbf{Ablation on RDM:} We give detailed ablation studies on each component in RDM by removing it from the complete RDM in Tab.~\ref{tab:ablations}(c). We set the cascade number to 1 and do not use PGM. Removing $F_{low}$ or $F_{high}$ in the first step or the second step of RDM results in mIoU decline of 0.4\% and 0.8\%, respectively. Removing both of them results in mIoU degradation of 1.0\%. After removing the subtraction operation and residual learning, there is a clear drop on all metrics indicating the importance of boundary sharpening.

\noindent \textbf{Ablation on boundary refinement:} We present a detailed comparison on boundary using F1-score~\cite{gated-scnn} with four different thresholds in Tab.~\ref{tab:ablations}(d) where the cascade is not used. For each threshold, results obtain significant gains after using RDM. The gain becomes more significant when the neighbor pixels decrease (from 12 pixels to 3 pixels). These results indicate that our method can generate a precise boundary. The improvement of mIoU also proves our motivation that better boundary prediction is beneficial to produce better segmentation results. After using PGM, there are more gains in all the metrics in this strong baseline.

\noindent \textbf{Application on more segmentation methods:} In addition to DeeplabV3+, we carry on experiments on various other network architectures including,
 FCN~\cite{fcn}, PSPNet~\cite{pspnet} and DANet~\cite{DAnet}. Our modules are appended at the end of the head of these networks, and are trained with the same setting as DeeplabV3+. As shown in Tab.~\ref{tab:ablations}(e), our modules improve those methods by a large margin, which proves the generality and effectiveness of our modules.















\noindent \textbf{Comparison with the state-of-the-arts:} Following~\cite{trans10k_xieenze}, we report mIoU, Acc, mAE, mBER on Trans10k test set. As shown in Tab.~\ref{tab:results_trans10k}, our method achieves the state-of-the-art results on four metrics with output stride 16 for fair comparison with Translab~\cite{trans10k_xieenze}. After changing the output stride to 8 in the backbone, our method obtains a better result.




\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.63}{
			\begin{tabular}{l c  c c c  }
				\toprule[0.2em]
				Method   & mIoU $\uparrow$  &  Acc$\uparrow$  & mAE $\downarrow$  &  mBER$\downarrow$  \\
				\toprule[0.2em]
    Deeplabv3+~(MobileNetv2)~\cite{deeplabv3p} & 75.27 & 80.92 & 0.130 & 12.49 \\
    HRNet~\cite{HRNet} & 74.56 & 75.82 & 0.134 & 13.52 \\
    BiSeNet~\cite{bisenet} & 73.93 & 77.92 & 0.140 & 13.96 \\
    \hline
    DenseAspp~\cite{denseaspp} & 78.11 & 81.22 & 0.114 & 12.19 \\
    Deeplabv3+~(ResNet50)~\cite{deeplabv3p} & 84.54 & 89.54 & 0.081 & 7.78 \\
    FCN~\cite{fcn} & 79.67 & 83.79 & 0.108 & 10.33 \\
    RefineNet~\cite{refinenet} & 66.03 & 57.97 & 0.180 & 22.22 \\
    Deeplabv3+~(Xception65)~\cite{deeplabv3p} & 84.26 & 89.18 & 0.082 & 8.00 \\
    PSPNet~\cite{pspnet} & 82.38 & 86.25 & 0.093 & 9.72 \\
    Translab~(ResNet50)~\cite{trans10k_xieenze} & 87.63 & 92.69 & 0.063 & 5.46 \\
    \hline
    \hline
    EBLNet~(ResNet50, OS16)   & 89.58 & 93.95 & 0.052 & 4.60 \\
    EBLNet~(ResNet50, OS8)   & \bf{90.28} & \bf{94.71} & \bf{0.048} & \bf{4.14} \\
	\bottomrule[0.1em]
	\end{tabular}}
		\caption{\small Comparison to state-of-the-art on Trans10k test set. All methods are trained on Trans10k training set under the same setting and all models use single scale inference. OS means the output stride in the backbone. }
		\label{tab:results_trans10k}
	\end{threeparttable}
	\vspace{-3mm}
\end{table}



\subsection{Experiment on GDD and MSD}
\noindent \textbf{Experiment on GDD:} We adopt the same setting as the original paper~\cite{tranparent_gdnet} for fair comparison where the input images are resized into $416 \times 416$ and are augmented by horizontally random flipping. The total training epoch is 200. The learning rate is initialized to 0.003 and decayed by the poly strategy with the power of 0.9. We also give results with different backbones for better comparison. As shown in Tab.~\ref{tab:results_GDD}, our method achieves the state-of-the-art result on this dataset on five different metrics without using conditional random fields~(CRF)~\cite{CRF} as the post-processing.

\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.66}{
			\begin{tabular}{l c c c c c c }
				\toprule[0.2em]
				Method   & IoU $\uparrow$ &  Acc$\uparrow$  & $F_\beta \uparrow$ & mAE$\downarrow$  &  BER$\downarrow$  \\
				\toprule[0.2em]
	PSPNet~\cite{pspnet}  &  84.06 & 0.916 & 0.906  & 0.084  & 8.79 \\
    DenseASPP~\cite{denseaspp} & 83.68 &  0.919 & 0.911  & 0.081  & 8.66 \\
    DANet~\cite{DAnet} &84.15  & 0.911 & 0.901  &0.089  & 8.96  \\
    CCnet~\cite{ccnet} & 84.29 & 0.915 & 0.904 & 0.085 &  8.63\\
    PointRend~\cite{kirillov2020pointrend} & 86.51 & 0.933 & 0.928 & 0.067 & 6.50 \\
    \hline
    DSS~\cite{Hou_2017_CVPR} & 80.24 & 0.898 & 0.890 & 0.123 &  9.73\\
    PiCANet~\cite{2017PiCANet} & 83.73 & 0.916 & 0.909 &0.093  & 8.26 \\
    BASNet~\cite{qin2019basnet}& 82.88  & 0.907 & 0.896 & 0.094 &8.70  \\
    EGNet~\cite{zhao2019egnet}& 85.04 & 0.920  & 0.916 & 0.083 &  7.43 \\
    \hline
    DSC~\cite{Hu_2018_CVPR} & 83.56 & 0.914 & 0.911 & 0.090 & 7.97 \\
    BDRAR~\cite{Zhu_2018_ECCV} \textdagger  & 80.01  & 0.902 & 0.908 &  0.098 & 9.87 \\
    MirrorNet~\cite{Mirror_net}~(ResNext101)  \textdagger  & 85.07 & 0.918  & 0.903 & 0.083 &  7.67 \\
    GDNet~\cite{tranparent_gdnet}~(ResNext101) & 87.63  & 0.939 & 0.937 & 0.063 & 5.62 \\
    \hline
    \hline
    EBLNet~(ResNet101)   &  88.16 & 0.941 &  0.939 &  0.059  & 5.58 \\
    EBLNet~((ResNext101)   & \bf{88.72} & 	\bf{0.944} &  \bf{0.940} &	\bf{0.055} & \bf{5.36} \\
	\bottomrule[0.1em]
	\end{tabular}}
		\begin{tablenotes}
			 \item {\scriptsize \textdagger CRF is used for post-processing.
			 }
		\end{tablenotes}
		\caption{\small Comparison to state-of-the-art on GDD test set. All methods are trained on GDD training set under the same setting and all models use single scale inference. }
		\label{tab:results_GDD}
	\end{threeparttable}
\end{table}


\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.61}{
			\begin{tabular}{l c c c c c c c }
				\toprule[0.2em]
				Method  &  CRF & IoU $\uparrow$ &  Acc$\uparrow$  & $F_\beta \uparrow$ & mAE$\downarrow$  &  BER$\downarrow$  \\
				\toprule[0.2em]
	PSPNet~\cite{pspnet} & - & 63.21 & 0.750 & 0.746 & 0.117 & 15.82 \\
    ICNet~\cite{ICnet} & - & 57.25 & 0.694 & 0.710 & 0.124 & 18.75 \\
    PointRend~\cite{kirillov2020pointrend} (Deeplabv3+) & - & 78.81 & 0.936 & 0.872 & 0.054 & 8.95 \\
\hline
    DSS~\cite{Hou_2017_CVPR} & - & 59.11 & 0.665 & 0.743 & 0.125 & 18.81 \\
    PiCANet~\cite{2017PiCANet} & - & 71.78 & 0.845 & 0.808 & 0.088 & 10.99 \\
    RAS~\cite{Chen_2018_ECCV} & - & 60.48 & 0.695 & 0.758 & 0.111 & 17.60 \\
    \hline
    DSC~\cite{Hu_2018_CVPR} & - & 69.71 & 0.816 & 0.812 & 0.087 & 11.77 \\
    BDRAR~\cite{Zhu_2018_ECCV} & $\surd$ & 67.43 & 0.821 & 0.792 & 0.093 & 12.41 \\
    MirrorNet~(ResNeXt101)~\cite{Mirror_net}  & $\surd$ & 78.95 & 0.935 & 0.857 & 0.065 & \bf{6.39} \\
    \hline
    \hline
    EBLNet~(ResNet101) & - & 78.84 & 0.946 & 0.873 & 0.054 & 8.84 \\
    EBLNet~(ResNext101) & - & \bf{80.33} & \bf{0.951} & \bf{0.883} & \bf{0.049} & 8.63 \\
	\bottomrule[0.1em]
	\end{tabular}
	}
	\caption{\small Comparison to state-of-the-art on MSD test set. All methods are trained on the MSD training set. All models use single-scale inference.}
	\label{tab:results_MSD}
	\end{threeparttable}
	\vspace{-2mm}
\end{table}

\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.439]{fig/compare_with_others.pdf}
	\vspace{-3mm}
	\caption{\small Visualization and comparison results on Three glass-like object segmentation datasets. The 1st and 2nd row of the 7th column are results of Translab~\cite{trans10k_xieenze}, the 3rd row of the 7th column is a result of GDNet~\cite{tranparent_gdnet}, the 4th row of the 7th column is a result of Mirrornet~\cite{Mirror_net}. Compared with other methods, our method has much better results.}
	\label{fig:trans10k_vis_res}
	\vspace{-2mm}
\end{figure*}


\noindent \textbf{Experiment on MSD:} Following the  setting in MirrorNet~\cite{Mirror_net}, input images are resized to a resolution of 384 $\times$ 384 and are augmented by horizontally random flipping. The learning rate is initialized to 0.002 and decayed by the poly strategy with the power of 0.9. We train our model for 160 epochs. As shown in Tab.~\ref{tab:results_MSD}, our method achieves state-of-the-art results in four different metrics.

\noindent \textbf{Visual Results Comparison:} Fig.~\ref{fig:trans10k_vis_res} gives the some visual results on all the three datasets. Our method can obtain superior segmentation results compared with other methods and has fine-grained boundary results on mirrors or glass. More results can be found in the supplemental file.

\begin{table}[!t]\setlength{\tabcolsep}{4pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.66}{
		\begin{tabular}{c c c c l l l l}
					\hline
					 Method  & Parameters $\downarrow$ & Inference Time $\downarrow$ & mIoU $\uparrow$ & BER $\downarrow$ & mAE $\downarrow$   \\  
					\hline
					 MirrorNet~\cite{Mirror_net} & 103.3M & 37ms & 81.3 & 8.98 & 0.094 \\
				     GDNet~\cite{tranparent_gdnet} &183.2M & 41ms & 82.6 & 8.42 & 0.088 \\
				     Translab~\cite{trans10k_xieenze} & 65.2M & 26ms & 85.1 & 7.43 & 0.081 \\
					 EBLNet & \bf{46.2M} & \bf{24ms} & \bf{86.0} & \bf{6.90} & \bf{0.074} \\
					\hline
				\end{tabular}}
		\vspace{-2mm}
		\caption{\small Comparison results with related methods on GDD test set. The speeds are tested with $416 \times 416$ inputs. For fair comparison, all models' backbone are ResNet50 and all the inference times are tested with one V100 GPU.
		}
		\label{tab:comparison_results_with_efficienct}
	\end{threeparttable}
	\vspace{-5mm}
\end{table}

\noindent \textbf{Efficiency and Effectiveness:} To further show the efficiency and effectiveness of our model on glass-like object segmentation, we compare parameters and inference time of our network with recent proposed glass-like object segmentation networks, including MirrorNet~\cite{Mirror_net}, GDNet~\cite{tranparent_gdnet} and Translab~\cite{trans10k_xieenze} on GDD test set. Results are shown in Tab.~\ref{tab:comparison_results_with_efficienct}. Our method has the fewest parameters and fastest inference speed. Our method also achieves the best precision.

\subsection{Generalization Experiments}
\label{generalization experiments}
In this section, we prove the generality of our method via two different experiment settings: (1)~training our model on one glass-like object segmentation dataset then fine-tune on another glass-like object segmentation dataset, (2)~evaluating our model on other three general segmentation datasets, including Cityscapes~\cite{Cityscapes}, BDD~\cite{yu2020bdd100k} and COCO Stuff~\cite{coco_stuff}. Training settings can be found in the supplemental file. 
\begin{table}[!t]
	\begin{center}
	\scalebox{0.66}{
	\begin{tabular}{c c c c |c c c c}
		\hline
		OD & FTD & mIoU & SFT mIoU & OD & FTD & mIoU & SFT mIoU \\
		\hline
		Trans10k & MSD & 78.43 & 78.84 & MSD & Trans10k & 86.42 & 90.28 \\
		Trans10k & GDD & 87.94 & 88.16 & GDD & Trans10k & 89.01 & 90.28\\
		MSD & GDD & 87.07 & 88.16 & GDD & MSD & 79.41 & 78.84\\
		\hline
	\end{tabular}}
	\end{center}
	\vspace{-2mm}
		\caption{\small OD: original dataset. FTD: fine-tune dataset. Models are pre-trained on OD then fine-tuned on FTD and report mIoU on the test set of FTD. FTD mIoU: when model is trained from scratch on the FTD, the mIoU on the FTD test set. The mIoUs are very close to SFT mIoUs. Note that, the mIoU even \textbf{surpass} SFT mIoU when OD is GDD and FTD is MSD. Backbones are ResNet101.}
	\label{table:Generation evaluation1}
	\vspace{-2mm}
\end{table}
\begin{table}[!t]\setlength{\tabcolsep}{4pt}
	\centering
\begin{threeparttable}
		\scalebox{0.6}{
			\begin{tabular}{l c c c c c c c c }
				\toprule[0.2em]
				Method   & dataset & mIoU $\uparrow$ & F1(12px) $\uparrow$ & F1(9px) $\uparrow$ & F1(5px) $\uparrow$ & F1(3px) $\uparrow$ \\
				\toprule[0.2em]
		Deeplabv3+ \cite{deeplabv3p} & Cityscapes & 77.4 & 78.9 & 77.5 & 73.9 & 62.3 \\
    	+PointRend \cite{pspnet}  &  Cityscapes & 78.3 & 79.5 &  78.5 & 74.3 & 63.6 \\
        +ours   &  Cityscapes & 79.1 & 81.4 & 80.1 & 76.5 & 67.0  \\
        \hline
        Deeplabv3+ \cite{deeplabv3p} & BDD & 60.8 & 76.4 & 75.2 & 70.6 & 61.5\\
    	+PointRend \cite{pspnet}  & BDD & 61.2 & 78.4 & 77.2 & 72.3 & 63.6 \\
        +ours   &  BDD  & 63.1 & 80.6 & 79.4 & 75.4 & 66.4  \\
        \hline
        Deeplabv3+ \cite{deeplabv3p} &  COCO Stuff & 33.6 & 73.3 & 71.8 & 70.6 & 66.7\\
    	+PointRend \cite{pspnet}  & COCO Stuff & 34.1 & 73.8 & 72.3 & 71.1 & 67.4 \\
        +ours   &  COCO Stuff  & 34.7 & 74.9 & 72.9 & 71.5 & 67.7 \\
	\bottomrule[0.1em]
	\end{tabular}}
	\vspace{-2mm}
		\caption{\small Comparison results on Cityscapes, BDD and COCO Stuff datasets with DeeplabV3+ and PointRend where X-px means X pixels along the boundaries. All the models are trained and tested with the same setting. Backbones are ResNet50.
		}
		\label{tab:results_on_cityscapes_bdd}
		\vspace{-3mm}
	\end{threeparttable}
\end{table}

\noindent \textbf{Fine-tune Setting:} In this setting, we only fine-tune the models for \textbf{few} epochs in the new datasets~(2, 10, 8 epochs for Trans10k, GDD and MSD, respectively). Results are shown in Tab.~\ref{table:Generation evaluation1}, which indicates our method is \textbf{not} overfitted to one specific glass-like object segmentation dataset.

\noindent \textbf{General Segmentation Dataset Setting:} In this setting, we use the DeeplabV3+ as the baseline method. Results are shown in Tab.~\ref{tab:results_on_cityscapes_bdd}. For all the three datasets, both mIoU and F1 scores of our method are better than those of DeeplabV3+ and PointRend. Thus, our method is \textbf{not} overfitted to glass-like object segmentation dataset.