

\documentclass[12pt]{article}
\usepackage{fullpage}

\usepackage{graphicx,psfrag,amsmath,url}
\newcommand{\BEAS}{}
\newcommand{\BEQ}{}
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}

\newcommand{\eg}{{\it e.g.}}
\newcommand{\ie}{{\it i.e.}}

\newcommand{\ones}{\mathbf 1}
\newcommand{\reals}{{\mbox{\bf R}}}
\newcommand{\integers}{{\mbox{\bf Z}}}
\newcommand{\symm}{{\mbox{\bf S}}}  

\newcommand{\nullspace}{{\mathcal N}}
\newcommand{\range}{{\mathcal R}}
\newcommand{\Rank}{\mathop{\bf Rank}}
\newcommand{\Tr}{\mathop{\bf Tr}}
\newcommand{\diag}{\mathop{\bf diag}}
\newcommand{\lambdamax}{{\lambda_{\rm max}}}
\newcommand{\lambdamin}{\lambda_{\rm min}}

\newcommand{\Expect}{\mathop{\bf E{}}}
\newcommand{\Prob}{\mathop{\bf Prob}}
\newcommand{\Co}{{\mathop {\bf Co}}} \newcommand{\dist}{\mathop{\bf dist{}}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\epi}{\mathop{\bf epi}} \newcommand{\Vol}{\mathop{\bf vol}}
\newcommand{\dom}{\mathop{\bf dom}} \newcommand{\intr}{\mathop{\bf int}}

\newcounter{oursection}
\newcommand{\oursection}[1]{
 \addtocounter{oursection}{1}
 \setcounter{equation}{0}
 \clearpage \begin{center} {\Huge\bfseries #1} \end{center}
 {\vspace*{0.15cm} \hrule height.3mm} \bigskip
 \addcontentsline{toc}{section}{#1}
}
 \usepackage{array}
\usepackage{algorithm2e}
\usepackage{caption,subcaption}
\usepackage{multirow}
\usepackage{versions}

\excludeversion{derivations}

\includeversion{keywords}


\newif\ifarxiv
\arxivtrue



\usepackage{circuitikz}
\usepackage{tikz}


\title{Value Function Approximation for Direct Control of Switched Power Converters}

\author{
Nicholas Moehle\thanks{
Mechanical Engineering Department, Stanford University. 
\texttt{moehle@stanford.edu}}
\and Stephen Boyd\thanks{
Electrical Engineering Department, Stanford University. 
\texttt{boyd@stanford.edu}}
}
\date{\today}


\begin{document}

\maketitle

\begin{abstract}
We consider the problem of controlling switched-mode power converters
using model predictive control.  Model predictive control 
requires solving optimization problems in real time,
limiting its application to systems with small numbers of switches and a short horizon.  
We propose a technique for using off-line computation
to approximate the model predictive controller.
This is done by dividing the planning horizon into two segments,
and using a quadratic function to approximate the optimal cost over the second segment.
The approximate model predictive algorithm minimizes the true cost over the first segment,
and the approximate cost over the second segment,
allowing the user to adjust the computational requirements by changing the length
of the first segment.
We conclude with two simulated examples.
\end{abstract}



\section{Introduction}

\paragraph{Predictive control.}
Model predictive control (MPC) is emerging as a powerful tool for
controlling switched-mode power converters.
A model predictive control scheme uses a planning model
to predict the effects of control decisions
on the converter over some finite planning horizon.
It then compares the outcomes of these decisions using a cost criterion,
choosing an optimal control input sequence, \ie, one
that minimizes the cost over the horizon.
The controller then uses the first control input in the trajectory,
waits a specified time, then repeats the process.
Model predictive control is now widely used in many control applications,
particularly those that are sampled slowly enough that the required
minimization problem can be solved in the sample time.

Finite-control-set MPC (FCS-MPC)
refers to the case in which
the control input takes on a finite number of values.
This is particularly relevant for switched-mode power converters,
where the control inputs typically represent allowable switch configurations.
In this case, solving the optimization problem only requires evaluating
the objective function a finite number of times
and choosing the control inputs that achieve the lowest cost.
When the total number of control inputs over the planning horizon
is small,
this can be done quickly on modest computational platforms,
which is attractive for embedded control.



Unfortunately, the computational cost of solving the FCS-MPC problem
increases exponentially with the length of the planning horizon,
which means that checking all combinations
of control input trajectories becomes very difficult
for discrete horizon lengths of more than, say, four or five.
In practice, this means FCS-MPC is only useful
for converters with simple dynamics,
for which short planning horizons (and myopic controllers) suffice,
or converters with powerful processors available for control.
Even worse, many converter topologies have dynamics
that inherently require a long planning horizon for FCS-MPC to work well,
such as those with inverse response (\ie, non-minimum phase behavior).
For example, to increase the steady-state output voltage of an ideal boost converter,
it may be necessary to charge the inductor,
which can have the short-term effect of \emph{decreasing} the
output voltage.
A long planning horizon may be necessary
for the controller to predict the benefits of charging the inductor
before increasing the output voltage.
We illustrate this behavior for a boost converter in \S \ref{s-boost-example}.
(For further discussion on non-minimum phase behavior in power converters,
see \cite[Ch. 9]{erickson2007fundamentals} and \cite{karamanakos2014direct}.)
 
\paragraph{Our approach.}
In this paper, 
we provide a technique for \emph{approximately} solving
the optimization problems arising in FCS-MPC
with a long planning horizon, with very low computational cost.
This allows the resulting approximate FCS-MPC controller to make 
near-optimal decisions, with long prediction horizons,
at only a tiny fraction of the computational cost of
traditional long-horizon FCS-MPC.


To do this, we split the planning horizon into two segments.
The dynamics and cost function over the first segment are represented exactly,
and the problem is solved by exhaustive search,
as in traditional FCS-MPC.
The optimal cost over the second (typically much longer) 
segment is approximated using 
a multivariate quadratic function,
called the \emph{approximate value function}.
This function is determined during a
pre-solve phase (\ie, during controller synthesis),
using a desktop computer or server.
The controller itself simply compares a relatively
small number of possible input sequences with a short horizon,
and chooses the best one, taking into account the approximate value function.
Our method therefore shifts computational burden from
real-time execution to an off-line, precomputation phase,
which allows us to execute FCS-MPC-like controllers 
with very long horizon lengths on very modest computational hardware.

The resulting control algorithm, which we call 
\emph{value function approximation model predictive control}, 
which we shorten to \emph{approximate model predictive control} (A-MPC)
in this paper,
is very similar to short-horizon FCS-MPC,
differing only by the addition of an extra cost term 
(given by the approximate value function),
which represents the approximate long-term cost.
This resemblance is a practical and pedagogical advantage,
as our method can be explained and implemented as a simple modification
of short-horizon FCS-MPC
that imparts better dynamics on the closed-loop system,
since it has the advantage of a long planning horizon.

The price of doing this is decreased flexibility:
because deriving an approximate value function often 
requires extensive offline precomputation,
our formulation does not handle gross
changes in the objective function or dynamics,
although some moderate changes, such as changing the desired state value,
can be easily accommodated.
However, we suspect that such large-scale changes in the 
operating requirements are rare,
and in any case, can be handled by synthesizing several controllers,
and switching between them as desired.
(This is similar to gain scheduling linear controllers.)




\subsection{Related work}

\paragraph{FCS-MPC for power converters.}
A good introduction to FCS-MPC
is given by Kuoro \cite{kouro2009model}
and Cort\'{e}s \cite{cortes2008predictive},
which include many references to practical applications
as well as tricks that can be used with FCS-MPC
(\eg, shaping the switching spectrum
or controlling reactive power).
These papers primarily cover short-horizon FCS-MPC,
which is easily solved by brute-force enumeration of all possible switching trajectories.

For long-horizon FCS-MPC, enumerating all switching trajectories
is not computationally feasible,
and more sophisticated techniques are used to solve
(or approximately solve) the optimization problem.
For example, branch and bound uses lower bounds on the optimal cost 
to identify and remove suboptimal parts of
the decision tree (see \cite{boyd2007branch}).
Simple lower bounds for the FCS-MPC problem are proposed by Geyer 
\cite{geyer2011computationally},
by simply assuming zero future cost starting from any node in the decision tree.
Another general technique is move blocking
(\ie, artificially limiting the number of control input trajectories
to obtain a suboptimal but practical solution);
an application to boost converter control can be found in 
\cite{karamanakos2012direct}.
Another recent approach is to use a sphere decoding algorithm to solve
the FCS-MPC optimal control problem \cite{geyer2013multistep}. Additionally, techniques from hybrid system theory can be used to explicitly
write the solution of the FCS-MPC problem
as a lookup table with regions defined by linear or quadratic inqualities.
(see \cite{borrelli} for details).
Unfortunately, the same exponential blowup that occurs in long-horizon
FCS-MPC can appear in explicit solutions of hybrid MPC,
\ie, it is possible that there are an exponential number of regions
in the lookup table.
There have been many applications of this approach to power converters;
see \cite{quevedo2014predictive} and \cite{geyer2005model},
and references therein.


\paragraph{Other control techniques.}
Most traditional techniques for controlling power converters
are based on linear control.
Because power converters are switched nonlinear systems,
it is first necessary to linearize the system around an operating point,
and use PWM, or a similar modulation scheme, to convert the continuous
control decisions of the linear controller to discrete switching decisions.
For an introduction to these techniques, see 
\cite{kassakian1991principles} and \cite{erickson2007fundamentals}.
For a discussion of continuous MPC techniques (and others) applied to
controller power converters, see \cite{wang2014pid}.
Deadbeat control policies attempt to drive the system state to
a desired value as quickly as possible,
often using a short prediction horizon.
A good summary can be found in \cite{quevedo2014predictive};
applications to power converters can be found 
in the references of \cite{kouro2009model}.

Many control methods are based on explicitly defining the switching boundaries
in state space,
usually defined using first- or second-order equations;
this idea originated with Burns and Wilson
\cite{burns1976state}, 
\cite{burns1977analytic}.
For some simple converters,
these boundaries can be derived as 
solutions to certain optimal control problems 
using Pontryagin's maximum principle.
For example, the solution to the minimum-time state transfer problem
for a buck converter has a switching boundaries defined by second-order equations.
However, this approach becomes unwieldy for even slightly more complicated systems
(see \cite{dhople2014time} for a discussion).





\paragraph{Dynamic programming.}
Recovering optimal control policies through value function approximation,
which is the basis of the approach in this paper,
is the subject of approximate dynamic programming,
and has been studied extensively in 
operations research, control, robotics, and other fields
(see \cite{bertsekas1995dynamic} and \cite{powell2007approximate} for an introduction).
The general method of exactly solving an optimal control problem
over some time steps, and using an approximation of the value function
for the rest of the horizon, is well known in the dynamic programming
literature, with names such as roll-out policy or look-ahead policy.
Adaptive versions of these methods,
which change the approximate value function during operation of the system,
are possible;
algorithms of these types are the subject of reinforcement learning;
see \cite{sutton1998introduction} and \cite{bertsekas1995dynamic}.

Our algorithm relies on quadratic approximate value functions,
obtained by simple function fitting techniques;
other methods for obtaining quadratic approximate value functions for switched-affine
systems can be found in 
\cite{rantzer2006relaxed} and \cite{wang2015approximate}.
Fast evaluation of control policies involving quadratic value functions
(as well as much more general problems)
can be found in 
\cite{yang2011lyapunov}, \cite{mattingley2011receding}, and
\cite{yang2010fast}.

\paragraph{Mixed-integer nonlinear programming.}
The optimal control problem solved in an FCS-MPC control policy
is a special case of mixed-integer nonlinear programming (MINLP).
MINLP problems are difficult to solve in general,
but algorithms exist that appear to work well in practice.
Several special cases,
such as mixed integer linear programming \cite{wolsey1998integer},
and mixed integer second-order cone programming \cite{benson2013mixed},
have dedicated solvers
(see Gurobi \cite{gurobi}, Mosek \cite{andersen2000mosek}, and CPLEX),
which often outperform general MINLP solvers.
In many cases, small problems 
(such as the FCS-MPC problem for power converters)
can be solved on a desktop in a few seconds.
Furthermore, these solvers can often be interfaced through modeling languages,
such as CVXPY \cite{cvxpy}, and CVX \cite{cvx, gb08},
which increase accessibility and decrease development time.




\subsection{Contributions}
In this paper we present a tractable approximate method
for solving long-horizon predictive control (FCS-MPC)
problems for switched-mode power converters.
We provide a theoretical justification for our method,
as well as practical approach for computing this added extra cost term.
We give simulated results for controlling a boost converter control
and an inverter, 
and we show that our method performs markedly better short-horizon predictive control
and performs comparably to long-horizon predictive control,
at a tiny fraction of the computational cost.

We contrast our method with other methods to achieve long-horizon FCS-MPC.
Many methods attempt to solve the optimal control problem
on the fly, which often necessitates a fast control processor.
Our method, by contrast, 
can be viewed as a simple modification of short-horizon
(or even one-step) FCS-MPC.
This allows our method to work with very low computational cost,
so that our method can be executed
in microseconds or less on modest computational platforms,
while still achieving some of the benefits of long-horizon MPC methods.



\subsection{Outline}
In \S\ref{s-mpc}, we present the theoretical basis for our method,
starting with the mathematical optimal control problem
to be solved by the controller,
and our general approach for approximately solving it.
In \S\ref{s-function-fitting}
we show how to numerically derive the controller
by approximating the value cost as a quadratic function.
In \S\ref{s-implementation}, we show how to implement the controller,
including several special cases in which extra computational savings can be attained.
In \S\ref{s-examples}, we demonstrate our approach on 
two example converters, a boost converter and an inverter.
In \S\ref{s-conclucion}, we conclude with some ideas for extending our method,
and alternative techniques for computing the extra cost term.


\section{Model predictive control}
\label{s-mpc}

\paragraph{Switched dynamical systems.}
We consider the switched dynamical system

with state  and 
(finite) control input .
The subscript  denotes (sampled) time.

In a converter, the state typically consists of physical variables 
such as inductor currents and capacitor voltages, 
but it can also include other, non-physical variables such as
a running sum of errors.
The input corresponds to the allowed switch configurations, and  is
the number of allowed switch positions.
The dynamics function  gives the state at the next sample time,
given the current state and choice of switch configuration.

\paragraph{Control.}
Our task is to choose a \emph{control policy} 

that maps the system state  at time  to a control input , so that

We design the policy according to some objectives, 
such as maintaining the state near some desired value,
or minimizing the converter power loss.
The policy divides the state space into  
(possibly disconnected) regions,
so that if the state is in the th region,
the controller chooses input .
The boundaries of these surfaces are often known as switching surfaces
or switching boundaries;
when the state of the system crosses a switching surface, the 
input (\ie, switch configuration) changes.
Implicit here is that we can measure the state; when this is not the case,
the state can be estimated using, for example, a Kalman filter.

\paragraph{Model predictive control.}


Under a model predictive control policy ,
the control input is chosen by solving an optimal control problem:
to find ,
we first solve the finite-horizon optimal control problem

The decision variables are the predicted system states 
and the planned control inputs 
.
The integer  is the horizon; it determines how far ahead in time we plan
when we choose the current input.
The function  is the stage cost function,
and the function  is the terminal cost function.
Then  is chosen to be , where  is
any solution of the optimal control problem~(\ref{e-opt-ctrl}).










To specify an MPC policy, the designer must choose the horizon ,
the stage cost function , and the terminal cost .
The main difficulty of implementing the MPC policy 
is quickly solving (\ref{e-opt-ctrl}), 
which is (in general) a mixed-integer nonlinear program (MINLP).
In principle the problem can be solved by 
checking the objective value for each possible control input trajectory,
\ie, for each of the  possible trajectories,
and simply simulating the dynamics for each choice of input sequence.
However, for large values of  and  (larger than, say  and ),
this approach is not practical.
In this case, (\ref{e-opt-ctrl}) is very difficult to solve exactly 
on an embedded processor at sub-millisecond speeds,
required for real-time control of switched power converters.
We note, however, that many effective algorithms exist
to solve MINLP problems reasonably quickly 
(if not at sub-millisecond time scale) on desktop computers or servers,
which we will return to later.



A large value of the planning horizon  allows a receding-horizon controller
to consider the impact of current actions far in the future,
and almost always results in a superior control law.
Using a short planning horizon results in a myopic controller,
which can seriously degrade performance. 
Unfortunately, our desire for long horizon directly contradicts
our desire to easily solve \eqref{e-opt-ctrl} using low-cost, embedded processors,
because a large value of  makes \eqref{e-opt-ctrl} much more difficult to solve numerically.
This effectively limits receding-horizon controllers based on \eqref{e-opt-ctrl}
to converters with simple dynamics 
(for which myopic controllers perform well)
or converters with powerful processors available for control.
In the following sections we discuss an alternative approach 
to approximately solve (\ref{e-opt-ctrl}), 
with a long planning horizon ,
in a manner suitable for real-time, embedded control.
We will do this by moving most of the computational burden offline
(\ie, to controller synthesis, not execution),
using value function approximation.


\subsection{Value functions}
\label{s-value-functions}

We will use a standard trick in dynamic programming to express
the problem \eqref{e-opt-ctrl} as a similar one, with a shorter
time horizon .
Define  as the optimal value of the problem

with variables 
, .
Thus  is the optimal value of the control 
problem \eqref{e-opt-ctrl},
starting from state  at time .
It is also called the (Bellman) value function.



We can use the value function to rewrite \eqref{e-opt-ctrl} as

with variables  and 
.

Note that the new problem \eqref{e-one-step-policy}
has the same form as the MPC problem \eqref{e-opt-ctrl},
but with shorter horizon length  and terminal cost .
We can solve it by evaluating the cost of  input sequences,
a far smaller number than the original problem, which requires evaluating
 input sequences.
In the general case this advantage is an illusion, since 
the cost of evaluating  is  
evaluations of value input sequences.
But if the value function  were known explicitly,
with a simple form, solving \eqref{e-one-step-policy}
would be easier than solving (\ref{e-opt-ctrl}).
For example, if  is very small, say, 2 or 3, we could evaluate 
 on a fine grid of points offline and store the values
in a table.


If we take , we have

In this case, the control policy can be carried out 
using just  evaluations of  and .
If  were easy to evaluate (which it is not in general), this 
would give a very simple way to evaluate the MPC policy.



\subsection{Approximate value functions}
Unfortunately, computing, storing, and evaluating 
is difficult in all but a few special cases.
We therefore settle for an approximate or surrogate value 
function  which approximates .
We consider the problem

which is an approximation to \eqref{e-one-step-policy} 
with the same decision variables.
This optimization problem is the basis for the approximate MPC (A-MPC)
policy, .
We define  as an optimal value of 
for \eqref{e-multi-step-policy}.
If we have 
(\ie,  is a perfect approximation),
we recover the MPC policy,
regardless of the shortened horizon length .
Alternatively, if we take  and , 
we recover the MPC policy.

\section{Obtaining an approximate value function}
\label{s-deriving-value-function}
In order to use an approximate MPC policy,
we first need to synthesize an approximate value function .
In practice, good control can be obtained with very simple forms for ,
even when  is not a particularly good estimate of .
(Indeed, the success of short-horizon predictive control 
suggests that even a greedy policy, obtained using , works well
in some cases.)

\paragraph{Quadratic functions.}
We limit our attention to quadratic value function approximations,
\ie, functions of the form

where  is a symmetric positive definite matrix,
 is a scalar,
and
 is a desired system state,
representing an area of the state space with low cost.
The scalar  has no effect on the A-MPC control policy, but is included
since we will choose it and  so that .

For example, one simple quadratic approximate value function is given by

where the symmetric matrix  is such that
 gives the total energy stored in the converter.
Control policies based on the energy function appear
to have good stability properties;
for details, see \cite{sanders1992lyapunov} and \cite{midya1992optimal}.














\paragraph{Function fitting.}
\label{s-function-fitting}
We first choose state vectors ,
which are points at which we desire an accurate approximation.
Then for each state vector , we compute the value function
 by solving problem \eqref{e-opt-ctrl} with .
This is done offline, possibly using substantial computational resources.
We then choose the parameters  and  of the approximate value function
as the solution to the least-squares approximation problem

The decision variables are the approximate value function parameters
 and , as well as the scaling factor .
Note that  and  enter linearly in ,
which allows us to solve this problem as a least-squares problem.
The parameter  is a regularization parameter;
if we choose  to be small,
the optimal  and  almost minimize
mean-squared error between the approximate value function
and the sampled values,
thus returning a good value function approximation at the points.
By choosing  large,
we force  to be close to (a scaled version of) ;
this tends to make the resulting control more robust,
especially if we have few samples.

With values of  and  that solve \eqref{e-approx-problem},
we expect that  
in areas of the state space that are well represented by the state samples.
Because the flexibility (and therefore, accuracy)
of the approximate value function is a limited resource,
it is important to chose state samples that are likely
to be encountered in normal operation,
and not to include samples
far from relevant areas of the state space,
since including them may decrease accuracy in relevant areas.

We can explicitly add the constraints that  be positive definite,
and ; the approximation problem above is no longer
a least-squares problem but it is still convex and readily solved
\cite{boyd2004convex}.
But when the parameter  and the sample points are appropriately
chosen, these constraints are typically satisfied automatically.
We also note that since we are approximating the value function, we 
do not need to evaluate  exactly; good approximations are
good enough.



\section{Evaluating the approximate policy}
\label{s-implementation}
In this section we first show how to evaluate the A-MPC policy in general;
we then discuss several special cases
in which this process can be simplified,
often with great computational benefits.
We also discuss how to (approximately) handle switching costs.


\subsection{General  and }
To carry out the approximate MPC policy,
we first measure the current system state.
We then solve \eqref{e-multi-step-policy},
with  set to the current state,
by first finding the state trajectory 
(simulating the system for each of the  different input trajectories),
and evaluating the cost function for each simulated trajectory.
Once we have obtained the input trajectory that attains the lowest cost,
we apply the optimal first control input .
The process is then repeated at the next sample time.
This is the same process that can be used to solve the MPC problem \eqref{e-opt-ctrl}
when the horizon length is short enough,
and reflects the fact that 
\eqref{e-opt-ctrl} and \eqref{e-multi-step-policy}
have the same form.

Evaluating the value function requires finding a full desired state .
This contrasts with the MPC problem (\ref{e-opt-ctrl}),
in which a full desired state is not required,
and often only a few (but not all) state variables appear in the cost function.
One might ask why state variables appear in the value function
that do not appear in any cost function.
This is because, although some state variables do not appear explicitly
in the value function, 
they can play an important role in the future evolution of the system,
so that deviation of these state variables from `good' values
may result in high cost in the future,
\ie, result in deviation of other, explicitly penalized variables from `good' values.

For example, for the boost converter example in \S\ref{s-examples},
we explicitly penalize only output voltage deviation from a desired value,
and not inductor current deviation.
However, we expect the approximate value function to penalize
inductor current deviation,
because the inductor current has a substantial effect
on \emph{future} values of the output voltage;
a deviation of the inductor current from `good' values
results in a future deviation of capacitor voltage from the
desired target in the future.

Finding appropriate values of  is case specific,
but usually involves only simple computations;
in the boost converter example,
we take the desired inductor current to simply be equal to the load current
when the output voltage is equal to the desired value.



\paragraph{One-step horizon.}
In the case , we can simplify 
\eqref{e-multi-step-policy}
by eliminating the states as variables,
allowing us to write the policy as

In this case the approximate MPC policy requires only 
 evaluations of  and .


\subsection{Switched-affine systems}
In this section we present a solution approach
for switched-affine dynamics and quadratic costs.
More specifically, we assume the system dynamics \eqref{e-sw-sys}
can be written as

for matrices  and vectors ,
and that the stage costs  are quadratic, so that

where we have ,
\ie, the desired state is an affine function of the current state.

In this case, 
the optimization problem \eqref{e-multi-step-policy}
can be written as:

The variable is the vector 
.
The functions , , 
are given by


\begin{derivations}
We have

with


The total cost is

where

\end{derivations}
We use the shorthand  if ,
and  if .

We can precompute the parameters of these functions
for each of the  different values of .
To solve \eqref{e-opt-ctrl-sw-aff},
we evaluate each of the  different quadratic functions at 
and note which quadratic function attains the lowest value;
the corresponding index  is a solution to 
\eqref{e-opt-ctrl-sw-aff}.
By precomputing , , and ,
we can acheive significant computational savings
compared to a simulation-based approach.


\subsection{Linear systems with switched input}
We consider a special case of the switched-affine systems.
We assume quadratic stage costs and dynamics of the form

Because this is a special case of the switched affine case above,
the A-MPC problem \eqref{e-opt-ctrl}
can also be written as \eqref{e-opt-ctrl-sw-aff}.
However, because the parameter  does not depend on the control input ,
it can be dropped from the optimization problem 
without affecting the optimal decision variables,
so that the A-MPC problem \eqref{e-opt-ctrl} is equivalent to

The variable is the vector 
.
The functions ,  are given above.
As before, we can precompute the parameters of the affine functions
in the objective of \eqref{e-opt-ctrl-sw-lin}
for each of the  different values of .



\paragraph{One-step horizon.} 
If we have  in addition to 
linear systems with switched inputs and quadratic costs,
then (\ref{e-multi-step-policy}) can be written as

In this case we interpret  
to give minimum over all indices of the vector .
The matrix  and 
the vector  can be precomputed as

Note that if  changes during the operation of the converter,
only  needs to be updated.

\section{Summary of approximate MPC method}
Here we summarize the steps needed to use the approximate MPC policy.

First, the parameters of the underlying MPC policy are determined.
This means that a dynamics model \eqref{e-sw-sys}
of the system is derived and
the cost function and horizon length are determined.
The dynamics model should be simple enough, and the horizon length modest enough,
for problem \eqref{e-opt-ctrl}
to be solved fairly quickly on a desktop computer,
so that many (hundreds or thousands) of samples can be generated reasonably quickly.

Second,
the reduced horizon length  is chosen,
based on the computational capabilities of the embedded control platform
(\ie, a large value of  
results in a more accurate approximation of the MPC problem,
but also in a higher computational burden,
as the controller will need to evaluate  different trajectories each time step.)
Sample states likely to be encountered in normal operation are then chosen,
and the value function 
is evaluated for each sample  by solving \eqref{e-value}.
Once the value function has been evaluated for all sample points,
we choose the regularization tradeoff parameter 
and solve the approximation problem \eqref{e-approx-problem}
to obtain the approximate value function
parameters  and .

Once the approximate value function is obtained,
the control policy can be evaluated by solving the short-horizon policy 
\eqref{e-multi-step-policy},
implementing the optimal first control input ,
waiting until the next sampling time, and repeating,
as discussed in \S\ref{s-implementation}.




\section{Extensions}
\subsection{Switching cost}
In this section, we describe how to include switching costs,
\ie, a cost associated with switching from one control input to another,
so that (\ref{e-opt-ctrl}) becomes

where  is a switching cost function,
and the parameter  represents the control input
most recently implemented.
In theory,
this problem
can be put into the form of problem (\ref{e-opt-ctrl})
by encoding the switch position as a state variable.


However, a more convenient way to handle the switching cost is to 
include it in the first  time steps 
and simply ignore it during the last  time steps.
By doing this, the process of deriving the value function 
(see \S\ref{s-deriving-value-function})
remains the same,
but the control policy is modified to reflect the switching costs:

Trivial modifications to this setup allow us to include
many of the standard tricks for FCS-MPC can be applied directly,
such as using a frequency-based switching cost
to enforce a constant switching frequency.
(See \cite{kouro2009model, cortes2008predictive}, for details.)
We expect this approach to work well for
converters in which the switching dynamics do not
have a significant long-term effect,
\eg, converters with a high switching frequency.



\section{Examples}
\label{s-examples}

\subsection{Boost converter example}




\label{s-boost-example}
We consider the ideal boost converter shown in figure~\ref{f-boost}.
The system state is

where  and  are the inductor current and capacitor
voltage at discrete epoch .
We (approximately) discretize the converter dynamics
to have the form \eqref{e-sw-sys}.
The details of the discretization are given appendix \ref{s-boost-derivation}.

\begin{figure}
\centering
\resizebox{.45\textwidth}{!}{

\begin{tikzpicture}[scale=1.1]
\tikzstyle{every node}=[font=\huge]
\tikzstyle motor=[color=gray!50!white,draw=black,thick]
\ctikzset{bipoles/diode/height=.375}
\ctikzset{bipoles/diode/width=.3}

\draw[scale=1] (0,0) 
to [L, l=] ++(2,0)
to [R, l=] ++(2,0) node(N_Q){}
to [full diode] ++(3,0) node(N_C){}
-- ++(2,0) node(N_load){}
to [R, l=] ++(0,-3)
-- (0, -3)
to [american voltage source, l=] ++(0,3)
;

\draw[scale=1] 
(N_Q) ++ (0,-3) to [cspst, bipoles/length=3cm, l=] (N_Q)
;

\draw[scale=1] 
(N_C) to [C, l_=] ++(0,-3)
;

\end{tikzpicture}
}
\caption{
Boost converter model.
}
\label{f-boost}
\end{figure}




We use the cost function to 
penalize deviation of the capacitor voltage from a desired value:

With this cost function, 
and with the discretized dynamics, problem \eqref{e-opt-ctrl}  can be written as a mixed-integer linear program (MILP).


We used the parameters of \cite{karamanakos2013direct},
so that
 V,
 H,
 ,
 F,
and
 .
We took  V
(instead of  V, as in \cite{karamanakos2013direct}),
because this increased target voltage means that the controller must
charge up the inductor at the cost of temporarily increasing voltage error.
(For  V,
the controller can simply turn off the switch,
charging the inductor and capacitor simultaneously
until the output voltage reaches the desired value.)

Following \S\ref{s-function-fitting},
we randomly generated  initial states
by sampling uniformly over a box with
a lower bound of 
and an upper bound of ,
which are reasonable limits for normal operation of the converter.
We then solved problem \eqref{e-opt-ctrl} for each of the initial states,
to within one percent accuracy.
Using these sample points,
we solved the regularized approximation problem 
\eqref{e-approx-problem}
to obtain an approximate value function,
with parameter , using a positive semidefinite constraint on .
The resulting value function is shown in figure \ref{f-V}
along with the  randomly sampled values.
To evaluate the ADP policy, we must explicitly define a desired state vector ,
which we take to be ).


\begin{figure}
\centering
\psfrag{V}[bc]{\small }
\psfrag{v}[cr]{\small  ()}
\psfrag{i}[bl]{\small  ()}
\ifarxiv
\includegraphics[width=.4\textwidth]{boost_V.eps}
\else
\includegraphics[width=.4\textwidth]{jl/boost_V.eps}
\fi
\qquad\qquad
\psfrag{V}[bc]{\small }
\ifarxiv
\includegraphics[width=.4\textwidth]{boost_Vhat.eps}
\else
\includegraphics[width=.4\textwidth]{jl/boost_Vhat.eps}
\fi
\caption{
For the boost converter example,
the  sampled points of the value function are shown on the left,
and the fitted quadratic approximate value function 
is shown on the right,
as a function of the inductor current 
and the capacitor voltage .
}
\label{f-V}
\end{figure}

In figure \ref{f-traj-mpc} we show the closed-loop response of the system
under FCS-MPC controllers with horizon lengths of , , and ,
with initial condition  
(\ie, zero inductor current and capacitor voltage).
To evaluate the FCS-MPC control policy,
(\ref{e-opt-ctrl}) was solved to one percent accuracy.
With a horizon length of ,
the controller does not reach the desired steady-state
value of  V
and for , the desired steady-state value is reached,
but only slowly.
For , the desired steady-state value is reached very quickly;
however, using such a long planning horizon is not practical.
(In our desktop implementation, 
each MPC evaluation took around 10 minutes on average.)

In figure \ref{f-traj},
we show the response using
the A-MPC controller, using  and using the
approximate value function derived from function fitting.
We can see that the A-MPC controller outperforms the
MPC controllers with short horizons ( and ),
and is on par with the long-horizon MPC policy ().


\begin{figure}
\centering
\psfrag{v}[bc]{Cap.\ voltage ()}
\psfrag{i}[bc]{Ind.\ current ()}
\psfrag{t}[c]{time ()}
\ifarxiv
\includegraphics[width=.75\textwidth]{boost_mpc.eps}
\else
\includegraphics[width=.75\textwidth]{jl/boost_mpc.eps}
\fi
\caption{
Closed-loop trajectories for the boost converter example with the MPC policy,
for different values of the planning horizon .
The desired output voltage value is  V.
}
\label{f-traj-mpc}
\end{figure}

\begin{figure}
\centering
\psfrag{v}[bc]{Cap.\ voltage ()}
\psfrag{i}[bc]{Ind.\ current ()}
\psfrag{t}[c]{time ()}
\ifarxiv
\includegraphics[width=.75\textwidth]{boost_adp.eps}
\else
\includegraphics[width=.75\textwidth]{jl/boost_adp.eps}
\fi
\caption{
Closed-loop trajectories for the boost converter example,
using the A-MPC policy.
The desired output voltage value is  V.
}
\label{f-traj}
\end{figure}
















\subsection{Inverter example}
\label{s-inverter-example}
In this example, we consider a three-phase inverter
with an LCL filter.
The input is a constant DC source,
and the output is a three-phase AC voltage load.
The goal is to maintain a sinusoidal output current waveform
with a desired amplitude and a high power factor.

\paragraph{Model.}
The inverter is shown in figure~\ref{f-inverter}.
The AC output voltage signals are sinusoidal,
each with amplitude of 
and with a phase difference of  between them.
The control input to the system is the positions of the six switches;
we assume the two switches in each bridge cannot both be open or closed.

The inverter dynamics can be expressed in the form of (\ref{e-lin-sys-sw-input}),
\ie, as a linear system with a switched input.
For the full derivation of the dynamics,
see appendix \ref{s-inverter-derivation}.
We note here that to put the dynamics in this form,
we include in the state vector not only the
currents of the six inductors
and the voltages of the three capacitors,
but also the values  and .
These extra state variables are used
to express both the time-varying output voltage signal and desired output current signals
in a linear, time-invariant form.


\begin{figure}
\centering
\resizebox{.9\textwidth}{!}{

\def\bridgewidth{1.5}
\def\bridgeswheight{2}
\def\bridgevertsep{.5}
\def\bridgetoLCL{2}
\def\LCLheight{2.5}
\def\capsep{2}
\def\capdistance{.5}
\def\sourcebridgesep{2}

\begin{tikzpicture}[scale=1.1]
\tikzstyle{every node}=[font=\LARGE]
\tikzstyle motor=[color=gray!50!white,draw=black,thick]
\ctikzset{bipoles/diode/height=.375}
\ctikzset{bipoles/diode/width=.3}




\draw[scale=1] (0,0) 
-- ++(\sourcebridgesep, 0) node(N_bridge1top){}
-- ++(\bridgewidth, 0) node(N_bridge2top){}
-- ++(\bridgewidth, 0) node(N_bridge3top){}
;

\draw[scale=1] (N_bridge1top)
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight) node(N_bridge1center){}
-- ++(0,-1)
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight)
;

\draw[scale=1] (N_bridge2top)
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight)
-- ++(0,-.5) node(N_bridge2center){}
-- ++(0,-.5)
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight)
;

\draw[scale=1] (N_bridge3top)
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight)
-- ++(0,-1) node(N_bridge3center){}
to [cspst, bipoles/length=3cm] ++(0,-\bridgeswheight) node(N_bridge3bottom){}
;

\draw[scale=1] (N_bridge3bottom)
-- ++(-2*\bridgewidth-\sourcebridgesep,0)
to [american voltage source, l=] ++(0,2*\bridgeswheight+2*\bridgevertsep)
;


\draw[scale=1] (N_bridge1center) -- ++(0,0)
-- ++(0,0)
-- ++(2*\bridgewidth+\bridgetoLCL,0)
-- ++(0,\LCLheight/2 -\bridgevertsep)
to [L, l=, ] ++(3,0) node(N_cap1top){}
-- ++(2*\capsep, 0)
to [L, l=] ++(3,0)
to [sinusoidal voltage source] ++(3,0)
-- ++(0,-\LCLheight)
;

\draw[scale=1] (N_bridge2center) -- ++(0,0)
-- ++(\bridgewidth+\bridgetoLCL,0)
to [L, l=] ++(3,0)
-- ++(\capsep, 0) node(N_cap2top){}
-- ++(\capsep, 0)
to [L, l=] ++(3,0)
to [sinusoidal voltage source] ++(3,0)
;

\draw[scale=1] (N_bridge3center) -- ++(0,0)
-- ++(\bridgetoLCL,0)
-- ++(0,-\LCLheight/2 +\bridgevertsep)
to [L, l=] ++(3,0)
-- ++(2*\capsep, 0) node(N_cap3top){}
to [L, l=] ++(3,0)
to [sinusoidal voltage source] ++(3,0)
;

\draw[scale=1] (N_cap1top) -- ++(0,0)
-- ++(0, -\LCLheight)
-- ++(0, -\capdistance)
to [C, l_=] ++(0,-1.5)
-- ++(2*\capsep,0)
;

\draw[scale=1] (N_cap2top) -- ++(0,0)
-- ++(0, -\LCLheight/2)
-- ++(0, -\capdistance)
to [C, l_=] ++(0,-1.5)
;

\draw[scale=1] (N_cap3top) -- ++(0,0)
-- ++(0, -\capdistance)
to [C, l_=] ++(0,-1.5)
;

\end{tikzpicture}
}
\caption{
Inverter model.
}
\label{f-inverter}
\end{figure}

We use the cost function to 
penalize deviation of the load currents from desired values:

where  is the desired peak current value,
and 
,
,
and

are the values of the output currents
(\ie, the currents through the inductors on the righthand side of figure~\ref{f-inverter}).
Because each of the three sinusoidal terms can be written
as a linear combination of  and ,
including these values as state variables
allows us to write the righthand side in terms of the state vector,
independent of .
With this cost function, 
and with the discretized dynamics given in appendix \ref{s-inverter-derivation},
problem \eqref{e-opt-ctrl}  can be written as a mixed-integer quadratic program (MIQP).


We used the parameters  V,
 H,
 H,
 F,
 V,
and
 A.



\paragraph{ADP controller.}
Following \S\ref{s-function-fitting},
we randomly generated  initial states
by sampling uniformly over a box.
The bounds of the box were chosen to be 
 A and  A for state variables representing inductor currents,
 V and  V for state variables representing capacitor voltages,
and  and  for the state variables  and .
We solved problem \eqref{e-opt-ctrl} for each of the initial states,
to within one-percent accuracy.
Using these sample points,
we then solved the regularized approximation problem 
\eqref{e-approx-problem}
to obtain an approximate value function,
with parameter  and no constraints on  or .
The energy corresponding of the terms 
 and ,
used to define the matrix  in (\ref{e-approx-problem}),
was taken to be .

When evaluating the controllers 
(using (\ref{e-multi-step-policy-switching})),
we incorporate a switching cost into the control.
We use a constant switching cost,
so that  if ,
and  otherwise.

To use (\ref{e-quad-value-fun}),
we must find a desired state vector ,
as an affine function of the current state.
Note that for a given value of  and ,
there is a unique set of load currents 
,
,
and

that result in zero cost.
Corresponding other inductor currents and capacitor voltages
are found using phasor calculations,
and expressed as an affine function of the state variables
(namely  and ).



\paragraph{Results.}
In figure~\ref{f-inverter-traj-adp}
we show the closed-loop response of the system
under the ADP controller with horizon length .
The initial condition was taken to be in steady-state,
\ie, by choosing an initial state ,
computing the corresponding desired state 
,
and starting the simulation from .

We compare the 1-step ahead ADP controller
to the FCS-MPC policy with  and ,
solved to one percent accuracy.
We note that the system is unstable for ,
and that a brute force solution of problem (\ref{e-multi-step-policy}) for 
requires comparing  trajectories, 
which is not computationally feasible for real-time control.
In table~\ref{t-inverter}, 
we see the accumulated stage costs and switching costs over the simulation horizon,
for each of the the three control policies.
We see that the ADP controller performs comparably to the FCS-MPC policies,
both in terms of cumulative stage costs and switching costs.
In figure 
\ref{f-inverter-traj-adp},
we show the response using the A-MPC controller.

\begin{table}
\begin{center}
\begin{tabular}{lcc}
{\bf Control policy}             & {\bf State cost}   & {\bf Switching cost}   \\
\hline \hline                                                                 
ADP policy,            & 0.70               & 0.66        \\
FCS-MPC policy,           & 0.45               & 0.50        \\
FCS-MPC policy,          & 0.30               & 0.53        \\
\hline
\end{tabular}
\end{center}
\caption{
The average state cost and the average switching cost 
(\ie, the average values of  and 
over the simulation),
for different control policies.
}
\label{t-inverter}
\end{table}






\begin{figure}
\centering
\psfrag{ii}[bc]{Input curr.\ ()}
\psfrag{v}[bc]{Cap.\ voltage ()}
\psfrag{io}[bc]{Output curr.\ ()}
\psfrag{t}[cc]{time ()}
\ifarxiv
\includegraphics[width=.85\textwidth]{adp_traj.eps}
\else
\includegraphics[width=.85\textwidth]{jl/adp_traj.eps}
\fi
\caption{
Closed-loop trajectories for the inverter example with the ADP policy,
for .
}
\label{f-inverter-traj-adp}
\end{figure}
















\section{Conclusion}
\label{s-conclucion}
We propose a method for approximately solving long-horizon FCS-MPC
problems that avoids extensive computation on the control processor.
Our method relies on estimating a quadratic approximate value function,
which we do by sampling.

We conclude by noting some other ways to obtain 
a quadratic approximate value function .
For example, if we minimize a quadratic cost function
over an approximate, linearized system,
a quadratic value function can be obtained by solving a Riccatti equation;
this method can be used instead of the function fitting method described in 
\S\ref{s-function-fitting}.
Another possible way involves solving linear matrix inequalities
corresponding to a relaxed Bellman equation;
for details, see \cite{rantzer2006relaxed} or \cite{wang2015approximate}.

\appendix

\clearpage
\bibliographystyle{alpha}
\bibliography{pow_elec_ctrl}

\clearpage
\section{Derivation of boost converter dynamics}
\label{s-boost-derivation}




Here we give the discrete dynamics for the boost converter example.
The discrete-time dynamics of the converter are approximated as:

We have

where  denotes the matrix exponential.
The CCM/DCM threshold parameters  and  are chosen
to predict whether the inductor current drops below zero by the next time interval:



\section{Derivation of inverter dynamics}
\label{s-inverter-derivation}
Here we show how to write the dynamics of the inverter in figure~\ref{f-inverter} 
in the form (\ref{e-sw-sys}).
The converter can be defined by the continuous-time differential algebraic equations

where  is the continuous system state,
consisting of all inductor currents and capacitor voltages in 
figure~\ref{f-inverter}:

Also,  is the vector of bridge voltages,
 is the vector of (sinusoidal) load voltages,
 is a vector of floating node voltages.
We have


We can solve these differential algebraic equations to obtain:

where 
, 
,
,
and .



We now show how to represent the time varying, uncontrolled vector
 in terms of  and .
We have

By incorporating  and  as state variables,
we can then express the system in a time-invariant form:

Because the input voltage is constant over each time epoch,
we can exactly discretize this differential equation using a matrix exponential:

where  is the discretization timestep.

Note that in each time epoch,
 can take one of seven values:

Premultiplying these vectors by  yields
the vectors  in equation 
(\ref{e-lin-sys-sw-input}).











\end{document}
