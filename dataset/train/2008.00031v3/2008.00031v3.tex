
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage[normalem]{ulem}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hhline}

\ifCLASSOPTIONcompsoc
    \usepackage[caption=false, font=normalsize, labelfont=sf, textfont=sf]{subfig}
\else
\usepackage[caption=false, font=footnotesize]{subfig}
\fi
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
    
\begin{document}

\title{No-Reference Video Quality Assessment Using Space-Time Chips}

\author{\IEEEauthorblockN{Joshua P. Ebenezer,\textsuperscript{\textsection}\footnotemark
\IEEEauthorrefmark{1} Zaixi Shang,\textsuperscript{\textsection}\IEEEauthorrefmark{1} Yongjun Wu,\IEEEauthorrefmark{2} Hai Wei,\IEEEauthorrefmark{2} Alan C. Bovik\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1} Laboratory for Image and Video Engineering (LIVE), The University of Texas at Austin \\
 \IEEEauthorrefmark{2} Amazon Prime Video \\
 joshuaebenezer@utexas.edu, zxshang@utexas.edu, yongjuw@amazon.com, haiwei@amazon.com, bovik@ece.utexas.edu}
}

\IEEEoverridecommandlockouts
\IEEEpubid{\makebox[\columnwidth]{978-1-4799-7492-4/15/\I_TTI_{T-T'+1}..I_{T-1}T-T'+1M \times NT'TT'(i,j)w = \{w(k,l), k\in -K,..,K, l\in -L,..,L\}T-T'+1TTTR \times R\hat{I}_{T-T'+1}\hat{I}_TS_TR\times RRxyR=T'S_T M' \times N'M'=T'\lfloor \frac{M}{T'} \rfloorN'= T' \lfloor \frac{N}{T'}\rfloorT'T\Gamma(.)\alphaTS_T\nu\sigma_l\sigma_r\eta,\nu,\sigma_l^2,\sigma_r^2T'=5f_1-f_4f_5-f_{36}f_{37}-f_{40}f_{41}-f_{72}f_{73}-f_{109}\times95\%$ confidence level to evaluate the statistical signifiance of the results. The results show that ChipQA-0 is statistically superior to all other algorithms on the LIVE-APV Livestream VQA database.

\color{black}
\begin{table*}
\caption{Results of one-sided t-test performed between SROCC values of various algorithms on the LIVE-APV database. '1' ('-1') indicates that the row algorithm is statistically superior (inferior) to the column algorithm. The matrix is symmetric}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
\textsc{Method}  &  NIQE & BRISQUE & HIGRADE & CORNIA & TLVQM & VIIDEO &  V-BLIINDS & ChipQA-0  \\
\hline
NIQE & - & -1 & -1 & -1 & -1 & 1 & -1 & -1 \\
\hline
BRISQUE & 1 &  - & -1 & -1 & -1 & 1 & -1 & -1 \\
\hline
HIGRADE  &  1 & 1 & - & 1 & -1 & 1 & -1 & -1  \\
\hline
CORNIA  & 1 & 1 & -1 & - & -1 & 1 & -1 & -1  \\
\hline
TLVQM &  1 & 1 & 1 & 1 & - & 1 & 1 & -1\\
\hline
VIIDEO &  -1 &-1 & -1 & -1 & -1 & - & -1 & -1\\
\hline
V-BLIINDS & 1 & 1 & 1 & 1 & -1 & 1 & - & -1 \\
\hline
ChipQA-0 &  1 & 1 & 1 & 1 & 1 & 1 & 1 & -  \\
\hline
\end{tabular}
\label{stat}
\end{center}
\end{table*}


\color{black}
\subsection{Computational cost}
Table \ref{compcost} shows the computation times required to extract features for each algorithm on a single 4K video from the LIVE-APV database. Costs for the IQA algorithms were estimated by multiplying the computation time for a single frame by the total number of frames in the video. VIIDEO, VBLIINDS, and ChipQA-0 were implemented with Python. The other algorithms were implemented with MATLAB\textsuperscript{\textregistered}. It is not possible to directly compare these numbers because they were implemented on different platforms with different optimization strategies, but they can serve as a rough estimate, and ChipQA-0 is reasonably efficient. It was run on an Intel i9 9820X CPU with 10 cores and a maximum frequency of 4.1 GHz. All other algorithms were run on a AMD Ryzen 5 3600 with a maximum frequency of 4.2 GHz.

\color{black}
\section{Conclusion}
We have proposed the novel concept of ST-Chips, and defined how they are extracted and described why they are relevant to video quality. We used the statistics of these chips to model 'naturalness' and deviations from naturalness, and proposed parameterized statistical fits to their statistics. We further used the parameters from these statistical fits to map videos to subjective opinions of video quality without explicitly finding distortion-specific features and without reference videos. We showed that our prototype distortion-agnostic, no-reference video quality assessment algorithm, ChipQA-0, is highly competitive with other state-of-the-art models on a number of databases. We continue to refine the model, with one aim being to eliminate the need for an optical flow algorithm.

\section{Acknowledgments}

The authors thank the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported in this paper. URL: http://www.tacc.utexas.edu.



\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,conference_101719.bib}


\end{document}
