
\section{Experiments and Results}
\subsection{Implementation Details}
\textbf{Dataset and Protocol:} For our experiments, we use evaluation protocols similar to iCARL \cite{rebuffi2017icarl}. We incrementally learn  classes on CIFAR-100 in groups of ,  and  at a time. For ImageNet, we use the same subset as \cite{rebuffi2017icarl} comprising of  classes and incrementally learn them in groups of . After training on a new group of classes, we evaluate the trained model on test samples of all seen classes (including current and previous tasks). Following iCARL \cite{rebuffi2017icarl}, we restrict exemplar memory budget to k samples for CIFAR-100 and ImageNet datasets. Note that unlike iCARL, we randomly select our exemplars and do not employ any herding and exemplar selection mechanism.

We also experiment our model with MNIST and SVHN datasets. For this, we resize all images to  and keep a random exemplar set of k, as in \cite{Hsu18_EvalCL}. We group  consecutive classes into one task and incrementally learn  five tasks. For evaluation, we report the average over all classes ().  

\begin{figure*}[htp]
\center
\includegraphics[width=0.45\textwidth]{figs/CIFAR100_20.pdf}
\includegraphics[width=0.45\textwidth]{figs/CIFAR100_10.pdf}
\includegraphics[width=0.45\textwidth]{figs/CIFAR100_5.pdf}
\includegraphics[width=0.45\textwidth]{figs/CIFAR100_2.pdf}
\caption{ \emph{Results on CIFAR-100}. Evaluations are performed with 20, 10, 5 and 2 tasks (from \emph{left} to \emph{right}). We surpass state-of-the-art results on all four set of experiments. }\label{fig:iCIFAR} 
\end{figure*}

\textbf{Training:}
For the CIFAR100 dataset, we use \texttt{resnet-18} along with max pooling after 5th, 7th blocks and global generalized mean pooling (GeM)~\cite{GeM_pooling} with pooling parameter 3 is used after 9th block. For ImageNet dataset, we use the standard \texttt{resnet-18} architecture as in  \cite{rebuffi2017icarl}. After the GAP layer, a single fully connected layer with weights   is used as a classifier. For MNIST, a simple 2 layered MLP (with  neurons each), whereas for SVHN \texttt{resnet-18} is used, similar to \cite{Hsu18_EvalCL}.

For each task, we train our model for  epochs using Adam~\cite{kingma2014adam} with , with learning rate starting from  and divided by  after  epochs. We set the  controller's scaling factor to  and  respectively for CIFAR and ImageNet datasets. We use the ratio between the number of training samples for a task and the fixed number of exemplars as the value for . We fix  through out the experiments. We do not use any weight or network regularization scheme such as dropout in our model. For augmentation, training images are randomly cropped, flipped and rotated (). For each task, we train  models in parallel using a Nvidia-DGX-1 machine. These models come from the randomly sampled paths in our approach and may have some parts frozen due to an overlap with previous tasks. 


\subsection{Results and Comparisons}

We extensively compare the proposed technique with existing state-of-the-art methods for incremental learning. These include Elastic Weight Consolidation (EWC) \cite{kirkpatrick2017overcoming}, Riemannian Walk (RWalk) \cite{chaudhry2018riemannian}, Learning without Forgetting (LwF) \cite{li2018learning}, Synaptic Intelligence (SI) \cite{zenke2017continual}, Memory Aware Synapses (MAS) \cite{aljundi2018memory}, Deep Model Consolidation DMC \cite{zhang2019class} and Incremental Classifier and Representation Learning (iCARL) \cite{rebuffi2017icarl}. We further evaluate on three baseline approaches: Fixed Representation (\textit{FixedRep}) where the convolution part of the model is frozen and only the classifier is trained for newly added classes, \textit{FineTune} where the complete previously learnt model is tuned for the new data, and \textit{Oracle} where the model is trained on all samples  from previous and current tasks. 

Fig.~\ref{fig:iCIFAR} compares different methods on CIFAR-100 datasets, where we incrementally learn groups of ,  and  classes at a time. The results indicate superior performance of the proposed method in all settings. For the case of learning  classes at a time, we improve upon iCARL \cite{rebuffi2017icarl} by an absolute margin of . Compared with the second best method, our approach achieves a relative gain of  and  respectively for the case of incrementally learning  and  classes on CIFAR-100 dataset. For the case of  classes per task, our performance is  only  below the \textit{Oracle} approach, where all current and previous class samples are used for training. 

\setlength{\tabcolsep}{0.2cm}
\begin{table*}[h]
    \caption{Large Scale experiments on ImageNet-1K and and MS-Celeb-10K show that \ours{} performs favourably against all the state-of-the-art methods. Note that reported task  accuracy is an average of all  tasks.}
    \label{tbl:imagenet}
    \begin{center}
        \begin{tabular}{c l c c c c c c c c c c}
        \toprule[0.4mm]
        \textbf{Datasets} & \textbf{Methods} & \textbf{1} & \textbf{2}& \textbf{3}& \textbf{4}& \textbf{5}& \textbf{6}& \textbf{7}& \textbf{8}& \textbf{9}& \textbf{Final} \\
            \midrule
            \multirow{6}{*}{ImageNet-100/10}
            & Finetuning & 99.3&49.4&32.6&24.7&20.0&16.7&13.9&12.3&11.1&9.9 \\
            & FixedRep & 99.3&88.1&73.7&62.6&55.7&50.2&42.9&41.3&39.2&35.3 \\
            & LwF \cite{li2018learning} & 99.3&95.2&85.9&73.9&63.7&54.8&50.1&44.5&40.7&36.7 \\
            & iCaRL \cite{rebuffi2017icarl} & 99.3&97.2&93.5&91.0&87.5&82.1&77.1&72.8&67.1&63.5 \\
            & Ours & 100.0&97.4&94.3&92.7&89.4&86.6&83.9&82.4&79.4& \ \ \ \ \ \ \ \ \ \ 
             \\
            \midrule
            \multirow{5}{*}{ImageNet-1K/10}
            & Finetuning & 90.2&43.1&27.9&18.9&15.6&14.0&11.7&10.0&8.9&8.2 \\
            & FixedRep & 90.1&76.1&66.9&58.8&52.9&48.9&46.1&43.1&41.2&38.5 \\
            & LwF \cite{li2018learning} & 90.2&77.6&63.6&51.6&42.8&35.5&31.5&28.4&26.1&24.2 \\
            & iCaRL \cite{rebuffi2017icarl} & 90.1&82.8&76.1&69.8&63.3&57.2&53.5&49.8&46.7&44.1 \\
            & Ours & 90.2&88.4&82.4&75.9&66.9&62.5&57.2&54.2&51.9& \ \ \ \ \ \ \ \  \\
            \midrule
            \multirow{2}{*}{MS-Celeb-10K/10}
            & iCaRL \cite{rebuffi2017icarl} & 94.2&93.7&90.8&86.5&80.8&77.2&74.9&71.1&68.5&65.5 \\
            & Ours & 92.9&94.6&93.6&90.9&87.2&82.3&78.6&77.0&74.0&  \ \ \ \ \ \ \ \  \\
            \bottomrule[0.4mm]
        \end{tabular}
    \end{center}
\end{table*}



\begin{table}[htp]
\begin{minipage}{0.48\textwidth}
\vspace{0.5cm}


\captionof{table}{\emph{Comparison on MNIST and SVHN datasets.} Ours is a  memory based approach (denoted by `') that outperforms  state-of-the-art and performs quite close to Joint training (oracle case). }
            \begin{center}
            \scalebox{1.1}{
            \begin{tabular}{l c c}
            \toprule
           \textbf{Methods} & \textbf{MNIST}() & \textbf{SVHN}()  \\
             \midrule
             Joint training &  97.53\%  &  93.23\% \\ \midrule
              EWC \cite{kirkpatrick2017overcoming} & 19.80\%  &  18.21\% \\
            Online-EWC  \cite{schwarz2018progress}  & 19.77\%  & 18.50\%  \\
            SI   \cite{zenke2017continual}     & 19.67\%  &  17.33\% \\
            MAS  \cite{aljundi2018memory}     & 19.52\%  &  17.32\% \\
            LwF  \cite{li2018learning}     & 24.17\%  &  - \\
            \midrule
            GEM \cite{lopez2017gradient}       &   92.20\%  &  75.61\% \\
             DGR \cite{shin2017continual}       &   91.24\%  & -  \\
            RtF \cite{van2018generative}       &   92.56\%  & - \\
            \midrule
             {Ours}  &   \textbf{96.16}\%  & \textbf{90.83}\%  \\
            \bottomrule
            \end{tabular}}
            \end{center}
            \label{tbl:mnsit_svhn}
\end{minipage}  
\end{table}


Fig.~\ref{tbl:imagenet} compares different methods on ImageNet dataset. The results show that for experimental settings consistent with iCARL \cite{rebuffi2017icarl}, our proposed method achieves a significant absolute performance gain of  compared with the existing state-of-the-art~\cite{rebuffi2017icarl}. Our experimental results indicate that commonly used technique of fine-tuning a model on new classes is clearly an inferior approach, and results in catastrophic forgetting.
Table~\ref{tbl:mnsit_svhn} compares different methods on MNIST and SVHN datasets following experimental setting of \cite{Hsu18_EvalCL}. The results show that \ours{}, surpasses all previous methods with a margin of  and  respectively for MNIST and SVHN datasets. The results further indicate that the methods which do not use a memory perform relatively lower.












\subsection{Ablation Studies and Analysis}\label{sec:ablation}
In this section, we report ablation experiments and analyze the behaviour of our approach under different configurations of hyper-parameters in our approach. 

\textbf{What really matters?} Fig.~\ref{fig:indv_contrib} studies the impact of progressively integrating individual components of our \ours{}. We begin with a simple baseline model with a single path that achieves  classification accuracy on CIFAR100 dataset. When distillation loss is used alongside the baseline model, the performance increases to . The addition of our proposed controller   in the loss function further gives a significant boost of , resulting in an overall accuracy of . Finally, the proposed multi-path selection algorithm along with above mentioned components increases the classification accuracy up to . This demonstrates that our two contributions, controller and multi-path selection, provide a combined gain of  over baseline + distillation. Note that these experimental results are obtained without our proposed dynamic path switching rule and the attention function whose effect is extensively explore later in this section.


\textbf{Model Size Comparison:} Although our approach dynamically increases the network's capacity to allow learning new tasks, it is important to note that the parametric complexity remains bounded for a large number of tasks. Fig.~\ref{fig:params} compares total parameters across tasks for Progressive Nets \cite{rusu2016progressive}, iCARL \cite{rebuffi2017icarl} and our \ours{} on CIFAR100. Our model effectively reuses previous parameters, and the model size does not increase significantly with tasks. After 10 tasks, {RPS-Net} has 72.26M parameters on average, compared with iCARL  (21.3M) and Progressive Nets (932.84M). This shows that in RPS-Net, learnable parameters increase logarithmically, while for Progressive Nets they increase quadratically.

\textbf{Varying Number of Exemplars:} Ours is a memory based approach that keeps a small set of exemplars for memory replay at later stages to avoid catastrophic forgetting. Generally, a larger set of exemplars should help improve the performance. It is therefore important to study how the RPS-Net performs for different sizes of exemplar set. Fig.~\ref{fig:exemplars} compares RPS-Net with the best existing method (iCARL) for various memory budgets of exemplars on CIFAR100 dataset. Overall, an improvement is observed as the memory budget is increased for both approaches, but our proposed RPS-Net consistently performs better across all budgets. 

\textbf{Attention function:} In this section, we study several other attention mechanisms in comparison to our proposed peak response function (see~Fig.~\ref{fig:gamma_mapping}). To this end, we try path mean response function, in which we simply take the average of the activations after a convolution module in the last layer which results in a scalar value and can be directly used to re-weight the path responses. We also use an attention function with one convolution layer and a shared fully connected layer (). First a convolution layer  converts  tensor to  matrix, and  such feature maps are concatenated to form a  tensor and fed into a shared fully connected layer to output a  dimensional vector and its corresponding dimensions are used to re-weight the  output tensors. In the cases where a path is not activated, a zero tensor is used. Similarly, for , maximum activation values of the tensor along the channels are used to get  features per path (in this case, last layer ), and a shared fully connected layer is used to get  re-weight coefficients.


\begin{figure*}
    \centering
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[scale=0.28]{figs/ab_1.pdf}
        \caption{Individual Contributions}\label{fig:indv_contrib}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[scale=0.28]{figs/ab_2.pdf}
        \caption{Parameters}\label{fig:params}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \includegraphics[scale=0.28]{figs/ab_3.pdf}
        \caption{Exemplars}\label{fig:exemplars}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \includegraphics[scale=0.28]{figs/gamma_mapping.pdf}
        \caption{Attention functions}\label{fig:gamma_mapping} 
    \end{subfigure}
    \caption{\emph{Detailed experimental analysis of \ours{}.} (a) Ablation experiments to study the contribution from individual components of our approach, (b) The effect of number of tasks on the overall parametric complexity of \ours{} versus iCARL \cite{rebuffi2017icarl} and Progressive Networks \cite{rusu2016progressive}. (c) The effect of changing the number of exemplars on ours and iCARL \cite{rebuffi2017icarl}. (d) The comparison between different attention functions used to re-weight the path responses in \ours{}. The classification accuracy on CIFAR100 with 10 tasks is reported. }
    \label{fig:analysis}
\end{figure*}


\begin{figure*}[!htb]
    \centering
    \begin{subfigure}{.245\textwidth}
        \centering
            \includegraphics[width=0.99\textwidth]{figs/gamma.pdf}
        \caption{Parameter }\label{fig:gamma}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figs/J.pdf}
        \caption{Parameter }\label{fig:j_var}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figs/M.pdf}
        \caption{Parameter }\label{fig:m_var}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=0.86\textwidth, angle=-90]{figs/flops2.pdf}
        \caption{ with FLOPS}\label{fig:flops}
    \end{subfigure}
   
    \caption{\emph{Parameter Sensitivity Analysis.}  Performance trend with different settings of parameters ,  \&  are reported (from \textit{left to right}). The effect of changing parameters  and  on the computational complexity (in terms of FLOPS) of \ours{} is shown in the right-most plot. }
    \label{fig:ablation}
\end{figure*}

\textbf{Scaling Factor :} It controls the equilibrium between cross-entropy and distillation losses (or the balance between new and old tasks). In Fig.~\ref{fig:gamma}, for smaller , the network tends to forget old information while learning the new tasks well and vice versa. For example, when  (same as loss function used in iCaRL~\cite{rebuffi2017icarl}) the performance drops, showing the model is not at its equilibrium state. On the other hand,  also shows a drop in performance towards the later tasks ( at task ). Empirically, we found the optimal value for , to keep the equilibrium till last tasks. In this ablation experiment, we keep the network configuration same for all cases, thus we manually change the paths every two task. This is to remove the effect of dynamic path switching which change the effect of . For example for a small value of , network may not remember all the past information, thus a single path can be used many times, while for a higher value of , paths will saturate faster. 


\textbf{Varying Blocks and Paths:} 
 One of the important restriction in \ours{} design is the networks' capacity, upper-bounded by  modules. As proposed in the learning strategy, a module is trained only once for a path. Hence, it is interesting to study whether the network saturates for a high  number of tasks. To analyze this effect, we change the parameters  and . Here, M is the number of modules in a layer, and  is the number of tasks a path is trained without switching. Here we do not use our dynamic path selection strategy since we cannot control when switching happens in that case.
 Our results with varying  are reported in Fig.~\ref{fig:m_var}, which demonstrate that the network can perform well even when all paths are saturated. This effect is a consequence of our residual design where the last classification layer and skip connections are always trained, thus helping to continually learn new tasks even if the network is saturated. If saturation occurs, the model has already seen many tasks, therefore it learns generalizable features that can work well for future tasks with the help of residual signal (carrying complementary information) via skip connections and adaptation of the final classification layer weights.
 
 \begin{figure*}[t]
\center
\includegraphics[width=0.37\textwidth]{figs/saturation.pdf}\qquad
\includegraphics[width=0.37\textwidth]{figs/saturation_acc.pdf}
\caption{\emph{Trend for saturation coefficient and model performance under different path switching rules.}  \emph{Left}, we can see a sawtooth type pattern, this is mainly because once a saturation coefficient passes the threshold value, the network will chose a new path (with possibly several untrained modules). Thus the saturation coefficient will drop with a new path. Dashed line show the case when paths are changed manually, while solid lines denote the case when paths are changed automatically with a given threshold on saturation coefficient. \emph{Right}, the plot shows that higher number of paths  results in better performance. However, too many paths will introduce more computational overhead and memory requirements. Using an automatic path switching rule (based on a threshold for saturation coefficient), our method finds the best place to switch to a new task, and therefore balance the trade-off between performance and computational complexity. }\label{fig:saturation} 
\end{figure*}
 
In Fig.~\ref{fig:j_var}, we illustrate results with varying paths (paths ) in the network. We note that learning a high number of paths degrades performance as the previously learned parameters are less likely to be effectively reused. On the other hand, we obtain comparable performance with fewer paths (e.g., 2 for CIFAR-100).

\textbf{FLOPS comparison:} As the number of tasks increase, the network's complexity grows. As shown in Fig.~\ref{fig:flops}, with different configurations of modules and paths, the computational complexity of our approach scales logarithmically. This proves that the complexity of \ours{} is bounded by  . This is due to the fact that the overlapping modules increase as the training progresses. Further, in our setting we chose new paths after every  tasks. Hence, in practice our computational complexity is well below the worst-case logarithmic curve. For example with a setting of  the computational requirements reduces by  while achieving the best performance. We also show that even when a single path is used for all the tasks (), our model achieves almost the same performance as state-of-the-art with constant computational complexity. 




\textbf{Saturation coefficient:}
One of the important hyper-parameter of \ours{} is , which controls the trade-off between the network resource allocation and performance. Compared to the case when a fixed rule is used to switch paths (e.g., after every 2 tasks resulting in 5 paths), an adaptive rule based on network saturation with  helps us achieve similar performance with only 3 paths (see~Fig.~\ref{fig:saturation}). This illustrates that our proposed dynamic path switching scheme that intelligently expands model capacity can help in significant reduction of the computational requirements in \ours{}. Specifically, the best performance is achieved by changing paths for every two tasks, manually. Further, we can surpass iCaRL~\cite{rebuffi2017icarl} performance with only two different paths (). This shows that, not only we able to learn the places where we need to jump the paths, this helps in maximum utilization of available resources. 

\begin{figure}[t]
\begin{center}
         \includegraphics[width=0.4\textwidth]{figs/conv.pdf}
\end{center}
\caption{\emph{Forward Transfer}. \ours{} converges fast for the final tasks, showing forward transfer as the learning progresses.}
\label{fig:conv}
\end{figure}

\begin{figure*}[t]
\center
\includegraphics[trim={0cm 0cm 0.94cm 0cm},clip,scale=0.6, angle=-90]{figs/cf.pdf}
\caption{\label{fig:cfmatrix} \emph{Bakward Transfer.} Confusion matrices  
over 10 incremental tasks on CIFAR-100, showing backward knowledge transfer. For better visualization, a transformation of log(x+1) is applied to the confusion matrices. }
\end{figure*}

\textbf{Forward Transfer:}
The convergence trends shown in Fig.~\ref{fig:conv} demonstrate the forward knowledge transfer for \ours{}. We can see that for task-, the model takes relatively longer to converge compared with task-10. Precisely, for the final task, the model achieves  of the total performance within only one epoch, while for the second task it starts with  and takes up-to  epochs to achieve  of the final accuracy. This trends shows the faster convergence of our model for newer tasks
This effect is due to residual learning as well as overlapping module sharing in \ours{} design, demonstrating its forward transfer capability.





\textbf{Backward Transfer:}
Fig.~\ref{fig:cfmatrix} shows evolution of our model with new tasks. We can see that the performance of the current task (\textit{k}) is lower than the previous tasks (\textit{<k}). Yet, as the model evolves, the performance of task  gradually increases. This demonstrates models' capability of backward knowledge transfer, which is also reflected in biological aspects of human brain. Specifically, hippocampus in human brain accomplishes fast learning which is later slowly consolidated with the slow learning at neocortex~\cite{DBLP:journals/corr/abs-1802-07569}. In Fig.~\ref{fig:cfmatrix}, we can see the pattern of slow learning, with the performance on new tasks gradually maturing.




\textbf{Comparison with a Genetic Algorithm:} We compare our random selection approach with a genetic algorithm i.e., Binary Tournament Selection (BTS) \cite{miller1995genetic} for 25 maximum generations, on MNIST with 5 tasks (each of 2 classes), using a simple 2 layer (100 neurons) MLP with . On 5 runs, our proposed random selection achieves an average accuracy of  vs BTS gets . For same time complexity as ours, BTS has an average accuracy of  for the \textit{first} generation models. For BTS to gain similar performance as our random selection, it needs an average of  generations (much higher than the number of random paths), hence BTS has more compute complexity. Although BTS is a simple genetic algorithm and more sophisticated genetic algorithms may outperform random selection, but likely with a high compute cost, which is not suitable for an incremental classifier learning setting having multiple tasks.


\begin{table}[h]
\begin{minipage}{1\columnwidth}
\vspace{0.5cm}
            \captionof{table}{{\emph{Comparison between RPS-Net and \ours{}.} Ours is a modified version of RPS-Net with adaptive path selection strategy.}}
            \begin{center}
            \begin{tabular}{l c c c}
            \toprule
           \textbf{Dataset} & \textbf{Tasks} & \textbf{RPS-Net} & \textbf{Adaptive RPS-Net}  \\
             \midrule
            CIFAR100 & 10   & 56.48\%  &  60.83\% \\
            CIFAR100 & 20   & 50.83\%  &  53.54\%  \\
            SVHN     & 5    & 88.91\%  &  90.83\%  \\
            MS-Celeb & 10   & 65.00\%  &  69.20\%  \\
            \bottomrule
            \end{tabular}
            \end{center}
            \label{tbl:old_vs_new}
\end{minipage}  
\end{table}

\textbf{RPS-Net vs. \ours{}:} In terms of computational efficiency, our proposed adaptive path selection algorithm uses less number of FLOPS to gain the same performance as RPS-Net~\cite{rajasegaran2019random}. For example, without the adaptive path selection, RPS-Net achieves 59.93\% on CIFAR-100 with 10 tasks utilizing 5 number of paths. However, with \ours{}, we achieve 59.15\% with  and only  paths. This shows a 40\% improvement in computationally efficiency at inference time. Further, performance-wise \ours{} surpasses \cite{rajasegaran2019random} by a considerable margin on all datasets. As an example, we consider CIFAR-100 with 10 and 20 incremental tasks, the modified RPS-Net with adaptive path selection and path attention surpasses RPS-Net~\cite{rajasegaran2019random} by 4.35\% and 2.71\% respectively. In Table~\ref{tbl:old_vs_new}, we provide more comparisons on the performance gain by \ours{}.



\section{Conclusion}
In real-life setting, learning tasks appear in a sequential order and an autonomous agent must continually increment its existing knowledge. Deep neural networks excel in the cumulative learning setting where all tasks are available all together, but their performance deteriorates for the incremental learning case. In this paper, we propose a scalable approach to class-incremental learning that aims to keep the right balance between previously acquired knowledge and the newly presented tasks. We achieve this using an optimal path selection approach that supports parallelism and knowledge exchange between old and new tasks. Our approach can automatically estimate path capacity with in the network and subsequently decide if a new path is required to continue learning new tasks. Further, a controlling mechanism is introduced to maintain an equilibrium between the stability and plasticity of the learned model. Our approach delivers strong performance gains on MNIST, SVHN, CIFAR-100, MS-Celeb and ImageNet datasets for incremental learning problem.