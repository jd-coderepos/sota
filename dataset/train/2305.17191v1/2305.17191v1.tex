\documentclass{INTERSPEECH2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{tabularx,colortbl}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{textcomp}
\addtolength{\parskip}{-0.5mm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
    




\interspeechcameraready 




\title{MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations}

\name{Calum Heggan, Tim Hospedales , Sam Budgett, Mehrdad Yaghoobi}

\address{
   School Of Engineering, University of Edinburgh, Scotland,\\
   School Of Informatics, University of Edinburgh, Scotland,\\
   Thales UK RTI}
\email{s1529508@ed.ac.uk, t.hospedales@ed.ac.uk, Samuel.BUDGETT@uk.thalesgroup.com, m.yaghoobi-vaighan@ed.ac.uk}

\begin{document}

\maketitle
\begin{abstract}
Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide  augmentation invariance,  which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them.
\end{abstract}

\noindent\textbf{Index Terms}: few-shot, multi-task, augmentation-invariance, speech classification

\section{Introduction}
Few-shot learning, which aims to learn with limited data, has become increasingly popular in response to the lack of large labelled datasets for many practical applications. Models trained using self-supervision (where a deep neural network (DNN) is trained with pseudo-labels that define pre-text tasks) have demonstrated strong success on few-shot learning tasks, with contrastive objectives among the most successful. Contrastive methods' efficacy is attributed to learning an inductive bias in the form of invariances to applied augmentations \cite{simclr,linus_invariance}. For example, affine transformation invariance is typically useful for object category recognition, where pose is a nuisance factor \cite{simclr}. However, the ideal type and degree of invariance is not known apriori, and varies across downstream tasks. So, contrastively trained invariant features do not provide a \emph{one size fits all} solution \cite{linus_invariance,aug_self,hyper_simclr}. For example, a model learned to be pitch-shift invariant \cite{clar} would likely fail a task which relies on pitch sensitivity features. To learn a model which can successfully solve various downstream tasks, we require a feature representation with both invariant and transformation-sensitive properties.

\noindent We propose a parameter-efficient multi-task learning framework to address this limitation of existing contrastive learners. We simultaneously learn a contrastive objective (to learn augmentation invariances) and a transformation prediction objective (to learn augmentation sensitivity), thus providing a more flexible feature for downstream tasks. 
Our contributions include: 1) A novel multi-task learning  framework; 2) A parameter-efficient solution to multi-task learning based on task-agnostic and task-specific features; 3) Evaluation of few-shot classification over 10 datasets, spanning audio and speech domains; 4) Analysis of learnt invariance strength and its relation to performance.  \href{https://github.com/CHeggan/MT-SLVR}{Code can be found here}.

\section{Self-Supervision for Few-Shot Classification}
A common goal of using self-supervision is to learn a powerful data representation without the need for large corpuses of labelled training data. This representation can then be used for other downstream tasks, where it can either be fine-tuned, using some labelled data from the target domain, or left as a static feature extractor. This type of approach is a particularly strong candidate for use in few-shot learning, where training a model from scratch for the task is difficult due to limited amount of labelled examples. 

\noindent This use case of self-supervision is utilised in our work. In particular, we use pre-trained self-supervised models (used as static feature extractors) and a linear classifier in order to solve few-shot classification tasks. 

\noindent Such few-shot problems can be formalised in terms of containing a support set  with a few training samples per class and a query set  with test samples. These tasks are typically expressed as N-Way K-Shot tasks, with N being the number of classes and K being the number of examples per class. More formally, task components look like:


where each example  consists of an input  and a class label , with  and  being the total number of support and query examples respectively.




\section{Related work}
\textbf{Self-Supervised Learning:} Since self-supervised learning is a large topic \cite{linus_survey,ssl_audio_survey,ssl_speech_survey}, we focus on relevant trends for brevity. One key trend is the success of methods which utilise augmentations for learning, including many contrastive methods \cite{simclr,simsiam,moco,barlow_twins} as well as predictive ones \cite{rot_net}. These approaches learn invariances or sensitivity to applied augmentations, respectively. Audio-specialised contrastive variants include COLA \cite{cola}, CLAR \cite{clar}, and the work by Fonseca et al. \cite{ucl_ser}. In this work, we focus on SimCLR \cite{simclr}, SimSiam \cite{simsiam} and a custom transformation prediction framework. In the SimCLR/SimSiam methods, augmentation pipelines generate multiple 'views' of each data point and the DNN is trained to map them to a similar area in the feature space, allowing the model to learn an augmentation-invariant representation. SimCLR \cite{simclr} and SimSiam \cite{simsiam} are distinct in a few ways; SimCLR uses implicit negative sampling and a temperate scaled cross-entropy loss, while SimSiam only uses positive-pair contributions and optimizes for cosine similarity. Utilising the same augmentation pipelines as described above, transformation prediction algorithms instead try to predict how or if specific augmentations have been applied to input samples \cite{rot_net}. Unlike contrastive learning, algorithms with this objective learn sensitivity to augmentations. Since multiple augmentations can be applied to each input, we implement a multi-label TP model, where each augmentation is predicted independently.

\noindent\textbf{Few-Shot Classification for Audio \& Speech:} Currently, only a handful of works exist investigating the few-shot learning regime for acoustic data \cite{transient, fs_fsd, metaaudio}. Within these, few-shot speech classification, especially over different types of speech (language, accent, emotion etc) is heavily underrepresented. We make extensive use of the MetaAudio \cite{metaaudio} benchmark due to its publicly available codebase. Additionally, we propose an extension to MetaAudio, including 3 new speech datasets suitable for few-shot classification \cite{cremad, saa, common_voice}.

\noindent \textbf{Multi-Task Learning \& Invariances:}
Most highly related to this work are others which deal with multi-task learning and/or the study of invariances/equivariances learnt by self-supervision. In particular, our work relates to: \cite{linus_invariance}, which showed that different computer vision tasks benefit from different (in)variances; HyperSimCLR \cite{hyper_simclr}, which demonstrated that a hypernetwork can adapt a representation to the (in)variances needed for downstream tasks; and AugSelf \cite{aug_self} that also investigates co-learning contrastive and predictive self-supervision in computer vision. Our work differentiates itself in a few key ways, including: a parameter-efficient solution to multi-task learning via the use of adapters \cite{residual_adapters}; the application to acoustic data; the extent and complexity of applied augmentations; and the diversity of downstream tasks considered. Other related works include those which investigate multi-task learning in the audio domain, such as PASE \cite{pase}.

\section{MT-SLVR}
Motivated by the intuition that solely learning invariances to augmentations may be suboptimal for specific downstream tasks, we propose to co-learn opposing objectives. Specifically, we learn a feature space using both contrastive and predictive self-supervision. We name our approach \textbf{MT-SLVR} (\textbf{M}ulti-\textbf{T}ask \textbf{S}elf-Supervised \textbf{L}earning for Transformation In/(\textbf{V}ariant) \textbf{R}epresentations). 
We conjecture that \emph{different downstream tasks benefit from different type and strength of invariance, and that providing both augmentation sensitive and invariant features will lead to superior performance}.

\noindent\textbf{Objective:} We introduce the notation  to denote applied augmentation pipelines, where  is a composition of individual augmentations () and their parametrisations (), and  is the set of augmentations used during training (e.g. ). For our contrastive component (), we calculate loss in the same manner as the original works \cite{simclr,simsiam}. For the predictive component, we propose a Multi-Label Augmentation Prediction (MLAP) framework, where augmentations are independently predicted for input samples. Formally, given a base feature extractor , a multi-layer MLP for transformation prediction , the Binary Cross-Entropy loss (BCE), and augmented samples  and , our predictive loss is defined as:

where 

and  is the indicator function which takes a value of 0 if  has not been applied to , and 1 if it has. For a given , sampled augmentation pipelines  and  consist of the same type and ordering of augmentations, however do not share augmentation specific parameters. This is done to keep alignment with original SimCLR \cite{simclr} and SimSiam \cite{simsiam} works, which also make this restriction. The total objective for the multi-task problem can be expressed as:

Where  is a hyperparameter which balances the individual losses. Optimising for this total objective encourages the shared extractor  to learn both augmentation-invariant and augmentation-sensitive features.

\noindent\textbf{Architecture:} We propose jointly optimising the objectives by utilising both task-specific and task-agnostic features within the neural network. More formally, we introduce the notation ,  and  to represent shared, contrastive specific and predictive specific parameters respectively. Objectives for our multi-task approach are then:


where the task-specific parameters are defined by architectural changes made to assist multi-task. In particular, we employ two of these changes: 1) Splitting the final output layer of the network such that each task corresponds to the outputs of half of the final layer neurons; and 2) The fitting of residual or batch-normalisation adapters throughout the model, as in \cite{residual_adapters}. We use adapters in the same way as proposed in the original work, where lightweight modules are added around residual blocks. These modules take the form:

where  can either be a batch normalisation or 1x1 convolutional layer. Although lightweight, the included adapters do influence parametrisation. As a multiplier compared to the base model, models fit with adapters have the following parametrisation: Batch Normalisation (BN) , Series Adapters (Series) , Parallel Adapters (Parallel) .


    \setlength{\tabcolsep}{10pt} \renewcommand{\arraystretch}{1} \begin{table}[h]
    
        \caption{Details of augmentations used, along with their respective parameters. We introduce shorthand for later use.}
        
        \centering
        \resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
			 \toprule    
             Augmentaton & Shorthand & Parameter & Value(s)  \\
            \toprule
            Pitch Shift & PS & Min / Max Transpose Semitones & -15 / 15 \\
            
            \midrule
            
            & & Shape & Lin, Log, Exp\\
            \multirow{-2}{*}{Fade} & \multirow{-2}{*}{FD}& Max In / Out Ratio & 0.5 / 0.5 \\
            
            \midrule
            
            & & Min / Max SNR in dB & 3 / 30 \\
            \multirow{-2}{*}{White Noise} & \multirow{-2}{*}{WN} & Min Max f-Decay & -1 / 0  \\
            
            \midrule
            
            & & Min / Max SNR in dB & 3 / 30 \\
            \multirow{-2}{*}{Mixed Noise} & \multirow{-2}{*}{MN} & Min Max f-Decay & -2 / 2 \\

            \midrule

            Time Masking & TM & Max Mask Ratio & 0.125 \\

            \midrule

            Time Shift & TS & Min / Max Shift Ratio & 0.5 \\

            \midrule

            Time Stretch & TS & Min / Max Stretch Factor & 0.5 / 1.5 \\

             \bottomrule

             

		    \end{tabular}}
        \label{table:augs}
    \end{table}



 
\noindent\textbf{Augmentations:} We use the augmentations and corresponding parameters from CLAR \cite{clar}, see Table \ref{table:augs}. These encompass seven temporal or frequency-based augmentations. For sampling, we place no restrictions on the number of augmentations per sample, nor in which order they appear, except that at least one augmentation must be present. Each augmentation (except for the first) is activated with its own Bernoulli probability, allowing cases in which all augmentations are present.  


\setlength{\tabcolsep}{20pt} \renewcommand{\arraystretch}{0.8} \begin{table*}[t]
  \caption{High level details of all datasets considered. Split into environmental sounds (TOP) and different types of speech (BOTTOM). Included datasets originating from MetaAudio are marked with *.}
  \centering
   \resizebox{2\columnwidth}{!}{\begin{tabular}{cccccc}
    \toprule
    Name & Setting &  Classes &  Samples & Format & Sample Length \\
    \midrule
    Balanced AudioSet \cite{audioset} & Mixed & 527 & 20,550 & Fixed & 10s \\
    \midrule
    ESC-50  \cite{esc} *       & Environmental         & 50   & 2,000   & Fixed    & 5s\\
    NSynth \cite{nsynth} *     & Instrumentation       & 1,006 & 305,978 & Fixed    & 4s \\
    FDSKaggle18 \cite{kaggle_18} *  & Mixed                 & 41   & 11,073  & Variable & 0.3s - 30s \\
    Watkins Marine Mammal Sounds \cite{watkins} *      & Marine Mammals         & 32   & 1,698   & Variable    & 0.1 - 150s\\
    BirdCLEF 2020 (Pruned) \cite{birdclef_2020} * & Bird Song    & 715  & 63,364  & Variable & 3s - 180s \\
    \midrule

    VoxCeleb1  \cite{vox_celeb_1} *  & Speaker              & 1,251 & 153,516 & Variable & 3s - 180s\\
    SpeechCommandsV2 \cite{speech_commands} *   & Keyword       & 35 & 105,829 & Fixed    & 1s\\
    Crema-D \cite{cremad} & Emotion & 6 & 7,442 & Variable & 1s - 5s\\
    Speech Accent Archive \cite{saa} & Accent & 122 & 2,060 & Variable & 17s - 110s \\
    Common Voice v12 Delta \cite{common_voice} & Language & 88 & 256,243 & Variable & 5s - 30s\\
    
    \bottomrule
  \end{tabular}}
  \label{table:datasets}
\end{table*}
 
\section{Setup}
\label{section:setup}
\noindent\textbf{Pre-Training}: Our pre-training pipeline consists of two distinct parts, self-supervised learning on the balanced training subset of the popular AudioSet \cite{audioset} (containing  of audio), and hyperparameter optimisation based on average performance over the validation splits of the MetaAudio benchmark \cite{metaaudio}. More specifically, we selected learning rates for each approach by comparing the average rank of trained models on tasks drawn from MetaAudio. Learning rates tested were between 1x10 and 1x10. Rates selected were 1x10 for baselines and 0.5x10 for multi-task approaches.  All included models were trained for 1,000 epochs on the ResNet-18 backbone (with a final dense output of 1,000), using the Adam \cite{adam_opt} optimiser. We generate sample-wise augmentations, where 1 to 7 augmentations (see Table \ref{table:augs}) are selected and applied in a random order. Models were trained on a mix of RTX GPUs and on average took 30 hrs to complete. 
\newline\noindent\textbf{Data Processing:} Like other works \cite{clar,cola} we utilise a 2-d 3-channel spectrogram-based representation for input to the model. For pre-training, augmentations are applied before this conversion. For variable length sets at evaluation time, we utilise fixed length splitting and majority voting for classification, as described in \cite{metaaudio}. 
\newline\noindent\textbf{Few-Shot Classification:} We evaluate our models on few-shot classification tasks drawn from a variety of datasets. Within our selection, we consider both the general audio and speech domains. For general audio, we make use of the MetaAudio \cite{metaaudio} benchmark, while for speech we source additional datasets \cite{cremad, saa, common_voice}. For those included in MetaAudio, we use the test split presented by the original work, while for our own speech datasets, we utilise all classes for testing. We detail all of these datasets in Table~\ref{table:datasets}. Following the methodology from \cite{linear_eval}, we freeze our learnt ResNet-18 backbone after pre-training (hence no fine-tuning) and solve tasks using a per few-shot task linear classifier. More specifically, we use a log-loss instantiation of the SGDClassifier as provided in sklearn \cite{sklearn}. For models which have multiple heads, we concatenate features before input to the classifier. Performance on each downstream dataset is reported as the average 5-way 1-shot task performance,  the 95\% Confidence Interval (CI), taken over 10,000 tasks. 
\newline\noindent\textbf{Competitors:} We compare the following methods: \underline{Contrastive} learning only \cite{simclr,simsiam}; Multi-label transformation \underline{Predictive} learning only; \underline{MT-Simple} denoting our multi-task loss on a simple ResNet backbone; \underline{MT-Split} denoting a ResNet backbone split at the final layer with one loss applied to each branch; \underline{MT-\{BN, Series, Parallel\}} denoting a parameter-efficient multi-task split with shared ResNet blocks and task-specific BN, Series, or Parallel adapters. We note that we exclude Wav2Vec \cite{wav2vec2} and other Contrastive Predictive Coding (CPC) based methods from our comparison as they do not explicitly learn either augmentation invariances or variances, and hence fall out of scope of our research question.
\newline\noindent\textbf{Invariance Analysis:} We also analyse our model in terms of measuring the learned augmentation (in)variance of the multi-task learned representation.  We follow the work by Ericsson et al. \cite{linus_invariance} by utilising the Mahalanobis distance between our original training samples and their transformed counterparts. Like in \cite{linus_invariance}, given a feature extractor  with feature space covariance , a transformation  whose parameters belong to a set of all possible , and a dataset , we measure strength of invariance as:


where 

and  is the transformed input sample . A feature extractor with zero total Mahalanobis distance between the original input samples and their transformed counterparts is perfectly invariant, while values greater represent increasing sensitivity.


\section{Results}
\subsection{Few-Shot Learning Results}

    \setlength{\tabcolsep}{10pt} \renewcommand{\arraystretch}{1} \begin{table*}[t]
    
        \caption{5-Way 1-Shot Performance Comparison between \textbf{SimCLR} methods. We compare SimCLR on its own (Baseline), Multi-Task Learning with no, or simple tricks (MT-Simple / Split), and Multi-Task with adapters (MT-Bn / Series / Parallel).}
        
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{c|cccccccccc|c}
   
			 \toprule
             Model () & ESC-50 & NSynth & Kaggle18 &  Watkins & BirdClef & VoxCeleb & SCv2 & Crema-D & SAA & C-Voice & Avg Rank \\

\toprule 

            Cont Only & 63.40 & 66.44 & 37.64 & 52.91 & \textbf{30.93} & 31.18 & \textbf{25.68} & 29.10 & 26.16 & 33.33 & 3.9\\

            Pred Only & 37.76 & 62.52 & 21.72 & 28.88 & 21.04 & 21.68 & 20.08 & 21.68 & 23.08 & 23.00 & 7.0\\

            \midrule

            MT-Simple & 64.23 & 66.73 & 36.70 & 55.26 & 29.39 & 30.91 & 24.02 & 29.07 & 26.32 & 33.21 & 4.4\\


            MT-Split & 61.23 & 65.29 & 33.42 & 53.19 & 27.38 & 29.71 & 23.40 & 28.66 & 26.27 & 31.80 & 5.8 \\

            MT-Bn & 69.17 & \textbf{72.44} & \textbf{39.11} & 58.80 & 30.32 & 32.10 & 24.40 & \textbf{30.03} & 28.61 & 34.72 & 2.1 \\


            MT-Series & 69.00 & 71.25 & 37.28 & 58.92 & 28.82 & 33.26 & 24.66 & 29.57 & 28.74 & 34.23 & 2.9\\
 

            MT-Parallel & \textbf{69.53} & 71.81 & 38.36 & \textbf{59.49} & 29.49 & \textbf{33.58} & 23.65 & 29.61 & \textbf{28.92} & \textbf{35.22} & \underline{\textbf{1.9}}\\

            
             \bottomrule

             

		    \end{tabular}}
        \label{table:simclr_results}
        \vspace{-5pt}
    \end{table*}

 
    \setlength{\tabcolsep}{10pt}[!htb] \renewcommand{\arraystretch}{1} \begin{table*}[t]
    
        \caption{5-Way 1-Shot Performance Comparison between \textbf{SimSiam} methods. We compare SimSiam on its own (Baseline), Multi-Task Learning with no, or simple tricks (MT-Simple / Split), and Multi-Task with adapters (MT-Bn / Series / Parallel).}
        
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{c|cccccccccc|c}
   
			 \toprule
             Model () & ESC-50 & NSynth & Kaggle18 &  Watkins & BirdClef & VoxCeleb & SCv2 & Crema-D & SAA & C-Voice & Avg Rank \\

\toprule 

            Cont Only & 51.74 & 68.78 & 31.72 & 48.29 & 23.94 & 24.13 & \textbf{23.80} & 28.11 & 23.51 & 28.50 & 5.0\\

           Pred Only & 37.76 & 62.52 & 21.72 & 28.88 & 21.04 & 21.68 & 20.08 & 21.68 & 23.08 & 23.00 & 7.0\\

            \midrule

            MT-Simple & 51.87 & 69.68 & 29.45 & 53.13 & 24.14 & 25.84 & 21.81 & 26.42 & 27.65 & 28.96 & 4.4\\

            MT-Split &52.07 & 68.26 & 28.68 & 52.04 & 24.47 & 25.58 & 22.08 & 26.69 & 26.70 & 28.58 & 4.8\\

            MT-Bn & 58.41 & 73.42 & 31.69 & 55.46 & 25.44 & 26.71 & 21.99 & 28.90 & 27.38 & 29.64 & 3.0\\

            MT-Series & 57.24 & 74.37 & 37.31 & 54.70 & 25.20 & 26.87 & 22.64 & 30.62 & 26.44 & 31.07 & 2.7 \\

            MT-Parallel & \textbf{60.61} & \textbf{76.36} & \textbf{37.59} & \textbf{57.98} & \textbf{25.45} & \textbf{28.66} & 23.08 & \textbf{30.72} & \textbf{27.94} & \textbf{32.72} & \underline{\textbf{1.1}}\\
            
             \bottomrule





		    \end{tabular}}
        \label{table:simsiam_results}
        \vspace{-5pt}
    \end{table*}

 Across experiments (see Tables \ref{table:simclr_results} and \ref{table:simsiam_results}), we observe strong improvements over both baselines (contrastive and predictive only), across all datasets. Ranked, the top 3 consist of the batch-normalisation, series and parallel adapters, followed by a mix of the others. Notably, both the naive multi-task approach (MT-Simple), where all features are shared between tasks, and the split branch counterpart (MT-Split) both yield worse (SimCLR) or only marginal improvements (SimSiam) on the baseline contrastive approaches. This shows that a richer multi-task architecture is necessary, and our parallel adapter approach provides this. We also observe some differences between contrastive methods used. Specifically, for SimCLR we observe a much higher spread of top ranking methods, while for SimSiam the parallel adapter method performs best in 9/10 cases, typically with much larger margins between it and the next best. We also observe that out of all 10 datasets, our absolute top performances in 8/10 are from SimCLR based methods.

    \setlength{\tabcolsep}{10pt} \renewcommand{\arraystretch}{1} \begin{table}[h]
        \centering
        \caption{Measured average Mahalanobis distance between original and augmented training (AudioSet) samples for \textbf{SimCLR} based models' (P)redictive or (C)ontrastive heads. Lower values indicate more invariance to the transformation. }


        \resizebox{\columnwidth}{!}{\begin{tabular}{c|cccccccc|c}
   
			 \toprule
             Model () & Head & PS & FD & WN &  MN & TM & TS &  & Avg\\

             \toprule 

            Cont Only & - & 32.01 & 31.23 & 31.67 & 31.66 & 30.96 & 31.61 & 31.35 & 31.5 \\

            \midrule
            
            Pred Only & - & 38.17 & 40.81 & 41.26 & 40.73 & 54.21 & 30.39 & 43.83 & 41.34\\

            \midrule

            MT-Simple & - & 29.09 & 32.62 & 31.86 & 31.01 & 29.05 & 22.45 & 36.77 & 30.41 \\

            \midrule
            
            & C & 37.22 & 38.66 & 39.16 & 37.86 & 37.50 & 30.77 & 43.01 & 37.74 \\
            \multirow{-2}{*}{MT-Split} & P & 37.31 & 38.73 & 39.24 & 37.95 & 37.78 & 30.77 & 43.15 & 37.85 \\

            \midrule
            
            & C & 27.62 & 31.93 & 28.48 & 27.51 & 29.98 & 21.38 & 35.68 & 28.94 \\
            \multirow{-2}{*}{MT-Bn} & P & 29.58 & 34.95 & 31.98 & 30.79 & 35.39 & 21.42 & 40.67 & 32.11 \\

            \midrule
            
            & C &22.71 & 26.16 & 23.38 & 23.15 & 21.00 & 21.33 & 30.24 & 24.00 \\
            \multirow{-2}{*}{MT-Series} & P & 36.59 & 37.71 & 39.09 & 38.78 & 44.44 & 21.28 & 42.12 & 37.14 \\
            
            \midrule
            
            & C & 31.41 & 32.37 & 31.69 & 31.51 & 30.07 & 30.34 & 33.88 & 31.61 \\
            \multirow{-2}{*}{MT-Parallel} & P & 35.67 & 42.53 & 39.99 & 40.01 & 42.74 & 30.24 & 41.21 & 38.91 \\

            
             \bottomrule

             

		    \end{tabular}}
        \label{table:simclr_inv}
        \vspace{-6pt}
    \end{table}     \setlength{\tabcolsep}{10pt} \renewcommand{\arraystretch}{1.1} \begin{table}[h]
    
        \caption{Average linear classifier feature weight for the (P)redictive and (C)ontrastive heads in multi-task \textbf{SimCLR}.}

        \centering
        \resizebox{\columnwidth}{!}{\begin{tabular}{c|cccccccc|c}
   
			 \toprule
             Model () & Head & ESC-50 & NSynth & BirdClef &  Crema-D & SAA & C-Voice\\

             \toprule 

            & C & 0.43& 0.41 &0.40 &0.46& 0.41 &0.36 \\
            \multirow{-2}{*}{MT-Split} & P & 0.57& 0.59 &0.60 &0.54& 0.59 &0.64 \\

            \midrule
            
            & C & 0.41& 0.39 &0.41 &0.43& 0.40 &0.37 \\
            \multirow{-2}{*}{MT-Bn} & P & 0.59& 0.61 &0.59 &0.57& 0.60 &0.63 \\

            \midrule
            
            & C &0.39& 0.33 &0.36 &0.39& 0.37 &0.30 \\
            \multirow{-2}{*}{MT-Series} & P & 0.61& 0.67 &0.64 &0.61& 0.63 &0.70 \\
            
            \midrule
            
            & C & 0.41& 0.37 &0.36 &0.38& 0.38 &0.31 \\
            \multirow{-2}{*}{MT-Parallel} & P & 0.59& 0.63 &0.64 &0.62& 0.62 &0.69 \\

            
             \bottomrule

             

		    \end{tabular}}
        \label{table:simclr_weights}
        \vspace{-13pt}
    \end{table} 

\subsection{Invariance Analysis}
To illustrate what (in)variances our framework has learned, we measure the distance between  original and augmented samples (Sec \ref{section:setup}) for our training set. The results in Tab.~\ref{table:simclr_inv} show a few key trends. In particular, we note that: 1) Different heads of our multi-task approaches do indeed learn significantly different degrees of invariance to applied augmentations; and 2) On average, even the simple multi-task approaches decrease invariance strength compared to the contrastive baseline. Interestingly, we observe that the naive multi-task baselines (MT-Simple, MT-Split) do not successfully learn distinct invariances in either case, which may explain their weaker performance relative to other proposed approaches. We do not see a clear trend where a larger difference in augmentation strength between heads is predictive of final performance ranking. For example, the series adapter has the largest invariance strength difference, however does not rank first for either contrastive framework. Thus, although diverse (in)variance strength is important in providing a flexible representation, there is a more complex relationship that still needs to be understood.  Finally, we expand our analysis by considering the average weight norms learned for each of the multi-task heads by our linear classifier for a representative set of datasets in Tab.~\ref{table:simclr_weights}.Our results illustrate that across different downstream tasks, the relative importance of contrastive versus predictive heads varies. This illustrates why the presence of both is advantageous for the numerical results in Tab~\ref{table:simclr_results}, and shows how downstream tasks can easily tune the degree of importance attributed to each feature by learning the linear combination, removing the need for human intervention at either the pre-train or downstream task steps.

\section{Conclusion \& Future Work}
We considered the idea that different downstream tasks may prefer different degrees of (in)variance in a pre-trained representation. Leveraging this insight, we developed a novel multi-task learner that exploits both contrastive and predictive learning, providing both augmentation invariant and augmentation sensitive features. To this end, we developed a novel multi-task architecture that provides both features by sharing most parameters and exploiting compact task-specific adapters. Our analysis showed that this multi-task architecture indeed learns substantially different invariances with each head. Each downstream task learning a linear combination of these features, is free to select its own operating point on the (in)variance spectrum., reducing the need for specific pre-train to downstream task tuning. We evaluated our approach on a diverse suite of few-shot classification tasks from a total of 10 audio and speech datasets and two contrastive learners (SimSiam and SimCLR). The results showed that our multi-task features improve on pure contrastive learning and provides the best performance in nearly all cases. In particular, we highlight that SimCLR with parallel adapters performed best on average. This work showed that multi-task learning produces more general features. This will enable faster adaptation to diverse downstream applications where lots of labelled data is not available, such as for voice recognition, speaker identification and emotion detection.

\section{Acknoledgement}
This work is supported by the Engineering and Physical
Sciences Research Council of the UK (EPSRC) Grant number
EP/S000631/1 and the UK MOD University Defence Research
Collaboration (UDRC) in Signal Processing, EPSRC iCASE account EP/V519674/1 and Thales UK Ltd. 

\newpage
\bibliographystyle{IEEEtran}
\bibliography{mybib}
\end{document}
