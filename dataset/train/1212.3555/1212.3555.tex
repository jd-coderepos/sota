\documentclass[10pt,conference,compsocconf]{IEEEtran}


\usepackage{psfrag}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{color}
\usepackage[active]{srcltx}
\usepackage{amsthm, amssymb}
\usepackage{amsmath,wasysym}
\usepackage{color}
\usepackage{cite}
\allowdisplaybreaks[2]          

\usepackage{amssymb} 


\usepackage{amsmath}
\allowdisplaybreaks[2]          \newcommand{\token}[1]{\textsc{#1}}


\usepackage[figure,boxed,noend]{algorithm2e}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}


\usepackage{float}
\usepackage[noend]{distribalgo}
\newfloat{algo}{htbp}{algo}
\floatname{algo}{Algorithm}

\usepackage[active]{srcltx}
\usepackage{amssymb} \newcommand{\becomes}{\leftarrow}
\newcommand{\upto}{\to\'|}
\newcommand{\itc}[1]{\textit{#1}}


\newcommand{\fixme}{\textbf{FIXME}}
\newcommand{\protocol}{PoWerStore}
\newcommand{\mprotocol}{M-PoWerStore}
\newcommand{\complete}{\textsc{complete}}
\newcommand{\nonce}{N}
\newcommand{\hash}{\overline{N}}
\newcommand{\scheme}{storage technique}


\newtheorem{defn}{Definition}[section]
\newtheorem{la}[defn]{Lemma}
\newtheorem{cor}[defn]{Corollary}
\newtheorem{theo}[defn]{Theorem}

\newenvironment{prooff}{\vspace{1ex}\noindent{\bf Proof:}\hspace{0.5em}}
	{\hfill\qed\vspace{1em}}




\title{Proofs of Writing for Efficient and Robust Storage}
\vspace{1em}
\author{Dan Dobre, Ghassan Karame, Wenting Li, Matthias Majuntke, Neeraj Suri and Marko Vukoli\'{c} \\
\{dan.dobre, ghassan.karame, wenting.li\}@neclab.eu,  \{majuntke, suri\}@cs.tu-darmstadt.de, vukolic@eurecom.fr}
\IEEEoverridecommandlockouts
\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}







We present \protocol, the first efficient robust storage protocol that
achieves \emph{optimal latency} without using digital signatures.

\protocol's \emph{robustness} comprises  tolerating asynchrony, maximum
number of Byzantine storage servers, any number of Byzantine readers
and crash-faulty writers, and guaranteeing \emph{wait-freedom} and
\emph{linearizability} of read/write operations. Furthermore,
\protocol's \emph{efficiency} stems from combining \emph{lightweight
authentication}, \emph{erasure coding} and \emph{metadata write-backs}
where readers write-back only metadata to achieve linearizability.


At the heart of \protocol\ are \emph{Proofs of Writing (PoW)}: a novel
\scheme\ based on lightweight cryptography. PoW enable reads and
writes in the single-writer variant of \protocol\ to have latency of
2 rounds of communication between a client and storage servers in the
\emph{worst-case} (which we show optimal).

We further present and implement a multi-writer \protocol\ variant
featuring 3-round writes/reads where the third read round is invoked
only under active attacks, and show that it outperforms existing
robust storage protocols, including crash-tolerant ones.









\end{abstract}

\section{Introduction}


Byzantine fault-tolerant (BFT) distributed protocols have recently been attracting considerable research attention, due to their appealing promise of masking various system issues ranging from simple crashes, through software bugs and misconfigurations, all the way to intrusions and malware. However, there are many issues that render the use of existing BFT protocols questionable in practice; these include, e.g., weak guarantees under failures (e.g., \cite{SinghDMDR08,CWADM09,AmirCKL11}) or high cost in performance, deployment and maintenance when compared to \emph{crash-tolerant} protocols \cite{KR09}. This can help us derive the following requirements for the design of future BFT protocols:

\begin{itemize}
\item A BFT protocol should be \emph{robust}, i.e., it should tolerate actual Byzantine faults and  actual \emph{asynchrony} (modeling network outages) while maintaining correctness and providing sustainable progress even under \emph{worst-case} conditions that still meet the protocol assumptions. This requirement has often been neglected in BFT protocols that focus primarily on \emph{common}, failure-free operation modes (e.g., \cite{HendricksGR07,KotlaADCW09,GV10}).

\item A \emph{robust} protocol should be \emph{efficient} (e.g., \cite{GarciaRP11, VeroneseCBLV13}). We believe that the efficiency of a robust BFT protocol is best compared to the efficiency of its \emph{crash-tolerant} counterpart. Ideally, a robust protocol should not incur significant performance and resource cost penalty with respect to a crash-tolerant implementation, hence making the replacement of a crash-tolerant protocol a viable option.
\end{itemize}

We stand to the point that achieving these goals may require challenging existing approaches and revisiting the use of fundamental abstractions (such as cryptographic primitives).

In this paper, we focus on read/write \emph{storage} \cite{Lam86}, where a set of client (readers and writers) processes share data leveraging a set of storage
\emph{server} processes. Besides being fundamental, the read/write storage abstraction is also practically appealing given that it lies at the heart of the Key-Value Store (KVS) APIs --- a de-facto standard of modern cloud storage offerings.

In this context, we tackle the problem of developing a \emph{robust} and \emph{efficient} asynchronous distributed storage protocol. More specifically, storage \emph{robustness} implies \cite{ABND95}: \emph{(i)} \emph{wait-freedom} \cite{Her91}, i.e., read/write operations invoked by correct \emph{clients} always eventually return,
and \emph{(ii)} \emph{optimal resilience}, i.e., ensuring correctness despite the largest possible number  of Byzantine server  failures; in the Byzantine model, this mandates using  servers \cite{MAD02}.


Our main contribution is \protocol, the first efficient robust storage protocol that
achieves \emph{optimal latency}, measured in \emph{maximum} (worst-case) number of communication \emph{rounds} between a client and storage servers, without using digital signatures. Perhaps surprisingly, the efficiency of \protocol\ does not come from sacrificing consistency; namely, \protocol\ ensures \emph{linearizability} \cite{HW90} (or \emph{atomicity} \cite{Lam86}) of read/write operations. In fact, the efficiency of \protocol{} stems from combining \emph{lightweight
authentication}, \emph{erasure coding} and \emph{metadata write-backs}
where readers write-back only metadata, avoiding much expensive data write-backs.


At the heart of \protocol\ is a novel data storage technique we call \emph{Proofs of Writing} (PoW). PoW are inspired by commitment schemes~\cite{Halevi1996};  PoW incorporate a 2-round write procedure, where the second round of write effectively serves to ``prove'' that the first round has actually been completed before it is exposed to a reader.
More specifically, PoW rely on the construction of a secure token that is \emph{committed} in the first write round, but can
only be verified after it is \emph{revealed} in the second round. Here, token security means that the adversary cannot predict nor forge the token
before the start of the second round. We construct PoW using cryptographic hash functions and efficient message authentication codes (MACs); in addition, we also propose an instantiation of PoW using
Shamir's secret sharing scheme \cite{Sha79}.

As a result, PoW allow \protocol\ to achieve 2-round read/write operations in the single writer (SW) case. This, in case of reads, matches the theoretical minimum for \emph{any} robust atomic storage implementation that supports arbitrary number of readers, \emph{including} crash-only implementations \cite{DGLV10}. Notably, \protocol\ is the first robust BFT storage protocol to achieve the 2-round latency of reading without using digital signatures.
On the other hand, we prove the 2-round write inherent, by showing a 2-round write lower bound for any robust BFT storage that features metadata write-backs using less than  storage servers.



In addition, \protocol\ employs \emph{erasure coding} at the client side to offload the storage servers and to reduce the amount of data transferred over the network. Besides being the first robust BFT storage protocol to feature metadata write-backs, \protocol\ is also the first robust BFT storage protocol to tolerate an unbounded number of Byzantine readers \cite{LR06} and unbounded number of writers' crash-faults.

Finally, while our SW variant of \protocol\ demonstrates the utility of PoW, for practical applications we propose a multi-writer (MW) variant of \protocol\ (referred to as \mprotocol). \mprotocol\ features 3-round writes and reads, where the third read round is invoked only under active attacks. \mprotocol{} also resists attacks specific to multi-writer setting that exhaust
the timestamp domain \cite{BD04}. We evaluate \mprotocol\ and demonstrate its superiority even with respect to existing crash-tolerant robust atomic storage implementations. Our results show that in typical settings, the peak throughput achieved by \mprotocol{} improves over existing crash-tolerant \cite{ABND95} and Byzantine-tolerant \cite{Phalanx} atomic storage implementations, by 50\% and 100\%, respectively.




The remainder of the paper is organized as follows. In Section~\ref{sec:model}, we outline our system model. In Section~\ref{sec:pow}, we introduce \protocol{} and we analyze its correctness.
In Section~\ref{sec:mpow}, we present the multi-writer variant of \protocol, \mprotocol{}. In Section~\ref{sec:implementation}, we evaluate an implementation of \mprotocol{}. In Section~\ref{sec:relwork}, we overview related work and we conclude the paper in Section~\ref{sec:conclusion}.


\section{System Model}
\label{sec:model}

We consider a distributed system that consists of three \emph{disjoint}
sets of processes: a set \emph{servers} of size , where  is the failure threshold parameter, containing
processes ; a collection \emph{writers} 
and a collection \emph{readers} .
The collection \emph{clients} is the union of writers and readers.
We assume the \emph{data-centric} model \cite{CMD03,ChocklerGKV09,SMMK10} where every client may \emph{asynchronously} communicate with any server by message passing using point-to-point reliable
communication channels; however, servers cannot communicate
among each other, nor send messages to clients other
than in reply to clients' messages.

We further assume that each server  pre-shares one symmetric group key with all writers
in the set ; in the following, we denote this key by . In addition, we assume the existence of a cryptographic (i.e., one way and collision resistant) hash function , and a secure message authentication function , where  is a -bit symmetric key.


We model processes as probabilistic I/O automata \cite{WSS94} where a distributed algorithm is a collection of such processes. Processes that follow the algorithm are called \emph{correct}. We assume that any number of readers exhibit \emph{Byzantine} \cite{LSP82} (or \emph{arbitrary} \cite{JCT98}) faults. Moreover, up to  servers may be Byzantine. Byzantine processes can exhibit arbitrary behavior; however, we assume that Byzantine processes are computationally bounded and cannot break cryptographic hash functions or forge message authentication codes. Finally, any number of writers may fail by crashing.





We focus on a read/write storage abstraction \cite{Lam86} which exports two operations: \textsc{write}(), which stores value  and \textsc{read}(), which returns the stored value. While every client may invoke the \textsc{read} operation, we assume that \textsc{write}s are invoked only by writers. We say that an
operation (invocation)  is \emph{complete} if the client receives the \emph{response}, i.e., if the client returns from the invocation. We further assume that each correct client invokes at most one operation at a time (i.e., does not invoke the next operation
until it receives the response for the current operation).
We further assume that the initial value of a storage is a special value , which is not a
valid input value for a write operation.

In any execution of an algorithm, we say that a complete
operation  \emph{precedes} operation  (or  \emph{follows} )
if the response of  precedes the invocation of
 in that execution.


We focus on the strongest storage progress consistency and progress semantics, namely \emph{linearizability} \cite{HW90} (or \emph{atomicity} \cite{Lam86}) and \emph{wait-freedom} \cite{Her91}.
Wait-freedom states that if a correct client invokes an operation , then
 eventually completes. Linearizability gives an illusion that a complete operation  by a correct client is executed instantly at some point in time between its invocation and response,  whereas the operations invoked by faulty clients appear either as complete or not invoked at all.

Finally, we measure the time-complexity of an atomic storage
implementation in terms of number of \emph{communication round-trips}
(or simply \emph{rounds}) between a client and servers. Intuitively, a \emph{round} consists of a client sending the message to (a subset of) servers and receiving replies. A formal definition can be found in, e.g.,~\cite{GNS09,DGLV10}.


\section{\protocol}\label{sec:pow}
In this section, we provide a detailed description of the \protocol\ protocol and we analyze its correctness. In addition, we show that \protocol{} exhibits
optimal (worst-case) latency in both \textsc{READ} and \textsc{WRITE} operations.

\subsection{Proofs of Writing}

At the heart of \protocol\ is a novel technique we call Proofs of Writing (PoW). Intuitively, PoW enable \protocol{} to complete in a 2-round \textsc{WRITE} procedure, where the second round of \textsc{WRITE} effectively serves to ``prove'' that the data is written in a quorum of servers (at least ) before it is exposed to a client. As such, PoW obviate the need for writing-back data, enabling efficient metadata write-backs and support for Byzantine readers.

PoW are inspired by commitment schemes~\cite{Halevi1996}; PoW consist of the construction of a secure token that can only be verified after the second round is invoked. Here, token security means that the adversary cannot predict nor forge the token before the start of the second round.
More specifically, our PoW implementation relies on the use of one-way collision-resistant functions seeded with pseudo-random input. We construct PoW as follows.

In the first round, the writer first generates a pseudo-random nonce and stores the hash of the nonce together with the data in a quorum of servers. The writer then discloses the nonce to the servers in the second round; this nonce provides sufficient \emph{proof} that the first round has actually completed. In fact, during the first round of a \textsc{READ} operation, the client collects the received nonces into a set and sends (writes-back) this set to the servers in the second round. The server then verifies the PoW by checking whether the received nonce matches the hash of the stored nonce. Note that since the writer keeps the nonce secret until the start of the second round, it is computationally infeasible for the adversary to acquire the nonce unless the first round of \textsc{WRITE} has been completed.

PoW are not restricted to the use of cryptographic hash functions. In Section~\ref{subsec:shamir}, we propose another possible instantiation of PoW using Shamir's secret sharing scheme \cite{Sha79}.

\subsection{Overview of \protocol}

In \protocol, the \textsc{WRITE} operation completes in two rounds, called \textsc{store} and \textsc{\complete}. Likewise, the \textsc{READ} performs in two rounds, called \textsc{collect} and \textsc{filter}. For the sake of convenience, each round \textsc{store}, \textsc{\complete}, \textsc{collect}, \textsc{filter} is wrapped by a procedure . In each round , the client sends a message of type  to all servers. A round completes at the latest when the client receives messages of type  from  correct servers. The server maintains a variable  to store the timestamp of the last completed \textsc{WRITE}, and a variable , of the same structure, to maintain a set of timestamps written-back by clients. In addition, the server keeps a variable  storing the history of the data written by the writer in the \textsc{store} round, indexed by timestamp.

\subsection{\textsc{WRITE} Implementation}

The \textsc{WRITE} implementation is given in Algorithm~\ref{alg:writer}. To write a value , the writer\footnote{Recall that \protocol\ is a single-writer storage protocol.} increases its timestamp , computes a nonce  and its hash , and invokes \textsc{store} with ,  and . When the \textsc{store} procedure returns, the writer invokes \textsc{\complete} with  and . After \textsc{\complete} returns, the \textsc{WRITE} completes.

In \textsc{store}, the writer encodes  into  fragments  (, such that  can be recovered from any subset of  fragments. Furthermore, the writer computes a cross-checksum  consisting of the hashes of each fragment. For each server  , the writer sends a \textsc{store} message to . On reception of such a message, the server writes  into the history entry  and replies to the writer.
After the writer receives  replies from different servers, the \textsc{store} procedure returns, and the writer proceeds to \textsc{\complete}.

In \textsc{\complete}, the writer sends a \textsc{\complete} message to all servers. Upon reception of such a message, the server changes the value of  to  if  and replies to the writer. After the writer receives  replies from different servers, the \textsc{\complete} procedure returns.


\begin{algo}[t]
\small
\newcounter{alg:client1:lines}

\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE  : structure  initially 
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}

\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\vspace{-2 em}
\begin{distribalgo}[1]  \setcounter{ALC@line}{\value{alg:client1:lines}}
\INDENT {\textbf{operation} \textsc{WRITE}}
\STATE  \label{alg:writer:timestamp}
\STATE 
\STATE 
\STATE  \label{alg:writer:store}
\STATE  \label{alg:writer:complete}
\STATE return \textsc{ok}
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{store}()}
\STATE 
\STATE 
\STATE \textbf{for}  \textbf{do} send \textsc{store} to 
\STATE \textbf{wait for} \textsc{store\_ack} from  servers
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{\complete}()}
\STATE send \textsc{\complete} to all servers
\STATE \textbf{wait for} \textsc{\complete\_ack} from  servers
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}

\end{distribalgo}

\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}

\caption{{Algorithm of the writer in \protocol.}}\label{alg:writer}
\end{algo}


\begin{algo*}[t]
\small
\centering
\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE  : structure , initially \hfill //last completed write
\STATE  : set of structure , initially \hfill     //set of written-back candidates
\STATE  : vector of  indexed by , initially 
\ENDINDENT
\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.97\textwidth}\mbox{}\end{tabular}
\begin{minipage}[t]{0.5\textwidth}
\begin{distribalgo}[1]  \setcounter{ALC@line}{\value{alg:client1:lines}}
\vspace{-1 em}
\INDENT {\textbf{upon} receiving \textsc{store} from the writer}
\STATE  \label{alg:server:hist-store}
\STATE send \textsc{store\_ack} to the writer
\ENDINDENT

\medskip
\INDENT {\textbf{upon} receiving \textsc{\complete} from the writer}
\STATE  \textbf{if} {} \textbf{then}  \label{alg:server:update-complete}
\STATE send \textsc{\complete\_ack} to the writer
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{gc}}
\STATE 
\STATE \}) \label{alg:server:valid-check}
\STATE \textbf{if} {} \textbf{then}  \label{alg:server:update-gc}
\STATE   \label{alg:server:gc}
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\end{minipage}\hfill
\begin{minipage}[t]{0.5\textwidth}
\begin{distribalgo}[1]   \setcounter{ALC@line}{\value{alg:client1:lines}}
\vspace{-1 em}
\INDENT {\textbf{upon} receiving \textsc{collect} from client }
\STATE \textsc{gc}
\STATE send \textsc{collect\_ack} to client 
\ENDINDENT

\medskip

\INDENT {\textbf{upon} receiving \textsc{filter} from client }
\STATE  \hfill //write-back


\STATE 
\STATE  \label{alg:server:valid-check2}
\STATE 
\STATE send \textsc{filter\_ack} to client  \label{alg:server:ret}
\ENDINDENT

\medskip
\INDENT {\textbf{Predicates:}}
\STATE \textsf{valid}()  \label{alg:server:valid-pred}
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}

\end{minipage}

\medskip
\begin{tabular}{c}\hline\mbox{}\hspace{0.97\textwidth}\mbox{}\end{tabular}

\caption{{Algorithm of server  in \protocol.}}\label{alg:server}
\end{algo*}



\subsection{\textsc{READ} Implementation}

The \textsc{READ} implementation is given in Algorithm~\ref{alg:reader}; it consists of the \textsc{collect} procedure followed by the \textsc{filter} procedure. In \textsc{collect}, the client reads the tuples  included in the set  at the server, and accumulates these tuples in a set  together with the tuples read from other servers. We call such a tuple a \emph{candidate} and  a \emph{candidate set}. Before responding to the client, the server garbage-collects old tuples in  using the  procedure. After the client receives candidates from  different servers, \textsc{collect} returns.

In \textsc{filter}, the client submits  to each server. Upon reception of , the server performs a write-back of the candidates in  (\emph{metadata write-back}). In addition, the server picks  as the candidate with the highest timestamp in  such that \textsf{valid}() holds. The predicate \textsf{valid}() holds if the server, based on the history, is able to verify the integrity of  by checking that  equals . The server then responds to the client with a message including the timestamp , the fragment  and the cross-checksum . The client waits for the responses from servers until there is a candidate  with the highest timestamp in  such that \textsf{safe}() holds, or until  is empty, after which \textsc{collect} returns. The predicate \textsf{safe}() holds if at least  different servers  have responded with timestamp , fragment  and cross-checksum  such that . If , the client selects the candidate with the highest timestamp  and restores value  by decoding  from the  correct fragments received for . Otherwise, the client sets  to the initial value . Finally, the \textsc{READ} returns .


\begin{algo}[t]
\small

\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE : , initially 
\STATE : set of , initially 
\STATE : set of , initially 
\STATE : vector of , initially 
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\vspace{-2 em}
\begin{distribalgo}[1]   \setcounter{ALC@line}{\value{alg:client1:lines}}

\INDENT {\textbf{operation} \textsc{READ}}
\STATE 
\STATE 
\STATE 
\STATE 
\IF {}
\STATE ~\label{alg:reader:select}
\STATE  ~\label{alg:reader:restore}
\ENDIF
\STATE \textbf{else} 
\STATE return 
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{collect}()}
\STATE send \textsc{collect} to all servers
\STATE \textbf{wait until} 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{upon} receiving \textsc{collect\_ack} from server }
\STATE 
\STATE 
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{filter}()}
\STATE send \textsc{filter} to all servers
\STATE \textbf{wait until} \\ 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{upon} receiving \textsc{filter\_ack} from server }
\STATE ; 
\STATE 
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{restore}()}
\STATE  s.t. \\  \label{alg:reader:cc}
\STATE  \label{alg:reader:fr}
\STATE  \label{alg:reader:decode}
\STATE return 
\ENDINDENT

\medskip
\INDENT {\textbf{Predicates:}}
\STATE \textsf{highcand}()  (
\smallskip
\STATE \textsf{safe}()  \\ \label{alg:reader:safe}
\\

\smallskip
\STATE \textsf{invalid}()  
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\caption{{Algorithm of client  in \protocol.}}\label{alg:reader}
\end{algo}

\subsection{Correctness Sketch} \label{sec:pow-sketch}

In what follows, we show that \protocol{} satisfies linearizability and wait-freedom in all \textsc{filter}. Due to lack of space, we refer the readers to the Appendix for a detailed proof of \protocol's correctness.



We first explain why linearizability is satisfied by arguing that if a \textsc{READ} follows after a completed \textsc{WRITE}() (resp. a completed \textsc{READ} that returns ) then the \textsc{READ} does not return a value older than .

\paragraph{\textsc{READ}/\textsc{WRITE} Linearizability}
Suppose a \textsc{READ}  by a correct client follows after a completed \textsc{WRITE}(). If  is the timestamp of \textsc{WRITE}, we argue that  does not select a candidate with a timestamp lower than . Since a correct server never changes  to a candidate with a lower timestamp, after \textsc{WRITE} completed,  correct servers hold a valid candidate with timestamp  or greater in . Hence, during \textsc{collect} in , a valid candidate  with timestamp  or greater is included in . Since  is valid, at least  correct servers hold history entries matching  and none of them respond with a timestamp lower than . Consequently, at most  timestamps received by the client in \textsc{filter} are lower than  and thus  is never excluded from . By Algorithm~\ref{alg:reader},  does not select a candidate with timestamp lower than .

\paragraph{\textsc{READ} Linearizability}
Suppose a \textsc{READ}  by a correct client follows after . If  is the candidate selected in , we argue that  does not select a candidate with a timestamp lower than .
By the time  completes,  correct servers hold  in . According to Algorithm~\ref{alg:server}, if a correct server excludes  from , then the server changed  to a valid candidate with timestamp  or greater. As such,  correct servers hold in  a valid candidate with timestamp  or greater and during \textsc{collect} in , a valid candidate  with timestamp  or greater is included in . By applying the same arguments as above,  does not select a candidate with timestamp lower than .

We now show that \protocol{} satisfies wait-freedom; here, we argue that the \textsc{READ} does not block in \textsc{filter} while waiting for the candidate  with the highest timestamp in  to become \textsf{safe}() or  to become empty.

\paragraph{Wait-freedom}
Suppose by contradiction that  blocks during \textsc{filter} after receiving \textsc{filter\_ack} messages from all correct servers. We distinguish two cases: (Case 1)  contains a valid candidate and (Case 2)  contains no valid candidate.

\begin{itemize}
\item \textbf{Case 1:} Let  be the valid candidate with the highest timestamp in . As  is valid, at least  correct servers hold history entries matching . Since no valid candidate in  has a higher timestamp than , these  correct servers responded with timestamp , corresponding erasure coded fragment  and cross-checksum  such that . Therefore,  is \textsf{safe}. Furthermore, all correct servers (at least ) responded with timestamps at most . Hence, every candidate  such that  becomes \textsf{invalid}  and is excluded from . As such,  is also \textsf{highcand} and we conclude that  does not block.
\item \textbf{Case 2:} Since none of the candidates in  is valid, all correct servers (at least ) responded with timestamp , which is lower than any candidate timestamp. As such,  every candidate   becomes \textsf{invalid} is excluded from . We therefore conclude that  does not block.
\end{itemize}



\subsection{PoW based on Shamir's Secret Sharing Scheme}\label{subsec:shamir}

In what follows, we propose an alternative construction of PoW based on Shamir's secret sharing scheme~\cite{Sha79}.
Here, the writer constructs a polynomial  of degree  with coefficients  chosen at random from , where  is a public parameter. That is,
.

The writer then constructs the PoW as follows: for each server , the writer picks a random point , and constructs the share , where . As such, the writer constructs  different shares, one for each server, and sends a \textsc{store} message to each server  over a \emph{confidential channel}.
Upon reception of such a message, server  stores  in . Note that since there are at most  Byzantine servers, these servers cannot reconstruct the polynomial  from their shares, even if they collude. In the \textsc{\complete} round, the writer reveals the polynomial  in a \textsc{\complete} message. This enables a client to determine for a candidate  =  that the corresponding \textsc{store} round completed by checking that  servers  stored , without the servers revealing their share. For this purpose, the \textsf{valid} predicate at server  changes to .



By relying on randomly chosen , and the fact that correct servers never divulge their share, our construction prevents an adversary from fabricating a \emph{partially corrupted} polynomial after the disclosure of . To see why, note that with the knowledge of  and  held by a correct server , the adversary could fabricate a partially corrupted polynomial  that intersects with  only at  (i.e., ). This would lead to a situation in which a candidate  is neither \textsf{safe} nor \textsf{invalid} and thus, the \textsc{READ} would block.

We point that, unlike the solution based on hash functions, this construction provides information-theoretic guarantees for the PoW~\cite{LCAA07,AAB07} during the \textsc{store} round.


\subsection{Optimality of \protocol{}}
In this section, we prove that \protocol\ features optimal \textsc{WRITE} latency, by showing that writing in two rounds is necessary. We start by giving some informal definitions.

A distributed algorithm  is a collection of automata \cite{LT89}, where automaton  is assigned to process . Computation proceeds in steps of  and a \emph{run} is an infinite sequence of steps of . A \emph{partial run} is a finite prefix of some run. We say that a (partial) run  \emph{extends} some partial run  if  is a \emph{prefix} of .

We say that an implementation of linearizable storage is \emph{selfish}, if readers write-back only metadata instead of the full value. Intuitively the readers are selfish because they do not help the writer complete a write. For a formal definition, we refer the readers to~\cite{FL03}. Furthermore, we say that a \textsc{WRITE} operation is \emph{fast} if it completes in a single round. We now proceed to proving the main result. 



\begin{theo} \label{theo:lb}
There is no \emph{fast} \textsc{WRITE} implementation  of a multi-reader \emph{selfish} robust linearizable storage that makes use of less than  servers.
\end{theo}
\noindent \textbf{Preliminaries}.
We prove Theorem~\ref{theo:lb} by contradiction assuming at most  servers. We partition the set of servers into four distinct subsets (we call \emph{blocks}), denoted by , ,  each of size exactly , and  of size at least  and at most . Without loss of generality we assume that each block contains at least one server. We say that an operation  \emph{skips} a block , () when all messages by  to  are delayed indefinitely (due to asynchrony) and all other blocks  receive all messages by  and reply.

\begin{figure}[tbp]
 \begin{center}
 \subfigure[]{\label{Fig:r1}\includegraphics[scale=0.75]{atomic_r1.mps}}
 \subfigure[]{\label{Fig:r2}\includegraphics[scale=0.75]{atomic_r2.mps}}

 \subfigure[]{\label{Fig:r3}\includegraphics[scale=0.75]{atomic_r3.mps}}
 \subfigure[]{\label{Fig:r4}\includegraphics[scale=0.75]{atomic_r4.mps}}

  \subfigure[]{\label{Fig:r5}\includegraphics[scale=0.75]{atomic_r5.mps}}
  \subfigure[]{\label{Fig:r6}\includegraphics[scale=0.75]{atomic_r6.mps}}

\end{center}
 \vspace{-0.5cm}
 \caption{Sketch of the runs used in the proof of Theorem \ref{theo:lb}.}
\label{fig:proof_lb}
\end{figure}

\begin{prooff}
We construct a series of runs of a linearizable implementation  towards a partial run that violates linearizability.
\begin{itemize}
\item Let  be the partial run in which all servers are correct except  which crashed at the beginning of . Let  be the operation invoked by the writer  to write a value  in the storage. The \textsc{WRITE}  is the only operation invoked in  and  crashes after writing  to . Hence,  skips blocks ,  and .
\item Let  be the partial run in which all servers are correct except , which crashed at the beginning of . In ,  is correct and  completes by writing  to all blocks except , which it skips.
\item Let  be the partial run similar to , in which all servers except  are correct, but due to asynchrony, all messages from  to  are delayed. Like in ,  completes by writing  to all servers except , which it skips. To see why, note that  cannot distinguish  from . After  completes,  fails Byzantine by reverting its memory to the initial state.
\item Let  extend  by appending a complete \textsc{READ}  invoked by . By our assumption,  is wait-free. As such,  completes by skipping  (because  crashed) and returns (after a finite number of rounds) a value .
\item Let  extend  by appending . In , all servers except  are correct, but due to asynchrony all messages from  to  are delayed indefinitely. Moreover, since  reverted its memory to the initial state,  is held only by . Note that  cannot distinguish  from  in which  has crashed. As such,  completes by skipping  and returns . By linearizability, .
\item Let  be similar to  in which all servers except  are correct but, due to asynchrony, all messages from  to  are delayed. Note that  cannot distinguish  from . As such,  returns  in , and by , . After  completes,  fails by crashing.
\item Let  extend  by appending a \textsc{READ}  invoked by  that completes by returning . Note that in , \emph{(i)}  is the only server to which  was written, \emph{(ii)}  did not write-back  (to any other server) before returning , and \emph{(iii)}  crashed before  is invoked. As such,  does not find  in any server and hence , violating linearizability.
\end{itemize}
\vspace{-1.5 em}
\end{prooff}

Notice that Theorem~\ref{theo:lb} applies even to implementations relying on self-verifying data and/or not tolerating Byzantine readers. Furthermore, the proof extends to crash-tolerant storage when deleting the Byzantine block  in the above proof; the result being that there is no selfish implementation of a multi-reader crash-tolerant linearizable storage with less than  servers in which every \textsc{WRITE} is fast.


\section{\mprotocol}\label{sec:mpow}

The \protocol\ protocol, as presented in Section~\ref{sec:pow}, has a drawback in having potentially very large candidate sets that servers send to clients and that clients write-back to servers. As a result, a malicious adversary can exploit the fact that in \protocol\ candidate sets can (theoretically) grow without bounds and mount
a denial of service (DoS) attack by fabricating huge sets of bogus candidates. While this attack can be contained by a robust implementation of the point-to-point channel assumption using, e.g., a separate pair of network cards for each channel (in the vein of \cite{CWADM09}), this may impact practicality of \protocol. To rectify this issue, and for practical applications, we propose a multi-writer variant of our protocol called \mprotocol.

\subsection{Overview}

\mprotocol\ (Algorithms~\ref{alg2:writer},~\ref{alg2:server} and~\ref{alg2:reader}) supports an unbounded number of clients. In addition, \mprotocol\ features optimal \textsc{READ} latency of two rounds in the \emph{common case}, where no process is Byzantine. Outside the common case, under active attacks, \mprotocol\ gracefully degrades to guarantee reading in at most three rounds. The \textsc{WRITE} has a latency of three rounds, featuring non-skipping timestamps \cite{BD04}, which prevents attacks specific to multi-writer setting that exhaust the timestamp domain.

The main difference between \mprotocol\ and \protocol\ is that, here, servers keep a single written-back candidate instead of a set. To this end, it is crucial that servers are able to determine the validity of a written-back candidate without consulting the history. For this purpose, we enhance our original PoW scheme by extending the candidate with message authentication codes (MACs) to authenticate the timestamp and the nonce's hash, one for each server, using the corresponding group key. As such, a valid MAC proves to a server that the timestamp has been issued by a writer in \complete, and thus, constitutes a PoW that a server can obtain even without the corresponding history entry. Note that in case of a candidate incorporating corrupted MACs, servers might disagree about the validity of a written-back candidate. Hence, a correct client might not be able to write-back a candidate to  correct servers as needed. To solve this issue, \mprotocol\ "pre-writes" the MACs in the \textsc{store} round, enabling the client to repair the broken MACs of a selected candidate. We point out that the adversary cannot forge the MACs (and therefore bypass the PoW) before the start of the \complete, since the constructed MACs, besides the timestamp, also include the nonce's hash.

To support multiple-writers, \textsc{WRITE} in \mprotocol\ comprises an additional clock synchronization round, called \textsc{clock}, which is prepended to \textsc{store}. The \textsc{READ} performs an additional round called \textsc{repair}, which is appended to \textsc{collect}. The purpose of \textsc{repair} is to repair and write-back candidates if necessary, and is invoked only under active attack by a malicious adversary that actually corrupts candidates.

Similarly to \protocol, the server maintains the variable  to store the history of the data written by the writer in the \textsc{store} round, indexed by timestamp. In addition, the server keeps the variable  to store the timestamp of the last completed write.

\subsection{\textsc{WRITE} Implementation}
The full \textsc{WRITE} implementation is given in Algorithm~\ref{alg2:writer}. In the following, we simply highlight the differences to \protocol.

As outlined before, \mprotocol\ is resilient to the adversary skipping timestamps. This is achieved by having the writer authenticate the timestamp of a \textsc{WRITE} with a key  shared among the writers. Note that such a shared key can be obtained by combining the different group keys; for instance, .

To obtain a timestamp, in the \textsc{clock} procedure, the writer retrieves the timestamp (held in variable ) from a quorum of  servers and picks the highest timestamp  with a valid MAC. Then, the writer increases  and computes a MAC for  using . Finally, \textsc{clock} returns .

To write a value , the writer, \emph{(i)} obtains a timestamp  from the \textsc{clock} procedure, \emph{(ii)} authenticates  and the nonce's hash  by a vector of MACs , with one entry for each server  using group key , and \emph{(iii)} stores  both in \textsc{store} and \complete. Upon reception of a \textsc{store} message, the server writes the tuple  into the history entry . Upon reception of a \complete message, the server changes the value of  to  if .


\begin{algo}[t]
\small


\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE : set of , (process id) initially 
\STATE : structure ,\\ initially   
\ENDINDENT
\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\vspace{-2 em}
\begin{distribalgo}[1]  \setcounter{ALC@line}{\value{alg:client1:lines}}
\INDENT {\textbf{operation} \textsc{WRITE}}
\STATE 
\STATE  \label{alg2:writer:clock}
\STATE 
\STATE 
\STATE 
\STATE  \label{alg2:writer:store}
\STATE  \label{alg2:writer:complete}
\STATE return \textsc{ok}
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{clock}()}
\STATE send \textsc{clock} to all servers
\STATE \textbf{wait until} 
\STATE 
\STATE 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{upon} receiving \textsc{clock\_ack} from server }
\STATE 
\STATE \textbf{if}  \textbf{then}  \label{alg2:writer:ts-integrity}
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \textsc{store}()}
\STATE 
\STATE 
\STATE \textbf{foreach} server  send \textsc{store} to 
\STATE \textbf{wait for} \textsc{store\_ack} from  servers
\ENDINDENT

\medskip
\INDENT {\textbf{procedure} \complete()}
\STATE send \complete to all servers
\STATE \textbf{wait for} \textsc{\complete\_ack} from  servers
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}

\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\caption{{Algorithm of writer  in \mprotocol.}}\label{alg2:writer}
\end{algo}


\begin{algo*}[t]
\small
\centering
\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE : structure , initially  \hfill
\STATE : vector of  indexed by , initially 
\ENDINDENT
\end{distribalgo}



\begin{tabular}{c}\hline\mbox{}\hspace{0.97\textwidth}\mbox{}\end{tabular}
\begin{minipage}[t]{0.44\textwidth}
\begin{distribalgo}[1]  \setcounter{ALC@line}{\value{alg:client1:lines}}
\vspace{-1 em}
\INDENT {\textbf{upon} receiving \textsc{clock} from writer }
\STATE send \textsc{clock\_ack} to writer 
\ENDINDENT

\medskip
\INDENT {\textbf{upon} receiving \textsc{store}\\ from writer }
\STATE 
\STATE send \textsc{store\_ack} to writer 
\ENDINDENT

\medskip
\INDENT {\textbf{upon} receiving \complete from writer }
\STATE  \textbf{if} {} \textbf{then} \label{alg2:server:update-complete}
\STATE send \textsc{\complete\_ack} to writer 
\ENDINDENT

\medskip
\INDENT {\textbf{upon} receiving \textsc{collect} from client }
\STATE send \textsc{collect\_ack} to client 
\ENDINDENT


\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\end{minipage}\hfill
\begin{minipage}[t]{0.56\textwidth}
\begin{distribalgo}[1]   \setcounter{ALC@line}{\value{alg:client1:lines}}
\vspace{-1 em}
\INDENT {\textbf{upon} receiving \textsc{filter} from client }
\STATE 
\STATE  \label{alg2:server:valid-filter}
\STATE \textbf{if}  \textbf{then} \label{alg2:server:update-filter} \hfill //write-back
\smallskip
\STATE  \label{alg2:server:valid-nonce}
\STATE 
\STATE send \textsc{filter\_ack} to client  \label{alg2:server:ret}
\ENDINDENT

\medskip
\INDENT {\textbf{upon} receiving \textsc{repair} from client }
\STATE \textbf{if}  \textbf{then}  \label{alg2:server:update-repair}\label{alg2:server:valid-repair}
\STATE send \textsc{repair\_ack} to client 
\ENDINDENT

\medskip
\INDENT {\textbf{Predicates:}}
\STATE \textsf{valid}()  (  ) \label{alg2:server:valid-pred}
\STATE \textsf{validByHist}()  ()
\ENDINDENT

\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}

\end{minipage}

\begin{tabular}{c}\hline\mbox{}\hspace{0.97\textwidth}\mbox{}\end{tabular}
\vspace{-1 em}
\caption{{Algorithm of server  in \mprotocol.}}\label{alg2:server}
\end{algo*}


\subsection{\textsc{READ} Implementation}
The full \textsc{READ} implementation is given in Algorithm~\ref{alg:reader}.
The \textsc{READ} consists of three consecutive rounds, \textsc{collect}, \textsc{filter} and \textsc{repair}. In \textsc{collect}, a client reads the candidate triple  stored in variable  in the server, and inserts it into the candidate set  together with the candidates read from other servers. After the client receives  candidates from different servers, \textsc{collect} returns.

In \textsc{filter}, the client submits  to each server. Upon reception of , the server chooses a candidate  to write-back, as the candidate with the highest timestamp in  such that \textsf{valid}() holds, and sets  to  if . Roughly speaking, the predicate \textsf{valid}() holds if the server, verifies the integrity of the timestamp  and nonce  either by the MAC, or by the corresponding history entry. Besides that, the server chooses a candidate  to return, as the candidate with the highest timestamp in  such that  holds. The predicate \textsf{validByHist}() holds, if the server keeps a matching history entry for . The server then responds to the client with a message including the timestamp , the fragment , the cross-checksum  and the vector of MACs .

The client waits for the responses from servers until there is a candidate  with the highest timestamp in  such that \textsf{safe}() holds, or until  is empty, after which \textsc{filter} returns. The predicate \textsf{safe}() holds if at least  different servers  have responded with timestamp , fragment ,  cross-checksum  such that , and vector .
If  is empty, the client sets  to the initial value . Otherwise, the client selects the highest candidate  and restores value  by decoding  from the  correct fragments received for .

In \textsc{repair}, the client verifies the integrity of  by matching it against the vector  received from  different servers. If  and  match then \textsc{repair} returns. Otherwise, the client repairs  by setting  to  and invokes a round of write-back by sending a \textsc{repair} message to all servers. Upon reception of such a message, the server sets  to  if  and \textsf{valid} holds and responds with an \textsc{repair\_ack} message to the client. Once the client receives acknowledgements from  different servers, \textsc{repair} returns. After \textsc{repair} returns, the \textsc{READ} returns .

\begin{algo}[t]
\small

\begin{distribalgo}[1] \setcounter{ALC@line}{\value{alg:client1:lines}}
\smallskip
\INDENT {\textbf{Definitions:}}
\STATE : , initially 
\STATE : set of , initially 
\STATE : set of , initially 
\STATE : vector of , initially 
\ENDINDENT
\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\vspace{-2 em}
\begin{distribalgo}[1]   \setcounter{ALC@line}{\value{alg:client1:lines}}
\INDENT {\textbf{operation} \textsc{READ}}
\STATE 
\STATE 
\STATE 
\STATE 
\IF {}
\STATE ~\label{alg2:reader:select}
\STATE  ~\label{alg2:reader:restore}
\STATE 
\ENDIF
\STATE \textbf{else} 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{procedure} \textsc{collect}()}
\STATE send \textsc{collect} to all servers
\STATE \textbf{wait until} 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{upon} receiving \textsc{collect\_ack} from server }
\STATE 
\STATE \textbf{if}  \textbf{then} 
\ENDINDENT

\smallskip
\INDENT {\textbf{procedure} \textsc{filter}()}
\STATE send \textsc{filter} to all servers
\STATE \textbf{wait until} \\ 
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{upon} receiving \textsc{filter\_ack} from server }
\STATE ; 
\STATE 
\ENDINDENT

\smallskip
\INDENT {\textbf{procedure} \textsc{restore}()}
\STATE  s.t. \\  \label{alg2:reader:cc}
\STATE  \label{alg2:reader:fr}
\STATE  \label{alg2:reader:decode}
\STATE return 
\ENDINDENT

\smallskip
\INDENT {\textbf{procedure} \textsc{repair}()}
\STATE  s.t. \\ 
\IF {} \label{alg2:reader:vec-integrity}
\STATE  \label{alg2:reader:repair}\hfill //repair
\STATE send \textsc{repair} to all servers
\STATE \textbf{wait for} \textsc{repair\_ack} from  servers
\ENDIF
\ENDINDENT

\smallskip
\INDENT {\textbf{Predicates:}}
\STATE \textsf{highcand}()  (
\smallskip
\STATE \textsf{safe}()  \\ \label{alg2:reader:safe}
\\
\\
)
\smallskip
\STATE \textsf{invalid}()  
\ENDINDENT
\setcounter{alg:client1:lines}{\value{ALC@line}}
\end{distribalgo}
\begin{tabular}{c}\hline\mbox{}\hspace{0.45\textwidth}\mbox{}\end{tabular}
\caption{{Algorithm of client  in \mprotocol.}}\label{alg2:reader}
\vspace{-1 em}
\end{algo}


\subsection{Correctness Sketch}
We show that \mprotocol\ satisfies \textsc{READ} linearizability as follows. We show that if a completed \textsc{READ}  returns  then a subsequent \textsc{READ}  does not return a value older than . Note that the arguments for \textsc{READ}/\textsc{WRITE} linearizability and wait-freedom are very similar to those of \protocol{} (Section~\ref{sec:pow-sketch}), and therefore omitted.

Suppose a \textsc{READ}  by a correct client follows after  that returns . If  is the candidate selected in , we argue that  does not select a candidate with a timestamp lower than . Note that besides  being a valid candidate, in \textsc{repair}, the client also checks the integrity of . If  passes the integrity check in  (line~\ref{alg2:reader:vec-integrity} of Algorithm~\ref{alg2:reader}), then the integrity of  has been fully established. Otherwise,  fails the integrity check. In that case the client repairs  (line~\ref{alg2:reader:repair} of Algorithm~\ref{alg2:reader}) and subsequently writes-back  to  correct servers. In both cases,  correct servers have set  to  or to a valid candidate with a higher timestamp. As such, during \textsc{collect} in , a valid candidate  such that  is included in . Since  is valid,  correct servers hold history entries matching  and none of them responds with a timestamp lower than . Consequently, at most  timestamps received by the client in \textsc{filter} are lower than  and thus  is never excluded from . By Algorithm~\ref{alg2:reader},  does not select a candidate with timestamp lower than .



\section{Implementation \& Evaluation}\label{sec:implementation}

In this section, we describe an implementation modeling a Key-Value Store (KVS) based on \mprotocol. More specifically, to model a KVS, we use multiple instances of \mprotocol\ referenced by keys. We then evaluate the performance of our implementation  and we compare it both: \emph{(i)} M-ABD, the multi-writer variant of the crash-only atomic storage protocol of~\cite{ABND95}, and \emph{(ii)} Phalanx, the multi-writer robust atomic protocol of~\cite{Phalanx}.

\subsection{Implementation Setup}\label{subsec:setup}

Our KVS implementation is based on the JAVA-based framework Neko~\cite{Neko09} that provides support for inter-process communication, and on the Jerasure~\cite{Jerasure08} library for constructing the dispersal codes.
To evaluate the performance of our \mprotocol\, we additionally implemented two KVSs  based on M-ABD and Phalanx.


In our implementation, we relied on 160-bit SHA1 for hashing purposes, 160-bit keyed HMACs to implement MACs, and 1024-bit DSA to generate signatures. For simplicity, we abstract away the effect of message authentication in our implementations; we argue that this does not affect our performance evaluation since data origin authentication is typically handled as part of the access control layer in all three implementations, when deployed in realistic settings (e.g., in Wide Area Networks).

We deployed our implementations on a private network consisting of a 4-core Intel Xeon E5443 with 4GB of RAM, a 4 core Intel i5-3470 with 8 GB of RAM,
an 8 Intel-Core i7-37708 with 8 GB of RAM, and a 64-core Intel Xeon E5606 equipped with 32GB of RAM. In our network, the communication between various machines was bridged
using a 1 Gbps switch. All the servers were running in separate processes on the Xeon E5606 machine, whereas the clients were distributed among the Xeon E5443, i7, and the i5 machines. Each client invokes operation in a closed loop, i.e., a client may have at most one pending operation. In all KVS implementations,  all \textsc{WRITE} and \textsc{READ} operations are served by a local database stored on disk.


We evaluate the peak throughput incurred in \mprotocol\ in \textsc{WRITE} and \textsc{READ} operations, when compared to M-ABD and Phalanx with respect to: \emph{(i)} the file size (64 KB, 128 KB, 256 KB, 512 KB, and 1024 KB), and \emph{(ii)}
to the server failure threshold  (1, 2, and 3, respectively). For better evaluation purposes, we vary each variable separately and we fix
the remaining parameters to a default configuration (Table~\ref{tab:nominal}).
We also evaluate the latency incurred in \mprotocol\ with respect to the attained throughput.










\begin{table}
\centering
\scalebox{1.1}{\begin{tabular}{|c|c|}
  \hline
  \textbf{Parameter} & \textbf{Default Value}\\
  \hline
  \hline
Failure threshold                           & 1 \\
File size                                & 256 KB\\
Probability of Concurrency & 1\%\\
Workload Distribution & 100\% \textsc{READ} /100\% \textsc{WRITE}\\
  \hline
\end{tabular}}
\caption{Default parameters used in evaluation.}
\vspace{-1 em}
\label{tab:nominal}
\end{table}

We measure peak throughput as follows. We require that each writer performs back to back \textsc{WRITE} operations; we then increase the number of writers in the system until
the aggregated throughput attained by all writers is saturated. The peak throughput is then computed as the maximum aggregated amount of data (in bytes) that can be written/read to the servers per second.

Each data point in our plots is averaged over 5 independent measurements; where appropriate, we include the corresponding 95\% confidence intervals. as data objects. On the other hand, \textsc{READ} operations request the data pertaining to randomly-chosen keys.
For completeness, we performed our evaluation \emph{(i)} in the Local Area Network (LAN) setting comprising our aforementioned network (Section~\ref{subsec:lan}) and \emph{(ii)} in a simulated Wide Area Network (WAN) setting (Section~\ref{subsec:wan}). Our evaluation results are presented in Figure~\ref{fig:results}.

\subsection{Evaluation Results within a LAN Setting}\label{subsec:lan}

\begin{figure*}[tb]
  \centering
   \subfigure[Throughput vs. latency in a LAN setting.
     ]{
          \label{fig:latency}
          \includegraphics*[width=0.47\linewidth]{latency.pdf}}
           \hspace{.01in}
     \subfigure[Peak throughput with respect to the failure threshold in a LAN setting.
     ]{
          \label{fig:newfaulty}
          \includegraphics*[width=0.47\linewidth]{newfaulty.pdf}}\\
     \hspace{.01in}
     \subfigure[Peak throughput with respect to the file size in a LAN setting.
     ]{
         \label{fig:newfile}
         \includegraphics*[width=0.47\linewidth]{newfile.pdf}}
     \subfigure[Latency vs the failure threshold in a simulated WAN setting.
     ]{
          \label{fig:latencypareto}
          \includegraphics*[width=0.47\linewidth]{latencypareto.pdf}}
     \caption{Evaluation Results. Data points are averaged over 5 independent runs; where appropriate, we include the corresponding 95\% confidence intervals.}
    \label{fig:results}
\end{figure*}

Figure~\ref{fig:latency} depicts the latency incurred in \mprotocol{} when compared to M-ABD and Phalanx, with respect to the achieved throughput (measured in the number of operations per second).
Our results show that, by combining metadata write-backs with erasure coding, \mprotocol{} achieves lower latencies than M-ABD and Phalanx for both \textsc{READ} and \textsc{WRITE} operations.
As expected, \textsc{READ} latencies incurred in \protocol{} are lower than those of \textsc{WRITE} operations since a \textsc{WRITE} requires an extra communication round corresponding to the \textsc{clock} round. Furthermore, due to PoW and the use of lightweight cryptographic primitives, the \textsc{READ} performance of \protocol{} considerably outperforms M-ABD and Phalanx. On the other hand, writing in \mprotocol{} compares similarly to the writing in M-ABD.

Figure~\ref{fig:newfaulty} depicts the peak throughput achieved in \mprotocol{} with respect to the number of Byzantine servers .
As  increases, the gain in peak throughout achieved in \mprotocol's \textsc{READ} and \textsc{WRITE} increases compared to M-ABD and Phalanx. This mainly stems from the reliance on erasure coding, which ensures that
the overhead of file replication among the servers is minimized when compared to M-ABD and Phalanx. In typical settings, featuring  and the default parameters of Table~\ref{tab:nominal}, the peak throughput achieved in \mprotocol's \textsc{READ} operation is almost twice as large as that in M-ABD and Phalanx.

In Figure~\ref{fig:newfile}, we measure the peak throughout achieved in \mprotocol{} with respect to the file size. Our findings clearly show that as the file size increases, the performance gain of \mprotocol{} compared to M-ABD and Phalanx becomes considerable. For example, when the file size is 1 MB, the peak throughput of \textsc{READ} and \textsc{WRITE} operations in \mprotocol{} approaches the (network-limited) bounds of 50 MB/s\footnote{Note that an effective peak throughout of 50MB/s in \mprotocol{} reflects an actual throughput of almost 820 Mbps when .} and 45 MB/s, respectively.



\subsection{Evaluation Results within a Simulated WAN Setting}\label{subsec:wan}

We now proceed to evaluate the  performance of \mprotocol{} when deployed in WAN settings. For that purpose, we rely on a 100 Mbps switch to bridge the network outlined in Section~\ref{subsec:setup} and
we rely on NetEm~\cite{netem} to emulate the packet delay variance specific to WANs. More specifically, we add a Pareto distribution to our links, with a mean of 20 ms and a variance of 4 ms.

We then measure the latency incurred in \mprotocol{} in the emulated WAN setting. Our measurement results (Figure~\ref{fig:latencypareto}) confirm our previous analysis conducted in the LAN scenario and demonstrate the superior performance of \mprotocol{} compared to M-ABD and Phalanx in realistic settings. Here, we point out that the performance of \mprotocol{} incurred in both \textsc{READ} and \textsc{WRITE} operations does not deteriorate as the number of Byzantine servers in the system increases. This is mainly due to the reliance on erasure coding. In fact, the overhead of transmitting an erasure-coded file  to the  servers, with a reconstruction threshold
of  is given by . It is easy to see that, as  increases, this overhead is asymptotically increases towards .



\section{Related Work}\label{sec:relwork}

A seminal crash-tolerant \emph{robust} \emph{linearizable} read/write storage implementation assuming a majority of correct processes was presented in \cite{ABND95}. In the original single-writer variant of \cite{ABND95},  read operations always take 2 rounds between a client and servers with \emph{data} write-backs in the second round. On the other hand all write operations complete in a single round; in the multi-writer variant \cite{LS02}, the second write round is necessary. Server state modifications by readers introduced by \cite{ABND95} are unavoidable; namely, \cite{FL03} showed a  lower bound on the number of servers that a reader has to modify in any wait-free linearizable storage. However, robust storage implementations differ in the strategy employed by readers: in some protocols readers write-back data (e.g., \cite{ABND95,Phalanx,AAB07,GNS09,GV10,DGM11}) whereas in others readers only write metadata to servers (e.g., \cite{FL03,DGLV10,CA12}).

The only two asynchronous storage protocols that feature \emph{metadata write-backs} are multi-writer crash-tolerant protocols of \cite{FL03} and \cite{CA12} that are both also linearizable, wait-free and optimally resilient. The read/write protocol of \cite{FL03} fails to achieve optimal latency featuring 3-round writes and reads. Vivace \cite{CA12} is a key-value storage system tailored for WANs (geo-replication); it features 3-round reads and 2-round writes, saving on a communication round by relying on synchronized clocks (NTP, GPS), which are used as counters. In comparison, \protocol\ features optimal latency without synchronized clocks and is the first protocol to implement metadata write-backs while tolerating Byzantine failures.

Data write-backs are also not needed in case of \emph{fast} robust storage implementations that feature single round reads and writes \cite{DGLV10}. Namely, \cite{DGLV10} presents fast single-writer crash-tolerant and BFT storage implementations in which readers only write metadata \emph{while} reading data in the single round of read and hence, without any write-back. However, fast implementations are fundamentally limited and cannot be optimally resilient, since the number of required servers is inherently linear in number of readers \cite{DGLV10}. The limitation on the number of readers of \cite{DGLV10} was relaxed in~\cite{GNS09}, where a single-writer crash-tolerant robust linearizable storage implementation was presented, in which most of the reads complete in a single round, yet a fraction of reads is permitted to be ``slow'' and complete in 2 rounds.

Clearly, most BFT storage implementations have been focusing on using as few servers as possible, ideally , which defines optimal resilience in the Byzantine model \cite{MAD02}. This is achieved by Phalanx \cite{Phalanx}, a BFT variant of \cite{ABND95}. Phalanx uses digital signatures, i.e., \emph{self-verifying data}, to port \cite{ABND95} to the Byzantine model, maintaining the latency of~\cite{ABND95}, as well as its data write-backs. However, since digital signatures introduce considerable overhead~\cite{Rei94,MR97}, research attention has shifted from protocols that employ self-verifying data \cite{Phalanx, MR98, CT06, LR06} to protocols that feature lightweight authentication, or no data authentication at all (unauthenticated model).

In the unauthenticated model,~\cite{ACKM06} ruled out the existence of optimally resilient robust Byzantine fault-tolerant storage implementation where all write operations finish after a single communication round. This explained the previous difficulties in reaching optimal resilience in unauthenticated BFT storage implementations where several protocols have used  servers \cite{GWGR04,BD04}. Furthermore,~\cite{DGM11} showed the impossibility of reading from a robust optimally resilient linearizable storage in two communication rounds; in addition, if \textsc{WRITE} operations perform a constant number of rounds, even reading in three rounds is impossible~\cite{DGM11}. These results imply that the optimal latency of a robust optimally resilient and linearizable BFT storage in the unauthenticated model is 2 rounds for writes and 4 rounds for reads, even in the single writer case. This can be achieved by the regular-to-linearizable transformation of the regular \cite{Lam86} storage protocol of \cite{GV06}. Hence, it is not surprising that other robust BFT storage protocols in the unauthenticated model focused on optimizing common-case latency with either an unbounded number of read rounds in the worst case \cite{GLV06,GV10} or a number of read rounds dependent on the number of faulty processes  \cite{MAD02,AAB07}.


Clearly, there is a big gap between storage protocols that use self-verifying data and those that assume no authentication. Loft \cite{HendricksGR07} aims at bridging this gap and implements erasure-coded optimally resilient linearizable storage  while optimizing the failure-free case. Loft uses homomorphic fingerprints and MACs; it features 3-round wait-free writes, but reads are based on data write-backs and are only obstruction-free \cite{HLM03}, i.e., the number of read rounds is unbounded in case of read/write concurrency. Similarly, our \emph{Proofs of Writing} (PoW) incorporate lightweight authentication that is, however, sufficient to achieve optimal latency and to facilitate metadata write-backs. We find PoW to be a fundamental improvement in the light of BFT storage implementations that explicitly renounce linearizability in favor of weaker regularity due to the high cost of data write-backs \cite{BessaniCQAS11}.

\section{Concluding Remarks}
\label{sec:conclusion}

In this paper, we presented \protocol, the first efficient
robust storage protocol that achieves optimal latency, measured
in maximum (worst-case) number of communication rounds between a client and storage servers, without resorting to digital signatures.
We also separately presented a multi-writer variant of our protocol called \mprotocol.  The \emph{efficiency} of our proposals stems from combining \emph{lightweight cryptography}, \emph{erasure coding} and  \emph{metadata writebacks}, where readers write-back only metadata to achieve linearizability. While robust BFTs have been often criticized for being prohibitively inefficient, our findings suggest that efficient and robust BFTs can be realized
in practice by relying on lightweight cryptographic primitives without compromising \emph{worst-case} performance.

At the heart of both \protocol\ and \mprotocol{} protocols are \emph{Proofs of Writing (PoW)}: a novel \scheme\ inspired by commitment schemes in the flavor of~\cite{Halevi1996}, that enables \protocol\ to write and read in 2 rounds in case of the single-writer \protocol\ which we show optimal.
Similarly, by relying on PoW, \mprotocol{} features 3-round writes/reads where the third read round is only invoked under active attacks.
Finally, we  demonstrated \mprotocol's superior performance compared to existing crash and Byzantine-tolerant atomic storage implementations.

We point out that our protocols assume unbounded storage capacity to maintain a version history of
the various updates performed by the writers in the system. We argue, however, that this limitation is not particular to our proposals and is inherent to all
solutions that rely on erasure-coded data~\cite{Soules2003}. Note that prior studies have demonstrated that it takes several weeks to exhaust the capacity of versioning systems~\cite{Strunk2000}; in case the storage capacity is bounded, the servers can rely on efficient garbage collection mechanisms~\cite{GWGR04} to avoid possible exhaustion of the storage system.




\begin{thebibliography}{10}

\bibitem{Jerasure08}
{{Jerasure}}.
\newblock \url{https://github.com/tsuraan/Jerasure}, 2008.

\bibitem{Neko09}
{{The Neko Project}}.
\newblock \url{http://ddsg.jaist.ac.jp/neko/}, 2009.

\bibitem{ACKM06}
Ittai Abraham, Gregory Chockler, Idit Keidar, and Dahlia Malkhi.
\newblock {Byzantine Disk Paxos: Optimal Resilience with Byzantine Shared
  Memory}.
\newblock {\em Distributed Computing}, 18(5):387--408, 2006.

\bibitem{AAB07}
Amitanand~S. Aiyer, Lorenzo Alvisi, and Rida~A. Bazzi.
\newblock {B}ounded {W}ait-free {I}mplementation of {O}ptimally {R}esilient
  {B}yzantine {S}torage {W}ithout ({U}nproven) {C}ryptographic {A}ssumptions.
\newblock In {\em Proceedings of DISC}, 2007.

\bibitem{AmirCKL11}
Yair Amir, Brian~A. Coan, Jonathan Kirsch, and John Lane.
\newblock Prime: {B}yzantine replication under attack.
\newblock {\em IEEE Trans. Dependable Sec. Comput.}, 8(4):564--577, 2011.

\bibitem{ABND95}
Hagit Attiya, Amotz Bar-Noy, and Danny Dolev.
\newblock Sharing {M}emory {R}obustly in {M}essage-{P}assing {S}ystems.
\newblock {\em J. ACM}, 42:124--142, January 1995.

\bibitem{BD04}
Rida~A. Bazzi and Yin Ding.
\newblock {Non-skipping Timestamps for Byzantine Data Storage Systems}.
\newblock In {\em Proceedings of DISC}, pages 405--419, 2004.

\bibitem{BessaniCQAS11}
Alysson~Neves Bessani, Miguel~P. Correia, Bruno Quaresma, Fernando Andr{\'e},
  and Paulo Sousa.
\newblock Depsky: dependable and secure storage in a cloud-of-clouds.
\newblock In {\em Proceedings of EuroSys}, pages 31--46, 2011.

\bibitem{CT06}
Christian Cachin and Stefano Tessaro.
\newblock {Optimal Resilience for Erasure-Coded Byzantine Distributed Storage}.
\newblock In {\em Proceedings of DSN}, pages 115--124, 2006.

\bibitem{CA12}
Brian Cho and Marcos~K. Aguilera.
\newblock Surviving congestion in geo-distributed storage systems.
\newblock In {\em Proceedings of USENIX ATC}, pages 40--40, 2012.

\bibitem{ChocklerGKV09}
Gregory Chockler, Rachid Guerraoui, Idit Keidar, and Marko Vukoli\'c.
\newblock Reliable distributed storage.
\newblock {\em IEEE Computer}, 42(4):60--67, 2009.

\bibitem{CMD03}
Gregory Chockler, Dahlia Malkhi, and Danny Dolev.
\newblock Future directions in distributed computing.
\newblock chapter A data-centric approach for scalable state machine
  replication, pages 159--163. 2003.

\bibitem{CWADM09}
Allen Clement, Edmund~L. Wong, Lorenzo Alvisi, Michael Dahlin, and Mirco
  Marchetti.
\newblock Making byzantine fault tolerant systems tolerate byzantine faults.
\newblock In {\em Proceedings of NSDI}, pages 153--168, 2009.

\bibitem{DGM11}
Dan Dobre, Rachid Guerraoui, Matthias Majuntke, Neeraj Suri, and Marko
  Vukoli\'{c}.
\newblock {The Complexity of Robust Atomic Storage}.
\newblock In {\em Proceedings of PODC}, pages 59--68, 2011.

\bibitem{DGLV10}
Partha Dutta, Rachid Guerraoui, Ron~R. Levy, and Marko Vukoli\'{c}.
\newblock {Fast Access to Distributed Atomic Memory}.
\newblock {\em SIAM J. Comput.}, 39:3752--3783, December 2010.

\bibitem{FL03}
Rui Fan and Nancy Lynch.
\newblock {Efficient Replication of Large Data Objects}.
\newblock In {\em Proceedings of DISC}, pages 75--91, 2003.

\bibitem{GarciaRP11}
Rui Garcia, Rodrigo Rodrigues, and Nuno~M. Pregui\c{c}a.
\newblock Efficient middleware for byzantine fault tolerant database
  replication.
\newblock In {\em Proceedings of EuroSys}, pages 107--122, 2011.

\bibitem{GNS09}
Chryssis Georgiou, Nicolas~C. Nicolaou, and Alexander~A. Shvartsman.
\newblock {Fault-tolerant Semifast Implementations of Atomic Read/Write
  Registers}.
\newblock {\em J. Parallel Distrib. Comput.}, 69(1):62--79, January 2009.

\bibitem{GWGR04}
Garth~R. Goodson, Jay~J. Wylie, Gregory~R. Ganger, and Michael~K. Reiter.
\newblock {Efficient Byzantine-Tolerant Erasure-Coded Storage}.
\newblock In {\em Proceedings of DSN}, 2004.

\bibitem{GLV06}
Rachid Guerraoui, Ron~R. Levy, and Marko Vukolic.
\newblock {Lucky Read/Write Access to Robust Atomic Storage}.
\newblock In {\em Proceedings of DSN}, pages 125--136, 2006.

\bibitem{GV06}
Rachid Guerraoui and Marko Vukoli\'{c}.
\newblock {How Fast Can a Very Robust Read Be?}
\newblock In {\em Proceedings of PODC}, pages 248--257, 2006.

\bibitem{GV10}
Rachid Guerraoui and Marko Vukoli\'c.
\newblock Refined quorum systems.
\newblock {\em Distributed Computing}, 23(1):1--42, 2010.

\bibitem{Halevi1996}
Shai Halevi and Silvio Micali.
\newblock Practical and provably-secure commitment schemes from collision-free
  hashing.
\newblock In {\em Proceedings of CRYPTO}, pages 201--215, 1996.

\bibitem{HendricksGR07}
James Hendricks, Gregory~R. Ganger, and Michael~K. Reiter.
\newblock Low-overhead {B}yzantine fault-tolerant storage.
\newblock In {\em Proceedings of SOSP}, pages 73--86, 2007.

\bibitem{Her91}
Maurice Herlihy.
\newblock Wait-{F}ree {S}ynchronization.
\newblock {\em ACM Trans. Program. Lang. Syst.}, 13(1), 1991.

\bibitem{HLM03}
Maurice Herlihy, Victor Luchangco, and Mark Moir.
\newblock Obstruction-{F}ree {S}ynchronization: {D}ouble-{E}nded {Q}ueues as an
  {E}xample.
\newblock In {\em Proceedings of ICDCS}, 2003.

\bibitem{HW90}
Maurice~P. Herlihy and Jeannette~M. Wing.
\newblock Linearizability: {A} {C}orrectness {C}ondition for {C}oncurrent
  {O}bjects.
\newblock {\em ACM Trans. Program. Lang. Syst.}, 12(3), 1990.

\bibitem{JCT98}
Prasad Jayanti, Tushar~Deepak Chandra, and Sam Toueg.
\newblock Fault-tolerant {W}ait-free {S}hared {O}bjects.
\newblock {\em J. ACM}, 45(3), 1998.

\bibitem{KotlaADCW09}
Ramakrishna Kotla, Lorenzo Alvisi, Michael Dahlin, Allen Clement, and Edmund~L.
  Wong.
\newblock Zyzzyva: Speculative {B}yzantine fault tolerance.
\newblock {\em ACM Trans. Comput. Syst.}, 27(4), 2009.

\bibitem{KR09}
Petr Kuznetsov and Rodrigo Rodrigues.
\newblock Bftw: {W}hy? {W}hen? {W}here? workshop on the theory and
  practice of {B}yzantine fault tolerance.
\newblock {\em SIGACT News}, 40(4):82--86, 2009.

\bibitem{Lam86}
Leslie Lamport.
\newblock {On Interprocess Communication.}
\newblock {\em Distributed Computing}, 1(2):77--101, 1986.

\bibitem{LSP82}
Leslie Lamport, Robert~E. Shostak, and Marshall~C. Pease.
\newblock The byzantine generals problem.
\newblock {\em ACM Trans. Program. Lang. Syst.}, 4(3):382--401, 1982.

\bibitem{LCAA07}
Harry~C. Li, Allen Clement, Amitanand~S. Aiyer, and Lorenzo Alvisi.
\newblock {The Paxos Register}.
\newblock In {\em Proceedings of SRDS}, pages 114--126, 2007.

\bibitem{LR06}
Barbara Liskov and Rodrigo Rodrigues.
\newblock {Tolerating Byzantine Faulty Clients in a Quorum System}.
\newblock In {\em Proceedings of ICDCS}, 2006.

\bibitem{LS02}
Nancy~A. Lynch and Alexander~A. Shvartsman.
\newblock {RAMBO: A Reconfigurable Atomic Memory Service for Dynamic Networks}.
\newblock In {\em Proceedings of DISC}, pages 173--190, 2002.

\bibitem{LT89}
Nancy~A. Lynch and Mark~R. Tuttle.
\newblock An introduction to input/output automata.
\newblock {\em CWI Quarterly}, 2:219--246, 1989.

\bibitem{MR97}
Dahlia Malkhi and Michael~K. Reiter.
\newblock {A High-Throughput Secure Reliable Multicast Protocol}.
\newblock {\em J. Comput. Secur.}, 5(2):113--127, March 1997.

\bibitem{MR98}
Dahlia Malkhi and Michael~K. Reiter.
\newblock {Byzantine} {Q}uorum {S}ystems.
\newblock {\em Distributed Computing}, 11(4):203--213, 1998.

\bibitem{Phalanx}
Dahlia Malkhi and Michael~K. Reiter.
\newblock {Secure and Scalable Replication in Phalanx}.
\newblock In {\em Proceedings of SRDS}, pages 51--58, 1998.

\bibitem{MAD02}
Jean-Philippe Martin, Lorenzo Alvisi, and Michael Dahlin.
\newblock {Minimal Byzantine Storage}.
\newblock In {\em Proceedings of DISC}, pages 311--325, 2002.

\bibitem{netem}
NetEm.
\newblock {NetEm, the Linux Foundation}.
\newblock Website, 2009.
\newblock Available online at
  \url{http://www.linuxfoundation.org/collaborate/workgroups/networking/netem}.

\bibitem{Rei94}
Michael~K. Reiter.
\newblock {Secure Agreement Protocols: Reliable and Atomic Group Multicast in
  Rampart}.
\newblock In {\em Proceedings of CCS}, pages 68--80, 1994.

\bibitem{Sha79}
Adi Shamir.
\newblock {How to Share a Secret}.
\newblock {\em Commun. ACM}, 22(11):612--613, November 1979.

\bibitem{SMMK10}
Alexander Shraer, Jean-Philippe Martin, Dahlia Malkhi, and Idit Keidar.
\newblock Data-centric reconfiguration with network-attached disks.
\newblock In {\em Proceedings of LADIS}, pages 22--26, 2010.

\bibitem{SinghDMDR08}
Atul Singh, Tathagata Das, Petros Maniatis, Peter Druschel, and Timothy Roscoe.
\newblock Bft protocols under fire.
\newblock In {\em Proceedings of NSDI}, pages 189--204, 2008.

\bibitem{Soules2003}
Craig A.~N. Soules, Garth~R. Goodson, John~D. Strunk, and Gregory~R. Ganger.
\newblock Metadata efficiency in versioning file systems.
\newblock In {\em Proceedings of FAST}, pages 43--58, 2003.

\bibitem{Strunk2000}
John~D. Strunk, Garth~R. Goodson, Michael~L. Scheinholtz, Craig A.~N. Soules,
  and Gregory~R. Ganger.
\newblock Self-securing storage: protecting data in compromised system.
\newblock In {\em Proceedings of OSDI}, pages 12--12, 2000.

\bibitem{VeroneseCBLV13}
Giuliana~Santos Veronese, Miguel Correia, Alysson~Neves Bessani, Lau~Cheuk
  Lung, and Paulo Ver\'{\i}ssimo.
\newblock Efficient byzantine fault-tolerance.
\newblock {\em IEEE Trans. Computers}, 62(1):16--30, 2013.

\bibitem{WSS94}
Sue-Hwey Wu, Scott~A. Smolka, and Eugene~W. Stark.
\newblock Composition and behaviors of probabilistic i/o automata.
\newblock In {\em Proceedings of CONCUR}, CONCUR '94, pages 513--528, 1994.

\end{thebibliography}


\newpage

\appendix

\section{Correctness Proofs}
\subsection{Correctness of \protocol}

\begin{defn}[Valid candidate] \label{def:validcand}
A candidate  is \emph{valid} if \emph{\textsf{valid}} holds at some correct server.
\end{defn}


\begin{defn}[Timestamps of operations]
A \textsc{read} operation  by a correct reader has timestamp  iff the reader in  selected 
in line~\ref{alg:reader:select} such that . A \textsc{write} operation  has timestamp  iff
the writer increments its timestamp to  in line~\ref{alg:writer:timestamp}.
\end{defn}

\begin{la}[Validity]\label{la:validity}
Let  be a completed \textsc{read} by a correct reader. If  returns value  then  was written.
\end{la}
\begin{prooff} We show that if  is the value decoded in line~\ref{alg:reader:decode}, then  was indeed written. To show this, we argue that the fragments used to decode  were written. Note that prior to decoding  from a set of fragments, the reader establishes the correctness of each fragment as follows. First, in line~\ref{alg:reader:cc}, the reader chooses a cross-checksum that was received from  servers. Since one of these servers is correct, the chosen cross-checksum was indeed written. Secondly, the reader checks in line~\ref{alg:reader:fr} that each of the  fragments used to decode  hashes to the corresponding entry in the cross-checksum.  By the collision-resistance of , all fragments that pass this check were indeed written. Therefore, if  is the value decoded from these fragments, we conclude that  was written.
\end{prooff}

\begin{la}[Proofs of Writing] \label{la:pow}
If  is a valid candidate, then there exists a set  of  correct servers such that each server  changed  to .
\end{la}
\begin{prooff}
If  is valid, then by Definition~\ref{def:validcand}, \textsf{valid}() is true at some correct server . Hence,  holds at . By the pre-image resistance of , no computationally bounded adversary can acquire  from the sole knowledge of . Hence,  stems from the writer in a \textsc{write} operation  with timestamp . By Algorithm~\ref{alg:writer}, line~\ref{alg:writer:complete}, the value of  is revealed after the \textsc{store} phase in  completed. Hence, there exists a set  of  correct servers such that each server  changed  to .
\end{prooff}

\begin{la}[No exclusion]\label{la:noexclusion}
Let  be a valid candidate and let  be a \textsc{read} by a correct reader that includes  in  during \textsc{collect}. Then  is never excluded from .
\end{la}
\begin{prooff}
As  is valid, by Lemma~\ref{la:pow} a there exists a set  of  correct servers such that each server  changed  to . Hence, \textsf{valid}() is true at every server in . Thus, no server in  replies with a timestamp  in line~\ref{alg:server:ret}.
Therefore, at most  timestamps received by the reader in the \textsc{filter} phase are lower than , and so  is never excluded from .
\end{prooff}

\begin{la}[\textsc{read}/\textsc{write} Atomicity]~\label{la:rwatomic}
Let  be a completed \textsc{read} by a correct reader. If  follows some complete \textsc{write}, then  does not return a value older than .
\end{la}
\begin{prooff}
If  is the timestamp of \textsc{write}, it is sufficient to show that the timestamp of  is not lower than . To prove this, we show that  such that \emph{(i)}  and \emph{(ii)}  is never excluded from .

By the time  completes,  correct servers hold in  a candidate whose timestamp is  or greater. According to lines~\ref{alg:server:update-complete},~\ref{alg:server:update-gc} of Algorithm~\ref{alg:server}, a correct server never changes  to a candidate with a lower timestamp. Hence, when  is invoked,  correct servers hold candidates with timestamp  or greater in . Hence, during the \textsc{collect} phase in , some candidate received from a correct server with timestamp  or greater is inserted in . Such a candidate is necessarily valid because either the server received it directly from the writer, or the server checked its integrity in line~\ref{alg:server:valid-check}. Let  be the valid candidate with the highest timestamp in . Then by Lemma~\ref{la:noexclusion},  is never excluded from . By line~\ref{alg:reader:select}, no candidate  such that  is selected. Since , no candidate with a timestamp lower than  is selected in .
\end{prooff}

\begin{la}[\textsc{read} atomicity]\label{la:ratomic}
Let  and  be two completed \text{read} operations by correct readers. If  follows  that returns , then  does not return a value older than .
\end{la}
\begin{prooff} If  is the candidate selected in , it is sufficient to show that the timestamp of  is not lower than . We argue that
 contains a candidate  such that \textit{(i)}  and \textit{(ii)}  is never excluded from .

By the time  completes,  correct servers hold  in . As  was selected in  in line~\ref{alg:reader:select}, some correct server asserted that  is valid in line~\ref{alg:server:valid-check}. According to Algorithm~\ref{alg:server}, if a correct server excludes  from  in line~\ref{alg:server:gc}, then the server changed  to a valid candidate with timestamp  or greater in line~\ref{alg:server:update-gc}. Consequently,  correct servers hold in  a valid candidate with timestamp  or greater. As such, during \textsc{collect} in , a valid candidate  such that  is included in , and by Lemma~\ref{la:noexclusion},  is never excluded from . By line~\ref{alg:reader:select}, no candidate with a timestamp lower than  is selected. Since , no candidate with a timestamp lower than  is selected in .
\end{prooff}

\begin{theo}[Atomicity]
Algorithms~\ref{alg:writer},~\ref{alg:server} and~\ref{alg:reader} are atomic.
\end{theo}
\begin{prooff} This proof follows directly from Lemmas~\ref{la:validity}~\ref{la:rwatomic}~\ref{la:ratomic}.
\end{prooff}

We now proceed to proving wait-freedom.
\begin{theo}[Wait-freedom] Algorithms~\ref{alg:writer},~\ref{alg:server} and~\ref{alg:reader} are wait-free.
\end{theo}
\begin{prooff} We show that no operation invoked by a correct client ever blocks. The wait-freedom argument of the \textsc{write} is straightforward; in every phase, the writer awaits acks from the least number  of correct servers. The same argument holds for the \textsc{collect} phase of the \textsc{read}. Hence, in the remainder of the proof, we show that no \textsc{read} blocks in the \textsc{filter} phase. By contradiction, consider a \textsc{read}  by reader  that blocks during the \textsc{filter} phase after receiving \textsc{filter\_ack} messages from all correct servers. We distinguish two cases: (Case 1)  includes a valid candidate and (Case 2)  includes no valid candidate.

\begin{itemize}
\item Case 1: Let  be the highest valid candidate included in . We show that \textsf{highcand}()  \textsf{safe}() holds. Since  is valid, by Lemma~\ref{la:pow}, there exists a set  of  correct servers such that each server  changed  to . Thus, during the \textsc{filter} phase, \textsf{valid}() holds at every server in . As no valid candidate in  has a higher timestamp than , \textit{(i)} all servers  (at least ) responded with timestamp , corresponding erasure coded fragment , cross-checksum  in line~\ref{alg:server:ret} and \textit{(ii)} all correct servers (at least ) responded with timestamps at most . By \emph{(i)},  is \textsf{safe}. By \emph{(ii)}, every  such that  became \textsf{invalid} and was excluded from , implying that  is \textsf{highcand}.

\item Case 2: Here, we show that . As none of the candidates in  is valid, during the \textsc{filter} phase, the integrity check in line~\ref{alg:server:valid-check} failed for every candidate in  at all correct servers. Hence, at least  servers responded with timestamp . Since  is lower than any candidate timestamp, all candidates were classified as \textsf{invalid} and were excluded from .
\end{itemize}
\end{prooff}



\begin{theo}[Latency]
Algorithms~\ref{alg:writer},~\ref{alg:server} and~\ref{alg:reader} feature a latency of \emph{two} communication rounds for the \textsc{write} and \emph{two} for the \textsc{read}.
\end{theo}
\begin{prooff} By Algorithm~\ref{alg:writer}, the \textsc{write} completes after two phases, \textsc{store} and \textsc{\complete}, each taking one communication round. By Algorithm~\ref{alg:reader}, the \textsc{read} completes after two phases, \textsc{collect} and \textsc{filter}, each incurring one communication round.
\end{prooff}

\subsection{Correctness of \mprotocol}

\begin{defn}[Valid candidate] \label{def2:validcand}
A candidate  is \emph{valid} iff \emph{\textsf{valid}(c)} is true at some correct server.
\end{defn}

\begin{defn}[Timestamps of operations]
A \textsc{read} operation  by a non-malicious reader has timestamp  iff the reader in  selected 
in line~\ref{alg2:reader:select} such that . A \textsc{write} operation  has timestamp  iff the \textsc{clock} procedure in  returned  in line~\ref{alg2:writer:clock}.
\end{defn}

\begin{la}[Validity]\label{la2:validity}
Let  be a completed \textsc{read} by a correct reader. If  returns value  then  was written.
\end{la}
\begin{prooff} We show that if  is the value decoded in line~\ref{alg2:reader:decode}, then  was indeed written. To show this, we argue that the fragments used to decode  were written. Note that prior to decoding  from a set of fragments, the reader establishes the correctness of each fragment as follows. First, in line~\ref{alg2:reader:cc}, the reader chooses a cross-checksum that was received from  servers. Since one of these servers is correct, the chosen cross-checksum was indeed written. Secondly, the reader checks in line~\ref{alg2:reader:fr} that each of the  fragments used to decode  hashes to the corresponding entry in the cross-checksum.  By the collision-resistance of , all fragments that pass this check were indeed written. Therefore, if  is the value decoded from these fragments, we conclude that  was written.
\end{prooff}

\begin{la}[\textsc{write} atomicity]\label{la2:powr}
Let  be a completed operation by a correct client and let  be a completed  such that  precedes . If  and  are the timestamps of  and  respectively, then .
\end{la}
\begin{prooff} By the time  completes,  correct servers hold in  a candidate whose timestamp is  or greater. According to lines~\ref{alg2:server:update-complete},~\ref{alg2:server:update-filter},~\ref{alg2:server:update-repair} of Algorithm~\ref{alg2:server}, a correct server never updates  with a candidate that has a lower timestamp. Hence, the writer in  obtains from the \textsc{clock} procedure a timestamp that is greater or equal to  from some correct server . Let  be the candidate held in  by server , and let  be the timestamp reported to the writer. We now argue that  is not fabricated. To see why, note that prior to overwriting  with  in line~\ref{alg2:server:update-filter} (resp. ~\ref{alg2:server:update-repair}), server  checks that  is valid in line \ref{alg2:server:valid-filter} (resp. ~\ref{alg2:server:valid-repair}). The \textsf{valid} predicate as defined in line~\ref{alg2:server:valid-pred} subsumes an integrity check for . Hence,  passes the integrity check in line~\ref{alg2:writer:ts-integrity}; according to the \textsc{write} algorithm, .
\end{prooff}

\begin{la}[Proofs of Writing] \label{la2:pow}
If  is a valid candidate, then there exists a set  of  correct servers such that each server  changed  to .
\end{la}
\begin{prooff}
If  is valid, then by Definition~\ref{def2:validcand}, \textsf{valid}() is true at some correct server . Hence, either  or ) must hold at . By the pre-image resistance of , no computationally bounded adversary can acquire  from the sole knowledge of . Hence,  stems from some writer in a \textsc{write} operation  with timestamp . By Algorithm~\ref{alg2:writer}, line~\ref{alg2:writer:complete}, the value of  is revealed after the \textsc{store} round in  completed. Hence, there exists a set  of  correct servers such that each server  changed  to .
\end{prooff}

\begin{la}[No exclusion]\label{la2:noexclusion}
Let  be a valid candidate and let  be a \textsc{read} by a correct reader that includes  in  during \textsc{collect}. Then  is never excluded from .
\end{la}
\begin{prooff}
As  is valid, by Lemma~\ref{la2:pow} a there exists a set  of  correct servers such that each server  changed  to . Hence, \textsf{validByHist}() is true at every server in . Thus, no server in  replies with a timestamp  in line~\ref{alg2:server:ret}.
Therefore, at most  timestamps received by the reader in the \textsc{filter} round are lower than , and so  is never excluded from .
\end{prooff}

\begin{la}[\textsc{read}/\textsc{write} Atomicity]~\label{la2:rwatomic}
Let  be a completed \textsc{read} by a correct reader. If  follows some complete \textsc{write}, then  does not return a value older than .
\end{la}
\begin{prooff}
If  is the timestamp of \textsc{write}, it is sufficient to show that the timestamp of  is not lower than . To prove this, we show that  such that \emph{(i)}  and \emph{(ii)}  is never excluded from .

By the time  completes,  correct servers hold in  a candidate whose timestamp is  or greater. According to lines~\ref{alg2:server:update-complete},~\ref{alg2:server:update-filter},~\ref{alg2:server:update-repair} of Algorithm~\ref{alg2:server}, a correct server never changes  to a candidate with a lower timestamp. Hence, when  is invoked,  correct servers hold candidates with timestamp  or greater in . Hence, during \textsc{collect} in , some candidate received from a correct server with timestamp  or greater is inserted in . Such a candidate is necessarily valid by the integrity checks in lines~\ref{alg2:server:valid-filter},~\ref{alg2:server:valid-repair}. Let  be the valid candidate with the highest timestamp in . Then by Lemma~\ref{la2:noexclusion},  is never excluded from . By line~\ref{alg2:reader:select}, no candidate  such that  is selected. Since , no candidate with a timestamp lower than  is selected in .
\end{prooff}

\begin{la}
\label{la2:rratomic}
\emph{(\textsc{read} atomicity).}
Let  and  be two completed \text{read} operations by correct readers. If  follows  that returns , then  does not return a value older than .
\end{la}
\begin{prooff} If  is the candidate selected in , it is sufficient to show that the timestamp of  is not lower than . We argue that
 contains a candidate  such that \textit{(i)}  and \textit{(ii)}  is never excluded from .

As  is selected in  in line~\ref{alg2:reader:select} only if \textsf{safe}() holds, some correct server verified the integrity of  and  in line~\ref{alg2:server:valid-nonce}. In addition, in \textsc{repair}, the reader in  checks the integrity of . We distinguish two cases:
\begin{itemize}
\item Case 1: If  passes the integrity check in line~\ref{alg2:reader:vec-integrity}, then the integrity of  has been fully established. Hence, by the time  completes,  correct servers validated  in line~\ref{alg2:server:valid-filter} and changed  to  or to a higher valid candidate.
\item Case 2: If vector  fails the integrity check in line~\ref{alg2:reader:vec-integrity}, then in \textsc{repair},  is repaired in line~\ref{alg2:reader:repair} and subsequently written back to  correct servers. Hence, by the time  completes,  correct servers validated  in line~\ref{alg2:server:valid-repair} and changed  to  or to a higher valid candidate.
\end{itemize}
Consequently, in the \textsc{collect} round in  a valid candidate  such that  is included in , and by Lemma~\ref{la2:noexclusion},  is never excluded from . By line~\ref{alg2:reader:select}, no candidate with a timestamp lower than  is selected. Since , no candidate with a timestamp lower than  is selected in .
\end{prooff}

\begin{theo}[Atomicity]
Algorithms~\ref{alg2:writer},~\ref{alg2:server} and~\ref{alg2:reader} are atomic.
\end{theo}
\begin{prooff} This proof follows directly from Lemmas~\ref{la2:validity}~\ref{la2:powr}~\ref{la2:rwatomic}~\ref{la2:rratomic}.
\end{prooff}

We now proceed to proving wait-freedom.
\begin{theo}[Wait-freedom] Algorithms~\ref{alg2:writer},~\ref{alg2:server},and~\ref{alg2:reader} are wait-free.
\end{theo}
\begin{prooff} We show that no operation invoked by a correct client ever blocks. The wait-freedom argument of the \textsc{write} is straightforward; in every round, the writer awaits acks from the least number  of correct servers. The same argument holds for the \textsc{collect} and the \textsc{repair} rounds of the \textsc{read}. Hence, in the remainder of the proof, we show that no \textsc{read} blocks in the \textsc{filter} round. By contradiction, consider a \textsc{read}  by reader  that blocks during the \textsc{filter} round after receiving \textsc{filter\_ack} messages from all correct servers. We distinguish two cases: (Case 1)  includes a valid candidate and (Case 2)  includes no valid candidate.

\begin{itemize}
\item Case 1: Let  be the highest valid candidate included in . We show that \textsf{highcand}()  \textsf{safe}() holds. Since  is valid, by Lemma~\ref{la2:pow}, there exists a set  of  correct servers such that each server  changed  to . Thus, during the \textsc{filter} round, \textsf{validByHist}() holds at every server in . As no valid candidate in  has a higher timestamp than , \textit{(i)} all servers  (at least ) responded with timestamp , corresponding erasure coded fragment , cross-checksum  and repair vector  in line~\ref{alg2:server:ret} and \textit{(ii)} all correct servers (at least ) responded with timestamps at most . By \emph{(i)},  is \textsf{safe}. By \emph{(ii)}, every  such that  became \textsf{invalid} and was excluded from , implying that  is \textsf{highcand}.

\item Case 2: Here, we show that . As none of the candidates in  is valid, during \textsc{filter}, the integrity check in line~\ref{alg2:server:valid-nonce} failed for every candidate in  at all correct servers. Hence, at least  servers responded with timestamp . Since  is lower than any candidate timestamp, all candidates were classified as \textsf{invalid} and were excluded from .
\end{itemize}
\end{prooff}


\begin{theo}[Non-skipping Timestamps]
Algorithms~\ref{alg2:writer},~\ref{alg2:server} and~\ref{alg2:reader} implement non-skipping timestamps.
\end{theo}
\begin{prooff} By construction, a fabricated timestamp would fail the check in line~\ref{alg2:writer:ts-integrity}. Hence, no fabricated timestamp is ever used in a \textsc{write}. The Lemma then directly follows from the algorithm of \textsc{write}.
\end{prooff}


\begin{theo}[Latency]
Algorithms~\ref{alg2:writer},~\ref{alg2:server} and~\ref{alg2:reader} feature a latency of \emph{three} communication rounds for the \textsc{write} and \emph{two} for the \textsc{read} in the absence of attacks. In the worst case, the \textsc{read} latency is \emph{three} communication rounds.
\end{theo}
\begin{prooff} By Algorithm~\ref{alg2:writer}, the \textsc{write} completes after three rounds, \textsc{clock}, \textsc{store} and \textsc{\complete}, each taking one communication round. In the absence of attacks, by Algorithm~\ref{alg2:reader}, the \textsc{read} completes after two rounds, \textsc{collect} and \textsc{filter}, each taking one communication round. Under BigMac~\cite{CWADM09} attacks the \textsc{read} may go to the \textsc{repair} round, incurring one additional communication round.
\end{prooff}


\end{document}
