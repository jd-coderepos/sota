
\section{Experiment}\label{sec:exp}
We detail our experimental setup in \Cref{sec:exp_setups}, introduce the main results in \Cref{sec:main_results}, and results on additional Pix2Struct tasks in \Cref{sec:p2s_tasks}.

\subsection{Experimental Setups}\label{sec:exp_setups}

\paragraph{Pretraining datasets/tasks.} Overall, we create a mixture of pretraining task that has 40\% of math reasoning, 40\% of chart derendering, and 20\% screenshot parsing. The weight for specific task/dataset is listed in \Cref{tab:mixture_rate}. For chart derendering, we have four sources of data: (1) chart-table pairs synthesized by ourselves, (2) from ChartQA, (3) synthesized in PlotQA, and (4) chart-to-code data. We initially assigned equal weight to the four tasks however noticed training instability since chart-to-code is very hard (the pretraining data is noisy). We thus lower chart-to-code to 4\% and increase all chart-to-table tasks to 12\%. For math reasoning, we assign equal weights to MATH and DROP (both are 20\%).

For pretraining dataset ablation studies, see \Cref{sec:ablations}.

\begin{table}[t!]
    \small
    \centering
  \scalebox{0.85}{
    \begin{tabular}{llcc}
    \toprule
 Component &    Task/Dataset &  Rate & Size  \\
    \midrule
\multirow{2}{*}{\shortstack[c]{Math\\reasoning}}& MATH dataset & 20\% & 2M\\
& DROP & 20\% & 96K \\
\midrule
\multirow{4}{*}{\shortstack[c]{Chart\\derendering}} & Chart-to-code (GitHub; \textbf{ours}) & 4\% & 23M \\
& Chart-to-table (synthetic; \textbf{ours}) & 12\% & 270K \\
& Chart-to-table (ChartQA) &  12\% & 22K \\
& Chart-to-table (PlotQA) & 12\% & 224K \\
\midrule
Pix2Struct & Screenshot parsing &  20\% & 80M \\
\bottomrule
\end{tabular}
}
\caption{Mixture rates for all tasks in pretraining and the absolute size of each dataset. The mixture rate is used to sample each example within the batch.}
\label{tab:mixture_rate}
\end{table}

\begin{table}[t!]
    \small
    \centering
  \scalebox{0.85}{
    \begin{tabular}{llcc}
    \toprule
 Task &    Dataset & \# Tables & \# Pairs  \\
    \midrule
\multirow{4}{*}{\shortstack[c]{Chart\\Question\\Answering}} 
& ChartQA (Human) & 4.8K & 9.6K \\
& ChartQA (Machine) & 17.1K & 23.1K \\
& PlotQA (v1) & 224K & 8M \\
& PlotQA (v2) & 224K & 29M \\
\midrule
\multirow{2}{*}{\shortstack[c]{Chart\\Summarization}} & Chart-to-Text (Pew) & 9K & 9K \\
& Chart-to-Text (Statista) & 35K & 35K \\
\bottomrule
\end{tabular}
}
\caption{Statistics of the finetuning datasets.}
    \vspace{-0.5em}
\label{tab:finetune_data}
\end{table}

\paragraph{Evaluation datasets.} We evaluate \model{} on multimodal English QA and generation tasks including ChartQA \citep{masry-etal-2022-chartqa}, PlotQA \citep{methani2020plotqa},\footnote{There exists othear chart domain QA datasets such as DVQA \citep{kafle2018dvqa} and FigureQA \citep{kahou2017figureqa}. However, they are both synthetic and SOTA models have already reached $>95\%$ accuracy. We thus focus on more challenging datasets.} and Chart-to-Text summarization \citep{kantharaj-etal-2022-chart}. Both ChartQA and PlotQA are chart domain QA datasets where the input is an image of a chart and a query and the target is an answer string. ChartQA has two subsets: (1) augmented and (2) human where the augmented set is machine generated and thus more extractive and the human set is human written and requires more complex reasoning. PlotQA also has two sets v1 and v2. Similarly, v1 focuses more on extractive questions and v2 requires more numerical reasoning. However, both v1 and v2 are machine generated. Chart-to-Text has two sets as well. They are ``Pew'' and ``Statista'' where the names describe the source of the image examples. For Pew, the gold summaries are automatically extracted from areas around the image. For Statista, the summaries are human written. The sizes of each dataset are described in
~\Cref{tab:finetune_data}.




Beyond chart domain datasets, we additionally evaluate on other datasets used in Pix2Struct \citep{lee2022pix2struct}. We follow the exact same setups and protocols of Pix2Struct by rerunning Pix2Struct experiments but replacing the initial checkpoint with \model{}. See \citet{lee2022pix2struct} for more experimental details. 

\paragraph{Metrics.} For ChartQA and PlotQA, following previous works \citep{masry-etal-2022-chartqa,methani2020plotqa,lee2022pix2struct}, we use relaxed correctness (exact match but tolerating 5\% of numerical error). For Chart-to-Text, we use BLEU4. For all Pix2Struct experiments, we use identical metrics introduced in \citet{lee2022pix2struct}.

\paragraph{Training and inference details.}
We save checkpoint every 200 steps and keep the checkpoint that produces the highest validation score.
Following \citet{lee2022pix2struct}, we finetune models on the ChartQA aug. and human sets together (i.e., one checkpoint for two sets) and use the checkpoint selected on human val set as the final checkpoint for testing.
For PlotQA and Chart-to-Text, we train standalone models for v1, v2, Pew, and Statista sets. For pretraining, we use a batch size of 512 and max sequence length of 192. We pretrain for 100k steps and the final \model{} checkpoint is selected at the 90k step (where the average exact match validation score is the highest).
For downstream tasks finetuning, we use a batch size of 256 and max sequence length of 128.
For ChartQA and Chart-to-Text we finetune for 10k steps and for PlotQA we finetune for 20k steps (since it is significantly larger). Setups for Pix2Struct tasks are the same as the original paper. As for the PaLI baselines, we use the larger 17B variant and finetune for 5k steps and save checkpoints every 1000 steps. All \model{} and Pix2Struct models are pretrained/finetuned with 64 GCP-TPUv3 while PaLI models are finetuned with 128 GCP-TPUv4.

Note that since \model{} is an image-to-text model (without a textual input branch), whenever it is required to input text to the model, the text is rendered as an image. As an example, for QA tasks, we prepend the question as a header above the chart and input the image with question header as a whole to the model. 


\subsection{Main Results}\label{sec:main_results}

\begin{table*}[!ht]
    \centering
  \scalebox{0.9}{
    \begin{tabular}{lccccccccccccc}
    \toprule
& \multirow{2}{*}{\shortstack[c]{Gold\\Table?}} & \multicolumn{3}{c}{ChartQA} & & \multicolumn{3}{c}{PlotQA} & &  \multicolumn{3}{c}{Chart-to-Text} & \multirow{2}{*}{\shortstack[c]{avg.\\(all)}}  \\ 
     \cline{3-5}      \cline{7-9}   \cline{11-13}
Model &   &  aug. & human & avg. & & v1 & v2 & avg. & & Pew & Statista & avg. & \\
\midrule
T5 & yes & - & - & 59.8 & & 93.2 & 85.6 & 89.4 & & - & 37.0 & - & -\\
VL-T5 & yes & - & - & 59.1 & & \textbf{96.4} & 84.7 & 90.6 & & - & - & - & -\\
VisionTaPas  & yes & - & - & 61.8 & & 80.2 & 58.3 & 69.3 & & - & - & - & -\\
\midrule 
CRCT & no & - & - & - & & 76.9 & 34.4 & 55.7 & & - & - & - & - \\
VL-T5-OCR  & no & - & - & 41.6 & & 75.9 & 56.0 & 66.0 & & - & - & - & -\\
T5-OCR & no & - & - & 41.0 & & 72.6 & 56.2 & 64.4 & & 10.5 & 35.3 & 22.9 & 42.8 \\
VisionTaPas-OCR & no & - & - & 45.5 & & 65.3 & 42.5 & 53.9 & & - & - & - & - \\
PaLI-17B (res. 224) & no & 11.2  & 15.2 & 13.2 & & 56.9 & 13.1 & 35.0 & & 10.0 & 40.2 & 25.1 & 24.4 \\
PaLI-17B (res. 588) & no & 64.9 & 30.4 & 47.6 & & 64.5 & 15.2 & 39.8 & & 11.2 & \textbf{41.4} & \textbf{26.3} & 37.9 \\
Pix2Struct & no & 81.6 & 30.5 & 56.0 & & 73.2 & 71.9 & 72.5 & & 10.3 & 38.0 & 24.2 & 50.9 \\
\rowcolor{applegreen!20} \model{}  & no &  \textbf{90.2} & \textbf{38.2} & \textbf{64.2} & & 92.3 & \textbf{90.7} & \textbf{91.5} & &  \textbf{12.2} & 39.4 & 25.8 & \textbf{60.5} \\
\bottomrule
\end{tabular}
}
\caption{Main experimental results on two chart QA benchmarks ChartQA \& PlotQA and a chart summarization benchmark Chart-to-Text. Detailed introduction of the baselines can be found in \Cref{sec:baselines}.}
\label{tab:main}
\end{table*}


We summarize the main results in \Cref{tab:main} where we compare \model{} with a wide range of baselines and SOTA models\footnote{For brief introduction of baselines used, please see \Cref{sec:baselines}.} across three chart/plot-domain benchmarks ChartQA, PlotQA, and Chart-to-Text Summarization. On ChartQA, \model{} beats the previous SOTA (without access to the underlying gold data table) Pix2Struct by 8.2\%. Even if we consider models that do assume the existence of gold data tables, they generally underperform \model{} by 3-5\%. The best performing baseline VisionTaPas has a specialized module for modeling tables but still lags behind \model{} by 2.4\%. On PlotQA, \model{} is again the best performing model overall. On the v1 set, VL-T5 with access to underlying data table performs better than \model{} by $\approx4\%$ which is intuitive since PlotQA is a synthetic dataset thus containing relative simple queries and the v1 is the extractive set where queries are even more straightforward. On v2 where questions are related to numerical reasoning, \model{} outperforms all models including the models with access to underlying gold tables.  On Chart-to-Text summarization, \model{} improves upon Pix2Struct on both Pew and Staista and is the new SOTA on Pew. However, \model{} underperforms PaLI-17B (res. 588) on Statista.

Overall, \model{} is clearly the best-performing model with SOTA or competitive performance on every setup and all tasks. All baselines without access to gold tables lag behind significantly -- \model{} outperforms the strongest baseline without gold table access Pix2Struct by $\approx10\%$ if we average the performance scores across all datasets.

Among the baselines, we would like to highlight PaLI which is the SOTA for a series of multimodal text-image tasks such as VQA and captioning on natural images and is of a much larger size (i.e., 17B parameters vs. 300M in \textsc{MatCha}). PaLI fails significantly on ChartQA and PlotQA since the challenge in the visual language is distinctly different from that in the natural image domain. Increasing input resolution substantially helps the model's performance (likely due to the better text reading with higher resolution) but this also increases the sequence length (thus also memory and compute) quadratically. PaLI performs reasonably well in Chart-to-Text. We believe this is because the Chart-to-Text task (evaluated by BLEU4) might be more sensitive to textual fluency but less sensitive to factuality as compared with the other two QA tasks. It is expected that PaLI trained with a language modeling objective on natural text will have more advantage under this evaluation setup. 



\subsection{Results on Pix2Struct Tasks}\label{sec:p2s_tasks}

Besides chart/plot domain datasets, we would also like to examine if \model{} transfers to other visual language datasets such as documents, user interfaces, and natural images. We rerun all Pix2Struct finetuning experiments with a \model{} checkpoint and the results are shown in \Cref{tab:matcha_vs_p2s}. On average across all tasks, \model{} outperforms Pix2Struct by 2.3\%. Besides ChartQA, the improvement is also observed in AI2D (QA on textbook diagrams), Widget Captioning (recognizing and captioning widgets in screenshots), DocVQA (QA on scanned documents), etc. Even if we exlucde ChartQA, \model{} can outperform Pix2Struct by 1.6\% on average, suggesting that knowledge learned through \model{} pretraining can be transferred to visual language domains out side of plots/charts.

\begin{table*}[!ht]
    \centering
\setlength{\tabcolsep}{5pt}
  \scalebox{0.9}{
    \begin{tabular}{lccccccccccc}
    \toprule
\multirow{2}{*}{Tasks$\rightarrow$} & \multirow{2}{*}{ChartQA} & \multirow{2}{*}{AI2D} & \multirow{2}{*}{\shortstack[c]{OCR-\\VQA}} & \multirow{2}{*}{RefExp} & \multirow{2}{*}{\shortstack[c]{Widget-\\Cap}} & \multirow{2}{*}{\shortstack[c]{Screen-\\2Words}} & \multirow{2}{*}{\shortstack[c]{Text-\\Caps}} & \multirow{2}{*}{\shortstack[c]{Doc-\\VQA}} & \multirow{2}{*}{\shortstack[c]{Info-\\VQA}} & \multirow{2}{*}{avg.} & \multirow{2}{*}{\shortstack[c]{avg. (excl.\\ChartQA)}} \\
 & & \\
\midrule
Pix2Struct & 56.0 & 40.9 & \textbf{69.4} & 92.2 & 133.1 & \textbf{107.0} & 88.0 & 72.1 & \textbf{38.2} & 77.4 & 80.1 \\
\rowcolor{applegreen!20} \model{} & \textbf{64.2} & \textbf{42.6} & 68.9 & \textbf{94.2} & \textbf{137.7} & 106.2 & \textbf{92.4} & \textbf{74.2} & 37.2 & \textbf{79.7} & \textbf{81.7} \\
\bottomrule
\end{tabular}
}
\caption{\model{} vs. Pix2Sturct on Pix2Sturct tasks.}
\label{tab:matcha_vs_p2s}
\end{table*}


\section{Analyses and Discussions}
In this section, we first conduct pretraining ablations in  \Cref{sec:ablations} to understand the usefulness of each pretraining component, then in \Cref{sec:finegrained_analysis} we conduct fine-grained analysis and error analysis to probe \model' strengths and weaknesses.


\subsection{Ablation Study}\label{sec:ablations}

\begin{table}[!ht]
\centering
\setlength{\tabcolsep}{2.2pt}
  \scalebox{0.9}{
    \begin{tabular}{lccccccc}
    \toprule
Setup$\downarrow$ &  aug. &  human &  avg. \\
\midrule
\model{} (full; 50k steps) & 88.6 & \textbf{37.4} & \textbf{63.0} \\
\midrule
\multicolumn{4}{c}{\emph{Component-level ablations}} \\
\midrule
- no math reasoning & 88.2 & 33.0 & 60.6\\
- no chart derendering & 83.7 & 34.4 & 59.1\\
- no Pix2Struct screenshot parsing & 87.8 & 34.9 & 61.4 \\
\midrule
\multicolumn{4}{c}{\emph{Single-task ablations}} \\
\midrule
- no MATH dataset & 88.2 & 36.7 & 62.5 \\
- no DROP dataset & 88.2 & 34.3 & 61.3 \\
- no real-world chart-table pairs & 87.4 & 34.5 & 61.0\\
- no chart-to-code & \textbf{89.1} & 34.6 & 61.9 \\
\bottomrule
\end{tabular}
}
\caption{\model{} pretraining ablations on ChartQA.}
\label{tab:ablations}
\end{table}

We conduct two types of ablations. First, we remove a whole type of pretraining datasets. For example, `no math reasoning' means removing the whole math reasoning component and drops the MATH and DROP datasets. The weights of other datasets in the mixture are proportionally increased. Second, we remove an individual dataset within a component. For example, `no MATH dataset' means removing just MATH dataset but keep other datasets in the math reasoning component untouched. In this scenario, we increase the weight of other math datasets (in this case just DROP) proportionally to maintain the overall weight of the component in the mixture. To reduce compute used, we train one full \model{} model and all its ablated models with 50k steps (the original full \model{} is trained for 100k steps). As a result the \model{} model performance in \Cref{tab:ablations} is slightly lower than the 100k model (63.0 vs. 64.2). The pretrained models are then finetuned and evaluated on ChartQA only.
The full ablation study table is shown in \Cref{tab:ablations} where the first half is component-level ablations and the second half is individual dataset ablation.

\paragraph{The impact of each pretraining component.} On the component-level, we found that removing any major component (math reasoning, chart derendering, and screenshot parsing) would cause a performance drop. The most important component is chart derendering, the removal of which causes a decrease of $\approx 4\%$ averaging across the two sets. Removing math reasoning decreases the avg. score by 2.4\% and removing the continual pretraining of screenshot parsing causes a drop of 1.6\%. We notice that math reasoning is more important to the human set while chart derendering is more important on the augmented set. The findings are likely due to the fact that the human set contains more numerical reasoning questions while the augmented set contains more extractive questions. We also conducted ablations of specific datasets/tasks which we discuss in paragraphs below.



\paragraph{MATH vs. DROP dataset for learning to reasoning.} We have used two datasets, i.e. MATH and DROP, for injecting numerical reasoning capabilities to \model{}. According to \Cref{tab:ablations}, we observe that DROP seems to be more important (the removal of which causes a performance drop of 1.7\% vs. a drop of 0.5\% from the removal of MATH). We conjecture that it is because the reading-comprehension-QA format of DROP is more similar to the downstream task of QA on visual language, where information extraction and reasoning needs to be jointly performed.

\paragraph{Synthetic vs. real-world corpus as pretraining chart-table pairs.} 
We perform another ablation to justify the choice of chart derendering pretraining corpus. Real-world chart-table pairs can increase the diversity and coverage of chart derendering pretraining however we need to explicitly scrape such data from the web. We are interested in understanding to what extent our manually synthesized charts and plots with existing libraries can improve model's performance. The row `no real-world chart-table pairs' shows results of only using synthesized chart-table data by us (i.e., no ChartQA and PlotQA chart-table data). The overall performance drops by 2\%. Interestingly, for the augmented set, the performance only drops 1.2\% but almost 3\% is dropped on the human set. This indicates that extractive questions can usually be solved with synthetic pretraining but the more diverse real-world data (also usually having more sophisticated layout) can benefit reasoning learning more.

\paragraph{The impact of chart-to-code pretraining.} While much of the information in a chart is provided by data table, the code that is used to render the table decides the visual layout (e.g., type of chart and orientation) and attributes (e.g., color) of the data. To test the importance of the chart-to-code pretraining component, we remove it in an ablated pretrained model and the model performance on ChartQA drops by 1.1\% overall. The drop is mainly on the human set where more complex reasoning is required.

\subsection{Fine-grained Analysis and Error Analysis}\label{sec:finegrained_analysis}

\paragraph{Fine-grained analysis.}
To understand the specific aspects of strengths and weaknesses of the models and breakdown the challenges into fine-grained categories, we sample 100 test examples from ChartQA (both augmented and human sets) for further analyses. Specifically, we summarize the challenges of ChartQA into three categories: (1) data extraction (where the model needs to parse a complex query with sophisticated coreference resolution or needs to read numbers when numbers are not explicitly written out), (2) math reasoning (where the model needs to perform one or multiple numerical operations such as min/max/sort/average/etc.), and (3) plot attributes (where the query asks about color/shape/location of specific objects/labels). We manually classify the 100 examples into the three categories and allow an instance to belong to multiple categories when the challenge is multifaceted. After excluding 7 annotation errors, we find 55.9\% questions need complex data extraction, 45.2\% involve math reasoning, and 7.5\% concern plot attributes. We plot the per-category performance of PaLI (res. 588), Pix2Struct and \model{} in \Cref{fig:finegrained}. Overall, all models perform the best on data extraction while math reasoning and plot attributes are more challenging. When compared across models, \model{} improves Pix2Struct in every category and beats PaLI in both data extraction and math reasoning. However, for plot attributes, \model{} lags behind PaLI. This is not significantly reflected in the overall ChartQA performance since plot attribute only concerns less than 10\% of the examples.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.96\linewidth]{figs/matcha_perf_per_cat_v2.png}
\caption{Fine-grained by-category performance comparison on ChartQA.}
\label{fig:finegrained}
\end{figure}


\paragraph{Error analysis.} 
Similar to the fine-grained analysis, we sample 100 errors made by \model{} on ChartQA test set and manually classify the 100 errors into the three categories. After exluding 21 annotation errors, we find 48.3\% of the errors are related to math reasoning, 43.4\% are related to data extraction, and 8.0\% concern plot attributes. We conclude that math reasoning remains to be the major challenge even if \model{} has improved its math reasoning capability compared with Pix2Struct and PaLI. We notice that \model{} still struggles with sophisticated math reasoning questions or numerical computation that requires high precision. An example is shown in Appendix \Cref{tab:hard_numer}.

\paragraph{Case study.} To concretely understand what type of questions \model{} can do better than the baselines, we present several case studies. In \Cref{tab:case_numerical}, we show an example which requires computing average of multiple numbers. Besides \model, PaLI and Pix2Struct's answers are far from the ground truth. In \Cref{tab:case_coref}, we demonstrate an example that requires resolving complex coreference resolution of multiple data points. The model needs to accurately parse the query and find the referenced data points in the chart, then perform a simple numerical computation. \model{} is the only model that gets the correct answer.

\begin{table}[ht!]
  \centering
  \small
  \begin{tabular}{c}
    \begin{minipage}{.45\textwidth}
      \frame{\includegraphics[width=\linewidth, height=75mm]{figs/case_study_numerical_question.png}}
    \end{minipage} 
    \\
    \\
    \begin{minipage}[t]{.45\textwidth}
    \texttt{What is the average of last 4 countries' data?} \\
    PaLI: \textcolor{red}{\textbf{40.94}} \ \ \ Pix2Struct: \textcolor{red}{\textbf{40.5}} \ \ \ \model: \textcolor[rgb]{0.4,0.8,0}{\textbf{50.5}}
    \end{minipage}
  \end{tabular}
  \caption{An example that requires strong numerical reasoning skills. \textcolor{red}{\textbf{Red}} and \textcolor[rgb]{0.4,0.8,0}{\textbf{green}} indicate correct and wrong answers respectively.}\label{tab:case_numerical}
\end{table}



\begin{table}[ht!]
  \centering
  \small
  \begin{tabular}{c}
    \begin{minipage}{.48\textwidth}
      \frame{\includegraphics[width=\linewidth]{figs/case_study_coref_and_numerical.png}}
    \end{minipage} 
    \\
    \\
    \begin{minipage}[t]{.48\textwidth}
    \texttt{What percentage does 'don't known' and 'just the right number' make up for Oct'17?} \\
    PaLI: \textcolor{red}{\textbf{10}}\ \ \ Pix2Struct: \textcolor{red}{\textbf{21}} \ \ \ \model: \textcolor[rgb]{0.4,0.8,0}{\textbf{63}}
    \end{minipage}
  \end{tabular}
  \caption{An example that requires resolving both coreference resolution and math reasoning.}\label{tab:case_coref}
\end{table}

Besides cases where \model{} succeeded, we also present an example where all models have failed (\Cref{tab:hard_numer}). Questions which require very accurate numerical computation are still very challenging to \model.

\begin{table}[ht!]
  \centering
  \small
  \begin{tabular}{c}
    \begin{minipage}{.48\textwidth}
      \frame{\includegraphics[width=\linewidth]{figs/case_study_errors.png}}
    \end{minipage} 
    \\
    \\
    \begin{minipage}[t]{.48\textwidth}
    \texttt{Is the sum of all last three places more than Oceania?} \\
    PaLI: \textcolor{red}{\textbf{Yes}}\ \ \ Pix2Struct: \textcolor{red}{\textbf{Yes}} \ \ \ \model: \textcolor{red}{\textbf{Yes}}
    \end{minipage}
  \end{tabular}
  \caption{An error made by all models including \model{} which requires very accurate numerical computation. The answer should be `No' since 6.67+5.8+5.63=18.1<18.18.}\label{tab:hard_numer}
\end{table}


\paragraph{Continue pretraining Pix2Struct with its original objective.}
It is commonly known that BERT \citep{devlin-etal-2019-bert} is undertrained and simply continuing training BERT with the same objective and on the same data for longer can slightly improve a model's performance \citep{liu2019roberta}. To understand whether such phenomenon persists for \model{} and to what extent does continue pretraining on Pix2Struct screenshot parsing task would improve the model's performance, we continue pretraining Pix2Struct with its original objective and data for 50k steps. We found that continue pretraining indeed improves Pix2Struct's performance (56.0$\rightarrow$57.0 on ChartQA) but is to a much less extent without using the \model{} pretraining components (improving from 56.0 to 64.2).