

\documentclass[10pt,twocolumn,letterpaper]{article}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[pagenumbers]{wacv} 

\usepackage{times}
\usepackage{epsfig}

\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{color, colortbl}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{threeparttable}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{appendix}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{adjustbox}
\usepackage{units}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\wacvPaperID{447} \def\confName{WACV}
\def\confYear{2024}

\def\etal{\textit{et al. }}
\definecolor{Gray}{gray}{0.9}

\begin{document}

\title{Focusing on what to decode and what to train: Efficient Training with HOI Split Decoders and Specific Target Guided DeNoising}

\author{Junwen Chen\qquad Yingcheng Wang \qquad Keiji Yanai\\
Department of Informatics, The University of Electro-Communications, Tokyo, Japan\\
{\tt\small chen-j@mm.inf.uec.ac.jp, wang-y@mm.inf.uec.ac.jp, yanai@cs.uec.ac.jp}
}
\maketitle

\begin{abstract}
    Recent one-stage transformer-based methods achieve notable gains in the Human-object Interaction Detection (HOI) task by leveraging the detection of DETR.
    However, the current methods redirect the detection target of the object decoder, and the box target is not explicitly separated from the query embeddings, which leads to long and hard training.
    Furthermore, matching the predicted HOI instances with the ground-truth is more challenging than object detection, simply adapting training strategies from the object detection makes the training more difficult.
    To clear the ambiguity between human and object detection and share the prediction burden, we propose a novel one-stage framework (SOV), which consists of a subject decoder, an object decoder, and a verb decoder.
    Moreover, we propose a novel Specific Target Guided (STG) DeNoising training strategy, which leverages learnable object and verb label embeddings to guide the training and accelerate the training convergence.
    In addition, for the inference part, the label-specific information is directly fed into the decoders by initializing the query embeddings from the learnable label embeddings.
    Without additional features or prior language knowledge, our method (SOV-STG) achieves higher accuracy than the state-of-the-art method in one-third of training epochs.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Recent Human-Object Interaction (HOI) detection studies are mainly built on the object detection framework.
The most widely used datasets, HICO-DET~\cite{chao2018learning} and V-COCO~\cite{gupta2015visual}, share the same object categories as the MS-COCO dataset~\cite{lin2014microsoft}.
Following the definition of the HOI instance , which is a tuple of the subject (human) box , the object box  with class , and the verb class , detecting methods are split into one-stage and two-stage methods. 
In the beginning, a multi-stream architecture built on top of a CNN-based object detector is commonly adopted in the two-stage methods~\cite{chao2018learning,gkioxari2018detecting,qi2018learning,gao2018ican}.
Multi-stream methods resolve the HOI detection problem in split parts and have a great potential to improve.
By introducing the human pose information~\cite{kim2020detecting,li2020detailed,zhong2021polysemy}, the language priors~\cite{gao2020drg,zhong2021polysemy}, or graph structure~\cite{gao2020drg,ulutan2020vsgnet,zhang2020spatio}, CNN-based two-stage methods achieve considerable accuracy.
On the other hand, CNN-based one-stage methods~\cite{liao2020ppdm,zhong2021glance,wang2020learning} leverage interaction points to detect possible interaction between the subject and object and achieve promising performance.
\begin{figure}[!t]
    \centering
    \resizebox{\linewidth}{!}{
        \includegraphics[]{img/SOV-STG-pipeline.jpg}
    }
    \caption{
        \textbf{End-to-end training pipeline of our SOV-STG.} Our SOV framework splits the decoding process into three parts for each element of the HOI instance.
        Our STG training strategy efficiently transfers the ground-truth information to label embeddings through additional denoising queries.
    }
    \label{fig:SOV_STG_pipeline}
\end{figure}
\begin{figure}[!t]
    \centering
    \resizebox{\linewidth}{!}{
        \includegraphics[]{code/ap_compare_sota.png}
    }
    \caption{
        Comparison of the training convergence curves of the state-of-the-art methods on the HICO-DET dataset.
        The mAP is evaluated under the default setting of HICO-DET.
    }
    \label{fig:ap_compare_sota}
\end{figure}
The attention mechanism of the transformer is more flexible than the CNN architecture in handling the relationships of features at different locations in the feature map and extracting global context information~\cite{dosovitskiy2021an}.
At first, the transformer-based methods~\cite{tamura2021qpic,zou2021end,chen2021reformulating,kim2021hotr} show the advantage of the attention mechanism by adopting DETR~\cite{carion2020end} in the HOI detection task.
QPIC~\cite{tamura2021qpic} and HOITrans~\cite{zou2021end} follow the same training pipeline as the DETR by viewing the HOI detection problem as a set prediction problem.
Without the matching process in one-stage and two-stage CNN-based methods, QPIC and HOITrans adopt a compact encoder-decoder architecture to predict the HOI instances directly.
However, the compact architecture with a single decoder binds the feature of the subject and object localization and verb recognition together.
As a result, even leveraging the DETR model pretrained on the COCO dataset, the finetuning for QPIC and HOITrans still needs 150 and 250 epochs.
Following one-stage methods~\cite{zhang2021mining,liao2022gen,yuan2022detecting,iftekhar2022look,zhou2022human,yuan2022rlip} improve the single decoder design by disentangling the object localization and the verb recognition in a cascade manner.
Specifically, GEN-VLKT~\cite{liao2022gen} improves the cascade decoder design of CDN~\cite{zhang2021mining} by introducing two isolated queries of humans and objects in an instance decoder and fusing the human and object features in an interaction decoder.
However, the subject and object detection are still tangled in the instance decoder, and the spatial information is implicitly represented by the query embeddings.
Consequently, the training of GEN-VLKT is still hard and slow, which needs 90 epochs.
On the other hand, the two-stage transformer-based methods~\cite{Zhang_2022_CVPR,Liu_2022_CVPR,zhang2022exploring} stack additional interaction pair detection modules on top of the object decoder without modifying the subject and object detection part.
Thus, compared with one-stage methods, two-stage methods can focus on filtering the interaction pairs and achieve higher accuracy than the one-stage transformer-based methods with fewer training epochs.

To relieve the training burden caused by the ambiguity of the query and decoding process in recent one-stage methods~\cite{tamura2021qpic,zhang2021mining,liao2022gen}, our motivation can be summarized in two aspects: 1) how to focus on decoding specific targets and 2) how to guide the training with specific priors.
For the first aspect, we revisit the decoding pipeline of the transformer-based method.
Recent one-stage methods~\cite{tamura2021qpic,zhang2021mining,liao2022gen} redirect the decoding target of the decoder pretrained from the object detection task, which leads to slow training convergence.
To this end, as shown in Figure~\ref{fig:SOV_STG_pipeline}, according to the definition of the HOI instance, we propose a new framework (SOV), which fully splits the decoding process into three parts: \textbf{S}ubject detection, \textbf{O}bject detection, and \textbf{V}erb recognition.
Specifically, the object decoder, subject decoder, and verb decoder are assigned to decode the object, subject, and verb class, respectively.
Furthermore, the spatial information (anchor boxes) and label information (label queries) are explicitly disentangled and fed into the decoders to guide the feature extraction.
By doing so, each decoder can focus on specific targets and share the training burden.
In Figure~\ref{fig:ap_compare_sota}, we compare the training convergence with recent SOTA methods.
From the results, SOV takes advantage of the balanced decoding pipeline and the training converges faster than the current SOTA methods.
Moreover, the object detection part of SOV is the same as the pretrained detection model, which makes the training more stable and achieves a notable high accuracy at the early stage of the training.

For the second aspect, we focus on how to obtain specific label priors to initialize the label queries for HOI detection with an effective training strategy.
As shown in Figure~\ref{fig:SOV_STG_pipeline}, we introduce a novel \textbf{S}pcific \textbf{T}arget \textbf{G}uided (STG) denoising training strategy for HOI detection, which constructs a connection between the ground-truth label information and predefined label priors (embeddings) to guide the training.
With the specific priors, the queries of the inference part are able to be represented as the weighted sum of the label embeddings by the learnable coefficient matrices.
Moreover, we leverage the verb label embeddings to guide the verb recognition in the verb recognition part to improve the verb representation learning capabilities.
In Figure~\ref{fig:ap_compare_sota}, we illustrate the training convergence of SOV and QPIC with STG, and the results show that our STG strategy effectively accelerates the training convergence before the learning rate drops and finally improves the performance.



In summary, our contributions are mainly in two aspects: (1) we propose a novel one-stage framework (SOV) to enable the model to concentrate on what to detect and what to recognize;
(2) we propose a novel training strategy (STG) to allow the model to learn label-specific information from the ground-truth.
With the SOV framework design and the STG training strategy, we achieve a new state-of-the-art performance on the HOI detection benchmark with 3× fewer training epochs (30 epochs on HICO-DET) than the current state-of-the-art method.



\begin{figure*}[!ht]
    \centering
    \resizebox{\linewidth}{!}{
        \includegraphics[]{img/SOV-STG-overview.jpg}
    }
    \caption{
        \textbf{The overall framework of our SOV-STG.}
        SOV is composed of the feature extractor and SOV decoders.
The label embeddings  and  learned by our STG training strategy are used to initialize the label queries  with learned coefficient matrices  and .
        The subject and object decoder leverage the learnable anchor boxes  and  to predict the subject and object boxes, and the verb boxes  are generated by the adaptive shifted MBR according to the subject and object boxes.
}
    \label{fig:overall_architecture}
\end{figure*}

\section{Related Work}
\noindent{\textbf{Predicting interactions with specific priors.}}\quad For one-stage transformer-baesd methods, how to extract the interaction information under a predefined representation of the interaction region is a key issue.
Recent studies~\cite{cjw_qahoi,ma2023fgahoi,Kim_2022_CVPR} attempt to leverage the deformable attention mechanism~\cite{zhu2020deformable} to guide the decoding by reference points.
QAHOI~\cite{cjw_qahoi} and FGAHOI~\cite{ma2023fgahoi} view the deformable transformer decoder's reference point as the HOI instance's anchor and use the anchor to guide the subject and object detection.
However, QAHOI and FGAHOI still use the HOI query embeddings to predict all the elements of the HOI instance.
MSTR~\cite{Kim_2022_CVPR} proposes to use the subject, object, and context reference points to represent the HOI instance and predict the subject, object, and verb based on the reference points.
The context reference point is defined as the center of the subject and object reference point, which follows the idea of the interaction point~\cite{liao2020ppdm,wang2020learning,zhong2021glance}.
Nevertheless, the query embedding in MSTR is used to predict the final boxes and labels of the HOI instance and still suffers from ambiguous representations.
Besides, QAHOI, FGAHOI, and MSTR use x-y coordinates as the spatial priors to guide the decoding, while the box size priors are not considered.
In contrast, our SOV explicitly defines the subject and object anchor boxes as the spatial priors and refines the anchor boxes layer by layer.
Moreover, for the verb recognition part of our SOV, we introduce a verb box, which is directly generated from the subject and object boxes, to guide the verb feature extraction.

\noindent{\textbf{Effective learning with ground-truth guided.}}\quad For the object detection methods of the DETR family~\cite{carion2020end,zhu2020deformable,liu2022dabdetr}, DN-DETR~\cite{Li_2022_CVPR} shows that using the ground-truth information to guide the training can accelerate the training convergence and improve the performance.
In the HOI detection task, HQM~\cite{zhong2022towards} encodes the shifted ground-truth boxes as hard-positive queries to guide the training.
However, the ground-truth label information is not considered in HQM.
DOQ~\cite{qu2022distillation} introduces the oracle queries to implicitly encode the ground-truth boxes of human-object pairs and the object labels, and guide the decoder to learn to reconstruct the ground-truth HOI instances.
However, DOQ implicitly encodes the same number of oracle queries as the ground-truth with learnable weights and only uses the learned weights during training.
Without a complete and clear usage of ground-truth information, both HQM and DOQ still need 80 epochs to converge.
Different from DOQ and HQM, we introduce denoising queries to encode the ground-truth information and guide the training.
Moreover, our STG is used to learn the label priors for our model, and we intuitively use a "select" and a "weighted sum" approach to transfer the ground-truth label information to the denoising queries and inference queries, respectively.



\section{HOI Efficient Decoding and Training}

Figure~\ref{fig:overall_architecture} shows the overall architecture of our framework.
First, a feature extractor is used to extract the multi-scale global features.
Then, the global features are fed into the object and subject decoder with learnable anchor boxes and label queries to predict pairs of subjects and objects.
The label queries are initialized by the label embeddings and learnable coefficient matrices.
The STG training strategy (In Sec~\ref{sec:split_label_embedding}) is used to learn the label embeddings with the ground-truth information.
Finally, pairs of subject and object embeddings and boxes are fed into the verb recognition part to predict the verb classes.
The \textbf{S}ubject-\textbf{O}bject (S-O) attention module and \textbf{A}daptive \textbf{S}hifted \textbf{M}inimum \textbf{B}ounding \textbf{R}ectangle (ASMBR) used to fuse the subject and object embeddings and generate the verb box for the verb decoder are introduced in Sec~\ref{sec:split_decoder}.


\begin{figure}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
        \includegraphics[]{img/so_attn.jpg}
    }
    \caption{
        \textbf{Illustration of S-O attention module.}
        The S-O attention consists of three parts: a sum operation to fuse the subject and object embeddings, a cross-attention layer to integrate the verb priors, and a bottom-up path to amplify the intermidiate embeddings.
    }
    \label{fig:so_attn}
\end{figure}

\subsection{HOI split decoders}
\label{sec:split_decoder}
To clarify the decoding target, the design of the split decoders is crucial for our framework.
Different from recent one-stage transformer-based methods~\cite{tamura2021qpic,zhang2021mining,liao2022gen}, which use a single decoder to detect objects and subjects,
We split the detection part into two decoders, the subject decoder, and the object decoder, and share the prediction burden of the verb decoder.
Moreover, we explore the design of the multi-branch feature fusion module and a new way to represent the interaction region for the verb decoder.

\noindent{\textbf{Subject Decoder and Object Decoder.}}\quad The same as recent deformable transformer based methods~\cite{cjw_qahoi,Kim_2022_CVPR,ma2023fgahoi}, we leverage a hierarchical backbone and deformable transformer encoder~\cite{zhu2020deformable} as the feature extractor to extract the multi-scale global features , where  is the number of the total pixels of the multi-scale feature maps and  is the hidden dimension of the embeddings in the whole transformer architecture.
For the decoders, we adopt an improved deformable transformer decoder proposed in the recent object detection method~\cite{liu2022dabdetr}, which is able to process the label queries with the constraint of anchor boxes.
As shown in Figure~\ref{fig:overall_architecture}, the global features are fed into the subject and object decoder with the learnable anchor boxes.
To maintain the detection capability of the object detector, the object decoder with the feed-forward heads is the same as the one trained in the detection task.
Furthermore, we clone the object decoder to initialize the subject decoder and alleviate the learning burden of the subject decoder.
The subject and object decoder both use the label queries  as the input queries, where  is the number of queries and  is the hidden dimension of the embeddings.
With the same query input, the pair of subject embeddings  and object embeddings  can share the same prior label information.
Besides, the subject and object embeddings with corresponding learnable anchor boxes  and  are updated layer by layer during decoding.
Then, the object embeddings from the object decoder are used to predict the object classes, and the subject and object boxes are used to generate the verb boxes .
Next, the object and subject embeddings are fed into the S-O attention module to fuse the verb embeddings.
Finally, the verb boxes generated from the subject and object boxes with the verb embeddings are fed into the verb recognition part to predict the verb classes.

\noindent{\textbf{Verb Decoder with S-O attention module.}}\quad As shown in Figure~\ref{fig:overall_architecture}, the verb decoder receives the input queries from the outputs of the subject and object decoder.
For the label queries used in the verb decoder, we introduce S-O attention module to fuse the subject and object embeddings in a multi-layer manner.
In Figure~\ref{fig:so_attn}, we illustrate the fusion process of our S-O attention module.
Given the subject embedding  and object embedding  from the -th layer (), first, we sum the subject and object embeddings to fuse an intermidiate embedding .
Then, to guide the verb recognition with our predefined label priors, the intermidiate embeddings  are used to absorb the prior knowledge from the learnable verb label embeddings  (in Sec~\ref{sec:split_label_embedding}).
Specifically, we use  as the query and  as the key and value to perform a cross-attention operation.
Furthermore, we introduce a bottom-up path to amplify the information from the bottom to the top layer.
Finally, the verb embedding  after the bottom-up path can be defined as:

Then, the verb embeddings from the last layer are fed into the verb decoder to further extract the global semantic information based on the global feature  and the verb boxes.

\begin{figure}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
        \includegraphics[]{img/Adaptive_Shifted_MBR.jpg}
    }
    \caption{
        \textbf{Illustration of ASMBR.} The Shifted MBR is generated by shifting the center of the MBR.
        The Adaptive Shifted MBR is generated by shrinking the width and height of the Shifted MBR.
    }
    \label{fig:asmbr}
\end{figure}

\noindent{\textbf{Verb box represented by ASMBR}}\quad To constrain the verb feature extraction with positional information in the verb decoder, as shown in Figure~\ref{fig:asmbr}, we introduce a novel representation, adaptive shifted minimum bounding rectangle (ASMBR) for initializing the verb box.
Unlike the previous CNN-based method, UnionDet~\cite{kim2020uniondet}, which learns a union box to guide the verb recognition, the verb box of SOV is directly initialized from the last layer subject and object boxes from the subject and object decoder.
To balance the attention between the subject and object, we shift the center of the MBR to the center of the subject and object boxes.
Considering the boxes will overlap with each other, we shrink the width and height of the MBR according to the spatial relationship between the two boxes.
With the shift and adapt operation, the verb box can constrain the interaction region for sampling points of the deformable attention and extract interaction information from specific subject and object pairs.
Finally, given the last layer subject box  and object box , where  indicates the box center, the verb box is defined as:



\subsection{Specific Target Guided DeNoising Training}
The object and verb labels are the targets of HOI detection, the two label embeddings, which are used to initialize the label queries can be viewed as the specific target priors.
Since the denoising queries are generated from the specific target priors and learned during the denoising training, we call our training strategy as \textbf{S}pecific \textbf{T}arget \textbf{G}uided (STG) denoising.
In this subsection, we first introduce the definition and usage of the label priors, then we introduce the STG denoising training strategy.

\label{sec:split_label_embedding}
\noindent{\textbf{Label-specific Priors}}\quad To explicitly equip the prior label knowledge into the decoders and disentangle the training and decoding target,
as shown in Figure~\ref{fig:SOV_STG_pipeline}, two kinds of learnable label embeddings are used to initialize the query embeddings for SOV decoders.
Specifically, in Figure~\ref{fig:overall_architecture}, we define the object label embeddings  as the object label priors, which consist of  vectors with  dimensions, where  is the number of object classes.
Similarly, the verb label embeddings  are defined as the verb label priors.
With the object label and verb label priors, we first initialize the query embeddings of object label  and verb label  by linear combining the object label and verb label embeddings with two learnable coefficient matrices  and , respectively.
Then, we add the object and verb label embeddings to obtain the inference query embeddings .
The initialization of , , and  is defined as follows:


Different from DN-DETR~\cite{Li_2022_CVPR} and DOQ~\cite{qu2022distillation}, which learn an encoding weight to generate queries only used in training,
we use the label embeddings both in the denoising and inference parts and enable the inference part to obtain the input query with label-specific information from the beginning.

\begin{figure}[!t]
  \centering
  \resizebox{0.99\linewidth}{!}{
     \includegraphics[]{img/dn_init_2.jpg}
  }
  \caption{
      \textbf{Illustration of adding noise to a ground-truth HOI instance.}
      The initialization consists of two parts, the object label and the verb label DN queries initialization.
      The final DN query embeddings  are concatenated with the object label DN queries  and the verb label DN queries .
}
\label{fig:dn_query_init}
\end{figure}

\noindent{\textbf{Learning Priors with DeNoising Training}}\quad In Figure~\ref{fig:dn_query_init}, we show the initialization of the DN (DeNoising) query embeddings and visualize the process of adding noise to a ground-truth HOI instance.
Given the ground-truth object label set  and verb label set  of an image, where  and  are the labels of the object and verb classes,  is the number of ground-truth HOI instances, we generate  groups of noised labels for each of the ground-truth HOI instances.
For the -th ground-truth HOI instance, the noised object labels are obtained by randomly flipping the ground-truth index of the object label  to another object class index.
Because the verb label  consists of co-occurrence ground-truth classes, to keep the co-occurrence ground-truth indices appearing in the noised verb labels, we randomly flip the other indices of the ground-truth verb label to generate the noised verb labels.
Two flipping rate hyper-parameters  and  are used to control the percentage of the noised object labels and verb labels, respectively.
Besides, a verb class flipping rate hyper-parameter  is used to control the class-wise flipping rate in the verb labels.
Next, we introduce a "select" approach to "encode" the noised labels to DN query embeddings.
Specifically, we directly compose the object DN query embeddings  by selecting class-specific vectors  from the object label embeddings  according to the indices of the noised object labels.
For encoding of the noised verb labels, we select and sum the class-specific vectors to construct multi-class vectors , and compose the verb DN query embeddings .
Finally, we concatenate the object DN query embeddings  and verb DN query embeddings  to form the DN query embeddings  for the denoising training.
Since the specific target priors learned by the denoising training are also used to guide the inference during end-to-end training, our STG can accelerate the training convergence and improve the inference performance at the same time.
In addition, motivated by the box denoising strategy of DN-DETR~\cite{Li_2022_CVPR}, we scale and shift pairs of ground-truth subject and object boxes to generate  groups of noised anchor boxes for corresponding DN query embeddings.

\subsection{Training and Inference}
\label{sec:training_inference}
As shown in Figure~\ref{fig:SOV_STG_pipeline}, our proposed method SOV-STG is trained in an end-to-end manner.
For the inference queries , the Hungarian algorithm~\cite{kuhn1955hungarian} is used to match the ground-truth HOI instances with the predicted HOI instances, and the matching cost and the training loss of predicted HOI instances follow the previous deformable transformer based method~\cite{cjw_qahoi}.
For the DN queries , the ground-truth indices used in query initialization are used to match the predicted HOI instances, and the loss function is the same as the inference queries.
With the basic concept that the same ground-truth label flipping rate is difficult for the model to denoise at the beginning of the training but becomes acceptable during the training, we further improve the denoising strategy by introducing a dynamic DN scale factor  to control the object label flipping rate  and the verb label denoising rate  according to the training epochs.
With the dynamic DN scale strategy, the label flipping rate  will be set to  at the beginning of the training and linearly increase to  during the training.
As the label embeddings used in the denoising training part are also the specific target priors of the inference part, SOV-STG uses all of the parameters in training and inference.

\begin{table*}[!t]
    \begin{minipage}{0.64\linewidth}
        \centering
        \renewcommand\thetable{1}
        \resizebox{0.95\linewidth}{!}{
            \begin{tabular}{@{}ccccccccc@{}}
                \hline
                \multicolumn{1}{c|}{}                                       & \multicolumn{1}{c|}{}                & \multicolumn{1}{c|}{}                & \multicolumn{3}{c|}{Default} & \multicolumn{3}{c}{Known Object} \\
                \multicolumn{1}{c|}{Method}                                 & \multicolumn{1}{c|}{Epoch}           & \multicolumn{1}{c|}{Backbone}        & \textit{Full}                & \textit{Rare}    & \multicolumn{1}{c|}{\textit{Non-Rare}} & \textit{Full}    & \textit{Rare}    & \textit{Non-Rare} \\ \hline \hline
                \multicolumn{1}{l}{\textbf{Two-stage}} \\ \hline   
\multicolumn{1}{l|}{CATN~\cite{dong2022category}}           & \multicolumn{1}{c|}{12}              & \multicolumn{1}{c|}{ResNet-50}       & 31.86           & 25.15            & \multicolumn{1}{c|}{33.84}             & 34.44            & 27.69            & 36.45    \\
                \multicolumn{1}{l|}{STIP~\cite{zhang2022exploring}}         & \multicolumn{1}{c|}{30}              & \multicolumn{1}{c|}{ResNet-50}       & 32.22           & 28.15            & \multicolumn{1}{c|}{33.43}             & 35.29            & 31.43            & 36.45    \\
                \multicolumn{1}{l|}{UPT~\cite{Zhang_2022_CVPR}}             & \multicolumn{1}{c|}{20}              & \multicolumn{1}{c|}{ResNet-101-DC5}  & 32.62           & 28.62            & \multicolumn{1}{c|}{33.81}             & 36.08            & 31.41            & 37.47    \\ 
                \multicolumn{1}{l|}{Liu~\etal~\cite{Liu_2022_CVPR}}         & \multicolumn{1}{c|}{129}             & \multicolumn{1}{c|}{ResNet-50}       & 33.51           & 30.30            & \multicolumn{1}{c|}{34.46}             & 36.28            & 33.16            & 37.21    \\ \hline \hline
                \multicolumn{1}{l}{\textbf{One-stage}} \\ \hline            
\multicolumn{1}{l|}{QAHOI~\cite{cjw_qahoi}}                 & \multicolumn{1}{c|}{150}             & \multicolumn{1}{c|}{ResNet-50}       & 26.18           & 18.06            & \multicolumn{1}{c|}{28.61}             & -                & -                & -                 \\ \multicolumn{1}{l|}{QPIC~\cite{tamura2021qpic}}             & \multicolumn{1}{c|}{150}             & \multicolumn{1}{c|}{ResNet-50}       & 29.07           & 21.85            & \multicolumn{1}{c|}{31.23}             & 31.68            & 24.14            & 33.93             \\ 
\multicolumn{1}{l|}{MSTR~\cite{Kim_2022_CVPR}}              & \multicolumn{1}{c|}{50}              & \multicolumn{1}{c|}{ResNet-50}       & 31.17           & 25.31            & \multicolumn{1}{c|}{32.92}             & 34.02            & 28.83            & 35.57             \\
                \multicolumn{1}{l|}{SSRT~\cite{iftekhar2022look}}           & \multicolumn{1}{c|}{150}             & \multicolumn{1}{c|}{ResNet-101}      & 31.34           & 24.31            & \multicolumn{1}{c|}{33.32}             & -                & -                & -                 \\
                \multicolumn{1}{l|}{CDN-S~\cite{zhang2021mining}}           & \multicolumn{1}{c|}{100}             & \multicolumn{1}{c|}{ResNet-50}       & 31.44           & 27.39            & \multicolumn{1}{c|}{32.64}             & 34.09            & 29.63            & 35.42             \\
                \multicolumn{1}{l|}{Zhou~\etal\cite{zhou2022human}}         & \multicolumn{1}{c|}{80}              & \multicolumn{1}{c|}{ResNet-50}       & 31.75           & 27.45            & \multicolumn{1}{c|}{33.03}             & 34.50            & 30.13            & 35.81             \\
\multicolumn{1}{l|}{CDN-L~\cite{zhang2021mining}}           & \multicolumn{1}{c|}{100}             & \multicolumn{1}{c|}{ResNet-101}      & 32.07           & 27.19            & \multicolumn{1}{c|}{33.53}             & 34.79            & 29.48            & 36.38             \\
                \multicolumn{1}{l|}{HQM (CDN-S)~\cite{zhong2022towards}}    & \multicolumn{1}{c|}{80}              & \multicolumn{1}{c|}{ResNet-50}       & 32.47           & 28.15            & \multicolumn{1}{c|}{33.76}             & 35.17            & 30.73            & 36.50             \\
                \multicolumn{1}{l|}{RLIP-ParSe~\cite{yuan2022rlip}}         & \multicolumn{1}{c|}{90}              & \multicolumn{1}{c|}{ResNet-50}       & 32.84           & \textbf{34.63}   & \multicolumn{1}{c|}{26.85}             & -                & -                & -                 \\
                \multicolumn{1}{l|}{DOQ (CDN-S)~\cite{qu2022distillation}}  & \multicolumn{1}{c|}{80}              & \multicolumn{1}{c|}{ResNet-50}       & 33.28           & 29.19            & \multicolumn{1}{c|}{34.50}             & -                & -                & -                 \\
                \multicolumn{1}{l|}{GEN-VLKT-S~\cite{liao2022gen}}          & \multicolumn{1}{c|}{90}              & \multicolumn{1}{c|}{ResNet-50}       & 33.75           & 29.25            & \multicolumn{1}{c|}{35.10}             & 36.78            & 32.75            & 37.99             \\ 
                \multicolumn{1}{l|}{GEN-VLKT-M~\cite{liao2022gen}}          & \multicolumn{1}{c|}{90}              & \multicolumn{1}{c|}{ResNet-101}      & 34.78           & 31.50            & \multicolumn{1}{c|}{35.77}             & 38.07            & 34.94            & 39.01             \\
                \multicolumn{1}{l|}{GEN-VLKT-L~\cite{liao2022gen}}          & \multicolumn{1}{c|}{90}              & \multicolumn{1}{c|}{ResNet-101}      & 34.95           & 31.18   & \multicolumn{1}{c|}{36.08}             & \textbf{38.22}   & \textbf{34.36}   & \textbf{39.37}    \\ 
                \rowcolor{Gray}
                \multicolumn{1}{l|}{QAHOI-Swin-L~\cite{cjw_qahoi}}          & \multicolumn{1}{c|}{150}             & \multicolumn{1}{c|}{Swin-Large-22K}  & 35.78           & 29.80            & \multicolumn{1}{c|}{37.56}             & 37.59            & 31.36            & 39.36             \\
                \rowcolor{Gray}
                \multicolumn{1}{l|}{FGAHOI-Swin-L~\cite{ma2023fgahoi}}      & \multicolumn{1}{c|}{190}             & \multicolumn{1}{c|}{Swin-Large-22K}  & 37.18           & 30.71            & \multicolumn{1}{c|}{39.11}             & 38.93            & 31.93            & 41.02             \\ \cline{1-9}
                \multicolumn{1}{l|}{\textbf{SOV-STG-S}}                     & \multicolumn{1}{c|}{30}              & \multicolumn{1}{c|}{ResNet-50}       & 33.80	          & 29.28            & \multicolumn{1}{c|}{35.15}             & 36.22            & 30.99            & 37.78             \\
                \multicolumn{1}{l|}{\textbf{SOV-STG-M}}                     & \multicolumn{1}{c|}{30}              & \multicolumn{1}{c|}{ResNet-101}      & 34.87           & 30.41            & \multicolumn{1}{c|}{36.20}             & 37.35            & 32.46            & 38.81             \\
                \multicolumn{1}{l|}{\textbf{SOV-STG-L}}                     & \multicolumn{1}{c|}{30}              & \multicolumn{1}{c|}{ResNet-101}      & \textbf{35.01}  & 30.63            & \multicolumn{1}{c|}{\textbf{36.32}}    & 37.60            & 32.77            & 39.05             \\ 
                \rowcolor{Gray}   
                \multicolumn{1}{l|}{\textbf{SOV-STG-Swin-L}}                & \multicolumn{1}{c|}{30}              & \multicolumn{1}{c|}{Swin-Large-22K}  & \textbf{43.35}  & \textbf{42.25}   & \multicolumn{1}{c|}{\textbf{43.69}}    & \textbf{45.53}   & \textbf{43.62}   & \textbf{46.11}    \\ \hline
                 
            \end{tabular}
        }
        \caption{
            Comparison to state-of-the-arts on the HICO-DET.
        }
        \label{tab:tab1}
        \vspace{4.7mm}
        \renewcommand\thetable{5}
        \resizebox{0.76\linewidth}{!}{
           \begin{tabular}{@{}ccccccc@{}}
                \hline
                \multicolumn{1}{c|}{\multirow{2}{*}{\#}} & \multicolumn{3}{c|}{S-O Attention Designs} & \multicolumn{3}{c}{Default}\\
                \multicolumn{1}{c|}{}    & \multicolumn{1}{c}{last layer} & \multicolumn{1}{c}{multi-layer} & \multicolumn{1}{c|}{Attention}         & \multicolumn{1}{c}{\textit{Full}} & \multicolumn{1}{c}{\textit{Rare}} & \multicolumn{1}{c}{\textit{Non-Rare}} \\ \hline \hline
                \multicolumn{1}{c|}{(1)} & \multicolumn{1}{c}{\ding{51}}  & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{S-O Fusion}        & \textbf{33.80}    & \textbf{29.28}  & \textbf{35.15}  \\ 
                \multicolumn{1}{c|}{(2)} & \multicolumn{1}{c}{\ding{51}}  & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{S-O w/o bottom-up} & 33.40    & 28.69  & 34.81  \\
                \multicolumn{1}{c|}{(3)} & \multicolumn{1}{c}{}           & \multicolumn{1}{c|}{\ding{51}}  & \multicolumn{1}{c|}{S-O Fusion}        & 32.77    & 27.09  & 34.46  \\
                \multicolumn{1}{c|}{(4)} & \multicolumn{1}{c}{\ding{51}}  & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{Sum Fusion}        & 33.01    & 27.83  & 34.55  \\
                \multicolumn{1}{c|}{(5)} & \multicolumn{1}{c}{}           & \multicolumn{1}{c|}{\ding{51}}  & \multicolumn{1}{c|}{Sum Fusion}        & 32.23    & 27.49  & 33.65  \\ \hline
\end{tabular}
        }
        \caption{Ablation studies for S-O attention. The "multi-layer" indicates feeding the fused verb embeddings to the verb decoder in a multi-layer manner.}
        \label{tab:tab5}
    \end{minipage}
    \hfill
    \begin{minipage}{0.34\linewidth}
        \centering
        \renewcommand\thetable{2}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{@{}ccccc@{}}
                \hline
                \multicolumn{1}{c|}{Method}                                  & \multicolumn{1}{c|}{Fea.} & \multicolumn{1}{c|}{Backbone} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}  \\ \hline \hline
\multicolumn{1}{l|}{RLIP-ParSe~\cite{yuan2022rlip}}          & \multicolumn{1}{c|}{L}        & \multicolumn{1}{c|}{ResNet-50}       & 61.9          & 64.2  \\
                \multicolumn{1}{l|}{MSTR~\cite{Kim_2022_CVPR}}               & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-50}       & 62.0          & 65.2  \\
\multicolumn{1}{l|}{ParSe~\cite{yuan2022rlip}}               & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-50}       & 62.5          & 64.8  \\
                \multicolumn{1}{l|}{GEN-VLKT-M~\cite{liao2022gen}}           & \multicolumn{1}{c|}{L}        & \multicolumn{1}{c|}{ResNet-101}      & 63.3          & 65.6  \\
                \multicolumn{1}{l|}{GEN-VLKT-L~\cite{liao2022gen}}           & \multicolumn{1}{c|}{L}        & \multicolumn{1}{c|}{ResNet-101}      & 63.6          & 65.9  \\
                \multicolumn{1}{l|}{CDN-L~\cite{zhang2021mining}}            & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-101}      & 63.9          & 65.9  \\
                \multicolumn{1}{l|}{SSRT~\cite{iftekhar2022look}}            & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-101}      & 65.0          & 67.1  \\ \hline
                \multicolumn{1}{l|}{\textbf{SOV-STG-M}}                      & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-101}      & 63.7          & 65.2  \\
                \multicolumn{1}{l|}{\textbf{SOV-STG-L}}                      & \multicolumn{1}{c|}{A}        & \multicolumn{1}{c|}{ResNet-101}      & 63.9          & 65.4  \\ 
                \hline
            \end{tabular}
        }
        \caption{Comparison on V-COCO. 'A' and 'L' indicate the appearance and language features, respectively.}
        \label{tab:tab2}
        \vspace{4.3mm}
        \renewcommand\thetable{4}
        \resizebox{0.87\linewidth}{!}{
            \begin{tabular}{@{}ccccc@{}}
                \hline
                \multicolumn{1}{c|}{\multirow{2}{*}{\#}} & \multicolumn{1}{c|}{\multirow{2}{*}{Verb Box}} & \multicolumn{3}{c}{Default}  \\
                \multicolumn{1}{c|}{}                    & \multicolumn{1}{c|}{}                          & \textit{Full} & \textit{Rare} & \textit{Non-Rare} \\ \hline \hline
                \multicolumn{1}{c|}{(1)}                 & \multicolumn{1}{c|}{Object Box}                & 33.16  & 27.21  & 34.94    \\
                \multicolumn{1}{c|}{(2)}                 & \multicolumn{1}{c|}{Subject Box}               & 32.78  & 28.01  & 34.21    \\
                \multicolumn{1}{c|}{(3)}                 & \multicolumn{1}{c|}{MBR}                       & 33.44  & 27.84  & 35.11    \\ 
                \multicolumn{1}{c|}{(4)}                 & \multicolumn{1}{c|}{SMBR}                      & 33.41  & 28.22  & 34.97    \\ 
                \multicolumn{1}{c|}{(5)}                 & \multicolumn{1}{c|}{ASMBR}                     & \textbf{33.80}  & \textbf{29.28}  & \textbf{35.15} \\ \hline
            \end{tabular}
        }
        \caption{Different designs for the verb box. The "SMBR" indicates the Shifted MBR.}
        \label{tab:tab4}
        \vspace{4.3mm}
        \renewcommand\thetable{6}
        \resizebox{0.9\linewidth}{!}{
            \begin{tabular}{@{}ccccccc@{}}
                \hline
                \multicolumn{4}{c|}{Denoising Strategies}                                      & \multicolumn{3}{c}{Default} \\ \cline{1-4}
                \multicolumn{1}{c|}{\#}    & \multicolumn{1}{c}{Box} & \multicolumn{1}{c}{Obj} & \multicolumn{1}{c|}{Verb} & \multicolumn{1}{c}{\textit{Full}} & \multicolumn{1}{c}{\textit{Rare}} & \multicolumn{1}{c}{\textit{Non-Rare}} \\ \hline \hline
                \multicolumn{1}{c}{(1)}    & \multicolumn{1}{c}{}              & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{}          & 32.99   & 28.28  & 34.40  \\
                \multicolumn{1}{c}{(2)}    & \multicolumn{1}{c}{\ding{51}}     & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{}          & 33.27   & 29.07  & 34.53  \\
                \multicolumn{1}{c}{(3)}    & \multicolumn{1}{c}{\ding{51}}     & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{\ding{51}} & 33.28   & 28.57  & 34.69  \\
                \multicolumn{1}{c}{(4)}    & \multicolumn{1}{c}{}              & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{\ding{51}} & 33.39   & 28.82  & 34.76  \\
                \multicolumn{1}{c}{(5)}    & \multicolumn{1}{c}{\ding{51}}     & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{}          & 33.51   & 29.05  & 34.84  \\
                \multicolumn{1}{c}{(6)}    & \multicolumn{1}{c}{\ding{51}}     & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{\ding{51}} & \textbf{33.80}   & \textbf{29.28}  & \textbf{35.15}  \\ \hline      
            \end{tabular}
        }
        \caption{Ablation studies for denoising strategies. The symbol of \ding{51} means adding nosie to the ground-truth.}
        \label{tab:tab6}
    \end{minipage}
\end{table*}
\begin{table}[!t]
    \centering
    \renewcommand\thetable{3}
    \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{@{}cccccccc@{}}
            \hline
            \multicolumn{1}{c|}{\multirow{2}{*}{\#}} & \multicolumn{1}{c}{\multirow{2}{*}{oDec}} & \multicolumn{1}{c}{\multirow{2}{*}{sDec}} & \multicolumn{1}{c}{\multirow{2}{*}{vDec}} & \multicolumn{1}{c|}{\multirow{2}{*}{STG}} & \multicolumn{3}{c}{Default}\\
            \multicolumn{1}{c|}{}                    & \multicolumn{1}{c}{}                      & \multicolumn{1}{c}{}                      & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{}                     & \multicolumn{1}{c}{\textit{Full}} & \multicolumn{1}{c}{\textit{Rare}} & \multicolumn{1}{c}{\textit{Non-Rare}} \\ \hline \hline
            \multicolumn{1}{c|}{(1)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{}                      & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{\ding{51}}            & 32.68          & 28.21  & 34.02  \\ 
            \multicolumn{1}{c|}{(2)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{\ding{51}}            & 32.35          & 27.64  & 33.63  \\
            \multicolumn{1}{c|}{(3)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{}                      & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{}                     & 30.14          & 22.82  & 32.32  \\
            \multicolumn{1}{c|}{(4)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{}                      & \multicolumn{1}{c|}{}                     & 30.62          & 24.60  & 32.42  \\
            \multicolumn{1}{c|}{(5)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{}                     & 31.90          & 25.92  & 33.69  \\
            \multicolumn{1}{c|}{(6)}                 & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c}{\ding{51}}             & \multicolumn{1}{c|}{\ding{51}}            & \textbf{33.80} & \textbf{29.28} & \textbf{35.15} \\ \hline      
        \end{tabular}
    }
    \caption{Contributions of each module. The "oDec", "sDec", and "vDec" denote the object, subject, and verb decoder, respectively.}
    \label{tab:tab3}
\end{table}

\section{Experiments}

We evaluate our proposed SOV-STG on the HICO-DET~\cite{chao2018learning} and V-COCO~\cite{gupta2015visual} datasets to compare with current SOTA methods and conduct extensive ablation studies to analyze the contributions of each component and show the effectiveness of our proposed method.

\subsection{Experimental Settings}

\noindent{\textbf{Dataset and Metric.}}\quad The HICO-DET~\cite{chao2018learning} dataset contains 38,118 images for training and 9,658 images for the test.
The 117 verb classes and 80 object classes in HICO-DET form 600 HOI classes.
According to the number of HOI instances appearing in the dataset, the HOI classes are divided into three categories: \textit{Full}, \textit{Rare}, and \textit{Non-Rare}.
Moreover, considering HOI instances including or not including the unknown objects, the evaluation of HICO-DET is divided into two settings: Default and Known Object.
The V-COCO~\cite{gupta2015visual} dataset contains 5,400 images for training and 4,946 images for the test.
In V-COCO, 80 object classes and 29 verb classes are annotated, and two scenarios are considered: scenario 1 with 29 verb classes and scenario 2 with 25 verb classes.
We follow the standard evaluation~\cite{chao2018learning} and report the mAP scores.

\noindent{\textbf{Implementation Details.}}\quad We use the DAB-Deformable-DETR trained on COCO~\cite{lin2014microsoft} to initialize the weight of the feature extractor, the subject decoder, and the object decoder.
The feature extractor consists of a ResNet-50~\cite{he2016deep} backbone and a 6-layer deformable transformer encoder.
Similar to GEN-VLKT~\cite{liao2022gen}, we implement three variants of SOV-STG by adjusting the backbone and the number of layers in all the decoders, which are denoted as \textbf{SOV-STG-S} with ResNet-50 and 3-layer decoders, \textbf{SOV-STG-M} with ResNet-101 and 3-layer decoders, and \textbf{SOV-STG-L} with ResNet-101 and 6-layer decoders.
The hidden dimension of the transformer is , and the number of the query is set to .
For the DN part,  groups of noised labels are generated for each ground-truth HOI instance.
The dynamic DN scale is set to , and we define the maximum denoising level by setting the noising rate of the box to , the object label flipping rate to , the verb denoising rate to , and the verb label flipping rate to .
We train the model with the AdamW optimizer~\cite{loshchilov2018decoupled} with a learning rate of 2e-4 (except for the backbone, which is 1e-5 for HICO-DET, 2e-6 for V-COCO) and a weight decay of 1e-4.
The batch size is set to 32 (4 images per GPU), and the training epochs are 30 (learning rate drops at the 20th epoch), which is one-third of the GEN-VLKT~\cite{liao2022gen}, and one-fifth of the QPIC~\cite{tamura2021qpic} and QAHOI~\cite{cjw_qahoi}.
All of the experiments are conducted on 8 NVIDIA A6000 GPUs.

\subsection{Comparison to State-of-the-Arts}

In Table~\ref{tab:tab1}, we compare our proposed SOV-STG with the recent SOTA methods on the HICO-DET dataset.
Our SOV-STG-S with ResNet-50 backbone achieves 33.80 mAP on the \textit{Full} category of the Default setting.
Compared with the transformer-based one-stage methods, QAHOI and MSTR, which are based on the reference point,
SOV-STG benefits from the anchor box priors and label priors and achieves 7.62 (29.11\%) and 2.63 (8.44\%) mAP improvements, respectively.
Note that, without any extra language prior knowledge~\cite{radford2021learning}, SOV-STG-M outperforms GEN-VLKT-M by 0.26\% in one-third of the training epochs.
Our proposed framework and learning strategy close the gap in training efficiency between transformer-based one-stage and two-stage methods.
As a result, compared with UPT, SOV-STG-L achieves 3.42 (10.56\%) mAP improvements with only 10 more training epochs.
Since our SOV-STG explicitly makes full use of the ground-truth information, compared with DOQ, which also uses ground-truth to guide the training, SOV-STG-S achieves 1.05\% mAP improvement with less than half of the training epochs of DOQ.
Furthermore, our best model SOV-STG-Swin-L with Swin-Large~\cite{liu2021swin} backbone achieves a new SOTA performance of 43.35 mAP, which outperforms FGAHOI-Swin-L by 14.23\%.
Similarly, in Table~\ref{tab:tab2}, SOV-STG-M achieves 63.7 mAP on  and surpasses UPT and GEN-VLKT-L by 3.92\% and 0.63\%, respectively.

\subsection{Ablation Study}

We conduct all the ablation experiments on the HICO-DET dataset with the SOV-STG-S model, and if not explicitly noticed, the same training setting is used as the training of our SOTA model.

\noindent{\textbf{Contributions of proposed modules.}}\quad SOV-STG is composed of flexible decoding architecture and training strategies.
To clarify the contributions of each proposed module, in Table~\ref{tab:tab3}, we remove the proposed modules one by one and conduct ablation studies on the HICO-DET dataset.
The row of (5) indicates the experiment removing the STG strategy and the S-O attention module is degraded to a sum fusion module which is similar to GEN-VLKT~\cite{liao2022gen}.
From the result, the STG strategy and S-O attention improve the performance by 5.96\% on the \textit{Full} category.
Moreover, without the STG strategy, our framework also achieves a significant improvement over QPIC (ResNet-50) by 9.74\% with one-fifth of the training epochs.
Next, in (4), we remove the verb decoder in (5).
As the result, comparing (4) with (5), without the verb decoder, the performance drops by 4.01\%.
Then, in (3), we remove the subject decoder and the sum fusion module, and update both the subject and object boxes by the object decoder.
Without balancing the decoding burden of the detection, compared with (4), the performance drops by 1.57\%.
Furthermore, in (1) and (2), we conduct drop-one-out experiments on the subject and verb decoder, respectively.
Compared with (1) and (2), the model without the verb decoder is worse than the model without the subject decoder, which indicates that the verb decoder plays a more critical role.

\noindent{\textbf{S-O attention module.}}\quad The S-O attention module is the core module of the SOV model, which is responsible for the fusion of the object and subject features.
To explore the strength of the S-O attention mechanism, different variants of designs we have attempted are shown in Table~\ref{tab:tab5}.
The result of (1) indicates the S-O attention module used in SOV-STG-S.
In (2), we remove the bottom-up path in S-O attention.
Since the bottom-up path strengthens the feature fusion, without the information flow from lower layers, the performance drops by 1.18\%.
Our verb decoder uses the fused embeddings from the last layer of the S-O attention module as the input query, and updates the embeddings layer by layer.
In (3), we attempt to feed all layers' fused embeddings as query positional embeddings for the verb decoder.
However, compared with our design in (1), the accuracy drops by 3.05\%.

The cross-attention enables the fused embeddings to be enriched by the verb label embeddings.
In (4), we remove the cross-attention of S-O attention, and the attention module is degraded to a sum fusion module.
From the result, the performance drops by 2.34\% compared with (1).
Similarly, in (5), we also attempt the multi-layer design of the (4), which is similar to GEN-VLKT~\cite{liao2022gen}, and the performance also drops.
We consider that the multi-layer design is not suitable for the verb prediction as the deformable transformer attention mechanism is a local attention mechanism, which focuses on different parts of the source feature in different layers.
Specifically, the sampling points in the verb decoder are not related to the sampling points in the object and subject decoders, which focuses on different positions of the global semantic feature.
Consequently, the multi-layer design forces the verb decoder to match the attention of the object and subject decoders, which leads to the performance drop.

\noindent{\textbf{Denoising Strategies.}}\quad In Table~\ref{tab:tab6}, we investigate the denoising strategies of three parts of the targets, i.e., the box coordinates, the object labels, and the verb labels.
The result of (6) indicates the result of SOV-STG-S.
In (1), we set the noise rate of box coordinates to , the object label filp rate to , and the verb label filp rate to , thus, the ground-truth box coordinates, object labels, and verb labels are directly fed into the model without any noise.
From the result, the accuracy drops by 2.40\% compared with the full denoising training in (6).
In (3), (4), and (5), we conduct drop-one-out experiments, and the results show that each part of the denoising strategy is effective.
For the results between (2) and (3), and (4) and (3), the verb denoising increases the performance while it used with the object denoising.

\begin{figure}[!t]
  \centering
  \resizebox{0.97\linewidth}{!}{
      \includegraphics[]{code/table7.png}
  }
  \caption{The effects of different dynamic DN scales , where  means the DN rate  is start from 0 and end at , and  means do not use the dynamic DN strategy.}
  \label{fig:dynamic_noise_scale}
\end{figure}

\noindent{\textbf{Formulations of the verb box.}}\quad To verify the effectiveness of ASMBR, we use the verb box degraded from the ASMBR to conduct ablation studies, and the results are shown in Table~\ref{tab:tab4}.
From the results of (3) to (5), the adaptive and shift operations for the MBR promote the performance of the verb box, by 1.08\% on the \textit{Full} category and 5.17\% on the \textit{Rare} category.
Furthermore, in (1) and (2), we directly use the object or subject box as the verb box, and the results show that the object box is better for non-rare class recognition, while the subject box is better for rare class recognition.

\noindent{\textbf{Dynamic DN scales.}}\quad The dynamic DN scale is used to adjust the denoising training difficulties during the whole training session. 
In Figure~\ref{fig:dynamic_noise_scale}, we adjust the dynamic DN scale () to reveal the effects of different dynamic DN scales.
Compared with , while , the best performance is achieved, and the dynamic DN strategy mainly improves the performance on the \textit{Full} and \textit{Rare} categories.

\section{Conclusion and Future Work}
In this paper, we proposes a novel one-stage framework, SOV with HOI split decoders for target-specific decoding and a specific target guided denoising strategy, STG, for efficient training.
Our framework SOV-STG adopts a new format to represent HOI instances in boxes and learns HOI-specific priors for decoding.
With the well-designed architecture and efficient training strategy, our framework achieves state-of-the-art performance with less training cost.
Since our architecture disentangles the HOI detection by specific priors and decoders, it is easy to improve any one of them.
In the future, we are going to incorporate the knowledge from the language models to improve performance.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
