\documentclass[runningheads,a4paper]{llncs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{color}
\renewcommand{\rmdefault}{ptm}
\fontfamily{ptm}
\begin{document}
\frontmatter          
\pagenumbering{arabic}
\pagestyle{headings}  
\title{Using Volunteer Computing for Mounting SAT-based Cryptographic Attacks}
\author{Alexander Semenov \and Oleg Zaikin \and Ilya Otpuschennikov}
\authorrunning{A.\,Semenov \and O.\,Zaikin \and I.\,Otpuschennikov}
\institute{Institute for System Dynamics and Control Theory SB RAS, Irkutsk, Russia
\email{biclop.rambler@yandex.ru, zaikin.icc@gmail.com, otilya@yandex.ru}}

\maketitle              
\begin{abstract}
In this paper we describe the volunteer computing project SAT@home, developed and maintained by us. This project is aimed at solving hard instances of the Boolean satisfiability problem (SAT). We believe that this project can be a useful tool for computational study of inversion problems of some cryptographic functions. In particular we describe a series of experiments performed in SAT@home on the cryptanalysis of the widely known keystream generator A5/1. In all experiments we analyzed one known burst (114 bits) of keystream produced by A5/1. Before the cryptanalysis itself there is a stage on which the partitioning of the original problem to a family of subproblems is carried out. Each of subproblems should be easy enough so that it could be solved in relatively small amount of time by volunteer's PC. We construct such partitioning using the special technique based on the Monte Carlo method and discrete optimization algorithms for special predictive functions. Besides this in the paper we describe the technique for reducing inversion problems of cryptographic functions to SAT.
\end{abstract}
\keywords{Boolean satisfiability problem, SAT, volunteer computing, BOINC, cryptanalysis, A5/1, SAT@home}
\section{Introduction}

In this paper we present our experience of the development of the volunteer computing project SAT@home aimed at solving hard instances of the Boolean satisfiability problem (SAT). In particular this project can be used to justify from the practical point of view the resistance of some cryptographic functions to attacks based on the algorithms for solving SAT. We confirm this assertion by successfully solving in SAT@home the series of cryptanalysis problems of widely known keystream generator A5/1, based on one known burst (114 bits) of keystream. 

The Boolean satisfiability problem is formulated as follows: for an arbitrary Boolean formula to determine if there exists such assignment of its variables, that makes this formula true. This problem is NP-complete, and if  there are no polynomial algorithms that could solve it. On the other hand, the class of practically important problems that can be effectively reduced to SAT is very wide \cite{DBLP:series/faia/2009-185}. Because of this fact it is important to develop the algorithms that show good effectiveness in application to various cases of SAT.

In practice it is usually convenient to consider satisfiability problem for a formula presented in the Conjunctive Normal Form (CNF). Using the Tseitin transformations \cite{Tseitin83} it is possible to effectively (in polynomial time) reduce the SAT for an arbitrary Boolean formula to the SAT for some CNF.

In the last 10 years the effectiveness of SAT solving algorithms has significantly increased. Today SAT approach is used to solve various problems from the areas of symbolic verification, bioinformatics, discrete optimization, combinatorial design, cryptanalysis, etc. There are several basic classes of algorithms used to construct modern SAT solvers. All the results presented in this paper were obtained using the so called Conflict Driven Clause Learning (CDCL) solvers, the description of main architectural principles of which can be found, for example, in \cite{DBLP:series/faia/SilvaLM09}.

To solve SAT instances encoding the problems of cryptanalysis of ciphering systems used in the real world it is insufficient to employ only the resources of a single PC. In this context the development of SAT solving methods, designed for parallel and distributed computing environments, is highly relevant. There are several general concepts of solving SAT in parallel. From our point of view for solving SAT instances, encoding cryptographic attacks, the volunteer computing concept looks promising, because this kind of distributed computing makes it possible to solve one particular computing problem for a prolonged time with relatively high performance (on average).

It should be noted, that when solving a particular problem in a volunteer computing project there apply some restrictions. First, the problem should be divided into subproblems in such a way that each subproblem can be solved by a volunteer's PC in relatively small amount of time (several hours at most). Also it is worth mentioning that volunteers PCs do not interconnect directly with each other. From these restrictions it is natural to draw the conclusion that to parallelize the original problem it is best to use the partitioning approach. The partitioning method must make it possible to estimate the time required to solve all produced subproblems. Below we describe such method and apply it to the parallelization of cryptanalysis problems for the widely known keystream generator A5/1. Then we process the partitionings obtained in parallel using the volunteer computing project SAT@home.

Let us give a brief outline of the paper. In the second section we present the general technique used to reduce cryptographic problems to SAT and describe the \textsc{Transalg} program system implementing this technique. In the third section we give brief description of main SAT solving algorithms that we applied to cryptographic problems. In the fourth section we propose an approach to the parallelization of SAT instances that encode cryptanalysis problems. Thus, we describe the general technique of constructing SAT partitionings that can be processed in distributed computing environments. In the fifth section we present the results of the computational experiments on the search for SAT partitionings, with the use of the algorithms described in the previous section. In the sixth section we describe the volunteer computing project SAT@home developed by us specifically for the purpose of solving hard SAT instances, and present the results of computational experiments held in SAT@home on the cryptanalysis of the A5/1 keystream generator.

\section{Reducing Inversion Problems of Cryptographic Functions to SAT}

Hereinafter by  we denote the set formed by all possible binary words of length . By  we denote a discrete function that transforms binary words of length  to binary words of length . For each function considered below we assume that this function is defined everywhere on  and is specified by some algorithm . By ,  we denote the set of all output values of the function . Let us consider the following problem: for a given  and the known algorithm  we need to find such , that . We will refer to this general problem as to the problem of inversion of function . Many cryptanalysis problems can be considered in its context. For example, let  be a function specified by the algorithm of some keystream generator. Then  is the secret key and  is the keystream produced by the generator considered. In the vast majority of keystream ciphers the ciphertext is formed by adding keystream and some secret message  in arithmetics modulo 2. Sometimes, it is possible (usually when the protocol is inconsistent) that some message  becomes known. It may represent some technical data and thus not even contain any valuable information (as in case of the ciphering protocol used in the GSM standard). However, if we know  and the corresponding ciphertext then we can find  and consider the problem of search for the secret key . Knowing  makes it possible to read any message ciphered using this secret key. The problem of determining  based on a given  and known algorithm specifying function  is usually called a known plaintext attack. Hereinafter we will study this particular cryptographic problem.

\subsection{Theoretical Basis of SAT Encoding of Discrete Functions Inversion Problems}

Let us describe the general scheme of encoding the problem of inversion of  to SAT. The main idea that underlies such reductions is based on the Cook theorem \cite{DBLP:conf/stoc/Cook71}. It consists in forming a correspondence between the computation of function  by algorithm  and some system of Boolean equations. The complexity of the procedure that constructs the system of Boolean equations is bounded above by a polynomial of the complexity of algorithm .

In more detail the transition from the problem of inversion of  to the CNF encoding looks as follows. By  we denote the Boolean Circuit over the basis  that implements function . In the circuit  there are  inputs and  outputs. Let us link Boolean variables , forming the set , with inputs of the circuit, and Boolean variables , forming the set , with its outputs. We also mark each logical gate  by an auxiliary variable  such that . The set of all auxiliary variables we denote by . Note that . Let  be an arbitrary variable from  and  be a corresponding gate. If  is a NOT-gate and  is the variable linked with the input of  then we encode the gate  with the Boolean formula  (hereinafter by  we mean logical equivalence). If  is an AND-gate and  are variables linked with the gate inputs then we encode  with the formula . CNF-representations of Boolean functions specified by formulas  and  look as follows (respectively):

Thus with an arbitrary gate  of circuit  we associate a CNF  of the kind \eqref{f1}. We will say that the CNF  encodes the gate , and the CNF 

encodes the circuit . The described technique of constructing a CNF for a circuit  is known as Tseitin transformations \cite{Tseitin83}. 

Now let  be an arbitrary assignment of variables . Consider the CNF

where 

From the properties of the Tseitin transformations it follows that if  then the CNF \eqref{f2} is satisfiable and from any of its satisfying assignments one can effectively extract such assignment , , , that .

Thus, using the technique described above we reduce the problem of inversion of function  in an arbitrary point  to SAT for the CNF \eqref{f2}. It makes it possible to apply the SAT solvers to the latter problem.

\subsection{Encoding Inversion Problems of Cryptographic Functions to SAT Using the \textbf{Transalg} System}

In this section we will briefly describe the \textsc{Transalg} program system \cite{DBLP:journals/corr/OtpuschennikovSK14} that was designed specifically to reduce inversion problems of cryptographic functions to SAT. The original function  should first be represented in the form of the special TA-program . TA-programs have C-like syntax. The translation of an arbitrary TA-program  essentially involves the symbolic execution of  \cite{King:1976:SEP:360248.360252}, and results in the propositional encoding of  in the form of CNF. In other words the result of compilation of the program  is not a machine code but a set of Boolean formulas constructed according to the Tseitin transformations. In more detail, the result of the compilation of the TA-program is the CNF  encoding the circuit . By fixing the values of output variables  (essentially transitioning to CNF \eqref{f2}) we obtain a SAT instance that encodes the inversion problem of function  in the point .

Below the main object of our study is the A5/1 keystream generator that has been used to encrypt data within the GSM standard for more than 15 years.
On Figure \ref{a5_1_scheme} we demonstrate the scheme of the A5/1 generator from the paper \cite{DBLP:conf/fse/BiryukovSW00}. 

\begin{figure}[ht]
	\centering
		\includegraphics[width=7.5cm]{a5_1_scheme.eps}
	\caption{The A5/1 keystream generator}
	\label{a5_1_scheme}
\end{figure}

The generator contains three linear feedback shift registers (LFSRs) specified by the following feedback polynomials:


The registers in A5/1 are shifted asynchronously. Assume that at the initial time moment  the registers contain the secret key. By ,  denote the value of the clocking bit of the -th register at time moment . The clocking bits are contained in cells ,  and . With each clocking bit the special function

is associated. Here . The register number  is shifted at time moment  only if . Otherwise the register retains the condition in which it was at the previous time moment. At each time moment the generator outputs one bit of keystream that is produced as a sum modulo 2 of bits contained in cells ,  and . The length of the secret key in the A5/1 keystream generator is  bits.

On Figure \ref{a5_1_ta_program} we present the TA-program specifying the discrete function

This function generates 114 bits of keystream from the secret key of length 64 bits according to the A5/1 algorithm. The length of keystream fragment is chosen to be equal to 114 bits because in the GSM standard the information is encrypted by blocks of 114 bits. Such blocks are called bursts. 

The result of the compilation of the program from the Figure \ref{a5_1_ta_program} is the CNF with 7816 variables and 35568 clauses (size of the file in DIMACS format is 730 kilobytes).

\begin{figure}[ht]
	\centering
		\includegraphics[width=10cm]{a5_1_ta.eps}
	\caption{TA-program specifying function .}
	\label{a5_1_ta_program}
\end{figure}

\section{Algorithms for Solving SAT Instances that Encode Inversion Problems of Cryptographic Functions}

Main classes of the SAT solving algorithms were extensively described in \cite{DBLP:series/faia/2009-185}. After many computational experiments we made the conclusion that the algorithms based on the concept of Conflict Driven Clause Learning (CDCL) \cite{DBLP:series/faia/SilvaLM09} perform best on cryptographic problems. All the modern CDCL solvers that show good effectiveness on wide classes of industrial tests are based on the DPLL algorithm \cite{Davis:1962:MPT:368273.368557}. However, in its pure form the DPLL algorithm has relatively low performance. The main idea of the CDCL concept is to complement the DPLL with an ability to use memory to store the information about traversed parts of the search tree. This information is stored in the form of restrictive constraints which are called conflict clauses. The solver GRASP \cite{DBLP:journals/tc/Marques-SilvaS99} was the first full-fledged CDCL solver. In the subsequent papers \cite{DBLP:conf/iccad/ZhangMMM01,DBLP:conf/dac/MoskewiczMZZM01} several important algorithmical constructions were described that made it possible to improve the CDCL concept. It led to the creation of SAT solvers widely applied nowadays to combinatorial problems from various areas. 

Active development of parallel SAT solving algorithms started quite recently. First parallel SAT solvers competition was held in 2008. There are two main approaches to parallel SAT solving: the partitioning approach and the portfolio approach. In the partitioning approach the search space is divided into separate subspaces that are processed in parallel. In the portfolio approach all computing processes work with the same original search space, but they use different initial parameters and search heuristics, and also can share the collected data with each other.

The SAT instances encoding the inversion problems of cryptographic functions used in the real world usually are very hard, and to solve them it is insufficient to use the resources of a usual PC. The graphical processors (GPU) in the context of solving SAT nowadays do not yield any advantages in comparison with traditional CPUs. That is explained by the fact that modern GPUs have relatively high memory latencies, which significantly decrease the performance of CDCL algorithms. 

For solving hard SAT instances one can use computing clusters with nodes based on CPUs. The portfolio approach makes it impossible to effectively utilize a large number of cluster nodes because intensive conflict clauses exchange greatly taxes the interconnection environment. The advantage of the partitioning approach is that it doesn't limit the amount of computating nodes of the cluster that can be used. The low intensity of data exchange between computing processes in this case makes it possible to employ not only computing clusters but also distributed computing systems. The use of such systems (including volunteer computing projects) suits better for solving hard SAT instances because of the low computing costs (in comparison to that of clusters). To organize the computations in the distributed system the original problem should be divided into a family of significantly less complex subproblems in such a way that each subproblem can be solved by a computing node (such as, for example, a usual PC) in relatively small amount of time. In the next section we will describe the SAT partitioning technique that we used to organize the computations in grid systems.

\section{Partitioning Procedures for SAT Instances Encoding Inversion Problems of Cryptographic Functions}

So as we explained above we use the partitioning approach to parallelize the SAT solving process. There exist many various ways of constructing SAT partitionings \cite{Hyvarinen11}. Moreover, even following one way it is possible to make several different partitionings for the same SAT instance. The main problem that arises in this context is to justify why one partitioning is better than another. The basic partitioning effectiveness criterion is the time required to process it in a distributed computing environment. Unfortunately, not for all types of SAT partitionings it is possible to evaluate their effectiveness. In this section we will describe one type of SAT partitioning for which there exists quite natural procedure for constructing estimations of time required to process it. We used this type of partitioning to solve inversion problems of some cryptographic functions in parallel.

Assume that  is the CNF encoding the inversion problem of some discrete function. By  denote the set of Boolean variables that appear in CNF . By  denote an arbitrary subset of . Let  be an arbitrary set of values of variables from . By  we denote the CNF produced from  by setting values from  to corresponding variables. Let us consider the set . It is easy to see that the CNF  is satisfiable if and only if at least one CNF from the set  is satisfiable. Therefore, the set  is a partitioning of the SAT instance  \cite{Hyvarinen11}. We call the set  that produces the partitioning  a \textit{decomposition set}.

Now let us describe the approach to constructing time estimations for the processing of SAT partitionings that was proposed in \cite{DBLP:journals/corr/SemenovZ13}. First let us fix some particular SAT solving algorithm . Hereinafter we assume that  is a complete algorithm, i.e. it has finite runtime for an arbitrary input. Now consider an arbitrary CNF  over the set  of Boolean variables. Let  be an arbitrary decomposition set. Consider the set . Let us define the uniform distribution on this set. With a randomly selected vector  we associate the runtime of algorithm  given the CNF  as an input. This time can be considered as a value of some random variable which we denote below as . Since  is a complete algorithm, then the random variable  takes only finite values with some probabilities. Thus, the expected value of  is finite. By  denote the time required to process the SAT partitioning  sequentially by algorithm . In \cite{DBLP:journals/corr/SemenovZ13} it was shown that under listed conditions the following equality holds:


To estimate the expected value  it is possible to use the computational scheme of the Monte Carlo method in its classical form \cite{Metropolis49}. Namely, for an arbitrary decomposition set  we randomly select vectors  from . Then for each ,  we measure the time required by algorithm  to solve the SAT for CNF . We denote the time obtained by . After this we compute the value . In accordance with the theoretical basis of the Monte Carlo method \cite{Metropolis49,Kalos:109491}, when  is large enough, the value  becomes a good estimation of the expected value . Therefore it follows from \eqref{f4} that the value

is the estimation of time required by algorithm  to sequentially process the whole SAT partitioning  and this estimation is the more accurate the larger  is. Below we will refer to the function defined according to \eqref{f5} as to the \textit{predictive function}.

As we already noted above, different partitionings of the same SAT problem can have different values of . In practice it is important to be able to find partitionings that can be processed in affordable time. Below we will describe a scheme of automatic search for good partitionings that is based on the procedure minimizing the predictive function value in a special search space.

So we consider the satisfiability problem for some CNF . Let  be a set of all Boolean variables in this CNF and  be an arbitrary decomposition set. The set  can be represented by the binary vector . Here 

With an arbitrary vector  we associate the value of function  computed in the following manner. For vector  we construct the corresponding set  (it is formed by variables from  that correspond to  positions in ). Then we generate a random sample ,  and solve SAT for CNFs . For each of these SAT instances we measure  --- the runtime of algorithm  on the input . After this we calculate the value of  according to \eqref{f5}. As a result we have the value of  in the considered point of the search space.

Now we will solve the problem  over the set . Of course, the problem of search for the exact minimum of function  is extraordinarily complex. Therefore our main goal is to find in affordable time the points in  with relatively good values of function . For this purpose we propose two minimization algorithms. The first employs the simulated annealing algorithm, while the second is based on the tabu search concept. We will compare the effectiveness of these algorithms on the example of the problem of search for the good SAT partitioning for the SAT instances encoding the cryptanalysis of the A5/1 keystream generator (i.e. the inversion problem of \eqref{f3}). 

First we need to introduce the notation. By  we denote the search space, for example, , however, as we will see later, for the problems considered one can use the search spaces of much less power. The minimization of function  is considered as an iterative process of transitioning between the points of the search space:

By  we denote the neighborhood of point  of radius  in the search space . The point from which the search starts we denote as . The current Best Known Value of  is denoted by . The point in which the  was achieved we denote as . By  we denote the point the neighborhood of which is processed at the current moment. We call the point, in which we computed the value  a \textit{checked point}. The neighborhood  in which all the points are checked is called \textit{checked neighborhood}. Otherwise the neighborhood is called \textit{unchecked}. 

According to the scheme of the simulated annealing \cite{Kirkpatrick83optimizationby}, the transition from  to  is performed in two stages. First we choose a point  from . The point  becomes the point  with the probability denoted as . This probability is defined in the following way:

In the pseudocode of the algorithm demonstrated below the function that tests if the point  becomes , is called \texttt{PointAccepted} (this function returns the value of \texttt{true} if the transition occurs and \texttt{false} otherwise). The change of parameter  corresponds to decreasing the ``temperature of the environment'' \cite{Kirkpatrick83optimizationby} (in the pseudocode by \texttt{decreaseTemperature()} we denote the function which implements this procedure). Usually it is assumed that , , where . The process starts at some initial value  and continues until the temperature drops below some threshold value  (in the pseudocode the function that checks this condition is called \texttt{temperatureLimitReached()}). 


\begin{algorithm}[htb]
 \DontPrintSemicolon
 \SetKwData{false}{false}
 \SetKwData{true}{true}
 \SetKwData{bestValueUpdated}{bestValueUpdated}
 \SetKwFunction{PointAccepted}{PointAccepted}
 \SetKwFunction{timeExceeded}{timeExceeded}
 \SetKwFunction{temperatureLimitReached}{temperatureLimitReached}
 \SetKwFunction{decreaseTemperature}{decreaseTemperature}
 \caption{Simulated annealing algorithm for minimization of the predictive function}
 \KwIn{CNF , initial point }
 \KwOut{Pair , where  is a prediction for ,  is a corresponding decomposition set}
	\;
	\Repeat{\timeExceeded{} or \temperatureLimitReached{}} {
		\bestValueUpdated  \false\;
		\;
		\Repeat(\tcp*[f]{check neighborhood}){\bestValueUpdated}{
			 any unchecked  point from  \;
			compute \;
			mark  as checked point in \;
			\If{\PointAccepted{}}{
				\;
				\;
				\bestValueUpdated  \true\;
			}
			\If{( is checked) and (not \bestValueUpdated)}{
				\;
			}
			\decreaseTemperature{}\;
		}
	}
\Return{}\;
\end{algorithm}

In the other approach to the minimization of  we employed the tabu search scheme \cite{Glover:1997:TS:549765}. According to this approach the points from the search space, in which we already calculated the values of function  are stored in special tabu lists. When we try to improve the current Best Known Value of  in the neighborhood of some point  then for an arbitrary point  from the neighborhood we first check if we haven’t computed  earlier. If we haven’t and, therefore, the point  is not contained in tabu lists, then we compute . This strategy is justified in the case of the minimization of predictive function  because the computing of values of the function in some points of the search space is very expensive. The use of tabu lists makes it possible to significantly increase the number of points of the search space processed per time unit.

Let us describe the Tabu Search algorithm (TS-algorithm) for minimization  in more detail. To store the information about points, in which we already computed the value of  we use two tabu lists  and . The  list contains only points with checked neighborhoods. The  list contains checked points with unchecked neighborhoods. Below we present the pseudocode of the TS-algorithm for  minimization. 

\begin{algorithm}
 \DontPrintSemicolon
 \SetKwData{false}{false}
 \SetKwData{true}{true}
 \SetKwData{bestValueUpdated}{bestValueUpdated}
 \SetKwFunction{markPointInTabuLists}{markPointInTabuLists}
 \SetKwFunction{getPoint}{getPoint}
 \SetKwFunction{timeExceeded}{timeExceeded}
 \caption{Tabu search altorithm for minimization of the predictive function}
 \KwIn{CNF , initial point }
 \KwOut{Pair , where  is a prediction for ,  is a corresponding decomposition set}
	\;
	 \tcp*[r]{initialize tabu lists}
	\Repeat{\timeExceeded{} or } {
		\bestValueUpdated  \false\;
		\Repeat(\tcp*[f]{check neighborhood}){ is checked}{
			 any unchecked point from \;
			compute \;
			\markPointInTabuLists{} \tcp*[r]{update tabu lists}
			\If{}{
				\;
				\bestValueUpdated  \true\;
			}
		}
		\lIf{\bestValueUpdated} {
			\;
		}
		\lElse{
			 \getPoint{}\;
		}
	}
\Return{}\;
\end{algorithm}

In this algorithm the function \texttt{markPointInTabuLists} adds the point  to  and then marks  as checked in all neighborhoods of points from  that contain . If as a result the neighborhood of some point  becomes checked, the point  is removed from  and is added to . If we have processed all the points in the neighborhood of  but could not improve the  then as the new point  we choose some point from . It is done via the function \texttt{getPoint()}. To choose the new point in this case one can use various heuristics. At the moment the TS-algorithm employs the following heuristics: it chooses the point for which the total conflict activity of Boolean variables, contained in the corresponding decomposition set, is the largest.

As we already mentioned above, taking into account the features of the considered SAT problems makes it possible to significantly decrease the size of the search space. For example, knowing the so called Backdoor Sets \cite{DBLP:conf/ijcai/WilliamsGS03} can help in that matter. In case of the SAT instances encoding the inversion problems of functions of the kind  the set  formed by the variables encoding the inputs of the Boolean circuit  is the so called Strong Unit Propagation Backdoor Set \cite{DBLP:journals/constraints/JarvisaloJ09}. It means that if we use  as a decomposition set, then the CDCL solver will solve SAT for any CNF of the kind ,  on the preprocessing stage, i.e. very fast. Therefore the point  that corresponds to the set  can be used as an initial point in the predictive function minimization procedure. Moreover, it is possible to reduce the search space to the set . In all our computational experiments we followed this path. 

\section{Computing Cluster Implementation of the Monte Carlo Algorithms for Constructing SAT Partitionings}

The minimization algorithms suggested above were implemented as a parallel MPI-program \textsc{\textsc{PDSAT}} \cite{DBLP:journals/corr/SemenovZ13}. In \textsc{\textsc{PDSAT}} there is one leader process, all the other are follower processes (each process corresponds to 1 CPU core).The leader process in the \textsc{PDSAT} program selects point  of the search space and creates random sample . Follower processes solve corresponding SAT instances of the kind ,  (see section 4). The value of predictive function is always computed assuming that all the subproblems from the partitioning  will be processed by 1 CPU core. 

We considered the inversion problem of the function \eqref{f3}: given the 114 bits of keystream we needed to find the secret key of length 64 bits, which produces this keystream (in accordance with the A5/1 algorithm). The \textsc{\textsc{PDSAT}} program was used to find partitionings with good time estimations for CNFs encoding this problem. The computational experiments were performed on the computing cluster Blackford ISDCT SB RAS\footnote{http://hpc.icc.ru/} --– in each experiment \textsc{\textsc{PDSAT}} was launched for 1 day using 16 Intel Xeon 5345 EM64T CPUs (64 cores in total). We used random samples of size .

On Figures \ref{a5_1_set_S1}, \ref{a5_1_set_S2}, \ref{a5_1_set_S3} three decompositions sets are shown. We described the first set (further referred to as S1) in the paper \cite{DBLP:conf/pact/SemenovZBP11} earlier. This set (of 31 variables) was constructed ``manually'' based on the analysis of algorithmic features of the A5/1 generator. The second one (S2), consisting of 31 variables, was found as a result of the minimization of  by the simulated annealing algorithm (see section 4). The third set (S3), consisting of 32 variables, was found as a result of minimization of  by the TS algorithm. In the table \ref{a5-1_results} the values of  (in seconds) for all three sets are shown. Note that each of sets  and  was found for one 114 bit fragment of keystream that was generated according to the A5/1 algorithm for a randomly chosen 64-bit secret key.

\begin{figure}[!ht]
	\centering
		\includegraphics[width=7.5cm]{31_var_choose_a5_1_scheme.eps}
	\caption{Decomposition set S1}
	\label{a5_1_set_S1}
\end{figure}

\begin{figure}[!ht]
	\centering
		\includegraphics[width=7.5cm]{31_var_choose_a5_1_scheme_simul.eps}
	\caption{Decomposition set S2}
	\label{a5_1_set_S2}
\end{figure}

\begin{figure}[!ht]
	\centering
		\includegraphics[width=7.5cm]{32_var_choose_a5_1_scheme.eps}
	\caption{Decomposition set S3}
	\label{a5_1_set_S3}
\end{figure}

\begin{table}
\caption{Decomposition sets fot the A5/1 cryptanalysis problem and corresponding values of the predictive function. }
\label{a5-1_results}
\centering
\begin{tabular}{p{1.8cm}|p{1.8cm}|p{1.8cm}}
Set & Power of set & \\
\hline 
\hline
S1 & 31 & 4.45140e+08 \\
\hline 
S2 & 31 & 4.78318e+08 \\
\hline 
S3 & 32 & 4.64428e+08 \\
\end{tabular}
\end{table}

\section{Cryptanalysis of the A5/1 Keystream Generator  in the Volunteer Computing Project SAT@home}

Volunteer computing \cite{DBLP:journals/jnca/DurraniS14} is a type of distributed computing which uses computational resources of PCs of private persons called volunteers. Each volunteer computing project is designed to solve one or several hard problems. When PC is connected to the project, all the calculations are performed automatically and do not inconvenience user since only idle resources of PC are used.

The first volunteer project was the GIMPS\footnote{http://www.mersenne.org/} project launched in 1996. Nowadays the most popular platform for organizing volunteer computing projects is BOINC \cite{DBLP:conf/grid/Anderson04} that was developed in Berkeley in 2002. Today there are about 70 active volunteer projects, the majority of them based on BOINC. The total performance of these projects exceeds 10 petaflops. Among the most important results obtained via volunteer computing are the discoveries of the largest prime number in the GIMPS project and of several radiopulsars in the Einstein@home\footnote{http://einstein.phys.uwm.edu/} project. 

Volunteer computing project consists of the following basic parts: server daemons, database, web site and client application. Client application should have versions for the widespread computing platforms. One of the attractive features of volunteer computing is its low cost --- to maintatin a project one needs only a dedicated server working 24/7. Another advantage of this type of computing is that volunteer project can solve some particular hard problem for months or even years with good average performance.

SAT@home\footnote{http://sat.isa.ru/pdsat/} \cite{journals/csj/Posypkin12} is volunteer computing project aimed at solving hard combinatorial problems that can be effectively reduced to SAT. It was launched on September 29, 2011 by ISDCT SB RAS and IITP RAS. SAT@home was developed with the help of BOINC platform  \cite{DBLP:conf/grid/Anderson04} and SZTAKI Desktop Grid package \cite{sztaki6003}. On February 7, 2012 SAT@home was added to the official list of BOINC projects\footnote{http://boinc.berkeley.edu/projects.php} with alpha status. Recently its status was improved to beta.

The SAT@home server uses a number of standard BOINC daemons responsible for sending and processing tasks (transitioner, feeder, scheduler, etc.). Such daemons as work generator, validator and assimilator were implemented taking into account the specificity of the project. The work generator decomposes the original SAT problem to subproblems according to the partitioning approach (see section 4). It creates 2 copies of each task in accordance with the concept of redundant calculations used in BOINC. The validator checks the correctness of the results, and the assimilator processes correct results. The client application is based on the SAT solver \textsc{MiniSat} \cite{DBLP:conf/sat/EenS03}.

The characteristics of the SAT@home project as of 12 of November 2014 are (according to BOINCstats\footnote{http://boincstats.com/}):
\begin{itemize}
	\item 3113 active PCs (active PC in volunteer computing is a PC that sent at least one result in last 30 days) about 80\% of them use Microsoft Windows OS;
	\item 1337 active users (active user is a user that has at least one active PC);
	\item	versions of the client application for CPU: Windows x86, Windows x86-64, Linux x86, Linux x86-64;
	\item average real performance: 3.2 teraflops, maximal performance: 7.9 teraflops.
\end{itemize}
The dynamics of the real performance of SAT@home can be seen at the SAT@home performance page\footnote{http://sat.isa.ru/pdsat/performance.php}.

The experiment consisting in solving 10 inversion problems of function  was held in SAT@home from December 2011 to May 2012. To construct the corresponding tests we used the known rainbow-tables for the A5/1 algorithm\footnote{https://opensource.srlabs.de/projects/a51-decrypt}. These tables provide about 88\% probability of success when analyzing 8 bursts of keystream (i.e. 914 bits). We randomly generated 1000 problems and applied the rainbow-tables technique to analyze 8 bursts of keystream, generated by A5/1. Among these 1000 problems the rainbow-tables could not find the secret key for 125 problems. From these 125 problems we randomly chose 10 and in the computational experiments applied the SAT approach to the analysis of first bursts of the corresponding keystream fragments (114 bits). For each SAT instance we constructed the partitioning generated by the S1 decomposition set (see Figure \ref{a5_1_set_S1}) and processed it in the SAT@home project. All 10 instances constructed this way were successfully solved in SAT@home in about 5 months (the average performance of the project at that time was about 2 teraflops). After finding the satisfying assignment the processing of the corresponding partitioning was interrupted. On average to solve one SAT instance the SAT@home project had to process about 1 billion subproblems from the corresponding partitioning.

The second experiment on the cryptanalysis of A5/1 was launched in SAT@home in May 2014. It was done with the purpose of testing the decomposition sets found automatically with the help of the procedures described in section 4. In particular we took the decomposition set S3 (see Figure \ref{a5_1_set_S3}) that was constructed for the SAT instance encoding the cryptanalysis of one particular 114-bit A5/1 keystream fragment, and then used this set to solve the series of tests with 10 similar instances. On September 26 we successfully solved in SAT@home all 10 instances from the considered series. It should be noted that in all the experiments the time required to solve the problem agrees with the predictive function value computed for the S3 decomposition set.

The problem of cryptanalysis of the generator A5/1 is also interesting because the same keystream of an arbitrary length can be generated from different secret keys. This fact was noted by Jovan Golic in \cite{Golic:1997:CAA:1754542.1754566}. We refer to these situations as ``collisions'' using the evident analogy with the corresponding notion from the theory of hash functions. The approach presented in our paper allows us to solve the problem of finding all the collisions of the generator A5/1 for a given fragment of a keystream. Using the SAT@home project collisions for several SAT instance were found (see Table \ref{Collisions}).

\begin{table}
\renewcommand{\arraystretch}{1.3}
\caption{Original keys and collisions of the generator A5/1 \mbox{(in hexadecimal format)} }
\label{Collisions}
\centering
\begin{tabular}{c|c|c}
\textbf{instance} & \textbf{original secret key} & \textbf{collision} \\ 
\hline
1 & F43FF04CD4F45660 & 7A1FF04CD4F45660 \\
\hline 
2 & B95654F2242C6DF1 & 5CAB34F2242C6DF1 \\
\hline 
3 & 67685940B034EF78 & B3B43940B034EF78 \\
\hline 
4 & 41038717BBA05E57 & FB164EC44124398E \\
\end{tabular}
\end{table}

Our computational experiments clearly demonstrate that the proposed method of automatic search for decomposition sets makes it possible to
construct SAT partitionings with the properties close to that of ``reference'' partitionings, i.e. partitionings constructed based on the analysis of algorithmic features of the considered cryptographic functions.

\section{Related Works}
In the book \cite{DBLP:series/faia/2009-185} the Boolean satisfiability problem is considered in the variety of aspects. The problems regarding the organization of the SAT solving process in distributed computing environments have been studied in \cite{DBLP:journals/jsc/ZhangBH96,DBLP:journals/pc/BlochingerSK03,DBLP:conf/sat/HyvarinenJN06,DBLP:journals/pc/ChrabakhW06,DBLP:conf/hvc/HeuleKWB11,Hyvarinen11}. The partitioning approach was analyzed in detail in \cite{DBLP:conf/sat/HyvarinenJN06,DBLP:conf/lpar/HyvarinenJN10,Hyvarinen11}. A number of approaches to estimating the runtime for Constraint Satisfaction Problem and SAT can be found in \cite{DBLP:journals/tcad/AloulSS03,DBLP:conf/aaai/KilbySTW06,DBLP:conf/sat/HaimW08,DBLP:journals/jair/XuHHL08}.

The first example of the application of the SAT approach to the cryptanalysis problems is \cite{DBLP:journals/jar/MassacciM00}. In \cite{techrep/Mcdonald07,DBLP:conf/sat/EibachPV08,DBLP:conf/sat/SoosNC09,DBLP:conf/tools/Soos10} several estimations of time required to solve SAT for CNFs encoding the Bivium cipher cryptanalysis were proposed. The ideas underlying the cited works are close to the ones we use. The main distinction between our results and that of \cite{techrep/Mcdonald07,DBLP:conf/sat/EibachPV08,DBLP:conf/sat/SoosNC09,DBLP:conf/tools/Soos10} is that the decomposition sets that were constructed in those papers were found based on either the structure of the encryption algorithm or randomly. Our main achievement consists in constructing automatic procedures to solve this problem.

The most effective in practice method of cryptanalysis of A5/1 is the Rainbow-method, partial description of which can be found on the A5/1 Cracking Project site\footnote{https://opensource.srlabs.de/projects/a51-decrypt}. In \cite{Guneysu:2008:CC:1446228.1446266} the description of a number of techniques, used in the A5/1 Cracking Project to construct Rainbow tables, was presented. The cryptanalysis of A5/1 via Rainbow tables has the success rate of approximately 88\% if one uses 8 bursts of keystream. The success rate of the Rainbow method if one has only 1 burst of keystream is about 24\%. In all our computational experiments we analyzed the keystream fragment of size 114 bits, i.e. one burst. In \cite{DBLP:conf/pact/SemenovZBP11} we described our first experience on the application of the SAT approach to A5/1 cryptanalysis in the specially constructed grid system BNB-Grid. The decomposition set used in \cite{DBLP:conf/pact/SemenovZBP11} was constructed manually based on the peculiarities of the A5/1 algorithm.
 
The grid systems aimed at solving SAT are relatively rare. In \cite{DBLP:journals/grid/SchulzB10} a desktop grid for solving SAT which used conflict clauses exchange via a peer-to-peer protocol was described. Apparently, \cite{DBLP:conf/grid/BlackB11} became the first paper about the use of a desktop grid based on the BOINC platform for solving SAT. Unfortunately, it did not evolve into a full-fledged volunteer computing project. As we noted above, the predecessor of the SAT@home was the BNB-Grid system \cite{DBLP:journals/ife/EvtushenkoPS09,DBLP:conf/pact/SemenovZBP11}, that was used to solve first large scale cryptographic problems in 2009. 

\section{Conclusions}
In the present paper we describe the volunteer computing project SAT@home designed specifically for the purpose of long-term solving of hard SAT instances. We also propose the partitioning method for SAT instances encoding inversion problems of cryptographic functions. The applicability of proposed technologies is demonstrated on the problem of cryptanalysis of the A5/1 keystream generator. In the SAT@home project we managed to solve several cryptanalysis instances for this generator. In all the experiments we analyzed only the first burst (114 bits) of keystream.

\section{Acknowledgements}
Authors thank Stepan Kochemazov for numerous valuable comments that allowed us to significantly improve the quality of the paper, Alexey Ignatiev and Irina Bogachkova for constructive feedback and helpful discussions. This work was partly supported by Russian Foundation for Basic Research (grants 14-07-00403-a and 14-07-31172-mol-a) and by the President of Russian Federation grants for young scientists (SP-1855.2012.5 and SP-3667.2013.5).

\bibliographystyle{splncs03}
\bibliography{j_network_computer_applications_fin}

\end{document}
