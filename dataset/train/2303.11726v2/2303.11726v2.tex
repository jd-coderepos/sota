

\label{sec:experiments}

\begin{table*}[ht]
\center
\small
\setlength{\tabcolsep}{7pt}
\resizebox{6.0in}{!}{
\begin{tabular}{l l | c c c | c c c}
    \hline 
    \multirow{2}{*}{Method} & Intermediate & \multicolumn{3}{c}{H3.6M} & \multicolumn{3}{|c}{3DPW} \\

    \cline{3-5} \cline{6-8}
    & Representation & MPVE & MPJPE & PA-MPJPE & MPVE & MPJPE & PA-MPJPE \\
    \hline 
     Arnab \etal \cite{arnab2019exploiting} CVPR'19  & 2D skeleton & - & 77.8 & 54.3 & - & - & 72.2 \\
     HMMR \cite{kanazawa2019learning} CVPR'19 & - & - & - & 56.9 & 139.3 & 116.5 & 72.6 \\
     DSD-SATN \cite{sun2019human} ICCV'19 & 3D skeleton & - & 59.1 & 42.4 & - & - & 69.5 \\
     VIBE \cite{kocabas2020vibe} CVPR'20 & - & - & 65.9 & 41.5 & 99.1 & 82.9 & 51.9 \\
     TCMR \cite{choi2021beyond} CVPR'21 & - & - & 62.3 & 41.1 & 102.9 & 86.5 & 52.7 \\
     MAED \cite{wan2021encoder} ICCV'21 & 3D skeleton & - & 56.3 & 38.7 & 92.6 & 79.1 & 45.7 \\
    \hline
    SMPLify \cite{bogo2016keep} ECCV'16 & 2D skeleton & - & - & 82.3 & - & - & - \\
HMR \cite{kanazawa2018end} CVPR'18 & - & 96.1 & 88.0 & 56.8 & 152.7 & 130.0 & 81.3 \\
    GraphCMR \cite{kolotouros2019convolutional} CVPR'19 & 3D vertices & - & - & 50.1 & - & - & 70.2 \\
    SPIN \cite{kolotouros2019learning} ICCV'19 & - & - & - & 41.1 & 116.4 & 96.9 & 59.2 \\
    DenseRac \cite{xu2019denserac} ICCV'19 & IUV image & - & 76.8 & 48.0 & - & - & - \\
    DecoMR \cite{zeng20203d} CVPR'20 & IUV image & - & 60.6 & 39.3 & - & - & - \\
    ExPose \cite{choutas2020monocular} ECCV'20 & - & - & - & - & - & 93.4 & 60.7 \\
    Pose2Mesh \cite{choi2020pose2mesh} ECCV'20 & 3D skeleton & 85.3 & 64.9 & 46.3 & 106.3 & 88.9 & 58.3 \\
    I2L-MeshNet \cite{moon2020i2l} ECCV'20 & 3D vertices & 65.1 & 55.7 & 41.1 & 110.1 & 93.2 & 57.7 \\
    PC-HMR \cite{luan2021pc} AAAI'21 & 3D skeleton & - & - & - & 108.6 & 87.8 & 66.9  \\
    HybrIK \cite{li2021hybrik} CVPR'21 & 3D skeleton & 65.7 & 54.4 & 34.5 & 86.5 & 74.1 & 45.0  \\
    METRO \cite{lin2021end} CVPR'21 & 3D vertices & - & 54.0 & 36.7 & 88.2 & 77.1 & 47.9 \\
    ROMP \cite{sun2021monocular} ICCV'21 & - & - & - & - & 108.3 & 91.3 & 54.9 \\
    Mesh Graphormer\cite{Lin_2021_ICCV} ICCV'21 & 3D vertices & - & 51.2 & 34.5 & 87.7 & 74.7 & 45.6 \\
    PARE \cite{Kocabas_2021_ICCV} ICCV'21 & Segmentation & - & - & - & 88.6 & 74.5 & 46.5 \\
    THUNDR \cite{zanfir2021thundr} ICCV'21 & 3D markers & - & 55.0 & 39.8 & 88.0 & 74.8 & 51.5 \\
    PyMaf \cite{zhang2021pymaf} ICCV'21 & IUV image & - & 57.7 & 40.5 & 110.1 & 92.8 & 58.9 \\
    ProHMR \cite{Kolotouros_2021_ICCV} ICCV'21 & - & - & - & 41.2 & - & - & 59.8\\
    OCHMR \cite{Khirodkar_2022_CVPR} CVPR'22 & 2D heatmap & - & - & - & 107.1 & 89.7 & 58.3 \\
    3DCrowdNet \cite{Choi_2022_CVPR} CVPR'22 & 3D skeleton & - & - & - & 98.3 & 81.7 & 51.5 \\
    CLIFF \cite{li2022cliff} ECCV'22 & - & - & \textbf{47.1} & 32.7 & 81.2 & 69.0 & 43.0 \\
    FastMETRO \cite{cho2022FastMETRO} ECCV'22 & 3D vertices & - & 52.2 & 33.7 & 84.1 & 73.5 & 44.6 \\
    VisDB \cite{yao2022learning} ECCV'22 & 3D vertices & - & 51.0 & 34.5 & 85.5 & 73.5 & 44.9 \\

    \rowcolor{mygray}
    \textbf{Ours} & Virtual marker & \textbf{58.0} & 47.3 & \textbf{32.0} & \textbf{77.9} & \textbf{67.5} & \textbf{41.3} \\

    \hline 
\end{tabular}}
\caption{Comparison to the state-of-the-arts on H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets.  means using temporal cues. The methods are not strictly comparable because they may have different backbones and training datasets. We provide the numbers only to show proof-of-concept results.}
\label{tab:state_of_the_art}
\vspace{-0.4cm}
\end{table*}


\subsection{Datasets and metrics}
\label{subsec:dataset}
\noindent\textbf{H3.6M \cite{h36m_pami}.} We use (S1, S5, S6, S7, S8) for training and (S9, S11) for testing. 
As in \cite{kanazawa2018end, choi2020pose2mesh, lin2021end, Lin_2021_ICCV}, we report MPJPE and PA-MPJPE for poses that are derived from the estimated meshes. We also report Mean Per Vertex Error (MPVE) for the whole mesh.  \\


\noindent\textbf{3DPW \cite{vonMarcard2018}} is collected in natural scenes. 
Following the previous works \cite{lin2021end, Lin_2021_ICCV, Kocabas_2021_ICCV, zanfir2021thundr}, we use the train set of 3DPW to learn the model and evaluate on the test set. The same evaluation metrics as H3.6M are used.  \\

\noindent\textbf{SURREAL \cite{varol2017learning}} is a large-scale synthetic dataset with GT SMPL annotations and has diverse samples in terms of body shapes, backgrounds, \etc. We use its training set to train a model and evaluate the test split following \cite{choi2020pose2mesh}.


\subsection{Implementation Details}
\label{subsec:implementation}
We learn  virtual markers on the H3.6M \cite{h36m_pami} training set. We use the same set of markers for all datasets instead of learning a separate set on each dataset. Following \cite{kanazawa2018end, choi2020pose2mesh, moon2020i2l, zanfir2021thundr, kolotouros2019convolutional, kocabas2020vibe, Lin_2021_ICCV, lin2021end}, we conduct mix-training by using MPI-INF-3DHP \cite{mehta2017monocular}, UP-3D \cite{lassner2017unite}, and COCO \cite{lin2014microsoft} training set for experiments on the H3.6M and 3DPW datasets. 
We adapt a 3D pose estimator \cite{sun2018integral} with HRNet-W48 \cite{sun2019deep} as the image feature backbone for estimating the 3D virtual markers. We set the number of voxels in each dimension to be , \ie  for 3D heatmaps. Following \cite{kanazawa2018end, kolotouros2019convolutional, moon2020i2l}, we crop every single human region from the input image and resize it to .
We use Adam \cite{kingma2015adam} optimizer to train the whole framework for  epochs with a batch size of . The learning rates for the two branches are set to  and , respectively, which are decreased by half after the  epoch.
Please refer to the supplementary for more details.





\begin{table}[t]
\center
\small
\setlength{\tabcolsep}{2pt}
\resizebox{3.4in}{!}{
\begin{tabular}{l l | c c c}
    \hline
\multirow{2}{*}{Method} & Intermediate & \multirow{2}{*}{MPVE} & \multirow{2}{*}{MPJPE} & \multirow{2}{*}{PA-MPJPE} \\
    & Representation &  &  &  \\
    \hline
    HMR \cite{kanazawa2018end} CVPR'18 & - & 85.1 & 73.6 & 55.4 \\
    BodyNet \cite{varol2018bodynet} ECCV'18 & Skel. + Seg. & 65.8 & - & - \\
    GraphCMR \cite{kolotouros2019convolutional} CVPR'19 & 3D vertices & 103.2 & 87.4 & 63.2  \\
    SPIN \cite{kolotouros2019learning} ICCV'19 & - & 82.3 & 66.7 & 43.7 \\
    DecoMR \cite{zeng20203d} CVPR'20 & IUV image & 68.9 & 52.0 & 43.0 \\
    Pose2Mesh \cite{choi2020pose2mesh} ECCV'20 & 3D skeleton & 68.8 & 56.6 & 39.6 \\
    PC-HMR \cite{luan2021pc} AAAI'21 & 3D skeleton & 59.8 & 51.7 & 37.9  \\
     DynaBOA \cite{guan2022out} TPAMI'22 & - & 70.7 & 55.2 & 34.0 \\
    \rowcolor{mygray}
    \textbf{Ours} & Virtual marker & \textbf{44.7} & \textbf{36.9} & \textbf{28.9} \\
    
    \hline 
\end{tabular}}
\caption{Comparison to the state-of-the-arts on SURREAL \cite{varol2017learning} dataset.  means training on the test split with 2D supervisions. ``Skel. + Seg.'' means using skeleton and segmentation together.}
\label{tab:state_of_the_art_surreal}
\vspace{-0.4cm}
\end{table}

\subsection{Comparison to the State-of-the-arts}
\label{subsec:sota}

\noindent\textbf{Results on H3.6M.}
Table \ref{tab:state_of_the_art} compares our approach to the state-of-the-art methods on the H3.6M dataset. Our method achieves competitive or superior performance. In particular, it outperforms the methods that use skeletons (Pose2Mesh \cite{choi2020pose2mesh}, DSD-SATN \cite{sun2019human}), body markers (THUNDR) \cite{zanfir2021thundr}, or IUV image \cite{zeng20203d, zhang2021pymaf} as proxy representations, demonstrating the effectiveness of the virtual marker representation. \\

\noindent\textbf{Results on 3DPW.}
We compare our method to the state-of-the-art methods on the 3DPW dataset in Table \ref{tab:state_of_the_art}. Our approach achieves state-of-the-art results among all the methods, validating the advantages of the virtual marker representation over the skeleton representation used in Pose2Mesh \cite{choi2020pose2mesh}, DSD-SATN \cite{sun2019human}, and other representations like IUV image used in PyMAF \cite{zhang2021pymaf}. In particular, our approach outperforms I2L-MeshNet \cite{moon2020i2l}, METRO \cite{lin2021end}, and Mesh Graphormer \cite{Lin_2021_ICCV} by a notable margin, which suggests that 
virtual markers are more suitable and effective representations than detecting all vertices directly as most of them are not discriminative enough to be accurately detected. \\

\noindent\textbf{Results on SURREAL.}
This dataset has more diverse samples in terms of body shapes. The results are shown in Table \ref{tab:state_of_the_art_surreal}. Our approach outperforms the state-of-the-art methods by a notable margin, especially in terms of MPVE. Figure \ref{fig:surreal_shape} shows some challenging cases without cherry-picking. The skeleton representation loses the body shape information so the method \cite{choi2020pose2mesh} can only recover mean shapes. In contrast, our approach generates much more accurate mesh estimation results. 

\begin{table}[t]
\center
\small
\setlength{\tabcolsep}{10pt}
\resizebox{2.8in}{!}{
\begin{tabular}{c | l | c | c }
    \hline 
    \multirow{2}{*}{No.} & Intermediate & \multicolumn{2}{c}{MPVE} \\

    \cline{3-4}
    & Representation & H3.6M & SURREAL \\
    \hline
    (a) & Skeleton & 64.4 & 53.6 \\ 
    (b) & Rand virtual marker & 63.0 & 50.1 \\
    \rowcolor{mygray}
    (c) & Virtual marker & \textbf{58.0} & \textbf{44.7}\\
    \hline 
\end{tabular}}
\caption{Ablation study of the virtual marker representation for our approach on H3.6M and SURREAL datasets. 
``Skeleton'' means the sparse landmark joint representation is used. 
``Rand virtual marker'' means the virtual markers are randomly selected from all the vertices without learning. (c) is our method, where the learned virtual markers are used. }
\label{tab:ba_effect}
\vspace{-0.4cm}
\end{table}


\subsection{Ablation study}
\label{subsec:ablation}
\noindent\textbf{Virtual marker representation.}
\label{subsubsec:ablation_effect}
We compare our method to two baselines in Table \ref{tab:ba_effect}. First, in baseline (a), we replace the virtual markers of our method with the skeleton representation. The rest are kept the same as ours (c). Our method achieves a much lower MPVE than the baseline (a), demonstrating that the virtual markers help to estimate body shapes more accurately than the skeletons. In baseline (b), we randomly sample  from the  mesh vertices as virtual markers. We repeat the experiment five times and report the average number. We can see that the result is worse than ours, which is because the randomly selected vertices may not be expressive to reconstruct the other vertices or can not be accurately detected from images as they lack distinguishable visual patterns. The results validate the effectiveness of our learning strategy. 





\begin{figure}[t]
	\centering
	\includegraphics[width=3.2in]{imgs/experiments/joint_diff.pdf}
	\caption{Mesh estimation results of different methods on H3.6M test set. Our method with virtual marker representation gets better shape estimation results than Pose2Mesh which uses skeleton representation. Note the waistline of the body and the thickness of the arm.}
	\label{fig:joint_diff}
\vspace{-0.4cm}
\end{figure}




Figure \ref{fig:surreal_shape} shows some qualitative results on the SURREAL test set. The meshes estimated by the baseline which uses skeleton representation, \ie Pose2Mesh \cite{choi2020pose2mesh}, have inaccurate body shapes. This is reasonable because the skeleton is oversimplified and has very limited capability to recover shapes. Instead, it implicitly learns a mean shape for the whole training dataset. In contrast, the mesh estimated by using virtual markers has much better quality due to its strong representation power and therefore can handle different body shapes elegantly. 
Figure \ref{fig:joint_diff} also shows some qualitative results on the H3.6M test set. For clarity, we draw the intermediate representation (blue balls) in it as well.













\begin{figure}[t]
	\centering
	\includegraphics[width=3.3in]{imgs/experiments/different_K_vis.pdf}
	\caption{Visualization of the learned virtual markers of different numbers of , from left to right, respectively.
    }
	\label{fig:different_K_vis}
\end{figure}






\vspace{0.5em}
\noindent\textbf{Number of virtual markers.} 
\label{subsubsec:ablation_K}
We evaluate how the number of virtual markers affects estimation quality on H3.6M \cite{h36m_pami} dataset. Figure \ref{fig:different_K_vis} visualizes the learned virtual markers, which are all located on the body surface and close to the extreme points of the mesh. This is expected as mentioned in Section \ref{subsec:body_arche}.  Table \ref{tab:different_K} (GT) shows the mesh reconstruction results when we have GT 3D positions of the virtual markers in objective (\ref{eq:aa}). When we increase the number of virtual markers, both mesh reconstruction error (MPVE) and the regressed landmark joint error (MPJPE) steadily decrease. This is expected because using more virtual markers improves the representation power. However, using more virtual markers cannot guarantee smaller estimation errors when we need to estimate the virtual marker positions from images as in our method. This is because the additional virtual markers may have large estimation errors which affect the mesh estimation result. The results are shown in Table \ref{tab:different_K} (Det). Increasing the number of virtual markers  steadily reduces the MPVE errors when  is smaller than . However, if we keep increasing , the error begins to increase. This is mainly because some of the newly introduced virtual markers are difficult to detect from images and therefore bring errors to mesh estimation. 



\begin{table}[t]
\center
\small
\setlength{\tabcolsep}{13pt}
\resizebox{3.0in}{!}{
\begin{tabular}{c | c | c | c | c }
    \hline 
    \multirow{2}{*}{} & \multicolumn{2}{c|}{GT}  & \multicolumn{2}{c}{Det} \\

    \cline{2-3} \cline{4-5}
    & MPVE & MPJPE & MPVE & MPJPE\\
    \hline
    16 & 46.8 & 39.8 & 58.7 & 47.8 \\ 
    32 & 20.1 & 14.2 & 58.2 & 48.3 \\
    64 & 11.0 & 7.5 & \textbf{58.0} & \textbf{47.3} \\
    96 & \textbf{9.9} & \textbf{5.6} & 59.6 & 48.2\\
    \hline 
\end{tabular}}
\caption{Ablation study of the different number of virtual markers () on H3.6M \cite{h36m_pami} dataset. (GT) Mesh reconstruction results when GT 3D positions of the virtual markers are used in objective (\ref{eq:aa}). (Det) Mesh estimation results obtained by our proposed framework when we use different numbers of virtual markers (). 
}
\label{tab:different_K}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/experiments/blending_diff.pdf}
	\caption{Mesh estimation comparison results when using (a) fixed coefficient matrix , and (b) updated . 
    Please zoom in to better see the details. 
    }
	\label{fig:blending_nr_diff}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=6.8in]{imgs/experiments/quality_result.pdf}
	\caption{ \textbf{Top:} Meshes estimated by our approach on images from 3DPW test set. The rightmost case in the dashed box shows a typical failure. \textbf{Bottom:} Meshes estimated by our approach on Internet images with challenging cases (extreme shapes or in a long dress).}
	\label{fig:quality_result_w_failure}
\vspace{-0.3cm}
\end{figure*}





\vspace{0.5em}
\noindent\textbf{Coefficient matrix.}
\label{subsubsec:blending_effect}
We compare our method to a baseline which uses the fixed coefficient matrix . 
We show the quality comparison in Figure \ref{fig:blending_nr_diff}. We can see that the estimated mesh by a fixed coefficient matrix (a) has mostly correct pose and shape but there are also some artifacts on the mesh while using the updated coefficient matrix (b) can get better mesh estimation results. 
As shown in Table \ref{tab:quan_bm_nr_effect}, using a fixed coefficient matrix gets larger MPVE and MPJPE errors than using the updated coefficient matrix. This is caused by the estimation errors of virtual markers when occlusion happens, which is inevitable since the virtual markers on the back will be self-occluded by the front body. As a result, inaccurate marker positions would bring large errors to the final mesh estimates if we directly use the fixed matrix.



\begin{table}[]
\center
\small
\setlength{\tabcolsep}{4pt}
\renewcommand\arraystretch{1.35}
\resizebox{3.0in}{!}{
\begin{tabular}{l l | c c |c c}
    \hline 
    No. & Method & Fixed  & Updated  & MPVE & MPJPE \\
    \hline
    (a) & Ours (fixed) & \cmark & \xmark  & 64.7 & 51.6 \\
    \rowcolor{mygray}
    (b) & Ours  & \xmark & \cmark  & \textbf{58.0} & \textbf{47.3} \\
    \hline 
\end{tabular}}
\caption{Ablation study of the coefficient matrix for our approach on H3.6M dataset. ``fixed'' means using the fixed coefficient matrix  to reconstruct the mesh. 
}
\label{tab:quan_bm_nr_effect}
\end{table}












\subsection{Qualitative Results}


\label{subsec:quality}
Figure \ref{fig:quality_result_w_failure} (top) presents some meshes estimated by our approach on natural images from the 3DPW test set. The rightmost case shows a typical failure where our method has a wrong pose estimate of the left leg due to heavy occlusion. We can see that the failure is constrained to the local region and the rest of the body still gets accurate estimates. We further analyze how inaccurate virtual markers would affect the mesh estimation, \ie when part of human body is occluded or truncated. According to the finally learned coefficient matrix  of our model, we highlight the relationship weights among virtual markers and all vertices in Figure \ref{fig:locality}. We can see that our model actually learns \emph{local and sparse} dependency between each vertex and the virtual markers, \eg for each vertex, the virtual markers that contribute the most are in a near range as shown in Figure \ref{fig:locality} (b). Therefore, in inference, if a virtual marker has inaccurate position estimation due to occlusion or truncation, the dependent vertices may have inaccurate estimates, while the rest will be barely affected. Figure \ref{fig:body_arche} (right) shows more examples where occlusion or truncation occurs, and our method can still get accurate or reasonable estimates robustly. Note that when truncation occurs, our method still guesses the positions of the truncated virtual markers.

\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{imgs/experiments/locality.pdf}
	\caption{(a) For each virtual marker (represented by a star), we highlight the top 30 most affected vertices (represented by a colored dot) based on average coefficient matrix . (b) For each vertex (dot), we highlight the top 3 virtual markers (star) that contribute the most. We can see that the dependency has a strong locality which improves the robustness when some virtual markers cannot be accurately detected. }
	\label{fig:locality}
\vspace{-0.3cm}
\end{figure}

Figure \ref{fig:quality_result_w_failure} (bottom) shows our estimated meshes on challenging cases, which indicates the strong generalization ability of our model on diverse postures and actions in natural scenes. Please refer to the supplementary for more quality results. Note that since the datasets do not provide supervision of head orientation, face expression, hands, or feet, the estimates of these parts are just in canonical poses inevitably. Apart from that, most errors are due to inaccurate 3D virtual marker estimation which may be addressed using more powerful estimators or more diverse training datasets in the future. 






