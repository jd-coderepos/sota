\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{microtype}

\usepackage[pdftex]{graphicx}
\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathabx}

\usepackage{fullpage}

\newcommand{\eps}{\varepsilon}
\newcommand{\Ot}{\tilde{O}}
\def\polylog{\operatorname{polylog}}
\def\poly{\operatorname{poly}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\usepackage{authblk}
\title{On the Hardness of Partially Dynamic Graph Problems and Connections to
Diameter}

\author{SÃ¸ren Dahlgaard\thanks{Research partly supported by Mikkel Thorup's
        Advanced Grant DFF-0602-02499B from the Danish Council for Independent
Research under the Sapere Aude research career programme.}}
\affil{University of Copenhagen\\\texttt{soerend@di.ku.dk}}
\date{}



\begin{document}

\maketitle

\begin{abstract}
    Conditional lower bounds for dynamic graph problems has received a great
    deal of attention in recent years. While many results are now known for the
    fully-dynamic case and such bounds often imply worst-case bounds for the
    partially dynamic setting, it seems much more difficult to prove amortized
    bounds for incremental and decremental algorithms. In this paper we
    consider partially dynamic versions of three classic problems in graph
    theory. Based on popular conjectures we show that:
    \begin{itemize}
        \item No algorithm with amortized update time  exists
            for incremental or decremental maximum cardinality bipartite
            matching. This significantly improves on the 
            bound for sparse graphs of Henzinger et al. [STOC'15] and
             bound of Kopelowitz, Pettie and
            Porat\footnote{Kopelowitz et al. showed this
                result at SODA'16, and after posting their result online it was
                improved in an online version of the paper by Henzinger et al.
                Kopelowitz et al. also showed a slightly stronger bound of
             if node insertions are allowed.}. Our linear
            bound also appears more natural.
            In addition, the result we present separates
            the node-addition model from the edge insertion model, as an
            algorithm with total update time  exists for the
            former by Bosek et al. [FOCS'14].
        \item No algorithm with amortized update time  exists
            for incremental or decremental maximum flow in directed and
            weighted sparse graphs. No such lower bound was known for
            partially dynamic maximum flow previously. Furthermore no algorithm
            with amortized update time  exists for directed and
            unweighted graphs or undirected and weighted graphs.
        \item No algorithm with amortized update time 
            exists for incremental or decremental -approximating
            the diameter of an unweighted graph. We also show a slightly stronger
            bound if node additions are allowed. The result is then extended to
            the static case, where we show that no 
            algorithm exists. We also extend the result to the case when an
            additive error is allowed in the approximation. While our bounds
            are weaker than the already known bounds of Roditty and Vassilevska
            Williams [STOC'13], it is based on a weaker conjecture of Abboud et
            al. [STOC'15] and is the first known reduction from the 3SUM
            and APSP problems to diameter.
            Showing an equivalence between APSP and diameter is a major open
            problem in this area (Abboud et al. [SODA'15]), and thus showing
            even a weak connection in this direction is of interest.
    \end{itemize}
\end{abstract}

\section{Introduction}
Arguably one of the most important goals of computer science is to understand
the complexity of natural computational problems. For many such problems we
know of polynomial time algorithms, but getting matching unconditional lower
bounds seem far beyond the scope of our current techniques. Therefore a recent
and very active line of research at top-level conferences concerns itself with
hardness results in the class \textbf{P}
\cite{Patrascu10,AbboudV14,AbboudVY15,HenzingerKNS15,KopelowitzPP16,AbboudGV15,ChechikLRSTW14,AbboudHVW16,CairoGR16,BringmannK15,BackursI15,AbboudBW15,AbboudBW15a}. Such
results are obtained by reducing from classic problems like 3SUM, APSP and
CNF-SAT, for which there exist very popular conjectures about the running time.
We call such a hardness result a \emph{conditional lower bound (CLB)} as it is
based (conditioned) on the truthfulness of some popular conjecture. The main
goal of CLBs is to explain barriers in algorithm development and provide
``warning signs'' that improving an algorithm for some problem has major and
surprising consequences for a classic problem like the ones mentioned above,
which researchers have worked on for decades, and trying to do so may be
ill-advised.

One particular area that has received a lot of attention from this perspective
is dynamic graph problems
\cite{RodittyZ11,Patrascu10,AbboudV14,AbboudVY15,HenzingerKNS15,KopelowitzPP16}.
In dynamic graph problems we are asked to maintain some property about a
graph such as reachability or shortest paths distances as the graph undergoes
changes (typically edge insertions and deletions). One may also consider the
partially dynamic cases where only edge insertions are allowed (incremental) or
edge deletions (decremental) or cases where node insertion and deletion is
allowed. Several conditional lower bounds are known for both partially and
fully dynamic problems such as shortest paths \cite{RodittyZ11,HenzingerKNS15},
maximum bipartite matching \cite{AbboudV14,HenzingerKNS15,KopelowitzPP16},
maximum flow \cite{AbboudVY15}, reachability
\cite{Patrascu10,AbboudV14,HenzingerKNS15}, and many more.

\subsection{Difficulties of partially dynamic}
Most of the research on CLBs for dynamic graph problems has been focused on the
fully dynamic case, however such results do not translate well into CLBs for
incremental or decremental algorithms. A typical reduction works by 1) building
a structured base graph, 2) for each element in some subset of the input
perform a series of insertions and queries to decide whether this element is in
a possible solution, 3) perform a series of deletions returning the graph to
its base state. From a partially dynamic perspective we may use the above
procedure to get similar \emph{worst-case} bounds, by keeping track of the data
structure state and simulating step 3 by \emph{rolling back} the insertions,
however this kills any hope of good amortized bounds. As noted in
\cite{AbboudV14,HenzingerKNS15,KopelowitzPP16} it seems more difficult to
obtain good bounds in this case, and specialized reductions are often
needed.

\subsection{Bounds under weaker assumptions}
While proving higher lower bounds is the main goal of CLBs, a simultaneous goal
is to prove similar CLBs under weaker assumptions, thus lending more credibility
to the belief that a problem is difficult or even impossible. Several recent
papers concerns themselves with this be either replacing a conjecture with a
weaker version as done by Abboud et al. in~\cite{AbboudHVW16} or by showing
similar reductions under several conjectures
\cite{VassilevskaW09,AbboudL13,AbboudVW14,AbboudVY15,HenzingerKNS15}.
As an example Abboud, Vassilevska Williams, and Yu~\cite{AbboudVY15} showed that
3SUM, APSP and CNF-SAT can all be reduced to the same problem of finding
triangles in a node-colored graph and showed several interesting results based
on the following conjecture:
\begin{conjecture}[\cite{AbboudVY15}]\label{conj:tc}
    At least one of the following is true:
    \begin{enumerate}
        \item There is no algorithm for the 3-SUM problem running in
             for any .
        \item There is no algorithm for the APSP problem on weighted graphs
            running in  for any .
        \item For every  there is an integer  such that
            -SAT on  variables and  clauses cannot be solved in
             time.
    \end{enumerate}
\end{conjecture}
The third item in Conjecture~\ref{conj:tc} is what is known as the strong
exponential time hypothesis (SETH) \cite{ImpagliazzoP01} and the  bound
on the number of clauses follows from the sparsification lemma of Impagliazzo,
Paturi, and Zane \cite{ImpagliazzoPZ01}.


\subsection{Our results}
In this paper we consider three of the perhaps most classic problems in graph
theory, namely maximum flow, maximum bipartite matching and diameter in the
partially dynamic setting.
For maximum flow and maximum bipartite matching we
show new, stronger, and more natural conditional lower bounds. For diameter we
show a new reduction from Conjecture~\ref{conj:tc} to both the partially
dynamic version of diameter and, perhaps more interestingly, the static case.
This is the first known connection from APSP and 3SUM to diameter in graphs and
addresses one of the main open problems in the area as stated in
\cite{AbboudGV15}.

\subparagraph*{Maximum bipartite matching}
In dynamic maximum cardinality bipartite matching we wish to maintain the size
of a maximum matching in a dynamic graph . One can trivially do this in
 time by finding an augmenting path. Sankowski \cite{Sankowski07} gave a fully
dynamic algorithm with update time  by using fast matrix
multiplication. In the incremental setting, one may consider a node-addition version in which the right-hand side of the
bipartite graph is given and the left-hand side arrives one node at a time with
all its incident edges. In this model Bosek et al.~\cite{BosekLSZ14} gave an
algorithm with total running time of . From a hardness
perspective, Abboud and Vassilevska Williams \cite{AbboudV14} gave reductions
from 3SUM, triangle detection and boolean matrix multiplication to
fully-dynamic maximum cardinality bipartite matching. In particular, they
showed that a  algorithm would imply a faster \emph{combinatorial} boolean
matrix multiplication algorithm. Their reductions, however, only imply
worst-case bounds in the case of partially dynamic algorithms. This was
addressed by Kopelowitz, Pettie and Porat~\cite{KopelowitzPP16} who revisited
P\v{a}tra\c{s}cu's reductions from \cite{Patrascu10} and showed that any
 algorithm for incremental MCM would imply a truly
subquadratic algorithm for 3SUM. They also showed the same result for
 algorithms when node insertions are allowed. Subsequently,
in an online version of \cite{HenzingerKNS15}, it was shown how to obtain a CLB
of  in sparse graphs by reducing from the online matrix-vector
multiplication (OMv) problem.

In this paper we show the following theorem:
\begin{theorem}\label{thm:matching}
    There is no algorithm for solving incremental (or decremental) maximum
    cardinality
    bipartite matching with amortized time  per insertion (or
    deletion) and  time per query unless the OMv conjecture of
    \cite{HenzingerKNS15} is false.
\end{theorem}
One thing to note about Theorem~\ref{thm:matching} is that it separates the
node-addition model from the edge-insertion model as it implies a total running
time of  in contrast to the  running time of the algorithm from \cite{BosekLSZ14}. Furthermore, the reduction used to
prove Theorem~\ref{thm:matching} also rules out any efficient incremental (or
decremental) approximation algorithm that works by ruling out the existence of
short augmenting paths. Ruling out such paths is a popular way of ensuring a
good approximation ratio \cite{NeimanS13}.

\subparagraph*{Maximum flow}
Single-source single-sink maximum flow ( Max-Flow) is one of the most
classic problems in graph theory.
In recent
years there have been several breakthrough results for  maximum flow using
the powerful tools of Laplacian system solvers and interior point methods
\cite{Madry13,Sherman13,KelnerLOS14,LeeS14}. These algorithms seem to take
near-line time in practice, and the limits of our current analysis might be the
bottleneck in proving such upper bounds. Proving super-linear conditional
lower bounds for this problem may
thus be difficult if not impossible. Therefore, Abboud et al.~\cite{AbboudVY15}
considered different variants of the problem such as single-source maximum flow
and  maximum flow. They also showed that any algorithm solving the
fully-dynamic version of  maximum flow with amortized update and query time
 for any  would refute Conjecture~\ref{conj:tc}.
Finally, we note that it is possible to modify the  CLB for fully
dynamic \#SSR of Abboud and Vassilevska Williams \cite{AbboudV14} to obtain a
 CLB for fully-dynamic  max-flow in sparse graphs.

In this paper we show that even in the incremental and decremental case 
maximum flow exhibit the same kind of CLB, but based solely on SETH. This is
summarized in the following theorem:
\begin{theorem}\label{thm:maxflow2}
    There is no algorithm for solving incremental (or decremental) max 
    flow on a weighted and directed graph with  nodes and  edges with
    amortized time  per operation for any  unless SETH
    is false.
\end{theorem}
Our bound shows that we cannot hope to get incremental maximum flow in offline
time as is the case for other problems.
We note that the above result only holds for directed
and weighted graphs. We show similar results for other types of graphs:
\begin{theorem}
    There is no algorithm for solving incremental (or decremental) max 
    flow on unweighted directed graphs or weighted undirected graphs on 
    nodes with amortized time  per operation for any 
    unless the OMv conjecture is false.
\end{theorem}
This result follows directly from Theorem~\ref{thm:matching} by using textbook
reductions from maximum bipartite matching to directed flow (see
e.g.~\cite{Cormen09book}) and from directed flow to undirected flow (see
e.g.~\cite{MadryThesis}).

\subparagraph*{Diameter}
The diameter problem asks us to compute the longest shortest-path distance in a
graph . Efficiently computing or approximating the diameter is a basic
problem in graphs
\cite{AbboudGV15,AingworthCIM99,ChechikLRSTW14,CyganGS15,RodittyW13}. One can
trivially compute the diameter in the same time as computing APSP, however in
general no
better algorithm is known. It remains a major open problem whether a reduction
exists in the other direction \cite{AbboudGV15} - that is, can we compute all
distances in the same time as the longest? One can, however, approximate the
diameter faster. Roditty and Vassilevska Williams \cite{RodittyW13} showed how
to compute a -approximation in time  randomized, and
Chechik et al.~\cite{ChechikLRSTW14} showed how to obtain the same guarantee
deterministically in time . More recently, it was
shown by Cairo, Grossi and Rizzi~\cite{CairoGR16} how to obtain a -approximation in time . From a
hardness perspective it is known that any algorithm able to distinguish between
diameter  and  in time  for sparse graphs would refute
SETH \cite{RodittyW13}. Chechik et al.~\cite{ChechikLRSTW14} showed that
approximating within a  factor with additive error  in time  for sparse graphs would also
refute SETH, and this bound was improved in \cite{CairoGR16} to rule out any
 approximation with the same additive error and time bounds based on
SETH (also for sparse graphs). From the perspective of dynamic algorithms
Abboud and Vassilevska Williams \cite{AbboudV14} showed that any algorithm for
-approximating the diameter in a fully dynamic graph with amortized
update time  would refute SETH. We also note, that the above
static reductions rules out any  amortized update time for
incremental algorithms.

We note that all the reductions mentioned above are based on SETH. Similar to
the work of \cite{AbboudVY15,AbboudHVW16} we seek to replace this assumption by
a weaker one. In this paper we show the first reduction from 3SUM and APSP to
the diameter problem. That is, we show that a fast algorithm for approximating
the diameter implies a faster algorithm for the APSP and 3SUM
problems. The bounds we achieve are not as strong as the known bounds based on
SETH \cite{RodittyW13,ChechikLRSTW14,CairoGR16}, however they are based on a
weaker conjecture and hold even if SETH turns out to be false, thus giving more
credibility to the difficulty of the problem. For the partially dynamic case we
show the following theorem:
\begin{theorem}\label{thm:dyn_diam}
    There exists no incremental (or decremental) algorithm that approximates
    the diameter of an unweighted graph within a factor of  running
    in amortized time  for any  unless
    Conjecture~\ref{conj:tc} is false. Furthermore, if we allow node insertions
    in the incremental case the bound is .
\end{theorem}
In order to achieve the result for node insertions, we use the technique of
Kopelowitz et al.~\cite{KopelowitzPP16} leveraging rollback with our standard
incremental bound. By doing this we obtain a graph with fewer nodes
and thus a better bound.
More interestingly, we are able to generalize our results from the incremental
case to the following result for static graphs:
\begin{theorem}\label{thm:sta_diam}
    There exists no static  approximation to the diameter on
    unweighted graphs running in  time for
    any  and any number of edges  unless
    Conjecture~\ref{conj:tc} is false.
\end{theorem}
As mentioned, this is the first known reduction from APSP to diameter and shows
at least some weak connection in this direction. An interesting property of
Theorem~\ref{thm:sta_diam} is that it holds \emph{for any}  as a function of
 and thus an algorithm need not exist \emph{for all} .
As a corollary of Theorem~\ref{thm:sta_diam} we see that no algorithm can
-approximate the diameter of static unweighted graph in time
 for any  unless Conjecture~\ref{conj:tc} is false.
This is reminiscent of the bounds from
\cite{RodittyW13,ChechikLRSTW14,CairoGR16}, however not quite as strong as it
does not hold for sparse graphs, for which we get a bound of .

Similar to \cite{ChechikLRSTW14,CairoGR16} we also extend the above bound to
the case of -approximations with additive error . We
show the following
\begin{corollary}\label{cor:diam_add}
    There exists no static  approximation with additive error
     with running time  or
    incremental/decremental algorithm with amortized time  for any  unless
    Conjecture~\ref{conj:tc} is false.
\end{corollary}

\subsection{A note on the decremental results and preprocessing}
We will in general only describe the reductions in the incremental case and
note that the decremental results are obtained by removing the edges in the
reverse order of insertions. This requires an assumption on the beginning
graph, and we will thus assume any suitable graph on  edges in the
sparse case and the complete graph in the dense case.

Furthermore, we do not assume that any of the algorithms are allowed to
preprocess the graph. It is often an assumption in the design of amortized
partially dynamic algorithms that one starts with the empty (or complete) graph
in order for the analysis to work. Thus, our results hold for this case.


\section{Preliminaries}

\subparagraph*{Notation}
Throughout the paper we assume that matrices are boolean. Thus the output
of a vector-matrix-vector multiplication will always be a single bit. We
use  to denote the set .

\subparagraph*{Online vector-matrix-vector multiplication}

We will consider the online vector-matrix-vector multiplication problem of
\cite{HenzingerKNS15}:

\begin{definition}[OuMv problem \cite{HenzingerKNS15}]\label{defn:oumv}
    Let  be a binary  matrix than can be preprocessed. After
    preprocessing  vector pairs  arrive one at
    a time and the task is to compute  before being presented with
    the th vector pair for every .
\end{definition}

In \cite{HenzingerKNS15} they showed that the OMv problem can be reduced to the
OuMv problem. They also came up with the following conjecture:

\begin{conjecture}[\cite{HenzingerKNS15}]\label{conj:omv}
    There is no algorithm for the OMv problem (and thus the OuMv problem)
    running in time  for any .
\end{conjecture}

\subparagraph*{Triangle collection}

We will also consider the triangle collection problem of \cite{AbboudVY15}:

\begin{definition}[Triangle collection \cite{AbboudVY15}]
    Given a node-colored graph , is it true that for every triplet of colors
     there exists a triangle  in  where  has color ,
     has color  and  has color ?
\end{definition}

In fact, we will consider the more structured triangle collection* (TC*)
problem which they also used in \cite{AbboudVY15}

\begin{definition}[Triangle collection* \cite{AbboudVY15}]
    Let  be parameters and let  be an undirected node-colored
    tripartite graph with partitions . Let  be any graph with the
    following structure:
    \begin{itemize}
        \item Each partition has its own  colors and we denote these by the
            numbers of  for each partition.
        \item  contains nodes of the form , where  is the
            color of the node and .
        \item  and  contains nodes of the form  and
             where  is the color of the node and .
    \end{itemize}
    And the edges of  are as follows:
    \begin{itemize}
        \item For each  and  there is an edge from
             to  for \emph{exactly one} . Similarly
            there is an edge from  to  for exactly one 
            (note that  and  need not be the same for the same  and
            ).
        \item There may be an edge between nodes  and
             \emph{only if} .
    \end{itemize}
    We ask the following question: Does there exist a triple of colors (one
    color per partition) such that  does not contain a triangle with these
    colors?
\end{definition}

In \cite{AbboudVY15} it was shown that this problem does not have a truly
subcubic algorithm unless Conjecture~\ref{conj:tc} is false.

It will be important that the reductions from these problems to TC* hold even
when  and  are bounded by .

\section{Incremental maximum matching}
We will reduce from the OuMv problem of Definition~\ref{defn:oumv}.
Observe that the OuMv problem is equivalent to the following statement:
For each vector pair  determine whether indices  exist, such that
. In order to model this as an incremental maximum
matching problem we construct the following graph:
Create  copies of  nodes and name these .
Partition  into  pairs of nodes . Do the same for . Add the edges  for
each  and do the same for . Now for each  add the edge  if . Observe that this graph has a unique maximum
matching each  pair. Observe also that the graph is bipartite.
Now we do the following  phases -- one for each  vector pair.
\begin{enumerate}
    \item For each  such that  add the edge .
    \item For each  such that  add the edge
        .
    \item Add the edges  and .
    \item Query the size of a maximum matching.
    \item Add the edges  and .
\end{enumerate}
This is illustrated in Figure~\ref{fig:matching}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\textwidth]{matching.pdf}
    \caption{Reduction to incremental maximum matching.}
    \label{fig:matching}
\end{figure}

\begin{lemma}\label{lem:max_match}
    Let the setting be as above and let the phases be numbered . Then the size of the maximum matching during the th phase is exactly
     if the resulting vector-matrix-vector product is  and
     otherwise.
\end{lemma}
\begin{proof}
    Note that prior to any of the  phases the size of the maximum matching
    is exactly , which is also a perfect matching of the graph induced
    by the edges. To see this observe that each  must be matched to its corresponding
    , and this is the only edge incident to the
    -nodes. As a consequence of this, each  must be matched
    with , and so on.

    Now consider the th phase. Adding any edge  or
     cannot increase the size of the maximum matching, as the
    size of the subgraph induced by the edges of the graph does not increase --
    i.e. all nodes with edges incident to them are already matched.

    Assume that adding the edges  and 
    increases the matching. The matching can increase by at most , as only
    two more nodes can be matched. Furthermore the matching must now contain
    edges as follows
    
    Now observe that each  for  \emph{must} be matched to
    , as the right nodes have no other incident edges and all nodes have
    to be matched for the size of the matching to increase. Thus we must have
     in the list above, but this means that we have exactly found a pair
     such that  and the vector-matrix-vector
    product is thus .

    Conversely, assume that the vector-matrix-vector product is , then such
    an index pair  must exist and we can find the following matching of
    size : Match the edges
    
    For all  add the edges  and  to
    the matching. For all  add the edges  and
     to the matching. And for all  and  add
    the edges  and  to the matching. This
    matches all nodes incident to an edge and has size . This is also
    exactly the matching illustrated in Figure~\ref{fig:matching} for .
\end{proof}

It follows from Lemma~\ref{lem:max_match} that we can solve the OuMv problem
correctly via this reduction. The reduction creates a graph with  nodes
and  edges. We perform  insertions and  queries giving
the result in Theorem~\ref{thm:matching}


\section{Maximum flow}
In order to show Theorem~\ref{thm:maxflow2} we will use a similar graph
construction as have been used numerous times before
\cite{PatrascuW10,RodittyW13,ChechikLRSTW14,AbboudV14}:
First partition the variables of the SAT problem into two
groups  and  of  variables each. For each possible assignment to the variables
in  we create a node in our graph  (and likewise for ). Furthermore,
for each clause of the SAT formula, we create a node as well. We denote the
corresponding sets of nodes by . Set . For
each pair of nodes  we add the directed edge  with
capacity  if the partial assignment  \emph{does not} satisfy the clause
. Similarly, for each pair of nodes  we add the directed
edge  with capacity  if  does not satisfy . Finally we add two
nodes  and add edges  with capacity  for each .

We now continue in phases with a phase for each . Denote these nodes
by :
\begin{enumerate}
    \item Add the edge  with capacity .
    \item Query the maximum flow between  and .
    \item Add the edge (``shortcut'')  with capacity .
\end{enumerate}
This is illustrated in Figure~\ref{fig:maxflow}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{max_flow.pdf}
    \caption{Illustration of the incremental construction for maximum flow.}
    \label{fig:maxflow}
\end{figure}

\begin{lemma}\label{lem:maxflow_correct}
    Let the setup be as described above. If the  flow returned during any
    of the  phases is , then the SAT formula is satisfiable.
    Otherwise the formula is not satisfiable.
\end{lemma}
\begin{proof}
    Observe, that prior to the th phase, the flow is exactly ,
    as we can use the paths  for each , which has
    capacity  and exactly  flow leaves .

    Now assume that the partial formula corresponding to  can be completed
    to a satisfying assignment. In this case, there must be some node ,
    for which there is no path from  to . This follows because such a
    path has to go through a node , but then both  and  do not
    satisfy the clause , which is a contradiction. However, the only way to
    send flow from  to  is through the nodes  and thus it is
    not possible to send all  units of flow from  to .

    Now assume that the flow is , then there must be
    some  such that there is no path from  to . Otherwise, we
    could route  units of flow from  to  via the nodes of  and
    the remaining  units through the ``shortcuts''. It now
    follows that  and  together satisfy all clauses (otherwise there
    would be a path) and thus the CNF formula is satisfiable.

    Since this is true for all of the  phases, the statement of the lemma
    follows.
\end{proof}

As a consequence of Lemma~\ref{lem:maxflow_correct} we may use the above
procedure to solve the given SAT problem. By the sparsification lemma of
\cite{ImpagliazzoPZ01} it follows that we can assume the graph has  nodes
and  edges and we perform a total of  insertions and queries.
The result of Theorem~\ref{thm:maxflow2} thus follows directly.


\section{Diameter}
In this section we show how to obtain conditional lower bounds for the problem
of approximating the diameter of an unweighted graph within a factor of
.

\subsection{A graph construction}
We will first describe the graph structure we use.

\begin{definition}
    Let  be an instance of the TC* problem as defined above. We will define
    the graph . The idea is that 
    ``corresponds'' to the colors  of .
    Thus  is a number in . The nodes of this graph are as
    follows:
    \begin{itemize}
        \item The nodes  and  of .
        \item For each color  of  we
            add the nodes  and
            .
        \item We also add several special nodes: A ``master node'' ,
             ``skip nodes''  and three ``connector nodes''
            .
    \end{itemize}
    For a color  we denote the nodes
     by  and the collection of all s by .
    We do the same for  and .

    The edges of  are as follows:
    \begin{itemize}
        \item Add the edges between  and  in .
        \item Connect the node  to each node of  and .
        \item Connect  to each node of  and  as well as  and
            the master node .
        \item Connect  to each node of .
        \item Connect  to all nodes .
        \item For each  do as follows:
            \begin{itemize}
                \item Connect  to all nodes of  and to all
                    nodes of .
                \item For each  and each edge  add the edge .
                \item For each  and each edge  add the edge .
            \end{itemize}
    \end{itemize}
\end{definition}
An overview of the graph  is illustrated in Figure~\ref{fig:34diam}
and a more detailed view in Figure~\ref{fig:tc_diam_det}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\textwidth]{tc_diam_ovw.pdf}
    \caption{Diameter structure.}
    \label{fig:34diam}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{tc_diam_det.pdf}
    \caption{Diameter structure.}
    \label{fig:tc_diam_det}
\end{figure}

The idea is that length three paths between  and  correspond to
triangles in  containing the color  of . Each of the  nodes in
 thus correspond to picking a color from  and each of the  nodes in
 correspond to picking a color from . If two such nodes don't have a
length three path there is no triangle in  of the corresponding triplet of
colors. In this case the connector nodes ensure that there is a length four
path between the nodes. The master and skip nodes ensure that all other nodes
have distance at most . This is captured by the following lemma:

\begin{lemma}\label{lem:diam_graph}
    Let  be an instance to the TC* problem and let  be as
    defined above. Let  be a color of 
    and let  be colors of  and  respectively. Then
    the distance from  to  in  is 
    if the colors  have a triangle in  and  otherwise.
\end{lemma}
\begin{proof}
    Assume first that there is a triangle  for some  in  (note that such a triangle can only
    occur if  is the same for all the three nodes). In this case there is a
    path  in
     and thus the distance is at most . Observe also, that
    no node is connected to both  and  and thus the distance is
    strictly greater than .

    Now assume that the distance from  to  is . Such
    a path has to go from  to  to  to  as any node  or  either has distance  to one of  or
     or it has distance  to both of them. Now consider a shortest
    path , where  and  are the nodes of 
    and  on this path. Clearly the node  must have color  in 
    as it would not have an edge to  otherwise, and similarly 
    must have color  in . Thus the path consists of nodes
    . Since no edge in
     goes between nodes with different -values we must have . It
    is now clear that the edge  corresponds to
    the edge  in  and the edge
     corresponds to the edge 
    in . Thus, these three nodes form a triangle of the correct color triple
    in .
\end{proof}
Furthermore it is easy to see that the longest distance in  is
at most , thus the diameter is  exactly when one of the corresponding
color triplets do not have a triangle in  and  otherwise.

\subsection{Dynamic}
We will first consider the problem without node additions. For simplicity we
only consider the incremental case and note that the decremental case follows
by deleting edges until we obtain the same graph\footnote{Under the assumption
that the algorithm starts with some suitable graph}.

Given an instance to the TC* problem we create the graph  (that is,
the graph representing all colors of ). This graph is created by adding
edges incrementally and has  nodes and edges. It follows that an edge
insertion must take  time unless Conjecture~\ref{conj:tc} is
false.

Next, we consider the problem with node additions. It was shown in
\cite{KopelowitzPP16} that if we allow node additions in the problem of
incremental maximum matching, it is possible to show stronger lower bounds by
leveraging the amortized running time with the widely used rollback technique.
We here apply the same argument to the problem of incremental diameter
approximation.

The goal is again to construct (a subgraph of)  but we do not start
with all nodes in the graph. We will assume that the amortized
running time of an insert operation is  for some . The goal
is to get a bound on  by expressing the total running time in terms of
 and using the assumption on running time for TC*. We let 
denote the current number of nodes in the graph . We continue as follows:
\begin{enumerate}
    \item Insert all nodes of  and  into the dynamic graph. Also insert
        the nodes  and . We also insert all the edges
        induced by these nodes in  into the graph.
    \item For each color  of  we do a phase:
        \begin{itemize}
            \item We insert the nodes of  into the dynamic graph
                and all the edges induced by these nodes and the current state
                of the dynamic graph in .
            \item Query the diameter of the graph.
            \item Assume we inserted  edges+nodes in this phase. If the
                total running time of all these insertions was greater than
                 we keep the nodes in the graph. Otherwise we
                rollback all operations of this phase.
        \end{itemize}
\end{enumerate}
We answer the question of the TC* problem according to whether the diameter was
 all the time or not similar to the proof of the case without
node additions.

The goal is now to bound  by using the method of \cite{KopelowitzPP16}. We will do
this by carefully counting the number of ``amortized credit units'' the data
structure has and using this to bound the total number of nodes added to the
graph (i.e. not rolled back).

Observe that after the first step, we have added  edges to the graph
and  nodes. Thus the data structure has at most 
credit at this point (this happens if almost all operations were ). Now
consider the total time spent by the algorithm. This can be bounded by
 where  is the number of nodes at the end of all
phases. This is the case since , where  is the number
of nodes after the first step and there are at most  total
operations. Note that this would not be the case if we did not have a bound on
the cost of the rolled back operations, but we only rollback the cheap
operations, so this is okay. We wish to express  in terms of  and
 in order to express the total running time in terms of these.

Observe, that every time we keep the added nodes in the graph, the data
structure spent at least twice the amortized cost. Since
we started out with  credit it must be true that

The worst case is if  is polynomially larger than , and thus
. It follows that .
Thus the total running time is . Now, by Conjecture~\ref{conj:tc} we must have . Solving this for  gives .


\subsection{Static}
\begin{proof}[Proof of Theorem~\ref{thm:sta_diam}]
    Let  be an instance of the TC* problem with parameters  with
     and  bounded by  and  as in
    \cite{AbboudVY15}.

    For a parameter  we
    create the graphs  and solve the diameter problem on these graphs up to a 
    approximation. This is sufficient to distinguish between diameters  and
     in all of the graphs. Now, if the diameter is  in just one of the
    graph we answer that there exists a triplet of colors such that there is no
    triangle in . This follows from Lemma~\ref{lem:diam_graph}.

    We note that the graphs  each have 
    nodes and  edges. Assume now that that any algorithm
    approximating the diameter within a factor of  in time
     for any 
    exists. Since we create  instances of the problem this would
    imply an  algorithm for the TC* problem for some .
\end{proof}

\subsection{Additive error}
To see Corollary~\ref{cor:diam_add} we fix  and consider TC* on a
graph  with  nodes and  edges such that .
We then create  and subdivide each edge into  nodes.
This graph now has  nodes and edges and any algorithm solving 
diameter with additive error  in time  time thus violates
Conjecture~\ref{conj:tc}.

\subparagraph*{Acknowledgements}
I would like to thank Amir Abboud and Virginia Vassilevska Williams for helpful
discussions and observations. I would also like to thank an anonymous referee
for pointing out the reduction from incremental matching to undirected flow.




\bibliographystyle{plain}
\bibliography{cond_lbs}


\end{document}
