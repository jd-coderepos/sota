\begin{table*}[!tbp]
\centering
\small
\tabcolsep 4pt
 \scalebox{0.9}{
\begin{tabular}{lcl|c|cccc|c}
\toprule
\multicolumn{3}{c|}{\multirow{2}{*}{Models}} & \multirow{2}{*}{$I_{\text{dec}}$} & \multicolumn{2}{c}{WMT14} &\multicolumn{2}{c|}{WMT16} &\multirow{2}{*}{Speed Up} \\
\multicolumn{3}{c|}{} &  & EN-DE & DE-EN & EN-RO & RO-EN &        \\
\midrule
\multicolumn{2}{c}{\multirow{2}{*}{AT Models}}  
  &Transformer~\citep{transformer} & T & 27.30 & / & / & / &  / \\
 \multicolumn{2}{c}{}  & Transformer~(ours)   & T & 27.48 & 31.27 & 33.70 & 34.05 & 1.0$\times$ \\
\midrule
 \multicolumn{2}{c}{\multirow{5}{*}{Iterative NAT}}& NAT-IR~\citep{iter_nat}   & 10   & 21.61 & 25.48 & 29.32 & 30.19 & 1.5$\times$ \\
 \multicolumn{2}{c}{}& LaNMT~\citep{shu2020latent}  & 4     & 26.30 & /     & /     & 29.10 & 5.7$\times$  \\
 \multicolumn{2}{c}{}& LevT~\citep{levT}  & 6+     & 27.27 & /     & /     & 33.26 & 4.0$\times$  \\
\multicolumn{2}{c}{}& Mask-Predict~\citep{mask_predict} & 10 &27.03& 30.53&\textbf{33.08} &
 \textbf{33.31} & 1.7$\times$    \\
 \multicolumn{2}{c}{}& JM-NAT~\citep{jm_nat} & 10 & \textbf{27.31} & \textbf{31.02} & / & / & 5.7$\times$\\
 \midrule
\multirow{18}{*}{\ \ Fully NAT\ \ } &
 & NAT-FT~\citep{nat}               & 1 & 17.69 & 21.47 & 27.29 & 29.06 & 15.6$\times$\\
& & Mask-Predict~\citep{mask_predict} & 1 & 18.05 & 21.83 & 27.32 & 28.20 &  /           \\
  & & imit-NAT~\citep{imitate_nat}          & 1 & 22.44 & 25.67 & 28.61 & 28.90 & 18.6$\times$ \\
 & & NAT-HINT~\citep{hint_nat}              & 1 & 21.11 & 25.24 & /     & /     & / \\& & Flowseq~\citep{flowseq}              & 1 & 23.72 & 28.39 & 29.73 & 30.72 &  1.1$\times$           \\ 
 & & NAT-DCRF~\citep{nat-crf}            & 1 & 23.44 & 27.22 & / & / & 10.4 $\times$ \\
\cmidrule{2-9}
  & \multirow{2}{*}{w/ CTC}
 & NAT-CTC~\citep{nat_ctc} & 1 & 16.56 & 18.64 & 19.54 & 24.67 & / \\
 & & Imputer~\citep{imputer}              & 1 & 25.80 & 28.40 & 32.30 & 31.70 & 18.6$\times$ \\
 
 \cmidrule{2-9}
& \multirow{5}{*}{w/ NPD} 
& NAT-FT + NPD~(m=100)      & 1  & 19.17 & 23.20 & 29.79 & 31.44 & 2.4$\times$  \\
& & imit-NAT + NPD~(m=7)   & 1  & 24.15 & 27.28 & 31.45 & 31.81 & 9.7$\times$ \\
  & & NAT-HINT + NPD~(m=9)       & 1  & 25.20 & 29.52 & /     & /     & / \\& & Flowseq + NPD~(m=30)      & 1  & 25.31 & 30.68 & 32.20 & 32.84 & /           \\
 & & NAT-DCRF + NPD~(m=9)     & 1  & 26.07 & 29.68 & / & / & 6.1$\times$\\


\cmidrule{2-9}
  & \multirow{5}{*}{\textbf{Ours}} 
  & NAT-base* & 1 & 20.36 & 24.81 & 28.47 & 29.43 &  15.3$\times$           \\
  & & CTC*              & 1 & 25.52 & 28.73 & 32.60 & 33.46 & 14.6 $\times$ \\
  & & \method    & 1    & 25.21& 29.84 & 31.19 & 32.04 & 15.3$\times$          \\
 & & \method + CTC            & 1 & 26.39 & 29.54 & 32.79 & \textbf{33.84} & 14.6 $\times$ \\
  & &\method + NPD ~(m=7)  & 1   & \textbf{26.55} &\textbf{31.02}&\textbf{32.87}& 33.51 & 7.9$\times$  \\
 \bottomrule
\end{tabular}
}
\caption{Results on WMT14 EN-DE/DE-EN and WMT16 EN-RO/RO-EN benchmarks. $I_{\text{dec}}$ is the number of decoding iterations and $m$ is the number of length reranking candidates. NPD represents noisy parallel decoding, CTC represents connectionist temporal classification. * indicate the results are obtained by our implementation.}\label{tb.main_result}
\end{table*}

In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines.
Ablation studies and further analysis are also included to verify the effects of different components used in \method.

\subsection{Experimental Settings}

\paragraph{Datasets}
We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs).
These datasets are tokenized and segmented into subword units using BPE encodings~\citep{bpe}.
We preprocess WMT14 EN-DE by following the data preprocessing in \citet{transformer}. For WMT16 EN-RO and IWSLT16 DE-EN, we use the processed data provided in \citet{iter_nat}.

\paragraph{Knowledge Distillation}
Following previous work~\citep{nat,iter_nat,nat_reg}, we also use sequence-level knowledge distillation for all datasets. 
We employ the transformer with the base setting in ~\citet{transformer} as the teacher for knowledge distillation. Then, we train our \method on distilled data.

\paragraph{Baselines and Setup}
We compare our method with the base Transformer and strong representative NAT baselines in Table~\ref{tb.main_result}.
For all our tasks, we obtain other NAT models' performance by directly using the performance figures reported in their papers if they are available.

We adopt the vanilla model which copies source input uniformly in~\citet{nat} as our base model~(NAT-base) and replace the \textit{UniformCopy} with attention mechanism using positions. 
Note that the output length does not equal the length of reference in models using CTC. Therefore, for GLAT with CTC, we adopt longest common subsequence distance for comparing $Y$ and $\hat{Y}$, and the glancing target is the target alignment that maximize the output probability $arg\max_{a\in \mathcal{B}^{-1} (Y)} P(a|X;\theta)$. $\mathcal{B}^{-1}$ is the mapping proposed in \citep{ctc}, which expand the reference to the length of output by inserting blanks or repeating words.

For WMT datasets, we follow the hyperparameters of the base Transformer in \citet{transformer}. And we choose a smaller setting for IWSLT16, as IWSLT16 is a smaller dataset. For IWSLT16, we use 5 layers for encoder and decoder, and set the model size $d_{\text{model}}$ to 256.
Using Nvidia V100 GPUs, We train the model with batches of 64k/8k tokens for WMT/IWSLT datasets, respectively. We set the dropout rate to 0.1 and use Adam optimizer~\citep{adam} with $\beta=(0.9,0.999)$.
For WMT datasets, the learning rate warms up to $5e-4$ in 4k steps and gradually decays according to inverse square root schedule in~\citet{transformer}. As for IWSLT16 DE-EN, we adopt linear annealing (from $3e-4$ to $1e-5$) as in \citet{iter_nat}. For the hyper-parameter $\lambda$, we adopt linear annealing from 0.5 to 0.3 for WMT datasets and a fixed value of 0.5 for IWSLT16.
The final model is created by averaging the 5 best checkpoints chosen by validation BLEU scores.

\begin{figure}[!tbp]
\centering
\small
\centering
\includegraphics[width=0.8\linewidth]{figure/trade_off8.pdf}
\caption{The trade-off between speed-up and BLEU on WMT14 DE-EN}
\label{fig.trade_off}
\end{figure}

\subsection{Main Results}
The main results on the benchmarks are presented in Table~\ref{tb.main_result}.
\method significantly improves the translation quality and outperforms strong baselines by a large margin.
Our method introduces explicit word interdependency modeling for the decoder and gradually learns simultaneous generation of whole sequences, enabling the model to better capture the underlying data structure. 
Compared to models with iterative decoding, our method completely maintains the inference efficiency advantage of fully non-autoregressive models, since \method generate with a single pass.
Compared with the baselines, we highlight our empirical advantages:
\begin{itemize}
    \item \method is highly effective. Compared with the vanilla NAT-base models, \method obtains significant improvements~(about 5 BLEU) on EN-DE/DE-EN. Additionally, \method also outperforms other fully non-autoregressive models with a substantial margin (almost +2 BLEU score on average). The results are even very close to those of the AT model, which shows great potential.
    
\item \method is simple and can be applied to other NAT models flexibly, as we only modify the training process by reference glancing while keeping inference unchanged. For comparison, NAT-DCRF utilizes CRF to generate sequentially; NAT-IR and Mask-Predict models need multiple decoding iterations.
    
    \item CTC and NPD use different approaches to determine the best output length, and they have their own advantages and disadvantages. CTC requires the output length to be longer than the exact target length. With longer output lengths, the training will consume more time and GPU memory. As for NPD, with a certain number of length reranking candidates, the inference speed will be slower than models using CTC. Note that NPD can use pretrained AT models or the non-autoregressive model itself to rerank multiple outputs.
\end{itemize}

We also present a scatter plot in Figure~\ref{fig.trade_off}, displaying the trend of speed-up and BLEU with different NAT models.
It is shown that the point of \method is located on the top-right of the competing methods.
Obviously, \method outperforms our competitors in BLEU if speed-up is controlled, and in speed-up if BLEU is controlled.
This indicates that \method outperforms previous state-of-the-art NAT methods.
Although iterative models like Mask-Predict achieves competitive BLEU scores, they only maintain minor speed advantages over AT. In contrast, fully non-autoregressive models remarkably improve the inference speed.

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.8\linewidth]{figure/len_res3.pdf}
\caption{Performance under different source input length on WMT14 DE-EN}
\label{fig.len_res}
\end{figure}
\subsection{Analysis}


\paragraph{Effect of Source Input Length}
To analyze the effect of source input length on the models' performance, we split the source sentences into different intervals by length after BPE and compute the BLEU score for each interval. 
The histogram of results is presented in Figure~\ref{fig.len_res}. 
NAT-base's performance drops sharply for long sentences, while the gradual learning process enables GLAT to boost the performance by a large margin, especially for long sentences. We also find that GLAT outperforms autoregressive Transformer when the source input length is smaller than 20.

\paragraph{GLAT Reduces Repetition}
We also measure the percentage of repeated tokens on test set of WMT14 EN-DE and WMT14 DE-EN. Table~\ref{tb.repetition} presents the token repetition ratio of sentences generated by NAT-base and GLAT.
The results show that GLAT significantly reduces the occurrence of repetition, and the repetition ratio can be further reduced with NPD. We think an important cause of the improvement is better interdependency modeling. Since GLAT explicitly encourages word interdependency modeling to better capture the dependency between target tokens, wrong generation patterns, such as repetition, can be largely avoided.

\begin{table}[!tbp]
\centering
\small
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{WMT14} \\
& EN-DE & DE-EN \\
\midrule
NAT-base & 8.32\% & 7.10\%\\
GLAT & 1.19\% & 1.05\% \\
GLAT w/ NPD & 0.32\% & 0.16\% \\
\bottomrule
\end{tabular}
\caption{Token repetition ratio on WMT14 EN-DE and WMT14 DE-EN}
\label{tb.repetition}
\end{table}

\subsection{Ablation Study}


\paragraph{Effectiveness of the Adaptive Sampling Number}
To validate the effectiveness of the adaptive sampling strategy for the sampling number $S(Y,\hat Y)$, we also introduce two fixed approaches for comparison. The first one decides the sampling number with $\lambda*T$, where $T$ is the length of $Y$, and $\lambda$ is a constant ratio. The second one is relatively flexible,  which sets a start ratio of $\lambda_s$ and an end ratio $\lambda_e$, and linearly reduces the sampling number from $\lambda_s*T$ to $\lambda_e*T$ along the training process.

As shown in Table~\ref{tb.constant_base} and Table~\ref{tb.adaptive_base}, clearly, our adaptive approach~(Adaptive in the table) outperforms the baseline models with big margins. The results confirm our intuition that the sampling schedule affects the generation performance of our NAT model.
The sampling strategy, which first offers relatively easy generation problems and then turns harder, benefits the final performance.
Besides, even with the simplest constant ratio, \method still achieves remarkable results. When set $\lambda=0.2$, it even outperforms the baseline $\lambda=0.0$ by 2.5 BLEU score.

The experiments potentially support that it is beneficial to learn the generation of fragments at the start and gradually transfer to the whole sequence. The flexible decreasing ratio method works better than the constant one, and our proposed adaptive approaches achieve the best results.

\begin{table}[!tbp]
\centering
\small
\begin{tabular}{lcc}
\toprule
Sampling Number &  $\lambda$  & BLEU \\
\midrule
\multirow{5}{*}{Fixed}  & 0.0  & 24.66  \\
                        & 0.1  & 24.91 \\
                        & 0.2  & 27.12  \\
                        & 0.3  & 24.98  \\
                        & 0.4  & 22.96  \\
\midrule
Adaptive                & -  & \textbf{29.61}  \\
\bottomrule
\end{tabular}
\caption{Performances on IWSLT16 with fixed sampling ratio.}
\label{tb.constant_base}
\end{table}


\begin{table}[!tbp]
\small
\centering
\begin{tabular}{lccc}
\toprule
Sampling Number& $\lambda_s$ & $\lambda_e$ & BLEU \\
\midrule
\multirow{3}{*}{Decreasing} & 0.5 & 0 & 27.80  \\
& 0.5 & 0.1 & 28.21  \\
& 0.5 & 0.2 & 27.15 \\
& 0.5 & 0.3 & 23.37 \\
\midrule
Adaptive & \multicolumn{2}{c}{-} & \textbf{29.61}  \\
\bottomrule
\end{tabular}
\caption{\label{tb.adaptive_base} Performances on IWSLT16 with decreasing sampling ratio.}
\end{table}
\paragraph{Influence of Reference Word Selection}
To analyze how the strategies of selecting reference words affect glancing sampling, we conduct experiments with different selection strategies. By default, we assume all the words in the reference are equally important and randomly choose reference words for glancing.
Besides the random strategy, we devise four other selection methods considering the prediction of first decoding. 
For $p_\text{ref}$ and $1-p_\text{ref}$, the sampling probability of each reference word is proportional to the output probability for the reference word $p_\text{ref}$ and the probability $1-p_\text{ref}$, respectively. Similar to the word selection strategy for masking words during inference in Mask-Predict, we also add two strategies related to the prediction confidence: "most certain" and "most uncertain." We choose the positions where predictions have higher confidence for "most certain", and vise versa for "most uncertain." The results for different selection methods are listed in Table~\ref{tb.sp_strategy}.

In comparisons, the model with the selection strategy $1-p_\text{ref}$ outperforms the one with $p_\text{ref}$, indicating that words hard to predict are more important for glancing in training. 
And we find that the random strategy performs a little better than the two confidence-based strategies. We think this indicates that introducing more randomness in sampling enable \method to explore more interdependency among target words. We adopt the random strategy for its simplicity and good performance.

\begin{table}[!t]
\centering
\small
\tabcolsep 4pt
\begin{tabular}{lcc}
\toprule
Selection Strategy & \method & \method w/ NPD \\
\midrule
random             & 25.21  & 26.55        \\
$p_\text{ref}$            & 24.87  & 25.83        \\
$1-p_\text{ref}$          & 25.37  & 26.52        \\
most certain              & 24.99  & 26.22        \\
most uncertain            & 24.86  & 26.13       \\
\bottomrule
\end{tabular}
\caption{Performance on WMT14 EN-DE with different reference word selection strategies.}
\label{tb.sp_strategy}
\end{table}

\begin{table}[!tp]
\centering
\small
\tabcolsep 4pt
\begin{tabular}{l|cc}
\toprule
\multirow{2}{*}{Method}& \multicolumn{2}{c}{WMT14} \\
& EN-DE & DE-EN\\
\midrule
\method w/ uniform sampling &  19.16 &  23.56 \\
\method w/ \texttt{[MASK]} inputs & 24.99 & 29.48 \\
\method & 25.21 & 29.84  \\
\bottomrule
\end{tabular}
\caption{\label{tb.ablation} Ablation study for comparing GLAT and Mask-Predict on WMT14 EN-DE and DE-EN.}
\end{table}




\paragraph{Advantages of \method over Mask-Predict}
To study the effects of sampling strategy and decoder inputs of \method, we conduct experiments for replacing these two modules in \method with the corresponding part in Mask-Predict, respectively. 
The results are presented in Table~\ref{tb.ablation}. \method employs glancing sampling strategy instead of the uniform sampling strategy used in Mask-Predict, and replaces the \texttt{[MASK]} token inputs with source representations from the encoder. The results show that the glancing sampling strategy outperforms the uniform sampling strategy by 5$\sim$6 BLEU points, and feeding representations from the encoder as the decoder input could still improve the strong baseline by 0.2$\sim$0.3 BLEU points after adopting glancing sampling.
To sum up, the adaptive glancing sampling approach contributes the most to the final improvement, and the use of representations from the encoder also helps a bit.

\paragraph{More Analysis}
We also conduct experiments for: a) comparison of different distance metrics for glancing sampling, b) \method with more than one decoding iteration. The details are in Appendix.









%
