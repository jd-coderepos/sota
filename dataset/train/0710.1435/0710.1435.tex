\documentclass[11pt]{article}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{algorithm2e,framed}


\newcommand{\Probab}[1]{\mbox{}{\bf{Pr}}\left[#1\right]}
\newcommand{\Expect}[1]{\mbox{}{\bf{E}}\left[#1\right]}
\newcommand{\Varnce}[1]{\mbox{}{\bf{Var}}\left[#1\right]}
\newcommand{\Trace }[1]{\mbox{}{\bf{Tr}}\left(#1\right)}
\newcommand{\Sqrt  }[1]{\mbox{}\left(#1\right)^{1/2}}
\newcommand{\Qdrt  }[1]{\mbox{}\left(#1\right)^{1/4}}
\newcommand{\FNorm }[1]{\mbox{}\left\|#1\right\|_F  }
\newcommand{\FNormS}[1]{\mbox{}\left\|#1\right\|_F^2}
\newcommand{\FNormQ}[1]{\mbox{}\left\|#1\right\|_F^4}
\newcommand{\TNorm }[1]{\mbox{}\left\|#1\right\|_2  }
\newcommand{\TNormS}[1]{\mbox{}\left\|#1\right\|_2^2}
\newcommand{\TNormQ}[1]{\mbox{}\left\|#1\right\|_2^4}
\newcommand{\XNorm }[1]{\mbox{}\left\|#1\right\|_{\xi}  }
\newcommand{\XNormS}[1]{\mbox{}\left\|#1\right\|_{\xi}^2}
\newcommand{\XNormQ}[1]{\mbox{}\left\|#1\right\|_{\xi}^4}
\newcommand{\VTNorm }[1]{\mbox{}\left|#1\right|  }
\newcommand{\VTNormS}[1]{\mbox{}\left|#1\right|^2}
\newcommand{\VTNormQ}[1]{\mbox{}\left|#1\right|^4}
\newcommand{\VTTNorm }[1]{\mbox{}\left\|#1\right\|_2  }
\newcommand{\VTTNormS}[1]{\mbox{}\left\|#1\right\|_2^2}
\newcommand{\VTTNormQ}[1]{\mbox{}\left\|#1\right\|_2^4}
\newcommand{\VINorm }[1]{\mbox{}\left\|#1\right\|_{\infty}  }
\newcommand{\VINormS}[1]{\mbox{}\left\|#1\right\|_{\infty}^2}
\newcommand{\VINormQ}[1]{\mbox{}\left\|#1\right\|_{\infty}^4}

\newcommand{\setlinespacing}[1]{\setlength{\baselineskip}{#1 \defbaselineskip}}
\newcommand{\doublespacing}{\setlength{\baselineskip}{2.0 \defbaselineskip}}
\newcommand{\singlespacing}{\setlength{\baselineskip}{\defbaselineskip}}

\newcommand{\rank}[1]{{\bf rank}{\left(#1\right)}}
\newcommand{\abs }[1]{\left|#1\right|}
\newcommand{\absS}[1]{\left|#1\right|^{2}}
\newcommand{\spann}[1]{\mbox{\bf span}\left(#1\right)}
\newcommand{\mcut}[1]{\mbox{\bf MAX-CUT}\left[#1\right]}
\newcommand{\qe}{\hfill  \rule{2mm}{3mm}}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newenvironment{Proof}{\noindent {\em Proof:}}{\\\hspace*{\fill}\mbox{}}



\long\def\killtext#1{}

\def\Prob{\hbox{\bf{Pr}}}
\def\E{\hbox{\bf{E}}}
\def\Exp{\hbox{\bf{E}}}
\def\Pick{\hbox{pick}}
\def\var{\hbox{\bf{Var}}}
\def\bG{\hbox{\bf{G}}}
\def\tr{\hbox{\bf{Tr}}}
\def\Tr{\hbox{\bf{Tr}}}
\def\U{\tilde{U}}
\def\Q{\tilde{Q}}
\def\R{\tilde{R}}
\def\n{n}
\def\l{\ell}
\def\sn{n_\lambda}

\def\IP{\hbox{\bf{IP}}}
\def\LP{\hbox{\bf{LP}}}
\def\diag{\hbox{\bf{diag}}}

\def\Vol{\hbox{\bf{Vol}}}
\def\Sketch{\hbox{{\bf Sketch}}}

\newcommand{\qed}{\hfill  \rule{2mm}{3mm}}

\addtolength{\textwidth}{1.4in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.7in}
\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}


\begin{document}

\title{Faster Least Squares Approximation}

\author{
Petros Drineas
\thanks{
Department of Computer Science,
Rensselaer Polytechnic Institute,
Troy, NY,
drinep@cs.rpi.edu.
}
\and Michael W. Mahoney
\thanks{
Department of Mathematics,
Stanford University,
Stanford, CA,
mmahoney@cs.stanford.edu.
}
\and S. Muthukrishnan
\thanks{
Google, Inc., New York, NY, muthu@google.com.}
\and Tam\'{a}s Sarl\'{o}s
\thanks{
Yahoo! Research, Sunnyvale, CA, stamas@yahoo-inc.com.}
}

\date{}
\maketitle
\vspace{5mm}

\begin{abstract}
Least squares approximation is a technique to find an approximate solution to a system of linear equations that has no exact solution. In a typical setting, one lets  be the number of constraints and  be the number of variables, with . Then, existing exact methods find a solution vector in  time. We present two randomized algorithms that provide accurate relative-error approximations to the optimal value and the solution vector of a least squares approximation problem more rapidly than existing exact algorithms. Both of our algorithms preprocess the data with the Randomized Hadamard Transform. One then uniformly randomly samples constraints and solves the smaller problem on those constraints, and the other performs a sparse random projection and solves the smaller problem on those projected coordinates. In both cases, solving the smaller problem provides relative-error approximations, and, if  is sufficiently larger than , the approximate solution can be computed in  time.
\end{abstract}

\vspace{5mm}
\section{Introduction}
\label{sxn:intro}

In many applications in mathematics and statistical data analysis, it is of interest to find an approximate solution to a system of linear equations that has no exact solution. For example, let a
matrix  and a vector  be given. If , there will not in general exist a vector  such that , and yet it is
often of interest to find a vector  such that  in some precise sense. The method of least squares, whose original formulation is often credited to Gauss and Legendre~\cite{Stigler86}, accomplishes this by minimizing the sum of squares of the elements of the residual vector, i.e., by solving the optimization problem

It is well-known that the minimum -norm vector among those satisfying eqn.~(\ref{eqn:orig_ls_prob}) is

where  denotes the Moore-Penrose generalized inverse of the matrix ~\cite{BIG03,GVL96}. This solution vector has a very natural statistical interpretation as providing an optimal estimator among all linear unbiased estimators, and it has a very natural geometric interpretation as providing an orthogonal projection of the vector  onto the span of the columns of the matrix .

Recall that to minimize the quantity in eqn.~(\ref{eqn:orig_ls_prob}), we can set the derivative of  with respect to  equal to zero, from which it follows that the minimizing vector  is a solution of the so-called normal equations

Geometrically, this means that the residual vector  is required to be orthogonal to the column space of , i.e., . While solving the normal equations squares the condition number of the input matrix (and thus is not recommended in practice), direct methods (such as the QR decomposition~\cite{GVL96}) solve the problem of eqn.~(\ref{eqn:orig_ls_prob}) in  time assuming that . Finally, an alternative expression for the vector  of eqn.~(\ref{eqn:xopt_orig_ls_prob}) emerges by leveraging the Singular Value Decomposition (SVD) of . If  denotes the SVD of , then



\subsection{Our results}\label{sxn:intro:ourresults}

In this paper, we describe two randomized algorithms that will provide accurate relative-error approximations to the minimal -norm solution vector  of eqn.~(\ref{eqn:xopt_orig_ls_prob}) faster than existing exact algorithms for a large class of overconstrained least-squares problems. In particular, we will prove the following theorem.

\begin{theorem}
\label{thm:main_result}
Suppose , , and let . Then, there exists a randomized algorithm that returns a vector  such that, with probability at least , the following two claims hold: first,  satisfies

and, second, if  is the condition number of  and if we assume that  is the fraction of the norm of  that lies in the column space of  (i.e., , where  is an orthogonal basis for the column space of ), then  satisfies

Finally, the solution  can be computed in  time if  is sufficiently larger than  and less than .
\end{theorem}
We will provide a precise statement of the running time for our two algorithms (including the -dependence) in Theorems~\ref{thm:alg_sample_fast} (Section~\ref{sxn:sampling})
and~\ref{thm:alg_projection_fast} (Section~\ref{sxn:projection}), respectively. It is worth noting that the claims of Theorem~\ref{thm:main_result} can be made to hold with probability , for any , by repeating the algorithm  times. For example, one could run ten independent copies of the algorithm and keep the vector  that minimizes the residual. This clearly does not increase the running time of the algorithm by more than a constant factor, while driving the failure probability down to (approximately) . Also, we will assume that  is a power of two and that the rank of the  matrix  equals . (We note that padding  and  with all-zero rows suffices to remove the first assumption.)

We now provide a brief overview of our main algorithms. Let the matrix product  denote the  Randomized Hadamard Transform (see also Section~\ref{sxn:RHT}). Here the  matrix  denotes the (normalized) matrix of the Hadamard transform and the  diagonal matrix  is formed by setting its diagonal entries to  or  with equal probability in   independent trials. This transform has been used as one step in the development of a ``fast'' version of the Johnson-Lindenstrauss lemma~\cite{AC06,Matousek08_RSA}. Our first algorithm is a random sampling algorithm. After premultiplying  and  by , this algorithm samples uniformly at random  constraints from the preprocessed problem. (See eqn.~(\ref{eqn:rvaluefinal}), as well as the remarks after Theorem~\ref{thm:alg_sample_fast} for the precise value of .) Then, this algorithm solves the least squares problem on just those sampled constraints to obtain a vector  such that Theorem~\ref{thm:main_result} is satisfied. Note that applying the randomized Hadamard transform to the matrix  and vector  only takes  time. This follows since we will actually sample only  of the constraints from the Hadamard-preprocessed problem~\cite{AL08}. Then, exactly solving the  sampled least-squares problem will require only  time. Assuming that  is a constant and , it follows that the running time of this algorithm is  when .

In a similar manner, our second algorithm also initially premultiplies  and  by . This algorithm then multiplies the result by a  sparse projection matrix , where . This matrix  is described in detail in Section~\ref{sxn:review_previous:projecting}. Its construction depends on a sparsity parameter, and it is identical to the ``sparse
projection'' matrix in Matou\v{s}ek's version of the Ailon-Chazelle result~\cite{AC06,Matousek08_RSA}. Finally, our second algorithm solves the least squares problem on just those  coordinates to
obtain  such that the three claims of Theorem~\ref{thm:main_result} are satisfied. Assuming that  is a constant and , it follows that the running time of this algorithm is  when .

It is worth noting that our second algorithm has a (marginally) less restrictive assumption on the connection between  and . However, the first algorithm is simpler to implement and easier to describe. Clearly, an interesting open problem is to relax the above constraints on  for either of the proposed algorithms.

\subsection{Related work}

We should note several lines of related work.
\begin{itemize}
\item
First, techniques such as the ``method of averages''~\cite{DSS68} preprocess
the input into the form of eqn.~(\ref{eqn:orig_ls_prob_Xrotated}) of
Section~\ref{sxn:precond} and can be used to
obtain exact or approximate solutions to the least squares problem of
eqn.~(\ref{eqn:orig_ls_prob}) in  time under strong statistical
assumptions on  and .
To the best of our knowledge, however, the two algorithms we present and
analyze are the first algorithms to provide nontrivial approximation
guarantees for overconstrained least squares approximation problems in
 time, while making no assumptions at all on the input data.
\item
Second, Ibarra, Moran, and Hui~\cite{IMH82} provide a reduction of the least
squares approximation problem to the matrix multiplication problem.
In particular, they show that  time, where  is the time
needed to multiply two  matrices, is sufficient to solve this
problem.
All of the running times we report in this paper assume the use of standard
matrix multiplication algorithms, since  matrix multiplication
algorithms are almost never used in practice.
Moreover, even with the current best value for the matrix multiplication
exponent, ~\cite{CW87}, our algorithms are still
faster.
\item
Third, motivated by our preliminary results as reported in~\cite{DMM06}
and~\cite{Sarlos06}, both Rokhlin and Tygert~\cite{RT08} as well as Avron,
Maymounkov, and Toledo~\cite{AMT09_DRAFT,AMT10} have empirically evaluated
numerical implementations of variants of one of the algorithms we introduce.
We describe this in more detail below in Section~\ref{sxn:intro-empirical}.
\item
Fourth, very recently, Clarkson and Woodruff proved space lower bounds on
related problems~\cite{CW09}; and Nguyen, Do, and Tran achieved a small
improvement in the sampling complexity for related
problems~\cite{NDT09}.
\end{itemize}

\subsection{Empirical performance of our randomized algorithms}
\label{sxn:intro-empirical}

In prior work we have empirically evaluated randomized algorithms that rely on
the ideas that we introduce in this paper in several large-scale data
analysis tasks. Nevertheless, it is a fair question to ask whether our ``random
perspective'' on linear algebra will work well in numerical
implementations of interest in scientific computation.
We address this question here.
Although we do \emph{not} provide an empirical evaluation in this paper,
in the wake of the original Technical Report version of this paper in
2007~\cite{DMMS07_FastL2_TR}, two groups of researchers have
demonstrated that numerical implementations of variants of the
algorithms we introduce in this paper can perform very well in practice.
\begin{itemize}
\item
In 2008, Rokhlin and Tygert~\cite{RT08} describe a variant of our random
projection algorithm, and they demonstrate that their algorithm runs in
time

where  is an ``oversampling'' parameter and  is a condition
number.
Importantly (at least for very high-precision applications of this random
sampling methodology), they reduce the dependence on  from
 to .
Moreover, by choosing , they demonstrate that .
Although this bound is inferior to ours, they also consider a class of
matrices for which choosing  empirically produced a condition
number , which means that for this class of matrices their
running time is

Their numerical experiments on this class of matrices clearly indicate that
their implementations of variants of our algorithms
perform well for certain matrices as small as thousands of rows by hundreds
of columns.
\item
In 2009, Avron, Maymounkov, Toledo~\cite{AMT09_DRAFT,AMT10}
introduced a randomized least-squares solver based directly on our
algorithms.
They call it Blendenpik, and by considering a much broader class of
matrices, they demonstrate that their solver ``beats \textsc{LAPACK}'s
direct dense least-sqares solver by a large margin on essentially any
dense tall matrix.''
Beyond providing additional theoretical analysis, including backward error
analysis bounds for our algorithm,
they consider five (and numerically implement three) random projection
strategies (i.e., Discrete Fourier Transform, Discrete Cosine
Transform, Discrete Hartely Transform, Walsh-Hadamard Transform, and a Kac
random walk), and they evaluate their algorithms on a wide range of matrices
of various sizes and various ``localization '' or ``coherence'' properties.
Based on these results that empirically show the superior performance of
randomized algorithms such as those we introduce and analyze in this paper
on a wide class of matrices, they go so far as to ``suggest that
random-projection algorithms should be incorporated into future versions of
\textsc{LAPACK}.''
\end{itemize}

\subsection{Outline}

After a brief review of relevant background in Section~\ref{sxn:review_la}, Section~\ref{sxn:precond} presents a structural result outlining conditions on preconditioner matrices that are sufficient for relative-error approximation. Then, we present our main sampling-based algorithm for approximating least squares approximation in Section~\ref{sxn:sampling} and in Section~\ref{sxn:projection} we present a second projection-based algorithm for the same problem.
Preliminary versions of parts of this paper have appeared as conference
proceedings in the 17th ACM-SIAM Symposium on Discrete
Algorithms~\cite{DMM06} and in the 47th IEEE Symposium on Foundations of
Computer Science~\cite{Sarlos06}; and the original Technical Report version
of this journal paper has appeared on the arXiv~\cite{DMMS07_FastL2_TR}.
In particular, the core of our analysis in this paper was introduced in~\cite{DMM06}, where an expensive-to-compute probability distribution was used to construct a relative-error approximation sampling algorithm for the least squares approximation problem. Then, after the development of the Fast Johnson-Lindenstrauss transform~\cite{AC06}, \cite{Sarlos06} proved that similar ideas could be used to improve the running time of randomized algorithms for the least squares approximation problem. In this paper, we have combined these ideas, treated the two algorithms in a manner to highlight their similarities and differences, and considerably simplified the analysis.

\section{Preliminaries}
\label{sxn:review_la}

\subsection{Notation}

We let  denote the set ;  denotes the natural logarithm of  and  denotes the base two logarithm of . For any matrix ,  denotes the -th row of  as a row vector and  denotes the -th column of  as a column vector. Also, given a random variable , we let  denote its expectation and  denote its variance.

We will make frequent use of matrix and vector norms. More specifically, we let  denote the square of the Frobenius norm of , and we let  denote the spectral norm of . For any vector , its -norm (or Euclidean norm) is equal to the square root of the sum of the squares of its elements, while its  norm is defined as .

\subsection{Linear Algebra background}

We now review relevant definitions and facts from linear algebra; for more details, see~\cite{Stewart90,GVL96,Bhatia97,BIG03}. Let the rank of  be . The Singular Value Decomposition (SVD) of  is denoted by , where  is the matrix of left singular vectors,  is the diagonal matrix of non-zero singular values, and  is the matrix of right singular vectors. Let , denote the -th non-zero singular value of , and  and  denote the maximum and minimum singular value of . The condition number of  is . The Moore-Penrose generalized inverse, or pseudoinverse, of  may be expressed in terms of the SVD as ~\cite{BIG03}. Finally, for any orthogonal matrix , let  denote an orthogonal matrix whose columns are an orthonormal basis spanning the subspace of  that is orthogonal to the column space of .  In terms of , the optimal value of the least squares residual of eqn.~(\ref{eqn:orig_ls_prob}) is

\subsection{Markov's inequality and the union bound}

We will make frequent use of the following fundamental result from probability theory, known as Markov's inequality~\cite{MotwaniRaghavan95}. Let  be a random variable assuming non-negative values with expectation . Then, for all ,

with probability at least .

We will also need the so-called union bound. Given a set of random events  holding with respective probabilities , the probability that all events hold (i.e., the probability of the union of those events) is upper bounded by .

\subsection{The Randomized Hadamard Transform}\label{sxn:RHT}

The Randomized Hadamard Transform was introduced in~\cite{AC06} as one step in the development of a fast version of the Johnson-Lindenstrauss lemma~\cite{AC06,Matousek08_RSA}. Recall that the (non-normalized)  matrix of the Hadamard transform  may be defined recursively as follows:

The  normalized matrix of the Hadamard transform is equal to ; hereafter, we will denote this normalized matrix by . Now consider a diagonal matrix  such that the diagonal entries  are set to +1 with probability  and to  with probability  in  independent trials. The product  is the Randomized Hadamard Transform and has two useful properties. First, when applied to a vector, it ``spreads out'' its energy, in the sense of providing a bound for its infinity norm (see Section~\ref{sxn:review_previous:hadamard}). Second, computing the product  for any vector  takes  time. Even better, if we only need to access, say,  elements in the transformed vector, then those  elements can be computed in  time~\cite{AL08}. We will expand on the latter observation in the proofs of Theorems~\ref{thm:alg_sample_fast} and~\ref{thm:alg_projection_fast}.


\section{Our algorithms as preconditioners}\label{sxn:precond}

Both of our algorithms may be viewed as preconditioning the input matrix  and the target vector  with a carefully-constructed data-independent random matrix . For our random sampling algorithm, we let , where  is a matrix that represents the sampling operation and  is the Randomized Hadamard Transform, while for our random projection algorithm, we let , where  is a random projection matrix. Thus, we replace the least squares approximation problem of eqn.~(\ref{eqn:orig_ls_prob}) with the least squares approximation problem

We explicitly compute the solution to the above problem using a traditional deterministic algorithm~\cite{GVL96}, e.g., by computing the vector

Alternatively, one could use standard iterative methods such as the the Conjugate Gradient Normal Residual method (CGNR, see~\cite{GVL96} for details), which can produce an -approximation to the optimal solution of eqn.~(\ref{eqn:orig_ls_prob_Xrotated}) in  time, where  is the condition number of  and  is the number of rows of .

\subsection{A structural result sufficient for relative-error approximation}
\label{sxn:sampling:proofs:structural}

In this subsection, we will state and prove a lemma that establishes sufficient conditions on any matrix  such that the solution vector  to the least squares problem of eqn.~(\ref{eqn:orig_ls_prob_Xrotated}) will satisfy relative-error bounds of the form~(\ref{eqn:result1_intro}) and~(\ref{eqn:result2_intro}). Recall that the SVD of  is . In addition, for notational simplicity, we let  denote the  part of the right hand side vector  lying outside of the column space of .

The two conditions that we will require of the matrix  are:

for some . Several things should be noted about these conditions. First, although condition~(\ref{eqn:lemma1_ass2}) depends on the right hand side vector , Algorithms~\ref{alg:alg_sample_fast} and~\ref{alg:alg_projection_fast} will satisfy it without using any information from . Second, although condition~(\ref{eqn:lemma1_ass1}) only states that , for all , for both of our randomized algorithms we will show that , for all . Thus, one should think of  as an approximate isometry. Third, condition~(\ref{eqn:lemma1_ass2}) simply states that  remains approximately orthogonal to . Finally, note that the following lemma is a deterministic statement, since it makes no explicit reference to either of our randomized algorithms. Failure probabilities will enter later when we show that our randomized algorithms satisfy conditions~(\ref{eqn:lemma1_ass1}) and~(\ref{eqn:lemma1_ass2}).

\begin{lemma} \label{lem:suff_cond}
Consider the overconstrained least squares approximation problem of eqn.~(\ref{eqn:orig_ls_prob}) and let the matrix  contain the top  left singular vectors of . Assume that the matrix  satisfies conditions~(\ref{eqn:lemma1_ass1}) and~(\ref{eqn:lemma1_ass2}) above, for some . Then, the solution vector  to the least squares approximation
problem~(\ref{eqn:orig_ls_prob_Xrotated}) satisfies:

\end{lemma}
\begin{Proof}
Let us first rewrite the down-scaled regression problem induced by
 as

(\ref{eqn:ds1}) follows since  and (\ref{eqn:ds2}) follows since the columns of the matrix  span the same subspace as the columns of . Now, let   be such that , and note that  minimizes eqn.~(\ref{eqn:ds2}). The latter fact follows since

Thus, by the normal equations~(\ref{eqn:normal_eqn}), we have that

Taking the norm of both sides and observing that under condition~(\ref{eqn:lemma1_ass1}) we have , for all , it follows that

Using condition~(\ref{eqn:lemma1_ass2}) we observe that


\noindent To establish the first claim of the lemma, let us rewrite
the norm of the residual vector as

where (\ref{eqn:pfCeq1}) follows by Pythagoras, since , which is orthogonal to , and consequently to
; (\ref{eqn:pfCeq2}) follows
by the definition of  and ; and (\ref{eqn:pfCeq3})
follows by (\ref{eqn:z-norm2}) and the orthogonality of . The first claim of the lemma follows since
.

To establish the second claim of the lemma, recall that . If we take the norm of both sides of this expression, we have that

where (\ref{eqn:pfDeq1}) follows since  is the
smallest singular value of  and since the rank of  is
; and (\ref{eqn:pfDeq2}) follows by
(\ref{eqn:z-norm2}) and the orthogonality of . Taking the square root, the second claim of the
lemma follows.
\end{Proof}

\noindent If we make no assumption on , then~(\ref{eqn:lemma1_eq4}) from
Lemma~\ref{lem:suff_cond} may provide a weak bound in terms of
. If, on the other hand, we make the additional
assumption that a constant fraction of the norm of  lies in
the subspace spanned by the columns of ,
then~(\ref{eqn:lemma1_eq4}) can be strengthened.
Such an assumption is reasonable, since most least-squares problems are
practically interesting if at least some part of  lies in the subspace
spanned by the columns of .

\begin{lemma}
\label{lem:suff_cond2}
Using the notation of Lemma~\ref{lem:suff_cond} and assuming
that , for some fixed
 it follows that

\end{lemma}
\begin{Proof}
Since , it follows that

This last inequality follows from , which implies 
By combining this with eqn. (\ref{eqn:lemma1_eq4}) of
Lemma~\ref{lem:suff_cond}, the lemma follows.
\end{Proof}

\section{A sampling-based randomized algorithm} \label{sxn:sampling}

In this section, we present our randomized sampling algorithm for the least squares approximation problem of eqn.~(\ref{eqn:orig_ls_prob}). We also state and prove an associated quality-of-approximation theorem.

\subsection{The main algorithm and main theorem} \label{sxn:sampling:result}

Algorithm~\ref{alg:alg_sample_fast} takes as input a matrix , a vector , and an error parameter . This algorithm starts by preprocessing the matrix  and the vector  with the Randomized Hadamard Transform. It then constructs a smaller problem by sampling uniformly at random a small number of constraints from the preprocessed problem. Our main quality-of-approximation theorem (Theorem~\ref{thm:alg_sample_fast} below) states that with constant probability over the random choices made by the algorithm, the vector
 returned by this algorithm will satisfy the relative-error bounds of eqns.~(\ref{eqn:result1_intro}) and~(\ref{eqn:result2_intro}) and will be computed quickly.

\begin{algorithm}[h]
\begin{framed}

\textbf{Input:} , , and an error parameter .

\vspace{0.1in}

\textbf{Output:} .

\begin{enumerate}

\item Let  assume the value of eqn.~(\ref{eqn:rvaluefinal}).

\item Let  be an empty matrix.

\item \textbf{For}  (i.i.d. trials with replacement) \textbf{select uniformly at random} an integer from .

\begin{itemize}

\item \textbf{If}  is selected, \textbf{then} append the column vector  to , where  is an all-zeros vector except for its -th entry which is set to one.

\end{itemize}

\item Let  be the normalized Hadamard transform
matrix.

\item Let  be a diagonal matrix with

\item
Compute and return .
\end{enumerate}

\end{framed}
\caption{A fast random sampling algorithm for least squares
approximation} \label{alg:alg_sample_fast}
\end{algorithm}


In more detail, after preprocessing with the Randomized Hadamard Transform of Section~\ref{sxn:RHT}, Algorithm~\ref{alg:alg_sample_fast} samples exactly  constraints from the preprocessed least squares problem, rescales each sampled constraint by , and solves the least squares problem induced on just those sampled and rescaled constraints. (Note that the algorithm explicitly computes only those rows of  and only those elements of  that need to be accessed.) More formally, we will let  denote a sampling matrix specifying which of the  constraints are to be sampled and how they are to be rescaled. This matrix is initially empty and is constructed as described in Algorithm~\ref{alg:alg_sample_fast}. Then, we can consider the problem

which is just a least squares approximation problem involving the  constraints sampled from the matrix  after the preprocessing with the Randomized Hadamard Transform. The minimum -norm vector  among those that achieve the minimum value  in this problem is

which is the output of Algorithm~\ref{alg:alg_sample_fast}.
\begin{theorem}
\label{thm:alg_sample_fast}
Suppose , , and let . Run Algorithm~\ref{alg:alg_sample_fast} with

and return . Then, with probability at least .8, the following two claims hold: first,  satisfies

and, second, if we assume that  for some , then  satisfies

Finally,

time suffices to compute the solution .
\end{theorem}
\noindent \textbf{Remark:} Assuming that , and using , we get that  Thus, the running time of Algorithm~\ref{alg:alg_sample_fast} becomes

Assuming that , the above running time reduces to  It is worth noting that improvements over the standard  time could be derived with weaker assumptions on  and . However, for the sake of clarity of presentation, we only focus on the above setting.

\noindent \textbf{Remark:}
The assumptions in our theorem have a natural geometric interpretation.\footnote{We would like to thank Ilse Ipsen for pointing out to us this
geometric interpretation.}
In particular, they imply that our approximation becomes worse as the angle
between the vector  and the column space of  increases.
To see this, let , and note that
.
Hence the assumption
 can be simply stated as

The fraction  is the sine of the angle between  and
the column space of ; see page 242 of~\cite{GVL96}.
Thus,  is a bound on the tangent between  and the
column space of ; see page 244 of~\cite{GVL96}.
This means that the bound for  is
proportional to this tangent.

\subsection{The effect of the Randomized Hadamard Transform}
\label{sxn:review_previous:hadamard}

In this subsection, we state a lemma that quantifies the manner in which  approximately ``uniformizes'' information in the left singular subspace of the matrix . We state the lemma for a general  orthogonal matrix  such that , although we will be interested in the case when  and  consists of the top  left singular vectors of the matrix .

\begin{lemma} \label{lem:HU}
Let  be an  orthogonal matrix and let the product  be the  Randomized Hadamard Transform of Section~\ref{sxn:RHT}. Then, with probability at least ,



\end{lemma}
\begin{Proof}
We follow the proof of Lemma 2.1 in~\cite{AC06}. In that lemma, the authors essentially prove that the Randomized Hadamard Transform  ``spreads out'' input vectors. More specifically, since the columns of the matrix  (denoted by  for all ) are unit vectors, they prove that for fixed  and fixed ,

(Note that we consider  vectors in  whereas~\cite{AC06} considered  vectors in  and thus the roles of  and  are inverted in our proof.) Let  to get

From a standard union bound, this immediately implies that with probability at least ,

holds for all  and  . Using

for all , we conclude the proof of the lemma.
\end{Proof}

\subsection{Satisfying condition~(\ref{eqn:lemma1_ass1})}

We now establish the following lemma which states that all the singular values of  are close to one. The proof of Lemma~\ref{lem:sample_lem20pf} depends on a bound for approximating the product of a matrix times its transpose by sampling (and rescaling) a small number of columns of the matrix. This bound appears as Theorem~\ref{thm:theorem7correct} in the Appendix and is an improvement over prior work of ours in~\cite{DMM08_CURtheory_JRNL}.

\begin{lemma}
\label{lem:sample_lem20pf}
Assume that eqn.~(\ref{eqn:lem:HU_eqn2}) holds. If

then, with probability at least .95,

holds for all .
\end{lemma}
\begin{Proof}
Note that for all 

In the above, we used the fact that . We now can view  as an approximation to the product of two matrices  and  by randomly sampling and rescaling columns of . Thus, we can leverage Theorem~\ref{thm:theorem7correct} from the Appendix. More specifically, consider the matrix . Obviously, since , , and  are orthogonal matrices,  and . Let ; since we assumed that eqn.~(\ref{eqn:lem:HU_eqn2}) holds, we note that the columns of , which correspond to the rows of , satisfy

Thus, applying Theorem~\ref{thm:theorem7correct} with  as above, , and  implies that

holds with probability at least . For the above bound to hold, we need  to assume the value of eqn.~(\ref{eqn:rvalue}). Finally, we note that since , the assumption of Theorem~\ref{thm:theorem7correct} on the Frobenius norm of the input matrix is always satisfied. Combining the above with inequality~(\ref{eqn:eqX31}) concludes the proof of the lemma.
\end{Proof}

\subsection{Satisfying condition~(\ref{eqn:lemma1_ass2})}

We next prove the following lemma, from which it will follow that condition~(\ref{eqn:lemma1_ass2}) is satisfied by Algorithm~\ref{alg:alg_sample_fast}. The proof of this lemma depends on bounds for randomized matrix multiplication algorithms that appeared in~\cite{dkm_matrix1}.

\begin{lemma}
\label{lem:sample_lem40pf}
If eqn.~(\ref{eqn:lem:HU_eqn2}) holds and , then with probability at least .9,

\end{lemma}
\begin{Proof}
Recall that  and that . We start by noting that since
 it follows that

Thus, we can view  as approximating the product of two matrices  and  by randomly sampling columns from  and rows/elements from . Note that the sampling probabilities are uniform and do not depend on the norms of the columns of  or the rows of . However, we can still apply the results of Table 1 (second row) in page 150 of~\cite{dkm_matrix1}. More specifically, since we condition on eqn.~(\ref{eqn:lem:HU_eqn2}) holding, the rows of  (which of course correspond to columns of ) satisfy

for . Applying the result of Table 1 (second row) of~\cite{dkm_matrix1} we get

In the above we used . Markov's inequality now implies that with probability at least .9,

Setting  and using the value of  specified above concludes the proof of the lemma.
\end{Proof}

\subsection{Completing the proof of Theorem~\ref{thm:alg_sample_fast}}
\label{sxn:sampling:proofs:complete}

We now complete the proof of Theorem~\ref{thm:alg_sample_fast}. First, let  denote the event that eqn.~(\ref{eqn:lem:HU_eqn2}) holds; clearly, . Second, let  denote the event that both Lemmas~\ref{lem:sample_lem20pf} and~\ref{lem:sample_lem40pf} hold conditioned on  holding. Then,

In the above,  denotes the complement of event . In the first inequality we used the union bound and in the second inequality we leveraged the bounds for the failure probabilities of Lemmas~\ref{lem:sample_lem20pf} and~\ref{lem:sample_lem40pf} given that eqn.~(\ref{eqn:lem:HU_eqn2}) holds. We now let  denote the event that both Lemmas~\ref{lem:sample_lem20pf} and~\ref{lem:sample_lem40pf} hold, without any a priori conditioning on event ; we will bound  as follows:

In the first inequality we used the fact that all probabilities are positive. The above derivation immediately bounds the success probability of Theorem~\ref{thm:alg_sample_fast}. Combining Lemmas~\ref{lem:sample_lem20pf} and~\ref{lem:sample_lem40pf} with the structural results of Lemma~\ref{lem:suff_cond} and setting  as in eqn.~(\ref{eqn:rvaluefinal}) concludes the proof of the accuracy guarantees of Theorem~\ref{thm:alg_sample_fast}.

We now discuss the running time of Algorithm~\ref{alg:alg_sample_fast}. First of all, by the construction of , the number of non-zero entries in  is . In Step  we need to compute the products  and . Recall that  has  columns and thus the running time of computing both products is equal to the time needed to apply  on  vectors. First, note that in order to apply  on  vectors in ,  operations suffice. In order to estimate how many operations are needed to apply  on  vectors, we use the results of Theorem~ (see also Section 7) of Ailon and Liberty~\cite{AL08}, which state that at most  operations are needed for this operation. Here  denotes the number of non-zero elements in the matrix , which is at most . After this preprocessing, Algorithm~\ref{alg:alg_sample_fast} must compute the pseudoinverse of an  matrix, or, equivalently, solve a least-squares problem on  constraints and  variables. This operation can be performed in  time since . Thus, the entire algorithm runs in time



\section{A projection-based randomized algorithm}
\label{sxn:projection}

In this section, we present a projection-based randomized algorithm for the least squares approximation problem of eqn. (\ref{eqn:orig_ls_prob}). We also state and prove an associated quality-of-approximation theorem.

\subsection{The main algorithm and main theorem}
\label{sxn:projection:result}

Algorithm~\ref{alg:alg_projection_fast} takes as input a matrix , a vector , and an error parameter . This algorithm also starts by preprocessing the matrix  and right hand side vector  with the Randomized Hadamard Transform. It then constructs a smaller problem by performing a ``sparse projection'' on the preprocessed problem. Our main quality-of-approximation theorem (Theorem~\ref{thm:alg_projection_fast} below) will state that with constant probability (over the random choices made by the algorithm) the vector
 returned by this algorithm will satisfy the relative-error bounds of eqns.~(\ref{eqn:result1_intro}) and~(\ref{eqn:result2_intro}) and will be computed quickly.

\begin{algorithm}[h]
\begin{framed}

\textbf{Input:} , , and an error parameter .

\vspace{0.1in}

\textbf{Output:} .

\begin{enumerate}

\item Let  and  assume the values of eqns.~(\ref{eqn:valueqfinal}) and~(\ref{eqn:valuekfinal}).

\item Let  be a random matrix with

for all  independently.
\item Let  be the normalized Hadamard transform
matrix.

\item Let  be a diagonal matrix with

\item
Compute and return
.
\end{enumerate}

\end{framed}
\caption{A fast random projection algorithm for least squares
approximation} \label{alg:alg_projection_fast}
\end{algorithm}


In more detail, Algorithm~\ref{alg:alg_projection_fast} begins by preprocessing the matrix  and right hand side vector  with the Randomized Hadamard Transform  of Section~\ref{sxn:RHT}. This algorithm explicitly computes only those rows of  and those elements of  that need to be accessed to perform the sparse projection. After this initial preprocessing, Algorithm~\ref{alg:alg_projection_fast} will perform a ``sparse projection'' by multiplying  and  by the sparse matrix  (described in more detail in
Section~\ref{sxn:review_previous:projecting}). Then, we can consider the problem

which is just a least squares approximation problem involving the matrix  and the vector . The minimum -norm vector  among those that achieve the minimum value  in this problem is

which is the output of Algorithm~\ref{alg:alg_projection_fast}.
\begin{theorem} \label{thm:alg_projection_fast}
Suppose , , and let . Run Algorithm~\ref{alg:alg_projection_fast} with\footnote{ and  are the unspecified constants of Lemma~\ref{lem:matousek}.}

and return . Then, with probability at least , the following two claims hold:
first,  satisfies

and, second, if we assume that
 for some 
then  satisfies

Finally, the expected running time of the algorithm is (at most)

\end{theorem}
\noindent \textbf{Remark:} Assuming that  we get that

Thus, the expected running time of Algorithm~\ref{alg:alg_projection_fast} becomes

Finally, assuming , the above running time reduces to  It is worth noting that improvements over the standard  time could be derived with weaker assumptions on  and .

\subsection{Sparse projection matrices}
\label{sxn:review_previous:projecting}

In this subsection, we state a lemma about the action of a sparse random matrix operating on a vector. Recall that given any set of  points in Euclidean space, the Johnson-Lindenstrauss lemma states that those points can be mapped via a linear function to  dimensions such that the distances between all pairs of points are preserved to within a multiplicative factor of ; see~\cite{Matousek08_RSA} and references therein for details.

Formally, let  be an error parameter,  be a failure probability, and  be a ``uniformity'' parameter. In addition, let  be a ``sparsity'' parameter defining the expected number of nonzero elements per row, and let  be the number of rows in our matrix. Then, define the  random matrix  as in Algorithm
\ref{alg:alg_projection_fast}. Matou\v{s}ek proved the following lemma, as the key step in his version of the Ailon-Chazelle result~\cite{AC06,Matousek08_RSA}.

\begin{lemma}\label{lem:matousek}
Let  be the sparse random matrix of Algorithm~\ref{alg:alg_projection_fast}, where  for some sufficiently large constant  (but still such that ), and ) for some sufficiently large constant  (but such that  is integral). Then for every vector  such that , we have that with probability at least 

\end{lemma}

\noindent \textbf{Remark:} In order to achieve sufficient concentration for all vectors , the linear mapping defining the Johnson-Lindenstrauss transform is typically
``dense,'' in the sense that almost all the elements in each of the  rows of the matrix defining the mapping are nonzero. In this case, implementing the mapping on  vectors (in, e.g., a matrix ) via a matrix multiplication requires  time. This is not faster than the  time required to compute an exact solution to the problem of eqn.~(\ref{eqn:orig_ls_prob}) if  is at least . The Ailon-Chazelle result~\cite{AC06,Matousek08_RSA} states that the mapping can be ``sparse,'' in the sense that only a few of the elements in each of the  rows need to be nonzero, provided that the vector  is ``well-spread,'' in the sense that  is close to . This is exactly what the preprocessing with the Randomized Hadamard Transform guarantees.

\subsection{Proof of Theorem~\ref{thm:alg_projection_fast}}
\label{sxn:projection:proofs}

In this subsection, we provide a proof of Theorem~\ref{thm:alg_projection_fast}. Recall that by the results of Section~\ref{sxn:sampling:proofs:structural}, in order to prove Theorem~\ref{thm:alg_projection_fast}, we must show that the matrix  constructed by Algorithm~\ref{alg:alg_projection_fast} satisfies conditions~(\ref{eqn:lemma1_ass1}) and~(\ref{eqn:lemma1_ass2}) with probability at least . The next two subsections focus on proving that these conditions hold; the last subsection discusses the running time of Algorithm~\ref{alg:alg_projection_fast}.

\subsubsection{Satisfying condition~(\ref{eqn:lemma1_ass1})}
In order to prove that all the singular values of  are close to one, we start with the following lemma which provides a means to bound the spectral norm of a matrix. This lemma is an instantiation of lemmas that appeared in~\cite{AHK06,FO05}.
\begin{lemma}\label{lem:AroraHazenKale}
Let  be a  symmetric matrix and define the grid

In words,  includes all -dimensional vectors  whose coordinates are integer multiples of  and satisfy . Then, the cardinality of  is at most . In addition, if for every  we have that , then for every unit vector  we have that .
\end{lemma}
We next establish Lemma~\ref{lem:project_lem20pf}, which states that all the singular values of  are close to one with constant probability. The proof of this lemma depends on the bound provided by Lemma~\ref{lem:AroraHazenKale} and it immediately shows that condition~(\ref{eqn:lemma1_ass1}) is satisfied by Algorithm~\ref{alg:alg_projection_fast}.
\begin{lemma}\label{lem:project_lem20pf}
Assume that Lemma~\ref{lem:HU} holds. If  and  satisfy:

then, with probability at least .95,

holds for all . Here  and  are the unspecified constants of Lemma~\ref{lem:matousek}.
\end{lemma}
\begin{Proof}
Define the symmetric matrix , recall that , and note that

holds for all . Consider the grid  of eqn.~(\ref{eqn:grid}) and note that there are no more than  pairs , since  by Lemma~\ref{lem:AroraHazenKale}. Since , in order to show that , it suffices by Lemma~\ref{lem:AroraHazenKale} to show that , for all . To do so, first, consider a single  pair. Let

and note that

By multiplying out the right hand side of the above equation and rearranging terms, it follows that

In order to use Lemma~\ref{lem:matousek} to bound the quantities , and , we need a bound on the uniformity ratio . To do so, note that

The above inequalities follow by  and Lemma~\ref{lem:HU}. This holds for both our chosen points  and  and in fact for all . Let  and let  (these choices will be explained shortly). Then, it follows from Lemma~\ref{lem:matousek} that by setting  and our choices for  and , each of the following three statements holds with probability at least :

Thus, combining the above with eqn.~(\ref{eqn:pd1}), for this single pair of vectors ,

holds with probability at least . Next, recall that there are no more than  pairs of vectors , and we need eqn.~(\ref{eqn:eqX33}) to hold for all of them. Since we set  then it follows by a union bound that eqn.~(\ref{eqn:eqX33}) holds for all pairs of vectors  with probability at least .95. Additionally, let us set , which implies that  thus concluding the proof of the lemma.

Finally, we discuss the values of the parameters  and . Since , , and , the appropriate values for  and  emerge after elementary manipulations from Lemma~\ref{lem:matousek}.
\end{Proof}

\subsubsection{Satisfying condition~(\ref{eqn:lemma1_ass2})}

In order to prove that condition~(\ref{eqn:lemma1_ass2}) is satisfied, we start with Lemma~\ref{lem:Tmatmult}. In words, this lemma states that given vectors  and  we can use the random sparse projection matrix  to approximate  by , provided that  (or , but not necessarily both) is bounded. The proof of this lemma is elementary but tedious and is deferred to Section~\ref{sxn:pf_of_technical_lemma} of the Appendix.
\begin{lemma}
\label{lem:Tmatmult} Let  be vectors in  such that . Let  be the  sparse projection matrix of Section~\ref{sxn:review_previous:projecting}, with sparsity parameter . If , then

\end{lemma}
\noindent The following lemma proves that condition~(\ref{eqn:lemma1_ass2}) is satisfied by Algorithm~\ref{alg:alg_projection_fast}. The proof of this lemma depends on the bound provided by Lemma~\ref{lem:Tmatmult}. Recall that  and thus .
\begin{lemma} \label{lem:project_lem40pf}
Assume that eqn.~(\ref{eqn:lem:HU_eqn2}) holds. If  and , then, with probability at least .9, 
\end{lemma}
\begin{Proof}
We first note that since , it follows that , for all . Thus, we have that

We now bound the expectation of the left hand side of eqn.~(\ref{eqn:eqX41}) by using Lemma~\ref{lem:Tmatmult} to bound each term on the right hand side of eqn.~(\ref{eqn:eqX41}). Using eqn.~(\ref{eqn:eqpd12}) of Lemma~\ref{lem:HU} we get that

holds for all . By our choice of the sparsity parameter  the conditions of Lemma~\ref{lem:Tmatmult} are satisfied. It follows from Lemma~\ref{lem:Tmatmult} that

The last line follows since , for all . Using Markov's inequality, we get that with probability at least ,

The proof of the lemma is concluded by using the assumed value of .
\end{Proof}

\subsubsection{Proving Theorem~\ref{thm:alg_projection_fast}}

By our choices of  and  as in eqns.~(\ref{eqn:valuekfinal}) and~(\ref{eqn:valueqfinal}), it follows that both conditions~(\ref{eqn:lemma1_ass1}) and~(\ref{eqn:lemma1_ass2}) are satisfied. Combining with Lemma~\ref{lem:suff_cond} we immediately get the accuracy guarantees of Theorem~\ref{thm:alg_projection_fast}. The failure probability of Algorithm~\ref{alg:alg_projection_fast} can be bounded using an argument similar to the one used in Section~\ref{sxn:sampling:proofs:complete}.

In order to complete the proof we discuss the running time of Algorithm~\ref{alg:alg_projection_fast}. First of all, by the construction of , the expected number of non-zero entries in  is . In Step  we need to compute the products  and . Recall that  has  columns and thus the running time of computing both products is equal to the time needed to apply  on  vectors. First, note that in order to apply  on  vectors in ,  operations suffice. In order to estimate how many operations are needed to apply  on  vectors, we use the results of Theorem~ (see also Section 7) of Ailon and Liberty~\cite{AL08}, which state that at most  operations are needed for this operation. Here  denotes the number of non-zero elements in the matrix , which -- in expectation -- is . After this preprocessing, Algorithm~\ref{alg:alg_projection_fast} must compute the pseudoinverse of a  matrix, or, equivalently, solve a least-squares problem on  constraints and  variables. This operation can be performed in  time since . Thus, the entire algorithm runs in expected time



\begin{thebibliography}{10}

\bibitem{AC06}
N.~Ailon and B.~Chazelle.
\newblock Approximate nearest neighbors and the fast {J}ohnson-{L}indenstrauss
  transform.
\newblock In {\em Proceedings of the 38th Annual ACM Symposium on Theory of
  Computing}, pages 557--563, 2006.

\bibitem{AL08}
N.~Ailon and E.~Liberty.
\newblock Fast dimension reduction using {R}ademacher series on dual {BCH}
  codes.
\newblock In {\em Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 1--9, 2008.

\bibitem{AHK06}
S.~Arora, E.~Hazan, and S.~Kale.
\newblock A fast random sampling algorithm for sparsifying matrices.
\newblock In {\em Proceedings of the 10th International Workshop on
  Randomization and Computation}, pages 272--279, 2006.

\bibitem{AMT09_DRAFT}
H.~Avron, P.~Maymounkov, and S.~Toledo.
\newblock Blendenpik: Supercharging {LAPACK}'s least-squares solver.
\newblock Manuscript. (2009).

\bibitem{AMT10}
H.~Avron, P.~Maymounkov, and S.~Toledo.
\newblock Blendenpik: Supercharging {LAPACK}'s least-squares solver.
\newblock {\em SIAM Journal on Scientific Computing}, 32:1217--1236, 2010.

\bibitem{BIG03}
A.~Ben-Israel and T.N.E. Greville.
\newblock {\em Generalized Inverses: Theory and Applications}.
\newblock Springer-Verlag, New York, 2003.

\bibitem{Bhatia97}
R.~Bhatia.
\newblock {\em Matrix Analysis}.
\newblock Springer-Verlag, New York, 1997.

\bibitem{CW09}
K.L. Clarkson and D.P. Woodruff.
\newblock Numerical linear algebra in the streaming model.
\newblock In {\em Proceedings of the 41st Annual ACM Symposium on Theory of
  Computing}, pages 205--214, 2009.

\bibitem{CW87}
D.~Coppersmith and S.~Winograd.
\newblock Matrix multiplication via arithmetic progressions.
\newblock {\em Journal of Symbolic Computation}, 9(3):251--280, 1990.

\bibitem{DSS68}
G.~Dahlquist, B.~Sj\"{o}berg, and P.~Svensson.
\newblock Comparison of the method of averages with the method of least
  squares.
\newblock {\em Mathematics of Computation}, 22(104):833--845, 1968.

\bibitem{dkm_matrix1}
P.~Drineas, R.~Kannan, and M.W. Mahoney.
\newblock Fast {Monte Carlo} algorithms for matrices {I}: Approximating matrix
  multiplication.
\newblock {\em SIAM Journal on Computing}, 36:132--157, 2006.

\bibitem{DMM06}
P.~Drineas, M.W. Mahoney, and S.~Muthukrishnan.
\newblock Sampling algorithms for  regression and applications.
\newblock In {\em Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 1127--1136, 2006.

\bibitem{DMM08_CURtheory_JRNL}
P.~Drineas, M.W. Mahoney, and S.~Muthukrishnan.
\newblock Relative-error {CUR} matrix decompositions.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 30:844--881,
  2008.

\bibitem{DMMS07_FastL2_TR}
P.~Drineas, M.W. Mahoney, S.~Muthukrishnan, and T.~Sarl\'{o}s.
\newblock Faster least squares approximation.
\newblock Technical report.
\newblock Preprint: arXiv:0710.1435 (2007).

\bibitem{FO05}
U.~Feige and E.~Ofek.
\newblock Spectral techniques applied to sparse random graphs.
\newblock {\em Random Structures and Algorithms}, 27(2):251--275, 2005.

\bibitem{GVL96}
G.H. Golub and C.F.~Van Loan.
\newblock {\em Matrix Computations}.
\newblock Johns Hopkins University Press, Baltimore, 1996.

\bibitem{IMH82}
O.H. Ibarra, S.~Moran, and R.~Hui.
\newblock A generalization of the fast {LUP} matrix decomposition algorithm and
  applications.
\newblock {\em Journal of Algorithms}, 3:45--56, 1982.

\bibitem{Matousek08_RSA}
J.~Matou\v{s}ek.
\newblock On variants of the {J}ohnson--{L}indenstrauss lemma.
\newblock {\em Random Structures and Algorithms}, 33(2):142--156, 2008.

\bibitem{MotwaniRaghavan95}
R.~Motwani and P.~Raghavan.
\newblock {\em Randomized Algorithms}.
\newblock Cambridge University Press, New York, 1995.

\bibitem{NDT09}
N.H. Nguyen, T.T. Do, and T.D. Tran.
\newblock A fast and efficient algorithm for low-rank approximation of a
  matrix.
\newblock In {\em Proceedings of the 41st Annual ACM Symposium on Theory of
  Computing}, pages 215--224, 2009.

\bibitem{Oli10}
R.~I. Oliveira.
\newblock {Sums of random Hermitian matrices and an inequality by Rudelson}.
\newblock Technical report.
\newblock Preprint: arXiv:1004.3821v1 (2010).

\bibitem{RT08}
V.~Rokhlin and M.~Tygert.
\newblock A fast randomized algorithm for overdetermined linear least-squares
  regression.
\newblock {\em Proc. Natl. Acad. Sci. USA}, 105(36):13212--13217, 2008.

\bibitem{RV07}
M.~Rudelson and R.~Vershynin.
\newblock Sampling from large matrices: an approach through geometric
  functional analysis.
\newblock {\em Journal of the ACM}, 54(4):Article 21, 2007.

\bibitem{Sarlos06}
T.~Sarl\'{o}s.
\newblock Improved approximation algorithms for large matrices via random
  projections.
\newblock In {\em Proceedings of the 47th Annual IEEE Symposium on Foundations
  of Computer Science}, pages 143--152, 2006.

\bibitem{Stewart90}
G.W. Stewart and J.G. Sun.
\newblock {\em Matrix Perturbation Theory}.
\newblock Academic Press, New York, 1990.

\bibitem{Stigler86}
S.M. Stigler.
\newblock {\em The History of Statistics: The Measurement of Uncertainty before
  1900}.
\newblock Harvard University Press, Cambridge, 1986.

\end{thebibliography}
\section{Appendix}

\subsection{Approximating matrix multiplication}

Let  be any matrix. Consider the following algorithm (which is essentially the algorithm in page 876 of~\cite{DMM08_CURtheory_JRNL}) that constructs a matrix  consisting of  rescaled columns of . We will seek a bound on the approximation error , which we will provide in Theorem~\ref{thm:theorem7correct}. A variant of this theorem appeared as Theorem 7 in~\cite{DMM08_CURtheory_JRNL}; this version modifies and supersedes eqn. (47) of Theorem 7 in the following manner: first, we will assume that the spectral norm of  is bounded and is at most one (this is a minor normalization assumption). Second, and most importantly, we will need to set  to be at least the value of eqn. (\ref{eqn:CboundAppendix}) for the theorem to hold. This second assumption was omitted from the statement of eqn. (47) in Theorem 7 of~\cite{DMM08_CURtheory_JRNL}.

\begin{algorithm}
\begin{framed}

\SetLine

\AlgData{
,
 s.t. ,
positive integer .}

\AlgResult{

}

Initialize  to be an all-zero matrix.

\For{}{
   Pick , where \;
   \;
}


Return \;

\end{framed}
\caption{
The \textsc{Exactly()} algorithm.
}
\label{alg:SDconstruct_exact}
\end{algorithm}



\begin{theorem}\label{thm:theorem7correct}
Let  with . Construct  using the \textsc{Exactly()} algorithm and let the sampling probabilities  satisfy

for all  for some constant . Let  be an accuracy parameter and assume . If

then, with probability at least ,

\end{theorem}

\begin{Proof}
Consider the \textsc{Exactly} algorithm. Then

Similar to~\cite{RV07} we shall view the matrix  as the true mean of a bounded operator valued random variable, whereas  will be its empirical mean. Then, we will apply Lemma 1 of~\cite{Oli10}. To this end, define a random vector  as

for . The matrix  has columns , where  are  independent copies of . Using this notation, it follows that

and

Finally, let

We can now apply Lemma 1, p. 3 of~\cite{Oli10}. Notice that from eqn. (\ref{eqn:expectyyt}) and our assumption on the spectral norm of , we immediately get that  Then, Lemma 1 of~\cite{Oli10} implies that

with probability at least . Let  be the failure probability of Theorem~\ref{thm:theorem7correct}; we seek an appropriate value of  in order to guarantee . Equivalently, we need to satisfy

Recall that , and combine eqns. (\ref{eqn:defM}) and (\ref{eqn:defPj}) to get . Combining with the above equation, it suffices to choose a value of  such that

or, equivalently,

We now use the fact that for any , if  then . Let , let , and note that  if , since , , and  are at most one. Thus, it suffices to set

which concludes the proof of the theorem.
\end{Proof}

\subsection{The proof of Lemma~\ref{lem:Tmatmult}}\label{sxn:pf_of_technical_lemma}

Let  be the sparse projection matrix constructed via Algorithm~\ref{alg:alg_projection_fast} (see Section~\ref{sxn:projection:result}), with sparsity parameter . In addition, given , let . We will derive a bound for 
Let  be the -th row of  \textit{as a row vector}, for , in which case

Rather than computing  directly, we will instead use that . We first claim that . By linearity of expectation,

We first analyze
 for some fixed  (w.l.o.g. ). Let  denote the -th element of the vector  and recall that ,  for ,
and also that .
Thus,

By combining the above with eqn.~(\ref{eqn:technical_pr_eq10}), it follows that
, and thus that .
In order to provide a bound for , note that

Eqn.~(\ref{eqn:technical_pr_eq20}) follows since the  random variables
 are independent (since the elements
of  are independent) and eqn.~(\ref{eqn:technical_pr_eq30})
follows since  is constant.
In order to bound eqn.~(\ref{eqn:technical_pr_eq30}), we first analyze
 for some  (w.l.o.g. ).
Then,

We will bound the  term directly:

Notice that if any of the four indices  appears only once,
then the expectation  corresponding
to those indices equals zero.
This expectation is non-zero if the four indices are paired in couples or
if all four are equal.
That is, non-zero expectation happens if

For case~(A), let  and let , in which case
the corresponding terms in eqn.~(\ref{eqn:star1}) become:

Similarly, cases~(B) and~(C) give:

Finally, for case~(D), let , in which case:

where we have used that .
By combining these four terms for each of the  terms in the sum, it follows
from eqns.~(\ref{eqn:technical_pr_eq30}) and~(\ref{eqn:eqX51}) that

In the above we used .
Since we assumed that , the second term on the right
hand side of eqn.~(\ref{eqn:eqX61}) is bounded by
 and the lemma follows since we have
assumed that .
\end{document}
