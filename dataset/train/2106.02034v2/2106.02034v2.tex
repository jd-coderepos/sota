\documentclass{article}





\usepackage[final]{neurips_2021}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[pagebackref=true,
      breaklinks=true,
      letterpaper=true,
      colorlinks = true,
      linkcolor = red,
      urlcolor  = blue,
      citecolor = green,
      anchorcolor = blue]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{color}
\usepackage{subcaption}
\usepackage{pifont}
\definecolor{Gray}{gray}{0.95}
\definecolor{Cyan}{rgb}{0.88,1,1}


\usepackage{tabu}           \usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}

\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand\hshline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 0.6pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\title{DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification}



\author{
Yongming Rao 
~~
Wenliang Zhao  
~~
Benlin Liu \
    \mathbf{z}^{\rm local} = \MLP(\mathbf{x})\in \mathbb{R}^{N\times C'},

    \mathbf{z}^{\rm global}=\Agg(\MLP(\mathbf{x}), \hat{\mathbf{D}})\in \mathbb{R}^{C'},

    \Agg(\mathbf{u}, \hat{\mathbf{D}}) = \frac{\sum_{i=1}^{N}\hat{\mathbf{D}}_i \mathbf{u}_i}{\sum_{i=1}^{N}\hat{\mathbf{D}}_i}, \quad \mathbf{u}\in \mathbb{R}^{N\times C'}.

    &\mathbf{z}_i = [\mathbf{z}_i^{\rm local}, \mathbf{z}_i^{\rm global}],\quad 1\le i\le N,\\
    &\bm{\pi} = \softmax(\MLP(\mathbf{z})) \in \mathbb{R}^{N\times 2},

    \hat{\mathbf{D}}\leftarrow \hat{\mathbf{D}} \odot \mathbf{D},

    \mathbf{D} = \gumbelsoftmax(\bm{\pi})_{*, 1}\in \{0, 1\}^{N},

    \mathbf{A} = \softmax\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{C}}\right)

    &\mathbf{P} = \mathbf{Q}\mathbf{K}^T/\sqrt{C} \in \mathbb{R}^{N\times N},\\
    &\mathbf{G}_{ij} = 
    \begin{cases}
    1,& i=j,\\
    \hat{\mathbf{D}}_j,& i\neq j.
    \end{cases}& 1\le i,j\le N,\label{equ:G_ij}\\
    &\tilde{\mathbf{A}}_{ij} =  \frac{\exp(\mathbf{P}_{ij})\mathbf{G}_{ij}}{\sum_{k=1}^N\exp(\mathbf{P}_{ik})\mathbf{G}_{ik}},& 1\le i,j\le N.\label{equ:attention_masking}

    \mathcal{L}_{\rm cls} = \CE(\mathbf{y}, \bar{\mathbf{y}}),

    \mathcal{L}_{\rm distill} = \frac{1}{\sum_{b=1}^B\sum_{i=1}^N \hat{\mathbf{D}}_i^{b, S}}\sum_{b=1}^B\sum_{i=1}^N \hat{\mathbf{D}}_i^{b, S} (\mathbf{t}_{i} - \mathbf{t}_i')^2,

    \mathcal{L}_{\rm KL} = \kld\left(\mathbf{y}\| \mathbf{y}'\right),

    \mathcal{L}_{\rm ratio} = \frac{1}{BS}\sum_{b=1}^{B}\sum_{s=1}^S \left(\rho^{(s)} - \frac{1}{N}\sum_{i=1}^N\hat{\mathbf{D}}^{b, s}_i\right)^2.

    \mathcal{L} = \mathcal{L}_{\rm cls} + \lambda_{\rm KL}\mathcal{L}_{\rm KL} +  \lambda_{\rm distill}\mathcal{L}_{\rm distill} + \lambda_{\rm ratio}\mathcal{L}_{\rm ratio},

\mathcal{I}^s = \argsort(\bm{\pi}_{*, 1})

be the indices sorted by the keeping probabilities , we can then keep the tokens of which the indices lie in  while discarding the others. In this way, our \dynamvit{} prunes less informative tokens dynamically at runtime, thus can reduce the computational costs during inference.


\section{Experimental Results}\label{sec:experiemnt}
\newcommand{\imnetacc}{ImageNet Acc. (\%)}
\newcommand{\throughput}{Throughput (im/s)}
\begin{table}[t]
  \centering
  \caption{\textbf{Main results on ImageNet.} We apply our method on three representative vision transformers: DeiT-S, LV-ViT-S and LV-ViT-M. DeiT-S~\cite{touvron2020deit} is a widely used vision transformer with the simple architecture. LV-ViT-S and LV-ViT-M~\cite{jiang2021token} are the state-of-the-art vision transformers. We report the top-1 classification accuracy, theoretical complexity in FLOPs and throughput for different ratio . The throughput is measured on a single NVIDIA RTX 3090 GPU with batch size fixed to 32. }
  \adjustbox{width=\textwidth}{
    \begin{tabu}to 1.16\textwidth{ll*{4}{X[l]}}\toprule
    \multirow{2}[0]{*}{Base Model} & \multicolumn{1}{c}{\multirow{2}[0]{*}{Metrics}} & \multicolumn{4}{c}{Keeping Ratio  at each stage} \\\cmidrule{3-6}
          &       & 1.0   & 0.9   & 0.8   & 0.7  \\\midrule
    \multirow{3}[0]{*}{DeiT-S~\cite{touvron2020deit}} & \imnetacc{}  & 79.8  & 79.8 {\cb(-0.0)}  & 79.6 {\cb(-0.2)}  & 79.3 {\cb(-0.5)} \\
          & GFLOPs & 4.6   & 4.0 {\cb(-14\%)}   & 3.4  {\cb(-27\%)}   & 2.9  {\cb(-37\%)}  \\
          & \throughput{} & 1337.7  & 1524.8  {\cb(+14\%)}  & 1774.6  {\cb(+33\%)}  & 2062.1  {\cb(+54\%)}  \\\midrule
    \multirow{3}[0]{*}{LV-ViT-S~\cite{jiang2021token}} & \imnetacc{}  & 83.3  & 83.3 {\cb(-0.0)}  & 83.2 {\cb(-0.1)}  & 83.0 {\cb(-0.3)}  \\
          & GFLOPs & 6.6   & 5.8 {\cb(-12\%)}    & 5.1 {\cb(-22\%)}    & 4.6 {\cb(-31\%)}   \\
          & \throughput{} & 993.3  & 1108.3 {\cb(+12\%)}  & 1255.6 {\cb(+26\%)}   & 1417.6 {\cb(+43\%)}   \\\midrule
    \multirow{3}[0]{*}{LV-ViT-M~\cite{jiang2021token}} & \imnetacc{}  & 84.0  & 83.9 {\cb(-0.1)}  & 83.9 {\cb(-0.1)}  & 83.8 {\cb(-0.2)}  \\
          & GFLOPs & 12.7  & 11.1 {\cb(-13\%)}  & 9.6 {\cb(-24\%)}    & 8.5 {\cb(-33\%)}   \\
          & \throughput{} & 589.5  & 688.5 {\cb(+17\%)}   & 791.2 {\cb(+34\%)}  & 888.2 {\cb(+50\%)}  \\\bottomrule
    \end{tabu}}
  \label{tab:main}\end{table}In this section, we will demonstrate the superiority of the proposed \dynamvit{} through extensive experiments. In all of our experiments, we fix the number of sparsification stages  and apply the target keeping ratio  as a geometric sequence  where  ranges from . During training DynamicViT models, we follow most of the training techniques used in DeiT~\cite{touvron2020deit}. We use the pre-trained vision transformer models to initialize the backbone models  and jointly train the whole model for 30 epochs. We set the learning rate of the prediction module to  and use  learning rate for the backbone model. We fix the weights of the backbone models in the first 5 epochs. All of our models are trained on a single machine with 8 GPUs. Other training setups and details can be found in the supplementary material.


\subsection{Main results} One of the most advantages of the~\dynamvit{} is that it can be applied to a wide range of vision transformer architectures to reduce the computational complexity with minor loss of performance. In Table~\ref{tab:main}, we summarize the main results on ImageNet~\cite{deng2009imagenet} where we evaluate our~\dynamvit{} used three base models (DeiT-S~\cite{touvron2020deit}, LV-ViT-S~\cite{jiang2021token} and LV-ViT-M~\cite{jiang2021token}). We report the top-1 accuracy, FLOPs, and the throughput under different keeping ratios . Note that our token sparsification is performed hierarchically in three stages, there are only  tokens left after the last stage. The throughput is measured on a single NVIDIA RTX 3090 GPU with batch size fixed to 32. We demonstrate that our \dynamvit{} can reduce the computational costs by  and accelerate the inference at runtime by , with the neglectable influence of performance ().

\subsection{Comparisons with the-state-of-the-arts}
In Table~\ref{tab:sota}, we compare the~\dynamvit{} with the state-of-the-art models in image classification, including convolutional networks and transformer-like architectures. We use the \dynamvit{} with LV-ViT~\cite{jiang2021token} as the base model and use the ``'' to indicate the keeping ratio. We observe that our \dynamvit{} exhibits favorable complexity/accuracy trade-offs at all three complexity levels. Notably, we find our DynamicViT-LV-M/0.7 beats the EfficientNet-B5~\cite{tan2019efficientnet} and NFNet-F0~\cite{brock2021nfnet}, which are two of the current state-of-the-arts CNN architectures. This can also be shown clearer in Figure~\ref{fig:sota_flops_acc}, where we plot the FLOPS-accuracy curve of \dynamvit{} series  (where we use DyViT for short), along with  other state-of-the-art models. We can also observe that \dynamvit{} can achieve better trade-offs than LV-ViT series, which strongly demonstrates the effectiveness of our method.



\begin{table}[t]
  \centering
  \caption{\textbf{Comparisons with the state-of-the-arts on ImageNet.} We compare our DynamicViT models with state-of-the-art image classifciation models with comparable FLOPs and number of parameters. We use the \dynamvit{} with LV-ViT~\cite{jiang2021token} as the base model and use the ``'' to indicate the keeping ratio. We also include the results of LV-ViT models as references. }
    \begin{tabu}to\textwidth{l*{4}{X[c]}}\toprule
    Model & Params (M) & GFLOPs & Resolution   & Top-1 Acc (\%) \\\midrule
    DeiT-S~\cite{touvron2020deit} & 22.1  & 4.6   & 224   & 79.8  \\
    PVT-Small~\cite{wang2021pvt} & 24.5  & 3.8   & 224   & 79.8  \\
    CoaT Mini~\cite{xu2021coat} & 10.0  & 6.8   & 224   & 80.8  \\
    CrossViT-S~\cite{chen2021crossvit} & 26.7  & 5.6   & 224   & 81.0  \\
    PVT-Medium~\cite{wang2021pvt} & 44.2  & 6.7   & 224   & 81.2  \\
    Swin-T~\cite{liu2021swin} & 29.0  & 4.5   & 766   & 81.3  \\
    T2T-ViT-14~\cite{yuan2021t2t} & 22.0  & 5.2   & 224   & 81.5  \\
    CPVT-Small-GAP~\cite{chu2021cpvt} & 23.0  & 4.6   & 817   & 81.5  \\
    CoaT-Lite Small~\cite{xu2021coat} & 20.0  & 4.0   & 224   & 81.9  \\\midrule
    LV-ViT-S~\cite{jiang2021token} & 26.2 & 6.6 & 224 & 83.3 \\
    DynamicViT-LV-S/0.5 & 26.9  & 3.7   & 224   & 82.0  \\
    DynamicViT-LV-S/0.7 & 26.9  & 4.6   & 224   & 83.0  \\\midrule \midrule
    RegNetY-8G~\cite{radosavovic2020designing} & 39.0  & 8.0   & 224   & 81.7  \\
    T2T-ViT-19~\cite{yuan2021t2t} & 39.2  & 8.9   & 224   & 81.9  \\
    Swin-S~\cite{liu2021swin} & 50.0  & 8.7   & 224   & 83.0  \\
    EfficientNet-B5~\cite{tan2019efficientnet} & 30.0  & 9.9   & 456   & 83.6  \\
    NFNet-F0~\cite{brock2021nfnet} & 72.0  & 12.4  & 256   & 83.6  \\\midrule
    DynamicViT-LV-M/0.7 & 57.1  & 8.5   & 224   & 83.8  \\\midrule \midrule
    ViT-Base/16~\cite{dosovitskiy2020vit} & 86.6  & 17.6  & 224   & 77.9  \\
    DeiT-Base/16~\cite{touvron2020deit} & 86.6  & 17.6  & 224   & 81.8  \\
    CrossViT-B~\cite{chen2021crossvit} & 104.7  & 21.2  & 224   & 82.2  \\
    T2T-ViT-24~\cite{yuan2021t2t} & 64.1  & 14.1  & 224   & 82.3  \\
    TNT-B~\cite{han2021transformer} & 66.0  & 14.1  & 224   & 82.8  \\
    RegNetY-16G~\cite{radosavovic2020designing} & 84.0  & 16.0  & 224   & 82.9  \\
    Swin-B~\cite{liu2021swin} & 88.0  & 15.4  & 224   & 83.3  \\\midrule
    LV-ViT-M~\cite{jiang2021token} & 55.8 & 12.7 & 224 & 84.0 \\
    DynamicViT-LV-M/0.8 & 57.1  & 9.6   & 224   & 83.9  \\\bottomrule
    \end{tabu}\label{tab:sota}\end{table}

\begin{figure}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[height=6cm]{figs/tradeoff_dyvit.pdf}
\caption{\small Model complexity (FLOPs) and top-1 accuracy trade-offs on ImageNet. We compare DynamicViT with the state-of-the-art image classification models. Our models achieve better trade-offs compared to the various vision transformers as well as carefully designed CNN models. }\label{fig:sota_flops_acc}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[height=6cm]{figs/fig_deit.pdf}
\caption{\small Comparison of our dynamic token sparsification method with model width scaling. We train our \dynamvit{} based on DeiT models with embedding dimension varying from 192 to 384 and fix ratio . We see dynamic token sparsification is more efficient than commonly used  model width scaling.}\label{fig:deit_acc}
\end{minipage}
\end{figure}


\subsection{Analysis}
\paragraph{\dynamvit{} for model scaling.} The success of EfficientNet~\cite{tan2019efficientnet} shows that we can obtain a model with better complexity/accuracy tradeoffs by scaling the model along different dimensions. While in vision transformers, the most commonly used method to scale the model is to change the number of channels, our \dynamvit{} provides another powerful tool to perform token sparsification. We analysis this nice property of \dynamvit{} in Figure~\ref{fig:deit_acc}. First, we train several DeiT~\cite{touvron2020deit} models with the embedding dimension varying from 192 (DeiT-Ti) to 384 (DeiT-S). Second, we train our \dynamvit{} based on those models with the keeping ratio . We find that after performing token sparsification, the complexity of the model is reduced to be similar to its variant with a smaller embedding dimension. Specifically, we observe that by applying our \dynamvit{} to DeiT-256, we obtain a model that has a comparable computational complexity to DeiT-Ti, but enjoys around  higher ImageNet top-1 accuracy.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/fig_viz_final.pdf}
    \caption{\textbf{Visualization of the progressively sparsified tokens.}  We show the original input image and the sparsification results after the three stages, where the masks represent the corresponding tokens are discarded. We see our method can gradually focus on the most representative regions in the image.  This phenomenon suggests that the~\dynamvit{} has better interpretability. }
    \label{fig:viz}
\end{figure}

\paragraph{Visualizations.} To further investigate the behavior of \dynamvit{}, we visualize the sparsification procedure in Figure~\ref{fig:viz}. We show the original input image and the sparsification results after the three stages, where the masks represent the corresponding tokens are discarded. We find that through the hierarchically token sparsification, our \dynamvit{} can gradually drop the uninformative tokens and finally focus on the objects in the images. This phenomenon also suggests that the~\dynamvit{} leads to  better interpretability, \ie, it can locate the important parts in the image which contribute most to the classification step-by-step.

Besides the sample-wise visualization we have shown above, we are also interested in the statistical characteristics of the sparsification decisions, \ie, what kind of general patterns does the \dynamvit{} learn from the dataset? We then use the \dynamvit{} to generate the decisions for all the images in the ImageNet validation set and compute the keep probability of each token in all three stages, as shown in Figure~\ref{fig:keep_probl}. We average pool the probability maps into  such that they can be visualized more easily. Unsurprisingly, we find the tokens in the middle of the image tend to be kept, which is reasonable because in most images the objects are located in the center. We can also find that the later stage generally has lower probabilities to be kept, mainly because that the keeping ratio at the  stage is , which decreases exponentially as  increases.


\begin{figure}[t]
\centering
\begin{minipage}{0.4\textwidth}
\captionsetup{type=table}
\caption{\small Effects of different losses. We provide the results after removing the distillation loss and the KL loss.}
\label{tab:loss}
\adjustbox{width=\linewidth}{
\begin{tabu}to 1.3\linewidth{lXX}\toprule
    Base Model&\multicolumn{1}{l}{DeiT-S}&\multicolumn{1}{l}{LVViT-S}\\\midrule
    DynamicViT&79.3\cb{(-0.5)}&83.0\cb{(-0.3)}\\
    w/o distill (Eq.13)&79.3\cb{(-0.5)}&82.7\cb{(-0.6)}\\
    w/o KL (Eq.14)&79.2\cb{(-0.6)}&82.9\cb{(-0.4)}\\
    w/o distill \& KL&79.2\cb{(-0.6)}&82.5\cb{(-0.8)}\\\bottomrule
    \end{tabu}}
\end{minipage}\hfill
\begin{minipage}{0.6\textwidth}
\centering
\includegraphics[height=2.6cm]{figs/keep_prob.pdf}
\caption{\small The keep probabilities of the tokens at each stage.}\label{fig:keep_probl}
\end{minipage}
\end{figure}



\begin{table}[t]
\caption{Comparisons of different sparsification strategies. We investigate different methods to select redundant tokens based on the DeiT-S model. We report the top-1 accuracy on ImageNet for different methods. We fix the complexity of the accelerated models to 2.9G FLOPs for fair comparisons. } \label{tab:ablation}
\centering
\begin{subtable}{.32\textwidth}
\caption{\small Dynamic sparsification \vs static/structural downsampling.}
\adjustbox{height=1cm}{
 \begin{tabu}to \linewidth{X[l]c}\toprule
Model & \multicolumn{1}{l}{Acc. (\%)}  \\\midrule
Structural & 78.2~\cb{(-1.6)} \\
Static & 73.4~\cb{(-6.4)}  \\
\rowcolor{Gray} Dynamic & 79.3~\cb{(-0.5)} \\\bottomrule
\end{tabu}}
\end{subtable}\hfill
\begin{subtable}{.32\textwidth}
\caption{\small Different redundant token removal methods.}
\adjustbox{height=1cm}{
\begin{tabu}to    \linewidth{X[l]c}
\toprule
Model &  Acc. (\%)\\ \midrule
Random & 77.5 \cb{(-2.3)} \\ 
Attention & 78.1 \cb{(-1.7)}\\
\rowcolor{Gray} Prediction & 79.3 \cb{(-0.5)} \\\bottomrule
\end{tabu}
}
\end{subtable}\hfill
\begin{subtable}{.32\textwidth}
\caption{\small Effects of number of sparsification stages.}
\adjustbox{height=1cm}{
  \begin{tabu}to \linewidth{X[l]c}\toprule
    Model  & Acc. (\%)  \\\midrule
    Single-stage & 77.4~\cb{(-2.4)} \\
    Two-stage & 79.2~\cb{(-0.6)}  \\
    \rowcolor{Gray} Three-stage & 79.3~\cb{(-0.5)} \\\bottomrule
\end{tabu}
}

\end{subtable}
\end{table}

\begin{table}[t]
\caption{Results on larger models. We apply our method to the model with larger width (\ie, DeiT-B) and the model with larger input size (\ie, DeiT-S with  input). } \label{tab:large}
\centering
\subfloat[\small Results on DeiT-B.]
{\makebox[0.48\linewidth][c]{
\tablestyle{12pt}{1.2}
\adjustbox{width=0.48\linewidth}{
\setlength{\tabcolsep}{7pt}
\begin{tabular}{lll}
    \toprule
        Model & GFLOPs & Acc. (\%) \\ \midrule
        DeiT-B & 17.5 & 81.8 \\
        DynamicViT-B/0.7 & 11.2 \cb{(-36\%)} & 81.3 \cb{(-0.5)} \\ \bottomrule
    \end{tabular}
\label{Tab:ablation:a}}
} 
}
\hfill
\subfloat[\small Results on the  input.]
{\makebox[0.48\linewidth][c]{
\tablestyle{12pt}{1.2}
\adjustbox{width=0.48\linewidth}{
\setlength{\tabcolsep}{7pt}
\begin{tabular}{lll}
    \toprule
        Model & GFLOPs & Acc. (\%) \\ \midrule
        DeiT-S & 15.5 & 81.6 \\ 
        DynamicViT-S/0.7 & 9.5 \cb{(-39\%)} & 81.4 \cb{(-0.2)} \\
        DynamicViT-S/0.5 & 7.0 \cb{(-55\%)} & 80.3 \cb{(-1.3)} \\ \bottomrule
    \end{tabular}
}
}
}
\vspace{-5pt}
\end{table}


\paragraph{Effects of different losses.} We show the effects of different losses in Table~\ref{tab:loss}.  We see the improvement brought by the distillation loss and the KL loss is not very significant, but it can consistently further boost the performance of various models.

\paragraph{Comparisons of different sparsification strategies.} As illustrated in Figure~\ref{fig:overall}, the dynamic token sparsification is unstructured. To discuss whether the dynamic sparsification is better than other strategies, we perform ablation experiments and the results are shown in Table~\ref{tab:ablation}. For the structural downsampling, we perform an average pooling with kernel size  after the sixth block of the baseline DeiT-S~\cite{touvron2020deit} model, which has similar FLOPs to our DynamicViT. The static token sparsification means that the sparsification decisions are not conditioned on the input tokens. We also compare our method with other token removal methods like randomly removing tokens or removing tokens based the attention score of the class token. We find through the experiments that although other strategies have similar computational complexities, the proposed dynamic token sparsification method achieves the best accuracy. We also show that the progressive sparsification method is significantly better than one-stage sparsification.

\paragraph{Accelerating larger models.} To show the effectiveness of our method on larger models, we apply our method to the model with larger width (\ie, DeiT-B) and models with larger input size (\ie, DeiT-S with  input). The results are presented in Table~\ref{tab:large}. We see our method also works well on the larger DeiT model. The accuracy drop become less significant when we apply our method to the model with larger feature maps. Notably, we can reduce the complexity of the DeiT-S model with  input by over 50\% with only 1.3\% accuracy drop.

\section{Conclusion}
\label{sec:con}
In this work, we open a new path to accelerate vision transformer by exploiting the sparsity of informative patches in the input image. For each input instance, our DynamicViT model prunes the tokens of less importance in a dynamic way according to the customized binary decision mask output from the lightweight prediction module, which fuses the local and global information containing in the tokens. The prediction module is added to multiple layers such that the token pruning is performed in a hierarchical way. Gumbel-Softmax and attention masking techniques are also incorporated for the end-to-end training of the transformer model together with the prediction module. During the inference phase, our approach can greatly improves the efficiency by gradually pruning 66\% of the input tokens, while the drop of accuracy is less than 0.5\% for different transformer backbone. In this paper, we focus on the image classification task. Extending our method to other scenarios like video classification and dense prediction tasks can be interesting directions.

\subsection*{Acknowledgment}

This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 62125603, Grant 61822603, Grant U1813218, Grant U1713214, in part by Beijing Academy of Artificial Intelligence (BAAI), in part by National Science Foundation  under grant IIS-1901527, IIS-2008173, IIS-2048280, 
and in part by a grant from the Institute for Guo Qiang, Tsinghua University.

\bibliographystyle{plain}
\bibliography{ref}

\appendix

\section{Implementation Details}\label{sec:details}

We conduct our experiments on the ImageNet (also known as ILSVRC2012)~\cite{deng2009imagenet} dataset. ImageNet is a commonly used benchmark for image classification. We train our models on the training set, which consists of 1.28M images. The top-1 accuracy is measured on the 50k validation images following common practice~\cite{he2016deep,touvron2020deit}. To fairly compare with previous methods, we report the single crop results. 

We fix the number of sparsification stages  in all of our experiments, since this setting can lead to a decent trade-off between complexity and performance. For the sake of simplicity, we set the target keeping ratio  as a geometric sequence , where  is the keeping ratio after each sparsifcation ranging from . For the prediction module, we use the identical architecture for different stages. We  use two \texttt{LayerNorm}  \texttt{Linear}(, )  \texttt{GELU} block to produce  and   respectively. We employ a \texttt{Linear}(, )  \texttt{GELU}  \texttt{Linear}(, )  \texttt{GELU}  \texttt{Linear}(, )  \texttt{Softmax} block to predict the probabilities.

During training our DynamicViT models, we follow most of the training techniques used in DeiT~\cite{touvron2020deit}. We use the pre-trained vision transformer models to initialize the backbone models and jointly train the backbone model as well as the prediction modules for 30 epochs. We set the learning rate of the prediction module to  and use   learning rate for the backbone model. The batch size is adjusted adaptively for different models according to the GPU memory.  We fix the weights of the backbone models in the first 5 epochs. All of our models can be trained on a single machine with 8 NVIDIA GTX 1080Ti GPUs.



\section{More Analysis}



In this section, we provide more analysis of our method. We investigate the effects of progressive sparsification, distillation loss, ratio loss, and keeping ratio. We also include more visualization results. The following describes the details of the experiments, results and analysis. 

\paragraph{Progressive sparsification. } To verify the effectiveness of the progressive sparsification strategy, we test different sparsification methods that result in similar overall complexity. Here we provide more detailed results and more analysis. We find that progressive sparsification is much better than single-shot sparsification. Increasing the number of stages will lead to better performance. Since further increasing the number of stages () will not lead to significantly better performance but add computation, we use a 3-stage progressive sparsification strategy in our main experiments. 

\begin{table}[!h]
    \centering \small
    \newcolumntype{g}{>{\columncolor{Gray}}l}
   \begin{tabular}{gll}\toprule
     & \multicolumn{1}{l}{Top-1 accuracy (\%)} & \multicolumn{1}{l}{GFLOPs} \\\midrule
    DeiT-S~\cite{touvron2020deit} & 79.8  & 4.6 \\\midrule
     (single-stage) & 77.4\cb{(-2.4)} & 2.9\cb{(-37\%)} \\
     (two-stage) & 79.2\cb{(-0.6)}  & 2.9\cb{(-37\%)} \\
     (three-stage) & 79.3\cb{(-0.5)}  & 2.9\cb{(-37\%)} \\\bottomrule
\end{tabular}
\end{table}

\paragraph{Ablation on the distillation loss and ratio loss. } The weights of the distillation losses and ratio loss are the key hyper-parameters in our method.  Since the token-wise distillation loss and the KL divergence loss play similar roles in our method, we set  in all of our experiments for the sake of simplicity. In this experiment, we fix the keeping ratio  to be 0.7.  We find our method is not sensitive to these hyper-parameters in general. The proposed ratio loss can encourage the model to reach the desired acceleration rate.  Distillation losses can improve the performance after sparsification.  We directly apply the best hyper-parameters searched on DeiT-S for all models.


\begin{figure}[!h]
\centering
    \newcolumntype{g}{>{\columncolor{Gray}}l}
\begin{minipage}{0.48\textwidth} \centering
\captionsetup{type=table}
\small
\begin{tabular}{gl}\toprule
     & \multicolumn{1}{l}{Top-1 accuracy (\%)} \\\midrule
    DeiT-S~\cite{touvron2020deit} & 79.8  \\\midrule
     & 79.17\cb{(-0.63)}  \\
     & 79.32\cb{(-0.48)} \\
     & 79.23\cb{(-0.57)} \\\bottomrule
\end{tabular}

\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\centering
\small
\begin{tabular}{gl}\toprule
     & \multicolumn{1}{l}{Top-1 accuracy (\%)} \\\midrule
    DeiT-S~\cite{touvron2020deit} & 79.8  \\\midrule
     & 79.15\cb{(-0.65)}  \\
     & 79.32\cb{(-0.48)} \\
      & 79.29\cb{(-0.51)} \\\bottomrule
\end{tabular}
\end{minipage}
\end{figure}

\paragraph{Smaller keeping ratio. } We have also tried applying a smaller keeping ratio (larger acceleration rate). The results based on DeiT-S~\cite{touvron2020deit} and LV-ViT-S~\cite{jiang2021token} models are presented in the following tables. We see that using  will lead to a significant accuracy drop while reducing fewer FLOPs. Since only 22\% and 13\% tokens are remaining in the last stage when we set  to 0.6 and 0.5 respectively, small  may cause a significant information loss. Therefore, we use  in our main experiments. Jointly scaling   and the model width can be a better solution to achieve a large acceleration rate as shown in Figure 4 in the paper.

\begin{figure}[!h]
\centering
    \newcolumntype{g}{>{\columncolor{Gray}}l}
\begin{minipage}{0.48\textwidth} \centering
\captionsetup{type=table}
\small
 \begin{tabular}{gll}\toprule
     & \multicolumn{1}{l}{Top-1 acc. (\%)} & \multicolumn{1}{l}{GFLOPs} \\\midrule
    DeiT-S~\cite{touvron2020deit} & 79.8  & 4.6 \\\midrule
      & 79.8\cb{(-0.0)} & 4.0\cb{(-14\%)} \\
     & 79.6\cb{(-0.3)}  & 3.4\cb{(-27\%)} \\
       & 79.3\cb{(-0.5)} & 2.9\cb{(-37\%)} \\
     & 78.5\cb{(-1.3)}  & 2.5\cb{(-46\%)} \\
     & 77.5\cb{(-2.3)}  & 2.2\cb{(-52\%)} \\\bottomrule
\end{tabular}

\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\centering
\small
 \begin{tabular}{gll}\toprule
     & \multicolumn{1}{l}{Top-1 acc. (\%)} & \multicolumn{1}{l}{GFLOPs} \\\midrule
    LV-ViT-S~\cite{jiang2021token} & 83.3  & 6.6 \\\midrule
      & 83.3\cb{(-0.0)} & 5.8\cb{(-12\%)} \\
     & 83.2\cb{(-0.1)}  & 5.1\cb{(-22\%)} \\
       & 83.0\cb{(-0.3)} & 4.6\cb{(-31\%)} \\
     & 82.6\cb{(-0.7)}  & 4.1\cb{(-38\%)} \\
     & 82.0\cb{(-1.3)}  & 3.7\cb{(-44\%)} \\\bottomrule
\end{tabular}
\end{minipage}
\end{figure}

\paragraph{More visual results. } We provide more visual results in Figure~\ref{fig:suppvis}. The input images are randomly sampled from the validation set of ImageNet. We see our method works well for different images from various categories.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/big_viz.pdf}
    \caption{\textbf{More visual results}. The input images are randomly sampled from the validation set of ImageNet. We see our method works well for different images from various categories.}
    \label{fig:suppvis}
\end{figure}


\end{document}
