\section{Experiments}
\begin{figure*}[th]
\centering
\includegraphics[width=1\textwidth]{Dataset.pdf}
\vspace{-2mm}
\caption{Examples of nominal and anomalous frames in the UCSD Ped2, CUHK Avenue and ShanghaiTech datasets. Anomalous events are shown with red box.}
\label{f:data}
\vspace{-2mm}
\end{figure*}


\label{s:experiments}

\subsection{Datasets}

We first evaluate our proposed method on three publicly available benchmark video anomaly data sets, namely the CUHK avenue dataset \cite{lu2013abnormal}, the UCSD pedestrian dataset \cite{mahadevan2010anomaly}, and the ShanghaiTech campus dataset \cite{luo2017revisit}. Their training data consists of nominal events only. We present some examples of nominal and anomalous frames in Figure \ref{f:data}. 

\textbf{UCSD Ped2:} The UCSD pedestrian data consists of 16 training and 12 test videos, each with a resolution of 240 x 360. All the anomalous events are caused due to vehicles such as bicycles, skateboarders and wheelchairs crossing pedestrian areas. 

\textbf{Avenue:} The CUHK avenue dataset contains 16 training and 21 test videos with a frame resolution of 360 x 640. The anomalous behaviour is represented by people throwing objects, loitering and running. 

\textbf{ShanghaiTech:} The ShanghaiTech Campus dataset is one of the largest and most challenging datasets available for anomaly detection in videos. It consists of 330 training and 107 test videos from 13 different scenes, which sets it apart from the other available datasets. The resolution for each video frame is 480 x 856. 

\subsection{Benchmark Algorithms}

In the context of video anomaly detection, to the best of our knowledge, there is no benchmark algorithm designed for continual learning. Hence, in Table \ref{tab:my-table}, we compare our proposed algorithm with the state-of-the-art deep learning-based methods, as well as methods based on hand-crafted features: MPPCA \cite{kim2009observe}, MPPC + SFA \cite{mahadevan2010anomaly}, Del et al. \cite{del2016discriminative}, Conv-AE \cite{hasan2016learning}, ConvLSTM-AE \cite{luo2017remembering}, Growing Gas \cite{sun2017online}, Stacked RNN \cite{luo2017revisit}, Deep Generic \cite{hinami2017joint}, GANs \cite{ravanbakhsh2018plug}, Liu et al. \cite{liu2018future}, Sultani et al. \cite{sultani2018real}. A popular metric used for comparison in the anomaly detection literature is the Area under the Curve (AuC) curve. Higher AuC values indicate better performance for an anomaly detection system. Following the existing works \cite{cong2011sparse,ionescu2019object,liu2018future}, we use the commonly used frame-level AuC metric for performance evaluation. 

\subsection{Impact of Sequential Anomaly Detection}

To demonstrate the importance of sequential anomaly detection in videos, we implement a nonsequential version of our algorithm by applying a threshold to the instantaneous anomaly evidence , given in \eqref{eq:evidence}, which is similar to the approach employed by many recent works \cite{liu2018future,sultani2018real,ionescu2019object}. As Figure \ref{f:sequential} shows, instantaneous anomaly evidence is more prone to false alarms than the sequential statistic of the proposed framework since it only considers the noisy evidence available at the current time to decide. Whereas, the proposed sequential statistic handles noisy evidence by integrating recent evidence over time. 

\begin{figure}[th]
\centering
\includegraphics[width=0.5\textwidth]{sequential.png}
\vspace{-2mm}
\caption{The advantage of sequential anomaly detection over single-shot detection in terms of controlling false alarms.}
\label{f:sequential}
\vspace{-2mm}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=0.5\textwidth]{hos.pdf}
\vspace{-2mm}
\caption{Optical flow statistics for the motion-based anomaly in the first test video of the UCSD Dataset.}
\label{f:hos}
\vspace{-2mm}
\end{figure}
\subsection{Impact of Optical Flow}
In Figure \ref{f:hos}, we present the optical flow statistics for the first test video of the UCSD dataset. Here, the anomaly pertains to a person using a bike on a pedestrian path, which is previously unseen in the training data. It is clearly visible that there is a significant shift in the optical flow statistics, especially in skewness and kurtosis. This is due to the higher speed of a bike as compared to a person walking. Also, this shows the efficacy of optical flow in detecting motion-based anomalies.     

\subsection{Results}
\textbf{Benchmark Results:}
To show the general performance of the proposed algorithm, not necessarily with continual learning, we compare our results to a wide range of methods in Table \ref{tab:my-table} in terms of the commonly used frame-level AuC metric. Recently, \cite{ionescu2019object} showed significant gains over the rest of the methods. However, their methodology of computing the AuC gives them an unfair advantage as they calculate the AuC for each video in a dataset, and then average them as the AuC of the dataset, as opposed to the other works which concatenate all the videos first and then determine the AuC as the datasets score.

\begin{table}[]
\centering
\resizebox{0.5\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|}
\hline
Methodology    & CUHK Avenue & UCSD Ped 2 & ShanghaiTech  \\ \hline
Conv-AE \cite{hasan2016learning}        & 80.0        & 85.0       & 60.9\\ \hline
ConvLSTM-AE\cite{luo2017remembering}    & 77.0        & 88.1       & -   \\ \hline
Stacked RNN\cite{luo2017revisit}    & 81.7        & 92.2       & 68.0\\ \hline
GANs \cite{ravanbakhsh2018plug}           & -           & 88.4       & -   \\ \hline
Liu et al. \cite{liu2018future}     & 85.1        & 95.4       & 72.8\\ \hline
Sultani et al. \cite{sultani2018real} & -           & -          & 71.5\\ \hline
Ours           & 86.4        & 97.8       & 71.62     \\ \hline
\end{tabular}}
\vspace{2mm}
\caption{AuC result comparison on three datasets.}
\label{tab:my-table}
\end{table}

As shown in Table \ref{tab:my-table}, we are able to outperform the existing results in the CUHK Avenue and UCSD datasets, and achieve competitive performance in the ShanghaiTech dataset. We should note here that our reported result in the ShanghaiTech dataset is based on online decision making without seeing future video frames.
A common technique used by several recent works such as \cite{liu2018future,ionescu2019object} is to normalize the computed statistic for each test video independently using the future frames. However, this methodology cannot be implemented in an online (real-time) system as it requires prior knowledge about the minimum and maximum values the statistic might take. 

\textbf{Continual Learning Results:}
Due to the lack of existing benchmark datasets for continual learning in surveillance videos, we first slightly modify the original UCSD dataset, where a person riding a bike is considered as anomalous, and assume that it is considered as a nominal behavior. Our goal here is to compare the continual learning capability for video surveillance of the proposed and state-of-the-art algorithms and see how well they adapt to new patterns. Initially, the proposed algorithm raises an alarm when it detects a bike in the testing data. Using the human supervision approach proposed in Section \ref{s:proposed}, the relevant frames are labelled as nominal and added to the training set. In Figure \ref{f:continual}, it is seen that the proposed algorithm clearly outperforms the state-of-the-art algorithms \cite{ionescu2019object,liu2018future} in terms of continual learning performance. More importantly, as shown in Table \ref{tab:time-table}, it achieves this superior performance by quickly updating its training with the new samples in a few seconds while the state-of-the-art algorithms need to retrain on the entire dataset for several hours to prevent catastrophic forgetting. Furthermore, it is important to note that the proposed algorithm is able to achieve a relatively high AuC score using only a few samples, demonstrating its few-shot learning ability. 


\begin{table}[h]
\tiny{
\centering
\resizebox{0.5\textwidth}{!}{\begin{tabular}{|l|l|l|l|}
\hline

            & Ours                        & Liu et al. \cite{liu2018future}                  & Ionescu et al. \cite{ionescu2019object}              \\ \hline
Update Time & \multicolumn{1}{c|}{10 sec} & \multicolumn{1}{c|}{4.8 hrs} & \multicolumn{1}{c|}{2.5 hrs} \\ \hline
\end{tabular}}
\vspace{2mm}
\caption{Time required to update the model for each batch of new samples.}
\label{tab:time-table}
}
\end{table}

\begin{figure}[th]
\centering
\includegraphics[width=0.5\textwidth]{comparison.pdf}
\vspace{-2mm}
\caption{Comparison of the proposed and the state-of-the-art algorithms Liu et al. \cite{liu2018future} and Ionescu et al. \cite{ionescu2019object} in terms of continual learning capability. The proposed algorithm is able to quickly train with new samples and significantly outperform both of the methods.}
\label{f:continual}
\vspace{-2mm}
\end{figure}

\textbf{Real-Time Surveillance Results:}
\begin{figure*}[th]
\centering
\includegraphics[width=1\textwidth]{new_data.pdf}
\vspace{-2mm}
\caption{The visualization of different causes for false alarm in the surveillance feed dataset. In the first case, the person stands in the middle of the street, which causes an alarm as this behavior was previously unseen in the training data. Similarly, in the second case, a change in the weather causes the street sign to move. In the third case, the appearance of multiple cars at the same time causes a shift in the distribution of the optical flow. Finally, in the fourth case a bike is detected, which was not previously seen in the training data.}
\vspace{-2mm}
\end{figure*}
Even though existing datasets such as ShanghaiTech, CUHK Avenue, and UCSD provide a good baseline for comparing video surveillance frameworks, they lack some critical aspects. Firstly, they have an underlying assumption that all nominal events/behaviors are covered by the training data, which might not be the case in a realistic implementation. Secondly, there is an absence of temporal continuity in the test videos, i.e., most videos are only a few minutes long and there is no specific temporal relation between different test videos. Moreover, external factors such as brightness and weather conditions that affect the quality of the images are also absent in the available datasets. Hence, we also evaluate our proposed algorithm on a publicly available CCTV surveillance feed\footnote{The entire surveillance feed is available here: https://www.youtube.com/watch?v=Xyj-7WrEhQw\&t=3460s}. The entire feed is of 8 hours and 23 minutes and continuously monitors a street. To make the problem more challenging we initially train only on 10 minutes of data and then continually update our model as more instances become available. 



\begin{figure}[th]
\centering
\includegraphics[width=0.5\textwidth]{cctv3.pdf}
\vspace{-2mm}
\caption{Continual learning ability of the proposed algorithm. Assuming an arbitrary constant threshold, we observe that the algorithm is able to quickly learn new nominal behaviors, and thus reduce the false alarm rate as compared to the same algorithm which does not continually update the learned model.}
\label{f:cctv}
\vspace{-2mm}
\end{figure}


In Figure \ref{f:cctv}, we demonstrate the continual learning performance of the proposed algorithm through reduced number of false alarms after receiving some new nominal labels.
It should be noted that our goal here is to emphasize the continual learning ability of our algorithm, rather than showing the general detection performance. Each
segment here corresponds to 20,000 frames. After each segment, a human roughly labels the false positive events. In this case, to reduce the computational complexity and examine the few-shot learning ability of the proposed algorithm, we only consider 20\% of all false positive events for updating the model. Although the number of frames might seem a lot, it roughly translates to 10 seconds of streaming video data, so it can still be considered as few-shot learning in video analysis. We observe that even with relatively small updates, the false alarm rate is significantly lower as compared to the same algorithm where we do not update the model continuously. This proves that the proposed algorithm is able to learn meaningful information from recent data using only few samples, and is able to incrementally update the model without accessing the previous training data.   

\section{Conclusion and Future Work}
\label{s:conclusion}

For video anomaly detection, we presented an continual learning algorithm which consists of a transfer learning-based feature extraction module and a statistical decision making module. The first module efficiently minimizes the training complexity and extracts motion, location, and appearance features. The second module is a sequential anomaly detector which is able to incrementally update the learned model within seconds using newly available nominal labels. Through experiments on publicly available data, we showed that the proposed detector significantly outperforms the state-of-the-art algorithms in terms of any-shot learning of new nominal patterns. The continual learning capacity of the proposed algorithm is illustrated on a real-time surveillance stream, as well as a popular benchmark dataset. 

The ability to continually learn and adapt to new scenarios would significantly improve the current video surveillance capabilities. In future, we aim to evolve our framework to work well in more challenging scenarios such as dynamic weather conditions, rotating security cameras and complex temporal relationships. Furthermore, we plan to extend the proposed continual learning framework to new anomalous labels, and other video processing tasks such as online object and action recognition. 
