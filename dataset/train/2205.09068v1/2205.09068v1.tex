\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \usepackage{amsmath}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amssymb}
\def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/Title (VRAG: Region Attention Graphs for Content-Based Video Retrieval)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}



\setcounter{secnumdepth}{0} 





\iffalse
\title{VRAG: Region Attention Graphs for Content-Based Video Retrieval}
\author{Kennard Ng, Ser-Nam Lim, Gim Hee Lee}
\fi
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi


\title{VRAG: Region Attention Graphs for Content-Based Video Retrieval}
\author {
Kennard Ng,\textsuperscript{\rm 1}
    Ser-Nam Lim, \textsuperscript{\rm 2}
    Gim Hee Lee \textsuperscript{\rm 1}
}
\affiliations {
\textsuperscript{\rm 1} Department of Computer Science, National University of Singapore \\
    \textsuperscript{\rm 2} Facebook AI\\
    kennard.ng.pool.hua@gmail.com, sernam@gmail.com, gimhee.lee@nus.edu.sg
}



\usepackage{bibentry}


\begin{document}

\maketitle

\begin{abstract}
Content-based Video Retrieval (CBVR) is used on media-sharing platforms for applications such as video recommendation and filtering. To manage databases that scale to billions of videos, video-level approaches that use fixed-size embeddings are preferred due to their efficiency.  In this paper, we introduce Video Region Attention Graph Networks (VRAG) that improves the state-of-the-art of video-level methods. We represent videos at a finer granularity via region-level features and encode video spatio-temporal dynamics through region-level relations. Our VRAG captures the relationships between regions based on their semantic content via self-attention and the permutation invariant aggregation of Graph Convolution. 
In addition, we show that the performance gap between 
video-level and frame-level methods can be reduced by segmenting videos into shots and using shot embeddings for video retrieval. We evaluate our VRAG over several video retrieval tasks and achieve a new state-of-the-art for video-level retrieval. Furthermore, our shot-level VRAG shows higher retrieval precision than other existing video-level methods, and closer performance to frame-level methods at faster evaluation speeds. Finally, our code will be made publicly available.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{images/new-teaser-v1.PNG}
    \caption{Our VRAG encodes video-level embeddings from spatial intra-frame and temporal inter-frame information for CBVR.} \vspace{-3mm}
    \label{fig:teaser}
\end{figure}

The volume of videos on the Internet has grown exponentially with the inception of media-sharing websites such as Facebook, Twitch and Youtube. 
Content-based Video Retrieval (CBVR) is important on these platforms for applications such as video recommendation and video filtering.

In CBVR, evaluating the video similarity is a key component. There are predominantly two types of approaches for inferring video-to-video similarity: frame-level approaches and video-level approaches. Video-level methods encode videos into fixed size embeddings, and measure video similarity through the similarity of their embeddings. On the other hand, frame-level methods derive video similarity by aggregating pairwise similarities between video frames. These two approaches form a dichotomy. Video-level approaches offer faster evaluation speeds, while frame-level methods provide more accurate video similarities at significant overheads. Although practical implementations of frame-level methods may employ optimizations such as summarizing the video into a shorter sequence of frames~\cite{vid-summary:1708-09545, vid-summary:1812-01969, vid-summary:zhou2018deep}, video-level methods remain orders of magnitude more efficient than frame-level methods when the number of videos scale to the size of billions. Consequently, video-level methods remain highly relevant for real-world applications despite its inferior performance to frame-level methods.

Recently, several video-level CBVR works~\cite{lbow, Baraldi2018LAMVLT, kordopatis2017dml, baseline:tmk} propose the same design principle of extracting local frame-level features, and then aggregating these features into fixed-size video embeddings. As a result, most of these works focus on proposing a better frame-level feature extractor and/or video-level feature pooling methods. For example, DML~\cite{kordopatis2017dml} and LBoW~\cite{lbow} extract Maximum Activation of Convolution (MAC) features from each frame and then apply mean pooling and bag-of-words, respectively, to get video-level representations. LAMV~\cite{Baraldi2018LAMVLT} later propose representing frames at a finer granularity using Region Maximum Activation of Convolutions (R-MAC) and learnable pooling of these frame descriptors in the Fourier Domain~\cite{baseline:tmk}. While prior works have made strides in frame-level feature extraction and video-level pooling, these approaches encode each frame separately, and do not model the spatial and temporal interactions inherent within videos. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/shot-level-video-retrieval-smaller.PNG}
    \caption{Our VRAG in a shot-level video retrieval setting.} \vspace{-3mm}
    \label{fig:shot-retrieval-diagram}
\end{figure}

In view that prior video-level approaches lack modelling of spatio-temporal interactions within videos, we propose VRAG: a region attention graph-based framework for content-based video retrieval. As shown in Figure~\ref{fig:teaser}, our VRAG models videos at the fine-grained region-level as a graph. Each node of the graph represents a R-MAC feature vector. To model interactions between region nodes, we encode spatial relations through complete subgraphs between regions in the same frame and temporal relations through fully connected edges across adjacent frames. We further augment these spatial and temporal connections through self-attention, which modulates the strength of these associations through the affinities between their region-level content. Consequently, we transform each region into context-aware embeddings by selectively aggregating features from neighboring regions via Graph Attention~\cite{graph-attention} layers. To generate video-level embeddings, we learn attention weights for each region and selectively aggregate region-level embeddings into a fixed size representation. We model our attention pooling on the intuition that important video regions add significant context to other regions and derive the attention weights from the pairwise affinities between regions across multiple Graph Attention layers. 

Our VRAG improves the state-of-the-art for video-level approaches across video retrieval tasks~\cite{dataset:fivr200k, dataset:evve, dataset:cc-web-video}, and reduces the performance gap between video-level and frame-level retrieval approaches. On Event Video Retrieval~\cite{dataset:evve}, our VRAG also achieves higher retrieval scores over the state-of-the-art for frame-level retrieval, ViSiL~\cite{kordopatiszilos2019visil}, while being more than 50 faster. We further propose to reduce the gap between video-level and frame-level approaches by segmenting videos into shots and representing videos over multiple shot embeddings using our VRAG, as illustrated in Figure~\ref{fig:shot-retrieval-diagram}. Our shot-level VRAG evaluates the similarity between videos by aggregating pairwise similarities between their shot embeddings. In our experiments, we show that our shot-level VRAG bridges the gap between video-level and frame-level approaches on most video retrieval tasks~\cite{dataset:fivr200k, dataset:cc-web-video}
at faster evaluation speeds than frame-level methods, and higher retrieval precision over video-level approaches. 

\section{Related Work}

\subsection{Video Retrieval}

\textbf{Multi-modal video retrieval} performs video retrieval through multi-modal embeddings. Recent methods use pretrained expert networks to extract video representations across different modalities, e.g. optical flow, audio, and introduce novel techniques to fuse these representations into a multi-modal embedding~\cite{collaborative-experts, moEE, mm-transformer}. \cite{collaborative-experts} models pairwise interactions between modalities and aggregates across modalities via attention pooling while \cite{mm-transformer} use Transformers~\cite{attention-is-all-you-need} to fuse features from different modalities.

\textbf{Video-to-text retrieval} is a popular instance of cross-modal video retrieval, where relevant videos are retrieved given a query sentence and vice versa. The video-to-text pipeline extracts video and text representations and map related video-text pairs into the same representation~\cite{collaborative-experts, howto100m}.

\subsection{Content-based Video Retrieval (CBVR)}
CBVR methods can be broadly classified into video-level~\cite{baseline:tmk, Baraldi2018LAMVLT, kordopatis2017dml, lbow} and frame-level~\cite{temporal_network, baseline:dp, kordopatiszilos2019visil} approaches.

\textbf{Video-level approaches} encode videos into fixed size embeddings, i.e.  vectors. These methods extract features from video frames and temporally pool frame features into a fixed size representation. The similarity between videos is then determined through the similarities between their embeddings e.g. cosine similarity. Compared to prior video-level~\cite{kordopatis2017dml, lbow, baseline:tmk, Baraldi2018LAMVLT} methods that encode video frames independently, our VRAG models spatio-temporal relations between/within frames through Graph Convolution, and adds video-level context into the frame representations.

\textbf{Frame-level approaches}~\cite{temporal_network, baseline:dp} represent videos as sequences of video frame features that scale with the video length. To evaluate video similarity, these methods aggregate pairwise similarities between video frames to derive the similarity between videos. Recently, ViSiL~\cite{kordopatiszilos2019visil} propose using Convolutional Neural Networks (CNN) layers and the (Symmetric) Chamfer Similarity to aggregate pairwise frame similarities, and achieves state-of-the-art frame-level CBVR. However, ViSiL requires  forward passes to evaluate pairwise similarities between  videos, while our VRAG is orders of magnitude faster since it uses only  forward passes to generate video embeddings.

\section{Our Approach}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{images/wider-fig.PNG}
    \caption{Our Video Region Attention Graph Network (VRAG). (Left) Graph structure. (Right) VRAG Network.}
    \label{fig:VRAG-architecture}
\end{figure*}

We present our Video Region Attention Graph (VRAG) network in Figure~\ref{fig:VRAG-architecture}. 

\textbf{\label{sect:frame-repr}Input Frame Representation.} Following ViSiL, we represent each video frame using concatenated Region Maximum Activation of Convolution~\cite{rmac} (R-MAC) features from intermediate convolution layers. We pass each video frame through a pretrained ConvNet, i.e. ResNet50~\cite{resnet} pre-trained on ImageNet, with  layers to generate intermediate activation maps, . Given that each intermediate activation map  has varying spatial dimensions, we transform the activation maps into the same spatial dimension by defining different R-MAC kernel sizes for each layer. Subsequently, we concatenate the intermediate R-MAC features channel-wise to represent each video frame, i.e. , where  refers to the number of R-MAC features in each frame, and 
 is the dimensions of the concatenated vector. We then encode a video of length  into  R-MAC features, i.e. .

\textbf{Spatial and Temporal Graph Structure.}
We represent each R-MAC feature vector  as a node  in the graph  of our VRAG.
We then add two sets graph edges: 1)  is a set of edges that connects region nodes in the same frame  to capture spatial relationships between these nodes with a frame and this includes a self referencing edge. 2)  refers to edges that connect the regions between consecutive frames  and  to capture temporal relations between video frames. 

\textbf{Learning Region Embeddings.} From the video region graph  constructed earlier, we learn region-level embeddings using Graph Convolutional Network~\cite{gcn, graph-attention} (GCN) layers. To increase the duration of videos that can be processed, we first reduce the dimensions of R-MAC region vectors, i.e.  using a fully-connected layer with non-linear activation, i.e. .
We then pass the region vectors through  Graph Attention~\cite{graph-attention} layers to generate intermediate region features, i.e.  for . Specifically, we aggregate the features from neighboring regions as follows: 

\paragraph{1)} To enable interactions between neighboring regions with complementary representations, we use the key-query self-attention mechanism~\cite{attention-is-all-you-need} to describe the similarity between regions. Specifically, we generate key and query embeddings using linear transformations from the input region vectors , i.e.:


\vspace{-2mm}
\paragraph{2)} The similarity between a region  and its neighboring region  is defined using the dot product of their query and key embeddings, i.e. . The output of the Graph Attention layer  is derived as:

where  is a linear layer with a non-linear activation.

\vspace{-2mm}
\paragraph{3)} Finally, given that adding graph convolution layers may lead to over-smoothing of region representations~\cite{gcn:over-smoothing1, chen2019measuring}, we propose concatenating region representations along the depth of our network to preserve discriminative features, i.e. .

\textbf{Video Embedding.} We aggregate region embeddings  using weighted sum aggregation to generate a pooled region representation:
,
where  is the attention weight for region . The pooled representation  is fed into two linear layers to generate the video-level embedding , i.e.: . We model our attention weights  from the intuition that important regions in a video act as anchor regions that add context to other regions, and should have high affinities with other regions in the video. Therefore, we compute the attention weights  as follows:
\vspace{-2mm}
\paragraph{1)} We reuse the key and query embeddings of each Graph Attention layer  from Equation~\ref{eqn:kq-embeddings} to compute  pairwise affinity matrices  between regions, i.e.

The row  returns the pairwise affinity between region  and every other region  at Graph Attention layer . For each region , we average the affinities in  and concatenate the averaged affinities over  Graph Attention layers to obtain the affinity vector . 

\vspace{-2mm}
\paragraph{2)} We feed  into a single linear layer to derive the unnormalized attention weights i.e.  and normalize the region attention weights 
via softmax, i.e.:


\subsection{Training VRAG}
We train VRAG using the triplet margin loss~\cite{triplet-margin-loss}. From a triplet of video-level embeddings  that correspond to the anchor, positive and negative video respectively, we compute the loss:

where  computes the cosine similarity between embeddings,  refers to the margin. During training, we sample triplets using the triplet mining scheme consistent with ~\cite{kordopatiszilos2019visil}.

\section{Shot Representations}

In this section, we introduce an intermediate approach utilizing shot representations for video retrieval. We use the shot-boundary algorithm in Section~\ref{sect:sba} to divide a video  into non-overlapping shots and use VRAG to encode each shot into embeddings. Using shots can reduce noise within VRAG embeddings by removing spurious edges across shot boundaries.
In our shot-level approach, we evaluate video similarity by aggregating the pairwise similarities between shot-level embeddings.

\textbf{\label{sect:sba}Shot Boundary Algorithm.} In our shot boundary algorithm, we identify the boundaries between video shots by comparing the similarities between consecutive frames. Specifically, we flatten the R-MAC features from each frame  into the frame representation vector . The similarities between consecutive frames is then the cosine similarity between their representations vectors . We mark the frame  as the start of a new video shot when the cosine similarity between  and  is lower than the minimum cosine similarity threshold . Generally, a higher  creates more shots and resembles frame-level approaches while a smaller  reduces the number of shots and closely approximates video-level methods.

\textbf{\label{sect:aggregate-shots}Aggregating Shot Similarities.} Under our shot-level approach, we evaluate the similarity between videos  and  from the pairwise similarities between their shot embeddings, i.e.  and . We compute the cosine similarity between pairs of shots and build a pairwise shot cosine similarity matrix  where  returns the cosine similarity between shots  and .

We aggregate these pairwise similarities into a similarity metric between videos. We use two aggregation schemes proposed in \cite{kordopatiszilos2019visil}: Chamfer similarity (CS) which takes the maximum similarity along the columns of  followed by average pooling of the maximum similarity vector, i.e. 

Symmetric Chamfer Similarity (SCS) which computes the average Chamfer similarity from  and , i.e.:


\section{Experiments}

We evaluate video retrieval performance using mean Average Precision~\cite{dataset:cc-web-video} (mAP). Additionally, we provide qualitative results in our supplementary material.

\subsection{Experiment Settings}

We evaluate VRAG and our shot-level approach over several video retrieval settings: Near-Duplicate Video Retrieval (NDVR); Event Video Retrieval (EVR); and Fine-grained Incident Video Retrieval (FIVR).

\vspace{0mm}
\paragraph{Datasets.} For evaluation, we use CC\_WEB\_VIDEO~\cite{dataset:cc-web-video} for NDVR, EVVE~\cite{dataset:evve} for EVR, and FIVR200K~\cite{dataset:fivr200k} and its subset FIVR5K~\cite{dataset:fivr200k, kordopatiszilos2019visil} for FIVR. During training, we sample triplets of videos from VCDB~\cite{dataset:vcdb}.

\noindent \textbf{CC\_WEB\_VIDEO}~\cite{dataset:cc-web-video} contains 13,129 videos with 24 query videos. In this dataset, near-duplicate videos correspond to videos stored in different formats e.g. .flv; .wmv; and videos with minor content differences e.g. photometric variations like lighting. We evaluate on the original annotations from \cite{dataset:cc-web-video} and the cleaned annotations from \cite{kordopatiszilos2019visil}. 

\noindent\textbf{EVVE}~\cite{dataset:evve} contains 2,995 videos from Youtube with 620 query videos over 13 events. 

\noindent \textbf{FIVR200K}~\cite{dataset:fivr200k} contains 100 query videos and a total of 225,960 videos from 4,687 Wikipedia events. The dataset groups similar videos into four categories: 1) Near-Duplicate (ND) videos which share all scenes with the query video; 2) Duplicate Scene (DS) videos that share at least one scene with the query video; 3) Complementary Scene (CS) videos that share at least segment with the query video, but from a different viewpoint; 4) Incident Scene (IS) videos that capture the same incident as the query video without sharing segments.
From the video categories, the dataset introduces three video retrieval settings: 1) Duplicate Scene Video Retrieval (DSVR) which only considers ND and DS videos as positives; 2) Complementary Scene Video Retrieval (CSVR) that extends from DSVR to include CS videos; and 3) Incident Scene Video Retrieval (ISVR) which marks all four categories of videos as similar videos.

\vspace{0mm}
\noindent \textbf{FIVR5K}~\cite{dataset:fivr200k, kordopatiszilos2019visil} is a subset of FIVR200K, with 50 query videos and 5,000 database videos. To create FIVR5K, the authors~\cite{kordopatiszilos2019visil} select the 50 hardest query videos on the DSVR task from FIVR200K using \cite{lbow} to measure difficulty.

\vspace{0mm}
\noindent \textbf{VCDB}~\cite{dataset:vcdb} consists of a core dataset with 528 videos and a background dataset with 100,000 distractor videos. The core dataset videos has a total of 9,236 pairs of partial copies between the 528 videos in the core dataset.

\paragraph{Implementation Details.}
For each video, we sample frames at one second intervals. We extract R-MAC features from each bottleneck layer of ResNet50~\cite{resnet}, i.e.  and . We set the number of regions per frame , intermediate dimensions , video-level embedding dimensions . We use  Graph Attention~\cite{graph-attention} layers and the ELU~\cite{nonlinearity:elu} non-linearity.
We train VRAG on a single GTX 1080 Ti using a batch size of one over 120 epochs. The maximum duration of each video clip in the training triplet is s, and we sample 1,000 triplets from each triplet pool, giving a total of 2,000 training iterations per epoch. We optimize VRAG using the triplet margin loss with  and the Adam~\cite{optimizer:adam} optimizer using a fixed learning rate at . During inference, VRAG processes up to 2,000 frames on 11GB of GPU memory, which amounts to over 30 minutes of video.


\subsection{Ablation Studies}





\paragraph{Our VRAG.}
At the region-level, we use attention to aggregate features from neighboring regions. We compare our attention aggregation mechanism with alternatives such as max aggregation and average aggregation, i.e. GCN~\cite{gcn}. In Table~\ref{tab:ablation:region-aggregation}, we show that our choice of region-level attention aggregation improves performance across all FIVR5K settings.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         Method & Region Agg. & DSVR & CSVR & ISVR \\
         \hline\hline
\multirow{3}{*}{VRAG} & Max & 0.520 & 0.537 & 0.508 \\
         \cline{2-5}
         &  Average & 0.518 & 0.532 & 0.506 \\
         \cline{2-5}
          &  Attention & \textbf{0.532} & \textbf{0.548} & \textbf{0.518} \\
         \hline
    \end{tabular}
    \caption{Comparison on FIVR5K over different choices of region-level aggregation. Attention-weighted summation is used to pool region embeddings for all choices of region-level aggregation.}
    \label{tab:ablation:region-aggregation}
\end{table}

At the video-level, we pool region embeddings to a fixed size vector  using an attention-weighted summation. We compare our attention pooling with standard methods, i.e. max pooling and average pooling. In Table~\ref{tab:ablation:region-pooling}, we demonstrate that our learnable pooling gives better performance over conventional pooling techniques.  

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         Method & Region Pooling & DSVR & CSVR & ISVR \\
         \hline\hline
         \multirow{3}{*}{VRAG} & Max & 0.423 & 0.435 & 0.415 \\
        \cline{2-5}
          & Average & 0.505 & 0.513 & 0.499 \\
         \cline{2-5}
          &  Attention & \textbf{0.532} & \textbf{0.548} & \textbf{0.518} \\
         \hline
    \end{tabular}
    \caption{\label{tab:ablation:region-pooling} {Comparison on FIVR5K over different video-level pooling techniques.}}
\end{table}
We also compare our choice of self-attention to an alternative with reduced parameters. In Table~\ref{tab:ablation:attention}, we observe that using separate key/query parameters allows for more complex relations between regions and significantly improves retrieval performance in FIVR5K.
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         Method & Attention & DSVR & CSVR & ISVR \\
         \hline\hline
         \multirow{2}{*}{VRAG} &  & 0.493 & 0.510 & 0.490 \\
         \cline{2-5}
          &   & \textbf{0.532} & \textbf{0.548} & \textbf{0.518} \\
         \hline
    \end{tabular}
    \caption{Comparison on FIVR5K using different implementations of attention.}
    \label{tab:ablation:attention}
\end{table}

Finally, we propose concatenating the region-level embeddings to reduce over-smoothing effects observed in GCNs~\cite{gcn:over-smoothing1, chen2019measuring}. We compare against three baselines: 1) Using only the output from the final Graph Attention layer; 2) Concatenating the outputs from all Graph Attention layers; 3) Concatenating the outputs from all Graph Attention layers and . In Table~\ref{tab:ablation:region-embed}, we show that  multi-layer concatenation of region-level embeddings greatly improves retrieval performance. Furthermore, we observe a monotonic increase in performance as we concatenate  region representations across more layers.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         Method & Concat. Layers & DSVR & CSVR & ISVR \\
         \hline\hline
         \multirow{4}{*}{VRAG} & Final Graph Attn. & 0.452 & 0.463 & 0.442 \\
        \cline{2-5}
         & All Graph Attn. & 0.481 & 0.497 & 0.477 \\
        \cline{2-5}
         & All Graph Attn. +  & 0.507 & 0.526 & 0.498 \\
        \cline{2-5}
         &  All Layers & \textbf{0.532} & \textbf{0.548} & \textbf{0.518} \\
         \hline
    \end{tabular}
    \caption{Comparison on FIVR5K over different levels of depth-wise concatenation.}
    \label{tab:ablation:region-embed}
\end{table}

\paragraph{Shot-level Video Retrieval.}
In shot-level video retrieval, i.e. VRAG-S, we compare different choices of  for our shot boundary algorithm. In Table~\ref{tab:ablation:shot-level}, we demonstrate that increasing the number of shots i.e  results in better performance in FIVR5K. Our results are also consistent with the frame-level approach ViSiL~\cite{kordopatiszilos2019visil}, where aggregating similarities using Chamfer Similarity (CS) has better performance than Symmetric Chamfer Similarity (SCS). We also observe that our shot-level approach bridges the large gap in performance between video level approaches, i.e. VRAG and frame-level approaches, i.e. ViSiL~\cite{kordopatiszilos2019visil}, in FIVR5K.



\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         Method &  & Shot Agg. & DSVR & CSVR & ISVR \\  
         \hline\hline
         VRAG & - & - & 0.532 & 0.548 & 0.518 \\
         \hline \hline
\multirow{4}{*}{VRAG-S} & \multirow{2}{*}{0.5} & CS & 0.493 & 0.512 & 0.483 \\
          \cline{3-6}
         && SCS & 0.463 & 0.477 & 0.464 \\
          \cline{2-6}
& \multirow{2}{*}{0.75} & CS & \textbf{0.709} & \textbf{0.704} & \textbf{0.636} \\
         \cline{3-6}
        & & SCS & 0.596 & 0.606 & 0.575 \\
         \hline \hline 
         \multirow{2}{*}{ViSiL} & \multirow{2}{*}{-} & CS & \textbf{0.880} & \textbf{0.869} & \textbf{0.777} \\
         \cline{3-6}
         & & SCS & 0.830 & 0.823 & 0.731 \\
         \hline \hline
    \end{tabular}
    \caption{Comparison on FIVR5K over different shot-level hyperparameters.}
    \label{tab:ablation:shot-level}
\end{table}

\subsection{Comparison with Baseline Methods}

We compare VRAG and our shot-level approach with existing approaches as baselines across several video retrieval tasks. We evaluate our performance on CC\_WEB\_VIDEO~\cite{dataset:cc-web-video} for NDVR, EVVE~\cite{dataset:evve} for EVR, and use FIVR200K~\cite{dataset:fivr200k}, and FIVR5K~\cite{kordopatiszilos2019visil, dataset:fivr200k}, for FIVR.
\vspace{0mm}
\paragraph{Video-level Methods.} For LBoW~\cite{lbow}, we use the results from \cite{video-verification-fake-news} for CC\_WEB\_VIDEO and FIVR200K. We re-implement LBoW, i.e. LBoW, with 1000 codebooks built on  VCDB using KMeans++, for EVVE and FIVR5K. For DML~\cite{kordopatis2017dml}, we obtain results for CC\_WEB\_VIDEO and FIVR200K from \cite{video-verification-fake-news}, and use the released source code\footnote{https://github.com/MKLab-ITI/ndvr-dml} for EVVE and FIVR5K, i.e. DML*. 
For TMK~\cite{baseline:tmk} and LAMV~\cite{Baraldi2018LAMVLT} on EVVE, we found discrepancies between their evaluation script and the original script from \cite{dataset:evve}. We provide corrected results, i.e. TMK*, LAMV*, using the original EVVE evaluation script~\cite{dataset:evve}. LAMV was not available in the released source code\footnote{\label{footnote:lamv}https://github.com/facebookresearch/videoalignment}, and we use TMK with frequency normalization (0.534 mAP on EVVE) as a close proxy for LAMV (0.536 mAP on EVVE~\cite{Baraldi2018LAMVLT}). We also evaluate TMK* and LAMV* on FIVR5K and FIVR200K. For other video-level methods, we report results from \cite{kordopatiszilos2019visil}.

\vspace{-3mm}
\paragraph{Frame-level Methods.} On EVVE, the authors of ViSiL~\cite{kordopatiszilos2019visil} were only able to download, process and evaluate on ~80\% of the dataset. We provide complete results for ViSiL using the released source code\footnote{https://github.com/MKLab-ITI/visil.git}, i.e. ViSiL* and ViSiL*. For other frame-level methods, we report the updated results using deep network features in \cite{kordopatiszilos2019visil}.

\textbf{Near-duplicate Video Retrieval.}

In Table~\ref{tab:results:ccweb}, we compare our approach with other video-level and frame-level approaches on CC\_WEB\_VIDEO~\cite{dataset:cc-web-video}, and demonstrate that our approach achieves state-of-the-art performance for video-level NDVR. Similar to FIVR5K, we observe that our shot-level approach has intermediate performance to frame-level and video-level approaches. 
We also compare the run times for video-level VRAG and shot-level VRAG with ViSiL~\cite{kordopatiszilos2019visil} after extracting R-MAC features from each video frame. Video-level and shot-level VRAG takes 33mins and 52mins, respectively, while ViSiL uses 109mins to infer video similarities. 

\begin{table}[!htbp]
\small
\setlength\tabcolsep{3pt}
    \begin{tabular}{|l|l|c|c|c|c|}
        \hline \hline
        Type & Method & cc\_web & cc\_web* & cc\_web & cc\_web* \\
        \hline \hline
        \multirow{3}{*}{Video} & \text{LBoW} & 0.957 & 0.906 & - & - \\
        & \text{DML} & \textbf{0.971} & 0.941 & 0.979 & 0.959 \\
        & \textbf{Ours} & \textbf{0.971} & \textbf{0.952} & \textbf{0.980} & \textbf{0.967} \\
        \hline \hline
\multirow{2}{*}{Shot} & \textbf{Ours (CS)} & 0.975 & 0.955 & \textbf{0.987} & \textbf{0.977} \\
        & \textbf{Ours (SCS)} & \textbf{0.976} & \textbf{0.959} & 0.986 & \textbf{0.977} \\
        \hline\hline
        \multirow{6}{*}{Frame} & \text{CTE} & \textbf{0.996} & - & - & - \\
        & \text{DP} & 0.975 & 0.958 & 0.990 & 0.982 \\
        & \text{TN} & 0.978 & 0.965 & 0.991 & 0.987 \\
        & \text{ViSiL} & 0.984 & 0.969 & 0.993 & 0.987 \\
        & \text{ViSiL} & 0.982 & 0.969 & 0.991 & 0.988 \\
        & \text{ViSiL} & 0.985 & \textbf{0.971} & \textbf{0.996} & \textbf{0.993} \\
        \hline
    \end{tabular}
    \caption{Results on four different versions of CC\_WEB\_VIDEO. (*) denotes evaluation on the entire data set and the subscript  uses cleaned annotations.} \vspace{-3mm}
    \label{tab:results:ccweb}
\end{table}

\begin{table*}[tb]
\setlength\tabcolsep{5pt}
\footnotesize
\centering
\begin{tabular}{|l||l||c||ccccccccccccc|}
\hline
Approach & Method & mAP & \multicolumn{13}{c|}{per event class} \\
\hline \hline
\multirow{9}{*}{Video}
    & LBoW & 0.469  & 0.323 & 0.373 & 0.062 & 0.392 & 0.306 & 0.232 & 0.205 & 0.127 & 0.060 & 0.376 & 0.233 & 0.769 & 0.713 \\    
    & \text{DML*} & 0.472  & 0.437 & 0.368 & 0.052 & 0.385 & 0.242 & 0.275 & 0.205 & 0.105 & 0.085 & 0.414 & 0.245 & 0.783 & 0.656 \\
    &\text{TMK*} & 0.469  & 0.508 & 0.306 & 0.139 & 0.366 & 0.294 & 0.244 & 0.208 & 0.125 & 0.152 & 0.287 & 0.213 & 0.810 & 0.614 \\
    & \text{LAMV*} & 0.493 & 0.649 & 0.321 & 0.157 & 0.411 & 0.319 & 0.241 & 0.224 & 0.124 & 0.194 & 0.257 & 0.191 & 0.857 & 0.660 \\
    & \text{LAMV+QE*} & 0.541 & 0.795 & 0.413 & \textbf{\underline{0.160}} & \underline{0.546} & \underline{0.376} & 0.297 & 0.235 & 0.124 & 0.236 & 0.257 & 0.185 & 0.907 & 0.754 \\
    & \text{LAMV} & 0.536 & 0.715 & 0.383 & 0.158 & 0.461 & 0.387 & 0.227 & 0.247 & 0.138 & 0.222 & 0.273 & 0.273 & 0.908 & 0.691 \\
    & \text{LAMV+QE} & 0.587 & 0.837 & 0.500 & 0.126 & \textbf{0.588} & \textbf{0.455} & \textbf{0.343} & \textbf{0.267} & 0.142 & 0.230 & 0.293 & 0.216 & \textbf{0.950} & 0.770 \\
    & \textbf{Ours} & 0.623 & 0.792  & 0.675 & 0.072 & 0.496 & 0.329 & 0.292 & \underline{0.256} & 0.241 & \underline{\textbf{0.497}} & 0.692 & 0.378 & 0.928 & 0.770 \\
    & \textbf{Ours+QE} & \textbf{\underline{0.653}}  & \textbf{\underline{0.888}} & \textbf{\underline{0.743}} & 0.042 & 0.505 & 0.342 & \underline{0.304} & 0.247 & \underline{\textbf{0.280}} & 0.489 & \underline{\textbf{0.782}} & \underline{\textbf{0.410}} & \underline{0.943} & \underline{\textbf{0.835}} \\
    \hline\hline
\multirow{2}{*}{Shot} & \textbf{Ours (CS)} & 0.539 & 0.796 & 0.599 & 0.077 & \textbf{0.515} & 0.203 & \textbf{0.266} & 0.190 & 0.098 & 0.222 & 0.589 & 0.299 & 0.836 & \textbf{0.775} \\
    & \textbf{Ours (SCS)}  & \textbf{0.606} & \textbf{0.832} &\textbf{ 0.722} & \textbf{0.155} & 0.494 & \textbf{0.336} & 0.265 & \textbf{0.236} & \textbf{0.177} & \textbf{0.366} & \textbf{0.620} & \textbf{0.304} & \textbf{0.925} & 0.670 \\
    \hline\hline
    \multirow{3}{*}{Frame} & \text{ViSiL} & 0.589 & 0.889 & 0.570 & 0.169 & 0.432 & 0.345 & \textbf{0.393} & \textbf{0.297} & 0.181 & 0.479 & 0.564 & 0.369 & 0.885 & 0.799 \\
    & \text{ViSiL}* & 0.612 & \textbf{0.923} & \textbf{0.724} & \textbf{0.301} & 0.573 & \textbf{0.418} & 0.276 & 0.291 & \textbf{0.200} & \textbf{0.544} & 0.396 & 0.339 & \textbf{0.938} & 0.753 \\
    & \text{ViSiL}* & \textbf{0.618} & 0.920 & 0.713 & 0.222 & \textbf{0.589} & 0.350 & 0.345 & 0.276 & 0.169 & 0.444 & \textbf{0.567} & \textbf{0.375} & 0.909 &\textbf{0.842} \\
    \hline \hline
  \end{tabular}
  \caption{Comparison with state-of-the-art EVR approaches on EVVE. We use the same event class ordering as \cite{dataset:evve}. For video-level approaches, we also underline the result with the highest mAP, excluding the results with discrepancies in the evaluation script, i.e. LAMV and LAMV+QE. We report results obtained from the original EVVE evaluation script.} \vspace{-2mm}
  \label{tab:results:evve}
\end{table*}

\textbf{Fine-grained Incident Video Retrieval.} We evaluate on FIVR5K and FIVR200K in Table \ref{tab:results:fivr5k} and Table \ref{tab:results:fivr200k} respectively. Our results include the run times for available frame-level and video-level methods after extracting frame-level features. VRAG achieves state-of-the-art performance over video-level methods on FIVR datasets.
Generally, we observe a huge difference in retrieval performance between video-level and frame-level approaches on FIVR. Although our shot-level approach bridges the gap between video-level and frame-level while taking   longer than video-level VRAG, the dramatic disparity between the frame-level and video-level approaches warrants a qualitative inspection of FIVR200K.

\begin{table}[ht]
    \footnotesize
    \begin{tabular}{|l|l|c|c|c|c|}
        \hline \hline
        Type & Method & DSVR & CSVR & ISVR & Time\\
        \hline \hline
        \multirow{6}{*}{Video} & \text{LBoW} & 0.351 & 0.320 & 0.298 & 4m 9s \\
        & \text{DML*} & 0.354 & 0.351 & 0.331 & 3m 29s \\
        & \text{TMK*} & 0.411 & 0.416 & 0.388  & 19m 9s \\ 
        & \text{LAMV*} & 0.498 & 0.488 & 0.426 & 20m 24s \\
        & \textbf{Ours} & \textbf{0.532} & \textbf{0.548} & \textbf{0.518} & 8m 27s \\
        \hline \hline
\multirow{2}{*}{Shot} & \textbf{Ours (CS)} & \textbf{0.709} & \textbf{0.704} & \textbf{0.636}  &  \multirow{2}{*}{10m 35s}\\
        & \textbf{Ours (SCS)} & 0.596 & 0.606 & 0.575 &  \\
        \hline\hline
        \multirow{3}{*}{Frame} 
        & \text{ViSiL} & 0.838 & 0.832 & 0.739 & - \\
        & \text{ViSiL} & 0.830 & 0.823 & 0.731 & 45m 24s \\
        & \text{ViSiL} & \textbf{0.880} & \textbf{0.869} & \textbf{0.777} & 46m 39s  \\
        \hline
    \end{tabular}
\caption{Results on FIVR5K} \vspace{-3mm}
    \label{tab:results:fivr5k}
\end{table}
\vspace{-3mm}
\begin{table}[ht]
    \footnotesize
    \begin{tabular}{|l|l|c|c|c|c|}
        \hline \hline
        Type & Method & DSVR & CSVR & ISVR & Time\\
        \hline \hline
        \multirow{6}{*}{Video} & \text{LBoW} & 0.378 & 0.361 & 0.297 & 3h 50m \\
        & \text{DML} & 0.398 & 0.351 & 0.331 & 3h 31m \\
        & \text{TMK*} & 0.417 & 0.394 & 0.319  & 18h 51m \\ 
        & \text{LAMV*} & \textbf{0.489} & 0.459 & 0.364 & 20h 30m \\
        & \textbf{Ours} & 0.484 & \textbf{0.470} & \textbf{0.399} & 6h 50m \\
        \hline \hline
       \multirow{2}{*}{Shot} & \textbf{Ours (CS)} & \textbf{0.723} & \textbf{0.678} & \textbf{0.554} & \multirow{2}{*}{9h 34m} \\
        & \textbf{Ours (SCS)} & 0.536 & 0.504 & 0.422 &  \\
        \hline\hline
        \multirow{5}{*}{Frame}
        & \text{DP} & 0.775 & 0.740 & 0.632 & - \\
        & \text{TN} & 0.724 & 0.699 & 0.589 & - \\
        & \text{ViSiL} & 0.843 & 0.797 & 0.660 & - \\
        & \text{ViSiL} & 0.830 & 0.823 & 0.731 & 63h 31m \\
        & \text{ViSiL} & \textbf{0.880} & \textbf{0.869} & \textbf{0.777} & 66h 18m\\
        \hline \hline
    \end{tabular}
\caption{Results on FIVR200K} \vspace{-2mm}
    \label{tab:results:fivr200k}
\end{table}

We found that most FIVR200K queries comprise of single shots while their Duplicate Scene (DS) videos are sequences of visually diverse shots. Additionally, most shots in the DS videos have low visual and semantic correspondence with the query segment. Consequently, video-level methods are likely to yield noisy representations that are distinct to the query. On the other hand, higher fidelity approaches, i.e. frame-level and shot-level retrieval, that segment the video into multiple representations can preserve the query video segment and obtain more accurate video similarities. We show examples in the supplementary material.




\subsubsection{Event Video Retrieval}
In Table~\ref{tab:results:evve}, we evaluate our performance on EVVE~\cite{dataset:evve}. Without test augmentations, i.e. Query Expansion, VRAG outperforms state-of-the-art video-level and frame-level methods. We apply Average Query Expansion~\cite{query-expansion} on VRAG, i.e. VRAG+QE and it demonstrates further performance gains. Our shot-level approach is competitive with video-level and frame-level approaches and use 75mins of evaluation time. In contrast, video-level VRAG takes approximately 12mins for inference on EVVE, while ViSiL~\cite{kordopatiszilos2019visil} uses 18hrs. 
We attribute the difference in efficiency to paradigm differences between video-level and frame-level approaches: Our video-level method requires  forward passes through VRAG to encode  video embeddings. In contrast, ViSiL directly outputs the similarity between every query-candidate video pair. This requires  forward passes through ViSiL, which is orders of magnitude larger than the number of videos . 





\section{Conclusion}

In this work, we introduce Video Region Attention Graph Network (VRAG) that utilizes self-attention during region aggregation and region pooling to generate video-level embeddings for efficient video retrieval. 
Specifically, we represent videos at a finer granularity through region-level features and encode video spatio-temporal dynamics through region-level relations. 
Our VRAG improves the state-of-the-art performance of video-level methods across multiple video retrieval datasets. We also introduce an intermediate approach to video-level and frame-level video retrieval that utilizes shots for video retrieval. We demonstrate that our shot-level approach bridges the gap in performance between video-level and frame-level approaches, where it obtains higher video retrieval performance at marginal increase in computational costs over video-level approaches.

\bibliography{aaai22}

\clearpage




\end{document}
