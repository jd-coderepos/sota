Our method aims at overcoming the limitations of semantic segmentation techniques when training on a small dataset or when training from scratch. An overview of our method is depicted in Figure  \ref{fig:overview}. In the following sections, we detail our process in choosing the labels, generating the dataset, and using these labels to pre-train a given network.

\subsection{Alternative Category Representation}\label{sec:label_choice}
The motivation to this work is to look at the depth channel as a source for labeling RGB images. RGBD sensors are used to easily collect aligned color and depth image pairs. Our aim is to automatically generate labels that are not necessarily designed for the target task (semantic segmentation), but could afford helpful information for it. Successfully transforming depth into labels makes this method capable of generating \emph{free} labels as compared to the tedious, time consuming, RGB pixel labeling needed for semantic segmentation. The first intuitive use of depth is to train a network to directly regress the depth from a single RGB image. However, depth requires complex knowledge of the environment and varies with camera location/viewpoint.

With the complexity of indoor scenes, regressing depth necessitates a high capacity network and can be tedious to train. On the other hand, with additional effort, one can transform depth into height above ground, which is view-independent. Taking the ground plane as reference, one can also transform normals relative to the camera viewpoint, which depend on camera angle, to view-independent normal angles relative to the ground plane.

Our method uses height and normal information as the main precursor for pre-training. We choose to bin the height into $n_h$ bins (or levels) and normals into $n_n$ bins and train a model to output the correct normal and height level per pixel. Since it learns to generate features specific to segmentation, this pre-training is closer to our end goal of semantic segmentation than to pre-train with models trained on other tasks, \eg image classification. In fact, one can think of height bins as semantic segmentation with object part specific labels: chair and table lower legs, lower part of wall and door, table top, and sofa back, \textit{etc}. This type of grouping consistently clusters objects and their parts into specific bins. To highlight this point, Figure \ref{fig:height_stats} shows the height distribution of 40 indoor objects on a subset of NYUv2 dataset \cite{Silberman2012}. For every object, we show the percentage of its occurrence in every height bin. As expected, objects like floor (label 2) and floor mat (label 20) only occur within the lowest height bin, whereas ceiling (label 22) occurs within the highest bins. Other objects almost always occur in the same bin, such as bathtub (label 36) and counter (label 12). Furthermore, when considering other object classes, there exists an overall trend in height, with object parts falling in specific height bins. In light of these observations, training a network to distinguish between different height levels will inherently train it to distinguish between some object classes and/or their parts. This information is expected to be useful for the task of semantic segmentation.



\begin{figure}[!h]
\begin{center}
\includegraphics[width=\linewidth]{figures/height_stats2.png}
\end{center}
\vspace{-5pt}
   \caption{\textbf{Object height distribution.} This colormap shows the distribution of height for various object classes in NYUv2 \cite{Silberman2012}. Colors represent the fraction of 3D points of the same label that are located at a particular height bin.} 
\label{fig:height_stats}
\end{figure}


\subsection{Automated Label Collection}\label{sec:data_collec}
\noindent\textbf{Floor Identification:} Our method mainly relies on the floor plane as a reference for the proposed labels. To identify a possible floor plane, we first use the  camera intrinsic parameters to map the depth image into a 3D point cloud. Next, we compute the normals at each 3D point using the method of \cite{rosman2014augmented}. We then use the normals to estimate the scene layout orientation using the method of \cite{ghanem2015robust}. Once the orientation is obtained, the room is rotated to make the ground normal aligned with the upward direction. Afterwards, we choose the ground location as the first percentile of points along the upward direction. The first percentile is chosen instead of the minimum in order to make the method more robust to sensor errors especially at far distances. Our method takes into account that in most indoor images captured by RGBD sensors, a part of the ground is visible. In order to account for cases where the ground is not visible, we discard scenes in which the majority of the normals from points on the hypothesized floor plane (first percentile) are not aligned with the upward direction.

\noindent\textbf{HN-Label Generation:} To transform depth into height, we calculate the distance between each 3D point and the floor plane. As for the normals, we compute the angle between every 3D point normal and the floor's upward direction. We only use one angle to keep labels consistent over multiple viewpoints. Normals and heights are then binned, and we define our proposed alternative categories as all possible combinations of the normal and height bins. Figure \ref{fig:view_indep} shows our proposed labels for the same scene from different viewpoints. Since we choose our labeling scheme to be view-independent, objects maintain the same labels throughout the scene recording. 

\begin{figure}[!h]
\begin{center}
\includegraphics[width=\linewidth]{figures/view_indep.png}
\end{center}
   \caption{\textbf{HN-labels for different images in the same room.} Our proposed labels are view-independent and are consistent among different viewpoints.}
\label{fig:view_indep}
\end{figure}

\subsection{Training with HN-Labels}\label{sec:train_alter}
Labels collected from height and normal information can be used to train the same network architecture used for semantic segmentation. Specifically, we choose the SegNet architecture \cite{badrinarayanan2015segnet} for this purpose, with an encoder network identical to the 13 convolutional layers in the VGG16 network \cite{Simonyan14c}, as well as, the DeepLabv3 architecture \cite{chen2017rethinking} with ResNetv2 layers \cite{he2016identity}. Note that our method can be easily implemented with any other architecture and that the  number of labels for semantic segmentation need not be similar to that of pre-training with HN-labels. In what follows, we will reference this trained network as the HN-network. 



Once we train our HN-network with labels that are self-supervised, we need to transfer the representation to another network that learns to segment RGB images. In both the SegNet and DeepLabv3 architectures, the only difference between the two networks is the number of object classes, \ie  the number of outputs from the last convolution layer. Transfer learning can be done in two ways. The first way is to initialize all the layers of the semantic segmentation network with what was learned in the first network, and then finetune all the layers based on the manually labeled segmentation dataset. Another way of transfer learning is to keep some of the first layers unchanged, especially when the dataset used for pre-training is much larger than the dataset used for finetuning. This is motivated by the idea that  generic information is usually shared at early layers.






















\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/cross-stitch.png}
\end{center}
   \caption{\textbf{Proposed fusion of HN-network and semantic segmentation network (Cross-Stitch).} The HN-network is considered as a shared representation whereas the semantic segmentation network is task specific). At every max-pooling layer, the network gets to choose between the pre-trained layers and the task specific layers.}
\label{fig:cross}
\end{figure}

Choosing to finetune only part of the network raises an important question: how many layers should be fixed and how many should be finetuned? Instead of manually selecting which layers to share and which layers to be task-specific, one can model the problem as a multi-task learning problem, in which sharing can be learned between the networks. This is mainly inspired by the work in \cite{misra2016cross}. Our joint network architecture, named cross-stitch \cite{misra2016cross}, is shown in Figure \ref{fig:cross}. The shared representation part of the network is extracted from a network that was trained to output HN-labels, whereas the task-specific part learns to semantically segment. The shared representation part is kept fixed, reasoned by the large data that can be used to train that part. Sharing of layers is allowed at the first 4 max-pooling layers. We denote the output at every max-pooling by $L_{xs}$ and $L_{xh}$, where $x$ represents the index of the max-pooling layer. $L_{xs}$ refers to the output generated in the ``semantic segmentation network", whereas $L_{xh}$ refers to output from the HN-network. At every max-pooling layer, both representations are merged into $$L = \alpha_{xh}L_{xh} + \alpha_{xs}L_{xs}$$ and $L$ is used as an input to the subsequent layer of the semantic segmentation part of the network.

The final target is to achieve a higher accuracy in semantic segmentation only, especially when semantic labels are scarce. This differs from \cite{misra2016cross}, which aims at achieving a better joint accuracy in multiple tasks.


\noindent \textbf{Implementation Details: } Our method is implemented using the Caffe framework \cite{jia2014caffe} for the SegNet architecture and Tensorflow \cite{abadi2016tensorflow} for the DeepLabv3 architecture, both running on Titan X and Titan Xp GPUs. We use the same image input and output size for all of our networks. In the cases where depth sensors output a different size image, we compute the height on the original size image, then crop and reshape the color image with bilinear interpolation, while using nearest-neighbour interpolation for the labels. As for the training loss, we use the cross-entropy loss \cite{long2015fully} in all our networks. 




