
\section{Experiments and Analysis}
\textbf{Benchmarks and Baselines.} We evaluate the performance of \DAA{} on three standard benchmarks: CIFAR-10, CIFAR-100, ImageNet, and compare it against a baseline based on standard augmentations (\textit{i.e.}, flip left-righ, pad-and-crop for CIFAR-10/100, and Inception-style preprocesing~\citep{szegedy2015going} for ImageNet) as well as nine existing automatic augmentation methods including (1) AutoAugment (AA) \citep{cubuk2019autoaugment}, (2) PBA \citep{ho2019population}, (3) Fast AutoAugment (Fast AA) \citep{lim2019fast}, (4) Faster AutoAugment \citep{hataya2020faster}, (5) DADA \citep{li2020differentiable}, (6) RandAugment (RA) \citep{cubuk2020randaugment}, (7) UniformAugment (UA) \citep{lingchen2020uniformaugment}, (8) TrivialAugment (TA) \citep{muller2021trivialaugment}, and (9) Adversarial AutoAugment (AdvAA) \citep{zhang2019adversarial}. 


\textbf{Search Space.} We set up the operation set  to include  commonly used operations (identity, shear-x, shear-y, translate-x, translate-y, rotate, solarize, equalize, color, posterize, contrast, brightness, sharpness, autoContrast, invert, \randCutout{}) as well as two operations (\textit{i.e.}, \randFlip{} and \randCrop{}) that are used as the default operations in the aforementioned methods.
Among the operations in ,  operations are associated with magnitudes.
We then discretize the range of magnitudes into  uniformly spaced levels and treat each operation with a discrete magnitude as an independent transformation. Therefore, the policy in each layer is a 139-dimensional categorical distribution corresponding to  operation, magnitude pairs. The list of operations and the range of magnitudes in the standard augmentation space are summarized in Appendix~\ref{app:augmentation_space}.


\subsection{Performance on CIFAR-10 and CIFAR-100}
\textbf{Policy Search.} 
Following \citep{cubuk2019autoaugment}, we conduct the augmentation policy search based on Wide-ResNet-40-2~\citep{zagoruyko2016wide}. We first train the network on a subset of  randomly selected samples from CIFAR-10. We then progressively update the policy network parameters  for  iterations for each of the  augmentation layers. We use the Adam optimizer~\citep{kingma2014adam} and set the learning rate to  for policy updating. 



\textbf{Policy Evaluation.} 
Using the publicly available repository of Fast AutoAugment~\citep{lim2019fast}, we evaluate the found augmentation policy on both CIFAR-10 and CIFAR-100 using Wide-ResNet-28-10 and Shake-Shake-2x96d models. The evaluation configurations are kept consistent with that of Fast AutoAugment.

\textbf{Results.} 
Table~\ref{tab:cifar} reports the Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d, respectively. The results of \DAA{} are the average of four independent runs with different initializations. We also show the  confidence interval of the mean accuracy. As shown, \DAA{} achieves the best performance compared against previous works using the standard augmentation space. \emph{
Note that TA(Wide) uses a wider (stronger) augmentation space on this dataset.}

\begin{table*}[h]
\centering
\resizebox{1.0\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} & \multicolumn{1}{c}{\textbf{Baseline}} & \multicolumn{1}{c}{\textbf{AA}} &
\multicolumn{1}{c}{\textbf{PBA}} &
\multicolumn{1}{c}{\textbf{FastAA}} &
\multicolumn{1}{c}{\textbf{FasterAA}} &
\multicolumn{1}{c}{\textbf{DADA}} &
\multicolumn{1}{c}{\textbf{RA}} &
\multicolumn{1}{c}{\textbf{UA}} &
\multicolumn{1}{c}{\textbf{TA(RA)}} &
\multicolumn{1}{c}{\textbf{TA(Wide) \footnotemark}} &
\multicolumn{1}{c}{\textbf{\DAA{}}}
\\ 
\midrule
\textbf{CIFAR-10} & & & & & & & & & & & \\
WRN-28-10 & 96.1 & 97.4 & 97.4 & 97.3 & 97.4 & 97.3 & 97.3 & 97.33 & 97.46 & 97.46  & \textbf{97.56}  0.14 \\
Shake-Shake \footnotesize{(26 2x96d)} & 97.1 & 98.0 & 98.0 & 98.0 & 98.0 & 98.0 & 98.0 & 98.1 & 98.05 & 98.21 & \textbf{98.11}  0.12 \\
\midrule
\textbf{CIFAR-100} & & & & & & & & & & \\
WRN-28-10 & 81.2 & 82.9 & 83.3 & 82.7 & 82.7 & 82.5 & 83.3 & 82.82 & 83.54 & 84.33 & \textbf{84.02}  0.18 \\
Shake-Shake \footnotesize{(26 2x96d)} & 82.9 & 85.7 & 84.7 & 85.1 & 85.0 & 84.7 & - & - & - & 86.19 & \textbf{85.19}  0.28 \\
\bottomrule
\end{tabular}
}
\caption{{\small Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d. The results of \DAA{} are averaged over four independent runs with different initializations. The  confidence interval is denoted by .}}
\label{tab:cifar}
\end{table*}


\vspace{-3mm}
\subsection{Performance on ImageNet}

\textbf{Policy Search.} 
We conduct the augmentation policy search based on ResNet-18~\citep{he2016deep}. We first train the network on a subset of  randomly selected samples from ImageNet for  epochs. We then use the same settings as in CIFAR-10 for updating the policy parameters.



\textbf{Policy Evaluation.} 
We evaluate the performance of the found augmentation policy on ResNet-50 and ResNet-200 based on the public repository of Fast AutoAugment~\citep{lim2019fast}. The parameters for training are the same as the ones of~\citep{lim2019fast}. In particular, we use step learning rate scheduler with a reduction factor of , and we train and evaluate with images of size 224x224.



\footnotetext{On CIFAR-10/100, TA (Wide) uses a wider (stronger) augmentation space, while the other methods including TA (RA) uses the standard augmentation space.}


\textbf{Results.}
The performance on ImageNet is presented in Table~\ref{tab:shakeshake}. As shown, \DAA{} achieves the best performance compared with previous methods without the use of default augmentation pipeline. In particular, \DAA{} performs better on larger models (\emph{i.e.} ResNet-200), as the performance of \DAA{} on ResNet-200 is the best within the  confidence interval. \emph{Note that while we train DeepAA using the image resolution (224224), we report the best results of RA and TA, which are trained with a larger image resolution (244224) on this dataset.} 

\addtocounter{footnote}{-1}
\begin{table*}[h]
\centering
\resizebox{1.0\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} & \multicolumn{1}{c}{\textbf{Baseline}} & 
\multicolumn{1}{c}{\textbf{AA}} &
\multicolumn{1}{c}{\textbf{Fast AA}} &
\multicolumn{1}{c}{\textbf{Faster AA}} &
\multicolumn{1}{c}{\textbf{DADA}} &
\multicolumn{1}{c}{\textbf{RA}} &
\multicolumn{1}{c}{\textbf{UA}} &
\multicolumn{1}{c}{\textbf{TA(RA)}\footnotemark} &
\multicolumn{1}{c}{\textbf{TA(Wide)\footnotemark}} &
\multicolumn{1}{c}{\textbf{\DAA{}}}
\\ 
\midrule 
ResNet-50 & 76.3 & 77.6 & 77.6 & 76.5 & 77.5 & 77.6 & 77.63 & 77.85 & 78.07 & \textbf{78.30  0.14} \\
ResNet-200 & 78.5 & 80.0 & 80.6 & - & - & - & 80.4 & - & - & \textbf{81.32  0.17}  \\
\bottomrule
\end{tabular}
}
\caption{{\small Top-1 test accuracy (\%) on ImageNet for ResNet-50 and ResNet-200. The results of \DAA{} are averaged over four independent runs with different initializations. The  confidence interval is denoted by .}}
\label{tab:shakeshake}
\end{table*}



\vspace{-2mm}
\subsection{Performance with Batch Augmentation}

Batch Augmentation (BA) is a technique that draws multiple augmented instances of the same sample in one mini-batch. It has been shown to be able to improve the generalization performance of the network~\citep{berman2019multigrain,hoffer2020augment}. AdvAA~\citep{zhang2019adversarial} directly searches for the augmentation policy under the BA setting whereas for TA and DeepAA, we apply BA with the same augmentation policy used in Table~\ref{tab:cifar}. Note that since the performance of BA is sensitive to the hyperparameters~\citep{fort2021drawing}, we have conducted a grid search on the hyperparameters of both TA and \DAA{} (details are included in Appendix~\ref{app:BA}). 
As shown in Table~\ref{tab:BA}, after tuning the hyperparameters, the performance of TA (Wide) using BA is already better than the reported performance in the original paper. The performance of \DAA{} with BA outperforms that of both AdvAA and TA (Wide) with BA.

\begin{table*}[h]
\centering
\resizebox{0.75\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{} &
\multicolumn{1}{l}{\textbf{AdvAA}} & 
\multicolumn{1}{c}{
\begin{tabular}[c]{@{}c@{}}\textbf{TA(Wide)}\\ (original paper) \end{tabular}} &
\multicolumn{1}{c}{
\begin{tabular}[c]{@{}c@{}}\textbf{TA(Wide)} \\ (ours) \end{tabular}} &
\multicolumn{1}{c}{\textbf{\DAA{}}}
\\ 
\midrule 
CIFAR-10 & 98.1  0.15 & 98.04  0.06 & 98.06  0.23 & \textbf{98.21  0.14} \\
CIFAR-100 & 84.51  0.18 & 84.62  0.14 & 85.40  0.15 & \textbf{85.61  0.17} \\
\bottomrule
\end{tabular}
}
\caption{{\small Top-1 test accuracy (\%) on CIFAR-10/100 dataset with WRN-28-10 with Batch Augmentation (BA), where eight augmented instances were drawn for each image. The results of \DAA{} are averaged over four independent runs with different initializations. The  confidence interval is denoted by .}}
\label{tab:BA}
\end{table*}

\addtocounter{footnote}{-1}
\footnotetext{TA (RA) achieves 77.55\% top-1 accuracy with image resolution 224224.}
\addtocounter{footnote}{1}
\footnotetext{TA (Wide) achieves 77.97\% top-1 accuracy with image resolution 224224.}


\subsection{Understanding \DAA{}}


\begin{wrapfigure}{t}{2.6in} \begin{minipage}{2.6in}
\centering
\vspace{-7mm}
\resizebox{2.6in}{!}{\includegraphics{DeepAA_simple.png}}
\vspace{-5mm}
\caption{{\small Top-1 test accuracy (\%) on ImageNet of \DAAsimple{}, \DAA{}, and other automatic augmentation methods on ResNet-50.}}
\vspace{-3mm}
\label{fig:deepaa_default}
\end{minipage}
\end{wrapfigure}


\textbf{Effectiveness of Gradient Matching.} 
One uniqueness of \DAA{} is the regularized gradient matching objective.
To examine its effectiveness, we remove the impact coming from multiple augmentation layers, and only conduct search for a single layer of augmentation policy. When evaluating the searched policy, we apply the default augmentation in addition to the searched policy. We refer to this variant as \DAAsimple{}. 
Figure~\ref{fig:deepaa_default} compares the Top-1 test accuracy on ImageNet using ResNet-50 between \DAAsimple{}, \DAA{}, and other automatic augmentation methods. While there is  performance drop compared to \DAA{}, \textit{with a single augmentation layer}, \DAAsimple{} still outperforms other methods and is able to achieve similar performance compared to TA (Wide) but with a standard augmentation space and trains on a smaller image size (224224 vs 244224).


\textbf{Policy Search Cost.} 
Table~\ref{tab:time} compares the policy search time on CIFAR-10/100 and ImageNet in GPU hours. 
\DAA{} has comparable search time as PBA, Fast AA, and RA, but is slower than Faster AA and DADA. Note that Faster AA and DADA relax the discrete search space to a continuous one similar to DARTS~\citep{liu2018darts}. While such relaxation leads to shorter searching time, it inevitably introduces a discrepancy between the true and relaxed augmentation spaces.



\begin{table*}[!h]
\centering
\resizebox{0.8\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} &  
\multicolumn{1}{c}{\textbf{AA}} &
\multicolumn{1}{c}{\textbf{PBA}} &
\multicolumn{1}{c}{\textbf{Fast AA}} &
\multicolumn{1}{c}{\textbf{Faster AA}} &
\multicolumn{1}{c}{\textbf{DADA}} &
\multicolumn{1}{c}{\textbf{RA}} &
\multicolumn{1}{c}{\textbf{\DAA{}}}
\\ 
\midrule 
CIFAR-10/100 & 5000 & 5 & 3.5 & 0.23 & 0.1  & 25  & 9   \\
ImageNet & 15000 & - & 450 & 2.3 & 1.3 & 5000 & 96 \\
\bottomrule
\end{tabular}
}
\caption{{\small Policy search time on CIFAR-10/100 and ImageNet in GPU hours.}}
\label{tab:time}
\vspace{-3mm}
\end{table*}


\textbf{Impact of the Number of Augmentation Layers.} 
Another uniqueness of \DAA{} is its multi-layer search space that can \textit{go beyond} two layers which existing automatic augmentation methods were designed upon.
We examine the impact of the number of augmentation layers on the performance of \DAA{}. 
Table~\ref{tab:cifar_layers} and Table~\ref{tab:imagenet_layers} show the performance on CIFAR-10/100 and ImageNet respectively with increasing number of augmentation layers. 
As shown, for CIFAR-10/100, the performance gradually improves when  more augmentation layers are added until we reach five layers. The performance does not improve when the  sixth layer is added.
For ImageNet, we have similar observation where the performance stops improving when more than five augmentation layers are included.




\vspace{2mm}
\begin{table*}[!h]
\centering
\resizebox{0.95\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} & 
\multicolumn{1}{c}{\textbf{1 layer}} & 
\multicolumn{1}{c}{\textbf{2 layers}} &
\multicolumn{1}{c}{\textbf{3 layers}} &
\multicolumn{1}{c}{\textbf{4 layers}} &
\multicolumn{1}{c}{\textbf{5 layers}} &
\multicolumn{1}{c}{\textbf{6 layers}}
\\ 
\midrule
CIFAR-10 & 96.3  0.21 & 96.6   0.18 & 96.9  0.12 & 97.4  0.14 & \textbf{97.56}  \textbf{0.14} & \textbf{97.6}  \textbf{0.12}  \\
CIFAR-100 & 80.9  0.31  & 81.7  0.24 & 82.2  0.21 & 83.7  0.24 & \textbf{84.02}  \textbf{0.18} & \textbf{84.0}  \textbf{0.19}   \\
\bottomrule
\end{tabular}
}
\caption{{\small Top-1 test accuracy of \DAA{} on CIFAR-10/100 for different numbers of augmentation layers. The results are averaged over 4 independent runs with different initializations with the  confidence interval denoted by .}}
\label{tab:cifar_layers}
\vspace{-3mm}
\end{table*}

\vspace{1mm}
\begin{table*}[!h]
\centering
\resizebox{0.7\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} & 
\multicolumn{1}{c}{\textbf{1 layer}} & 
\multicolumn{1}{c}{\textbf{3 layers}} &
\multicolumn{1}{c}{\textbf{5 layers}} &
\multicolumn{1}{c}{\textbf{7 layers}}
\\ 
\midrule
ImageNet & 75.27  0.19 & 78.18  0.22 & \textbf{78.30}  \textbf{0.14} & \textbf{78.30}   \textbf{0.14}  \\
\bottomrule
\end{tabular}
}
\caption{{\small Top-1 test accuracy of \DAA{} on ImageNet with ResNet-50 for different numbers of augmentation layers. The results are averaged over 4 independent runs w/ different initializations with the  confidence interval denoted by .}}
\label{tab:imagenet_layers}
\end{table*}


Figure~\ref{fig:transformation} illustrates the distributions of operations in the policy for CIFAR-10/100 and ImageNet respectively. 
As shown in Figure~\ref{fig:transformation}(a), the augmentation of CIFAR-10/100 converges to \textit{identity} transformation at the sixth augmentation layer, which is a natural indication of the end of the augmentation pipeline. 
We have similar observation in Figure~\ref{fig:transformation}(b) for ImageNet, where the \textit{identity} transformation dominates in the sixth augmentation layer. 
These observations match our results listed in Table~\ref{tab:cifar_layers} and Table~\ref{tab:imagenet_layers}. 
We also include the distribution of the magnitude within each operation for CIFAR-10/100 and ImageNet in Appendix~\ref{app:cifar} and Appendix~\ref{app:imagenet}.



\begin{figure}
\vspace{-10mm}
\centering
\includegraphics[width=0.7\linewidth]{operation_dist.png}
\vspace{-2mm}
\caption{{\small The distribution of operations at each layer of the policy for CIFAR-10/100 and ImageNet. The probability of each operation is summed up over all  discrete intensity levels (see Appendix~\ref{app:cifar} and~\ref{app:imagenet}) of the corresponding transformation.}}
\label{fig:transformation}
\vspace{-3mm}
\end{figure}


\begin{comment}
\vspace{1mm}
\textbf{Validity of Optimizing Gradient Matching.} 

Finally, we want to demonstrate the validity of optimizing gradient matching for automatic data augmentation as we claim throughout this work. 
To do so, we obtain a batch of augmented data by augmenting a single randomly sampled image for 512 times following  layers of augmentation policy. We then sample a batch of 64 original images within the same class as the augmented one. The cosine similarity is calculated between the gradients of the augmented image batch and original image batch. We report the cosine similarity over 64 independent runs.
As shown in Table~\ref{tab:cosine}, for CIFAR-10/100, the value of cosine similarity  gradually increases when  more augmentation layers are added and converges to  when five layers are added. Further adding more layers does not change the cosine similarity value.
For ImageNet, we have the similar observation where the cosine similarity value stops increasing when more than five augmentation layers are included.
These results again match our Top-1 test accuracies listed in Table~\ref{tab:cifar_layers} and Table~\ref{tab:imagenet_layers}.


\begin{table*}[!h]
\centering
\resizebox{1.0\textwidth}{!}
{
\begin{tabular}{@{}l|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} & 
\multicolumn{1}{c}{\textbf{1 layer}} & 
\multicolumn{1}{c}{\textbf{2 layers}} &
\multicolumn{1}{c}{\textbf{3 layers}} &
\multicolumn{1}{c}{\textbf{4 layers}} &
\multicolumn{1}{c}{\textbf{5 layers}} &
\multicolumn{1}{c}{\textbf{6 layers}}
\\ 
\midrule
\textbf{CIFAR-10/100} &  &  &  &  & \textbf{0.771}  \textbf{0.014} & \textbf{0.771}  \textbf{0.014} \\
\textbf{ImageNet} &  &  &  &  & \textbf{0.730}  \textbf{0.013} & \textbf{0.730}  \textbf{0.014}  \\
\bottomrule
\end{tabular}
}
\caption{The average cosine similarity over the course of searching for CIFAR-10/100 and ImageNet with  confidence interval calculated from 64 independent samples.}
\label{tab:cosine}
\end{table*}
\end{comment}


\vspace{1mm}
\textbf{Validity of Optimizing Gradient Matching with Regularization.}
To evaluate the validity of optimizing gradient matching with regularization, we designed a search-free baseline named ``DeepTA''. In DeepTA, we stack multiple layers of TA on the same augmentation space of DeepAA without using default augmentations. 
As stated in Eq.(\ref{eq:mean_minus_std}) and Eq.(\ref{eq:gradient_cosine_similarity2}), we explicitly optimize the gradient similarities with the average reward minus its standard deviation. 
The first term -- the average reward  -- \emph{encourages the direction of high cosine similarity}. 
The second term -- the standard deviation of the reward  -- acts as a regularization that \emph{penalizes the direction with high variance}. These two terms jointly maximize the gradient similarity along the direction with low variance. To illustrate the optimization trajectory, we design two metrics that are closely related to the two terms in Eq.(\ref{eq:mean_minus_std}): the mean value, and the standard deviation of the improvement of gradient similarity. The improvement of gradient similarity is obtained by subtracting the cosine similarity of the original image batch from that of the augmented batch. In our experiment, the mean and standard deviation of the gradient similarity improvement are calculated over 256 independently sampled original images. 


\begin{figure}[t]
    \begin{subfigure}[b]{0.325\linewidth}
    \centering
    \includegraphics[width=0.98\linewidth]{mean_cosine.png}
    \caption{Mean of the gradient similarity improvement}
    \label{fig:mean_traj}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\linewidth}
    \centering
    \includegraphics[width=0.98\linewidth]{std_cosine.png}
    \caption{Standard deviation of the gradient similarity improvement}
     \label{fig:std_traj}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{mean_accuracy.png}
    \caption{Mean accuracy over different augmentation depth}
     \label{fig:acc_traj}
    \end{subfigure}
\vspace{1mm}
\caption{{\small Illustration of the search trajectory of DeepAA in comparison with DeepTA on CIFAR-10.}}\label{fig:mean_std_acc}
\vspace{-0mm}
\end{figure}


As shown in Figure~\ref{fig:mean_std_acc}(a), the cosine similarity of DeepTA reaches the peak at the fifth layer, and stacking more layers decreases the cosine similarity. In contrast, for DeepAA, the cosine similarity increases consistently until it converges to \textit{identity} transformation at the sixth layer. 
In Figure~\ref{fig:mean_std_acc}(b), the standard deviation of DeepTA significantly increases when stacking more layers. In contrast, in DeepAA, as we optimize the gradient similarity along the direction of low variance, the standard deviation of DeepAA does not grow as fast as DeepTA. In Figure~\ref{fig:mean_std_acc}(c), both DeepAA and DeepTA reach peak performance at the sixth layer, but DeepAA achieves better accuracy compared against DeepTA. Therefore, we empirically show that DeepAA effectively scales up the augmentation depth by increasing cosine similarity along the direction with low variance, leading to better results.


\vspace{0mm}

\textbf{Comparison with Other Policies.}
In Figure \ref{fig:compare_policy} in Appendix~\ref{app:compare_policy}, we compare the policy of \DAA{} with the policy found by other data augmentation search methods including AA, FastAA and DADA. We have three interesting observations:
\begin{itemize}
    \item AA, FastAA and DADA assign high probability (over 1.0) on flip, Cutout and crop, as those transformations are hand-picked and applied by default. \DAA{} finds a similar pattern that assigns high probability on flip, Cutout and crop.
    \item Unlike AA, which mainly focused on color transformations, \DAA{} has high probability over both spatial and color transformations.
    \item FastAA has evenly distributed magnitudes, while DADA has low magnitudes (common issues in DARTS-like method). Interestingly, \DAA{} assigns high probability to the stronger magnitudes.
\end{itemize}


\begin{comment}
\begin{table*}[!h]
\centering
\resizebox{0.95\textwidth}{!}
{
\color{blue}
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} &  
\multicolumn{1}{c}{\textbf{Augmentation}} &
\multicolumn{1}{c}{\textbf{1 Layer}} &
\multicolumn{1}{c}{\textbf{2 Layers}} &
\multicolumn{1}{c}{\textbf{3 Layers}} &
\multicolumn{1}{c}{\textbf{4 Layers}} &
\multicolumn{1}{c}{\textbf{5 Layers}} &
\multicolumn{1}{c}{\textbf{6 Layers}} &
\multicolumn{1}{c}{\textbf{7 Layers}} &
\multicolumn{1}{c}{\textbf{8 Layers}}
\\ 
\midrule 
\multirow{2}{*}{CIFAR-10} & DeepTA & 96.3  0.14 & 96.72  0.16 & 97.18  0.07 & 97.19  0.10 & 07.18  0.17 & \textbf{97.22  0.13} & 97.11  0.15 & 97.06  0.1    \\
  & DeepAA &  96.3  0.21 & 96.6  0.18 & 96.9  0.12 & 97.4  0.14 & 97.56  0.14 & 97.6  0.12 & - & -  \\
\multirow{2}{*}{CIFAR-100} & DeepTA & 80.5  0.17 & 81.94  0.25 & 82.32  0.29 & 82.56  0.26 & 82.60  0.16 & 82.67  0.22 & 82.89  0.21 & 82.62  0.36  \\
 & DeepAA & 80.9  0.31 & 81.7  0.24 & 82.2  0.21 & 83.7  0.24 & 84.02  0.18 & 84.0  0.19 & - & -  \\
\bottomrule
\end{tabular}
}
\caption{{\small Comparison between DeepTA and DeepAA across layers.}}
\label{tab:parameters}
\end{table*}
\end{comment}



\begin{comment}
\subsection{Discussion}
We visualize the discovered augmentation policy found on CIFAR-10 data in Figure. \ref{fig:prob}. We found that that the first layer keeps a diverse set of transforms, while the 2-4th layers mainly converge to randCutout, randCrop and randFlip. More interestingly, the discovered policy in 2-4th layers coincides with the widely adopted default augmentation.
\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{layer_prob.png}
\caption{\textbf{The probability assigned to each transformation over at different layers.} (A) First layer augmentation: the distribution is spreaded across multiple augmentation operations. (B) Second layer: The augmentation converged to randCutout.  (C) Third layer: The augmentation converged to randCrop. (D) Fourth layer: The augmentation mainly concentrated to randFlip and randCutout.}
\label{fig:prob}
\end{center}
\end{figure}



In Figure \ref{fig:prob_magnitude}, we show the distribution of magnitude over each operation. It is clear seen that, in the first layer, the magnitudes are high for all operations. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{magnitude_dist.png}
\caption{\textbf{The distribution of magnitude for each operation \footnote{We do not show operatgions that do not have magnitude, which are identity, autoContrast, invert, equalize, randFlp, randCutout and randCrop}.} (A) First layer magnitude: the magnitudes are concentrated on both ends. (B-D) Second, third and fourth layer magnitudes: Except for solarize and posterize, the magnitude are small.}
\label{fig:prob_magnitude}
\end{center}
\end{figure}





\end{comment}
