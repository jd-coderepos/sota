\documentclass[11pt]{article}
\usepackage{bbding}
\fussy
\usepackage{pdfsync}
\usepackage{framed, xcolor}
\usepackage{tocvsec2}
\usepackage{datetime}
\usepackage{pifont}
\usepackage{pdflscape}
\usepackage{subfigure}          \usepackage{colortbl}
\usepackage{booktabs}
\usepackage{pdfsync}
\usepackage[font=scriptsize,bf]{caption}
\usepackage{tikz,subfigure}
\usepackage[active]{srcltx}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsfig,amssymb,amsfonts,amsmath,amsthm}
\usepackage{multirow}
\usepackage[numbers,sort&compress,sectionbib]{natbib}
\bibliographystyle{abbrvnat}
\newcommand{\tsum}{\textstyle\sum}
\newcommand{\tprod}{\textstyle\prod}
\usepackage{amsfonts}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{pstricks}
\usepackage{setspace}
\usepackage{xcolor}
\newcommand{\minitab}[2][l]{\begin{tabular}{#1}#2\end{tabular}}
\usepackage{rotating}
\usepackage{minitoc}





\usepackage{tikz}
\usepackage{enumerate}

\usepackage{hyperref}
\usetikzlibrary{chains,fit,shapes,arrows}
\usetikzlibrary{shapes.arrows}

\hypersetup{colorlinks=true, linkcolor=red}


\newcommand{\phiw}{\phi_{w}}
\newcommand{\epsI}{\eps_{I}}

\newcommand{\SDDM}{\mathrm{SDDM}}
\newcommand{\SDD}{\mathrm{SDD}}

\newcommand{\SPSD}{\mathrm{SPSD}}
\newcommand{\MDBD}{\mathrm{MDBD}}

\newcommand{\cC}{\mathcal{C}}

\newcommand{\clsGL}{\mathcal{GL}}
\newcommand{\GL}{\mathcal{T}}

\newcommand{\BNTap}{\mathcal{B}_{N,T}(\alpha,p)}
\newcommand{\GLB}{\mathrm{P}_{\mathcal{B}}}

\newcommand{\mGg}{\mathcal{G}_{\gamma}}

\newcommand{\mKLC}{\mathrm{\mathbf{mKLC}}}
\newcommand{\pKLC}{\mathrm{\mathbf{pKLC}}}
\newcommand{\iSS}{\mathrm{\mathbf{SS}}}

\newcommand{\mPS}{\mathrm{\mathbf{mPS}}}
\newcommand{\mSS}{\mathrm{\mathbf{mSS}}}
\newcommand{\fSS}{\mathrm{\mathbf{fSS}}}

\newcommand{\InitSS}{\mathrm{\mathbf{InitSS}}}
\newcommand{\SqrSS}{\mathrm{\mathbf{SqrSS}}}
\newcommand{\pSqrSS}{\mathrm{\mathbf{pSqrSS}}}
\newcommand{\IndSS}{\mathrm{\mathbf{IndSS}}}
\newcommand{\pIndSS}{\mathrm{\mathbf{pIndSS}}}

\newcommand{\PwrSS}{\mathrm{\mathbf{PwrSS}}}
\newcommand{\SSMDBD}{\mathrm{\mathbf{SS\_MDBD}}}
\newcommand{\pSSMDBD}{\mathrm{\mathbf{pSS\_MDBD}}}

\newcommand{\AppDscrPDF}{\mathrm{\mathbf{AppDscrPDF}}}

\newcommand{\pPS}{\mathbf{pPS}}
\newcommand{\PS}{\mathbf{PS}}
\newcommand{\STOVP}{\mathbf{STOVP}}


\newcommand{\wD}{\widetilde{D}}
\newcommand{\wA}{\widetilde{A}}
\newcommand{\wM}{\widetilde{M}}
\newcommand{\wO}{\widetilde{O}}

\newcommand{\hM}{\widehat{M}}
\newcommand{\hA}{\widehat{A}}
\newcommand{\hB}{\widehat{B}}
\newcommand{\hL}{\widehat{L}}
\newcommand{\hO}{\widehat{O}}
\newcommand{\hw}{\widehat{w}}

\newcommand{\Vp}{\mathbf{V}(p)}
\newcommand{\PK}[1]{\textcolor{blue}{#1}}

\newcommand{\Di}{D^{-1}}
\newcommand{\Dhp}{D^{1/2}}
\newcommand{\Dhm}{D^{-1/2}}

\newcommand{\mD}{\mathbf{D}}
\newcommand{\mL}{\mathbf{L}}
\newcommand{\mB}{\mathbf{B}}

\newcommand{\mBNp}{\mathbf{B}_{N}(p)}

\newcommand{\diag}{\mathrm{diag}}
\newcommand{\prm}{\prime}

\newcommand{\remove}[1]{}
\renewcommand{\textstyle}{}
\renewcommand{\d}{{\ensuremath{ \mathbf{d}}}}
\newcommand{\block}{\mathsf{Block}}
\newcommand{\periodic}{\mathsf{Periodic}}
\newcommand{\bitonic}{\mathsf{Bitonic}}
\newcommand{\ccc}{\mathsf{CCC}}
\newcommand{\Geo}{\mathsf{Geo}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\rot}{\mathrm{T}}
\newcommand{\ce}{\mathrm{e}}
\newcommand{\pat}{\mathcal{P}}
\newcommand{\gen}{\mathcal{G}}
\newcommand{\ext}{\mathsf{Ext}}
\newcommand{\cond}{\mathsf{Con}}
\newcommand{\calp}{\mathcal{P}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\gstar}{G^{\star}}
\newcommand{\Oh}{O}
\newcommand{\n}{1}
\newcommand{\taucont}{\tau_{\cont}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog}}
\renewcommand{\deg}{\mathrm{deg}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\Odd}{\mathsf{Odd}}
\newcommand{\eps}{\epsilon}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\OPT}{\operatorname{OPT}}
\newcommand{\sign}{\mathsf{sign}}
\newcommand{\de}{\operatorname{de}}
\newcommand{\even}{\mathsf{even}}
\newcommand{\odd}{\mathsf{odd}}
\newcommand{\parti}[1]{\operatorname{part}(#1)}
\newcommand{\subsec}[1]{\vspace{\baselineskip} \emph{#1} \vspace{\baselineskip}}

\newcommand{\PRG}{\textsf{PRGs}}
\newcommand{\UID}{\textsf{UID}}

\newcommand{\reg}{\mathrm{Reg}}

\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\mix}{\operatorname{mix}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\thmref}[1]{Theorem~\ref{thm:#1}}
\newcommand{\thmmref}[1]{Thm.~\ref{thm:#1}}
\newcommand{\thmrefs}[2]{Theorems~\ref{thm:#1} and~\ref{thm:#2}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
\newcommand{\lemrefs}[2]{Lemmas~\ref{lem:#1} and~\ref{lem:#2}}
\newcommand{\lemrefss}[3]{Lemmas~\ref{lem:#1},~\ref{lem:#2}, and~\ref{lem:#3}}
\newcommand{\corref}[1]{Corollary~\ref{cor:#1}}
\newcommand{\obsref}[1]{Observation~\ref{obs:#1}}
\newcommand{\defref}[1]{Definition~\ref{def:#1}}
\newcommand{\defrefs}[2]{Definitions~\ref{def:#1} and~\ref{def:#2}}
\newcommand{\assref}[1]{Assumption~\eqref{ass:#1}}
\newcommand{\conref}[1]{Conjecture~\ref{con:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\figrefs}[2]{Figures~\ref{fig:#1} and~\ref{fig:#2}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\secrefs}[2]{Sections~\ref{sec:#1} and~\ref{sec:#2}}
\newcommand{\charef}[1]{Chapter~\ref{cha:#1}}
\newcommand{\eq}[1]{\eqref{eq:#1}}
\newcommand{\eqs}[2]{equations~\eqref{eq:#1} and~\eqref{eq:#2}}
\newcommand{\eqss}[3]{equations~\eqref{eq:#1},~\eqref{eq:#2}, and~\eqref{eq:#3}}
\newcommand{\Eqs}[2]{Equations~\eqref{eq:#1} and~\eqref{eq:#2}}
\newcommand{\COOR}{\mathcal{PR}}
\newcommand{\pr}[1]{\mathbf{Pr} [\,#1\,]}
\newcommand{\Pro}[1]{\mathbf{Pr} \left[\,#1\,\right]}
\newcommand{\Prob}[2]{\mathbf{Pr}_{#1} \left[\,#2\,\right]}
\newcommand{\PRO}[1]{\widetilde{\mathbf{Pr}} \left[\,#1\,\right]}
\newcommand{\Proo}[1]{\mathbf{Pr}[\,#1\,]}
\newcommand{\e}{\mathbf{E}}
\newcommand{\ex}[1]{\mathbf{E}[\,#1\,]}
\newcommand{\Ex}[1]{\mathbf{E} \left[\,#1\,\right]}
\newcommand{\EX}[1]{\widetilde{\mathbf{E}} \left[\,#1\,\right]}
\newcommand{\EXX}[2]{\mathbf{E}_{#1} \left[\,#2\,\right]}
\newcommand{\Var}[1]{\mathbf{Var} \left[\,#1\,\right]}
\newcommand{\Cov}[1]{\mathbf{Cov} \left[\,#1\,\right]}
\newcommand{\degree}{\operatorname{deg}}
\newcommand{\girth}{\operatorname{girth}}
\renewcommand{\diam}{\operatorname{diam}}
\newcommand{\id}{\mathsf{id}}
\renewcommand{\tilde}{\widetilde}
\renewcommand{\eps}{\varepsilon}
\newcommand{\opt}{\mathrm{opt}}




\newcommand{\mycorollary}[2]{\begin{cor}\label{cor:#1}#2\end{cor}}
\newcommand{\againcorollary}[2]{\noindent\textbf{Corollary~\ref{cor:#1}}
    (from page \pageref{cor:#1})\textbf{.}\emph{#2}}

\newcommand{\mylemma}[2]{\begin{lem}\label{lem:#1}#2\end{lem}}
\newcommand{\mylem}[3]{\begin{lem}[#3]\label{lem:#1}#2\end{lem}}
\newcommand{\againlemma}[2]{\noindent\textbf{Lemma~\ref{lem:#1}}
    (from page \pageref{lem:#1})\textbf{.}\emph{#2}}
\newcommand{\againlem}[3]{\noindent\textbf{Lemma~\ref{lem:#1}}
    (#3)~(from page \pageref{lem:#1})\textbf{.}\emph{#2}}

\newcommand{\mytheorem}[2]{\begin{thm}\label{thm:#1}#2\end{thm}}
\newcommand{\mythm}[3]{\begin{thm}[#3]\label{thm:#1}#2\end{thm}}
\newcommand{\againtheorem}[2]{\noindent\textbf{Theorem~\ref{thm:#1}}
    (from page \pageref{thm:#1})\textbf{.}\emph{#2}}
\newcommand{\againthm}[3]{\noindent\textbf{Theorem~\ref{thm:#1}}
    (#3)~(from page \pageref{thm:#1})\textbf{.}\emph{#2}}

\newcommand{\myproposition}[2]{\begin{pro}\label{prop:#1}#2\end{pro}}
\newcommand{\againproposition}[2]{\noindent\textbf{Proposition~\ref{prop:#1}}
    (from page \pageref{prop:#1})\textbf{.}\emph{#2}}

\newcommand{\qedsymb}{\hfill{\rule{2mm}{2mm}}}

\newtheorem{problem}{Problem}
\newtheorem{thm}{Theorem}  \newtheorem{fact}[thm]{Fact}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{obs}[thm]{Observation}
\newtheorem{clm}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{fac}[thm]{Fact}
\newtheorem{rem}[thm]{Remark}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{prob}[thm]{Problem}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{construction}{Construction}
\newtheorem{protocol}{Protocol}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assumption}[thm]{Assumption}

\renewcommand{\qedsymbol}{\small }
\numberwithin{thm}{section}

\newcommand{\mat}[1]{\boldsymbol{\mathbf{#1}}}


\title{An Efficient Parallel Algorithm for Spectral Sparsification of Laplacian and SDDM Matrix Polynomials}




\author{
Gorav Jindal \qquad Pavel Kolev\footnote{This work has been funded by the Cluster of Excellence ``Multimodal Computing and Interaction" within the Excellence Initiative of the German Federal Government.}\\
Max-Planck-Institut f\"{u}r Informatik, Saarbr\"{u}cken, Germany\\
\{gjindal,pkolev\}@mpi-inf.mpg.de
}

\date{}


\begin{document}

\maketitle

\begin{abstract}
A mixture of discrete Binomial distributions (), denoted by , is a set of pairs , where  denotes the Binomial distribution, all  are distinct,  and all . A vector  is induced by  if  for all , where .

We prove for ``large'' class  of continuous probability density functions (p.d.f.), that for every  there exists  with  that -approximates a \emph{discretized} p.d.f.  for all , where . Moreover, we propose an efficient parallel algorithm that on input p.d.f.  and parameter , outputs  that induces a vector  which -approximates . Also, we give an efficient parallel algorithm that on input a discretized p.d.f.  induced by  with  and the corresponding vector , outputs exactly the coefficients .

Cheng et al.~\cite{CCLPT15} proposed the first sequential algorithm that on input a discretized p.d.f. ,  that is either Laplacian or  matrix and parameter , outputs in time \footnote{ notation hides  factors.} a spectral sparsifier of a matrix-polynomial . However, given   that induces a discretized p.d.f. , to apply the algorithm in~\cite{CCLPT15} one has to explicitly precompute in  time the vector . Instead, we give two algorithms (sequential and parallel) that bypass this explicit precomputation.

We propose a faster sequential algorithm that on input   with  for  outputs in  time the desired spectral sparsifier. Moreover, our algorithm is parallelizable and runs in  work and  depth. Our main algorithmic contribution is to propose the first efficient parallel algorithm that on input continuous p.d.f. , matrix  as above, outputs a spectral sparsifier of matrix-polynomial whose coefficients approximate component-wise the discretized p.d.f. .

Our results yield the first efficient and parallel algorithm that runs in nearly linear work and poly-logarithmic depth and analyzes the long term behaviour of Markov chains in non-trivial settings. In addition, we strengthen the Spielman and Peng's~\cite{PS14} parallel  solver by introducing a simple parallel preprocessing step.
\end{abstract}


\thispagestyle{empty}

\setcounter{page}{0}

\newpage
\tableofcontents
\newpage



\section{Introduction\label{sec:Intro}}

In their seminal work Spielman and Teng~\cite{ST14} introduced the
notion of spectral sparsifiers and proposed the first nearly linear
time algorithm for spectral sparsification. In consecutive work, Spielman
and Srivastava~\cite{SS08} proved that spectral sparsifiers with
 edges exist and can be computed
in \footnote{The  notation hides  factors.} time for any undirected graph . The computational bottleneck of their algorithm is to approximate
the solutions of logarithmically many \footnote{ is the class of symmetric and diagonally dominant matrices.} systems.

Recently, Koutis, Miller and Peng~\cite{KMP11} developed
an improved solver for  systems that works in 
time. In a survey result~\cite[Theorem 3]{KL13} Kelner and Levin showed that in  time all effective resistances can be approximated up to a constant factor. This yields a -spectral sparsifier with only a constant factor blow-up of non-zero edges . Although there are faster by a -factor sparsification algorithms~\cite{KLP12} they output spectral sparsifiers with -factor more edges.

Spielman and Peng~\cite{PS14} introduced the notion of
\emph{sparse approximate inverse chain} of \footnote{ is the class of positive definite  matrices with non-positive off-diagonal entries.} matrices. They proposed the first parallel algorithm that finds such chains and runs in work  and depth , where 
is the condition number of the  matrix with  non-zero entries and dimension . Furthermore, they showed that in 
 time a spectral sparsifier 
can be computed with .
In a follow up work, Cheng et al.~\cite{CCLPT14} designed an algorithm that computes a sparse approximate generalized chain  such that
 for any  matrix  and . The chain  is constructed iteratively and it involves a normalization step that produces a sparsifier  that is expressed in terms of the original diagonal matrix , for all iterations .

Sinclair and Jerrum~\cite{SJ89} analyzed Markov chains with transition matrices , corresponding to lazy random walks. They proved that these walks converge fast to stationary distribution, defined by , after \footnote{Graph conductance , where  and . } steps. Andersen et al.~\cite{ACL06} gave an efficient local clustering algorithm that relies on a lazy variation of PageRank, the transition matrix of which is defined by , where  is a parameter. Their local algorithm uses a truncated (finite summation) version of the preceding transition matrix.

Recently, Cheng et al.~\cite{CCLPT15} initiated the study of computing spectral sparsifiers  of random walk Laplacian matrix polynomials, where  is a probability distribution over ,  is a Laplacian matrix and  is a random walk transition matrix. These matrix polynomials capture the long term behaviour of Markov chains. Moreover, a sparsifier of a matrix polynomial yields a multiplicative approximation of the expected generalized ``escaping probability''~\cite{OT12,KS14} of random walks. Cheng et al.~\cite{CCLPT15} gave the first sequential algorithm that computes a spectral sparsifier of a random walk Laplacian matrix polynomial and runs in time  for some small constants .



\section{Our Results}

The lazy random walk length  in the regime of interest in~\cite{SJ89,ACL06,OT12} is of order . The quadratic runtime dependance on  makes the algorithm in~\cite{CCLPT15} prohibitively expensive for analysing the long term behaviour of Markov chains. In this paper, we overcome this issue for ``large'' class of probability distributions  over  that are induced by mixture of discrete Binomial distributions () with  for . Our results are summarized as follows.

In Subsection~\ref{subsec:RPMDBD}, we analyze the representational power of . In Subsection~\ref{subsec:SSMP}, we give a sequential and a parallel algorithm for computing a spectral sparsifier of matrix polynomials induced by . In Subsection~\ref{subsec:Apps}, we propose the first parallel algorithm that runs in nearly linear work and poly-logarithmic depth and analyzes the long term behaviour of Markov chains in non-trivial settings. In Subsection~\ref{subsec:FSSMS}, we strengthen the Spielman and Peng's~\cite{PS14} parallel  solver.


\subsection{Representational Power of }\label{subsec:RPMDBD}

Let  be Binomial distribution for some parameters  and .  is a set of pairs , denoted by , that satisfies the following two conditions:

1. (distinctness)  and  for all ;

2. (positive linear combination)  and  for all .


We prove in Section~\ref{sec:APMDBD} that for every function  in a ``large'' class of continuous p.d.f., there exists  that induces a component-wise approximation of .


\begin{thm}[ Yields a Component-Wise Approximation]\label{thmMDBD}
Let  be a four times differentiable p.d.f.,  a parameter and  an interval. Suppose there is an integer  and reals  and  such that:\\
1) , 2) ,3) ,\\
4)  ,5) ,\\
where the functions  are defined by 
and 
Then for every , any  and all  there is  such that

where  and .
\end{thm}

For every function  that satisfies the hypothesis in Theorem~\ref{thmMDBD} we associate   that is defined by  and  for all . Moreover, in Section~\ref{sec:approxDiscrPDFs} we give an efficient parallel algorithm that on input a continuous p.d.f.  (satisfying the conditions in Theorem~\ref{thm_AppDscrPDF}) and integer , outputs  that induces a discretized p.d.f. which approximates component-wise a desired discretized p.d.f. .

\begin{thm}[An Efficient Parallel Algorithm for Finding ]\label{thm_AppDscrPDF}
Let  be a p.d.f. that satisfies the hypothesis of Theorem~\ref{thmMDBD}. Suppose also ,\,\,\,\,,\,\,\,\,,\,\,\,\,\,\,\,\,. Then there is a parallel algorithm  that on input  as above, integer  and parameter , outputs in  work and  depth   that induces a discretized probability distribution  over  such that for all 

where the target discretized p.d.f. is defined by  for all , ,  and . Moreover, it holds that .
\end{thm}

In Appendix~\ref{appsec:AGPD}, we illustrate the representational power of  by applying Theorem~\ref{thm_AppDscrPDF} for two canonical continuous p.d.f.: the Uniform distribution and the Exponential Families.

\paragraph*{Exact Recovery}
Interestingly, in case when a discretized p.d.f.  is induced by   with exactly  distinct Binomial distributions, we give in Section~\ref{sec:StBVls} an efficient parallel algorithm that on input vectors  and , outputs the vector  in  work and  depth for some constant .

\begin{thm}[Canonical Instances Admit Exact Recovery]\label{thm_my_Inv_Bnp}
Suppose  is a vector
such that  for every  and  is a discretized p.d.f. that is induced by  
that satisfies 
for every . Then there is a parallel algorithm that
on input the vectors  and , outputs the vector  in  work and  depth, for some constant .
\end{thm}


\subsection{Spectral Sparsification of Matrix Polynomials induced by }\label{subsec:SSMP}

A matrix  is -matrix if it is either Laplacian or  matrix. To highlight that an algorithm  preserves the matrix type, we write that the algorithm  on input a -matrix  outputs a matrix  that is also -matrix.

Moreover, we say that a matrix  is a spectral sparsifier of a matrix  if it
satisfies , for short ,
where the partial relation  stands for  is symmetric
positive semi-definite (SPSD) matrix.

We denote by  or  the number of non-zero entries of matrix . When we write `` is -matrix'' we assume that  is positive diagonal matrix and . All algorithms presented in this paper output spectral sparsifiers \emph{with high probability}.

\paragraph{Sequential Algorithms}
Cheng et al.~\cite[Theorem 1.5]{arxivCCLPT15} gave an algorithm that on input a Laplacian matrix , even integer  and parameter , outputs in time  a spectral sparsifier  of a matrix-monomial such that . In Section~\ref{sec:CIA}, we give for  and  a -factor faster algorithm that computes a spectral sparsifier of -matrix monomials. Furthermore, for any -matrix  such that  is  matrix, we prove that the initial sparsification step dominates the algorithm's runtime.

\begin{thm}[Power Method for Monomials]\label{thmMyGLN}
There is an algorithm  that on input -matrix ,  for  and , outputs a spectral sparsifier
 that is -matrix with .
The algorithm runs in time

\end{thm}

Using Theorem~\ref{thmMyGLN}, we give in Section~\ref{sec:SSBGLPM} an algorithm
that runs by -factor faster than~\cite[Theorem 2]{CCLPT15} and computes a spectral sparsifier of a single Binomial -matrix polynomials of the form , where  and .

\begin{thm}[Single Binomial Matrix Polynomials]\label{thm_LazySS}
There is an algorithm 
that on input -matrix , number 
for , and parameters , outputs a spectral sparsifier

that is -matrix with at most  non-zero entries. The algorithm  runs in time

\end{thm}


In Section~\ref{sec:SSGLBM}, we give our main sequential algorithm that builds upon Theorem~\ref{thm_LazySS} and computes a spectral sparsifier of -matrix polynomials induced by .

\begin{thm}[Mixture of Binomial Matrix Polynomials]\label{thm_SS_MGL}
There is an algorithm  that on input -matrix , integer , ,   with  and parameter , outputs a spectral sparsifier  that is -matrix with  non-zero entries, where  is a discretized p.d.f. that satisfies  for all . The algorithm runs in time

\end{thm}


We motivate now the first conclusion of Theorem~\ref{thm_SS_MGL}. When  is a Laplacian matrix (of a graph with self-loops) associated with a Markov Chain with transition matrix  that corresponds to -lazy random walk process, it holds for  (c.f. \lemref{lemDDWWp}) that  is  matrix.

Given   that induces a vector , the algorithm in~\cite[Theorem 2]{CCLPT15} outputs a spectral sparsifier of the corresponding -matrix polynomial in time \footnote{ notation hides  factors.}, where the term  accounts for computing the vector . In comparison, our improved algorithm  runs in time  for any  with  for .


\paragraph{Parallel Algorithms}

Building upon the seminal works of Spielman and Teng~\cite{ST11}, Orecchia and Vishnoi~\cite{OV11} and Spielman and Peng~\cite{PS14}, we prove in Section~\ref{sec:effPrlAlg} that algorithm  can be efficiently parallelized. To the best of our knowledge, this is the first efficient and parallel algorithm that sparsifies -matrix polynomials induced by  with  for .


\begin{thm}[Efficient Parallel Spectral Sparsification of Matrix Polynomial induced by ]\label{thm_prl_SS_MGL}
There is a parallel algorithm  that on
input as in Theorem~\ref{thm_SS_MGL}, outputs a spectral sparsifier  that is -matrix with  for some constant , where  is a discretized p.d.f. such that  for all . The algorithm runs in work  and depth  for constants .
\end{thm}

By combining Theorem~\ref{thm_AppDscrPDF} and Theorem~\ref{thm_prl_SS_MGL}, we develop an efficient parallel algorithm that outputs a spectral sparsifier of a -matrix polynomial whose coefficients approximate component-wise a target discretized p.d.f. .

\begin{cor}[Approximating Target Transition Matrices]
There is a parallel algorithm that takes as input a continuous p.d.f.  satisfying the conditions of Theorem~\ref{thm_AppDscrPDF}, -matrix  and parameters , and it outputs in  work and  depth a spectral sparsifier  that is -matrix with  such that for all  it holds
.
\end{cor}


\subsection{Analyzing the Long Term Behaviour of Markov Chains}\label{subsec:Apps}

For many finite Markov chains~\cite{SJ89,ACL06,OT12,KS14} there exists  such that for every  certain phenomenon occurs with high probability - (local) mixing time, truncated PageRank, etc. Therefore, to analyze the long term behaviour of a finite Markov chain, it suffices to select the smallest  for  that is larger or equal to .

\begin{cor}[Capturing The Long Term Behaviour of Markov Chains]\label{cor:denseTmtx}
Suppose  is dense Laplacian matrix with , ,  is  such that , the degree  for  and the number of Binomials . Then algorithm  outputs in work  and depth  a spectral sparsifier  that is Laplacian matrix with  non-zero entries for some constants  and  is a probability distribution induced by the  .
\end{cor}


\paragraph*{Multiplicative Approximation of Generalized Escaping Probability}

Consider a Markov chain with transition matrix  that corresponds to a generalized random walk process of length . Perform a random walk of length  induced by  that starts at vertex . Then for any subset , the corresponding \emph{generalized escaping probability} is defined by , where we denote by  the characteristic vector of a subset .

We define the volume of  by  and let  be a probability distribution over  defined by  if  and  otherwise. The \emph{expected} generalized escaping probability (E.G.E.P.) with respect to  is defined by


We show in Appendix~\ref{appsec:GenEscProb} that a spectral sparsifier of a random walk Laplacian matrix polynomial, yields a multiplicative approximation of E.G.E.P. for all subsets .

\begin{lem}[Multiplicative Approximation of E.G.E.P.]\label{lem_mulApproxEGEP}
For any spectral sparsifier  of a random walk Laplacian matrix polynomial such that  is a probability distribution over , it holds for every subset  that

\end{lem}

Using Corollary~\ref{cor:denseTmtx} and Lemma~\ref{lem_mulApproxEGEP}, we propose the first efficient and parallel algorithm that runs in nearly linear work and poly-logarithmic depth that yields a multiplicative approximation of E.G.E.P. for Markov chains with transition matrices induced by .



\subsection{Faster  Solver}\label{subsec:FSSMS}

Spielman and Peng~\cite{PS14} gave the first parallel  solver that constructs in  work and  depth a sparse -approximate inverse chain that solves approximately to any  precision an  system in  work and  depth. In Section~\ref{sec:SDDMSolver}, we give a simple parallel preprocessing step that strengthens their algorithm.


\begin{thm}\label{thm_SDDM_Solver}
There is an algorithm that on input an -dimensional  matrix  with  non-zeros and condition number at most , produces with probability at least  a sparse -approximate inverse chain that can be used to solve any linear equation in  to any precision  in  work and  depth, for some constant . The algorithm runs in  work and  depth for some other constants .
\end{thm}

For the current state-of-the-art result on parallel  solvers we refer the reader to the work of Lee et al.~\cite{KLPSS15}.



\section{Algorithmic Background on Spectral Sparsification}\label{sec:BN}

We write  to indicate . Our analysis uses the following five basic facts (c.f.~\cite{ST14,BGHNT06}).

\begin{fact}\label{fact_fiveProps}
For positive semi-definite (PSD) matrices
 and  it holds

a. if  then ;

b. if  and  then ;

c. if  and 
then ;

d. if  and  are invertible matrices such that 
then , ;

e. for any matrix  if  then .
\end{fact}


\subsection{Prior Algorithms}

Our algorithms for computing spectral sparsifiers of matrix-polynomials use as a black-box several spectral sparsification algorithms for Laplacian and  matrices.

More precisely, our sequential algorithms build upon Theorem~\ref{thm_KL13} that relies on Kelner and Levin's~\cite[Theorem 3]{KL13} and Cohen et al.'s~\cite[Lemma 4]{CLMMPS15}, and Theorem~\ref{thm_PS14} proposed by Spielman and Peng~\cite[Corollary 6.4]{PS14}.

\begin{thm}\label{thm_KL13}\cite{KL13}
There is an algorithm  takes
as input parameter , matrices  and  such that  is positive diagonal and  is symmetric non-negative with  for all  such that  is Laplacian matrix. Then in  time outputs a positive diagonal matrix  and symmetric non-negative matrix  such that ,
 for all , and . Moreover,  is Laplacian matrix.
\end{thm}

\begin{thm}\label{thm_PS14}\cite{PS14}
There is an algorithm  that takes
as input  matrix  and parameter .
Then in  time outputs
a positive diagonal matrix  and symmetric non-negative
matrix  with 
and  for all , such that  and . Moreover,  is  matrix.
\end{thm}

Our parallel algorithm uses Theorem~\ref{thm_ParallelLaplacianSS}, which is the culmination of a research line conducted by Spielman and Teng~\cite{ST11}, Orecchia and Vishnoi~\cite{OV11} and Spielman and Peng~\cite{PS14}.

\begin{thm}\label{thm_ParallelLaplacianSS}\cite{PS14}
There is an algorithm  takes as input a Laplacian matrix  and parameter . Then it outputs a spectral sparsifier  with  for all  and  for some constant . Moreover, this algorithm requires  work and  depth, for some other constants  and .
\end{thm}

Based on Theorem~\ref{thm_ParallelLaplacianSS} Spielman and Peng~\cite{PS14} parallelized algorithm  (c.f. Theorem~\ref{thm_PS14}).

\begin{thm}\label{thm_pPS14}\cite{PS14}
There is a parallel algorithm that on input an  matrix  and parameter , outputs a spectral sparsifier  with ,  for all  and  for some constant . Moreover, this algorithm requires  work and  depth, for some other constants  and .
\end{thm}


\subsection{Spectral Sparsification of -Matrices}\label{subsec:SSGL}

We show that the algorithms  and  can be amended to produce -matrix sparsifiers that are in \emph{normalized form}, i.e. the sparsifiers are expressed in terms of the diagonal matrix  minus a symmetric non-negative matrix . Our analysis relies on several results established by Peng et al.~\cite{PengPhd13,PS14,CCLPT14,CCLPT15}.

\begin{lem}\label{lemSSDA}
There is an algorithm  that takes as input a positive diagonal matrix , symmetric non-negative matrix   such that  is Laplacian matrix and parameter . Then it outputs in 
time a spectral sparsifier  that is Laplacian matrix and satisfies  is symmetric non-negative matrix with .
\end{lem}

The next result implicitly appears in~\cite{CCLPT15}. For completeness we prove it in Appendix \ref{appsec:GLM}.

\begin{lem}\label{lem_my_DappToD}
Suppose  is Laplacian matrix  and  a sparsifier with  for every  such that .
Then the symmetric non-negative matrix 
satisfies .
\end{lem}

We present now the proof of Lemma \ref{lemSSDA}.

\begin{proof}[Proof of Lemma~\ref{lemSSDA}] Notice that , where  is positive diagonal matrix and  is symmetric non-negative matrix such that  for all .
By Theorem \ref{thm_KL13} we obtain a sparsifier . Then by Lemma \ref{lem_my_DappToD} we have  , where  is symmetric non-negative matrix. We define by  a non-negative diagonal matrix. Set  and observe that it is symmetric and non-negative matrix. Now the statement follows since .
\end{proof}

\paragraph*{-Matrices}
Building upon the work of Spielman and Peng~\cite[Proposition 5.6]{PS14} and Cheng et al.~\cite[Proposition 25]{CCLPT15}, we prove in Appendix~\ref{appsec:GLM} the following statement.

\begin{lem}[Closure]\label{lem_Closure}
Suppose  is -matrix. Then  is -matrix for every . Moreover, if  is a spectral sparsifier, then  is -matrix for every .
\end{lem}

\paragraph*{Normalized Algorithms} We present now two algorithms that sparsify matrices of the form  for  such that the resulting sparsifiers are in normalized form.


\begin{lem}[Normalized Spectral Sparsification]\label{lem_SS_GL}
There is an algorithm 
that takes as input -matrix  and parameter
, then it outputs in 
time a spectral sparsifier  that is -matrix and 
is symmetric non-negative matrix with .
\end{lem}

\begin{proof}
By definition  where  is non-negative diagonal matrix and  is Laplacian matrix. We obtain by Lemma \ref{lemSSDA} a sparsifier  that is Laplacian matrix. Now we consider two cases. If  then we are done. Otherwise  is PSD matrix and by Fact \ref{fact_fiveProps}.a we have . Since  is  matrix and the operator  preserves the kernel space, it follows that  is  matrix.
\end{proof}


We proceed by stating an interesting structural result that implicitly appears in~\cite{PS14} (c.f. Section ``Efficient
Parallel Construction''). For completeness we prove it in Appendix \ref{appsubsec:SR}.


\newcommand{\lemGLStruct}
{
Suppose  is -matrix. Let
 be a column vector,
 and 
numbers, and 
positive diagonal matrix for all , where . Let  be the th entry of a matrix with same dimensions as matrix  and  be a diagonal matrix.

Then it holds that
 where  is non-negative diagonal matrix,  is Laplacian matrix with
at most  non-zero entries and every  is Laplacian matrix corresponding to a clique with positively weighted edges that is induced by the neighbour set .
}
\mylemma{lemGLStruct}{\lemGLStruct}


Spielman and Peng~\cite{PS14} gave algorithm  (c.f. Theorem \ref{thm_PS14}) for sparsifying matrices of the form , where  is  matrix. We extend their result to -matrices and our algorithm outputs a spectral sparsifier in normalized form.

\begin{lem}[Normalized 2-Hops Spectral Sparsification]\label{lemmPS}
There is an algorithm  that on input a -matrix  and parameter , outputs in  time a spectral sparsifier  that is -matrix and  is symmetric non-negative matrix with .
\end{lem}


\begin{proof}
By Lemma \ref{lemSSDA} we have , where  is non-negative diagonal matrix and  is sum of Laplacian matrices. Using similar arguments as in ``Section 6 Efficient Parallel Construction''~\cite{PS14} we find a sparsifier . Moreover, we can compute the positive diagonal matrix  in  time (c.f. Appendix~\ref{appsubsec:SR}), and then by Lemma \ref{lem_my_DappToD} we obtain a sparsifier . Since  is PSD matrix the statement follows by Fact \ref{fact_fiveProps}.a.
\end{proof}



\section{Core Iterative Algorithm}\label{sec:CIA}

Our goal now is to prove Theorem~\ref{thmMyGLN}. We argue in a similar manner as in~\cite{arxivCCLPT15}, but in contrast our analysis shows that the initial sparsification step tolerates higher approximation error. This observation yields an improved algorithm whose runtime is faster by a -factor.

Moreover, we prove that for any -matrix  such that  is  matrix, one can construct a spectral sparsifier  by first computing  and then . This demonstrates that when  is  matrix, the runtime is dominated by the initial sparsification.

The rest of this section is organized as follows. In Subsection~\ref{subsec:Init_PrwSS} we describe the initial phase of algorithm . Then in Subsection~\ref{subsec:IterConst}, we present the iterative construction of the desired spectral sparsifier .


\subsection{Initialization}\label{subsec:Init_PrwSS}

We begin by extending~\cite[Lemma 4.3 and 4.4]{arxivCCLPT15}. For completeness, we provide a prove in Appendix~\ref{appsec:ASC} where in addition we generalize~\cite[Fact 4.2]{arxivCCLPT15}.


\newcommand{\lemSchurRec}
{
Suppose  is -matrix
and  is a spectral sparsifier.
If  is  matrix then it holds that .
}
\mylemma{lemSchurRec}{\lemSchurRec}


Based on \lemref{lemSchurRec}, we give a faster sparsification algorithm for -matrices  such that  is  matrix.

\begin{lem}\label{lem_my_GL}
There is an algorithm 
that takes as input -matrix  such that
 is  matrix, and parameter .
Then it outputs in 
time a spectral sparsifier 
that is -matrix with .
\end{lem}

\begin{proof}[Proof of Lemma~\ref{lem_my_GL}] We apply Lemma \ref{lem_SS_GL} to obtain
a sparsifier  in 
time with 
such that  is -matrix. Then by \lemref{lemSchurRec} we know that .
Now, by Lemma~\ref{lem_Closure} 
is -matrix. Then we apply Lemma~\ref{lemmPS}
to obtain in 
time a sparsifier 
with 
such that  is -matrix. The claims follows by Fact~\ref{fact_fiveProps}.c.
\end{proof}

\subsection{Iterative Construction}\label{subsec:IterConst}

Our analysis of the incurred approximation error after  consecutive square sparsification operations builds upon~\cite[Lemma 4.1]{arxivCCLPT15}. In contrast, we prove that for the initial and the final sparsifiers it suffices to have only an  approximation, while all intermediate spectral sparsifiers require finer  approximation. Due to this higher initial error tolerance, we improve the runtime of their algorithm by a -factor.

\begin{lem}[Accumulative Error]\label{lem_IndSS}
Let  and  be -matrices such that  and . There is an algorithm  that on input -matrix , integer  for  and parameter , outputs in time  a symmetric non-negative matrix  with

such that  is -matrix.
\end{lem}

Our goal now is to prove Lemma~\ref{lem_IndSS}. We establish next a useful algebraic property that all matrices of the form  have in common.

\begin{lem}\label{lem_DM_2k}
If  is symmetric matrix, then 
is  matrix for every .
\end{lem}

\begin{proof}
Let . Notice that ,
where . The statement follows since 
is  matrix.
\end{proof}

We present now the main iterative procedure used in algorithm .

\begin{lem}[Iterative Procedure]\label{lem_SqrSSREC}
Let  and  be -matrices such that , for . There is an algorithm 
that takes as input the -matrix 
and parameter , then it outputs in 
time a symmetric non-negative matrix  with

such that  is -matrix.
\end{lem}

\begin{proof}
By Lemma \ref{lem_DM_2k},  is 
matrix for any . By Lemma~\ref{lem_Closure} both  and 
are -matrices. Hence, by \lemref{lemSchurRec}
we have that . Now by Lemma~\ref{lemmPS} we have  and hence the statement follows by Fact~\ref{fact_fiveProps}.c.
\end{proof}

Based on the preceding results we are ready to prove Lemma~\ref{lem_IndSS}.

\begin{proof}[Proof of Lemma~\ref{lem_IndSS}]
By Theorem~\ref{thm_PS14} in time  we can compute a spectral sparsifier  with . Then we apply  times Lemma~\ref{lem_SqrSSREC} to obtain in  time a spectral sparsifier  with . The statement follows by applying Theorem~\ref{thm_KL13} with  to compute a refined spectral sparsifier of .
\end{proof}


\paragraph*{Proof of Theorem~\ref{thmMyGLN}}
In the initial phase we compute a sparsifier  with  using either Lemma~\ref{lem_my_GL} (when  is  matrix) or Lemma~\ref{lemmPS}. The statement follows by applying Lemma~\ref{lem_IndSS} with  to the sparsifier .



\section{Spectral Sparsification of Binomial -Matrix Polynomials}\label{sec:SSBGLPM}

In this section, we prove Theorem~\ref{thm_LazySS}. We analyze first the properties of matrices of the form  for . It is convenient to associate with them matrix-polynomials  such that . Since the coefficients of the matrix-polynomial  follow Binomial distribution , it follows that
 for every , where .

When  is a Laplacian matrix, the matrix  corresponds to the transition matrix of a -lazy random walk process of length  (c.f.~\cite{SJ89}). We associate to such a Markov chain a matrix-polynomial . We present now some useful algebraic properties of matrices of the form  and .

\newcommand{\lemDDWWp}
{
Suppose  is -matrix. Then
 is -matrix for every . Also 
is  matrix  and 
is  matrix  and .
}
\mylemma{lemDDWWp}{\lemDDWWp}

\begin{proof}
By definition of , we have  is -matrix. Suppose  is Laplacian matrix, then  is Laplacian matrix and by Lemma~\ref{lem_Closure},  is Laplacian matrix for every . Suppose now that  is  matrix, then  is  matrix and by Lemma \ref{lem_SDDM_Closure}  is  matrix for every .

By definition 
and since  is diagonally dominant, it holds that
 is  matrix for every .
Moreover, since  is symmetric matrix by Lemma \ref{lem_DM_2k} it holds that  is  matrix for every .
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm_LazySS}] The statement follows by \lemref{lemDDWWp} and Theorem~\ref{thmMyGLN}.
\end{proof}


\section{Spectral Sparsification of -Matrix Polynomials Induced by }\label{sec:SSGLBM}

Here we prove Theorem~\ref{thm_SS_MGL}. Our approach relies on the following key algorithmic idea.

\begin{lem}[Preprocessing of 2-Hop Spectral Sparsification]\label{lem_DMp2}
Let  be a -matrix,  and  are spectral sparsifiers. Then for every  the sparse matrix  yields a spectral sparsifier  that is -matrix.
\end{lem}

\begin{proof}
The statement follows by

\end{proof}

Our algorithm  builds upon Lemma~\ref{lem_DMp2} and Lemma~\ref{lem_IndSS}. Due to the preprocessing step in Lemma~\ref{lem_DMp2} we speed up the sparsification of each -matrix polynomial  for all . We present now the pseudo code of algorithm .

\begin{algorithm}[H]
\caption{}
\label{alg_SSMDBD}



1. Let ,  and  .

2. .\\
3. For every  do

3.1 Set  and .

3.2 
where  (c.f. Lemma~\ref{lem_IndSS}).

3.3 .

4. Sparsify 
by algorithm  (c.f. Lemma \ref{lem_SS_GL}).

5. Return .
\end{algorithm}


\begin{algorithm}[H]
\caption{}
\label{alg_Preprocess}



1. Sparsify  by algorithm  (c.f. Lemma \ref{lem_SS_GL}).

2. Sparsify 

2.1 If  is  matrix call algorithm  (c.f. Lemma~\ref{lem_my_GL}),

2.2 otherwise call algorithm  (c.f. Lemma~\ref{lemmPS}).

3. Return .
\end{algorithm}

Let . We denote a -matrix polynomial induced by   as
,
where . We are now ready to prove Theorem~\ref{thm_SS_MGL}.

\begin{proof}[Proof of Theorem \ref{thm_SS_MGL}] Let .
We perform first a preprocessing step. We apply Lemma \ref{lem_SS_GL}
to obtain a sparsifier .
Then depending on whether  is  matrix
we use either Lemma~\ref{lemmPS} or Lemma~\ref{lem_my_GL}
to obtain a sparsifier .
The run time is at most  or  respectively. Moreover, the sparsifiers satisfy .

We combine Lemma \ref{lem_DMp2} and Theorem~\ref{thmMyGLN}
to find each sparsifier 
by initializing algorithm  with a sparsifier .
Let , then by Fact~\ref{fact_fiveProps}.b it holds .
This phase has  runtime and each sparsifier satisfies .

However, matrix  can be dense. We
apply Lemma \ref{lem_SS_GL} to obtain a sparsifier 
in time 
such that .
\end{proof}

\section{Parallelization of Algorithm }\label{sec:effPrlAlg}

In this section we parallelize algorithm . This gives the first efficient parallel algorithm that computes a spectral sparsfier of any -matrix polynomial with coefficients induced by .

Our goal now is to prove Theorem~\ref{thm_prl_SS_MGL}. By construction of algorithms  and , it suffices to show that we can efficiently parallelize algorithms ,  and . Then the statement follows by noting that each  Binomial matrix-polynomial can be sparsified separately and in parallel.

We parallelize now algorithms ,  and .

\begin{lem}\label{lem_pKLC}
There is a parallel algorithm 
that on input -matrix  and parameter
, outputs a spectral sparsifier  that is -matrix such that 
is symmetric non-negative matrix with  for some constant . The algorithm runs in  work and  depth, for some other constants .
\end{lem}

\begin{proof}
We argue in a similar manner as in Lemma~\ref{lem_SS_GL} to show that the statement holds for -matrices (Laplacian or  matrices). Then the statement follows by Theorem~\ref{thm_ParallelLaplacianSS}.
\end{proof}

\begin{lem}\label{lem_pPS}
There is a parallel algorithm  that on input -matrix  and parameter , outputs a spectral sparsifier  that is -matrix such that ,  for all  and  for some constant . The algorithm runs in  work and  depth, for some other constants  and .
\end{lem}

\begin{proof}
Using similar arguments as in Lemma~\ref{lemmPS} we prove that the statement holds for -matrices. Then the statement follows by Theorem~\ref{thm_pPS14}.
\end{proof}

\newcommand{\lempSqrSSREC}
{
Let  and 
are -matrices such that , for . There is a parallel algorithm 
that on input the -matrix 
and parameter , outputs a spectral sparsifier  that is -matrix with  for some constant . The algorithm runs in work  and depth , for some other constants .
}
\mylemma{lempSqrSSREC}{\lempSqrSSREC}

\begin{proof}
We use similar arguments as in Lemma~\ref{lem_SqrSSREC}, but we substitute Lemma~\ref{lemmPS} with Lemma~\ref{lem_pPS}.
\end{proof}

\newcommand{\lempIndSS}
{
Let  and 
are -matrices such that  and  for some constant . There is a parallel algorithm  that on input -matrix , integer  for  and parameter , outputs a spectral sparsifier  that is -matrix and . The algorithm runs in work  and depth  for some constants .
}
\mylemma{lempIndSS}{\lempIndSS}

\begin{proof}
We argue in a similar manner as in Lemma~\ref{lem_IndSS}. By Lemma~\ref{lem_pKLC} we compute a sparsifier  with  in work  and depth . Then we apply  times \lemref{lempSqrSSREC} to obtain a spectral sparsifier  with  in  work and  depth. The statement follows by applying Lemma~\ref{lem_pKLC} with  to compute a refined spectral sparsifier of .
\end{proof}

We present now the proof of Theorem~\ref{thm_prl_SS_MGL} which yields the parallel algorithm .

\begin{proof}[Proof of Theorem~\ref{thm_prl_SS_MGL}]
We sketch first our parallel algorithm . We parallelize algorithm  based on algorithms  and . Then, we sparsify separately and in parallel each of the  distinct single Binomial -matrix polynomials by algorithm . The resulting  sparsifiers are scaled and merged into a -matrix polynomial induced by . Since this matrix-polynomial might be dense, we sparsify it using algorithm .

The correctness of algorithm  follows by Theorem~\ref{thm_SS_MGL}. We analyze now the work and the depth of algorithm . By Lemma~\ref{lem_pKLC} and Lemma~\ref{lem_pPS} the initial phase is dominated by  work and  depth. Moreover, each of the sparsifiers  and  has at most  non-zero entries.

Let . By \lemref{lempIndSS} for each  we apply algorithm  to compute a spectral sparsifier  with . This phase runs in work  and depth . Furthermore, the linear combination  (in Step 3.3) can be computed in depth .

Therefore, we approximate by  the desired -matrix polynomial induced by . Since matrix  might be dense, by Lemma~\ref{lem_pKLC} we compute a spectral sparsifier  with . Algorithm  runs in work
 and depth .
\end{proof}


\section{Faster  Solver}\label{sec:SDDMSolver}

In this section we prove Theorem~\ref{thm_SDDM_Solver}. We argue in a similar manner as in~\cite{PS14}, but in contrast our improved analysis relies on the refined initialization phase developed in Section~\ref{sec:CIA} and its consecutive parallelization in Section~\ref{sec:effPrlAlg}. Spielman and Peng's~\cite{PS14} proof involves two major steps: the first is to construct a sparse -approximate inverse chain, and the second is to apply this chain as a preconditioner into an algorithm known as ``Preconditioned Richardson Iteration''~\cite[Lemma 4.4]{PS14}.

We give now an improved construction for a sparse -approximate inverse chain. This directly implies the desired statement of Theorem~\ref{thm_SDDM_Solver}.

\begin{proof}[Proof of Theorem~\ref{thm_SDDM_Solver}]

We apply Lemma \ref{lem_pKLC} to compute a sparsifier  with  in work  and depth . Then by Fact~\ref{fact_fiveProps}.d we have .

Our goal now is to construct a sparse -approximate inverse chain of the sparsifier . The condition number of matrix  satisfies . Let ). Spielman and Peng~\cite{PS14} proved that  iterations suffice for the following iterative procedure to output a sparse -approximate inverse chain.

By Lemma~\ref{lem_pPS} we compute a spectral sparsifier  with  in work  and depth . By Lemma~\ref{lem_Closure} and \lemref{lempSqrSSREC} for each consecutive call we compute a sparsifier  with the at most  non-zero entries in work  and depth .

We combine now Fact \ref{fact_fiveProps} and apply recursively  times the relation

Spielman and Peng showed in~\cite[Corollary 5.5]{PS14} that  can be replaced with  for  and maintain the desired approximation. The statement follows by Fact~\ref{fact_fiveProps}.
\end{proof}



\section{Representational Power of }\label{sec:APMDBD}

In this section we prove Theorem~\ref{thmMDBD}. Our analysis relies on the following three influential works. Hald~\cite{Hald68} analyzed mixed Binomial distributions in continuous case. Cruz-Uribe and Neugebauer~\cite{UN03,CUN02} gave sharp guarantees for approximating integrals using the Trapezoid method. Doha et al.~\cite{DBS11} proved a simple closed formula for higher order derivatives of Bernstein basis.

Our goal now is to prove Theorem~Theorem~\ref{thmMDBD}. Hald~\cite{Hald68} proved the following result on mixed Binomial distributions.

\begin{thm}\cite{Hald68}\label{thm_Hald68}
Let  be a probability
density function that is four times differentiable. Then for every 
and  the Bernstein basis 
satisfies

where the functions are defined by 
and 
\end{thm}


We distinguish two types of approximation errors. The error term  (c.f. Equation~\ref{eq:fin_Approx} in Theorem~\ref{thmMDBD}) is caused by the error introduced in Equation \ref{eq:int_MDBD}. The second error type is due to the integral discretization with finite summation. The later approximation error in analyzed by Cruz-Uribe and Neugebauer~\cite{UN03,CUN02}. We summarize below their result.

\begin{thm}\cite{CUN02,UN03}\label{thm_approx_Def_Intergral}
Suppose  be continuous
and twice differentiable function,  is number, and the discrete
approximator of  is defined by .
Then the approximation error is given by the expression
,
where
 and .
\end{thm}


The rest of this section is devoted to prove Theorem~\ref{thmMDBD}. We use the following two results established by Cruz-Uribe and Neugebauer, and Doha et al.

\begin{lem}
\label{lem_int_n1}~\cite{CUN02,UN03} The Bernstein basis satisfies  for every .
\end{lem}

\begin{lem}
\label{lem_pder_Bni}~\cite{DBS11} The th derivative of a Bernstein
basis satisfies for every  that

\end{lem}


We propose an upper bound on the integral of th order derivative of Bernstein basis.
\begin{cor}
\label{cor_integral_derBB} For every  and
 such that ,
the Bernstein basis satisfies

\end{cor}

\begin{proof}
We combine Lemma \ref{lem_int_n1} and Lemma \ref{lem_pder_Bni} to obtain

\end{proof}


We are now ready to prove Theorem~\ref{thmMDBD}.

\begin{proof}[Proof of Theorem~\ref{thmMDBD}]
Recall that . By Theorem \ref{thm_approx_Def_Intergral} we have

Since ,
we consider following three cases:\\
\textbf{Case 1:} We combine 
and Lemma \ref{lem_int_n1} to obtain

\textbf{Case 2:} Using 
and Corollary \ref{cor_integral_derBB} it holds

\textbf{Case 3:} Combining 
and Corollary \ref{cor_integral_derBB} yields

The desired result follows from the preceding three cases and Theorem~\ref{thm_Hald68}.
\end{proof}


\section{Approximating Discretized PDF}\label{sec:approxDiscrPDFs}

In this section we prove Theorem~\ref{thm_AppDscrPDF}. We begin our discussion by presenting the pseudo code of algorithm .

\begin{algorithm}[H]
\caption{Approximate Discretized PDF by MDBD}




1. Compute  and , where .

2. Compute ,
where  for all .

3. Return .
\end{algorithm}


Before we prove Theorem~\ref{thm_AppDscrPDF}, we analyze the class of continuous probability density functions that admit a discretized approximation by .

\begin{lem}
\label{lem_RatioLambda} Let  be a twice differentiable p.d.f. such that for  it holds ,\,\,\,\,\,\,,\,\,\,\,\,\,,\,\,\,\,\,\,\,\,\,\,\,\,.
Then, for  with  it holds

\end{lem}

\begin{proof}
By Theorem~\ref{thm_approx_Def_Intergral} for the discrete approximator of 

it holds that

and similarly

By definition, 
and thus

Let . Straightforward checking shows that

We prove now the upper bound. By assumption  and since  we have

We can prove the lower bound  using similar arguments.
\end{proof}


We present now the proof of Theorem~\ref{thm_AppDscrPDF}.

\begin{proof}[Proof of Theorem~\ref{thm_AppDscrPDF}]
By definition ,  and the desired discretized p.d.f. is

By Theorem~\ref{thmMDBD}, it holds for all  that

We construct now   as follows: for all  we set

Moreover,  induces a vector  that satisfies  for all . By multiplying Equation \ref{eq:LAD} with  we obtain

Furthermore, since

by Lemma \ref{lem_RatioLambda} there is a small positive number 
such that

Hence, we have

Since , by Equation~\ref{eq:approxGamma} it follows that  is a discretized probability distribution over  that approximates component-wise the desired discretized p.d.f. .

We note that the summations  and  can be computed in  work and  depth. Hence, the statement follows.
\end{proof}


\section{Efficient Parallel Solver for Transpose Bernstein-Vandermonde Systems}\label{sec:StBVls}

\begin{problem}\label{prob_ExN_Bnk}
Suppose a vector  is
induced by a convex combination of exactly  discrete Binomial
distributions  such that  for all .
Find the unique vector  such that 
for all .
\end{problem}

The Bernstein basis is a well studied primitive in the literature for polynomial interpolations~\cite{UN03,CUN02}. It is defined by 
for any . Let  be  with . Then the Bernstein basis matrix is defined by , and it has a full rank (c.f. Appendix \ref{appsec:BBM}). Moreover, the vector  is the unique solution of the linear system .


In this section, we give an efficient parallel algorithm that solves Problem~\ref{prob_ExN_Bnk} and works in nearly linear work and poly-logarithmic depth. Our goal now is to prove Theorem~\ref{thm_my_Inv_Bnp}. We reduce a transpose Bernstein-Vandermonde system to a transpose Vandermonde system that can be solved efficiently and in parallel by a variation of an algorithm proposed by Gohberg and Olshevsky~\cite{GO94}. We present now their main algorithmic result.

\begin{thm}\cite{GO94}\label{thm_inv_VT}
There is an algorithm that on input two vectors
 such that  for all
, outputs the vector  in  time.
\end{thm}

Theorem~\ref{thm_inv_VT} follows by~\cite[Algorithm 2.1]{GO94} which relies on a non-trivial matrix decomposition of  to compute in  time the desired  matrix-vector product. It can be easily verified that~\cite[Algorithm 2.1]{GO94} can be amended to compute the vector  in  time. Furthermore, straightforward checking shows that this modified algorithm can be easily parallelized. We summarize below the resulting parallel algorithm.


\begin{thm}\cite{GO94}\label{thm_inv_pl_VT}
There is a parallel algorithm that on input two vectors  as in Theorem \ref{thm_inv_VT}, outputs the vector  in  work and  depth, for some constant .
\end{thm}

We prove now that the Bernstein basis matrix  admits the following decomposition.

\begin{lem}
\label{lem_BB_DVpD} Suppose  is vector
such that  for all , 
is Vandermonde matrix defined by ,

and 
are positive diagonal matrices. Then it holds that .\end{lem}
\begin{proof}
By definition .
\end{proof}

We are ready now to prove Theorem \ref{thm_my_Inv_Bnp}.

\begin{proof}[Proof of Theorem~\ref{thm_my_Inv_Bnp}] By Lemma \ref{lem_full_Rank} the Bernstein matrix  is invertible. Given a vector
 we want to find the vector .
By Lemma \ref{lem_BB_DVpD} we have . Moreover, we can compute a vector 
in  time. Using Theorem \ref{thm_inv_pl_VT}, we obtain a vector
 
in  work and  depth. The desired vector 
takes further  work and  depth to compute.
\end{proof}


\subparagraph*{Acknowledgements}

We are grateful to Arijit Ghosh and Kunal Dutta for the helpful discussions on mixture of Binomial distributions, and to Arnur Nigmetov and Shay Moran for pointing us to the Bernstein interpolation polynomials. We would also like to thank Kurt Mehlhorn for the insightful comments and suggestions to the early version of the manuscript.

This work has been funded by the Cluster of Excellence ``Multimodal Computing and Interaction'' within the Excellence Initiative of the German Federal Government.










\bibliography{reference}












\appendix

\section{Generalized Escaping Probability}\label{appsec:GenEscProb}

\begin{proof}[Proof of Lemma~\ref{lem_mulApproxEGEP}]
By definition, for every vector  the spectral sparsifier  preserves approximately the quadratic form

Hence, the statement follows by applying the identities

\end{proof}



\section{Spectral Sparsification of -Matrices}\label{appsec:GLM}


Our proof of Lemma \ref{lem_SDDM_Closure} is based on the Perron-Frobenius Theorem~\cite{M73} for non-negative matrices.

\begin{thm}\label{thm_PF_NNM}\cite[Perron-Frobenius]{M73}
Suppose  is symmetric nonnegative matrix. Then it has a nonnegative eigenvalue  which is greater than or equal to the modulus of all other eigenvalues.
\end{thm}

\begin{lem}\label{lem_SDDM_Closure}
Suppose  is  matrix. Then  is  matrix .
\end{lem}

\begin{proof}
Since  is  we have . By Fact \ref{fact_fiveProps}.e it holds . Hence, the largest eigenvalue . By Theorem \ref{thm_PF_NNM} the spectral radius , i.e.  for all . Since  is symmetric it has the form . Moreover, we have  for every . Thus the spectral radius of  satisfies .

Notice that  is symmetric non-negative matrix for every . By definition  is diagonally dominant and thus  component-wise. This implies that  is diagonally dominant matrix. Notice that  and since  it follows that  is positive definite and hence  matrix.
\end{proof}

\paragraph*{Proof of Lemma~\ref{lem_Closure}} We combine Lemma~\ref{lem_SDDM_Closure} with the following two statements.

\begin{fact}\label{fact_Lap_SDDM}
Spielman and Peng~\cite[Proposition 5.6]{PS14} showed that if  is  matrix, then  is  matrix. Also Cheng et al.~\cite[Proposition 25]{CCLPT15} showed that if  is Laplacian matrix then  is Laplacian matrix for every .
\end{fact}

Based on Lemma~\ref{lem_SS_GL} and Fact~\ref{fact_Lap_SDDM} we establish the following result.

\begin{lem}\label{lem_Lap_SDDM_Sparsifiers}
Suppose  is -matrix and  is a spectral sparsifier. Then  is -matrix for every .
\end{lem}


\paragraph*{Proof of Lemma~\ref{lem_my_DappToD}} We use the following result that appears in Peng's thesis~\cite{PengPhd13}.


\begin{lem}
\cite{PengPhd13}\label{lem_DA}
Suppose  is Laplacian matrix
, and 
a sparsifier with  for every  such that
.
Then the symmetric non-negative matrix 
satisfies .
\end{lem}


\begin{proof}[Proof of Lemma~\ref{lem_my_DappToD}]
Let 
and . Then 
and by Lemma \ref{lem_DA} the symmetric non-negative matrix 
satisfies 
Since  for every 
the statement follows.
\end{proof}





\subsection{Structural Result}\label{appsubsec:SR}

Suppose  is -matrix. We show that the matrix  can be expressed as a sum of a non-negative main diagonal matrix and a sum of Laplacian matrices.

\begin{proof}[Proof of \lemref{lemGLStruct}]
Let  and . We decompose the entries of matrix  into three types. We set type  to be the entries  for all . We note that all entries of type  can be computed in  time. We consider next the off-diagonal entries

Observe that the number of type  entries is at most . Now for a fixed  we note that the corresponding entries that appear in type  and type  form
a weighted clique (with self-loops) whose adjacency matrix is defined by .


Straightforward checking shows that .
By Lemma~\ref{lem_Closure}  is -matrix and thus diagonally dominant. Hence, the Laplacian matrices  and  for all  exist. Moreover, we can compute in  time the positive diagonal matrices  and  for all . To see this, observe that  and  can be computed in  time for all , and the number of elements in the disjoint union .
\end{proof}


\section{Bernstein Basis Matrix}\label{appsec:BBM}

We prove below that the Bernstein basis matrix in Problem \ref{prob_ExN_Bnk} has full rank.

\begin{lem}
\label{lem_full_Rank}
Suppose a vector  satisfies
 for all . Then the Bernstein
basis matrix  has a full rank.
\end{lem}

\begin{proof}
Suppose for contradiction that .
Then there is a vector  such that the
linear combination of the columns of 
satisfies .
Let  be a polynomial defined by .
Notice that  for every ,
i.e.  has  roots. However, since
the polynomial  has degree  it follows
that . Therefore, we obtained the desired
contradiction.
\end{proof}



\section{Approximating Two Canonical PDFs}\label{appsec:AGPD}

Here, we illustrate the representational power of . We show that there are  satisfying the hypothesis in Theorem~\ref{thm_AppDscrPDF} and approximate two canonical continuous p.d.f.: the Uniform distribution and the Exponential Families. More precisely, we prove that the Uniform distribution and the Exponential families admit a multiplicative and an additive approximation, respectively.

\subsection{Uniform Distribution}

\begin{lem}[Uniform Distribution]\label{lem_Approx_Bni}
Let ,  and . If  then it holds that

\end{lem}

\begin{proof}
By Theorem \ref{thm_approx_Def_Intergral} we have that

By combining Lemma~\ref{lem_int_n1} and Corollary \ref{cor_integral_derBB} for every 
it holds

We note that  since , and hence the statement follows.
\end{proof}

\begin{rem}
All conditions of Theorem~\ref{thm_AppDscrPDF} hold. Note that , i.e., .\\
a) , b) , c)  and d) , since .
\end{rem}


\subsection{Exponential Families}

\begin{lem}[Exponential Families]\label{lem_approxExpFam}
Let ,  and 
is a probability density function. For any  if  then for every  it holds for the function
 that

\end{lem}

\begin{proof}
The th derivative of function  satisfies . Let  be an interval. Straightforward checking shows that

By the definition of function  (c.f. Theorem~\ref{thm_Hald68}) we have

and we can show that . The function  satisfies

and we can show that . By Theorem~\ref{thmMDBD} it suffices to upper bound the following four cases.\\
\\
\textbf{Case 1:} By Theorem \ref{thm_Hald68} , since 
and .\\
\\
\textbf{Case 2:} We combine Equation \ref{eq:pthDerW}
and Lemma \ref{lem_int_n1} to obtain

\textbf{Case 3:} By combining Equation \ref{eq:pthDerW} and Lemma \ref{cor_integral_derBB} it holds

\textbf{Case 4:} We use again Equation \ref{eq:pthDerW} and Lemma \ref{cor_integral_derBB} to obtain

Recall that . By combining  and Theorem \ref{thm_approx_Def_Intergral} we have

Hence, the statement follows.
\end{proof}

\begin{rem}
The hypothesis in Theorem~\ref{thm_AppDscrPDF} holds. Note that  and . Thus  , ,  and . Furthermore,\\
a) , b) , c)  and for d) we have

\end{rem}





\section{Schur Complement}\label{appsec:ASC}


In this section we prove \lemref{lemSchurRec}. We use the following result proposed by Peng et al.~\cite{arxivCCLPT15}.

\begin{lem}
\label{lem_DPM}\cite[Lemma 4.3]{arxivCCLPT15} If  is \emph{}
matrix and 
then it holds that 
\end{lem}

We extend next two technical results on Schur complement that appeared in~\cite{PengPhd13,MP13,CCLPT15}.


\begin{clm}\label{clm_Schur}
Suppose  where  and  are symmetric positive definite matrices
and  is symmetric matrix. Then   for every .
\end{clm}

\begin{proof}
Suppose . Notice that  is minimized when , since .
Hence, it follows that 
\end{proof}

\begin{lem}
\label{lem_Schur}(Schur Complement) Suppose  are positive main diagonal matrices
and  are symmetric matrices. If  then it holds that .
\end{lem}

\begin{proof}
Here we show the upper bound, but
the lower bound follows by analogy. Let  be a vector such that
. We apply twice Claim \ref{clm_Schur}
to obtain the following chain of inequalities

\end{proof}


We prove \lemref{lemSchurRec} by arguing in a similar manner to in~\cite[Lemma 4.4]{arxivCCLPT15}. We present the proof here for completeness.

\begin{proof}[Proof of \lemref{lemSchurRec}] For any symmetric matrix , we denote
by . We prove the upper bound, but the lower bound follows by analogy.
Straightforward checking shows that

By Lemma \ref{lem_DPM} it holds that 
and thus we have

Hence, it follows that . Now, by Lemma \ref{lem_Schur} it holds that .
\end{proof}


\end{document} 