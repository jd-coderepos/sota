\section{Experimental Results}
\label{sec:results}
\newlength\ftqa
\setlength\ftqa{2.0cm}
\newlength\ftqb
\setlength\ftqb{0.2mm}
\newlength\ftqc
\setlength\ftqc{0.8mm}

\begin{figure*}[t]

\includegraphics[width=\ftqa]{figure_VCOCO/look_000000153717_329.jpg}\hfill \includegraphics[width=\ftqa]{figure_VCOCO/eat_000000550714_0.jpg}\hfill \includegraphics[width=\ftqa]{figure_VCOCO/eat_000000144863_0.jpg}\hfill \includegraphics[width=\ftqa]{figure_VCOCO/hit_000000543570_199.jpg}\hfill \includegraphics[width=\ftqa]{figure_VCOCO/hold_000000012991_50.jpg}\hfill \includegraphics[width=\ftqa]{figure_VCOCO/hold_000000427615_170.jpg}\vspace{\ftqb}\\\includegraphics[width=\ftqa]{figure_HICO/ride_000000003622_257.jpg}\hfill \includegraphics[width=\ftqa]{figure_HICO/ride_000000006887_149.jpg}\hfill \includegraphics[width=\ftqa]{figure_HICO/ride_000000007873_207.jpg}\hfill \includegraphics[width=\ftqa]{figure_HICO/ride_000000001551_241.jpg}\hfill \includegraphics[width=\ftqa]{figure_HICO/ride_000000003086_218.jpg}\hfill \includegraphics[width=\ftqa]{figure_HICO/ride_000000005048_214.jpg}\caption{\textbf{Sample HOI detections on V-COCO (first row) and HICO-DET (second row) \emph{test} set.}
\label{fig:visual}
}
\end{figure*}
\vspace{-3.0mm}






 In this section, we first outline our experimental setup, including datasets, metrics, and implementation details. 
We then report the quantitative results on two large-scale HOI benchmark datasets and compare the performance with the state-of-the-art HOI detection algorithms.
Next, we show sample visual results on HOI detection. 
We conduct a detailed ablation study to quantify the contributions from individual components and validate our design choices.
More results can be found in the supplementary material.
We will make the source code and pre-trained models publicly available to foster future research.

\begin{table}[t]
\centering
\caption{\tb{Comparison with the state-of-the-art on the V-COCO  set.} The best performance is in \first{bold} and the second best is \second{underscored}. Character  indicates that the method uses both VCOCO and HICO-DET training data.
``S-S only'' shows the performance of our spatial-semantic stream.
}
\label{tab:vcoco_comparison}
{
\begin{tabular}{lccl|c}
\toprule
Method & Use human pose &&  Feature backbone & \textbf{} \\
\midrule
VSRL~\cite{Gupta-SemanticRoleLabeling}              & - && ResNet-50-FPN    & 31.8 \\
InteractNet~\cite{Gkioxari-CVPR-InteractNet}        & - && ResNet-50-FPN    & 40.0 \\
BAR-CNN~\cite{Kolesnikov-BAR}                       & - && Inception-ResNet & 41.1 \\
GPNN~\cite{Qi-ECCV-GraphParsing}                    & - && ResNet-101       & 44.0 \\
iCAN~\cite{Gao-BMVC-iCAN}                           & - && ResNet-50        & 45.3 \\
Wang et al. ~\cite{wang2019deep} & - && ResNet-50 & 47.3 \\
RPNN~\cite{zhou2019relation} & \checkmark && ResNet-50 & 47.5 \\
~\cite{Li-CVPR-Interactiveness} & \checkmark && ResNet-50        & 48.7 \\
PMFNet \cite{Bo-PMFNet}                             & - && ResNet-50-FPN    & 48.6 \\
PMFNet \cite{Bo-PMFNet}                             & \checkmark && ResNet-50-FPN    & \first{52.0} \\
Ours (S-S only)                               & - & & -           & 47.1 \\
Ours                                                & - && ResNet-50-FPN    & \second{51.0} \\
\bottomrule
\end{tabular}
}
\end{table} 
\subsection{Experimental setup}
\para{Datasets.}
\tb{V-COCO dataset}~\cite{Gupta-SemanticRoleLabeling} is constructed by augmenting the COCO dataset~\cite{Lin-ECCV-MSCOCO} with additional human-object interaction annotations.
Each person is annotated with a binary label vector for 29 different action categories (five of them do not involve associated objects).
\tb{HICO-DET}~\cite{Chao-CVPR-HICO} is a larger dataset containing 600 HOI categories over 80 object categories (same as \cite{Lin-ECCV-MSCOCO}) with more than 150K annotated instances of human-object pairs.
For applying our method on the HICO-DET dataset, we disentangle the 600 HOI categories into 117 object-agnostic action categories and train our network over these 117 action categories.
At test time, we then combine the predicted action and the detected object and convert them back to the original 600 HOI classes.
Note that the evaluation for the HICO-DET dataset remains the same.


\para{Evaluation metrics.}
To evaluate the performance of our model, we adopt the commonly used role mean average precision (role mAP)~\cite{Gupta-SemanticRoleLabeling} for both V-COCO and HICO datasets.
The goal is to detect and correctly predict the    triplet.
We consider a triplet as true positive if and only if it localizes the human and object accurately (\ie with IoUs  w.r.t the ground truth annotations) and predicts the action correctly.


\begin{table*}[t]
\centering
\caption{\tb{Comparison with the state-of-the-art on HICO-DET \emph{test} set.} The best performance is in \first{bold} and the second best is \second{underscored}. Character  indicates that the method uses both VCOCO and HICO-DET training data.
For the object detector, ``COCO'' means that the detector is trained on COCO, while ``HICO-DET'' means that the detector is first pre-trained on COCO and then further fine-tuned on HICO-DET.
}
\label{tab:HICO}
\resizebox{\columnwidth}{!}{

\begin{tabular}{l cc l ccc c  ccc}
\toprule
\multirow{4}{*}{} & &&&
\multicolumn{3}{c}{Default} & &
\multicolumn{3}{c}{Known Object}\\
\cline{5-7} \cline{9-11}
Method & Detector & Use human pose & Feature backbone & Full & Rare & Non Rare &  & Full & Rare & Non Rare  \\
\midrule
Shen~\etal~\cite{Shen-WACV-Zeroshot} & COCO & - & VGG-19 & 6.46 & 4.24 & 7.12 & & - & - & -\\
HO-RCNN~\cite{Chao-WACV-HOI} & COCO & - & CaffeNet & 7.81 & 5.37 & 8.54 & & 10.41 & 8.94 & 10.85\\
InteractNet~\cite{Gkioxari-CVPR-InteractNet} & COCO & - & ResNet-50-FPN & 9.94 & 7.16 & 10.77 & & - & - & -\\
GPNN~\cite{Qi-ECCV-GraphParsing} & COCO & - & ResNet-101 & 13.11 & 9.34 & 14.23 &  & - & - & - \\
iCAN~\cite{Gao-BMVC-iCAN} & COCO & - & ResNet-50 & 14.84 & 10.45 & 16.15 & & 16.26 & 11.33 & 17.73 \\
Wang et al.~\cite{wang2019deep} & COCO & - & ResNet-50 & 16.24 & 11.16 & 17.75 && 17.73 & 12.78 & 19.21 \\
Bansal~\etal~\cite{bansal2019detecting} & COCO & - &  ResNet-101 & 16.96 & 11.73 & 18.52 &  & - & - & - \\
~\cite{Li-CVPR-Interactiveness} & COCO & \checkmark & ResNet-50 & 17.03 & 13.42 & 18.11 &  & 19.17 & 15.51 & 20.26 \\
~\cite{Li-CVPR-Interactiveness} & COCO & \checkmark & ResNet-50 & 17.22 & 13.51 & 18.32 &  & 19.38 & 15.38 & 20.57 \\
no-frills~\cite{Alex-No-Frills} & COCO & \checkmark & ResNet-152 & 17.18 & 12.17 & 18.68 && - & - & -\\
RPNN~\cite{zhou2019relation} & COCO & \checkmark & ResNet-50 & 17.35 & 12.78 & 18.71 & &-&- & - \\
PMFNet~\cite{Bo-PMFNet} & COCO & - & ResNet-50-FPN & 14.92 & 11.42 & 15.96 & & 18.83 & 15.30 & 19.89 \\
PMFNet~\cite{Bo-PMFNet} & COCO & \checkmark & ResNet-50-FPN & 17.46 & \second{15.65} & 18.00 & & \second{20.34} & \second{17.47} & \second{21.20} \\
Peyre~\etal~\cite{peyre2018detecting} & COCO & - & ResNet-50-FPN & \first{19.40} & 14.63 & \first{20.87} & & - & - & - \\
Ours (S-S only)  & COCO & -   & -      & 12.45 & 9.84  & 13.23 & & 15.77 & 12.76 & 16.66 \\
Ours & COCO & - &  ResNet-50-FPN & \second{19.26} & \first{17.74} & \second{19.71} & & \first{23.40} & \first{21.75} & \first{23.89} \\ \midrule
Bansal~\etal~\cite{bansal2019detecting} & HICO-DET & - &  ResNet-101 & \second{21.96} & \second{16.43} & \second{23.62} &  & - & - & - \\
Ours & HICO-DET & - &  ResNet-50-FPN & \first{24.53} & \first{19.47} & \first{26.04} &  & 27.98 & 23.11 & 29.43 \\
\bottomrule
\end{tabular}

}
\end{table*}
 

\para{Implementation details.} 
We build our network with the publicly available PyTorch framework.
Following Gao~\etal~\cite{Gao-BMVC-iCAN}, we use the Detectron \cite{Detectron} with a feature backbone of ResNet-50 to generate human and object bounding boxes.
For VCOCO, we conduct an ablation study on the validation split to determine the best threshold. We keep the detected human boxes with scores  higher than 0.8 and object boxes with scores  higher than 0.1. 
For HICO-DET, since there is no validation split available, we follow the setting in \cite{peyre2018detecting}. We use the score threshold 0.6 to filter out unreliable human boxes and threshold 0.4 to filter out unconfident object boxes.
To augment the training data, we apply random spatial jitterring to the human and object bounding boxes and ensure that the IOU with the ground truth bounding box is greater than 0.7.
We pair all the detected human and objects, and regard those who are not ground truth as negative training examples. 
We keep the negative to positive ratio to three.

We initialize our appearance feature backbone with the COCO pre-trained weight from Mask R-CNN \cite{He-ICCV-MaskRCNN}.
We perform two iterations of feature aggregation on both \emph{human-centric} and \emph{object-centric} subgraphs. 
We train the three streams (human appearance, object appearance, and spatial-semantic) using the V-COCO \emph{train} set.
We use early stopping criteria by monitoring the validation loss.
We train our network with a learning rate of , a weight decay of , and a momentum of  on both the V-COCO \emph{train} set and HICO-DET \emph{train} set. 
Training our network takes  hours on a single NVIDIA P100 GPU on V-COCO and  hours on HICO-DET.
At test time, our model runs at  fps for VCOCO and  fps for HICO-DET.




\subsection{Quantitative evaluation}


We report the main quantitative results in terms of  on V-COCO in \tabref{vcoco_comparison} and HICO-DET in \tabref{HICO}.
For the V-COCO dataset, our method compares favorably against state-of-the-art algorithms~\cite{Li-CVPR-Interactiveness,wang2019deep,zhou2019relation} except PMFNet \cite{Bo-PMFNet}, which uses human pose as an additional feature.
Since pose estimation required additional training data (with pose annotations), we expect to see performance gain using human pose.
PMFNet \cite{Bo-PMFNet} also reports the  \emph{without} human pose, which is  mAP lower to our method. 
We also note that the spatial-semantic stream alone \emph{without} using any visual features achieves a competitive performance ( mAP) when compared with the state-of-the-art. 
This highlights the effectiveness of the abstract spatial-semantic representation and contextual information.
Compared with methods that perform joint inference on densely connected graph~\cite{Qi-ECCV-GraphParsing},
our approach produces significant performance gains.

For the HICO-DET dataset, our method also achieves competitive performance with state-of-the-art methods~\cite{Alex-No-Frills,Li-CVPR-Interactiveness,Bo-PMFNet,Peyre-ICCV-Weakly}.
Our method achieves the best performance for the \emph{rare categories}, showing that our method handles the long-tailed distributions of HOI classes well.


We note that the current best performing model~\cite{bansal2019detecting} uses an object detector which is fine-tuned on HICO-DET \emph{train} set using the annotated object bounding boxes.
For a fair comparison, we also fine-tune our object detector on HICO-DET and report our result.
Note that we do \emph{not} re-train our model, but only replace the object detector at the test time.

Here, the large performance gain from fine-tuning the object detector may not reflect the progress on the HOI detection task.
This is because the objects (and the associated HOIs) in the HICO-DET dataset are \emph{not} fully annotated. 
Using a fine-tuned object detector can thus improve the HOI detection performance by exploiting such annotation biases. 








\vspace{-2.0mm}
\subsection{Qualitative evaluation}



\para{HOI detection results.}
Here we show sample results on the V-COCO dataset and the HICO-DET dataset in \figref{visual}. We highlight the detected person and the associated object with red and green bounding boxes, respectively.


\newlength\ftqd
\setlength\ftqd{1.9cm}
\begin{figure*}[t]
\centering

\mpage{0.01}{\rotatebox[origin=c]{90}{\small Human-centric}}
\mpage{0.95}{
\includegraphics[width=\ftqd]{figure_SSAR/human_000000235316.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000235316_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000235316_cut_plot.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000034837.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000034837_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000034837_lay_plot.png}\hfill
\\
\includegraphics[width=\ftqd]{figure_SSAR/human_000000421491.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000421491_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000421491_hit_plot.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000005689.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000005689_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/human_000000005689_sit_plot.png}\hfill
\\
}


\mpage{0.01}{\rotatebox[origin=c]{90}{\small Object-centric}}
\mpage{0.95}{
\includegraphics[width=\ftqd]{figure_SSAR/000000423602_2199.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000423602_2199_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000423602_2199_hold_plot.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000455667_2365.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000455667_2365_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000455667_2365_ride_plot.png}\hfill
\\
\includegraphics[width=\ftqd]{figure_SSAR/000000299932_1487.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000299932_1487_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000299932_1487_kick_plot.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000178874_901.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000178874_901_weight.png}\hfill \includegraphics[width=\ftqd]{figure_SSAR/000000178874_901_work_on_computer_plot.png}\hfill
\\
}


\mpage{0.04}\hfill
\mpage{0.13}{HOI det.} \hfill
\mpage{0.13}{H-O pair} \hfill
\mpage{0.13}{Score} \hfill
\mpage{0.13}{HOI det.} \hfill
\mpage{0.14}{H-O pair} \hfill
\mpage{0.13}{Score}\hfill
\\
\mpage{0.015}\hfill
\mpage{0.46}{

}\hfill
\mpage{0.485}{

}\hfill
\caption{\textbf{More iteration of feature aggregation leads to a more accurate prediction.} 
The human-centric and object-centric subgraph in the spatial-semantic stream propagates contextual information to produce increasingly accurate HOI predictions.
}
\label{fig:RBG}
\end{figure*}

\vspace{1.0mm} \begin{table*}[htbp]
\caption{\textbf{Ablation study on the V-COCO \emph{val} set.} We show the role mAP .
}
\label{tab:ablation}

\centering
\mpage{0.5}{
(a) \textbf{More message passing iters.}
} 
\hfill
\mpage{0.45}{
(b) \textbf{Feature used in DRG} 
}
\hfill
\mpage{0.45}{
\begin{tabular}{c|c|c|c}
 iter.    & H graph & O graph & H  + O \\
\hline
 0-iter.  &  48.78 & 47.47 & 50.14 \\
 1-iter.  &  48.83 & 47.35 & 50.74 \\
 2-iter.  &  50.20 & 47.87 & 51.37 \\
\end{tabular}
}
\hfill 
\mpage{0.5}{
\begin{tabular}{l|c}
 & mAP \\
\hline
App. feature (entire image)            & 35.69 \\
App. feature (H-O union box)           & 46.93 \\
Word2vec embedding                           & 37.36 \\
Spatial-semantic feature (ours)              & 51.37 \\
\end{tabular}
}
\hfill 
\\
\mpage{0.35}{
\centering
(c) \textbf{Different subgraph}
} 
\hfill
\mpage{0.6}{
\centering
(d) \textbf{Effectiveness of O subgraph} 
}
\hfill
\mpage{0.35}{
\centering
\begin{tabular}[t]{cc|c}
H graph     & O graph    & mAP   \\
\hline
      -     &      -     & 50.14 \\
 \checkmark &      -     & 51.10 \\
      -     & \checkmark & 50.78 \\
\checkmark & \checkmark  & 51.37 \\
\end{tabular}
}
\hfill
\mpage{0.6}{
\centering
\begin{tabular}[t]{l|cccc}
                             & 1-3 & 4-6 & 7+ & all  \\
\hline
H graph                      & 57.89 & 52.77 & 50.96 & 51.10 \\
H graph + O graph            & 58.28 & 53.75 & 51.06 & 51.37 \\
Margin                       & +0.39 & +0.98 & +0.10 & +0.27 \\
\% of testing images         & 68\%  & 12\%  & 20\%  & 100\% \\
\end{tabular}
}
\end{table*}

\vspace{-2.0mm}







































 \para{Visualizing the effect of the Dual Relation Graph.} 
In \figref{RBG}, we show the effectiveness of the proposed DRG. 
In the first two rows, we show that by aggregating contextual information, using the  \emph{human-centric} subgraph produces more accurate HOI predictions.
Another example in the top right image indicates that the \emph{human-centric} subgraph can also suppress the scores for unrelated human-object pairs. 
In the bottom two rows, we show four examples of how the \emph{object-centric} subgraph propagates contextual information in each step to produce increasingly more accurate HOI predictions.
For instance, the bottom right images show that for a person and an object without interaction, our model learns to suppress the predicted score by learning from the relationship of other HOI pairs associated with this particular object (laptop). 
In this example, the model starts with predicting a high score for the woman working on a computer. 
By learning from the relationship between the man and the computer, our model suppresses the score in each iteration.







\subsection{Ablation study}
\label{sec:ablation}
We examine several design choices using the V-COCO \emph{val} set.

\para{More iteration of feature aggregation.} 
\tabref{ablation}(a) shows the performance using different iterations of feature aggregation.
For either \emph{human-centric} or \emph{object-centric} subgraph, using more iterations of feature aggregation improves the overall performance.
This highlights the advantages of exploiting contextual information among different HOIs.
Performing feature aggregation on \emph{both} subgraphs further improves the final performance.

\para{Effectiveness of each subgraph.}
To validate the effectiveness of the proposed subgraph, we show different variants of our model in~\tabref{ablation}(c).
Adding only \emph{human-centric} subgraph improves upon the baseline model (without using any subgraph) by  absolute mAP, while adding only \emph{object-centric} subgraph gives a  absolute mAP.
More importantly, our results show that the performance gain of each subgraph is complementary to each other.
To further validate the effectiveness of the \emph{object-centric} subgraph, we show in \tabref{ablation}(d) the breakdown of \tabref{ablation}(c) in terms of the number of persons in the scene. The \emph{object-centric} subgraph is less effective for cases with few people. For example, if there is only one person, the \emph{object-centric} subgraph has no effect. For images with a moderate amount of persons (4-6), however, our \emph{object-centric} subgraph shows a clear  mAP gain. As the number of persons getting larger (7+), the \emph{object-centric} subgraph shows a relatively smaller improvement due to clutter. Among the 2,867 testing images, 68\% of them have only 1-3 persons. As a result, we do not see significant overall improvement.

\para{Spatial-semantic representation.}
To demonstrate the advantage and effectiveness of the use of the abstract spatial-semantic representation, we show in \tabref{ablation}(b) the comparison with alternative features, e.g., word2vec (as used in \cite{Lu-ECCV-Prior}) or appearance-based features.
By using our spatial-semantic representation in the dual relation graph, we achieve  mAP. This shows a clear margin over the other alternative options, highlighting the contribution of spatial-semantic representation.







\begin{figure*}[!t]
\centering
\includegraphics[width=\ftqa]{figure_failure/incorrect_obj_ski_000000110769_71.jpg}\hfill \includegraphics[width=\ftqa]{figure_failure/incorrect_obj_talk_000000020925_49.jpg}\hfill \includegraphics[width=\ftqa]{figure_failure/incorrect_obj_sit_000000472256_206.jpg}\hfill \includegraphics[width=\ftqa]{figure_failure/incorrect_action_sit_000000015846_212.jpg}\hfill \includegraphics[width=\ftqa]{figure_failure/incorrect_action_mcarry_000000191314_255.jpg}\hfill \includegraphics[width=\ftqa]{figure_failure/incorrect_action_sit_000000032334_96.jpg}\mpage{0.45}{Incorrect object} \hfill
\mpage{0.45}{Incorrect action}
\caption{
\textbf{Failure cases of our method.}
}
\label{fig:failure}
\end{figure*} 
\subsection{Limitations}
While we demonstrated improved performance, our model is far from perfect. 
Below, we discuss two main limitations of our approach, with examples in \figref{failure}.


First, we leverage the off-the-shelf object detector to detect object instances in an image. 
The object detection does \emph{not} benefit from the rich contextual cues captured by our method.
We believe that a joint end-to-end training approach may help reduce this type of errors. 

Second, our model may be confused by plausible spatial configuration and predicts incorrect action. 
In the third image, our model predicts that the person is sitting on a bench even though our model confidently predicts this person is standing and catching a Frisbee.
Capturing the statistics of co-occurring actions may resolve such mistakes.









%
