\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{verbatim}

\newcommand{\noun}[1]{\textsc{#1}}
\newcommand{\binom}[2]{{#1 \choose #2}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage{color}
\usepackage{times}
\usepackage{babel}

\newtheorem{theorem}{Theorem}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{prop}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}


\newcommand{\eps}{\varepsilon}
\newcommand{\D}{{\cal D}}
\newcommand{\E}{{\cal E}}
\newcommand{\calf}{{\cal F}}
\newcommand{\cube}{\operatorname{\{0, 1\}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\polyn}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog} }
\newcommand{\mem}{\mbox{\textsc{Mem}}}
\newcommand{\polyeval}{\mbox{\textsc{PolyEval}}}

\date{}


\begin{document}

\title{Efficient and Error-Correcting Data Structures for Membership and
Polynomial Evaluation}


\author{Victor Chen\thanks{MIT CSAIL, victor@csail.mit.edu. Supported by NSF award CCF-0829672. } \and Elena Grigorescu\thanks{MIT CSAIL, elena\_g@mit.edu. This work started when this author was
visiting CWI in Summer 2008. Supported by NSF award CCF-0829672. } \and Ronald de Wolf\thanks{CWI Amsterdam, rdewolf@cwi.nl. Supported by a Vidi grant
from the Netherlands Organization for Scientific Research (NWO).
}}


\maketitle
\setcounter{page}{0} 
\begin{abstract}
We construct efficient data structures that are resilient against
a constant fraction of adversarial noise. Our model requires that
the decoder answers \emph{most} queries correctly with high probability
and for the remaining queries, the decoder with high probability either
answers correctly or declares ``don't know.'' Furthermore, if
there is no noise on the data structure, it answers \emph{all} queries
correctly with high probability. Our model is the common generalization
of an error-correcting data structure model proposed recently by de~Wolf, 
and the notion of ``relaxed locally decodable codes'' developed in the PCP literature.

We measure the efficiency of a data structure in terms of its \emph{length},
(the number of bits in its representation), and query-answering
time, measured by the number of \emph{bit-probes} to the (possibly corrupted)
representation. We obtain results for the following two data structure problems: 
\begin{itemize}
\item (Membership) Store a subset  of size at most  from a universe
of size  such that membership queries can be answered efficiently,
i.e., decide if a given element from the universe is in . \\
 We construct an error-correcting data structure for this problem
with length nearly linear in  that answers membership queries
with  bit-probes. This nearly matches the asymptotically optimal
parameters for the noiseless case: length  and one bit-probe,
due to Buhrman, Miltersen, Radhakrishnan, and Venkatesh. 
\item (Univariate polynomial evaluation) Store a univariate polynomial 
of degree  over the integers modulo  such that
evaluation queries can be answered efficiently, i.e., evaluate the
output of  on a given integer modulo . \\
 We construct an error-correcting data structure for this problem
with length nearly linear in  that answers evaluation queries
with  bit-probes. This nearly matches
the parameters of the best-known noiseless construction, due to Kedlaya
and Umans.
\end{itemize}

\end{abstract}
\thispagestyle{empty} \newpage{}

\section{Introduction}
The area of data structures is one of the oldest and most fundamental
parts of computer science, in theory as well as in practice. The
underlying question is a time-space tradeoff: we are given a piece
of data, and we would like to store it in a short, space-efficient
data structure that allows us to quickly answer specific queries about
the stored data. On one extreme, we can store the data as just
a list of the correct answers to all possible queries. This is extremely
time-efficient (one can immediately look up the correct answer without
doing any computation) but usually takes significantly more space
than the information-theoretic minimum. At the other extreme, we can
store a maximally compressed version of the data. This method is extremely
space-efficient but not very time-efficient since one usually has to undo
the whole compression first. A good data structure sits somewhere
in the middle: it does not use much more space than the information-theoretic
minimum, but it also stores the data in a structured way that enables
efficient query-answering.

It is reasonable to assume that most practical implementations of
data storage are susceptible to \emph{noise}: over time some of the
information in the data structure may be corrupted or erased by various
accidental or malicious causes. This buildup of errors may cause the
data structure to deteriorate so that most queries are not answered
correctly anymore. Accordingly, it is a natural task to design data
structures that are not only efficient in space and time but also
resilient against a certain amount of \emph{adversarial} noise, where
the noise can be placed in positions that make decoding as difficult
as possible. 

Ways to protect information and computation against noise
have been well studied in the theory of error-correcting codes
and of fault-tolerant computation. In the data structure
literature, constructions under often incomparable models have been
designed to cope with noise, and we examine a few of these models.
Aumann and Bender~\cite{aumann&bender:ftdata} studied pointer-based
data structures such as linked lists, stacks, and binary search trees.
In this model, errors (adversarial but detectable) occur whenever
all the pointers from a node are lost. They measure the dependency
between the number of errors and the number of nodes that become irretrievable,
and designed a number of efficient data structures where this dependency is reasonable.

Another model for studying data structures with noise is the faulty-memory RAM model, 
introduced by Finocchi and Italiano~\cite{finocchi&italiano:faults}. 
In a faulty-memory RAM, there are  memory cells that cannot
be corrupted by noise. Elsewhere, errors (adversarial and undetectable)
may occur at any time, even during the decoding procedure.
Many data structure problems have been examined in this model, 
such as sorting~\cite{fgi:resilientsorting}, searching~\cite{fgi:resilientsearch},
priority queues~\cite{jmm:resilientpriority} and dictionaries~\cite{bffgijmm:resilientdict}.
However, the number of errors that can be tolerated is typically less than
a linear portion of the size of the input. Furthermore, correctness
can only be guaranteed for keys that are not affected by noise. For
instance, for the problem of comparison-sorting on  keys, the
authors of~\cite{fgi:resilientsorting} designed a resilient sorting
algorithm that tolerates  keys being corrupted and
ensures that the set of uncorrupted keys remains sorted.

Recently, de~Wolf~\cite{wolf:ecdata} considered another model of
resilient data structures. The representation of the data structure
is viewed as a bit-string, from which a decoding procedure can read
any particular set of bits to answer a data query. The representation
must be able to tolerate a constant fraction  of adversarial noise in the bit-string\footnote{We only consider bit-flip-errors here, not erasures. Since erasures
are easier to deal with than bit-flips, it suffices to design a data
structure dealing with bit-flip-errors.} 
(but not inside the decoding procedure). His model generalizes the
usual noise-free data structures (where ) as well as the
so-called ``locally decodable codes'' (LDCs)~\cite{katz&trevisan:ldc}.
Informally, an LDC is an encoding that is tolerant of noise and allows
fast decoding so that each message symbol can be retrieved correctly
with high probability. Using LDCs as building blocks, de~Wolf constructed
data structures for several problems. 

Unfortunately, de~Wolf's model has the drawback that the optimal
time-space tradeoffs are much worse than in the noise-free model.
The reason is that all known constructions of LDCs that make 
bit-probes~\cite{yekhanin:3ldcj,efremenko:ldc} have very poor encoding
length (super-polynomial in the message length). In fact, the encoding
length provably must be super-linear in the message length~\cite{katz&trevisan:ldc,kerenidis&wolf:qldcj,woodruff:ldclower}.
As his model is a generalization of LDCs, data structures cannot have
a succinct representation that has length proportional to the information-theoretic
bound.

We thus ask: what is a clean model of data structures that allows
efficient representations \emph{and} has error-correcting capabilities?
Compared with the pointer-based model and the faulty-memory RAM, de~Wolf's
model imposes a rather stringent requirement on decoding: \emph{every}
query must be answered correctly with high probability from the possibly corrupted
encoding. While this requirement is crucial in the definition of LDCs
due to their connection to complexity theory and cryptography, for
data structures it seems somewhat restrictive.

In this paper, we consider a broader, more relaxed notion of error-correcting for data structures. 
In our model, for most queries, the decoder has to return the correct answer with
high probability.  However, for the few remaining queries, the decoder may claim ignorance, 
i.e., declare the data item unrecoverable from the (corrupted) data structure. 
Still, for \emph{every} query, the answer is incorrect only with small probability.
In fact, just as de~Wolf's model is a generalization of LDCs, our model in this paper
is a generalization of the ``relaxed'' locally decodable codes (RLDCs) introduced
by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan~\cite{bghsv04}.
They relax the usual definition of an LDC by requiring the decoder
to return the correct answer on \emph{most} rather than all queries.
For the remaining queries it is allowed to claim ignorance, i.e.,
to output a special symbol `' interpreted as ``don't know''
or ``unrecoverable.'' As shown in~\cite{bghsv04}, relaxing the
LDC-definition like this allows for constructions of RLDCs
with  bit-probes of \emph{nearly linear} length. 

Using RLDCs as building blocks, we construct error-correcting data structures that are very efficient in terms of time
as well as space. Before we describe our results, let us define
our model formally. First, a \emph{data structure problem} is specified
by a set  of \emph{data items}, a set  of \emph{queries},
a set  of \emph{answers}, and a function 
which specifies the correct answer  of query  to data
item~. A data structure for  is specified by four parameters:
 the number bit-probes,  the fraction of noise,  an upper
bound on the error probability for each query, and  an upper bound on
the fraction of queries in  that are not answered correctly with high probability 
(the `' stands for ``lost'').

\begin{defn}\label{def:data structure}Let 
be a data structure problem. Let  be an integer, 
, , and . We
say that  has a \emph{-data structure}
of length  if there exist an encoder  and a
(randomized) decoder  with the following properties: for every  and every  at Hamming distance , 
\begin{enumerate}
\item  makes at most  bit-probes to , 
\item  for every , 
\item the set  has size at least
 (`' stands for ``good''),
\item if , then . 
\end{enumerate}
\end{defn}

Here  denotes the random variable which is the decoder's output on inputs  and .
The notation indicates that it accesses the two inputs in different ways: 
while it has full access to the query , it only has bit-probe access (or ``oracle access'') to the string .

We say that a -data structure is \emph{error-correcting},
or an \emph{error-correcting data structure}, if . 
Setting  recovers the original notion of error-correction
in de~Wolf's model~\cite{wolf:ecdata}. A \emph{-relaxed locally
decodable code (RLDC)}, defined in~\cite{bghsv04}, is an error-correcting
data structure for the membership function ,
where . A \emph{-locally decodable code (LDC)}, defined
by Katz and Trevisan~\cite{katz&trevisan:ldc}, is an RLDC with .

\begin{rem}For the data structure problems considered in this paper,
our decoding procedures make only \emph{non-adaptive} probes, i.e.,
the positions of the probes are determined all at once and sent simultaneously
to the oracle. For other data structure problems it may be natural
for decoding procedures to be adaptive. Thus, we do not require 
to be non-adaptive in Condition~1 of Definition~\ref{def:data structure}.\end{rem}


\subsection{Our results\label{sub:Our results}}

We obtain efficient error-correcting data structures for the following two data structure problems.

\paragraph{\textbf{\noun{Membership:}}}

Consider a universe  and some nonnegative integer
. Given a set  with at most  elements,
one would like to store  in a compact representation that can
answer ``membership queries'' efficiently, i.e., given an index
, determine whether or not . Formally ,
, and . The function  is 
if  and  otherwise.

Since there are at least  subsets of the universe of
size at most , each subset requiring a different instantiation
of the data structure, the information-theoretic lower bound on the
space of any data structure is at least 
bits.\footnote{Our logs are always to base~.}
An easy way to achieve this is to store  in sorted order. If
each number is stored in its own -bit ``cell,'' this
data structure takes  cells, which is  bits. To answer
a membership query, one can do a binary search on the list to determine
whether  using about  ``cell-probes,'' or 
bit-probes. The length of this data structure is essentially optimal,
but its number of probes is not. Fredman, Koml\'{o}s, and Szemer\'{e}di~\cite{fks:sparsetable}
developed a famous hashing-based data structure that has length 
cells (which is  bits) and only needs a \emph{constant}
number of cell-probes (which is  bit-probes). Buhrman,
Miltersen, Radhakrishnan, and Venkatesh~\cite{bmrv:bitvectorsj}
improved upon this by designing a data structure of length 
bits that answers queries with \emph{only one bit-probe} and a small error probability. 
This is simultaneously optimal in terms of time (clearly one bit-probe cannot
be improved upon) and space (up to a constant factor).

None of the aforementioned data structures can tolerate a constant fraction of noise.
To protect against noise for this problem, de~Wolf~\cite{wolf:ecdata}
constructed an error-correcting data structure with  using
a locally decodable code (LDC). That construction
answers membership queries in  bit-probes and has length roughly
, where  is the shortest length of an LDC encoding
 bits with bit-probe complexity . Currently, all known LDCs with  have  super-polynomial in ~\cite{bikr:improvedpir,yekhanin:3ldcj,efremenko:ldc}.
In fact,  must be super-linear for all constant , see e.g.~\cite{katz&trevisan:ldc,kerenidis&wolf:qldcj,woodruff:ldclower}.

Under our present model of error-correction, we can construct much
more efficient data structures with error-correcting capability. First,
it is not hard to show that by composing the BMRV data structure~\cite{bmrv:bitvectorsj}
with the error-correcting data structure for  (equivalently,
an RLDC)~\cite{bghsv04}, one can already obtain an error-correcting
data structure of length , where  is
an arbitrarily small constant. However, following an approach taken
in~\cite{wolf:ecdata}, we obtain a data structure of length ,
which is much shorter than the aforementioned construction if .

\begin{theorem}\label{thm:membership} For every ,
there exist an integer  and real , such that for
all  and , and every ,  has
a -data structure of length .
\end{theorem}

We will prove Theorem~\ref{thm:membership} in Section~\ref{sec:Membership problem}.
Note that the size of the good set  is at least .
Hence corrupting a -fraction of the bits of the data structure
may cause a decoding failure for at most half of the queries 
but not all. One may replace this factor  easily by
another constant (though the parameters  and  will then
change).


\paragraph{\bf{\noun{Polynomial evaluation:}}}

Let  denote the set of integers modulo  and 
be some nonnegative integer. Given a univariate polynomial 
of degree at most , we would like to store  in a compact representation
so that for each evaluation query ,  can be computed
efficiently. Formally, , ,
and , and the function is .

Since there are  polynomials of degree at most , with
each polynomial requiring a different instantiation of the data structure,
the information-theoretic lower bound on the space of any data structure
for this problem is at least 
bits. Since each answer is an element of  and must be
represented by  bits, 
is the information-theoretic lower bound on the bit-probe complexity.

Consider the following two naive solutions. On one hand, one can simply
record the evaluations of  in a table with  entries, each
with  bits. The length of this
data structure is  and each query requires reading only
 bits. On the other hand, 
 can be stored as a table of its  coefficients. This gives a data structure of
length and bit-probe complexity . 

A natural question is whether one can construct a data structure that
is optimal both in terms of space and time, i.e., has length 
and answers queries with  bit-probes. No such constructions
are known to exist. However, some lower bounds are known in the weaker
cell-probe model, where each cell is a sequence of 
bits. For instance, as noted in~\cite{miltersen:cellprobesurvey},
any data structure for  \noun{polynomial evaluation} that stores 
cells ( bits) requires reading at least 
cells ( bits). \begin{comment}
info above from Miltersen survey pg 17 para 2
\end{comment}
 Moreover, by~\cite{miltersen95polyeval}, if 
and the data structure is constrained to store  cells,
then its query complexity is  cells. This implies that
the second trivial construction described above is essentially optimal in the cell-probe model.

Recently, Kedlaya and Umans~\cite{kedl-umans} obtained a data structure
of length  (where  is an arbitrarily
small constant) and answers evaluation queries with 
bit-probes. These parameters exhibit the best tradeoff
between  and  so far. When  for some ,
the data structure of Kedlaya and Umans~\cite{kedl-umans} is much superior
to the trivial solution: its length is nearly optimal, and the query complexity
drops from  to only  bit-probes.

Here we construct an error-correcting data structure for the polynomial
evaluation problem that works even in the presence of adversarial noise,
with length nearly linear in  and bit-probe complexity . Formally:

\begin{theorem}\label{thm:poly eval} For every ,
there exists  such that for all positive integers ,
for all , the data structure problem 
has a -data structure 
of length . 
\end{theorem}

\begin{rem}We note that Theorem~\ref{thm:poly eval} easily holds when
. As we discussed previously, one can just store a
table of the  coefficients of . To make this error-correcting, 
encode the entire table by a standard error-correcting code. 
This has length and bit-probe complexity ).
\end{rem}

\subsection{Our techniques\label{sub:Our-technique}}

At a high level, for both data structure problems we build
our constructions by composing a relaxed locally decodable code with
an appropriate noiseless data structure. If the underlying probe-accessing
scheme in a noiseless data structure is ``pseudorandom,'' then
the noiseless data structure can be made error-correcting by appropriate
compositions with other data structures. By pseudorandom, we mean
that if a query is chosen uniformly at random from , then the positions of the probes selected
also ``behave'' as if they are chosen uniformly at random. Such
property allows us to analyze the error-tolerance of our constructions.

More specifically, for the \noun{membership} problem we build upon the noiseless data structure of Buhrman et al.~\cite{bmrv:bitvectorsj}.  While de Wolf~\cite{wolf:ecdata} combined this with LDCs to get a rather long data structure with , we will combine it here with RLDCs to get nearly optimal length with small (but non-zero) .  In order to bound  in our new construction, we make use of the fact that the \cite{bmrv:bitvectorsj}-construction is a bipartite \emph{expander graph}, as explained below after Theorem~\ref{thm:bmrv}.  This property wasn't needed in~\cite{wolf:ecdata}. The left side of the expander represents the set of queries, and a neighborhood of a query (a left node) represents the set of possible bit-probes that can be chosen to answer this query. The expansion property of the graph essentially implies that for a random query, the distribution of a bit-probe chosen to answer this query is close to uniform.\footnote{We remark that this is different from the notion of smooth decoding in the LDC literature, which requires that for every {\em fixed} query, each bit-probe by itself is chosen with probability close to uniform (though not independent of the other bit-probes).} 
This property allows us to construct an efficient, error-correcting data structure for this problem.

For the polynomial evaluation problem, we rely upon the noiseless data structure of Kedlaya and Umans~\cite{kedl-umans}, which has a decoding procedure that uses the reconstructive algorithm from the Chinese Remainder Theorem.  
The property that we need is the simple fact that
if  is chosen uniformly at random from , then for any
,  modulo  is uniformly distributed in .
This implies that for a random evaluation point , the distribution of certain tuples of cell-probes used to answer this evaluation point is close to uniform. This observation allows us to construct an efficient, error-correcting data structure for polynomial evaluation.
Our construction follows the non-error-correcting one of~\cite{kedl-umans}
fairly closely; the main new ingredient is to add redundancy to their Chinese Remainder-based reconstruction by using more primes, which gives us the error-correcting features we need. 

\paragraph{\textbf{Time-complexity of decoding and encoding.} }
So far we have used the number of bit-probes as a proxy for the actual time the decoder needs for query-answering.
This is fairly standard, and usually justified by the fact that the actual time complexity of decoding is 
not much worse than its number of bit-probes.  This is also the case for our constructions.
For \noun{membership}, it can be shown that the decoder uses  probes and  time
(as do the RLDCs of~\cite{bghsv04}).
For \noun{polynomial evaluation}, the decoder uses  probes and  time.

\iffalse
What about the efficiency of \emph{encoding}, i.e., the ``pre-processing'' of the data into the form of a data structure?
Our data structure for \noun{membership} is based on the noiseless one of Buhrman et al.~\cite{bmrv:bitvectorsj}, 
which uses a randomly constructed expander graph and a randomly chosen map from the set 
of all -element subsets of universe  to bit-strings of length .
The latter object takes roughly  bits to describe, which is exponentially larger than the size of the
data item that needs to be encoded. Ta-Shma~\cite{tashma:storing} gave a variant of their construction where 
the data structure is slightly longer, but efficiently constructible.
Together with a derandomization of Proposition~\ref{prop:balls vs bins} based on pairwise independence, 
we expect some variant of our data structure for \noun{membership} to be encodable in deterministic time .

Our data structure for \noun{polynomial evaluation} can be constructed deterministically in time . 
Note that this could be much worse than the length of the input polynomial , which is roughly  bits.
This contrasts with the work of Kedlaya and Umans~\cite{kedl-umans}, whose data structures can be constructed 
in time nearly linear in the length of the input.  In our construction we sacrificed this encoding-efficiency
in order to have the required error-correcting properties.
We leave the further optimization of these encodings to future work.
{\bf Check all this!!}
\fi



The efficiency of \emph{encoding}, i.e., the ``pre-processing'' of the data into the form of a data structure, for both our error-correcting data structures  \noun{membership} and \noun{polynomial evaluation} depends on the efficiency of encoding of the RLDC constructions in~\cite{bghsv04}.  This is not addressed explicitly there, and needs further study. 


\section{The \noun{Membership} problem\label{sec:Membership problem}}

In this section we construct a data structure for the membership problem
. First we describe some of the building blocks that
we need to prove Theorem~\ref{thm:membership}. Our first basic building
block is the relaxed locally decodable code of Ben-Sasson et al.~\cite{bghsv04}
with nearly linear length. Using our terminology, we can restate their
result as follows:

\begin{theorem}[BGHSV~\cite{bghsv04}]\label{thm:bghsv}For every
 and , there exist an integer 
and reals  and , such that for every 
and every , the membership problem 
has a -data structure for 
of length . \end{theorem}

Note that by picking the error-rate  a sufficiently small
constant, one can set  (the fraction of unrecoverable queries) to be very close to .

The other building block that we need is the following one-probe data
structure of Buhrman et al.~\cite{bmrv:bitvectorsj}.

\begin{theorem}[BMRV~\cite{bmrv:bitvectorsj}]\label{thm:bmrv}
For every  and for every positive integers ,
there is an -data structure for  of length
 bits. \end{theorem}

\emph{Properties of the BMRV encoding:} The encoding can be represented
as a bipartite graph  with  left vertices
and  right vertices, and regular left degree .
 is an expander graph: for each set  with
, its neighborhood  satisfies .
For each assignment of bits to the left vertices with at most 
ones, the encoding specifies an assignment of bits to the right vertices.
In other words, each  of weight  corresponds
to an assignment to the left vertices, and the -bit encoding of
 corresponds to an assignment to the right vertices.

For each  we write  to denote
the set of neighbors of . A crucial property of the encoding function
 is that for every  of weight , for each
, if  then .
Hence the decoder for this data structure can just probe a random
index  and return the resulting bit . Note
that this construction is not error-correcting at all, since 
errors in the data structure suffice to erase all information about
the -th bit of the encoded .\qed

\medskip

As we mentioned in the Section~\ref{sub:Our results}, by combining
the BMRV encoding with the data structure for  from Theorem~\ref{thm:bghsv},
one easily obtains an -data structure 
for  of length .
However, we can give an even more efficient, error-correcting data
structure of length . Our improvement follows 
an approach taken in de~Wolf~\cite{wolf:ecdata}, which
we now describe. For a vector  with ,
consider a BMRV structure encoding  bits into  bits. Now,
from Section~2.3 in~\cite{wolf:ecdata}, the following ``balls
and bins estimate'' is known:

\begin{prop}[From~\cite{wolf:ecdata}]\label{prop:balls vs bins}
For every positive integers , the BMRV bipartite graph 
for  with error parameter  has the following
property: there exists a partition of  into 
disjoint sets  of  vertices each, such
that for each , there are at least  sets 
satisfying . \end{prop}

Proposition~\ref{prop:balls vs bins} suggests the following encoding
and decoding procedures. To encode , we rearrange the  bits
of  into  disjoint blocks of 
bits each, according to the partition guaranteed by Proposition~\ref{prop:balls vs bins}.
Then for each block, encode these bits with the error-correcting
data structure (RLDC) from Theorem~\ref{thm:bghsv}. Given a received word  to decode , pick a block 
at random. With probability at least , 
for some . Run the RLDC decoder to decode the -th bit of the
-th block of . Since most blocks don't have much higher error-rate
than the average (which is at most ), with high probability
we recover , which equals  with high probability.
Finally, we will argue that most queries do not receive a blank symbol
 as an answer, using the expansion property
of the BMRV encoding structure. We now proceed with a formal proof
of Theorem~\ref{thm:membership}.

\begin{proof}[Proof of Theorem~\ref{thm:membership}]We only construct
an error-correcting data structure with error probability .
By a standard amplification technique 
we can reduce the error probability to any other positive constant (i.e., repeat the decoder  times).

By Theorem~\ref{thm:bmrv}, there exists an encoder 
for an -data structure for the membership problem
 of length . Let .
By Theorem~\ref{thm:bghsv}, for every , for some ,
and sufficiently small ,  has an -data structure of length . Let 
and  be its encoder and decoder, respectively.


\paragraph{\textbf{Encoding.}}

Let  be a partition of  as guaranteed by
Proposition~\ref{prop:balls vs bins}. For a string ,
we abuse notation and write  to denote
the string obtained from  by applying the permutation on 
according to the partition . In other words,
 is the concatenation of  where .
We now describe the encoding process.

Encoder : on input , , 
\begin{enumerate}
\item Let  and write . 
\item Output the concatenation . 
\end{enumerate}
The length of  is .

\paragraph{\textbf{Decoding.}}

Given a string , we write ,
where for ,  denotes the -bit string .

Decoder : on input  and with oracle access to a string , 
\begin{enumerate}
\item Pick a random . 
\item If , then output a random bit. \\
 Else, let . Run and output the answer
given by the decoder , with oracle access to the -bit
string . 
\end{enumerate}
\textbf{Analysis.} Fix  and  such that ,
where  is less than some small constant  to be specified later.
We now verify the four conditions of Definition~\ref{def:data structure}.
For Condition~, note that the number of probes the decoder 
makes is the number of probes the decoder  makes, which
is at most , a fixed integer.

We now examine Condition~. Fix . By Markov's inequality,
for a random , the probability that the relative Hamming
distance between  and  is greater
than  is at most . If  is chosen such
that the fraction of errors in  is at most 
and , then with probability at least
,  outputs  or . Let 
be the fraction of  such that .
Then 

To prove Condition~, we need the expansion property of the BMRV
structure, as explained after Theorem~\ref{thm:bmrv}. 
For , define  so that
 if .
In other words,  consists of indices in block  that
are answered correctly by  with high probability. By Theorem~\ref{thm:bghsv},
if the fraction of errors in  is at most ,
then  for some fixed constant
. Set , Since we showed
above that for a -fraction of , the fractional
number of errors in  is at most , we have
.

Recall that the BMRV expander has left degree . Take
 small enough that ; this determines
the value of  of the theorem. We need to show that for any
such small set , most queries  are answered correctly
with probability at least 0.51. It suffices to show that for most
, most of the set  falls outside of . To this
end, let . We
show that if  is small then  is small.

\begin{claim} For every  with ,
it is the case that  \end{claim}

\begin{proof} Suppose, by way of contradiction, that  contains
a set  of size .  is a set of left vertices in the underlying
expander graph , and since , we must have 
 By construction, each vertex in  has at most 
neighbors outside . Thus, we can bound the size of 
from above as follows
 This is a contradiction. Hence no such  exists and .
\end{proof}

Define  and notice that .
It remains to show that each query  is answered correctly
with probability . To this end, we have 
 Combining with Eq.~(\ref{eq:cond2}), for all  we have

 Finally, Condition~ follows from the corresponding condition
of the data structure for . \end{proof}

\section{The \noun{polynomial evaluation} problem\label{sec:polynomial evaluation}}

In this section we prove Theorem~\ref{thm:poly eval}. Given a polynomial
 of degree  over , our goal is to write down a data
structure of length roughly linear in  so that for each
,  can be computed with approximately 
bit-probes. Our data structure is built on the work of Kedlaya and
Umans~\cite{kedl-umans}. Since we cannot quite use their construction
as a black-box, we first give a high-level overview of our proof,
motivating each of the proof ingredients that we need.

\paragraph{\textbf{Encoding based on reduced polynomials:}}

The most naive construction, by recording  for each ,
has length  and answers an evaluation query with 
bit-probes. As explained in~\cite{kedl-umans}, one can
reduce the length by using the Chinese Remainder Theorem (CRT): If
 is a collection of distinct primes, then a nonnegative integer
 is uniquely specified by (and can be reconstructed
efficiently from) the values  for each , where
 denotes . 

Consider the value  over , which can be bounded above
by , for  Let  consist of the first
 primes. For each , compute the reduced
polynomial  and write down  for each .
Consider the data structure that simply concatenates the evaluation
table of every reduced polynomial. This data structure has length
, which is 
by the Prime Number Theorem (see Fact~\ref{fact:Prime Number Theorem}
in Appendix~\ref{sec:CRT-code}). Note that . 
So to compute , it suffices to apply CRT to reconstruct
 over  from the values  for
each . The number of bit-probes is ,
which is . 

\paragraph{\textbf{Error-correction with reduced polynomials:}}

The above CRT-based construction has terrible parameters, but it serves
as an important building block from which we can obtain a data structure
with better parameters. For now, we explain how the above CRT-based
encoding can be made error-correcting. One can protect the bits of
the evaluation tables of each reduced polynomial by an RLDC as provided
by Theorem~\ref{thm:bghsv}. However, the evaluation tables can have
non-binary alphabets, and a bit-flip in just one ``entry'' of
an evaluation table can destroy the decoding process. To remedy this,
one can first encode each entry by a standard error-correcting code
and then encode the concatenation of all the tables by an RLDC. This
is encapsulated in Lemma~\ref{lem:non-binary RLDC}, which can be
viewed as a version of Theorem~\ref{thm:bghsv} over non-binary alphabet.
We prove this in Appendix~\ref{sec:Non-binary-alphabets}. 

\begin{lem}\label{lem:non-binary RLDC} Let 
be a data structure problem. For every ,
there exists  such that for every
,  has an -data
structure of length . \end{lem}

To apply Lemma~\ref{lem:non-binary RLDC}, let  be the
set of degree- polynomials over ,  be the set
of all evaluation points of all the reduced polynomials of  (each specified by a pair ), and
the data structure problem  outputs evaluations of some reduced
polynomial of . 

By itself, Lemma~\ref{lem:non-binary RLDC} cannot guarantee resilience
against noise. In order to apply the CRT to reconstruct , all
the values  must be correct, which is
not guaranteed by Lemma~\ref{lem:non-binary RLDC}. To fix this,
we add redundancy, taking a larger set of primes than necessary so that the reconstruction
via CRT can be made error-correcting. Specifically, we apply a Chinese
Remainder Code, or CRT code for short, to the encoding process. 

\begin{defn}[CRT code]\label{def:crtcode}Let 
be distinct primes, , and .
The \emph{Chinese Remainder Code (CRT code)} with basis 
and rate  over message space  encodes  
as .
\end{defn}

\begin{rem} By CRT, for distinct , their encodings
agree on at most  coordinates. Hence the Chinese Remainder Code
with basis  and rate  has distance
. \end{rem}

It is known that good families of CRT code exist and that
unique decoding algorithms for CRT codes (see e.g., \cite{grs:crt-errors})
can correct up to almost half of the distance of the code. The following
statement can be easily derived from known facts, and we include a
proof in Appendix~\ref{sec:CRT-code}. 

\begin{theorem}\label{thm:CRT code}For every positive integer ,
there exists a set  consisting of distinct primes, with (1) 
and (2)  , such that a CRT
code with basis  and message space  has rate ,
and can correct up to a -fraction of errors. 
\end{theorem}

We apply Theorem~\ref{thm:CRT code} to a message space of size  to obtain 
a set of primes  with the properties described above.
Note that these primes are all within a constant
factor of one another, and in particular, the evaluation table of
each reduced polynomial has the same length, up to a constant factor.
This fact and Lemma~\ref{lem:non-binary RLDC} will ensure that our CRT-based
encoding is error-correcting.


\paragraph{\textbf{Reducing the bit-probe complexity:}}

We now explain how to reduce the bit-probe complexity of the CRT-based
encoding, using an idea from~\cite{kedl-umans}. Write , where
, , and  is a sufficiently
large constant. Consider the following multilinear extension map 
that sends a univariate polynomial of degree at most  to an -variate
polynomial of degree less than  in each variable. For every ,
write  in base . Define 
which sends  to  and extends
multilinearly to . 

To simplify our notation, we write  to denote the multivariate
polynomial . For every , define 
to be .
Note that for every ,  (mod ). Now
the trick is to observe that the total degree of the multilinear polynomial
 is less than the degree of the univariate polynomial ,
and hence its maximal value over the integers is much reduced.
In particular, for every , the value 
over the integers is bounded above by .

We now work with the reduced polynomials of  for our encoding.
Let  be the collection of primes guaranteed by Theorem~\ref{thm:CRT code}
when . For , let 
denote  and  denote the point .
Consider the data structure that concatenates the evaluation table
of  for each . For each ,
to compute , it suffices to compute 
over , which by Theorem~\ref{thm:CRT code} can be reconstructed
(even with noise) from the set . 

Since the maximum value of  is at most 
(whereas the maximum value of  is at most ),
the number of primes we now use is significantly less. This effectively
reduces the bit-probe complexity. In particular, each evaluation query
can be answered with 
bit-probes, which by our choice of  and  is equal to .
However, the \emph{length} of this encoding is still far from the information-theoretically
optimal  bits. We shall explain how to reduce the length,
but since encoding with multilinear reduced polynomials introduces
potential complications in error-correction, we first explain how
to circumvent these complications.

\paragraph{\textbf{Error-correction with reduced multivariate polynomials:}}

There are two complications that arise from encoding with reduced
multivariate polynomials. The first is that not all the points in
the evaluation tables are used in the reconstructive CRT algorithm.
Lemma~\ref{lem:non-binary RLDC} only guarantees that most of the
entries of the table can be decoded, not all of them. So if the entries
that are used in the reconstruction via CRT are not decoded by Lemma~\ref{lem:non-binary RLDC},
then the whole decoding procedure fails. 

More specifically, to reconstruct  over ,
it suffices to query the point  in the evaluation
table of  for each . Typically the set  
will be much smaller than , so not all the points in 
are used. To circumvent this issue, we only store the query points
that are used in the CRT reconstruction. Let .
For each , the encoding only stores the evaluation of
 at the points  instead of the entire domain
. The disadvantage of computing the evaluation at the
points in  is that the encoding stage takes time proportional
to . We thus give up on encoding efficiency (which was one of the main goals
of Kedlaya and Umans) in order to guarantee error-correction.

The second complication is that the sizes of the evaluation tables
may no longer be within a constant factor of each other. (This is
true even if the evaluation points come from all of .)
If one of the tables has length significantly longer than the others,
then a constant fraction of noise may completely corrupt the entries
of all the other small tables, rendering decoding via CRT impossible.
This potential problem is easy to fix; we apply a repetition code
to each evaluation table so that all the tables have equal length.

\paragraph{\textbf{Reducing the length:}}
Now we explain how to reduce the length of the data structure to nearly
, along the lines of Kedlaya and Umans~\cite{kedl-umans}. To reduce the length, we need to reduce the magnitude
of the primes used by the CRT reconstruction. We can effectively achieve
that by applying the CRT twice. Instead of storing the evaluation
table of , we apply CRT again and store evaluation
tables of the reduced polynomials of  instead. Whenever
an entry of  is needed, we can apply the CRT reconstruction to the
reduced polynomials of . 

Note that for , the maximum value of  (over the integers rather than mod )
is at most . Now apply Theorem~\ref{thm:CRT code}
with  the size of the message space to obtain a collection
of primes . Recall that each  is at most
. So each  is at most ,
which also bounds the cardinality of  from above. 

For each query, the number of bit-probes made is at most ,
which is at most . Recall that by our
choice  and , we have . Thus,
the bit-probe complexity is .

Next we bound the length of the encoding.
Recall that by the remark following Theorem~\ref{thm:poly eval},
we may assume without loss of generality that 
for some . This implies .
Then for each ,

Now, by Lemma~\ref{lem:non-binary RLDC}, the length of the encoding
is nearly linear in ,
which is at most .
Putting everything together, the length of the encoding is nearly
linear in . We now proceed with a formal proof.

\begin{proof}[Proof of Theorem~\ref{thm:poly eval}]We only construct
an error-correcting data structure with error probability .
By a standard amplification technique (i.e.,  repetitions)
we can reduce the error probability to any other positive constant.
We now give a formal description of the encoding and decoding algorithms.

\paragraph{\textbf{Encoding:}}
Apply Theorem~\ref{thm:CRT code} with  to obtain a collection of primes .
Apply Theorem~\ref{thm:CRT code} with  to obtain a collection of primes . 
Set . 

Now, for each , , define a collection of evaluation points . 
Fix a univariate polynomial  of degree at most . 
For every , ,
view each evaluation of the reduced multivariate polynomial 
as a bit-string of length exactly .
Let  and for each , , set 
Define  to be the concatenation of  copies of 
the string .
Define the string  

We want to apply Lemma~\ref{lem:non-binary RLDC} to protect the
string , which we can since  may be viewed as a data structure
problem, as follows. 
The set of data-items is the set of polynomials  as above.
The set of queries  is .
The answer to query 
is the -th copy of .

Fix . By Lemma~\ref{lem:non-binary RLDC},
for every  there exists  such that for every 
the data structure problem corresponding to  has a -data
structure. Let  be its encoder and decoder, respectively. 
Finally, the encoding of the polynomial  is simply 

Note that the length of  is at most ,
which as we computed earlier is bounded above by  for some arbitrarily small constant .

\paragraph{\textbf{Decoding:}}
We may assume, without loss of generality, that the CRT decoder 
from Theorem~\ref{lem:non-binary RLDC} outputs  when more
than a -fraction of its inputs are erasures (i.e.,  symbols).

The decoder , with input  and oracle access to ,
does the following: 
\begin{enumerate}
\item Compute  , and
for every , , compute the reduced
evaluation points . 
\item For every , , pick 
uniformly at random and run the decoder  with oracle access
to  to obtain the answers . 
\item For every  obtain 

\item Output . 
\end{enumerate}

\paragraph{\textbf{Analysis:}}
Fix a polynomial  with degree at most . Fix a bit-string 
at relative Hamming distance at most  from , where
 is at most .
We proceed to verify that the above encoding and decoding satisfy
the conditions of Definition~\ref{def:data structure}.

Conditions~1 and~4 are easily verified. For Condition~1, observe
that for each , ,  makes
at most  bit-probes. So  makes at most 
bit-probes, which as we calculated earlier is at most . 

For Condition~4, note that since  decodes correctly when
no noise is present,  is equal to .
By our choice of  and , after two applications of the Chinese Remainder Theorem, it is easy to see that
 outputs , which equals .

Now we verify Condition~2. Fix  We want to show that
with oracle access to , with probability at least , the decoder  on input  outputs either 
or .
For 
we say that a point  is \emph{incorrect} if . 

By Lemma~\ref{lem:non-binary RLDC}, for each  and ,  is incorrect with probability at most . Now fix . On expectation (over the decoder's randomness), at
most a -fraction of the points in the set 
are incorrect. By Markov's inequality, with probability at least ,
the fraction of points in the set 
that are incorrect is at most . If the fraction of
blank symbols in the set 
is at least , then  outputs , which
is acceptable. Otherwise, the fraction of errors and erasures (i.e.,  symbols) in
the set  is at most .
By Theorem~\ref{thm:CRT code}, the decoder  will output an incorrect 
with probability at most . 
Thus, on expectation, at most a -fraction of the points in  are incorrect.
By Markov's inequality again, with probability at least , at most a -fraction of the points in   are incorrect, which by Theorem~\ref{thm:CRT code} implies that  is either  or .
This establishes Condition~2.

We now proceed to prove Condition~3. We show the existence of a set
 such that  and for each ,
we have .
Our proof relies on the following observation: for any 
and , if  is chosen uniformly at random,
then the evaluation point  is like a uniformly chosen element . 
This observation implies that if a few entries in the evaluation tables of the
multivariate reduced polynomials are corrupted, then for most , the output of the decoder  on input  remains unaffected. We now formalize this observation. 

\begin{claim}\label{claim:crt-random}
Fix , , and a point
. Then 
\end{claim}

\begin{proof} For any pair of positive integers , the number
of integers in  congruent to a fixed integer mod  is at
most  and at least . 
Note that if  with , then for any integer , .
Thus,  

It is not hard to see that for a fixed ,
the number of integers  such that 
is at most . Furthermore,
for a fixed , the number of points in 
that are congruent to  mod  is at most .
Thus, for a fixed , the number of integers
 such that  is at
most ,
which is at most  since .
\end{proof}

Now, for every  and , we say that
a query  is \emph{bad}
if the probability that 
is greater than . By Lemma~\ref{lem:non-binary RLDC},
the fraction of bad queries in 
is at most . We say that a tuple of primes
 is \emph{bad} if more than a -fraction
of queries in  are bad (below,
\emph{good} always denotes not bad.) By averaging, the fraction of
bad tuples  is at most .

For a fixed good tuple , we say that an index 
is \emph{bad} if more than a -fraction of queries in the
copy  are bad. Since 
is good, by averaging, at most a -fraction
of  are bad. Recall that in Step~2 of the decoder
, the indices 
are chosen uniformly at random. So on expectation, the set of indices
 has at most a -fraction
of bad indices. By Markov's inequality, with probability at least
, the fraction of bad indices in the set 
is at most . We condition on
this event occurring and fix the indices  for each
, .

Fix a good tuple  and a good index .
By Claim~\ref{claim:crt-random}, for a uniformly random ,
the query  is bad with
probability at most . By linearity of expectation, for
a random , the expected fraction of bad queries in the
set 
is at most , which
is at most  by definition of . Thus, by Markov's
inequality, for a random , with probability at least
, the fraction of bad queries in the set  is at most
. By linearity of expectation, there exists some subset 
with  such that for every , the fraction of
bad queries in  is at most . 

Now fix . By definition, the fraction of bad queries in  is at
most , and furthermore, each of the good queries in  is incorrect with probability
at most . So on expectation, the fraction of errors and
erasures in  is at most . By Markov's inequality, with
probability at least , the fraction of errors and
erasures in the set 
is at most , which is at most . 
We condition on this event occurring. By averaging,
for more than a -fraction of the primes ,
the set  has at most 
-fraction of errors and erasures, which can be corrected
by the CRT decoder . Thus, after Step~3 of the decoder
, the set  has at most a -fraction
of errors and erasures, which again will be corrected by the CRT decoder
. Hence, by the union bound, the two events that we conditioned on
earlier occur simultaneously with probability at least , and  will
output . 
\end{proof}

\section{Conclusion and future work\label{sec:Conclusion}}
We presented a relaxation of the notion of error-correcting data structures
recently proposed in~\cite{wolf:ecdata}. While the earlier definition
does not allow data structures that are both error-correcting and
efficient in time and space (unless an unexpected breakthrough happens for
constant-probe LDCs), our new definition allows us to construct
efficient, error-correcting data structures for both the \noun{membership}
and the \noun{polynomial evaluation} problems. This opens up many
directions: what other data structures can be made error-correcting?

The problem of computing \emph{rank} within a sparse ordered set is a good target.
Suppose we are given a universe , some nonnegative
integer , and a subset  of size at most
. The rank problem is to store  compactly so that on input
, the value  can be computed efficiently.
For easy information-theoretic reasons, any data structure for this problem 
needs length at least  and makes
 bit-probes for each query. If , one
can trivially obtain an error-correcting data structure of optimal length 
with  bit-probes, which is only quadratically worse than optimal: 
write down  as a string of  bits, encode it with a good error-correcting code, 
and read the entire encoding when an index is queried.
However, it may be possible to do something smarter and more involved.
We leave the construction of near-optimal error-correcting data structures for rank with small 
(as well as for related problems such as \emph{predecessor}) as challenging open problems.

\subsection*{Acknowledgments}
We thank Madhu Sudan for helpful comments and suggestions on the presentation of this paper.

\bibliographystyle{plain}
\bibliography{qc}

\appendix
\section{Non-binary answer set\label{sec:Non-binary-alphabets}}
We prove Lemma~\ref{lem:non-binary RLDC}, a version of Theorem~\ref{thm:bghsv}
when the answer set  is non-binary. We first encode the -bit
string  by an RLDC, and
use the decoder of the RLDC to recover each of the  bits of
. Now it is possible that for each , the decoder
outputs some blank symbols  for some of the bits of ,
and no query could be answered correctly. To circumvent this, we first
encode each -bit string  with a good error-correcting
code, then encode the entire string by the RLDC. Now if the decoder
does not output too many errors or blank symbols among the bits of
the error-correcting code for , we can recover it. We need
a family of error-correcting codes with the following property, see
e.g. page  in~\cite{huffman-coding}.

\begin{fact}\label{fact:good code} For every 
there exists  such that for all , there exists a binary
linear code of block length , information length , Hamming
distance , such that the code can correct from  errors
and  erasures, as long as . \end{fact}

\begin{proof}[Proof of Lemma~\ref{lem:non-binary RLDC}]
We only construct an error-correcting data structure with error probability .
By a standard amplification technique (i.e.,  repetitions)
we can reduce the error probability to any other positive constant.
Let  be an asymptotically
good binary error-correcting code (from Fact~\ref{fact:good code}),
with  and relative distance , and decoder
. By Theorem~\ref{thm:bghsv}, there exist 
such that for every , there is a -relaxed locally decodable code (RLDC).
Let  and  denote its encoder and decoder, respectively. 


\paragraph{\textbf{Encoding.}}

We construct a data structure for  as follows. Define the encoder
, where ,
as 

\paragraph{\textbf{Decoding.}}

Without loss of generality, we may impose an ordering on the set 
and identify each  with an integer in .

The decoder , with input  and oracle access to ,
does the following: 
\begin{enumerate}
\item For each , let 
and set . 
\item If the number of blank symbols  in  is at least ,
then output . Else, output . 
\end{enumerate}

\paragraph{\textbf{Analysis.}}

Fix  and  such that ,
and , where  is the minimum of 
and . We need to argue that the above encoding
and decoding satisfies the four conditions of Definition~\ref{def:data structure}.
For Condition~, since  makes  bit-probes and 
runs this  times,  makes  bit-probes into
.

We now show  satisfies Condition~. Fix . We want
to show . By Theorem~\ref{thm:bghsv},
for each , with probability at most ,
. So on expectation, for at most a -fraction
of the indices , . By Markov's inequality,
with probability at least , the number of indices  such
that  is at most . If
the number of  symbols in  is at least 
then  outputs , so assume the number of  symbols
is less than . Those 's are viewed as erasures
in the codeword . Since  has relative
distance , by Fact~\ref{fact:good code}, 
will correct these errors and erasures and output .

For Condition~, we show there exists a large subset  of 's
satisfying . Let ,
which is a -bit string. Call an index  in  \emph{bad}
if  By Theorem~\ref{thm:bghsv},
at most a -fraction of the indices in  are bad.
We say that a query  is \emph{bad} if more than a -fraction
of the bits in  are bad. By averaging, the fraction of bad queries in  is at most 
, which is at most  by our choice of . 
We define  to be the set of  that are not bad. Clearly .

Fix . On expectation (over the decoder's randomness), the fraction of indices
in  such that  is at most . 
Hence by Markov's inequality,
with probability at least , the fraction
of indices in  such that  is at most . 
Thus, by Fact~\ref{fact:good code}, ) will recover from these
errors and erasures and output .

Finally, Condition~ follows since the pair 
satisfies Condition~4, finishing the proof.
\end{proof} 

\section{CRT codes\label{sec:CRT-code}}

In this section we explain how Theorem~\ref{thm:CRT code} follows from known facts.
In~\cite{grs:crt-errors}, Goldreich, Ron, and Sudan
designed a unique decoding algorithm for CRT code. 

\begin{theorem}[from~\cite{grs:crt-errors}]\label{thm:CRT decoding}
Given a CRT Code with basis  and rate ,
there exists a polynomial-time algorithm that can correct up to 
errors. 
\end{theorem}

By choosing the primes appropriately, we can establish Theorem~\ref{thm:CRT code}.
In particular, the following well-known estimate, essentially a consequence
of the Prime Number Theorem, is useful. See for instance Theorem~4.7
in~\cite{apostol} for more details.

\begin{fact}\label{fact:Prime Number Theorem} For an integer
, the th prime (denoted ) satisfies .
\end{fact}

\begin{proof}[Proof of Theorem \ref{thm:CRT code}] Let 
and  denote the -th prime. By Fact~\ref{fact:Prime Number Theorem},
 and .
Also, notice that 
Thus, by Definition \ref{def:crtcode}, the CRT code with basis  and message space , has rate at most .
Lastly, by Theorem~\ref{thm:CRT decoding}, the code can correct a fraction
 of errors.
\end{proof}

\end{document} 