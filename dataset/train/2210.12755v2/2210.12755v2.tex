\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{bbding}
\usepackage[misc]{ifsym}


\usepackage[switch]{lineno}

\begin{document}


\title{LCPFormer: Towards Effective 3D Point Cloud Analysis via Local Context Propagation in Transformers}

\author{Zhuoxu~Huang*,
        Zhiyou~Zhao*,
        Banghuai~Li \textsuperscript{\Envelope},
        and~Jungong~Han,~\IEEEmembership{Member,~IEEE,}
\thanks{Zhuoxu Huang is with the Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, U.K. (e-mail: zhh6@aber.ac.uk). Zhuoxu Huang is also with AnMai Technology (Ningbo) Co., Ltd.}
\thanks{Zhiyou Zhao is with the Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240 China. (e-mail: zhaozhiyou789@sjtu.edu.cn).}
\thanks{Banghuai Li is with the School of Electronics Engineering and Computer Science, Peking University,  Beijing 100871 China. (e-mail: libanghuai@pku.edu.cn).}
\thanks{Jungong Han is with the Department of Computer Science, University of Sheffield, Sheffield S10 2TN, U.K. (e-mail: jungonghan77@gmail.com).}
\thanks{* Authors have equal contributions in this work.}
\thanks{\textsuperscript{\Envelope} Corresponding author: libanghuai@pku.edu.cn}
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\3\times3F=\{f_i\}X=\{x_i\}W_m^Q, W_m^K, W_m^Vm\mathrm{PE}(\cdot)\sigma\mathrm{Softmax}d3\times3N{N'} = N / 2k = 16k \times N' = 8NNx_i\{G_1, \dots, G_m\}G_jx_if_i^jx_iw_jG_jNMF_{in} \in \mathbb{R}^{M \times K \times C}KCA \in \mathbb{R}^{M\times 2C}M1\times1W \in \mathbb{R}^{M\times C}\mathrm{Softmax}WF_{out} \in \mathbb{R}^{N \times C}\times\{f_i, x_i\}\{G_1, G_2, \dots, G_M\}Mf_ix_iCdjG_j=\{f_{i_j}, x_{i_j}\}K\times(d+C)KC'M\times C'k=16k_1^{*}_2_1^{*}k\sim\times[-5^{\circ}, 5^{\circ}][0.9, 1.1]0.67, 1.5-0.2, 0.2kkk \le 16k \ge 32k = 16kk$ & mAP@IoU0.25 & mAP@IoU0.5 \\
\toprule
4  & 56.92 & 37.53 \\
8  & 59.48 & 41.37 \\
16 & {\bf62.70} & 43.34 \\
32 & 61.52 & {\bf44.22} \\
\bottomrule
\end{tabular}}
\label{abla: knn}
\end{table}

{\bf Ablation of the transformer backbone architecture.} To deeper validate our transformer architecture, we conduct an ablation study on the number of LCPFormer blocks and the number of MHSA layers in each block. The results are summarized in Tab.\ref{abla: nblock} and Tab.\ref{abla:nlayer}. It is obvious that insufficient parameters will inevitably weaken the capacity of the network, but deeper models (block number = 5 or attention layer = 3) bring more parameters and unnecessary learning burdens, which makes the model extremely difficult to train. 

\begin{table}[htbp]
\caption{Ablation study on the number of LCPFormer blocks.} 
\centering
{\begin{tabular}{c|c|c}
\toprule
Blocks & mAP@IoU0.25 & mAP@IoU0.5 \\
\toprule
3  & 61.36 & 42.80 \\
4  & {\bf62.70} & {\bf43.34} \\
5  & 61.12 & 42.17 \\
\bottomrule
\end{tabular}}
\label{abla: nblock}
\end{table}


\begin{table}[htbp]
\caption{Ablation study on the number of attention layers. }
\centering
{\begin{tabular}{c|c|c}
\toprule
Attention Layers & mAP@IoU0.25 & mAP@IoU0.5 \\
\toprule
1  & 61.16 & 43.66 \\
2  & {\bf62.70} & 43.34 \\
3  & 62.08 & {\bf43.81} \\
\bottomrule
\end{tabular}}
\label{abla:nlayer}
\end{table}


{\bf Module efficiency analysis.}\label{lightweight}
Eq.\ref{eq:sum_weight}-\ref{eq:learnablemask} summarize our proposed LCP module.
To further validate the efficiency of our LCP module, we compare our method with two famous transformer-based methods for the 3D shape classification task and the results are shown in Tab.\ref{efficiency}. 
Common metrics including GFLOPs, model parameters, and latency are used. All the experiments are conducted on a single NVIDIA GeForce RTX 2080Ti.

\begin{table}[htbp]
\caption{Efficiency comparisons with previous methods on ModelNet40 \cite{modelnet40dataset}.}
\centering
{\begin{tabular}{c|c|c|c|c}
    \toprule
    Method & GFLOPs & Params & Latency & OA \\
    \toprule
    Point Transformer  & 147.2 & 9.58M & 18.7ms & 91.7 \\
    PCT  & 17.4 & 2.94M & 14.6ms &  93.2\\
    \textbf{LCPFormer} & 17.6 & 3.04M & 14.9ms & \textbf{93.6} \\
    \bottomrule
\end{tabular}}
\label{efficiency}
\end{table}

Note that our LCPFormer for the 3D shape classification task is designed on the basis of the PCT \cite{guo2021pct} structure as described in Sec.\ref{networkarchitecture}. The only difference between them is the extra LCP module in the LCPFormer. The comparison between PCT \cite{guo2021pct} and LCPFormer shows that our LCP module is lightweight enough yet effective. It achieves a considerable improvement of 0.4\% OA on the ModelNet40 dataset with a negligible amount of parameters and inference time.




\section{Conclusion}

This work explores the natural fit of the transformer in 3D point cloud perception and focuses on the destruction of the instance information caused by separate local regions. We present a novel and effective message exchange module named Local Context Propagation (LCP). Unlike the previous methods, our LCPFormer is tailored for irregular point clouds and enhances the inherent relationship among the neighboring local regions via local context propagation. Finally, our proposed method achieves considerable improvement compared with various transformer-based methods in multiple 3D tasks including shape classification, and dense prediction tasks such as object detection and semantic segmentation. As a sequence-to-sequence structure, transformers show great potential for sets embedded in the geometric space like point clouds. In future work, we would like to further explore the versatility of our work and implement it to even more datasets for different 3D tasks such as 3D pose estimation, 3D point cloud matching, etc.





\bibliographystyle{IEEEtran}
\bibliography{ieee}



\vspace{11pt}

\begin{IEEEbiographynophoto}{Zhuoxu Huang}
received a B.E. degree in Geodesy and Geomatics Engineering from Wuhan University, Wuhan, China, in 2021. He is currently pursuing a Ph.D. degree with the Department of Computer Science, Aberystwyth University, Aberystwyth, U.K. His research interests include video understanding, 3D vision, and artificial intelligence.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Zhiyou Zhao}
received a B.E. degree in Computer Science and Technology from Shanghai Jiao Tong University, Shanghai, China, in 2022. He is currently pursuing a M.S. degree in Data Science in the Department of Data Science, Chinese University of Hong Kong, Shenzhen. His research interests include 3D point cloud perception, object detection and artificial intelligence.


\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Banghuai Li}
received a B.E. degree in the School of Software from Wuhan University, Wuhan, China, in 2015 and an M.S. degree in the School of Electronics Engineering and Computer Science from Peking University, Beijing, China, in 2018. He is currently a Staff R\&D Engineer in Momenta, a leading autonomous driving technology company in China. . Before that, he was a Researcher at Megvii Company in China, where he finished this work. His research interests include 2D/3D vision, autonomous driving, and artificial intelligence.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Jungong Han}
is Chair Professor in Computer Vision at the Department of Computer Science, University of Sheffield, U.K. He holds the Honorary Professorship with Aberystwyth University and the University of Warwick, U.K. His research interests include computer vision, artificial intelligence, and machine learning.
\end{IEEEbiographynophoto}

\end{document}
