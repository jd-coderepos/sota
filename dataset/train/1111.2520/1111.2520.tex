\documentclass[prodmode,acmtissec]{acmsmall}  
\usepackage{amsfonts, amsmath, verbatim, algorithm, algpseudocode, latexsym}

\markboth{Joan Feigenbaum et al.}{Probabilistic Analysis of Onion Routing in a Black-box Model}
\title{Probabilistic Analysis of Onion Routing in a Black-box Model}

\author{JOAN FEIGENBAUM \affil{Yale University} AARON JOHNSON
  \affil{U.S. Naval Research Laboratory} PAUL SYVERSON \affil{U.S. Naval
    Research Laboratory}
}

\begin{abstract}
We perform a probabilistic analysis of onion routing.  The analysis is presented in a black-box model of anonymous communication in the Universally Composable framework that abstracts the essential properties of onion routing in the presence of an active adversary that controls a portion of the network and knows all \emph{a priori} distributions on user choices of destination.  Our results quantify how much the adversary can gain in identifying users by exploiting knowledge of their probabilistic behavior.  In particular, we show that, in the limit as the network gets large, a user 's anonymity is worst either when the other users always choose the destination  is least likely to visit or when the other users always choose the destination  chooses.  This worst-case anonymity with an adversary that controls a fraction  of the routers is shown to be comparable to the best-case anonymity against an adversary that controls a fraction .
\end{abstract}

\category{C.2.0}{Computer-Communication Networks}{General}[security and protection]
\category{C.2.4}{Computer-Communication Networks}{Distributed Systems}[Distributed applications]
\category{K.4.1}{Computers and Society}{Public Policy Issues}[privacy]
\category{G.3}{Probability and Statistics}{}[probabilistic algorithms]

\terms{Security, Theory}
\keywords{anonymous communication, onion routing, Tor}


\begin{document}
\begin{bottomstuff}
Joan Feigenbaum (email: Joan.Feigenbaum@yale.edu) was supported in part by NSF grants 0331548 and 0534052, ARO grant W911NF-06-1-0316, and US-Israeli BSF grant 2002065.
Aaron Johnson (email: aaron.m.johnson@nrl.navy.mil) did the majority of this work while at Yale University and was supported by NSF grant 0428422 and ARO grant W911NF-05-1-0417. Some work was also done while at The University of Texas at Austin. 
Paul Syverson (email: syverson@itd.nrl.navy.mil) was supported by ONR.

An extended abstract of this paper appears in the Proceedings of the 2007 ACM Workshop on Privacy in the Electronic Society.
\end{bottomstuff}
\maketitle

\section{Introduction}
Every day, half a million people use the onion-routing network Tor~\cite{tor-design} to anonymize their Internet communication.  However, the effectiveness of this service, and of onion routing in general, is not well understood.  The approach we take to this problem is to model onion routing formally all the way from the protocol details to the behavior of the users.  We then analyze the resulting system and quantify the anonymity it provides.  Key features of our model include  a black-box abstraction in the Universally Composable (UC) framework~\cite{cryptoeprint:2000:067} that hides the underlying operation of the protocol and  probabilistic user behavior and protocol operation.


Systems for communication anonymity generally have at most one of two desirable properties: provable security and practicality.  Systems that one can prove secure have used assumptions that make them impractical for most communication applications.  Practical systems are ultimately the ones we must care about, because they are the ones that will actually be used. However, their security properties have not been rigorously analyzed or even fully stated. This is no surprise, because practical anonymity systems have been deployed and available to study for perhaps a decade, while practical systems for communications confidentiality and/or authenticity have been in use almost as long as there have been electronic communications. It often takes a while for theory and practice to catch up to each other.

Of the many anonymous-communication design proposals (\emph{e.g.} \cite{chaum-mix,CHAUM,crowds:tissec,buses03,Salsa,Corrigan-Gibbs:2010:DAA:1866307.1866346}), onion routing
\cite{onion-routing:ih96} has had notable success in practice.
Several implementations have been made
\cite{onion-routing:ih96,onion-routing:pet2000,tor-design}, and there
was a similar commercial system, Freedom \cite{freedom1-arch}.  As of
October 2011, Tor \cite{tor-design}, the most recent iteration of the
basic design, consists of about 3000 routers, provides a total
bandwidth of over 1000 MB/s, and has an estimated total user
population of about 500,000~\cite{tor-metrics}.  Because of this
popularity, we believe it is important to improve our understanding of
the protocol.

Onion routing is a practical anonymity-network scheme with relatively low overhead and latency.  Users use a dedicated set of \emph{onion routers} to forward their traffic, obscuring the relationship between themselves and their destinations.  To communicate with a destination, a user selects a sequence of onion routers and constructs a \emph{circuit}, or persistant connection, over that sequence.  Messages to and from the destination are sent over the circuit.  Onion routing provides two-way, connection-based communication and does not require that the destination participate in the anonymity-network protocol.  These features make it useful for anonymizing much of the communication that takes place over the Internet today, such as web browsing, chatting, and remote login.  Thus, formal analysis and provable anonymity results for onion routing are significant.

As a step toward the overall goal of bridging the gap between provability and practicality in anonymous-communication systems, we have formally modeled and analyzed \emph{relationship anonymity} \cite{terminology,ShWa-Relationship} in Tor. Although this provides just a small part of the complete understanding of practical anonymity at which our research program is aimed, already it yields nontrivial results that require delicate probabilistic analysis. We hope that this aspect of the work will spur the Theoretical Computer Science community to devote the same level of attention to the rigorous study of anonymity as it has to the rigorous study of confidentiality. 

\subsection{Summary of Contributions}

{\bf Black-box abstraction:}
In the present paper, we treat the network simply as a ``black box''\footnote{We note that our use of a ``black box'' is slightly different than the more common uses in the literature. Black-box access to some cryptographic primitives is commonly used as a starting point to achieve some other desired functionality. Here we show how, for purposes of anonymity analysis, we need only consider a black-box abstraction.} to which users connect and
through which they communicate with destinations.  The abstraction
captures the relevant properties of a protocol execution that the
adversary can infer from his observations - namely, the observed
users, the observed destinations, and the possible connections between
the two.  In this way, we abstract away from much of the design
specific to onion routing so that our results apply both to onion
routing and to other low-latency anonymous-communication designs. 
We express the black-box model within the Universally Composable (UC) security
framework~\cite{cryptoeprint:2000:067}, which is a standard way to express the  
function and security properties of cryptographic protocols. We tie our functionality to 
the guarantees of an actual protocol by showing it reveals as much information about users' 
communication as the onion routing protocol we formalized~\cite{FC07} in an I/O-automata model. Moreover, we discuss how the functionality might be emulated by a protocol within the UC framework itself.

{\bf Probabilistic model:}
Our previous analysis in the I/O-automata model was possibilistic, a
notion of anonymity that is simply not sensitive enough. It makes no
distinction between communication that is equally likely to be from
any one of a hundred senders and communication that came from one
sender with probability  and from each of the other 99 senders
with probability .  An adversary in the real world is likely
to have information about which scenarios are more realistic than
others.  In particular, users' communication patterns are not totally
random.  When the adversary can determine with high probability,
\emph{e.g.}, the sender of a message, that sender is not anonymous in
a meaningful way.

Using this intuition, we include a probability measure in our black-box model.  For any set of actual sources and destinations, there is a larger set that is consistent with the observations made by an adversary.  The adversary can then infer conditional probabilities on this larger set using the measure.  This gives the adversary probabilistic information about the facts we want the network to hide, such as the initiator of a communication.


In the probability measure that we use, each user chooses a destination according to some probability distribution.  We model heterogeneous user behavior by  allowing this distribution to be different for different users.  We also assume that the users choose their circuits by selecting the routers on it independently and at random.


After observing the protocol, the adversary
can in principle infer some distribution on circuit source and
destination.  He may not actually know the underlying probability
measure, however.  In particular, it doesn't seem likely that the
adversary would know how every user selects destinations.  In our
analysis, we take a worst-case view and assume that the adversary
knows the distributions exactly.  Also, over time he might learn a
good approximation of user behavior via the long-term intersection
attack \cite{DanSer04}.  In this case, it may seem as though anonymity
has been essentially lost anyway.  However, even when the adversary
knows how a user generally behaves, the anonymity network may make it
hard for him to determine who is responsible for any specific action,
and the anonymity of a specific action is what we are interested in.

{\bf Anonymity metric:} We analyze \emph{relationship anonymity} \cite{terminology,ShWa-Relationship} in our onion routing model.  Relationship anonymity is obtained when the adversary cannot identify the destination of a user.  In terms of the conventional subject/action specification for anonymity \cite{terminology}, we can take the action to be communication from a given user  and the subject to be the destination.  Suggested probabilistic metrics for anonymity applied to this case include probability assigned to the correct destination \cite{crowds:tissec}, the entropy of the destination distribution \cite{Diaz02,Serj02}, and maximum probability within the destination distribution \cite{TOTH}, where the distribution in each case is a conditional distribution given the adversary's view.  We will use the probability assigned to the correct destination as our metric.  In part, this is because it is the simplest metric.  Also, any statements about entropy and maximum probability metrics only make loose guarantees about the probability assigned to the actual subject, a quantity that clearly seems important to the individual users.

We look at the value of this anonymity metric for a choice of
destination by a user.  Fixing a destination by just one user, say
, does not determine what the adversary sees, however.  The
adversary's observations are also affected by the destinations chosen
by the other users and the circuits chosen by everybody.  Because
those variables are chosen probabilistically under the measure we
added, the anonymity metric will have its own distribution.  Several
statistics about this distribution might be interesting; in this
paper, we look at its expectation. Unlike other common anonymity
metrics, our approach lets a user judge how secure he can expect a
specific communication activity to be and thus whether to do it or
not.

{\bf Bounds on Anonymity:}

The distribution of the anonymity metric for a given user and destination depends on the other users' destination distributions.  If their distributions are very different from that of the given user, the adversary may have an easy time separating out the actions of the user.  If they are similar, the user may more effectively hide in the crowd. We provide the following results on a user's anonymity and its dependence on other user behavior:

\begin{enumerate}
\item We show that a standard approximation to our metric
provides a lower bound on it (Thm.~\ref{thm:lwrbnd}).
\item We show that the worst case for anonymity over other users' behavior is
when every other user either always visits the destinations the user
is otherwise least likely to visit or always visits his actual
destination (Cor.~\ref{cor:worst}). The former will be the worst case in most situations.
\item We give an asymptotic expression for our metric in the worst cases (Thm.~\ref{thm:de}). The limit of this expression in the 
most common worst case with an adversary controlling a fraction  of the network is equal to the lower bound on the metric when the adversary controls a larger fraction  of the network. This is significantly worse than the standard analysis suggested, and shows the importance of carefully considering the adversary's knowledge of the system.
\item We consider anonymity in a more typical set of user distributions in which each user selects a destination from a common Zipfian distribution.  Because the users are identical, every user hides well among the others, and we show that, as the user population grows, the anonymity approaches the lower bound (Thm.~\ref{sec:typdist}). This shows you may be able to use the standard approximation with accurate results if you are able to make assumptions about user behavior.
\end{enumerate}

\subsection{Related Work}

Ours is not the first formalization of anonymous communication. Early formalizations used communicating sequential processes~\cite{schneider96}, graph theory and possible worlds~\cite{modular-approach}, and epistemic logic~\cite{GROUP,halpern-oneill-2003}.  These works focused primarily on formalizing the high-level concept of anonymity in communication. For this reason, they applied their formalisms to toy examples or systems that are of limited practical application and can only provide very strong forms of anonymity, \emph{e.g.}, dining-cryptographers networks.  Also, with the exception of \citeN{halpern-oneill-2003}, they have at most a limited ability to represent probability and probabilistic reasoning. We have focused in \citeN{FC07} on formalizing a widely deployed and used, practical, low-latency system.

Halpern and O'Neill~\shortcite{halpern-oneill-2003} give a general formulation of anonymity in systems that applies to our model.  They describe a ``runs-and-systems'' framework that provides semantics for logical statements about systems.  They then give several logical definitions for varieties of anonymity.  It is straightforward to	apply this framework to the network model and protocol that we give in~\cite{FC07}.  Our possibilistic definitions of sender anonymity, receiver anonymity, and relationship anonymity then correspond to the notion of ``minimal anonymity'' as defined in their paper.  The other notions of anonymity they give are generally too strong and are not achieved in our model of onion routing.

Later formalizations of substantial anonymous communication
systems~\cite{camlys05,MAUW,Wikstrom04} have not been directly based on the
design of deployed systems and have focused on provability without
specific regard for applicability to an implemented or implementable
design. Also, results in these papers are for message-based
systems: each message is constructed to be processed as a
self-contained unit by the appropriate router, typically using the
generally available public encryption key for that router. Such
systems typically employ mixing, changing the appearance and
decoupling the ordering of input to output messages at the router to
produce anonymity locally~\cite{chaum-mix}.  Onion routing, on the
other hand, is circuit based: before passing any messages with user
content, onion routing first lays a circuit through the routers that
provides those routers the keys to be used in processing the actual
messages. Mixing can be combined with onion routing in various
ways~\cite{onion-routing:jsac98}, although this is not
typical~\cite{tor-design}. Such circuit creation facilitates
bidirectional, low-latency coommunication and has been an identifying
feature of onion routing since the first public use of the
phrase~\cite{onion-routing:ih96}. Thus, while illuminating and
important works on anonymous communication, the formalizations
above are not likely to be applicable to low-latency communications, and,
despite the title of \cite{camlys05}, are not analyses of onion
routing.


In this paper, we add probabilistic analysis to the framework of \citeN{FC07}. Other works have presented probabilistic analysis of anonymous communication~\cite{crowds:tissec,SHMAT,WRIGHT,statistical-disclosure,DanSer04,e2e-traffic,stop-and-go} and even of onion routing~\cite{onion-routing:pet2000}.  The work of Shmatikov and Wang~\shortcite{ShWa-Relationship} is particularly similar to ours.  It calculates relationship anonymity in mix networks and incorporates user distributions for selecting destinations.  However, with the exception of~\cite{SHMAT}, these have not been formal analyses. Also, whether for high-latency systems such as mix networks, or low-latency systems, such as Crowds and onion routing, many of the attacks in these papers are some form of intersection attack. In an intersection attack, one watches repeated communication events for patterns of senders and receivers over time. Unless all senders are on and sending all the time (in a way not selectively blockable by an adversary) and/or all receivers receiving all the time, if different senders have different receiving partners, there will be patterns that arise and eventually differentiate the communication partners. It has long been recognized that no system design is secure against a long-term intersection attack. Several of these papers set out frameworks for making that more precise. In particular, \cite{statistical-disclosure}, \cite{DanSer04}, and \cite{e2e-traffic} constitute a progression towards quantifying how long it takes (in practice) to reveal traffic patterns in realistic settings.

We are not concerned herein with intersection attacks. We are effectively assuming that the intersection attack is done. The adversary already has a correct distribution of a user's communication partners.  We are investigating the anonymity of a communication in which a user communicates with one of those partners in the distribution.  This follows the anonymity analyses performed in much of the literature \cite{stop-and-go,MAUW,crowds:tissec,onion-routing:pet2000}, which focus on finding the source and destination of an individual communication.  Our analysis differs in that we take into account the probabilistic nature of the users' behavior.

We expect this to have potential practical applications. For example, designs for shared security-alert repositories to facilitate both forensic analysis for improved security design and quicker responses to widescale attacks have been proposed~\cite{LINCOLN04}.  A participant in a shared security-alert repository might expect to be known to communicate with it on a regular basis. Assuming reports of intrusions, etc., are adequately sanitized, the concern of the participant should be to hide when it is that updates from that participant arrive at the repository, \emph{i.e.}, which updates are likely to be from that participant as opposed to others.

\section{Technical Preliminaries}
\subsection{Model}
We describe our analysis of onion routing in terms of an ideal functionality in the Universal Composability framework \cite{cryptoeprint:2000:067} 
We use such a functionality for
three reasons: First, it abstracts away the details that aren't relevant to anonymity,
second, it precisely expresses the cryptographic protocol properties that are necessary for our analysis to apply, and
third, it immediately suggests ways to perform similar
analyses of other anonymous-communication protocols that may not strictly provide this functionality.

In the onion routing protocol on which we base our model, users choose
from a generally known set of onion routers a subset that will
comprise a circuit for communicating anonymously.  Circuit
construction has been done in various ways throughout the history of
onion routing.  In the first version of onion
routing~\cite{onion-routing:ih96}, and other early
versions~\cite{onion-routing:jsac98,freedom1-arch}, after a user selects a
sequence of onion routers from a publicly-known set, the user then creates a
circuit through this sequence using an \emph{onion}, a data structure
effectively composed only of layers with nothing in the middle. There
is one public-key-encrypted layer for each hop in the circuit, the
decryption of which contains the identity of the next hop in the
circuit (if there is one) and keying material for passing data over
the established circuit. In later protocols, such as used in
Cebolla~\cite{cebolla} and Tor~\cite{tor-design}, the circuit is built
via a telescoping protocol that extends the circuit hop-by-hop, using
the existing circuit for each extension. For all of these, each hop
only communicates with the routers before and after it in the
sequence, and the messages are encrypted once for each router in the
circuit so that no additional information leaks about the identities
of the other routers or the destination of the circuit.
Cryptographic techniques are used so that message forgery is
countered. Some later designs returned to the non-interactive
circuit construction of the
original~\cite{overlier:pet2007,pairing:pet2007}. It is trivial to see
that all of these fit directly within our model.

Some versions of onion routing, such as those that do iterative
discovery of onion routers via a
DHT~\cite{tarzan:ccs02,ccs09-shadowwalker,ccs09-torsk}, will not fit
within our model without some extensions that we do not pursue herein.
This is because the probability of first-last router choice and router
compromise within a circuit can no longer be assumed to be
independent. Some anonymity protocols that do not use onion routing
may nonetheless also fit within our model, appropriately extended.
For example, in Crowds~\cite{crowds:tissec}, the adversary can learn
from observing the first and last routers, but the connection to the
first router does not automatically identify the source. On the other
hand the destination is always know to every router in the
circuit. The probability that an observed circuit predecessor is the
source can thus be combined with the observed destination and the a
priori source-destination probability distribution.

The adversary is computationally bounded, non-adaptively compromises
an unknown subset of the onion routers, and can actively attack the
protocol. The design of our functionality is based on the assumption that the
ways that the adversary can narrow down the possible mappings of users
to destinations is determined by the set of circuits for which he
controls the first router and the set of circuits for which he
controls the last router. This assumption comes from the results of \citeN{FC07},
which we explicitly relate to our ideal functionality in Sec.~\ref{subsec:fc2uc}.



Our ideal functionality models anonymous communication over some period of time. It takes as input from each user the identity of a destination. For every such connection between a user and destination, the functionality may reveal to the adversary identity of the user, the identity of the destination, or both. Revealing the user corresponds in onion routing to the first router in the circuit being compromised, and revealing the destination corresponds to the last router being compromised. We note that we include only information flow to the adversary in this functionality rather than try to capture the type of communication primitive offered by onion routing because our focus is analyzing anonymity rather than defining a useful anonymous-communication functionality. This model is reminiscent of the general model of anonymous communication used by Kesdogan et al.~\shortcite{limits-open} in their analysis of an intersection attack. However, we do make a few assumptions that are particularly appropriate for onion routing.

First, the functionality allows the adversary to know whether or not he has directly observed the user. This is valid under the assumption that the initiating client is not located at an onion router itself. This is the case for the vast majority of circuits in Tor and in all significant deployments of onion routing and similar systems to date. We discuss this assumption further in Section~\ref{conclusions}.

Second, we assume that every user is
responsible for exactly one connection in a round.  Certainly users can
communicate with multiple destinations simultaneously in actual
onion-routing systems.  However, it seems likely that in practice most
users have at most some small (and fixed-bound) number of active
connections at any time. To the extent that multiple connections are
only slightly more likely to be from the same user than if all
connections were independently made and identically distributed, this
is a reasonable approximation. This is increasingly true as the
overall number of connections grows. To the extent that multiple
connections are less likely to be from the same user this is a
conservative assumption that gives the adversary as much power to
break anonymity as the limited number of user circuits can provide.

Third, the functionality omits the possibility that the adversary observes the user and destination but does not recognize that 
those observations are part of the some connection. This is another
conservative assumption that is motivated by the existence of timing
attacks that an active adversary can use to link traffic that it sees
at various points along its path through the network
\cite{onion-routing:pet2000}.  In a timing attack, the adversary
observes the timing of the messages going into the onion-routing
network and matches them to similar patterns of messages coming out of
the onion-routing networks slightly later.  Such attacks have been
experimentally demonstrated \cite{hs-attack06,bauer:wpes2007} and are
easy to mount.

Note that our model does not capture several known attacks on
anonymity in onion routing.  In particular, it does not include
attacks exploiting resource interference \cite{torta05,ccs06-hotclockskew},
heterogeneity on network latency \cite{tissec-latency-leak},
correlated destinations between rounds, and identifying patterns of
communication \cite{ccsw09-fingerprinting}.  We do not include such
attacks primarily to focus on the most important threats to anonymity,
because many of the omitted attacks are attacks on underlying systems
rather than on the protocol (e.g., interference) or have limited
effectiveness or are mitigated by improvements to the protocol.  Also,
we see the analysis of our simplified model as a first step in
establishing rigorous guarantees of anonymity in increasingly
realistic models.


Let  be the set of users with .  Let  be the set of destinations. Let  be the set of onion routers. Let  be the ideal functionality.  takes the set  of compromised parties from the adversary at the beginning of the execution. Let . When user  forwards his input from the environment to , the functionality checks to see if it is some . If so,  sends to the adversary one of the following, choosing each with the probability given:

\begin{tabular}{ll}
\centering
(1) &  with probability \\
(2) &  with probability \\
(3) &  with probability \\
(4) &  with probability .
\end{tabular}



To analyze the anonymity provided by the ideal functionality, we make two assumptions about the inputs from the environment. First, we assume that the environment selects the destination of user  from a distribution  over , where we denote the probability that  chooses  as . Second, we assume that the environment sends a destination to each user. Note that these assumptions need not be made when showing that a protocol UC-emulates . 

We refer to the combination of the adversary model, the assumptions about the environment, and the ideal functionality as the \emph{black-box model}. Let  be the relevant \emph{configuration} resulting from an execution.  includes a selection of a destination by each user, , a set of users whose inputs are observed, , and a set of users whose outputs are observed, . A user's input, output, and destination will be called its \emph{circuit}.

For any configuration, there is a larger set of configurations that are consistent with the outputs that the adversary receives from .  We will call two configurations \emph{indistinguishable} if the sets of inputs, outputs, and links between them that the adversary receives are the same.
We use the notation  to indicate that configurations  and  are indistinguishable.


\subsection{Probabilistic Anonymity}
A user performs an action anonymously in a possibilistic sense if there is an indistinguishable configuration in which the user does not perform the action.  For example, under this definition a user with observed output but unobserved input sends that output anonymously if there exists another user with unobserved input.  The probability measure we have added to configurations allows us to incorporate the degree of certainty that the adversary has about the subject of an action.  After making observations in the actual configuration, the adversary can infer a conditional probability distribution on configurations.  There are several candidates in the literature for assessing an anonymity metric from this distribution.  The probabilistic anonymity metric that we use is the posterior probability of the correct subject.  The lower this is, the more anonymous we consider the user.

\subsection{Relationship Anonymity}
We analyze the \emph{relationship anonymity} of users and destinations in our model, that is, how well the adversary can determine if a user and destination have communicated.  Our metric for the relationship anonymi\-ty of user  and destination  is the posterior probability  that  chooses  as his destination.  We study  directly, although the \emph{anonymity} of a user's communication with a destination is .

Using the posterior probability makes sense in this context because it
focuses on the information that users are trying to hide---their
actual destinations---without being affected by information the
adversary learns about other destinations.  Onion routing does leak
information, and using a metric such as the entropy of the posterior
distribution or the statistical distance from the prior may not give a
good idea of how well the adversary's can correctly guess the user's
behavior.  Designers may wish to know how well a system protects
communications on average or overall. But it is also important for a
user to be able to assess how secure he can expect a particular
communication to be in order to decide whether to create it or
not. This is the question we address. Moreover, the metric is
relatively simple to analyze. Furthermore, to the extent that the user
may not know how he fits in and thus wishes to know the worst risk for
any user, that is just a lower bound on our metric.

The relationship anonymity of  and  varies with the destination
choices of the other users and the observations of the adversary.  If,
for example, 's output is observed, and the inputs of all other
users are observed, then the adversary knows 's destination with
probability 1.  Because we want to examine the relationship anonymity
of  conditioned only on his destination, we end up with a
distribution on the anonymity metric.  We look at the expectation of
this distribution.  Moreover, because this distribution depends on the
destination distributions of all of the users, we continue by finding
the worst-case expectation in the limit for a given user and
destination and then examine the expectation in a more likely
situation.


\subsection{Emulating the Ideal Functionality} \label{subsec:fc2uc}
The anonymity analysis of the ideal functionality  that we perform in Sections~\ref{sec:expanon} and \ref{sec:typdist} is meaningful to the extent that  captures the information that an adversary can obtain by interacting with onion-routing protocols. We justify the functionality primarily by showing that it provides the same information about the source of a given connection as onion-routing as formalized by \citeN{FC07}. Furthermore, towards a more standard cryptographic analysis, we describe the way in which it should be possible to UC-emulate , although we do not provide such a result here.

{\bf Relationship to I/O-automata model}
\citeN{FC07} formalize onion routing using an I/O-automata model\cite{LYNCH} and an idealization of the cryptographic properties of the protocol. Their analysis identifies the user states that are information-theoretically indistinguishable. The black-box model we provide herein is a valid abstraction of that formalization because, under some reasonable probability measures on executions, it preserves the relationship-anonymity properties.

The I/O-automata model includes a set of users , a set of routers , an adversary , and a set of destinations , where we take the final router in the I/O-automata model to be the destination and assume that it is uncompromised.  A configuration  in the I/O-automata model is a mapping from each user  to a circuit , a destination , and a circuit identifier .  An \emph{execution} is a sequence of I/O-automaton states and actions, which must be consistent with the configuration.

Let users in the I/O-automata model choose the other routers in their circuits uniformly at random and choose the destination according to user-specific distributions.  Given these circuits and a set of adversary automata, \citeN{FC07} identifies an equivalence class of circuit and destination choices such that, for every pair of configurations in the class, a bijection exists between their executions such that paired executions are indistinguishable.  Let the indistinguishable executions thus paired have the same probability, conditional on their configuration.

Given this measure, the black-box model that abstracts the I/O-automata model has the same user set , the same destination set , an adversary parameter of , and the same destination distributions.  The following theorem shows that each posterior distribution on the destinations of users has the same probability under both the I/O-automata model and its black-box model.  Let  be a random I/O-automata execution.  Let  be a random I/O-automata configuration ( can be viewed as a function mapping a random execution to its configuration).  Let  be a random black-box configuration.  Let  be the posterior probability that  visited  in the I/O-automata model, \emph{i.e.}, the conditional given that the execution is indistinguishable from .  Let  be the posterior probability that  visited  in the black-box model, \emph{i.e.}, the conditional distribution given that the configuration is indistinguishable from .  Let  be a distribution over destinations  for every .
\begin{theorem}

\end{theorem}
\begin{proof}
Let  be the map from I/O-automata configurations to black-box configurations such that
\begin{enumerate}
\item 
\item 
\item .
\end{enumerate}
 essentially ``quotients out''  the specific router choices of each user, retaining the compromised status of the first and last routers as well as the destination.   It allows us to relate the posterior  in the I/O-automata model to the  in the black-box model.

Let  be any I/O-automata configuration.  Given any execution  of , the adversary's posterior probability on configurations is

if  and  otherwise, because we set equal the probability of two executions that are paired with each other in the bijection on executions constructed in \citeN{FC07}.  Because the configurations determine which destination each user visits, the distribution  can be determined from the posterior distribution on configurations.  Notice that this distribution only puts positive probability on the set  of configurations that are indistinguishable from .

The posterior distribution on I/O-automata configurations induces a posterior distribution on black-box configurations via .   preserves the destination of each user, and so the distribution  can be determined from this distribution on black-box configurations.  Notice that this distribution only puts positive probability on the set of black-box configurations  that are mapped to by I/O-automata configurations in .

To understand the set  and its posterior distribution given , consider the equivalence class  of the configuration .  Let  be those configurations in  that differ from  only in the destinations and the permutation of users.   From Theorems 1 and 2 in \citeN{FC07}, it follows that  is a bijection between  and .  The posterior probability of each  is proportional to  because the prior probability of  is  multiplied by the probability selecting its given routers (which are the same for all ) given that .  Moreover, all of the other configurations in  are reached by changing the unobserved routers of one of the configurations in .   is invariant under such a change.  Also, the posterior probability is invariant under such a change because the routers are chosen independently and uniformly at random.  Furthermore, the number of I/O-automata configurations that are reached by such a change from some  is the same for all .  Therefore, the posterior probability  is proportional to  for , and is zero otherwise.  Therefore, .

By this equality, the probability that a random execution  results in a given posterior  is equal to the probability that the I/O-automata configuration  maps under  to a black-box configuration  such that .  The probability  is equal to  because the probability of first-router compromise and the probability of an input being observed are both , last-router compromise and an output being observed are both independent events with probability , and user destinations are chosen independently in both models and follow the same distributions.  Therefore,

\end{proof}

{\bf UC-emulation}
Expressing our black-box model within the UC framework allows it to be compared to protocols expressed within the same framework. In particular, if a protocol can be shown to UC-emulate , then, making only common cryptographic assumptions, the adversary can make only negligibly better guesses about users' communication when interacting with that protocol than he can with the functionality. The results of \citeN{camlys05} suggest that such emulation is indeed possible. An onion routing protocol similar to their protocol combined with a message transmission functionality that hides messages not to corrupt parties (cf. \citeN{cryptoeprint:2000:067}), should indeed hide the routers that are not corrupt or next to corrupt routers on a circuit. Then  provides the adversary with all the information about user inputs that a simulator needs in order to simulate the rest of the protocol and confuse the adversary.

\section{Expected Anonymity} \label{sec:expanon}
Let the set  of all configurations be the sample space and  be a random configuration. Let  be the posterior probability of the event that  chooses  as a destination, that is, .   is our metric for the relationship anonymity of  and .

Let  represent the set of multisets over .  Let  be the maximum number of orderings of  such that the same destination is in any given location in every ordering:


Let  be the set of all injective maps .  The following theorem gives an exact expression for the conditional expectation of  in terms of the underlying parameters , , , and :

\begin{theorem} \label{yexp}





\end{theorem}
\begin{proof}
At a high level, the conditional expectation of  can be expressed as:


We calculate  for a configuration  by finding the relative weight of indistinguishable configurations in which  selects .  The adversary observes some subset of the circuits.  If we match the users to circuits in some way that sends users with observed inputs to their own circuits, the result is an indistinguishable configuration.  Similarly, we can match circuits to destinations in any way that sends circuits on which the output has been observed to their actual destination in .

The value of  is especially simple if 's input has been observed.  If the output has not also been observed, then .  If the output has also been observed, then .

For the case in which 's input has not been observed, we have to take into account the destinations of and observations on the other users.  Let  be the set of users  such that .  Note that .  Let  be the multiset of the destinations of circuits in  on which the input has not been observed, but the output has.

Let  be the probability that in a random configuration the set of unobserved inputs is  and the set of observed destinations with no corresponding observed input is :


Let  be the probability that in a random configuration the set of unobserved inputs is , the set of observed destinations with no corresponding observed input is , the output of  is observed, and the destination of  is :


Let  be the probability that in a random configuration the set of unobserved inputs is , the set of observed destinations with no corresponding observed input is , the output of  is unobserved, and the destination of  is :


Now we can express the posterior probability  as:


The expectation of  is a sum of the above posterior probabilities weighted by their probability.  The probability that the input of  has been observed but the output hasn't is .  The probability that both the input and output of  have been observed is .  These cases are represented by the first two terms in Equation~\ref{ey3}.

When the input of  has not been observed, we have an expression of the posterior in terms of sets  and .  The numerator () of Equation~\ref{yform} itself actually sums the weight of every configuration that is consistent with , , and the fact that the destination of  is .  However, we must divide by , because we condition on the event .

These observations give us the final summation in Equation~\ref{ey3}.
\end{proof}

\subsection{ Simple approximation of conditional expectation}
The expression for the conditional expectation of  in Equation~\ref{ey3} is difficult to interpret.  It would be nice if we could find a simple approximation.  The probabilistic analysis in \citeN{onion-routing:pet2000} proposes just such a simplification by reducing it to only two cases:  the adversary observes the user's input and output and therefore identifies his destination, and  the adversary doesn't observe these and cannot improve his \emph{a priori} knowledge.  The corresponding simplified expression for the expection is:

This is a reasonable approximation if the final summation in Equation~\ref{ey3} is about .  This summation counts the case in which 's input is not observed, and to achieve a good approximation the adversary must experience no significant advantage or disadvantage from comparing the users with unobserved inputs () with the discovered destinations ().

The quantity  does provide a lower bound on the final summation.  It may seem obvious that considering the destinations in  can only improve the accuracy of adversary's prior guess about 's destination.  However, in some situations the posterior probability for the correct destination may actually be smaller than the prior probability.  This may happen, for example, when some user , , communicates with a destination , , and only  is \emph{a priori} likely to communicate with .  If the adversary observes the communication to , it may infer that it is likely that  was responsible and therefore didn't choose .

It is true, however, that in expectation this probability can only increase.  Therefore Equation~\ref{lower} provides a lower bound on the anonymity metric.

The proof of this fact relies on the following lemma.  Let  be an event in some finite sample space .  Let  be a set of disjoint events such that , and let .  Let .  Finally, let  (where  is the indicator function for ).  is thus the conditional probability , where .

\begin{lemma} \label{minlem}

\end{lemma}
\begin{proof}

\end{proof}


\begin{theorem} \label{thm:lwrbnd}

\end{theorem}
\begin{proof}
As described in the proof of Theorem~\ref{yexp}:


To apply Lemma~\ref{minlem}, take the set of configurations  to be the sample space .  Take  to be the event .  Take the indistinguishability equivalence relation to be the sets .  Finally, take  to be .  Then the lemma shows that .
\end{proof}

\subsection{Worst-case Anonymity}
To examine the accuracy of our approximation, we look at how large the final summation in Equation~\ref{ey3} can get as the users' destination distributions vary.  Because this is the only term that varies with the other user distributions, this will also provide a worst-case guarantee on expected anonymity metric.  Our results will show that, in the limit as the number of users grows, the worst case can occur when the users other than  act as differently from  as possible by always visiting the destination  is otherwise least likely to visit.  Less obviously, we show that the limiting maximum can also occur when the users other than  always visit .  This happens because it makes the adversary observe destination  often, causing him to suspect that  chose .  Our results also show that the worst-case expectation is about , which is significantly worse than the simple approximation above.

As the first step in finding the maximum of Equation~\ref{ey3} over , we observe that it is obtained when every user  chooses only one destination , \emph{i.e.}  for some .

\begin{lemma} \label{vertex}
  A maximum of  over  must occur when, for all , there exists some  such that .
\end{lemma}
\begin{proof}
Take some user  and two destinations .  Assign arbitrary probabilities in  to all destinations except for , and let .  Then .  Consider  as a function of .  The terms  of Equation~\ref{ey3} that correspond to any fixed  and  are of the following general form, where :


This is a convex function of :


The leading two terms of  are constant in , and the sum of convex functions is a convex function, so  is convex in .  Therefore, a maximum of  must occur when .\hfill\end{proof}

Order the destinations  such that  for .  The following lemma shows that we can further restrict ourselves to distribution vectors in which, for every user except , the user either always chooses  or always chooses .

\begin{lemma} \label{ef}
A maximum of  must occur when, for all users , either  or .
\end{lemma}
\begin{proof}
Assume, following Lemma~\ref{vertex}, that  is an extreme point of the set of possible distribution vectors.

Equation~\ref{ey3} groups configurations first by the set  with unobserved inputs and second by the observed destinations .  Instead, group configurations first by  and second by the set  with observed outputs.  Because every user except  chooses a destination deterministically,  only depends on the sets  and .  Let  be this value.


Select two destinations .  We break up the sum in Equation~\ref{ey4} and show that, for every piece, the sum can only be increased by changing  so that any user that always chooses  always chooses  instead.

Fix  such that .  Let  be such that  if and only if , and  if and only if .  Fix  and some .

Let  be the sum of terms in Equation~\ref{ey4} that are indexed by  and some  such that  and .  To calculate , group its terms by the number  of users  in  such that .  Let  be the number for these terms of users  in  such that , .  The number  of users  such that  for these terms is then .  Let  be the number of users  in  such that . The number of terms in  with a given  is then

For each of these terms,  is the same.  To calculate it, let  be the number of configurations that yield the given  and  and are such that 's output is observed with destination :

and let  be the number of configurations that yield the same  and  and are such that 's output is unobserved:

Then the posterior probability given  and  is

Therefore, letting ,


The binomial coefficients of  and  in the numerator and denominator largely cancel, and the whole expression can be simplified to

for some .



This can be seen as the weighted convolution of binomial coefficients.  Unfortunately, there is no obvious way to simplify the expression any further to find the maximum as we trade off  and .  There is a closed-form sum if the coefficient of the binomial product is a fixed-degree polynomial, however.  Looking at the coefficient, we can see that it is concave.


We can use this fact to bound the sum above by replacing  with a line tangent at some point .  Call this approximation .  Holding  constant, this approximation is in fact equal at  because the sum has only one term.  Then, if  still maximizes the sum, the theorem is proved.  Let .



The linear approximation will be done around the point .  This results in a simple form for the resulting approximation, and also the mass of the product of binomial coefficients concentrates around this point.  Set  to examine the tradeoff between  and .



Lemma~\ref{lem:d2f} in the Appendix shows that  is convex in .  Thus, the maximum of  must exist at  or .  Observe that when ,

and when 

Therefore, because ,  is larger when .  As stated, this implies that  itself is maximized when .\hfill

\end{proof}

Therefore, in looking for a maximum we can assume that every user except  either always visits  or always visits .  To examine how anonymity varies with the number of users in each category, we derive an asymptotic estimate for large .  A focus on large  is reasonable because anonymity networks, and onion routing in particular, are understood to have the best chance at providing anonymity when they have many users.  Furthermore, Tor is currently used by an estimated 500,000 people.

Let  be the fraction of users that always visit .  Theorem~\ref{thm:de} gives an asymptotic estimate for the expected posterior probability given a constant .  It shows that, in the limit, the maximum expected posterior probability is obtained when all users but  always visit  or when they always visit .
\begin{theorem} \label{thm:de}
Assume that, for all , either   or .  Then, if ,

if 

and, if ,

\end{theorem}
\begin{proof}
Let  and .  The expected posterior probability can be given in the following variation on Equation~\ref{ey4}:

Here  is the value of  when the users with unobserved inputs consist of ,  users  with , and  users  with ; and the users with unobserved inputs and observed outputs consist of  users  with  and  users  with .  Given such a configuration, the number of indistinguishable configurations in which  has observed destination  is , the number of indistinguishable configurations in which  has observed destination  is , and the number of indistinguishable configuration in which  has an unobserved destination is .  Thus, we can express  as

The binomial coefficients largely cancel, and so we can simplify this equation to


We observe that  and  are binomially distributed.  Therefore, by the Chernoff bound, they concentrate around their means as  and  grow.  Let  be the mean of  and   be the mean of .  We can approximate the tails of the sums over  and  in Equation~\ref{eq:yasymp} and sum only over the central terms:


As  and  concentrate around their means,  will approach its value at those means.  Let

be the difference of  from its value at  and +u, where  indicates if 's output is observed.

 is non-increasing in  and is non-decreasing in :
1+f-j-u) (p^u_d (e+1) +(1-p^u_d-p^u_{d_{|\Delta|}})(1+e-k)) \end{array} \right)^2}\\
&\le 0.\\
D_k \Psi_2 &= \frac{(1+e) (1+f-j) p^u_d  (p^u_{d_{|\Delta|}}(1+f)+(1-p^u_d-p^u_{d_{|\Delta|}})(1+f-j))}{\left( \begin{array}{l} (1+f) (1+e-k-u)p^u_{d_{|\Delta|}} +\\ (1+f-j) ((1+e) p^u_d +(1+e-k-u) (1-p^u_d-p^u_{d_{|\Delta|}}) ) \end{array} \right)^2}\\
&\ge 0.

&\max_{\substack{\sigma\in \{-1,1\}\\ u\in \{0,1\} }}\left(\left| \varepsilon_1 \left(\mu_1 + \sigma \sqrt{c_1 f},\mu_2 + \sigma \sqrt{c_2 e},u \right) \right| \right)\\
&\qquad = \max_{\substack{\sigma\in \{-1,1\}\\ u\in \{0,1\} }} \left| \Psi_2(e,f,\mu_1 + \sigma \sqrt{c_1 f},\mu_2 + \sigma \sqrt{c_2 e}+u) - \Psi_2(e,f,\mu_1,\mu_2+u) \right|\\
&\qquad = O\left( \sqrt{c_1/f} \right) + O\left( \sqrt{c_2/e} \right),

\Psi_2(e,f,j,k+u) &= \Psi_2(e,f,\mu_1,\mu_2+u) + \varepsilon_1(j,k,u)\\
&= \Psi_2(e,f,\mu_1,\mu_2+u) + O\left( \sqrt{c_1/f} \right) + O\left( \sqrt{c_2/e} \right).
 \label{eq:yasymp2.5}
\begin{split}
  &E[\Psi | X_D(u)=d] = b(1-b)p^u_d + b^2 + \\
  &\qquad (1-b) \sum_{e=0}^{n_e} \binom{n_e}{e} (1-b)^e b^{n_e-e} \sum_{f=0}^{n_f} \binom{n_f}{f} (1-b)^f b^{n_f-f} \cdot\\
  &\qquad \bigg[ b \Psi_2(e,f,\mu_1,\mu_2+1) + (1-b)\Psi_2(e,f,\mu_1,\mu_2) +\\
  &\qquad \qquad O\left( \sqrt{\log(f)/f} \right) + O\left( \sqrt{\log(e)/e} \right) \bigg].
\end{split}

\sum_{e=0}^{n_e} \binom{n_e}{e} (1-b)^e b^{n_e-e} \sum_{f=0}^{n_f} \binom{n_f}{f} (1-b)^f b^{n_f-f} O\left(\sqrt{\log(e)/e}\right) = O\left(\sqrt{\log(n_e)/n_e} \right).

\sum_{e=0}^{n_e} \binom{n_e}{e} (1-b)^e b^{n_e-e} \sum_{f=0}^{n_f} \binom{n_f}{f} (1-b)^f b^{n_f-f} O\left(\sqrt{\log(f)/f} \right) = O\left(\sqrt{\log(n_f)/n_f} \right).
 \label{eq:yasymp3}
\begin{split}
  &E[\Psi | X_D(u)=d] = b(1-b)p^u_d + b^2 +\\
  &\qquad O\left( \left( log(n_e)/n_e \right)^{-1/2} \right) + O\left( \left( log(n_f)/n_f \right)^{-1/2} \right) + O\left( e^{-2 c_3} \right) + O\left( e^{-2 c_4} \right) + \\
  &\qquad (1-b) \sum_{e: |e-\mu_3|<\sqrt{c_3 n_e}} \binom{n_e}{e} (1-b)^e b^{n_e-e} \sum_{f: |f-\mu_4| < \sqrt{c_4 n_f}} \binom{n_f}{f} (1-b)^f b^{n_f-f} \cdot\\
  &\qquad \qquad \left[ b \Psi_2(e,f,\mu_1,\mu_2+1) + (1-b)\Psi_2(e,f,\mu_1,\mu_2) \right].
\end{split}

\varepsilon_2(e,f,u) = \Psi_2(e,f,\mu_1,\mu_2+u) - \Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u)

D_e \Psi_2(e,f,\mu_1,\mu_2) &= \frac{(1+(1-b) f) b p^u_d  ((f+1)(1-p^u_d) - fb(1-p^u_d-p^u_{d_{|\Delta|}}))}{\left( \begin{array}{l} (1+(1-b)f)(1+(1-b)e) + \\ (1+(1-b) f)(be)p^u_d + \\ b f (1+(1-b) e+u) p^u_{d_{|\Delta|}} \end{array} \right)^2}\\
&\ge 0.

D_e \Psi_2(e,f,\mu_1,\mu_2) &= \frac{(1+(1-b) f) (1-b) p^u_d  (fb(1-p^u_{d_{|\Delta|}}-p^u_d)-(f+1)(1-p^u_d))}{\left( \begin{array}{l} ((1-b)f)(1+(1-b)e) + \\ (1+(1-b) f)(be+1)p^u_d + \\ b f ((1-b) e) p^u_{d_{|\Delta|}} \end{array} \right)^2}\\
&\le 0.

D_f \Psi_2(e,f,\mu_1,\mu_2+u) &= \frac{ - b(1+e)(1+(1-b) e+u) p^u_d p^u_{d_{|\Delta|}} }{\left( \begin{array}{l} (1+(1-b)f)(1+(1-b)e+u) +\\  (1+(1-b)f)(be+u)p^u_d + \\ bf(1+(1-b)e+u)p^u_{d_{|\Delta|}} \end{array} \right)^2}\\
&\le 0.

\max_{ \substack{\sigma\in\{-1,1\}\\ u\in \{0,1\} }} \left( \left| \varepsilon_2 \left(\mu_3+\sigma\sqrt{c_3 n_e},\mu_4+\sigma\sqrt{c_4 n_f},u \right) \right| \right).

\varepsilon_2 \left(\mu_3+\sigma\sqrt{c_3 n_e},\mu_4+\sigma\sqrt{c_4 n_f},u \right) = &\Psi_2(\mu_3+\sigma\sqrt{c_3 n_e},\mu_4+\sigma\sqrt{c_4 n_f},\mu_1,\mu_2+u) - \\
&\Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u)\\
= &O\left( \sqrt{c_3/n_e} \right) + O\left( \sqrt{c_4/n_f} \right) .

\varepsilon_2 \left(0,\mu_4+\sigma\sqrt{c_4 n_f},u \right) &= \Psi_2(0,\mu_4+\sigma\sqrt{c_4 n_f},\mu_1,u) - \Psi_2(0,\mu_4,\mu_1,u)\\
&= O\left( \sqrt{c_4/n_f} \right).

\varepsilon_2 \left(\mu_3+\sigma\sqrt{c_3 n_e},0,u \right) &= \Psi_2(\mu_3+\sigma\sqrt{c_3 n_e},0,0,\mu_2+u) - \Psi_2(\mu_3,0,0,\mu_2+u)\\
&= O\left( \sqrt{c_3/n_e} \right).

\Psi_2(e,f,\mu_1,\mu_2+u) &= \Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u) + \varepsilon_2(e,f,u)\\
&= \Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u) + O\left( \sqrt{c_3/n_e} \right) + O\left( \sqrt{c_4/n_f} \right).
 \label{eq:yasymp4}
\begin{split}
&E[\Psi | X_D(u)=d] = b(1-b)p^u_d + b^2 + \\
&\qquad (1-b) \left[ b \Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+1) + (1-b) \Psi_2(\mu_3,\mu_4,\mu_1,\mu_2) \right] + \\
&\qquad O\left( \left( log(n_e)/n_e \right)^{-1/2} \right) + O\left( \left( log(n_f)/n_f \right)^{-1/2} \right).
\end{split}

\Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u) &= \Psi_2((1-b)n_e,(1-b)n_f,b(1-b)n_f,b(1-b)n_e+u)\\
&= \frac{p^u_d (1-b)^3 n_e n_f + c_1 n_e + c_2 n_f + c_3 }{\left( \begin{array}{l} ((1-b)^4 + p^u_d (1-b)^3 b + p^u_{d_{|\Delta|}}(1-b)^3 b) n_e n_f +\\ c_4 n_e + c_5 n_f + c_6 \end{array} \right)}\\
&= \frac{p^u_d }{1-b + p^u_d b + p^u_{d_{|\Delta|}}b} + O(1/n_e) + O(1/n_f) + O(1/(n_e n_f)),

\Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u) &= \Psi_2(0,(1-b)n_f,b(1-b)n_f,u)\\
&= \frac{p^u_d (1-b) n_f + c_1}{ ((1-u)(1-b) + p^u_d u (1-b) + p^u_{d_{|\Delta|}} (1-u)b ) n_f + c_2}\\
&= \frac{p^u_d (1-b)}{ ((1-u)(1-b) + p^u_d u (1-b) + p^u_{d_{|\Delta|}} (1-u)b ) } + O(1/n_f),

\Psi_2(\mu_3,\mu_4,\mu_1,\mu_2+u) &= \Psi_2((1-b)n_e,0,0,b(1-b)n_e+u)\\
&= \frac{p^u_d n_e + c_1}{((1-b) + p^u_d b)n_e +c_2} \\
&= \frac{p^u_d}{1-b + p^u_d b} + O(1/n_e),

 \frac{p^u_d }{1-b + p^u_d b + p^u_{d_{|\Delta|}}b} \le \frac{p^u_d}{1-b + p^u_d b}.

p^u_{d_{|\Delta|}} \ge \frac{(1-b)(1-p^u_d)^2}{p^u_d(1+b)-b}.

E[\Psi|X_D(u)=d] = b^2+(1-b^2)p^u_d + O(1/n)
 \label{eq:typdist}
E[\Psi|X_D(u)=d] = b^2 + b(1-b)p^u_d +\\
(1-b)\sum_{s=1}^n b^{n-s} (1-b)^{s-1} \sum_{t=0}^{s} (1-b)^{s-t} b^t \binom{n-1}{s-1} \cdot \\
\left[\binom{s-1}{t-1} \sum_{\Delta \in D^t:\Delta_1=d} \prod_{i=2}^t p_{\Delta_i} \Psi_4(s,\Delta) + \binom{s-1}{t} \sum_{\Delta \in D^t} \prod_{i=1}^t p_{\Delta_i} \Psi_4(s,\Delta) \right].

\Psi_4(s,\Delta) &= \frac{\Delta_d (s-1)^{|\Delta|-1} + p_d (s-1)^{|\Delta|}}{s^{|\Delta|}}\\
&= (\Delta_d + p_d(s-t))/s.

\sum_{\Delta \in D^t:\Delta_1=d} \prod_{i=2}^t p_{\Delta_i} \Psi_4(s,\Delta)

\sum_{\Delta \in D^t} \prod_{i=1}^t p_{\Delta_i} \Psi_4(s,\Delta)

b\frac{p_d (s-1)+1}{s} + (1-b) p_d.

E[\Psi|X_D(u)=d] = &b^2 + b(1-b)p^u_d + \\
&(1-b)\sum_{s=1}^n b^{n-s} (1-b)^{s-1} \binom{n-1}{s-1} \left[ b\frac{p_d (s-1)+1}{s} + (1-b) p_d \right]\\
=  &b^2 + b(1-b)p^u_d + \\
&(1-b)\left[b\left(p_d + \frac{(1-p_d)(1-(1-b)^{n+1})}{b(n+1)}\right) + (1-b)p_d \right]\\
=  &b^2 + (1-b^2)p^u_d + O(1/n).

\tilde{f} = \frac{(\nu +i \mu ) (\nu +(\nu -i) \mu )}{p^u_{d_j} \nu (\nu +i \mu ) (1-i+\nu )+(1+i) p^u_{d_i} \nu  (\nu -i \mu +\nu  \mu )+\beta  (\nu +i \mu ) (\nu -i \mu +\nu  \mu )}.

D^2_{s_{d_i}} \tilde{f} = \frac{N}{D},

N = &-\bigg( (2 (i+j) (-i-j+\mu ) \\
&\qquad \left(-i^3 (p^u_{d_i}-p^u_{d_j}) \mu ^3 ((i+j) (p^u_{d_i}+p^u_{d_j})+\beta  \mu )+ \right. \\
&\qquad 3 i^2 (i+j) \mu^2 (p^u_{d_i}+p^u_{d_j}+p^u_{d_i} \mu ) ((i+j) (p^u_{d_i}+p^u_{d_j})+\beta  \mu )-\\
&\qquad 3 i (i+j)^2 \mu  ((i+j) (p^u_{d_i}+p^u_{d_j})+\beta  \mu ) \left(-p^u_{d_j}+p^u_{d_i} (1+\mu )^2\right)+\\
&\qquad (i+j)^3 \left((i+j) (p^u_{d_i})^2 (1+\mu )^3+p^u_{d_j} ((i+j) p^u_{d_j}+\beta  \mu )+ \right. \\
&\qquad\qquad \left. \left. p^u_{d_i} \left(\beta  \mu  (1+\mu )^3+p^u_{d_j} (2+\mu ) \left(-i-j+2 \mu +(1+i+j) \mu^2\right)\right)\right)\right)\bigg)

D = \bigg((i+j)^2 (p^u_{d_i}+i p^u_{d_i}+p^u_{d_j}+j p^u_{d_j}+\beta )+\\
(i+j) (i (p^u_{d_j}+\beta )+j (p^u_{d_i}+i p^u_{d_i}+i p^u_{d_j}+\beta )) \mu +i j \beta  \mu^2\bigg)^3,

&2 p_{d_j} \beta (i+j)(i+j-\mu ) \mu  (i+j+i \mu )^3 + \\
&2 p_{d_i} \beta (i+j) (i+j-\mu ) \mu  (i+j+j \mu )^3 +\\
&2 (p^u_{d_j})^2 (i+j)^2 (i+j-\mu )(i+j+i \mu )^3 +\\
&2 (p^u_{d_i})^2 (i+j)^2 (i+j-\mu )(i+j+j \mu )^3 +\\
&2 p^u_{d_i} p^u_{d_j} (i+j)^2 (i+j-\mu ) (i+j)(2+\mu )\cdot \\
&\qquad \left(i^2 \left(-1+\mu ^2\right)+j \left(\mu  (2+\mu )+j \left(-1+\mu ^2\right)\right)+i \left(\mu  (2+\mu )-j \left(2+\mu ^2\right)\right)\right).

&i^3 \left((p^u_{d_i})^2+(p^u_{d_j})^2 (1+\mu )^3+p^u_{d_i} p^u_{d_j} \left(-2-\mu +2 \mu ^2+\mu ^3\right)\right) +\\
&j^3 \left((p^u_{d_j})^2+(p^u_{d_i})^2 (1+\mu )^3+p^u_{d_i} p^u_{d_j} \left(-2-\mu +2 \mu ^2+\mu ^3\right)\right) +\\
&i^2 p^u_{d_i} p^u_{d_j} \mu  (2+\mu )^2 +\\
&j^2 p^u_{d_i} p^u_{d_j} \mu  (2+\mu )^2 +\\
&2 i j p^u_{d_i} p^u_{d_j} \mu  (2+\mu )^2 +\\
&3 i^2 j \left((p^u_{d_i})^2 (1+\mu )+(p^u_{d_j})^2 (1+\mu )^2-p^u_{d_i} p^u_{d_j} (2+\mu )\right) +\\
&3 i j^2 \left((p^u_{d_j})^2 (1+\mu )+(p^u_{d_i})^2 (1+\mu )^2-p^u_{d_i} p^u_{d_j} (2+\mu )\right).

p_{d_i} = \frac{4 \zeta +5 \zeta  \mu +2 \zeta  \mu ^2}{2 (2+\mu )^2}.

\frac{\zeta ^2 \mu  \left(8+11 \mu +4 \mu ^2\right)}{4 (2+\mu )^2},

\frac{4 \zeta +3 \zeta  \mu }{2 (2+\mu )^2}.

\frac{\zeta ^2 \mu  (8+\mu  (11+4 \mu ))}{4 (2+\mu )^2},

which is non-negative.  Therefore, the whole  term is non-negative.  This implies that  is non-negative, and thus that  is non-negative.
\end{proof}

\bibliographystyle{acmsmall}
\bibliography{tor_prob_tissec}
\end{document}
