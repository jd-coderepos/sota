

\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{nicefrac}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{url}
\usepackage{bm}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[section]

\usepackage{booktabs}
\newcommand{\diag}[1]{ \text{diag}\!\left(#1\right)}





\title{AutoSF:
	Searching Scoring Functions for  
	Knowledge Graph Embedding}

\author{\IEEEauthorblockN{Yongqi Zhang\IEEEauthorrefmark{2},
			Quanming Yao\IEEEauthorrefmark{3}, Wenyuan Dai\IEEEauthorrefmark{3} and
			Lei Chen\IEEEauthorrefmark{2}}
		\IEEEauthorblockA{\IEEEauthorrefmark{2}The Hong Kong University of Science and Technology, Hong Kong SAR, China\\
			\IEEEauthorrefmark{3}4Paradigm Inc., Beijing, China\\
\IEEEauthorrefmark{2}\{yzhangee,leichen\}@cse.ust.hk,
			\IEEEauthorrefmark{3}\{yaoquanming,daiwenyuan\}@4paradigm.com}}
\begin{document}	

\maketitle

\begin{abstract}
Scoring functions (SFs), which measure the plausibility
of triplets in knowledge graph (KG), have become the crux of KG embedding. 
Lots of SFs,
which target at capturing different kinds of relations in KGs,
have been designed by humans in recent years.
However, 
as relations can exhibit complex patterns that are hard to infer before training,
none of them can consistently perform better than others on existing benchmark data sets. 
In this paper,
inspired by the recent success of automated machine learning (AutoML), 
we propose to automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques.
However,
it is non-trivial to explore domain-specific information here to make AutoSF efficient and effective.
We firstly identify a unified representation over popularly used SFs, 
which helps to set up a search space for AutoSF. 
Then, we propose a greedy algorithm to search in such a space efficiently.
The algorithm
is further sped up by a filter and a predictor,
which can avoid repeatedly training SFs with same expressive ability 
and
help removing bad candidates during the search before model training.
Finally,
we perform extensive experiments on benchmark data sets.
Results on link prediction and triplets classification
show that the searched SFs by AutoSF,
are KG dependent, new to the literature, and outperform the state-of-the-art SFs designed by humans.
\end{abstract}

\setcounter{figure}{0}
\setcounter{table}{0}

\section{Introduction}
\label{sec:intro}

Knowledge Graph (KG) \cite{singhal2012introducing,nickel2016review,wang2017knowledge}, 
as a special kind of graph structure with entities as nodes and relations as edges, 
is important to both data mining and machine learning,
and has inspired various downstream applications, 
e.g., structured search \cite{singhal2012introducing,dong2014knowledge,tang2016aminer},
question answering \cite{lukovnikov2017neural} and
recommendation \cite{zhang2016collaborative}.
In KGs, each edge is represented as a triplet with form \textit{(head entity, relation, tail entity)},
denoted as .
A fundamental issue is how to quantize
the plausibility 
of triplets s
\cite{getoor2007introduction,wang2017knowledge}.
KG embedding (KGE) has recently emerged and been developed as 
a promising method serving this purpose
\cite{nickel2011three,yang2014embedding,nickel2016holographic,dettmers2017convolutional,trouillon2017knowledge,liu2017analogical,kazemi2018simple,lacroix2018canonical,zhang2018nscaching}. 
Basically,
given a set of observed triplets, 
KGE attempts to learn low-dimensional vector representations of entities and relations
so that the plausibility of triplets can be quantized.
Scoring function (SF),
which returns a score for  based on the embeddings,
is used to measure the plausibility.
Generally,
SF is designed and chosen by humans
and it has significant effects on embeddings' quality \cite{nickel2016review,wang2017knowledge,lin2018knowledge}.

Ever since the invention of KGE, 
many SFs have been proposed in the literature.
Let the embedded vectors of
,
 and  
be ,
 and
 respectively.
TransE \cite{bordes2013translating},
a representative embedding model,
interprets the triplet  as a translation 
from head entity  to tail entity ,
i.e.  
the embeddings satisfy .
Variants of TransE like 
TransH \cite{wang2014knowledge} and
TransR \cite{lin2015learning},
project the embedding vector into different spaces and 
enables the embedding to model relationships that are one-to-many, many-to-one or many-to-many.
These models are categorized into translational distance models (TDMs).
However, as proved in \cite{wang2017multi,wang2018evaluating},
TDMs are not fully expressive 
and their empirical performance is inferior to other models.
RESCAL \cite{nickel2011three},
DistMult \cite{yang2014embedding},
ComplEx \cite{trouillon2017knowledge},
Analogy \cite{liu2017analogical}
and more recently proposed SimplE \cite{kazemi2018simple,lacroix2018canonical},
use a bilinear function  to model the plausibility of triplets,
where  is a square matrix related to relation embedding.
These models belong to the bilinear model (BLMs).
Different BLMs use different constrains to regularize the relation matrix 
in order to adapt to different datasets.
Inspired by the success of deep networks \cite{bengio2013representation},
some neural network models (NNMs) have also been explored as SFs,
e.g., MLP \cite{dong2014knowledge}, 
NTM \cite{socher2013reasoning},
Neural LP \cite{yang2017differentiable}
and ConvE \cite{dettmers2017convolutional}.
Even though neural networks are powerful and have strong expressive ability,
the NNMs do not perform well in KGE domain because of not being well-regularized.


Among the existing SFs,
BLM-based ones
are the most powerful 
as indicated by both the state-of-the-art results \cite{lacroix2018canonical} and theoretical guarantees on expressiveness \cite{wang2017multi, kazemi2018simple}.
However,
since
different KGs have distinct patterns in relations \cite{paulheim2017knowledge},
a SF which adapts well to one KG may not perform consistently well on other KGs.
Besides,
designing new SFs to outperform state-of-the-art SFs 
is challenging.
Therefore,
how to choose and design a good SF for a certain KG is a non-trivial and difficult problem.

Recently,
automated machine learning (AutoML) \cite{quanming2018auto,automl_book}
has exhibited its power in
many machine learning tasks and applications,
e.g. model selection \cite{feurer2015efficient},
image classification \cite{liu2018darts,yao2019differentiable}
and recommendation \cite{yao2019efficient}.
In order to select proper models and hyper-parameters for different tasks,
hyperparameter optimization (HPO) has been proposed \cite{feurer2015efficient,falkner2018bohb}
to effectively and efficiently find better configurations,
which previously requires great human efforts.
Another hot trend in AutoML is to search better neural networks for deep learning models.
Neural architecture search (NAS) \cite{zoph2017neural} 
has identified networks with fewer parameters and better performance than networks designed by humans.


Inspired by the success of AutoML,
we aim to design better and novel data-dependent SFs for KGE.
Specifically,
we propose automated scoring function (AutoSF)
which can automatically search an SF for a given KG.
It can not only reduce human's effort in designing new SFs,
but also make adaptation to different KGs.
However,
it is not easy to achieve the above goal.
When applying AutoML,
two important perspectives,
i.e. search space, which helps to figure out important properties of the underling problems,
and search algorithm, which determines the efficiency of finding better points in the space,
need serious consideration.
In this work,
we have made the following contributions to achieve the goals:
\begin{itemize}[leftmargin = 10px,itemsep=1px]
\item 
First,
we make an important observation over existing SFs,
which allows us to represent the BLM-based SFs in a unified form.
Based on the unified representation,
we formulate the design of SFs as an AutoML problem (i.e. AutoSF),
and set up the corresponding search space.
The space is not only specific enough
to cover good SFs designed by humans,
but also general enough to include 
novel SFs not visited in the literature.

\item 
Second, we observe it is common that different KGs have distinct properties
on relations
that are
symmetric, asymmetric, inverse, etc.
This inspires us to conduct domain-specific analysis on the KGE models,
and design constraints 
to effectively guide subsequent searches in the space.

\item 
Third, 
we propose a progressive greedy algorithm to search through such a space.
We further build a filter to avoid training redundant SFs
and a predictor with specifically designed symmetry-related features (SRF) to select promising SFs.
The search algorithm can significantly reduce the search space size
by capturing the domain specific properties of candidate SFs.

\item
Finally, experimental results on five popular benchmarks on link prediction and triplet classification tasks
demonstrate that the SFs searched by AutoSF
outperform the start-of-the-art SFs
designed by humans.
In addition,
the searched SFs
are KG dependent and new to the literature.
We further conduct case study on the searched SFs 
to provide means for analyzing KGs,
which can inspire better understanding of embedding techniques for future researches.
\end{itemize}





{
\noindent
\textit{Notations.}
We denote vectors by lowercase boldface, and matrix by uppercase boldface.
A KG contains a set of triplets  with  and ,
where  and  are the set of entities and relations, respectively.
For simplicity, the embeddings are represented by letters of indices in boldface, 
e.g. , ,  are embeddings of , respectively,
and  share the same set of embedding parameters .
 is the triple dot product 
and can be alternatively represented as ,
where  is the diagonal matrix of .
We denote the complex vector  with .
The conjugate of a complex vector is .
}




\section{Related Works}
\label{sec:relworks}


\subsection{Knowledge graph embedding (KGE)}
\label{ssec:score}


\begin{table*}[ht]
	\centering
	\caption{Existing SFs covered by our search space.
		For Analogy and SimplE,
		the embedding splits into two parts,
		i.e.  
		and  (same for  and ).
		The relation types are summarized in Tab.~\ref{tab:comrel}.
	}
	\vspace{-7px}
	\label{tab:scofun}
	\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c|c|C{4cm}|c}
		\hline
		scoring function                               &                                                                   embeddings                                                                    &                                                                                                 definition                                            &      relation types that can model                    \\ \hline
		DistMult \cite{yang2014embedding}                      &            symmetric                                                                         &                                                                                                                                      &      symmetric               \\ \hline
		{ComplEx \cite{trouillon2017knowledge} / HolE \cite{nickel2016holographic}} &                                                                                    &                                                     Re                                       &    symmetric, anti-symmetric, asymmetric, inverse       \\ \hline
		Analogy \cite{liu2017analogical}              & ,  &  + Re &	symmetric, anti-symmetric, asymmetric, inverse	\\ \hline
		{SimplE \cite{kazemi2018simple} / CP \cite{lacroix2018canonical}}      &      ,        &                     +               &  symmetric, anti-symmetric, asymmetric, inverse   \\ \hline
	\end{tabular} 
	\vspace{-10px}
\end{table*}

Given a set of observed (positive) triplets,
the goal of KGE is to learn low-dimensional vector representations of 
entities and relations so that 
the 
plausibility measured by  of observed triplets s
are maximized while those of non-observed ones are minimized \cite{wang2017knowledge}.
To build a KGE model,
the most important thing is to design and choose a proper SF ,
which measures the triplets' plausibility based on embeddings.
Since different SFs have different strengths and weaknesses,
the choice of  is critical for the KGE's performance \cite{wang2017knowledge,lin2018knowledge}.
A large amount of KGE models with popular SFs
follow the same framework (Alg.\ref{alg:general}) \cite{wang2017knowledge}
using stochastic gradient descent.
At step~5,
negative triplets are sampled from ,
which contains all non-observed triplets for a current positive triplet ,
by some fixed distributions \cite{wang2014knowledge} or dynamic sampling schemes \cite{zhang2018nscaching}. 
Next,
the gradients
are computed 
based on the given SF and embeddings,
and
are used to update the model parameters (step~6).
Hinge loss \cite{bordes2013translating} and logistic loss \cite{yang2014embedding} are popularly used as .
In this paper, we use the multi-class loss \cite{lacroix2018canonical}
since it
currently achieves the best performance and has little variance.
 
\begin{algorithm}[ht]
	\caption{Stochastic training of KGE \cite{wang2017knowledge}.}
	\label{alg:general}
	\small
	\begin{algorithmic}[1]
		\REQUIRE training set , scoring function  and loss function ;
		
		\STATE initialize embeddings  for each  and .
		\FOR{}
		
		\STATE sample a mini-batch  of size ;
\FOR{each }
		
		\STATE  sample  negative triplets 
		for the positive triplet ; 
		
		\STATE update embedding parameters based on loss  using selected positive and negative triplets;
		
		\ENDFOR
		\ENDFOR
		\RETURN embeddings of entities in  and relations in .
	\end{algorithmic}
\end{algorithm} 

Existing human-designed SFs mainly fall into three types:
\begin{itemize}[leftmargin=10px,itemsep = 5px]
	\item 
	\textit{Translational distance models (TDMs):}
	The translational approach exploits the distance-based SFs. 
	Inspired by the word analogy results in word embeddings \cite{bengio2013representation}, 
	the plausibility is measured based on the distance between two entities, 
	after a translation carried out by the relation. 
	In TransE \cite{bordes2013translating}, 
	the SF is defined by the (negative) distance between  and , 
	i.e.  . 
	Other TDMs-based SFs,
	e.g.,
	TransH \cite{wang2014knowledge}, TransR \cite{fan2014transition},
	enhance over TransE by introducing extra mapping matrices.
	
	
	\item 
	\textit{BiLinear models (BLMs):}
	SFs in this group exploit the plausibility of a triplet by the product-based similarity.
Generally,
	they share the form as 
	 
	where  is a matrix referring to the embedding of relation  \cite{wang2017knowledge,wang2017multi}. 
	RESCAL \cite{nickel2011three} models the embedding of each relation by directly using .
DistMult \cite{yang2014embedding} 
	overcomes the overfitting problem of RESCAL by constraining  to be diagonal.
	ComplEx \cite{trouillon2017knowledge} allows  and  to be complex values,
	which enables handling asymmetric relations.
	HolE \cite{nickel2016holographic} uses a circular correlation to replace the dot product operation, 
	but is proven to be equivalent to ComplEx \cite{hayashi2017equivalence}. 
	Other variants like Analogy \cite{liu2017analogical}, SimplE \cite{kazemi2018simple}
	regularize the matrix  in different ways.

	\item 
	{
	\textit{Neural network models (NNMs):}
	Neural models aim to output the probability of the triplets based on neural networks 
	which take the entities' and relations' embeddings as inputs.
	MLP proposed in \cite{dong2014knowledge} and NTN proposed in \cite{socher2013reasoning} are representative neural models.
	Both of them use a large amount of parameters to combine entities' and relations' embeddings.
	ConvE \cite{dettmers2017convolutional} takes advantage of 
	convolutional neural network to increase the interaction among different dimensions of the embeddings.}
	
\end{itemize}


{
As proved in \cite{wang2017multi}, 
TDMs have less expressive ability than BLMs,
which further leads to their inferior empirical performance.
Based on the power of deep networks,
NNMs are also introduced for KGE.
However, 
due to the huge model complexity and increasing difficulty of training,
as well as the lack of domain-specific constraints,
their performance is still worse than BLMs
\cite{dettmers2017convolutional,lacroix2018canonical}.
Therefore,
we focus on BLMs in the sequel.
The most representative BLMs are listed in Tab.~\ref{tab:scofun}.}



\subsection{Automated machine learning (AutoML)}
\label{sec:automl}
Automated machine learning (AutoML) \cite{automl_book,quanming2018auto}
has recently exhibited its power
in easing the usage of and designing better machine learning models.
Basically,
AutoML can be regarded as a bi-level optimization problem
where we need to update model parameters by the training data sets
and tune hyper-parameters by the validation data sets.
Regarding the success of AutoML,
there are two important perspectives:
\begin{itemize}[leftmargin=8px]
	\item
	\textit{ Search space:}
	This helps to figure out important properties of the 
	underlying learning models and set up the search space for an AutoML problem.
	First,
	the space needs to be general enough
	to cover human wisdom as special cases.
	However,
	the space cannot be too general,
	otherwise searching in the space will be too expensive.
	\item
	\textit{ Search algorithm}:
	Unlike convex optimization,
	there is no universal and efficient optimization tools.
Once the search space is determined,
	efficient algorithms should be developed to search good points in the space.
\end{itemize}

We take NAS and HPO as examples.
The search space in NAS is spanned by network operations,
e.g., convolution with different sizes, skip-connections.
Various tailor-made algorithms,
such as 
reinforcement learning \cite{zoph2017neural}, 
evolution algorithms \cite{xie2017genetic},
and one-shot algorithms \cite{liu2018darts,yao2019differentiable},
have been proposed for efficient optimization.
For HPO,
Bayesian optimization \cite{feurer2015efficient,falkner2018bohb} is usually customized to search the space
made up by the hyper-parameters of the learning tools.

This paper is the first step towards automated embedding of knowledge graphs.
However,
such a step is not trivial
since previous AutoML methods used in NAS and HPO
cannot be directly applied to KGE.
The main problem is that we need to explore domain-specific properties 
in defining the search space and
designing efficient search algorithm
to achieve effectiveness with less cost.



\begin{figure*}[ht]
	\centering
	\subfigure[DistMult.]
	{\includegraphics[height=3cm]{figure/distmult}}\ 
\subfigure[ComplEx.]
	{\includegraphics[height=3cm]{figure/complex}}\ 
\subfigure[Analogy.]
	{\includegraphics[height=3cm]{figure/analogy}}\ 
\subfigure[SimplE.]
	{\includegraphics[height=3cm]{figure/simple}}
	\quad\quad
	\subfigure[AutoSF.]
	{\includegraphics[height=3cm]{figure/autokge}}
	\vspace{-5px}
	\caption{A graphical illustration of  for existing SFs in Tab.~\ref{tab:scofun}
		and the search space of AutoSF.
		Blank space is for zero matrix
		and .}
	\label{fig:graphsf}
	\vspace{-15px}
\end{figure*}


\section{The Search Problem} 
\label{sec:search}

As mentioned in Sec.\ref{sec:relworks}, 
new designs of SFs have continuously boosted
the performance of KGEs in recent years.
However, there is no absolute winner among the human-designed SFs.
Besides,
as different KGs usually exhibit distinct patterns in relations,
how to choose a proper SF to achieve good performance is non-trivial.
These raise one question:
\textit{can we automatically design a SF for a given KG with good performance guarantee}?
In this part,
we define AutoSF as a searching problem and make deep analysis on the search space based on KG properties
to address the question.

\subsection{AutoSF: Searching for SFs}
\label{ssec:searSFs}
Since SF is the crux to KGE
and different KG has distinct properties,
we are motivated to form the problem of designing new and better SFs
as a searching problem.
Specifically,
we define it as follows:


\begin{definition}[AutoML Problem]\label{def:autoSF}
Let  be a KGE model (with indexed embeddings  and structure ),
 
measures the performance (the higher the better) of a KGE model  on a set of triplets .
The problem of searching the SF is formulated as:

where  contains all possible choices of ,
 and 
denote training and validation data sets.
\end{definition}

Same as NAS \cite{xie2017genetic,zoph2017neural} 
and HPO \cite{feurer2015efficient,falkner2018bohb},
AutoSF is formulated as a bi-level optimization problem.
We firstly need to train the model to obtain  (converged model parameters) 
on the training set  by \eqref{eq:autosf},
and then search for a better 
which is measured by the performance  on the validation set 
by \eqref{eq:autosf:l1}.
However,
in this sequel
we can see the search space of  and search strategy in AutoSF are
fundamentally different from previous AutoML works.
They are closely related to KGE's domain and new to the AutoML literature.





\subsection{Search space: a unified representation of BLM SFs}
\label{ssec:unified}

{
To solve the AutoSF problem,
the first question is:
\textit{what is a good search space }?
As discussed in Sec.\ref{sec:automl},
the space can neither be too specific nor too general.
To motivate a good search space,
let us look at some commonly used SFs (Tab.~\ref{tab:scofun})
and dig out what are important properties of .

As discussed in Sec.\ref{ssec:score},
the state-of-the-art performance of KGE models
is achieved by BLMs \cite{lacroix2018canonical,kazemi2018simple},
thus we limit our scope to them. 
RESCAL \cite{nickel2011three} is not considered 
since it does not have good scalability \cite{liu2017analogical,trouillon2017knowledge}
and neither empirically perform well.
The other models in Tab.~\ref{tab:scofun}
regularize the number of trainable parameters of
square matrix  to be the same as entity embedding dimensions.
Therefore, we constrain the relation embedding size to be the same as the entity's
and learn different ways of mapping the relation embedding 
into a square matrix .
Besides, 
as the summary of relation types in Tab.~\ref{tab:scofun},
important properties are symmetric, anti-symmetric, asymmetric and inverse.
These are important properties of good SFs.
Thus, a search space should be able to handle the symmetric related properties.
In addition,
as will be discussed in Remark~\ref{rmk:reg},
different SFs in BLMs differ in their way of regularizing the square matrix .
Therefore, we are motivated to adaptively search how to regularize the relational matrix on different KGs.

To motivate such a space,
we can see that there are two main differences among these SFs.
\begin{itemize}[leftmargin=15px]
\item
The embedding can be either real or complex,
e.g. DistMult v.s. ComplEx.

\item
When embedding vectors are split,
different SFs combine them in distinct manners,
e.g., Analogy v.s. SimplE.
\end{itemize}


\begin{table*}[ht]
\caption{Common relations in KGs and resulting requirements on , and search candidates in .}
\centering
\vspace{-8px}
\renewcommand{\arraystretch}{1.2}
\label{tab:comrel}
\begin{tabular}{c | c | c | c}
	\hline
	                         common relations                          & requirements on                                                             & requirements on        & examples from WN18/FB15K                \\ \hline
	                symmetric \cite{yang2014embedding}                 &                           &     & \textit{IsSimilarTo}, \textit{Spouse}   \\ \hline
	anti-symmetric \cite{trouillon2017knowledge,nickel2016holographic} &                          &     & \textit{LargerThan}, \textit{Hypernym}  \\ \hline
	           general asymmetric \cite{liu2017analogical}             &                         &  & \textit{LocatedIn}, \textit{Profession} \\ \hline
	                 inverse \cite{kazemi2018simple}                   &  &    & \textit{Hypernym}, \textit{Hyponym}     \\ \hline
\end{tabular}
\vspace{-15px}
\end{table*}


\subsubsection{Dealing with complex embeddings}
A complex vector  with 
is composed of a real part  and an imaginary part .
To deal with the complex embeddings, we can use -dimensional real vector  to represent the -dimensional complex vector  \cite{trouillon2017knowledge,balavzevic2019tucker}.
Let the complex embedding ,
where  (same for ),
then ComplEx can be expressed  as 

Similarly, DistMult \cite{yang2014embedding} with -dimensional embeddings can also be denoted by  and represented as two parts


\subsubsection{Dealing with different splits}
To make the training parameters consistent,
we also use -dimensional real valued embeddings to represent Analogy and SimplE.
As given in Tab.~\ref{tab:scofun},
embeddings in Analogy \cite{liu2017analogical} are split into a real part ,
and a complex part ,
which can be denoted as a concatenated real vector 
in the similar way as ComplEx. And the SF is split as

In SimplE \cite{kazemi2018simple}, 
two independent embedding vectors  and 
are used to represent each entity and relation.
The resulting SF becomes



\subsubsection{The unified representation}
In order to deal with the two different partitions,
i.e. ComplEx v.s. DistMult 
and Analogy v.s. SimplE,
we split embedding 
 as 
(same for  and )
to cover \eqref{eq:complex}, \eqref{eq:distmult}, \eqref{eq:analogy} and \eqref{eq:simple}.
Note that any splits  (with  and  is even) can be used to cover the SFs in Tab.~\ref{tab:scofun}.
We take  in order to ensure a tractable search space.
The transformation of each SF is then summarized as:
{
\small

}

\vspace{-10px}

Based on above formulations,
all the scoring functions can be formed as .
Let  for ,,,,
the forms of   for these SFs can be graphically represented as Fig.~\ref{fig:graphsf}.
In this way, we can see that the main difference between the four SFs is 
their way of filling the  block
matrix (see Fig.~\ref{fig:graphsf}(e)).
Based on such a pattern,
we identify
the search space of BLM-based SFs in Definition~\ref{def:unify}.

}





\begin{definition}[Search space ] \label{def:unify}
Let  return
a  block matrix,
of which the elements in each block is given by 

where 

for .
Then,
SFs can be represented by

\end{definition}


\begin{remark}[Searching to regularize BLMs]
\label{rmk:reg}
Note that the SFs shown in Fig.~\ref{fig:searchedsf} constrain the relation matrix  in different forms,
which can be regarded as different regularization schemes.
Viewed in this way, AutoSF aims to search how to regularize the relational matrix 
that can adapt to different relation properties in different  KGs.
In addition,
the data dependent regularization cannot be easily formed as a constraint in training procedure,
which motivates us to use AutoML to search based on validation sets performance.
\end{remark}

{
\begin{remark}[General Search Space]
Due to the recent success of deep networks \cite{goodfellow2016deep}
and the approximation ability of multilayer perceptron (MLP) \cite{csaji2001approximation},
one may want to use a MLP as  for \eqref{eq:autosf}.
However,
the design of the MLP is also a searching problem,
which is 
very time-consuming \cite{zoph2017neural}.
Besides, an arbitrarily large MLP will lead to an extremely large space.
As verified in \cite{yao2019searching},
the general approximator MLP is a bad choice for NAS 
and performs worse than those using reinforcement learning \cite{zoph2017neural}.
\end{remark}
}

\section{The Search Strategy}
\label{sec:progred}
Here,
we propose an efficient search strategy to address the AutoSF problem
based on domain-specific properties in KGs.

\subsection{Challenges in algorithm design}
\label{ssec:challange}

Same as the other AutoML problems,
the search problem of AutoSF is black-box,
the search space is huge,
and each step in the search is very expensive since both model training and evaluation should be involved.
These problems have previously been touched by algorithms such as reinforcement learning \cite{zoph2017neural}, 
Bayes optimization \cite{feurer2015efficient}
and genetic programming \cite{xie2017genetic}.
However,
they are not a good choice here
since we have domain-specific problems,
i.e.  expressiveness and invariance,
 in KGE,
which are more challenging.

\subsubsection{Expressiveness}
\label{ssec:expre}

It is clear that
not all SFs  (from Definition~\ref{def:unify}) are equally good.
Expressiveness (Definition~\ref{def:exp}), 
which means  should be better able to handle common relations in KGs ,
is of big concern for SFs.
Their consequent requirements on  and  are summarized in Tab.~\ref{tab:comrel}.

\begin{definition}[Expressiveness \cite{trouillon2017knowledge,kazemi2018simple,wang2017multi}] \label{def:exp}
	If  can handle 
	symmetric,
	anti-symmetric,
	general asymmetric,
	and inverse relations,
	then  is expressive.
\end{definition}


\begin{figure*}[ht]
	\centering
	\subfigure[SimplE.]
	{\includegraphics[width=0.32\columnwidth]{figure/simple}}
	\subfigure[sym.]
	{\includegraphics[width=0.32\columnwidth]{figure/simple-sym}}
	\subfigure[skew-sym.]
	{\includegraphics[width=0.32\columnwidth]{figure/simple-asym}}
	\subfigure[permute  and .]
	{\includegraphics[width=0.32\columnwidth]{figure/simple-perm-ent}}
	\subfigure[permute .]
	{\includegraphics[width=0.32\columnwidth]{figure/simple-perm-rel}}
	\subfigure[flip sign.]
	{\includegraphics[width=0.32\columnwidth]{figure/simple-sign}}
	
	\vspace{-6px}
	\caption{Illustration of \textit{Invariance} and \textit{Expressiveness}.
		(a) SimplE model;
		(b) assign ;
		(c) assign ;
		(d) permute  into  and do the same for ;
		(e) permute  into ;
		(f) flip the signs of  and .
	}
	\label{fig:invariance}
	\vspace{-10px}
\end{figure*}

To ensure that
 can handle those common relations,
we propose Proposition~\ref{pr:expre}.




\begin{prop}\label{pr:expre}
	If  can 
	be symmetric for some ,
	i.e. ,	
	and skew-symmetric
	for some ,
	i.e. .
	Then
	the formulated   is expressive. (Proofs in Appendix \ref{app:proof:expre}).
\end{prop}

With this Proposition
and to avoid trivial solutions,
we introduce the following constraints on :

\begin{itemize}[leftmargin=25pt,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item[(C1).] 
	 can be  symmetric 
with proper 
	and skew-symmetric
with proper .
	\item[(C2).]  has no zero rows/columns, 
	covers all  to , 
	and has no repeated rows/columns.
\end{itemize}
For (C1), 
the symmetric property of  determines what kind of relation the given SF can model
based on Proposition~\ref{pr:expre}.
For (C2), if there are zero rows/columns in , the corresponding embedding dimensions will be useless.
It means that these dimensions will never be optimized during training.

The above constraints are important 
for finding potentially good candidate ,
and they play a key role in filtering out the bad 's for  the design of an efficient search algorithm. 
As in Definition~\ref{def:exp} and Proposition~\ref{pr:expre},
we need to deal with Constraint~(C1) for expressiveness.
It is challenging since 
 only represents a structure,
however the exact check of (C1) relies on the values in ,
which are unknown in advance.
Fortunately, we can check it by value assignment.
Take the SF in Fig.~\ref{fig:invariance}(a) for example.
We can see that  can be symmetric by assigning 
and  as in 
Fig.~\ref{fig:invariance}(b),
and skew-symmetric by setting 
and  like in Fig.~\ref{fig:invariance}(c).
This is the key idea of addressing expressiveness.

\subsubsection{Invariance}
As defined in Sec.\ref{ssec:unified},
the embeddings are split into 4 parts, i.e.
.
Prior to the embedding training,
permuting the 's will lead to equivalent structure 
since``1,2,3,4" here are only identity of each component
and these components are equivalent at this stage.
For example, we can permute  into .
Even though  and  change their position,
the learned embedding could be the same by changing corresponding values after training.
Therefore, the structure of SFs is invariance to permutation of 's.
Similarly,
since  and  share the same embedding parameters , 
the generated SFs are also equivalent by simultaneously permuting the 's and 's.
Moreover, if we flip the signs of some ,
we can learn equal embeddings by flipping the true value of  those  after training.
In summary,
there exist three kinds of invariance: 
permuting entity embedding 's and 's,
permuting relation embedding 's, 
and flipping signs.
The example of three cases are given in Fig.~\ref{fig:invariance}(d-f).
Let  be the embeddings of the SF 
 and  be the embedding of another SF ,
 and  is formed through the invariance changes of .
 Then we will have  
 after the model training.
 Therefore,
it is tedious to train and evaluate the equivalents once we know the performance of one SF among them.


\subsection{Progressive greedy search}
\label{ssec:greedy}

As in Sec.\ref{ssec:unified},
adding one more block into  indicates adding
one more nonzero multiplicative term into ,
namely

where  and .
In order to search efficiently, 
we propose a progressive greedy algorithm based on the inductive rule \eqref{eq:progressive},
which can significantly cut down the search space in a stage-wise manner.
The intuition of using \eqref{eq:progressive} to progressively generate SFs it to
gradually adjust the relation matrix .
However,
greedy search usually leads to sub-optimal solutions \cite{tropp2004greed},
which can be more serious when faced with the expressive and invariance challenges in AutoSF. 
Therefore,
we enhance the greedy search with a filter and a predictor
to specifically deal with  the expressiveness and invariance
discussed in Sec.\ref{ssec:challange}.

\begin{algorithm}[ht]
	\caption{Progressive greedy search algorithm.}
	\label{alg:greedy}
	\small
	\begin{algorithmic}[1]
		\REQUIRE : number of nonzero blocks in , a learnable predictor ;
		\FOR{ in }
		\REPEAT	 \label{step:gen-start}
		\STATE randomly select a top- model ; 
		\STATE generate 6 integers  and  from , 
		and form
		;
		\label{step:gen}
		
		\STATE \textbf{if}  satisfies filter  \textbf{then} ;
\label{step:filter}
		\UNTIL{}  \label{step:gen-end}

		{
		\STATE select top- s in  based on the \textit{predictor} ;
		\label{step:predict}
		
		\STATE \textit{train} embeddings with the selected s;
		
		\STATE \textit{evaluate} the obtained embedding from the selected s;	
		}
		
		\STATE  add and record  and their performance;
		\label{step:record}
\label{step:top}
		\STATE update the predictor  with records in . 
		\label{step:update} 
		\ENDFOR
		\RETURN desired SFs in .
	\end{algorithmic}
\end{algorithm}

\subsubsection{Complete procedures}
Alg.\ref{alg:greedy} shows our progressive greedy algorithm.
As in Definition~\ref{def:unify},
let the number of 
nonzero blocks in  be 
and the SF in this group be .
The idea of progressive search is that
given the desired ,
we start from small blocks  and then gradually add in more blocks until .
Thus, 
we can greedily generate candidates based on the top SFs in  at step~\ref{step:gen-start}-\ref{step:gen-end}
to reduce search space.
Specifically, we greedily pick up the top-  in the previously evaluated models in .
 candidates
will then be generated by adding two more multiplicative term in step~\ref{step:gen}
to deal with Constraint (C1)
since adding one block each step will result in simply lying on the diagonal.
All the candidates are generated from  
and are checked through the Filter  (see Sec.\ref{ssec:filter}) to guarantee Constraint~(C2)
and avoid training equivalents.
Next,
we use the predictor  (see Sec.\ref{ssec:pred}) to further select  promising candidates,
which will then be trained and evaluated using Alg.\ref{alg:general},
in 
step~\ref{step:predict} of Alg.\ref{alg:greedy}.
The training data for  is gradually collected with the trained SFs in  at step~\ref{step:record}.


\subsubsection{Invariance - Using a filter}
\label{ssec:filter}

The filter  we used in Alg.\ref{alg:greedy} has two functions:
1) deal with Constraint~(C2) 
and 2) remove equivalent structures due to invariance.
Constraint~(C2) is easy to check,
given the structure of ,
we can directly map it into a  substitute matrix 
and use 
to represent 
.
Then checking requirements in (C2) is a trivial task,
i.e.  checking if the  substitute matrix 
satisfies the Constraint~(C2).

For the invariance,
once a candidate  fulfilling Constraint~(C2) is generated, we use the invariance property to generate a set of equivalents .
Specifically,
we can permute the entity parts, relation parts,
or flip signs
to get  equivalents of .
If 
,
we throw  away since there are equivalent structures in the sampled set 
and history record .
This step can dramatically help us to reduce the cost in training equivalent structures.
Take  as an example, the whole space is reduced from  to 5 through the filter,
namely there are only five good and unique candidates in .
Besides, we add exception for the condition in step~5  of Alg.\ref{alg:greedy} for  
since the number of candidates is smaller than .




\subsubsection{Expressiveness - Constructing a predictor}
\label{ssec:pred}

Even though the filter helps to throw away many unpromising candidates, 
it does not deal with Constraint~(C1).
Hence, after collecting  candidates, 
we use the predictor  to further select  promising ones among them.
Considering that the performance of SFs on a specific KG is closely related to how the SF is formed,
we can use a learning model, i.e. the predictor  
to predict the performance and select good candidates in advance.
In general,
we need to extract features for points 
which have been visited by the search algorithm,
and then use a learning model to predict validation performance based on those features \cite{feurer2015efficient,liu2018progressive}.
The following are principles
a good predictor needs to meet
\begin{itemize}[leftmargin=26px]
	\vspace{-1px}
	\item[(P1).] 
	\textit{Correlate well with true performance}:
	the predictor needs not to accurately predict the exact values of validation performance,
	instead it should rank good candidates over bad ones;
	
	\item[(P2).] 
	\textit{Learn from small samples}:
	as the real performance of each point in the search space is expensive to acquire,
	the complexity of the predictor should be low so that it can learn from few samples. 
	\vspace{-1px}
\end{itemize}



Based on Principle~(P1),
the extracted features from  should be closely related to the quality of defined SF.
Meanwhile, the features should be cheap to construct,
i.e. they should not depend on values of ,
which are unknown before training.
For (P2), the number of features should be small to guarantee a simple predictor.
Therefore, we are motivated to design the
symmetry-related features (SRFs),
which can effectively capture to what extent  can be symmetric or skew-symmetric (Proposition~\ref{pr:srfs}),
and has low complexity.



Similar as the filter, we also use a  substitute matrix to represent .
As in Fig.~\ref{fig:feat-gen},
we use  to represent ,
then the symmetric and skew-symmetric property of  can be checked through
 and .
Since  is a simple  matrix, the checking procedure is very cheap.
Then by assigning different values to  (details in Appendix~\ref{app:srf}),
a 22-dimensional SRF will be returned.
Considering that the correlation of SRFs with SFs' performance is guaranteed under Proposition~\ref{pr:srfs},
we can use a simple two-layer MLP (22-2-1) as the predictor .
Other regression models with low complexity may also work here.




\begin{figure}[ht]
	\centering
	\vspace{-5px}
	\includegraphics[width=1.0\columnwidth]{figure/feat-gen}
	\vspace{-20px}
	\caption{Example of generating a feature of SRF.}
	\vspace{-10px}
	\label{fig:feat-gen}
\end{figure}

\begin{prop} 
	\label{pr:srfs}
	The extracted SRFs (see Appendix~\ref{app:srf})
	are (i) invariant to both the permutations and flipping signs of blocks in  
	and 
	(ii) give predictions related to symmetric or anti-symmetric properties.
\end{prop}


\subsection{Search complexity analysis}
\label{ssec:complexity}

There are 16 blocks and each can be filled with 9 different contents 
, , , , .
Thus the whole space size is , which is extremely large.
The greedy strategy,
predictor
and filter
cut down the space 
in different perspectives.
Specifically,
in each greedy step:
\begin{itemize}[leftmargin=10px,itemsep = 5px]


\item \textit{Greedy}: 
Considering that  is progressively generated on  for , 
there can be  candidates
( is to choose location,  is for the two s and  is for signs).
In comparison,
there can be  possible SFs in .
Take  for example, 
there are   possible candidates.
Since  is generated based on the 5 good candidates in ,
we reduce the space size from   to approximately 
based on the greedy scheme.

\item \textit{Filter}:
The filter we designed is mainly used to deal with invariance properties.
Permuting 's leads to  equivalent structures.
Simultaneously permuting 's
and 's also gives 24 equivalents.
Besides, there are  possible signs patterns.
Therefore, given a , 
we can generate at most 
(there may exist same structures in this set) 
 equivalent SFs,
which should perform the same.
Besides, by constraining the SF 
under Constraint~(C2),
many bad candidates can also be filtered out.
Take  as an example,
only 5 candidates are selected to be trained among approximately  possible structures.


\item \textit{Predictor}:
Once  candidates are generated, 
the predictor will select  ones based on their predicted performance.
Thus, the reducing ratio of predictor is about .

\end{itemize}

While 
it is difficult to 
directly quantize
to which extend
the three steps together 
can help to reduce the search space,
we can observe the significance of efficiency gained through each component.
Besides, 
we perform an empirical study
in Sec.\ref{ssec:ablation}
to show the performance gaining of these steps.   



\begin{table*}[ht]
	\centering
	\caption{Statistics of the data sets used in experiments. ``sym'' and ``anti-sym'' denotes the symmetric and anti-symmetric relations.}
	\label{tab:dataset}
	\vspace{-9px}
	\begin{tabular}{c|ccccc|cccc}
		\hline
		                data set                 & \#entity & \#relation &  \#train  & \#valid & \#test & \#sym & \#anti-sym & \#inverse & \#general \\ \hline
		   WN18 \cite{bordes2013translating}     &  40,943  &     18     &  141,442  &  5,000  & 5,000  &   4   &     7      &     7     &     0     \\
		  FB15k  \cite{bordes2013translating}    &  14,951  &   1,345    &  484,142  & 50,000  & 59,071 &  66   &     38     &    556    &    685    \\
		WN18RR  \cite{dettmers2017convolutional} &  40,943  &     11     &  86,835   &  3,034  & 3,134  &   4   &     3      &     1     &     3     \\
		 FB15k237 \cite{toutanova2015observed}   &  14,541  &    237     &  272,115  & 17,535  & 20,466 &  33   &     5      &    20     &    179    \\
		 YAGO3-10 \cite{mahdisoltani2013yago3}   & 123,188  &     37     & 1,079,040 &  5,000  & 5,000  &   8   &     0      &     1     &    28     \\ \hline
	\end{tabular}
	\vspace{-5px}
\end{table*}

\subsection{Comparison with existing AutoML approaches}
The most related work in the AutoML literature is PNAS \cite{liu2018progressive},
which combines a greedy algorithm with a performance predictor to search 
a cell structure for the convolutional neural network (CNN).
However,
the filter is not used in PNAS 
as the search space for AutoSF is fundamentally different from that of CNN.
Besides,
PNAS adopts direct one-hot encoding for the predictor,
which has a bad empirical performance here (see Sec.\ref{sub:fpref}).
As for the other AutoML approaches, 
even though the search problem of AutoSF is similarly defined as HPO \cite{bergstra2011algorithms,eggensperger2015efficient}
and NAS \cite{elsken2019neural},
the search space and search algorithm of AutoSF
are novel and specifically designed for KGE.
There is no direct way for them to deal with the challenges
in Sec.\ref{ssec:challange}.

\begin{table*}[ht]
	\caption{Comparison of the best SF identified by AutoSF and the state-of-the-art SFs. 
		The bold number means the best performance, and the underline  means the second best. 
		DistMult, ComplEx, Analogy and SimplE are obtained from our implementation,
		others are copied from the corresponding reference paper.
		STD is less than 0.001, thus not reported.}
	\vspace{-9px}
	\label{tb:comparison}
	\centering
\begin{tabular}{c|c|ccc|ccc|ccc|ccc|ccc}
		\hline
		&                                         &      \multicolumn{3}{c|}{WN18}       &     \multicolumn{3}{c|}{FB15k}      &     \multicolumn{3}{c|}{WN18RR}      &    \multicolumn{3}{c|}{FB15k237}     &     \multicolumn{3}{c}{YAGO3-10}     \\ \cline{3-17}
		type &                  model                  &        MRR      & H@1  &       H@10       &       MRR       & H@1  &       H@10       &        MRR        & H@1 &       H@10       &        MRR       & H@1  &       H@10       &        MRR       & H@1  &       H@10       \\ \hline
		TDM  &    TransE \cite{zhang2018nscaching}     &       0.500  &   ---      &       94.1       &      0.495     &      ---    &       77.4       &       0.178    &   ---        &       45.1     &    0.256       &  ---      &       41.9       &         ---              &   ---     &        ---         \\
		&    TransH \cite{zhang2018nscaching}     &       0.521     & ---   &       94.5       &      0.452    &      ---     &       76.6       &       0.186     &    ---      &       45.1       &       0.233     &      ---    &       40.1       &         ---        &      ---   &        ---         \\
		&                 RotatE \cite{sun2019rotate}                 &       0.949     & 94.4   &       95.9       &      0.797      &  74.6  &       88.4       &       \underline{0.476}    &    42.8   &       \textbf{57.1}       &       0.338  &  24.1   &       53.3       &        ---      &     ---     &       ---        \\ \hline\hline
		NNM  &      NTN \cite{yang2014embedding}       &       0.53       &     ---    &       66.1       &       0.25       &        &       41.4       &         ---       &     ---     &        ---         &     ---           &    ---      &        ---           &         ---        &     ---        &        ---         \\
		& Neural LP \cite{yang2017differentiable} &       0.94         &   ---   &       94.5       &       0.76        &  ---  &       83.7       &         ---         &      ---      &        ---         &       0.24         &   ---  &       36.2       &         ---        &      ---       &        ---         \\
		& ConvE \cite{dettmers2017convolutional}  &       0.942         &  93.5   &      {95.5}      &      0.745          &      67.0   &       87.3       &       0.46        &  39.    &       {48.}       &      {0.316}       & 23.9    &       49.1       &       0.52        &       45.     &      {66.}      \\ \hline\hline
		BLM  &   TuckER \cite{balavzevic2019tucker}    &  \textbf{0.953}     & \textbf{94.9}    &       95.8       &      0.795       &   74.1   &       89.2       &       0.470        &  \underline{44.3}     &       52.6       & \underline{0.358}    &   \underline{26.6}   &       54.4       &         ---           &    ---      &        ---         \\
		&      HolEX \cite{xue2018expanding}      &       0.938      &    93.0   &       94.9       &      0.800        &    75.0   &       88.6       &         ---         &     ---       &        ---         &         ---         &     ---       &        ---         &         ---          &    ---       &        ---         \\
		&	QuatE  \cite{zhang2019quaternion}        &   0.950    &  94.5  & 95.9   & 0.782  &  71.1  &  90.0  &  \underline{0.488}  &  43.8  &  \textbf{58.2} &   0.348  &  24.8 &  55.0  &  ---  &  ---  &   ---  \\ 
		&                DistMult                 &       0.821      &      71.7       &       95.2       &      0.817      &        77.7     &       89.5       &       0.443         &  40.4   &       50.7       &      {0.349}       &  25.7  &       53.7       &       0.552          &   47.6    &       69.4       \\
		&                 ComplEx                 &       0.951          &     94.5      &       95.7       &      \underline{0.831}       &    79.6   &  \underline{90.5}   &      {0.471}        &   43.0 	 &       55.1       &       0.347         &     25.4		 &       54.1       & \underline{0.566}      &    \underline{49.1}   &       70.9       \\
		&                 Analogy                 &       0.950         &    94.6   &       95.7       &      0.829      &   79.3   &  \underline{90.5}   & {0.472}    &    43.3   & {55.8} &       0.348       &  25.6    & \underline{54.7} &       0.565         &   49.0    & \underline{71.3} \\
		&                SimplE/CP                &       0.950        &   94.5   & \underline{95.9} &      0.830        &   \underline{79.8}    &      {90.3}      &      {0.468}         &   42.9    &       55.2       &      {0.350}    &    26.0    &       54.4       &       0.565      &   \underline{49.1}   & {71.0} \\ \hline\hline
		\multicolumn{2}{c|}{AnyBURL \cite{meilicke2019anytime}}       &  0.95 & 94.6 &  \underline{95.9}  & 0.83   &  80.8 &  87.6  &  0.48  & 44.6   &  55.5   &  0.31   &  23.3 &    48.6       &   0.54  &  47.7   &  47.3        \\ \hline \hline
		\multicolumn{2}{c|}{AutoSF}          & \underline{0.952}    &   \underline{94.7}    &  \textbf{96.1}   &  \textbf{0.853}     &  \textbf{82.1}   & \textbf{91.0} &  \textbf{0.490}      &   \textbf{45.1}   &  \underline{56.7}   &  \textbf{0.360}    &    \textbf{26.7}  &  \textbf{55.2}   &  \textbf{0.571}     &  \textbf{50.1}  &  \textbf{71.5}   \\ \hline
	\end{tabular}
	\vspace{-13px}
\end{table*}

\section{Empirical Study}

All of the algorithms are written in python with PyTorch framework \cite{paszke2017automatic}.
Experiments are run
on 8 TITAN Xp GPUs. 


\subsection{Experiment setup}
\label{ssec:setup}
\subsubsection{Datasets}
Five data sets, i.e.  WN18, FB15k, WN18RR, FB15k237 and YAGO3-10 are considered
(statistics in Tab.~\ref{tab:dataset}).
WN18RR and FB15k237 are variants 
that remove near-duplicate or inverse-duplicate relations from WN18 and FB15k respectively,
\cite{wang2018evaluating,toutanova2015observed}.
YAGO3-10 is much larger than the others.
These are benchmark datasets,
and are popularly used to compare KGE models in the literature
\cite{bordes2013translating,yang2014embedding,trouillon2017knowledge,liu2017analogical,kazemi2018simple,lacroix2018canonical}.
The number of symmetric, anti-symmetric, inverse pairs and general asymmetric are computed in the following way:
Given a relation , let the number of positive triplets  be .
(i) If the number of  is larger than , then we regard it as symmetric;
(ii) If the number of  is zero and the size of joint set of  and  is at least 
(this is to ensure that they have same type),
we regard as anti-symmetric;
(iii) If there exist another relation  that has at least  ,
then  and  are inverse pairs;
(iv) others are regarded as general asymmetric.
The threshold 0.9 and 0.1 are hand-made and just used to roughly (other values are fine) indicate the relation properties for each data set.






\subsubsection{Hyper-parameters}
\label{ssec:hyper:setting}
Since the searched embedding models belong to BLMs,
we fairly compare different SFs with a fixed set of hyper-parameters.
In order to reduce training time,
we set the dimension  as 64 during the search procedure.
First,
we use SimplE \cite{kazemi2018simple} as the benchmark model 
and tune hyper-parameters with the help of HyperOpt,
a hyper-parameter optimization framework based on TPE \cite{bergstra2011algorithms}. 
The searching ranges are given as follows: 
learning rate  in , 
L2 penalty  in ,
decay rate in ,
batch size  in .
All the models are trained until converge
to avoid the influence of different convergence speed.
Besides, we use Adagrad \cite{duchi2011adaptive} as the optimizer
since it tends to perform better as indicated in \cite{lacroix2018canonical,trouillon2017knowledge}.
Once a good hyper-parameter configuration is selected,
we use it to train and evaluate different searched SFs.
After the search procedure, we pick up the best SF evaluated by the MRR performance 
on the validation data set
as the searched SF.
When comparing the searched SFs with human-designed ones,
we increase the dimension from 64 to  as in \cite{lacroix2018canonical}.
As mentioned in \cite{wang2018evaluating},
KGE models are sensitive to hyper-parameters.
For a fair comparison,
we use the same set of  hyper-parameters to train and evaluated different models on each dataset.


\subsubsection{Meta Hyper-parameters}
The hyper-parameters
 and  have little influence to the search procedure. 
We use  and  for all data sets.
Besides, steps~\ref{step:gen-start}-\ref{step:update} in Alg.\ref{alg:greedy} run based on an inner loop. 
We train 8 models in parallel and iterate for 32 times (16 times for YAGO3-10),
namely we evaluate 256 s for each .

\begin{figure*}[ht]
	\centering
	\subfigure[WN18.]
	{\includegraphics[height=2.7cm]{figure/WN18_perf}}
	\subfigure[FB15k.]
	{\includegraphics[height=2.7cm]{figure/FB15k_perf}}
	\subfigure[WN18RR.]
	{\includegraphics[height=2.7cm]{figure/WN18RR_perf}}
	\subfigure[FB15k237.]
	{\includegraphics[height=2.7cm]{figure/FB15k237_perf}}
	\subfigure[YAGO3-10.]
	{\includegraphics[height=2.7cm]{figure/YAGO_perf}}
	\vspace{-6px}
	\caption{Comparison on clock time (in hours) of model training
		v.s testing MRR 
		between search SFs (by AutoSF) and 
		human-designed ones.}
	\label{fig:curve}
		\vspace{-5px}
\end{figure*}

\begin{figure*}[ht]
	\centering
	\subfigure[WN18.]
	{\includegraphics[width=0.32\columnwidth]{figure/wn18}}
	\quad
	\subfigure[FB15k.]
	{\includegraphics[width=0.32\columnwidth]{figure/fb15k}}
	\quad
	\subfigure[WN18RR.]
	{\includegraphics[width=0.32\columnwidth]{figure/wn18rr}}
	\quad
	\subfigure[FB15k237.]
	{\includegraphics[width=0.32\columnwidth]{figure/fb15k237}}
	\quad
	\subfigure[YAGO3-10.]
	{\includegraphics[width=0.32\columnwidth]{figure/yago}}
	\vspace{-6px}
	\caption{A graphical illustration of SFs identified by our AutoSF on each data set.}
	\label{fig:searchedsf}
	\vspace{-12px}
\end{figure*}

\subsection{Comparison with existing SFs on link prediction}
\label{ssec:compstat}

We compare our AutoSF with the state-of-the-art KGE models
discussed in Sec.\ref{ssec:score},
which are designed by humans,
i.e. 
TransE \cite{bordes2013translating},
TransH \cite{wang2014knowledge},
and
RotatE \cite{sun2019rotate}
from TDMs;
NTM \cite{socher2013reasoning},
Neural LP \cite{yang2017differentiable},
and 
ConvE \cite{dettmers2017convolutional}
from NNMs;
{
TuckER \cite{balavzevic2019tucker},
HolE/HolEX \cite{nickel2016holographic,xue2018expanding}, 
Quat \cite{zhang2019quaternion},
DistMult \cite{yang2014embedding},
ComplEx \cite{trouillon2017knowledge},
Analogy \cite{liu2017analogical}
and
SimplE \cite{kazemi2018simple}
from BLMs;
and a rule-based method AnyBURL \cite{meilicke2019anytime}.
}
Hyper-parameters are selected by the MRR value on the validation set.


Following \cite{yang2014embedding,trouillon2017knowledge,liu2017analogical,kazemi2018simple,dettmers2017convolutional},
we test KGE's performance based on 
\textit{link prediction}.
For each triplet , where  is the validation or testing set,
we compute the score of  for all  and get the rank of ,
the same for  based on scores of  over all ,
 is not compared as in the literature \cite{wang2017knowledge}.
Same as above mentioned papers, 
we adopt the following metrics: 
(i) 
Mean reciprocal ranking (MRR): 
{\small },
where  is a set of ranking results
and
(ii) H@10: 
{\small }, 
where  is the indicator function.
We report the performance in a ``filtered'' setting as in \cite{bordes2013translating,wang2014knowledge},
where larger MRR and H@10 indicate higher embedding quality.





\subsubsection{Effectiveness}
A comparison of the testing performance
of AutoSF and 
the current state-of-the-art SFs
are shown in 
Tab.~\ref{tb:comparison}.
Firstly, we can see that there is no absolute winner among the baseline SFs.
For example,
TuckER is the best on WN18,
but is the worst among human-designed BLMs on FB15k.
DistMult generally performs worse on the benchmarks except for FB15k237
since it does not follow Proposition~\ref{pr:expre}.
A single model is hard to adapt to different KGs.
However,
AutoSF
performs consistently well among
these five data sets.
i.e. 
the best among FB15k, WN18RR, FB15k237 and YAGO3-10,
and the runner-up on WN18.

Besides, we plot the learning curves of DistMult, Analogy, ComplEx, SimplE and the best SF
searched by AutoSF in Fig.~\ref{fig:curve}.
As shown, the searched SFs not only outperform baselines, but also converge faster,
which may due to these SFs can better capture relations in these datasets.



\subsubsection{Case study: Distinctiveness}
\label{ssec:casestufy}

To show the searched SFs are KG-dependent and novel to the literature,
we plot them in Fig.~\ref{fig:searchedsf}.
It is obvious that these SFs are different from each other,
and they are not equivalent regarding invariance properties.
As shown in Tab.~\ref{tab:dataset}, WN18 and FB15k have many symmetric, anti-symmetric relations
and inverse relation pairs, the best SF searched on them are very similar
and have the same SRF.
The other three data sets are more realistic
and contains less symmetric, anti-symmetric and inverse relations,
thus have different SRFs with fewer entry being non-zero.

The most special case is FB15k237, which can only be symmetric under (S11).
Viewing the values in Tab.~\ref{tb:comparison},
we can see that the leading performance on FB15k237 is achieved by DistMult and AutoSF,
both of which cannot be skew-symmetric.
As given in the statistic information in Tab.~\ref{tab:dataset},
FB15k237 has relatively fewer anti-symmetric relations.
This may explain why skew-symmetric is not that important for .
However,
SRFs still work for these cases
since it can be aware that skew-symmetric property is not that essential
and focus more on searching different local structures.

\begin{table}[ht] 
\centering
\vspace{-10px}
\caption{MRRs of applying SF searched from one data set (indicated by each row) on another data set (indicated by each column).}
\vspace{-10px}
\label{tab:dist}
\begin{tabular}{c|c|c|c|c|c}
	\hline
	                     &      WN18      &     FB15k      & WN18RR & FB15k237 & YAGO3-10 \\ \hline
	        WN18         & \textbf{0.952} &     0.841      &       0.473        &        0.349         &        0.561         \\ \hline
	       FB15k         &     0.950      & \textbf{0.853} &       0.470        &        0.350         &        0.563         \\ \hline
	       WN18RR        &     0.951      &     0.833      &   \textbf{0.490}   &        0.345         &        0.568         \\ \hline
	FB15k237 &     0.894      &     0.781      &       0.462        &    \textbf{0.360}    &        0.565         \\ \hline
	YAGO3-10 &     0.885      &     0.835      &       0.466        &        0.352         &    \textbf{0.571}    \\ \hline
\end{tabular}
\vspace{-5px}
\end{table}


Besides,
we pick up the best SF searched from one data set
and test it on another data set
in Tab.~\ref{tab:dist}.
We can readily
find that these SFs get the best performance
on the data sets where they are searched.
This again demonstrate that
SFs found by AutoSF on different KGs are distinct from each other.


\subsection{Comparison with existing SFs on triplet classification}
\label{ssec:tripclass}

To further demonstrate the effectiveness of the searched SFs,
we do triplet classification as in \cite{wang2014knowledge}.
This task is to confirm whether a given  is correct or not
and is more helpful in answering yes-or-no questions.
The decision rule of classification is as follows:
for each ,
if its score is larger than the relation-specific threshold ,
which we predict to be positive,
otherwise negative.
The threshold  is determined by maximizing the accuracy 
of the validation set.
We test this task on FB15k, WN18RR and FB15k237, in which the positive and negative triplets are provided.
As shown in 
Tab.~\ref{tab:trip:class}, searched SFs consistently outperform human-designed BLMs.


\begin{table}[H]
\centering
	\vspace{-5px}
	\caption{Comparison of searched SFs with the state-of-the-art SFs on accuracy (in \%) for triplet classification. STD0.2.}
	\label{tab:trip:class}
	\vspace{-10px}
	\begin{tabular}{c|ccc}
\hline &     FB15k     &    WN18RR     &   FB15k237    \\ \hline
		DistMult      &     80.8      &     84.6      &     79.8      \\ \hline
		Analogy       &     82.1      &     86.1      &     79.7      \\ \hline
		ComplEx       &     81.8      &     86.6      &     79.6      \\ \hline
		SimplE       &     81.5      &     85.7      &     79.6      \\ \hline
		AutoSF       & \textbf{82.7} & \textbf{87.7} & \textbf{81.2} \\ \hline
	\end{tabular}
	\vspace{-5px}
\end{table}






\subsection{Comparison with other AutoML approaches} 
\label{ssec:comprand}

In this part, we compare AutoSF with the other search algorithms.
WN18RR and FB15k237 are used here, and all algorithms share the same set of hyper-parameters.
First, to show the effectiveness of the search space in BLM, 
we train a general approximator (\textit{Gen-Approx}), i.e.  MLP (in Appendix~\ref{app:mlp}), on the validation set.
Then, AutoSF is compared with \textit{Random} search and \textit{Bayes} algorithm 
\cite{bergstra2011algorithms}
on . 
As shown in Fig.~\ref{fig:automl},
the general approximator performs much worse than BLM
since it is too flexible to consider domain-specific constraints and easily overfits.
For BLM settings, the Bayes algorithm can improve the efficiency upon random search.
However, it will easily fall into local optimum
and does not take the domain property into account.
Among them,
AutoSF is the most efficient and has the best any-time performance.


\begin{figure}[ht]
	\centering
	\vspace{-10px}
	\includegraphics[height=3cm]{figure/WN18RR_auto}
	\quad
	\includegraphics[height=3cm]{figure/FB15k237_auto}
	\vspace{-8px}
	\caption{Comparison of AutoSF with other AutoML approaches.}
	\label{fig:automl}
	\vspace{-15px}
\end{figure}

\subsection{Ablation study}
\label{ssec:ablation}
We use WN18RR and FB15k237 to illustrate the importance of different components
in the proposed searching algorithm.

\subsubsection{Filter and predictor}
\label{sub:fpref} 
To show the effectiveness of the filter and predictor, 
we remove them from AutoSF and make comparisons in 
Fig.~\ref{fig:filpre}.
As shown,
the greedy algorithm is more efficient than random search.
Both filter and predictor are important.
Removing either the filter or predictor will lead to degenerated efficiency.
Besides,
compared with \textit{Greedy},
i.e. no filter and no predictor,
they can both improve efficiency through reducing the search space.


\begin{figure}[ht]
	\centering
	\vspace{-8px}
	\includegraphics[height=3cm]{figure/WN18RR_abla}
	\quad
	\includegraphics[height=3cm]{figure/FB15k237_abla}
	\vspace{-10px}
	\caption{Ablation study on the predictor and filter in AutoSF.}
	\label{fig:filpre}
	\vspace{-8px}
\end{figure}

\subsubsection{SRF features} 
As in Sec.\ref{ssec:comprand},
one-hot representation \cite{liu2018progressive} can also be used as an alternative to SRFs.
We compare the two kind of features in Fig.~\ref{fig:srf}.
For AutoSF (with one-hot), a 96-8-1 fully connected neural network is used,
and a 22-2-1 network is used for AutoSF (with SRF).
AutoSF (no predictor) is shown here as a baseline 
and it is the same as that in Fig.~\ref{fig:filpre}.



\begin{figure}[ht]
	\centering
	\vspace{-8px}
	\includegraphics[height=3cm]{figure/WN18RR_onehot}
	\quad
	\includegraphics[height=3cm]{figure/FB15k237_onehot}
	\vspace{-10px}
	\caption{Comparison of the proposed SRF with one-hot encoding in \cite{liu2018progressive}.}
	\label{fig:srf}
	\vspace{-5px}
\end{figure}

\subsubsection{Sensitivity of meta hyper-parameters}
\label{ssec:hyper}

There are three meta hyper-parameters  used in our search Alg.\ref{alg:greedy}.
The results we reported in previous parts are based on .
We change the value of  to 128 and 512,
 to 4 and 16, and show the searching curve on  in Fig.~\ref{fig:hyper}.
The parameter
 
that selects top candidates in 
is not compared for  since there are only 5 candidates in .
As can be seen,
all the different settings perform similar and obviously outperform the Greedy baseline.

\begin{figure}[ht]
	\centering
	\includegraphics[height=3cm]{figure/WN18RR_hyper}
	\quad
	\includegraphics[height=3cm]{figure/FB15k237_hyper}
	\vspace{-10px}
	\caption{Comparison of different meta hyper-parameters. 
		Greedy is added here as a contrast. 
		256 models are evaluated in each setting.}
	\label{fig:hyper}
	\vspace{-8px}
\end{figure}



{
\subsubsection{Running time analysis}
\label{ssec:time}

We show the running time of different components in AutoSF in 
Tab.~\ref{tab:efficiency}.
First, the filter and the predictor (including SRF computation) take
much shorter running time compared with that of the model training.
Then, 
as each greedy step contains 256 model training,
the best SFs can be searched within only several hours (on 8 GPUs), 
except for YAGO3-10
which takes more than one day to evaluate 128 candidates.
In comparison, 
search problem based on 
reinforcement learning \cite{zoph2017neural} runs over 4 days across 500 GPUs;
genetic programming \cite{xie2017genetic} takes 17 days on single GPU;
and Bayes optimization \cite{feurer2015efficient} trains for several days on CPUs.
Thus,
the proposed AutoSF makes the search problem on KGE tractable,
and it is very efficient in 
AutoML literature.
}


\begin{table}[ht]	
	\centering
	\vspace{-10px}
\caption{RUNNING TIME ANALYSIS. 
		We show the running time (min) per greedy step (step~2-11 in Alg.\ref{alg:greedy}).
		Apart from step~2-6 (filter), step~7,10-11 (predictor), step~8 (train)
		and step~9 (evaluation),
		all other steps take less than 0.1 minutes.}
	\vspace{-5px}
	\label{tab:stepcost}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c | c | c | c | c}
		\hline
		\multirow{2}{*}{steps} & filtering & predictor & train           & evaluate \\ \cline{2-5}
		& 2-6       & 7,10-11   & 8               & 9        \\ \hline
		WN18          &  15.90.5   &   1.80.1  & 475.99.5  &  41.30.8    \\ \hline
		FB15K          &   16.80.7  &   1.90.1 & 886.321.8 &  153.73.9   \\ \hline
		WN18RR         &   16.11.0  &   1.80.1   & 271.45.1   & 27.90.5         \\ \hline
		FB15k237        &    16.61.1 &  1.90.1   & 439.211.2  &  63.51.9     \\ \hline
		YAGO3-10        &  16.60.9    &  1.70.1   & 1631.185.5 &  141.98.9        \\ \hline
	\end{tabular}
\label{tab:efficiency}
	\vspace{-3px}
\end{table}

In addition,
since AutoSF search SFs with dimension  and then fine-tune the hyper-parameters with , , ,  as in \cite{kazemi2018simple,lacroix2018canonical}.
In comparison, the searching cost is comparable with the fine-tune cost
which generally needs to train and evaluate hundreds of hyper-parameter settings with large dimension size.
In this view, the searching cost is not that expensive.






\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose AutoSF, an algorithm to automatically design and discover better SFs for KGE.
By using a progressive greedy search algorithm enhanced by a filter and a predictor with domain-specific knowledge,
AutoSF can efficiently design promising SFs
that are KG dependent, new to the literature,
and outperform the state-of-the-art SFs designed by humans
from the huge search space.
{In future work,
a promising direction is to explore how to efficiently search the network structure for NNMs 
under domain-specific constraints.}
The greedy algorithm used in AutoSF somehow limits the exploration in the search space,
which is also a potential problem to be addressed.



\section*{Acknowledgment}

The work is partially supported by 
the Hong Kong RGC GRF Project  16202218, 
CRF project  C6030-18G,  
AOE project AoE/E-603/18,  
the National Science Foundation of China (NSFC) under Grant No. 61729201, 
Science and Technology Planning Project of Guangdong Province, China, No. 2015B010110006,  
Hong Kong ITC grants  ITS/044/18FX  and  ITS/470/18FX,  
Didi-HKUST joint research lab Grant, 
Microsoft Research Asia Collaborative Research Grant, 
Wechat Research Grant and Webank Research Grant.



\bibliographystyle{plain}
\bibliography{bib}

\appendix

\subsection{Proof of Proposition \ref{pr:expre}}
\label{app:proof:expre}
\begin{proof}
	We consider the four cases separately:
	\begin{itemize}[leftmargin=10px]
		\item \textit{symmetric relations}: 
		If  for some ,
		then given a triplet ,
		,
		which means  can handle symmetric relations.
		
		\item \textit{anti-symmetric relations}:
		If  for some ,
		then given a triplet ,
		,
		which means  can handle anti-symmetric relations.
		
		
		\item \textit{general asymmetric relation}:
		Since , 
		then for any scalar , 
		.
		This leads to .
		Similarly, we have  for scalar .
		
		If  for some  and  for another , then for any general assymetric relation , let . We have
		
		Similarly, we can obtain
		
		Then, for any value of the pair  
		and , 
		there exist appropriate scalars  by solving \eqref{eq:asym:forward} and \eqref{eq:asym:backward} 
		to obtain .
		
		\item \textit{inverse relations}:
		Let  and  be two relations, 
		and assume  
		and 
		given  based on general asymmetric property.
		Let  and , then
		
		This means  and  are a pair of inverse relations.
	\end{itemize}
	Thus,
	we obtain the proposition.
\end{proof}

\subsection{Proof of Proposition \ref{pr:srfs}}
\label{app:proof:srfs}
\begin{proof}
	(i) For each case (S1-11), the SRF is generated based on permutation and flipping signs of the 4 basic values .
	Thus, no matter how the structure  changes due to permutation or flipping signs,
	they will lead to the same feature.
	Once the matrix  can be symmetric or anti-symmetric under one assignment S,
	its corresponding feature will not change regardless of permutation or flipping signs.
	(ii) 
	The SRF is generated based on the symmetric and skew-symmetric property
	and each dimension corresponds to a specific case of symmetric or skew-symmetric.
	Then the predictor can learn higher weights to the dimensions correlates the data's symmetric property well.
	Besides, this pattern can be easily learned through  a few samples.
\end{proof}



\subsection{Design of SRFs}
\label{app:srf}

\begin{remark}[SRF] \label{def:SRFs}
Let the 1-dimensional degeneration of , , ,  be scalars .
We give  with following assignments:
	\begin{itemize}[leftmargin=10pt,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item Four values are non-zero:
	Four values are non-zero: (S1). All of them have different absolute value, like ;
	(S2). Two have the same absolute value, and the other two have another same absolute value,	like ;
	(S3). Two of them have the same absolute value while the other two not, like ;
	(S4). Three of them have the same absolute value while another one not, like ; 
	(S5). All have the same absolute value, like .
	
	\item Three values are non-zero:
    (S6). All of them have different absolute value, like ;
    (S7). Two of them have same absolute value, like ;
    (S8). All of them have same absolute value, like .
    
    \item Two values are non-zero:
    (S9). They have different absolute value, like ;
    (S10). They have the same absolute value, like .
    
    \item Only one is non-zero: (S11). .
	\end{itemize}
	\noindent
	For each (S1)-(S11),
	we use permutation and flipping the signs based on the given examples to check if  can be symmetric or skew-symmetric under each case.
	As a result, a  dimensional SRF returns with little extra cost.
\end{remark}


The 11 cases exhaustively enumerate the possible conditions of  being symmetric or skew-symmetric.
What we care most is what kind of symmetric properties  can be under these cases.
Some data sets may need more features being 1 if it has more symmetric, anti-symmetric and inverse relations like FB15k,
but some may not like FB15k237.
The process of SRF generation is given in Alg.\ref{alg:srf}.



\begin{algorithm}[ht]
\caption{SRF generation for each (S1-11)}
\label{alg:srf}
\small
\begin{algorithmic}[1]
	\REQUIRE the structure of , =[0,0] for ; 
	\FOR{ in the assignment candidates of S through permuting and flipping signs}
	\IF {}
	\STATE [0] = 1 \quad // symmetric
	\ENDIF
	\IF{}
	\STATE [1] = 1 	\quad // skew-symmetric
	\ENDIF
	\ENDFOR
	\RETURN  \qquad // 2-dimensional components for each S.
\end{algorithmic}
\end{algorithm}

\section{Additional Materials for the Experiments}

\subsection{Details of Taking MLP as }
\label{app:mlp}

To ensure quick training and testing \cite{dettmers2017convolutional},
we use two fully-connected neural networks as the MLP.
Specifically, to predict the tail entity, we use  to combine  and  into .
Then we use the dot product of  and  as the score.
To test the head entity, another network  is built in similar way and final score is 
.
The two networks share the same structure (128-64-64) and are trained jointly based on Alg.\ref{alg:general}.


\end{document}
