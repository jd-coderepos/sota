

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{ranlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{url}
\usepackage{soul}

\aclfinalcopy \def\aclpaperid{15} 



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Neural Reranking for Named Entity Recognition}

\author{Jie Yang \and Yue Zhang \and Fei Dong\\ 
  Singapore University of Technology and Design \\
  {\tt \{jie\_yang, fei\_dong\}@mymail.sutd.edu.sg} \\
  {\tt yue\_zhang@sutd.edu.sg} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  We propose a neural reranking system for named entity recognition (NER). The basic idea is to leverage recurrent neural network models to learn sentence-level patterns that involve named entity mentions. In particular, given an output sentence produced by a baseline NER model, we replace all entity mentions, such as \textit{Barack Obama}, into their entity types, such as \textit{PER}. The resulting sentence patterns contain direct output information, yet is less sparse without specific named entities. For example, ``PER was born in LOC'' can be such a pattern. LSTM and CNN structures are utilised for learning deep representations of such sentences for reranking. Results show that our system can significantly improve the NER accuracies over two different baselines, giving the best reported results on a standard benchmark.
\end{abstract}


\section{Introduction}

Shown in Figure \ref{fig:nerdemo}, named entity recognition aims to detect the entity mentions in a sentence and classify each entity mention into one out of a given set of categories. NER is typically solved as a sequence labeling problem. Traditional NER systems use Hidden Markov Models (HMM) \cite{zhou2002named} and Conditional Random Fields (CRF) \cite{lafferty2001conditional} with manually defined discrete features. External resources such as gazetteers and human defined complex global features are also incorporated to improve system performance \cite{ratinov2009design,che2013named}. Recently, deep neural network models have shown the ability of learning more abstract features compared with traditional statistical models with indicator features for NER \cite{zhang2015neural}.

Recurrent Neural Network (RNN), in particular Long Short-Term Memory (LSTM) \cite{hochreiter1997long}, shows the ability to automatically capture history information over input sequences, which makes LSTM a proper automatic feature extractor for sequence labeling tasks. Different methods have been proposed by stacking CRF over LSTM in NER task \cite{chiu2015named,huang2015bidirectional,lample2016neural,ma2016end}. In addition, it is possible to combine discrete and neural features for enriched information, which helps improve sequence labeling preformance \cite{zhang2016libn3l}.


\begin{figure}[!t] 
  \resizebox{\columnwidth}{!}{\begin{tabular}{c}
  \hline
  \colorbox{red!20}{\textbf{[Barack Obama]}} was born in \colorbox{blue!20}{\textbf{[hawaii]}} .\\
  \hline
  Rare \colorbox{red!20}{\textbf{[Hendrix]}} song draft sells for almost \_{ORG}_{LOC}_{MISC}_{ORG}C_3C_2C_3C_2w_i,w_iw_{i+1}Sh(w_i),Ca(w_i)Ca(w_i)w_iCo(w_i)Ca(w_i)Co(w_i)Cl(w_i),Cl(w_iw_{i+1})Pr(w_i), Su(w_i)P(w_i,w_iw_{i+1},w_{i-1}w_1w_{i+1})P(w_0)w_0i\in\{-1,0 \}StS=\{w_1,w_2,...,w_t\}SL = \{l_1, l_2, ..., l_t\}l_i = p_i e_ip_ip_i \in \{B, I, O\}BIOp_i=Oe_ie_ie_i \in\{ \it PER, ORG, LOC, MISC\} PERLOCORGMISC\{L_1,L_2,...,L_i,...,L_n\}L_i = \{l_{i1},l_{i2},...,l_{it}\}s(L_i)L_iL_iC_ih(C_i)L_iC_iL_iC_iL_il_{i1}l_{i2}\{l_{i1},l_{i2}\}\rightarrowC_{i1}C_{ix} = w_xS=L_3=C_3=C_ih(C_i)C_i(-\sqrt{\frac{3.0}{wordDim}},\sqrt{\frac{3.0}{wordDim}})wordDim\{x_1,x_2,...,x_t\}\odot\sigma\{W,b,\mu\} \in \Thetai_t, f_t, M_to_th_tth_{LSTM}W \in R^{h\times k}ku_jz_ju_1, u_2, ..., u_Lkz_jjdh=[r_1, ..., r_i,..., r_d]h_{CNN}C_i\oplush(C_i)C_is(C_i)C_iSC(S) = \{C_1,C_2,...,C_n\}\alpha \in [0,1]p(L_i)L_i\lambda\beta_1\beta_2\epsilon\{S,L_i, C_i\}L_{golden}y_i \in [0,1]L_iL_{golden}(C_i, y_i)l_2\Theta\mathcal{D}\lambda\alpha[0,1]np<0.05\Delta\Delta_{ORG}_{LOC}_{ORG}_{PER}_{LOC}_{MISC}_{PER}_{MISC}_{PER}_{LOC}_{ORG}_{LOC}_{LOC}_{LOC}_{PER}_{LOC}_{LOC}_{LOC}_{ORG}_{LOC}_{LOC}$ INTO BAT.\\
  \hline
  \end{tabular}
  }
  \caption{Output examples. The first two examples illustrate the correction of entity boundary errors and the followings show the correction of entity type errors.}
  \label{fig:resultexample}
\end{figure}

\section{Conclusion}
We proposed a neural reranking architecture for NER by exploiting neural structure to learn sentence patterns. Given the candidate label sequences generated from a baseline tagger, we replace the predicted entity words with the corresponding entity type names to build collapsed sentences, which are used as inputs of a neural reranking model. A mixture reranking strategy is used to combine both the knowledge of the probability from the baseline tagger and the reranker score. Experiments on both discrete and neural baselines show our reranking system improves NER performance significantly, obtaining the best results on CoNLL 2003 English task .

\textcolor{black}{
One problem of current method is that all the candidates share the same non-entity words, which lead their neural representations quite similar and hard to distinguish, especially for long sentences. In the future work, we will develop the neural tree structures based on entity position which can enlarge the difference between candidate sequences. Intuitively, we believe the entities contribute more than non-entity when modeling the sequence vector, \textit{attention} model \cite{bahdanau2014neural} may help collect more information from the intermediate vector of sentences. Besides, Using semi-supervised methods to construct a bigger training data can help reranker learn more sentence patterns. Moreover, we also want to bring in an auxilliary classifier of predicting the probability of the replaced words being a real entity, this inside-entity information may be an important compensation for the outside-entity sentence patterns.
}



\section*{Acknowledgments}
\textcolor{black}{
We thank the anonymous reviewers for their insightful comments. Yue Zhang is the corresponding author.
}

\bibliography{ranlp2017}
\bibliographystyle{acl_natbib}
\end{document}
