


\documentclass[11pt,a4paper]{article}



\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}


\usepackage{color}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{commath}
\usepackage{amsmath,bm}
\usepackage{tabularx}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{pbox}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{array}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[detect-all]{siunitx}

\usepackage{amssymb}
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}



\aclfinalcopy 





\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Sept. 20, 2018}
\newcommand{\styleFileVersion}{tacl2018v2}
\newcommand{\ex}[1]{{\sf #1}}

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\todo}[1]{\textsf{\textbf{\textcolor{cyan}{[[#1]]}}}}
\newcommand{\eg}{{e.g.}}
\newcommand{\ie}{{i.e.}}
\newcommand{\vs}{{vs.}}
\newcommand{\hhide}[1]{}






\title{Improving Machine Reading Comprehension with \\General Reading Strategies}



\author{
  Kai Sun\textsuperscript{1}\Thanks{This work was done when K. S. was an intern at the Tencent AI Lab, Bellevue, WA.}
  ~~ Dian Yu\textsuperscript{2} ~~ Dong Yu\textsuperscript{2} ~~ Claire Cardie\textsuperscript{1}\\
 \textsuperscript{1}Cornell University, Ithaca, NY, USA \\
 \textsuperscript{2}Tencent AI Lab, Bellevue, WA, USA\\
   ks985@cornell.edu, \{yudian, dyu\}@tencent.com, cardie@cs.cornell.edu \\
}


\date{}

\begin{document}
\maketitle


\begin{abstract}

Reading strategies have been shown to improve comprehension levels, especially
for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a deep language model via pre-training. Inspired by reading strategies identified in cognitive science, and given limited computational resources -- just a pre-trained model and a fixed number of training instances -- we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) {\sc Back and Forth Reading} that considers both the original and reverse order of an input sequence, (ii) {\sc Highlighting}, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) {\sc Self-Assessment} that generates practice questions and candidate answers directly from the text in an unsupervised manner. 



By fine-tuning a pre-trained language model~\cite{radfordimproving} with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a  absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies. We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of  in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (\ie, ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of our fine-tuned models that incorporate these strategies. Core code is available at \url{https://github.com/nlpdata/strategy/}.















\end{abstract} \section{Introduction}

Recent years have seen a growing interest in machine reading comprehension (MRC)~\cite{rajpurkar2016squad,choi2018quac,kovcisky2018narrativeqa,reddy2018coqa}. In this paper, we mainly focus on \textit{non-extractive MRC}~\cite{khashabi2018looking,ostermann2018semeval,clark2018think}, in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to \textit{extractive MRC} tasks (Section~\ref{sec:task:extractive}), non-extractive MRC (Section~\ref{sec:task:multiple}) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams~\cite{lai2017race}. 

Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained general-purpose language model~\cite{radfordimproving,bert2018}. 
However, similar to the process of knowledge accumulation for human readers, it is time-consuming and resource-demanding to impart massive amounts of general domain knowledge from external corpora into a deep language model via pre-training. For example, it takes a month to pre-train a -layer transformer on eight P GPUs over the BooksCorpus~\cite{zhu2015aligning,radfordimproving};~\newcite{bert2018} pre-train a -layer transformer using  TPUs for four days on the BooksCorpus plus English Wikipedia, a feat not easily reproducible considering the tremendous computational resources ( one year to train on eight P GPUs).  

From a practical viewpoint, given a limited number of training instances and a pre-trained model, can we improve machine reading comprehension during fine-tuning instead of imparting more prior knowledge into a model via expensive pre-training? Inspired by reading strategies identified in cognitive science research that have been shown effective in improving comprehension levels of human readers, especially those who lack adequate prior knowledge of the topic of the text~\cite{mokhtari2002measuring,mokhtari2002assessing,mcnamara2004istart}, we propose three corresponding domain-independent strategies to improve MRC based on an existing pre-trained transformer (Section~\ref{sec:trans}): 


\vspace{-\topsep}
\begin{itemize}
\setlength\itemsep{-0.25em}
    \item {\sc Back and Forth Reading} (\emph{``I go back and forth in the text to find relationships among ideas in it.''}):\\
    consider both the original and reverse order of an input sequence (Section~\ref{sec:fb})
    
    \item {\sc Highlighting} (\emph{``I highlight information in the text to help me remember it.''}):\\
    add a trainable embedding to the text embedding of those tokens deemed relevant to the question and candidate answers (Section~\ref{sec:hl})
    
    \item {\sc Self-Assessment} (\emph{``I ask myself questions I would like to have answered in the text, and then I check to see if my guesses about the text are right or wrong.''}):\\
    generate practice questions and their associated span-based candidate answers from the existing reference documents (Section~\ref{sec:sa})
\end{itemize}
\vspace{-\topsep}


By fine-tuning a pre-trained transformer~\cite{radfordimproving} according to our proposed strategies on the largest general domain multiple-choice MRC dataset RACE~\cite{lai2017race} collected from language exams, we obtain a  absolute improvement in accuracy over the previous best result achieved by the same pre-trained transformer fine-tuned on RACE without the use of strategies (Section~\ref{sec:eval:race}). We further fine-tune the resulting model on a target MRC task. Experiments show that our method achieves new state-of-the-art results on six representative non-extractive MRC datasets that require a range of reading skills such as commonsense and multi-sentence reasoning (\ie, ARC~\cite{clark2016combining,clark2018think}, OpenBookQA~\cite{mihaylov2018can}, MCTest~\cite{richardson2013mctest}, SemEval-2018 Task 11~\cite{yang2017semi}, ROCStories~\cite{mostafazadeh2016corpus}, and MultiRC~\cite{khashabi2018looking}) (Section~\ref{sec:eval:five}). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies.







































% \section{Task Introduction}
\label{sec:mrc}

We roughly categorize machine reading comprehension tasks into two groups: extractive (Section~\ref{sec:task:extractive}) and non-extractive (Section~\ref{sec:task:multiple}) based on the expected answer types. 


\begin{table*}[htbp!]
\centering
\scriptsize
\begin{tabular}{lccccccc}
\toprule
&\textbf{RACE}   &\textbf{ARC}  &\textbf{OpenBookQA}    &\textbf{MCTest}  &\textbf{SemEval-2018 Task 11} &\textbf{ROCStories} &\textbf{MultiRC}\\
\midrule            
construction method    & exams   & exams      & crowd.     & crowd.    & crowd.    & crowd.  & crowd.            \\
sources of documents   & general & science    & science   &  stories  & narrative text   & stories & mixed-domain    \\
average \# of answer options & 4.0  & 4.0     & 4.0       & 4.0       & 2.0       & 2.0    & 5.4             \\
\# of documents     &  27,933    & 14M & 1,326    & 660       &  2,119   & 3,742 & 871   \\
\# of questions     &  \bf 97,687     & 7,787      & 5,957     & 2,640   &   13,939       &  --  &   9,872           \\

\midrule
non-extractive answer (\%)       & 87.0      & 43.3   & 83.8   & 45.3   &   89.9  &  100.0 & 82.1      \\
\bottomrule
\end{tabular}
\caption{Statistics of multiple-choice machine reading comprehension datasets. Some values come from~\newcite{reddy2018coqa},~\newcite{kovcisky2018narrativeqa}, and~\newcite{lai2017race} (crowd.: crowdsourcing; : regarding each sentence/claim as a document~\cite{clark2018think}; : correct answer options that are not text snippets from reference documents).}
\label{tab:related:answer}
\end{table*}



\subsection{Extractive MRC}
\label{sec:task:extractive}
Recently large-scale extractive MRC datasets have been constructed~\cite{hermann2015teaching,hill2015goldilocks,onishi2016did,chen2016character,mostafazadeh2016corpus,bajgar2016embracing,nguyen2016ms,triviaQA,ma2018challenging}, such as SQuAD~\cite{rajpurkar2016squad} and NewsQA~\cite{trischler2017newsqa}. Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA~\cite{dunn2017searchqa} and NarrativeQA~\cite{kovcisky2018narrativeqa} are free-form human generated texts based on given documents~\cite{nguyen2016ms,reddy2018coqa,choi2018quac}. However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive~\cite{reddy2018coqa,kovcisky2018narrativeqa}. 

\subsection{Non-Extractive MRC}
\label{sec:task:multiple}
In this section, we primarily discuss multiple-choice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive
human efforts to build such a dataset (\eg, MCTest~\cite{richardson2013mctest}, SemEval- Task ~\cite{ostermann2018semeval}, MultiRC~\cite{khashabi2018looking}, and OpenBookQA~\cite{mihaylov2018can}) by crowdsourcing. Besides crowdsourcing, datasets such as RACE~\cite{lai2017race} and ARC~\cite{clark2018think} are collected from language or science exams designed by educational experts~\cite{penas2014overview,shibuki2014overview,tseng2016towards} to evaluate the comprehension level of human participants. Compared to questions in extractive MRC tasks, besides surface matching, there are various types of complicated questions such as math word problems, summarization, logical reasoning, and sentiment analysis, requiring advanced reading skills and prior world knowledge. Besides, in most cases, we can adopt an objective evaluation criteria such as accuracy to evaluate system performance~\cite{clark2016combining,lai2017race}. As these kind of datasets are relatively difficult to construct or collect, most existing datasets are small in size, which hinders the development of state-of-the-art deep neural models. 


In response, in this paper we investigate how to make use of limited resources to improve MRC, using seven representative multiple-choice MRC datasets as case studies. As shown in Table~\ref{tab:related:answer}, the majority of the correct answer options in most of the datasets (except for ARC and MCTest) are non-extractive. Except for MultiRC, there is exactly one correct answer option for each question. For ARC and OpenBookQA, a reference corpus is provided instead of a single reference document associated with each question.

Here we give a formal \textbf{task definition}. Given a reference document , a question , and associated answer options , the goal is to select the correct answer option(s). We can easily adapt our method to an MRC task that only provides a reference corpus (Section~\ref{sec:eval:five}).









 \section{Approach}
\label{sec:method}

\begin{figure*}[htbp!]
   \begin{center}
   \includegraphics[width=0.98\textwidth]{Figures/strategy_overview.pdf}
   \end{center}
\caption{Framework Overview. Strategy 1, 2, and 3 refer to back and forth reading (BF) (Section~\ref{sec:fb}), highlighting (HL) (Section~\ref{sec:hl}), and self-assessment (SA) (Section~\ref{sec:sa}), respectively.}
 \label{fig:overview}
\end{figure*}


We first introduce a neural reader based on a pre-trained transformer (Section~\ref{sec:trans}) and then elaborate on the strategies that are applied during the fine-tuning stage ---  back and forth reading (Section~\ref{sec:fb}), highlighting (Section~\ref{sec:hl}), and self-assessment (Section~\ref{sec:sa}). 




\subsection{Framework Overview}
\label{sec:trans}





Our neural reader follows the framework of discriminatively fine-tuning a generative pre-trained transformer (GPT)~\cite{radfordimproving}. It adapts a pre-trained multi-layer transformer~\cite{vaswani2017attention,liu2018generating} language model to a labeled dataset , where each instance consists of a sequence of input tokens , along with a label , by maximizing:

where  is the likelihood from the language model,  is the weight of language model, and  is obtained by a linear classification layer over the final transformer block's activation of the language model. For multiple-choice MRC tasks,  come from the concatenation of a start token, a reference document, a question, a delimiter token, an answer option, and an end token;  indicates the correctness of an answer option. We refer readers to~\newcite{radfordimproving} for more details.



Apart from placing a delimiter to separate the answer option from the document and question, the original framework pays little attention to task-specific structures in MRC tasks. Inspired by reading strategies, with limited resources and a pre-trained transformer, we propose three strategies to improve machine reading comprehension. We show the whole framework in Figure~\ref{fig:overview}. 










 \subsection{Back and Forth Reading (BF)}
\label{sec:fb}



For simplicity, we represent the original input sequence of GPT during fine-tuning~\cite{radfordimproving} as [ \, o\
\small
\bm{h_i^j} =
\begin{cases}
 \bm{\ell^{+}} &\text{if the POS tag of  belongs to ,}   \\
 & \text{and  appears in either  or }\\
 \bm{\ell^{-}} &\text{otherwise}
\end{cases} 
\label{eq:hi}

\noindent where  and  are two trainable vectors of the same dimension as .

Following the above definition, the sequence of the highlight embedding  is of the same length as . We replace  with  when we encode a document. More specifically, we use the concatenation of , , , , , and  as the new input of GPT during fine-tuning (Section~\ref{sec:trans}), where , , and  denote the embedding of the start token, delimiter token, and end token, respectively, and  and  represent the sequence of the text embedding of  and , respectively.






\subsection{Self-Assessment (SA)}
\label{sec:sa}
While in previous work~\cite{radfordimproving}, the original GPT is directly fine-tuned on
an MRC end task, we instead develop a fine-tuning approach inspired by the self-assessment reading strategy.
In particular, we propose a simple method to generate questions and their associated multiple span-based answer options, which cover the content of multiple sentences from a reference document. By first fine-tuning a pre-trained model on these \emph{practice} instances, we aim to render the resulting fine-tuned model more 
aware of the input structure and to integrate information across multiple sentences as may be required to answer a given question. 

Concretely, we randomly generate no more than  questions and associated answer options based on each document from the end task (\ie, RACE in this paper). We describe the steps as follows.


\vspace{-\topsep}
\begin{itemize}
\setlength\itemsep{-0.25em}
\item \textbf{Input:}  a reference document from the end task.
    \item \textbf{Output:} a question and four answer options associated with the reference document.
    \item[1.] Randomly pick no more than  sentences from the document and concatenate these sentences together.
    \item[2.] Randomly pick no more than  non-overlapping spans from the concatenated sentences. Each span randomly contains no more than  tokens within a single sentence. We concatenate the selected spans to form the correct answer option. We remove the selected spans from the concatenated sentences and use the remaining text as the question.
    \item[3.] Generate three distractors (\ie, wrong answer options) by randomly replacing spans in the correct answer option with randomly picked spans from the document.
\end{itemize}
\vspace{-\topsep}
where , , , and  are used to control the number and difficulty level of the questions.






























































 




 \section{Experiment}
\label{sec:experiment}

\subsection{Experiment Settings}
For most of the hyperparameters, we follow the work of~\newcite{radfordimproving}. We use the same preprocessing procedure and the released pre-trained transformer. We generate k instances based on the reference documents from the training and development set of RACE~\cite{lai2017race}, with , , , and  (Section~\ref{sec:sa}). We first fine-tune the original pre-trained model on these automatically generated instances with  training epoch (data flow  boxed in Figure~\ref{fig:overview}). We then fine-tune the model on a large-scale general domain MRC dataset RACE with  training epochs (data flow  boxed in Figure~\ref{fig:overview}). Finally, we fine-tune the resulting model on one of the aforementioned six out-of-domain MRC datasets (at max  epochs). See data flow  boxed in Figure~\ref{fig:overview}. When we fine-tune the model on different datasets, we set the batch size to , language model weight  to . We ensemble models by averaging logits after the linear layer. For strategy highlighting (Section~\ref{sec:hl}), the content-word POS tagset  \{NN, NNP, NNPS, NNS, VB, VBD, VBG, VBN, VBP, VBZ, JJ, JJR, JJS, RB, RBR, RBS, CD, FW\}, and we randomly initialize  and . 





\begin{table}[h!]
\centering
\scriptsize
\begin{tabular}{llcc}
\toprule
\multicolumn{2}{l}{\textbf{Approach}}   & \textbf{\#} & \textbf{RACE-MRACE-HRACE} \\ 
\midrule
\multicolumn{2}{l}{MMN~\cite{tang2019multi}}         & 9     & 64.755.558.2     \\
\multicolumn{2}{l}{GPT~\cite{radfordimproving}}      & 1     & 62.957.459.0     \\ 
\multicolumn{2}{l}{Human performance~\cite{lai2017race}} & 1     & 85.169.473.3     \\

\midrule
\multicolumn{2}{l}{\multirow{3}{*}{\shortstack[l]{}}}                                        
                                                     & 1    & 60.957.858.7                           \\
\multicolumn{2}{l}{}                                 & 2    & 62.658.459.6                           \\
\multicolumn{2}{l}{}                                 & 9    & 63.559.360.6                           \\
\midrule
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}} \\ + \\Strategies\end{tabular}} 
    & SA           & 1      & 63.259.260.4   \\
    & HL           & 1      & 67.461.563.2                           \\
    & BF           & 2      & 67.360.762.6                           \\
    & SA + HL      & 1      & \textbf{69.2}\textbf{61.5}\textbf{63.8} \\
    & SA + HL + BF & 2      & \textbf{70.9}\textbf{63.2}\textbf{65.4} \\
    & SA + HL + BF & 9      & \textbf{72.0}\textbf{64.5}\textbf{66.7} \\ 
\bottomrule
\end{tabular}
\caption{Accuracy () on the test set of RACE (\#: number of (ensemble) models; SA: Self-Assessment; HL: Highlighting; BF: Back and Forth Reading; : our implementation).} 
\label{tab:eval:race}
\end{table}


\subsection{Evaluation on RACE}
\label{sec:eval:race}


In Table~\ref{tab:eval:race}, we first report the accuracy of the state-of-the-art models (MMN and original fine-tuned GPT) and Amazon Turkers (Human performance). We then report the performance of our implemented fine-tuned GPT baselines and our models (GPT+Strategies). Results are shown on the RACE dataset~\cite{lai2017race} and its two subtasks: RACE-M collected from middle school exams and RACE-H collected from high school exams.

Our single and ensemble models outperform previous state-of-the-art (\ie, GPT and GPT ()) by a large margin ( \vs~;  \vs~). 
The two single-model strategies -- self-assessment and highlighting -- improve over the single-model fine-tuned GPT baseline () by  and , respectively. 
Using the back and forth reading strategy, which involves two models, gives a  improvement in accuracy compared to the ensemble of two original fine-tuned GPTs  (). Strategy combination further boosts the performance. By combining self-assessment and highlighting, our single
model achieves a  improvement in accuracy over the fine-tuned GPT baseline ( \vs~). We apply all the strategies by ensembling two such single models that read an input sequence in either the original or the reverse order, leading to a  improvement in accuracy over the ensemble of two original fine-tuned GPTs ( \vs~). 

To further analyze performance, we roughly divide the question types into five categories: detail (\emph{facts and details}), inference (\emph{reasoning ability}), main (\emph{main idea or purpose of a document}), attitude (\emph{author's attitude toward a topic or tone/source of a document}), and vocabulary (\emph{vocabulary questions})~\cite{qian2004evaluation,lai2017race} and annotate all the instances of the RACE development set. As shown in Figure~\ref{fig:eval:question_type}, compared to the fine-tuned GPT baseline, our single-model strategies (SA and HL) consistently improve the results across all categories. Compared to other strategies, highlighting is likely to lead to bigger gains for most question types.








\begin{figure}[!h]
   \begin{center}
   \includegraphics[width=0.48\textwidth]{Figures/question_type.pdf}
\end{center}
 \caption{Performance on different question types.}
 \label{fig:eval:question_type}
\end{figure}


Compared to human performance, there is still a considerable room for improvements, especially on RACE-M. We take a close look at the instances from the RACE-M development set that all our implementations fail to answer correctly. We notice that  of them require one or multiple types of world knowledge (\eg, negation resolution, commonsense, paraphrase, and mathematical/logic knowledge~\cite{sugawara2017prerequisite,sugawara2017evaluation,sugawara2018makes}), especially when correct answer options are not explicitly mentioned in the reference document. For example, we need the knowledge --- \emph{the type of thing that is written by a writer can probably be a book} --- to answer the question \emph{``follow your heart is a \underline{~~~}''} from the context \emph{``Follow your heart by Andrew Matthews, an Australian writer, tells us that making our dreams real is life's biggest challenge''}. Besides,  of these failed instances require coreference resolution. It might be promising to leverage coreference resolvers to connect nonadjacent relevant sentences.

















































\begin{table*}[h!]
\centering
\scriptsize
\begin{tabular}{p{2.1cm}llp{0.4cm}cccc}
\toprule
\bf Task  & \bf Metric   &  \multicolumn{2}{c}{\bf Previous STOA} & \bf GPT & \bf GPT (2) & \bf GPT+Strategies & \bf GPT+Strategies (2) \\
\midrule
ARC-Easy     & Acc. & \newcite{clark2018think}  & 62.6 & 57.0    &  57.1  & 66.6  & 68.9       \\
ARC-Challenge  &  Acc.  & \newcite{ni2018learning}  & 36.6  & 38.2  & 38.4  & 40.7  & 42.3     \\
OpenBookQA  &  Acc.  & \newcite{mihaylov2018can} & 50.2  & 52.0  & 52.8  & 55.2  & 55.8        \\
MCTest-MC160  &  Acc.  & \newcite{chung2017supervised}  & 76.4 & 65.4  & 65.8  & 80.0  & 81.7    \\
MCTest-MC500  &  Acc.  & \newcite{chung2017supervised}  & 72.3 & 61.5  & 61.0  & 78.7  & 82.0    \\
SemEval     &  Acc.  & \newcite{chen2018hfl}          & 84.1 & 88.0  & 88.6  & 88.8  & 89.5    \\
ROCStories  &  Acc.  & \newcite{radfordimproving}     & 86.5  & 87.1  & 87.5  & 88.0  & 88.3    \\
\multirow{3}{*}{MultiRC} & & \newcite{khashabi2018looking} 
                                                      & 66.5  & 69.3  & 70.3  & 71.5  & 73.1   \\
                         &   & \newcite{khashabi2018looking}             
                                                      & 63.2  & 67.2  & 67.7  & 69.2  & 70.5   \\
                         &  Acc.  & \newcite{khashabi2018looking}              
                                                      & 11.8  & 15.2  & 16.5  & 22.6  & 21.8    \\
\midrule
\bf Average              & Acc.     &                 & 60.1  & 58.1  & 58.5  & \bf 65.1  & \bf 66.3    \\             
\bottomrule
\end{tabular}
\caption{Performance (\%) on the test sets of ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, and ROCStories and the development set of MultiRC (Acc.: Accuracy; : macro-average F1; : micro-average F1; : using the joint exact match accuracy (\ie,  reported by the official evaluation~\cite{khashabi2018looking})). RACE is used as the source task for all our implementations.} \label{tab:eval:transfer}
\end{table*}





\begin{table*}[h!]
\scriptsize
\centering
\begin{tabular}{lcccccc|c}
\toprule
\multirow{2}{*}{\bf Approach}     & \bf ARC   & \bf OpenBookQA  & \bf MCTest & \bf SemEval  & \bf ROCStories & \bf MultiRC & \bf Average \\

                     & Easy  Challenge    & - & MC160  MC500   & -            & -  & - & - \\
               & Acc.      & Acc.     & Acc.   & Acc.  & Acc. &     Acc. & Acc. \\
\midrule
GPT   & 54.0  30.3   & 50.0 & 58.8  52.0   & 87.3 & 86.7 & 69.3  66.2  11.9 & 53.9 \\
GPT (2)  & 53.9  30.7 &  50.0 & 60.0  54.0 & 88.0  & 87.0 &  69.3  66.5  13.1 & 54.6 \\
GPT+Strategies  &  61.9  35.0 &  54.2 & 67.5  64.7 &87.6  &87.4 &68.8	 67.4   16.2 & \bf 59.3 \\
GPT+Strategies (2)  &  63.1   35.4  &   55.0 &  70.8   64.8  &  88.1 &  88.1 &   69.7   67.9   16.9 & \bf 60.3 \\
\bottomrule
\end{tabular}
\caption{Performance (\%) on the test sets of ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, and ROCStories and the development set of MultiRC using the target data only (\ie, without the data flow 1 and 2 boxed in Figure~\ref{fig:overview}) (Acc.: Accuracy; : macro-average F1; : micro-average F1; : using the joint exact match accuracy (\ie,  reported by the official evaluation~\cite{khashabi2018looking})). \label{tab:eval:notransfer}}
\end{table*}



\subsection{Further Discussions on Strategies}
\label{sec:eval:other_strategy}


Besides the strategies introduced in Section~\ref{sec:method}, we also explore other reading strategies such as {\sc summarization} (\emph{``I take an overall view of the text to see what it is about before carefully reading it.''}) by appending an extractive summary~\cite{boudin2015concept} before each reference document, which is shown less effective for machine reading comprehension in our experiments compared to the strategies we focus on. In this section, we further discuss the three strategies. 



\textbf{Back and Forth Reading} We notice that the input order difference between two ensemble models is likely to yield performance gains. Besides ensembling two models that use input sequence [\, o o\,  () or [\, oo\,  (), compared to the ensemble of two original fine-tuned GPTs (both of them use [\,qo59.6\%60.6\%61.0\%63.2\%59.7\%58.7\%50\text{F1}_a7.8\%58.5\%6.2\%60.1\%354.6\%10.4\%60.3\%5.8\%$ absolute improvement in accuracy over the state-of-the-art performance on the RACE dataset. By fine-tuning the resulting model on a new target task, we achieve new state-of-the-art results on six representative non-extractive MRC datasets from multiple domains that require a diverse range of reading skills. These results consistently indicate the effectiveness of our proposed strategies and the general applicability of our fine-tuned model that incorporates these strategies. 








% 







\section*{Acknowledgments}
We would like to thank the anonymous reviewers for their constructive suggestions. We thank Hai Wang, Chengzhu Yu, and Chao Weng for their useful discussions. We especially thank Chao for helping us speed up the release of the preprint\footnote{\url{https://arxiv.org/abs/1810.13441v1}.} with technical supports. We thank Rishi Bommasani for proofreading the paper and Saku Sugawara for sharing annotations with us.


\bibliography{naaclhlt2019}
\bibliographystyle{acl_natbib}







\end{document}
