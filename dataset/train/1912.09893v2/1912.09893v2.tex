\documentclass{article} 

\usepackage{multirow}
\usepackage{rotating}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{iclr2020_conference,times}

\title{A Fair Comparison of Graph Neural Networks for Graph Classification}



\author{Federico Errica{\hypersetup{hidelinks}\thanks{Equal contribution.}} \\
Department of Computer Science \\
University of Pisa \\
\texttt{federico.errica@phd.unipi.it}
\And
Marco Podda \\
Department of Computer Science \\
University of Pisa \\
\texttt{marco.podda@di.unipi.it}
\And
Davide Bacciu \\
Department of Computer Science \\
University of Pisa \\
\texttt{bacciu@di.unipi.it}
\And
Alessio Micheli \\
Department of Computer Science \\
University of Pisa \\
\texttt{micheli@di.unipi.it} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ie}{i.e., }
\newcommand{\eg}{e.g., }
\newcommand{\quotes}[1]{``#1''}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\iclrfinalcopy \begin{document}
\maketitle

\begin{abstract}
Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works.
As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.
\end{abstract}

\section{Introduction}\label{sec:introduction}


Over the years, researchers have raised concerns about several flaws in scholarship, such as experimental reproducibility and replicability in machine learning \citep{ai-meets-stupidity, troubling-trends-ml} and science in general \citep{nsa-report-reproducibility}. These issues are not easy to address, as a collective effort is required to avoid bad practices. Examples include the ambiguity of experimental procedures, the impossibility of reproducing results and the improper comparison of machine learning models. As a result, it can be difficult to uniformly assess the effectiveness of one method against another. This work investigates these issues for the graph representation learning field, by providing a uniform and rigorous benchmarking of state-of-the-art models.

Graph Neural Networks (GNNs) \citep{nn4g-micheli,scarselli2009graph} have recently become the standard tool for machine learning on graphs. These architectures effectively combine node features and graph topology to build distributed node representations. GNNs can be used to solve node classification \citep{gcn} and link prediction \citep{link-prediction-gnn} tasks, or they can be applied to downstream graph classification \citep{cgmm}. In literature, such models are usually evaluated on chemical and social domains \citep{how-powerful-gnn}. \\
Given their appeal, an ever increasing number of GNNs is being developed \citep{mpnn-gilmer}. However, despite the theoretical advancements reached by the latest contributions in the field, we find that the experimental settings are in many cases ambiguous or not reproducible.

Some of the most common reproducibility problems we encounter in this field concern hyper-parameters selection and the correct usage of data splits for model selection versus model assessment. Moreover, the evaluation code is sometimes missing or incomplete, and experiments are not standardized across different works in terms of node and edge features. 

These issues easily generate doubts and confusion among practitioners that need a fully transparent and reproducible experimental setting. As a matter of fact, the evaluation of a model goes through two different phases, namely \textit{model selection} on the validation set and \textit{model assessment} on the test set. Clearly, to fail in keeping these phases well separated could lead to over-optimistic and biased estimates of the true performance of a model, making it hard for other researchers to present competitive results without following the same ambiguous evaluation procedures.

With this premise, our primary contribution is to provide the graph learning community with a fair performance comparison among GNN architectures, using a standardized and reproducible experimental environment. More in detail, we performed a large number of experiments within a rigorous model selection and assessment framework, in which all models were compared using the same features and the same data splits. 

Secondly, we investigate if and to what extent current GNN models can effectively exploit graph structure. To this end, we add two domain-specific and structure-agnostic baselines, whose purpose is to disentangle the contribution of structural information from node features. Much to our surprise, we found out that these baselines can even perform better than GNNs on some datasets; this calls for moderation when reporting improvements that do not clearly outperform structure-agnostic competitors. \\ Our last contribution is a study on the effect of node degrees as features in social datasets. Indeed, we show that providing the degree can be beneficial in terms of performances, and it has also implications in the number of GNN layers needed to reach good results. \\
We publicly release code and dataset splits to reproduce our results, in order to allow other researchers to carry out rigorous evaluations with minimum additional effort\footnote{Code available at: \url{https://github.com/diningphil/gnn-comparison}}.

\paragraph{Disclaimer} Before delving into the work, we would like to clarify that this work does \emph{not} aim at pinpointing the best (or worst) performing GNN, nor it disavows the effort researchers have put in the development of these models. Rather, it is intended to be an attempt to set up a standardized and uniform evaluation framework for GNNs, such that future contributions can be compared fairly and objectively with existing architectures.
 \section{Related Work}
\paragraph{Graph Neural Networks} At the core of GNNs is the idea to compute a state for each node in a graph, which is iteratively updated according to the state of neighboring nodes. Thanks to layering \citep{nn4g-micheli} or recursive \citep{scarselli2009graph} schemes, these models propagate information and construct node representations that can be \quotes{aware} of the broader graph structure. GNNs have recently gained popularity because they can efficiently and automatically extract relevant features from a graph; in the past, the most popular way to deal with complex structures was to use kernel functions \citep{wl-kernel} to compute task-agnostic features. However, such kernels are  non-adaptive and typically computationally expensive, which makes GNNs even more appealing.

Even though in this work we specifically focus on architectures designed for graph classification, all GNNs share the notion of \quotes{convolution} over node neighborhoods, as a generalization of convolution on grids. For example, GraphSAGE \citep{graphsage} first performs sum, mean or max-pooling neighborhood aggregation, and then it updates the node representation applying a linear projection on top of the convolution. It also relies on a neighborhood \textit{sampling} scheme to keep computational complexity constant. Instead, Graph Isomorphism Network (GIN) \citep{how-powerful-gnn} builds upon the limitations of GraphSAGE, extending it with arbitrary aggregation functions on multi-sets. The model is proven to be as theoretically powerful as the Weisfeiler-Lehman test of graph isomorphism. Very recently, \citet{limitations-functions-sets} gave an upper bound to the number of hidden units needed to learn permutation-invariant functions over sets and multi-sets. 
Differently from the above methods, Edge-Conditioned Convolution (ECC) \citep{ecc} learns a different parameter for each edge label. Therefore, neighbor aggregation is weighted according to specific edge parameters.
Finally, Deep Graph Convolutional Neural Network (DGCNN) \citep{dgcnn} proposes a convolutional layer similar to the formulation of \cite{gcn}. \\
Some models also exploit a pooling scheme, which is applied after convolutional layers in order to reduce the size of a graph. For example, the pooling scheme of ECC coarsens graphs through a differentiable pooling map that can be pre-computed. Similarly, DiffPool \citep{diffpool} proposes an adaptive pooling mechanism that collapses nodes on the basis of a supervised criterion. In practice, DiffPool combines a differentiable graph encoder with its pooling strategy, so that the architecture is end-to-end trainable. Lastly, DGCNN differs from other works in that nodes are sorted and aligned by a specific algorithm called SortPool \citep{dgcnn}. 

\paragraph{Model evaluation} The work of \cite{pitfalls-gnn-evaluation} shares a similar purpose with our contribution. In particular, the authors compare different GNNs on node classification tasks, showing that results are highly dependent on the particular train/validation/test split of choice, up to the point where changing splits leads to dramatically different performance rankings. Thus, they recommend to evaluate GNNs on multiple test splits to achieve a fair comparison. Even though we operate in a different setting (graph instead of node classification), we follow the authors' suggestions by evaluating models under a controlled and rigorous assessment framework. Finally, the work of \citet{are-we-really-making-progress} criticizes a large number of neural recommender systems, most of which are not reproducible, showing that only one of them truly improves against a simple baseline.
 \section{Risk assessment and model selection} \label{sec:model-selection-evaluation}
Here, we recap the risk assessment (also called model evaluation or model assessment) and model selection procedures, to clearly layout the experimental procedure followed in this paper. For space reasons, the overall procedure is visually summarized in Appendix \ref{appendix:evaluation-procedure}.

\subsection{Risk assessment}
The goal of risk assessment is to provide an estimate of the performance of a class of models. When a test set is not explicitly given, a common way to proceed is to use \emph{k-fold Cross Validation} (CV) \citep{stone-cv,varma-cv, overfitting-model-selection}. -fold CV uses  different training/test splits to estimate the generalization performance of a model; for each partition, an internal model selection procedure selects the hyper-parameters using the training data only. This way, test data is \textbf{never} used for model selection. As model selection is performed independently for each training/test split, we obtain different \quotes{best} hyper-parameter configurations; this is why we refer to the performance of a class of models.

\subsection{Model Selection}
\label{sec:model-selection}
The goal of model selection, or hyper-parameter tuning, is to choose among a set of candidate hyper-parameter configurations the one that works best on a specific \textit{validation} set. If a validation set is not given, one can rely on a holdout training/validation split or an inner -fold. Nevertheless, the key point to remember is that validation performances are \textit{biased} estimates of the true generalization capabilities. Consequently, model selection results are generally over-optimistic; this issue is thoroughly documented in \cite{overfitting-model-selection}. This is why the main contribution of this work is to clearly separate model selection and model assessment estimates, something that is lacking or ambiguous in the literature under consideration. \section{Overview of Reproducibility Issues} \label{sec:reproducibility-issues}
To motivate our contribution, we follow the approach of \cite{are-we-really-making-progress} and briefly review recent papers describing five different GNN models, highlighting problems in the experimental setups as well as reproducibility of results. We emphasize that our observations are based solely on the contents of their paper and the available code\footnote{As of the date of this submission.}. Suitable GNN works were selected according to the following criteria: i) performances obtained with 10-fold CV; ii) peer reviewed; iii) strong architectural differences; iv) popularity. In particular, we selected DGCNN \citep{dgcnn}, DiffPool \citep{diffpool}, ECC \citep{ecc}, GIN \citep{how-powerful-gnn} and GraphSAGE \citep{graphsage}. For a detailed description of each model we refer to their respective papers. Our criteria to assess quality of evaluation and reproducibility are: \emph{i}) code for data preprocessing, model selection and assessment is provided; \emph{ii}) data splits are provided; \emph{iii}) data is split by means of a stratification technique, to preserve class proportions across all partitions; \emph{iv}) results of the 10-fold CV are reported correctly using standard deviations, and they refer to model evaluation (test sets) rather than model selection (validation sets). Table \ref{tab:reproducibility} summarizes our findings.
\begin{table}[h]
\begin{center}
\caption{Criteria for reproducibility considered in this work and their compliance among considered models. (Y) indicates that the criterion is met, (N) indicates that the criterion is not satisfied, (A) indicates ambiguity (i.e. it is unclear whether the criteria is met or not), (-) indicates lack of information (i.e. no details are provided about the criteria). Note that GraphSAGE is excluded from this comparison, as it was not directly applied by authors to graph classification tasks.}
\label{tab:reproducibility}
\begin{tabular}{l c c c c}
    \toprule
    & \textbf{DGCNN} & \textbf{DiffPool} & \textbf{ECC} & \textbf{GIN}\\
    \midrule
    Data preprocessing code     & Y    & Y    & -     & Y \\ 
    Model selection code        & N    & N    & -     & N \\
    Model evaluation code       & Y    & Y    & -     & Y \\
    Data splits provided        & Y    & N    & N     & Y \\
    Label Stratification        & Y    & N    & -     & Y \\
    Report accuracy on test     & Y    & A    & A     & N \\
    Report standard deviations  & Y    & N    & N     & Y \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}
\paragraph{DGCNN} The authors evaluate the model on 10-fold CV. While the architecture is fixed for all dataset, learning rate and epochs are tuned using only one random CV fold, and then reused on all the other folds. While this practice is still acceptable, it may lead to sub-optimal performances. Nonetheless, the code to reproduce model selection is not available. Moreover, the authors run CV 10 times, and they report the average of the 10 final scores. As a result, the variance of the provided estimates is reduced. However, the same procedure was not applied to the other competitors as well. Finally, CV data splits are correctly stratified and publicly available, making it possible to reproduce at least the evaluation experiments.
\paragraph{DiffPool} From both the paper and the provided code, it is unclear if reported results are obtained on a test set rather than a validation set. Although the authors state that 10-fold CV is used, standard deviations of DiffPool and its competitors are not reported. Moreover, the authors affirm to have applied early stopping on the validation set to prevent overfitting; unfortunately, neither model selection code nor validation splits are available. Furthermore, according to the code, data is randomly split (without stratification) and no random seed is set, hence splits are different each time the code is executed.
\paragraph{ECC} The paper reports that ECC is evaluated on 10-fold CV, but results do not include standard deviations. Similarly to DGCNN, hyper-parameters are fixed in advance, hence it is not clear if and how model selection has been performed. Importantly, there are no references in the code repository to data pre-processing, data stratification, data splitting, and model selection. \paragraph{GIN} The authors correctly list all the hyper-parameters tuned. However, as stated explicitly in the paper and in the public review discussion, they report the \textit{validation} accuracy of 10-fold CV. In other words, reported results refer to model selection and not to model evaluation. The code for model selection is not provided.
\paragraph{GraphSAGE} The original paper does not test this model on graph classification datasets, but GraphSAGE is often used in other papers as a strong baseline. It follows that GraphSAGE results on graph classification should be accompanied by the code to reproduce the experiments. Despite that, the two works which report results of GraphSAGE (DiffPool and GIN) fail to do so.
\paragraph{Summary} Our analysis reveals that GNN works rarely comply with good machine learning practices as regards the quality of evaluation and reproducibility of results. This motivates the need to re-evaluate all models within a rigorous, reproducible and fair environment. \section{Experiments}
In this section we detail our main experiment, in which we re-evaluate the above-mentioned models on 9 datasets (4 chemical, 5 social), using a model selection and assessment framework that closely follows the rigorous practices described in Section \ref{sec:model-selection-evaluation}. In addition, we implement two baselines whose purpose is to understand the extent to which GNNs are able to exploit structural information.
All models have been implemented by means of the Pytorch Geometrics library \citep{pytorch-geometric}, which provides graph pre-processing routines and makes the definition of graph convolution easier to implement. We sometimes found discrepancies between papers and related code; in such cases, we complied with the specifications in the paper. Because GraphSAGE was not applied to graph classification in the original work, we opted for a max-pooling global aggregation function to classify graph instances; further, we do not use the sampled neighborhood aggregation scheme defined in \cite{graphsage}, in order to allow nodes to have access to their whole neighborhood.

\paragraph{Datasets} All graph datasets are publicly available \citep{datasets-dortmund} and represent a relevant subset of those most frequently used in literature to compare GNNs. Some collect molecular graphs, while others contain social graphs. In particular, we used D\&D \citep{dd}, PROTEINS \citep{proteins}, NCI1 \citep{nci1} and ENZYMES \citep{enzymes-dataset} for binary and multi-class classification of chemical compounds, whereas IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-5K and COLLAB \citep{deep-graphlet-kernel-imdb-reddit-collab} are social datasets. Dataset statistics are reported in Table \ref{appendix:dataset-stats}.

\paragraph{Features} In GNN literature, it is common practice to augment node descriptors with structural features. For example, DiffPool adds the degree and clustering coefficient to each node feature vector, whereas GIN adds a one-hot representation of node degrees. The latter choice trades off an improvement in performances (due to injectivity of the first sum) with the inability to generalize to graphs with arbitrary node degree.\\
In general, good experimental practices suggest that all models should be consistently compared to the same input representations. This is why we re-evaluate \textit{all} models using the same node features. In particular, we use one common setting for the chemical domain and two alternative settings as regards the social domain. As regards the chemical domain, nodes are labeled with a one-hot encoding of their atom type, though on ENZYMES we follow the literature and use 18 additional features available. As regards social graphs, whose nodes do not have features, we use either an uninformative feature for all nodes or the node degree. As such, we are able to reason about the effectiveness of the structural inductive bias imposed by the model; that is if the model is able to implicitly learn structural features or not. The effect of adding structural features to general machine learning models for graphs has been investigated in \cite{label-independent-features}; here, we focus on the impact of node degrees on performances for social datasets. 

\paragraph{Baselines} We adopt two distinct baselines, one for chemical and one for social datasets. On all chemical datasets but for ENZYMES, we follow \citet{graph-kernels-baldi, lio-baseline-mlp} and implement the Molecular Fingerprint technique, which first applies global sum pooling (\ie counts the occurrences of atom types in the graph by summing the features of \textit{all nodes} in the graph together) and then applies a single-layer MLP with ReLU activations. On social domains and ENZYMES (due to the presence of additional features), we take inspiration from the work of \citet{deep-sets} to learn permutation-invariant functions over sets of nodes: first, we apply a single-layer MLP on top of node features, followed by global sum pooling and another single-layer MLP for classification. Note that both baselines do not leverage graph topology. Using these baselines as a reference is of fundamental importance for future works, as they can provide feedback on the effectiveness of GNNs on a specific dataset. As a matter of fact, if GNN performances are close to the ones of a structure-agnostic baseline, one can draw two possible conclusions: the task does not need topological information to be effectively solved, or the GNN is not exploiting graph structure adequately. While the former can be verified through domain-specific human expertise, the second is more difficult to assess, as multiple factors come into play such as the amount of training data, the structural inductive bias imposed by the architecture and the hyper-parameters used for model selection. Nevertheless, \textit{significant} improvements with respect to these baselines are a strong indicator that graph topology has been exploited. Therefore, structure-agnostic baselines become vital to understand if and how a model can be improved.

\begin{table}[h]
    \footnotesize
    \caption{Pseudo-code for model assessment (left) and model selection (right). In Algorithm 1, \quotes{Select} refers to Algorithm 2, whereas \quotes{Train} and \quotes{Eval} represent training and inference phases, respectively. After each model selection, the best configuration best is used to evaluate the external test fold. Performances are averaged across  training runs, where  in our case is set to 3.}
    \label{tab:selection-assessment}
    \begin{minipage}{0.46\textwidth}
    \begin{algorithm}[H]
        \centering
        \caption{Model Assessment (-fold CV)}
        \label{algo:model-assessment}
        \begin{algorithmic}[1]
            \State Input: Dataset , set of configurations 
            \State Split  into  folds 
            \For{} 
            \State train, test  , 
            \State best  Select(train, )
            \For{}
                \State model  Train(train, best)
                \State p  Eval(model, test)
            \EndFor
            \State perf  
            \EndFor
            \State \textbf{return} 
        \end{algorithmic}
    \end{algorithm}\end{minipage}
    \hfill
    \begin{minipage}{0.46\textwidth}
    \begin{algorithm}[H]
        \centering
        \caption{Model Selection}
        \label{algo:model-selection}
        \begin{algorithmic}[1]
            \State Input: train, 
            \State Split train into \textit{train} and \textit{valid}
            \State p = 
            \ForEach {}
                \State model  Train(train, )
                \State p  p Eval(model, \textit{valid})
            \EndFor
            \State best  argmax p
            \State \textbf{return} best
        \end{algorithmic}
    \end{algorithm}
    \end{minipage}
\end{table}

\paragraph{Experimental Setting} Our experimental approach is to use a -fold CV for model assessment and an inner holdout technique with a 90\%/10\% training/validation split for model selection. After \emph{each} model selection, we train three times on the whole training fold, holding out a random fraction (10\%) of the data to perform early stopping. These three separate runs are needed to smooth the effect of unfavorable random weight initialization on test performances. The final test fold score is obtained as the mean of these three runs; Table \ref{tab:selection-assessment} reports the pseudo-code of the entire evaluation process. To be consistent with literature, we implement early stopping with patience parameter , where training stops if  epochs have passed without improvement on the validation set. A high value of  can favor model selection by making it less sensitive to fluctuations in the validation score at the cost of additional computation. Importantly, all data partitions have been pre-computed, so that models are selected and evaluated on the same data splits. Moreover, all data splits are stratified, \ie class proportions are preserved inside each -fold split as well as in the holdout splits used for model selection.

\paragraph{Hyper-parameters}
Hyper-parameter tuning is performed via grid search. For the sake of conciseness, we list all hyper-parameters in Section \ref{appendix:hyperparameters}. Notice that we always include those used by other authors in their respective papers. We select the number of convolutional layers, the embedding space dimension, the learning rate, and the criterion for early stopping (either based on the validation accuracy or validation loss) for all models. Depending on the model, we also selected regularization terms, dropout, and other model-specific parameters.

\paragraph{Computational considerations}
Our experiments involve a large number of training runs. For all models, grid sizes range from 32 to 72 possible configurations, depending on the number of hyper-parameters to choose from. However, we tried to keep the upper bound on the number of parameters as similar as possible across models. 
The total effort required, in terms of the number of single training runs, to complete model assessment procedures exceeded 47000. Such a large number required extensive use of parallelism, both in CPU and GPU, to conduct the experiments in a reasonable amount of time. We emphasize that in some cases (e.g. ECC in social datasets), training on a \emph{single} hyper-parameter configuration required more than 72 hours, which would have made the sequential exploration of one single grid last months. Therefore, due to the large amount of experiments to conduct and to the computational resources available, we limited the time to complete a single training to 72 hours. \section{Results and Discussion}
Tables \ref{tab:chemical-results} and \ref{tab:social-results} show the results of our experiments. Overall, GIN seems to be effective on social datasets. Importantly, we discover that on D\&D, PROTEINS and ENZYMES none of the GNNs are able to improve over the baseline. On the contrary, on NCI1 the baseline is clearly outperformed: this result suggests that the GNNs we analyzed can actually exploit the topological information of the graphs in this dataset. Moreover, we observe that an overly-parameterized baseline is not able to overfit the NCI1 training data completely. To see this, consider that a baseline with 10000 hidden units and no regularization reaches around 67\% training accuracy, while GIN can easily overfit () the training data. This indicates that structural information hugely affects the ability to fit the training set. 
On social datasets, we observe that adding node degrees as features is beneficial, but such an effect is more noticeable for REDDIT-BINARY, REDDIT-5K and COLLAB. 

\subsection{The Importance of Baselines}
Our results also show that structure-agnostic baselines are an essential tool to understand the effectiveness of GNNs and extract useful insights. As an example, since none of the GNNs surpasses the baseline on D\&D, PROTEINS and ENZYMES, we argue that the state-of-the-art GNN models we analyzed are not able to fully exploit the structure on such datasets yet; indeed, in chemistry, structural features are known to correlate with molecular properties \citep{chemical-structure-is-important}. For all these reasons, we suggest putting small performance gains on these datasets into the right perspective, at least until the baseline will clearly be outperformed. Currently, small average fluctuations on these datasets are likely to be caused by other factors, such as random initializations, rather than a successful exploitation of the structure. In conclusion, we warmly recommend GNN practitioners to include baseline comparisons in future works, in order to better characterize the extent of their contributions.

\subsection{The Effect of node degree}
Based on our results, using node degrees as input features is almost always beneficial to increase performances on social datasets, sometimes by a large amount. As an example, degree information is sufficient for our baseline to improve performances of \%, hence being competitive on many datasets; in particular, the baseline achieves the best performance on IMDB-BINARY. In contrast, adding node degrees is less relevant for most GNNs, since they can automatically infer such information from the structure. One notable exception is DGCNN, which explicitly needs node degrees to perform well on all datasets. Moreover, we observe that the ranking of all models, after the addition of the degrees, drastically changes; this raises the question about the impact of other structural features (such as clustering coefficient) on performances, which we leave to future works. \\ However, one may also wonder whether the addition of the degree has an influence on the number of layers that are necessary to solve the task or not. We therefore investigated the matter by computing the median number of layers across the 10 different folds. We observed a general trend across models, with GraphSAGE being the only exception, where the addition of the degree reduces the number of layers needed by  as shown in Table \ref{tab:effect-of-degree}. This may be due to the fact that most architectures find useful to compute the degree at the very first layer, as such information seems useful to the overall performances.




\begin{table}[t]
\scriptsize
\centering
\caption{Results on chemical datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold.}
\label{tab:chemical-results}
\begin{tabular}{l c c c c c}
\toprule
     & \textbf{D\&D} & \textbf{NCI1} & \textbf{PROTEINS} & \textbf{ENZYMES}\\
\midrule
 Baseline &  &   &   &   \\
 DGCNN &  &   &   &     \\
 DiffPool &  &   &   &     \\
 ECC &  &   &   &     \\
 GIN &  &   &   &     \\
 GraphSAGE &  &   &   &     \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[t]
\scriptsize
\renewcommand{\arraystretch}{1.1}
\centering
\caption{Results on social datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time ( 72 hours for a single training) or GPU memory. }
\label{tab:social-results}
\begin{tabular}{llccccc}
\toprule
     & & \textbf{IMDB-B} & \textbf{IMDB-M} & \textbf{REDDIT-B} & \textbf{REDDIT-5K} & \textbf{COLLAB}\\
    \midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{\textsc{No Features}}}
& Baseline &  &   &   &   &     \\
& DGCNN &  &   &   &   &     \\
& DiffPool &  &   &   &   &     \\
& ECC &  &   &   OOR &   OOR &   OOR   \\
& GIN &  &   &   &   &     \\
& GraphSAGE &  &   &   &   &     \\

\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{\textsc{With Degree}}}
& Baseline &  &   &   &   &     \\
& DGCNN &  &   &   &   &     \\
& DiffPool &  &   &   &   &     \\
& ECC &  &   &   OOR &   OOR &   OOR   \\
& GIN &  &   &   &   &     \\
& GraphSAGE &  &   &   &   &     \\
\bottomrule
\end{tabular}
\label{tab:results-table}
\end{table}


\begin{figure}[ht]
   \includegraphics[width=\linewidth]{fig1.eps}
   \caption{Chemical and social (with degree) benchmark results are shown together with published results (when available). For each of them, we report validation and test accuracies of the evaluated models, together with published results if available.}
   \label{fig:results-plot}
\end{figure}

\subsection{Comparison with published results}
Figure \ref{fig:results-plot} compares the average values of our test results with those reported in literature. In addition, we plot the average of our validation results across the 10 different model selections. The plots show how our test accuracies are in most cases different from what reported in the literature, and the gap between the two estimates is usually consistent. In contrast, our average validation accuracies are always higher or equal to our test results; this is expected, as discussed in Section \ref{sec:model-selection}.
Finally, we emphasize once again that our results are \emph{i}) obtained within the framework of a rigorous model selection and assessment protocol; \emph{ii}) fair with respect of data splits and input features assigned to all competitors; \emph{iii}) reproducible. In contrast, we saw in Section \ref{sec:reproducibility-issues} how published results rely on unclear or poorly documented experimental settings. \section{Conclusions}
In this paper, we wanted to show how a rigorous empirical evaluation of GNNs can help design future experiments and better reason about the effectiveness of different architectural choices. To this aim, we highlighted ambiguities in the experimental settings of different papers, and we proposed a clear and reproducible procedure for future comparisons. We then provided a complete re-evaluation of five GNNs on nine datasets, which required a significant amount of time and computational resources. This uniform environment helped us reason about the role of structure, as we found that structure-agnostic baselines outperform GNNs on some chemical datasets, thus suggesting that structural properties have not been exploited yet. Moreover, we objectively analyzed the effect of the degree feature on performances and model selection in social datasets, unveiling an effect on the depth of GNNs. Finally, we provide the graph learning community with reliable and reproducible results to which GNN practitioners can compare their architectures. We hope that this work, along with the library we release, will prove useful to researchers and practitioners that want to compare GNNs in a more rigorous way.
 
\subsubsection*{Acknowledgments}
D. Bacciu would like to acknowledge support from the Italian Ministry of Education, University, and Research (MIUR) under project SIR 2014 LIST-IT (grant n. RBSI14STDE).

\bibliography{main}
\bibliographystyle{main}

\newpage

\appendix
\section{Appendix}

\subsection{Visualization of the Evaluation Framework}
\label{appendix:evaluation-procedure}

\begin{figure}[ht]
   \includegraphics[width=\linewidth]{Assessment.eps}
   \caption{We give a visual representation of the evaluation framework. We apply an external -fold CV to get an estimate of the generalization performance of a model, and we use an hold-out technique (bottom-left) to select the best hyper-parametres. For completeness, we show that it is also possible to apply an inner -fold CV (implementing a complete \textit{Nested Cross Validation}), which obviously amounts to multiplying the computational costs of model selection by a factor .}
   \label{fig:experimental-procedure}
\end{figure}

\newpage

\subsection{Dataset Statistics}
\label{appendix:dataset-stats}
\begin{table}[ht]
    \caption{Dataset Statistics. Note that, when node labels are not present, we either assigned the same feature of 1 or the degree to all nodes in the dataset. Moreover, following the literature, we use the 18 additional node attributes for ENZYMES.} \label{tab:dataset-table}
    \begin{center}
    \begin{tabular}{c l c c c c c }
    \toprule
    &  & \# Graphs & \# Classes & \# Nodes & \# Edges & \# Node labels\\
    \toprule
    \multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{Chem.}}}
    &DD            & 1178 & 2 & 284.32 & 715.66 & 89 \\
    &ENZYMES       &  600 & 6 &  32.63 &  64.14 &  3 \\
    &NCI1          & 4110 & 2 &  29.87 &  32.30 & 37 \\
    &PROTEINS      & 1113 & 2 &  39.06 &  72.82 &  3 \\
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{\textsc{Social}}}
    &COLLAB     & 5000 & 3 & 74.49 & 2457.78 &  - \\
    &IMDB-BINARY   & 1000 & 2 &  19.77 &  96.53 &  - \\
    &IMDB-MULTI    & 1500 & 3 &  13.00 &  65.94 &  - \\
    &REDDIT-BINARY & 2000 & 2 & 429.63 & 497.75 &  - \\
    &REDDIT-5K     & 4999 & 5 & 508.82 & 594.87 &  - \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsection{Effect of Node Degree on Layering}
\begin{table}[ht]
\renewcommand\arraystretch{1.2}
\label{tab:effect-of-degree}
\caption{The table displays the median number of selcted layers in relation to the addition of node degrees as input features on all social datasets. 1 indicates that an uninformative feature is used as node label.}
\begin{center}
\begin{tabular}{lcccccccccc}
\cmidrule{2-11}
          & \multicolumn{2}{c}{\textbf{IMDB-B}} & \multicolumn{2}{c}{\textbf{IMDB-M}} & \multicolumn{2}{c}{\textbf{REDDIT-B}} & \multicolumn{2}{c}{\textbf{REDDIT-M}} & \multicolumn{2}{c}{\textbf{COLLAB}} \\
          \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
          & \textbf{1}           & \textbf{DEG} & \textbf{1}           & \textbf{DEG} & \textbf{1}           & \textbf{DEG} & \textbf{1}           & \textbf{DEG} & \textbf{1}           & \textbf{DEG}          \\
          \midrule
\textbf{DGCNN}     & 3           & 3            & 3.5          & 3           & 4          & 3            & 3          & 2            & 4           & 2            \\
\textbf{DiffPool}  & 1           & 2            & 2            & 1           & 2          & 2            & 2          & 1            & 2           & 1.5          \\
\textbf{ECC}       & 1           & 2            & 1            & 1           & -          & -            & -          & -            & -           & -            \\
\textbf{GIN}       & 3           & 2            & 4            & 2           & 4          & 4            & 4          & 3            & 4           & 4            \\
\textbf{GraphSAGE} & 4           & 3            & 5            & 4           & 3          & 4            & 3          & 5            & 3           & 5\\           
\bottomrule
\end{tabular}
\end{center}
\end{table}

\newpage

\begin{sidewaystable}[ht]
    \subsection{Hyper-parameters table}
    \label{appendix:hyperparameters}
    \caption{Hyper-parameters used for model selection.}
    \label{tab:hyperparameters}
\small
\begin{tabular}{l|cccccccccccccc}
\toprule
                                                            & Layers                                                       & \begin{tabular}[c]{@{}c@{}}Convs\\ per \\ layer\end{tabular} & Batch size                                                   & \begin{tabular}[c]{@{}c@{}}Learning \\ rate\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Hidden \\ units\end{tabular}                                               & Epochs & L2                                                         & Dropout                                             & Patience                                                       & Optimizer & Scheduler                                                                  & \begin{tabular}[c]{@{}c@{}}Dense\\ dim\end{tabular} & \begin{tabular}[c]{@{}c@{}}Embed.\\ dim\end{tabular} & \begin{tabular}[c]{@{}c@{}}Neighbors \\ Aggregation\end{tabular}                                              \\ \midrule
\begin{tabular}[c]{@{}l@{}}Baseline\\ chemical\end{tabular} & -                                                            & -                                                            & \begin{tabular}[c]{@{}c@{}}32\\ 128\end{tabular}             & \begin{tabular}[c]{@{}c@{}}1e-1\\ 1e-3\\ 1e-6\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 128\\ 256\end{tabular}                                                & 5000   & \begin{tabular}[c]{@{}c@{}}1e-2\\ 1e-3\\ 1e-4\end{tabular} & -                                                   & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & -                                                   & -                                                    & sum                                                      \\ \midrule
\begin{tabular}[c]{@{}l@{}}Baseline IMDB\end{tabular}   & -                                                            & -                                                            & \begin{tabular}[c]{@{}c@{}}32\\ 128\end{tabular}             & \begin{tabular}[c]{@{}c@{}}1e-1\\ 1e-3\\ 1e-6\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 128\\ 256\end{tabular}                                                & 3000   & \begin{tabular}[c]{@{}c@{}}1e-2\\ 1e-3\\ 1e-4\end{tabular} & -                                                   & 

\begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & -                                                   & -                                                    & sum                                                      \\ \midrule
\begin{tabular}[c]{@{}l@{}}Base. COLLAB \\ and REDDIT\end{tabular}   & -                                                            & -                                                            & \begin{tabular}[c]{@{}c@{}}32\\ 128\end{tabular}             & \begin{tabular}[c]{@{}c@{}}1e-1\\ 1e-3\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 128\end{tabular}                                                & 3000   & \begin{tabular}[c]{@{}c@{}}1e-2\\ 1e-3\\ 1e-4\end{tabular} & -                                                   & 

\begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & -                                                   & -                                                    & sum                                                      \\ \midrule
\begin{tabular}[c]{@{}l@{}}Baseline\\ ENZYMES\end{tabular}  & -                                                            & -                                                            & 32                                                           & \begin{tabular}[c]{@{}c@{}}1e-1\\ 1e-3\\ 1e-6\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 64\\ 128\\ 256\end{tabular}                                           & 5000   & \begin{tabular}[c]{@{}c@{}}1e-2\\ 1e-3\\ 1e-4\end{tabular} & -                                                   & \begin{tabular}[c]{@{}c@{}}1000, loss\\ 1000, acc\end{tabular} & Adam      & -                                                                          & -                                                   & -                                                    & sum                                                      \\ \midrule
DGCNN                                                       & \begin{tabular}[c]{@{}c@{}}2\\ 3\\ 4\end{tabular}            & 1                                                            & \begin{tabular}[c]{@{}c@{}}50 (cpu)\\ 16 (gpu)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}1e-4\\ 1e-5\end{tabular}        & \begin{tabular}[c]{@{}c@{}}32\\ 64\end{tabular}                                                       & 1000   & -                                                          & 0.5                                                 & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & 128                                                 & -                                                    & mean                                                    \\ \midrule
DiffPool                                                    & \begin{tabular}[c]{@{}c@{}}1\\ 2\end{tabular}                & 3                                                            & \begin{tabular}[c]{@{}c@{}}20 (cpu)\\ 8 (gpu)\end{tabular}   & \begin{tabular}[c]{@{}c@{}}1e-3\\ 1e-4\\ 1e-5\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 64\end{tabular}                                                       & 3000   & -                                                          & -                                                   & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & 50                                                  & \begin{tabular}[c]{@{}c@{}}64\\ 128\end{tabular}     & mean                                                      \\ \midrule
ECC                                                         & \begin{tabular}[c]{@{}c@{}}1\\ 2\end{tabular}                & 3                                                            & \begin{tabular}[c]{@{}c@{}}32 (cpu)\\ 8 (gpu)\end{tabular}   & \begin{tabular}[c]{@{}c@{}}1e-1\\ 1e-2\end{tabular}        & \begin{tabular}[c]{@{}c@{}}32\\ 64\end{tabular}                                                       & 1000   & -                                                          & \begin{tabular}[c]{@{}c@{}}0.05\\ 0.25\end{tabular} & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & SGD       & ECC-LR                                                                     & -                                                   & -                                                    & sum                                                      \\ \midrule
GIN                                                         & \begin{tabular}[c]{@{}c@{}}see\\ hidden\\ units\end{tabular} & 1                                                            & \begin{tabular}[c]{@{}c@{}}32\\ 128\end{tabular}             & 1e-2                                                       & \begin{tabular}[c]{@{}c@{}}32 (5 layers)\\ 64 (5 layers)\\ 64 (2 layers)\\ 32 (3 layers)\end{tabular} & 1000   & -                                                          & \begin{tabular}[c]{@{}c@{}}0\\ 0.5\end{tabular}     & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & \begin{tabular}[c]{@{}c@{}}Step-LR\\ (step: 50,\\ gamma: 0.5)\end{tabular} & -                                                   & -                                                    & sum                                                      \\ \midrule
GraphSAGE                                                   & \begin{tabular}[c]{@{}c@{}}3\\ 5\end{tabular}                & 1                                                            & \begin{tabular}[c]{@{}c@{}}32 (cpu)\\ 16 (cuda)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1e-2\\ 1e-3\\ 1e-4\end{tabular} & \begin{tabular}[c]{@{}c@{}}32\\ 64\end{tabular}                                                       & 1000   & -                                                          & -                                                   & \begin{tabular}[c]{@{}c@{}}500, loss\\ 500, acc\end{tabular}   & Adam      & -                                                                          & -                                                   & -                                                    & \begin{tabular}[c]{@{}c@{}}mean\\ max\\ sum\end{tabular} \\
\bottomrule
\end{tabular}
\end{sidewaystable} 
\end{document}