
\section{Experiments}

\subsection{Experimental Setups}




\begin{table*}[t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{0.7 mm}
\caption{
        \textbf{Comparison with state-of-the-art architectures on the CLUSTER, ZINC, CIFAR10 and TSP datasets. }
        \textcircled{m} denotes the architecture is mannually designed. 
        The indicator \textbf{E} denotes whether the architecture can learn edge feature. 
        The ARGNP without edge feature means that the relation space is removed from relation-aware graph search space. 
        Note that mean and standard deviation are computed across 4 independently searched GNN architectures. 
}
    \label{main}
    \begin{tabular}{@{}cccccccccccccc@{}}
    \toprule
                                                             &              & \multicolumn{3}{c}{\textbf{Node Level}}                             & \multicolumn{6}{c}{\textbf{Graph Level}}                                                                                           & \multicolumn{3}{c}{\textbf{Edge Level}}          \\ \cmidrule(l){3-5} \cmidrule(l){6-11} \cmidrule(l){12-14} 
    \multicolumn{1}{l}{\bf{Architecture}}                    &              & \multicolumn{3}{c}{\textbf{CLUSTER}}                                & \multicolumn{3}{c}{\textbf{ZINC}}                & \multicolumn{3}{c}{\textbf{CIFAR10}}                                            & \multicolumn{3}{c}{\textbf{TSP}}                 \\ \cmidrule(l){3-5} \cmidrule(l){6-8} \cmidrule(l){9-11} \cmidrule(l){12-14} 
                                                             & \textbf{E}   & \textbf{Metric}             & \textbf{Params}    & \textbf{Search}  & \textbf{Metric}               & \textbf{Params}  & \textbf{Search} & \textbf{Metric}                & \textbf{Params} & \textbf{Search}     & \textbf{Metric}         & \textbf{Params}   & \textbf{Search}       \\ 
                                                             & \CheckedBox  & \textbf{(AA \%) $\uparrow$} & \textbf{(M)}       & \textbf{(day)}   & \textbf{(MAE) $\downarrow$}   & \textbf{(M)}     & \textbf{(day)}  & \textbf{(OA \%) $\uparrow$}    & \textbf{(M)}    & \textbf{(day)}      & \textbf{(F1) $\uparrow$}& \textbf{(M)}      & \textbf{(day)}        \\ \midrule 
    \multicolumn{1}{l}{GCN~\cite{GCN}}                       & $\times$     & $68.50_{\pm0.98}$           &  $0.50$            & \textcircled{m}  & $0.367_{\pm0.011}$            & $0.50$           & \textcircled{m} & $56.34_{\pm0.38}$              & $0.10$          & \textcircled{m}     & $0.630_{\pm0.001}$      &  $0.10$           & \textcircled{m}       \\
    \multicolumn{1}{l}{GIN~\cite{GIN}}                       & $\times$     & $64.72_{\pm1.55}$           &  $0.52$            & \textcircled{m}  & $0.526_{\pm0.051}$            & $0.51$           & \textcircled{m} & $55.26_{\pm1.53}$              & $0.10$          & \textcircled{m}     & $0.656_{\pm0.003}$      &  $0.10$           & \textcircled{m}       \\
    \multicolumn{1}{l}{GraphSage~\cite{SAGE}}                & $\times$     & $63.84_{\pm0.11}$           &  $0.50$            & \textcircled{m}  & $0.398_{\pm0.002}$            & $0.51$           & \textcircled{m} & $65.77_{\pm0.31}$              & $0.10$          & \textcircled{m}     & $0.665_{\pm0.003}$      &  $0.10$           & \textcircled{m}       \\
    \multicolumn{1}{l}{GAT~\cite{GAT}}                       & $\times$     & $70.59_{\pm0.45}$           &  $0.53$            & \textcircled{m}  & $0.384_{\pm0.007}$            & $0.53$           & \textcircled{m} & $64.22_{\pm0.46}$              & $0.11$          & \textcircled{m}     & $0.671_{\pm0.002}$      &  $0.10$           & \textcircled{m}       \\
    \multicolumn{1}{l}{GatedGCN~\cite{GG}}                   & \checkmark   & $76.08_{\pm0.34}$           &  $0.50$            & \textcircled{m}  & $0.214_{\pm0.013}$            & $0.51$           & \textcircled{m} & $67.31_{\pm0.31}$              & $0.10$          & \textcircled{m}     & $0.838_{\pm0.002}$      &  $0.53$           & \textcircled{m}       \\
    \multicolumn{1}{l}{PNA~\cite{PNA}}                       & $\times$     & N/A                         &  N/A               & N/A              & $0.320_{\pm0.032}$            & $0.39$           & \textcircled{m} & $70.46_{\pm0.44}$              & $0.11$          & \textcircled{m}     & N/A                     &  N/A              & N/A                   \\
    \multicolumn{1}{l}{PNA~\cite{PNA}}                       & \checkmark   & N/A                         &  N/A               & N/A              & $0.188_{\pm0.004}$            & $0.39$           & \textcircled{m} & $70.47_{\pm0.72}$              & $0.11$          & \textcircled{m}     & N/A                     &  N/A              & N/A                   \\
    \multicolumn{1}{l}{DGN~\cite{DGN}}                       & $\times$     & N/A                         &  N/A               & N/A              & $0.219_{\pm0.010}$            & $0.39$           & \textcircled{m} & $72.70_{\pm0.54}$              & $0.11$          & \textcircled{m}     & N/A                     &  N/A              & N/A                   \\
    \multicolumn{1}{l}{DGN~\cite{DGN}}                       & \checkmark   & N/A                         &  N/A               & N/A              & $0.168_{\pm0.003}$            & $0.39$           & \textcircled{m} & $72.84_{\pm0.42}$              & $0.11$          & \textcircled{m}     & N/A                     &  N/A              & N/A                   \\
    \multicolumn{1}{l}{GNAS-MP~\cite{Cai2021RethinkingGN}}   & $\times$     & $74.77_{\pm0.15}$           &  $1.61$            & $0.80$           & $0.242_{\pm0.005}$            & $1.20$           & $0.40$          & $70.10_{\pm0.44}$              & $0.43$          & $3.20$              & $0.742_{\pm0.002}$      &  $1.20$           & $2.10$                \\ \midrule 
    \multicolumn{1}{l}{ARGNP (2)}                            & $\times$     & $61.61_{\pm0.27}$           &  $0.07$            & $0.04$           & $0.430_{\pm0.003}$            & $0.09$           & $0.01$          & $66.55_{\pm0.13}$              & $0.10$          & $0.11$              & $0.655_{\pm0.003}$      &  $0.09$           & $0.05$                \\ 
    \multicolumn{1}{l}{ARGNP (4)}                            & $\times$     & $64.06_{\pm0.45}$           &  $0.14$            & $0.07$           & $0.303_{\pm0.013}$            & $0.14$           & $0.01$          & $66.65_{\pm0.39}$              & $0.18$          & $0.14$              & $0.668_{\pm0.003}$      &  $0.17$           & $0.06$                \\ 
    \multicolumn{1}{l}{ARGNP (8)}                            & $\times$     & $68.73_{\pm0.12}$           &  $0.25$            & $0.20$           & $0.239_{\pm0.009}$            & $0.27$           & $0.02$          & $67.37_{\pm0.32}$              & $0.33$          & $0.48$              & $0.674_{\pm0.002}$      &  $0.29$           & $0.21$                \\ 
    \multicolumn{1}{l}{ARGNP (16)}                           & $\times$     & $71.92_{\pm0.29}$           &  $0.53$            & $0.71$           & $0.221_{\pm0.004}$            & $0.51$           & $0.06$          & $67.10_{\pm0.51}$              & $0.58$          & $1.77$              & $0.684_{\pm0.002}$      &  $0.56$           & $0.76$                \\ \midrule
    \multicolumn{1}{l}{ARGNP (2)}                            & \checkmark   & $64.99_{\pm0.31}$           &  $0.08$            & $0.06$           & $0.318_{\pm0.009}$            & $0.08$           & $0.01$          & $69.14_{\pm0.30}$              & $0.10$          & $0.17$              & $0.773_{\pm0.001}$      &  $0.08$           & $0.08$                \\ 
    \multicolumn{1}{l}{ARGNP (4)}                            & \checkmark   & $74.75_{\pm0.25}$           &  $0.15$            & $0.09$           & $0.197_{\pm0.006}$            & $0.15$           & $0.01$          & $71.83_{\pm0.32}$              & $0.17$          & $0.23$              & $0.821_{\pm0.001}$      &  $0.14$           & $0.10$                \\ 
    \multicolumn{1}{l}{ARGNP (8)}                            & \checkmark   & $76.32_{\pm0.03}$           &  $0.29$            & $0.31$           & $0.155_{\pm0.003}$            & $0.28$           & $0.04$          & $73.72_{\pm0.32}$              & $0.33$          & $0.84$              & $0.841_{\pm0.001}$      &  $0.30$           & $0.39$                \\ 
    \multicolumn{1}{l}{ARGNP (16)}                           & \checkmark   & \textcolor{red}{$\bm{77.35_{\pm0.05}}$}  &  $0.52$            & $1.10$           & \textcolor{red}{$\bm{0.136_{\pm0.002}}$}   & $0.52$           & $0.15$          & \textcolor{red}{$\bm{73.90_{\pm0.15}}$}     & $0.64$          & $2.95$              & \textcolor{red}{$\bm{0.855_{\pm0.001}}$}   &  $0.62$                & $1.23$                     \\ 
    \bottomrule 
    \end{tabular}
    \vspace{-1em}
\end{table*}
     
\noindent
\textbf{Dataset. }
We evaluate our method on six datasets, \ie, ZINC~\cite{ZINC}, CLUSTER~\cite{bench}, CIFAR10~\cite{CIFAR10}, TSP~\cite{bench}, ModelNet10~\cite{ModelNet}, ModelNet40~\cite{ModelNet} across four different graph learning tasks (node classification, edge classification, graph classification and graph regression). 
ZINC is one popular real-world molecular dataset of 250K graphs, whose task is graph property regression, out of which we select 12K for efficiency following works~\cite{Cai2021RethinkingGN, PNA, bench}. 
CLUSTER is the node classification tasks generated via Stochastic Block Models~\cite{SBM}, which are used to model communications in social networks by modulating the intra-community and extra-community connections. 
CIFAR10 is the original classical image classification dataset and converted into graphs using superpixel~\cite{SLIC} algorithm to test graph classification task. 
TSP dataset is based on the classical \emph{Travelling Salesman Problem}, which tests edge classification on 2D Euclidean graphs to identify edges belonging to the optimal TSP solution. 
ModelNet is a dataset for 3D object recognition with two variants, ModelNet10 and ModelNet40, which comprise objects from 10 and 40 classes, respectively. 
We sample 1024 points for each object as input and use \emph{k-NN} algorithm to construct the edges ($k = 9$ by default unless it is specified). 



\noindent
\textbf{Searching settings. }
The original architecture is initialized with 2 feature vertices. 
We perform \emph{network proliferation} for 4 iterations to obtain a sequence of GNN architectures with the size of $\{2,4,8,16\}$. 
Specifically, we choose SGAS~\cite{SGAS} as the search strategy that can differentiate the local supernet to a specific subnet. 
During the network differentiation, after warming up for 10 epochs, SGAS begins to simultaneously determine one node-learning operation and one relation-mining operation for every 5 epochs. 
Thus, the search epoch is set to $\{25, 25, 45, 85\}$ for 4 sequential iterations. 
To carry out the architecture search, we hold out half of the training data as the validation set. 
For one-shot differentible search strategies (SGAS~\cite{SGAS} and DARTS~\cite{DARTS}), there are operation weights $\bm{w}$ and architectural parameters $\bm{\alpha}$ to be optimized. 
We use momentum SGD to optimize the weights $\bm{w}$, with initial learning rate $\eta_{\bm{w}}=0.025$ (anneald down to zero following a cosine schedule without restart), momentum $0.9$, and weight decay $3 \times 10^{-4}$. 
We use Adam~\cite{ADAM} as the optimizer for $\bm{\alpha}$, with initial learning rate $\eta_{\alpha} = 3 \times 10^{-4}$, momentum $\beta = (0.5, 0.999)$ and weight decay $10^{-3}$. 



\noindent
\textbf{Training settings. }
We follow all the training settings~(data splits, optimizer, metrics, \etc) in work~\cite{Cai2021RethinkingGN,bench}. 
Specifically, we adopt Adam~\cite{ADAM} with the same learning rate decay for all runs. 
The learning rate is initialized with $10^{-3}$, which is reduced by half if the validation loss stops decreasing after $20$ epochs. 
The weight decay is set to $0$. 
The dropout is set to $0.5$ to alleviate the overfitting. 
Our architectures are all trained for $400$ epochs with a batch size of $32$. 
We report the mean and standard deviation of the metric on the test dataset of $4$ discovered architectures. 
These experiments are run on a single NVIDIA GeForce RTX 3090 GPU. 


\subsection{Results and Analysis}

In Table~\ref{main} and Table~\ref{modelnet}, we compare our ARGNP with the state-of-the-art hand-crafted and search-based GNN architectures on the CLUSTER, ZINC, CIFAR10, TSP, ModelNet10, and ModelNet40 datasets. 
The evaluation metric is the average accuracy (AA) for CLUSTER, mean absolute error (MAE) for ZINC, F1-score (F1) for TSP. 
For CIFAR10, ModelNet10, and ModelNet40, we use the overall accuracy (OA) as the evaluation metric. 
To make a fair comparison, we also report the architecture parameters, the search cost, and the mean and standard deviation of all the metrics. 
We can see that, on all the six datasets for four classical graph learning tasks, the GNN architectures discovered by our ARGNP surpass the state-of-the-art architectures by a large margin in terms of both mean and standard deviation. 
Compared with the state-of-the-art search-based method GNAS-MP \cite{Cai2021RethinkingGN}, our searched architecture can easily achieve better performance with only $\frac{1}{10}\sim\frac{1}{4}$ parameters. 
This benefits from that the relation-aware graph search space can mine hierarchical relation information (such as local structural similarity) to guide anisotropic message passing. 
Moreover, the network proliferation search paradigm can efficiently and effectively explore the proposed search space. 
We visualize the best-performed GNN architecture with the size of 4 in Figure \ref{ZINC_cell}, which is searched on the ModelNet40 dataset. 
Other examples are provided in the supplementary material. 


\begin{figure*}[t]

    \centering
    \includegraphics[scale = 0.55, trim = 30 30 30 30, clip]{figures/CIFAR10_cell4.pdf}
    \vspace{-0.5em}
    \caption{
        \textbf{The best GNN architecture with the network size of 4 searched on the ModelNet40 dataset. }
    }
    \vspace{-1em}
    \label{ZINC_cell}

\end{figure*}

\subsection{Ablation of Search Space}

We study the influence of the relation search space in our proposed relation-aware graph search. 
First, we construct a search space variant by removing the relation search space. 
Then we perform GNN architecture search on this variant using the network proliferation search paradigm and obtain a sequence of GNN architectures with the size of $\{2,4,8,16\}$. 
These GNN architectures are evaluated on six datasets. 
For a fair comparison, we increase the dimension of the node features to keep the architectural parameters comparable. 
As shown in Table \ref{main} and Table \ref{modelnet}, the best performance of the search space variant without relation learning descends by a large margin. Under all the different network size settings, relation learning can significantly improve the capability of graph reasoning. 
Interestingly, this improvement is also observed on the CLUSTER, CIFAR10, and ModelNet datasets which don't have original edge features. 
Taking the CLUSTER dataset as an example, it aims at identifying the community clusters, where the graphs represent the community networks. 
The edges play a role in connecting two nodes and have no original meaningful features. 
In this case, relation learning can mine hierarchical relational information by extracting local structural similarities between nodes. 
This can help distinguish between intra-community and extra-community connections for learning better discriminative node features. 

\begin{table}[t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{0.5 mm}
    \caption{
        \textbf{Comparision with state-of-the-art architectures on the ModelNet10 and ModelNet40 datasets at 3D point cloud recognition task. }
        \textbf{L} denotes the size of GNN architecture. 
    }
    \label{modelnet}
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
                                                        &                       &                   & \textbf{ModelNet10}        &   & \textbf{ModelNet40}           &                       &                       \\ \cmidrule(l){4-4} \cmidrule(l){6-6}
    \textbf{Architecture}                               & \textbf{L}            & \textbf{E}        & \textbf{Metric}            &   & \textbf{Metric}               & \textbf{Params}       & \textbf{Search}       \\
                                                        & \textbf{(\#)}         & \CheckedBox       & \textbf{(OA \%) $\uparrow$}&   & \textbf{(OA \%) $\uparrow$}   & \textbf{(M)}          & \textbf{(Day)}        \\ \midrule
    \multicolumn{1}{l}{3DmFV~\cite{3DmFV}}              & /                     &  /                & $95.2$                     &   & $91.6$                        & $45.77$               & \textcircled{m}       \\
\multicolumn{1}{l}{PointNet++~\cite{PointNetA}}     & /                     &  /                & N/A                        &   & $90.7$                        & $1.48$                & \textcircled{m}       \\
    \multicolumn{1}{l}{PCNN~\cite{PCNN}}                & /                     &  /                & N/A                        &   & $92.3$                        & $8.20$                & \textcircled{m}       \\
    \multicolumn{1}{l}{PointCNN~\cite{PointCNN}}        & /                     &  /                & N/A                        &   & $92.2$                        & $0.60$                & \textcircled{m}       \\
    \multicolumn{1}{l}{DGCNN~\cite{DGCNN}}              & /                     &  /                & N/A                        &   & $92.2$                        & $1.84$                & \textcircled{m}       \\
    \multicolumn{1}{l}{KPConv~\cite{KPConv}}            & /                     &  /                & N/A                        &   & $92.9$                        & $14.3$                & \textcircled{m}       \\
    \multicolumn{1}{l}{SGAS~\cite{SGAS}}                & $9$                   & \checkmark        & N/A                        &   & $92.93_{\pm 0.19}$            & $8.87$                & $0.19$                \\ \midrule
    \multicolumn{1}{l}{ARGNP}                           & $2$                   & $\times$          & $93.20_{\pm 0.24}$         &   & $91.11_{\pm 0.24}$            & $1.80$                & $0.03$                \\ 
    \multicolumn{1}{l}{ARGNP}                           & $4$                   & $\times$          & $93.86_{\pm 0.25}$         &   & $91.30_{\pm 0.22}$            & $2.27$                & $0.04$                \\ 
    \multicolumn{1}{l}{ARGNP}                           & $8$                   & $\times$          & $94.23_{\pm 0.22}$         &   & $91.85_{\pm 0.18}$            & $3.20$                & $0.15$                \\ \midrule
    \multicolumn{1}{l}{ARGNP}                           & $2$                   & \checkmark        & $95.07_{\pm 0.31}$         &   & $92.47_{\pm0.23}$             & $2.50$                & $0.04$                \\ 
    \multicolumn{1}{l}{ARGNP}                           & $4$                   & \checkmark        & $95.35_{\pm 0.23}$         &   & $92.80_{\pm0.19}$             & $3.05$                & $0.06$                \\ 
    \multicolumn{1}{l}{ARGNP}                           & $8$                   & \checkmark        & \textcolor{red}{$\bm{95.87_{\pm0.22}}$}     &   & \textcolor{red}{$\bm{93.33_{\pm0.15}}$}        & $4.15$                & $0.20$                      \\ 
    \bottomrule
    \end{tabular}
    \vspace{-1.0em}
\end{table}
 
\subsection{Ablation of Search Paradigm}

To investigate the effectiveness of our \emph{Network Proliferation Search Paradigm (NPSP)}, we conduct the ablation experiments on ZINC dataset around \emph{network size}, \emph{search strategy}, \emph{whether to use cell trick} and \emph{whether to use NPSP}. 
We run 14 different experiments and report the results in Table \ref{proliferation}. 
We observe the following phenomena. 
First, \emph{the cell trick improves the search efficiency but weakens the expressive capability of graph search space}.
This results from its original assumption where the GNN architecture is a stack of the same building cells that narrows our relation-aware graph search space. 
Therefore, the search strategy with the cell trick performs worse than that without it, which is demonstrated by the contrast between exp 2 and exp 3, exp 5 and exp 6. 
Second, our \emph{NPSP can both significantly improves the search efficiency and search effect with different search strategies}. 
The performance improvement benefits from that the \emph{NPSP} can alleviate the subnet interference and mitigate the shrink of search space by breaking away from the cell assumption. 
The efficiency improvement lies in that NPSP shifts the training object from global supernet to sequential local supernets. 
They are supported by exp 4, 7, 11, and 14, where NPSP achieves the best performance with less time cost under all the experimental settings. 











\begin{table}[t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{0.6 mm}
    \caption{
        \textbf{Performance of the relation-aware graph search space under different  settings. }
        \textbf{Cell} is an indicator of whether to use the cell trick. 
        \textbf{NPSP} is an indicator of whether to use the network proliferation search. 
        OOM denotes out of memory. 
    }
    \label{proliferation}
    \begin{tabular}{@{}ccccccccc@{}}
    \toprule
                          &                                                         & \multicolumn{7}{c}{\textbf{ZINC}}                                                                                                                                                                      \\ \cmidrule(l){3-9} 
     \textbf{\#}          & \textbf{Method}                                         & \textbf{L}                          & \textbf{Search}          & \textbf{Cell}  & \textbf{NPSP}                    & \textbf{Metric}                                 & \textbf{Params}               & \textbf{Search}                                    \\
                          &                                                         & \textbf{(\#)}                       & \textbf{Strategy}        & \CheckedBox    & \CheckedBox                      & \textbf{(MAE) $\downarrow$}                     & \textbf{(M)}                  & \textbf{(Day)}                                     \\ \midrule
1                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  Random                  &  $\times$      &  $\times$                        & $0.303_{\pm0.058}$                              & $0.27$                        &  $0.$                                            \\
       2                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  DARTS                   &  \checkmark    &  $\times$                        & $0.160_{\pm0.005}$                              & $0.28$                        &  $0.17$                                            \\
       3                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  DARTS                   &  $\times$      &  $\times$                        & $0.157_{\pm0.008}$                              & $0.28$                        &  $0.30$                                            \\
       4                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  DARTS                   &  $\times$      &  \checkmark                      & $\bm{0.150_{\pm0.006}}$                         & $0.29$                        &  $\bm{0.08}$                                       \\
       5                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  SGAS                    &  \checkmark    &  $\times$                        & $0.165_{\pm0.008}$                              & $0.30$                        &  $0.13$                                            \\
       6                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  SGAS                    &  $\times$      &  $\times$                        & $0.161_{\pm0.008}$                              & $0.30$                        &  $0.25$                                            \\
       7                  & \multicolumn{1}{l}{R-space}                             & $8$                                 &  SGAS                    &  $\times$      &  \checkmark                      & $\bm{0.155_{\pm0.003}}$                         & $0.28$                        &  $\bm{0.06}$                                       \\ \cmidrule{2-9}
       8                  & \multicolumn{1}{l}{R-space}                             & $16$                                &  Random                  &  $\times$      &  $\times$                        & $0.185_{\pm0.024}$                              & $0.51$                        &  $0.$                                            \\
       9                  & \multicolumn{1}{l}{R-space}                             & $16$                                &  DARTS                   &  \checkmark    &  $\times$                        & $0.144_{\pm0.004}$                              & $0.57$                        &  $0.38$                                            \\
       10                 & \multicolumn{1}{l}{R-space}                             & $16$                                &  DARTS                   &  $\times$      &  $\times$                        &  N/A                                            &  N/A                          &  OOM                                               \\
       11                 & \multicolumn{1}{l}{R-space}                             & $16$                                &  DARTS                   &  $\times$      &  \checkmark                      & $\bm{0.139_{\pm0.005}}$                         & $0.56$                        &  $\bm{0.24}$                                       \\
       12                 & \multicolumn{1}{l}{R-space}                             & $16$                                &  SGAS                    &  \checkmark    &  $\times$                        & $0.140_{\pm0.003}$                              & $0.60$                        &  $0.32$                                            \\ 
       13                 & \multicolumn{1}{l}{R-space}                             & $16$                                &  SGAS                    &  $\times$      &  $\times$                        &  N/A                                            &  N/A                          &  OOM                                               \\
       14                 & \multicolumn{1}{l}{R-space}                             & $16$                                &  SGAS                    &  $\times$      &  \checkmark                      & $\bm{0.136_{\pm0.002}}$        & $0.52$                        &  $\bm{0.21}$                                       \\
    \bottomrule
    \end{tabular}
    \vspace{-1.0em}
\end{table}


 








\begin{figure*}[t]

    \centering
    \includegraphics[scale = 0.68, trim = 0 0 0 0, clip]{figures/pointcloudVEN.pdf}
\caption{
        \textbf{Visualization of the learned hierarchical features for 3D point cloud recognition} (taking table as an example). 
        Relation features with different edge color distribution have different message passing preferences. 
        Node features with different node color distribution represent different clustering effects. 
    }
    \vspace{-1.0em}
    \label{pointcloud}
\end{figure*}

\subsection{Visualizing Hierarchical Features}



To better demonstrate the effectiveness of the relation learning, we provide relation and node features visualization on ModelNet40 dataset. 
During the inference, we feed forward one 3D pointcloud object into the network with the input $\{\bm{V}_{in_0}, \bm{V}_{in_1}, \bm{E}_{in_0}, \bm{E}_{in_1}\}$, where $\bm{V}_{in_0} = \bm{V}_{in_1} \in \mathbb{R}^{1024 \times 3}$ are the 3D coordinates and $\bm{E}_{in_0} = \bm{E}_{in_1} = \bm{1} \in \mathbb{R}^{(1024 \times 9)\times 1}$ are the pseudo relation features. 
The hierarchical node/relation features generated from each layer is denoted as $\{\bm{V}_{1}, \cdots, \bm{V}_{4}\}$ and $\{\bm{E}_1, \cdots, \bm{E}_{4}\}$, respectively. 
For better visualization on the point cloud graph, we reduce the feature dimension to 1 through principal component analysis (PCA).
The edges with a similar color are considered to have the same message passing preferences, while the nodes with a similar color are considered to belong to a similar cluster. 
A visualization example from ARGNP and a version without relation learning architecture are shown in Figure \ref{pointcloud}A and \ref{pointcloud}B, respectively. 

As shown in Figure \ref{pointcloud}A, ARGNP can capture the structural information and well discriminate different parts of the object (\eg, the legs, desktop, and border of the table in $V_4^{(A)}$). 
In contrast, as shown in Figure \ref{pointcloud}B, the original GNN without relation learning architecture can only gradually propagate the node information through the input graph based on 3D coordinates. 
As a result, the similar nodes that are distant in the 3D space can not be well clustered (\eg, 4 legs of the table in $V_4^{(B)}$). 
The above comparison shows that the relation features can guide better message passing mechanisms to learn more effective node features. 
To show the role of the relation learning more specifically, accompanied with the searched GNN in Figure \ref{ZINC_cell}, we analyze the features in Figure \ref{pointcloud}A. 
For example, the $E_1^{(A)}$ is learned by \emph{substraction} operation. The \emph{substraction} operation is similar to an ``border detection" operation that can discriminate the directions between two nodes. 
In this way, the ARGNP can directly distinguish groups of parallel components of an object (\eg, legs, parallel borders, \etc in $V_2^{(A)}$). 
However, without using the relation features, it requires numerous rounds of message passing which may cause the over smoothing problem. 
With the help of relation learning, the components with similar structures can be well clustered. 








