
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{capt-of}
\usepackage{ulem}

\title{Minimalist and High-Performance Semantic Segmentation with Plain Vision Transformers}



\author{Yuanduo Hong$^1$ \quad Jue Wang$^1$ \quad Weichao Sun$^1$ \quad Huihui Pan$^1$\thanks{: Corresponding author.}
\\
$^1$Research Institute of Intelligent Control and Systems, Harbin Institute of Technology \\
{\tt\small ydhong@hit.edu.cn, huihuipan@hit.edu.cn}
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
In the wake of Masked Image Modeling (MIM), a diverse range of plain, non-hierarchical Vision Transformer (ViT) models have been pre-trained with extensive datasets, offering new paradigms and significant potential for semantic segmentation. Current state-of-the-art systems incorporate numerous inductive biases and employ cumbersome decoders. Building upon the original motivations of plain ViTs, which are simplicity and generality, we explore high-performance `minimalist' systems to this end. Our primary purpose is to provide simple and efficient baselines for practical semantic segmentation with plain ViTs. Specifically, we first explore the feasibility and methodology for achieving high-performance semantic segmentation using the last feature map. As a result, we introduce the PlainSeg, a model comprising only three 3$\times$3 convolutions in addition to the transformer layers (either encoder or decoder). In this process, we offer insights into two underlying principles: (i) high-resolution features are crucial to high performance in spite of employing simple up-sampling techniques and (ii) the slim transformer decoder requires a much larger learning rate than the wide transformer decoder. On this basis, we further present the PlainSeg-Hier, which allows for the utilization of hierarchical features. Extensive experiments on four popular benchmarks demonstrate the high performance and efficiency of our methods. They can also serve as powerful tools for assessing the transfer ability of base models in semantic segmentation. Code is available at \url{https://github.com/ydhongHIT/PlainSeg}.
\end{abstract}

\section{Introduction}

In modern semantic segmentation models, a typical architecture comprises a hierarchical pre-trained backbone serving as the feature extractor and a decoder adapting the extracted features to per-pixel predictions \citep{long2015fully,ronneberger2015u,badrinarayanan2017segnet}. In the CNN era, ResNets \citep{he2016deep} with different depths are widely adopted as feature extractors. On top of the hierarchical features extracted by backbones, various decoders are proposed to extract contextual information and refine feature maps \citep{ghiasi2016laplacian,chen2017deeplab,zhao2017pyramid,lin2017refinenet,peng2017large,zhang2018context}. For the last two years, vision transformers were introduced to semantic segmentation and showed significant improvements over strong CNN models \citep{zheng2021rethinking,liu2021swin,xie2021segformer,strudel2021segmenter,yuan2021hrformer}. Naturally, researchers have explored decoder designs tailored for ViT backbones \citep{zheng2021rethinking,xie2021segformer,cao2022swin,yan2022lawin}. A promising discovery is that cumbersome decoders are unnecessary for ViT-based models \citep{xie2021segformer}, which brings out a series of practical segmentation models relying on efficient backbones and decoders \citep{xie2021segformer,gu2022multi,guo2022segnext}. However, most of the existing works in this area have focused on hierarchical ViTs, which are considered more suitable for dense prediction tasks than plain ViTs.

Recently, masked image modeling (MIM) has given rise to potent and scalable pre-trained plain ViTs, which substantially outperform their supervised counterparts in downstream tasks \citep{he2022masked,bao2022beit,zhou2022image,peng2022beit}. The application of plain ViTs, originally designed for natural language processing (NLP), to the domain of semantic segmentation, is an intriguing endeavor as it contributes to the development of a more versatile foundational model capable of excelling in two distinct tasks. Early works such as SETR \citep{zheng2021rethinking} and DPT \citep{ranftl2021vision} design simple convolution-based decoders to adapt plain ViTs for semantic segmentation. In Segmenter \citep{strudel2021segmenter}, the authors propose a mask transformer to construct a full transformer model for semantic segmentation. More recently, SegViT \citep{zhang2022segvit} proposes the attention-to-mask (ATM) module to harness the off-the-shelf features of plain ViTs. In contrast, ViT-Adapter \citep{chen2023vision} introduces an auxiliary top-down branch with numerous induction biases to unleash the potential of plain backbones. Notably, ViT-adapter with the Mask2Former \citep{cheng2022masked} framework achieves state-of-the-art performance on multiple semantic segmentation datasets. However, despite its very impressive performance and considerable influence, we note that there are several overlooked issues with this approach. Firstly, when viewed within the context of the complete system, the introduction of numerous induction biases contradicts the original essence of plain vision transformers. Subsequent works built upon it may result in increasingly complex systems. If the sole objective is achieving state-of-the-art performance, hierarchical backbones \citep{wang2022internimage} may offer a more suitable alternative. Secondly, from the perspective of transfer learning, employing cumbersome decoders is unnecessary. This is because a larger number of randomly initialized parameters typically necessitates a greater amount of training data and labeled samples from downstream tasks to realize their full potential, which deviates from the primary goal of transfer learning. There is a fact that the proportion of randomly initialized parameters to pre-trained parameters is close to 90 $\%$ for the state-of-the-art ViT-L-Adapter-Mask2Former.


In the light of the above analysis, we aim to develop high-performance `minimalist' systems for segmentation segmentation with plain ViTs. The `minimalist' pursuit encompasses both the reduction of inductive biases and the simplification of decoders, which are inherently interconnected. Recent research in objective detection \citep{li2022exploring} and human pose estimation \citep{xu2022vitpose} has suggested that leveraging the last feature maps from MIM pre-trained plain ViTs, along with simple decoders, can yield satisfactory results. It implies that plain backbones can learn the prior knowledge from data, rendering the sophisticated and dedicated designs unnecessary. These findings establish the foundation for the feasibility of high-performance `minimalist' systems in semantic segmentation. Motivated by these insights, we initially opt for a hard setting, focusing solely on utilizing the output of the last transformer layer (eliminating the use of hierarchical features). The proposed method named \textbf{PlainSeg} consists of a pre-trained plain ViT, three 3$\times$3 convolution layers, and several lightweight transformer decoder layers, as illustrated in Fig. \ref{fig1}. While developing such systems, we offer insights into the underlying principles that render efficacy and promote efficiency: (i) high-resolution features are crucial to high performance in spite of employing simple up-sampling techniques and (ii) the slim transformer decoder requires a much larger learning rate than the wide transformer decoder, for instance, 10$\times$ difference. On this basis, we further present \textbf{PlainSeg-Hier} which incorporates hierarchical features, as seen in previous works. We benchmark the proposed methods on four popular datasets (\textit{i.e.} ADE20K, PASCAL Context, COCO-Stuff-10K, and COCO-Stuff-164K) with various pre-trained plain ViTs, reporting performance and inference efficiency of numerous models.

Our contributions can be summarized in the following three aspects: (i) we develop high-performance `minimalist' systems for semantic segmentation with plain ViTs, which achieve highly competitive performance compared to ViT-Adapter and outperform SegViT; (ii) we offer insights into the practical principles for adapting potent plain ViTs to semantic segmentation tasks; (iii) the combination of high performance, elegant simplicity, and efficient inference and parameter utilization in our methods establishes solid baselines for future research in this field. Moreover, these methods serve as powerful tools for assessing the transfer ability of forthcoming plain ViT backbones in the context of semantic segmentation.

Note that we do not claim any algorithmic superiority over the current state-of-the-art. We do not conduct absolutely fair comparison experiments with previous methods as we train the models with fewer iterations to facilitate faster research. As stated, the contributions of this study are simple and efficient baselines and several practical principles.

\section{Related Work}

\textbf{Vision Transformers.} Since the introduction of transformer architecture in NLP by Dosovitskiy \textit{et al.} \citep{vaswani2017attention,dosovitskiy2021an}, vision transformers have made measurable progress in the field of computer vision. There are two main streams during the development of vision transformers: some introduce the inductive biases of convolutional networks such as the hierarchical features and locality of convolutions for better image modeling \citep{wang2021pyramid,liu2021swin,yuan2021tokens,chu2021twins,wu2021cvt,wang2022pvt} while others explore the potential of original (plain) transformers. The latter covers the improvement of supervised training strategies \citep{touvron2021training,steiner2021train,touvron2022deit}, masked image modeling for self-supervised learning \citep{he2022masked,bao2022beit,zhou2022image,chen2023context,peng2022beit,fang2023eva}, and the utilization of multi-model data \citep{wang2023image,fang2023eva}. In this study, we transfer pre-trained plain ViTs to semantic segmentation and continue its `minimalist' pursuit.

\textbf{General Semantic Segmentation.} There have been numerous works in applying convolution neural networks (CNN) to semantic segmentation \citep{chen2017deeplab,zhao2017pyramid,peng2017large,lin2017refinenet,zhang2018context,chen2017rethinking,fu2019dual,huang2019ccnet,takikawa2019gated,yuan2021ocnet}. They generally follow the encoder-decoder paradigm established by seminal works \citep{long2015fully,ronneberger2015u}. Due to the limited receptive fields of local convolutions, previous works mainly aim at better capturing contextual information, in both encoders and decoders \citep{chen2017deeplab,zhao2017pyramid,peng2017large,zhang2018context,chen2017rethinking,fu2019dual,huang2019ccnet,yuan2021ocnet}.
With the rise of vision transformers, recent research shows that the performance can be boosted by only replacing the CNN backbones with various pyramid ViTs \citep{liu2021swin,xie2021segformer,yuan2021hrformer,gu2022multi}. Moreover, \cite{cheng2021per,cheng2022masked} decouple semantic segmentation into mask classification and prediction , which has been widely adopted by state-of-the-art methods in semantic segmentation.



\textbf{Plain Vision Transformers for Semantic Segmentation.} Plain ViTs are characterized by a patch embedding layer and stacked transformer layers with a constant sequence length or feature resolution. Consequently, they operate quite differently from conventionally hierarchical architectures. It is worth noting that while several prior works have explored plain ViTs for semantic segmentation \citep{zheng2021rethinking,ranftl2021vision,strudel2021segmenter,lin2023structtoken,chen2023vision,zhang2022segvit}, this area remains relatively underexplored, particularly when compared to the extensive exploration of semantic segmentation with hierarchical backbones.
In recent works, there has been a trend towards increasing complexity in decoders or adapters \citep{lin2023structtoken,chen2023vision}. Simultaneously, in the context of masked image modeling pre-training with plain ViTs \citep{bao2022beit,he2022masked,chen2023context,peng2022beit}, there is still a prevalent use of the UperNet decoder for transfer learning in semantic segmentation due to its simplicity. Although SegViT utilizes the off-the-shelf features of plain ViTs to keep the method straight and efficient, it falls short of achieving a similar level of performance as ViT-Adapter-Mask2Former. Furthermore, the dedicated ATM module used in SegViT impacts its generality to some extent. In this study, we develop high-performance `minimalist' systems to fill in the gaps.

\section{Approach}


\subsection{Motivations}

Our motivations stem from reconsideration of the existing methods in the context of plain ViTs. As discussed above, the current state-of-the-art ViT-Adapter-MaskFormer is not elegant and efficient for practical semantic segmentation with plain ViTs. It motivates us to develop `minimalist' systems that incorporate fewer induction biases and feature architectural simplicity. Additionally, we assume that the potent representations learned by MIM plain ViTs will obviate the need for certain sophisticated and dedicated designs. This belief underscores the power of representational learning and inspires us to unearth the underlying principles that render efficacy and promote efficiency in this context.

\subsection{Case Studies about UperNet decoder}

We start with the widely used UperNet decoder to conduct some case studies for more insight. All the experiments are conducted with 80k train iterations and a 512$\times$512 training crop size on ADE20K. Other settings are identical to those in the original paper of BEiT \citep{bao2022beit}. As shown in Table \ref{tab:1}, the UperNet decoder outperforms the linear decoder by about two points, demonstrating its effectiveness for plain ViTs. The linear decoder incorporates a single linear layer upon the final output of the plain ViT, which is a very simple baseline used in \citep{strudel2021segmenter}. We emphasize this as the UperNet decoder has been proven unnecessary for advanced pyramid ViTs \citep{xie2021segformer,gu2022multi}. Furthermore, we observe that the auxiliary supervision is unnecessary and the usage of the pyramid pooling module (PPM) affects the systematic performance slightly.

After removing the unnecessary components, the UperNet decoder indeed incorporates two main characteristics: utilizing hierarchical features and fusing these features at high resolution. Inspired by recent findings in object detection and human pose estimation that the last feature map of MIM plain ViTs is sufficient\citep{li2022exploring,xu2022vitpose}, we design a simple up-sampling decoder that only utilizes the last feature map and gradually increases the feature resolution. Specifically, we apply bilinear interpolation twice with 2$\times$ up-sampling, followed by a 3$\times$3 convolution. Finally, a point-wise convolution is used to obtain the final segmentation map:
\begin{equation}
out = Conv_{1\times1}(Conv_{3\times3}(Up(Conv_{3\times3}(Up(x))))),
\label{eq2}
\end{equation}
where each $Conv_{3\times3}$ is a sequence of Conv, Batch Normalization, and ReLU. We keep the input dimension and output dimension consistent for each 3$\times$3 convolution. Table \ref{tab:2} shows that such a simple design outperforms the linear decoder by a large margin. When we reduce the channel number of the UperNet head to bridge the computational gap, the simple decoder performs slightly better. It suggests that up-sampling the last feature map is highly competitive compared to the UperNet decoder and high-resolution features are crucial to high-performance semantic segmentation. Our conclusion can not be drawn from some early works such as SETR where the PUP decoder does not reflect an obvious advantage over the Naive decoder (48.64 $vs.$ 48.18). Given that we utilize plain ViTs with MIM pre-training, it may be caused by different pre-trained weights, different training strategies, or even different details of decoder design.


\begin{table}[h]
\small
\begin{minipage}[c]{0.48\textwidth}
\caption{Ablation study on components of the UperNet decoder. `aux.' denotes the auxiliary supervision of deep features}
\begin{center}
\label{tab:1}
\begin{tabular}{lc}
\toprule
decoder &mIoU\\\midrule
Linear &49.6  \\
UperNet &51.5 \\
UperNet w/o aux. &51.6 \\
UperNet w/o aux. and PPM&51.9 \\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\hspace{.15in}
\begin{minipage}[c]{0.48\textwidth}
\caption{Comparison between the UperNet decoder and our simple up-sampling decoder. `Slim UperNet' denotes that we reduce the channel number from 768 to 320. The GFLOPs of decoders are reported}
\begin{center}
\label{tab:2}
\begin{tabular}{lcc}
\toprule
decoder &mIoU & GFLOPs \\\midrule
Improved UperNet &51.9 & 493 \\
Simple up-sampling&51.1 &110.6\\
Slim UperNet& 51.0 & 101.4\\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\end{table}


\subsection{PlainSeg}

\begin{figure}
  \centerline{\includegraphics[width=\textwidth]{plainseg.pdf}}
  \caption{The detailed architecture of PlainSeg.}
  \label{fig1}
\end{figure}

In this section, we continue with the idea of simple up-sampling to develop a high-performance `minimalist' system. As shown in Fig. \ref{fig1}, our approach consists of a pre-trained plain ViT backbone to extract features, a lightweight transformer decoder to classify masks, and a refiner to restore the feature resolution.

\textbf{Encoder.} A plain ViT mainly consists of three parts: a patch embedding layer, position embedding, and transformer layers. Among them, position embedding is necessary for the transformer to provide absolute or relative positional information. For instance, MAE \citep{he2022masked} uses the one-dimensional absolute position embedding while BEiT \citep{bao2022beit} adopts the two-dimensional relative position embedding. Two-dimensional position embedding usually performs better in downstream vision tasks so it is widely adopted in object detection and semantic segmentation tasks. Note that it also introduces inductive biases and this is why we emphasize fewer inductive biases rather than none.

\textbf{Transformer Decoder.} The motivation for incorporating the transformer decoder into our system comes from the transformer-based encoder-decoder architecture of NLP \citep{vaswani2017attention} as well as the more general concept of image segmentation \citep{cheng2021per,cheng2022masked}. Specifically, our `minimalist' system is built on the Mask2Former framework \citep{cheng2022masked}. We observe that the different configurations are employed by ViT-Adapter and SegViT with regard to the width of transformer decoder layers. These two works, to the best of our knowledge, are the only ones that combine plain ViTs with the mask classification paradigm. In detail, ViT-Adapter utilizes a slim transformer decoder with width 256 for ViT-B and a wide transformer decoder with width 1024 for ViT-L while SegViT sets the decoder width to half of the encoder width (384 for ViT-B and 512 for ViT-L). We note that the width of transformer decoder layers has a substantial impact on parameter efficiency as they are usually stacked many times. Therefore, other than them, we adopt a slim transformer decoder with width 256 for all the models. The original Mask2Former framework heavily relies on the hierarchical features of pyramid backbones so applying it to non-hierarchical vision transformers is not trivial. In the next section, we introduce our refiner which generates high-resolution features and bridges the gap between the transformer encoder and decoder.

\textbf{Refiner.} We firstly up-sample the final output of plain ViTs through bilinear interpolation and utilize a 3$\times$3 convolution to refine the feature map:
\begin{equation}
F_{refine} = Act(Norm(Conv_{3\times3}(Norm(Up(F_{vit}))))),
\label{eq3}
\end{equation}
where $F_{vit}$ denotes the last feature of plain ViT, $Up$ is the bilinear interpolation, $Norm$ is the normalization layer such as BN or LN, and $Act$ is the activation layer.
The resulting 1/8 resolution $F_{refine}$ will be used for cross-attention. To align the dimensions, a naive solution is compressing the channels of $F_{vit}$ by the 3$\times$3 convolution. However, it possibly leads to a loss of valid information. Another solution is generating multi-scale feature maps from $F_{vit}$ like ViTDet, and then compressing them to 256 channels. Nevertheless, this will increase architectural complexity and we maintain that lowering feature resolution is unnecessary since we only utilize the last feature map of abundant global semantics. To this end, we propose a simple width-to-depth technique, splitting $F_{refine}$ into several groups along the channel dimension and passing each grouped feature into cross-attention of sequential decoder layers. In detail, we perform a grouped 3$\times$3 convolution based on $F_{refine}$ and split the feature:
\begin{equation}
F_{cross-attn} = Split(Norm(Conv_{3\times3, group=n}(F_{refine}))),
\label{eq4}
\end{equation}
where $F_{cross-attn}$ is the N-group feature passed into cross-attention and each grouped feature has 256 channels. As shown in Fig. \ref{fig1}, the i-th grouped feature is associated with the i-th transformer decoder layer. This pattern is repeated once in a round robin fashion following Mask2Former. The feature for mask prediction is obtained based on $F_{refine}$ in a similar way:
\begin{equation}
F_{mask} = Conv_{1\times1}(Act(Norm(Conv_{3\times3}(Up(F_{refine}))))),
\label{eq5}
\end{equation}
where the output dimension of $Conv_{3\times3}$ is 256 and the last $Conv_{1\times1}$ is added following Mask2Former. We use Batch Normalization and ReLU activation following the widely used UperNet decoder. The group number of $F_{cross-attn}$ is set to 3 for the base model and 4 for the large model. Therefore, the number of transformer decoder layers is 6 for the base model and 8 for the large model.

\subsection{Improved Learning Rate Strategy}

Layer-wise learning rate decay (LLRD) has been widely adopted in fine-tuning MIM plain ViTs. It typically leads to smaller learning rates for shallow layers and larger learning rates for deep layers. In the context of semantic segmentation with plain ViTs, the decoder is usually considered as the `last layer'. However, it is unreasonable to treat the randomly initialized decoder and the pre-trained transformer layers equally despite the existence of layer-wise decay. Therefore, we introduce a scale factor $s$ greater than 1 for the randomly initialized parameters. For a base learning rate $l$ and decay factor $r$, the learning rates of the decoder and the \textit{i-th} layer from the bottom of the plain ViT are $l \times s$ and $l \times r^{i}$ ($i > 1$). We note that employing a larger learning rate for the randomly initialized parameters is a common configuration. However, we highlight these details because they significantly contribute to the effective optimization of slim transformer decoders. To our best knowledge, previous works usually opt to use a larger $l$ along with a smaller $r$ rather than combining a scale factor with LLRD.

\subsection{PlainSeg-Hier}
\label{PlainSeg-Hier}

There are two motivations behind presenting the PlainSeg-Hier. Firstly, spatial details from shallow layers are helpful for fine-grained semantic segmentation. Secondly, we aim to align with SegViT and ViT-Adapter which also utilize hierarchical features and develop a straightforward counterpart. Note that we still adhere to the `minimalist' pursuit and the practical principles in the development of PlainSeg, including creating high-resolution features by simple up-sampling techniques and using slim transformer decoders. Given the utilization of hierarchical features, we naturally associate it with multi-scale. Specifically, we employ deconvolutions for up-sampling and max pooling for down-sampling, with a reduction in feature map width by half when the size is doubled. To fuse multi-scale features (1/8, 1/16, 1/32) in a minimal yet efficient manner, we adopt a single deformable transformer encoder layer with multi-scale deformable attention \citep{zhu2021deformable}. Finally, we fuse the 1/4 features and the enhanced 1/8 features to obtain the 1/4 mask features and pass the enhanced multi-scale features (1/8, 1/16, 1/32) into the transformer decoder layers. The width of both the deformable transformer encoder and the transformer decoder is set to 256.

Although we have explored the potential of using the last feature map and demonstrated its effectiveness compared to UperNet, we do not claim that hierarchical features are dispensable, as suggested by \cite{li2022exploring}. There are various approaches to fuse hierarchical features and the original FPN \citep{lin2017feature} is relatively outdated. In PlainSeg-Hier, we adopt a single deformable transformer encoder layer, which is a simplification of the pixel decoder in Mask2Former. However, it is sufficient to achieve our goals. More advanced designs and techniques may offer further improvements and we leave these for future research endeavors. In essence, our PlainSeg and PlainSeg-Hier serve as powerful baselines for further exploring the necessity of hierarchical features.







\section{Experiments}


\subsection{Implementation Details}

Extensive experiments are conducted on \textbf{ADE20K} \citep{zhou2017scene}, \textbf{PASCAL Context} \citep{mottaghi2014role}, \textbf{COCO-Stuff 10K} \citep{caesar2018coco}, and \textbf{COCO-Stuff 164K} \citep{caesar2018coco}. We use the $\mathit{MMSegmentation}$ \citep{contributors2020mmsegmentation} toolbox for all the experiments. For the selection of pre-trained plain ViTs, we tame the BEiT \citep{bao2022beit}, BEiTv2 \citep{peng2022beit}, and EVA-02-L \citep{fang2023eva} models. We generally follow the training recipes of their original papers where the UperNet decoder is the default decoder. In the following, we only introduce the differences and more details are in the appendix. Specifically, we multiply by 10 the learning rate of randomly initialized decoder heads and apply gradient clipping following Mask2Former \citep{cheng2022masked}. In addition, we train the models with fewer iterations to facilitate research (80K, 20K, 20K, 80K iterations for ADE20K, PASCAL Context, COCO-Stuff 10K, COCO-Stuff 164K). The sliding window strategy is adopted for inference and evaluation following previous works. Only single-scale inference results are reported since one of our major considerations is practicality. We conduct all the experiments using eight RTX 3090.

\subsection{Comparisons with State-of-the-art Methods}

\begin{table}[h]
\small
\caption{Comparisons with ViT-Adapter \citep{chen2023vision} in terms of parameters and test time. We benchmark the test time by the public tool\protect\footnotemark[1] with a single RTX 3090 and batch size 1. $\ast$ denotes the result is reproduced by ourselves and $\dagger$ denotes the crop size during train and test is 896. In addition to the total parameters (PRM), we report the proportion of randomly initialized parameters to pre-trained parameters (R/P). Note that the test time may not be directly proportional to the test crop size due to the use of sliding window inference}
\begin{center}
\label{tab:4}
\begin{tabular}{l|ccccc}
\toprule
method & framework & backbone   &mIoU(SS) $\uparrow$  & PRM (R/P) $\downarrow$  &test time $\downarrow$\\\midrule
\multicolumn{6}{c}{ADE20K} \\ \midrule
ViT-Adapter$\ast$  &mask cls &BEiT-B  &\uline{54.65} &\uline{121M (41$\%$)} &\uline{194ms}  \\
PlainSeg &mask cls &BEiT-B  &\textbf{55.70} &105M (22$\%$)  & 138ms(\textbf{-29$\%$})\\
PlainSeg-Hier &mask cls &BEiT-B  &54.62 &106M (23$\%$)  & 143ms(\textbf{-26$\%$}) \\
ViT-Adapter  &mask cls &BEiT-L  &\uline{58.32} &\uline{568M (87$\%$)} &  \uline{535ms}\\
PlainSeg &mask cls &BEiT-L  &58.14 &333M (10$\%$) & 331ms(\textbf{-38$\%$}) \\
PlainSeg-Hier &mask cls &BEiT-L  &\textbf{58.17} &322M (6$\%$) & 308ms(\textbf{-42$\%$}) \\\midrule\midrule
\multicolumn{6}{c}{Pascal Context} \\ \midrule
ViT-Adapter  &mask cls  &BEiT-B &\uline{64.00} &\uline{120M (40$\%$)}  & \uline{240ms} \\
PlainSeg &mask cls &BEiT-B &63.74  &105M (22$\%$) &156ms(\textbf{-35$\%$})  \\
PlainSeg-Hier &mask cls &BEiT-B &\textbf{64.92}  &105M (22$\%$) &174ms(\textbf{-28$\%$})  \\
ViT-Adapter  &mask cls  &BEiT-L &\uline{67.79} &\uline{568M (87$\%$)} &\uline{578ms}  \\
PlainSeg &mask cls &BEiT-L &67.25  &332M (10$\%$)  &325ms(\textbf{-44$\%$}) \\
PlainSeg-Hier &mask cls &BEiT-L &\textbf{67.66}  &326M (8$\%$)  &317ms(\textbf{-45$\%$}) \\\midrule\midrule
\multicolumn{6}{c}{COCO-Stuff 10K} \\ \midrule
ViT-Adapter  &mask cls  &BEiT-B &\uline{50.00} &\uline{120M (40$\%$)}  & \uline{149ms}\\
PlainSeg &mask cls &BEiT-B &\textbf{51.09}  &105M (22$\%$) &107ms(\textbf{-28$\%$})  \\
PlainSeg-Hier &mask cls &BEiT-B &51.01  &105M (22$\%$) &107ms(\textbf{-28$\%$})  \\
ViT-Adapter  &mask cls  &BEiT-L &\uline{53.2} &\uline{568M (87$\%$)}   &\uline{342ms}\\
PlainSeg &mask cls &BEiT-L &\textbf{53.02}  &332M (10$\%$) &195ms(\textbf{-43$\%$})  \\
PlainSeg-Hier &mask cls &BEiT-L &52.99  &326M (8$\%$) &188ms(\textbf{-45$\%$})  \\\midrule\midrule
\multicolumn{6}{c}{COCO-Stuff 164K} \\ \midrule
ViT-Adapter$\dagger$  &mask cls  &BEiT-L &\uline{51.68} &\uline{571M (88$\%$)}  &\uline{1449ms} \\
PlainSeg &mask cls &BEiT-L &51.14  &333M (10$\%$) &358ms(\textbf{-75$\%$})   \\
PlainSeg-Hier &mask cls &BEiT-L &\textbf{51.75}  &327M (8$\%$) &336ms(\textbf{-77$\%$})   \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[h]
\small
\begin{minipage}[c]{0.56\textwidth}
\caption{Comparisons with SegViT \citep{zhang2022segvit,zhang2023segvitv2} in terms of parameters and test time. We benchmark the test time by the public tool\protect\footnotemark[1] with a single RTX 3090 and batch size 1. $\ast$ denotes the result is reproduced by our environment with identical training settings}
\begin{center}
\label{tab:5}
\begin{tabular}{l|cccc}
\toprule
method   & backbone   &mIoU  & PRM  &test time \\\midrule
\multicolumn{5}{c}{ADE20K} \\ \midrule
SegViT  &BEiTv2-B  &54.0 &109M &  69ms\\
PlainSeg-Hier  &BEiTv2-B  &\textbf{55.38} &106M  & 99ms\\
SegViT  &BEiTv2-L  &58.0 &345M & 287ms \\
PlainSeg-Hier  &BEiTv2-L  &\textbf{59.77} &322M & 308ms \\\midrule\midrule
\multicolumn{5}{c}{Pascal Context} \\ \midrule
SegViT  &BEiTv2-L &66.61 &344M &245ms  \\
PlainSeg-Hier  &BEiTv2-L &\textbf{69.60}  &326M  &317ms \\\midrule\midrule
\multicolumn{5}{c}{COCO-Stuff 10K} \\ \midrule
SegViT  &BEiTv2-L &52.00 &344M   &158ms\\
PlainSeg-Hier  &BEiTv2-L &\textbf{54.56}  &326M &188ms  \\\midrule\midrule
\multicolumn{5}{c}{COCO-Stuff 164K} \\ \midrule
SegViT$\ast$   &BEiT-B &48.53 &109M  &120ms \\
PlainSeg &BEiT-B &49.32  &105M &155ms   \\
PlainSeg-Hier &BEiT-B &\textbf{49.84}  &106M &155ms   \\
SegViT$\ast$   &BEiT-L &50.17 &345M  &305ms  \\
PlainSeg &BEiT-L &51.14  &333M &358ms   \\
PlainSeg-Hier &BEiT-L &\textbf{51.75}  &327M &336ms   \\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\hspace{.15in}
\begin{minipage}[c]{0.40\textwidth}
\caption{Performance with different pre-trained plain ViTs. `super.' denotes the supervised pre-trained backbones from ViT-AugReg \citep{steiner2021train}. We report both single-scale and multi-scale test results for comparison. All the methods employ ViT-L as backbones}
\begin{center}
\label{tab:6}
\begin{tabular}{lcc}
\toprule
backbone &method & mIoU \\\midrule
\multicolumn{3}{c}{ADE20K} \\ \midrule
super. &Segmenter &51.8/53.6  \\
super. &StructToken &52.8/54.2\\
super. &SegViT &54.6/55.2\\
super. &ViT-Adapter &56.8/57.7\\
super. &PlainSeg &55.0/56.0\\
super. &PlainSeg-Hier &54.7/56.4\\\midrule
EVA-02 &PlainSeg &61.7/62.0\\
EVA-02 &PlainSeg-Hier &61.7/62.0\\ \midrule\midrule
\multicolumn{3}{c}{Pascal Context} \\ \midrule
EVA-02 &PlainSeg &70.2/70.8\\
EVA-02 &PlainSeg-Hier &70.6/71.0\\ \midrule\midrule
\multicolumn{3}{c}{COCO-Stuff 164K} \\ \midrule
EVA-02 &PlainSeg &53.4\\
EVA-02 &PlainSeg-Hier &53.7\\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\end{table}

\footnotetext[1]{https://github.com/open-mmlab/mmsegmentation/blob/main/tools/analysis\_tools/benchmark.py}

In this section, we mainly compare our approaches with two state-of-the-art methods, ViT-Adapter-Mask2Former and SegViT. Table \ref{tab:4} shows that our methods achieve highly competitive performance compared to the state-of-the-art ViT-Adapter while significantly reducing the number of parameters and test time. More importantly, our results demonstrate that it is possible to build a `minimalist' system with comparable performance to complex systems. To better understand the parameter efficiency of our methods, we report the proportion of randomly initialized parameters to pre-trained parameters (R/P). Given that the same backbone is utilized, a lower R/P indicates that our methods have fewer randomly initialized parameters. Remarkably, the R/P of our large models is nearly 10 times lower than that of ViT-L-Adapter-Mask2Former. We note that this reduction in R/P can have a positive impact on label-limited subtasks of semantic segmentation, such as continual semantic segmentation where each new scenario only contains labels for a subset of categories. We further compare PlainSeg-Hier with SegViT which is the previous state-of-the-art method in terms of trade-off between performance and efficiency. As presented in Table \ref{tab:5}, our method significantly outperforms SegViT in terms of performance while maintaining decent inference efficiency. Fig. \ref{fig2} offers a broader perspective by including more models. It can be seen that our methods achieve a new state-of-the-art trade-off, superior to SegViT across different backbones and datasets. Table \ref{tab:6} illustrates the effect of different pre-training strategies. Using plain ViTs with supervised pre-training, our method outperforms most competitive methods but is inferior to ViT-Adapter. It underscores the effectiveness of our methods and indicates that MIM pre-training narrows and even bridges the gap between our methods and ViT-Adapter-Mask2Former. Furthermore, consistent performance gains on multiple datasets are achieved with EVA-02-L, the current leading open-source plain ViT model utilizing multi-model MIM pre-training on extensive data. It demonstrates the potential of our methods in assessing the transfer ability of potent pre-trained plain ViTs.




\begin{figure}
  \centerline{\includegraphics[width=\textwidth]{test_time.pdf}}
  \caption{Trade-off between accuracy and test time on ADE20K (left) and COCO-Stuff 164K (right). We exclude methods with significantly lower accuracy. We report the accuracy and test time of sliding window inference for Swin backbones as a reference.}
  \label{fig2}
\end{figure}


\begin{table}
\small
\begin{minipage}[c]{0.48\textwidth}
\caption{Ablation study on the refiner. We replace all the 3$\times$3 convolutions with 1$\times$1 convolutions (`w/o 3$\times$3 convolution') or remove all the up-sampling operations (`w/o high resolution') or remove 3$\times$3 convolutions and up-sampling at the same time (`w/o both'). The GFLOPs of decoders are reported}
\begin{center}
\label{tab:7}
\begin{tabular}{lcc}
\toprule
refiner &mIoU &GFLOPs\\\midrule
default &48.7 &66.0  \\
w/o width-to-depth &48.4 & 27.3\\
w/o 3$\times$3 convolution &48.3 &14.5\\
w/o high resolution &47.8 & 11.9\\
w/o both &47.2 & 3.9 \\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\hspace{.15in}
\begin{minipage}[c]{0.48\textwidth}
\caption{Ablation study on the learning rate and the width of transformer decoder. `default' denotes we do not apply 10$\times$ learning rate to the decoder. `gradclip' is as same as the gradient clipping in Mask2Former. `wide decoder' means that the width of transformer decoder is 768 rather than 256. N/A: fail to converge }
\begin{center}
\label{tab:8}
\begin{tabular}{lc}
\toprule
Method &mIoU  \\\midrule
default & 47.5 \\
4$\times$ lr, gradclip &48.3  \\
10$\times$ lr, gradclip &48.7 \\
wide decoder& 48.5 \\
wide decoder, 4$\times$ lr, gradclip &48.7 \\
wide decoder, 10$\times$ lr, gradclip &N/A \\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\end{table}


\subsection{Ablation Study}


The ablation study is conducted on COCO-Stuff 164K using BEiT-B backbone. We also train all the models with 80K iterations and a crop size of 512$\times$512. Although PlainSeg is simple in both concept and implementation, we demonstrate that several key architectural designs and training settings contribute to its high performance. In Table \ref{tab:5}, we observe a 0.3$\%$ decrease in mIoU when directly compressing the output feature of a plain ViT. Either reducing feature resolution or removing 3$\times$3 convolutions has a negative impact on performance and high-resolution features are more crucial. Our approach obtains high-resolution features by directly up-sampling low-resolution features without fusing shallow high-resolution features, showcasing a different paradigm from previous high-performance methods. Table \ref{tab:6} reveals that using a larger learning rate significantly improves the performance of a slim transformer decoder, making it comparable to a wide transformer decoder. However, we do not observe an obvious improvement when applying it to a wide transformer decoder. It can be seen from Fig. \ref{fig3} that the 3$\times$3 convolution refines the feature maps effectively and the `width-to-depth' generates more abundant refined feature maps for the transformer decoder. For different groups, the network learns to have a specific focus on different regions.




\begin{figure}
\centering
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{ADE_val_00000129.jpg}
\end{minipage}
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{feature_map_before3x3_640_s8_129.png}
\end{minipage}
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{feature_map_after3x3_640_s8_129.png}
\end{minipage}
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{feature_map2_group1_129.png}
\end{minipage}
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{feature_map2_group2_129.png}
\end{minipage}
\begin{minipage}{0.85in}
\includegraphics[width=0.85in, height=0.85in]{feature_map2_group3_129.png}
\end{minipage}
\caption{From left to right, they are the input image, visualized feature maps before and after a 3$\times$3 convolution, and visualized feature maps of different groups in Refiner. Different grouped features highlight distinct regions marked with red boxes.}
\label{fig3}
\end{figure}


\section{Conclusion}

In this work, we develop high-performance `minimalist' systems for semantic segmentation with plain ViTs and provide simple and efficient baselines for the field. In the meanwhile, we identify the underlying principles that contribute to the success of our methods. The `minimalist' systems have many potential benefits. They are more friendly to deployment and acceleration with dedicated devices, facilitating ultimate engineering optimization. They will also help carry out complex multi-model tasks through a single model. We hope our methodology encourages more research toward practical semantic segmentation with plain ViTs. One limitation is that our methods may still be computationally expensive for very high-resolution semantic segmentation tasks. We leave it for future work.

\normalem
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\section{Appendix}

\iffalse
\subsection{Additional Ablation Study}


To offer more insights into why PlainSeg-Hier with less complexity achieves similar performance to ViT-Adapter-Mask2Former, we continue our ablation study to investigate the effectiveness of ViT-Adapter and a deeper deformable transformer encoder. Note that we employ identical training protocols in our ablation study and all the results are obtained by our environment. The only difference between the baseline in Table \ref{tab:13} and PlainSeg-Hier is that the feature width is not reduced during up-sampling. This modification slightly improves the performance and ensures that the width of multi-scale features is identical to that of ViT-Adapter. As can be seen from Table \ref{tab:13}, incorporating ViT-Adapter offers little performance improvements and leveraging a much deeper deformable transformer encoder results in marginal performance gains. In addition, employing both of them does not bring more improvements in performance.

\begin{table}[h]
\small
\caption{Ablation study on the effectiveness of ViT-Adapter and a deeper deformable transformer encoder}
\begin{center}
\label{tab:13}
\begin{tabular}{lc}
\toprule
method  &mIoU \\\midrule
baseline  &49.3\\
+ ViT-Adapter  &49.4(+0.1) \\
+ 6 encoders  &49.7(+0.4) \\
+ both of them  &49.6(+0.3) \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\fi
\subsection{Implementation Details}

We provide detailed training hyper-parameters in Table \ref{tab:9}. We do not search for optimal hyper-parameters on each dataset; instead, we generally follow precedent training recipes and pursue the unified hyper-parameters of different datasets.
\begin{table}[h]
\caption{Training hyper-parameters on four semantic segmentation datasets. Since crop size and total iterations are specific to each dataset, we list every value in the order of ADE20K, PASCAL Context, COCO-Stuff 164K, and COCO-Stuff 10K. Models with EVA-02-L are only trained on the first three datasets}
\begin{center}
\label{tab:9}
\begin{tabular}{l|ccc|c}
\toprule
hyper-parameters & BEiT-B & BEiT-L & BEiTv2-L  &EVA-02-L \\\midrule

crop size &\multicolumn{3}{c|}{640,480,640,512} &640,480,640 \\
learning rate &3e-5 &2e-5  &3e-5 &2e-5 \\
layer-wise lr decay &0.90 &0.95 &0.90 &0.90 \\
batch size &16 &16 &16 &16\\
total iterations&\multicolumn{3}{c|}{80K,20K,80K,20K} & 80K,20K,120K\\
warm up iterations&1500 &1500 &1500 &1500 \\
optimizer& AdamW & AdamW & AdamW & AdamW\\
drop path rate &0.1 &0.3 &0.3 &0.3 \\
weight decay &0.05 &0.05 &0.05 &0.05 \\
grad clip &0.01 &0.01 &0.01 &0.01 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{More Details of PlainSeg-Hier}

As discussed in Section \ref{PlainSeg-Hier}, the method to generate multi-scale features is similar to the feature pyramid in BEiT-UperNet \citep{bao2022beit} except that we reduce the output width of deconvolutions for higher parameter and computational efficiency. For the features to generate masks, we up-sample the 1/8 output of the deformable transformer encoder by bilinear interpolation and add it to 1/4 features and then use a 3$\times$3 convolution for refinement. In addition, features of three scales are fed into 9 transformer decoder layers in a round robin fashion except for the ViT-L models on ADE20K. We employ 6 transformer decoder layers for them as slight performance drops are observed with more transformer decoder layers.


\subsection{Additional Experimental Results}

\textbf{Detailed Results of Mask2Former (Swin).} Table \ref{tab:10} shows the detailed results of Mask2Former (Swin) on ADE20K and COCO-Stuff 164K.

\begin{table}[h]
\caption{Detailed results of Mask2Former \citep{cheng2022masked} on ADE20K and COCO-Stuff 164K. $\ast$ denotes the result is reproduced by our environment with identical training settings. For ADE20K, we use the models reproduced by $\mathit{MMSegmentation}$. Results in parentheses are obtained with the default whole image inference}
\begin{center}
\label{tab:10}
\begin{tabular}{l|cccc}
\toprule
method  & backbone   &mIoU(SS)  & PRM  &test time\\\midrule
\multicolumn{5}{c}{ADE20K} \\ \midrule
Mask2Former &Swin-B &54.0(53.9) & 107M & 143ms(94ms)\\
Mask2Former &Swin-L &55.8(56.1) & 215M & 175ms(115ms)\\\midrule
\multicolumn{5}{c}{COCO-Stuff 164K} \\ \midrule
Mask2Former$\ast$ &Swin-B &48.6 & 107M & 161ms\\
Mask2Former$\ast$ &Swin-L &49.5 & 216M & 198ms\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Performance Comparison on Cityscapes.} We conduct experiments on Cityscapes whose lots of thin and small objects pose more challenges to semantic segmentation with plain ViTs. As shown in Table \ref{tab:11}, our methods outperform the competitive CNN-based methods, which demonstrates the potential of `minimalist' systems in challenging traffic scenes. However, they still lag behind advanced segmentation models based on hierarchical ViTs due to the loss of spatial details.

\begin{table}[h]
\caption{Performance comparison on Cityscapes val set. Results in the second line are obtained from $\mathit{MMSegmentation}$ model zoo, which are higher than those of original papers in most cases}
\begin{center}
\label{tab:11}
\begin{tabular}{l|cc}
\toprule
Model                                     & mIoU(SS)                    & Params.     \\ \midrule
\multicolumn{3}{c}{hierarchical backbones} \\ \midrule
FCN\cite{long2015fully}                   & 75.5                       & 69M       \\
EncNet\cite{zhang2018context}             & 78.6                        & 55M       \\
PSPNet\cite{zhao2017pyramid}              & 79.8                       & 68M       \\
HRNet\cite{wang2020deep}                  & 80.7                       & 66M       \\
DANet\cite{fu2019dual}                    & 80.5                     & 69M       \\
DeepLabV3+\cite{chen2018encoder}          & 81.0                      & 63M       \\
OCRNet\cite{yuan2020object}               & 81.4                     & 70M       \\ \midrule
HRFormer-B + OCR\cite{yuan2021hrformer}   & 81.9                       & 56M           \\
SegFormer-B5\cite{xie2021segformer}       & 82.4                      & 85M       \\
Mask2Former-Swin-B\cite{cheng2022masked}  & 83.3                      & 107M    \\
Mask2Former-Swin-L\cite{cheng2022masked}  & 83.3                      & 215M\\ \midrule
\multicolumn{3}{c}{plain backbones} \\ \midrule
PlainSeg-B                                & 81.7  & 106M \\
PlainSeg-Hier-B                                & \textbf{82.3}  & 106M \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Comparison with SegViT and UperNet on ADE20K.} We provide detailed results of our methods, SegViT, and UperNet on ADE20K using the same backbone and crop size. As shown in Table \ref{tab:12}, SegViT only achieves marginal performance improvements upon UperNet while our methods significantly outperform both of them in accuracy.

\begin{table}[h]
\caption{Comparison with SegViT \cite{zhang2022segvit} and UperNet on ADE20K. $\ast$ denotes the result is reproduced by our environment with identical training settings}
\begin{center}
\label{tab:12}
\begin{tabular}{l|ccccc}
\toprule
method & crop size & backbone   &mIoU(SS)  & PRM &test time \\\midrule
UperNet & 640  &BEiT-B &53.6 &163M &206ms \\
SegViT$\ast$ & 640  &BEiT-B &53.8 &109M &113ms \\
PlainSeg & 640 &BEiT-B & 55.7 &105M &138ms \\
PlainSeg-Hier & 640 &BEiT-B & 54.6 &106M &143ms \\\midrule
UperNet & 512  &BEiTv2-B &53.5 &163M &122ms \\
SegViT & 512  &BEiTv2-B &54.0 &109M &69ms \\
PlainSeg-Hier & 512 &BEiTv2-B & 55.4 &106M &99ms \\\midrule
UperNet & 512  &BEiTv2-L &57.5 &440M &262ms \\
SegViT & 512   &BEiTv2-L &56.5 &344M &148ms \\
PlainSeg-Hier & 512 &BEiTv2-L & 59.4 &326M &177ms \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Trade-off between Accuracy and Parameters on ADE20K and COCO-Stuff 164K.} As shown in Fig. \ref{fig4}, our methods are superior to SegViT and UperNet.


\begin{figure}
  \centerline{\includegraphics[width=\textwidth]{para.pdf}}
  \caption{Trade-off between accuracy and parameters on ADE20K (left) and COCO-Stuff 164K (right). We exclude methods with significantly lower accuracy. We report the accuracy of sliding window inference for Swin backbones as a reference.}
  \label{fig4}
\end{figure}


\subsection{Visualization Results}

\textbf{Visualized Feature Maps.} Fig. \ref{fig5} and Fig. \ref{fig6} show more visualized feature maps about the Refiner.

\begin{figure}
\centering
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{ADE_val_00000129.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group1_129.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group2_129.png}
\end{minipage}
\vspace{0.2em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group3_129.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{ADE_val_00000267.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group1_267.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group2_267.png}
\end{minipage}
\vspace{0.2em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group3_267.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{ADE_val_00001378.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group1_1378.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group2_1378.png}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.3in]{feature_map2_group3_1378.png}
\end{minipage}
\caption{Visualized feature maps of different groups in Refiner. Different grouped features highlight distinct regions marked with red boxes.}
\label{fig5}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{ADE_val_00000129.jpg}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_before3x3_640_s8_129.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_after3x3_640_s8_129.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{ADE_val_00000267.jpg}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_before3x3_640_s8_267.png}
\end{minipage}
\vspace{0.2em}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_after3x3_640_s8_267.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{ADE_val_00000370.jpg}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_before3x3_640_s8_370.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_after3x3_640_s8_370.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{ADE_val_00000781.jpg}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_before3x3_640_s8_781.png}
\end{minipage}
\begin{minipage}{0.8in}
\includegraphics[width=0.8in, height=0.8in]{feature_map_after3x3_640_s8_781.png}
\end{minipage}
\caption{Visualized feature maps before and after a 3$\times$3 convolution.}
\label{fig6}
\end{figure}


\textbf{Visualized Segmentation Maps.} Fig. \ref{fig7} and Fig. \ref{fig8} show the competitive visualization results on ADE20K, PASCAL Context, and Cityscapes. Our methods are competent for segmenting small objects and have decent boundary details.

\begin{figure}
\centering
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000033_plain.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000049_plain.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000267_plain.jpg}
\end{minipage}
\vspace{0.3em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000691_plain.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_001843.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_000452.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_002349.jpg}
\end{minipage}
\vspace{0.3em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2009_000719.jpg}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000000_000576_CITY.png}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000000_009969_CITY.png}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000001_002646_CITY.png}
\end{minipage}
\caption{Visualized segmentation maps of PlainSeg on ADE20K (top), PASCAL Context (middle), and Cityscapes (bottom).}
\label{fig7}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000033_hier.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000049_hier.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000267_hier.jpg}
\end{minipage}
\vspace{0.3em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{ADE_val_00000691_hier.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_001843_hier.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_000452_hier.jpg}
\end{minipage}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2008_002349_hier.jpg}
\end{minipage}
\vspace{0.3em}
\begin{minipage}{1.3in}
\includegraphics[width=1.3in, height=1.0in]{PASCAL_2009_000719_hier.jpg}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000000_000576_hier.png}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000000_009969_hier.png}
\end{minipage}
\begin{minipage}{1.75in}
\includegraphics[width=1.75in]{frankfurt_000001_002646_hier.png}
\end{minipage}
\caption{Visualized segmentation maps of PlainSeg-Hier on ADE20K (top), PASCAL Context (middle), and Cityscapes (bottom).}
\label{fig8}
\end{figure}

\end{document}
