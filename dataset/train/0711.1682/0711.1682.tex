\newcommand{\Xcomment}[1]{}

\Xcomment{
\documentclass[twoside,leqno,twocolumn]{article}
\usepackage{ltexpprt}
}

\documentclass[12pt]{article}
\topmargin-.75in \oddsidemargin.0in \textwidth6.5in \textheight9in
\usepackage{setspace}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newcommand{\proof}{\noindent \emph{Proof.}\ }
\newcommand{\proofend}{\\}



\Xcomment{
JOURNAL VERSION:
\begin{itemize}
\item Insert appendices;
\item Insert alternative analysis in Section 3.2 (time not cost);
\item Can the proof of Lemma 2 be restated as a potential argument?
\item Leaf deletes?
\item Definition of ?
\end{itemize}
}




\usepackage{epsfig}
\usepackage{latexsym}
\addtolength{\leftmargini}{-1.5mm}
\addtolength{\leftmarginii}{-1.5mm}
\addtolength{\leftmarginiii}{-1.5mm}
\addtolength{\itemsep}{-1.5mm}





\newcommand{\parent}{\mathit{parent}}
\newcommand{\assign}{\leftarrow}
\newcommand{\rank}{\mathit{rank}}
\newcommand{\myqed}{\hfill }
\newcommand{\opinsert}{\mathit{insert}}
\newcommand{\deletemin}{\mathit{deletemin}}
\newcommand{\binlog}{\mathrm{lg}\,}
\newcommand{\size}{\mathit{size}}
\newcommand{\merge}{\mathit{merge}}
\newcommand{\pred}{\mathit{pred}}
\newcommand{\nca}{\mathit{nca}}
\newcommand{\topn}{\mathit{top}}
\newcommand{\nul}{\mathit{null}}
\newcommand{\labeln}{\mathit{\ell}}
\newcommand{\rootn}{\mathit{root}}
\newcommand{\bottom}{\mathit{bottom}}
\newcommand{\topp}{\mathit{top}}
\newcommand{\treemin}{\mathit{treemin}}
\newcommand{\pathmin}{\mathit{pathmin}}


\begin{document}





\title{\Large Data Structures for Mergeable Trees\footnote{A preliminary version of some of this material appeared in the conference paper ``Design of data structures for mergeable trees'', \emph{Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms}, pages 394--403, 2006.}}
\author{Loukas Georgiadis \and Haim Kaplan \and Nira Shafrir \and Robert E. Tarjan \and Renato F. Werneck}
\date{\today}

\maketitle




\begin{abstract} \small\baselineskip=9pt

Motivated by an application in computational topology, we consider a novel variant of the problem of efficiently maintaining dynamic rooted trees.  This variant requires merging two paths in a single operation.  In contrast to the standard problem, in which only one tree arc changes at a time, a single merge operation can change many arcs.  In spite of this, we develop a data structure that supports merges on an -node forest in  amortized time and all other standard tree operations in  time (amortized,  worst-case, or randomized depending on the underlying data structure).  For the special case that occurs in the motivating application, in which arbitrary arc deletions (\emph{cuts}) are not allowed, we give a data structure with an  time bound per operation.  This is asymptotically optimal under certain assumptions.  For the even-more special case in which both cuts and parent queries are disallowed, we give an alternative -time solution that uses standard dynamic trees as a black box. This solution also applies to the motivating application.  Our methods use previous work on dynamic trees in various ways, but the analysis of each algorithm requires novel ideas.  We also investigate lower bounds for the problem under various assumptions.
\end{abstract}


\footnotetext[1]{Hewlett-Packard Laboratories, Palo Alto, CA, 94304.
Part of this work was done while this author was at Princeton University. E-mail: {\tt loukas.georgiadis@hp.com}.}
\footnotetext[2]{Tel-Aviv University, Tel-Aviv, Israel. E-mail: {\tt haimk@math.tau.ac.il} and {\tt shafrirn@post.tau.ac.il}.}
\footnotetext[3]{Department of Computer Science, Princeton University, 35 Olden Street, Princeton, NJ 08540 and Hewlett-Packard
Laboratories, Palo Alto, CA, 94304. E-mail: {\tt ret@cs.princeton.edu}.}
\footnotetext[4]{Microsoft Research Silicon Valley, 1065 La Avenida, Mountain View, CA, 94043. Part of this work
was done while this author was at Princeton University. E-mail: {\tt renatow@microsoft.com}.

\vspace{.1cm}
\noindent Research by Loukas Georgiadis, Robert E. Tarjan, and Renato F. Werneck at Princeton University was partially supported by the Aladdin Project, NSF Grant No 112-0188-1234-12. Work by Haim Kaplan and Nira Shafrir was partially supported by Grant 975/06 from the Israel Science Foundation (ISF).}


\thispagestyle{empty}

\setcounter{page}{1}
\section{Introduction}
\label{sec:intro}

A \emph{heap-ordered forest} is a set of node-disjoint rooted trees, in which each node  has a real-valued \emph{label} , and the labels are in heap order: if  is the parent of , .  We consider the problem of maintaining a heap-ordered forest, initially empty, subject to an arbitrary intermixed sequence of the following kinds of operations:

\begin{itemize}
\vspace{-.75mm}
\addtolength{\itemsep}{-1.75mm}
\item {\em parent}: Return the parent  of , or null if  is a tree root.
\item {\em root}: Return the root of the tree containing .
\item {\em nca}: Return the nearest common ancestor of  and , or null if  and  are in different trees.
\item {\em insert}: Create a new, one-node tree consisting of node  with label ;  must be in no other tree.
\item {\em link}: Make  the parent of  by adding the arc ;  must be a root,  must be in another tree,
and .
\item {\em cut}: Delete the arc from  to its parent, making  a root; do nothing if  is already a root.
\item {\em delete}: Delete  from the forest;  must be a leaf (a node with no children).
\item {\em merge}: Let  and , respectively, be the paths from  and  to the roots of their respective trees.  Restructure the tree or trees containing  and  by merging the paths  and  while preserving heap order. See Figure~\ref{fig:examples}.
\end{itemize}

\begin{figure}[h]
\addtolength{\abovecaptionskip}{-.5cm}
\begin{center}
\resizebox{1\textwidth}{!} {\includegraphics{examples.eps}}
\end{center}
\caption{\label{fig:examples} Two successive merges. The nodes are
identified by label.}
\end{figure}

This is the \emph{mergeable trees problem}.  This problem arises in an algorithm of Agarwal et al.~\cite{AEHW04,AEHW06} that computes the structure of 2-manifolds embedded in .  In this application, the tree nodes are the critical points of the manifold (local minima, local maxima, and saddle points), with labels equal to their heights.  The algorithm computes the critical points and their heights during a sweep of the manifold, and pairs up the critical points into so-called \emph{critical pairs} using mergeable tree operations.  This use of mergeable trees is actually a special case: there are no cuts.  As we shall see, one can also avoid parent operations, by changing the pairing process to do two sweeps, one upward and one downward, instead of a single sweep.

The mergeable trees problem is a new variant of the well-studied \emph{dynamic trees} problem, which calls for the maintenance of a forest of trees subject to all the mergeable tree operations except merge.  Nodes are not heap-ordered by label; instead, each node or arc has an arbitrary associated value, and values can be accessed or changed one node or arc at a time, an entire path at a time, or even an entire tree at a time.  The original use of dynamic trees was in a network flow algorithm~\cite{ST83}.  In that application, each arc has an associated real value, its residual capacity.  The maximum value on a path can be computed in a single operation, and a given value can be subtracted from all arcs on a path in a single operation.

There are several versions of the dynamic trees problem that differ in what kinds of values are allowed, whether values can be combined over paths or over entire trees (or both) at a time, whether the trees are unrooted, rooted, or ordered (each set of siblings is ordered), and exactly what operations are allowed.  For all these versions of the problem, there are algorithms that perform a sequence of tree operations in logarithmic time per operation,  either amortized~\cite{ST85,TW05}, worst-case~\cite{AHTdL05,Fre85,ST83}, or randomized~\cite{ABHVW04}. The nca operation is not completely standard for dynamic trees; it accesses two paths rather than one.  But it is easy to extend any of the efficient implementations of dynamic trees to support nca in  time.  Indeed, Sleator and Tarjan~\cite{ST83} and Alstrup et al.~\cite{AHTdL05} explicitly describe how to do this.


The main novelty, and the main difficulty, in the mergeable trees problem is the merge operation.  Although dynamic trees support global operations on node and arc values, the underlying trees change only one arc at a time, by links and cuts (and deletions, which are in effect cuts).  In contrast, a merge operation can delete and add many arcs, even a linear number, simultaneously.  Nevertheless, there are efficient implementations of mergeable trees.  We give three.  In Section \ref{sec:dyntrees} we show that the amortized number of arcs changed by a merge is logarithmic.  This allows us to implement mergeable trees using dynamic trees directly, with a logarithmic (or better) time bound for every operation except merge, and a log-squared amortized bound for merge.  In Section \ref{sec:part-rank} we consider the special case in which there are no cuts.  For this case we combine ideas in a previous implementation of dynamic trees with a novel analysis, to obtain an algorithm with a logarithmic or better time bound for every operation.  In Section \ref{sec:weak} we consider the special case in which there are neither cuts nor parent operations.  For this case we give an alternative logarithmic-time solution that represents mergeable trees implicitly as dynamic trees: a merge becomes either a link, or a cut followed by a link.  Either of the methods of Sections \ref{sec:part-rank} and \ref{sec:weak} can be used to efficiently pair critical points.  We discuss this application in Section \ref{sec:pairing}, including filling in a gap in the pairing algorithm of Agarwal et al.~\cite{AEHW04,AEHW06}.  In Section \ref{sec:complexity} we discuss lower bounds and related issues for various forms of the mergeable trees problem.

In discussing the mergeable trees problem we shall use the following terminology.  Each tree arc is directed from child to parent, so that a path leads from a node toward the root of its tree, a direction we call \emph{upward}.  Node  is a \emph{descendant} of , and  is an \emph{ancestor} of , if the path from  to  contains .  (This includes the case  = .)  We also say  is \emph{below} , and  is \emph{above} .  If  is neither an ancestor nor a descendant of , then  and  are \emph{unrelated}. We denote by  the number of descendants of , including .  We denote by  the path from node  to node , and by  and , respectively, the subpath of  obtained by deleting  or deleting ; if ,  and  are empty.  By extension, .  We denote by  and  the first (bottommost) and last (topmost) nodes on a path , and by  the number of nodes on .

We replace each operation  by , since they have the same effect.  This avoids the need to consider link explicitly as a mergeable tree operation.  We denote by  the number of merges, including those replacing links. We denote by  the number of inserts of nodes that are eventually in trees that participate in merges; any node that does not participate remains in a single-node tree, on which all operations take constant time. The definition of  implies that . All our data structures take space linear in the number of insertions.

In a merge, the merge order is unique if all nodes have distinct labels.  If not, we can break ties using node identifiers.  To simplify things, and without loss of generality, we shall assume that the node labels are the nodes themselves, and that the nodes that are in trees that eventually participate in merges are the integers  through , numbered in label order.  We treat a node that is deleted and reinserted as an entirely new node when it is reinserted, with a new number. We also treat null as being less than any node.


This paper is a major reworking of a conference paper~\cite{GTW06}.  We have simplified the analysis of the algorithms in Sections \ref{sec:dyntrees} and \ref{sec:part-rank}, added a detailed description of the critical point pairing application (Section \ref{sec:pairing}), and added the algorithm in Section \ref{sec:weak}, which is the contribution of the two new authors (Kaplan and Shafrir).



\section{Mergeable Trees as Dynamic Trees}
\label{sec:dyntrees}

In this section we explore the obvious way to implement mergeable trees, which is to represent them by dynamic trees of exactly the same structure.  Then the mergeable tree operations parent, root, nca, insert, cut, and delete become exactly the same operations on dynamic trees.  In order to do merges, we need one additional operation on heap-ordered dynamic trees:

\begin{itemize}
\item : Return the smallest (topmost) ancestor of  that is strictly greater than , assuming .
\end{itemize}

This operation accesses the path .  It is easy to extend any of the efficient implementations of dynamic trees to support topmost in  time.

To perform , begin by computing .  Stop if  or .  Otherwise, walk down the paths  and  toward  and , merging them step-by-step.  Maintain two current nodes  and , initially  and , respectively.  If , swap  and  and  and , respectively.  If , do .  While , repeat the following step:

\begin{description}
\item[Merge Step:] Let .  Do  and . Set  equal to , set  equal to , and swap  and . (See Figure \ref{fig:topmost}.)
\end{description}

To finish the merge, do .

\begin{figure}
\addtolength{\abovecaptionskip}{-.5cm}
\begin{center}
\resizebox{0.85\textwidth}{!} {\includegraphics{figure2.eps}}\\
\end{center}
\caption{\label{fig:topmost} A merge step. Straight lines are arcs, wavy lines are tree paths. Primed variables are values after the step.}
\end{figure}


After the merge initialization,  and  are the children of  that are ancestors of  and , respectively, or  and  if  is null.  At the beginning of a merge step,   and  are the tops of the paths remaining to be merged,  is a root, and .  The merge step finds the node  whose parent is the parent of  after the merge and updates values appropriately.  The correctness of the merging algorithm follows.

Each merge step takes a constant number of dynamic tree operations, as do the initialization and finalization.  There is one parent change per merge step plus one per merge.  We shall obtain an  bound on the amortized number of parent changes per merge, which implies an  amortized time bound for merging, assuming that the underlying dynamic tree data structure has an  time bound per operation.

\begin{lemma}
\label{lemma:dyn-trees}
The total number of parent changes over all merges is .
\end{lemma}
\proof We use an amortized analysis~\cite{Tar85}.  Each state of the data structure has a non-negative potential; the initial, empty structure has a potential of zero.  We define the cost of an operation to be the number of parent changes it causes; we define the amortized cost of an operation to be its cost plus the net decrease in potential it causes.  Then the sum of the amortized costs of all the operations is an upper bound on the total number of parent changes caused by all the operations.

With each arc  we associate  units of potential, where  is the base-two logarithm. Of this amount, we assign  to ,  to , and  to .  Thus each node has potential associated with its parent (if it has a parent) and with each of its children.  We call the former its \emph{parent potential} and the sum of the latter its \emph{child potential}.  The total potential is the sum of the potentials.

The only operations that affect the structure of the forest are merges, cuts, and node deletions.  A cut or node deletion creates at most one new (null) parent and decreases the potential by at least one, so its amortized cost is non-positive. Consider a merge. If , the initial link of  increases the potential by at most . Every other change in a node potential is non-positive. Consider a merge step. Let  be the parent of  before the cut of  occurs, and let  be the new parent of , which it acquires either in the next merge step, or at the end of the merge if this is the last step.  Then .  If , then the parent potential of  decreases by at least one as a result of its parent changing.  If , then the child potential of  decreases by at least one as a result of  losing  as a child but gaining .  One of these two cases must occur. The amortized cost of a merge is thus  (for the initial link of  if ) plus a net of at most zero per merge step (one parent creation minus at least one unit of potential) plus one (for one extra parent creation per merge).
\proofend

If we use any of the several implementations of dynamic trees that support all operations in  time, Lemma \ref{lemma:dyn-trees} gives an  amortized time bound for merge; all the other operations have the same time bound as in the underlying dynamic tree structure.  With this method one can maintain parent pointers explicitly, which makes the worst-case time for the parent operation .

The proof of Lemma \ref{lemma:dyn-trees} gives something a little stronger: the amortized cost of a merge is  unless the merge combines two trees.  If there are no cuts, then the number of merges that can combine two trees is at most ,
which means that the total number of parent changes is . This bound is tight, as we show in Section \ref{sec:complexity}.  The total time for merges becomes .  In the absence of cuts, we can get an even better bound on the merge time by changing the algorithm, as we shall see in the next two sections.

In a preliminary version of their paper~\cite{AEHW04}, Agarwal et al. proposed representing mergeable trees by dynamic trees as we do here, but they suggested a different merging algorithm, in which the nodes of the shorter merge path are inserted one-by-one into the longer merge path.  Although they claimed an  bound on the total number of node insertions (assuming no cuts), this bound is incorrect: the worst-case number of node insertions in the absence of cuts is , as we show in Section \ref{sec:complexity}.  Thus this method of merging does not give even a polylogarithmic amortized bound for merge.




\section{Mergeable Trees via Partition by Rank}
\label{sec:part-rank}


In this and the next section we develop two different methods to achieve an  bound per merge, if there are no cuts.  For the moment we also ignore leaf deletions; we discuss how to handle them at the end of the section.  Our first method uses an idea from Sleator and Tarjan's~\cite{ST83, ST85} implementation of dynamic trees: we partition each tree into node-disjoint paths, and implement the various tree operations as appropriate sequences of path operations.  The updates we need on paths are deletions of top nodes and arbitrary insertions of single nodes.  We also need a variant of the topmost query defined in Section \ref{sec:dyntrees}.

We define the \emph{rank} of a node  to be .  Ranks are integers in the range from zero to .  We
decompose the forest into solid paths by defining an arc  to be \emph{solid} if  and \emph{dashed} otherwise. Since a node can have at most one solid arc from a child, the solid arcs partition the forest into node-disjoint solid paths. See Figure \ref{fig:solid}.  Our path partition is a variant of one used by Sleator and Tarjan~\cite{ST83}: theirs makes an arc  solid if ;  our solid arcs are a subset of theirs. We call a node a \emph{top node} if it is the top of its solid path. We call a non-root node a \emph{solid child} if its arc to its parent is solid and a \emph{dashed child} otherwise.



\begin{figure} [htb]
\addtolength{\abovecaptionskip}{-.5cm}
\begin{center}
\resizebox{0.5\textwidth}{!} {\includegraphics{solid.eps}}
\end{center}
\caption{\label{fig:solid} A tree partitioned by rank into solid
paths, with the corresponding sizes and, in parentheses, ranks.}
\end{figure}



The merging algorithm uses the same approach as in Section \ref{sec:dyntrees}: to merge  and , first ascend the paths from  and  to find their nearest common ancestor , then merge the traversed paths top-down. Each of the merge paths is a sequence of parts of solid paths.  An added complication in merging is that we must update the path partition, which requires keeping track of ranks.  In the absence of cuts, no node can ever decrease in rank, and we can charge the work of merging against rank changes.  By using an appropriate form of binary search tree to represent solid paths, we can obtain a logarithmic amortized time bound for merging.



The remainder of this section develops and analyzes this method. Section \ref{sec:nca} discusses the access operations, parent, root, and nca. Section \ref{sec:merge} describes the merging algorithm. Section \ref{sec:merge-analysis} analyzes the running time of merging. Section \ref{sec:merge-representation} discusses the use of search trees to represent solid paths and completes the analysis of merging. Section \ref{sec:deletions} describes how to extend the method to support leaf deletions.


\subsection{Access Operations}
\label{sec:nca}

We represent the path partition using three sets of pointers and a set of headers, one for each path.  Each node has pointers to its parent (null if it is a root) and to its solid child (null if it has none).  This makes each solid path a doubly-linked list, and allows accessing the parent or solid child of a node in constant time.  Efficient computation of roots and of nearest common ancestors requires fast access from any node  to , the top of the solid path containing .  To allow such access while also allowing fast updating, we use one level of indirection, which is the purpose of the path headers:  each node points to the header of its path; the header points to the top of the path.

Both  and  take  worst-case time.  To do , traverse the path from  to the root, step-by-step.  A step is from a node to its parent if it is a top node or to the top of its solid path if it is not. Each such step takes constant time via either a parent pointer or a path header.  The traversal reaches a new solid path, of higher-rank nodes, in at most two steps, and thus reaches the root in at most  steps.  The nca operation is similar but requires traversing two paths concurrently.  To do , traverse the paths from  and  bottom-up, taking the next step from the larger of the two current nodes, and stop when reaching a common solid path or reaching two roots.  If  and  are the last nodes reached by the concurrent traversals, the nearest common ancestor of  and  is  if  and  are on a common solid path, null otherwise.  The concurrent traversal reaches a common solid path or a pair of roots in at most  steps.


\subsection{Merging}
\label{sec:merge}

Merging requires the ability to keep track of ranks, which we do by keeping track of sizes.  To make this efficient, we store explicitly
only the sizes of top nodes.  Since we can access a top node from any node on its solid path in constant time, and since all nodes on a solid path have the same rank, we can compute the rank of any node in constant time.  To help maintain the sizes of top nodes, we also store with every node  its \emph{dashed size} , defined to be one (to count  itself) plus the sum of the sizes of the dashed children of . We can compute the size of any solid child  from that of its parent in constant time using the following equation:



Merging uses the following variant of the topmost query:

\begin{itemize}
\item : Return the topmost node on the solid path containing  that is strictly greater than , or null if there is no such node.
\end{itemize}

To perform , begin by computing the nearest common ancestor  of  and  by the method of Section \ref{sec:nca}, keeping track of the two sequences of nodes visited by the traversals from  and . Stop if  or . Otherwise, traverse the paths to  from  and  top-down, merging them step-by-step, updating solid paths as necessary. To do this, maintain two current nodes  and , initially the children of  that are ancestors of  and , respectively, or  and  if .  If , swap  and  and  and , respectively.  While , repeat the following step:
\begin{description}
\item[Merge Step:] Let  be the first (bottommost) node on the solid path containing  that was reached during the traversal from .  If , let ; otherwise, let  be the node below  that was reached during the traversal from .  (If , such a  exists because .)  Make  the parent of . (Node  is a descendant of .)  Update the solid paths that change as a result of this parent change.  Set  equal to .  If , swap  and  and  and , respectively.
\end{description}
To finish the merge, make  the parent of  and update the solid paths accordingly.\\


This algorithm is like the merging algorithm in Section \ref{sec:dyntrees}, but it can make more parent changes, because it proceeds one solid path at a time.  Such extra changes occur only in merge steps for which , which proceed from one solid path to another without doing a topmost query.  If ,  is on the solid path containing  and , since .

Some details of the algorithm remain to be filled in.  To compute the initial values of  and  if , let  and , respectively, be the last nodes on the paths traversed from  and , respectively, other than . If , then ; otherwise,  is the solid child of .  Similarly, if , then ; otherwise,  is the solid child of .


We also need to update solid paths.  In our description of how to do this, primes denote updated values.  Let : in a merge step, ; in the finalization of a merge, .  The only nodes whose rank can change are ancestors of  on the same solid path as .  Apply the appropriate one of the following two cases:

\begin{description}
\item[Case 1:] Node  is a solid child.  Then  is a dashed child, and all nodes on  change rank to .  Make  the parent of , make  the solid child of , and set .  If  has a solid child , compute its size by walking down along the path  applying equation (\ref{eq:size}), and then set .  This makes  a dashed child.   Make  the new solid child of .  Change the header of every node on  to that of , and make the header of  (if c exists) point to . 

\item[Case 2:] Node  is not a solid child.  Compute the old rank of .   Make  a dashed child of  by setting  and either setting  if  is a root (this only occurs at the beginning of a merge of two nodes in different trees) or setting  if  is not a root.  Make  the parent of .  Now all values are correct for the current path partition, but the partition is not by rank.  To correct this, find the nodes that change rank by starting at  and walking down the solid path computing the new size (using equation (\ref{eq:size})) and rank of each node reached, until reaching a node whose rank does not change or walking off the bottom of the path.

    This identifies the nodes whose rank changes and the arcs that change type.  There are at most three such arcs:  can become solid;  can become solid; and either the arc to  from its old solid child if it has one or an arc on the path  can become dashed.  This follows from an examination of three cases.  If , then zero or more nodes on  increase in rank by one.  If , then all nodes on  increase in rank to .  If , then all nodes on  increase in rank to  or .  In each case at most one arc becomes dashed.

    In addition to updating solid child pointers, change a dashed arc  to solid or vice-versa by subtracting or adding the new size of  to the dashed size of , respectively: if  becomes dashed,  increases in rank, and , which is needed both to update the dashed size of  and since  becomes a top node, is computed during the walk down the solid path.  Update the headers as follows.  If  is not a root, make the header of  also the header of each node whose rank increases to .  (In this case .)  For each node whose rank increases to , change its header to that of .  If  becomes dashed, make the header of  point to .  If one or more nodes have their headers change to that of , make the header of  point to the topmost such node. \end{description}

This completes the description of merging and of the data structure, except for the implementation of topmost queries.  To make these queries efficient, we represent each solid path by a suitable kind of search tree (in addition to parent and solid child pointers and headers).  In addition to topmost queries, this structure supports insertion of a node into a solid path above or below a given node, and deletion of the top of a solid path.  We do such insertions and deletions during Cases 1 and 2, as follows.  Walk down along the solid path from .  Delete from this solid path each node whose rank changes, and insert the node either above  (if its new rank is that of ) or below its parent (in Case 2 if  is not a root and the new rank of the node is the rank of its parent).




\subsection{Analysis of Merging}
\label{sec:merge-analysis}

In this section we analyze the running time of merging, independent of the type of search tree used to represent solid paths.  We use this analysis in the next section to choose search trees that give an  amortized time for merging.

\begin{lemma}
\label{lemma:number-of-steps} The total number of solid path insertions and deletions is .  The total number of merge steps is .
\end{lemma}
\proof
Since there are no cuts, no node can decrease in rank, and the total number of increases in rank is at most .  Each solid path deletion or insertion is of a node whose rank increases, so there are at most  of each.  There are  merge steps that cause a rank increase.  A merge step that does not cause a rank increase must result in Case 2 with .  Consider how  changes as the result of a merge step.  The value of  is between 0 and  and cannot increase.  A merge step that does not cause a rank increase is either the last step of the merge, or decreases  by at least one (if ), or is followed by a merge step resulting in Case 1 and hence causing a rank increase (if ).  It follows that the number of merge steps that do not cause a rank increase, and hence the total number of merge steps, is . \proofend


\begin{corollary}
\label{cor:number-of-steps} The total time for all merges is  plus the time for  topmost queries and  solid path insertions and deletions.
\end{corollary}
\proof There is at most one topmost query per merge step.  Not counting the time for topmost queries and solid path insertions and deletions, the time for a merge step is  plus  per solid path deletion.  The bound follows from Lemma \ref{lemma:number-of-steps}. \proofend

By Corollary \ref{cor:number-of-steps}, the amortized time for a merge is , not counting the time for topmost queries and solid path insertions and deletions.  If we represent each solid path by a binary search tree such as a red-black tree~\cite{GS78} or a splay tree~\cite{ST85}, then the time for an insertion, deletion, or topmost query is , giving the same  amortized bound for merging as in Section \ref{sec:dyntrees}.  To obtain a better bound, we need a more-refined analysis of the topmost queries and we use a more-sophisticated kind of search tree to represent solid paths.

We define the \emph{cost} of a topmost query  to be .

\begin{lemma}
\label{lemma:topmost}
The total cost of all the topmost queries over all merges is .
\end{lemma}
\proof
We estimate the cost of two types of topmost queries separately.  We call a query \emph{type-1} if it occurs in a merge step such that  and \emph{type-2} otherwise.  The total cost of the type-1 queries is easy to bound.  The parent change after a type-1 query causes every node on  to increase in rank.  It follows that the total cost of type-1 queries is at most .

The parent change after a type-2 query does not necessarily cause the nodes on  to increase in rank, but it does cause their sizes to increase, and by analyzing these size increases we can get a bound on the total cost of type-2 queries.  Consider a type-2 query.  Let  and .  The cost of the query is , of which we charge  to each node on  and the residue to .  The residual charge to  is

We complete the proof by showing that the total charge over all type-2 queries is .  When  is charged to a node  on , the size of  increases by at least .  While at a given rank, such a node  accumulates less than one unit of total charge, because if it accumulates one or more units of charge, its size grows by at least , causing its rank to change.  It follows that the total of all such charges to all nodes at all ranks is at most ,  for charges that do not cause rank increases and  for those that do.  Suppose a node  receives a residual charge, of  or less.  The next merge step increases the rank of  to at least  (by at least ) because  becomes a descendant of .  Thus the residual charge to  is at most its rank increase, and the total of all such charges is at most .  \proofend



\subsection{Solid Paths as Search Trees}
\label{sec:merge-representation}

If we represent solid paths by certain kinds binary search trees, we are able to obtain a logarithmic time bound for merging.  We present three different solutions.  The first two use different kinds of finger search trees.  A \emph{finger search tree} is a form of search tree that supports an insertion or deletion at a given position in constant amortized time and a search from a given position to a position  away in  time.  The type of finger search tree that applies most directly to our problem is a \emph{homogeneous finger search tree}, such as a homogeneous red-black finger search tree~\cite{finger_trees:tvw88}.  This is a red-black tree whose leaves in left-to-right order store the items of a list, in our case the nodes of a solid path in top-to-bottom order.  Every tree node contains pointers to its left and right children and to its parent.  In addition, every black node has \emph{level links} connecting it to its left and right neighbors at the same black height.  The internal nodes contain values, derived from the items, that make searching efficient.  This data structure supports insertion or deletion in constant amortized time, given a pointer to the position of the insertion or deletion.  It also supports  in  worst-case time.  For details see~\cite{finger_trees:tvw88}.  (But be aware that the captions are reversed on Figures 22 and 23 of that paper.)  If  we represent each solid path by a homogeneous red-black finger search tree, an  amortized time for merging follows immediately from Corollary \ref{cor:number-of-steps} and Lemma \ref{lemma:topmost}.

Homogeneous finger search trees are actually a heavyweight data structure to use in our situation.  A simpler data structure that works, although not quite as directly, is a \emph{heterogeneous finger search tree}, such as a heterogeneous red-black finger search tree~\cite{finger_trees:tvw88}.  This is a red-black tree with the items of a list stored in its nodes in symmetric order.  (In \cite{finger_trees:tvw88} the items are stored in the leaves, but it is simpler to store the items in the internal nodes, and the same time bounds hold.)  Each node contains pointers to its left and right children, except that the pointers along the left and right \emph{spines} (the paths from the root to the first and last node, respectively) are reversed: every node on the left (right) spine points to its right (left) child and to its parent.  Access to the tree is by pointers to the first and last nodes.  This data structure supports an insertion or deletion at a position  away from either end in  time.  It also supports a search, such as a topmost query, to a position  away from either end in  worst-case time.  Finally, it supports catenation of two trees (if the order of their items is compatible) in  amortized time, and splitting a tree in two at a position  away from either end in  time.

To obtain an  amortized time for merging using heterogenous red-black finger search trees, we represent each solid path by a search tree, except that we split the paths containing  and  in two, just above  and , respectively.  Then all insertions and deletions are at the ends of paths, so each one takes  amortized time. This includes the insertions and deletions at the end of the merge, when  becomes the parent of . After each topmost query, we split the path containing the returned node just above that node.  When updating  and , we do a catenation if necessary to reflect the new state.  Each query  in a merge step takes  time, as does the split just above , because when this query is done  is at one end of the path containing . Thus the time for the query and the split is at most  plus a constant times the cost of the query. An  amortized time for merging follows from Corollary \ref{cor:number-of-steps} and Lemma \ref{lemma:topmost}.

We note that adding parent pointers for all nodes in each red-black tree, as well as doubly linking the nodes in symmetric order, allows an insertion or deletion at an arbitrary position to be done in  time.  In our application the solid paths are already doubly linked, but the ability to do arbitrary constant-time insertions and deletions does not help us, because this representation does not support fast searching from an arbitrary position, and splitting the paths to speed up the topmost queries results in all the insertions and deletions being at the ends of paths.  Thus this representation does not help us here. 

An even simpler data structure that (almost) works is the \emph{splay tree}~\cite{ST85}, a form of self-adjusting search tree.  This is thanks to the amazing proof by Richard Cole et al.~\cite{dynamic_finger:c00,dynamic_finger:cmss00} that splay trees are as efficient as finger search trees in the amortized sense.  Specifically, Cole's~\cite{dynamic_finger:c00} proof of the dynamic finger conjecture for splay trees gives the following bound.  Consider a splay tree representing a solid path, initially a single node, on which a sequence of insertions, deletions, and topmost queries is done.  Let  be the \emph{finger} of the  operation, defined to be the node inserted in the case of an insertion, the new top node in the case of a deletion, or the node returned in the case of a topmost query.  Let  be the single node on the initial path.  Then the amortized time of the  operation is , where  is the number of nodes between  and , inclusive, in the tree just after the operation.


To get a logarithmic bound for merging, we combine this bound with an additional amortization.  We also need to delay certain problematic insertions of nodes into the bottom of solid paths.  To do this we represent each solid path by two parts: the \emph{top part}, represented by a splay tree, and the \emph{bottom part}, which is a doubly-linked list of its nodes, top-to-bottom.  The parent and solid child pointers provide the necessary links.  We insert a node into the bottom part merely by inserting it into the doubly-linked list.  We move nodes from the bottom part into the top part only when they are involved in a topmost query.  Specifically, to do a query , if the bottom node of the top part is less than , delete nodes one-by-one from the bottom part and insert them into the top part (the splay tree) until reaching a node greater than ;  return this node as  (leave it in the bottom part).  Otherwise, do the query on the top part, as a splay tree operation. We call this the \emph{hybrid representation} of solid paths.


\begin{theorem}
\label{theorem:splay}
With the hybrid representation of solid paths, the amortized time per merge is .
\end{theorem}
\proof Assume that the running times are scaled so that the time bound per splay tree operation is .  We shall show that the total time for all the insertions, deletions, and topmost queries is  per operation plus a constant times the total cost of the topmost queries.  The theorem then follows from Corollary \ref{cor:number-of-steps} and Lemma \ref{lemma:topmost}.

As merges proceed, we keep track of the locations where previous splay tree operations occurred and where future operations can occur.  We will define a potential function based on these locations, from which we derive the bound.  We define the \emph{finger} of a splay tree to be the finger of the most recent operation on it. A merge has one or two \emph{current nodes},  and possibly .  Node  is initially current.  It becomes non-current each time it is popped from its solid path and current each time it is updated (replaced by ). Given a solid path, let  be the number of nodes less than or equal to its current node if it has one, zero if not, and let  be the number of nodes less than or equal to its finger if it has one, zero if not (the top part is empty). We define the potential of the path to be .  The total potential is the sum of the potentials of all the solid paths.

This potential has several important properties.  It is initially zero and is always non-negative, so the sum of the amortized times of a sequence of operations is an upper bound on the sum of their actual times. The following operations take  amortized time: creating a one-node solid path; removing the current node of a solid path ( becomes zero, which does not increase the potential); making the top node of a solid path current ( changes from zero to one, increasing the potential by );  and moving the current node to the finger (the potential does not increase).  Moving the finger to the current node decreases the potential by , which makes the amortized time for an insertion or deletion at the current node, or just above or below it, .

Merge initialization takes  time and increases the potential by , for a total of  amortized time.  Consider a merge step.  Each insertion of a node into a bottom part takes  time.  If  is in the top part of its path, each insertion above  takes  amortized time since  is a current node.  On the solid path containing , a topmost query and one or more deletions at the top may be done.  Each deletion from the top takes  time if the top part is empty,   amortized time if it is non-empty, because the first such deletion is of the current node  and each subsequent deletion is at the finger.  Consider  a  query .  The cost of the query is .  If  is in the top part, the query is done as a splay tree operation and the finger moves to . The time for the query by Cole's bound is .  The amortized time for the query is

That is, the amortized time for the query is  plus at most twice the cost of the query.  If  is the top node of the bottom part, the query takes  time and there are no splay tree operations.  If  is in the bottom part but not the top node, each node on the bottom part above  is inserted into the splay tree, and the finger moves to , now in the top part.  If there are  insertions into the top part, the time of the query by Cole's bound is at most  for the first insertion plus  for the rest.
The amortized time for the query is at most

which is  plus at most twice the cost of the query.

Finally, consider the effect of updating  and  at the end of the merge step.  This makes  a new current node.  If  is the result of a topmost query,  or its parent is the finger of its path; otherwise,  is the top of its path.  In either case, making  a current node increases the potential of its path by only .  \proofend

We conjecture that Theorem \ref{theorem:splay} is true if solid paths are represented entirely by splay trees.  We claimed such a result in the conference version of our paper~\cite{GTW06}, but our proof is incorrect.  With such a representation, the proof of Theorem \ref{theorem:splay} fails for certain insertions of nodes into the bottoms of solid paths.  We want such an insertion to take  amortized time, but this is not true for an insertion into a path not having a current node and whose finger is far from the bottom of the path.  Such an insertion can occur in a merge step after a step in which , so that a topmost query does not occur.  One way to get a correct proof would be to extend Cole's proof of the dynamic finger conjecture to show that the extra time needed for  arbitrary interspersed insertions at one end is .  We conjecture that this is true, but proving it may require delving into the details of Cole's very-complicated proof.


\subsection{Leaf Deletions}
\label{sec:deletions}

The easiest way to handle leaf deletions is just to ignore them, since deleted nodes play no role in future operations.  To reinsert a deleted node, we create a new version of it and treat it as a new node.  If there are enough deletions that the number of nodes decreases by a constant factor,  we may wish to entirely rebuild the data structure each time this happens.  This takes linear time, which is  per deletion.  With such rebuilding, the space used is always linear in the number of undeleted nodes, and the amortized merge time is logarithmic in the number of undeleted nodes.


\section{Implicit Mergeable Trees}
\label{sec:weak}

We now consider the special case of mergeable trees in which there are neither cuts nor parent queries.  In this case we need not store parent pointers, and indeed we do not need to explicitly maintain the trees at all.  Instead, we represent each mergeable tree by a dynamic tree of possibly different structure but \emph{equivalent} in that an nca query or a merge operation can be simulated by  dynamic tree operations.  This gives us an  time bound for each mergeable tree operation, worst-case, amortized, or randomized, depending on the bound of the underlying dynamic tree structure.  Since the mergeable trees are implicitly represented, we call the resulting solution \emph{implicit mergeable trees}.

In order to develop this approach, we need to introduce a little terminology.  Let  a rooted tree whose nodes are selected from a totally ordered set;  need not be heap-ordered.  Let  and  be any nodes in .  We denote by  the (unique) path connecting  and  in , ignoring arc directions.  In general this path consists of two parts, connecting  and , respectively, with .  When used in an argument of ,  and  denote the node sets of  and , respectively.  If  is heap-ordered,  and .  Thus we can find roots and nearest common ancestors by computing minima over appropriate sets.  Furthermore, we need not do this the original tree; we can use any tree  that is \emph{equivalent} to  in the following sense:  and  have the same node sets and  for all pairs of nodes ,  in .

Thus we shall represent a forest of mergeable trees by a forest of equivalent dynamic trees in which we simulate each merge by a link, or by a cut followed by a link.  We need the following additional operations on rooted but not necessarily heap-ordered dynamic trees:
\begin{itemize}
\item : Return the minimum node in the tree containing .
\item : Return the minimum node on the path from  to .
\item : Make  the root of the tree containing it, by reversing the direction of each arc on the path .
\end{itemize}
These dynamic tree operations are standard: see \cite{AHTdL05,GGT91,ST83,ST85,TW05}.  We implement the mergeable tree operations, excluding parent and cut, by simulating them on the equivalent dynamic trees  as follows:
\begin{itemize}
\item : Return .
\item : If , return null. Otherwise, do  and return .
\item : Create a new dynamic tree having the single node .
\item : Use the method in Section \ref{sec:deletions}.  Specifically, ignore leaf deletions; optionally, rebuild the entire forest each time the number of nodes decreases by a constant factor.
\item : If , do  and then . Otherwise, do  and let ; if , do  and then .
\end{itemize}
We shall show that the dynamic trees maintained by the implementation of the merge operations are equivalent to the corresponding mergeable trees.  Assuming that this is true, the \emph{root} and \emph{nca} functions return the correct values: if  and  are in the same  mergeable tree , then they will be in the same dynamic tree ; the value returned by   is , and the value returned by  is , where the subscript ``'' indicates the tree in which the value (root or nca) is defined.

In an operation , if  and  are in the same mergeable tree  and the same dynamic tree ,  .  If  and  are unrelated in , the merge cuts the first arc on the path in  connecting  and , and then links  and .

It remains to show that the implementation of merging maintains equivalence.  We do this by a sequence of lemmas. We start with the simpler case, that of a merge that combines two different mergeable trees.  Suppose  and  are in different mergeable trees  and , respectively, and let  be the mergeable tree produced by the operation .  Let  and  be nodes in .  Assume without loss of generality (which we can do by the symmetry of  and  and  and , respectively) that  is in .



\begin{figure*}[tb]
\begin{center}
\resizebox{1.\textwidth}{!} {\includegraphics{figure41a.eps}}\\
\vspace{1cm} \resizebox{1.\textwidth}{!}
{\includegraphics{figure41b.eps}}
\end{center}
\caption{\label{fig:weak-merge-1} Proof of Lemma~\ref{lem:weak1}.  Nodes in the tree containing  are black and nodes in the tree containing  are white. Grey nodes can be either black or white depending on the node labels. Solid lines are single arcs; wavy lines are tree paths. (a) Node  is in . Here we assume .  After the merge,  is on  and  is on . (b) Node  is in . Here we assume .  After the merge,  is on  and  is on .}
\end{figure*}


\begin{lemma}
\label{lem:weak1} If  is in , then .  If  is in , then .
\end{lemma}
\proof
Suppose  is in . (See Figure \ref{fig:weak-merge-1}(a).) Let . The effect of the merge on the path between  and  is to insert zero or more nodes of  into either the part of the path from  to  or into the part of the path from  to .  Any such inserted node must be larger than .  Thus , giving the first part of the lemma.  Suppose that  is in . (See Figure \ref{fig:weak-merge-1}(b).) Let  and .  In ,  and  are related.  The path  is a catenation of , a path of descendants of , and .  Thus , giving the second part of the lemma. \proofend

Now suppose that  and  are trees equivalent to  and , respectively, and that  is formed from  and
 by rerooting  at  and adding the arc .

\begin{lemma}\label{lem:weak2} Tree  is equivalent to .
\end{lemma}
\proof Clearly  and  contain the same nodes.  We need to show that  for every pair of nodes , , in .  Assume without loss of generality that  is in .  If  is in , then , and  follows from the first part of Lemma~\ref{lem:weak1} and the equivalence of  and .  If  is in , then  is a catenation of  and , and  follows from the second part of Lemma~\ref{lem:weak1} and the equivalence of  and  and of  and . \proofend


The case of a merge that restructures a single tree is similar but more complicated.  Consider an operation  of two nodes that are in the same tree .  Let .  Assume that  is neither  nor ; otherwise the merge does nothing.  Let  be the child of  that is an ancestor of , let  be the subtree of  with root , and let  be the tree produced by the merge.  Finally, let  and  be any nodes of .  The next lemma is the analogue of Lemma~\ref{lem:weak1} for this case.

\begin{figure*}\begin{center}
\resizebox{1.\textwidth}{!} {\includegraphics{figure42a.eps}}\\
\vspace{.5cm}
\resizebox{1.\textwidth}{!} {\includegraphics{figure42b.eps}}\\
\vspace{.5cm}
\resizebox{1.\textwidth}{!} {\includegraphics{figure42c.eps}}
\end{center}
\caption{\label{fig:weak-merge-2} Proof of Lemma~\ref{lem:weak3}. Node  is the child of  in  that is an ancestor of .
Nodes in the subtree of  are black and the rest are white. Grey nodes can be either black or white depending on the node labels.
(a) Both  and  are in . After the merge,  is on . (b) Neither is in . Here we assume .  After the merge,  is on . (c) Only  is in ; the situation is similar if only  is in . Here we assume .  After the merge,  is on .}
\end{figure*}

\begin{lemma}\label{lem:weak3} If both  and  are in , or neither  nor  is in , then .  If exactly one of  and , say , is in , then
.
\end{lemma}
\proof If both  and  are in , then  is entirely in  (see Figure \ref{fig:weak-merge-2}(a)); if neither is in , then  is entirely outside of  (see Figure \ref{fig:weak-merge-2}(b)).  Suppose one of these cases is true.  Let .  The effect of the merge on the path between  and  is to insert into the path zero or more nodes, all of which must be larger than .  Thus , giving the first part of the lemma.  Suppose that exactly one of  and , say , is in .  Let  and . In ,  and  are related. (See Figure \ref{fig:weak-merge-2}(c)). Path  is a catenation of , a path of descendants of  and . Thus .\proofend


Now suppose that  is a tree equivalent to .  Reroot  at , which does not affect the equivalence, and let  be the parent of  in . Deleting the arc from  to  breaks  into two trees; let  be the one that contains  (and ).  Finally, let  be the tree formed from  by deleting the arc from  to  and then adding an arc from  to . We shall show that Lemma~\ref{lem:weak2} holds in this case; that is,  is equivalent to .  This would be easy (and analogous to the proof of Lemma~\ref{lem:weak2}, but using Lemma~\ref{lem:weak3} in place of Lemma~\ref{lem:weak1}) if  and  were equivalent.  This is not necessarily true, however.  Fortunately, what is true suffices for our purpose.

\begin{lemma}
\label{lem:weak4}  contains all the nodes in .  Any node in  but not in  is not a descendant of  in .
\end{lemma}

\begin{figure*}\begin{center}
\resizebox{.65\textwidth}{!} {\includegraphics{figure43a.eps}}\\
\vspace{1cm}
\resizebox{.65\textwidth}{!} {\includegraphics{figure43b.eps}}
\end{center}
\caption{\label{fig:weak-merge-3} Proof of Lemma~\ref{lem:weak4}.
Node ,  is the child of  in  that is an ancestor of , and  is the parent of  in .
(a) Assuming  is not in  implies , a contradiction. (b) If  is in  but not in  then
.}
\end{figure*}

\proof Let  be a node in . Then .  Since  and  are equivalent, .  But then  must be in , because if it were not,  would be on ,  which would imply
, a contradiction. (See Figure \ref{fig:weak-merge-3}(a).) This gives the first part of the lemma.  Suppose  is in  but not in .  Since  is not in ,  is not a descendant of  in , which implies .  But since  and  are equivalent, and  but not  is in , . (See Figure \ref{fig:weak-merge-3}(b).) Thus , which implies the second part of the lemma. \proofend

\begin{lemma}\label{lem:weak5} Tree  is equivalent to .
\end{lemma}
\proof Trees  and  contain the same nodes.  We need to show that  for every pair of nodes  and  in .  The first part of Lemma~\ref{lem:weak4} gives six cases to consider, depending upon which of the trees  and  contain  and .  If  and  are both in , or both in  but not in , or both not in , then  by the first part of Lemma~\ref{lem:weak3}, the equivalence of  and , and the construction of .  If one of  and , say , is in , and the other, , is not in , then  by the second part of Lemma~\ref{lem:weak3}, the equivalence of  and , and the construction of .  These cases are analogous to the two cases in the proof of Lemma~\ref{lem:weak2}.

\begin{figure*}\begin{center}
\resizebox{.65\textwidth}{!} {\includegraphics{figure44a.eps}}\\
\vspace{.5cm}
\resizebox{.65\textwidth}{!} {\includegraphics{figure44b.eps}}
\end{center}
\caption{\label{fig:weak-merge-4} Proof of Lemma~\ref{lem:weak5}. Node ,  is the child of  in  that is an ancestor of , and  is the parent of  in . After the merge,  and  are on .
(a) Node  is in  and  is in  but not in . Then  is not a descendant of  in , so . Since , . These two facts imply . (b) Node  is in  but not in  and  is not in . Then . All nodes in  are greater than or equal to ; thus .}
\end{figure*}

The remaining two cases are new.  Suppose one of  and , say , is in , and the other, , is in  but not in .
(See Figure \ref{fig:weak-merge-4}(a).) By the second part of Lemma~\ref{lem:weak4},  is not a descendant of  in .  Then . Also , since  is in .  Thus  by the second part of
Lemma~\ref{lem:weak3}, the equivalence of  and , and the construction of .  Last, suppose one of  and , say ,
is in  but not in , and the other, , is not in . (See Figure \ref{fig:weak-merge-4}(b).) Then  is on .
It follows that  by the first part of Lemma~\ref{lem:weak3} and the equivalence of  and .  But  since  is not a descendant of  by Lemma~\ref{lem:weak4}.  Thus .  Paths  and  contain the same nodes except possibly for nodes of the path , all of which must must be at least , since . Thus .
\proofend

Lemmas \ref{lem:weak1} and \ref{lem:weak4} give us the following theorem:
\begin{theorem}
\label{theorem:implicit} The implementation of implicit mergeable trees using dynamic trees is correct.
\end{theorem}
Thus if there are no cuts and no parent queries, we can simulate each mergeable tree operation by  dynamic tree operations, giving an  time bound per operation, worst-case, amortized, or randomized depending upon the efficiency of the underlying dynamic trees.  Since the roots of the dynamic trees are irrelevant to the representation, we can use unrooted dynamic trees, such as top trees~\cite{AHTdL05} or topology trees~\cite{Fre97a} instead.  We can avoid the need for the \emph{treemin} operation on dynamic trees by using a separate disjoint set data structure~\cite{Tar75,Smi90} to handle root queries.  The disjoint sets are the node sets of the trees; each root query is a find query in the disjoint set structure, and each merge of two different trees requires a union of their node sets.  The extra time per merge is  worst-case and the time bound per root query is logarithmic worst-case and inverse-Ackermann amortized.

It seems hard if not impossible to extend the method presented here to handle parent queries or cuts, because the connection between mergeable trees and the equivalent dynamic trees can be quite loose.  In particular, a mergeable tree that is a single path can be represented by a dynamic tree consisting of one node that is adjacent to all the other nodes in the tree: consider the sequence , , , ,  applied to an initial set of singleton trees.  For such an example, performing a \emph{parent} query or a \emph{cut} on the mergeable tree will take at least  time on the equivalent dynamic tree.



\section{Persistence Pairings via Mergeable Trees}
\label{sec:pairing}

Our motivating application for mergeable trees is a problem in computational topology, that of computing an \emph{extended persistence pairing} of the critical points of a 2-dimensional connected manifold embedded in .  An algorithm for this problem was proposed by Agarwal et al.~\cite{AEHW04,AEHW06}.  The use of mergeable trees in this algorithm gives an -time implementation, where  is the number of critical points.  We shall describe the pairing algorithm in some detail, because the exact form it takes affects the set of operations needed on the mergeable trees.  In particular by modifying their algorithm, we are able to avoid the need for parent queries, thereby allowing the use of the implicit mergeable trees of Section \ref{sec:weak}.  We also fill in a lacuna in their algorithm.

The critical points of a manifold are the local minima, local maxima, and saddle points in a particular direction, say increasing -coordinate.  The algorithm of Agarwal et al.\ computes a directed acyclic graph called the \emph{Reeb graph} that represents the skeleton of the manifold, during a -increasing sweep over the manifold.  The Reeb graph is actually a multigraph; that is, multiple arcs (arcs with the same start and end vertices) can occur. The vertices of the Reeb graph correspond to the critical points. Agarwal et al.\ assume that the manifold is perturbed so that the critical points all have different -coordinates, and so that the skeleton of the manifold has no vertex of degree exceeding three. In particular, each vertex is of one of four kinds: a \emph{source}, with in-degree zero and out-degree one; a \emph{sink}, with in-degree one and out-degree zero; an \emph{up-fork}, with in-degree one and out-degree two; or a \emph{down-fork}, with in-degree two and out-degree one.  The vertices of the Reeb graph are topologically ordered by the -coordinate of the corresponding critical point. We call this the \emph{canonical order} (there may be other topological orderings).

The algorithm of Agarwal et al.\ pairs the vertices of the Reeb graph, and hence the critical points of the manifold, during a sweep of the graph that visits the vertices in canonical order, modifying the graph as it proceeds.  This is the part of the algorithm that uses mergeable trees.  The pairing can be done during the sweep over the manifold that builds the graph, but for our purposes it is better to think of the pairing process as a separate sweep.  We identify each vertex with its number in canonical order.  The pairing sweep maintains three invariants: (1) each vertex, once visited, has in-degree at most one; (2) a visited vertex is paired if and only if both its in-degree and its out-degree are one, or both its in-degree and out-degree are zero; and (3) the vertex order is topological.  When visiting a vertex , the pairing sweep applies the appropriate one of the following cases:
\begin{itemize}
\item Case 1:  is a source. Do nothing.
\item Case 2:  is an up-fork. Do nothing.
\item Case 3:  is a down-fork, with incoming arcs from  and  (which may be equal).  Concurrently walk backward along the paths ending at  and at , each time taking a step back from the larger of the two vertices on the two paths, until reaching a vertex  common to the two paths, or trying to take a step back from a source .  Pair  with .  Merge the two paths traversed, arranging the vertices in order along the merged path.
\item Case 4:  is a sink, with an incoming arc from .  Delete .  While  is paired, delete  and replace it by its predecessor (before the deletion).  Pair  with .
\end{itemize}

It is straightforward to prove by induction on the number of visited vertices that the pairing sweep maintains invariants (1)--(3).  If the manifold is connected, as we have assumed, the second alternative in invariant (2) applies only after the last vertex (a sink) is processed; it is paired (in Case 4) with the first vertex, which becomes the only vertex of in-degree and out-degree zero.  If the manifold is disconnected, there will eventually be one instance of the second alternative in invariant (2) for each connected component of the manifold, corresponding to the pairing of its global minimum and global maximum points.

The pairing sweep algorithm can be implemented directly.  Vertices need not be deleted in Case 4 but merely traversed, since each such vertex is traversed only once; deleting them merely makes the statement of invariant (2) simpler.  The running time is  plus the time spent walking backward along paths in Case 3, which can be .  To reduce this time we use mergeable trees.

Specifically, we store the set of visited vertices as the nodes of a collection of mergeable trees and perform appropriate mergeable tree operations in Cases 1--4.  When visiting a vertex , we first make it into a new, one-node mergeable tree and then apply the appropriate one of the following cases:

\begin{itemize}
\item Case :  is a source.  Do nothing.
\item Case :  is an up-fork, with an incoming arc from . Do .
\item Case :  is a down-fork, with incoming arcs from  and .  If  and  are in different trees, pair  with ; otherwise, pair  with .  In either case do  and .
\item Case :  is a sink, with an incoming arc from .  Do .  While  is paired, replace  by its parent in its mergeable tree.  Pair  with .
\end{itemize}
See Figure~\ref{fig:reeb-fwd}. This is a restatement of the algorithm of Agarwal et al.\ that explicitly uses mergeable trees, with a lacuna corrected in Case 4: Agarwal et al.\ imply that the predecessor of  is unpaired, but this need not be true.  Edelsbrunner (private communication, 2006) suggested fixing this problem by eliminating paired nodes from the mergeable trees, replacing each paired degree-two node by an arc from its child to its parent.  But we prefer the method above, since it requires no restructuring of the trees, and it leads to the two-pass pairing algorithm we develop below.  Agarwal also pair the first and last vertex separately, but this is redundant, since this pair is found by the method above in Case .

The total number of mergeable tree operations done by this method is , since each case except  does  tree operations, and the total number of tree operations done by all executions of Case  is : any particular vertex  can be replaced by its parent in at most one execution of Case ,  since such a replacement corresponds to the deletion of  in the corresponding execution of Case 4.  The time spent in addition to mergeable tree operations is . The mergeable tree implementations discussed in Sections~\ref{sec:dyntrees} and \ref{sec:part-rank} can be used, since no cuts are needed.  The fastest method of Section~\ref{sec:part-rank} has an amortized  time bound per mergeable tree operation, giving an  time bound for pairing.

We can avoid the need for parent queries in the mergeable trees by doing two passes of a streamlined version of the method above, one in topological order and the other in reverse topological order.  This allows the use of the mergeable tree implementation described in Section~\ref{sec:weak}, which uses ordinary dynamic trees as a black box.  In order to obtain this result, we need an observation about the pairing that the algorithm produces.  Each pair is of one of four types: (a) a down-fork and an up-fork, found in Case 3; (b) a down-fork and a source, also found in Case 3; (c) a sink and an up-fork, found in Case 4; or (d) a source and a sink, also found in Case 4.  As mentioned above, there is exactly one source-sink pair if the manifold is connected, as we are assuming. If we reverse the direction of all the arcs of the Reeb graph, then every source becomes a sink and vice-versa, and every up-fork becomes a down-fork and vice versa.  If we run the pairing algorithm on the reversed graph using as the topological order the reverse of the original topological order, we compute the same pairing, except that every type-(b) pair becomes a type-(c) pair and vice-versa; type-(a) pairs and type-(d) pairs remain type (a) or (d), respectively.  But this means that \emph{every} pair except the unique type-(d) pair will be found in Case 3 of either the forward sweep or the reverse sweep.  Thus we can find all the pairs by pairing the first and last vertices, and running forward and reverse sweeps of the above method with Case  replaced by the following:
\begin{itemize}
\item Case :  is a sink, with an incoming arc from . Do .
\end{itemize}
See Figure~\ref{fig:reeb-rev}. With this method the only operations needed on the mergeable trees are insert, root, nca, and merge.  Use of the mergeable tree implementation of Section~\ref{sec:weak} gives an -time pairing algorithm.  Though this does not improve the asymptotic time bound, it avoids the complexities of Section~\ref{sec:part-rank}, and it avoids the iteration in Case 4.


\begin{figure*}\begin{center}
\resizebox{.22\textwidth}{!} {\includegraphics{reeb-fwd.eps}} \hspace{.3cm}
\resizebox{.22\textwidth}{!} {\includegraphics{slide4.eps}} \hspace{.3cm}
\resizebox{.22\textwidth}{!} {\includegraphics{slide5.eps}} \hspace{.3cm}
\resizebox{.22\textwidth}{!} {\includegraphics{slide6.eps}}
\end{center}
\caption{\label{fig:reeb-fwd}Example of the execution of the single-pass pairing algorithm that uses mergeable trees that support the parent operation. (a) The forward Reeb graph. (b)-(d) The mergeable tree rooted at 1.
(b) The mergeable tree after processing vertices 1 to 7. For each vertex  the number in the parenthesis is the vertex paired with . (c) After processing vertex 8, which is a down-fork with incoming arcs from 6 and 7. Vertex 8 is paired with  and then we perform  and . (d) After processing vertex 9; the arc (9,8) is inserted as a result of . To pair 9 we perform successive \emph{parent} operations, starting from 9, until we reach the first unpaired vertex, which is 6.}
\end{figure*}

\begin{figure*}\begin{center}
\resizebox{.22\textwidth}{!} {\includegraphics{slide7.eps}} \hspace{1.5cm}
\resizebox{.22\textwidth}{!} {\includegraphics{reeb-rev.eps}} \hspace{1.5cm}
\resizebox{.22\textwidth}{!} {\includegraphics{slide10.eps}}
\end{center}
\caption{\label{fig:reeb-rev}Example of the execution of the two-pass pairing algorithm. (a) The mergeable tree (rooted at 1) produced during the first pass; all pairs except (6,9) were found. (b) The reverse Reeb graph used in the second pass. (c) The mergeable tree rooted at 10, after processing vertex 6 which is a down-fork in the reverse graph with incoming arcs from 10 and 8. Vertex 6 is paired with  (which has label greater than the label of 10 in the reverse graph) and then we perform  and .}
\end{figure*}




\section{Complexity}
\label{sec:complexity}

In this section we make several observations related to the inherent complexity of the mergeable trees problem, in an effort to clarify under what circumstances further improvements or alternative methods might be possible.  We begin by bounding the number of possible merge operations in the absence of cuts.  If each merge is of two leaves, then the number of merges is at most , since each merge reduces the number of leaves by one; a leaf deletion cannot increase the number of leaves.  On the other hand, if merges are of arbitrary nodes, there can be  merges, each of which changes the forest.  Figure \ref{fig:n2} gives an example with  merges.  Since any merge that changes the forest must make at least one pair of nodes related, there can be at most  merges.

\begin{figure*}
\begin{center}
\resizebox{1\textwidth}{!} {\includegraphics{n2.eps}}
\end{center}
\caption{\label{fig:n2} An example with  merges. The
sequence of merges consists of  rounds, each with  merges;
the  merge of round  merges nodes  and . The
number of nodes is . Figure \ref{fig:n2}(a) is the initial tree,
Figure \ref{fig:n2}(b) is the tree after the first round of  merges, and
Figure \ref{fig:n2}(c) is the tree after all  merges.}
\end{figure*}

We can also bound the number of parent changes in the absence of cuts.  Section \ref{sec:dyntrees} gives a bound of .  The example in Figure \ref{fig:n2} gives a lower bound of .  The following example gives a bound of .  Combining this example and the one in Figure \ref{fig:n2} gives an example with a bound of , thus showing that the  bound is tight. Start with  one-node trees.  Merge these in pairs to form two-node paths, then merge the pairs in pairs to form four-node paths, and so on, until there is only a single tree, consisting of a single path.  Order the nodes so that in each merge the nodes of the two merged paths are perfectly interleaved.  Then the number of merges is  and the number of parent changes is .

Next, we consider the merging method originally proposed by Agarwal et al.~\cite{AEHW06} and mentioned in Section~\ref{sec:dyntrees}: to do a merge, insert the nodes of the shorter merge path one-by-one into the longer merge path.  We shall show that in the absence of cuts and if all merges are of leaves, the total number of nodes on the shorter of each pair of merge paths is , thus showing that this method of merging does not give a polylogarithmic amortized time bound for merging, though it does give a sublinear bound.  We denote by  the number of nodes on the shorter of the merge paths in the  merge.

\begin{figure*}
\addtolength{\abovecaptionskip}{-.5cm}
\begin{center}
\resizebox{0.30\textwidth}{!} {\includegraphics{nsqrtn.eps}}
\end{center}
\caption{\label{fig:nsqrtn}
Initial tree for a sequence of merges whose shorter merge paths have  nodes.  The  merge is of the shallowest leaf and the leaf that is  deeper.}
\end{figure*}

To obtain the lower bound, start with the tree in Figure~\ref{fig:nsqrtn}, where  and  is an arbitrary perfect square.  Do a sequence of  merges in which the  merge is .  Each merge is of the shallowest leaf with the leaf that is  deeper.  For each merge, the longer merge path is the one starting from the deeper leaf; it contains  nodes.  The shorter merge path contains two nodes for the first  merges, three for the next , four for the next , and so on.  Thus
\vspace{-.1cm}


To show that this bound is tight to within a constant factor, assume without loss of generality that all insert operations precede all other operations.  Considering only nodes that participate in merge operations, let  be the number of unrelated pairs of nodes in the forest of mergeable trees.  After all insertions but before any merges, .  As merges take place,  cannot increase but must remain non-negative.  The  merge decreases  by at least .  Thus .  Subject to this constraint, the sum of 's is maximized when they are all equal, say to .  Then , which implies  and .  Since all merges are of leaves, , giving .


Finally, we discuss lower bounds for three versions of the mergeable trees problem.  If cuts are allowed, the lower bound of P\u{a}tra\c{s}cu and Demaine~\cite{loglb:pd06} for the dynamic trees problem applies.  They show that in the cell probe model of computation, a sequence of intermixed insert, link, cut, and root operations take  amortized time per operation.  This bound applies to mergeable trees even if there are no merge operations.  The data structure of Section \ref{sec:dyntrees} meets this bound except for merges, for which there is a logarithmic gap.  We conjecture that there is a solution to the mergeable trees problem with an amortized logarithmic bound for all operations; specifically, we think that the structure of Section \ref{sec:dyntrees} implemented using Sleator and Tarjan's self-adjusting dynamic trees~\cite{ST85} attains this bound.  We leave this question as the most interesting open problem emerging from our work.

If there are no cuts, we can obtain a lower bound by reducing sorting to the mergeable trees problem.  Specifically, we can sort  numbers using a sequence of insert, merge, and parent operations, as follows.  We construct a one-node tree out of the first number by an insert.  For each successive number, we first construct a one-node tree by an insert and then merge it with the existing tree, which is a path.  We keep track of the maximum in this tree and use it as one parameter of the merge so that the new tree is also a path.  Finally we retrieve the numbers in reverse sorted order by starting at the maximum and doing  parent queries.  Thus any data structure that supports insert, merge, and parent needs  amortized time per operation, in any computation model in which sorting takes  time, such as a binary decision model.  In such models the structures of Sections \ref{sec:part-rank} and \ref{sec:weak} are optimum to within a constant factor.

In the absence of both cuts and parent queries, we can obtain a non-constant lower bound by reducing a form of disjoint set union to the mergeable trees problem.  The \emph{Boolean union-find} problem is that of maintaining a set of  sets, initially singletons, under an intermixed sequence of two kinds of operations: , which adds all elements in set  to set , destroying set , and , which returns true if  is in set  and false otherwise.  Kaplan et al.~\cite{KST02} showed that a sequence of  finds and intermixed unites takes  time in the cell probe model with cells of size , where  is an inverse of Ackermann's function.  To solve the Boolean union-find problem using mergeable trees, we maintain for each set a tree, whose nodes are its elements and that is a path.  As the set identifier we use the maximum element in the set (with respect to an arbitrary total order); we can use an array to maintain the mapping from the names used by the set operations to the corresponding maximum nodes.  Initialization takes  insert operations.  Each unite becomes a merge (of two different trees).  Each find can be done either by a single nca query or by two root queries.  The Kaplan et al.\ bound implies an  amortized time bound per mergeable tree operation for any structure that supports insert, merge, and either nca or root, for the cell probe model.  We conjecture that this lower bound is far from tight.


\vspace{.2cm}
\noindent{\emph{Acknowledgement.}} We thank Herbert Edelsbrunner for posing the mergeable trees problem and for sharing the preliminary journal version of~\cite{AEHW04} with us.

\bibliographystyle{plain}
\bibliography{rfw,ltg}

\end{document} 