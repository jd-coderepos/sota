\documentclass{article}





\usepackage[nonatbib,preprint]{neurips_2023}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{microtype}
\usepackage{empheq,graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{booktabs} \DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\topfraction}{0.9}\renewcommand{\bottomfraction}{0.8}\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.99}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm,wrapfig}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{mathtools}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\makeatletter
\usepackage[textsize=tiny]{todonotes}

\title{Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation}




\author{Shahar Lutati \quad Eliya Nachmani \quad Lior Wolf\\
Department of Computer Science \quad School of Electrical Engineering\\ Tel Aviv University\\
\texttt{\{shahar761,enk100\}@gmail.com}\\
\texttt{wolf@mail.tau.ac.il}
}



\begin{document}


\maketitle

\begin{abstract}
The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals.
Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models.
Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In particular, for two speakers, our method is able to surpass what was previously considered the upper performance bound.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Here is a simple algorithm for improving source separation. Given a test mixture  of  speakers, (1) apply a deep neural architecture  to  and obtain multiple approximated sources  for , (2) apply a generative diffusion model  using each of the approximations obtaining , and (3) apply a shallow convolutional neural network  to  and  to obtain mixing weights  and combine the two approximations linearly in the frequency domain to obtain the output .

The need to combine in the frequency domain arises because the reconstructed phase in each segment can be arbitrary and phase compensation is needed. In other words, since  and  are inferred by different processes, they may be at different phases. 

In our experiments, out of the three networks (), we only train network , as this training takes place on the same training set used to train network . The other networks are taken, as is, from published models. Specifically,  is either Gated-LSTM \cite{pmlr-v119-nachmani20a} or SepFormer \cite{subakan2021attention} and  is the DiffWave network \cite{kong2020diffwave}, which is trained, in an unsupervised way, on the LibriMix dataset \cite{panayotov2015librispeech} and WSJ0 \cite{wsj0} in order to generate speech signals. 

Our empirical results, presented in Sec.~\ref{sec:experiments}, demonstrate that across multiple deep architectures and methods, applying a pretrained generative diffusion model, in the most straightforward way, pushes the envelope of results further by a significant gap. Our goal is to shed light on this phenomenon. We show the following two results:
(1) the mutual information between the best combination of  and  and the underlying ground truth signal  is bounded by twice the mutual information between the mixture and the ground truth signal, (2) we provide a bound for the signal-to-distortion ratio (the SDR error metric) of such combinations , which depends on the quality of network  and the mutual information between  and each source .








\section{Related work}

Single-channel speech separation is a fundamental problem in speech and audio processing that has been extensively studied over the years~\cite{logeshwari2012survey,martin2018single}. 
Recently, deep learning models have been proposed for speech separation, resulting in a significant improvement in performance compared to traditional methods. Hershey et al. proposed a clustering method that utilizes trained speech embeddings for separation~\cite{hershey2016deep}. Yu et al. proposed the Permutation Invariant Training (PIT) at the frame level for source separation~\cite{yu2017permutation}, while Kolbaek et al. extended this approach by proposing the utterance-level Permutation Invariant Training (uPIT)~\cite{kolbaek2017multitalker}.
An influential deep learning method for  speech separation over the time domain was introduced by Luo et al.~\cite{luo2018tasnet}. This method employs three components: an encoder, a separator, and a decoder. Subsequently, in Conv-Tasnet the separator network was replaced with a fully convolutional model, using a block of time-depth separable dilated convolution~\cite{luo2019conv}. Conv-Tasnet was scaled by training several separator networks in parallel to perform an ensemble~\cite{zhang2020furcanext}. Dual Path RNN blocks were used to reorder the encoded representation and process it across different dimensions~\cite{luo2019dual}.  The so-called MulCat blocks were presented as a way to eliminate the need for a masking sub-network~\cite{pmlr-v119-nachmani20a}. 

One of the limitations of current methods is their inability to effectively train neural networks for a large number of speakers, due to the reliance on Permutation Invariant Training (PIT) methods, which have a time complexity of , where  is the number of speakers.  Instead, one can use a permutation-invariant training method that employs the Hungarian algorithm, reducing the time complexity to ~\cite{dovrat2021many}. 

Lutati et al. introduced an upper bound for audio source separation~\cite{sepit}. By dividing the speech signal into short segments of sounds, a known distribution is used to describe the signal. Then, using the relation between mutual information and Cramer Rao lower bound, the authors managed to demonstrate an upper bound for speech separation for any deterministic model. This bound depends on the mutual information between the mixture and the sources. As the number of sources increases, the mutual information decreases, and so does the upper bound. The recent advent of very successful randomized generative models points in a new research direction: while the previous bound is applicable to any deterministic processing, it remains an open question whether it holds for generative models too.



{The Diffusion Probabilistic Model has been successfully applied to various domains, such as time series and images~\cite{sohl2015deep}. A major limitation of this model is that it requires a significant number of iterative steps to generate valid data samples. This was addressed by a diffusion generative model based on Langevin dynamics and the score matching method~\cite{song2019generative}. This model estimates the Stein score function~\cite{liu2016kernelized}, which is the gradient of the logarithm of data density, and uses it to generate data points. Generative neural diffusion processes for speech generation were developed based on score matching~\cite{chen_wavegrad_2020,kong2020diffwave}. These models have achieved state-of-the-art results for speech generation, demonstrating superior performance compared to well-established methods, such as Wavernn~\cite{kalchbrenner2018efficient}, Wavenet~\cite{oord2016wavenet}, and GAN-TTS~\cite{binkowski2019high}.}

Jayaram et al. introduced the use of generative models as priors for the separation of different sources from a mixture~\cite{jayaram2020source}. This was tested on mixtures of images such as MNIST \cite{lecun-mnisthandwrittendigit-2010} and LSUN \cite{yu2015lsun} datasets. DiffSep~\cite{scheibler2022diffusion} performs audio separating using a diffusion model. The underlying SDE is an affine transformation of a mixing matrix . While the authors showed the ability to separate sources from a single channel, their result falls behind deterministic models by a large margin.
Although they set the ground for source separation using generative models, neither work presented state-of-the-art performance, nor proposed a bound for generative methods.

We are unaware of similar generalization bounds developed for diffusion models. Our method can be applied to other domains, such as image denoising and conditional generation, given suitable data on the underlying distribution of the training data. For example, for image denoising, there are known results from the field of natural image statistics~\cite{weiss2007makes} that state that while segmenting the image into patches a Gaussian Scale Mixture is able to model the statistics of the underlying natural images. A similar derivation to what is done in our work for audio would obtain an upper bound for the maximal improvement achievable given the noise prior.


\section{Analysis}

Let  be a mixture of  speakers, each with a ground truth signal , . Let  be a source separation model that returns  signals and  be a vocoder diffusion model that is trained on the domain of clear, single-speaker voice signals. The method described at the beginning of Sec.~\ref{sec:introduction} can be summarized by the following set of equations, where STFT and iSTFT
are the Short-Time Fourier transform and its inverse, respectively.





Eq.~\ref{eq:bmodel} applies  to separate the mixed signal. Eq.~\ref{eq:gm} applies the vocoder to denoise each output of  separately. Eq.~\ref{eq:f} computes the combination weights of the output of  and the corresponding output of . Finally, the combination is performed by linearly mixing in the spectral domain in Eq.~\ref{eq:combine}. Where  is the Hadamard product.

As we show in Sec.~\ref{sec:experiments}, this simple method empirically surpasses the current state-of-the-art. Evidently, when applied correctly, a vocoder model can suppress the errors that the deterministic source separation model has. Below, we analyze the source of this improvement and derive an upper bound for the level of improvement. 

\begin{definition}[Signal-to-Distortion Ratio (SDR)]
Let the error  be defined as the difference between the source and the estimated source. The Signal-to-Distortion Ratio (SDR) is .
\end{definition}
Given a mixture of sources, , the ground truth signal , and the estimated signal, , \cite{sepit} have found that 

where  is the estimation of the deterministic backbone network ,  is the length of the signal,  is the segment width,  is the ground truth source,  is the mixture and the subscript  describes the mixture and the ground truth signal in the r-th segment.

Furthermore, \cite{sepit} show that the mutual information between the deterministic estimation and the source is upper bounded by the mutual information between the source and the mixture.


In what follows, we first lay down the basis for upper bounding the combination of the generative and the deterministic signal. Next, by modeling the noise that is added to the generated signal, the mutual information between the combination of the signals and the sources is bounded. Finally, by applying the changes to Eq.~\ref{eq:init_bound}, a new upper bound is found.

\begin{lemma}[Mutual Information Chain Rule]
For any random variables ,, the following 
chain rule holds .
\label{lemma:chain_info}
\end{lemma} See proof in the supplementary. 
Using Lemma.~\ref{lemma:chain_info} for the source, , the deterministic estimation, , and the generative estimation, , we have,

Plug Eq.~\ref{eq:sum_of_mut_infos} to Eq.~\ref{eq:upper_info_det}, we have the following bound


Our goal is now to bound the maximal mutual information achievable between the source, , and the generative signal,.
\subsection{Generative Bound}

The inherent noise of GM is modeled as additive noise. For any practical GM there is an inherent error in reconstructing a perfect signal from a perfect prior. 

The pretrained generative model (GM) is optimized to generate samples  that minimize the ELBO loss when given a prior for . 

Denote the observed prior as . GM generates  that it is the optimizer for ELBO loss:


Given the aforementioned upper bound , Eq.~\ref{eq:upper_info_det}, it follows that     
when . This implies that the , will sample from the same distribution as .
Using Eq.~\ref{eq:lema} We have,  is upper bounded by , and .


Let us assume the added noise is Gaussian noise. From \cite{cover_book}, we have that among all distributions Gaussian noise is the worst-case additive noise in terms of mutual information, that is, normal additive noise will degrade most of the information in the generated signal. As shown below, for the pretrained GM, the worst case coalesces with the best case, where the inequality in Eq.~\ref{eq:intermiddiate_result} becomes equality. Thus, making Eq.~\ref{eq:intermiddiate_result} tight.









For additive white Gaussian noise (AWGN) it holds that  where  is the desired part of the signal, based on the prior, and  is the normal noise.

Where  is the variance of the additive noise.
 \cite{sepit} show empirically that for short segments of [ms] the audio is distributed in the time domain as a Laplace distribution, .


Computing explicitly the mutual information we obtain
\\
where  is the probability of ,  is the probability of the generated voice, and  is the joint probability of the signals.\\
The joint probability can be written explicitly as,

Plugging in the known distributions reads

The distribution of  can be formulated using marginalization of the joint probability, 

Recall that the generated voice without noise can at best (maximum) obtain the mutual information between the mixture and the sources. 
Let  be the ratio between the mutual information of the source and the generative estimation, and the mutual information of the source and mixture, i.e.,    . 
The factor  is computed numerically from equation Eq.~\ref{eq:numeric}, by integrating the double integral. 
\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\includegraphics[trim={0 .2cm 0 0},clip,width=.88565\linewidth]{figures/Mutual_info.png}
    \caption{ as a function of the noise variance.} \label{fig:mut_numeric}
    \end{wrapfigure}
Figure~\ref{fig:mut_numeric} depicts the ratio  as defined above. This ratio is computed for numerous sources. Each trial is with a different color.
Note that for low-quality generative models (i.e, ) the ratio degrades at a faster rate when the number of sources is lower.
Recall that when the number of sources is larger, the mixture tends to have a normal distribution. Thus, the addition of another independent noise with a smaller variance has a negligible effect in terms of the mutual information (which is already low).
Observe that  is approximately one when . Since, for any well-functioning pretrained GM, the error in reconstructing the audio signal is below  \cite{kong2020diffwave,chen_wavegrad_2020,chen2021wavegrad}, we obtain that 


Therefore, rewriting Eq.~\ref{eq:intermiddiate_result} with the definition of  reads,


Using the result of Eq.~\ref{eq:approximate_rho} one obtains,

which implies that the bound is tight. 
Also, from the bound, under the reasonable assumptions made, the deterministic approximation and the generative approximations would have similar error magnitudes.

Finally, updating Eq.~\ref{eq:init_bound} with the new bound obtains,



One can see a maximum addition of 3dB to the upper bound when combining both generative results with deterministic processing. Note that this bound is tight in the sense that the worst-case additive independent noise is taken into account and still the achievable mutual information is at maximum. 

\section{Method}
\begin{figure*}
    \centering
\includegraphics[trim={0 2cm 0 .45cm},clip, width=1\linewidth,height=0.20\linewidth]{figures/arch.png}
    \caption{The suggested system architecture. Red blocks indicate pretrained models, Yellow blocks indicate non-learned operations, and Green blocks indicate learned neural models.}
    \label{fig:arc}
\end{figure*}
Given a mixture signal , the deterministic estimation  is obtained using the backbone network .
Then, a Mel-Spectrogram is computed over all  estimations. \\


Using a pretrained vocoder, , specifically DiffWave, and the priors obtained from the deterministic backbone , a generative estimation is obtained .


Since the generative vocoder is not given any phase information, the generated signal has a phase shift that can vary over time. To combine both estimations, an alignment procedure is employed.

In order to align the signals, both estimations are transformed into the frequency domain, where the aligning operation dual is multiplication by a phasor. The transformation is done through a short-time Fourier transform with  frequency bins and  segments.

Denote the spectrogram of  and , as ,  respectively, . The absolute phase of  is of no importance; what is important is its trend over short segments.
The phase of  and the relative phase between  and  is concatenated into a 2-channel tensor, , i.e.,


where  is the Hadamard product, the star superscript denotes conjugate, and the following definition is used.
\begin{definition}
    The angle, denoted as , of a complex number is computed by
    
    where ,  is the imaginary part, and  is the real part.
\end{definition}


In addition to the relative phase tensor , the magnitude of both  and  is concatenated into tensor, ,


The alignment network that we train  is a dual 6-layer convolutional neural network, with residual connections \cite{he2016deep}, termed ResNet Head in Fig.~\ref{fig:arc}. 
The magnitude and phase heads share the same hyperparameters. The processed tensors are combined into the complex factor ,

where  and , both in , represent the magnitude and phase, respectively, of a complex tensor . This dual representation is used as an effective representation for complex tensors.  is then computed explicitly by combining the magnitude and phase 

Below, we denote the different channels of  by a subscript . Define as  and the second channel as . 

 and  are the coefficients used in Eq.~\ref{eq:combine}. Written more explicitly, the weighted sum of  and  is computed as


Finally, the time-domain signal is obtained by employing the inverse short-time Fourier transform.



\paragraph{Objective Function}

Following  \cite{luo2019conv,subakan2021attention,sepit,scheibler2022diffusion}, the objective function is the Scale-Invariant SDR. 
The scale-invariant SDR is agnostic to the scale of the estimated signal, and also to spurious errors, as presented in~\cite{le2019sdr}.
First, project the source onto the estimated signal.

Second, compute the normalized error

Third, the scale-invariant SDR is computed as follows,


For a large number of speakers (), the Hungarian algorithm obtains better results than the permutation invariant loss, by explicitly assigning the different estimated sources to the ground truth sources~\cite{dovrat2021many}. Therefore, network  is trained using the assignment obtained by the Hungarian method for the deterministic voice separation network. 

\section{Experiments}
\label{sec:experiments}


For all datasets, the same alignment network architecture is employed, with the same hyper-parameters. It is a CNN with six layers,  kernels, and 32,32,64,64,64 hidden channels. For Librimix and WSJ0 datasets the DiffWave was trained separately over the training sets' sources (no mixing). The pretrained models for separation are taken from the official publication when available and from HuggingFace hub otherwise. The optimization procedure is done with Adam \cite{kingma2014adam} optimizer with a learning rate of 1E-3 and a batch size of 3. The setting involved 3 GPUs each Nvidia A5000.

\smallskip
{\bf Datasets\quad}The LibriSpeech dataset \cite{panayotov2015librispeech} is a large corpus of read English speech, designed for training and evaluating automatic speech recognition systems. It consists of over 1000 hours of audio from audiobooks read by professional and non-professional speakers.

The Wall Street Journal (WSJ0) \cite{wsj0} dataset is a collection of read English speech samples and is widely used by the research community. The dataset includes a total of 80 hours of audio.

 Following previous work \cite{subakan2021attention,dovrat2021many}, mixtures are generated by a random process. The speakers are divided into training speakers and test speakers. A training- or a test sample is created by combining random speakers out of the respective set of speakers, with random SNR values between  dB. 

\smallskip
{\bf Baseline methods\quad} We compare our method to the state-of-the-art methods in voice separation, including DiffSep~\cite{scheibler2022diffusion} that employs diffusion models, SepFormer~\cite{subakan2021attention}, which is a transformer model, and SepIt~\cite{sepit}, which extends the Gated-LSTM method~\cite{pmlr-v119-nachmani20a} by running it in an iterative manner.

As a generative model in our experiments, we employ a pretrained DiffWave model~\cite{kong2020diffwave}. 

In addition to running our method, we perform an ablation that is aimed at verifying the need for a learned combination model . In this ablation, we align (recover the phase) the generative approximation with the deterministic one and then average the two. Another ablation is using the GAN-based model HiFiGAN~\cite{kong2020hifi}, trained on WSJ0, which is a deterministic generative model (no noise is given to the generator). Since this generative model is deterministic the previous upper bound applies and we verify experimentally that no significant improvement is obtained. 

First, the cross-correlation function is used as a measure of similarity. In the frequency domain, the cross-correlation dual is computed using conjugate multiplication.

where  is the frequency domain. For each segment, the inverse Fourier transform is conducted, and then the time for which the signal is at maximum is searched.

where  is the time delay within the i-th segment,  is the i-th frequency domain cross correlation segment.

A complex phase shift corresponding to the time delay found per segment is computed via the following,

where  is the complex Fourier factor of time delay, and  is the frequency that matches each frequency bin in the spectral transform.

The combination of the two estimations now reads,


The time domain signal is obtained as before, using the inverse short-time Fourier transform.




\begin{figure*}[t]
\begin{tabular}{@{}ccc@{}}
     \includegraphics[width=0.31295\linewidth]{figures/both_signals.png}&
     \includegraphics[width=0.31295\linewidth]{figures/zoom_in.png}&
     \includegraphics[width=0.31295\linewidth]{figures/sudden_error.png}\\
     (a)&(b)&(c)\\
\end{tabular}
    \caption{(a) Visualization of , and , (b) A zoom-in that shows the phase difference between two estimations. Blue-, Orange-. (c) An example where  is more precise than , and the resulting combination that tracks the source . Blue-, Orange-, Yellow-, Purple-.}
    \label{fig:maj_imp}
    \label{fig:vis_zoom}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{minipage}{.66\linewidth}\begin{tabular}{cc}
\includegraphics[width=.46346791\linewidth]{figures/histogram_error_per_frame_vd_vg.png}&
   \includegraphics[width=.46346791\linewidth]{figures/scatter_plot.png}\\
   (a) & (b)\\
   \end{tabular}
\end{minipage}\hfill \begin{minipage}{.30\linewidth}\caption{Comparing the error rates of the deterministic and the generative parts. (a) The histograms of the Mean Square Error (MSE) between  to  (Blue), and  to  (Orange). (b) Scatter plot of the MSE for  (x-axis) vs.  (y-axis).}
    \label{fig:hist_error}
    \label{fig:scatter_plot}
    \end{minipage}
\end{figure*}
\begin{table*}[t]
\caption{The performance obtained by our method on the various WSJ0 and LibriSpeech benchmarks. The reported values are SI-SDRi  (higher is better).  -F refers to cross-correlation ablation, where the mixing network  is not used. These missing runs of applying our method to baseline models are due to the inability to obtain the pre-trained baseline models.}
\label{tab:result_wsj0}
\label{tab:result_25spkrs}
\label{tab:result_1020spkrs}
\begin{center}
 \begin{tabular}{lcccccc} 
 \toprule
& \multicolumn{2}{c}{WSJ0} & \multicolumn{4}{c}{LibriSpeech}\\
 \cmidrule(lr){2-3}
 \cmidrule(lr){4-7}Method&  2Mix &  3Mix & 2Mix &  5Mix &10Mix &  20Mix \\
\midrule
Classical Upper Bound (Lutati et al.) &23.1 & 21.2 & 23.1 & 14.5 & 12.0 & 8.0\\
Generative Upper Bound (ours)&26.1 &24.2& 26.1 & 17.5 & 15.0 & 11.0\\
\midrule
DiffSep~\cite{scheibler2022diffusion}& 14.3&-&-&-&-&-\\
SepIt~\cite{sepit} & 22.4 & 20.1 &- & 13.7 & 8.2 & -\\
\midrule
SepFormer~\cite{subakan2021attention} & 22.3 & 19.8 & 20.6& -&- &-\\
SepFormer + HiFiGAN~\cite{kong2020hifi} (ablation)& 22.3 & 20.0 & - & -& -&-\\
SepFormer + DiffWave -F (ablation) & 22.6 & 20.3 & 20.8 & -& -&-\\
\textbf{SepFormer + DiffWave (ours)} & \textbf{23.9} & \textbf{20.9} & \textbf{21.5} &- &- &-\\
\midrule
Gated LSTM~\cite{pmlr-v119-nachmani20a} & 20.1&16.9 & - & 12.7 &7.7 & 4.3\\
Gated LSTM + DiffWave -F (ablation) &-&-&- &13.0 & 8.1 & 4.5\\
\textbf{Gated LSTM + DiffWave  (ours)} &- &- &-& \textbf{14.2} & \textbf{9.0} & \textbf{5.2}\\
 \bottomrule
\end{tabular}
\end{center}
\end{table*}

\paragraph{Qualitative Results}
A visualization of  and  is depicted in Fig.~\ref{fig:vis_zoom}. Evidently, both signals have a phase difference, which changes over time. For example, in panel (b), at [0.52,0.56] the phase of the generative estimation tracks the phase of the deterministic model. At the other end, at [0.56,0.64], the phase between the signal is offset by . Recall that while  is given the full information about the phase,  is given only magnitude information and thus the absolute phase is not synched. Therefore, the alignment procedure is necessary.

 The generated signal, , does not track the deterministic part, , one-to-one, but sometimes fixes sudden errors of other sources that the deterministic model could not handle correctly. This phenomenon is depicted in Fig.~\ref{fig:maj_imp}(c), where the deterministic signal has some bursts. As can be seen, the combined signal is more related to the ground truth signal. 





The supplementary presents samples of both the deterministic and the generative estimation. While the deterministic estimation presents less background noise, it misses multiple temporal parts, which are filtered out by the consecutive chain of static filters of the deterministic model. The signal produced by the generative model, on the other hand, maintains a considerable amount of background noise but is able to sample parts that were previously omitted. The combination of both estimations yields a signal that is closer to the desired source.
 
 To estimate the relative quality of the deterministic and generative estimations, the Mean Square Error between the different estimations and the aligned sources is calculated in short segments of [ms]. The histogram depicted in Fig.~\ref{fig:hist_error}(a) shows that the generative signal  has a slight tendency to greater error, but has the same performance overall. This is in agreement with Eq.~\ref{eq:complete_ineq_done}, where we expect that both , and  obtain similar performances. This result confirms that a combination of both estimations should improve the overall estimation of sources. A scatter plot of the errors is depicted in Fig.~\ref{fig:scatter_plot}(b). The red line indicates a ratio of one to one between the deterministic error and the generative one. As can be seen, the error of the generative estimation is within the same range as that of the deterministic estimation, and there is no estimation that is markedly better than the other.














\noindent{\bf Source separation results\quad} The voice separation results for all the benchmarks are depicted in Tab.~\ref{tab:result_1020spkrs}. For the {WSJ0} dataset, 
when separating a mixture of two sources, the improvement in terms of SI-SDR is 1.5dB over the current state-of-the-art. Using our method not only improved the SI-SDR beyond all other methods, but also broke the previous bound for deterministic models. This emphasizes the strength of the diffusion models as holding independent modeling benefits that can be used to improve speech separation. For three sources, separation is improved by 1dB, again obtaining state-of-the-art results, but somewhat lower than the classical bound of deterministic methods. We note that the ablation that removes network  and introduces a heuristic alignment and equal mixing performs considerably better than the deterministic part (the SepFormer baseline), but only slightly better than the iterative SepIt.
The deterministic HiFiGAN ablation shows little to no improvement, supporting the divide we make between deterministic and nondeterministic models.

For {LibriSpeech}  we use an available SepFormer model as a baseline in the case of two speakers, and for the other mixtures, an available Gated LSTM model. As can be seen, for all numbers of sources, a major improvement is obtained over the current state of the art. The alignment ablation degrades the separation result and is inferior to the learned method, but is still competitive. Evidently, there is still room for improving separation methods. For five speakers, the result obtained  falls 0.3dB short of surpassing the classical upper bound. For other mixtures, the gap is larger.






\section{Limitations}

The new bound we propose applies to any possible non-deterministic model. A tighter bound can be obtained, for example, in a bound that is developed specifically for diffusion models. Its non-specific nature also means that it does not provide practical guidelines as to the development of specific methods, with the exception that it encourages the addition of non-deterministic post-processing by pointing out the limited nature of the existing bound. The bound is developed, similar to the deterministic bound, in the context of algorithms that work on chunks of a limited size. As the community focuses on efficient attention models for large contexts, new methods can emerge for which the bound would not hold. 

\section{Conclusions}
A general upper bound for source separation using generative models is proposed.
The generalization of the upper bound suggests that for a pretrained generative model, a maximal improvement of 3dB from the previous deterministic bound is achievable.
In addition, a simple yet effective method is suggested, where the deterministic signals and the generated signals are combined in the frequency domain.
The combination procedure is learned and shown to be superior to the classical alignment method. When tested on various numbers of speakers, the estimation always yields improvement. For two sources, the separation is able to surpass the previous upper bound, demonstrating the need to go beyond a deterministic model.

The upper bound derived here is general in the sense that a similar bound would hold when the underlying distributions are replaced to fit other data domains. This is left for future work.

\section{Acknowledgments}
This work was supported by a grant from the Tel Aviv University Center for AI and Data Science
(TAD), and the Blavatnik Family Foundation. The contribution of Shahar Lutati is part of a Ph.D. thesis research
conducted at Tel Aviv University.

\bibliography{refs}
\bibliographystyle{plain}
\newpage
\appendix
\section{Proof of Lemma 3.2}

The proof of Lemma 3.2 in the main text is below.
\begin{proof}
Using the definition of mutual information, and chain rule for entropy

\label{proof:lemma}
\end{proof}

\section{Sample spectrograms}
\label{app:spec_graphs}

Figure~\ref{fig:spec_graphs} presents sample spectrograms. Shown are the source, the deterministic estimation, the estimation obtained by the generative model, and their combination in the frequency domain using the coefficients produced by the function .

\begin{figure*}[t]
\centering
  \begin{tabular}{cc}
     \includegraphics[width=0.50\linewidth]{figures/source_spec_b.png}&
     \includegraphics[width=0.50\linewidth]{figures/v_d_spec_b.png}\\
     (a)&(b)\\
     \includegraphics[width=0.50\linewidth]{figures/v_g_spec_b.png}&
     \includegraphics[width=0.50\linewidth]{figures/bar_v_spec_b.png}\\
     (c)&(d)
  \end{tabular}
    \caption{(a) Spectrogram of a source. (b) Spectrogram of the deterministic estimation  (c) Spectrogram of the generated voice  (d) Spectrogram of the combined estimation .}
    \label{fig:spec_graphs}
\end{figure*}




































\end{document}
