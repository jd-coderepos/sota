\documentclass{bmvc2k}



\title{How to Train Your Energy-Based Model\\ for Regression}


\addauthor{Fredrik K.~Gustafsson}{fredrik.gustafsson@it.uu.se}{1}
\addauthor{Martin Danelljan}{martin.danelljan@vision.ee.ethz.ch}{2}
\addauthor{Radu Timofte}{radu.timofte@vision.ee.ethz.ch}{2}
\addauthor{Thomas B.~Sch\"on}{thomas.schon@it.uu.se}{1}

\addinstitution{
Department of Information Technology\\
Uppsala University\\
Sweden
}
\addinstitution{
Computer Vision Lab\\
ETH Z\"urich\\
Switzerland
}

\runninghead{F.K. Gustafsson et al.}{How to Train Your EBM for Regression}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\usepackage{xcolor, soul}
\sethlcolor{pink}

\newcommand{\MD}[1]{{\color{purple}\textbf{[MD:} \textit{#1}\textbf{]}}}

\usepackage{xparse, mathtools, array}
\DeclarePairedDelimiterX{\rvect}[1]{[}{]}{\,\makervect{#1}\,}
\ExplSyntaxOn
\NewDocumentCommand{\makervect}{m}
 {
  \seq_set_split:Nnn \l_tmpa_seq { , } { #1 }
  \begin{matrix}
  \seq_use:Nn \l_tmpa_seq { & }
  \end{matrix}
 }
\ExplSyntaxOff

\newcommand{\Transp}{\mathsf{T}}



\usepackage[title]{appendix}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\algnewcommand\Or{\textbf{or}}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{amssymb}

\newcommand{\parsection}[1]{\vspace{2mm}\noindent\textbf{#1}~ }

\usepackage{pgfplots}
    \usepgfplotslibrary{colorbrewer}
    \pgfplotsset{
cycle list/Dark2,
cycle multiindex* list={
            mark list*\nextlist
            Dark2\nextlist
        },
    }
\pgfplotsset{compat=1.14}
















\begin{document}

\maketitle

\begin{abstract}
Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves  AUC on LaSOT and  Success on TrackingNet. Code is available at \url{https://github.com/fregu856/ebms_regression}. 
\end{abstract}






\section{Introduction}
\label{section:introduction}
Energy-based models (EBMs)~\cite{lecun2006tutorial} have a rich history in machine learning \cite{teh2003energy, bengio2003neural, mnih2005learning, hinton2006unsupervised, osadchy2005synergistic}. An EBM specifies a probability density  directly via a parameterized scalar function . By defining  using a deep neural network (DNN),  becomes expressive enough to learn practically any density from observed data. EBMs have therefore become increasingly popular within computer vision in recent years, commonly being applied for various generative image modeling tasks \cite{xie2016theory, gao2018learning, nijkamp2019learning, du2019implicit, Grathwohl2020Your, nijkamp2019anatomy, gao2019flow}. 

Recent work \cite{gustafsson2019learning, danelljan2020probabilistic} has also explored conditional EBMs as a general formulation for regression, demonstrating particularly impressive performance on the tasks of object detection \cite{Ren2015FasterRT, law2018cornernet, zhou2019bottom} and visual tracking \cite{SiamRPN++, danelljan2019atom, bhat2019learning}. Regression entails predicting a continuous target~ from an input , given a training set of observed input-target pairs. This was addressed in \cite{gustafsson2019learning, danelljan2020probabilistic} by learning a conditional EBM , capturing the distribution of the target value  given the input . At test time, gradient ascent was then used to maximize  w.r.t.\ , producing highly accurate predictions. Regression is a fundamental problem within computer vision with many additional applications \cite{lathuiliere2019comprehensive, xiao2018simple, yang2019fsa, rothe2016deep, pan2018mean}, which all would benefit from such accurate predictions. In this work, we therefore study the use of EBMs for regression in detail, aiming to further improve its performance and applicability.

While the modeling capacity of EBMs makes them highly attractive for many applications, training EBMs is known to be challenging. This is because the EBM  involves an intractable integral, complicating the use of standard maximum likelihood (ML) learning. A variety of different techniques have therefore been explored in the generative modeling literature, including alternative estimation methods \cite{gutmann2010noise, gao2019flow, hyvarinen2005estimation, vincent2011connection, song2019generative} and approximations based on Markov chain Monte Carlo (MCMC) \cite{hinton2002training, du2019implicit, nijkamp2019learning, nijkamp2019anatomy}. The application of EBMs for regression is however not a particularly well-studied problem. \cite{gustafsson2019learning, danelljan2020probabilistic} both applied importance sampling to approximate intractable integrals, an approach known to scale poorly with the data dimensionality, and considered no alternative techniques. How EBMs  should be trained for best possible performance on computer vision regression tasks is thus an open question, which we set out to investigate in this work.

\parsection{Contributions}
We propose a simple yet highly effective extension of noise contrastive estimation (NCE) \cite{gutmann2010noise} to train EBMs  for regression tasks. Our proposed method, termed \textit{NCE+}, can be understood as a direct generalization of NCE, accounting for noise in the annotation process. We evaluate NCE+ on illustrative 1D regression problems and on the task of bounding box regression in object detection. We also provide a detailed comparison of NCE+ and \emph{six} popular methods from previous work, the results of which suggest that NCE+ should be considered the go-to training method. Lastly, we apply our proposed NCE+ to the task of visual tracking, achieving state-of-the-art results on \emph{five} common datasets. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8925\textwidth]{figures/img_combined6.jpg}\vspace{-2.0mm}
    \caption{We propose NCE+ to train EBMs  for tasks such as bounding box regression. NCE+ is a highly effective extension of NCE, accounting for noise in the annotation process of real-world datasets. Given a label  (red box), the EBM is trained by having to discriminate between  (yellow box) and noise samples  (blue boxes).}\vspace{-3mm}
    \label{fig:overview}
\end{figure} \section{Energy-Based Models for Regression}
\label{section:ebms}
We study the application of EBMs to important regression tasks in computer vision, using energy-based models of the conditional density . Here, we first define the general regression problem and our employed EBM in Section~\ref{section:ebms-problem}. Our prediction strategy based on gradient ascent is then described in Section~\ref{section:ebms-prediction}. Lastly, we discuss the challenges associated with training EBMs, and describe six popular methods from the literature, in Section \ref{section:ebms-training}.











\subsection{Problem \& Model Definition}
\label{section:ebms-problem}
In a supervised regression problem, we are given a training set  of i.i.d.\ input-target pairs, , . The task is then to learn how to predict a target  given a new input . The target space  is continuous,  for some , and the input space  usually corresponds to the space of images.

As in \cite{gustafsson2019learning, danelljan2020probabilistic}, we address this problem by creating an energy-based model  of the conditional target density . To that end, we specify a DNN  with model parameters . This DNN directly maps any input-target pair  to a scalar . The model  of the conditional target density is then defined as,

where the DNN output  is interpreted as the negative energy of the density, and  is the input-dependent normalizing partition function. Since  in (\ref{eq:ebm_def}) is directly defined by the DNN , minimal restricting assumptions are put on the true . The predictive power of the DNN can thus be fully exploited, enabling learning of, \eg, multi-modal and asymmetric densities directly from data. This expressivity however comes at the cost of  being intractable, which complicates evaluating or sampling from . 






















\subsection{Prediction}
\label{section:ebms-prediction}
At test time, the problem of predicting a target value  from an input  corresponds to finding a point estimate of the predicted conditional density . The most natural choice is to select the most likely target under the model, . The prediction  is thus obtained by directly maximizing the DNN scalar output  w.r.t.\ , not requiring  to be evaluated nor any samples from  to be generated. Following \cite{gustafsson2019learning, danelljan2020probabilistic}, we estimate  by performing gradient ascent to refine an initial estimate  and find a local maximum of . Starting at , we thus run  gradient ascent iterations, , with step-length . An algorithm for this prediction procedure is found in the supplementary material.






















\subsection{Training}
\label{section:ebms-training}
To train the DNN  specifying the EBM (\ref{eq:ebm_def}), different techniques for fitting a density  to observed data  can be used. In general, the most commonly applied such technique is ML learning, which entails minimizing the negative log-likelihood (NLL), 

w.r.t.\ the parameters . The integral in (\ref{eq:ebm_nll}) is however intractable, and exact evaluation of the NLL is thus not possible. \cite{gustafsson2019learning,danelljan2020probabilistic} employed importance sampling to approximate such intractable integrals, obtaining state-of-the-art performance on object detection and visual tracking. Recent work \cite{gao2019flow, saremi2018deep, li2019annealed, du2019implicit, nijkamp2019anatomy, Grathwohl2020Your} on generative image modeling has however applied a variety of different training methods not considered in \cite{gustafsson2019learning,danelljan2020probabilistic}, including the ML learning alternatives NCE~\cite{gutmann2010noise} and score matching~\cite{hyvarinen2005estimation}. How we should train the DNN  to obtain best possible regression performance is thus unclear. In this work, we therefore carefully compare our proposed method to six popular training methods from the literature.








\parsection{ML with Importance Sampling (ML-IS)}
A straightforward training method is proposed in \cite{gustafsson2019learning}, which we term \textit{ML with Importance Sampling (ML-IS)}. Using ML-IS, \cite{gustafsson2019learning} successfully applied the EBM (\ref{eq:ebm_def}) to the regression tasks of object detection, visual tracking, age estimation and head-pose estimation. In ML-IS, the DNN  is trained by directly minimizing the NLL (\ref{eq:ebm_nll}) w.r.t.\ , using importance sampling to approximate the intractable integral,

Here,  are  samples drawn from a proposal distribution  that depends on the ground truth target . In \cite{gustafsson2019learning},  is set to a mixture of  Gaussians centered at ,

The loss  is obtained by averaging over all pairs  in the current mini-batch,









\parsection{KL Divergence with Importance Sampling (KLD-IS)}
Instead of minimizing the NLL~(\ref{eq:ebm_nll}), \cite{danelljan2020probabilistic} considers the Kullback-Leibler (KL) divergence  between the EBM  and an assumed density  of the true target  given the label . The density  models noise in the annotation process of our given training set . In \cite{danelljan2020probabilistic}, , where  is a hyperparameter. As shown in \cite{danelljan2020probabilistic},

where  is a constant that does not depend on . \cite{danelljan2020probabilistic} approximates the integrals in (\ref{eq:kld-is_kldiv}) using importance sampling, employing the ML-IS proposal  in (\ref{eq:mle-is_proposal}). By then averaging over all pairs  in the current mini-batch, the loss  used to train  is obtained as,

where  are  samples drawn from the proposal . We term this training method \textit{KL Divergence with Importance Sampling (KLD-IS)}. When applied to visual tracking in \cite{danelljan2020probabilistic}, KLD-IS outperformed ML-IS and set a new state-of-the-art.








\parsection{ML with MCMC (ML-MCMC)}
To minimize the NLL (\ref{eq:ebm_nll}) w.r.t.\ the parameters , the following identity for the expression of the gradient  can be utilized \cite{lecun2006tutorial},
 
The expectation in (\ref{eq:mle-mcmc_gradient}) is then approximated using samples  drawn from , \textit{i.e.} from the EBM itself. To obtain each sample , MCMC is used. Specifically, we follow recent work \cite{xie2016theory, gao2018learning, du2019implicit, nijkamp2019learning, nijkamp2019anatomy, Grathwohl2020Your} on generative image modeling and run  steps of Langevin dynamics~\cite{welling2011bayesian}. Starting at , we thus update  according to,

and set . Here,  is a small constant step-length. Following the principle of contrastive divergence~\cite{lecun2006tutorial, hinton2002training, teh2003energy}, we start the Markov chain~(\ref{eq:mle-mcmc_langevin_dynamics}) at the ground truth target, . By approximating (\ref{eq:mle-mcmc_gradient}) with the samples , and by averaging over all pairs  in the current mini-batch, the loss  used to train the DNN  is obtained as,

We term this specific training method \textit{ML with MCMC (ML-MCMC)}.






\parsection{Noise Contrastive Estimation (NCE)}
As an alternative to ML learning, Gutmann and Hyv{\"a}rinen proposed NCE~\cite{gutmann2010noise} for estimating unnormalized parametric models. NCE entails generating samples from some noise distribution , and learning to discriminate between these noise samples and observed data examples. It has recently been applied to generative image modeling with EBMs \cite{gao2019flow}, and the NCE loss is also utilized in various frameworks for self-supervised learning \cite{hjelm2018learning, bachman2019learning, chen2020simple}. Moreover, NCE has been applied to train EBMs for supervised \emph{classification} tasks within language modeling \cite{mnih2012fast, mikolov2013distributed, jozefowicz2016exploring, ma2018noise}, where the target space  is a large but finite set of possible labels. We adopt NCE for regression by using a noise distribution  of the same form as the ML-IS proposal in~(\ref{eq:mle-is_proposal}),

and by employing the ranking NCE objective~\cite{jozefowicz2016exploring}, as described in \cite{ma2018noise}. We choose ranking NCE over the binary objective since it is consistent under a weaker assumption \cite{ma2018noise}. We thus define , and train the DNN  by minimizing the following loss,

where  are  noise samples drawn from  in (\ref{eq:nce_noise}).







\parsection{Score Matching (SM)}
Another alternative estimation method is score matching (SM), as proposed by Hyv{\"a}rinen~\cite{hyvarinen2005estimation} and further studied for supervised problems in \cite{sasaki2018neural}. The method focuses on the \emph{score} of , defined as , aiming for it to approximate the score of the true target density . Note that the EBM score  does not depend on the intractable . SM was applied to simple conditional density estimation problems in \cite{sasaki2018neural}, using a combination of feed-forward networks and reproducing kernels to specify the EBM. Following \cite{sasaki2018neural}, we train the DNN  by minimizing the loss, 

where only the diagonal of  actually is needed to compute the first term.











\parsection{Denoising Score Matching (DSM)}
By modifying the SM objective, denoising score matching (DSM) was proposed by Vincent~\cite{vincent2011connection}. DSM does not require computation of any second derivatives, improving its scalability to high-dimensional data. The method entails employing SM on noise-corrupted data points. Recently, DSM has been successfully applied to generative image modeling \cite{saremi2018deep, song2019generative, li2019annealed}. DSM was also extended to train EBMs of conditional densities in \cite{khemakhem2020ice}, where it was applied to a transfer learning problem. Following \cite{khemakhem2020ice}, we use a Gaussian noise distribution and train the DNN  by minimizing the loss,

where  are  samples drawn from the noise distribution . \section{Proposed Training Method}
\label{section:method}
To train the DNN  specifying our EBM  in (\ref{eq:ebm_def}), we propose a \emph{simple yet highly effective} extension of NCE~\cite{gutmann2010noise}. Motivated by the improved performance of KLD-IS compared to ML-IS on visual tracking \cite{danelljan2020probabilistic}, we extend NCE with the capability to model annotation noise. To that end, we adopt the standard NCE noise distribution  (\ref{eq:nce_noise}) and loss (\ref{eq:nce_loss}), but instead of defining , we sample  and define . The distribution  is a zero-centered version of  in which  are scaled with ,

Instead of training the DNN  by learning to discriminate between noise samples  and the label , it thus has to discriminate between the samples  and . Examples of  and  in the task of bounding box regression are visualized in Figure~\ref{fig:overview}. Similar to KLD-IS, in which an assumed density of the true target value  given  is employed, our approach thus accounts for possible noise and inaccuracies in the provided label . Specifically, our proposed training method entails sampling  and , setting , and minimizing the following loss,

As , samples  will concentrate increasingly close to zero, and the standard NCE method is in practice recovered. Our proposed training method can thus be understood as a direct generalization of NCE. Compared to NCE, our method adds no significant training cost and requires tuning of a single additional hyperparameter . A value for  is selected in a simple two-step procedure. First, we fix  and select the standard deviations  based on validation set performance, just as in NCE. We then fix  and vary  to find the value corresponding to maximum validation performance. Typically, we start this ablation with . We term our proposed training method \textit{NCE+}. \section{Comparison of Training Methods}
\label{section:comparison}
We provide a detailed comparison of the six training methods from Section~\ref{section:ebms-training} and our proposed NCE+. To that end, we perform extensive experiments on 1D regression (Section~\ref{section:comparison-1DRegression}) and object detection (Section~\ref{section:comparison-ObjectDetection}). Our findings are summarized in Section~\ref{section:comparison-Observations}. All experiments are implemented in PyTorch~\cite{paszke2019pytorch} and the code is publically available. For both tasks, further details and results are also provided in the supplementary material.




















\subsection{1D Regression Experiments}
\label{section:comparison-1DRegression}
We first perform experiments on illustrative 1D regression problems. The DNN  is here a simple feed-forward network, taking  and  as inputs. We employ two synthetic datasets, and evaluate the training methods by how well the learned model  (\ref{eq:ebm_def}) approximates the known ground truth , as measured by the KL divergence .



\parsection{Results}
A comparison of all seven training methods in terms of  and training cost (seconds per epoch) is found in Table~\ref{tab:comparison_1dregression}. For ML-MCMC, we include results for  Langevin steps (\ref{eq:mle-mcmc_langevin_dynamics}). We observe that ML-IS, KLD-IS, NCE and NCE+ clearly have the best performance. While ML-MCMC is relatively close in terms of  for , this comes at the expense of a massive increase in training cost. DSM outperforms SM in terms of both metrics, but is not close to the top-performing methods. The four best methods are further compared in Figure~\ref{fig:comparison_1dregression_performance_vs_M}, showing  as a function of . Here, we observe that NCE and NCE+ significantly outperform ML-IS and KLD-IS for small number of samples .

\begin{table}[t]
\centering
	\resizebox{1.0\textwidth}{!}{













\begin{tabular}{l@{\hspace{0.35cm}}ccccccccc}
\toprule
 &ML-IS &ML-MCMC-1 &ML-MCMC-16 &ML-MCMC-256 &KLD-IS &NCE &SM &DSM &\\
\midrule
          &\textbf{0.062} &0.865 &0.449 &0.106 &0.088 &0.068 &0.781 &0.395 &0.066\\
Training Cost   &\textbf{0.44} &0.54 &2.41 &30.8 &\textbf{0.44} &0.45 &0.60 &0.47 &0.46\\
\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
	\caption{Comparison of training methods for the illustrative 1D regression experiments.}\vspace{-1.5mm}
	\label{tab:comparison_1dregression}
\end{table} 

\begin{figure}\begin{minipage}{0.475\textwidth}\centering
        \begin{tikzpicture}[scale=0.655, baseline]
            \begin{axis}[
                xmode=log,
                log ticks with fixed point,
                xlabel={Number of samples },
                ylabel={},
xtick={1, 4, 16, 64, 256, 1024},
                xticklabels={1, 4, 16, 64, 256, 1024},
legend pos=north east,
                ymajorgrids=true,
grid style=dashed,
                every axis plot/.append style={thick},
            ]
            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
1 3.1431\\
            4 1.449767\\
            16 0.13682405\\
            64 0.07923745\\ 
            256 0.06401\\
            1024 0.062326016666666664\\
            };
            \addlegendentry{ML-IS}


            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
            1 2.512175\\
            4 0.9612085\\
            16 0.4151015\\ 
            64 0.21802749999999999\\
            256 0.12049625\\ 1024 0.08826348333333334\\
            };
            \addlegendentry{KLD-IS}


            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
            1 0.48795700000000003\\
            4 0.222217\\
            16 0.12022195\\
            64 0.07445745\\
            256 0.0630213\\ 
            1024 0.06818718333333332\\
            };
            \addlegendentry{NCE}


            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
            1 0.41710800000000003\\
            4 0.1957025\\ 
            16 0.10278555\\
            64 0.0797574\\ 
            256 0.0631705\\ 
            1024 0.06565895\\
            };
            \addlegendentry{\textbf{NCE+}}


            \end{axis}
        \end{tikzpicture}\caption{Detailed comparison of the top-performing methods for the illustrative 1D regression experiments. NCE and NCE+ here demonstrate clear superior performance for small number of samples .}\vspace{-3mm}
        \label{fig:comparison_1dregression_performance_vs_M}\end{minipage}
    \quad
    \begin{minipage}{0.475\textwidth}\begin{tikzpicture}[scale=0.655, baseline]
                \begin{axis}[
                    xmode=log,
                    log ticks with fixed point,
                    xlabel={Number of samples },
                    ylabel={AP (\%)},
ymin=32.5, ymax=39.73,
                    xtick={1, 2, 4, 8, 16, 32, 64, 128},
                    ytick={33, 34, 35, 36, 37, 38, 39},
                    legend pos=south east,
                    ymajorgrids=true,
grid style=dashed,
                    every axis plot/.append style={thick},
                ]
                \addplot
                 plot [error bars/.cd, y dir = both, y explicit]
                 table[row sep=crcr, x index=0, y index=1]{
8 33.06\\
                16 36.33\\
                32 37.68\\
                64 38.96\\
                128 39.11\\
                };
                \addlegendentry{ML-IS}
                
                \addplot
                 plot [error bars/.cd, y dir = both, y explicit]
                 table[row sep=crcr, x index=0, y index=1]{
4 32.77\\
                8 38.32\\
                16 38.81\\
                32 39.10\\
                64 39.21\\
                128 39.37\\
                };
                \addlegendentry{KLD-IS}
                
                \addplot
                 plot [error bars/.cd, y dir = both, y explicit]
                 table[row sep=crcr, x index=0, y index=1]{
                1 37.82\\
                2 38.16\\
                4 38.46\\
                8 38.75\\
                16 38.81\\
                32 38.91\\
                64 39.07\\
                128 39.17\\
                };
                \addlegendentry{NCE}
                
                \addplot
                 plot [error bars/.cd, y dir = both, y explicit]
                 table[row sep=crcr, x index=0, y index=1]{
                1 37.93\\
                2 38.27\\
                4 38.53\\
                8 38.82\\
                16 39.01\\
                32 39.18\\
                64 39.29\\
                128 39.36\\
                };
                \addlegendentry{\textbf{NCE+}}
                
                \end{axis}
            \end{tikzpicture}\caption{Detailed comparison of the top-performing methods for object detection, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}. Missing values for ML-IS and KLD-IS correspond to failed training due to numerical issues.}\vspace{-3mm}
        \label{fig:comparison_detection_AP_vs_M}\end{minipage}\end{figure}










\subsection{Object Detection Experiments}
\label{section:comparison-ObjectDetection}
Next, we evaluate the methods on the task of bounding box regression in object detection. We employ an identical network architecture for  as in \cite{gustafsson2019learning}. An extra network branch, consisting of three fully-connected layers with parameters , is thus added onto a pre-trained and fixed FPN Faster-RCNN detector~\cite{lin2017feature}. Given an image  and bounding box , the image is first processed by the detector backbone network (ResNet50-FPN), outputting image features . Using a differentiable PrRoiPool~\cite{jiang2018acquisition} layer,  is then pooled to extract features . Finally,  is processed by the added network branch, outputting . As in \cite{gustafsson2019learning}, predictions  are produced by performing guided NMS~\cite{jiang2018acquisition} followed by gradient-based refinement (Section~\ref{section:ebms-prediction}), taking the Faster-RCNN detections as initial estimates . Experiments are performed on the large-scale COCO dataset~\cite{lin2014microsoft}. We use the \textit{2017 train} split ( 118\thinspace000 images) for training, the \textit{2017 val} split ( 5\thinspace000 images) for setting hyperparameters, and report results on the \emph{2017 test-dev} split (~20\thinspace000 images). The standard COCO metrics AP, AP and AP are used, where AP is the primary metric.





\parsection{Results}
A comparison of the training methods in terms of the COCO metrics and training cost (seconds per iteration) is found in Table~\ref{tab:comparison_object_detection}. Since DSM clearly outperformed SM in the 1D regression experiments, we here only include DSM. For ML-MCMC, results for  are included. We observe that ML-IS, KLD-IS, NCE and NCE+ clearly have the best performance. In terms of the COCO metrics, NCE+ outperforms NCE and all other methods. ML-IS is also outperformed by KLD-IS. The four top-performing methods are further compared in Figure~\ref{fig:comparison_detection_AP_vs_M}, in terms of AP as a function of the number of samples . NCE and NCE+ here demonstrate clear superior performance for small values of , and do not experience numerical issues even for . KLD-IS improves this robustness compared ML-IS, but is not close to matching NCE or NCE+. In terms of training cost, the four top-performing methods are virtually identical. For ML-IS, \eg, we observe in Figure~\ref{fig:training_cost_vs_M} that setting  decreases the training cost with  compared to the standard case of . 



\parsection{Analysis of NCE+ Hyperparameters}
How the value of  in  (\ref{eq:nce_noise_}) affects validation performance is studied in Figure~\ref{fig:effect_of_beta}. Here, we observe that quite a large range of values improve the performance compared to the NCE baseline (), before it eventually degrades for . We also observe that the performance is optimized for . In Figure~\ref{fig:effect_of_beta}, the standard deviations  in ,  (\ref{eq:nce_noise_}) are set to . These values are selected in an initial step based on an ablation study for NCE, which is found in Table~\ref{tab:detection_ablation_nce}.


\begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{














\begin{tabular}{l@{\hspace{0.5cm}}cccccccc}
\toprule
 &ML-IS &ML-MCMC-1 &ML-MCMC-4 &ML-MCMC-8 &KLD-IS &NCE& DSM &\textbf{NCE+}\\
\midrule
AP (\%)              &39.4 &36.4 &36.4 &36.4 &39.6 &39.5 &36.3 &\textbf{39.7}\\
AP  &58.6 &57.9 &57.9 &58.0 &58.6 &58.6 &57.9 &\textbf{58.7}\\
AP  &42.1 &38.8 &39.0 &39.0 &42.6 &42.4 &38.9 &\textbf{42.7}\\
Training Cost      &1.03 &2.47 &7.05 &13.3 &\textbf{1.02} &1.04 &3.84 &1.09\\
\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
\caption{Comparison of training methods for the object detection experiments, on the \textit{2017 test-dev} split of COCO~\cite{lin2014microsoft}. Our proposed NCE+ achieves the best performance.}\vspace{-3.0mm}
	\label{tab:comparison_object_detection}
\end{table}





























\subsection{Discussion}\label{section:comparison-Observations}
The results on both set of experiments are highly consistent. First of all, ML-IS, KLD-IS, NCE and NCE+ are by far the top-performing training methods. ML-MCMC, the method commonly employed for generative image modeling in recent years, does not come close to matching these top-performing methods, especially not given similar computational budgets. When studying the performance as a function of the number of samples , NCE and NCE+ are the superior methods by a significant margin. In particular, this study demonstrates that the NCE and NCE+ losses are  numerically more stable than those of ML-IS and KLD-IS. In the 1D regression problems, which employ synthetic datasets without any annotation noise, NCE and NCE+ have virtually identical performance. In the object detection experiments however, where we employ real-world datasets, NCE+ consistently improves the NCE performance. On object detection, NCE+ also improves or matches the performance of KLD-IS, which explicitly models annotation noise and outperforms ML-IS. Overall, the results of the comparison suggest that our proposed NCE+ should be considered the go-to training method. 





\begin{figure}\begin{minipage}{0.475\textwidth}\begin{tikzpicture}[scale=0.655, baseline]
            \begin{axis}[
                xmode=log,
                log ticks with fixed point,
                xlabel={Number of samples },
                ylabel={Training Cost},
xtick={1, 2, 4, 8, 16, 32, 64, 128},
legend pos=south east,
                ymajorgrids=true,
grid style=dashed,
                every axis plot/.append style={thick},
            ]
            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
            1 0.793625\\
            2 0.7975\\
            4 0.79825\\
            8 0.8035\\
            16 0.8285\\
            32 0.848875\\
            64 0.90075\\
            128 1.03336\\
            };
            
            \end{axis}
        \end{tikzpicture}\caption{Effect of the number of samples  on training cost (seconds per iteration), for ML-IS on object detection.}\vspace{-1.5mm}
        \label{fig:training_cost_vs_M}\end{minipage}\quad
    \begin{minipage}{0.475\textwidth}\centering
        \begin{tikzpicture}[scale=0.655, baseline]
            \begin{axis}[
xlabel={},
                ylabel={AP (\%)},
xtick={0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8},
legend pos=south east,
                ymajorgrids=true,
grid style=dashed,
                every axis plot/.append style={thick},
            ]
            \addplot
             plot [error bars/.cd, y dir = both, y explicit]
             table[row sep=crcr, x index=0, y index=1]{
            0 39.17\\
            0.025 39.20\\ 0.05 39.27\\
            0.1 39.36\\
            0.15 39.32\\
            0.2 39.22\\
            0.4 39.07\\
            0.8 38.34\\
};
            
            \end{axis}
        \end{tikzpicture}\caption{Effect of the NCE+ hyperparameter  on object detection performance (), on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-1.5mm}
        \label{fig:effect_of_beta}\end{minipage}
\end{figure}

\begin{table}
\centering
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccccc}
\toprule
                       &\{0.0125, 0.025, 0.05\} &\{0.025, 0.05, 0.1\} &\{0.05, 0.1, 0.2\} &\{0.075, 0.15, 0.3\} &\{0.1, 0.2, 0.4\}\\
\midrule
AP (\%)             &38.58 &38.95 &39.12 &\textbf{39.17} &39.05\\
\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
\caption{Ablation study for NCE, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-3.0mm}
	\label{tab:detection_ablation_nce}
\end{table}



 \section{Visual Tracking Experiments}
\label{section: experiments}
Lastly, we apply our proposed NCE+ to the task of visual tracking. Specifically, we consider \emph{generic visual object tracking}, which entails estimating the bounding box  of a target object in every frame of a video. The target object does not belong to any pre-specified class, but is instead defined by a given bounding box in the initial video frame. We compare the performance both to NCE and KLD-IS, and to state-of-the-art trackers. Code and trained models are available at \cite{pytracking}. Further details are also found in the supplementary material.




\parsection{Tracking Approach}
We base our tracker on the recent DiMP~\cite{bhat2019learning} and PrDiMP~\cite{danelljan2020probabilistic}. The target object is thus first coarsely localized in the current video frame via 2D image-coordinate regression of its center point, emphasizing robustness over accuracy. Then, the full bounding box  of the target is accurately regressed by gradient-based refinement (Section~\ref{section:ebms-prediction}). The two stages employ separate network branches which are trained jointly end-to-end. As a strong baseline, we combine the DiMP method for center point regression with the PrDiMP bounding box regression approach. We term this resulting tracker \textit{DiMP-KLD-IS}. By also modifying common training parameters (batch size, data augmentation etc.), DiMP-KLD-IS significantly outperforms both DiMP and PrDiMP. Our proposed tracker, termed \textit{DiMP-NCE+}, is then obtained simply by using NCE+ instead of KLD-IS to train the bounding box regression branch. In both cases, the number of samples . As in \cite{bhat2019learning, danelljan2020probabilistic}, the training splits of TrackingNet~\cite{muller2018trackingnet}, LaSOT~\cite{fan2019lasot}, GOT-10k~\cite{huang2019got} and COCO~\cite{lin2014microsoft} are used for training. Similar to PrDiMP, our DiMP-NCE+ tracker runs at about 30 FPS on a single GPU.





\parsection{Results}
We evaluate DiMP-NCE+ on five commonly used tracking datasets. Tracking-Net~\cite{muller2018trackingnet} is a large-scale dataset containing videos sampled from YouTube. Results are reported on its test set of  videos. We also evaluate on the LaSOT~\cite{fan2019lasot} test set, containing  long videos ( frames on average). Moreover, we report results on the UAV123~\cite{UAV123} dataset, consisting of  videos which feature small targets and distractor objects. Results are also reported on the 30 FPS version of the need for speed (NFS)~\cite{NFS} dataset, containing  videos with fast motions. Finally, we evaluate on the 100 videos of OTB-100~\cite{OTB100}. Our tracker is evaluated in terms of overlap precision (OP). For a threshold , OP is the percentage of frames in which the IoU overlap between the estimated and ground truth target bounding box is larger than . By averaging OP over , the \textit{AUC} score is then obtained. For TrackingNet, the term \textit{Success} is used in place of AUC. Results in terms of AUC on all five datasets are found in Table~\ref{tab:tracking}. To ensure significance, the average AUC over  runs is reported for our trackers. We observe that DiMP-NCE+ consistently outperforms both our DiMP-KLD-IS baseline, and a variant employing NCE instead of NCE+. Compared to previous approaches, only the very recent SiamRCNN~\cite{voigtlaender2020siam} achieves results competitive with our DiMP-NCE+. SiamRCNN is however slower than DiMP-NCE+ (5~FPS vs 30 FPS) and employs a larger backbone network (ResNet101 vs ResNet50). Results for the ResNet50 version of SiamRCNN are only available on two of the datasets, on which it is outperformed by our DiMP-NCE+. More detailed results are provided in the supplementary material.



\begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{

























\begin{tabular}{l@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c}
	\toprule
	&MDNet &UPDT &DaSiamRPN &ATOM &SiamRPN++ &DiMP &SiamRCNN &PrDiMP &DiMP- &DiMP- &\textbf{DiMP-}\\
	&\cite{MDNet} &\cite{UPDT} &\cite{DaSiamRPN} &\cite{danelljan2019atom} &\cite{SiamRPN++} &\cite{bhat2019learning} &\cite{voigtlaender2020siam} &\cite{danelljan2020probabilistic} &KLD-IS &NCE &\textbf{NCE+}\\
	\midrule
	
	\multirow{1}{18mm}{TrackingNet}
	&60.6 &61.1 &63.8 &70.3 &73.3 &74.0 &\textbf{81.2} &75.8 &78.1 &77.1 &78.7\\
	
	\multirow{1}{15mm}{LaSOT}
	&39.7 &- &- &51.5 &49.6 &56.9 &\textbf{64.8} (62.3) &59.8 &63.1 &62.8 &63.7\\
	
	\multirow{1}{15mm}{UAV123}
	&52.8 &54.5 &57.7 &63.2 &61.3 &64.3 &64.9 &66.7 &66.6 &65.2 &\textbf{67.2}\\

	\multirow{1}{15mm}{NFS}
	&42.2 &53.7 &- &58.4 &- &62.0 &63.9 &63.5 &64.7 &64.3 &\textbf{65.0}\\
	
	\multirow{1}{15mm}{OTB-100}
	&67.8 &70.2 &65.8 &66.9 &69.6 &68.4 &70.1 (68.0) &69.6 &70.1 &69.3 &\textbf{70.7}\\
	
	\bottomrule
\end{tabular}

























 	}\vspace{-3.5mm}
\caption{Results for the visual tracking experiments. The AUC (Success) metric is reported on five common datasets. Our proposed DiMP-NCE+ tracker significantly outperforms strong baselines and achieves state-of-the-art performance on all five datasets. For SiamRCNN~\cite{voigtlaender2020siam}, results for the ResNet50 version are given in parentheses when available.}\vspace{-3.0mm}
	\label{tab:tracking}
\end{table}



 \section{Conclusion}
\label{section: conclusion}
We proposed a simple yet highly effective extension of NCE to train EBMs  for computer vision regression tasks. Our proposed method NCE+ can be understood as a direct generalization of NCE, accounting for noise in the annotation process of real-world datasets. We also provided a detailed comparison of NCE+ and six popular methods from literature, the results of which suggest that NCE+ should be considered the go-to training method. This comparison is the first comprehensive study of how EBMs should be trained for best possible regression performance. Finally, we applied our proposed NCE+ to the task of visual tracking, achieving state-of-the-art performance on five commonly used datasets. We hope that our simple training method and promising results will encourage the research community to further explore the application of EBMs to various regression tasks.


 
\parsection{Acknowledgments}
This research was financially supported by the Swedish Foundation for Strategic Research via the project \emph{ASSEMBLE}, the Swedish Research Council via the project \emph{Learning flexible models for nonlinear dynamics}, the ETH Z\"urich Fund (OK), a Huawei Technologies Oy (Finland) project, an Amazon AWS grant, and Nvidia.

\bibliography{references}








\clearpage

\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0}

\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{0}

\renewcommand{\thealgorithm}{S\arabic{algorithm}}
\setcounter{algorithm}{0}

\renewcommand{\theequation}{S\arabic{equation}}
\setcounter{equation}{0}



\section*{\centering{\\Supplementary Material}}

In this supplementary material, we provide additional details and results. It consists of Appendix~\ref{appendix:prediciton} - Appendix~\ref{appendix:visual_tracking}. Appendix~\ref{appendix:prediciton} contains a detailed algorithm for our employed prediction strategy. Further experimental details are provided in Appendix~\ref{appendix:1dregression} for 1D regression, and in Appendix~\ref{appendix:object_detection} for object detection. Lastly, Appendix~\ref{appendix:visual_tracking} contains details and further results for the visual tracking experiments. Note that equations, tables, figures and algorithms in this supplementary document are numbered with the prefix "S". Numbers without this prefix refer to the main paper.

\appendix
\begin{appendices}
\section{Prediction Algorithm}
\label{appendix:prediciton}
Our prediction procedure (Section 2.2) is detailed in Algorithm~\ref{algo:prediction}, where  denotes the gradient ascent step-length,  is a decay of the step-length and  is the number of iterations.

\begin{algorithm}
\caption{Prediction via gradient-based refinement.}
\label{algo:prediction}
\textbf{Input:} , , , , .
\begin{algorithmic}[1]
    \State .
    \For{\texttt{}}
        \State \texttt{PrevValue}  .
        \State .
        \State \texttt{NewValue}  .
        \If { }
            \State .
        \Else
            \State .
        \EndIf
    \EndFor
    \State \textbf{Return} .
\end{algorithmic}
\end{algorithm}\vspace{-3mm} \section{1D Regression}
\label{appendix:1dregression}
Here, we provide details on the two synthetic datasets, the network architecture, the evaluation procedure, and hyperparameters used for our 1D regression experiments (Section 4.1). For all seven training methods, the DNN  was trained (by minimizing the associated loss ) for  epochs with a batch size of  using the ADAM~\cite{kingma2014adam} optimizer.






\subsection{Datasets}
The ground truth  for the first dataset is visualized in Figure~\ref{fig:1dregression_1_gt}. It is defined by a mixture of two Gaussian components (with weights  and ) for , and a log-normal distribution (with , ) for . The training data  was generated by uniform random sampling of  in the interval , and is visualized in Figure~\ref{fig:1dregression_1_data}. The ground truth  for the second dataset is defined according to,

The training data  was generated by uniform random sampling of  in the interval , and is visualized in Figure~\ref{fig:1dregression_2_data}.

\begin{figure}[t]\begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/1_gt.png}\vspace{-7.0mm}
        \caption{Visualization of the true  for the first 1D regression dataset.}\vspace{-3mm}
        \label{fig:1dregression_1_gt}\end{minipage}
    \quad
    \begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/1_training_data.png}\vspace{-7.0mm}
        \caption{Training data  for the first 1D regression dataset.}\vspace{-3mm}
        \label{fig:1dregression_1_data}\end{minipage}\quad
    \begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/2_training_data.png}\vspace{-7.0mm}
        \caption{Training data  for the second 1D regression dataset.}\vspace{-3mm}
        \label{fig:1dregression_2_data}\end{minipage}\end{figure}








\subsection{Network Architecture}
The DNN  is a feed-forward network taking  and  as inputs. It consists of two fully-connected layers (dimensions: , ) for , one fully-connected layer () for , and four fully-connected layers (, , , ) processing the concatenated  feature vector.












\subsection{Evaluation}
\label{section:1dregression_evaluation}
The training methods are evaluated in terms of the KL divergence  between the learned EBM  and the true conditional density . To approximate , we compute  and  for all  pairs in a  uniform grid in the region . We then normalize across all values associated with each , employ the formula for KL divergence between two discrete distributions  and ,

and finally average over all  values of . For each dataset and training method, we independently train the DNN  and compute   times. We then take the mean of the  best runs, and finally average this value for the two datasets.













\subsection{Hyperparameters}
The number of samples  for all applicable training methods. All other hyperparameters were selected to optimize the performance, evaluated according to Section~\ref{section:1dregression_evaluation}.


\parsection{ML-IS}
Following \cite{gustafsson2019learning}, we set  in the proposal distribution  in (4). After ablation, we set , . 


\parsection{KLD-IS}
We use the same proposal distribution  as for ML-IS. After ablation, we set  in .


\parsection{ML-MCMC}
After ablation, we set the Langevin dynamics step-length .


\parsection{NCE}
To match ML-IS, we set  in the noise distribution  in (11). After ablation, we set , . 


\parsection{DSM}
After ablation, we set  in .


\parsection{NCE+}
We use the same noise distribution  as for NCE. After ablation, we set .













\subsection{Qualitative Results}
An example of  trained using NCE on the first dataset is visualized in Figure~\ref{fig:1dregression_1_example_nce}. As can be observed, this is quite close to the true  visualized in Figure~\ref{fig:1dregression_1_gt}. Similar results are obtained with all four top-performing training methods. Examples of  instead trained using DSM and SM are visualized in Figure~\ref{fig:1dregression_1_example_dsm} and Figure~\ref{fig:1dregression_1_example_sm}, respectively. These do not approximate the true  quite as well, matching the worse performance in terms of  reported in Table 1.

\begin{figure}[t]\begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/1_example_nce.png}\vspace{-8.0mm}
        \caption{Example of  trained with NCE.}\vspace{-0mm}
        \label{fig:1dregression_1_example_nce}\end{minipage}
    \quad
    \begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/1_example_dsm.png}\vspace{-8.0mm}
        \caption{Example of  trained with DSM.}\vspace{-0mm}
        \label{fig:1dregression_1_example_dsm}\end{minipage}\quad
    \begin{minipage}{0.3125\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/1dregression/1_example_sm.png}\vspace{-8.0mm}
        \caption{Example of  trained with SM.}\vspace{-0mm}
        \label{fig:1dregression_1_example_sm}\end{minipage}\end{figure}











 \begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}cccccccc}
\toprule
 &ML-IS &ML-MCMC-1 &ML-MCMC-4 &ML-MCMC-8 &KLD-IS &NCE& DSM &\textbf{NCE+}\\
\midrule
             &0.0004 &0.000025 &0.000025 &0.000025 &0.0004 &0.0004 &0.000025 &0.0008\\

            &0.0016 &0.0001 &0.0001 &0.0001 &0.0016 &0.0016 &0.0001 &0.0032\\
\bottomrule
\end{tabular}
 	}\vspace{-3.5mm}
\caption{Used step-lengths  and  for the object detection experiments.}\vspace{-4.0mm}
	\label{tab:comparison_object_detection_step}
\end{table}

\section{Object Detection}
\label{appendix:object_detection}
Here, we provide details on the prediction procedure and hyperparameters used for our object detection experiments (Section 4.2). We employ an identical network architecture and training procedure as described in \cite{gustafsson2019learning}, only modifying the loss when using a different method than ML-IS to train .









\subsection{Prediction}
Predictions  are produced by performing guided NMS~\cite{jiang2018acquisition} followed by gradient-based refinement (Algorithm~\ref{algo:prediction}), taking the Faster-RCNN detections as initial estimates . As in \cite{gustafsson2019learning}, we run  gradient ascent iterations. We fix the step-length decay to , which is the value used in \cite{gustafsson2019learning}. For each trained model, we select the gradient ascent step-length  to optimize performance in terms of AP on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}. Like \cite{gustafsson2019learning}, we use different step-lengths for the bounding box position () and size (). We start this ablation with , . The used step-lengths for all training methods are given in Table~\ref{tab:comparison_object_detection_step}.







\subsection{Hyperparameters}
The number of samples  for all applicable training methods. All other hyperparameters were selected to optimize performance in terms of AP on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.



\parsection{ML-IS}
Following \cite{gustafsson2019learning}, we set  in the proposal distribution  in (4) with , , .



\parsection{KLD-IS}
We use the same proposal distribution  as for ML-IS. Based on the ablation study in Table~\ref{tab:detection_ablation_kld-is}, we set  in .

\begin{table}
    \begin{minipage}{0.475\textwidth}\centering
        \resizebox{1.0\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccccc}
\toprule
                       &0.0075 &0.015 &0.0225 &0.03 &0.0375\\
\midrule
AP (\%)              &38.32 &39.19 &\textbf{39.38} &39.33 &39.23\\
\bottomrule
\end{tabular}         	}\caption{Ablation study for KLD-IS, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-1.0mm}
        	\label{tab:detection_ablation_kld-is}
    \end{minipage}
    \quad
    \begin{minipage}{0.475\textwidth}\centering
        \resizebox{0.825\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccc}
\toprule
                       &0.000001 &0.00001 &0.0001\\
\midrule
AP (\%)              &36.14 &\textbf{36.19} &36.04\\
\bottomrule
\end{tabular}         	}\caption{Ablation study for ML-MCMC-1, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-1.0mm}
        	\label{tab:detection_ablation_ml-mcmc}
    \end{minipage}\end{table}



\parsection{ML-MCMC}
Based on the ablation study in Table~\ref{tab:detection_ablation_ml-mcmc}, we set the Langevin dynamics step-length .

\parsection{NCE}
To match ML-IS, we set  in the noise distribution  in (11). Based on the ablation study in Table~\ref{tab:detection_ablation_nce}, we set , , .

\begin{table}
\centering
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccccc}
\toprule
                       &\{0.0125, 0.025, 0.05\} &\{0.025, 0.05, 0.1\} &\{0.05, 0.1, 0.2\} &\{0.075, 0.15, 0.3\} &\{0.1, 0.2, 0.4\}\\
\midrule
AP (\%)             &38.58 &38.95 &39.12 &\textbf{39.17} &39.05\\
\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
\caption{Ablation study for NCE, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-3.0mm}
	\label{tab:detection_ablation_nce}
\end{table}



\parsection{DSM}
Based on the ablation study in Table~\ref{tab:detection_ablation_dsm}, we set  in .

\begin{table}
    \begin{minipage}{0.475\textwidth}\centering
        \resizebox{0.805\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccc}
\toprule
                       &0.0375 &0.075 &0.15\\
\midrule
AP (\%)              &36.11 &\textbf{36.12} &36.05\\
\bottomrule
\end{tabular}         	}\caption{Ablation study for DSM, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-0.0mm}
        	\label{tab:detection_ablation_dsm}
    \end{minipage}
    \quad
    \begin{minipage}{0.475\textwidth}\centering
        \resizebox{0.805\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}ccc}
\toprule
                       &0.05 &0.1 &0.15\\
\midrule
AP (\%)             &39.27 &\textbf{39.36} &39.32\\
\bottomrule
\end{tabular}         	}\caption{Ablation study for NCE+, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}.}\vspace{-0.0mm}
        	\label{tab:detection_ablation_nce+}
    \end{minipage}\end{table}



\parsection{NCE+}
We use the same noise distribution  as for NCE. Based on the ablation study in Table~\ref{tab:detection_ablation_nce+}, we set .











\subsection{Detailed Results}
A comparison of the training methods on the \textit{2017 val} split of COCO~\cite{lin2014microsoft} is provided in Table~\ref{tab:comparison_object_detection_val}.

\begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l@{\hspace{0.5cm}}cccccccc}
\toprule
 &ML-IS &ML-MCMC-1 &ML-MCMC-4 &ML-MCMC-8 &KLD-IS &NCE& DSM &\textbf{NCE+}\\
\midrule
AP (\%)              &39.11 &36.19 &36.24 &36.25 &\textbf{39.38} &39.17 &36.12 &39.36\\
AP  &57.95 &57.34 &57.45 &57.28 &\textbf{58.07} &57.96 &57.29 &57.99\\
AP  &41.97 &38.77 &38.81 &38.88 &42.47 &42.07 &38.84 &\textbf{42.63}\\
Training Cost      &1.03 &2.47 &7.05 &13.3 &\textbf{1.02} &1.04 &3.84 &1.09\\
\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
\caption{Comparison of training methods for the object detection experiments, on the \textit{2017 val} split of COCO~\cite{lin2014microsoft}. NCE+ and KLD-IS achieve the best performance.}\vspace{-1.0mm}
	\label{tab:comparison_object_detection_val}
\end{table} \begin{table}[b]
\centering
\resizebox{1.0\textwidth}{!}{










\begin{tabular}{l@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c}
	\toprule
	&SiamFC &MDNet &UPDT &DaSiamRPN &ATOM &SiamRPN++ &DiMP &SiamRCNN &PrDiMP &DiMP- &DiMP- &\textbf{DiMP-}\\
	&\cite{SiameseFC} &\cite{MDNet} &\cite{UPDT} &\cite{DaSiamRPN} &\cite{danelljan2019atom} &\cite{SiamRPN++} &\cite{bhat2019learning} &\cite{voigtlaender2020siam} &\cite{danelljan2020probabilistic} &KLD-IS &NCE &\textbf{NCE+}\\
	\midrule
	
	\multirow{1}{25mm}{Precision }
	&53.3 &56.5 &55.7 &59.1 &64.8 &69.4 &68.7 &\textbf{80.0} &70.4 &73.3 &69.8 &73.7\\
	
	\multirow{1}{25mm}{Norm. Prec. }
	&66.6 &70.5 &70.2 &73.3 &77.1 &80.0 &80.1 &\textbf{85.4} &81.6 &83.5 &82.4 &83.7\\
	
	\multirow{1}{25mm}{Success (AUC) }
	&57.1 &60.6 &61.1 &63.8 &70.3 &73.3 &74.0 &\textbf{81.2} &75.8 &78.1 &77.1 &78.7\\
	
	\bottomrule
\end{tabular} 	}\vspace{-3.5mm}
\caption{Full results on the TrackingNet~\cite{muller2018trackingnet} test set, in terms of precision, normalized precision, and success (AUC). Our proposed DiMP-NCE+ is here only outperformed by the very recent SiamRCNN~\cite{voigtlaender2020siam}. SiamRCNN is however slower than DiMP-NCE+ (5~FPS vs 30 FPS) and employs a larger backbone network (ResNet101 vs ResNet50).}\vspace{-0.0mm}
	\label{tab:tracking_trackingnet}
\end{table}





\section{Visual Tracking}
\label{appendix:visual_tracking}
Here, we provide detailed results and hyperparameters for our visual tracking experiments (Section 5). We employ an identical network architecture, training procedure and prediction procedure for DiMP-KLD-IS, DiMP-NCE and DiMP-NCE+, only the loss is modified.






\subsection{Training Parameters}
DiMP-KLD-IS is obtained by combining the DiMP~\cite{bhat2019learning} method for center point regression with the PrDiMP~\cite{danelljan2020probabilistic} bounding box regression approach, and modifying a few training parameters. Specifically, we change the batch size from  to , we change the LaSOT sampling weight from  to , we change the number of samples per epoch from  to , and we add random horizontal flipping with probability . Since we increase the batch size, we also freeze conv1, layer1 and layer2 of the ResNet backbone to save memory.











\begin{figure}[t]\begin{minipage}{0.475\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/tracking/success_plot_lasot.pdf}\vspace{-7.0mm}
        \caption{Success plot on LaSOT~\cite{fan2019lasot}.}\vspace{-3mm}
        \label{fig:tracking_lasot}\end{minipage}
    \quad
    \begin{minipage}{0.475\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/tracking/success_plot_uav123.pdf}\vspace{-7.0mm}
        \caption{Success plot on UAV123~\cite{UAV123}.}\vspace{-3mm}
        \label{fig:tracking_uav123}\end{minipage}\end{figure}

\begin{figure}[t]\begin{minipage}{0.475\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/tracking/success_plot_nfs.pdf}\vspace{-7.0mm}
        \caption{Success plot on NFS~\cite{NFS}.}\vspace{-3mm}
        \label{fig:tracking_nfs}\end{minipage}
    \quad
    \begin{minipage}{0.475\textwidth}\centering
        \includegraphics[width=1.0\textwidth]{figures/appendices/tracking/success_plot_otb100.pdf}\vspace{-7.0mm}
        \caption{Success plot on OTB-100~\cite{OTB100}.}\vspace{-3mm}
        \label{fig:tracking_otb100}\end{minipage}\end{figure}

\subsection{Hyperparameters}
The number of samples  for all three training methods.


\parsection{DiMP-KLD-IS}
Following PrDiMP, we set  in the proposal distribution  in (4) with , , and we set  in .


\parsection{DiMP-NCE}
Matching DiMP-KLD-IS, we set  in the noise distribution  in (11) with , . A quick ablation study on the validation set of GOT-10k~\cite{huang2019got} did not find values of  resulting in improved performance.



\parsection{DiMP-NCE+}
We use the same noise distribution  as for NCE. We set , as this corresponded to the best performance on the object detection experiments (Table~\ref{tab:detection_ablation_nce+}).










\subsection{Detailed Results}
Full results on the TrackingNet~\cite{muller2018trackingnet} test set, in terms of all three TrackingNet metrics, are found in Table~\ref{tab:tracking_trackingnet}. Success plots for LaSOT, UAV123, NFS and OTB-100 are found in Figure~\ref{fig:tracking_lasot}-\ref{fig:tracking_otb100}, showing the overlap precision OP as a function of the overlap threshold .


 \end{appendices}

\end{document}
