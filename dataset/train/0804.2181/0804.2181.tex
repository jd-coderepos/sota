\documentclass{sig-alt-full}
\conferenceinfo{ISSAC'08,} {July 20--23, 2008, Hagenberg, Austria.}
\CopyrightYear{2008}
\crdata{978-1-59593-904-3/08/07}

\usepackage{amsmath,amssymb}
\usepackage{theorem}
\usepackage{graphics}

\usepackage{url}
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,citecolor=blue,hypertexnames=false]{hyperref}

\newcommand{\x}{X}
\newcommand{\y}{Y}
\newcommand{\Tx}{\theta}
\newcommand{\Dx}{\partial}

\newcommand{\bigO}{{\mathcal{O}}}
\newcommand{\bigOsoft}{\tilde{\mathcal{O}}}
\newcommand{\MM}{\mathsf{MM}}
\def\OMul#1#2#3{\langle #1,#2 \rangle_{#3}}
\def\MMul#1#2#3{\langle #1,#2,#3 \rangle}

\newcommand{\sC}{\mathsf{C}}
\newcommand{\sM}{\mathsf{M}}
\newcommand{\sT}{\mathsf{T}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\sF}{\mathsf{F}}

\def\gathen#1{{#1}}
\def\hoeven#1{{#1}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\def\tM{\tilde M}

\setlength{\unitlength}{0.8 cm}


\begin{document}

\title{Products of Ordinary Differential Operators by Evaluation~and~Interpolation}
\numberofauthors{3}

\author{
\alignauthor Alin Bostan\\
\affaddr{Algorithms Project-Team, INRIA Paris-Rocquencourt}\\
\affaddr{78153 Le Chesnay (France)}\\
\email{Alin.Bostan@inria.fr}
\alignauthor Fr\'ed\'eric Chyzak \\
\affaddr{Algorithms Project-Team, INRIA Paris-Rocquencourt}\\
\affaddr{78153 Le Chesnay (France)}\\
\email{Frederic.Chyzak@inria.fr}
\alignauthor Nicolas Le Roux\\
\affaddr{Algorithms Project-Team, INRIA Paris-Rocquencourt}\\
\affaddr{78153 Le Chesnay (France)}\\
\email{Nicolas.Le\_Roux@inria.fr}
}

\date{\today}
\maketitle

\begin{abstract}
It is known that multiplication of linear differential operators over ground fields of characteristic zero can be reduced to a constant number of matrix products.
We give a new algorithm by evaluation and interpolation which is faster than the previously-known one by a constant factor, and
prove that in characteristic zero, multiplication of differential operators and of matrices are computationally equivalent problems.
In positive characteristic, we show that differential operators can be multiplied in nearly optimal time. 
Theoretical results are validated by intensive experiments.
\end{abstract}

\vspace{1mm}
 \noindent
 {\bf Categories and Subject Descriptors:} \\
\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
  Manipulation -- \emph{Algebraic Algorithms}
 
 \vspace{1mm}
 \noindent
 {\bf General Terms:} Algorithms, Theory
 
 \vspace{1mm}
 \noindent
 {\bf Keywords:} Fast algorithms, differential operators.



\section{Introduction}

Multiplication in polynomial algebras  and  over a field~ has been intensively studied in the computer-algebra literature.
Since the discovery of Karatsuba's algorithm and the Fast Fourier Transform, hundreds of articles have been dedicated to theoretical and practical issues;
see, e.g., \cite[Ch.~8]{GaGe99}, \cite{Bernstein}, and the references therein.
Not only are many other operations built upon multiplication, but often their complexity can be expressed in terms of the complexity of multiplication---whether as a constant number of multiplications or a logarithmic number of multiplications.
In~, this is the case for Euclidean division, gcd and resultant computation, multipoint evaluation and interpolation, shifts, certain changes of bases, etc.

In the noncommutative setting of linear ordinary differential operators, the study is by far less advanced.
The complexity of the product has been addressed only recently, by van~der~Hoeven in the short paper~\cite{vdHoeven02}:
multiplication of operators over ground fields~ of characteristic zero can be reduced by an evaluation-interpolation scheme to a constant number~ of matrix multiplications with elements in~.
Work in progress~\cite{LCLMs} suggests that linear algebra is again the bottleneck for computations of GCRDs and LCLMs.

This work aims at deepening the study started in~\cite{vdHoeven02} for characteristic~0.
We improve van der Hoeven's result along several directions:
We make the constant factor~ explicit in~\S\ref{ssec:MatToOper} and improve it in~\S\ref{sec:better-constants}, and we prove in~\S\ref{sec:equiv} that multiplication of matrices and of differential operators are equivalent computational problems---that is, they share the same exponent, 
thus answering the question left open in~\cite[\S6, Remark~2]{vdHoeven02}.
As usual, those results hold for sufficiently large characteristic as well.
We prolong the study to the case of (small) positive characteristic, by giving in~\S\ref{sec:positive-char} an algorithm for computing the product of two differential operators in softly quadratic complexity, that is, nearly optimally in the output size.
This indicates that the equivalence result may fail to generalize to arbitrary fields.


In what follows, the field  has characteristic zero, unless stated otherwise.
 and  respectively denote the associative algebras  and .

\vspace{-0.3cm}
\begin{table}[ht]
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}[t]{c|cc|ccc}
 & \textsf{vdH}  & \textsf{IvdH} & \textsf{vdH} & \textsf{IvdH} & \textsf{MulWeyl}\\
\hline
Product by blocks & 37 & 24 & 96 & 48 & 12 \\
\hline
Zeros + Strassen  & 20 &  8 & 47 & 12 &  8
\end{tabular}
\caption{\label{table:MM} Number of  matrix products for multiplication in , resp.~, in bidegree .}
\end{center}
\end{table}


\vspace{-0.5cm}
Table~\ref{table:MM} encapsulates our improvements on the constant~.
It displays the cost of linear algebra in van der Hoeven's algorithms (\textsf{vdH}, resp.~\textsf{vdH})
and in the improved versions (\textsf{IvdH}, resp.~\textsf{IvdH}), which are described in~\S\ref{ssec:OperToMat}, resp.~\S\ref{ssec:vdH_D}, and in our algorithm (\textsf{MulWeyl}) in~\S\ref{sec:MulWeyl}.
The subscript~ refers to multiplication in ;
its absence means a product in . 
The first row provides bounds on the number of  matrix products used in each algorithm for multiplying operators in , resp.~, of degree at most  in  and in , resp.~,
under the naive complexity estimate~\eqref{eq:naive-abc} below. 
This estimate reflects the choice of multiplying rectangular matrices by decomposing them into square blocks.
The second row gives tighter bounds under the assumptions that: \emph{(i)\/} any product by a zero block is discarded; \emph{(ii)\/} when possible, a product of two  matrices of  blocks is computed as 7~block products, instead of~8, by using Strassen's algorithm~\cite{Strassen69};
\emph{(iii)\/} predicted non-trivial zero blocks in the output are not computed.


\paragraph*{Canonical form and bidegree}
In the algebra , resp.~, the commutation rule allows one to rewrite any given element into a so-called \emph{canonical form\/} with ~on the left of monomials and , resp.~, on the right, that is, as a linear combination of monomials~, resp.~, for uniquely-defined coefficients from~.
In either case, we speak of an element of bidegree , resp.~at most , when the degree of its canonical form in~ is~, resp.~at most~, and that in~, resp.~, is~, resp.~at most~.
With natural notation, the bidegree  of a product~ clearly satisfies  and .

The problem of computing the canonical form of the product of two elements of bidegree~ from , resp.\ from , given in canonical form, is denoted , resp.\ .


\paragraph*{Complexity measures}
All complexity estimates are given in terms of arithmetical operations in~, which we denote ``ops.''
We denote by  two functions such that Problems  and~ can be solved in  and , respectively.
We denote by  a function such that polynomials of degree at most~ in~ can be multiplied in ~ops.
Using Fast Fourier Transform algorithms,  can be taken in  over fields with suitable roots of unity, and  in the general case~\cite{ScSt71,CaKa91}. 
We use the notation  for  if  is in  for some .
For instance,  is in .
The problem of multiplying an  matrix by an  matrix is written . 
We let  be a function such that Problem   can be solved in ~ops.
We use the abbreviation  for .
The current tightest (strict) upper bound 2.376 for~ such that  is derived in~\cite{CoWi90}.
For the time being, this estimate is only of theoretical relevance.
Few practical algorithms with complexity better than cubic are currently known  for matrix multiplication, among which
Strassen's algorithm~\cite{Strassen69} with exponent  and the Pan--Kaporin algorithm~\cite{Kaporin04} with exponent .
For rectangular matrix multiplication,
we shall use the estimate

obtained by performing the naive product of  by  matrices whose coefficients are  blocks.

Furthermore, we assume that , ,  , and  satisfy the usual super-linearity assumption of~\cite[\S8.3, Eq.~(9)]{GaGe99} and also that, if  is any of these functions,  then  belongs to , for all positive constants~.





\paragraph*{Useful complexity results}
Throughout, we shall freely use several classical results on the complexity of basic polynomial operations.
They are encapsulated in Lemma~\ref{cost-results}.
The corresponding algorithms are found in: \cite[Algorithm~E]{GaGe97} for~(a); \cite[Chapter~10]{GaGe99} for~(b); \cite[Th.~2.4 and 2.5]{Gerhard00} for~(c); and \cite[Cor.~8.29]{GaGe99} for~(d).

\begin{lemma}\label{cost-results}
Let\/  be an arbitrary field. Let , let  be of degree less than  and  of degree at most  in  and  in .
One can perform:
\emph{(a)\/} the Taylor shift ;
\emph{(b)\/} the multipoint evaluation and interpolation of  on  if the characteristic of\/  is 0 or greater than ;
\emph{(c)\/} the base change between the monomial and  the falling factorial basis 
in ~ops.
Moreover, one computes:
\emph{(d)\/} the product  in ~ops.
\end{lemma}


\section{Naive algorithms}
\label{sec:naive-algos}

In this section, we provide complexity estimates for several known algorithms for~.
We set

For any~, we define


\paragraph*{Naive expansion}

The most naive calculation of~ is by expanding each~ in the equality

 Using Leibniz's formula  and the recurrences  and , the canonical form of  is computed in ~ops. 
This induces a complexity  for computing~.
The estimate simplifies to  if~.

\paragraph*{Iterative schemes}
Another calculation is by the formula

and the observation that ~has bidegree at most  and is computed from~ in ~ops.\ by the identity

Therefore, the overall complexity is .
When , this is~, and  if FFT is used.
Similar considerations based on

provide an algorithm in , and one can always use the better algorithm by first comparing  and~.

Another formula, attributed to Takayama and used in several implementations (Takayama's \textsf{Kan} system~\cite{Kan}; Maple's \textsf{Ore\_algebra} by Chyzak~\cite{Ore_algebra}), is given by the (finite) sum

where the products~ are computed formally as commutative products between canonical forms, the resulting sum being viewed as a canonical form.
Each of the derivatives has bidegree at most  and the derivative at order  can be computed in ~ops.\ from the one at order~.
The complexity is seen to be ~ops., by Lemma~\ref{cost-results}(d).
When , this is~, or  using FFT;
the scheme~\eqref{eq:iterative} is just a bit better than~\eqref{eq:takayama}.

\section{Equivalence between products of matrices and operators}
\label{sec:equiv}
Let  be a field of characteristic zero.
In~\cite{vdHoeven02}, van der Hoeven showed that  and  are in .
When , this improves upon the algorithms in~\S\ref{sec:naive-algos}.

In this section, we explain and improve this result along two directions: we make the constant factor explicit  in the estimate   and lessen it.
Then, we prove that , , and  are equivalent computational problems, in a sense made clear below.

\subsection{Product in   reduces to matrix product: van der Hoeven's algorithm revisited}\label{ssec:OperToMat}

A differential operator~ in  can be viewed as a -endomorphism of~, mapping a polynomial~ to~.
As such, it is represented, with respect to the canonical basis  of , by an (infinite) matrix  denoted .
The submatrix of  consisting of its first  rows and  columns is denoted .

Van der Hoeven's key observation is that an operator~ of bidegree  is completely determined by the matrix  .
Writing  and using the relation  yields

where the polynomials  are defined as  for all .
Thus the matrix  has the following rectangular banded form:




The knowledge of~ is equivalent to that of all  polynomials~.
Each of the latter having degrees bounded by~, this is also equivalent to the data of the values , for  and .
This is true by Lagrange interpolation.
Thus,  is indeed completely determined by the  polynomials , 
and also by the matrix~.

Now, let   and let~ be~.
Then .
If , , and~ have bidegrees , and ,  then the previous discussion implies the following ``finite version'' of this matrix equality:

which is the basis of the algorithm in~\cite{vdHoeven02}, described below.
\begin{figure}[ht]
  \begin{center}
    \fbox{\begin{minipage}{8cm}
  \begin{center}\textsf{Mul}() \end{center}
      \textbf{Input:} .\\
     \textbf{Output:} their product .\\bigOsoft\bigl(d_A r_C + d_B(d_A+r_C) + d_C r_C\bigr) \in  \bigOsoft\bigl( d_Cr_C + d_A d_B\bigr).
\begin{bmatrix}
I_n& 0 & 0 \\
M & I_n & 0 \\
0 & N & I_n
\end{bmatrix}^2
=
\begin{bmatrix}
I_n& 0 & 0 \\
2M & I_n & 0 \\
NM & 2N & I_n
\end{bmatrix}
\MM(n) \leq K\bigl( \sC_\Tx(n) +   n\,\sM(n) \log n\bigr).
\sC_\Tx(n) \leq C\,\bigl(\sC_\Dx(n) + n \, \sM(n) \log n\bigr), \\
\sC_\Dx(n) \leq C\,\bigl(\sC_\Tx(n) + n \, \sM(n) \log n\bigr) ,
 \label{matrixproduct-Laurent}
M^C_{0,r_C} = M^B_{-v_A,d_A+r_C} M^A_{0,r_C}.
-4.5mm]
        \begin{tabbing}
1. Convert  in . \\
2. Compute the product  in :\\
\qquad 2.1 From  and , compute the matrices   \\  \qquad \qquad  and  \\
\qquad 2.2 Compute the matrix  using Eq.~\eqref{matrixproduct-Laurent}. \\
\qquad 2.3 Recover  {}from  \\
3. Convert  in  and return it.
     \end{tabbing}
      \end{minipage}
    }\end{center}
\vskip-10pt
  \caption{Product of differential operators in .}
  \label{fig:JorisAlgo-Laurent}
\end{figure}

In what follows, we treat in more detail  the main case of interest, , as solved by Algorithm \textsf{Mul} in Fig.~\ref{fig:JorisAlgo-Laurent}. 
Van der Hoeven suggests to perform Steps 1 and~3 using matrix multiplications by Stirling matrices and their inverses~\cite[\S5.1, Eqs.~(12--13)]{vdHoeven02} and Steps 2.1 and~2.3 using matrix multiplications by Vandermonde matrices and their inverses~\cite[\S2 and~\S4]{vdHoeven02}.
The elements of all the needed Stirling and Vandermonde matrices (and their inverses) can be computed using ~ops.
A careful inspection of the matrix sizes involved in Algorithm \textsf{Mul} shows that: 
 \begin{enumerate}
\item Step~1 reduces to 2 instances of ;
\item Step~3 reduces to an instance of ;
\item Step~2.1 reduces to an instance of  and an instance of ;
\item Step~2.3 reduces to an instance of ;
\item Step~2.2 reduces to an instance of .
 \end{enumerate}
This variant of the algorithm is what we call \textsf{vdH}.
Using again the estimate~\eqref{eq:naive-abc} yields the constant~96 in Table~\ref{table:MM}.



\paragraph*{Several Improvements} A first improvement on \textsf{vdH} 
is to use fast multipoint evaluation and interpolation for Steps 2.1 and~2.3.
A second improvement concerns conversions back and forth between operators in  and in  (Steps 1 and~3).
Instead of using matrix products by Stirling matrices and their inverses, one can apply Lemma~\ref{cost-results}(c),
as explained in~\S\ref{EquivDandTheta}.
Both improvements in conjunction with FFT lessen the cost of Steps 1, 2.1, 2.3, and 3 to a negligible .
We call this improved algorithm \textsf{IvdH}.
Using~\eqref{eq:naive-abc} yields the constant~48 in column \textsf{IvdH} in Table~\ref{table:MM}.
The constants 47 and~12 on the last row of the table are more technical and will be proved in~\cite{LongVersion}.
They rely on observing that the output of \textsf{IvdH} requires partial calculation of~\eqref{matrixproduct-Laurent}, reducing to an instance of .



\subsection{A new, direct evaluation-interpolation algorithm}
\label{sec:MulWeyl}

Let  and~ be in   with respective bidegrees  and~.
We give here an evaluation-inter\-pol\-ation algorithm for computing  which essentially reduces to  for those bidegrees.

To achieve this, we interpret again a differential operator~ in  as a -endomorphism of~, and represent it in the canonical basis  by an (infinite) matrix  denoted .
The submatrix of  consisting of its first  rows and  columns is denoted~.

Then, much like Algorithm \textsf{Mul} in~\S\ref{ssec:OperToMat}, our new algorithm \textsf{MulWeyl} in Fig.~\ref{fig:AlgoMulWeyl} relies on the key observation that an operator  of bidegree  is uniquely determined by the submatrix  of~.
This key fact is proved in Theorem~\ref{interpol} below.
The principle of the algorithm is given in Fig.~\ref{fig:EvalInterp}, where evaluation and interpolation are performed by truncated-series products.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.8]{EvalInterpolFig}
\end{center}
\caption{\label{fig:EvalInterp} Evaluation-Interpolation w.r.t.~.}
\end{figure}
In the case of~, the corresponding matrices become , , and .

\begin{figure}[ht]
  \begin{center}
    \fbox{\begin{minipage}{8cm}
  \begin{center}\textsf{MulWeyl}() \end{center}
      \textbf{Input:} .\\
     \textbf{Output:} their product .\W_{d,r} = \lbrace\, P \in W \; : \;  \deg_{\x}(P) \leq d, \ \deg_\Dx(P) \leq r \,\rbrace.\begin{array}{lccc}
\operatorname{EvOp}_{d,r} \, : \, & W_{d,r} & \rightarrow & \bK^{(d+1)\times(r+1)}\\
                                                                           & P             & \mapsto  & \tM^P_{d,r}
\end{array}.P = \sum_{i=1}^r \ell_{-i}(\x\Dx) \Dx^i + \sum_{i=0}^d \x^i \ell_i(\x\Dx),\label{decomp}
	\sum_{s=1}^r \left(\sum_{j=0}^{\mu_{-s}} p_{j+s,j} \x^j \Dx^j \right) \Dx^s + \sum_{t=0}^d \x^t \left(\sum_{i=0}^{\mu_t} p_{i,t+i} \x^i \Dx^i \right).
\label{evaluation}
P\bigl(\x^k\bigr) = \sum_{i=1}^k \frac{k!}{(k-i)!} \ell_{-i}(k-i) \x^{k-i} + \sum_{i=0}^d \ell_{i}(k) \x^{k+i}.

P \left( \x^k \right) & = \sum_{i=0}^{\min(r,k)} \sum_{j=0}^{d} p_{i,j} \frac{k!}{(k-i)!} \x^{k+j-i} \\
  & = k! \, \x^k \Biggl( \sum_{\ell = - \min(r,k)}^d  \biggl(\sum_{i=\max(0,-\ell)}^{\min(r,d-\ell,k)} \frac{p_{i,i+\ell}}{(k-i)!}\biggr) \,\x^\ell\Biggr).
 \label{eq:convolution} S_\ell =  \biggl(\sum_{i=\max(0,-\ell)}^{\min(r,d-\ell)} p_{i,i+\ell}\x^i\biggr) \, \biggl(\sum_{j=0}^n \frac{\x^j}{j!} \biggr)-4.5mm]
\begin{tabbing}
1. For each , compute  \\
\qquad by using Eq.~\eqref{eq:convolution}. \\
2. Initialize  to be an  zero matrix. \\
3. For  and ,\\
\qquad.
\end{tabbing}
\end{minipage}
    }\end{center}
\vskip-10pt
  \caption{Evaluation in .}
  \label{fig:AlgoEval}
\end{figure}

\begin{proposition}\label{prop:Eval}
Algorithm \textsf{Eval} computes  in\/ ~ops.
\end{proposition}

\begin{proof}
The series  and the factorials 1, \dots,  are computed by recurrence relations in ~ops.
The computation of  can be done in~ for the size~ of the corresponding diagonal of .
Summing over~ and appealing to properties of~ leads to  , then to the announced complexity.
\end{proof}

\subsubsection{Interpolation step} 

Given a  matrix , Step~3 of Algorithm \textsf{MulWeyl} computes the only operator  satisfying .
This is done by inverting Eq.~\eqref{eq:convolution}.
The resulting algorithm is described in Fig.~\ref{fig:AlgoInterpol}.
A similar analysis to that of algorithm \textsf{Eval} leads to the estimate in Proposition~\ref{prop:Interpol}.
\begin{figure}[ht]
  \begin{center}
    \fbox{\begin{minipage}{8cm}
  \begin{center}\textsf{Interpol}() \end{center}
      \textbf{Input:} .\\
     \textbf{Output:}  such that .\-4.5mm]
        \begin{tabbing}
1. Rewrite  and  as  \\
\qquad and .\\
2. Compute the commutative bivariate products \\ \qquad , for .\\
3. Write  in canonical form; \\
\qquad return it.
     \end{tabbing}
      \end{minipage}
    }\end{center}
\vskip-10pt
  \caption{Product of differential operators in  over a field of positive characteristic.}
  \label{fig:pAlgo}
\end{figure}
We now describe proper algorithmic choices that perform each step of \textsf{Mul} in nearly optimal complexity.

Step~1 first rewrites  as   and  as , where 
 are polynomials in  of bidegree at most ; this costs no ops.
The commutation  then enables one to rewrite~ as , where  is . 
Thus, each  is obtained by computing  shifts of polynomials of degree at most .
By Lemma~\ref{cost-results}(a), this results in  ops.\ for Step~1.

Each product in Step~2 involves polynomials in  of bidegree at most .
Thus using Lemma~\ref{cost-results}(d), Step~2 is performed in
~ops.
Note that  has bidegree at most .



To perform Step~3, each  is first rewritten as
 by computing 
shifts of polynomials of degree at most . This can be done in  ops.
{}Finally,  ~ops. are sufficient to put  in canonical form.

Summarizing, we have just proved:
\begin{theorem}
Let\/  be a field of characteristic  and let  be one of the operators .
Then, two operators of bidegree  in  can be multiplied in ~ops., thus in ~ops.\ when FFT is used.
\end{theorem}




\section{Experiments}\label{sec:Experiments}

Table~\ref{table:exp} provides timings of calculations in \textsf{magma} by implementations of several algorithms and algorithmic variants.
Each row corresponds to calculations on the same pair of randomly generated operators in bidegree , for .
Coefficients are taken randomly from~ when~, the prime used being
 (largest prime to fit on 16~bits) and  (largest prime to fit on 32~bits).
When~, computations are performed over~, with random integer input coefficients on 16~bits.


\begin{table}[ht]
\begin{small}
\begin{center}
\setlength{\tabcolsep}{2.25pt}
\begin{tabular}{rr|rrr|rrr|rrrrr}
 &  & S & B & BZ & vdH & Iter & Tak & Rec & Int & BZI & vdHI \\
\hline
 & 3 & 0.25 & 0.26 & 0.25 & 0.39 & 0.32 & 1.23 & 0.01 & 0.64 & 5.22 & 59.8 \\
 & 4 & 0.95 & 0.97 & 0.95 & 1.68 & 4.13 & 12.09 & 0.03 & 4.37 & 35.0 & 418 \\
 & 5 & 4.08 & 4.11 & 4.34 & 8.10 & 37.2 & 123 & 0.20 & 30.2 & 240 & 2793 \\
 & 6 & 21.4 & 21.1 & 22.2 & 45.1 & 397 & 1407 & 1.56 & 209 & 1692 &  \\
 & 7 & 107 & 105 & 104 & 275 &  &  & 13.3 & 1507 &  &  \\
\hline
 & 3 & 0.50 & 0.63 & 0.62 & 1.08 & 2.25 & 5.61 & 0.08 & 1.10 & 8.00 & 82.2 \\
 & 4 & 2.24 & 2.66 & 2.68 & 4.52 & 19.07 & 67.73 & 0.35 & 9.22 & 58.2 & 602 \\
 & 5 & 12.2 & 14.5 & 14.1 & 24.4 & 187 & 926 & 1.63 & 75.6 & 420 &  \\
 & 6 & 88.1 & 111 & 114 & 172 & 2604 &  & 9.40 & 770 & 3146 &  \\
 & 7 & 1961 & 2452 & 2633 &  &  &  & 59.1 &  &  &  \\
\hline
0 & 3 & 9.93 & 12.0 & 11.3 & 28.4 & 6.99 & 24.3 & 0.07 & 0.93 & 16.9 & 309 \\
0 & 4 & 128 & 164 & 164 & 498 & 118 & 725 & 0.27 & 6.89 & 204 &  \\
0 & 5 & 2164 & 2737 & 2725 &  & 2492 &  & 4.37 & 51.4 & 3172 & 
\end{tabular}
\caption{\label{table:exp}Timings on input of bidegree~.}
\end{center}
\end{small}
\end{table}
\vskip-7pt

The calculations were performed on a Power Mac G5 with two CPUs at 2.7\,GHz, 512\,kB of L2 Cache per CPU, 2.5\,GB of memory, and a bus of speed 1.35\,GHz.
The system used was Mac OS X 10.4.10, running Magma V2.13-15.
Computations killed after one hour are marked~.

We provide several variants of our algorithm (S, B, and BZ), as well as various others:
{\bf S:}
direct call to \textsf{magma}'s matrix multiplication in order to compute
;
{\bf B and BZ:}
block decomposition into  matrices before calling \textsf{magma}'s  matrix multiplication on, respectively, 11~block products (using Strassen's algorithm) and by 8~block products (taking the nullity of 2~blocks into account as well);
{\bf vdH:}
Van der Hoeven's algorithm, as described in~\cite{vdHoeven02}, and optimized as much as possible as the implementation~S above;
{\bf Iter and Tak:}
iterative formulas \eqref{eq:iterative} and \eqref{eq:takayama};
{\bf Rec:}
\textsf{magma}'s multiplication of a -matrix by a -matrix, that is, essentially all the linear algebra performed in variant~S (in practice, almost always in the cubic regime for the objects of interest);
{\bf Int:}
fully interpreted implementation of Strassen's product with cubic loop under a suitable threshold;
{\bf BZI and vdHI:}
variants of the implementations BZ and vdH (with evaluation-interpolation steps improved) in which \textsf{magma}'s product of matrices has been replaced with~Int.

\begin{table}[ht]
\begin{small}
\begin{center}
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{cc|rr|rrr|rrr}
& & & & & & &    0 &    0 &    0 \\
& &    3 &    7 &    3 &    5 &    7 &    3 &    4 &    5 \\
\hline
LA &  &  4\% & 13\% & 17\% & 16\% & 39\% & 36\% & 41\% & 52\% \\
PP &  & 13\% & 25\% & 23\% & 23\% & 18\% & 36\% & 33\% & 24\% \\
OM &  & 38\% & 36\% & 30\% & 27\% & 11\% &  7\% &  6\% &  5\% \\
IO &  & 46\% & 27\% & 30\% & 33\% & 32\% & 21\% & 20\% & 19\%
\end{tabular}
\caption{\label{table:percentages}Fraction of time spent in matrix product (LA), polynomial products (PP), other matrix operations (OM), and other interpreted operations (IO).}
\end{center}
\end{small}
\end{table}
\vskip-7pt

Comparing the columns Rec and, for instance, S, shows that linear algebra does not take the main part of the calculation time, although its theoretical complexity dominates.
In this regard, we have been very cautious in our implementation to avoid any interpreted quadratic loops.
Still, the result is that those quadratic tasks dominate the computation time.
Details are given in Table~\ref{table:percentages}.
The conclusion is that having implemented the algorithms in an interpreted language tends to parasitize the benchmarks.
For comparison sake, we have also added timings for variants BZI and vdHI that use an interpreted matrix product.
They both show the growth expected in theory, as well as the ratio from~8 to~96 announced in Table~\ref{table:MM}.



\section{Conclusions, future work}

Because of space limitation, various extensions could not be covered here.
More results
on the complexity of non-commutative multiplication of skew polynomials
will be
presented in an upcoming extended version~\cite{LongVersion}.
Topics like multiplication of skew
polynomials with unbalanced degrees and orders, or with sparse support, will be treated there.
The case of
rational (instead of polynomial) coefficients will also be considered.
The methods of this
article extend to multiplication of more general skew polynomials, in one or several variables,
including for instance -recurrences and partial differential operators.

The constants in Table~\ref{table:MM} are all somewhat pessimistic.
Tighter bounds can be obtained by, on the one hand, relaxing the naive assumption~\eqref{eq:naive-abc}, on the other hand, taking advantage of the special shapes (banded, trapezoidal, etc) of the various matrices.

We also plan to provide a lower-level implementation.
Hopefully, the timings would then reflect the theoretical results even better and will be close to those of naked matrix products.



\smallskip\noindent{\bf Acknowledgments.} This work was supported in part by the French National Agency for Research (ANR Project ``Gecko'') and the Microsoft Research-INRIA Joint Centre.
We thank the three referees for their valuable comments.

\scriptsize
\begin{thebibliography}{10}

\bibitem{Bernstein}
D.~J. Bernstein.
\newblock Fast multiplication and its applications.
\newblock To appear in Buhler-Stevenhagen {\it Algorithmic number theory}.

\bibitem{LongVersion}
A.~Bostan, F.~Chyzak, and N.~Le~Roux.
\newblock Skew-polynomial products by evaluation and interpolation.
\newblock In preparation.

\bibitem{LCLMs}
A.~Bostan, F.~Chyzak, Z.~Li, and B.~Salvy.
\newblock Common multiples of linear ordinary differential and difference
  operators.
\newblock In preparation.

\bibitem{BoSc05}
A.~Bostan and {\'E}.~Schost.
\newblock Polynomial evaluation and interpolation on special sets of points.
\newblock {\em Journal of Complexity}, 21(4):420--446, August 2005.

\bibitem{CaKa91}
D.~G. Cantor and E.~Kaltofen.
\newblock On fast multiplication of polynomials over arbitrary algebras.
\newblock {\em Acta Inform.}, 28(7):693--701, 1991.

\bibitem{Ore_algebra}
F.~Chyzak.
\newblock \url{http://algo.inria.fr/chyzak/mgfun.html}.

\bibitem{CoWi90}
D.~Coppersmith and S.~Winograd.
\newblock Matrix multiplication via arithmetic progressions.
\newblock {\em Journal of Symbolic Computation}, 9(3):251--280, Mar. 1990.

\bibitem{GaGe97}
J.~\gathen{von zur} Gathen and J.~Gerhard.
\newblock Fast algorithms for {T}aylor shifts and certain difference equations.
\newblock In {\em Proceedings of ISSAC'97}, pages 40--47, New York, 1997. ACM
  Press.

\bibitem{GaGe99}
J.~\gathen{von zur} Gathen and J.~Gerhard.
\newblock {\em Modern computer algebra}.
\newblock Cambridge University Press, 1999.

\bibitem{Gerhard00}
J.~Gerhard.
\newblock Modular algorithms for polynomial basis conversion and greatest
  factorial factorization.
\newblock In {\em RWCA'00}, pages 125--141, 2000.

\bibitem{vdHoeven02}
J.~\hoeven{van der} Hoeven.
\newblock F{FT}-like multiplication of linear differential operators.
\newblock {\em Journal of Symbolic Computation}, 33(1):123--127, 2002.

\bibitem{Kaporin04}
I.~Kaporin.
\newblock The aggregation and cancellation techniques as a practical tool for
  faster matrix multiplication.
\newblock {\em Theor. Comput. Sci.}, 315(2-3):469--510, 2004.

\bibitem{ScSt71}
A.~Sch\"onhage and V.~Strassen.
\newblock {S}chnelle {M}ultiplikation gro\ss er {Z}ahlen.
\newblock {\em Computing}, 7:281--292, 1971.

\bibitem{Strassen69}
V.~Strassen.
\newblock {G}aussian elimination is not optimal.
\newblock {\em Numerische Mathematik}, 13:354--356, 1969.

\bibitem{Kan}
N.~Takayama.
\newblock \url{http://www.math.kobe-u.ac.jp/KAN/}.

\end{thebibliography}

\end{document}
