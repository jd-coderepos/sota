\documentclass{article}

\usepackage[preprint]{neurips_2020}

\usepackage{natbib}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{wrapfig}

\newcommand\hmmax{0}
\newcommand\bmmax{0}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{url}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{calc}
\usepackage{booktabs}
\usepackage{nccmath}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{multirow}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage[subrefformat=parens]{subcaption}

\title{gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window}

\author{Mocho Go \qquad Hideyuki Tachibana\\
    \\
    PKSHA Technology, Inc.\\
    Hongo, Bunkyo City, Tokyo, Japan\\
}

\begin{document}

\maketitle

\begin{abstract}
    Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently.
    Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.
\end{abstract}

\section{Introduction}
Since the great success of AlexNet~\citep{krizhevsky2012imagenet} in ILSVRC2012 ushered in the era of deep learning, convolutional neural networks (CNNs)~\citep{fukushima1980neocognitron,lecun1989backpropagation,zeiler2014visualizing,szegedy2015going,simonyan2014very,he2016deep,tan2019efficientnet}, which naturally take into account the spatial shift invariance and the scale hierarchy, have been dominant in the image domain for the past decade. On the other hand, looking outside the image domain, the Transformer architecture based on the stacked multi-head self-attention (MSA)~\citep{vaswani2017attention} that directly connects one token to another has been proposed in the language domain, and Transformer-based models such as BERT~\citep{devlin2018bert} and GPT~\citep{radford2018improving} have had great success.
Following the success in the language domain, there has been active research in recent years on the application of transformers in the vision domain, viz.\ the Vision Transformer (ViT)~\citep{dosovitskiy2020image}, with remarkably promising results.

A shortcoming of Transformer is that its multi-head attention is so huge that the module is computationally expensive and data inefficient.
For example, ViT has too large data capacity, and is not adequately trained on a medium-sized dataset such as ImageNet-1K~\citep{deng2009imagenet}.
To remedy such problems, a number of improved ViTs and learning strategies have been proposed (Section~\ref{label:005}).
In this vein, some Transformer architectures which explicitly incorporate the visual hierarchy have been proposed, including the Swin Transformer~\citep{liu2021swin}, with the motivation that the scale hierarchy, an important property of vision, is naturally incorporated in CNNs but not in Transformers (Section~\ref{label:007}).

Another important stream of research was model simplification, in particular a series of methods using only MLPs~\citep{tolstikhin2021mlp,touvron2021resmlp,liu2021pay}. These methods were proposed based on the question of whether self-attention modules are really necessary for the vision tasks and the fact that an MLP can approximate arbitrary functions~\citep{hornik1989multilayer} without special structures such as convolution or self-attention (Section~\ref{label:006}).

In this paper, we propose \textit{gSwin} as a method to reintegrate the two pathways that are evolving in different directions after breaking off from ViT. Our gSwin is based on the basic framework of gMLP, which consists of MLP and a simple gating mechanism, and the specific process uses the same window function as Swin Transformer to capture the visual hierarchical structure.
According to our experiments, gSwin-T outperforms Swin-T, +0.4 Top-1 accuracy on ImageNet-1K, +0.5 box AP and +0.4 mask AP on COCO and +1.9 mIoU on ADE20K, and gSwin-S is competitive with Swin-S, while both gSwin models are smaller than their Swin Transformer counterparts.

\section{Related Work}\label{label:000}
\begin{figure*}[t]
    \centering
    \subfloat[ViT]{
        \includegraphics[height=3.65cm]{fig_000.pdf}
        \label{label:001}
    }
    \centering
    \qquad
    \subfloat[gMLP]{
        \includegraphics[height=3.65cm]{fig_001.pdf}
        \label{label:002}
    }
    \centering
    \qquad
    \subfloat[Swin Transformer]{
        \includegraphics[height=3.65cm]{fig_002.pdf}
        \label{label:003}
    }
    \caption{
        Comparison of existing vision models.
        Layers for specific downstream tasks are omitted for brevity.
    }
    \label{label:004}
\end{figure*}

\subsection{Transformers for Vision}\label{label:005}

Although the mechanism of attention that focuses directly on a specific region in an image has actually been studied in the image domain for quite some time~\citep{mnih2014recurrent,xu2015show,parmar2018image,woo2018cbam,cordonnier2019relationship,hu2018squeeze,zhang2020resnest}, it is only recently that vision models built principally on self-attention mechanisms have been proposed~\citep{carion2020end,wu2020visual,dosovitskiy2020image}. Of these, ViT~\citep{dosovitskiy2020image} (Figure~\ref{label:001}) produced competitive results and marked the beginning of research on vision models based on the Transformer architecture.
Although it has only been a short time since they were proposed, Transformer for vision has become popular, 
and it has also been applied in related domains including video~\citep{liu2021video,arnab2021vivit} and 3D point cloud~\citep{guo2021pct}.

In the 2D vision domain, a vast amount of research to improve ViT has been actively conducted. 
One trend is oriented toward improving learning strategies, such as DeiT~\citep{touvron2021training} and MoCo~v3~\citep{chen2021empirical}.
Another direction is the improvement of model architectures,
including
CaiT~\citep{touvron2021going},
DeepViT~\citep{zhou2021deepvit},
HaloNet~\citep{vaswani2021scaling},
etc.
A major stream of research on improving model structure is the incorporation of spatial structure in images, which has not been explicitly used in Transformers, except for the customary use of positional encoding and the patch partitioning before the initial layer.
An approach of incorporating spatial structure is the consideration of hybrid architectures with the auxiliary use of convolution,
e.g.\ BoTNet~\citep{srinivas2021bottleneck}, ConViT~\citep{d2021convit}, LeViT~\citep{graham2021levit}, CvT~\citep{wu2021cvt}, LocalViT~\citep{li2021localvit}, etc.
Others adopted hierarchical structure that lacks in vanilla Transformer models despite its importance in vision domain (See Section~\ref{label:007}), e.g.\ 
T2T-ViT (local token aggregation)~\citep{yuan2021tokens}, HVT (pooling-based aggregation)~\citep{pan2021scalable},
TNT (alternating local/global self-attention)~\citep{han2021transformer},
MViT (fine-to-coarse aggregation using multi-head pooling attention)~\citep{fan2021multiscale},
PVT (pyramid structure for dense prediction)~\citep{wang2021pyramid}, ViL (efficient transformer on hierarchical structure)~\citep{zhang2021multi},
Swin Transformer (restricted self-attention in shifted windows)~\citep{liu2021swin}, etc.

\subsection{MLP Models for Vision}\label{label:006}

Looking back in history, the neural models of image recognition have been the multi-layer perceptron (MLP) since the early days when \citet{rosenblatt1958perceptron} proposed the perceptron in 1950s. Nevertheless, in the modern context, MLP-Mixer~\citep{tolstikhin2021mlp}, ResMLP~\citep{touvron2021resmlp} and gMLP~\citep{liu2021pay}, which were proposed almost simultaneously, would be the pioneers of the recent trend of using only axis-wise MLPs,
or more precisely, axis-wise dense affine projections with non-linear activation functions.

Of these, MLP-Mixer and ResMLP simply apply axis-wise MLPs in channel and spatial axes alternately. Following these studies, a number of vision models based on axis-wise MLPs have been proposed recently. One of the current trends is the approach of constructing complex receptive fields despite being MLPs, by reshaping the tensors in the intermediate layers,
e.g. Hire-MLP~\citep{guo2021hire}, Vision Permutator~\citep{hou2022vision}.
Some methods in this line even excluded the spatial-mixing MLPs, e.g.\ CycleMLP~\citep{chen2021cyclemlp}, S-MLP~\citep{yu2022s2}, AS-MLP~\citep{lian2021mlp}. In these methods, only the channel-mixing MLPs are used, and the exchange of information between tokens is taking place by spatially shifting each channel separately. 

A method that deserves special mention in a different line is the gMLP~\citep{liu2021pay} (Figure~\ref{label:002}). Its noteworthy characteristic is the use of the spatial gating unit (SGU), a gating mechanism for spatial mixing, which mimics an attention mechanism. Another feature of this method is its simplicity: only one linear (fully-connected projection) layer is used where the stacking of two or more dense layers are required in other related methods. The gMLP model has achieved competitive performance both in language and vision domains with standard Transformer models BERT~\citep{devlin2018bert} and ViT~\citep{dosovitskiy2020image}. It is also reported that the gMLP could learn shift-invariant features. 

\subsection{Hierarchy and Locality of Vision}\label{label:007}
In neuroscience, a hierarchical structure of visual cortex has already been argued in 1960s by~\citet{hubel1962receptive}.
It has been a common understanding that local patches of images are matched with Gabor-like patterns in the early stage viz.\ primary visual cortex (V1), and the results are integrated at later stages for more global and intelligent information processing~\citep{marcelja1980mathematical}. 
Neocognitron~\citep{fukushima1980neocognitron}, one of the earliest CNN models, was based on the insights from physiological hierarchy model above. Subsequent CNN models in the deep learning era also naturally utilize hierarchical structures of images. 

In contrast, neither Vision Transformers nor Vision MLPs had structures that inherently incorporates the `spatial inductive bias' of visual hierarchy.
Therefore, from such motivations it is natural to consider integrating the vision hierarchy model into Transformers, as shown in Section~\ref{label:005}. In addition to the physiological motivations, such a built-in hierarchical structure has also an advantage that the output of intermediate layers can be used in downstream tasks that require multiple resolutions, including the semantic segmentation as were in e.g.\ UPerNet~\citep{xiao2018unified}.
Furthermore, divide-and-conquer-like hierarchical architectures that often logarithmically reduce the computational complexity will naturally improve the efficiency.

One such proposal is the Swin Transformer~\citep{liu2021swin}. It first divides the image into many small patches and applies self-attention within those patches, which are shifted by half for each layer. The structure is then gradually integrated from local to global in a bottom-up manner.
(Figure~\ref{label:003}.)
Particular essentials in the structure of Swin Transformer are the bottom-up hierarchical structure for each resolution and the shifting of windows. Indeed, it has been reported that as long as this structure is maintained, the attention can actually be replaced by average pooling (PoolFormer), or even an identity map, although performance is slightly reduced~\citep{yu2021metaformer}. This fact would be a evidence that the hierarchical structure of the Swin Transformer is advantageous as a vision model.

\section{Methodology}\label{label:008}
\subsection{Overall Network Architecture of gSwin}
\begin{figure}
    \centering \includegraphics[width=0.7\linewidth]{fig_003.pdf}\caption{Overall Architecture of gSwin.}\label{label:009}\end{figure}
In this paper, we aim to achieve high performance while the computational and parameter efficiency is on par with the existing methods, by incorporating both merits of the two methods that have evolved in different directions from ViT~\citep{dosovitskiy2020image} (Figure~\ref{label:001}). In particular, we consider the two daughters of ViT, viz.\ gMLP~\citep{liu2021pay} (Figure~\ref{label:002}) and Swin Transformer~\citep{liu2021swin} (Figure~\ref{label:003}).
From Figure~\ref{label:004}, we may observe that gMLP’s overall structure is largely the same as ViT, but the structure of individual blocks has been significantly modified. On the other hand, Swin Transformer’s overall structure has been significantly modified in terms of the introduction of a hierarchical structure, but the internal structure of the individual blocks is largely the same as in ViT, except for the introduction of shifted windows.
Thus, there is little overlap and a high degree of independence in the differences from ViT in each of these methods. Therefore, by incorporating both at the same time, higher improvements can be expected.

Based on these observations, the authors propose \textit{gSwin} as a new vision model that inherits features of both gMLP and Swin Transformer. Figure~\ref{label:009} shows the architecture of gSwin. In the following subsection, we will discuss the methods of Window-SGU.

Note that this is not the same as nor a variant of MetaFormers~\citep{yu2021metaformer}, which is a general form for a series of models in which the self-attention module is replaced with another module in Swin Transformer. In this study, they considered using gMLP module instead of self-attention, but the architecture is not the same as our gSwin. For example, gSwin does not have subsequent LayerNorm, MLP and residual connections after each gMLP block. Our method is more parameter efficient, which will be shown experimentally later in this paper.

\subsection{Spatial Gating Unit (SGU) and Window-SGU}\label{label:010}

Let us first recall the gMLP~\citep{liu2021pay} model. Figure~\ref{label:002} shows the overall architecture of gMLP,
which consists of stacking of axis-wise fully-connected layers, layer normalization~\citep{ba2016layer}, GELU~\citep{hendrycks2016gaussian} activation function and the spatial gating unit (SGU).
Here, let us briefly introduce the SGU. Let  be the input features,
and let  be the reshaped and split features .
Then the SGU performs the following operation,

where  and   are trainable parameters, and  denotes the element-wise multiplication.
Such a gating mechanism allows us to explicitly incorporate the second-order terms  associated with the binomial relationship between all the site pairs  and .
The gMLP paper does not go so far as to say that this is the primary reason for the effectiveness of Transformers (in which up to ternary relations are incorporated), but the SGU could be an alternative mechanism to attention to explicitly incorporate higher-order relations, implicitly assuming that it could be a contributing factor. According to \citep[Figures 3]{liu2021pay}, the resulting learned weight parameters  were almost-sparse Gabor-like patterns. Furthermore, \citep[Figure 9]{liu2021pay} suggests that Toeplitz-like (i.e., shift invariant) structures are automatically acquired. These facts imply that it is not necessary to model  as a matrix with full degrees of freedom. Instead, it would be sufficient to consider only local interactions by partitioning the features into small patches.

Based on this motivation,
in this paper, we propose the Window-SGU module, which has basically the same structure as the SGU module of gMLP, but instead of the entire image, it is divided into small patches and token mixing is performed independently in each of them.
That is, we first split the input feature 
into small patches  where .
(The areas that extend beyond the  region during this patch partitioning are handled with zero padding as usual.)
After the patch partitioning, each patch  is reshaped and split into
, and then the SGU is applied to each patch independently as follows,

where 
.
The resulting -s are again tiled and the boundaries are processed appropriately to obtain .

If the patch partitioning is always fixed, there will always be elements that are in the neighborhood but do not interact, resulting in block noise. To prevent this, it is necessary to change the patch partitioning as necessary so that interaction between neighboring elements occurs.
For layers with shifts, we cannot use cyclic-shifting as proposed in \citep{liu2021swin} because SGUs lack the ability of masking, which self-attention has. Then naive approach is to add zero-padding so that we have uniform-shaped windows, which costs FLOPs. Here, we propose another approach equivalent to zero-padding but more efficient; the Padding-free shift.
Let the shape of windows be , then we have 9 groups of windows of shape , located at upper-left, upper, upper-right, left, center, etc. Calculate SGUs independently for each group and then concatenate all windows. The weight and bias of each group are copied from those of the main group at the center of shape  properly; e.g.,  is defined as

The Padding-free shift reduces FLOPs of gSwin-T from 3.8G (zero-padding) to 3.6G.

\section{Experiment}\label{label:012}

In this paper, following previous studies, we conducted evaluation experiments on the following three benchmark tasks: 
image classification using ImageNet-1K~\citep{deng2009imagenet}, and object detection
using MS-COCO~\cite{lin2014microsoft}
and semantic segmentation using ADE20K~\citep{zhou2019semantic} as downstream tasks.

\subsection{Image Classification}
\begin{table}[!t]
	\centering
    {
	\begin{tabular}{cccc}
        \toprule
        Method &  & \#layer & \#heads \\
        \midrule
        gSwin-VT &  &  &  \\
    	gSwin-T  &  &  & \\
    	gSwin-S  &  &  & \\
    	\bottomrule
    \end{tabular}
    }
	\caption{
		Parameter settings of our method.
	}
    \label{label:013}
\end{table}
\begin{table}[!t]
	\centering
    {
	\begin{tabular}{cccc}
        \toprule
        Method & ImageNet & COCO & ADE20K \\
        \midrule
        gSwin-VT &  &  &  \\
    	gSwin-T  &  &  & \\
    	gSwin-S  &  &  & \\
    	\bottomrule
    \end{tabular}
    }
	\caption{
		Parameter settings of drop path rates.
	}
    \label{label:014}
\end{table}
\begin{table}[!t]
	\centering
    {
	\begin{tabular}{lrrr}
		\toprule
		method          & \#param. & FLOPs & Top-1 acc. \\
		\midrule
		DeiT-S
		& 22M      & 4.6G  & 79.8\%     \\
		DeiT-B
		& 86M      & 17.5G & 81.8\%     \\
		Swin-T
		& 28M      & 4.5G  & 81.29(3)\%     \\
		Swin-S
		& 50M      & 8.7G  & 83.02(7)\%     \\
		\midrule
		gMLP-Ti
		& 6M       & 1.4G  & 72.3\%     \\
		gMLP-S
		& 20M      & 4.5G  & 79.6\%     \\
		gMLP-B
		& 73M      & 15.8G & 81.6\%     \\
		Hire-MLP-Ti
		& 18M      & 2.1G  & 79.7\%     \\
		Hire-MLP-S
		& 33M      & 4.2G  & 82.1\%     \\
		Hire-MLP-B
		& 58M      & 8.1G  & 83.2\%     \\ 
		\midrule
		PoolFormer-S12
		& 12M      & 2.0G  & 77.2\%     \\
		PoolFormer-S24
		& 21M      & 3.6G  & 80.3\%     \\
		PoolFormer-S36
		& 31M      & 5.2G  & 81.4\%     \\
		PoolFormer-M36
		& 56M      & 9.1G  & 82.1\%     \\
		PoolFormer-M48
		& 73M      & 11.9G & 82.5\%     \\
		\midrule
		gSwin-VT (ours) & 16M      & 2.3G  & 80.32(1)\%     \\
		gSwin-T (ours)  & 22M      & 3.6G  & 81.71(5)\%     \\
		gSwin-S (ours)  & 40M      & 7.0G  & 83.01(4)\%     \\
		\bottomrule
	\end{tabular}
	}
	\caption{
		ImageNet-1K~\cite{deng2009imagenet} classification top-1 accuracy . For all methods, the input image has  resolution.
	}
    \label{label:015}
\end{table}
The experimental condition was as follows.
The dataset was ImageNet-1K~\citep{deng2009imagenet}, divided into training and validation sets consisting of 1.3M and 50k images, respectively.

Table~\ref{label:013} shows the parameter settings
for three variants of the proposed method (we will explain about the \#heads column in Section~\ref{label:019}).
We set window size to  for all gSwin models.

The training conditions were as follows (most of them align with those of Swin Transformer~\citep{liu2021swin} for fair comparison).
The optimizer was AdamW~\citep{kingma2014adam}.
Data augmentation and regularization techniques
were adopted including 
randaugment~\citep{cubuk2020randaugment,rw2019timm},
and the drop-path regularization~\citep{larsson2016fractalnet}, whose rates are shown in Table~\ref{label:014} (tuned by grid search).
The initial learning rate was 0.001 and the weight decay was 0.05.
The input image size was , the batch size was 1024 and the number of epochs was 300 with cosine scheduler (first 20 epochs for linear warm-up).
Of these, the epoch with the best top 1 accuracy in the validation dataset was selected (in case of a tie, the selection was based on the top 5 accuracy). The training was performed twice with different seeds. We report the average (and unbiased variance) values of the two trials.

We compared the image recognition performance of our method with that of DeiT~\citep{touvron2021training}, Swin Transformer~\citep{liu2021swin}, gMLP~\citep{liu2021pay}
and Hire-MLP~\citep{guo2021hire}.
Table~\ref{label:015} shows the results.
Compared to Swin Transformers, gSwin-T achieves +0.4\% top 1 accuracy with 21\% less parameters, and gSwin-S achieves the same accuracy with 20\% less.
We may observe that gSwin (proposed) is more efficient in terms of the number of parameters and floating point operations to achieve the same level of accuracy with others.

\subsection{Object Detection}
\begin{table}[!t]
	\centering
	{
	\begin{tabular}{lrrr}
		\toprule
		method          & \#param. &  &  \\ \midrule
		Swin-T
		& 86M      & 50.52(5)\% & 43.78(6)\% \\
		Swin-S
		& 107M     & 51.98(3)\% & 44.99(6)\% \\ \midrule
		PoolFormer-S12
		& 32M & 37.3\% & 34.6\% \\
		PoolFormer-S24
		& 41M & 40.1\% & 37.0\% \\
		PoolFormer-S36
		& 51M & 41.0\% & 37.7\% \\ \midrule
		gSwin-VT (ours) & 73M      & 49.48(2)\% & 42.87(2)\%   \\
		gSwin-T (ours)  & 79M      & 50.97(2)\% & 44.16(3)\%   \\
		gSwin-S (ours)  & 97M      & 52.00(7)\% & 45.03(5)\%   \\
		\bottomrule
	\end{tabular}
	}
	\caption{
		COCO~\cite{lin2014microsoft} object detection and instance segmentation.
	}
	\label{label:016}
\end{table}
We performed an evaluation experiment with object detection and instance segmentation tasks, to test performance of gSwin on downstream tasks. The dataset we used in this experiment was COCO~\citep{lin2014microsoft}. The evaluation metric was box and mask AP (average precision).
We used Cascade Mask R-CNN~\citep{He2017MaskR, Cai2018CascadeRD} as object detection framework, as Swin Transformer~\citep{liu2021swin} does.

When we train the object detection, we reused checkpoints of the ImageNet-1K experiment rather than training a new model in a full-scratch fashion.
The specific procedure was as follows. Firstly, the checkpoint model that achieved the highest accuracy on ImageNet-1K validation dataset was used as the starting point. Then, we continued training for 36 epochs on 4 V100 GPUs, where the batch size was 4 for each GPU.
The optimizer was AdamW~\citep{kingma2014adam}, the initial learning rate was 0.0001, and the weight decay was 0.05.
We used multi-scale training (the input image was resized so that the shorter is between 480 and 800 pixels and the longer is at most 1333).
During the training, we evaluated the AP scores for every epoch and selected the best one with the highest box AP. This transfer learning was done three times using different random seeds, which resulted in 6 different models. We report the average score of these 6 models.

From results in Table~\ref{label:016}, we observe that gSwin-T achieves +0.45/+0.38 box/mask AP with 8\% less parameters, and gSwin-S achieves similar APs with 9\% less parameters.

\subsection{Semantic Segmentation}
\begin{table}[!t]
	\centering
	{
	\begin{tabular}{lrrr}
		\toprule
		method          & \#param. & mIoU    & mIoU(aug) \\ \midrule
		Swin-T
		& 60M      & 44.3(1)\% & 45.8(1)\% \\
		Swin-S
		& 81M      & 47.7(1)\% & 49.1(1)\% \\ \midrule
		PoolFormer-S12
		& 16M & 37.2\% & \\
		PoolFormer-S24
		& 23M & 40.3\% & \\
		PoolFormer-S36
		& 35M & 42.0\% & \\
		PoolFormer-M36
		& 60M & 42.4\% & \\
		PoolFormer-M48
		& 77M & 42.7\% & \\ \midrule
		gSwin-VT (ours) & 45M      & 43.4(1)\% & 45.1(1)\%   \\
		gSwin-T (ours)  & 52M      & 46.1(1)\% & 47.6(1)\%   \\
		gSwin-S (ours)  & 70M      & 48.2(1)\% & 49.7(1)\%   \\
		\bottomrule
	\end{tabular}
	}
	\caption{
		ADE20K~\cite{zhou2019semantic} semantic segmentation.
	}
	\label{label:017}
\end{table}
We also performed an evaluation experiment with an image segmentation task. The dataset we used in this experiment was ADE20K~\citep{zhou2019semantic}. The evaluation metric was mIoU (mean intersection over union).
We used UPerNet~\citep{xiao2018unified} as base framework, as in \citep{liu2021swin}.

When we train the semantic segmentation, we reused checkpoints of the ImageNet-1K experiment.
The specific procedure was as follows. Firstly, the checkpoint model that achieved the highest accuracy on ImageNet-1K validation dataset was used as the starting point. Then, we continued training for 160k steps with linear scheduler (first 1.5k steps for linear warm-up) on 4 GPUs, where the batch size was 4 for each GPU. The optimizer was AdamW~\citep{kingma2014adam}, the initial learning rate was , the weight decay was 0.01, and the input image size was . The augmentations we adopted was random horizontal flip, random rescaling (the resolution ratio within ) and random photometric distortion, as in Swin Transformer~\citep{liu2021swin}. During the training, we evaluated the mIoU score for every 4k steps and selected the best one with the highest validation score. This transfer learning was done three times using different random seeds, which resulted in 6 different models. We report the average score of these 6 models.

Results on ADE20-K are shown in Table~\ref{label:017}, where the mIoU(aug) column is the score with test-time multi-scale ( resolution) horizontal-flip augmentation.
These show that gSwin-T achieves +1.8/+1.85\% mIoU with/without augmentation with 13\% less parameters, and gSwin-S achieves +0.46/+0.57\% mIoU with 14\% less parameters, compared to Swin Transformer.

\subsection{Ablation Studies}

\begin{table}[!t]
	\centering
	{
	\begin{tabular}{l|r|rr}
		\toprule
		         & ImageNet     & \multicolumn{2}{c}{ADE20K} \\
		\#head.  & Top-1        & mIoU      & mIoU(aug) \\ \midrule
		1*       & 80.91\%      & 43.8(1)\% & 45.3(1)\% \\
		3*       & 81.46\%      & 45.5(3)\% & 47.2(3)\% \\
		6        & 81.61(1)\%   & 45.8(1)\% & 47.2(1)\% \\
		12       & 81.71(5)\%   & 46.1(1)\% & 47.6(1)\% \\
		24       & 81.80(9)\%   & 46.2(1)\% & 47.7(1)\% \\
		48       & 81.78(10)\%  & 45.7(1)\% & 47.1(1)\% \\ \midrule
		12            & 81.71(5)\% & 46.1(1)\% & 47.6(1)\% \\
		12 (no pos.)* & 81.27\%    & 44.8(1)\% & 46.5(1)\% \\
		\bottomrule
	\end{tabular}
	}
	\caption{
		Ablation study on the number of heads and relative positional bias in gSwin-T. * indicates that only one checkpoint for ImageNet-1K was trained.
	}
	\label{label:018}
\end{table}

\subsubsection{Multi-Head}\label{label:019}

The SGU in \cite{liu2021pay} is uniform in the channel axis, and we propose the \textit{multi-head} SGU, which improves the accuracy of Transformer-based models \cite{vaswani2017attention}.
The multi-head SGU changes the term  in \eqref{label:011} as follows,

and the channel-axis of  and  are grouped into  heads ( is assumed to be a divisor of ) as follows:
\begin{itemize}
    \item 1st head: ,
    \item 2nd head: ,
    \item ,
    \item -th head: .
\end{itemize}
In the case , this reduces to the normal SGU; .
The parameters to be learned increase linearly as  increases, but it can be easily checked that the FLOPs are independent from the choice of .
The Window-SGU limits the representation capacity of spatial interactions, which can be observed from the fact that the number of parameters in  is reduced from  to  (the original gMLP uses  tokens, therefore the typical ratio is ).
Increasing  allows the model to interact tokens in more complicated patterns at once and helps to keep the model capacity to represent complex spatial interactions.

We did ablation studies on the choice of the hyper parameter  using gSwin-T model.
Table~\ref{label:018} shows the results for , and we found that the single-head model is much worse than multi-head models, the accuracy increases as  increases, and it saturates at , which coincides the ratio  in the argument about the model capacity.
From these results, we chose  for gSwin-T and gSwin-S ( for gSwin-VT, to keep model small).

\subsubsection{Relative position bias}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig_004.pdf}
    \caption{A head of the 11th layer of the 3rd block of gSwin-T with relative positional bias.}
    \label{label:020}
    \centering
    \includegraphics[width=0.7\linewidth]{fig_005.pdf}
    \caption{A head of the 11th layer of the 3rd block of gSwin-T without relative positional bias.}
    \label{label:021}
\end{figure}

\cite{liu2021swin} used a relative positional bias to enhance attention mechanisms with the inductive bias of shift-invariance, which gives significant improvements to Swin Transformer, but as discussed in Section~\ref{label:010}, \cite{liu2021pay} showed that the SGU can learn local and shift-invariant spatial-interactions from scratch, which is not inherent in attention mechanisms.
We modified  in the SGU to , where bias term  has the same shape as  but its components depend only on relative positions.
We observed that out gSwin can be trained without this term, but it helps gSwin to achieve better accuracy with negligible increase in model size, as shown in Table~\ref{label:018}.
Figure~\ref{label:020} and Figure~\ref{label:021} show some (total) weights  in case with or without the bias term, and we may observe that both can learn shift-invariance.

\section{Conclusion}\label{label:022}

We proposed gSwin, a new vision MLP which merges (multi-head) gMLP and Swin Transformer, as a version of gMLP with the ability to generate hierarchical feature maps and compatible with downstream tasks where feature pyramids are crucial.
gSwin achieves more efficient performances than Swin Transformer on ImageNet-1K, COCO and ADE20K, and could be applied to other downstream tasks just as Swin Transformer is widely used.
We hope that vision MLPs attract attentions again.

\section*{Acknowledgments}
This paper is based on results obtained from a project subsidized by the New Energy and Industrial Technology Development Organization (NEDO).

\bibliography{main}
\bibliographystyle{mystyle}
\end{document}
