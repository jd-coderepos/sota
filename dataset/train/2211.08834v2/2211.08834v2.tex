\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{xfrac}
\usepackage{multirow}
\usepackage{pifont}

\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{breqn}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{wasysym}

\usepackage{stfloats}
\usepackage{float}
\usepackage{makecell}
\usepackage{wrapfig,lipsum}

\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}





\newcommand{\rone}{{\color{WildStrawberry}\textbf{R-Vo42}}}
\newcommand{\rtwo}{{\color{RoyalBlue}\textbf{R-ehDV}}}
\newcommand{\rthr}{{\color{ForestGreen}\textbf{R-uk8J}}}

\newcommand{\mfvis}{{\color{green}5}}
\newcommand{\mf}{{\color{green}6}}
\newcommand{\vita}{{\color{green}13}}
\newcommand{\minvis}{{\color{green}14}}
\newcommand{\ifc}{{\color{green}15}}
\newcommand{\seqformer}{{\color{green}31}}
\newcommand{\idol}{{\color{green}32}}
\newcommand{\tevit}{{\color{green}37}}

\newcommand{\ovis}{{\color{green}27}}

\newcommand{\memot}{{\color{green}2}}
\newcommand{\motr}{{\color{green}38}}
\newcommand{\trackformer}{{\color{green}24}}

\newcommand{\rr}{{\textcolor{WildStrawberry}{\textbf{R-Vo42}}}}
\newcommand{\rb}{{\textcolor{RoyalBlue}{\textbf{R-ehDV}}}}
\newcommand{\rg}{{\textcolor{ForestGreen}{\textbf{R-uk8J}}}}

\def\cvprPaperID{3917} \def\confName{CVPR}
\def\confYear{2023}

\begin{document}

\title{A Generalized Framework for Video Instance Segmentation}  

\maketitle
\thispagestyle{empty}
\appendix

We sincerely appreciate all three reviewers -- \rone{}, \rtwo{}, and \rthr{} -- for their thoughtful reviews and constructive feedback.
We are grateful for the positive evaluations from the reviewers that our work ``is well-motivated and clearly presented'' [\rr, \rb], ``studies a novel problem'' [\rg], and ``is worth reading by researchers in the community'' [\rr].
We also thank the remarks that GenVIS is a \textbf{\textit{general framework}} that can ``handle both online and semi-online settings by changing clip-length'' [\rr, \rb] and can be ``seamlessly inserted into existing methods'' [\rg].
We address all concerns and questions below, and we shall reflect every feedback for the revision.

\noindent \textbf{[\rr] Dependence on a large amount of labels.}
Designed from a frame-independent detector~[\mf{}], MinVIS~[\minvis{}] has a great video-data efficiency.
Despite using only 10 of the data, it still performs competitively, with  AP decrease compared to using all labels on OVIS~[\ovis{}].
On the contrary, the effectiveness of GenVIS is largely from its ability to model sequential characteristics of videos from consecutive clips.
Thus, GenVIS does require more video-wise labels than MinVIS.
Following the experimental settings of MinVIS, we trained GenVIS using only 10 of labels on OVIS with Swin-L.
As can be expected, the reduction of video-wise labels hinders the temporal understanding: it results in drop of  AP () compared to full labels.
Our understanding is that GenVIS, which necessitates consecutive frames, faces challenges in learning diverse appearances compared to MinVIS, which uniformly samples frames, despite having the same limited number of frames.





\noindent \textbf{[\rr] Online inference speed.}
In Fig {\color{Red}3}, we analyze the trade-offs between performance and speed in online \& semi-online setups.
Thanks to its generalized property, GenVIS provides multiple options where users can consider either complexity of datasets or targeted runtime.
Among the varying number of clip lengths, GenVIS shows 18.7 fps on the OVIS benchmark under the online setting ().
We measured the inference speeds of MinVIS~[\minvis{}] and IDOL~[\idol{}] under a fair evaluation environment, and each achieves 28.1 and 2.1\footnote{IDOL shows the slow inference speed due to its increasing computations with respect to the number of frames for post-processing.} fps, respectively.









\noindent \textbf{[\rr] Handling similar instances.}
The key observation of MinVIS~[\minvis{}] is that each object query implicitly embeds appearances and regional attributes.
Therefore, MinVIS associates instances without explicit properties (\emph{e.g.}, size, IoU, and centerness), but uses cosine similarities between query vectors.
Similarly, as we employ query embeddings as \textit{instance prototypes}, we find that GenVIS considers both appearances and regional properties of objects for the associations.
For example, although the animals in Fig {\color{Red}4} have analogous appearances, GenVIS successfully tracks all objects by considering their relative locations.



\noindent \textbf{[\rb, \rg] Accuracy with the increased number of frames in Tab 6 and Fig 3.}
Current offline VIS methods~[\mfvis{}, \vita{}, \seqformer{}] have difficulties in modeling sequential and temporal characteristics of long videos.
Therefore, as GenVIS uses such methods for predictions within a window, the longer window brings about \textbf{inaccurate intra-clip predictions} for videos with complicated trajectories, leading to the performance drop ( AP) on the challenging OVIS benchmark (see Fig {\color{Red}3} (b)).
On the contrary, there is a marginal performance drop ( AP) when using longer windows on YouTube-VIS 2019, which comprises relatively simple trajectories.
From the experiments, we would like to highlight our motivation that designing multiple inter-clip associations in a sequential manner is effective rather than simply enlarging a intra-clip window to handle challenging videos.





\noindent \textbf{[\rb] Accuracy gap between online and semi-online.}
Intuitively, as discussed in the above question, semi-online methods with an adequate window size have more potentials to achieve higher accuracy than online models.
However, compared to the semi-online versions of GenVIS, the online version can also achieve competitive accuracy as it stores more memories during evaluation.
Affected by these characteristics, the optimal accuracy of GenVIS can be obtained in different settings as each backbone and dataset have different aspects.
An interesting future direction would be to further enhance the intra-clip modeling, which would boost the performance of semi-online VIS, coupled with GenVIS.






\noindent \textbf{[\rg] Claim of generalization and ambiguity of title.}
As mentioned by the reviewers [\rr{}, \rb{}], we would like to note that our intention of the term, ``general'', is derived from the availability of running in both online and semi-online manners.
Although we agree that examining the plug-and-playability of our method on top of other offline modules could be an interesting experiment, we could not report the results due to the limited rebuttal period.
Taking the advice, we will further clarify the intention behind the name of GenVIS in the revision.


\noindent \textbf{[\rg] Performance compared to VITA in Table 1.}
In Tab {\color{Red}1}, we report the performance on YouTube-VIS 2019 and 2021, and we would like to point out that both benchmarks consist of relatively short sequences with monotonous trajectories.
Attributed to these characteristics, existing offline methods show competitive performance; for the coarse accuracy metric AP, VITA often achieves higher scores than GenVIS.
However, as shown in Tab {\color{Red}2} and {\color{Red}3}, GenVIS clearly shows significant improvements of \textit{all} metrics on the challenging benchmarks: YouTube-VIS 2022 and OVIS.


\noindent \textbf{[\rg] Analysis of failure cases.}
Due to the one-page limit, we shall add the analysis for the revision.



\end{document}
