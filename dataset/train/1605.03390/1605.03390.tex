\documentclass[proceedings]{aofa}



\usepackage[utf8]{inputenc}
\usepackage{subfigure}





\author[J. Gaither and M.~D. Ward]{Jeffrey Gaither\addressmark{1}\and
  Mark Daniel Ward\addressmark{2}\thanks{M.~D. Ward's work is
    supported by the Center for Science of Information (CSoI), an NSF
    Science and Technology Center, under grant agreement CCF-0939370;
    his work is also supported by NSF DMS-124681 and NSF DMS-1560332.}}

\title{Variance of the Internal Profile in Suffix Trees}

\address{\addressmark{1}Mathematical Biosciences Institute of The Ohio
  State University,
Columbus, OH 43210, USA\\
\addressmark{2}Department of Statistics, Purdue University,
West Lafayette, IN 47907, USA}
\keywords{suffix tree, asymptotic analysis, combinatorics on words, singularity analysis, Mellin transform}


\usepackage{comment}


\usepackage{amsmath}
\usepackage{multicol}

\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{ReMark}{Re:Mark}





\newcommand{\X}{X_{n,k}}
\newcommand{\Tst}{\textbf{T}_{\sigma,\theta}}
\newcommand{\thetauv}{\theta_{u,v}}
\newcommand{\Xnkt}{X_{n,k}}
\def\E{\mathop{\operatorname{\mathbb{E}}}}
\def\Var{\mathop{\operatorname{Var}}}
\def\Cov{\mathop{\operatorname{Cov}}}
\def\Res{\mathop{\operatorname{Res}}}
\newcommand{\qa}{\qquad}
\newcommand{\Iu}{I_{n,u}}
\newcommand{\Iv}{I_{n,v}}
\newcommand{\rhov}{\rho_{v}}
\newcommand{\Hr}{H_{3}^{(u,v)}}
\newcommand{\atrie}{\mathcal{T}}
\newcommand{\trie}{\mathcal{T}}
\newcommand{\trien}{\mathcal{T}_{n}}
\newcommand{\gt}{g_{w,\sigma,\theta}}
\newcommand{\gts}{g_{w,\sigma,\theta}^{*}}
\newcommand{\Calc}{\mathcal{C}}
\newcommand{\sumu}{\sum_{u\in\Ak}}
\newcommand{\sumv}{\sum_{v\in\Ak}}
\newcommand{\fubs}{\bar{f}_{u}^{*}}
\newcommand{\Ak}{\mathcal{A}^{k}}
\newcommand{\toh}{t_{0}}
\newcommand{\sumuva}{\sum_{u,v\in\Ak}}
\newcommand{\sumuv}{\sum_{\substack{u,v\in\Ak \\ u \neq v}}}
\newcommand{\xv}{\textbf{x}}
\newcommand{\hr}{h_{r,n}}
\newcommand{\fuvb}{\bar{f}_{u,v}}
\newcommand{\fuvbs}{\fuvb^{*}}
\newcommand{\jv}{\textbf{j}}
\newcommand{\tv}{\textbf{t}}
\newcommand{\gw}{g_{w,\sigma,\theta}}
\newcommand{\iuv}{i_{u,v}}
\newcommand{\jvu}{j_{v,u}}
\newcommand{\glj}{g_{k,\ell,j}}
\newcommand{\tzero}{t^{(0)}}
\newcommand{\Puv}{\Ps_{u,v}}
\newcommand{\tone}{t^{(1)}}
\newcommand{\gljs}{g_{k,\ell,j}^{*}}
\newcommand{\Bu}[1]{B_{u}^{(#1)}}
\newcommand{\Bv}[1]{B_{v}^{(#1)}}
\newcommand{\Ft}{\tilde{F}}
\newcommand{\Gn}{G_{n}}
\newcommand{\Gr}{G_{r}}
\newcommand{\Kuv}{K_{u,v}}
\newcommand{\suffixn}{\mathcal{T}_{n}}
\newcommand{\Buv}[1]{B_{uv}^{(#1)}}
\newcommand{\gu}{g_{u}}
\newcommand{\tm}{t_{m}}
\newcommand{\pro}{\mathbb{P}}
\newcommand{\hst}{h_{\sigma,\theta}^{*}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Tn}{\mathcal{T}_{n}}
\newcommand{\phiuv}{\phi_{u,v}}
\newcommand{\psiuv}{\psi_{u,v}}
\newcommand{\lar}{\lambda_{r}}
\newcommand{\Tuv}{\Theta_{u,v}}
\newcommand{\tb}{\overline{t}}
\newcommand{\Gbn}{\bar{G}_{n,r}}
\newcommand{\rhon}{O(\rho^{-n})}
\newcommand{\Gnr}{G_{n,r}}
\newcommand{\jover}{\frac{j}{\ell}}
\newcommand{\xiuv}{\xi_{\{u,v\}}}
\newcommand{\qq}{\qquad}
\newcommand{\rhoh}{\hat{\rho}}
\newcommand{\lambdauv}{\lambda_{u,v}}
\newcommand{\bigo}[1]{O\big(#1\big)}
\newcommand{\fhs}{f_{3}^{*}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\gst}{\tilde{g}^{*}}
\newcommand{\Thetauv}{\Theta_{u,v}}
\newcommand{\Taun}{\mathcal{T}_{n}}
\newcommand{\Pu}{\pro(u)}
\newcommand{\Pv}{\pro(v)}
\newcommand{\ds}{\displaystyle}
\newcommand{\twomat}[4]{ \left( \begin{matrix}
#1 & #2 \\
#3 & #4   \end{matrix} \right) }
\newcommand{\twovec}[2]{\begin{matrix}#1 \\ #2\end{matrix}}
\def\Cov{\mathop{\operatorname{Cov}}}
\newcommand{\Vt}{V_{2}^{(u,v)}}
\newcommand{\Vo}{V_{1}^{(u)}}
\newcommand{\gn}{g_{n}}
\newcommand{\hn}{h_{n}}
\newcommand{\Xnk}{X_{n,k}}
\newcommand{\sigtheta}{H_{\sigma,\theta}}
\newcommand{\la}{\lambda}
\newcommand{\Fuv}{F_{\{u,v\}}}
\newcommand{\Fu}{F_{\{u\}}}
\newcommand{\Ruvoverp}{\left(\frac{\log(\Ruv)}{\Ps}-1\right)}
\newcommand{\RuRvoverp}{\left(\frac{\log(\RuRv)}{\Ps}-1\right)}
\newcommand{\Fv}{F_{\{v\}}}
\newcommand{\zx}{(z,x_{1})}
\newcommand{\stdO}{\bigo{n^{ 1+(\alpha/2)\log(p)+\epsilon}}}
\newcommand{\zvv}{\zxx}
\newcommand{\zxx}{(z,x_{1},x_{2})}
\newcommand{\zff}{(z,0,0)}
\newcommand{\rhorth}{\rhort^{(h)}}
\newcommand{\rhortv}{\rhort^{(v)}}
\newcommand{\Guv}{G_{u,v}}
\newcommand{\Gu}{G_{u}}
\newcommand{\gr}{g_{r,c,d}}
\newcommand{\ka}{\kappa}
\newcommand{\kv}{\kappa_{v}}
\newcommand{\diffpt}[2]{\frac{\partial^{2} #1}{\partial #2^{2}}}
\newcommand{\diffp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\lpi}{\log(p^{-1})}
\newcommand{\lqi}{\log(q^{-1})}
\newcommand{\zo}{(z,0)}
\newcommand{\rhort}{\rho_{r,t}}
\newcommand{\K}{\Krt}
\newcommand{\Xs}{X_{\sigma,\theta}}
\newcommand{\Ys}{Y_{\sigma,\theta}}
\newcommand{\logn}{\log n}
\newcommand{\Krt}{K_{r,t}}
\newcommand{\diffxo}[1]{\frac{\partial #1}{\partial x_{1}}}
\newcommand{\diffxt}[1]{\frac{\partial #1}{\partial x_{2}}}
\newcommand{\diffxb}[1]{\frac{\partial^{2} #1}{\partial x_{1} \partial x_{2}}}
\newcommand{\zn}{[z^{n+k-1}]}
\newcommand{\fbus}{\bar{f}_{u}^{*}}
\newcommand{\Bk}{\mathcal{B}_{k}}
\newcommand{\guvs}{g_{u,v}^{*}}
\newcommand{\fub}{\bar{f}_{u}}
\newcommand{\conj}[1]{\bar{#1}}
\newcommand{\Ck}{\mathcal{C}_{k}}
\newcommand{\diffx}[1]{\frac{\partial #1}{\partial x}}
\newcommand{\Cuu}{C_{u,u}}
\newcommand{\Cuv}{C_{u,v}} 
\newcommand{\Cvu}{C_{v,u}}
\newcommand{\Cvv}{C_{v,v}}
\newcommand{\fbs}{\bar{f}^{*}}
\newcommand{\fos}{f_{1}^{*}}
\newcommand{\fts}{f_{2}^{*}}
\newcommand{\Luv}{L_{u,v}}
\newcommand{\fb}{\bar{f}}
\newcommand{\Psum}{\Psigma+\Ptheta}
\newcommand{\Pprod}{\Psigma\Ptheta}
\newcommand{\Ptheta}{\pro(\theta)}
\newcommand{\muv}{\mu_{u,v}}
\newcommand{\Psigma}{\pro(\sigma)}
\newcommand{\Uk}{\mathcal{U}_{k}}
\newcommand{\Vk}{\mathcal{V}_{k}}
\newcommand{\Cuuz}{C_{u,u}(z)}
\newcommand{\Cuvz}{C_{u,v}(z)}
\newcommand{\Cvuz}{C_{v,u}(z)}
\newcommand{\Cvvz}{C_{v,v}(z)}
\newcommand{\gel}{g_{j,r}}
\newcommand{\de}{\delta}
\newcommand{\mybound}{1-(\alpha/2)\log(p^{-1})+\epsilon}
\newcommand{\Pw}{\pro(w)}
\newcommand{\Psig}{\pro(\sigma)}
\newcommand{\Pthe}{\pro(\theta)}
\newcommand{\Astar}{\mathcal{A}^{*}}
\newcommand{\Phist}{\Phi_{\sigma,\theta}}
\newcommand{\Qst}{\textbf{Q}_{\sigma,\theta}}
\newcommand{\deltauv}{\delta_{u,v}}
\newcommand{\guv}{g_{u,v}}
\newcommand{\huv}{h_{u,v}}
\newcommand{\zone}{\mathbf{z}_{1}}
\newcommand{\ztwo}{\mathbf{z}_{2}}
\newcommand{\Gel}{G_{j,r}}
\newcommand{\eps}{\epsilon}
\newcommand{\Dv}{D_{v}}
\newcommand{\Du}{D_{u}}
\newcommand{\hkl}{h_{k,\ell}}
\newcommand{\phiuz}{\phi_{u}(z)}
\newcommand{\phivz}{\phi_{v}(z)}
\newcommand{\gstar}{g^{*}}
\newcommand{\phiu}{\phi_{u}}
\newcommand{\Vtb}{\tilde{V}_{2,n}^{(u,v)}} \newcommand{\phiv}{\phi_{v}}
\newcommand{\phib}{\bar{phi}}
\newcommand{\psib}{\bar{\psi}}
\newcommand{\Ga}{\Gamma}
\newcommand{\fl}{f_{\ell}}
\newcommand{\fll}{f_{\underline{\ell}}}
\newcommand{\flls}{f_{\underline{\ell}}^{*}}
\newcommand{\flu}{f_{\overline{\ell}}}
\newcommand{\flo}{f_{\ell,1}}
\newcommand{\flt}{f_{\ell,2}}
\newcommand{\Kr}{K_{r}}
\newcommand{\fls}{f_{\ell}^{*}}
\newcommand{\flso}{f_{\ell,1}^{*}}
\newcommand{\flst}{f_{\ell,2}^{*}}
\newcommand{\fr}{f_{r}}
\newcommand{\RuRv}{(\Ru\Rv)}
\newcommand{\frs}{f_{r}^{*}}
\newcommand{\Dub}{\bar{D_{u}}}
\newcommand{\Duu}{D_{u,u}}
\newcommand{\Duv}{D_{u,v}}
\newcommand{\rb}{\overline{b}}
\newcommand{\Dvu}{D_{v,u}}
\newcommand{\Dvv}{D_{v,v}}
\newcommand{\du}{\delta_{u}}
\newcommand{\dv}{\delta_{v}}
\newcommand{\duv}{\delta_{u,v}}
\newcommand{\xo}{x_{1}}
\newcommand{\Ru}{R_{u}}
\newcommand{\Rv}{R_{v}}
\newcommand{\Ruv}{R_{u,v}}
\newcommand{\Ht}{H_{2}^{(u,v)}}
\newcommand{\Mt}{M_{2}^{(u,v)}}
\newcommand{\Au}{A_{u}}
\newcommand{\Yk}{\mathcal{Y}_{k}}
\newcommand{\gljb}{\bar{\glj}}
\newcommand{\Xk}{\mathcal{X}_{k}}
\newcommand{\Zk}{\mathcal{Z}_{k}}
\newcommand{\Av}{A_{v}}
\newcommand{\Auv}{A_{u,v}}
\newcommand{\cu}[1]{c_{u}^{(#1)}}
\newcommand{\fuv}{f_{u,v}}
\newcommand{\hs}{h^{*}}
\newcommand{\hstar}{h^{*}}
\newcommand{\fstar}{f^{*}}
\newcommand{\gus}{g_{u}^{*}}
\newcommand{\hus}{h_{u}^{*}}
\newcommand{\fus}{f_{u}^{*}}
\newcommand{\fuvs}{\fuv^{*}}
\newcommand{\huvs}{h_{u,v}^{*}}
\newcommand{\cv}[1]{c_{v}^{(#1)}}
\newcommand{\Hs}{H_{\sigma,\theta}}
\newcommand{\Qk}{\textbf{Q}_{r,c,d}^{(k)}}
\newcommand{\Tk}{\textbf{T}_{r,c,d}^{(k)}}
\newcommand{\cuv}[1]{c_{u,v}^{(#1)}}
\newcommand{\fu}{f_{u}}
\newcommand{\fbu}{\bar{f_{u}}}
\newcommand{\fs}{f^{*}}
\newcommand{\Uset}{\mathcal{U}}
\newcommand{\gs}{g^{*}}
\newcommand{\hu}[1]{h_{#1}^{(u)}}
\newcommand{\Ps}{\textbf{P}}
\newcommand{\Vot}{\widetilde{V}_{1}}
\newcommand{\Vtt}{\widetilde{V}_{3}}
\newcommand{\Vtd}{\widetilde{\widetilde{V}_{2}}}
\newcommand{\Vou}{V_{1}^{(u)}}
\newcommand{\Vtw}{V_{2}^{(\sigma,w,\theta)}}
\newcommand{\Vox}{V_{1,\text{exact}}^{(u)}}
\newcommand{\Vtx}{V_{2,\text{exact}}^{(u,v)}}
\newcommand{\Voe}{V_{1,\text{exp}}^{(u)}}
\newcommand{\Vte}{V_{2,\text{exp}}^{(u,v)}}
\newcommand{\Vtp}{V_{2,\text{poly}}^{(u,v)}}

\begin{document}
\maketitle
\begin{abstract}
The precise analysis of the variance of the profile of a suffix tree
has been a longstanding open problem.
We analyze three regimes of 
the asymptotic growth of the variance of the profile of a
suffix tree built from a randomly generated binary string,
in the nonuniform case.  We utilize
combinatorics on words, singularity analysis, and the Mellin transform.
\end{abstract}

\section{Introduction}

One open problem about suffix trees is how to characterize the number
of internal nodes on the th level of a suffix tree that has  leaves.
Park~et al.~\cite{Park:2009} precisely analyzed the profile of
retrieval tries in 2009.  
Ward has been working on the analogous problem in suffix trees for a
decade; see, e.g.,~\cite{NicodemeWard:2011,Ward:2007}.
While the mean profile of retrieval trees and suffix trees are 
the same (asymptotically, up to first order, in the main range of
interest of the parameters), the variances of the profiles of these
two classes of trees are different.  The goal of this paper is to
precisely analyze the variance of the profile of suffix trees.

In retrieval trees, the strings inserted
into the tree structure are often considered to be independent;
such was the case in~\cite{Park:2009}.  In contrast to this, in
suffix trees, the strings inserted into the tree are suffixes of a
common string, so these strings are overlapping.  The overlaps
make the corresponding analysis much trickier, as compared to~\cite{Park:2009}.

We analyze a suffix tree built from the suffixes of a common string
, where the 's are randomly
generated, independent, and identically distributed.  We view each
 as a letter from the alphabet , where  and .  (Without loss of generality, we
assume throughout that .)  
We use  to denote the set of words of length .
For a word  that consists of  occurrences of letter
 and  occurrences of letter , we use  to denote the
probability that a randomly chosen word of length  is exactly
equal to , i.e., .

The th string to be inserted into the suffix tree is
.  We consider a randomly
generated suffix tree
 built over the first  suffixes of , i.e.,
built from the suffixes  through .
Briefly, all  of these suffixes can be viewed as initially being
placed at the root of the suffix tree.  The  suffixes are then
filtered down to the left or right children of the root, making the
classification of the suffixes according to whether the first letter
of each suffix is ``'' or ``'', respectively.  The filtering
continues down through the tree, with splitting at the th level
according to the th letter in the corresponding suffixes in that
portion of the tree.

For each word , the suffix tree  will
contain the internal node corresponding to  if and only if the base-string
 contains at least two copies of the word  within its first 
characters.  (Equivalently,  
contains the internal node corresponding to  if and only if at
least two of the suffixes  through  have  as a prefix.)
For this reason, we define

if  appears at least twice in , 
or  otherwise.
We use  to denote the number of internal nodes in
 at level .
With the above notation in place, we observe that 
.  This decomposition will be crucial to our
proofs, which start in Section~\ref{sec:combo}.

Finally, following the lead of \cite{Park:2009}, we assume that the limit  exists.

\section{Main Results}\label{intro}
The value of  depends qualitatively on the quantity  which describes the relationship between  and  via the relation . It turns out that there are two particular alpha-values of importance,

We do not attempt, as Park et al. did in \cite{Park:2009}, to analyze
the cases where  is exactly equal to one of these
, but instead assume that both  are
strictly positive. Given this restriction, it is permissible to take the approximation , which we do henceforth without comment.


The variance obeys different laws depending on where the value of  falls in the ranges defined by these . 
The range of most interest is (perhaps) the range in which ; we discuss this case in Theorem~\ref{saddlepointtheorem}.
(The case  is discussed in 
Theorem~\ref{smallalphatheorem};
 and the case  is handled in
Theorem~\ref{poletheorem}.)

When  is small, we have an easy and very strong bound on the decay of .
\begin{theorem}\label{smallalphatheorem}
When , there exists  such that 
 
\end{theorem}

The proof of Theorem \ref{smallalphatheorem} follows from lemmas that
mimic the techniques of \cite{Markthesis}; we omit it from this shortened version. The intuitive meaning behind Theorem \ref{smallalphatheorem}
is that level  of the suffix tree is extremely likely to be
completely filled (meaning the variance will be extremely small) if
 is sufficiently large in comparison to .

Our main results deal with the less trivial case when . We first introduce the functions involved in our main estimates, and provide a word on how we obtain them. 

\subsection{Functions Involved in Main Results; Methodology}\label{subsec:intro}
Our basic device for computing the variance of the internal profile is to write  as a sum of indicator variables , and then evaluate


Our final analysis of the sum of the  will be fairly simple: we will ultimately just have to evaluate the inverse Mellin integral

where the function  will be given by

(See~\cite{PF120} for more details about the Mellin transform.)
The function  is the same as analyzed in~\cite{Park:2009}, and their arguments extend seamlessly to our case. 

On the other hand, the terms  for   will be
novel and much more interesting. To deal with them, we will consider
all possible overlapping decompositions  of
.  To accomplish this, we observe that 

where  is defined as

Note: For ease of the (already cumbersome) notation, we have not written
 nor  as a parameter of .
We will substitute the right hand side of~(\ref{bigHanalogue}) for 
 into equation~(\ref{introinversemellin}).
We will use a technique for  similar to that used for , namely, summing over all
possible values  and  of 
and  respectively, and summing  into a closed form, as
was done at (\ref{introinversemellin}).

The dominant contribution to (\ref{bigHanalogue}) comes from terms
with small . Since , this
implies that  and  have
the same first-order asymptotic growth, as functions of .

We will evaluate the inverse Mellin integral at
(\ref{introinversemellin}) (and the analogous integral for ) by using either the saddle point method or by taking the residue of the pole of  at ; which device we use will depend on the value of . Before giving our main results, we list the saddle points of the functions  and , which are

It is also easy to verify that for any , the value
 is also a saddle point of , and
similarly,  is a saddle point of .

These saddle points will (at last) allow us to express an asymptotic value for  in the case where .
\subsection{Behavior in the main regime}
\begin{theorem}\label{saddlepointtheorem}
Assume  satisfies . Let  and  be as in (\ref{rhodef}). Then we have

The  is given by

where  and where .
Regarding , we define
, , ,
and then  is given by

with the function  given by

with

Furthermore, the outer sum in  satisfies the decay condition that for any positive integer , the sum over all  and  is  for a fixed .

\end{theorem}

\subsection{Behavior in the polar regime}
In the final -regime, where , the
asymptotics arise from the pole at , as the following theorem states.
\begin{theorem}\label{poletheorem}
Assume the parameter  satisfies . Then for some , we have

with  as defined in Theorem
\ref{saddlepointtheorem}, and ,  are given by

with the decay of  as in Theorem \ref{saddlepointtheorem}.
\end{theorem}

Having stated our main results, we now proceed to the proof of Theorems \ref{saddlepointtheorem} and \ref{poletheorem}, which will occupy the remainder of the paper.

\section{An Expression for the Variance}\label{sec:combo} 
Our first task in proving  Theorems \ref{saddlepointtheorem} and
\ref{poletheorem} is  to obtain an exact expression for the variance
of the internal profile .
Recalling equation~(\ref{newstarequation}),
we need to derive the values of  and , so we let
 denote the number of occurrences of  in the first 
characters of ,
and we define  analogously. Then inclusions-exclusion yields the representations

where we require  and  to be distinct. Thus, to obtain an expression for , we just have to evaluate all the probabilities in (\ref{indicatorvar}).

\section{Explicit Expressions for Word-Occurrence Probabilities}\label{sec:Cauchy}
To estimate the probabilities in (\ref{indicatorvar}), we use
generating functions, 
and complex analysis.
Motivated by~\cite{Bassino:2012}, 
we define

where the functions  are \emph{correlation polynomials},
the fundamental device for dealing with the phenomenon of
word-overlaps. With these functions in hand, we can define
generating-functions 
for all the probabilities in (\ref{indicatorvar}). We summarize the result in the following proposition.
\begin{proposition}
Let  and  be as defined at (\ref{psidef}), and define the functions 

with all -counting functions defined in a manner analogous to the -counting functions. Then we have the closed-form power series expressions

\end{proposition}


Now we must derive the st coefficients of these generating
functions. To do this, we use Cauchy's Integral Formula, following a standard argument in combinatorics on words.  Our specific methodology will rely on a vital fact about the denominators  and  of the probability generating functions in (\ref{Gudef}).

\begin{lemma}\label{uniquerootlemma}
There exist  such that for all  and all , 
each of the polynomials , , and  has a unique root (defined respectively as  and ) in the disc .
\end{lemma}
The proof for  and  is given in \cite{Jacquet:2005}; spatial constraints prevent us from giving the proof for the  portion.

Armed with Lemma \ref{uniquerootlemma}, we can estimate the word-counting coefficients of our generating functions to within a factor of  by applying Cauchy's Theorem to the contour . The following theorem gives the resultant estimates.
\begin{theorem}\label{restheorem}
Let the polynomials  and , etc.\ be as in (\ref{deltadef}) and
(\ref{Gudef}).
If we define

then we have the following estimates

and the error in each case is .

Similarly, for the joint events ,
and 
with  as in (\ref{Gudef}), we also obtain these
estimates, where again, the error in each case is :

\end{theorem}
Using these expressions, we can evaluate the expressions for
 and  at (\ref{indicatorvar}) to within a
factor of . In doing this, however, it will be helpful
to break up our estimates from Theorem \ref{restheorem} so that terms
of common order in  are denoted under a single variable. We
therefore define the upper-case constants (we suppress the dependence
on  and  in the notation)


Returning to the expression , we obtain an expression for our ultimate desired quantity.
\begin{corollary}\label{Acorr}
Let  be as defined in (\ref{Adef}). 
With  and  as in (\ref{Adef}),
we have the estimate

\end{corollary}


\subsection{High-Probability Approximations}
Our task is now to approximate the expression from Corollary~\ref{Acorr}. To achieve this, we follow the usual suffix-tree strategy: we compare the terms to simpler ones which will be accurate with very high probability, and use Mellin transforms to show that sum of the the differences between the old terms and the new ones is negligible. Our two main tools for demonstrating this negligibility are bounds provided by the following lemma.

\begin{lemma}\label{corrlemma}
We have the bounds

\end{lemma}
The first portion of Lemma \ref{corrlemma} is proved in
\cite{Jacquet:2005}; spatial constraints prevent us from proving the
second portion here. However, by rigorously expanding on the heuristic  and , we obtain the following theorem which is one of the major steps of the proof.
\begin{theorem}\label{mellintheorem}
We define the terms
,
,
and ,
and the expressions


Then, for every , we have the estimate

\end{theorem}
We mention that the term  has already been analyzed in Park~\cite{Park:2009}.
It gives the asymptotic variance of the internal profile in a
\emph{trie}.
The term  is negligible. Thus, after proving Theorem \ref{mellintheorem}, all that will remain will be to analyze .


\section{Distilling Essence of Estimate}
We must now analyze the estimate from Theorem \ref{mellintheorem}, which consists of the terms ,  and . We can deal with the first two of these terms in two quick theorems.
Theorem~\ref{Parktheorem} was proven in \cite{Park:2009}.
Theorem \ref{Ktheorem} has a short proof, which we omit  in this concise version.
\begin{theorem}\label{Parktheorem}
An asymptotic expression for  is given by the  portions from Theorems \ref{saddlepointtheorem} and \ref{poletheorem}.
\end{theorem} 
\begin{theorem}\label{Ktheorem}
The term  from Theorem \ref{thetatheorem} satisfies

for some .
\end{theorem}

For the rest of the paper, then, we concentrate on the portion , which contains the term  and constitutes the really novel part of the whole enterprise. We deal with  by nothing that, by Lemma \ref{corrlemma}, the quantities  and  are unlikely
to simultaneously be large, so the approximation
 is reasonable. From here, we note that for  to be nonzero we must have , in which case there exists some maximal suffix of  which is also a
prefix of . If we call this word , and then have the precise
equality  where
 are such that  and
. Then we employ the estimate , again as suggested by Lemma \ref{corrlemma}. We thus have the central estimate
.
Our strategy, then, is to make the substitutions ,
, and  in the summand of , 
and then sum over all possible such decompositions. In the proof and final result it will be  helpful to have the shorthand 
 and ,
The following theorem states that this heuristic can be rigorously justified.
\begin{theorem}\label{thetatheorem}
Let  be as defined above, and define the functions

and
.
Then for  as given in Theorem \ref{mellintheorem}, we have the estimate

\end{theorem}
One proves Theorem \ref{thetatheorem} by making the substitutions  and , and then using Mellin transforms and Lemma \ref{corrlemma} to show that the derived error-bound is satisfied.


\section{Derivation of Asymptotics}
To complete the main proof, it remains only to analyze . We present the key results in this process in a series of subsections.
\subsection{Partitioning the Sum}
Our first step is to partition the sum which comprises . 
into subsets which share a common value for the ordered pair . We can rewrite the function  from Thereom \ref{thetatheorem} as an infinite sum, 

with the function   given by
.
The terms  and  only depend on the \emph{probabilities} of  and ; their internal composition does not matter. This allows a great reduction in the number of terms to handle. With some abuse of notation, we define the terms

and then define the atom of all our remaining analysis, which is

With this notation, we have the following proposition.
\begin{proposition}\label{gprop}
Let  be as in (\ref{gdef}). Then  from Theorem \ref{thetatheorem} admits the representation 
\end{proposition}
Now we analyze .
\subsection{Analysis of }
All our final estimates rest on our analysis of the function  given in Proposition \ref{gprop}. To begin that analysis, we take the Mellin transform of  and, specifying the bounded portion

we obtain


We then consider the value of , which will be the integrand of our inverse Mellin integral. Using the relation , we can write
,
where the function  is as defined in Section \ref{subsec:intro}.
From here, we can recover the value of  via an inverse Mellin transform. We summarize the results in the following theorem.
\begin{theorem}\label{hasymptote}
Define the discriminant

Then the function  defined in (\ref{gdef}) obeys the following asymptotic scheme. \\
If , then  for every .\\
If , then

If , then 

for some .
\end{theorem}
The estimates of Theorem \ref{hasymptote} can be derived using techniques that are standard (albeit pretty technical) in the analysis of tree structures. In the first regime, one can show that  is always decreasing in , so integrating along  for  gives the desired bound. In the second regime we use the saddle-point method, and in the final regime, we derive the asymptotics by taking the residue from the pole of  at . 


Theorem \ref{hasymptote}, though certainly essential, is not in itself sufficient for our purposes, since we have to sum  over a set of triplets  that will grow unboundedly large as . The next lemma gives the needed statement about uniform convergence.
\begin{lemma}\label{uniformlemma}
Suppose . Then there exists  such that for all triplets  in the rectangle , we have , and the saddle-point estimate of Theorem~\ref{hasymptote} holds uniformly. Furthermore, the analogous result holds in the polar case, when .
\end{lemma}
The claims about  lying in particular ranges follow easily from the definition of . To show uniformity in the saddle point case, we use bounds from~\cite{Olver}, which are uniform on the compact set . In the polar regime, we again use the compactness of  to show that the -partial of  at  is bounded below by a positive constant, meaning that for some , we can uniformly take the left-hand side our Mellin box to be , thereby obtaining an error that is , with the  portion controlled by compactness.


\section{Bounding the Tail}\label{tailsection}
Theorem \ref{hasymptote} justifies the content of  in the main Theorems \ref{saddlepointtheorem} and \ref{poletheorem}. However, we still have to justify the uniform  error-bounds given in the leading equations of those theorems (which amounts to showing that our estimates for  are uniform outside the compact rectangle  as well as prove our claim about the decay of the outer sum in . 

We can accomplish both these tasks using the same argument. First, we unify the -arguments for  in the polar and saddle-point cases into a single term,

Then we note that if we define

then by Stirling's Formula we have 

where the function  is unimportant except for the fact that its growth/decay are in . We now state an important and somewhat surprising result about the function .
\begin{lemma}\label{maxlemma}
Let the function  be as in (\ref{Gdef}), and   the discriminant from Theorem~\ref{hasymptote}. Then for any fixed  such that the set\;  is nonempty, the map  attains its maximum at a unique ordered pair  on the diagonal of . 
\end{lemma}
The proof of Lemma \ref{maxlemma}, although not exceedingly difficult or technical, is rather long and (to us) not very intuitive. We therefore omit it.
Lemma \ref{maxlemma} allows us to define the function

for every  on which the set  defined in Lemma \ref{maxlemma} is nonempty. We now state two vital facts about this , which are exactly the results needed complete the proof.
\begin{lemma}\label{Flemma}
The function  defined at (\ref{Fdef}) is concave, and moreover .
\end{lemma}
The statements in Theorems \ref{saddlepointtheorem} and \ref{poletheorem} about the decay of  immediately follow from Lemma \ref{Flemma}, since we have
,
and one readily verifies that  in the saddle-point case and  in the polar case.
It remains only to justify the global -bounds at the beginning of Theorems \ref{saddlepointtheorem} and \ref{poletheorem} for those  outside the rectangle  given in Lemma \ref{uniformlemma}, which the following achieves.
\begin{lemma}\label{uniformboundlemma}
With  as defined at (\ref{Fdef}) and  as at (\ref{gdef}), for all sufficiently small  there exists  such that

for all  and all .
\end{lemma}
The main tool in proving Lemma \ref{uniformboundlemma} is Lemma \ref{Flemma}, although some work is required in proving uniformity in (for example) cases where the saddle point  is very close to the pole at .


\nocite{*}
\acknowledgements
\label{sec:ack}
Ward thanks the authors of~\cite{Bassino:2012} for their hospitality
during several visits to Paris, which enabled him to work on this
problem.  Both authors also thank the authors of~\cite{Park:2009} for
their pioneering work in this area.  Moreover, we both acknowledge
W.~Szpankowski as a constant source of encouragement during the years
in which we worked toward a method of solution.
Finally, we thank the three anonymous referees for their very
insightful comments, corrections, and suggestions.

\bibliographystyle{alpha}
\bibliography{gaitherward}
\label{sec:biblio}



\end{document}
