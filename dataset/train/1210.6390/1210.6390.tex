

\documentclass{scrartcl}



\usepackage{stmaryrd}
\usepackage{xspace}
\usepackage{color}
\usepackage{natbib}
\usepackage{amsthm}
\usepackage{subfig}

\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{example}{Example}




\usepackage{epigram}
\usepackage{epigram-desc}
\usepackage{epigram-tag}
\usepackage{epigram-enum}



\usepackage{epigram-nat}
\usepackage{epigram-list}
\usepackage{epigram-vec}
\usepackage{epigram-sum}


\usepackage{type-theory}
\usepackage{elaboration}

\newlength{\rulevgap}
\setlength{\rulevgap}{0.05in}
\newlength{\ruleheight}
\newlength{\ruledepth}
\newsavebox{\rulebox}
\newlength{\GapLength}
\newcommand{\gap}[1]{\settowidth{\GapLength}{#1} \hspace*{\GapLength}}


\newcommand{\Rule}[2]{\savebox{\rulebox}[\width][b]                         { #1  #2 }   \settoheight{\ruleheight}{\usebox{\rulebox}}          \addtolength{\ruleheight}{\rulevgap}                  \settodepth{\ruledepth}{\usebox{\rulebox}}            \addtolength{\ruledepth}{\rulevgap}                   \raisebox{0in}[\ruleheight][\ruledepth]               {\usebox{\rulebox}}}
\newcommand{\Axiom}[1]{\savebox{\rulebox}[\width][b]                        {#1}        \settoheight{\ruleheight}{\usebox{\rulebox}}          \addtolength{\ruleheight}{\rulevgap}                  \settodepth{\ruledepth}{\usebox{\rulebox}}            \addtolength{\ruledepth}{\rulevgap}                   \raisebox{0in}[\ruleheight][\ruledepth]               {\usebox{\rulebox}}}
\newcommand{\RuleSide}[3]{\gap{\mbox{}} \hspace*{0.1in}               \Rule{#1}{#3}                          \hspace*{0.1in}\mbox{}}
\newcommand{\AxiomSide}[2]{\gap{\mbox{}} \hspace*{0.1in}              \Axiom{#2}                            \hspace*{0.1in}\mbox{}}
 
\ColourEpigram



\newcommand{\todo}[1]{\red{#1}}

\newcommand{\ie}{i.e.\ }
\newcommand{\eg}{e.g.\ }



\title{Elaborating Inductive Definitions}
\author{Pierre-\'{E}variste Dagand \and Conor McBride}
\date{}

\begin{document}

\maketitle



\begin{abstract}






We present an elaboration of inductive definitions down to a universe
of datatypes. The universe of datatypes is an internal presentation of
strictly positive families within type theory. By elaborating an
inductive definition -- a syntactic artifact -- to its code -- its
semantics -- we obtain an internalized account of inductives inside
the type theory itself: we claim that reasoning about inductive
definitions could be carried in the type theory, not in the
meta-theory as it is usually the case. Besides, we give a formal
specification of that elaboration process. It is therefore amenable to
formal reasoning too. We prove the soundness of our translation and
hint at its correctness with respect to Coq's \texttt{Inductive}
definitions. The practical benefits of this approach are numerous. For
the type theorist, this is a small step toward bootstrapping, \ie
implementing the inductive fragment in the type theory itself. For the
programmer, this means better support for generic programming: we
shall present a lightweight \texttt{deriving} mechanism, entirely
definable by the programmer and therefore not requiring any extension
to the type theory.

\end{abstract}



In a dependent type theory, inductive types come in various shapes and
forms. Unsurprisingly, we can define data-types \`{a} la ML,
following the \emph{sum-of-product} recipe: we offer a choice of
constructors and, for each constructor, comes a product of
arguments. An example of such definition is the vintage and classic
 datatype:

For the working semanticist, this brings fond memory of a golden era:
this syntax has a trivial categorical interpretation in term of
\emph{signature functor}, here . Without a
second thought, we can brush away the syntax, mapping the syntactic
representations of sum and product to their categorical
counterpart. Handling parameters comes at a minor complexity cost: we
merely parameterize the functor itself, for instance with  here.



However, these data-type definition are child's play in a
dependently-typed setting: they do not let us use any type dependency
in their definition, nor do they let us define inductive dependent 
types. To obtain grown-ups' datatypes, we move to inductive
families~\citep{dybjer:inductive-families}: we first introduce the notion of
\emph{index} that, unlike parameters, we can \emph{constrain} to a
particular value. The archetypal example of an inductive family is the
 datatype, which can understood as a list indexed by its
length:

In our syntax, we are purposely explicit about constraints on
indices. Indeed, these constraints eventually turn into a statement
using whatever propositional equality the underlying type theory has
to offer.




Alternatively, our model of inductive families~\citep{alti:lics09}
suggests that vectors could be defined by first \emph{matching} over
the index : if  is , then the only possible
constructor is , otherwise, if  is  for some
, it must be a  which tail is of length . We
reflect this definition style -- computing over indices -- by the
following syntax:

Note that we are here using the Epigram \emph{by} (
gadget~\citep{mcbride.mckinna:view-from-the-left} to perform the case
analysis on . A pattern-matching
notation~\citep{coquand:pattern-matching,sozeau:equations} could be
used as well. When the patterns are unsurprising, we shall abuse
notation and write a standard pattern match.

This definition style lets us maximally use the information provided
by the indices to structure datatypes. In the constraint-based
definition, we have to store an index  against which we
constrain . In the computation-based definition, we simply match
against the index. Such difference of presentation has been studied by
\citet{brady:index-inductive-families} in the context of various
optimizations on inductive types. In our work on
ornaments~\citep{mcbride:ornament,dagand:fun-orn}, this definition
style is instrumental in structuring our universes of functional
ornament and enables us to lift functions across ornamented types.



Now, we ought to make sure that our language of data-type is correct,
let alone semantically meaningful. Indeed, if we were to accept the
following definition
\Spacedcommand{\Bottom}{\Canonical{Bad}}

we would make many formal developments a lot easier to prove! To ban
these bogus definitions, theorem provers such as
Agda~\citep{norell:agda} or Coq~\citep{coq} rely on a positivity
checker to ensure that all recursive arguments are in a
strictly positive position.
The positivity checker is therefore part of the trusted computing base
of the theorem prover. Besides, by working on the syntactic
representation of datatypes, it is a non negligible piece of software
that is a common source of frustration: it either stubbornly prevents
perfectly valid definitions -- as it sometimes is the case in Coq --
or happily accepts obnoxious definitions -- as Agda users discover
every so often.



For the working semanticist, this is an awakening and a rude one:
while reasoning about ML datatypes used to be at a functor away, she
now has to cope with equality constraints and computations on
indices. Most infuriatingly, we have some elegant models for inductive
families but we seem stuck with some clumsy syntactic presentation:
quoting \citet{harper:elaboration}, ``the treatment of datatypes is
technically complex, but conceptually straightforward''. Following
Stone and Harper, most authors~\citep{coen:refinement,luo:utt,
  mcbride:construction-constructor} have no choice but to throw in the
towel and proceed over a ``\ldots''-filled skeleton of inductive
definition. While this does not make these works any less correct, it
makes them hard for the author to get right and for the reader to
understand.



We attribute these difficulties to the formal gap between the syntax
of inductive definitions and their semantics. While inductive families
have an interpretation in term of strictly positive functors and their
initial algebra, we are unable to leverage this knowledge. Being stuck
with a syntactic artifact, the ghost of the de Bruijn criterion haunts
our type theories: inductive definitions elude the type checker and
must be enforced by a not-so-small positivity checker. Besides, since
the syntax of inductive definitions is hardly amenable to formal
reasoning, we are left wondering if its intended semantics is indeed
always respected. How many inductive skeletons might be hidden in the
dark closet of your favorite theorem prover?



An alternative to a purely syntactic approach is to reflect inductive
types inside the type theory itself. Following
\citet{benke:universe-generic-prog}, we extend a Martin-L\"{o}f type
theory with a universe of inductive families, all that for a minor
complexity cost~\citep{dagand:levitation}. From within the type
theory, we are then able to create and manipulate datatypes but also
compute over them. However, from a user perspective, these codes are a
no-go: manually coding datatypes, for instance, is too cumbersome. Rather
than writing low-level codes, we would like to write a
honest-to-goodness inductive definition and get the computer to
automatically \emph{elaborate} it to a code in the universe.



In this paper, we elaborate upon (pun intended) the syntax of
datatypes introduced in \citet{dagand:fun-orn}. In our previous work,
we presented a conservative extension of an \texttt{Inductive}-like
syntax, the twist being in our support of computation over
indices. While this syntax had been informally motivated, this paper
gives a formal specification of its elaboration down to our universe
of datatypes. Our contributions are the following:

\begin{itemize}



\item In Section~\ref{sec:elab-course}, we give a crash course in
  elaboration for dependent types. We will present a bidirectional
  type checker~\citep{pierce:bidirectional-tc} for our type theory. We
  then extend it to make programming a less cryptic experience. To
  that purpose, we shall use types as \emph{presentations} of more
  high-level concepts, such as the notions of finite set or of
  datatype constructor. While this Section does not contain any new
  result \emph{per se}, we aim at introducing the reader to a coherent
  collection of techniques that, put together, form a general
  framework for type-directed elaboration ;



\item In Section~\ref{sec:elab-data-types}, we specify the elaboration
  of inductive types down to a simple universe of inductive
  types. While the system we present in this Section is restricted to
  strictly positive types, we take advantage of its simplicity to
  develop our intuition. The same ideas are at play in the case of
  inductive families ;
  



\item In Section~\ref{sec:elab-data-family}, we specify the
  elaboration of inductive families down to our universe of inductive
  families. This system subsumes the one introduced in
  Section~\ref{sec:elab-data-types} but we should reuse much of the
  concepts developed in that Section. The novelty of our syntax is to
  support computation on indices, as made possible by our model of
  inductive families and its universe presentation ;




\item In Section~\ref{sec:discussion}, we draw the consequences of our
  design choice. For the proof-assistant implementer, we show how
  meta-theoretical results on inductives, such
  as~\citet{mcbride:construction-constructor}, can be internalized and
  formally presented in the type theory. For the programmer, we show
  how a generic \texttt{deriving} mechanism \`{a} la Haskell
  can be implemented from within the type theory.

\end{itemize}



\paragraph{Scope of this work:} 

This paper aims at \emph{specifying} an elaboration procedure from an
inductive definition down to its representation in a universe of
inductive types. At the risk of disappointing implementers, we are
not describing an implementation. In particular, we shall present the
elaboration in a relational style, hence conveniently glancing over
the operational details. Our goal is to ease the formal study of
inductive definitions, hence the choice of this more abstract style.
Nonetheless, this paper is not entirely disconnected from
implementation. First, it grew out of our work on the Epigram
system~\citep{pigs:epigram}, in which Peter Morris implemented a
tactic elaborating an earlier form of inductive definition down to our
universe of code. Second, a tutorial implementation of an elaborator
for inductive types is currently underway.



\section{The Type Theory}
\label{sec:type-theory}



For this paper to be self-contained, we shall recall a few definitions
from our previous work~\citep{dagand:levitation}. We shall not dwell
on the meta-theoretical properties of this system: the interested
reader should consult~\citet{luo:utt} for a study of Martin-L\"{o}f
type theory and~\citet{dagand:levitation} for its extension with a
universe of datatypes.



We present our core type theory in Figure~\ref{fig:type-theory}. It is
a standard Martin-L\"{o}f type theory, with -types and
-types. We shall write  for the hierarchy of types,
implicitly assuming cumulativity of universes. In order to be
equality-agnostic, we simply specify our expectations through a
judgmental presentation. Our presentation is hopefully not
controversial and should adapt easily to regional variations, such as
Coq, Agda or an observational type theory~\citep{altenkirch:ott}.

\renewcommand{\EpigramEquality}{
\begin{array}{@{}c}
\Rule{\begin{array}{@{}l}
        \TypeJudgment{\Gamma}{S}{\Set} \quad
        \TypeJudgment{\Gamma ; \XS}{t}{T} \\
        \TypeJudgment{\Gamma}{s}{S}
      \end{array}}
     {\EqualJudgment{\Gamma}{(\LamAnn{\X}{S} t)\:s}{t[s/\X]}{T[s/\X]}}
\\
\Rule{\begin{array}{@{}l}
       \TypeJudgment{\Gamma}{s}{S} \quad
       \TypeJudgment{\Gamma ; \XS}{T}{\Set} \\
       \TypeJudgment{\Gamma}{t}{T[s/\X]}
      \end{array}}
     {\EqualJudgment{\Gamma}{\Fst[(\PairAnn{s}{t}{\X.T})]}{s}{S}}
\\
\Rule{\begin{array}{@{}l}
       \TypeJudgment{\Gamma}{s}{S} \quad
       \TypeJudgment{\Gamma ; \XS}{T}{\Set} \\
       \TypeJudgment{\Gamma}{t}{T[s/\X]}
      \end{array}}
     {\EqualJudgment{\Gamma}{\Snd[(\PairAnn{s}{t}{\X.T})]}{t}{T[s/\X]}}
\end{array}
}

\renewcommand{\EpigramContextValidity}{
\Axiom{\ContextValid{}}
    \\
\Rule{\ContextValid{\Gamma} \quad
          \TypeJudgment{\Gamma}{\Meta{S}}{\Set[k]}}
         {\ContextValid{\Gamma ; \XS}}\;\X\not\in\Gamma
}


\begin{figure}[tbp]
\centering
\begin{tabular}{l@{\qquad}l}
\begin{tabular}{c}
\subfloat[][Context validity]
         {
{\small
}}  \\
\subfloat[][Judgmental equality]{\small }
\end{tabular}
&
\subfloat[][Typing judgments]{\small }
\end{tabular}

\caption{Type theory}
\label{fig:type-theory}

\end{figure}



\Spacedcommand{\spi}{\Function{}}

A first addition to this core calculus is a universe of
enumerations~(Fig.~\ref{fig:enum-universe}). The purpose of this
universe is to let us define finite collections of labels. Labels are
introduced through the  type and are then used to define
finite sets through the  universe. To index a specific
element in such a set, we write an  code. To eliminate
finite sets, we form a small -type  that builds a lookup tuple mapping, for all label  in
the enumeration, a value of type . To perform the lookup, we
define the eliminator:


\begin{example}[Coding ]

We define this set by merely enumerating its labels, in effect
building a list of tags:


\end{example}

\begin{example}[Coding ]

We define this function by a straightforward application of the
 eliminator:


\end{example}

\begin{figure}[tb]

{\small
\begin{tabular}{lll}
\subfloat[][Tags]{ \label{fig:label}} &
\subfloat[][Enumeration]{} &
\subfloat[][Index]{}
\end{tabular}
}

\caption{Universe of enumerations}
\label{fig:enum-universe}

\end{figure}



We recall the definition of our universe of inductive types in
Figure~\ref{fig:universe-types}. This universe captures
strictly positive types, a generalization of ML datatypes to dependent
types. For pedagogical reason, we choose this simple universe as a
first step toward a full-blown universe of inductive families.
The idea at work behind this presentation is the following: to define
new datatypes, we give their code by \emph{desc}ribing their signature
functor in . The interpretation function
 turns such a description into the corresponding
endofunctor over . Its definition is obvious from the codes: a
 is interpreted into a -type, and so on. The
notable exception is , which describes the identity
functor. Remark that the resulting functor are strictly positive, by
construction. Hence, we can construct their least fix-point using
 and safely provide a generic elimination principle,
. The  function computes the inductive
hypothesis. The interested reader will find their precise definition
in \citet{dagand:levitation} but the basic intuition we have given
here is enough to understand this paper.

\begin{figure}[tb]

\centering
\subfloat[][Codes]{\small}
\qquad
\subfloat[][Fix-point]{\small}
\\
\subfloat[][Elimination principle]{\small
}

\caption{Universe of inductive types}
\label{fig:universe-types}

\end{figure}



\begin{example}[Describing natural numbers]

\renewcommand{\NatDDef}{
  \Let{\NatD}{\Desc}{
    \Return{\NatD}
           {\Dsigma[\Collection{
                        \begin{array}{l}
                          \Tag{0},\\ 
                          \Tag{\CN{suc}}
                        \end{array}}\:
                    \CollectionElim{
                      \begin{array}{l@{\DoReturn}l}
                        \Tag{0} & \DUnit, \\
                        \Tag{\CN{suc}} & \DVar
                      \end{array}
                    }]}}
}


Natural numbers can be presented as the least fix-point of the
functor . This is described by:

Note that we are using a small  here: a datatype definition
always starts with a finite choice of constructors. This corresponds
to the \emph{tagged descriptions} of \citet{dagand:levitation}: using
this structure, we can implement some generic constructions -- such as
the Zipper or the free monad -- and generic theorems -- such as in
Section~\ref{sec:const-on-const}.

\end{example}

\newcommand{\TreeD}{\Function{TreeD}}

\begin{example}[Describing binary trees]

Categorically, binary trees are modeled by the least fix-point of the
functor . In our universe, we obtain trees by the following definitions:

\end{example}





Finally, we recall the universe of indexed descriptions
(Fig.~\ref{fig:universe-families}), which captures inductive
families. This universe subsumes the previous one by not only allowing
parameters but also indices: thanks to this universe, we can describe
datatypes such as vector. Just as before, we give a universe of codes,
, that are then interpreted to build the least
fix-point. Note that the argument of  is a \emph{function}
from indices to codes: this particularity lets us define datatypes by
computation over the index. We illustrate this point through two
possible implementations of the vector datatype.

\renewcommand{\InterpretIDescDef}{
\Code{
  \Let{\InterpretIDesc{\PiTel{\Var{D}}{\IDesc[\Var{I}]}} & 
              \PiTel{\Var{X}}{\Var{I} \To \Set}}
      {\Set}{}\\
\Case{
\Return{\InterpretIDesc{\DVar[\Var{i}]} & \Var{X}}{\Var{X}\: \Var{i}}
\Return{\InterpretIDesc{\DUnit} & \Var{X}}{\Unit}
\Return{\InterpretIDesc{\Var{A} \DTimes \Var{B}} & \Var{X}}
       {\InterpretIDesc{\Var{A}}[\Var{X}] \Times \InterpretIDesc{\Var{B}}[\Var{X}]}
\Return{\InterpretIDesc{\DPi[\Var{S}\: \Var{T}]} & \Var{X}}{\PiTo{\Var{s}}{\Var{S}}{\InterpretIDesc{\Var{T}\: \Var{s}}[\Var{X}]}}
\Return{\InterpretIDesc{\DSigma[\Var{S}\: \Var{T}]} & \Var{X}}{\SigmaTimes{\Var{s}}{\Var{S}}{\InterpretIDesc{\Var{T}\: \Var{s}}[\Var{X}]}}
\Return{\InterpretIDesc{\Dsigma[\Var{E}\: \Var{T}]} & \Var{X}}
       {\SigmaTimes{\Var{e}}{\EnumT[\Var{E}]}
                   {\InterpretIDesc{\Var{T}\: \Var{e}}[\Var{X}]}}
}}}


\begin{figure}[tbp]

\centering
\subfloat[][Codes]{\small }
\qquad
\subfloat[][Fix-point]{\small }
\\
\subfloat[][Elimination principle]{
\small

}

\caption{Universe of inductive families}
\label{fig:universe-families}

\end{figure}




\if 0

\begin{example}[Describing natural numbers]

\renewcommand{\NatDDef}{
  \Let{\NatD}{\IDesc[\Unit]}{
    \Return{\NatD}
           {\Dsigma[\Collection{
                        \begin{array}{l}
                          \Tag{0},\\ 
                          \Tag{\CN{suc}}
                        \end{array}}\:
                    \CollectionElim{
                      \begin{array}{l@{\DoReturn}l}
                        \Tag{0} & \DUnit, \\
                        \Tag{\CN{suc}} & \DVar[\Void]
                      \end{array}
                    }]}}
}


Natural numbers can be presented as the least fix-point of the
functor . This is described by:

Note that we are using a small  here: a datatype definition
always starts with a finite choice of constructors. This corresponds
to the \emph{tagged descriptions} of \citet{dagand:levitation}: using
this structure, we can implement some generic constructions -- such as
the Zipper or the free monad -- and generic theorems -- such as in
Section~\ref{sec:const-on-const}.

\end{example}

\newcommand{\TreeD}{\Function{TreeD}}

\begin{example}[Describing binary trees]

Categorically, binary trees are modeled by the least fix-point of the
functor . In our universe, we obtain trees by the following definitions:

\end{example}

\fi

\begin{example}[Vector, \`{a} la Coq]
  
  The standard presentation of inductive families is by using equality
  constraints to force the indexing strategy. In effect, this
  corresponds to the following definition of vectors:


\end{example}

\begin{example}[Vector, alternatively]

  Interestingly, in the previous definition, we are defining
   as a function from  to
   but we are not making use of our ability to inspect
  : we merely constrain it to be what we wish it was. We
  can make full use of that function space by \emph{first} checking
  whether  is  or  and \emph{only
    then} give a code for the corresponding constructor, respectively
   and :

  

\end{example}





\section{First Steps in Elaboration}
\label{sec:elab-course}

In this Section, we shall make our first steps in elaboration. First,
we present a bidirectional type system for the calculus introduced in
the previous section. Using the flow of information from type
synthesis to type checking, we are then able to declutter our term
language. Secondly, we adapt the notion of programming
problem~\citep{mcbride.mckinna:view-from-the-left} to our system and
we shall see how, with little effort, we can move away from an austere
calculus and closer to a proper programming language.



\subsection{Bidirectional type checking}



The idea of bidirectional type
checking~\citep{pierce:bidirectional-tc} is to capture, in the
specification of the type checker, the local flow of typing
information. On the one hand, we will \emph{synthesize} types from
variables and functions while, on the other hand, we will \emph{check}
terms against these synthesized types. By checking terms against their
types, we can use types to structure the term language: for example,
we remove the need for writing the domain type in an abstraction, or
we can use a tuple notation for telescopes of
-types. Following \citep{harper:elaboration}, we distinguish
two categories of terms. The core type theory defines the
\emph{internal} language. The bidirectional approach lets us then
extend this core language into a more convenient \emph{external}
language.
We shall hasten to add that using a bidirectional approach in a
dependently-typed framework is not a novel idea: for instance, it is
the basis of Matita's refinement system~\citep{coen:refinement}. Also,
we gave a similar presentation in some earlier
work~\citep{dagand:levitation}.

\begin{figure}[tb]

\centering
\begin{tabular}{cc}
\subfloat[][Type synthesis]{
 
\label{fig:type-synthesis}} &
\subfloat[][Type checking]{
 \label{fig:type-checking}}
\end{tabular}

\caption{Bidirectional type checker}
\label{fig:bidir-tc}

\end{figure}






Let us present type synthesis~(Fig.~\ref{fig:type-synthesis})
first. The judgment  states that the
external term  elaborates to the internal term  of
type  in the context . By convention, we shall keep
inputs to the relation to the left of the  symbol, while
outputs will be on the right. We expect the following soundness
property to hold:
\begin{theorem}[Soundness of type synthesis]
If ,
then .
\end{theorem}







While type synthesis initiates the flow of typing information, type
checking (Fig.~\ref{fig:type-checking}) lets us make use of this
information to enrich the language of terms. The interpretation of the
judgment  is that the external term
 is checked against the type  in context  and
elaborates to an internal term . Again, we expect the following
soundness property to hold:
\begin{theorem}[Soundness of type checking]
If , then .
\end{theorem}



\subsection{Putting types at work}


While the type checker we have specified so far only allows untyped
abstractions, we extend it with some convenient features. 



\paragraph{Tuples:}
By design of the interpretation of our universes of enumeration and
inductive types, we are often going to build inhabitants of
-telescope of the form
. To reduce the syntactic burden of
these nested pairs, we elaborate a LISP-inspired tuple notation:




\paragraph{Finite sets, introduction and elimination:}
In Section~\ref{sec:type-theory}, we have used an informal set-like
notation for enumerations: we can make that notation formal through
elaboration. To do so, we extend the type checker with the
following rules:



\paragraph{Indexing finite sets:}
Also, rather than indexing into enumerations through the 
codes, we would like to be able to write the label and have it
elaborate to the corresponding index. This is achieved by the
following extension:





\paragraph{Datatype constructors:}
Finally, while we do not yet have a proper syntax for \emph{declaring}
inductive types, we can already extend our term language with
constructors. Upon elaborating an external term  against the fix-point of a tagged description, we replace
this elaboration problem with the one consisting of elaborating the
tuple consisting of the constructor label and the arguments:






\if 0



\newcommand{\Label}{\Canonical{label}}

\newcommand{\InterpretParam}[1]{\green{\llbracket}  #1 \green{\rrbracket_{P}}}
\newcommandx{\InterpretIndex}[2][2=\!]{\green{\llbracket} #1 \green{\rrbracket_{I}} \xspace\:#2}



\newcommand{\Presentation}[3]{\blue{\langle} #1 \blue{\cdot} #2 \mathop{\blue{:}} #3 \blue{\rangle}}
\Spacedcommand{\PresRet}{\Constructor{return}}
\newcommand{\PresCall}[4]{\Function{call}\: 
                              \Presentation{#1}{#2}{#3}\: #4}







\fi



\subsection{Elaborating Programs}

\newcommand{\Label}{\Canonical{label}}
\newcommand{\LabelDef}[2]{\blue{\langle} #1 \mathop{\blue{:}} #2 \blue{\rangle}}
\Spacedcommand{\LabelRet}{\Constructor{return}}
\newcommand{\LabelCall}[3]{\Function{call} \green{\langle} #1 \mathop{\blue{:}} #2 \green{\rangle} #3}



In the previous Section, we have define type-specific languages,
relying on the types to guide the elaboration process. In this
Section, we go a step further and purposely define finely indexed
types for the purpose of elaboration. This idea of using types as a
\emph{presentation} of our high-level intentions rather than a
\emph{representation} of low-level restrictions originated
in~\citet{mcbride.mckinna:view-from-the-left}. To illustrate our
point, we redevelop the elaboration of programs presented in that
paper and adapt it to our framework. We shall see how these
presentation types guide the elaboration of programs and let us regain
a pattern-matching notation without hard-wiring pattern-matching
itself.



\begin{figure}[tbp]

\centering

\subfloat[][Programming label]{

}
\\
\subfloat[][Program declaration]{

\label{fig:program-decl}
}
\\
\subfloat[][Program definition]{

\label{fig:program-def}}


\caption{Programming labels}
\label{fig:prog-label}

\end{figure}





To guide elaboration of programs, we define \emph{programming label}
types~(Fig.~\ref{fig:prog-label}). In essence, a programming label
 is a phantom type around the type . However,
the label  is crucial in that it represents the arguments of the
function we are implementing. A motivating example of label types is
the definition of addition by explicitly using the elimination
principle of natural numbers:

Thanks to the label types, we can relate every case of the elimination
principle with the state of our programming problem. 







Using these label types, we can extend our external language to ease
programming. First, we introduce a programming problem by the
 command~(Fig.~\ref{fig:program-decl}). The elaboration
judgment is subject to the following soundness property:
\begin{theorem}[Soundness of program elaboration]
If , 
then .
\end{theorem}

To elaborate the definition of the program, we restrict our attention
to two constructs: returning a value using the \emph{Return}
( gadget and appealing to an elimination principle using
the \emph{By} ( gadget. We shall ignore views and other
syntactic conveniences originally introduced
by~\citet{mcbride.mckinna:view-from-the-left}. The elaboration rules
are given in Figure~\ref{fig:program-def}. The idea is that they are
two rules to realize a programming label: either we return a term, or
we decompose the problem further using an eliminator. We have coded
the \emph{programming grammar} within this specification. Remark the
presence of patterns  on the left hand side to mimic a
pattern-matching notation. Intuitively, the relation
 makes sure that the pattern entered by user
corresponds to the label computed by the type-checker. The relation
 abstracts away the task of generating a
term  in the internal language from an elimination form ,
as described in \citet{mcbride:elim-2,mcbride:elim}. We will not
recall this construction and simply assume that the generated term is
well-typed with respect to T, \ie .



\if 0

\begin{example}[Elaborating addition]

Rather than dwelling on the abstract specification, let us work
through an example. We shall elaborate the following definition of
addition down to a raw term in our type theory. We assume that natural
numbers and their eliminators  and  to be
provided: we shall see later that they are actually available already.

In the external language, we write:



\end{example}

\fi



\section{Elaborating Datatypes}
\label{sec:elab-data-types}



In this Section, we specify the elaboration of inductive types down to
our  universe. While this universe only captures strictly
positive types, it is a good exercise to understand the general idea
governing the elaboration of inductive definitions. Besides, because
the syntax is essentially the same, our presentation should be easy to
understand for readers familiar with Coq or Agda.



We adopt the standard sum-of-product high-level notation:

Where the arguments  are parameters. A  can be
recursive, \ie refers to . Note that it is
crucial that the parameters are the same in the definition and the
recursive calls.

Our translation to code follows the structure of the definition. The
first level structure consists of the choice of constructors and is
translated to a  code over the finite set of
constructors. The second level structure consists of the
-telescope of arguments: these are translated to
right-nested  codes. When parsing arguments, we must make
sure that the recursive arguments are valid and translate them to the
 code.



\subsection{Description labels}
\label{sec:desc-labels}

\newcommand{\datatel}[1]{\mathrm{datatel}}
\newcommand{\TypeDatatel}[2]{\Judgment{#1}{#2 \:\datatel{}}}

\newcommand{\IntDatatel}[1]{\green{\llbracket} #1 \green{\rrbracket}_P}

\newcommand{\LabelDesc}[1]{\langle #1 \rangle}
\Spacedcommand{\ReturnDesc}{\Constructor{return}}
\newcommand{\CallDesc}[2]{\Function{call}\: \LabelDesc{#1}\: #2}



To guide the elaboration of inductive definitions, we extend the type
theory with \emph{description
  labels}~(Fig.~\ref{fig:desc-label}). Their role is akin to
programming labels: they structure the elaboration task and are used
to ensure that recursive arguments are correctly elaborated.

A description label  is a list starting with the name
of the datatype being defined and followed by the parameters of that
datatype. It can be thought as a phantom type around . We can
introduce such type using  that takes the (finite) set
of constructors and their respective code: doing so, we ensure that we
are only accepting tagged descriptions. With , we
eliminate  by joining constructors and their codes
in a  code, effectively interpreting the choice of
constructors.


\begin{figure}[bt]



\caption{Description label}
\label{fig:desc-label}

\end{figure}



\subsection{Elaborating inductive types}




\Spacedcommand{\Tree}{\Canonical{Tree}}
\Spacedcommand{\FatTree}{\mathbf{Tree}}
\newcommand{\TreeLeaf}{\Constructor{leaf}}
\Spacedcommand{\TreeNode}{\Constructor{node}}


We shall present our translation in a top-down manner: from a complete
definition, we show how the pieces fit together, giving some intuition
for the subsequent translations. We then move on to disassemble and
interpret each sub-component separately. As we progress, the reader
should check that the intuition we gave for the whole is indeed
valid. Every elaboration step is backed by a soundness property:
proving these properties is inherently bottom-up. Only after having
developed all our definition will we be able to prove the soundness of
elaboration of inductive definitions. However, the proof is
technically unsurprising: we shall briefly sketch it at the end of
this Section.  To further ease the understanding of our machinery, we
shall illustrate every step by elaborating the following definition of
binary trees:




\begin{figure}[htp]

\centering



\subfloat[][Elaboration of definition]{

\label{fig:elab-desc-data}
}
\\
\subfloat[][Elaboration of choices]{

\label{fig:elab-desc-choices}
}
\\
\subfloat[][Elaboration of constructor]{

\label{fig:elab-desc-constr}
}
\\
\subfloat[][Elaboration of arguments]{

\label{fig:elab-desc-arg}
}
\\
\subfloat[][Recursion matching]{

\label{fig:elab-desc-rec}
}



\caption{Elaboration of inductive types}
\label{fig:elab-desc}

\end{figure}




\paragraph{Elaboration of an inductive definition (Fig.~\ref{fig:elab-desc-data}):}
Elaborating an inductive definition extends the input context with the
datatype definition. To obtain this definition, we first elaborate the
parameters and move onto elaborating the choice of constructors,
introducing a description label in the process.

\begin{example}[Elaborating ]

Applied to our example, we obtain:

where


\end{example}



\paragraph{Elaboration of constructors (Fig.~\ref{fig:elab-desc-choices}):} 
To elaborate the choice of constructors, we elaborate each individual
constructor, hence obtaining their respective label and code. We then
return the finite collection of constructor names and their
corresponding codes. This elaboration step is subject to the soundness
property:
\begin{lemma}\label{lemma:elab-desc-choices}
If , 
then 
\end{lemma}

\begin{example}[Elaborating ]

Applied to our example, we obtain:

Where  and  have been defined
above.

\end{example}




\paragraph{Elaboration of constructor (Fig.~\ref{fig:elab-desc-constr}):}
The role of this elaboration step is twofold. First, we extract the
constructor name and elaborate it into a label. Second, we elaborate
the arguments of that constructor, hence obtaining a 
code. We return the pair of the label and the arguments' code, subject
to the following soundness property:
\begin{lemma}\label{lemma:elab-desc-constr}
If 
, then
.

\end{lemma}

\begin{example}[Elaborating ]

Since our datatype definition has two constructors, there are two
instances of constructor elaboration:


\end{example}




\paragraph{Elaboration of arguments (Fig.~\ref{fig:elab-desc-arg}):}
The last and perhaps most interesting rule is the elaboration of
arguments. Intuitively, the arguments form a telescope of
-types, hence our translation to  and 
codes. The first two rules are non-deterministic:  could either
be a proper type or a recursive call. In the first case, this maps to
a standard  code, while in the second case, we must make
sure that the recursive call is valid and, if so, we generate a
 code. We also support exponentials in the definitions,
mapping them to the  code. Once all arguments have been
processed, we conclude by generating the  code. This
translation is subject to the following soundness property:
\begin{lemma}\label{lemma:elab-desc-arg}
If 
, then
.
\end{lemma}

\begin{example}[Elaborating ]

Elaborating the arguments of the  constructor is trivial:

As for the  constructor, we obtain its code through the
following sequence of elaborations:


\end{example}





We can now prove the soundness of the whole translation. We
formulate soundness as follow:
\begin{theorem}[Soundness of elaboration]

If , 
then .

\end{theorem}
\begin{proof}

First, we prove Lemma~\ref{lemma:elab-desc-arg} by induction on the
list of arguments. We then obtain
Lemma~\ref{lemma:elab-desc-constr}. By applying this lemma to all
constructors, we obtain Lemma~\ref{lemma:elab-desc-choices}. From this
Lemma, we finally obtain the soundness theorem.

\end{proof}



While our soundness theorem gives some hint as to the correctness of
our specification, we could obtain a stronger result by proving an
equivalence between Coq's \texttt{Inductive} definitions and the
corresponding datatype declaration in our system. This equivalence
amounts to proving the equivalence of the associated elimination
forms, \ie \texttt{Fix} in Coq and  in our
system. However, since we do not know of any formal description of
elimination principles generated from an \texttt{Inductive}
definition, we shall use the simpler presentation given in
\citet{gimenez:coq-induction}.

\newcommand{\InterpretConstForm}[1]{\lfloor #1 \rfloor}

The fact that our induction principle is provable in Coq is a known
result~\citep{dagand:levitation}. The other direction consists in
proving that any Coq \texttt{Fix} definition can be implemented using
our induction principle. To this end, we use Gim\'{e}nez reduction of
\texttt{Fix} definitions down to elimination rules. To prove this
result, we first translate Gim\'{e}nez constructor forms to our
universe of code:

We thus get a translation from his recursive types declaration to a code in our
universe:

Having done that, it is then a straightforward symbol-pushing exercise
to prove that Coq's elimination rules (Section~3.1.1,
\citep{gimenez:coq-induction}) can be reduced to our generic
elimination principle. The crux of the matter consists in showing that
the minor premises -- defined by  in that paper --
maps to the induction hypothesis described by
 in our system.







\section{Elaborating inductive families}
\label{sec:elab-data-family}



In this Section, we extend our treatment of inductive definitions to
inductive families. To do so, we add support for indices and
computation on these indices. The resulting system subsumes the one
presented in the previous Section. Hence, we shall reuse many
notations used previously: this should not affect reasoning or
implementing this elaborator, since only that last system is
necessary. We shall however develop our intuition of this more
powerful presentation from the simpler translation of inductive types.



The syntax we elaborate is strongly inspired by the syntax used by
Agda and Coq. However, to support computation over indices, we support
the Epigram-style \emph{by} gadget (. Our language of
inductive definition is therefore more complex, following the
skeleton:



For anyone wanting to reason about Agda or Coq definitions, it is
straightforward to simply discard the elaboration of computation over
indices. Thus, with minor adjustement, our formalization of
elaboration gives a translation semantics to inductive definitions
used in these main-stream theorem provers.



\subsection{Description labels}

\newcommand{\idatatel}[1]{\mathrm{idatatel}_{#1}}
\newcommand{\TypeIDataTel}[2]{\Judgment{#1}{#2 \:\idatatel{}}}
\newcommand{\InterpretIDataTel}[1]{\green{\llbracket} #1 \green{\rrbracket_D}}



In Section~\ref{sec:desc-labels}, we have introduced the notion of
description labels. While it was enough to deal with
parameterized definitions, we have to extend this notion to
account for indexing. Besides, indexing can be either unconstrained --
some index  -- or it can be constrained to some
particular value -- such as . Besides, a label is
not a phantom type around a description but around an 
where  corresponds to the product of all index types the label is
taking as arguments. 



These requirements lead us to the definition of description labels
given in Figure~\ref{fig:idesc-label}. We define label's arguments
through a telescope extended with indices and constraints. To compute
the index type of the resulting description, we introduce the
 function. Accordingly, we update the introduction
and elimination rule of labels to account for the index type computed
from the telescope.

\begin{figure}


\caption{Description label (indexed)}
\label{fig:idesc-label}

\end{figure}





\newcommand{\IDataPattern}{\mathrm{patt}}
\newcommand{\TypeIDataPattern}[2]{\Judgment{#1}{#2 \:\IDataPattern}}

\if 0

Unfortunately, this is not enough. Since we want to be explicit about
constraints on indices, we must introduce another syntactic artifact
to represent these constraints. 




\fi



\renewcommand{\ElabChoices}[5]
           {#1 \vdash #3 \ni #2 
                 \ElabRel{Cs}
                     [#4 \mapsto #5]}


\begin{figure}[tbp]

\centering



\subfloat[][Elaboration of definition]{

\label{fig:idesc-elab-idata}
}
\\
\subfloat[][Elaboration of patterns]{

\label{fig:idesc-elab-datapatts}
}
\\
\subfloat[][Elaboration of choices]{

\label{fig:idesc-elab-choices}
}
\\
\subfloat[][Elaboration of constructor]{

\label{fig:idesc-elab-constr}
}

\caption{Elaboration of inductive families (1)}
\label{fig:elab-families-1}

\end{figure}

\begin{figure}

\centering
\subfloat[][Elaboration of arguments]{

\label{fig:idesc-elab-arg}
}
\\
\subfloat[][Pattern validation]{

\label{fig:idesc-elab-indices}
}
\\
\subfloat[][Elaboration of constraints]{

\label{fig:idesc-elab-eqs}
}
\\
\subfloat[][Extraction of indices]{

\label{fig:idesc-elab-recargs}
}

\caption{Elaboration of inductive families (2)}
\label{fig:elab-families-2}

\end{figure}



As for inductive types, we shall present the elaboration process in a
top-down manner. This presentation shares some strong commonality with
the simpler elaboration of inductive types: we elaborate choices of
constructors (Fig.~\ref{fig:idesc-elab-choices}), followed by
individual constructors (Fig.~\ref{fig:idesc-elab-constr}), and
finally processing the telescope of arguments
(Fig.~\ref{fig:idesc-elab-arg}). However, the presence of indices
introduces some new steps too. We support constraint of indices and
computation over them through a new top-level rule
(Fig.~\ref{fig:idesc-elab-datapatts}). Besides, we must translate the
constraints to actual equalities (Fig.~\ref{fig:idesc-elab-eqs}) and
pass the correct indices when elaborating a recursive call
(Fig.~\ref{fig:idesc-elab-recargs}).



\subsection{Elaborating inductive families}



To give a better intuition of a perhaps intricate system, we should
illustrate every inference rule with two examples. Our first example
is the definition of vectors relying on constraints to enforce the
indexing discipline:

While our second example consists of the alternative definition of
vector, where we compute over the index to determine which constructor
to offer:


\Spacedcommand{\FatVector}{\mathbf{Vec}}



\newcommand{\choicesVecEq}{\mathrm{choices}_{=}}
\newcommand{\codeVecEq}{\mathrm{code}_{=}}

\paragraph{Elaboration of inductive families (Fig.~\ref{fig:idesc-elab-idata}):} 
The elaboration of an inductive definitions sets up the environment to
trigger the elaboration of the choices of constructors. To do so, we
first elaborate the telescope of parameters and indices types. We can
then translate the choices by elaborating against the label type
corresponding to the given inductive type.

\begin{example}[Vector, constrained]
The elaboration of constraint-based vectors starts as follow:

where


\end{example}

\newcommand{\choicesVecComp}{\mathrm{choices}_{\to}}
\newcommand{\codeVecComp}{\mathrm{code}_{\to}}

\begin{example}[Vector, computed]

The same skeleton is used in the alternative definition of vectors,
but the choices of constructors -- and therefore the resulting code --
are different:
{\small

}

\end{example}



\paragraph{Elaboration of pattern choices (Fig.~\ref{fig:idesc-elab-datapatts}):}
This elaboration process is an extra step that was not necessary with
inductive types. With inductive families, we can both constrain the
index to some particular value or compute over the index to refine the
choice of constructors. Hence, an inductive definition is a list of
pattern choices, potentially ending with a computation over the
indices. Since the case where no index computation is performed is
merely a special case of the rule we give, we save space and do not
write it down explicitly.

The elaboration of pattern choices consists in interpreting the
datatype patterns of each constructor choices. Then, the resulting
labels are used to elaborate these constructors choices. If there is a
computation over indices, we rely on elimination with a motive
\citep{mcbride:elim-2,mcbride:elim} to generate a type theoretic term
from the elimination principle provided by the user. We then interpret
each resulting sub-branch as a pattern choice itself. All in all, this
elaboration step satisfies the following invariant:
\begin{lemma}\label{lemma:idesc-elab-datapatts}

If 
, then


\end{lemma}

Note that we rely on the translation from datatype patterns to data
telescope (Fig.~\ref{fig:idesc-elab-indices}): since we will be
elaborating each individual constructor against these labels, we will
generate the valid equality constraints at the end of each telescope
of arguments.



\begin{example}[Vector, constrained]

The elaboration of datatype patterns simply proceeds over the choices
of constructors, triggering the elaboration of choices on 
and :

where  and  are the same as above.

\end{example}



\begin{example}[Vector, computed]

Similarly for the alternative definition of vectors, this triggers the
elaboration of a motive, of the  and  patterns:

with  and  defined above.
In turn, this triggers the elaboration of datatype choices for the
 and  patterns:




\end{example}




\paragraph{Elaboration of choices (Fig.~\ref{fig:idesc-elab-choices}):}
The elaboration of the choice of datatypes is the same as for
inductive types: we merely collect the tag and code of each individual
constructor and return these as enumerations. This step is subject to
the following soundness property:
\begin{lemma}\label{lemma:idesc-elab-choices}
If 
, then


\end{lemma}



\begin{example}[Vector, constrained]

In the particular example of vector, there is only one choice of
constructor when  -- namely 
-- and when  -- namely
. Therefore, we obtain the elaboration of choices from the
elaboration of the unique constructor, in both cases:


\end{example}



\begin{example}[Vector, computed]

The same situation arises in the alternative definition of vector:
once we have determined which index we are dealing with, there is a
single constructor available. Hence, we move from the elaboration of
choices to the elaboration of the unique constructor:


\end{example}




\paragraph{Elaboration of constructor (Fig.~\ref{fig:idesc-elab-constr}):}
Again, the elaboration of constructor is not any different from the
inductive types case. It is subject to the following invariant:
\begin{lemma}\label{lemma:idesc-elab-constr}
If
, then

\end{lemma}



\begin{example}[Vector, constrained]

Elaboration of the constructors is straightforward, simply switching
to the elaboration of the arguments:


\end{example}



\begin{example}[Vector, computed]

Similarly for the alternative definition, we have:


\end{example}




\paragraph{Elaboration of arguments (Fig.~\ref{fig:idesc-elab-arg}):}
The elaboration of arguments follows the same principle as for
inductive types. However, when elaborating a recursive argument, we
must extract the indices for which that recursive step is
taken. Besides, after having encoded the list of arguments, we must
switch to translating the potential equality constraints, which are
dictacted by the label type we are elaborating against. This step is
subject to the following soundness property:
\begin{lemma}\label{lemma:idesc-elab-arg}
If 
, then

\end{lemma}



\begin{example}[Vector, constrained]

We then elaborate the arguments by unfolding the telescope, at which
point we switch to elaborating the constraints. We do so immediatly in
the  case:

While a few steps are necessary to elaborate the arguments in the
 case, including the elaboration of the recursive call:


\end{example}



\begin{example}[Vector, computed]

In the alternative definition, the process is similar:


\end{example}




\paragraph{Elaboration of constraints (Fig.~\ref{fig:idesc-elab-eqs}):}
In order to generate the equality constraints, we simply traverse the
label we are elaborating against. On constraints, we generate the
corresponding equality constraint, using whatever propositional
equality the system has to offer. On parameters and (unconstrained)
indices, we simply go through. This step satisfies the following
property:
\begin{lemma}\label{lemma:idesc-elab-eqs}
If 
, then
.

\end{lemma}



\begin{example}[Vector, constrained]

We elaborate the constraints in the  case -- constraining
 to  -- and in the  case -- constraining 
to :


\end{example}



\begin{example}[Vector, computed]

No equations are generated and, indeed, needed for the alternative
definition of vectors:


\end{example}




\paragraph{Extraction of indices (Fig.~\ref{fig:idesc-elab-recargs}):}
On the recursive calls, we must extract the indices at which the call
is made. To do so, we match the type definition with the datatype
label. Parameters are ignored while indices are paired together. On
the datatype name, we inhabit . This ensures the following
soundness property:
\begin{lemma}\label{lemma:idesc-elab-recargs}
If 
, then
.
\end{lemma}



\begin{example}[Vector, constrained]

There is only one instance of recursive definition, in the 
case. Its elaboration goes as follow:


\end{example}



\begin{example}[Vector, computed]

Similarly, the elaboration of the index for the recursive definition
is as follow:


\end{example}




Having stated the soundness properties of each individual elaboration
steps, we can now state and prove the soundness of the elaboration of
inductive families:
\begin{theorem}[Soundness of elaboration]

If 
, then
.

\end{theorem}
\begin{proof}

First, we prove that labels elaborate to a type correct index
(Lemma~\ref{lemma:idesc-elab-recargs}). We then prove that the
constraints generated by interpreting the label are valid descriptions
(Lemma~\ref{lemma:idesc-elab-eqs}). From these lemmas, we can prove
the soundness of the elaboration of arguments by induction over the
telescope of arguments (Lemma~\ref{lemma:idesc-elab-arg}). This gives
straightforwardly the validity of the elaboration of a constructor
(Lemma~\ref{lemma:idesc-elab-constr}). Using that Lemma over each
constructors, we thus obtain the soundness of the elaboration of
choices (Lemma~\ref{lemma:idesc-elab-choices}). Using this result and
assuming the soundness of elimination with a motive, we prove
Lemma~\ref{lemma:idesc-elab-datapatts}. This gives the desired result.

\end{proof}



\section{Reflections on Inductives}
\label{sec:discussion}




Having described our infrastructure to elaborate inductive definitions
down to descriptions, we would like to give an overview of the
possibilities offered by such a system. Indeed, in a purely syntactic
presentation of inductives, we are stuck at the meta-level of the type
theory: if we want to provide support to manipulate inductive types,
it must be implemented as part of the theorem prover, out of the type
theory. Alternatively, a quoting/unquoting mechanism could be provided
but guaranteeing the safety of such an extension is likely to be
tricky.

Because our type theory reflects inductives in itself, the meta-theory
of inductive types is no more than a universe. What used to be
meta-theoretical constructions can now be implemented from within the
type theory, benefiting from the various amenities offered by a
dependently-typed programming language. In this Section, we present
two examples of such ``reflection on inductives''. Our first example,
reflecting constructions on
constructors~\citep{mcbride:construction-constructor}, will appeal to
the implementers: we hint at the possibility of implementing key
features of the type theory within itself, a baby step toward
bootstrapping. Our second example, providing a user defined
\texttt{deriving} mechanism, should appeal to programmers: we
illustrate how programmers could provide generic operations over
datatypes and see them automatically integrated in their development.
For simplicity and conciseness, we shall define these mechanisms over
our universe of inductive types, . Nonetheless, it is
straightforward, but more verbose, to extend these constructions to
inductive families.



\subsection{A few constructions on constructors, internalized}
\label{sec:const-on-const}

\citet{mcbride:construction-constructor} describe a collection of
lemmas that theorem prover's implementer would like to export with
every inductive type. In that paper, the authors first show how one
can reduce case analysis and course-of-value recursion to standard
induction. Then, they describe two lemmas over datatype constructors:
\emph{no confusion} -- constructors are injective and disjoint -- and
\emph{acyclicity} -- we can automatically disprove equalities of the
form  where  appears constructor-guarded in .

\Spacedcommand{\CaseDesc}{\Function{case}}

However, since this paper works on the syntactic form of datatype
definitions, it is rife with ``\ldots'' definitions. For instance, the
authors reduce case analysis to induction with no less than ten
ellipsis in the construction. In our system, we generically derive
case analysis by a mere definition \emph{within the type theory}:


\Spacedcommand{\NoConfusion}{\Function{NoConfusion}}
\Spacedcommand{\noConfusion}{\Function{noConfusion}}
\Spacedcommand{\DecideEqEnum}{\Function{decideEq-EnumT}}
\Spacedcommand{\DecideEqEqual}{\Constructor{equal}}
\Spacedcommand{\DecideEqNotEqual}{\Constructor{not-equal}}
\Spacedcommand{\DescEq}{\Function{DescEq}}

Similarly, the authors specify and prove the no confusion lemma over
the skeleton of an inductive definition. In our system, this result is
internalized through two definitions. In the following, we will assume
that  is a tagged description, \ie  where  is the finite sets of constructor
labels. An inhabitant of  is therefore a pair
 with  representing the constructor name and
 the tuple of arguments. We state the  lemma over
such code:

and then prove it by deciding the equality of enumeration index to
discriminate constructor names:

At this stage, we have proved this lemma generically, for all tagged
descriptions. Hence, after having defined a new datatype, a user can
directly use this lemma on her definition. For convenience, a
subsequent elaboration phase could also specialize such lemma to the
particular definition.



\subsection{Deriving operations on datatypes}

Another possible extension of our system is a generic
\texttt{deriving} mechanism. In the \textsc{Haskell} language, we can
write a definition such as

that automatically generates an equality test for the given
datatype. Again, since datatypes are a meta-theoretical entity, this
deriving mechanism has to be provided by the implementer and, template
programming aside, they cannot be implemented by the programmers
themselves. 

\Spacedcommand{\Derivable}{\Canonical{Derivable}}
\Spacedcommand{\Decidable}{\Canonical{Decidable}}
\Spacedcommand{\SemiDecidable}{\Canonical{SemiDecidable}}
\newcommand{\subDesc}{\CN{subDesc}}
\newcommand{\decideIn}{\CN{membership}}
\newcommand{\derive}{\CN{derive}}

In our framework, we could extend the elaborator for datatypes with a
\texttt{deriving} mechanism. However, for such a mechanism to work, we
must restrict ourselves to decidable properties: for example, if the
user asks to derive equality on a datatype that do not have a
decidable equality (\eg, Brouwer ordinals), the system should fail
immediately. To solve this issue, we add one level of
indirection\if 0 \footnote{Quoting David Wheeler, "all problems in computer
  science can be solved by another level of indirection"}\fi: while we
cannot decide equality for \emph{any} datatype, we can decide whether
the datatype belongs to a sub-universe \emph{for which} equality is
decidable. Hence, to introduce a derivable property  in the type
theory, the programmer would populate the following record structure:


\newcommand{\eqDesc}{\Function{eqDesc}}
\newcommand{\decideInEq}{\Function{membershipEq}}
\newcommand{\deriveEq}{\Function{deriveEq}}

For example, in the case of equality, the programmer has first to
provide a function . One
possible (perhaps simplistic, but valid) sub-universe consists only of
products, finite sums, recursive call, and unit: it is enough to describe
natural numbers and variants thereof. She then implements a procedure
 deciding whether a given
 code fits into this sub-universe or not. Recall that
 corresponds to . It should be clear that the membership of a
 code to our sub-universe of finite products and sums is
decidable. Finally, she implements the key operation
 that
decides equality of two objects, assuming that they belong to the
sub-universe. This implements the structure
.

\Spacedcommand{\NatEq}{\Function{Nat-eq}}
\Spacedcommand{\Witness}{\Function{witness}}

While elaborating a datatype, it is then straightforward -- and
automatic -- for us to generate its derivable property, or reject it
immediately: we simply compute  on the specific code. If
we obtain a negative response, we report an error. If we obtain a
positive witness, we pass that witness to  and instantiate
the derived property. 

For example, since natural numbers fit into the  universe,
the elaboration machinery would automatically generate the following
decision procedure

without any input from the user but the \texttt{deriving}
 clause. Here,  is a library function
that extracts the witness from a true decidable property: applied to
, the function  computes a positive witness
that we can simply extract.



\newpage
\section{Conclusion}



In this paper, we have striven to give a coherent framework for
elaboration in type theory. We have organized our system around two
structuring ideas. First, by using the flow of typing information, we
obtain a richer and more flexible term language. Second, by using
types as presentation of high-level concepts, such as inductive
definition, we can effectively guide the elaboration process. This
technique is conceptually simple and therefore amenable to formal
reasoning. This simplicity together with the soundness proofs should
convince the reader of its validity.



From there, we believe that reasoning on inductive definitions can be
liberated from the elusive ellipsis: proofs and constructions on
inductives ought to happen within the type theory itself. After
\citet{harper:elaboration}, we claim that if the treatment of
datatypes is conceptually straightforward then it ought to be
technically straightforward and implemented as a generic program in
the type theory. For non-straightforward properties, our results
should be reusable across calculi -- such as the Calculus of Inductive
Constructions -- and not too rigidly tied to our universe of
datatypes. Besides, we were careful to present elaboration as a
relation rather than a mere program, making it more amenable to
abstract reasoning.



To support our claim that inductives should be bootstrapped, we have
presented two possible extensions to the elaboration process. While we
did not formalize these examples, our expectations seem reasonable and
our experience modeling them in Agda further supports this
impression. We have seen how generic theorems on inductive types can
be internalized as generic programs: besides the benefit of reducing
the trusted computing base, their validity is guaranteed by type
checking. Also, we have glimpsed at a generic \texttt{deriving}
mechanism: with no extension to the type theory, we are able to
let the user define sub-universes that support certain
operations. These operations could then be automatically specialized
to the datatypes that support them, all that without any user
intervention.



\paragraph{Future work:} A next step would be to adapt our syntax to coinductive
types. Similarly, it would be interesting to see if it scales to
inductive-recursive definitions. On the implementation side, we are
currently implementing these various systems in a toy type
theory. This step will require a formalization of our
\texttt{deriving} mechanism. Also, we will have to generate the
specialized induction principles (such as case analysis and
course-of-value recursion) as well as the constructions on
constructors. Finally, an interesting challenge would be to
internalize the elaboration process \emph{itself} in type theory,
hence obtaining a correct-by-construction translation.

\paragraph{Acknowledgements} 
We would like to thank Pierre Boutillier for pointing us to the
relevant literature on Coq's treatment of inductives. We also thank
our colleagues Guillaume Allais and Stevan Andjelkovic for many
stimulating discussions and for their input on this paper. The authors
are supported by the Engineering and Physical Sciences Research
Council, Grant EP/G034699/1.



\newpage
\bibliographystyle{abbrvnat}
\bibliography{paper,../thesis-2011-phd/levitation,../thesis-2011-phd/funorn} 





\end{document}
