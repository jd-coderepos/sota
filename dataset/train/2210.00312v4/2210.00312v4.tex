
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{color}
\usepackage{array}

\newtheorem{remark}{\noindent \textbf{Remark}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Multimodal Analogical Reasoning over Knowledge Graphs}





\author{Ningyu Zhang \quad Lei Li\quad Xiang Chen\quad Xiaozhuan Liang \quad Shumin Deng \quad \textbf{Huajun Chen}\thanks{Corresponding author.}\\
  Zhejiang University, AZFT Joint Lab for Knowledge Engine\\
National University of Singapore \\
  \texttt{\{zhangningyu,leili21,xiang\_chen,liangxiaozhuan,231sm,huajunsir\}@zju.edu.cn}\\ 
}

 

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\ours}{MarT}
\newcommand{\data}{MARS}
\newcommand{\kg}{MarKG}

 \iclrfinalcopy \begin{document}


\maketitle
\begin{abstract}
Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a \textbf{M}ultimodal \textbf{A}nalogical \textbf{R}easoning data\textbf{S}et (\textbf{MARS}) and a multimodal knowledge graph \textbf{MarKG}. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic \textbf{M}ultimodal \textbf{a}nalogical \textbf{r}easoning framework with \textbf{T}ransformer (\textbf{\ours}) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research\footnote{Code and datasets are available in \url{https://github.com/zjunlp/MKG_Analogy}.}. 
\end{abstract}

\section{Introduction}


Analogical reasoning – the ability to perceive and use relational similarity between two situations or events – holds an important place in human cognition \citep{DBLP:journals/jetai/Johnson-Laird06,DBLP:journals/corr/abs-2007-11668,DBLP:journals/cacm/BengioLH21,E-KAR} and can provide back-end support for various fields such as education \citep{thagard1992analogy}, creativity \citep{goel1997design}, thus appealing to the AI community.
Early, \cite{mikolov-etal-2013-linguistic,BATs,DBLP:conf/acl/EthayarajhDH19a} propose visual analogical reasoning aiming at lifting machine intelligence in Computer Vision (CV) by associating vision with relational, structural, and analogical reasoning.
Meanwhile, researchers of Natural Language Processing (NLP) hold the connectionist assumption \citep{smp}  of linear analogy \citep{ethayarajh-etal-2019-towards}; for example, the relation between two words can be inferred through vector arithmetic of word embeddings.
However, it is still an open question whether artificial neural networks are also capable of recognizing analogies among different modalities.


Note that humans can quickly acquire new abilities based on finding a common relational system between two exemplars, situations, or domains.
Based on Mayer's Cognitive Theory of multimedia learning \citep{hegarty1993constructing,mayer2002multimedia}, human learners often perform better on tests with analogy when they have learned from multimodal sources than single-modal sources. 
Evolving from recognizing single-modal analogies to exploring multimodal reasoning for neural models, we emphasize the importance of a new kind of analogical reasoning task with Knowledge Graphs (KGs).


In this paper, we introduce the task of multimodal analogical reasoning over knowledge graphs to fill this blank.
Unlike the previous multiple-choice QA setting, we directly predict the analogical target and formulate the task as \textbf{link prediction without explicitly providing relations}.
Specifically, the task can be formalized as  with the help of background multimodal knowledge graph , in which ,  or  have different modalities.
We collect a \textbf{M}ultimodal \textbf{A}nalogical \textbf{R}easoning data\textbf{S}et (\textbf{{\data}}) and a multimodal knowledge graph \textbf{{\kg}} to support this task. 
These data are collected and annotated from seed entities and relations in E-KAR \citep{E-KAR} and BATs \citep{BATs}, with linked external entities in Wikidata and images from  Laion-5B \citep{Laion400M}.

To evaluate the multimodal analogical reasoning process, we follow the guidelines from psychological theories and conduct comprehensive experiments on {\data} with multimodal knowledge graph embedding baselines and multimodal pre-trained Transformer baselines.
We further propose a novel \textbf{M}ultimodal \textbf{a}nalogical \textbf{r}easoning framework with \textbf{T}ransformer, namely {\textbf{\ours}}, which is readily pluggable into any multimodal pre-trained Transformer models and can yield better performance.


To summarize, our contributions are three-fold:
(1) We advance the traditional setting of analogy learning by introducing a new multimodal analogical reasoning task.
Our work may open up new avenues for improving analogical reasoning through multimodal resources.
(2) We collect and build a dataset {\data} with a multimodal knowledge graph \kg, which can be served as a scaffold for investigating the multimodal analogy reasoning ability of neural networks.
(3) We report the performance of various multimodal knowledge graph embedding, multimodal pre-trained Transformer baselines, and our proposed framework {\ours}.
We further discuss the potential of this task and hope it facilitates future research on zero-shot learning and domain generalization in both CV and NLP.


 
 

\section{Background}

\subsection{Analogical Reasoning in Psychological}
To better understand analogical reasoning, we introduce some crucial theories from cognitive psychology, which we take as guidelines for designing the multimodal analogical reasoning task.

\textbf{Structure Mapping Theory (SMT)} \citep{smp}\textbf{.}
SMT is a theory that takes a fundamental position in analogical reasoning. 
Specifically, SMT emphasizes that humans conduct analogical reasoning depending on the shared \textit{relations} structure rather than the superficial \textit{attributes} of domains and distinguishes analogical reasoning with literal similarity.
\cite{minnameier2010abduction} further develops the inferential process of analogy into three steps: abduction, mapping and induction, 
which inspires us to design benchmark baselines for multimodal analogical reasoning.
 
\textbf{Mayer's Cognitive Theory} \citep{hegarty1993constructing,mayer2002multimedia}\textbf{.} 
Humans live in a multi-source heterogeneous world and spontaneously engage in analogical reasoning to make sense of unfamiliar situations in everyday life~\citep{vamvakoussi2019use}. 
Mayer's Cognitive Theory shows that human learners often perform better on tests of recall and transfer when they have learned from multimodal sources than single-modal sources. 
However, relatively little attention has been paid to multimodal analogical reasoning, and it is still unknown whether neural network models have the ability of multimodal analogical reasoning.

 
\subsection{Analogical Reasoning in CV and NLP}

\textbf{Visual Analogical  Reasoning.}
Analogical reasoning in CV aims at lifting machine intelligence by associating vision with relational, structural, and analogical reasoning~\citep{CLEVR,DBLP:conf/ijcai/PradeR21,DBLP:conf/aaai/HuMLWB21,DBLP:journals/corr/abs-2201-12382}.
Some datasets built in the context of Raven's Progressive Matrices (RPM) are constructed, including PGM~\citep{PGM} and RAVEN~\citep{RAVEN}. 
Meanwhile, \cite{DBLP:conf/iclr/HillSBML19} demonstrates that incorporating structural differences with structure mapping in analogical visual reasoning benefits the machine learning models.
\cite{DBLP:conf/cvpr/HayesK21} investigates online continual analogical reasoning and demonstrates the importance of the selective replay strategy. 
However, these aforementioned works still focus on analogy reasoning among visual objects while ignoring the role of complex texts.


\textbf{Natural Language Analogical Reasoning.}
In the NLP area, early attempts devote to word analogy recognition~\citep{mikolov-etal-2013-linguistic,BATs,DBLP:conf/semeval/JurgensMTH12,DBLP:conf/acl/EthayarajhDH19a,DBLP:conf/naacl/GladkovaDM16} which can often be effectively solved by vector arithmetic for neural word embeddings Word2Vec~\citep{word2vec} and Glove~\citep{glove}. 
Recent studies have also evaluated on the pre-trained language models~\citep{bert,gpt,DBLP:conf/acl/UshioASC20}.
However, word analogies mainly measure the quality of word representations and do not explore the analogical reasoning ability of models. 
Thus, \cite{E-KAR} builds a knowledge-intensive benchmark to evaluate the analogical reasoning ability of neural models.
Nevertheless, \cite{E-KAR} mainly focuses on reasoning in the textual domain and does not consider using external knowledge graphs. 
In this work, we take the first step to investigate multimodal analogical reasoning over knowledge graphs.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.75]{figs/task.pdf}
\caption{
Overview of the Multimodal Analogical Reasoning task. 
We divide the task into single and blended settings with a multimodal knowledge graph.
Note that the relation marked by dashed arrows () and the text around parentheses under images are \textbf{only for annotation} and \textbf{not provided in the input}.}
\label{fig:task}
\vspace{-0.3cm}
\end{figure}

\section{The Multimodal Analogical Reasoning Task}
\subsection{Task Definition}

In this section, we introduce the task of Multimodal Analogical Reasoning that can be formulated as link prediction  \textbf{without explicitly providing relations}. 
As shown in Figure \ref{fig:task}, given an analogy example  and a question-answer entity pair  where
 and  ,
the goal of analogical reasoning is to predict the missing entity . 
Moreover, multimodal analogical reasoning is based on
background multimodal knowledge graph , where  and 
are sets of entities and relations,
 and  represent images and textual descriptions of entities. 
Note that the relations of  and  are identical but unavailable, and the relation structure can be analogized implicitly from source domain to target domain without knowing the relations.
Specifically, the task can be formalized as , further divided into \emph{Single Analogical Reasoning} and \emph{Blended Analogical Reasoning} according to different modalities of  and .

\textbf{Single Analogical Reasoning.}
In this setting, the analogy example and the question-answer entity pair involve only one modality.
As shown in the middle column of Figure \ref{fig:task},
the modalities of the analogy example  are identical and opposite to the analogy question-answer pair .
Based on both visual and textual modalities, this setting can be further divided into  and  where  represent the modality of  is visual or textual respectively.

\textbf{Blended Analogical Reasoning.}
In the setting, the modality of analogy example  are unidentical, which is similar to real-world human cognition and perception\footnote{For example, humans invented hieroglyphics by analogy from the concrete world.}.
Note that Mayer's theory indicates that humans can have powerful transfer and knowledge recall abilities in multimodal scenarios. 
Inspired by this, we propose the blended analogical reasoning that can be formalized as , which means the modalities between  () and  () are different.

\subsection{Data Collection and Preprocessing}
\label{sec:data_collect}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.7]{figs/flowchart.pdf}
\caption{
An illustration of data collection and processing steps to create {\data} and {\kg}.}
\label{fig:flowchart}
\end{figure}

\begin{table*}[!htp]
\centering
\small
\scalebox{0.7}{
\begin{tabular}{lcccccccc}
\toprule
Dataset &
\begin{tabular}[c]{@{}c@{}}Size \\ 
    (train / dev / test)
\end{tabular} 
& KB  & Modality & \# Entity & \# Relation & \# Images & 
\begin{tabular}[c]{@{}c@{}}Knowledge \\ Intensive
\end{tabular} 
& Task Format \\
\midrule
    RAVEN & 42,000 / 14,000 / 14,000 & \ding{56} & Vision & - & 8 & 1,120,000 &  \ding{56} & Classification  \\
SAT & 0 / 37 / 337  & \ding{56} & Text  & - & 19 & - & \ding{56}  & Linear Word Analogy \\
    Google  & 0 / 50 / 500  & \ding{56}  & Text & 919 & 14 & - & \ding{56} & Linear Word Analogy \\
    BATs    & 0 / 199 / 1,799 & \ding{56} & Text & 6,218 & 40 & - & \ding{56} & Linear Word Analogy\\
    E-KAR   & 870 / 119 / 262 & \ding{56} & Text & 2,032 & 28 & - & \ding{52} & Multiple Choice QA \\

{\data}     & 10,685 / 1,228 / 1,415 & {\kg} & Vision+Text & 2,063 & 27 & 13,398 & \ding{52} & Entity Prediction \\
\bottomrule
\end{tabular}
}
\caption{
Comparison between {\data} and previous analogical reasoning datasets. ``KB'' refers to the knowledge base, \# denotes the number. ``Knowledge Intensive'' means reasoning requires external knowledge. 
Our {\kg} focuses on knowledge-intensive reasoning across multiple modalities.}
\label{tab:mars}
\end{table*}

We briefly introduce the construction process of the dataset in Figure~\ref{fig:flowchart}.
Firstly, we collect a multimodal knowledge graph dataset {\kg} and a multimodal analogical reasoning dataset {\data}, 
which are developed from seed entities and relations in E-KAR \citep{E-KAR} and BATs \citep{BATs}. 
Secondly, we link these seed entities into the free and open knowledge base Wikidata\footnote{\url{https://www.wikidata.org}} for formalization and normalization. 
Thirdly, to acquire the image data, we further search from the Google engine and query from the multimodal data Laion-5B \citep{Laion400M} by the text descriptions of entities. 
Then, an image validation strategy is applied to filter low-quality images. 
Lastly, we sample high-quality analogy data to construct {\data}. 
A detailed description of the data collection and processing to create our datasets are in Appendix \ref{adx:construction} and \ref{adx:sample}.
 


\subsection{Dataset Statistics}
{\data} is the evaluation dataset of the multimodal analogical reasoning task that contains analogy instances, while {\kg} can provide the relative structure information of those analogy entities retrieved from Wikidata.
The statistics of {\data} and  {\kg}  are shown in Table~\ref{tab:mars}
and Table~\ref{tab:adx_static_markg}. 
{\kg} has 11,292 entities, 192 relations and 76,424 images,  include 2,063 analogy entities and 27 analogy relations. 
{\data} has 10,685 training, 1,228 validation and 1,415 test instances, which are more significant than previous language analogy datasets. \textcolor{black}{The original intention of MarKG is to provide prior knowledge of analogy entities and relations for better reasoning.}
We release the dataset with a leaderboard at \url{https://zjunlp.github.io/project/MKG_Analogy/}.
More details including \textbf{quality control} can be found in Appendix~\ref{sec:adx_dataset_static}.


\subsection{Evaluation Metrics}
Previous study \citep{E-KAR} adopts the multiple-choice QA to conduct analogical reasoning and leverage the accuracy metric for evaluation. 
However, the multiple-choice QA setting may struggle to handle the \emph{one-to-more entities},
which is very common in real-world analogy scenarios.
Thus, we formulate the task as link prediction that directly predicts the answer entity .
Our evaluation metrics include Hits@k scores (proportion of valid entities ranked in top k) and MRR (reciprocal value of the mean rank of correct entities). 
More details  can be found in Appendix~\ref{adx:metric}.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.95]{figs/model.pdf}
\caption{
Overview of baseline methods. 
(a) Pipeline of MKGE methods for multimodal analogical reasoning. 
(b) and (c) are two stages of multimodal pre-trained Transformer (MPT) baselines.}
\label{fig:model}
\end{figure}


\section{Benchmark Methods}

In this section, we introduce some baselines to establish the initial benchmark results on {\data}, including multimodal knowledge graph embedding baselines and multimodal pre-trained Transformer baselines. 
We further propose {\ours}: a multimodal analogical reasoning framework with Transformer, which can capture fine-grained associations between one analogy example and one analogy question-answer pair for better multimodal analogy abilities.


\subsection{Multimodal Knowledge Graph Embedding Baselines}

We consider three multimodal knowledge embedding (MKGE) approaches as our baselines, including \textbf{IKRL} \citep{IKRL}, \textbf{TransAE} \citep{TransAE},  and \textbf{RSME} \citep{RSME}.  
These methods are typically based on TransE \citep{TransE} or ComplEx \citep{complex} and combine with visual encoders to encode images for multimodal knowledge representation learning.
They can not be directly applied to the multimodal analogical reasoning task.
To successfully utilize MKGE methods, we first pre-train them on {\kg} to obtain entity embeddings and then follow the structure-mapping theory~\citep{minnameier2010abduction} to leverage the \textit{Abduction-Mapping-Induction} as \textbf{explicit
pipline} steps for MKGE methods.
As shown in Figure~\ref{fig:model}.a, \textit{Abduction} aims to predict the relation  of  similar to the relation classification task, \textit{Mapping} represents that the structural relation is mapped onto entity candidates, analogous to template-filling, and \textit{Induction} utilizes the relation  to predict the tail entity of  similar to the link prediction task.

Despite the previous MKGE methods achieving excellent performance for KG-related tasks, the backbone, such as TransE, is not designed for analogy reasoning, which may hinder performance. 
Thus, we replace the backbone of MKGE methods with ANALOGY~\cite{icml_analogy} that models analogical structure explicitly as baselines.
 

\subsection{Multimodal Pre-trained Transformer Baselines}
We select multimodal pre-trained Transformer (MPT) approaches including the single-stream models \textbf{VisualBERT} \citep{VisualBERT}, \textbf{ViLT} \citep{ViLT}, the dual-stream model \textbf{ViLBERT} \citep{ViLBERT}, and the mixed-stream model \textbf{FLAVA} \citep{FLAVA} and \textbf{MKGformer}~\cite{MKGformer} as the strong baselines.
However, the current multimodal pre-trained Transformer cannot directly deal with analogical reasoning.
To address the bottleneck above, we devise an \textbf{end-to-end} approach to empower the MPT with analogical reasoning ability.
As shown in Figure \ref{fig:model}, we first leverage {\kg} to pre-train the model over sparese {\kg} to obtain the representation of entities and relations. 
We then present the prompt-based analogical reasoning over {\data}.


\subsubsection{Pre-train over MarKG}
\label{sec:pre-train}
We represent the entities  and relations  as special tokens and denote  as the learnable embedding of these special tokens in the word vocabulary of language models.
In the pre-train stage, we design masked entity and relation prediction like the Masked Language Modeling (MLM) task to learn the embeddings of the special tokens over the {\kg} dataset.
As shown in Figure~\ref{fig:model}.b, we devise a prompt template to convert the input as predicting the missing entity and relation via \texttt{[MASK]} token.
In addition, we mix missing relation and entity prediction in the pre-train stage and consider different modalities of input entities.
Specifically, we represent the visual entity  by its image  and special entity embedding , and the text entity  by its text description  and special entity embedding  , respectively. 
Benefiting from the mixed entity and relation prediction with the multimodal entity in the pre-train stage, we can obtain KG embedding with multimodal semantics over the current knowledge graph \kg.

\subsubsection{Prompt-based Analogical Reasoning}

Based on the above-pre-trained entity and relation embeddings over \kg, we propose prompt-based analogical reasoning with implicit structure mapping on downstream \data. 


Taking the blended analogical reasoning as an example, we feed the analogy example  and analogy question-answer pair  as input, and the goal is to predict the missing answer entity .
We leverage an analogical prompt template to convert the input as  follows:

where  represents concatenate operation in the template input,  and  represent the images of the entity  and ,  is the text description of the entity . \textcolor{black}{Moreover,  are entity ids and will be encoded to special entity tokens  in word embedding layer.}
Since the relations are not explicitly provided in the actual analogical reasoning task, we assign  as a special token to denote the explicit relation between , which is initialized with the average relation embeddings.
\textcolor{black}{
Finally, we train the model to predict the \texttt{[MASK]} over the special token embedding  via cross-entropy loss, which likes the MLM task.
}
\begin{remark}
We summarize the two parts of  and  in the template as the implicit \textit{Abduction} and  \textit{Induction} respectively, which are unified in an end-to-end learning manner with prompt tuning.
In addition, the analogical reasoning is reformulated as predicting the  over the multimodal analogy entity embeddings to obtain .
\end{remark}




\subsection{{\ours}: A Multimodal Analogical Reasoning Framework with Transformer}

\begin{wrapfigure}{L}{0.44\textwidth}
\centering \includegraphics[width=0.44\textwidth]{figs/mart.pdf}
\caption{The {\ours} framework.}
\label{fig:mart}
\vspace{-0.2cm}
\end{wrapfigure}

Although the approach above-mentioned can enable multimodal pre-trained Transformer models to multimodal analogical reasoning, they only superficially consider implicit \textit{Abduction} and  \textit{Induction}, ignoring the fine-grained associations between the analogy example and analogy question-answer pair.


\textbf{Adaptive Interaction Across Analogy.}
Since the analogy question may interfere with the representation of the analogy example and the inevitable noisy data issue, we propose adaptive interaction across analogy in encoding process to interact between the analogy example and question-answer pair adaptively, as shown in Figure~\ref{fig:mart}.
Denote the input to a Transformer layer as , where  and  
 denote the hidden representation of analogy example  and question-answer pair  respectively. 
In each attention head of layer, the query and key representation can be formalized as:
\begin{small}

\end{small}where  are project matrices. 
A similar expression also holds for values .
Then the attention probability matrix  can be defined in terms of four sub-matrices:
\begin{small}

\end{small}where ,  (diagonal of ) are intra-analogy attentions and ,  (anti-diagonal of ) are inter-analogy attentions. We use the gate  to regulate the inter-analogy interactions adaptively:
\begin{small}

\end{small}where  is adaptive association gate which has two learnable variables .
\begin{remark}
On the one hand, the query from  may interfere with the example from .
On the other hand,  may have a weaker impact on  in \textcolor{black}{noisy} data. 
Adaptive association gates can increase and decrease inter-analogy interaction automatically based on the intimacy of  and .
\end{remark}

\textbf{Relation-Oriented Structure Mapping.}
The structure mapping theory \textcolor{black}{emphasizes the relation transfer rather than object similarity in analogical reasoning, it is} \textit{relations between objects, rather than attributes of objects, are mopped from base to target}.  For example, \textit{battery} can make an analogy to \textit{reservoir} because they both store potential, rather than their shapes being cylindrical. 
Motivated by this, we propose the \textbf{relaxation loss} to bring the relations closer and alienate the entities:
where  is the total number of the training set ,  is the hidden feature of  in analogy example  output from the MLM head,  is the cosine similarity.
We leverage the masked entity prediction task to obtain the answer entity  with a cross-entropy loss: 

Afterwards, we interpolate the relaxation loss  and the masked entity prediction loss  using parameter  to produce the final loss :

\begin{remark}
The relaxation loss is composed of pull-in and pull-away that correspond to the close relation and alienate entity terms, respectively, which can constrain the model's focus on relation structure transfer and implicitly realize the Structure Mapping process. 
\end{remark}


\begin{table*}[!t]
\small
\centering
\scalebox{0.8}{
\begin{tabular}{c|lcccccc}
\toprule
     Method & Baselines & Backbone & Hits@1 & Hits@3 & Hits@5 & Hits@10 & MRR \\
\midrule
   \multirow{6}{*}{MKGE} & IKRL  & TransE & 0.254 & 0.285 & 0.290 & 0.304 & 0.274  \\
    & TransAE  & TransE  & 0.203 & 0.233 & 0.241 & 0.253 & 0.223 \\
    & RSME  & ComplEx  & 0.255 & 0.274 & 0.282 & 0.291 & 0.268 \\
\cmidrule{2-8}
    & IKRL  & ANALOGY & 0.266 & 0.294 & 0.301 & 0.310 & 0.283  \\
    & TransAE & ANALOGY & 0.261 & 0.285 & 0.289 & 0.293 & 0.276 \\
    & RSME & ANALOGY & 0.266 & 0.298 & 0.307 & 0.311 & 0.285  \\
\midrule
\specialrule{0em}{1.0pt}{1.0pt}
\midrule
    \multirow{10}{*}{MPT} & 
     VisualBERT & Single-Stream  & 0.247 & 0.281 & 0.289 & 0.303 & 0.269\\
        
    & ViLT  & Single-Stream & 0.235 & 0.266 & 0.274 & 0.286 & 0.257 \\
        
    & ViLBERT  & Dual-Stream & 0.252 & 0.308 & 0.320 & 0.338 & 0.287 \\
       
    & FLAVA  & Mixed-Stream & 0.257 & 0.299 & 0.312 & 0.325 & 0.284 \\
        
    & MKGformer & Mixed-Stream  &  0.293 & 0.335  & 0.344 & 0.367 & 0.321 \\
\cmidrule{2-8}
    & {\ours}\_VisualBERT & Single-Stream & 0.261  & 0.292 & 0.308 & 0.321  & 0.284  \\
    & {\ours}\_ViLT &Single-Stream&0.245 & 0.275 & 0.287 & 0.303  & 0.266   \\
    & {\ours}\_ViLBERT &  Dual-Stream& 0.256  & 0.312 & 0.327  & 0.347  & 0.292 \\
    & {\ours}\_FLAVA & Mixed-Stream & 0.264 & 0.303 & 0.309 & 0.319 & 0.288  \\
    & {\ours}\_MKGformer & Mixed-Stream & \textbf{0.301}  & \textbf{0.367} & \textbf{0.380} & \textbf{0.408} & \textbf{0.341} \\
    
\bottomrule
\end{tabular}
 }
\caption{
The main performance results on {\data}. 
We report pipeline baselines with multimodal knowledge graph embedding (MKGE) methods and replace their backbone models with analogy-aware model ANALOGY.
We also utilize our {\ours} on end-to-end baselines with multimodal pre-trained Transformer (MPT) methods and obtain the best performance in {\ours}\_MKGformer.
}
    \label{tab:main_res}

\end{table*}

\section{Results and Analysis} 
\label{sec:experiments}


\subsection{Main Results}

The main performance results of all benchmark methods can be seen in Table \ref{tab:main_res}. 
In general, we find the performance of multimodal knowledge graph embedding (MKGE) baselines and multimodal pre-trained Transformer (MPT) baselines is comparable except MKGformer, which establishes a competitive baseline of {\data}.
In addition, when replacing the backbone of MKGE methods with ANALOGY that models analogical structure explicitly, the performance is significantly improved. 
Meanwhile, the MPT models without analogy-related structures obtain substantial performance with the analogical reasoning ability enhanced by {\ours}.
For example, although MKGformer achieves outstanding performance, {\ours}\_MKGformer further improves and obtains state-of-the-art performance, exceeding other methods by 4.9\%-12.4\% points in the MRR metric.
It reveals that 
 the
{\ours} framework stimulates the ability of the Transformer-based model for multimodal analogical reasoning.
We also report the pre-training results on MarKG in Appendix~\ref{adx:pre_train}.

\subsection{Generalize to Novel Relation}
\begin{wraptable}{r}{0.50\textwidth}
\small
\centering
\scalebox{0.8}{
\begin{tabular}{lcccc}
\toprule
    \multirow{2}{*}{Model} & \multicolumn{4}{c}{Novel Relation Transfer} \\
    \cmidrule{2-5}
    & Hits@1 & Hits@3 & Hits@10 & MRR \\
\midrule
    {\ours}\_MKGformer & 0.254 & 0.285 & 0.292 & 0.273 \\
    \quad w/o {\kg} & 0.217 & 0.228 & 0.231 & 0.224 \\
    \quad w/ Full {\data} & 0.365 & 0.419 & 0.433 & 0.395 \\
\bottomrule
\end{tabular}
}
\caption{
Results of MKGformer on novel relation generalization. 
``w/ Full MARS'' is the result trained with full data (upper bound). 
}
    \label{tab:transfer}
\end{wraptable}
Making analogies from one domain to another novel domain is a fundamental ingredient for human creativity. 
In this section, we conduct a novel relation transfer experiment (including both task settings) to measure how well the models generalize by analogy to unfamiliar relations. 
Specifically, we randomly split the 27 analogy relations into the source and target relations.
The models are then trained on the source and tested on the novel target relations. As shown in Table~\ref{tab:transfer}, we observe that {\ours}\_MKGformer can indeed learn to make sense of unfamiliar relations, respectively. 
We further evaluate the model without pre-training on {\kg} and find the performance decreased, which indicates that the structure knowledge provided by {\kg} is critical for generalization.
Note that the novel relation transfer setting is somewhat similar to zero-shot or domain generalization, and we hope our work can benefit other communities.

\subsection{Ablation Study}

\begin{table*}[!t]
\small
\centering
\scalebox{0.85}{
\begin{tabular}{lccccc}
\toprule
    Model & Hits@1 & Hits@3 & Hits@5 & Hits@10 & MRR \\
\cmidrule{1-6}
    TransAE & 0.203 & 0.233 & 0.241 & 0.253 & 0.223 \\
    \quad w/o {\kg} & 0.191 & 0.224 & 0.235 & 0.245 & 0.214 \\
    {\ours}\_ViLBERT & 0.256  & 0.312 & 0.327  & 0.347  & 0.292 \\
    \quad w/o {\kg} & 0.253 & 0.292 & 0.297 & 0.310 & 0.270 \\
    \textcolor{black}{\quad w/o Analogy example} & \textcolor{black}{0.113} & \textcolor{black}{0.143} & \textcolor{black}{0.162} & \textcolor{black}{0.179} & \textcolor{black}{0.138} \\
\midrule
    {\ours}\_MKGformer & \textbf{0.301} & \textbf{0.367} & \textbf{0.380} & \textbf{0.408} & \textbf{0.341} \\
    \quad w/o {\kg} & 0.270 & 0.305 & 0.309 & 0.315 & 0.289 \\
    \quad w/o Relaxation loss & 0.295 & 0.349 & 0.373 & 0.399 & 0.332  \\
    \quad w/o Adaptive interaction & 0.285 & 0.345 & 0.365 & 0.395 & 0.324 \\
    \quad w/o MarT & 0.293 & 0.335 & 0.344 & 0.367 & 0.321 \\
    \textcolor{black}{\quad w/o Analogy example} & \textcolor{black}{0.101} & \textcolor{black}{0.123} & \textcolor{black}{0.132} & \textcolor{black}{0.149} & \textcolor{black}{0.120} \\
\bottomrule
\end{tabular}
}
\caption{
Ablation experiments on {\data}.w/o {\kg} refers to the model without pre-training on {\kg} dataset. 
w/o {\ours} refers to ablate all components of {\ours} that equivalents to MKGformer. 
}
    \label{tab:ablation}

\end{table*}

To validate the effectiveness of {\kg} and  {\ours}, we conduct an ablation study as shown in Table~\ref{tab:ablation}. 
We observe that discarding pre-train on {\kg} results in worse performance for both MKGE  and MPT baselines. 
It indicates that the knowledge structure information provided by {\kg} helps learn the representation of entities and relations, further benefiting analogical reasoning. 
We also find that the performance clearly drops when ablating each component of {\ours} and reaches the valley when ablating all, proving the effectiveness of each analogical component of our {\ours}. 
\textcolor{black}{
Moreover, we ablate the analogy example in the input and find the performance drops a lot, which reveals the importance of analogical prompts.
}




\subsection{Analysis}

\begin{wrapfigure}{L}{0.4\textwidth}
\centering \includegraphics[width=0.4\textwidth]{figs/res_task.pdf}
\caption{Performance on {\data} in different sub-task settings.}
\label{fig:analysis_task}
\vspace{-0.1cm}
\end{wrapfigure}
\textbf{Analysis across Different Sub-Tasks.}
In previous Table~\ref{tab:main_res}, we are amazed by ANALOGY significantly improving the performance of MKGE baselines.
Therefore, we further compare the performance of vanilla baselines to the addition of analogical components in different sub-task settings. 
As shown in Figure~\ref{fig:analysis_task}, we observe that vanilla TransAE performs poorly in the blended task setting. 
However, when replacing the backbone TransE with ANALOGY, TransAE is competent in blended analogical reasoning setting and even outperforms the single setting. 
On the other side, RSME with ComplEx as backbone can handle the blended setting reluctantly but perform worse than the single setting.
ANALOGY improves the performance of RSME in this situation. 
Meanwhile, {\ours} further explores the potential of MKGformer and improves its performance in various tasks. 
All in all, the analogical components consistently improve the multimodal analogical reasoning ability of all baseline methods,  especially in blended analogical reasoning, which \textcolor{black}{supports} Mayer's theory~\citep{mayer2002multimedia} that \textbf{analogical reasoning is more affinity for multimodal scenarios}.

 
\begin{figure}[!t]
\centering
\includegraphics[scale=1]{figs/case.pdf}
\caption{
Case examples of {\data}. We show the analogy example and analogy question-answer pair with their implicit relations.
``Top-3 Entity'' means top-3 ranking entities in the prediction.
``Gold Rank'' refers to the rank of the gold answer entity in the prediction. * denotes the baseline model with analogical components ({\ours} or ANALOGY).}
\label{fig:case}
\end{figure}


\textbf{Case Analysis.}
As shown in Figure~\ref{fig:case}, we provide case analysis and observe that the top ranking entities (film, life, etc.) of the baselines without analogical components are usually irrelevant to the question entity ``campaign''\footnote{A Huggingface Demo at \url{https://huggingface.co/spaces/zjunlp/MKG_Analogy}.}.
Analogical components make the predictions more reasonable and successfully predict the answer entity ``battle''.
In the difficult blended analogical reasoning setting, the blended modal input of visual and text is challenging. We find that vanilla MKGformer and TransAE fail to understand the visual semantic of ``apple'' and incorrectly linked with ``capital, phone, shipping'' that related to ``Apple Company''. 
We also notice that TransAE with ANALOGY as backbone significantly decreases the prediction error but incorrectly predicts ``plant'' as the top-1 entity due to the interference of ``Panax notoginseng''.
On the contrary, {\ours}\_MKGformer with relaxation loss can alienate the entities and focus on relation structures transfer and obtain reasonable predictions. 
These observations reveal that multimodal analogical reasoning is a highly challenging task, and analogy-aware components could enhance the analogical ability of models. 
Besides, we discuss \textbf{limitations} in Appendix \ref{adx:limitation} and provide a comprehensive error analysis in Appendix~\ref{adx:error_case}. 




\section{Discussion and Conclusion} 

In this work, we introduce the new task of multimodal analogical reasoning over knowledge graphs.Preliminary experiments show that this task brings a rather difficult challenge and is worth further exploration. 
Besides evaluating the analogical reasoning ability of models, there are some potential applications to explore: 
(1) knowledge graph completion with analogies,
(2) transfer learning and zero-shot learning by analogy and 
(3) analogical question answering. 
We hope our work inspires future research on analogical reasoning and applications, especially in the multimodal world.


\section*{Reproducibility Statement}



The source {\data} and  {\kg} datasets will be released on Github soon. 
In order to provide support to reproduce our experiments in Section~\ref{sec:experiments}, we provide the detailed source code of all pipeline baselines (IKRL, TransAE, RSME) and end-to-end baselines (VisualBERT, ViLBERT, ViLT, FLAVA, MKGformer) in the supplementary materials with all scripts and hyper-parameters. 
We also provide a README script to instruct how to run the codes. 

\section*{Acknowledgment}

We would like to express gratitude to the anonymous reviewers for their kind comments. 
This work was supported by the National Natural Science Foundation of China (No.62206246 and U19B2027), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G), CAAI-Huawei MindSpore Open Fund, and NUS-NCS Joint Laboratory (A-0008542-00-00).



\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

 
\appendix


\section{Limitations}
\label{adx:limitation}
The proposed work still has some limitations. 
We try to simulate the real-world multimodal analogy reasoning setting; however, it still can not predict analogical entities that are \textbf{not existing} in the multimodal knowledge graph.
Such an issue is also known as inductive knowledge graph completion, and we leave this for future works.
Besides, we have not evaluated the very large-scale pre-trained models on the {\data} due to the GPU resources, and it is well worth investigating whether large-scale pre-trained models can emerge the multimodal analogy reasoning ability.


\section{Additional Datasets Information}
\label{adx:dataset}

\subsection{Dataset Construction}
\label{adx:construction}

\begin{figure}[!htp]
\centering
\includegraphics[scale=0.65]{figs/distribution_rel.pdf}
\caption{Relation distribution of {\data}.}
\label{fig:adx_dis_rel}
\end{figure}

\textbf{Step 1: Collect Analogy Entities and Relations.} 
Since E-KAR and BATs are widely used text analogy datasets with high-quality and semantically specific entities, we collect the analogy seed entities  and relations from them according to the following criteria: 
\textcolor{black}{
(1) The relations and entities that have the same meanings will be merged. For example, we merge the relation \textit{is\_a} of E-KAR and the relation \textit{Hypernyms} of BATs since they both represent the hypernym relationship     of entities. We obtain 38 relations after this step.}
(2) The relation must imply analogical knowledge reasoning rather than simple word linear analogy. \textcolor{black}{For example, we discard the analogy relations that only reflect simple word changes of BATs dataset such as \textit{Inflections (Nouns, Verbs, etc.)} and \textit{Derivation (Stem change, etc.)}. After this step, we filter 11 relations and retain 27 analogy relations.}

(3) The entity must be visualizable and realistic. \textcolor{black}{We filter those entities that cannot be linked into Wikidata and drop out the extremely abstract entities such as \textit{virtue} by hand (some entities that have no image after Step 3 are also filtered). We discard a total of 463 entities after filtering. Finally}, we obtained 2,063 seed entities and 27 relations.


\textbf{Step 2: Link to Wikidata and Retrieve Neighbors.} 
Consider that complex analogical reasoning is difficult through individual information (descriptions or images) of entities. 
We link the analogy seed entities to Wikidata by Mediawiki API \footnote{\url{https://www.wikidata.org/w/api.php}} and retrieve the one-hop neighbors of seed entities as well as the possible relationships between the seed entities to obtain their neighbor structure information. \textcolor{black}{In this step, we also take the retrieved descriptions from Wikidata as the textual information of entities and relations.
}

\textbf{Step 3: Acquire and Validate Images.}
We collect images from two sources: Google Engine and Laion-5B query service\footnote{\url{https://knn5.laion.ai/}}. We search from Google Engine with the descriptions of entities and crawl 5 images per entity. 
Laion-5B service depends on Clip retrieval and query by \textit{k}nn index; we leverage the clip text embedding of the description and also query 5 images for each entity. 
Then we apply four filters to the above images: 
(1) we check the format of the images and filter invalid files, 
(2) we remove corrupted \textcolor{black}{(the images are damaged and cannot be opened)}, low-quality \textcolor{black}{(image size less than  or non-panchromatic images)} and duplicate images, 
(3) we use CLIP~\citep{CLIP} to remove the images with outlier visual embeddings,
(4) we delete unreasonable images manually. 


\textbf{Step 4: Sample Analogical Reasoning Data.}
From Step 1 to Step 3, we obtain the {\kg},
which includes 2,063 analogy entities, 8,881 neighbor entities, 27 analogy relations and 165 other relations. 
To construct the {\data} dataset, we sample analogy example  and analogy question-answer pair  with the same relation  from 2,063 analogy entities, but we do not explicitly provide the relation in the input.
Then we split the data into different task settings evenly.
More details about the sample strategy of {\data} can be seen in Section~\ref{adx:sample}.

\subsection{Sample Strategy of {\data}}
\label{adx:sample}

In Section~\ref{adx:construction}, we obtain the analogy seed entities  and the analogy relations between . Then we sample analogy example  and analogy question-answer pair  from . Guided by SMT, we make sure that  and  have the same relation . Specifically, we divide the entity pairs that share the same relation into two categories to avoid overlap issues. Then we randomly sample the analogy examples from one category and the analogy question-answer pairs from another to construct analogy input instances. 
Last, we split the instances into different task settings evenly.



\subsection{Dataset Details}
\label{sec:adx_dataset_static}

\begin{table*}[!htp]
\small
\centering
\begin{tabular}{cccccc}
\toprule
  & \# entity & \# relation & \# triple & \# image & source \\
\cmidrule{1-6}
WN9-IMG & 6,555 & 9 & 14,319 & 65,550 & WordNet \\
FB15k-IMG & 11,757 & 1,231 & 350,293 & 107,570 & Freebase \\
{\kg} & 11,292 & 192 & 34,420 & 76,424 & Wikidata \\
\bottomrule
\end{tabular}
\caption{Data statistics of {\kg}. \# refers to the number of.}
\label{tab:adx_static_markg}
\end{table*}


The statistical comparison of {\kg} with two multimodal knowledge graph datasets WN9-IMG~\citep{IKRL} and FB15k-IMG~\citep{MMKG} as shown in Table~\ref{tab:adx_static_markg}, we report the number of entity, relation, triple, image and the data source. Note that WN9-IMG and FB15k-IMG aim for knowledge completion and triple classification tasks while our {\kg} aims to support {\data} to do multimodal analogical reasoning. We also show the complete relations of our {\data} in Table~\ref{tab:adx_complete_relation} and the distribution of relation categories in Figure~\ref{fig:adx_dis_rel}.

\begin{table*}[!htp]
\centering
\small
\scalebox{0.85}{
\begin{tabular}{lp{9.0cm}l}
\toprule
    \textbf{Relations} & \textbf{Definition} & \textbf{Example} \\
\cmidrule{1-3}

part of & Object of which the subject is a part.  & mouse : computer \\
corresponds to & Terms generally correspond to each other. & entrepreneur : laborer \\
juxtaposition to & Two terms belong to the same hypernym or have the same properties or functions. & child : minor \\
synonym  & Sense of another lexeme with the same meaning as this sense. & tired : exhausted \\
made from material & Material the subject or the object is made of or derived from. & building : cement \\
antonym & Sense of a lexeme with the opposite meaning to this sense. & warm : cool \\
has cause & Underlying cause, thing that ultimately resulted in this effect. & cleaning : tidy \\
opposite of & Item that is the opposite of this item. & black : white \\
follow & The terms have a chronological or other sequential relationship, but one term does not cause the other. & implement : evaluate \\
intersection to & The extension of the two terms intersects. & odd : integer \\
takes place in & A term takes place in the other. & doctor : hospital \\
prerequisite & Prior event or achievement that a person or team needs to complete before joining or obtaining the item topic. & aim : shoot \\
subject-object & The originator and receiver of an action.  & school : education \\
contradictory to & Two term are contradictory to each other. & english : chinese \\
identical to & The meanings of two terms are identical. & highway : road \\
head-modifier & The preceding term modifies the other.  & affluence : living \\
different from & Item that is different from another item, with which it may be confused. & apple : nuts \\
probabilistic attribute & One term is probably the attribute of the other.  & liquid : fluidity \\
instance of & That class of which this subject is a particular example and member. & coffee : drink \\
has use & Main use of the subject.  & ballot : election \\
location & Location of the object, structure or event. & student : classroom \\
verb-object & The action and the object on which the action acts. & drilling : petroleum \\
has quality & The entity has an inherent or distinguishing non-material characteristic. & knife : sharp \\
tool of & One term is the tool of the other.  & piano : play \\
subject-predicate & The originator of the action and the action itself.  & stone : throwing \\
target of & One term is the target of the other. & harvest : sow \\
metaphor & A term is the metaphor of the other, reflecting something abstract indirectly. & pigeon : peace \\

\bottomrule
\end{tabular}
}
\caption{The complete relations with definitions, examples of {\data}. Some relations and definitions refer to~\citep{E-KAR} and Wikidata Properties.}
\label{tab:adx_complete_relation}
\end{table*}


\textbf{Quality Control of Datasets.}
We devise some quality control strategies while construct our {\kg} and {\data} datasets: 
(1) Entity and relation formalization and normalization. We link the analogy entities collected from E-KAR and SAT to Wikidata and filter non-link items.
Since Wikidata is a knowledge base with quality-assured, some rare or worthless entities are excluded.
(2) Image validation mechanism. We devise complex image filter strategies to control the robustness of image data, as mentioned in Section~\ref{adx:construction}.
(3) Control of text description. We take the description in Wikidata as the textual information of entities.


\begin{table*}[!htp]\color{black}
\small
\centering
\begin{tabular}{ccccc}
\toprule
  Method & Hit@1 & Hit@3 & Hit@5 & Accuracy \\
\cmidrule{1-5}
    TransAE & 0.15 & 0.37 & 0.50 & - \\
    {\ours}\_VisualBERT & 0.15 & 0.28 & 0.53 & - \\
    {\ours}\_MKGformer & 0.16 & 0.36 & 0.59 & - \\
    Human & - & - & - & 0.64 \\
\bottomrule
\end{tabular}
\caption{\textcolor{black}{Human evaluation on {\data}.}}
\label{tab:adx_human_eva}
\end{table*}
\textcolor{black}{
\textbf{Human evaluation on \data.}
To evaluate the complexity and difficulty of the multimodal analogical reasoning task, we build a human evaluation in this section. However, humans encounter the following problems in this entity prediction task: (1) The candidate entity set is too huge for humans to select one entity. (2) Hit@k metric is not available since human hard rank predictions. Therefore, we utilize the multiple-choice format for human beings and apply the Accuracy metric to evaluate. Specifically, we randomly sample 100 instances from the test set to construct the evaluation set, and we use the top 10 ranking entities in TransAE prediction as candidate choices for each instance. If the golden answer entity is not in the top 10 entities, we will randomly replace one candidate with the golden entity. Then humans must select one entity from the candidate choices as the answer entity. The results can be seen in Table~\ref{tab:adx_human_eva}. We limit the prediction space of baseline models in candidate choices for a fair comparison. We find that the performance of the baselines in the Hit@1 metric  has a large gap with human, which indicates the difficulty of the multimodal analogical reasoning task. 
}


\subsection{Detailed Evaluation Metrics}
\label{adx:metric}

The evaluation method of \citep{E-KAR} can not reflect one-to-more entities and does not fully explore the internal knowledge in the models due to the limited search space. 
Thus, we follow the link prediction task and choose Hits@k and MRR as our evaluation metrics. Both metrics are in the range . The bigger, the better performance.
The Hits at k metric (Hits@k) is acquired by counting the number of times the golden entity appears at the first k positions in the predictions. 

Given the prediction score of each entity in the candidate entity set, we sort the score and obtain the ranking of each entity. Denote the rank of the gold entity of  triple as , and the reciprocal rank is . The Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks across all triples in the knowledge graph:

where  is the total number of the training set.


\section{Additional of Experiments}
\label{adx:experiment}

\subsection{Implementation Details}

\begin{table*}[!htbp]
\small
    \centering
    \begin{tabular}{ccc}
\toprule
      Hyper-parameters & MKGE Baselines  & MPT Baselines \\
\midrule
    epoch & \{300, 1000\} & 15 \\
   sequence length & - &  128 \\
   learning rate & \{1e-2, 5e-3\} & \{3e-5, 4e-5, 5e-5\} \\
   batch size & 1000 & 64 \\
   optimizer & \{Adagrad, SGD\} & AdamW \\
   adam epsilon & - & 1e-8 \\
    & - & \{0.38, 0.43, 0.45\} \\
\bottomrule
    \end{tabular}
\caption{Hyper-parameter settings. We use the same parameter settings of MKGE baseline methods as the original paper except for the learning rate. }
\label{tab:adx_hyper_param}
\end{table*}

This section detail the training procedures and hyper-parameters for various models.
For multimodal knowledge representation methods, we first use {\kg} to do knowledge representation learning and obtain the entity and relation matrix embeddings.
Then we apply abduction and induction processes to continue training the models on the {\data} dataset. Note that these processes are serial and share models.  
For multimodal pre-trained Transformer models, we also use {\kg} to pre-train the models and then fine-tune on {\data} end-to-end with our analogy prompt tuning strategy. 
We utilize Pytorch to conduct all experiments with 1 Nvidia 3090 GPU. 
The details of hyper-parameters can be seen in Table~\ref{tab:adx_hyper_param}.

\subsection{Results of Pre-training on MarKG.}
\label{adx:pre_train}

\begin{table*}[!htp]
\centering
\small
\scalebox{0.8}{
\begin{tabular}{l|lcccccccc}
\toprule
\multirow{2}{*}{Method} &
    \multirow{2}{*}{Baselines} & \multicolumn{4}{c}{\textbf{Entity Prediction}} & \multicolumn{4}{c}{\textbf{Relation Prediction}}\\
\cmidrule{3-10}
    & & Hits@1 & Hits@3 & Hits@10 & MRR & Hits@1 & Hits@3 & Hits@10 & MRR \\
\cmidrule{1-10}
    \multirow{3}{*}{MKGE}
    & IKRL   & 0.157 & 0.257 & 0.338 & 0.272 & - & - & - & -\\
    & TransAE & 0.307 & 0.361 & 0.442 & 0.353 & - & - & - & - \\
    & RSME  & 0.417 & 0.460 & 0.520 & 0.452 & - & - & - & - \\
\cmidrule{1-10}
    \multirow{5}{*}{MPT}
    & {\ours}\_VisualBERT & 0.466 & 0.598 & 0.692 & 0.546 & 0.758 & 0.873 & 0.927 & 0.822 \\
    & {\ours}\_ViLT  & 0.466 & 0.586 & 0.675 & 0.539 & 0.737 & 0.847 & 0.902 & 0.799 \\
    & {\ours}\_ViLBERT  & 0.489 & 0.621 & 0.711 & 0.569 & 0.764 & 0.876 & \textbf{0.930} & 0.827 \\
    & {\ours}\_FLAVA  & 0.506 & 0.634 & 0.716 & 0.582 & \textbf{0.771} & \textbf{0.877} & 0.921 & \textbf{0.829} \\
\cmidrule{2-10}
    & {\ours}\_MKGformer & \textbf{0.527} & \textbf{0.670} & \textbf{0.779} & \textbf{0.616} & 0.762 & 0.870 & 0.923 & 0.823 \\
\bottomrule
\end{tabular}
}
\caption{
Pre-training results on {\kg}.
Note that these results are from the training process as we do not divide {\kg}. 
Since we follow the link prediction task to pre-train the models for MKGE baselines, we only report the entity prediction results.}
\label{tab:adx_pretrain_res}

\end{table*}

\begin{figure}[!htp]
\centering
\includegraphics[scale=0.65]{figs/ptft.pdf}
\caption{The results of pre-training on {\kg} and fine-tuning on {\data}. * refers to the baseline model applied {\ours}.}
\label{fig:adx_pt_ft}
\end{figure}

We report the pre-train results on {\kg} in Table~\ref{tab:adx_pretrain_res}. We find that MPT baselines perform better than MKGE baselines consistently. 
It reveals the strong fit ability of Transformer-based models. 
As shown in Figure~\ref{fig:adx_pt_ft}, we can observe that pre-training and fine-tuning stages trends are roughly the same, especially in the same type of baselines, which indicates that pre-train on {\kg} benefits analogical reasoning on {\data}.




\subsection{\textcolor{black}{Results of Implicit Relation Inference of MPT.}}

\begin{table*}[!htp]\color{black}
\centering
\scalebox{0.8}{
\begin{tabular}{l|lccc|c}
\toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Baselines} & \multicolumn{3}{c}{Relation Prediction} & \multirow{2}{*}{Distance} \\
\cmidrule{3-5}
    & & Hits@3 & Hits@5 & Hits@10 &   \\
\cmidrule{1-6}
    \multirow{2}{*}{MKGE} & IKRL & 0.160 & 0.234 & 0.405 & -  \\
    & TransAE & \textbf{0.179} & 0.242 & 0.491 & -  \\
\cmidrule{1-6}
    \multirow{5}{*}{MPT}
    & {\ours}\_VisualBERT & 0.107 & 0.181 & 0.340 & 1.418 \\
    & {\ours}\_ViLT & 0.126 & 0.181 & 0.332 & 1.419   \\
    & {\ours}\_ViLBERT & 0.078 & 0.189 & 0.333 & 1.412 \\
    & {\ours}\_FLAVA & 0.078 & \textbf{0.587} & \textbf{0.709} & \textbf{1.380} \\
    & {\ours}\_MKGformer & 0.049 & 0.209 & 0.512 & 1.405 \\
\bottomrule
\end{tabular}
}
\caption{
\textcolor{black}{Relation evaluation of MPT baselines.}}
\label{tab:adx_mpt_relation_Res}
\end{table*}

\textcolor{black}{We conduct an evaluation experiment on the relation inference of MKGE and MPT methods. For MKGE methods, we evaluate the relation predicted of \textit{Abuduction} process with hit@k metrics. Since MPT methods solve the analogical reasoning task end-to-end without any explicit relation prediction process, we use two ways to evaluate their relation-aware abilities. The first one is that we predict the relation via the special relation token \texttt{[R]}, which is similar to mask entities prediction and evaluate the predictions with Hit@k metrics. However, this evaluation method does not precisely reflect the relation-aware abilities of models since \texttt{[R]} is an abstract virtual token that may aggregate multiple relation information. Therefore, we devise the second method that computes the Euclidean distance as follows: }

\textcolor{black}{
where  is the total number of the test set,  is the hidden state of \texttt{[R]} in the last transformer layer,  is the special relation embedding (described in Section~\ref{sec:pre-train}) of the golden relation .  is the  of vectors,  is the Euclidean distance function.}

\textcolor{black}{
The evaluation results are shown in Table~\ref{tab:adx_complete_relation}, we find that MKGE methods perform better than most MPT methods on Hit@k metrics, especially on Hit@3 metric, which may benefit from the explicit relation perception in the pipeline process. Moreover, {\ours}\_FLAVA achieves the best relation-aware performance on Hit@k and Euclidean distance metrics, but {\ours}\_FLAVA performs worse than {\ours}\_MKGformer in answer entity prediction as shown in Table~\ref{tab:main_res}. We speculate that the special token \texttt{[R]} contains not only the golden relation but also other related relation information.
}














\subsection{\textcolor{black}{Comparison of Performance and Model Size}}

\begin{figure}[!htp]
\centering
\includegraphics[scale=0.75]{figs/scaling.pdf}
\caption{\textcolor{black}{Comparison of performance and model size of MPT baselines.}}
\label{fig:adx_sclaing}
\end{figure}

\textcolor{black}{In this section, we detail the size of MPT baseline models and compare them with their performance. In MPT models, the single-stream models (VisualBERT, ViLT) are the smallest, the dual-stream models (ViLBERT) are the middle and the mixed-stream models (FLAVA, MKGformer) are the biggest. The performance of models is roughly proportional to their sizes, as shown in Figure~\ref{fig:adx_sclaing}. MKGformer outperforms all other models, including the biggest FLAVA model.}


\section{Error Case Analysis}
\label{adx:error_case}

\begin{figure}[!t]
\centering
\includegraphics[scale=1.1]{figs/error_case.pdf}
\caption{Error case examples.}
\label{fig:adx_error_case}
\end{figure}

In this section, we conduct an error case study on {\data} in Figure~\ref{fig:adx_error_case}. From the error cases, we can see the hardship of the multimodal analogical reasoning task: 
1) \textbf{Imbalance of multimodal.} The semantic scales of images and text are inconsistent, which leads to incorrect matching~\citep{DBLP:journals/corr/abs-2202-05786}. Although we filter some hard-to-visualize entities in data collection in Section~\ref{adx:construction}, the high semantic entities exist. As shown in example (a), ``management''  and ``control'' are abstract entities that are difficult to find equivalent images. Moreover, the uncoordinated convergence problem in multimodal learning further exacerbates the difficulty of the multimodal analogical reasoning task~\citep{DBLP:journals/corr/abs-2203-15332, DBLP:conf/cvpr/WangTF20}.
2) \textbf{One-to-more problem.} It is challenging for the models to solve one-to-more entities. In example (b), ``Memba’’ is an instance of both ``snake’’ and ``animal’', which is confusing to MKGformer.
3) \textbf{Unintuitive relations.} In our {\data} dataset, some relations are not intuitive, requiring models to have strong relation reasoning ability. As shown in example (c), the relation ``intersection to’’  means the extension of the head and tail entity intersects. All four models are struggling and far away from the golden answer entity.




\end{document}
