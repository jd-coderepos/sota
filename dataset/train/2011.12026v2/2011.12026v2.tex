\documentclass[rebuttal]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bm}



\usepackage[
pagebackref=true,
breaklinks=true,
colorlinks,
bookmarks=false,
urlcolor=blue]{hyperref}

\def\cvprPaperID{8135} 

\def\cvprPaperID{8135} \def\confYear{CVPR 2021}

\usepackage{bm}
\usepackage{textcomp} 

\usepackage{multirow}
\usepackage{subcaption}
\usepackage{amssymb} \usepackage{pifont} \usepackage{makecell} 

\usepackage{xcolor}
\definecolor{azure(colorwheel)}{rgb}{0.0, 0.5, 1.0}

\newcommand{\D}{\mathsf{D}}
\newcommand{\G}{\mathsf{G}}
\newcommand{\F}{\mathsf{F}_{\bm\theta}}
\newcommand{\ffhq}{FFHQ 10241024}
\newcommand{\ours}{\textcolor{azure(colorwheel)}{(ours)}}


\newcommand{\todo}[1]{\textcolor{blue}{TODO: \textit{#1}}}
\newcommand{\question}[1]{\textcolor{red}{Question: \textit{#1}}}
\newcommand{\rephrase}[1]{\textcolor{red}{\textit{#1} [rephrase]}}

\newcommand{\R}{\mathbb{R}} 



\newcommand{\ReviewerA}{\textcolor{cyan}{}}
\newcommand{\ReviewerB}{\textcolor{magenta}{}}
\newcommand{\ReviewerC}{\textcolor{violet}{}}
\newcommand{\ReviewerAsays}[1]{\textbf{\ReviewerA: \textbf{#1}}}
\newcommand{\ReviewerBsays}[1]{\textbf{\ReviewerB: \textbf{#1}}}
\newcommand{\ReviewerCsays}[1]{\textbf{\ReviewerC: \textbf{#1}}}

\newcommand{\mohamed}[1]{{\textcolor{red}{mohamed: #1}}}
\newcommand{\ivan}[2]{\textcolor{red}{#1} (\textit{#2})}

\captionsetup{belowskip=-0.4cm, aboveskip=0.1cm}


\begin{document}



\thispagestyle{empty}

We thank all the reviewers for their valuable response. We are glad they found our experiments to be impressive (\ReviewerB) and well thought (\ReviewerC), the writing to be clear (\ReviewerA, \ReviewerC), and the ideas interesting (\ReviewerB).
Below, we address the reviewers' concerns and will incorporate all the feedback.

\ReviewerAsays{For 2D, the pixel representations are very efficient.}
Not for generation:
Table 1 in the paper and Table~\ref{rebuttal:table:improved-results} here show that an INR-based generator has  times fewer MACs and larger img/sec throughput. \ReviewerAsays{...and conv-based decoders achieve much better image quality}.
Not for too long!
We developed a ``vanilla'' INR-based decoder \textit{without} any spatial operations like convolutions/upsampling/pooling/etc \textit{and} any training tricks/heavy compute.
But INR-based models can deliver SotA when sufficiently tuned: we \textit{outperformed} SG2 on Churches and came close on Bedrooms (see Table~\ref{table:improved-results})
by incorporating bilinear upsampling in , SG2's equalized LR [\href{https://arxiv.org/abs/1710.10196}{Sec. 4.1 of 34}; \href{https://arxiv.org/abs/1812.04948}{35}; \href{https://arxiv.org/abs/1912.04958}{36}], SG2's noise injection [\href{https://arxiv.org/abs/1812.04948}{Sec. 2 of 35}; \href{https://arxiv.org/abs/1912.04958}{36}], and SG2-sized discriminator [``config-f'' of \href{https://arxiv.org/abs/1912.04958}{36}].
The goal of our paper is to bring INRs into the game and explore their properties in 2D and \textit{not} to set a new GAN SotA, which typically requires a lot of GPUs: for example, the SG2 project took 51 V100 GPU-years [\href{https://arxiv.org/abs/1912.04958}{Sec. 6 of 36}], while ours only 2.5.
Also note that the convolutional decoders have 10+ years of highly active research, while INRs are just starting out.

\begin{table}
\caption{FID scores for additional ablations of our multi-scale INR-based GAN (INR-GAN).
Removing  from FMM \textit{worsens} the scores.
Incorporating StyleGAN2's architecture and bilinear upsampling allows the INR-based generator to rival its convolution-based counterpart.
Img/sec of  is measured on 1 NVidia V100 32GB.}
\label{table:improved-results}
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{|l|cc|c|}
\hline
Decoder type & Churches  & Bedrooms  & img/sec  \\
\hline
INR-GAN w/o FMM activation & 10.35 & 11.73 & \textbf{267.3} \\
INR-GAN & 7.12 & 6.27 & 267.1 \\
~+ StyleGAN2 architecture & 5.09 & 4.96 & 265.1 \\
~+ bilinear upsampling & \textbf{3.12} & 3.41 & 203.4 \\
\hline
StyleGAN2 & 3.86 & \textbf{2.65} & 85.5 \\
\hline
\end{tabular}
}
\end{table}
 
\ReviewerAsays{FMM novelty is limited due to [RHH]. Is activation needed in it?}
[RHH] proposes an ``FMM-w/o-activation'' \textit{only} as a concept, and (to the best of our knowledge) our work is the first one to actually try it.
Moreover, \textit{the activation is crucial in FMM} (see Table \ref{table:improved-results}): it bounds the multiplicative factor's magnitude and makes the training more stable.
We thank \ReviewerA\ for the reference and added the discussion in Sec 2 \& 3.2 and the ablation in Sec 4.

\ReviewerAsays{Super-resolution and extrapolation are useful only for real images.}
We disagree. Super-resolution for fake images gives you a high-res generator ``for free'' after training the model on a low-res dataset.
Besides, we believe it to be an interesting property of 2D INRs that deserves attention in its own right --- just like the extrapolation.

\ReviewerAsays{The proposed KPL metric is not useful since some real images can be embedded poorly in the latent space.}
Not quite. Only high-frequency details can be lost during the embedding process, while structure/keypoints are preserved well [\href{https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper}{Abdal et al.}].
Thus, KPL is not affected.
\ReviewerAsays{...and some latent variables might result in the same image.}
We verified for diversity by computing 1) KPL on a shuffled dataset (see Tab. 2 in the paper); and 2) FID, which is sensitive to mode collapse [\href{https://papers.nips.cc/paper/2018/hash/f7696a9b362ac5a51c3dc8f098b73923-Abstract.html}{Sajjadi et al.}].
We created appx. F with the discussion and embedding examples.


\ReviewerBsays{What is the relation between  and ?}
We construct  by reshaping a subset of  into two matrices , multiplying them together to produce a matrix , then applying sigmoid  to it and multiplying it by matrix  (which is shared across all images).
This is an attention-like modulation of  via context vector .
It is described in Sec 3.2 and we illustrate it in Fig.~\ref{rebuttal:fig:fmm}.

\begin{figure}[t]
\centering
\caption{Illustration of FMM. See Section 3.2 for details.}
\label{rebuttal:fig:fmm}
\vspace{-0.4em}
\end{figure}

\begin{table}
\caption{FID \& IS at 300k iterations on multi-class datasets.}
\label{table:diverse-datasets}
\centering
\begin{tabular}{|l|cc|cc|}
\hline
\multirow{2}{*}{Decoder type} & \multicolumn{2}{c|}{LSUN-10} & \multicolumn{2}{c|}{MiniImageNet} \\
& FID  & IS  & FID  & IS  \\
\hline
Basic INR decoder & 216.8 & 1.0 & 271.5 & 1.03 \\
+~Hypernetwork-based decoder & OOM & OOM & 112.9 & 8.76 \\
+~Fourier embeddings & OOM & OOM & 102.8 & 9.85 \\
+~FMM & 23.78 & 2.48 & 84.66 & 9.32 \\
+~Multi-scale INR & 12.47 & 3.02 & 59.63 & 11.29 \\
\hline
StyleGAN2 & 8.99 & 3.18 & 52.94 & 12.32 \\
\hline
Validation set & 0.42 & 9.93 & 0.39 & 61.79 \\
\hline
\end{tabular}
\end{table}
 
\ReviewerBsays{No clear explanation about the training process, loss function, and how the parameters are learned.} Sec 3.1 \& 4.1 state that our  is a small version [``config-e'' of \href{https://arxiv.org/abs/1912.04958}{36}] of SG2's one, so the same non-saturating logistic loss is used for training.
Like SG2, we learn the parameters with Adam optimizer for 800k iterations with a batch size of 32.
We added the details in Sec 4.1 and appx. A \& B.
Also, note that the provided code in the supp. covers all the details.


\ReviewerBsays{Experiments do not cover multi-class/diverse datasets.}
Table~\ref{table:diverse-datasets} shows the results for unconditional generation on LSUN-10  (100k images from each of 10 categories) and \href{https://github.com/yaoyao-liu/mini-imagenet-tools}{MiniImageNet}  (1k images from 100 ImageNet categories).
Each model was trained for 300k iterations with identical hyperparameters.
The proposed architectural changes are effective in this setting as well.

\ReviewerBsays{The paper lacks clarity}.
We are doing what is in our hands to improve it: rewrote Sec 3.2 \& 3.3, added new illustrations for them, revised Sec 1, Sec 4 and the appendix, added pseudo-code on the KPL computation in Sec 4.2.

\ReviewerCsays{Why the projection matrix in  is inherently huge?}
Under the hood,  produces a very large parameter vector  (of size  when without FMM).
And to produce a huge vector, one needs a huge () projection matrix. \ReviewerCsays{several minor comments.} We thank \ReviewerC\ for the remarks and will incorporate them for the final version.

\end{document}
