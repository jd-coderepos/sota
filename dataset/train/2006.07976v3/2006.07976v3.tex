\section{Experiments on AVA}
\label{sec:Experiments}

AVA \cite{gu2018ava} is a video dataset for spatio-temporally localizing atomic visual actions. For AVA, box annotations and their corresponding action labels are provided on key frames of 430 15-minute videos with a temporal stride of 1 second. We use version 2.2 of AVA dataset by default.
In addition to the current AVA dataset, Kinetics-700~\cite{carreira2019short} videos with AVA~\cite{gu2018ava} style annotations are also introduced. The new AVA-Kinetics dataset~\cite{li2020ava} contains over 238k unique videos and more than 624k annotated frames. However, only a single frame is annotated for each video from Kinetics-700.
Following the guidelines of the benchmarks, we only evaluate 60 action classes with mean Average Precision (mAP) as the metric, using a frame-level IoU threshold of . 






\subsection{Implementation Details}


{\flushleft \bf Person Detector.} For person detection on key frames, we use the human detection boxes from \cite{wu2019long}, which are generated by a Faster R-CNN~\cite{ren2015faster} with a ResNeXt-101-FPN~\cite{xie2017aggregated,lin2017feature} backbone. The model is pre-trained with Detectron~\cite{girshick2018detectron} on ImageNet~\cite{deng2009imagenet} as well as the COCO human keypoint images~\cite{lin2014microsoft}, and fine-tuned on the AVA dataset.
\vspace{-2mm}
{\flushleft \bf Backbone Network.} We use SlowFast networks~\cite{feichtenhofer2019slowfast} as the backbone in our localization framework and increase the spatial resolution of res by . We conduct ablation experiments using a SlowFast R-50  instantiation (without non-local blocks). The inputs are 64-frame clips, where we sample  frames with a temporal stride  for the slow pathway, and  frames for the fast pathway.  The backbone is pre-trained on the Kinetics-400 dataset\footnote{The pre-trained SlowFast R-50 and SlowFast R-101+NL (in the following section) are downloaded from SlowFast's official repository.
}.


\vspace{-3mm}
\textbf{\flushleft Training and inference.} 
 In AVA, actions are grouped into 3 major categories: poses (e.g. stand, walk), human-object and human-human interactions. Given that poses are mutually exclusive and interactions are not, we use softmax for poses and sigmoid for interactions before binary cross-entropy loss for training.
We train all models end-to-end (except for
the feature bank part) using synchronous SGD with a batch size of 32 clips.
We train for 35k iterations with a base learning rate of 0.064, which is then decreased by a factor of 10 at iterations 33k and 34k. 
We perform linear warm-up~\cite{goyal2017accurate} during the first 6k iterations. We use a weight decay of  and Nesterov momentum of 0.9. 
We use both ground-truth boxes and predicted human boxes from \cite{wu2019long} for training. 
For inference, we scale the shorter side of input frames to 256 pixels and use detected person boxes with scores greater than 0.85 for final action classification.


\begin{table*}[!t]\vspace{-4mm}
\centering
\subfloat[\textbf{Relation Modeling}\label{tab:ablation:type}]{\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}lx{20}@{}}
 & mAP \\
\shline
{Baseline + STO} \cite{wu2019long} & 26.10\\
 {Baseline + ACRN \cite{sun2018actor} } & 26.71\\
 {Baseline + AIA} \cite{tang2020asynchronous} & 26.79\\
 {Baseline + HRO} & \textbf{27.83}\\
\end{tabular}}\hfill
\subfloat[\textbf{Component Analysis}\label{tab:ablation:structure}]{\tablestyle{4.8pt}{1.1}
\begin{tabular}{@{}lx{20}@{}}
& mAP \\
\shline
{Baseline} & 25.39\\
{Baseline + HRO} & {27.83}\\
{ACAR} & \textbf{28.84}\\
\multicolumn{2}{c}{~}\\
\end{tabular}}\hfill
\subfloat[\textbf{HRO Design}\label{tab:ablation:function}]{\tablestyle{4.8pt}{1.1}
\begin{tabular}{@{}lx{25}@{}}
 & mAP\\
\shline
 {Avg \ \ \ \ \ } & 26.97 \\ {RN} & 27.18 \\
{NL} & \textbf{27.83}\\
 \multicolumn{2}{c}{~}\\
\end{tabular}}\hfill
\subfloat[\textbf{Relation Order}\label{tab:ablation:order}]{\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}lx{20}@{}}
 & mAP \\
\shline
 {Actor First} & 27.62\\
 {Context First} & \textbf{27.83}\\
\multicolumn{2}{c}{~}\\
\multicolumn{2}{c}{~}\\
\end{tabular}}\hfill
\subfloat[\textbf{Relation Depth}\label{tab:ablation:layer}]{\tablestyle{4.8pt}{1.1}
\begin{tabular}{@{}lx{20}@{}}
 & mAP\\
\shline
 {HRO-1L\ \ \ } & {27.63}\\
 {HRO-2L} & \textbf{27.83}\\
 {HRO-3L} & 27.25\\
\multicolumn{2}{c}{~}\\
\end{tabular}}\hfill
\subfloat[\textbf{Feature Bank}\label{tab:ablation:bank}]{\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}lx{20}@{}}
 & mAP\\
\shline
 {HRO} & 27.83\\
 {HRO + LFB \cite{wu2019long}} & 27.75\\
 {HRO + ACFB} & \textbf{28.84}\\
\multicolumn{2}{c}{~}\\
\end{tabular}}

\vspace{2mm}
\caption{\textbf{Ablation study on AVA dataset}. The ``Baseline" of our framework only consists of the video backbone, actor detector and one-layer action classifier. HRO: High-order Relation Reasoning Operator. ACFB: Actor-Context Feature Bank. \label{tab:ablations}}
\vspace{-3mm}
\end{table*}



\subsection{Ablation Study}
We conduct ablation experiments to investigate the effect of different components in our framework on AVA v2.2. The baseline of our framework only consists of the video backbone (SlowFast R-50), the actor detector and the single-layer action classifier (denoted as ``Baseline'' in Table \ref{tab:ablations}).
\vspace{-3mm}
{\flushleft \bf Relation Modeling - Comparison.} In order to show the effectiveness of our actor-context-actor relation reasoning module, we compare against several previous approaches that leverage relation reasoning for action localization based on our baseline. Here we focus on validating the effect of relation modeling only, thus we disable long-term support in this study.
We adapt their reasoning modules such that all methods use the same baseline as our ACAR-Net in order to fairly compare only the impact of relation reasoning. We evaluate 
ACRN that focuses on learning actor-context relations; STO \cite{wu2019long} (a degraded version of LFB) that only captures actor interactions within the current short clip; AIA (w/o memory) \cite{tang2020asynchronous} that aggregates both actor-actor and actor-object interactions. As listed in Table~\ref{tab:ablation:type}, our proposed actor-context-actor relation modeling (``Baseline + HRO'' in Table~\ref{tab:ablation:type}) significantly improves over the compared methods. We observe that AIA with both actor and context relations performs better than ACRN and STO which only model one type of first-order relations, yet our method based on high-order relation modeling outperforms all compared methods by considerable margins. 

We further break down the performances of different relation reasoning modules into three major categories of the AVA dataset, which are poses (\textit{e.g.} stand, sit, walk), human-object interactions (\textit{e.g.} read, eat, drive) and human-human interactions (\textit{e.g.} talk, listen, hug). Fig.~\ref{fig:catgain} compares the gains of different approaches with respect to the baseline in terms of mAP on these major categories. We can see that our HRO gives more performance boosts 
on two interaction categories compared to the pose category, which is consistent with our motivation to model indirect relations between actors and context. Once equipped with ACFB, our framework can further improve on the pose category as well.

Finally, we contrast our ACAR with existing relation reasoning approaches in AVA. We visualize attention maps from different reasoning modules over an example key frame in Fig.~\ref{fig:compare_attmap}. Without needing object proposals, ACAR is capable of localizing free-form context regions for indirectly establishing relations between two actors (the actor of interest is listening to the supporting actor reading a report). In comparison, the attention weights of STO as well as AIA are distributed more diversely and do not have a clear focus point. Note that we do not show the attention map of ACRN since it assigns equal weights to all context regions. 
{\flushleft \bf Component Analysis.} To validate our design, we first ablate the impacts of different components of our ACAR as shown in Table \ref{tab:ablation:structure}. We can observe that both HRO and ACFB lead to significant performance gains over baseline. 
{\flushleft \bf HRO Design.} We test different instantiations of the High-order Relation Reasoning Operator on top of our baseline in Table \ref{tab:ablation:function}. 
Our modified non-local (denoted as ``NL'') mechanism works better than simply designing  as an average function (denoted as ``Avg''), \ie . In addition, the instantiation with relation network (RN) described in Section \ref{sec:hro} also works 
alright. Nonetheless, the modified non-local attention is computationally more efficient than RN with feature triplets and has better performance.
{\flushleft \bf Relation Ordering.} There are two possible orders for reasoning actor-context-actor relations: 1) aggregating actor-actor relations first, or 2) encoding actor-context relations first. Note that our ACAR-Net adopts the latter one. We implement the former order by performing self-attention between actor features with the modified non-local attention before incorporating context features in our baseline. The results in Table~\ref{tab:ablation:order} validate that context information should be aggregated earlier for better relation reasoning.
{\flushleft \bf HRO Depth.} In Table~\ref{tab:ablation:layer}, we observe that stacking two modified non-local blocks in HRO has higher mAP than the one-layer version, yet adding one more non-local block produces worse performance, possibly due to overfitting. We therefore adopt two non-local blocks as the default setting.{\flushleft \bf Actor-Context Feature Bank.} In this set of experiments, we validate the effectiveness of the proposed ACFB. We set the ``window size''  to 21s due to memory limitations, and longer temporal support is expected to perform better \cite{wu2019long}. 
As presented in Table~\ref{tab:ablation:bank}, adding long-term support with ACFB significantly improves the baseline (HRO's 27.83 \ HRO + ACFB's \textbf{28.84}).
We also test replacing the ACFB in our framework with the long-term feature bank (LFB)~\cite{wu2019long} (denoted as ``HRO + LFB''). However, LFB even fails to match the baseline performance. This drop might be because LFB encodes only ``zeroth-order'' actor features, which cannot provide enough relational information from neighboring frames for assisting interaction recognition.




\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{fig/category_gain.pdf}
\caption{\textbf{Gains of mAP on three major categories of the AVA dataset with respect to Baseline.} Our ACAR consistently outperforms other relation reasoning methods, and achieves larger performance gains on the two interaction categories. \label{fig:catgain}}

\end{figure}
















\begin{table}[!t]
\centering
\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}l|x{23}|x{37}@{}}
 model & inputs & val mAP\\
\shline
{ACRN, S3D} \cite{sun2018actor} & V+F & 17.4 \\
{Zhang \etal} \cite{zhang2019structured}, I3D & V & 22.2 \\
 {Action TX, I3D} \cite{girdhar2019video} & V & 25.0 \\
 {LFB}, R-50+NL \cite{wu2019long} & V & 25.8 \\
 {LFB}, R-101+NL \cite{wu2019long} & V & 27.4 \\
 {SlowFast, R-50, } \cite{feichtenhofer2019slowfast} & V & 24.8 \\
 {SlowFast, R-101, } \cite{feichtenhofer2019slowfast} & V & 26.3 \\
\hline
\textbf{Ours}, R-50,  & V & 28.3 \\
\textbf{Ours}, R-101,  & V & {\bf 30.0} \\
\end{tabular}
\vspace{2mm}
\caption{\textbf{Comparison with state-of-the-arts on AVA v2.1.} 
All models are pre-trained on Kinetics-400. V and F refer to visual frames and optical flow respectively. \label{tab:sota_v2.1}}
\end{table}


\begin{table}[!t]
\centering
\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}l|x{51}|x{37}@{}}
 model & pre-train & val mAP \\
\shline
{SlowFast, R-101+NL} \cite{feichtenhofer2019slowfast} & Kinetics-600 & 29.0 \\
{AIA, R-101+NL} \cite{tang2020asynchronous} & Kinetics-700 & 32.3 \\
 \hline
Ours, R-101+NL & Kinetics-600 & 31.4 \\
 \textbf{Ours}, R-101 & Kinetics-700 & {\bf 33.3} \\
\end{tabular}
\vspace{2mm}
\caption{\textbf{Comparison with state-of-the-arts on AVA 2.2.} We do not conduct testing with multiple scales and flips. All models use . \label{tab:sota_v2.2}}
\end{table}


\begin{table}[!t]
\centering
\tablestyle{4.8pt}{1.1}\begin{tabular}{@{}l|x{37}|x{37}@{}}
 model & val mAP & test mAP\\
\shline
 {AIA++, ensemble \cite{xia2020report}}& - & 32.91 \\
 {MSF, ensemble \cite{Zhu2020report}}& - & 31.88 \\
\hline
 {SlowFast, R-101,  (our impl.)} & 32.98 & - \\
{Ours, R-101, } & 35.84 & - \\ 
 {Ours++, R-101, } & 36.36 & - \\
 {Ours++, ensemble}  & \textbf{40.49} & \textbf{39.62}
\end{tabular}
\vspace{2mm}
\caption{\textbf{AVA-Kinetics results}. ``++'' refers to inference with 3 scales and horizontal flips. Models submitted to the test server are trained on both training and validation sets.
\label{tab:sota_ak}}
\vspace{-3mm}
\end{table}




\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{fig/compare_att.pdf}
\caption{\textbf{Comparison of attention maps from different approaches of relation modeling for action detection.}
Our method is able to attend the contextual region (some document) that relates the \textcolor{red}{actor of interest} marked in red (performing ``listen to'') and the \textcolor{green}{supporting actors} in the green box (performing ``read''), while other methods fail to achieve similar effects.  \label{fig:compare_attmap}}
\vspace{-4mm}
\end{figure}


\subsection{Comparison with State-of-the-arts on AVA}

We compare our ACAR-Net with state-of-the-art methods on the validation set of both AVA v2.1 (Table~\ref{tab:sota_v2.1}) and v2.2 (Table~\ref{tab:sota_v2.2}). Note that we also provide results with more advanced video backbones, \ie two SlowFast R-101 instantiations (with / without NL). On AVA v2.1, our framework achieves {30.0 mAP} and outperforms all prior results with pre-trained Kinetics-400 backbone. On AVA v2.2, our ACAR-Net reaches {33.3 mAP} with only single-scale testing, establishing a new state-of-the-art. Note that our method surpasses AIA \cite{tang2020asynchronous} with only  of temporal support. The results indicate that with proper modeling of higher-order relations, our approach extracts more informative cues from the context. 

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{fig/attention.pdf}
\caption{\textbf{Visualization of actor-context-actor attention maps on AVA.} \textcolor{red}{Actors of interest} are marked in red and \textcolor{green}{supporting actors} in green. Heat maps illustrate the context regions' attention weights  from actor-context-actor relation reasoning. We observe that our model has learned to attend to useful relations between actors and context, and the context serves as the bridge for connecting actors.
\label{fig:attn}}
\vspace{-4mm}
\end{figure*}



We present our results on AVA-Kinetics in Table~\ref{tab:sota_ak}. Our baseline is already highly competitive (33 mAP). Yet integrating our ACAR modeling still leads to a significant gain of +2.86 mAP. This demonstrates that performance enhancement brought by high-order relation modeling can generalize to this new dataset.
With an ensemble of models, we achieve \textbf{39.62 mAP} on the test set, ranking first in the AVA-Kinetics task of ActivityNet Challenge 2020 and outperforming other entries by a large margin (+6.71 mAP). More details on our winning solution are provided in the technical report \cite{chen20201st}.











\subsection{Qualitative Results} 

Our proposed ACAR operates fully convolutionally on top of spatio-temporal features, and this allows us to visualize the actor-context-actor relation maps  generated by our High-order Relation Reasoning Operator. As shown in Fig.~\ref{fig:attn}, the first two columns include the key frame as well as the corresponding relation map from the same clip, and the last three columns show the relation map denoting interactions with actors and context from a neighboring clip.
We can observe that the attended regions usually include the actor of interest, supporting actors' body parts (\ie head, hands and arm) and objects being in interaction with the actors. Take the first example on the left as an example. The green supporting actor  is taking a package from the red actor of interest . Such information is well encoded by our ACAR-Net in the form of actor-context-actor relations: packages, hands and arms of both actors are highlighted.






%
