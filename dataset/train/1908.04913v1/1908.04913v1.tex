\section{Experiments}

\begin{figure}
    \centering
      \includegraphics[width=0.5\textwidth]{figures/race_.png}
  \caption{Racial Compositions in Various Face Datasets}
\label{fig:race-comp}
\end{figure}



\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.33\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{figures/fairface_whole.png}
      \caption{FairFace}
     \end{subfigure}
    \begin{subfigure}[t]{0.33\textwidth}  \centering
      \includegraphics[width=1\textwidth]{figures/utkface_whole.png}
      \caption{UTKFace}
     \end{subfigure}
    \begin{subfigure}[t]{0.33\textwidth}  \centering
      \includegraphics[width=1\textwidth]{figures/lfwa_whole.png}
      \caption{LFWA+}
     \end{subfigure}
  \caption{t-sne visualizations \cite{maaten2008visualizing} of faces in datasets. Larger plots can be found in Supplementary Materials. }
\label{fig:tsne}
\end{figure*}


\subsection{Measuring Bias in Datasets}
We first measure how skewed each dataset is especially in terms of race composition. 
For the datasets with race annotations, we use the reported statistics. For other datasets, we annotated the race label for 3,000 random samples drawn from each dataset. See Figure~\ref{fig:race-comp} for the result. As expected, most existing face attribute datasets, especially the ones focusing on celebrities or politicians, are biased toward the white race. 
Unlike race, we find that most datasets are relatively more balanced on gender ranging from 40\%-60\% male ratio. 


\subsection{Model and Cross-Dataset Performance}
Since the main focus of our paper is analyzing the effects of different datasets, we use an identical model to train face attribute classifiers to be trained from different sets. We chose ResNet-34~\cite{he2016deep} as the model architecture, which is widely used for visual modeling. Recent works also show that ResNets perform very well on gender classification~\cite{ranjan2019hyperface}. We use ADAM optimization \cite{kingma2014adam} with a learning rate of 0.0001, with PyTorch. 
Given an image, we detect faces using the dlib's CNN-based face detector~\cite{king2015max} and run the attribute classifiers on each face. 

Throughout the evaluations, we compare our dataset with three other datasets: UTKFace~\cite{zhang2017age}, LFWA+, and CelebA~\cite{liu2015faceattributes}. Both UTKFace and LFWA+ have annotations on race, and thus are suitable for comparison with our dataset. CelebA does not have race annotations but we additionally use it for gender classification. See Table~\ref{table:stat} for more details. 



Using models trained from these datasets, we first performed cross-dataset classifications, by alternating training sets and test sets. Note that FairFace is the only dataset with 7 races. To make it compatible with other datasets, we merged our fine racial groups when tested on other datasets. CelebA does not have race annotations but was included for gender classification. 

Tables~\ref{table:cross_white} and \ref{table:cross_non_white} show the classification results for race, gender, and age on the datasets across subpopulations. As expected, each model tends to perform better on the same dataset on which it was trained. However, the accuracy of our model was highest on some variables on the LFWA+ dataset and also very close to the leader in other cases. This is partly because LFWA+ is the most biased dataset and ours is the most diverse, and thus more generalizable dataset. 


\subsection{Generalization Performance}
\subsubsection{Datasets}
To test the generalization performance of the models, we consider three novel datasets. Note that these datasets were collected from completely different sources than our data from Flickr and not used in training. Since we want to measure the effectiveness of the model on diverse races, we chose the test datasets that contain people in different locations as follows.

\textbf{Geo-tagged Tweets.} 
First we consider images uploaded by Twitter users whose locations are identified by geo-tags (longitude and latitude), provided by \cite{steinert2018twitter}. From this set, we chose four countries (France, Iraq, Philippines, and Venezuela) and randomly sampled 5,000 faces. 

\textbf{Media Photographs.} 
Next, we also use photographs posted by 500 online professional media outlets. Specifically, we use a public dataset of tweet IDs \cite{DVN/2FIFLH_2017} posted by 4,000 known media accounts, \eg, $@$nytimes. Note that although we use Twitter to access the photographs, these tweets are simply external links to pages in the main newspaper sites. Therefore this data is considered as media photographs and different from general tweet images mostly uploaded by ordinary users. We randomly sampled 8,000 faces from the set.

\textbf{Protest Dataset.} 
Lastly, we also use a public image dataset collected for a recent protest activity study \cite{won2017protest}. The authors collected the majority of data from Google Image search by using keywords such as ``Venezuela protest'' or ``football game'' (for hard negatives). The dataset exhibits a wide range of diverse race and gender groups engaging in different activities in varying countries. We randomly sampled 8,000 faces from the set.

All these sample faces were also annotated for gender, race, and age by Amazon Mechanical Turk Workers. 

\subsubsection{Classification}
Given these datasets, the classification was performed for each of the models (See Table~\ref{table:validation}). Again, because UTKFace and LFWA+ do not have annotations in all 7 race groups, we used the same 4-race model when comparing on the overlapping races. For East Asian, Southeast Asian, Latino, Middle Eastern, we report accuracy of our full model. Unfortunately, we cannot compare the accuracy because other datasets can't produce outputs for those race groups. 

Here, the result is very clear and our dataset outperforms the other datasets for race, gender, and age, on the novel datasets which have never been used in training and also come from different data source as training set. More importantly, our model shows very consistent result -- for race, gender, age classification -- across different race groups compared to other datasets, especially LFWA+, suffering more on non-white faces. 


\subsection{Discussion}
The evaluations show a clear advantage of our dataset achieving a better performance on novel datasets. We further investigate detailed dataset characteristics to understand the behaviors. 

First of all, since our dataset is more balanced on race and has more coverage on non-white races, it is straightforward to understand why it performs better on non-white faces in the generalization test. 

Does this help achieve a better coverage, \ie, does our dataset have more diverse faces? To answer the question, we first visualize randomly sampled faces in 2D space using t-sne \cite{maaten2008visualizing} as shown in Figure~\ref{fig:tsne}. LFWA+ was derived from LFW, which was developed for face recognition, and therefore contains multiple images of the same individuals, \ie, clusters. UTKFace also tends to focus more on local clusters compared to FairFace. 

To explicitly measure the diversity of faces in these datasets, we examine the distributions of pairwise distance between faces (Figure~\ref{fig:cdf}). On the random subsets, we first obtained 128-dimensional facial embedding using an off-the-shelf face model (ResNet) offered by dlib and measured pair-wise distance. Figure~\ref{fig:cdf} shows the CDF functions for 3 datasets. As conjectured, UTKFace had more faces that are tightly clustered together and very similar to each other, compared to our dataset. Surprisingly, the faces in LFWA+ were shown very diverse and far from each other, even though the majority of examples were the white face. We believe this is mostly due to the fact the face embedding was also trained on a very similar white-oriented dataset which will be effective in separating white faces, not because the appearance of their faces are actually diverse. (See~Figure~\ref{fig:grid})



\begin{figure}
  \centering
      \includegraphics[width=0.4\textwidth]{figures/cdf-face.png}
  \caption{Distribution of pairwise distances of face in 3 datasets measured by L1 distance on face embedding. }
\label{fig:cdf}
\end{figure}



\begin{table*}
\small
\caption{Cross-Dataset Classification Accuracy on White Race.}
\label{table:cross_white}
\vspace{-10pt}
\centering
\scalebox{1.0}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{}           & \multicolumn{10}{c|}{Tested on}                                                                                                                                       \\ \cline{2-11} 
                            & \multicolumn{4}{c|}{Race}                                   & \multicolumn{4}{c|}{Gender}                                           & \multicolumn{2}{c|}{Age}        \\ \cline{2-11} 
                            &          & FairFace       & UTKFace        & LFWA+          & FairFace       & UTKFace        & LFWA+          & CelebA* & FairFace       & UTKFace        \\ \hline
\multirow{4}{*}{Trained on} & FairFace & \textbf{.981} & .959          & .971          & \textbf{.944} & \textbf{.948} & \textbf{.929} & .976              & \textbf{.574} & .577          \\ \cline{2-11} 
                            & UTKFace  & .775          & .975          & .886          & .851          & .943          & .897          & .963              & .285          & \textbf{.649} \\ \cline{2-11} 
                            & LFWA+    & .945          & \textbf{.981} & \textbf{.997} & .758          & .851          & .924          & .932              & -              & -              \\ \cline{2-11} 
                            & CelebA   & -              & -              & -              & .866          & .893          & .921          & \textbf{.985}     & -              & -              \\ \hline
                            \multicolumn{11}{r}{* CelebA doesn't provide race annotations. The result was obtained from the whole set (white and non-white). }
\end{tabular}
}
\end{table*}



\begin{table*}
\small
\caption{Cross-Dataset Classification Accuracy on non-White Races.}
\vspace{-10pt}
\label{table:cross_non_white}
\centering
\scalebox{1.0}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{}           & \multicolumn{10}{c|}{Tested on}                                                                                                                                       \\ \cline{2-11} 
                            & \multicolumn{4}{c|}{Race}                                   & \multicolumn{4}{c|}{Gender}                                           & \multicolumn{2}{c|}{Age}        \\ \cline{2-11} 
                            &          & FairFace       & UTKFace        & LFWA+          & FairFace       & UTKFace        & LFWA+          & CelebA* & FairFace       & UTKFace        \\ \hline
\multirow{4}{*}{Trained on} & FairFace & \textbf{.743} & .549          & .396          & \textbf{.932} & \textbf{.929} & \textbf{.895} & .976              & \textbf{.588} & .609          \\ \cline{2-11} 
                            & UTKFace  & .488          & \textbf{.591} & .316          & .823          & .907          & .877          & .963              & .328          & \textbf{.685} \\ \cline{2-11} 
                            & LFWA+    & .437          & .353          & \textbf{.439} & .734          & .852          & .887          & .932              & -              & -              \\ \cline{2-11} 
                            & CelebA   & -              & -              & -              & .794          & .886          & .884          & \textbf{.985}     & -              & -              \\ \hline
                            \multicolumn{11}{r}{* CelebA doesn't provide race annotations. The result was obtained from the whole set (white and non-white). }
                            \end{tabular}
}
\end{table*}



\begin{table*}[]
\small
\caption{Gender classification accuracy on external validation datasets, across race and age groups.}
\vspace{-10pt}
\label{table:mean_var}
\centering
\scalebox{1.0}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{}                                                                  & Mean across races & SD across races & Mean across ages & SD across ages \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Model\\  trained on\end{tabular}} & FairFace & \textbf{94.93\%}  & \textbf{2.43\%} & \textbf{91.84\%} & \textbf{7.58\%}      \\ \cline{2-6} 
                                                                             & UTKFace  & 89.56\%           & 3.49\%          & 87.01\%          & 9.56\%               \\ \cline{2-6} 
                                                                             & LFWA+    & 82.45\%           & 5.40\%          & 81.58\%          & 9.66\%               \\ \cline{2-6} 
                                                                             & CelebA   & 90.19\%           & 3.06\%          & 85.95\%          & 14.90\%              \\ \hline
\end{tabular}
}
\end{table*}























\begin{table}[]
\caption{Confusion Matrix of Race Classification}
\label{table:conf}
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
  & W     & B     & L     & E     & S     & I     & M     \\ \hline
W & 0.868 & 0.003 & 0.094 & 0.014 & 0.003 & 0.009 & 0.009 \\ \hline
B & 0.004 & 0.930 & 0.028 & 0.000 & 0.007 & 0.031 & 0.000 \\ \hline
L & 0.161 & 0.029 & 0.670 & 0.022 & 0.034 & 0.073 & 0.012 \\ \hline
E & 0.011 & 0.002 & 0.021 & 0.910 & 0.055 & 0.002 & 0.001 \\ \hline
S & 0.006 & 0.004 & 0.031 & 0.478 & 0.455 & 0.027 & 0.000 \\ \hline
I & 0.013 & 0.031 & 0.117 & 0.004 & 0.015 & 0.790 & 0.030 \\ \hline
M & 0.188 & 0.027 & 0.274 & 0.000 & 0.013 & 0.184 & 0.314 \\ \hline
\end{tabular}
}
\end{table}


