


\begin{figure*}[t!]
    \centering\small
    \begin{tabular}{c|c|c|c|c|c|c}
        Architecture & Share task weights & $m_1$ & $m_2$ & $K$ & mAP & Inference time (ms per frame) \\
        \hline
        RetinaNet & No & - & - & 6 & 36.17 & 45 \\
        RetinaNet & Yes & - & - & 6 & 35.35 & 40 \\
        RetinaNet & No & - & - & 1 & 31.45  & 37 \\
        RetinaNet & Yes & - & - & 1 & 30.71 & 30 \\
        \modelname & - & 1 & 3 & 6 & 35.11 & 83 \\
        \modelname & - & 2 & 2 & 6 & 35.55 & 75 \\
        \modelname & - & 3 & 1 & 6 & 35.74 & 74 \\
    \end{tabular}\vspace{-2mm}
    
    \caption{\footnotesize  \textbf{COCO17 ablations.} Performance of vanilla RetinaNet and
\modelname (without tracking embedding layers) in terms of single 
image object detection performance on COCO17.  $m_1$ denotes the number of
task-shared post-FPN layers and $m_2$ denotes the number of task-specific
post-FPN layers.
}\vspace{-3mm}
    \label{tab:per_anchor_ablations_coco}
\end{figure*}

In our experiments we  focus on the recently released
Waymo Open dataset~\cite{waymo_open_dataset} v1 (\emph{Waymo} for short).
We also report results
on the larger v1.1 release in Section~\ref{sec:waymov11}.
This dataset
contains 
annotations on 200K frames collected at 10 Hz in Waymo vehicles and 
covers various geographies and weather conditions.  
Frames come from 5 camera positions (front and sides).
For the purposes of this paper,
we focus on 2d detection and tracking  and more specifically only
on the `vehicle' class as the dataset has major class imbalance,
which is not our main focus. 
In addition to Waymo, we report ablations
on the COCO17 dataset~\cite{lin2014microsoft}.

Finally we evaluate both detection and tracking metrics as measured by 
standard mean AP~\cite{everingham2015pascal,lin2014microsoft,girdhar2018detect} (mAP) as well as
CLEAR MOT tracking metrics~\cite{bernardin2008evaluating,milan2016mot16}, specifically using the COCO AP (averaging over IOU thresholds between 0.5 and 0.95) and the py-motmetrics library.\footnote{\url{https://github.com/cheind/py-motmetrics}}
We also benchmark using Nvidia V100 GPUs reporting inference time 
in milliseconds per frame.  For all models we only benchmark the ``deep learning part'',
ignoring any bookkeeping logic required by the tracker
which is typically very lightweight.

Evaluating a model simultaneously for detection and tracking requires some care.
Detection mAP measures a model's average ability to trade off 
between precision and recall without requiring a hard operating point 
 --- it's therefore better to use a low or zero score threshold for detection mAP.
However CLEAR MOT tracking metrics such as MOTA  require selecting a
single operating point as they directly reference true/false positives and in practice are fairly sensitive to these hyperparameter choices.
It is often better to use a higher
 score threshold to report tracking metrics so as to not admit too many false positives.  
In our experiments we simple use separate thresholds for evaluation: we evaluate our model as a detector using a near-zero 
score threshold and as a tracker using a higher score threshold.





\begin{figure*}
    \centering\small
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        Architecture &  Share task weights & $m_1$ & $m_2$ & $m_3$ & $K$ & MOTA & mAP & Inference time (ms per frame) \\
        \hline
        RetinaNet & No & - & - & - & 6   & -         & 38.19 & 34 \\
        RetinaNet$^*$ & No & - & - & - & 6   & 38.02         & 37.43  & 44 \\
        RetinaNet & Yes & - & - & - & 6  & -         & 37.95 & 30 \\
        RetinaNet$^*$ & Yes & - & - & - & 6  & 37.63         & 36.75 & 40 \\
        RetinaNet & No & - & - & 2 & 1   & 30.94     & 35.20 & 33 \\
        RetinaNet & Yes & - & - & 2 & 1  & 31.20     & 35.08 & 29 \\
        \modelname & - & 1 & 3 & 2 & 6 & 38.71     & 37.96 & 88 \\
        \modelname & - & 2 & 2 & 2 & 6 & 39.08     & 38.14 & 81 \\
        \modelname & - & 3 & 1 & 2 & 6 & 39.12     & 38.24 & 70
    \end{tabular}\vspace{-2mm}
    \caption{\footnotesize  \textbf{Waymo ablations.}  Performance of vanilla RetinaNet and
\modelname (including tracking embedding layers) in terms of detection mAP and tracking MOTA on the Waymo Open Dataset.  $m_1$ denotes the number of
task-shared post-FPN layers, $m_2$ denotes the number of task-specific
post-FPN layers, and $m_3$ denotes the number of embedding layers. 
RetinaNet$^*$ is a vanilla RetinaNet model (with $K=6$) trained with tracking
losses where instance embedding vectors are shared among ``colliding anchors''.
}\vspace{-3mm}
    \label{tab:waymo_ablations}
\end{figure*}


\vspace{-2mm}
\begin{figure}
    \centering\small
    \begin{tabular}{c|c|c}
        \# embedding layers & MOTA & mAP  \\
        \hline
        0 & 38.52 & 37.93 \\
        2 & 39.19 & 38.24  \\
        4 & 38.85 & 38.24  
    \end{tabular}\vspace{-2mm}
    \caption{\footnotesize  Track embedding subnetwork
    depth ablation. 
    We train versions of \modelname with $m_3=0$, $2$, and $4$ projection layers.}\vspace{-3mm}
    \label{fig:depth_ablation}
\end{figure}




\subsection{Evaluating \modelname as a  detector}
\vspace{-1mm}

As a preliminary ablation (Table~\ref{tab:per_anchor_ablations_coco}), 
we study the effect of our architectural 
modifications to RetinaNet on standard single image detection by 
evaluating on COCO17.  In these experiments we drop the embedding layers
of \modelname since COCO is not a video dataset.

For these experiments only, 
we train with a slightly different setup compared to our later
Waymo experiments.  We use Resnet-50 as a base feature extractor
(Imagenet initialized),
and train at $896\times 896$ resolution with 
bfloat16~\cite{wang2019} mixed precision. 
We train with batches of size 64 split across 8 TPU v3 cores,
and performing per-core batch normalization.
We use a linear learning rate warmup for the first 2K steps increasing to a base learning rate of 0.004, then use a cosine annealed learning rate~\cite{loshchilov2016sgdr} 
for 23K steps.
Note that we could use 
heavier feature extractors or higher image resolutions 
to improve performance, but the main objective of these ablations is to shed
light on variations of the Post-FPN subnetworks of RetinaNet and
\modelname.


Recall that $m_1$ and $m_2$ refer to the number of convolutions 
 for the task-shared and task-specific post-FPN subnetworks
respectively.  
We set $m_1+m_2=4$ so as to be comparable to RetinaNet.  $K$ is the number of anchor
shapes per location which we set to 6 by default but to show that
having multiple anchor shapes per location is important for detection, we also
compare against
a simplified RetinaNet which uses only 1 box per location.
Finally we experiment with a version of vanilla RetinaNet 
where the task-specific subnetworks are forced to share their
weights (the ``Share task weights'' column in Table~\ref{tab:per_anchor_ablations_coco}) since this is
closer to the task-shared post-FPN layers of  \modelname.


\looseness -1 We note firstly that using $K=6$ anchors per location is very important
to strong performance on COCO and that it is better to have separate
task-specific subnetworks than it is to share, confirming observations
by~\cite{lin2017focal}.  
We also observe that by using \modelname, we are able to 
extract per-instance features by design
(which we will next use for tracking, but could be generally useful)
while achieving similar detection performance on COCO.
If one does not need per-instance level features, one can still
get slightly better numbers with the original prediction head
layout of RetinaNet (which is similar 
to that of SSD~\cite{liu2016ssd} 
and the RPN used by many papers, e.g.,~\cite{ren2015faster,he2017mask}). 
Among the 3 settings of $(m_1, m_2)$ for \modelname, 
we find that using 3 task-shared layers ($m_1=3$) followed a single 
 task-specific layer ($m_2=1$), has a slight edge over the other
 configurations.

We report running times (averaged over 500 COCO  images)
in Table~\ref{tab:per_anchor_ablations_coco}.
Our modifications increase running time over vanilla RetinaNet --- this is
unsurprising since the cost of the post-FPN subnetworks have now been
multiplied by $K$.  Among the three variants
of \modelname, $(m_1=3, m_2=1)$ is again the fastest.




\begin{figure*}
    \centering\small
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
        Model & MOTA & TP  & FP  & ID switches & mAP & Inference time (ms per frame) \\
        \hline
        Tracktor &   35.30 & 106006 & 15617 & 16652 & 36.17  & 45 \\
        Tracktor++ & 37.94 & 112801 & 15642 & 10370 & 36.17 & 2645 \\
        \modelname & 39.19 & 112025 & 11669 & 5712 & 38.24 & 70 
    \end{tabular}\vspace{-2mm}
    \caption{\footnotesize We compare \modelname to
Tracktor/Tracktor++~\cite{bergmann2019tracking}
which are currently state of the art on the 
MOT17 Challenge. }\vspace{-2mm}
    \label{tab:vs_tracktor}
\end{figure*}



\begin{figure}[t!]
    \centering\small
    \begin{tabular}{c|c|c|c}
        Model & MOTA & mAP & Inference \\
         &  &  &  time (ms) \\
        \hline
        IOU baseline & 35.36 & 38.53 & 70 \\
        \modelname w/o triplet loss & 37.92 & 38.58 & 70 \\
        \modelname, w/R-50 ReID & 37.39 & 38.58 & 80 \\
        \modelname & 39.19 & 38.24 & 70 
    \end{tabular}\vspace{-2mm}
    \caption{\footnotesize Comparison of joint training (\modelname) with alternatives:
    (1) IOU based similarity tracker, (2) \modelname w/o triplet loss,
    (3) \modelname w/R-50 ReID, 
    }\vspace{-3mm}
    \label{fig:joint}
\end{figure}


\vspace{-1mm}
\subsection{Architectural ablations}
\vspace{-1mm}

For our remaining experiments we evaluate on  Waymo,
this time including the embedding network with triplet loss training
and additionally evaluating tracking performance
using the system described in Section~\ref{sec:trackingsystem}.

We first ablate the depth of the embedding network (see Table~\ref{fig:depth_ablation})
in which we train models using $m_3=0$, 2 and 4 projection layers (fixing $m_1=3$ and $m_2=1$
as was shown to be best on the COCO ablation above), obtaining best
performance for both detection and tracking with 2 layers.

Setting $m_3=2$ layers for the embedding subnetwork, we present our ablations on the
Waymo dataset in Table~\ref{tab:waymo_ablations}, training
via the method described in Section~\ref{sec:training}.

To demonstrate the value of \modelname's
anchor-level features for tracking,
we evaluate two baseline versions of the vanilla RetinaNet architecture ---
(1) one where we use $K=1$ anchor shapes since in this case it is possible to extract 
per-instance feature vectors, and (2) the standard $K=6$ setting where during tracking
we simply force embeddings for anchors that ``collide'' at the same spatial
center to be the same (we refer to this baseline as RetinaNet$^*$).

As with the COCO ablations, we see that using multiple
($K=6$) anchor shapes is important to both detection
and tracking metrics.
Thus it is unsurprising that
\modelname significantly outperforms the RetinaNet based ($K=1$) tracking baseline likely mostly by virtue of being a stronger detector.
However both RetinaNet$^*$ rows exhibit lower MOTA and mAP results compared to their
non-starred counterparts, suggesting that ``abusing'' vanilla RetinaNet to perform
tracking by ignoring colliding anchors is harmful both for detection and tracking, 
thus underscoring the importance of \modelname's per-anchor embeddings.

Our best \modelname configuration reaches 39.12 MOTA and has a mAP of 38.24.
In contrast to the COCO ablations where vanilla RetinaNet retains a
slight edge over \modelname, here 
we see that \modelname outperforms  RetinaNet as a detector, suggesting that by
 including  tracking losses, we are able to boost detection performance.

 

 

Finally with a running time of 70ms per frame, we note that
inference with \modelname is faster than
the sensor framerate (10 Hz) in the Waymo dataset. 
Compared to the COCO setting, \modelname must run additional
convolution layers for embeddings, but since COCO has 80 classes which
makes the top of the network slightly heavier, the final running time is
slightly lower in the Waymo setting.

    
\vspace{-1mm}
\subsection{Joint vs Independent training}\label{sec:joint}
\vspace{-1mm}

To demonstrate the benefit of joint training with detection and 
tracking tasks, we now compare \modelname against three natural baselines
which use the same tracking system as \modelname but change the underlying
data association similarity function
(Table~\ref{fig:joint}):
\vspace{-2mm}
\begin{itemize}\denselist
    \item An \emph{IOU baseline}, where  detection similarity
is measured only by IOU overlap  (with no embeddings),
    \item \emph{\modelname w/o triplet loss}, in which we ignore
the triplet loss (and thus do not train the model specifically for tracking) 
and measure
embedding similarity via the per-instance
feature vectors $F_{i,k}$, and
    \item \emph{\modelname w/R-50 ReID}, in which again we ignore
triplet loss when training \modelname and feed the detections
to an offline-trained re-identification (ReID) model. For the ReID model, we train a Resnet-50 based TriNet model~\cite{hermans2017defense} 
to perform ReID on Waymo.
\end{itemize}
We observe that even the IOU-only  tracker provides
a reasonably strong baseline on  Waymo, most likely
by virtue of have a strong detection model --- it is likely
that this tracker is more accurate when the car is driving slowly
(compared to, e.g., highway driving).  However, using visual embeddings allows us to outperform this simple baseline in all cases,
and \modelname when trained with detection and metric learning
losses jointly outperforms these baselines.







      
















\vspace{-2mm}
\subsection{Comparison against state of the art}\label{sec:waymov11}
\vspace{-2mm}








We finally compare (Table~\ref{tab:vs_tracktor}) 
against the recent Tracktor and Tracktor++ algorithms
which are currently state of the art on MOT Challenge. 
For these experiments we use our own Tensorflow reimplementations
of Tracktor and Tracktor++ which adds a ReID component and 
camera motion compensation (CMC). 
Our implementation differs in some details
from that described in the original paper in that it is based on the
Tensorflow Object Detection API~\cite{huang2017speed}
and does not use an FPN.  
We use the same ReID model as the one in Section~\ref{sec:joint}, 
which matches the approach taken in the Tracktor paper.
To verify that our reimplementations are
competitive, we submitted results
from our Resnet-101
based Tracktor models to the official MOT Challenge server, which 
achieve nearly identical MOTA numbers as the official submission which 
uses an FPN (53.4 vs. 53.5).  We also submitted results from
a Resnet-152 based Tracktor which currently outperforms all 
entries on the public leaderboard (with 56.7 MOTA).  

On  Waymo, we use a  Resnet-50 based Tracktor  running at 
$1024\times 1024$ resolution to be comparable to our model.
If we compare the  Tracktor (without
CMC or ReID) MOTA score to the IOU tracking performance in Table~\ref{fig:joint},
we see that the two approaches are roughly on par.  We believe that IOU based tracking can achieve parity with 
Tracktor here due to (1) having highly accurate detections to begin with,
and (2) significant camera motion which hurts  Tracktor.

In fact we observe that
Tracktor needs the `++` to significantly outperform the IOU based tracker.
However it is far slower --- in addition to running
Faster R-CNN, it must run a second Resnet-50 model for ReID
followed by CMC (which is time consuming).\footnote{
To benchmark the runtime of CMC on Waymo, we use the same function used
by the authors of~\cite{bergmann2019tracking} 
(OpenCV's \emph{findTransformECC} function with `MOTION\_EUCLIDEAN` option),
and run on a workstation with 56 Intel(R) Xeon(R)  E5-2690 v4 2.60GHz CPUs (w/14 cores/CPU).
}

\modelname outperforms both variants on tracking and detection. It 
is able to achieve these improvements by significantly reducing the number of
false positives and ID switches.  And despite being slower than  vanilla
Tracktor  (whose running time is dominated by Faster R-CNN), \modelname is 
significantly faster than Tracktor++.


\paragraph{Evaluation on the Waymo v1.1 dataset.}
As a baseline for future comparisons, we also reproduce our evaluations on the 
Waymo v1.1 release with $\sim 800$K frames for
training containing $\sim 1.7$M annotated vehicles.
For these evaluations, we train for 100K steps with a base learning rate of 0.004 
(and all other hyperparameters fixed).
Results are shown in Table~\ref{fig:waymo1.1}, where we again see the same
trends with \modelname significantly outperforming a baseline IOU based tracker as
well as outperforming Tracktor++  
with a significantly faster running time.





\begin{figure}[t!]
    \centering\small
    \begin{tabular}{c|c|c|c}
        Model & MOTA & mAP & Inference \\
         &  &  &  time (ms) \\
        \hline
        IOU baseline & 38.25  & 45.78 & 70 \\
        Tracktor++ & 42.62 & 42.41 & 2645 \\
        \modelname & 44.92 & 45.70 &  70  
    \end{tabular}\vspace{-2mm}
    \caption{\footnotesize Evaluations on the Waymo v1.1 dataset (which has a $4\times$
    larger training set than the v1 dataset).
    }\vspace{-3mm}
    \label{fig:waymo1.1}
\end{figure}











