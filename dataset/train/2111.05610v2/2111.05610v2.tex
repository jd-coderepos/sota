
\section{Experiments}
\subsection{Datasets and Evaluation Metric}
We report our experimental results on five public available video-text retrieval datasets. Following \cite{luo2021clip4clip}, we use recall at rank  (R@), median rank (MdR) and mean rank (MnR) as metrics to evaluate our model.

\noindent\textbf{MSR-VTT} \cite{xu2016msrvtt} is the most popular benchmark for video-text retrieval task. It consists of 10000 videos with 20 captions for each. We report our results on the standard full split \cite{dzabraev2021mdmmt} and 1K-A split \cite{yu2018joint}. The former contains 6513 videos for training, 497 videos for validating and 2990 videos for testing. And the latter uses 9000 videos for training and 1000 video-text pairs for testing.

\noindent\textbf{MSVD} \cite{chen2011collecting} collects 1970 videos with 1200 for training, 100 for validation and 670 for testing. Each video has a length of during 1 to 62 seconds and is paired with approximately 40 sentences.

\noindent\textbf{VATEX} \cite{wang2019vatex} is a large-scale multilingual video description dataset including over 41,250 videos and 825,000 captions. For fair comparison, we follow evaluation protocal used in HGR \cite{chen2020fine} with 25,991 videos for training, 1500 videos for validation and another 1500 videos for testing.

\noindent\textbf{DiDeMo} \cite{anne2017localizing} includes 10,611 videos collected from Flicker and length of each is a maximum of 30 seconds. Following \cite{liu2019use}, \cite{lei2021less}, \cite{bain2021frozen}, we treat it as a video-paragraph retrieval task that all of the captions belonging to the same video will be concatenated to form a paragraph.

\noindent\textbf{ActivityNet} \cite{krishna2017dense} is comprised of 19,994 videos downloaded from YouTube. We follow \cite{zhang2018cross}, \cite{gabeur2020multi} to concatenate all of the video related descriptions to form a paragraph for retrieval and evaluate our model on 'val1' split.

\subsection{Implementations Details}
The basic video encoder and text encoder are both initialized by CLIP \cite{radford2021clip}. Besides basic encoders, frame position embedding is initialized with position embedding used in CLIP's text encoder. Both temporal transformer and multi-modal transformer are initialized by the part of layers from text encoder in CLIP. Concretely, the width, heads, and layers are ,  and , respectively. The fixed video length is  and caption length is  for MSR-VTT \cite{xu2016msrvtt}, MSVD \cite{chen2011collecting}, VATEX \cite{wang2019vatex}. As for DiDeMo \cite{anne2017localizing} and ActivityNet \cite{krishna2017dense}, we set both video length and caption length to .  and  for similarity distillation are  and , respectively. Following \cite{luo2021clip4clip}, we finetune our model with Adam optimizer in  epochs with  batch size and  for . The learning rate is  for the basic video encoder and text encoder, and  for new layers such as frame position embedding, temporal transformer and multi-modal encoder.

\subsection{Comparison with State-of-the-Art Methods}

\begin{table}[ht]
	\begin{center}
    	\caption{Retrieval result on MSR-VTT 1kA split. \textbf{SD} means Similarity Distillation. And the CLIP2TV-ViT16 is always combined with SD. We copy the results of other methods from corresponding papers. Recall at rank (R@1), rank (R@5), rank (R@10), Median Rank(MdR) and Mean Rank(MnR) are reported.}
    	\label{tab:MSRVTT-1k}
		\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{c|c|ccccc|ccccc} \hline
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{5}{c}{Text-to-Video} &\multicolumn{5}{c}{Video-to-Text} \\
        \hline
	    Type & Method & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\
        \hline
	    \multicolumn{1}{c|}{\multirow{8}{*}{\rotatebox[origin=c]{90}{Others}}} & JSFusion \cite{yu2018joint}             & 10.2 & 31.2 & 43.2 & 13.0 & -    & -    &    - & -    &   - & - \\
	    & HT-pretrained \cite{miech2019howto100m} & 14.9 & 40.2 & 52.8 & 9.0  & -    & -    &    - & -    &   - & - \\
	    & CE \cite{liu2019use}                    & 20.9 & 48.8 & 62.4 & 6.0  & 28.2 & 20.6 & 50.3 & 64.0 & 5.3 & 25.1 \\
	    & ClipBERT \cite{lei2021less}             & 22.0 & 46.8 & 59.9 & 6.0 & - & - & - & - & - & - \\
	    & TACo \cite{yang2021taco}                & 26.7 & 54.5 & 68.2 & 4.0  & -    & -    & -    & -    & -   & - \\
	    & MMT-pretrained \cite{gabeur2020multi}   & 26.6 & 57.1 & 69.6 & 4.0  & 24.0 & 27.0 & 57.5 & 69.7 & 3.7 & 21.3 \\
	    & SUPPORT-SET \cite{patrick2020support}   & 27.4 & 56.3 & 67.7 & 3.0  & -    & 26.6 & 55.1 & 67.5 & 3.0 & - \\
	    & FROZEN \cite{bain2021frozen}            & 31.0 & 59.5 & 70.5 & 3.0  & -    & -    &    - & -    &   - & - \\
	    & HIT-pretrained \cite{liu2021hit}        & 30.7 & 60.9 & 73.2 & 2.6  & -    & 32.1 & 62.7 & 74.1 & 3.0 & - \\
	    \hline
	    \hline
	    \multicolumn{1}{c|}{\multirow{6}{*}{\rotatebox[origin=c]{90}{CLIP-based}}} & CLIP-straight \cite{portillo2021straightforward} & 31.2 & 53.7 & 64.2 & 4.0  & -    & 27.2 & 51.7 & 62.6 & 5.0 & - \\
	    & MDMMT \cite{dzabraev2021mdmmt}          & 38.9 & 69.0 & 79.7	& 2.0  & 16.5 & -    &    - & -    &   - & - \\
	    & CLIP4Clip-meanP \cite{luo2021clip4clip} & 43.1 & 70.4 & 80.8	& 2.0  & 16.2 & 43.1 & 70.5 & 81.2 & 2.0 & 12.4 \\	
        & CLIP4Clip-seqTransf \cite{luo2021clip4clip} & 44.5 & 71.4 & 81.6	& 2.0  & 15.3 & 42.7 & 70.9 & 80.6 & 2.0 & 11.6 \\
	    & CLIP2Video \cite{fang2021clip2video} & 45.6 & \textbf{72.6} & 81.7 & 2.0 & \textbf{14.6} & 43.5 & 72.3 & 82.1 & 2.0 & \textbf{10.2} \\
	    \hdashline & 70.8 & 81.1 & 2.0 & 15.4 & 45.6 & 72.3 & 83.2 & 2.0 & 11.5 \\ \cdashline{2-12}
	   \multicolumn{1}{c|}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Ours}}} & \textbf{CLIP2TV} & 45.6 & 71.1 & 80.8 & \textbf{2.0} & 15.0 & \textbf{43.9} & 70.9 & \textbf{82.2} & \textbf{2.0} & 12.0 \\
	   & \textbf{CLIP2TV+SD} & \textbf{46.1} & 72.5 & \textbf{82.9} & \textbf{2.0} & 15.2 & 43.9 & \textbf{73.0} & 82.8 & \textbf{2.0} & 11.1 \\
	   \cdashline{2-12}
	   & \textbf{CLIP2TV-ViT16} & \textbf{49.3} & \textbf{74.7} & \textbf{83.6} & \textbf{2.0} & \textbf{13.5} & \textbf{46.9} & \textbf{75.0} & \textbf{85.1} & \textbf{2.0} & \textbf{10.0} \\
    	\hline
		\end{tabular}}
	\end{center}
\end{table}

\begin{table}[t]
    \begin{minipage}{0.48\linewidth}
    \begin{center}
    \caption{MSR-VTT full}
    \label{tab:msrvtt full}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{c|ccccc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Text-to-Video} \\ \cline{2-6}
     & R@1 & R@5 & R@10 & MdR & MnR \\
    \hline
    CE \cite{liu2019use} & 10.0 & 29.0 & 41.2 & 16.0 & 86.2 \\
    HT-pretrained \cite{miech2019howto100m} & 14.9 & 40.2 & 52.8 & 9.0 & - \\
    CLIP-straight \cite{portillo2021straightforward} & 21.4 & 41.1 & 50.4 & 10.0 & - \\
    MDMMT \cite{dzabraev2021mdmmt} & 23.1 & 49.8 & 61.8 & 6.0 & 52.8 \\
    CLIP2Video \cite{fang2021clip2video} & 29.8 & 55.5 & 66.2 & 4.0 & 45.5 \\
    \hline
    \textbf{CLIP2TV} & 29.6 & 55.4 & 66.0 & 4.0 & 48.2 \\
    \textbf{CLIP2TV+SD} & \textbf{29.9} & \textbf{55.6} & \textbf{66.3} & \textbf{4.0} & 48.1 \\
    \hdashline
    \textbf{CLIP2TV-ViT16} & \textbf{32.4} & \textbf{58.2} & \textbf{68.6} & \textbf{3.0} & \textbf{43.6} \\
\hline
    \end{tabular}}
    \end{center}
    \end{minipage}
    \begin{minipage}{0.48\linewidth}
    \begin{center}
    \caption{MSVD}
    \label{tab:msvd}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{c|ccccc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Text-to-Video} \\ \cline{2-6}
     & R@1 & R@5 & R@10 & MdR & MnR \\
    \hline
    CE \cite{liu2019use} & 19.8 & 49.0 & 63.8 & 6.0 & - \\
    SUPPORT-SET \cite{patrick2020support} & 28.4 & 60.0 & 72.9 & 4.0 & - \\
    Frozen \cite{bain2021frozen} & 33.7 & 64.7 & 76.3 & 3.0 & - \\
    CLIP-straight \cite{portillo2021straightforward} & 37.0 & 64.1 & 73.8 & 3.0 & - \\
    CLIP4Clip-meanP \cite{luo2021clip4clip} & 46.2 & 76.1 & 84.6 & 2.0 & 10.0 \\
    CLIP4Clip-seqTransf \cite{luo2021clip4clip} & 45.2 & 75.5 & 84.3 & 2.0 & 10.3 \\
CLIP2Video \cite{fang2021clip2video} & \textbf{47.0} & \textbf{76.8} & \textbf{85.9} & 2.0 & \textbf{9.6} \\
    \hline
    \textbf{CLIP2TV} & 46.3 & 76.1 & 85.3 & 2.0 & 10.0 \\
    \textbf{CLIP2TV+SD} & \textbf{47.0} & 76.5 & 85.1 & 2.0 & 10.1 \\
    \hdashline
    \textbf{CLIP2TV-ViT16} & \textbf{50.2} & \textbf{79.8} & \textbf{87.9} & \textbf{1.0} & \textbf{8.6} \\
\hline
    \end{tabular}}
    \end{center}
    \end{minipage}
\end{table}

\begin{table}[t]
    \begin{minipage}{0.48\linewidth}
    \begin{center}
    \caption{DiDeMo}
    \label{tab:didemo}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{c|ccccc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Text-to-Video} \\ \cline{2-6}
     & R@1 & R@5 & R@10 & MdR & MnR \\
    \hline
    CE \cite{liu2019use} & 16.1 & 41.1 & - & 8.3 & 43.7 \\
    ClipBERT \cite{lei2021less} & 20.4 & 48.0 & 60.8 & 6.0 & - \\
    Frozen \cite{bain2021frozen} & 34.6 & 65.0 & 74.7 & 3.0 & - \\
    CLIP4Clip-meanP \cite{luo2021clip4clip} & 43.4 & 70.2 & 80.6 & 2.0 & 17.5 \\
    CLIP4Clip-seqTransf \cite{luo2021clip4clip} & 43.4 & 69.9 & 80.2 & 2.0 & 17.5 \\
    CLIP4Clip-meanP* \cite{luo2021clip4clip} & 42.1 & 69.3 & 79.6 & 2.0 & 18.0 \\
    \hline
    \textbf{CLIP2TV} & 43.9 & \textbf{70.5} & 79.8 & 2.0 & \textbf{16.6} \\
    \textbf{CLIP2TV+SD} & \textbf{45.5} & 69.7 & \textbf{80.6} & \textbf{2.0} & 17.1 \\
    \hline
    \end{tabular}}
    \end{center}
    \end{minipage}
    \begin{minipage}{0.48\linewidth}
    \begin{center}
    \caption{ActivityNet}
    \label{tab:activitynet}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{c|ccccc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Text-to-Video} \\ \cline{2-6}
     & R@1 & R@5 & R@50 & MdR & MnR \\
    \hline
    CE \cite{liu2019use} & 18.2 & 47.7 & 91.4 & 6.0 & 23.1 \\
    ClipBERT \cite{lei2021less} & 21.3 & 49.0 & - & 6.0 & - \\
    SUPPORT-SET \cite{patrick2020support} & 26.8 & 58.1 & 93.5 & 3.0 & -\\
    MMT-pretrained \cite{gabeur2020multi} & 28.7 & 61.4 & 94.5 & 3.3 & 16.0 \\
    HIT-pretrained \cite{liu2021hit} & 29.6 & 60.7 & 95.6 & 3.0 & - \\
    CLIP4Clip-meanP \cite{luo2021clip4clip} & 40.5 & 72.4 & 98.1 & 2.0 & 7.4 \\
    CLIP4Clip-seqTransf \cite{luo2021clip4clip} & 40.5 & 72.4 & 98.2 & 2.0 & 7.5 \\
    CLIP4Clip-meanP* \cite{luo2021clip4clip} & 40.0 & 71.2 & 98.0 & 2.0 & 7.7 \\
    \hline
    \textbf{CLIP2TV} & 40.8 & 72.9 & 98.0 & 2.0 & 7.3 \\
    \textbf{CLIP2TV+SD} & \textbf{44.1} & \textbf{75.2} & \textbf{98.4} & \textbf{2.0} & \textbf{6.5} \\
    \hline
    \end{tabular}}
    \end{center}
    \end{minipage}
\end{table}

\begin{table}[t]
    \begin{center}
    \caption{VATEX. CLIP4Clip-seqTransf* means there are no results about VATEX of HGR split reported in \cite{luo2021clip4clip} and \cite{fang2021clip2video}, so we conduct it by ourselves.}
    \label{tab:vatex}
    \footnotesize
    \setlength{\tabcolsep}{6pt}{
    \begin{tabular}{c|ccccc}
    \hline
    Method & R@1 & R@5 & R@10 & MdR & MnR \\
    \hline
    CLIP-straight \cite{portillo2021straightforward} & 39.7 & 72.3 & 82.2 & 2.0 & 12.8 \\
    SUPPORT-SET \cite{patrick2020support} & 44.9 & 82.1 & 89.7 & 1.0 & - \\
    CLIP4Clip-seqTransf* \cite{luo2021clip4clip} & 60.3 & 89.1 & 94.4 & 1.0 & 4.5 \\
    CLIP2Video \cite{fang2021clip2video} & 61.2 & 90.9 & 95.6 & 1.0 & \textbf{3.4} \\
    \hline
\textbf{CLIP2TV} & 61.4 & 90.6 & 95.2 & 1.0 & 3.7 \\
    \textbf{CLIP2TV+SD} & \textbf{61.5} & \textbf{90.9} & \textbf{95.6} & 1.0 & 3.7 \\
    \hdashline
    \textbf{CLIP2TV-ViT16} & \textbf{65.4} & \textbf{92.7} & \textbf{96.6} & 1.0 & \textbf{2.9} \\
\hline
    \end{tabular}}
    \end{center}
\end{table}

We compare our model with other state-of-the-art methods. Table.\ref{tab:MSRVTT-1k} shows the text-to-video and video-to-text retrieval results of CLIP2TV on MSR-VTT 1kA.
As non-CLIP-based methods are usually trained beyond feature-level, which means models adopt video frame features extracted by off-the-shelf pretrained visual models as inputs, they cannot exceed the zero-shot performance of CLIP which is trained end-to-end with much more data.
We categorize these approaches into CLIP-based and others for a fair comparison.
On MSR-VTT 1kA, we achieve state-of-the-art results on both text-to-video and video-to-text retrieval.
Upon strong baseline of CLIP4clip, CLIP2TV improves R1 of text-to-video to 45.6, and R1 of video-to-text to 43.9.
CLIP2TV+SD shows the results with similarity distillation. It further improves CLIP2TV on nearly on all metrics.
Particularly, when replace ViT-32 with ViT-16, we can acquire higher performance(with 3.2\% improvement in text-to-video retrieval at R@1).

Table.\ref{tab:msrvtt full}-Table.\ref{tab:vatex} present the text-to-video retrieval results on MSR-VTT full, MSVD, DiDeMo, ActivityNet and VATEX. For DiDeMo and ActivityNet, we reproduce results based on the provided code of CLIP4Clip and denote as CLIP4Clip-meanP*. On MSR-VTT full, MSVD, and VATEX, we obtain comparable performance to previous SOTA CLIP2Video with ViT32.
On MSVD, though the temporal transformer hardly learns temporal representation with very few frames \cite{luo2021clip4clip,fang2021clip2video}, we still boost the performance close to CLIP2Video.
We also note that on VATEX, similarity distillation boosts the performance with small margin. This is mainly due to the reason that captions in VATEX are relatively detailed and distinct, which just validates our claim.
Our method outperforms previous SOTA methods by a large margin on paragraph-to-video retrieval on DiDeMo and ActivityNet, which further demonstrates our superiority on longer videos with longer descriptions.
Since our method is orthogonal to CLIP2Video, it is expected that combining CLIP2TV with CLIP2Video might further improve the results. We leave this for future study.

\subsection{Ablation Study}

\begin{table}[ht]
    \begin{center}
    \caption{Evaluation of Multi-Modal Fusion with vanilla vtm on different datasets. Recall at rank  (R@1) and Mean Rank (MnR) of text-to-video retrieval are reported. M-V and Anet mean MSR-VTT and ActivityNet, respectively. The underline indicates that retrieval results are inferred from this module.}
\label{tab:ablation on vtm}
    \setlength{\tabcolsep}{2.5pt}{
    \begin{tabular}{cc|cc|cc|cc|cc|cc|cc}
        \hline
        \multicolumn{1}{c}{\multirow{2}{*}{vta}} & \multicolumn{1}{c|}{\multirow{2}{*}{vtm}} & \multicolumn{2}{c|}{M-V 1kA} & \multicolumn{2}{c|}{M-V full} & \multicolumn{2}{c|}{MSVD} & \multicolumn{2}{c|}{VATEX} & \multicolumn{2}{c|}{DiDeMo} & \multicolumn{2}{c}{Anet} \\
         & & R@1 & MnR & R@1 & MnR & R@1 & MnR  & R@1 & MnR & R@1 & MnR & R@1 & MnR \\
         \hline\hline
        \checkmark &   & 43.8 & 15.8 & 29.2 & 49.5 & 45.8 & 11.1 & 60.3 & 4.5 & 41.6 & 18.9 & \textbf{42.2} & \textbf{7.4} \\
         & \checkmark & 30.4 & 67.4 & 17.3 & 357.4 & 31.7 & 16.1 & 56.2 & 4.0 & 16.7 & 53.2 & 0.2 & 1851.2 \\
        \uline{\checkmark} & \checkmark & \textbf{45.6} & \textbf{15.0} & \textbf{29.6} & \textbf{48.2} & \textbf{46.3} & \textbf{10.0} & \textbf{61.4} & \textbf{3.7} & \textbf{43.9} & \textbf{16.6} & 40.8 & \textbf{7.3} \\
\checkmark & \uline{\checkmark} & 39.9 & - & 26.2 & - & 37.2 & - & 59.0 & - & 34.6 & - & 33.5 & - \\
        \hline
    \end{tabular}}
    \end{center}
\end{table}

\textbf{Why alignment with matching?} To evaluate the effectiveness of alignment and matching, we test various combinations of vta and vtm. The ablation results are presented in Table.\ref{tab:ablation on vtm}. 
The 1st row shows the result of vta without vtm. It can be seen as fine-tuning CLIP on other datasets. The result is still competitive due to the outstanding ability of CLIP pretrained on huge data.

However, as shown in the 2nd row of Table.\ref{tab:ablation on vtm}, when we remove the alignment part and train our model with only vtm, the result is severely degraded. Worse on ActivityNet, the training is totally decayed. We attribute this catastrophe to the nature of the dataset. On the one hand, videos in ActivityNet are usually longer than 60 seconds, and some videos can be as long as 100 seconds or even more than 200 seconds, so that input frames with fixed length of 64 cannot cover all the scenes described in paragraph. On the other hand, the paragraph paired with a video consists of several sentences only describing the corresponding video segments, while such video segments may be discontinuous in the video, which means some input frames sampled from the interval of two segments are invalid. These two aspects make it difficult for multi-modal encoder to model the relationship between text words and video frames. This justifies the necessity of aligning two modalities before sending them to multi-modal transformer.

The 3rd row is CLIP2TV combining vta with vtm, retrieving the result from vta. The result shows that it consistently outperforms vta except on ActivityNet.
The last row shares the same structure with the 3rd row, but retrieves the result from vtm. For inference efficiency, we only select top-200 candidates from vta as inputs to vtm for re-ranking. Since the ground truth target might not be in the 200 candidates, the mean ranking of ground truth will be absent from vtm in such case, therefore we neglect MnR results here. We can see that inference result from vtm is much worse than it from vta. This observation is different from \cite{li2021align} on image-text retrieval. We speculate the reason is vtm pretrained on images and texts lacks enough videos for further pretrain.

To sum up, vta and vtm work in a coordinated manner and benefit each other. The alignment is essential and vta can generate hard negatives to vtm. In reverse, vtm tunes vta more finely via back propagating gradients. Moreover, since we retrieve the targets using vta, CLIP2TV avoids computing candidates in vtm with the heavy load, and is practically friendly.

\noindent\textbf{What is the best negative sampling strategy for vtm?} As we know, negative samples plays a crucial role in contrastive learning. Will a larger number of negatives bring benefits as in self-supervised learning? With these questions, we design thorough experiments to find the answer. We choose MSR-VTT 1kA with noisy captions and VATEX with more detailed descriptions. We use  to denote number of negatives. Results shown in Table.\ref{tab:ablation on K} demonstrate that with the increase of , it brings more noise and ambiguity in pairs which hurts the performance. The model decays or saturates when the number of negative samples reaches a certain amount. As MSR-VTT has more noisy samples, a small  is good for it, while VATEX can support larger  value.

\begin{table}[ht]
    \begin{center}
    \caption{Evaluation of the number of negative pairs . We report our experiment results on MSR-VTT 1kA and VATEX.}
    \label{tab:ablation on K}
    \setlength{\tabcolsep}{3.0pt}{
    \begin{tabular}{c|ccccc|ccccc}
        \hline
        \multicolumn{1}{c|}{\multirow{2}{*}{K}} & \multicolumn{5}{c|}{MSR-VTT 1kA} & \multicolumn{5}{c}{VATEX} \\
         & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\
         \hline\hline
        4 & 45.1 & 71.1 & 81.7 & 2.0 & 15.3 & 60.7 & 90.4 & 95.4 & 1.0 & 3.7 \\
        \hline
        8 & 45.6 & 71.1 & 80.8 & 2.0 & 15.0 & 61.4 & 90.6 & 95.2 & 1.0 & 3.7 \\
        \hline
        16 & 44.5 & 71.0 & 81.1 & 2.0 & 14.5 & 61.3 & 90.6 & 95.4 & 1.0 & 3.8 \\
        \hline
    \end{tabular}}
    \end{center}
    \vspace{-6mm}
\end{table}

\noindent\textbf{How to distill similarity for vtm?} 
We show performance of three variants of similarity distillation on MSR-VTT 1kA in Table.\ref{tab:ablation on soft label}. 
We can see that all three variants can improve the results above vanilla vtm.
Specifically, results from joint and intra are nearly the same, and intra is slightly better than inter. This implies that intra-modal similarity reflects more accurate relative distances between an anchor representation of text/video, towards a group of video/text representations. Moreover, joint inner-modal metric from video and text is more accurate than single modal. To better illustrate the difference of intra- and inter-modal similarity, we show similarity matrices in Figure.\ref{fig:heatmap}, we can see that overall similarity values are gradually increasing along (a)text-video, (b)video-video, and (c)text-text. (d) is the average similarity of text-text and video-video. 

In Table.\ref{tab:ablation on vtm type}, we show the results of vanilla vtm, soft-vtm and mean of them by changing the value of , across datasets of MSR-VTT, DiDeMo and ActivityNet. Since these datasets contain relatively more data noise, thus similarity distillation contributes more on them. Also, combining vanilla vtm and soft-vtm achieves the best results. We leave more choices of  for future study.


\begin{table}[ht]
    \begin{center}
    \caption{Evaluation of soft-label variants. We report our experimental results on the MSR-VTT 1kA.}
    \label{tab:ablation on soft label}
    \setlength{\tabcolsep}{10.0pt}{
    \begin{tabular}{c|ccccc}
        \hline
        soft-label & R@1 & R@5 & R@10 & MdR & MnR \\
        \hline
        inter & 45.8 & 72.5 & 81.6 & 2.0 & 14.6 \\
        \hline
        intra & 45.9 & 72.3 & 81.5 & 2.0 & 15.3 \\
        \hline
        joint & 46.1 & 72.5 & 82.9 & 2.0 & 15.2 \\
        \hline
    \end{tabular}}
    \end{center}
\end{table}

\begin{table}[ht]
    \begin{center}
    \caption{Evaluation of soft vtm. We report our experimental results on MSR-VTT 1kA, DiDeMo and ActivityNet.}
    \label{tab:ablation on vtm type}
\resizebox{\textwidth}{!}{
    \begin{tabular}{c|cccc|cccc|cccc}
        \hline
        \multicolumn{1}{c|}{\multirow{2}{*}{vtm}} & \multicolumn{4}{c|}{MSR-VTT 1kA} & \multicolumn{4}{c|}{DiDeMo} & \multicolumn{4}{c}{ActivityNet} \\
         & R@1 & R@5 & R@10 & MnR & R@1 & R@5 & R@10 & MnR & R@1 & R@5 & R@50 & MnR \\
        \hline\hline
        vanilla & 45.6 & 71.1 & 80.8 & 15.0 & 43.9 & 70.5 &79.8 & 16.6 & 40.8 & 72.9 & 98.0 & 7.3 \\
        \hline
        soft() & 44.8 & 71.8 & 82.3 & 15.2 & 42.7 & 68.3 & 79.3 & 17.5 & 43.4 & 74.7 & 98.1 & 6.8 \\
        \hline
        soft() & 46.1 & 72.5 & 82.9 & 15.2 & 45.5 & 69.7 & 80.6 & 17.1 & 44.1 & 75.2 & 98.4 & 6.5 \\
        \hline
    \end{tabular}}
    \end{center}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{heatmap.pdf}
    \caption{Visualized similarity matrix. (a) text-video similarity. (b) text-text similarity. (c) video-video similarity. (d) joint-modal similarity which is average of text-text and video-video similarity.}
    \label{fig:heatmap}
\end{figure}

\subsection{Qualitative Results}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{qualitative.pdf}
    \caption{Visualized text-to-video retrieval results on MSR-VTT 1kA. The 1st row is the query, and the 2nd row is the corresponding ground-truth video followed by ranks acquired from CLIP4Clip, CLIP2TV, and CLIP2TV+SD, respectively. The 3rd-5th rows are the retrieved top-1 video by each method and its paired caption. The word ``judes'' in red is a typo.}
    \label{fig:qualitative}
\end{figure}

To illustrate the advantage of the proposed method and the impact of noise, we visualize some text-to-video retrieval results on MSR-VTT 1kA shown in Fig.\ref{fig:qualitative}. The 1st row is the text for query, and the 2nd row is the corresponding ground-truth video followed by its retrieval rank obtained from CLIP4Clip, CLIP2TV, and CLIP2TV+SD, respectively. The 3rd-5th rows present the retrieved R@1 video by each method and its paired caption. 

As shown in Fig.~\ref{fig:qualitative}, our proposed CLIP2TV can perform better and obtain more accurate retrieval results than CLIP4Clip. With the help of similarity distillation from joint soft-label on vtm module, CLIP2TV+SD can make a great improvement when querying a detailed text. On the other hand, CLIP2TV+SD is more robust to vague (3rd column of Fig.\ref{fig:qualitative}) and typo (4th column of Fig.\ref{fig:qualitative}) query, which illustrates that  similarity distillation can alleviate the negative effects of data noise.