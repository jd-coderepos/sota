\begin{table*}[t]
\centering
    \begin{tabular}{l @{\hspace{0.8em}}|@{\hspace{0.8em}} c @{\hspace{1em}} c @{\hspace{1em}} c @{\hspace{1em}} c}  \setlength{\tabcolsep}{12pt}
 
   & Proxy Exists? & Caching & Rewriting & Redirection  \\ \hline
    AT\&T & \checkmark &  &  &   \\ 
    {\hspace{0.8em}}Black Wireless & \checkmark &  & \checkmark & \\ 
    T-Mobile &  & &  &   \\ 
    {\hspace{0.8em}}Simple Mobile &  &  &  & \\  
    Sprint & \checkmark & \checkmark & \checkmark &   \\ 
    {\hspace{0.8em}}Boost & \checkmark  & \checkmark  & \checkmark  &   \\ 
    Verizon & \checkmark &  &  &  \\ 
    Rogers Wireless &  &  &  & \\ 
    3 (Hutchison) &  &  &  & \\ 
    China Mobile &  &  &  &   \\ 
    China Unicom &  &  &  &   \\ 
    \end{tabular}  
\caption{\textbf{Web proxy deployments observed in our study.} Several popular carriers in the US---AT\&T, Verizon, Sprint, Black Wireless, and Boost---use proxies. However, we found no evidence of proxies for the carriers we measured in Canada, Austria, and China . We also explore how MVNO Web proxy deployments compare with their host network (MVNOs are indented below their host network). We looked at Black Wireless (uses AT\&T), Simple Wireless (uses T-Mobile), and Boost (uses Sprint). All MVNOs we measured except Black Wireless support the same Web proxy features as their host network. Black Wireless deploys a compression proxy while AT\&T does not.}
\label{tab:summaryresults}
\vspace{-2em}
\end{table*}
\vspace{-1em}
\section{Dataset and Methodology}
In this section, we describe the dataset we use to develop and validate our proxy detection approach, as well as to 
understand proxy deployments in the wild. Our study has the following goals. First, we want to determine if a Web proxy exists in a cellular network and how it affects different destinations. Second, we want to enable these measurements from unprivileged devices, allowing average users to help characterize proxy deployments worldwide. Last, we want to understand the impact of these proxies on client-perceived performance.

\noindent\textbf{Dataset.} 
To validate our approach and to characterize US networks, we collected data using controlled experiments from 
Sprint, Verizon, T-Mobile, AT\&T, Boost, Simple Wireless, and Black Wireless.
This dataset was collected between July, 2015 and October, 2015, and comprises more than two million measurement 
samples, from which we selected 500 complete set of measurement to present here. 
The destinations of our traffic are top Web sites according to Alexa~\cite{topweb}, in addition to a server under our control 
running in EC2, which we use for validation of our methodology.

In addition, we released an Android app that implements our approach on Google Play.\footnote{{https://play.google.com/store/apps/details?id=edu.northeastern.ccs.proxydetect}}
Users who downloaded and ran this app provided 23 sets of measurements (260 samples) from 11 networks. 
The study was conducted with IRB approval (NEU IRB\#15-04-07); users are consented before participation and absolutely 
no personal information is collected.

\vspace{-1em}
\subsection{Web Proxy Detection Methodology}
A key challenge for our work is detecting proxies \emph{without access to low-level packet traces, nor access to Web servers}. 
Our methodology must therefore provide a way to infer that there is a Web proxy based only at information gleaned from 
the application layer at the client.  Achieving this goal requires us to address two problems: 1) controlling when traffic 
is subject to proxy interposition, and 2) detecting the impact of this interposition. 

To address the first problem, we rely on results from previous studies (and reconfirmed in our own study), that Web 
proxies operate on traffic only to certain ports (\eg, port 80 for HTTP), but not others (\eg, port 443 for HTTPS). We 
thus run back-to-back experiments to fetch the same Web page over HTTP and HTTPS from the same server,\footnote{As such, our methodology is limited to sites that support both protocols, which is common among top Web sites.} where the latter is not 
subject to proxy interposition. To address the second problem, we use a combination of features that include 
latency differences and content modification. In the following subsections, we provide details about each test.

\vspace{-1em}
\subsubsection{Proxy Existence Test.}

\begin{figure}[tb]
\centering
\includegraphics[width=1\columnwidth]{figure/Diagram.pdf}
\caption{{Diagram depicting a typical Web proxy that splits TCP connections on port 80 (HTTP) but not on port 443 (HTTPS). The Web proxy sends a SYN/ACK immediately in response to a client's SYN on port 80, but does not do so on port 443. Thus,  should be less than  if a Web proxy is present. }} 
\vspace{\postfigspace}

\label{fig:Proxy Detection.}
\end{figure}
We use a latency-based approach to infer the existence of Web proxies. Specifically, for each destination we create a TCP socket, and measure round-trip time (RTT) between the SYN and SYN/ACK as measured by the client. We do this on port 80 and port 443 in back-to-back measurements, then repeat this pair of measurements multiple times to account for noise in cellular network performance.\footnote{We also fix the IP address of the destination Web server to ensure that we contact the same host on both ports.} Based on our observations, a Web proxy will send a SYN/ACK to client's SYN immediately; however, a SYN packet sent to the same server on 443 will bypass the Web proxy and the reply will come directly from the destination server. 

We thus compare  with . We assume that the Web proxy is located somewhere on the path between client and Web server, and so if a Web proxy exists in the network, then  should be less than . Otherwise, the RTT measured on these two ports should be nearly identical.  

Because cellular network latency can vary significantly over time due to channel conditions and congestions, it is important to use multiple samples. However, there is a trade-off between increasing the number of samples and minimizing the data consumed by our tests for users running our experiments. To explore this issue, we investigated the impact of the number of tests on system accuracy (not shown). When varying the number of tests from 2 to 10 we found that 4 probes was the best number for accurately identifying proxies even in the presence of noise while minimizing the number of tests.

To understand the impact of proxies on popular Internet destinations, we selected Web sites from Alexa's top 100 global sites~\cite{topweb}. However, there are two challenges when selecting sites to use for identifying proxies. First, to achieve good QoE for Web users, many destination servers are already placed close to (or inside of) mobile networks, meaning the proxy and destination server paths are nearly identical. Using only such sites in our measurements can result in false negatives, since  will be similar to  (we explore this in Fig.~\ref{fig:detection-bad}). 

Second, noise in the cell network due to congestion, signal strength, or other factors can impact our inference by causing latency to vary significantly between tests. To reliably identify proxy behavior, we thus need to select destinations that are relatively ``far'' from the client, much more so than any variability in performance due to the network. 

To account for this noise, we propose the following dynamic approach that adjusts to the variance in any given measurement environment. We first measure    for the 100 top Web sites once. We then use the standard deviation (SD) of across RTT measurements, and use 2*SD as a threshold to exclude destinations that are ``too close'' to the network to be used for proxy inference. Specifically, for each destination, we compare  with 2*SD, and see if the former is larger than the latter. We remove the websites whose  is less than 2*SD.

When testing for a proxy, our client sets up TCP sockets with target Web servers on port 80 and port 443.  We use 4 probes for each test, \ie, for each destination we gather four pairs of  and . For each pair, we compute the difference between  and . After that, we compute the standard deviation (SD) of these four difference values. For each site, compare the average difference and the SD. If the difference value is greater than zero and is greater than SD, we infer that is a Web proxy in the network. 

We validated this approach for all US carriers in our study, using tests against a controlled server located in Amazon EC2 in Virginia. We found that our inference technique yielded identical results.

\vspace{-1em}
\subsubsection{Caching.}

To detect caching at Web proxies, we make the assumption that the fetch time for content from cache is smaller than from the origin server, based on the assumption that the cache is closer to the client in terms of latency. We thus use a latency-based methodology to infer whether there is a caching proxy in the network, identifying caching based on differences in fetch times when repeatedly fetching cacheable content. 

Specifically, we send two consecutive HTTP GETs to fetch the same (cacheable) object twice. Assuming an empty cache at the start of the experiment, and that the Web proxy caches the fetched content, the fetched object will be cached after the first HTTP GET. In this case, the second HTTP GET will be served from cache. We then compute the difference in download times for the two GETs ( and ) and infer that an object was cached if  is statistically significantly larger than . 

Because a given object may have been cached by a previous fetch, we use multiple sources of content (some of which should not be cached) and ensure that our experiments leave sufficient time between pairs of GETs to ensure that any cached content expires.  We validated this approach with our controlled server, confirming that each entries expire. For the two carriers that deployed Web caches (Sprint and Boost), we found that cache entries will expire after about 5 minutes.


We dynamically determine which objects to fetch to test for caching by downloading the index page for popular Web sites and extracting URLs for embedded objects. 
We found that Web caches do not cache all file types. To explore this behavior, we include CSS, JavaScript, JPG, PNG, GIF and HTML files in our experiment because they are commonly embedded in Web pages.


Similar to the proxy detection scenario, we may not detect a proxy's caching behavior if the cache and origin server are relatively near each other and the file sizes are small. Thus, we reuse the ``far'' websites which we include in proxy detection experiment for URL extraction, and we exclude small objects (less than 5 KB).


\vspace{-1em}
\subsubsection{Object Rewriting.}
To detect object rewriting, we rely on the fact that a Web proxy cannot modify HTTPS traffic without breaking encryption, but is free to do so with HTTP traffic. We thus compare the results of fetching the same Web object over HTTPS and HTTP to determine if content modification on plaintext traffic. 

We use a similar approach to the cache-detection methodology for identifying objects to fetch, using embedded objects on popular Web sites \emph{and ensuring that servers hosting the content support both HTTP and HTTPS}. We fetch each object over HTTP (port 80) and HTTPS (port 443), and compare the contents of the fetched objects to identify rewriting. 

It is, of course, possible that Web content fetched from port 80 and 443 is not identical, and thus differences in content are not necessarily due to rewriting. To exclude this false positive, we use a server we control to verify wether the Web content differs when fetched using HTTP and HTTPS, based on the assumption that such behavior will not change based on the client's location. If the content is the same over both protocols for our control server, but they are different when fetched over a cellular network, we conclude that a Web proxy is rewriting HTTP content. 

We include CSS, JavaScript, PNG, GIF, JPG, and HTML files in the experiment to explore what file types are rewritten and how. Our approach allows us to detect behaviors such as transcoding, compression, header modification, and content injection.



\vspace{-1em}
\subsubsection{Redirection.}

Xu et al.~\cite{xu-transparent-proxies} showed that some Web proxies ignore the IP address provided HTTP GET requests, and instead perform a new DNS lookup to determine the destination IP based on resolving domain name in Host: field of HTTP GET request header. In this case, an HTTP flow may be sent to a different server than specified by the client. To detect such redirection behavior, we were unable to use latency-based techniques effectively so we use two servers under our control, E1 and E2, each having a different IP address. We modify the HTTP GET header content, putting the domain name of E1 in the {\tt Host:} field and sending the GET request to E2 (providing the IP address of E2 in the IP header). 

We do this both for E1 and E2, and check which server receives the HTTP request. If E1 receives the GET request, it indicates that a Web proxy redirected the GET request to E2 by resolving domain name of E1 into the IP address of E1. Otherwise, the proxy does not do redirection.