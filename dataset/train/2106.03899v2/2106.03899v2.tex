\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  




\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage{xcolor,colortbl}
\colorlet{newColor}{green!5!orange!95!}

\newcommand{\TODO}[1]{\textcolor{red}{(#1)}}
\newcommand{\sina}[1]{\textcolor{blue}{#1}}
\newcommand{\AV}[1]{{\color{teal}[AV: #1]}}
\newcommand{\reviseda}[1]{\textcolor{teal}{#1}}

\usepackage{wrapfig,lipsum}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{pifont}


\usepackage{makecell}

   
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsmath}



\usepackage{textcomp}
\usepackage{siunitx}

\newcommand{\rulesep}{\unskip\ \vrule\ }
\newcommand{\blambda}{{\boldsymbol{\lambda}}}
\newcommand{\bLambda}{{\boldsymbol{\Lambda}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}}


\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}



\setcounter{secnumdepth}{0} 





\title{Shifting Transformation Learning for Out-of-Distribution Detection} \author{
    Sina Mohseni, 
    Arash Vahdat, 
    Jay Yadawa
}
\affiliations{


NVIDIA\\
    USA\\
\{smohseni, avahdat, jyadawa\}@nvidia.com
}

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author{
First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations{
\textsuperscript{\rm 1} Sina Mohseni\\
    \textsuperscript{\rm 2} Arash Vahdat\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi



\usepackage{bibentry}



\begin{document}

\maketitle





\begin{abstract}
Detecting out-of-distribution (OOD) samples plays a key role in open-world and safety-critical applications such as autonomous systems and healthcare. Recently, self-supervised representation learning techniques (via contrastive learning and pretext learning) have shown effective in improving OOD detection. However, one major issue with such approaches is the choice of shifting transformations and pretext tasks which depends on the in-domain distribution.
In this paper, we propose a simple framework that leverages a \textit{shifting transformation learning} setting for learning multiple shifted representations of the training set for improved OOD detection. To address the problem of selecting optimal shifting transformation and pretext tasks, we propose a simple mechanism for automatically selecting the transformations and modulating their effect on representation learning without requiring any OOD training samples. In extensive experiments, we show that our simple framework outperforms state-of-the-art OOD detection models on several image datasets. We also characterize the criteria for a desirable OOD detector for real-world applications and demonstrate the efficacy of our proposed technique against state-of-the-art OOD detection techniques. 
\end{abstract}


 


\section{Introduction}
\label{sec:intro}

Despite advances in representation learning and their generalization to unseen samples, learning algorithms are bounded to perform well on source distribution and vulnerable to out-of-distribution (OOD) or outlier samples.
For example, it has been shown that the piece-wise linear decision boundaries in deep neural network (DNN) with ReLU activation are prune to OOD samples as they can assign arbitrary high confidence values to samples away from the training distribution \cite{hein2019relu}. 
Recent work on machine learning trustworthiness and safety have shown that
OOD detection plays a key role in open-world and safety-critical applications such as autonomous systems \cite{mohseni2019practical} and healthcare \cite{ren2019likelihood}. 
However, OOD detection in high dimensional domains like image data is a challenging task and often requires great computational resource~\cite{gal2016dropout}. 

The recent surge in self-supervised learning techniques shows that learning pretext tasks can result in better semantic understanding of data by learning invariant representations~\cite{dosovitskiy2014discriminative} and can increase model performance in different setups \cite{gidaris2018unsupervised}.
Self-supervised learning has also been shown effective in OOD detection. For example, \citet{golan2018deep} and \citet{Hendrycks2019sv} show that simple geometric transformations improve OOD detection performance, and \citet{tack2020csi} leverage shifting data transformations and contrastive learning for OOD detection. However, these works manually design the transformations and pretext tasks. 


Inspired by the recent works, we study the impact of representation learning on OOD detection when training a model on artificially transformed datasets. We observe that training on a diverse set of dataset transformations jointly, termed as \textit{shifting transformation learning} here, further improves the model's ability to distinguish in-domain samples from outliers.
However, we also empirically observe that the choice of effective data transformations for OOD detection depends on the in-domain training set. In other words, the set of transformations effective for one in-domain dataset may no longer be effective for another dataset.



To address this problem, we make the following contributions in this paper:
\textbf{(i)} We propose a simple framework for transformation learning from multiple shifted views of the in-domain training set in both self-supervised and fully-supervised settings (when data labels are available) for OOD detection.
\textbf{(ii)} We propose a framework that selects effective transformation and modulates their impact on representation learning. We demonstrate that the optimally selected transformations result in better representation for both main classification and OOD detection compared to data augmentation-based approaches.
\textbf{(iii)} We propose an ensemble score for OOD detection that leverages multiple transformations trained with a shared encoder. In particular, our technique achieves new state-of-the-art results in OOD detection on multi-class classification by improving averaged area under the receiver operating characteristics (AUROC) +1.3\% for CIFAR-10, +4.37\% for CIFAR-100, and +1.02\% for ImageNet-30 datasets. 
\textbf{(iv)} To the best of our knowledge, this paper is the first to introduce criteria for ideal OOD detection and to analyze a diverse range of techniques along with these criteria. Albeit the simplicity, we show that our proposed approach outperforms the state-of-the-art techniques on robustness and generalization criteria. 


 


\section{Related Work}
\label{sec:background}
Here, we review OOD detection methods related to this work:

\paragraph{Distance-based Detection:} 
Distance-based methods use different distance measures between the unknown test sample and source training set in the representation space. 
These techniques involve preprocessing or test-time sampling of the source domain distribution to measure their averaged distance to the novel input sample.  
The popular distance measures include Mahalanobis distance \cite{lee2018simple,sehwag2021ssd}, cosine similarity \cite{techapanurak2020hyperparameter,tack2020csi} and others semantic similarity metrics \cite{rafiee2020unsupervised}.
These techniques usually work well with unlabeled data in unsupervised and one-class classification setups. 
For example, \citet{ruff2018deep} present a deep learning one-class classification approach to minimize the representation hypersphere for source distribution and calculate the detection score as the distance of the outlier sample to the center of the hypersphere. 
Recently, \citet{mukhoti2021deterministic} proposed using distance measures for model features to better disentangle model uncertainty from dataset uncertainty. 
Distance-based methods can benefit from ensemble measurements over input augmentations \cite{tack2020csi} or transformations \cite{bergman2020classification}, network layers \cite{lee2018simple,sastry2019detecting}, or source domain sub-distributions \cite{oberdiek2020detection} to improve detection results.
For instance, \citet{tack2020csi} present a detection score based on combining representation norm with cosine similarity between the outlier samples and their nearest training samples for one-class classification problem. 
They also show that OOD detection can be improved with ensembling over random augmentations, which carries a higher computational cost. 



\paragraph{Classification-based Detection:}
These OOD detection techniques avoid costly distance-based and uncertainty estimation techniques (e.g., \citet{gal2016dropout}) by seeking effective representation learning to encode normality together with the main classification task. 
Various detection scores have been proposed including maximum softmax probability \cite{hendrycks2016baseline}, maximum logit scores \cite{hendrycks2019benchmark}, prediction entropy \cite{mohseni2020ood}, and KL-divergence score \cite{Hendrycks2019sv}. 
To improve the detection performance, \cite{lee2017training,hsu2020generalized} proposed a combination of temperature scaling and adversarial perturbation of input samples to calibrate the model to increase the gap between softmax confidence for the inlier and outlier samples. 
Another line of research proposed using auxiliary unlabeled and disjoint OOD training set to improve OOD detection for efficient OOD detection without architectural changes \cite{hendrycks2018deep,mohseni2020ood}. 

Recent work on self-supervised learning shows that adopting pretext tasks results in learning more invariant representations and better semantic understanding of data \cite{dosovitskiy2014discriminative} and which significantly improves OOD detection \cite{golan2018deep}. 
Hendrycks et al. \cite{Hendrycks2019sv} extended self-supervised techniques with a combination of geometric transformation prediction tasks. 
Self-supervised contrastive training \cite{chen2020simple} is also shown to be effective to leverage from multiple random transformations to learn in-domain invariances, resulting in better OOD detection \cite{winkens2020contrastive,sehwag2021ssd,tack2020csi}.  





\begin{figure}[!t]
\begin{center}
    \begin{subfigure}[b]{0.49\columnwidth}
        \centerline{\includegraphics[width=0.99\columnwidth]{figures/tsne-c10-places-baseline.png}}
        \captionsetup{justification=centering} \caption{Supervised Learning} \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \centerline{\includegraphics[width=0.99\columnwidth]{figures/tsne-c10-places-ens-feats.png}}
        \captionsetup{justification=centering}
        \caption{Shifting Transformations}
    \end{subfigure}
\caption{The t-SNE visualization of the penultimate layer features in a ResNet-18 network trained on CIFAR-10 using (a) supervised learning with cross-entropy loss and (b) our method with shifting transformation learning. OOD samples (Place365 dataset) are presented in gray.} \label{fig:tsne}
\end{center}
\vskip -0.2in
\end{figure}


%
  

\section{Method}
\label{sec:method}



In this paper, we propose shifting transformation learning for explicit and efficient training of in-domain representation for improved OOD detection.  
Intuitively, we simultaneously train a base encoder on \textit{multiple shifting transformations} of the training data using auxiliary self-supervised objectives for unlabeled data and fully-supervised objectives for labeled data.
To illustrate the impact of our approach on OOD detection, Figure~\ref{fig:tsne} shows t-SNE visualization~\cite{van2008visualizing} of the CIFAR-10 examples obtained from ResNet-18~\cite{he2016deep} trained with the cross-entropy loss (left) compared to our multitask transformation learning (right) with Places365 examples as the OOD test set.
The visualization intuitively shows how training with multiple shifted in-domain samples improves the separation between OOD and in-domain samples without the need for any OOD training set. 

In this section, we first present our shifting transformation learning framework. Then, we present our model for selecting optimal transformation for a given in-domain set. Finally, we present our detection score. 





\begin{figure*}[t]
\vskip 0.1in
\begin{center}
\begin{subfigure}[b]{0.32\textwidth}
\centerline{\includegraphics[width=0.97\textwidth]{figures/c-10-res18.PNG}}
        \vskip 0.1in
        \centerline{\includegraphics[width=0.97\textwidth]{figures/w-cifar10.png}}
        \caption{CIFAR-10}
    \end{subfigure}
    \rulesep
    \begin{subfigure}[b]{0.32\textwidth}
\centerline{\includegraphics[width=0.97\textwidth]{figures/c-100-res18.PNG}}
        \vskip 0.1in
        \centerline{\includegraphics[width=0.97\textwidth]{figures/w-cifar100.png}}
        \caption{CIFAR-100}
    \end{subfigure}
    \rulesep
    \begin{subfigure}[b]{0.32\textwidth}
\centerline{\includegraphics[width=0.97\textwidth]{figures/imagenet-resnet18-png.PNG}}
        \vskip 0.05in
        \centerline{\includegraphics[width=0.97\textwidth]{figures/w-imagenet-7.PNG}}
        \caption{ImageNet-30}
    \end{subfigure}
\caption{Our studies show that the optimal transformation set  and their weights  depend on the in-domain training set. \textbf{Top:} Ablation study to measure effects of individual and paired transformations on OOD detection performance. \textbf{Bottom:} Optimizing transformation weights () for auxiliary self-supervised tasks for each training set. Experiments are done in multi-class classification setup on different training sets.}
\label{fig:all-trans}
\end{center}
\vskip -0.2in
\end{figure*} 

\subsection{Shifting Transformation Learning} \label{sec:losses}
Our transformation learning technique trains a multi-tasked network using self-supervised and fully-supervised training objectives. We consider a set of geometric (translation, rotation) and non-geometric (blurring, sharpening, color jittering, Gaussian noise, cutout) shifting transformations and we train the network with dedicated loss functions for each transformation.  
For the self-supervised transformation learning, given an unlabeled training set of  , we denote the set of domain invariant transformations  by . 
We generate a self-labeled training set  for each self-supervised transformation  where  are the transformation labels. 
For example, we consider the image rotation task with four levels of \{0, 90, 180, 270\} self-labeled rotations and  in this case. 
The self-supervised loss  is the weighted average of loss across all transformations in :

where  is a classification network with parameters  for the  task,  are transformation weights, and  is the multi-class cross-entropy loss.
When labels are available, given the labeled training set of  , we generate transformed copies of the original training sets  where training samples retain their original class labels. 
The supervised loss  is defined by: 

which measures the classification loss for transformed copies of the data with  as transformation coefficients in .  In labeled setup, we combine  and  with the main supervised learning loss  (e.g., the cross-entropy loss for classifying the in-domain training set):

In all unlabeled detection setups, we define  and discard the main classification task. 
In the rest of the paper, for the ease of notation, we use  to refer to all the coefficients , and we drop  when it is clear from the context.  


Instead of training a separate network  or  for each task, all the auxiliary tasks and the main classification task share a feature extraction network and each only introduces an additional 2-layer fully-connected head for each task. Training is done in a multi-task fashion in which the network is simultaneously trained for the main classification (if applicable) and all weighted auxiliary tasks using standard cross-entropy loss. 


\subsection{Learning to Select Optimal Transformations} 
\label{sec:method-opt}
Previous work on self-supervised learning used ad-hoc heuristics for choosing data transformations for the training set \cite{Hendrycks2019sv,golan2018deep,tack2020csi}.
However, the optimal choice of effective transformations depends on the source distribution and heuristic approaches cannot scale up to diverse training distributions when there are many potential transformations. 
To illustrates this, we train a ResNet-18 \cite{he2016deep} with one or two self-supervised transformations that are selected from a pool of seven transformations. Here, we use the training objective in Eq. \ref{eq:eq_ssl} with equal weights for all transformations.
The OOD detection results are reported in Figure \ref{fig:all-trans}-top with CIFAR-10, CIFAR-100, and ImageNet-30 \cite{Hendrycks2019sv} datasets as in-distribution and CIFAR-100, CIFAR-10, and Pets \cite{parkhi2012cats} as example OOD test sets, respectively. The heatmap visualization presents a clear view of how different transformations (and the combinations of two) have a different impact on the OOD detection performance depending on the source distribution. 
For example, although rotation is the most effective transformation on CIFAR-10 and ImageNet-30, it is among the least effective ones for CIFAR-100. On the other hand, sharpening and color jittering are among the most effective transformations for CIFAR-100, but they perform worse on CIFAR-10. 



To tackle the problem of selecting optimal transformations, we propose a simple two-step transformation selection framework. Our approach relies on Bayesian optimization to first select effective transformation set . It then uses meta-learning to learn  for OOD detection as discussed next.




\paragraph{Optimizing Transformations Set :} We use Bayesian optimization to identify effective transformations for each in-domain training set as the first step shown in Alg.~\ref{alg:only_alg}. Here, we assume that transformation weights  are equal to one and we only search for effective transformations set from a pool of available transformations. Due to the small  search space (i.e.,  for  transformations), we use a low-cost off-the-shelf Bayesian optimization library \cite{akiba2019optuna} with Tree-Parzen estimators to find the optimum self-supervised task set. 
The Bayesian optimization objective seeks to minimize the main classification loss  on , the validation set for the in-domain training data. 


\paragraph{Optimizing Transformations Weights :} Next, we optimize  coefficients for the selected transformation from the previous step to improve the effect of shifting transformation on representation learning. 
This step is important because the  coefficients modulate the impact of different transformations in the training objective in Eq.~\ref{eq:eq_ssl} and Eq.~\ref{eq:eq_sup}. 
Here, we assume that  is a ``meta-parameter'' and we use a differentiable hyperparameter optimization algorithm~\cite{maclaurin2015gradient} for optimizing it as the second step shown in Alg.~\ref{alg:only_alg}. 
Our optimization algorithm consists of inner training updates that trains network parameters  using  on  for  steps. 
Given the current state of parameters , we update  in the outer loop such that  is minimized on . 
Note that the gradient of  w.r.t.  is defined only through the gradient updates in the inner loop. Thus, the  updates in the outer loop require backpropagating through the gradients updates in the inner loop which can be done easily using differentiable optimizers \cite{grefenstette2019generalized}.
We use  step for the inner-loop optimization with SGD when updating  and we use Adam \cite{kingma2014adam} to update  with small  learning rate of 0.01. 
Figure \ref{fig:all-trans}-bottom presents  values during optimization from a study on three training sets. 



\begin{algorithm}[tb]
\caption{Transformations  and  Optimization}
\label{alg:only_alg}
\textbf{Input:} Available transformation set , learning rate , inner steps   \\
\textbf{Output:} Optimal  and  sets \\
\textbf{Step 1:} Transformations Selection 
\begin{algorithmic}[1] 
    \WHILE{not converged}
        \STATE Sample a new  set with .
        \STATE Train a classifier with  loss.
        \STATE Calculate  on  as fitness measure.
        \STATE Update the acquisition function.
    \ENDWHILE
\end{algorithmic}
\textbf{Step 2:}  Weights Optimization
\begin{algorithmic}[1] 
\STATE Initialize with .
\WHILE{not converged}
    \FOR{ steps}
\STATE   on 
    \ENDFOR
    \STATE  on 
\ENDWHILE
\end{algorithmic}
\end{algorithm}







%
 


Because the choice of effective shifting transformations depends on the in-domain distribution, our optimization framework avoids the need for  samples and only relies on in-domain validation loss as a proxy for representation learning. 
Our ablation studies show that multi-task training of shifting transformations with this objective function is an effective proxy for selecting optimal transformations for both OOD detection and in-domain generalization. 



\begin{table*}[t]
\centering
\caption{Comparison of OOD detection results (AUROC \%) with the supervised Baseline, state-of-the-art self-supervised \cite{Hendrycks2019sv}, contrastive learning \cite{khosla2020supervised,sehwag2021ssd,tack2020csi} and our technique with multi-task self-supervised () and hybrid () transformation learning tasks.} 
\label{tab:main-results}
\resizebox{0.82\textwidth}{!}{
\begin{tabular}{ccccclccc}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{7}{c}{Detection AUROC} \\ \cline{3-9} 
 &  & Baseline & 
 \begin{tabular}[c]{@{}c@{}}Geometric\end{tabular} & 
 \begin{tabular}[c]{@{}c@{}}SupSimCLR\end{tabular} & 
 \begin{tabular}[c]{@{}c@{}}SSD+\end{tabular} & 
 \begin{tabular}[c]{@{}c@{}}CSI (ens)\end{tabular} & 
 \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Ours\\mathcal{L}_{\text{ssl}}D^{in}\mathcal{L}_{\text{ssl}}D_{test}^{out}\mathcal{T}\lambda_nD^{out}\lambda_n\mathcal{T}D^{in}D^{in}D_{test}^{out}D^{out}D_{test}^{out}D^{out}D^{out}_{test}D^{out}_{train}D^{out}_{test}\blambdaD^{out}_{test}D^{out}_{train}D^{out}_{tune}D^{out}_{train}D^{out}_{train}D^{out}_{test}D^{in}_{test}D^{out}_{test}D_{train}^{in}D_{test}^{out}D_{test}^{out}D_{train}^{in}\mathcal{T}\mathcal{T}\blambda\mathcal{T}\lambda\mathcal{T}\lambda\lambda_0 = 4.0760\lambda_0 = 8.6546\lambda_0 = 9.3599(\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\})D_{test}^{out}D_{test}^{out}D_{test}^{in}D_{test}^{out}D_{test}^{in}\timesD^{out}_{test}D_{test}^{out}D_{train}^{in}D_{test}^{out}D_{train}^{in}D_{test}^{out}D_{train}^{in}D_{test}^{out}D_{test}^{out}$ sets.}
\label{tab:tiny-time}
\resizebox{0.27\textwidth}{!}{
\begin{tabular}{@{}cc@{}}
\toprule
Detector & Inference Time (s) \\ \midrule
\multicolumn{1}{c|}{Baseline} & 9.3s \\ 
\multicolumn{1}{c|}{OE} & 9.3s \\ 
\multicolumn{1}{c|}{Geometric} & 39.1s \\ 
\multicolumn{1}{c|}{Ours} & 76.3s \\
\multicolumn{1}{c|}{SSD} & 23.6 \\ 
\multicolumn{1}{c|}{CSI} & 18.7s\\ 
\multicolumn{1}{c|}{CSI-ens} & 163.2s \\ 
\multicolumn{1}{c|}{Gram} & 323.9 \\ \midrule 
\end{tabular}}
\end{table}
 %
 
\end{document}
