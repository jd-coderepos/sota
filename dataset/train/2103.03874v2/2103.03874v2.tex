\documentclass{article}
\pdfoutput=1 


\usepackage[nonatbib, final]{neurips_data_2021}
\usepackage[round,semicolon]{natbib}




\usepackage[compact]{titlesec}

\usepackage[breaklinks=true,colorlinks,citecolor=black,bookmarks=false]{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue,
	citecolor=black,
  pdfinfo={
    Title={Measuring Mathematical Problem Solving With the MATH Dataset},
    Author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
    Subject={mathematics, transformers},
    Keywords={math, gpt, competition math, proofs, reasoning, logic, problem solving, transformers, mathematics, theorem proving, math dataset}
  }
}

\usepackage{breakcites}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      







\usepackage{times}
\usepackage{epsfig}
\usepackage{wrapfig}

\usepackage[utf8]{inputenc} \usepackage{amsmath, amssymb}
\usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{amsfonts}       \usepackage{microtype}      \usepackage{wrapfig}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{stfloats}
\usepackage{mmacells}

\usepackage{cleveref}
\usepackage{appendix}
\usepackage[most]{tcolorbox}

\usepackage{asypictureB}

\DeclareMathOperator{\dif}{d\!}         

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\diveq}{\mathrel{/}=}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{}}}}}
\makeatother

\makeatletter
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\makeatother


\newcommand\NameCite[1]{\citeauthor{#1}~[\citeyear{#1}]}

\newcommand{\reviewer}[3]{
	\expandafter\newcommand\csname #1\endcsname[1]{
		\textcolor{#3}{[#2: ##1]}
	}
}
\reviewer{collin}{Collin}{red}
\definecolor{neonpurple}{rgb}{0.3,0,1}
\reviewer{dan}{Dan}{neonpurple}
\reviewer{js}{JS}{blue}



\title{Measuring Mathematical Problem Solving With the\\ MATH Dataset}
\date{}


\author{Dan Hendrycks\\
UC Berkeley\\
\And
Collin Burns\\
UC Berkeley\\
\And
Saurav Kadavath\\
UC Berkeley\\
\And
Akul Arora\\
UC Berkeley\\
\And
Steven Basart\\
UChicago\\
\And
Eric Tang\\
UC Berkeley\\
\And
Dawn Song\\
UC Berkeley\\
\And
Jacob Steinhardt\\
UC Berkeley\\
}

\begin{document}

\maketitle


















\begin{abstract}





Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of  challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue.
While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.\looseness=-1



\end{abstract}


\section{Introduction}















Mathematics is a highly effective tool in many intellectual endeavors.
It enables us to count and quantify objects, and it can be relied upon because it is consistent and based on logic.
Mathematics pervades the sciences and can be used to model planetary orbits, atomic motion, signal frequencies, and much more. These phenomena can be encoded with mathematics precisely and concisely. This has even led some to describe mathematics as being ``unreasonably effective'' \citep{Wigner1960TheUE}. 
These observations speak to the broad reach and domain-generality of mathematics.


In machine learning, mathematics 
is a valuable testbed 
for \emph{problem-solving ability}: the ability to analyze a problem, pick out good heuristics from a large set of possibilities, and chain them together to produce an answer. This contrasts with 
plug-and-chug calculations, 
a skill which ML models can already exhibit \citep{Henighan2020ScalingLF}. Visual or linguistic reasoning may involve limited problem-solving ability for tasks such as image classification, but unlike math this is not the focus of these domains.



To measure the problem-solving 
ability of machine learning models, we introduce the MATH 
dataset, which consists of 
 problems from high school math competitions. 
Given a problem from MATH, machine learning models generate a sequence, such as \verb==, that encodes the final answer. These 
answers are unique after normalization, allowing MATH to be scored with exact match rather than with heuristic metrics such as BLEU. In addition, MATH problems are tagged by difficulty from 1 to 5, and span seven subjects including geometry, where diagrams can be specified in text with the Asymptote language.
This enables a fine-grained assessment of mathematical problem-solving ability across difficulties and subjects.
Finally, problems come with full step-by-step solutions. 
By training on these, models can learn to generate their own step-by-step solutions, which can facilitate learning and make model outputs more interpretable.















\begin{figure*}[t]
\noindent
\begin{minipage}{0.50\textwidth}
\begin{center}
\large{Metamath Theorem Proving}
\end{center}
\begin{flushleft}
.
GPT-'s generated proof:
\begin{verbatim}
|- ((N e. NN0 /\ ((N + 1)/2) e.
   NN0) -> ((N - 1) / 2) e. NN0)
|- (N e. NN0 -> N e. CC)
|- 1 e. CC
|- ((N e. CC /\ 1 e. CC) ->
   (N - 1) e. CC )
\end{verbatim}
\vspace{-10pt}
\begin{center}

\end{center}

\hrule
\begin{center}
\large{DeepMind Mathematics Dataset}
\end{center}

\begin{verbatim}
Divide 1136975704 by -142121963.
A: -8
Let k(u) = u**2+u-4. Find k(0).
A: -4
Sort 2, 4, 0, 6.
A: 0, 2, 4, 6
Solve 4 - 4 - 4 = 188*m for m.
A: -1/47
\end{verbatim}

\end{flushleft}
\end{minipage}\vline\phantom{v}\hfill \begin{minipage}{0.50\textwidth} 
\begin{flushleft}
\begin{center}
\large{MATH Dataset (Ours)}
\end{center}

\textbf{Problem:}\quad Tom has a red marble, a green marble, a blue marble, and three identical yellow marbles. How many different groups of two marbles can Tom choose?

\textbf{Solution:}\quad There are two cases here: either Tom chooses two yellow marbles (1 result), or he chooses two marbles of different colors ( results). The total number of distinct pairs of marbles Tom can choose is .



\textbf{Problem:}\quad The equation  has two complex solutions. Determine the product of their real parts.

\textbf{Solution:}\quad Complete the square by adding 1 to each side. Then , so . The desired product is then .


\end{flushleft}
\end{minipage}

\caption{Previous work is based on formal theorem provers or straightforward plug-and-chug problems. Our dataset, MATH, has competition mathematics problems with step-by-step solutions written in \LaTeX{} and natural language. Models are tasked with generating tokens to construct the final (boxed) answer.}
\label{fig:splash}
\vspace{-10pt}
\end{figure*}














The MATH dataset is challenging: large language models achieved 
accuracies ranging from  to . Despite these low accuracies, models clearly possess some 
mathematical knowledge: they achieve up to  accuracy on the easiest difficulty level, and they are able to generate step-by-step solutions that are coherent and on-topic even when incorrect. We also evaluated humans on MATH, and found that a computer science PhD student who does not especially like mathematics attained approximately  on MATH, while a three-time IMO gold medalist attained , indicating that MATH can be challenging for humans as well. 

The presence of step-by-step solutions allows models to utilize ``scratch space'': rather than having to generate 
a final answer immediately, models can first generate solutions that may contain intermediate computations. Interestingly, we found that having models generate step-by-step solutions before producing an answer actually \emph{decreased} accuracy relative to immediately outputting a final answer without generating solutions, indicating the solutions are currently not useful for models at test time. In contrast, having models \emph{train} on solutions increases relative accuracy by  compared to training on the questions and answers directly. We also find that models do better with hints in the form of partial solutions. Our results show that models can make use of actual step-by-step solutions provided to them in various ways, but that they are still unable to effectively use their own generated solutions. Bridging this gap poses an interesting direction for further research.






While MATH covers advanced problem-solving techniques, models 
may first need to be trained thoroughly on the fundamentals of 
mathematics. To address this, 
we create the first large-scale mathematics pretraining dataset with hundreds of thousands of step-by-step solutions in natural language and \LaTeX{}.
We call this dataset the Auxiliary Mathematics Problems and Solutions (AMPS) pretraining corpus, which consists of Khan Academy and Mathematica data. AMPS has over  Khan Academy problems with step-by-step solutions in \LaTeX{}; these exercises are used to teach human students concepts ranging from basic addition to Stokes' Theorem.
It also contains over  million problems generated using Mathematica scripts, based on  hand-designed modules covering topics such as conic sections, div grad and curl, KL divergence, eigenvalues, polyhedra, and Diophantine equations. In total AMPS contains 
23GB of problems and solutions.
Pretraining on AMPS enables a  billion parameter model to perform comparably to a fine-tuned model that is  larger.



Altogether, 
while large Transformer models \citep{Vaswani2017AttentionIA} make some progress on the MATH dataset, such as by AMPS pretraining or by training with step-by-step solutions, accuracy nonetheless remains relatively low.
While enormous Transformers pretrained on massive datasets can now solve most existing text-based tasks, this low accuracy indicates that our MATH dataset is distinctly harder. 
Accuracy also increases only modestly with model size: 
assuming a log-linear scaling trend, models would need around  parameters to achieve  accuracy on MATH, which is impractical. 
Instead, to make large strides on the MATH dataset with a practical amount of resources, we will need new algorithmic advancements from the broader research community.








 \section{Related Work}

\textbf{Neural Theorem Provers.}\quad
Much of the existing work on machine learning models for mathematical reasoning relies on automated theorem proving benchmarks.
\citet{Huang2019GamePadAL} use the Coq theorem proving environment to create a machine learning benchmark with  theorems and lemmas. \citet{Bansal2019HOListAE} introduce the HOList benchmark for automated theorem proving, which uses a formal language to enable automatic evaluation. Rather than use HOList, \citet{Polu2020GenerativeLM} use the Metamath formalization language for automated theorem proving with promising results. We show an example of Metamath in \Cref{fig:splash}. These benchmarks can be approached with seq2seq \citep{Sutskever2014SequenceTS} Transformers which have traction on the problem \citep{Polu2020GenerativeLM, Rabe2020MathematicalRV, Li2020ModellingHM}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/dataset_accuracy_comparison.pdf}
    \caption{Compared to existing proof and plug-and-chug tasks, our mathematical problem solving task is considerably more challenging. HOList results are from \citet{Wu2021LIMELI}. HOLStep results are from \citet{Crouse2019ImprovingGN}. DeepMind Math accuracy is the median IID accuracy from \citet{Henighan2020ScalingLF}. Symbolic Integration accuracy is from \citet{Lample2020DeepLF}.}
    \label{fig:datasetsacc}
    \vspace{-10pt}
\end{wrapfigure}

Rather than prove theorems with standard pretrained Transformers, \citet{McAllester2020MathZeroTC} proposes that the community create theorem provers that bootstrap their mathematical capabilities through open-ended self-improvement. For bootstrapping to be feasible, models will also need to understand mathematics as humans write it, as manually converting advanced mathematics to a proof generation language is extremely time-consuming. This is why \citet{Szegedy2020APP} argues that working on formal theorem provers alone will be an impractical path towards world-class mathematical reasoning. We address \citet{Szegedy2020APP}'s concern by creating a dataset to test understanding of mathematics written in natural language and commonplace mathematical notation. 
This also means that the answers in our dataset can be assessed without the need for a cumbersome theorem proving environment, which is another advantage of our evaluation framework.





\textbf{Neural Calculators.}\quad Recent work shows that Transformers can sometimes perform laborious calculations around as well as calculators and computer algebra systems. \citet{Lample2020DeepLF} use Transformers to solve algorithmically generated symbolic integration problems and achieve greater than 95\% accuracy. \citet{Amini2019MathQATI,Ling2017ProgramIB} introduce plug-and-chug multiple choice mathematics problems and focus on sequence-to-program generation.
\citet{Saxton2019AnalysingMR} introduce the DeepMind Mathematics dataset, which consists of algorithmically generated plug-and-chug problems such as addition, list sorting, and function evaluation, as shown in \Cref{fig:splash}. Recently, \citet{Henighan2020ScalingLF} show that, excluding problems with astronomically large numbers, the vast majority of the problems in the DeepMind Mathematics dataset can be straightforwardly solved with large Transformers.








\textbf{Benchmarks for Enormous Transformers.}\quad
There are few existing natural language benchmarks left to solve, as tasks that aggregate multiple subtasks such as SuperGLUE \citep{Wang2019SuperGLUEAS} are solved by simply training enormous Transformers \citep{He2020DeBERTaDB}.
\citet{Kaplan2020ScalingLF, Henighan2020ScalingLF} show that the performance of Transformers predictably increases with an increase in model size and dataset size, raising the question of whether natural language processing can be solved by simply increasing compute and funding.
Additionally, \citet{Chen2021EvaluatingLL,Austin2021ProgramSW} show that code generation models scale reliably across several orders of magnitude, and, should scaling continue, \citet{Chen2021EvaluatingLL}'s HumanEval code generation dataset should be solved in a few orders of magnitude.
In the Supplementary Materials, we even find that large GPT-3 models can perform remarkably well on a sequence completion test similar to an IQ test, the C-Test \citep{ctest,Legg2007UniversalIA}. Even difficult logical understanding tasks such as LogiQA \citep{Liu2020LogiQAAC} will soon be straightforwardly solved by enormous Transformers should trends continue, which we also show in the Supplementary Materials. \citet{Hendrycks2020MeasuringMM} create a multiple-choice benchmark covering  subjects. However, unlike our benchmark, which is a text generation task with  mathematical reasoning questions, their benchmark is a multiple choice task that includes only a few hundred questions about mathematics. 
In contrast to these benchmarks, we find that our MATH benchmark is unusually challenging for current models and, if trends continue, simply using bigger versions of today's Transformers will not solve our task in the foreseeable future.




 \begin{table*}[t]
	\centering
	\setlength\tabcolsep{10pt}
	\begin{tabular}{l|l}
        Algebra         & Conic sections, polynomial GCD, De Moivre's theorem, function inverses, ... \\ Calculus        & Arclength, Jacobian, Laplacian, divergence, curl, gradients, integrals, ... \\ Statistics      & Expectation, geometric mean, harmonic mean, KL divergence, variance, ... \\ Geometry        & Triangle area, triangle inradius, polygon angles, polyhedron diameter, ...\\ Linear Algebra      & Characteristic polynomials, eigenvalues, reduced row echelon form, ... \\ Number Theory       & Modular inverse, Euler's totient function, Chinese remainder theorem, ... \\ \end{tabular}
	\caption{A subset of the topics covered by our  hand-designed Mathematica scripts, which is part of our Auxiliary Mathematics Problems and Solutions (AMPS) pretraining dataset. Of these scripts,  also generate step-by-step solutions. We generated around  exercises with each Mathematica script, or around  million problems.}\label{fig:mesa}
	\vspace{-5pt}
\end{table*}

\section{Datasets}


In this section, we introduce two new datasets, one for benchmarking mathematical problem-solving ability (MATH) and one for pretraining (AMPS).


\subsection{The MATH Dataset}\label{subsec:math}
The MATH dataset consists of problems from mathematics competitions including the AMC 10, AMC 12, AIME, and more. Many of these problems can be collected from \href{https://artofproblemsolving.com/community/c3158_usa_contests}{aops.com/community/c3158\_usa\_contests}. These competitions span decades and assess the mathematical problem-solving ability of the best young mathematical talent in the United States.
Unlike most prior work, most problems in MATH cannot be solved with a straightforward application of standard K-12 mathematics tools.
Instead, humans often solve such problem by applying problem solving techniques and ``heuristics'' \citep{Polya1945HowTS}.

The Mathematics Aptitude Test of Heuristics dataset, abbreviated MATH, has  problems ( training and  test). With this many training problems, models can learn many useful heuristics for problem solving. Each problem has a step-by-step solution and a final boxed answer. Example problems with step-by-step solutions are shown in \Cref{fig:splash}.



\textbf{Categorizing Problems.}\quad Problems span various subjects and difficulties. The seven subjects are 
Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. While subjects like Prealgebra are generally easier than Precalculus, within a subject problems can take on different difficulty levels. We encode a problem's difficulty level from `' to `,' following AoPS. A subject's easiest problems for humans are assigned a difficulty level of `,' and a subject's hardest problems are assigned a difficulty level of `.' Concretely, the first few problems of an AMC 8 exam are often level , while AIME problems are level . 
This allows us to assess performance across both different subjects and different levels of difficulty.




\textbf{Formatting.}\quad Problems and solutions are consistently formatted using \LaTeX{} and the Asymptote vector graphics language. Our usage of \LaTeX{} allows us to flexibly encode mathematical problems while avoiding unusual symbols or cumbersome formal languages. Meanwhile, mathematical figures are encoded in the Asymptote language rather than as raster images. 
This enables pure language models to process figures, diagrams, and graphics, making it possible to assess these models on subjects such as geometry for the first time. 

To assess models using exact match, we force the final boxed answers to follow consistent formatting rules.
Specifically, probabilities are expressed as simplified fractions. Moreover, matrix entry fractions are encoded with \verb|x/y|, while all other fractions are consistently encoded with the \verb|\frac{x}{y}| command. Coefficients are encoded without a multiplication symbol (e.g. \verb|5x| not \verb|5*x|). Expressions with multiple variables are entered in alphabetical order; polynomials are expressed in decreasing degree order. Different fraction encodings equivalent, such as \texttt{\textbackslash frac\{x\}\{y\}} and \texttt{\textbackslash dfrac\{x\}\{y\}} and \texttt{x/y}. Different parenthesis encodings, such as \texttt{\textbackslash left(} and \texttt{(}, are treated as equivalent.

We also allow units to be included or omitted from an answer, we ignore spaces, and we treat common equivalent ways of expressing the same number (e.g.,  and , or  and ) as the same. When the answer is a factorized polynomial, we permit different orderings of the factors, so that  is equivalent to , and so on. These rules cover nearly all ways that different generated or actual solutions can be equivalent in practice. 


\textbf{Automatically Assessing Generated Answers.}\quad Due to design choices in MATH, we can assess the answers generated by a model \emph{automatically}, even though the space of model outputs is combinatorially large. Automatic assessment starts by determining the beginning and end of the answer. This is possible to do even if a model generates step-by-step solutions because the final answers in MATH are wrapped and delimited with the \verb|\boxed{}| command. We can consequently evaluate a model's output by parsing what is inside the \verb|\boxed{}| command and comparing that with the ground truth answer, while accounting for the equivalent ways of formatting a string described above.
Together, the box delimiter and formatting rules provide a unique answer in a well-defined location, which allows us to test for equivalence and use accuracy as our primary metric.

\textbf{Human-Level Performance.}\quad To provide a rough but informative comparison to human-level performance, we randomly sampled  problems from the MATH test set and gave them to humans. We artificially require that the participants have 1 hour to work on the problems and must perform calculations by hand. All participants are university students. One participant who does not like mathematics got  correct. A participant ambivalent toward mathematics got . Two participants who like mathematics got  and . A participant who got a perfect score on the AMC 10 exam and attended USAMO several times got . A three-time IMO gold medalist got , though missed questions were exclusively due to small errors of arithmetic. 
Expert-level performance is theoretically  given enough time. Even  would accuracy for a machine learning model would be impressive but have ramifications for cheating on homework.

\begin{table*}[t]
    \vspace{-15pt}
	\centering
	\setlength\tabcolsep{2pt}
	\begin{tabularx}{1.00\textwidth}{*{1}{>{\hsize=1.2\hsize}X}
	*{1}{>{\hsize=0.75\hsize}Y} *{1}{>{\hsize=0.75\hsize}Y} *{1}{>{\hsize=0.75\hsize}Y} *{1}{>{\hsize=0.9\hsize}Y} *{1}{>{\hsize=0.75\hsize}Y} *{1}{>{\hsize=0.95\hsize}Y} *{1}{>{\hsize=0.85\hsize}Y}
	| *{1}{>{\hsize=1.0\hsize}Y}}
		Model & {\fontsize{9}{7.2}\selectfont Prealgebra}
		& {\fontsize{9}{7.2}\selectfont Algebra}
		& {\fontsize{9}{7.2}\selectfont Number Theory}
		& {\fontsize{9}{7.2}\selectfont {Counting\,\&} Probability}
		& {\fontsize{9}{7.2}\selectfont Geometry}
		& {\fontsize{9}{7.2}\selectfont Intermediate Algebra}
		& {\fontsize{9}{7.2}\selectfont Precalculus}
		& {\fontsize{9}{7.2}\selectfont Average} \\
        \hline
        GPT-2 0.1B         & 5.2 & 5.1 & 5.0 & 2.8 & 5.7 & 6.5 & 7.3 & 5.4 {\fontsize{8}{7.2}\selectfont \color{gray}{\phantom{5}}} \\
        GPT-2 0.3B         & 6.7 & 6.6 & 5.5 & 3.8 & 6.9 & 6.0 & 7.1 & 6.2 {\fontsize{8}{7.2}\selectfont\color{gray}{}} \\
        GPT-2 0.7B         & 6.9 & 6.1 & 5.5 & 5.1 & 8.2 & 5.8 & 7.7 & 6.4 {\fontsize{8}{7.2}\selectfont\color{gray}{}} \\
        GPT-2 1.5B         & 8.3 & 6.2 & 4.8 & 5.4 & 8.7 & 6.1 & 8.8 & 6.9 {\fontsize{8}{7.2}\selectfont\color{gray}{}} \\
        \hline
		GPT-3 13B*          & 4.1 & 2.4 & 3.3 & 4.5 & 1.0 & 3.2 & 2.0 & 3.0  {\fontsize{8}{7.2}\selectfont\color{gray}{}} \\
		GPT-3 13B          & 6.8 & 5.3 & 5.5 & 4.1 & 7.1 & 4.7 & 5.8 & 5.6 {\fontsize{8}{7.2}\selectfont\color{gray}{}\phantom{5}} \\
		GPT-3 175B*          & 7.7 & 6.0 & 4.4 & 4.7 & 3.1 & 4.4 & 4.0 & 5.2 {\fontsize{8}{7.2}\selectfont\color{gray}{\phantom{5}}} \\
        \Xhline{2\arrayrulewidth}
	\end{tabularx}
	\caption{MATH accuracies across subjects. `*' indicates that the model is a few-shot model. The character `B' denotes the number of parameters in billions. The {\color{gray}gray} text indicates the \emph{relative} improvement over the 0.1B baseline. All GPT-2 models pretrain on AMPS, and all values are percentages. GPT-3 models do not pretrain on AMPS due to API limits. Model accuracy is increasing very slowly, so much future research is needed.}\label{tab:math}
	\vspace{-10pt}
\end{table*}





\subsection{AMPS (Khan + Mathematica) Dataset}
Since pretraining data can greatly influence performance \citep{Hernandez2021ScalingLF,Gururangan2020DontSP} and since mathematics is a small fraction of online text, we introduce a large and diverse mathematics pretraining corpus. Our pretraining dataset, the Auxiliary Mathematics Problems and Solutions (AMPS) dataset, has problems and step-by-step solutions typeset in \LaTeX{}. AMPS contains over  problems pulled from Khan Academy and approximately  million problems generated from manually designed Mathematica scripts.

\textbf{Khan Academy.}\quad
The Khan Academy subset of AMPS has  exercise types with over  problems and full solutions. Problem types range from elementary mathematics (e.g.~addition) to multivariable calculus (e.g.~Stokes' theorem), and are used to teach actual K-12 students. The exercises can be regenerated using code from \href{https://github.com/Khan/khan-exercises/}{github.com/Khan/khan-exercises/}. We show the full list of problem types in the Supplementary Materials. 

\textbf{Mathematica.}\quad
To make AMPS larger, we also contribute our own Mathematica scripts to generate approximately  more problems than our Khan Academy dataset. With Mathematica, we designed  scripts that test distinct mathematics concepts,  of which include full step-by-step \LaTeX{} solutions in addition to final answers. We generated around  exercises from each of our scripts, or around  million problems in total. This results in over  GB of mathematics problems, making it larger than the  GB of natural language used to train BERT \citep{Devlin2019BERTPO}.

Problems include various aspects of algebra, calculus, counting and statistics, geometry, linear algebra, and number theory (see \Cref{fig:mesa} for a sampling of topics). Unlike prior approaches to algorithmically generating mathematics problems, we use Mathematica's computer algebra system so that we can manipulate fractions, transcendental numbers, and analytic functions. 


    
    
    























 
\section{Experiments}


In this section, we perform experiments to investigate performance on the MATH dataset. 
We find that accuracy remains low even for the best models. Furthermore, unlike for most other text-based datasets, we find that accuracy is increasing very slowly with model size. If trends continue, then we will need algorithmic improvements, rather than just scale, to make substantial progress on MATH. Nevertheless, we show that making progress is also possible today. We find that pretraining on AMPS enables a small 0.1B parameter model to perform similarly to a large fine-tuned 13B parameter model. 

We also experiment with using step-by-step solutions. We find that having models generate their own step-by-step solutions before producing an answer actually \emph{degrades} accuracy. We qualitatively assess these generated solutions and find that while many steps remain illogical, they are often related to the question. Finally, we show that step-by-step solutions can still provide benefits today. We find that providing partial ground truth step-by-step solutions can improve performance, and that providing models with step-by-step solutions at training time also increases accuracy.

\subsection{Experimental Setup} 

\begin{figure*}[t]
\vspace{-15pt}
\noindent

\textbf{Problem:} Suppose  and  are positive real numbers with  and   Find the minimum value of 

\textbf{Generated solution:} 
Let   By QM-AM,
 Then  so   Hence,


\textbf{Ground truth solution:} We can write
 By AM-GM,
 Equality occurs when  and   We can solve these equations to find  and   Thus, the minimum value is 

\hrulefill

\textbf{Problem:} Right  has legs measuring 8 cm and 15 cm. The triangle is rotated about one of its legs. What is the number of cubic centimeters in the maximum possible volume of the resulting solid? Express your answer in terms of .

\textbf{Generated solution:} Rotating the triangle about one leg gives us the following triangle:


\begin{minipage}{0.4\textwidth}
\includegraphics{figures/main-foo.pdf}
\end{minipage}\begin{minipage}{0.6\textwidth} 

\end{minipage}

\textbf{Ground truth solution:} If the triangle is rotated about the shorter leg, then the radius is the longer leg and the height is the shorter leg, and the volume is . If the triangle is rotated about the longer leg, then the radius is the shorter leg and the height is the longer leg, and the volume is , which is  of the volume we found earlier.  So, the maximum possible volume is  cubic centimeters.

\caption{Problems, step-by-step solutions generated by our GPT-2 1.5B model, and ground truth solutions. Observe that models can provide the right answer yet generate a misleading and wrong explanation. The second generated solution demonstrates that models are capable of generating Asymptote commands to create figures and graphics.}
\label{fig:generated_examples1}
\vspace{-5pt}
\end{figure*}




\begin{figure*}[t]
\vspace{-15pt}
\textbf{Problem:} The three-digit number "'' is divisible by 3. How many different three-digit numbers can "'' represent?

\textbf{Generated solution:} A number is divisible by 3 if and only if the sum of its digits is divisible by 3. So  is divisible by  if and only if the two-digit number  leaves a remainder of 1 when divided by 3. There are 90 two-digit numbers, of which  leave a remainder of 1 when divided by 3.

\textbf{Ground truth solution:} If the number is divisible by , the sum of the digits must be divisible by .  So  must be one more than a multiple of , since  is one less than a multiple of .  We have several options:
If , it must be , one possibility.
For ,  can be  to , for four possibilities.
For ,  can be  to , so seven possibilities.
For , anything from  to  works, so nine possibilities.
For , the pairs range from  to , for six possibilities.
If , we can have , , or , so three possibilities.
 and up is not possible.
So the total number is 

\hrulefill

\begin{minipage}{0.86\textwidth} 
\begin{flushleft}
\textbf{Problem:}\quad Beginning at point  in the diagram, Dora selects one of the four possible directions with equal probability. Each time she comes to an intersection, she again randomly selects one of the possible directions. What is the probability that, in her first four steps, she will walk completely around the gray square? Express your answer as a common fraction.
\end{flushleft}
\end{minipage}\hfill \begin{minipage}{0.14\textwidth} 
\begin{flushright}
\includegraphics{figures/main-foobar.pdf}
\end{flushright}
\end{minipage}

\textbf{Generated solution:} The total probability that Dora chooses a direction completely around the gray square is .

\textbf{Ground truth solution:} The only way for the Dora to end up at her starting point in four steps is for her to traverse the four sides of the gray square.  She can do this in two ways: clockwise and counterclockwise. The probability of each of these two paths is .  Therefore, the probability that she ends up where she started is .





\caption{Additional example problems, generated solutions, and ground truth solutions from our MATH dataset. The first problem's generated solution has the right answer with a correct and simple explanation. The second problem is a combinatorics problem specified with a figure, which the model gets wrong.\looseness=-1}
\label{fig:generated_examples2}
\vspace{-5pt}
\end{figure*}


\textbf{Models and Hyperparameters.}\quad
Because MATH answers must be generated, we use autoregressive language models, namely GPT-2 \citep{Radford2016UnsupervisedRL} and GPT-3 \citep{Brown2020LanguageMA}, which are decoder models pretrained on natural language text. Our GPT-2 models tokenizes numbers so that one digit is processed at a time \citep{Henighan2020ScalingLF}.
T5's \citep{Raffel2020ExploringTL} tokenizer removes many \LaTeX{} symbols, so after a broad hyperparameter sweep lasting two weeks, its performance was not competitive. We show results with the BART architecture in the Appendix.


Before fine-tuning on MATH, models pretrain on AMPS.
We pretrain for one epoch, using AdamW \citep{Loshchilov2019DecoupledWD}, using a batch size of , and using a weight decay of . We use the standard autoregressive language modeling objective.
During pretraining, we upsample Khan Academy data by a factor of  and we downsample Mathematica by a factor of  to account for the large difference in dataset sizes.

During fine-tuning, models predict final answers and solutions. Concretely, if  is the problem statement, we train with an equal mix of ``\texttt{ Final Answer: <Answer>}'' and ``\texttt{ Full Solution: <Step-by-Step Solution>}'' sequences. This makes it possible for the model to both generate full solutions and also to output just the  final answer. For fine-tuning we use the same batch size and weight decay as in pretraining. Models are trained with 8 A100 GPUs, each requiring less than a day.

Unless otherwise specified, for GPT-2 we use the default HuggingFace \citep{wolf-etal-2020-transformers} generation parameters, except that we use beam search. Our beam search has a beam size of  when only generating the final answer, and a beam size of  when generating full step-by-step solutions. By default, we evaluate models by prompting them with ``\texttt{ Final Answer:}'' so that they directly generate the final answer to each problem, not the step-by-step solution.

We also evaluate GPT-3 with fine-tuning and also in a few-shot setting using the OpenAI API. We use the `Curie' GPT-3 model which has approximately  billion parameters, and the `Davinci' model which has approximately  billion parameters. When performing few-shot evaluation, we construct our prompt by prepending  problems with correct answers (but not step-by-step solutions due to space). Using temperature , models output up to  tokens for the final answer. The OpenAI API also allows users to fine-tune models up to 13B parameters at the time of writing, but their API does not have the option to pretrain on datasets as large as AMPS. 

\subsection{Analyzing Model Performance}

\textbf{AMPS Pretraining.}\quad 
As an ablation, we test how models with AMPS pretraining compare with models that were not pretrained on AMPS.
Without pretraining on AMPS, a GPT-3 (13B) model fine-tuned on MATH attains  accuracy. In contrast, a GPT-2 (0.1B) model both pretrained on AMPS and fine-tuned on MATH attains . 
Consequently AMPS increases accuracy about as much as a  increase in parameters, demonstrating its value as a pretraining dataset.

We additionally tried pretraining on StackExchange, a real-world but less  curated source of mathematics text. 
A GPT-2 (0.3B) model pretrained on both AMPS and questions and answers from Math StackExchange ( GB) had  accuracy. This is actually less than the  accuracy attained by pretraining on AMPS alone. Thus our dataset is more useful for pretraining even than diverse real-world mathematics data.


\textbf{Model Size.}\quad While increasing model parameters often automatically solves many tasks \citep{Brown2020LanguageMA}, we find that MATH is unusually challenging for enormous Transformers. \Cref{tab:math} shows that the average accuracy across subjects for the smallest model, GPT-2 with 0.1 billion parameters, is . Meanwhile, a GPT-2 model with  the number of parameters attains  accuracy, a  relative improvement. This indicates that while having more parameters helps, absolute accuracy remains far from the ceiling and is only increasing slowly, quite unlike most other text-based tasks. 

\textbf{Problem Difficulty.}\quad
We also analyze model accuracy while controlling for problem difficulty. Higher levels of difficulty correspond to lower accuracy, as expected. These results are visualized in the Supplementary Materials. The accuracy of GPT-2 (1.5B) is around  for level 1 (easy) and around  for level 5 (hard). Even our benchmark's easiest problems 
are more challenging than previous benchmarks that focused on straightforward plug-and-chug problems.






\textbf{Error Detection.}\quad To determine whether we can trust the answers from a model, we analyze model confidence to see whether confidence tends to be higher for correct answers. We define confidence as the average prediction probability of the tokens that make up a generated answer. GPT-2 (1.5B) is highly overconfident, with confidences that are often around . Moreover, there is substantial overlap between correct and incorrect answers. Following \citet{hendrycks17baseline}, we computed the probability that a correct answer has higher confidence than an incorrect answer. To do this, we compute the Area Under the Receiver Operating Characteristic curve (AUROC). An AUROC of  corresponds to being able to perfectly detect correct and incorrect answers, while  corresponds to random chance. We find that with GPT-2 (1.5B), the AUROC is quite low at . This suggests there is substantial room for improvement in detecting model errors.



\subsection{Analyzing Step-by-Step Solutions}

\begin{wrapfigure}{R}{0.58\textwidth}
	\begin{center}
	\includegraphics[width=0.58\textwidth]{figures/solution_peaking.pdf}
	\end{center}
	\vspace{-10pt}
	\caption{
    Models conditioned on most of a problem's step-by-step solution can often understand the solution to predict the final answer. Not all solutions have an answer that is immediate from the preceding solution text, though many are. `99\%' of a solution is all the solution text before the final answer. This demonstrates that, even with substantial help, models are still struggling.
	}
	\label{fig:peaking}
\end{wrapfigure}

\textbf{Scratch Space.}\quad
Our MATH dataset and AMPS pretraining dataset provide full step-by-step solutions, an important and rare type of side information \citep{Murty2020ExpBERTRE} that can in principle teach models how to derive answers and use scratch space. By training a language model on these solutions, we can have models generate full step-by-step solutions. This may be especially useful for difficult problems, for which outputting the correct answer after just a few forward passes may be insufficient. By allowing the model to use several steps of processing before outputting a final answer, the model could adaptively use computation and have higher performance, in addition to making its reasoning more interpretable.

We test this by prompting models with ``\texttt{ Full Solution:}'' to generate a full solution along with a final boxed answer, rather than the boxed answer alone. We evaluated this for GPT-2 (1.5B) and found that this actually makes performance worse, dropping accuracy to . We hypothesize that the drop in accuracy from using scratch space arises from a snowballing effect, in which partially generated ``solutions'' with mistakes can derail subsequent generated text. Nevertheless, when generation becomes more reliable and models no longer confuse themselves by their own generations, our dataset's solutions could in principle teach models to use scratch space and attain higher accuracy.

\textbf{Examples.}\quad
We can also qualitatively assess the step-by-step solutions that the model generates. We show examples of generated solutions in \Cref{fig:generated_examples1,fig:generated_examples2}.
We find that the model can consistently generate correct \LaTeX{} and often performs steps that appear related to the question at hand, but still makes many logical mistakes, both in terms of what the question seems to be asking and in individual steps that are part of a larger derivation. 

\textbf{The Benefits of MATH Solutions.}\quad
We find that giving models partial step-by-step MATH solutions during inference can improve accuracy.
We test performance when we allow models to predict the final answer given a ``hint'' in the form of a portion of the ground truth step-by-step solution. 
To do so, for this experiment we prompt models with ``\texttt{ <Partial Step-by-Step Solution without Final Answer> Final Answer:}'' during both fine-tuning and evaluation for different partial fractions of the step-by-step solution. 
This is the same as the default setting when we let models see  of the step-by-step solution. 
When models see ``'' of the solution, they are given the whole step-by-step solution except for the final answer. 
We show results with GPT-2 (0.7B) for different fractions of the solution in \Cref{fig:peaking}. Observe that the model still only attains approximately  when given  of the solution, indicating room for improvement.

Finally, we also find that providing models with step-by-step during training can further improve performance. We run an ablation by fine-tuning models on MATH with the same setup as before, except that we only show examples with the final answer and no step-by-step solution. If we fine-tune with only the final answer, the GPT-2 (1.5B) accuracy decreases by  to .











 \section{Conclusion}
In this paper, we laid groundwork for future research in machine learning for mathematical problem solving. We introduced the MATH benchmark, which enables the community to measure mathematical problem-solving ability. In addition to having answers, all MATH problems also include answer explanations, which models can learn from to generate their own step-by-step solutions. We also introduce AMPS, a diverse pretraining corpus that can enable future models to learn virtually all of K-12 mathematics. While most other text-based tasks are already nearly solved by enormous Transformers, MATH is notably different. We showed that accuracy is slowly increasing and, if trends continue, the community will need to discover conceptual and algorithmic breakthroughs to attain strong performance on MATH. Given the broad reach and applicability of mathematics, solving the MATH dataset with machine learning would be of profound practical and intellectual significance.


 


\bibliography{main}
\bibliographystyle{plainnat}



\newpage
\appendix
\section{Appendix}
In this appendix, we have more comparisons with previous datasets, a discussion of logic and intelligence tests, further  AMPS and MATH details, an analysis of model performance as difficulty level changes, and results with the BART architecture.

\subsection{Expanded Dataset Comparisons} We compared to ten datasets in the main paper, and now we will further describe plug-and-chug datasets. Dolphin18K \citep{Huang2016HowWD} is one of the first modern datasets in this space and is based on Yahoo{!} Answers and includes questions such as ``help!!!!!!!(please) i cant figure this out!? what is the sum of 4 2/5 and 17 3/7 ?''. MathQA \citep{Amini2019MathQATI} builds on AQuA-RAT \citep{Ling2017ProgramIB} and claims AQuA-RATs ``rationales are noisy, incomplete and sometimes incorrect.'' MathQA then cleans AQuA-RAT, though cleaning led the dataset size to be reduced by half of an order of magnitude. \citet{Miao2020ADC} analyze MathQA and observe ``the annotated formulas of 27\% of the problems do not match their labeled answers,'' and they obtain  accuracy on a cleaned version of MATH-QA. In contrast AMPS is large and clean as questions are algorithmically generated, and our MATH dataset is carefully curated by the competition mathematics community and contains competition-level problems that are difficult.

\subsection{Logic and Intelligence Tests}\label{app:iq}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/logiqa.pdf}
    \caption{Difficult natural language tasks such as LogiQA will soon be solved just by making models larger, assuming trends continue. The Transformers in this figure are UnifiedQA \citep{khashabi2020unifiedqa} models of various sizes.}
    \label{fig:logiqa}
\end{figure}

While enormous Transformers perform poorly on MATH, they do well on other logic and intelligence tests.

We analyze Transformers on LogiQA \citep{Liu2020LogiQAAC}, a task with logical reasoning questions such as ``David knows Mr. Zhang's friend Jack, and Jack knows David's friend Ms. Lin. Everyone of them who knows Jack has a master's degree, and everyone of them who knows Ms. Lin is from Shanghai. Who is from Shanghai and has a master's degree?'' As shown in \Cref{fig:logiqa}, Transformers are improving on LogiQA, so much so that they will attain human-level performance relatively soon, should trends continue. 

We also find that Transformers also do well on the C-Test, a pattern completion test that has a  correlation with human IQ \citep{HernndezOrallo2000BeyondTT}. An example of a problem from C-Test is the sequence ``a, a, z, c, y, e, x, \_'' which has the answer ``g.'' We regenerated hundreds of C-Test examples to test GPT-3 (175B) in a 5-shot setting. While GPT-3 had abysmal performance when the sequences were letters, converting letters to numbers helped. After changing `a' to 0, `b' to 1, , and `z' to 25, accuracy became approximately  on the hardest examples (C-Test questions with complexity ``13''). For comparison, on these same examples, average humans attained around  accuracy \citep{HernndezOrallo2000BeyondTT}.










\subsection{Further Dataset Information}\label{app:further_data_info}



\noindent\textbf{Rendering Graphics.}\quad For the first time, our dataset makes it possible for text-based models to process graphical mathematical figures by expressing figures in asymptote code. For example, \Cref{fig:asy} shows asymptote code and the figure it produces. In short, it is possible to concisely specify many visual mathematics problems with code, sidestepping the complexity of multi-modal models. 

\begin{figure*}
    \centering
    \begin{verbatim}
size(40);
draw(shift(1.38,0)*yscale(0.3)*Circle((0,0), .38));

draw((1,0)--(1,-2));
draw((1.76,0)--(1.76,-2));

draw((1,-2)..(1.38,-2.114)..(1.76,-2));
path p =(1.38,-2.114)..(1.74,-1.5)..(1,-0.5)..(1.38,-.114);
pair a=(1.38,-2.114), b=(1.76,-1.5);
path q =subpath(p, 1, 2);
path r=subpath(p,0,1);
path s=subpath(p,2,3);
draw(r);
draw(s);
draw(q, dashed);

label("",midpoint((1.76,0)--(1.76,-2)),E);
    \end{verbatim}
\includegraphics{figures/main-example.pdf}
    
    
    
\caption{Example of asymptote code and the figure it produces.}
    \label{fig:asy}
\end{figure*}


\noindent\textbf{AMPS Examples.}\quad We show concrete examples from AMPS in \Cref{fig:amps}. AMPS is a mixture of examples from Khan Academy and our 100 Mathematica modules.

\begin{figure*}
Example from a Khan Academy \href{https://github.com/Khan/khan-exercises/blob/master/exercises/ratio_word_problems.html}{module}:\\
Problem: In history class, the girl to boy ratio is  to . If there are a total of  students, how many boys are there?\\
\\
Solution: A ratio of  girls to  boys means that a set of  students will have  girls and  boys. A class of  students has  sets of  students. Because we know that there are  boys in each set of  students, the class must have  groups of  boys each. There is a total of  boys in history class.
\vspace{5pt}
\hrule
\vspace{5pt}
Example Mathematica code that generates practice problems:
\begin{mmaCell}[moredefined={i, roundbasis, d1, d2, q, p, j}]{Input}
  For[i=0,i<50000,i++,
  roundbasis = RandomChoice[\{0.8,0.1,0.05,0.05\}->\{1,1/2,1/3,1/5\}];
  d1 = RandomInteger[\{1,6\}];
  d2 = RandomInteger[\{1,3\}];
  q=0;
  p=0;
  While[q==0,
  For[j=0,j<d1,j++,
  q += Round[RandomReal[\{-5,5\}], roundbasis]*x^j;
  ];
  ];
  While[p==0,
  For[j=0,j<d2,j++,
  p += Round[RandomReal[\{-5,5\}], roundbasis]*x^j;
  ];
  ];
  p = RandomChoice[\{p,Expand[q*p]\}];
  Export["/amps/mathematica/algebra/polynomial_gcd/"<>ToString[i]<>".txt",
\{"Problem:\textbackslash{}nFind the greatest common divisor of \ and \.",
"Answer:\textbackslash{}n\"\}]
  ]
\end{mmaCell}
\caption{A Khan Academy problem and solution, followed by the code for a simple Mathematica module used to generate polynomials GCD problems. These problems are available in AMPS.}
\label{fig:amps}
\end{figure*}

\noindent\textbf{Contrasting AMPS and DeepMind Mathematics.}\quad AMPS has several hundred exercise types or modules (Khan Academy has 693 modules and Mathematica has 100), while DeepMind mathematics (DM) has only a few dozen. We show all Khan Academy modules in \Cref{fig:khan,fig:khan2,fig:khan3,fig:khan4}. Most DM exercises increase the diversity of problems by simply having a wide range of coefficients and constants. For example, its derivatives module exclusively covers polynomial derivatives with wide-ranging coefficients, while ours covers mixtures of dozens of major analytic functions. DM opts not to cover concepts and subjects such as logarithms and geometry, unlike AMPS. While DM is formatted in plaintext, AMPS is formatted in \LaTeX{}. Finally, while DM solely has final answers, all 693 Khan Academy modules and 37 of our Mathematica modules have full step-by-step solutions.


\subsection{Difficulty Analysis}\label{app:difficulty_analysis}
We break down MATH accuracy by difficulty levels. In \Cref{fig:difficulty_analysis}, we observe that human difficulty and machine difficulty track each other. In \Cref{fig:levels_by_subject}, we find that accuracy can vary by level and subject substantially. Finally, in \Cref{fig:problength} and \Cref{fig:sollen}, we analyze the relation between accuracy and problem and solution length, and find that problems with long questions or ground truth solutions indeed tend to be more difficult than problems with short questions or solutions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/math_difficulty.pdf}
    \caption{Problems that are more difficult for humans are also more difficult for GPT-2.}
    \label{fig:difficulty_analysis}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/levels_per_subject.pdf}
    \caption{Accuracy per subject per difficulty level.}
    \label{fig:levels_by_subject}
\end{figure*}

\begin{figure*}[h]
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/acc_vs_problem_length_arrows.pdf}
\caption{Subject accuracy vs problem length. Each point represents a subject at a specific difficulty level. We exclude problems with asymptote figures. Results are from GPT-2 (1.5B).}\label{fig:problength}
\end{subfigure}\hfill \begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/acc_vs_sol_length_arrows.pdf}
\caption{Subject accuracy vs solution length. Each point represents a subject at a specific difficulty level. We exclude problems with asymptote figures. Results are from GPT-2 (1.5B).}\label{fig:sollen}
\end{subfigure}
\end{figure*}

\subsection{Results with the BART Architecture}
We use BART \citep{Lewis2020BARTDS} to determine whether other existing architectures can improve performance.
In the main paper we analyzed the performance of various GPT models, which are unidirectional decoder models. 
\cite{Lewis2020BARTDS} introduce BART, which has a bidirectional encoder and unidirectional decoder. While T5 has a similar architecture to BART, its tokenizer removes \LaTeX{} symbols, while BART's tokenizer does not. Hence we use BART in this paper. After pretraining BART-Large (0.4B) on AMPS and fine-tuning BART on MATH, we find that it obtains  on MATH's test set, which is slightly worse than our smallest GPT-2 model. Consequently models with a bidirectional encoder and unidirectional decoder do not yield marked changes in MATH accuracy.

\subsection{Further Human Evaluation Details}
Because MATH requires a strong mathematical background to perform well on, and a long amount of time to solve problems, we were restricted to assessing six human participants and could not rely on crowdsourcing sites such as Amazon Mechanical Turk. All participants are university students studying computer science. Four of the participants are authors on the paper. The other two participants are friends or acquaintances of the authors, as this survey was only to give a rough sense of human-level performance. All participants gave consent to use their name, though we opt not to include it. Participants had one hour to complete the questions, all without using a calculator. Participants were offered remuneration, though they all offered to volunteer to work on the problems. The instructions and questions used are as follows.


``This research study is being conducted by the Steinhardt Group at UC Berkeley. For questions about this study, please contact Dan Hendrycks at hendrycks@berkeley.edu. In this study, you will have sixty minutes to complete twenty mathematics problems, all without using a calculator. We would like to remind you that participation in our study is voluntary and that you can withdraw from the study at any time.''

\begin{enumerate}
    \item A 6-sided die is weighted so that the probability of any number being rolled is proportional to the value of the roll. (So, for example, the probability of a 2 being rolled is twice that of a 1 being rolled.) What is the expected value of a roll of this weighted die? Express your answer as a common fraction.
    \item The square of 15 is 225. The square of what other number is 225?
    \item Find the sum of all values of  such that .
    \item The parabolas defined by the equations  and  intersect at points  and , where . What is ? Express your answer as a common fraction.
    \item If , what is the value of ?
    \item Let  be a cubic polynomial such that , and . Find .
    \item Let  be the set of complex numbers of the form , where  and  are integers. We say that  is a unit if there exists a  such that . Find the number of units in .
    \item Find the remainder when  is divided by .
    \item The length of a rectangle is  feet and its width is  feet. If the perimeter of the rectangle is  feet, how many square feet are in the area of the rectangle?
    \item  A European train compartment has six seats. Four of the seats are broken. Wilhelm needs to fill out a form to indicate that there are broken seats. If he randomly checks off four of the seats in the diagram, what is the probability that he marked the correct seats? Express your answer as a common fraction.
    \item We have a triangle  where , , and . Let  be the midpoint of . What is the length of ?
    \item If  gives a remainder of  when divided by , then what remainder does  give when divided by ?
    \item  Our club has  members, and wishes to pick a president, secretary, and treasurer. In how many ways can we choose the officers, if individual members are allowed to hold , but not all , offices?
    \item Find the minimum possible value of
    
    where ?
    \item Let , and  be the roots of . Find .
    \item Let  be the hyperbola with foci at  and vertices at , and let  be the circle with center  and radius . Given that  and  intersect at four points, what is the area of the quadrilateral formed by the four points?
    \item If  and  what is the value of ?
    \item Find the value of  such that .
    \item  For , the area of the triangle with vertices  and  is  square units. What is the value of ?
    \item Find the units digit of the following within the indicated number base: .

\end{enumerate}

\begin{figure*}
\textbf{Khan Academy Modules (1/4)}:
2 step equations; 2-step addition word problems within 100; 2-step subtraction word problems within 100; 2-step word problems; absolute minima and maxima (closed intervals); absolute minima and maxima (entire domain); absolute value equations; absolute value of complex numbers; add and subtract complex numbers; add and subtract matrices; add and subtract polynomials; add and subtract rational expressions; add and subtract rational expressions: factored denominators; add and subtract rational expressions: like denominators; add and subtract rational expressions: unlike denominators; add and subtract vectors; add 1 or 10; add 1s or 10s (no regrouping); add 3 numbers; add and subtract fractions; add and subtract fractions word problems; add and subtract within 20 word problems; add fractions with unlike denominators; add within 10; add within 1000; add within 20; add within 5; adding and subtracting decimals word problems; adding and subtracting in scientific notation; adding and subtracting negative fractions; adding and subtracting negative numbers; adding and subtracting rational numbers; adding and subtracting decimals word problems; adding and subtracting fractions; adding and subtracting mixed numbers 0.5; adding and subtracting mixed numbers 1; adding and subtracting polynomials; adding and subtracting radicals; adding and subtracting rational expressions 0.5; adding and subtracting rational expressions 1; adding and subtracting rational expressions 1.5; adding and subtracting rational expressions 2; adding and subtracting rational expressions 3; adding and subtracting rational numbers; adding and subtracting with unlike denominators 5; adding and subtracting with unlike denominators 6; adding decimals (hundredths); adding decimals (tenths); adding decimals and whole numbers (hundredths); adding decimals and whole numbers (tenths); adding decimals: thousandths; adding fractions; adding fractions 0.5; adding up to four 2-digit numbers; adding vectors; addition and subtraction word problems; addition and subtraction word problems 2; addition word problems within 100; age word problems; amplitude of sinusoidal functions from equation; analyze concavity; angle addition postulate; angle of complex numbers; approximation with local linearity; arc length; area and perimeter of rectangles word problems; area between two curves; area between two curves given end points; area between two polar curves; area bounded by polar curves; area bounded by polar curves intro; area of a circle; area of parallelograms; area problems; areas of circles and sectors; arithmetic sequences 1; arithmetic sequences 2; arithmetic series; average value of a function; average word problems; basic division; basic multiplication; basic partial derivatives; basic set notation; binomial probability formula; calculating binomial probability; center and radii of ellipses from equation; chain rule capstone; chain rule intro; change of variables: bound; change of variables: factor; circles and arcs; circulation form of green's theorem; classifying critical points; combinations; combined vector operations; combining like terms; combining like terms with distribution; combining like terms with negative coefficients; combining like terms with rational coefficients; complementary and supplementary angles; complete solutions to 2-variable equations; completing the square; completing the square (intermediate); completing the square (intro); complex numbers from absolute value and angle; complex plane operations; composite exponential function differentiation; composite numbers; conditional statements and truth value; construct exponential models; construct sinusoidal functions; continuity at a point (algebraic); converting between point slope and slope intercept form; converting between slope intercept and standard form; converting decimals to fractions 1; converting decimals to fractions 2; converting decimals to percents; converting fractions to decimals; converting mixed numbers and improper fractions; converting multi digit repeating decimals to fractions; converting multi-digit repeating decimals to fractions; converting percents to decimals; converting recursive and explicit forms of arithmetic sequences; converting recursive and explicit forms of geometric sequences; counting 1; counting 2; cube roots; cube roots 2; cumulative geometric probability; defined and undefined matrix operations; definite integral as the limit of a riemann sum; definite integrals of piecewise functions; definite integrals: common functions; definite integrals: reverse power rule; degrees to radians; density word problems; dependent probability; derivatives 1; derivatives of  and ; derivatives of sin(x) and cos(x); derivatives of tan(x), cot(x), sec(x), and csc(x); derivatives of  and ln(x); determinant of a 2x2 matrix; determinant of a 3x3 matrix; difference of squares; differentiability at a point: algebraic; differential equations: exponential model equations; differentiate integer powers (mixed positive and negative); differentiate polynomials; differentiate products; differentiate quotients; differentiate rational functions; differentiate related functions; differentiating using multiple rules; direct comparison test; direct substitution with limits that don't exist; direction of vectors; disc method: revolving around other axes; disc method: revolving around x- or y-axis; discount, markup, and commission word problems; discount, tax, and tip word problems; disguised derivatives; distance between point and line; distance formula; distributive property with variables; divide by 1; divide by 10; divide by 2; divide by 3; divide by 4; divide by 5; divide by 6; divide by 7; divide by 8; divide by 9; divide complex numbers; divide decimals by whole numbers; ...
\caption{Khan Academy modules in our AMPS pretraining dataset (Part 1).}
\label{fig:khan}
\end{figure*}

\begin{figure*}
\textbf{Khan Academy Modules (2/4)}: divide fractions by whole numbers; divide mixed numbers; divide polynomials by linear expressions; divide polynomials by monomials (with remainders); divide polynomials by x (no remainders); divide polynomials by x (with remainders); divide polynomials with remainders; divide powers; divide quadratics by linear expressions (no remainders); divide quadratics by linear expressions (with remainders); divide whole numbers by 0.1 or 0.01; divide whole numbers by decimals; divide whole numbers by fractions; divide whole numbers to get a decimal (1-digit divisors); divide whole numbers to get a decimal (2-digit divisors); divide with remainders (2-digit by 1-digit); dividing complex numbers; dividing decimals 1; dividing decimals 2; dividing decimals: hundredths; dividing decimals: thousandths; dividing fractions; dividing fractions word problems; dividing fractions word problems 2; dividing mixed numbers with negatives; dividing negative numbers; dividing polynomials by binomials 1; dividing polynomials by binomials 2; dividing polynomials by binomials 3; dividing positive and negative fractions; dividing positive fractions; dividing rational numbers; dividing whole numbers by fractions; dividing whole numbers by unit fractions; dividing whole numbers like 56/35 to get a decimal; divisibility 0.5; divisibility tests; domain of a function; double integrals with variable bounds; empirical rule; equation of a circle in factored form; equation of a circle in non factored form; equation of a hyperbola; equation of a parabola from focus and directrix; equation of an ellipse; equation of an ellipse from features; equations and inequalities word problems; equations of parallel and perpendicular lines; equations with parentheses; equations with parentheses: decimals and fractions; equations with variables on both sides; equations with variables on both sides: decimals and fractions; equivalent fractions; estimating square roots; evaluate composite functions; evaluate function expressions; evaluate functions; evaluate logarithms; evaluate logarithms (advanced); evaluate logarithms: change of base rule; evaluate piecewise functions; evaluate radical expressions challenge; evaluate sequences in recursive form; evaluating composite functions; evaluating expressions in 2 variables; evaluating expressions in one variable; evaluating expressions with multiple variables; evaluating expressions with multiple variables: fractions and decimals; evaluating expressions with one variable; evaluating expressions with variables word problems; evaluating logarithms; evaluating logarithms 2; expected value; explicit formulas for arithmetic sequences; explicit formulas for geometric sequences; exponent rules; exponential expressions word problems (algebraic); exponential model word problems; exponential vs. linear growth over time; exponents with integer bases; exponents with negative fractional bases; expressing ratios as fractions; expressions with unknown variables; expressions with unknown variables 2; extend arithmetic sequences; extend geometric sequences; extend geometric sequences: negatives and fractions; extraneous solutions to rational equations; factor higher degree polynomials; factor polynomials using structure; factor quadratics by grouping; factor using polynomial division; factor with distributive property (variables); factoring difference of squares 1; factoring difference of squares 2; factoring difference of squares 3; factoring linear binomials; factoring polynomials by grouping; factoring polynomials with two variables; factoring quadratics 1; factoring quadratics with a common factor; features of a circle from its expanded equation; features of a circle from its standard equation; features of quadratic functions; find area elements; find composite functions; find critical points; find critical points of multivariable functions; find inflection points; find inverses of rational functions; find missing divisors and dividends (1-digit division); find missing factors (1-digit multiplication); find missing number (add and subtract within 20); find the inverse of a 2x2 matrix; find the missing number (add and subtract within 1000); find trig values using angle addition identities; finding absolute values; finding curl in 2d; finding curl in 3d; finding derivative with fundamental theorem of calculus; finding derivative with fundamental theorem of calculus: chain rule; finding directional derivatives; finding divergence; finding gradients; finding inverses of linear functions; finding partial derivatives; finding percents; finding perimeter; finding tangent planes; finding the laplacian; finite geometric series; finite geometric series word problems; foci of an ellipse from equation; fraction word problems 1; fractional exponents; fractional exponents 2; fractions as division by a multiple of 10; function as a geometric series; function inputs and outputs: equation; function rules from equations; gcf and lcm word problems; general triangle word problems; geometric probability; geometric sequences 1; geometric sequences 2; geometric series formula; graphing points and naming quadrants; graphing systems of equations; greatest common factor; greatest common factor of monomials; higher order partial derivatives; identify composite functions; identify separable equations; identifying numerators and denominators; identifying slope of a line; imaginary unit powers; implicit differentiation; improper integrals; increasing and decreasing intervals; indefinite integrals:  and 1/x; indefinite integrals: sin and cos; independent probability; inequalities word problems; infinite geometric series; integer sums; integral test; integrals and derivatives of functions with known power series; integrals in spherical and cylindrical coordinates; ...\looseness=-1
\caption{Khan Academy modules in AMPS (Part 2).}
\label{fig:khan2}
\end{figure*}

\begin{figure*}
\textbf{Khan Academy Modules (3/4)}:
integrate and differentiate power series; integrating trig functions; integration by parts; integration by parts: definite integrals; integration using completing the square; integration using long division; integration using trigonometric identities; integration with partial fractions; intercepts from an equation; interpret quadratic models; interval of convergence; inverse of a 3x3 matrix; inverses of functions; iterated integrals; jacobian determinant; l'hopital's rule (composite exponential functions); l'hopital's rule: 0/0; l'hopital's rule: /; lagrange error bound; least common multiple; limits at infinity of quotients; limits at infinity of quotients with square roots; limits at infinity of quotients with trig; limits by direct substitution; limits by factoring; limits of piecewise functions; limits of trigonometric functions; limits using conjugates; limits using trig identities; line integrals in vector fields; linear equation and inequality word problems; linear equations with unknown coefficients; linear equations word problems; linear models word problems; logical arguments and deductive reasoning; maclaurin series of sin(x), cos(x), and ; make 10; manipulate formulas; markup and commission word problems; matrix addition and subtraction; matrix dimensions; matrix elements; matrix equations: addition and subtraction; matrix equations: scalar multiplication; matrix row operations; matrix transpose; mean, median, and mode; midline of sinusoidal functions from equation; midpoint of a segment; miscellaneous; model with one-step equations and solve; modeling with multiple variables; modeling with sinusoidal functions; modeling with sinusoidal functions: phase shift; motion along a curve (differential calc); motion problems (differential calc); motion problems (with integrals); multi-digit addition; multi-digit division; multi-digit multiplication; multi-digit subtraction; multi-step linear inequalities; multi-step word problems with whole numbers; multiplication and division word problems; multiplication and division word problems (within 100); multiply and divide complex numbers in polar form; multiply and divide powers (integer exponents); multiply and divide rational expressions (advanced); multiply binomials; multiply binomials by polynomials; multiply binomials intro; multiply by 0 or 1; multiply by 2 and 4; multiply by 5 and 10; multiply by tens word problems; multiply complex numbers; multiply decimals (1 and 2-digit factors); multiply decimals (up to 4-digit factors); multiply difference of squares; multiply matrices; multiply matrices by scalars; multiply mixed numbers; multiply monomials; multiply monomials by polynomials; multiply powers; multiply unit fractions and whole numbers; multiply whole numbers and decimals; multiplying and dividing in scientific notation; multiplying a matrix by a matrix; multiplying a matrix by a vector; multiplying and dividing complex numbers in polar form; multiplying and dividing negative numbers; multiplying and dividing rational expressions  1; multiplying and dividing rational expressions 2; multiplying and dividing rational expressions 3; multiplying and dividing rational expressions 4; multiplying and dividing rational expressions 5; multiplying and dividing scientific notation; multiplying by multiples of 10; multiplying complex numbers; multiplying decimals like 0.847x3.54 (standard algorithm); multiplying decimals like 2.45x3.6 (standard algorithm); multiplying decimals like 4x0.6 (standard algorithm); multiplying expressions 1; multiplying fractions; multiplying fractions by integers; multiplying mixed numbers 1; multiplying negative numbers; multiplying polynomials; multiplying polynomials 0.5; multiplying positive and negative fractions; multiplying rational numbers; multivariable chain rule; multivariable chain rule intro; negative exponents; new operator definitions 1; new operator definitions 2; normal form of green's theorem; number of solutions of quadratic equations; one step equations; one step equations with multiplication; one-step addition and subtraction equations; one-step addition and subtraction equations: fractions and decimals; one-step equations with negatives (add and subtract); one-step equations with negatives (multiply and divide); one-step inequalities; one-step multiplication and division equations; one-step multiplication and division equations: fractions and decimals; operations with logarithms; order of operations; order of operations (no exponents); order of operations 2; order of operations challenge; order of operations with negative numbers; ordered pair solutions to linear equations; p-series; parametric curve arc length; parametric equations differentiation; parametric velocity and speed; partial derivatives of vector valued functions; partial fraction expansion; partial sums intro; particular solutions to differential equations; particular solutions to separable differential equations; parts of complex numbers; percent problems; perfect squares; period of sinusoidal functions from equation; permutations; permutations and combinations; planar motion (differential calc); planar motion (with integrals); polar and rectangular forms of complex numbers; polynomial special products: difference of squares; polynomial special products: perfect square; positive and zero exponents; positive exponents with positive and negative bases; potential functions; power rule (negative and fractional powers); power rule (positive integer powers); power rule (with rewriting the expression); powers of complex numbers; powers of fractions; powers of powers; prime numbers; probabilities of compound events; probability 1; probability in normal density curves; probability of ``at least one'' success; probability with permutations and combinations; problems involving definite integrals (algebraic); ...\looseness=-1
    \caption{Khan Academy modules in AMPS (Part 3).}
    \label{fig:khan3}
\end{figure*}

\begin{figure*}
\textbf{Khan Academy Modules (4/4)}:
properties of exponents (rational exponents); proportion word problems; pythagorean identities; pythagorean theorem; quadratic word problems (factored form); quadratic word problems (standard form); quadratic word problems (vertex form); quadratics by factoring; quadratics by taking square roots; radians and degrees; radians to degrees; radical equations; radius, diameter, and circumference; range of a function; rate conversion; rate problems; rate problems 2; rates of change in other applied contexts (non-motion problems); rates with fractions; ratio test; ratio word problems; reciprocal trig functions; recursive formulas for arithmetic sequences; recursive formulas for geometric sequences; regroup when adding 1-digit numbers; relate addition and subtraction; related rates (advanced); related rates (multiple rates); related rates (pythagorean theorem); related rates intro; relationship between exponentials and logarithms; relative minima and maxima; remainder theorem; remainder theorem and factors; removable discontinuities; represent linear systems with matrices; represent linear systems with matrix equations; reverse power rule; reverse power rule: negative and fractional powers; reverse power rule: rewriting before integrating; reverse power rule: sums and multiples; rewriting decimals as fractions challenge; right triangle trigonometry word problems; roots of decimals and fractions; sample and population standard deviation; scalar matrix multiplication; scalar multiplication; scientific notation; secant lines and average rate of change; secant lines and average rate of change with arbitrary points; secant lines and average rate of change with arbitrary points (with simplification); second derivative test; second derivatives (implicit equations); second derivatives (parametric functions); second derivatives (vector-valued functions); segment addition; separable differential equations; significant figures; simplify roots of negative numbers; simplify square roots (variables); simplify square-root expressions; simplifying expressions with exponents; simplifying fractions; simplifying radicals; simplifying radicals 2; simplifying rational expression with exponent properties; simplifying rational expressions 2; simplifying rational expressions 3; simplifying rational expressions 4; sinusoidal models word problems; slope-intercept from two points; solid geometry; solutions to quadratic equations; solutions to systems of equations; solve equations using structure; solve exponential equations using exponent properties; solve exponential equations using exponent properties (advanced); solve exponential equations using logarithms: base-10 and base-e; solving equations in terms of a variable; solving for the x intercept; solving for the y intercept; solving proportions; solving quadratics by completing the square 1; solving quadratics by completing the square 2; solving quadratics by factoring; solving quadratics by factoring 2; solving quadratics by taking the square root; solving rational equations 1; solving rational equations 2; special right triangles; square and cube challenge; square roots of perfect squares; standard deviation; standard deviation of a population; stokes' theorem; substitution with negative numbers; subtract decimals (hundredths); subtract decimals and whole numbers (hundredths); subtract within 10; subtract within 1000; subtract within 20; subtract within 5; subtracting decimals (tenths); subtracting decimals and whole numbers (tenths); subtracting decimals: thousandths; subtracting fractions; subtracting fractions with common denominators; subtracting fractions with unlike denominators; subtraction word problems within 100; summation notation intro; sums of consecutive integers; surface integrals to find surface area; switching bounds on double integrals; symbols practice: the gradient; systems of equations; systems of equations with elimination; systems of equations with simple elimination; systems of equations with substitution; systems of equations word problems; tangents to polar curves; taylor and maclaurin polynomials; the derivative and tangent line equations; the divergence theorem; the fundamental theorem of calculus and definite integrals; the hessian matrix; translate one-step equations and solve; trigonometry 0.5; trigonometry 1; trigonometry 1.5; trigonometry 2; triple integrals; two-step equations; two-step equations with decimals and fractions; two-step equations word problems; u-substitution: definite integrals; u-substitution: indefinite integrals; unit circle; unit vectors; use arithmetic sequence formulas; use geometric sequence formulas; use the properties of logarithms; use the pythagorean identity; using the mean value theorem; using the quadratic formula; using units to solve problems; variance; vector word problems; vector-valued functions differentiation; verify solutions to differential equations; vertex of a parabola; volume word problems; volumes with cross sections: squares and rectangles; volumes with cross sections: triangles and semicircles; washer method: revolving around other axes; washer method: revolving around x- or y-axis; word problems with ``more'' and ``fewer'' 2; write common decimals as fractions; write common fractions as decimals; write decimals as fractions; write differential equations; write equations of parallel and perpendicular lines; writing basic expressions with variables; writing basic expressions word problems; writing expressions; writing expressions 2; writing expressions with variables; writing expressions word problems; writing functions with exponential decay; writing linear functions word problems; writing proportional equations; writing proportions; wrong statements in triangle proofs; z scores 1; z scores 2; z scores 3; zero product property.\looseness=-1
    \caption{Khan Academy modules in AMPS (Part 4).}
    \label{fig:khan4}

\end{figure*}

\section{Checklist Information}\label{appendix:checklist}
\paragraph{Legal Compliance.}
We create and collect various mathematics problems to create MATH and AMPS.

AMPS consists of problems generated with Mathematica and Khan Academy code. Mathematica serves as a calculator and does not copyright its numerical answer outputs, in much the same way that other calculators do not copyright computations such as . Khan Academy's exercise framework follows an MIT License. Since we provide attribution, reuse is not restrictive save for attribution requirements.

MATH problems are created by the Mathematical Association of America (MAA). Although we do not commercialize MATH, we should like to demonstrate that we are far from the boundary for action or infringement. For decades, the MAA has not protected its problem IP even from separate organizations which sell MAA problems, such as AoPS. Courts have ruled that this implies the IP rights are permanently forfeited. We raise this point only to demonstrate the extent to which our reuse for research is within the law, because even commercial reuse of MAA problems is within the law and commonplace. Even so, the MATH dataset is not sold and is likely to have no effect on the value of the original problems. This analysis would be pertinent in the hypothetical situation where Fair Use doctrine did not exist, but MATH and AMPS are covered by Fair Use.

For MATH and AMPS, we abide by Fair Use 107: ``the fair use of a copyrighted work, including such use by ... scholarship, or research, is not an infringement of copyright'', where fair use is determined by ``the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes'' and ``the effect of the use upon the potential market for or value of the copyrighted work.''

\paragraph{Dataset Intended Uses.} We document the dataset within the paper and note that the dataset and code for reproducing results is available at \href{https://github.com/hendrycks/apps}{https://github.com/hendrycks/apps}. We do not intend for this dataset to train models that help students cheat on mathematics exams. We intend for others to use this dataset in order to better forecast reasoning capabilities.

\paragraph{Author Statement and License.}
We bear all responsibility in case of violation of rights. The MATH data, AMPS data, and our open source code are under an MIT license.
 
\end{document}
