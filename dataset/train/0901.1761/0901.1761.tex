\documentclass[a4paper,10pt]{article}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\usepackage[algoruled,english,linesnumbered,lined]{algorithm2e} 

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcounter{todo}
\newcommand{\todo}[1]{\addtocounter{todo}{1}
{\bf TODO \thetodo: #1}}


\newcommand{\rank}{p}
\newcommand{\Exp}[1]{{\rm E}[ #1 ]}
\newcommand{\Expbig}[1]{{\rm E}\left[ #1 \right]}

\newcommand{\seq}[1]{\left\langle #1\right\rangle}
\newcommand{\seqGilt}[2]{\left\langle #1\gilt #2\right\rangle}
\newcommand{\Id}[1]{\ensuremath{\mathit{#1}}}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\enorm}[1]{\norm{#1}_{2}}
\newcommand{\sumnorm}[1]{\norm{#1}_{1}}
\newcommand{\maxnorm}[1]{\norm{#1}_{\infty}}
\newcommand{\xor}{\oplus}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\gilt}{:}
\newcommand{\sodass}{\mid}
\newcommand{\setGilt}[2]{\left\{ #1\gilt #2\right\}}
\newcommand{\Def}{:=}
\newcommand{\zvektor}[2]{\left(#1,#2\right)}
\newcommand{\vektor}[2]{\left(\begin{smallmatrix}#1\\#2\end{smallmatrix}\right)}
\newcommand{\condition}[1]{\left[#1\right]}
\newcommand{\binomial}[2]{\binom{#1}{#2}}
\newcommand{\even}{\mathrm{even}}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\mymod}{\,\bmod\,}



\newcommand{\nat}{\mathbb{N}}
\newcommand{\natnull}{\mathbb{N}_{0}}
\newcommand{\natless}[1]{\mathbb{N}_{#1}}
\newcommand{\nplus}{\mathbb{N}_+}
\newcommand{\real}{\mathbb{R}}
\newcommand{\rplus}{\mathbb{R}_+}
\newcommand{\rnneg}{\mathbb{R}_*}
\newcommand{\integer}{\mathbb{Z}}
\newcommand{\intint}[2]{{#1}..{#2}}
\newcommand{\realrange}[2]{\left[#1, #2\right]}
\newcommand{\realrangeo}[2]{\left(#1, #2\right)}
\newcommand{\realrangelo}[2]{\left(#1, #2\right]}
\newcommand{\realrangero}[2]{\left[#1, #2\right)}
\newcommand{\unitrange}[2]{\realrange{0}{1}}
\newcommand{\bool}{\set{0,1}}
\newcommand{\mapping}[2]{{#2}^{#1}}
\newcommand{\powerset}[1]{{\cal P}\left(#1\right)}
\newcommand{\NP}{\mathbf{NP}}
\newcommand{\Bild}{\mathbf{Bild}\:}

\newcommand{\withtype}[1]{\in#1}

\newcommand{\prob}[1]{{\mathbb{P}}\left[#1\right]}
\newcommand{\condprob}[2]{{\mathbb{P}}\left(#1\;|\;#2\right)}
\newcommand{\condexpect}[2]{{\mathbb{E}}\left(#1\;|\;#2\right)}
\newcommand{\expect}{{\mathbb{E}}}
\newcommand{\var}{{\mathbb{V}}}
\newcommand{\quant}[2]{\tilde{#1}_{#2}}

\newcommand{\whpO}[1]{\tilde{\mathrm{O}}\left( #1\right)}
\newcommand{\Oschlange}{}
\newcommand{\Ohh}[1]{\mathcal{O}\!\left( #1\right)}
\newcommand{\Oh}[1]{\mathcal{O}\!\left( #1\right)}
\newcommand{\Ohlarge}[1]{\mathcal{O}\!\left( #1\right)}
\newcommand{\Ohsmall}[1]{\mathcal{O}(#1)}
\newcommand{\oh}[1]{\mathrm{o}\!\left( #1\right)}
\newcommand{\Th}[1]{\Theta\!\left( #1\right)}
\newcommand{\Om}[1]{\Omega\left(#1\right)}
\newcommand{\om}[1]{\omega\!\left( #1\right)}
\newcommand{\Oleq}{\preceq}

\newcommand{\lref}[1]{\ref{\labelprefix:#1}}
\newcommand{\llabel}[1]{\label{\labelprefix:#1}}
\newcommand{\labelprefix}{} 

\newcommand{\discussionsize}{\small}
\newenvironment{discussion}{\par\discussionsize}{\par}

\newcommand{\frage}[1]{{ \sf[#1]\marginpar{?} }}

\newcommand{\mysubsubsection}[1]{\vspace{2mm}\noindent{\bf #1 }}

\newcommand{\punkt}{\enspace .}

\newenvironment{code}{\noindent \begin{tabbing}\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\hspace{2em}\=\kill}{\end{tabbing}}

\newcommand{\labelcommand}{}
\newcommand{\captiontext}{}
\newsavebox{\codeparam}
\newcounter{lineNumber}
\newenvironment{disscodepos}[3]{\renewcommand{\labelcommand}{#2}\renewcommand{\captiontext}{#3}\sbox{\codeparam}{\parbox{\textwidth}{#3}}\begin{figure}[#1]\begin{center}\begin{code}\setcounter{lineNumber}{1}}{\end{code}\end{center}\caption{\llabel{\labelcommand}\captiontext}\end{figure}}

\newcommand{\Set}      {{\bf set\ }}
\newcommand{\Boolean}  {\ensuremath{\set{0,1}}}
\newcommand{\Integer}  {}
\newcommand{\True}     {\mathtt{1}}
\newcommand{\False}    {\mathtt{0}}
\newcommand{\Bitand}   {{\bf bitand\ }}
\newcommand{\Xor}       {\ensuremath{\oplus}}
\newcommand{\Not}       {\ensuremath{\neg}}
\newcommand{\Or}       {\ensuremath{\vee}}
\newcommand{\Div}       {{\bf\ div\ }}
\newcommand{\Mod}       {{\bf\ mod\ }}
\newcommand{\Decrement}  {\ensuremath{\mathbf{-}\mathbf{-}\ }}
\newcommand{\Increment}  {\ensuremath{\mathbf{+}\mathbf{+}\ }}
\newcommand{\End}       {{\bf end\ }}
\newcommand{\Endfor}       {{\bf endfor\ }}
\newcommand{\Rem}[1]   {{\bf //\hspace{0.5mm}{\rm#1}}}
\newcommand{\RRem}[1]   {\`\blau{\bf //\hspace{0.5mm}~}{\blau{\rm#1}}}
\newcommand{\Flush}[1]   {\`{\bf \hspace{0.5mm}~}{\rm#1}}
\newcommand{\RRemNL}[1]   {\`{\bf (*~ }{\rm#1}{\bf ~*)}{\tiny\arabic{lineNumber}}\stepcounter{lineNumber}}
\newcommand{\Declare}[2]{#1\mbox{ \rm : }#2}
\newcommand{\DeclareInit}[3]{#1#3 \mbox{ \rm : }#2}
\newcommand{\At}[1]{\left\langle#1\right\rangle}
\newcommand{\NL}{\`{\tiny\arabic{lineNumber}}\stepcounter{lineNumber}}

\newcommand{\iProc}{i_\mathrm{PE}}

\newcommand{\dissepslong}[5]{\begin{figure}[#1]\begin{center}\epsfxsize#2\leavevmode\epsfbox{#3.eps}\end{center}\caption{\llabel{#4}#5}\end{figure}}

\newcommand{\dissepspos}[4]{\dissepslong{#1}{#2}{\labelprefix/#3}{#3}{#4}}
\newcommand{\disseps}[3]{\dissepspos{htb}{#1}{#2}{#3}}

\newdimen\endofsize\endofsize=0.5em
\def\endofbeweis{~\quad\hglue\hsize minus\hsize
                 \hbox{\vrule height \endofsize width
\endofsize}\par}
\newenvironment{myproof}{\begin{proof}}{\endofbeweis\end{proof}}




\newcommand{\invisible}[1]{\setbox0=\hbox{#1}\makebox[\wd0]{\raisebox{0pt}[\ht0][\dp0]{}}}

\newcommand{\Z}[0]{{\mathbb{Z}}}



\newcommand{\R}[0]{{\mathbb{R}}}

\newcommand{\C}[0]{{\mathbb{C}}}

\newcommand{\F}[0]{{\rm\sf F}}

\newcommand{\GL}[0]{{\rm\sf GL}}

\newcommand{\GF}[1]{{\mathbb{F}_{#1}}}

\newcommand{\bigsqcap}[1]{
  \hspace{-1.0em}\raisebox{-1.0ex}{
    \renewcommand{\arraystretch}{0.5}
    \sqcap#1}\!\!}

\newcommand{\combinations}[2]{{\renewcommand{\arraystretch}{0.8}\setlength{\arraycolsep}{0.2em}\left( \begin{array}{c} {#1} \\ {#2} \end{array} \right)}}

\newcommand{\smallexpr}[1]{\mbox{\footnotesize}}

\newcommand{\smallsmallexpr}[1]{\mbox{\scriptsize}}











\newenvironment{example}{\par\vspace{1.0ex}\noindent{\bf Example}\ }{\par\vspace{0.0ex}\noindent}








 \begin{document}


\title{Towards Optimal Range Medians}

\author{Beat Gfeller \\ {\small ETH Zurich, Switzerland} \\ \small{ \texttt{gfeller@inf.ethz.ch}} \and Peter Sanders\thanks{Partially supported by DFG grant SA 933/3-1.}  \\ {\small Universit\"at Karlsruhe, Germany} \\ {\small \texttt{sanders@ira.uka.de}} \smallskip}

\maketitle

\begin{abstract}
  We consider the following problem: given an unsorted array of 
  elements, and a sequence of intervals in the array,
  compute the median in each of
  the subarrays defined by the intervals.  We describe a simple
  algorithm which uses  space and needs  time
  to answer the first   queries.
  This improves previous algorithms by a logarithmic factor and matches a lower bound
  for .
Since the algorithm decomposes the range of element values rather than the array,
  it has natural generalizations to higher dimensional problems -- it reduces a range median query
  to a logarithmic number of range counting queries. 
\end{abstract}


\section{Introduction and Related Work}

The classical problem of finding the \emph{median} is to find the element of rank  in an unsorted array of  elements.\footnote{An element has rank  if it is the -th element in some sorted order. Actually, any specified rank might be of interest. We restrict 
ourselves to the median to simplify notation but a generalization to arbitrary ranks will be straightforward for all our results.}
Clearly, the median can be found in  time by sorting the elements. However, a classical algorithm 
finds the median in  time \cite{DBLP:conf/stoc/BlumFPRT72}, which is asymptotically optimal.



More recently, the following generalization, called the \emph{Range Median Problem (RMP)}, has been considered \cite{DBLP:journals/njc/KrizancMS05,DBLP:conf/esa/Har-PeledM08}:

\noindent{\bf Input:} An unsorted array  with  elements, each
having a \emph{value}. Furthermore, a sequence of  \emph{queries}
, each defined as an interval .
In general, the sequence is given in an online
fashion, 
we want an answer after every query, and  is not known in advance.

\noindent{\bf Output:} A sequence  of values, where
 is the median of the elements in .  Here, 
denotes the set of all elements whose index in  is at least
 and at most .


This RMP naturally fits into a larger group of problems, in which an
unsorted array is given, and in a query one wants to compute a certain function of all the elements in a
given interval. 
Instead of the median, natural candidates for such a function are:
\begin{itemize}
 \item Sum: this problem can be trivially solved in  preprocessing time and  query time by computing prefix sums.
\item Semigroup operator: this problem is significantly more difficult than the sum. However, there exists a very efficient solution: for any constant , a preprocessing in  time and space allows to answer queries in  time,
where  is a certain functional inverse of the Ackerman function \cite{802185}. A matching lower bound was given in \cite{DBLP:journals/siamcomp/Yao85a}.
\item Maximum, Minimum: This is a special case of a semigroup operator, for which the problem can be solved slightly more efficiently:  preprocessing time and space is sufficient to allow  time queries (see e.g. \cite{DBLP:journals/tcs/BenderF04}).
\end{itemize}
In addition to being a natural extension of the median problem, the RMP has some applications in practice, namely obtaining a ``typical'' element in a given time series out of a given time interval \cite{DBLP:conf/esa/Har-PeledM08}.

Natural special cases of the RMP are an \emph{offline} variant, where all queries are given 
in a batch and a variant where we want to do all \emph{preprocessing up front} and are then interested in
good worst case bounds for answering a single query.


The authors of \cite{DBLP:conf/esa/Har-PeledM08} give a solution of
the RMP which requires  time and
 space. In addition, they give a lower bound of
 time for comparison-based algorithms.
They basically use a one-dimensional range tree over the input array, where each inner node corresponds to a
subarray defined by an interval. Each such subarray is  sorted, and stored with the node. 
A range median query then corresponds to selecting the median from   sorted subarrays (whose union is the queried subarray) of total length , which requires  time. 
The main difficulty of their approach is to show that  the subarrays need  not be fully sorted, but only presorted in a particular way, which 
reduces the construction time of the tree from  to .

Concerning the preprocessing variant of the RMP,
\cite{DBLP:journals/njc/KrizancMS05} give a data structure and answers queries in  time,
which uses  space
They do not analyze the required preprocessing time, but it is clearly at least as large as the required space in words.
Moreover, they give a structure which uses only  space, but query time  for arbitrary .
 Another data structure given in \cite{DBLP:conf/sofsem/Petersen08a}
can answer queries in  time and uses  space, where  is an arbitrary integer, and  is the  times iterated logarithm of .\footnote{Note that the data structures in \cite{DBLP:journals/njc/KrizancMS05,DBLP:conf/sofsem/Petersen08a} 
work only for a specific quantile (e.g. the median), which must be the same for all queries.}


\subsubsection*{Our results.}
First, in Section~\ref{s:pointermachine} we give an algorithm for the
pointer-machine model which solves the RMP in 
time and  space. This improves the running time of  reported in
\cite{DBLP:conf/esa/Har-PeledM08} for .  Our
algorithm is also considerably simpler.  The idea is to reduce a
range median query to a logarithmic number of related range counting
queries by recursively partitioning the values in array  in a tree
in a fashion similar to Quicksort.  The final time bound is achieved
using the technique of Fractional Cascading.
In Section~\ref{ss:lowerbound}, we explain why our algorithm is optimal for  and at most
 from optimal for .



Section~\ref{s:RAM} achieves linear space in the RAM model using
techniques from succinct data structures -- the range counting
problems are reduced to rank computations in bit arrays. To achieve
the desired bound, we compress the recursive subproblems in such a way
that the bit arrays remain dense at all times.

The latter algorithm can be easily modified to obtain a data structure using  space, can be constructed in  time, and allows to answer arbitrary range median queries (or an arbitrary rank, which may be specified online together with the query interval) in  time. 
Note that the previously best linear-space data structure required  query time \cite{DBLP:journals/njc/KrizancMS05}.


After a few remarks on generalizations for higher dimensional inputs
in Section~\ref{s:highd}, we discuss dynamic variants of the RMP problem in Section~\ref{s:dynamic}, before Section~\ref{s:conclusions} concludes with a
summary and some open problems.



\section{A Pointer Machine Algorithm}
\label{s:pointermachine}

\begin{figure}
\begin{center}
\includegraphics{figs/key-observation}
\end{center}
\caption{An example for the array .
The median value used to split the set of points is . For the query , there are two elements inside  and four elements in . Hence, the median of  is the element of rank  in . }
\label{fig:key-observation}
\end{figure}

Our algorithm is based is the following key observation
(see also Figure~\ref{fig:key-observation}): Suppose that we partition
the elements in array  of length  into two smaller arrays:
 which contains all elements with the 
smallest\footnote{To simplify
  notation we ignore some trivial rounding issues and also sometimes
  assume that all elements have unique values. This is without loss of
  generality because we could artificially expand the size  to the next power of two and
  because we can use the position of an element in  to break ties in element comparisons.} values in , and  which contains all elements with the
 largest values. The elements in  and  are sorted
by their index in , and each element  in  and  is
associated with its index  in the original input array, and its value .  Now, if
we want to find the element of rank  in the subarray ,
we can do the following: We count the number  of elements in 
which are contained in .  To obtain , we can do a binary
search for both  and  in  (using the  fields).  If
, then the element of rank  in  is
 the element of rank  in . Otherwise, the
element of rank  is  the element of rank  in
.

Hence, using the partition of  into  and , we can reduce the problem of finding an element of a given rank in array 
to the same problem, but on a smaller array (either  or ). Our algorithm applies this reduction recursively.

\paragraph*{Algorithm overview.}
The basic idea is therefore to subdivide the  elements in the array
into two parts of (almost) equal size by computing the median of
their values and using it to split the list into a list of
the 
elements with smaller values and a list of the 
elements with larger values.  The two parts are recursively subdivided
further, but only when required by a query.
To answer a range median query, we determine in which of the two parts
the element of the desired rank lies (initially, this rank corresponds
to the median, but this may change during the search).  Once this is
known, the search continues recursively in the appropriate part until
a trivial problem of constant size  is encountered.


We will show that the total work involved in splitting the subarrays
is  and that the search required for any query can be completed in  time using Fractional
Cascading \cite{fractionalcascading}.
Hence, the total running time is .

\paragraph*{Detailed description and analysis.}
\begin{algorithm}
\caption{Query()} \label{alg:query}
\dontprintsemicolon
{\bf Input:} range select data structure  of elements, query range , desired rank \;
\lIf{}{\Return } \;
\uIf{ is undefined}{
  Compute median  value of the pairs in  \;
\;
  

    is an array containing all elements  of  satisfying the given condition , ordered as in   
}
\{ Find() returns   (with Find)  \} \; 
 Find(  \tcp*{\# of low elements left of }\;
 Find(  \tcp*{\# of low elements up to }\;
 \tcp*{\# of low elements between  and }\;
  \lIf{  }{  \Return Query.low, , , }\;  \lElse{\hspace*{1.75cm}  \Return Query.high, , , } 

\end{algorithm}

Algorithm~\ref{alg:query} gives pseudo-code for the query,
which performs preprocessing (i.e., splitting the array into two smaller arrays) 
only where needed. Note that we have to keep three things separate here: values that
are relevant for median computation and partitioning the input, positions in the input sequence
that are relevant for finding the elements within the range , and positions in 
the subdivided arrays that are important for counting elements.

Let us first analyze the time required for processing a query not
counting the `preprocessing' time within lines 4--6: The query
descends  levels of recursion. On each level,
\Id{Find}-operations for  and  are performed on the lower half
of the current subproblem.  If we  used binary search, we would
get a total execution time of up to .  However, the fact that in all
these searches, we search for the same key ( or ) allows us to use
a standard technique called Fractional Cascading
\cite{fractionalcascading} that reduces the search time to a constant,
once the position in the first search is known. Indeed, we only need a rather basic
variant of Fractional Cascading, which applies when each
successor list is a sublist of the previous one \cite{BKOS97}.  Here,
it suffices to augment an element  of a list with a pointer to the
position of some element  in each subsequent list (we
have two successors --  and ).  In our case, we need
to point to the largest element in the successor that is no larger
than .  We get a total search time of . 



Now we turn to the preprocessing code in lines~4--6 of
Algorithm~\ref{alg:query}.  Let  denote the level of recursion
at which query~ encountered an undefined array  for the
first time. Then the preprocessing time performed during query  is
 if a linear time algorithm is used for median
selection \cite{DBLP:conf/stoc/BlumFPRT72} (note that we have a linear
recursion with geometrically decreasing execution times).
This preprocessing time also includes the cost of 
finding the pointers for Fractional Cascading while splitting the list in lines~4--6.
Since the preprocessing time during query 
decreases with , the total preprocessing time is maximized if
small levels  appear as often as possible.  However, level~
can appear no more than  times in the
sequence .\footnote{Indeed, for  the maximal number is  since the other half of the available subintervals have already been covered by the preprocessing happening in the layer above.}  Hence,
we get an upper bound for the preprocessing time when the smallest 
levels are used as often as possible (`filled') and the remaining
levels are .  The preprocessing time at every used
level is  giving a total time of . The same bound
applies to the space consumption since we never allocate memory that
is not used later.
We summarize the main result of this section in a theorem:
\begin{theorem}
  The online range median problem (RMP) on an array with  elements
  and  range queries can be solved in time   and
  space .
\end{theorem}


Another variant of the above algorithm invests  time and space
into complete preprocessing up front. Subsequently,  any range median query can be answered in  time.
This improves the preprocessing
space of the corresponding result in \cite{DBLP:journals/njc/KrizancMS05} by a factor  and the preprocessing time by at least this factor.


\subsection{Lower Bounds}
\label{ss:lowerbound}


We shortly discuss how far our algorithm is from optimality.
In \cite{DBLP:conf/esa/Har-PeledM08}, 
a comparison-based lower bound of  is shown for the range median problem 
\footnote{The authors derive a lower bound of , where  is a multiple of . Unfortunately, the analysis of the asymptotics of  given in \cite{DBLP:conf/esa/Har-PeledM08} is erroneous; however, a corrected analysis shows that the claimed  bound holds.}. 
As our algorithm shows, this bound is (asymptotically) tight if .
For larger , the above lower bound is no longer valid, as the construction requires . 
Yet, a lower bound of  is immediate for , considering only the first  queries.
Furthermore,  is a trivial lower bound.
Note that in the analysis of our algorithm, the number of levels of the recursion is
actually bounded by , and 
thus
for any  our algorithm has running time ,
which is up to  from optimal for any .

In a very restricted model (sometimes called ``Pointer Machine''), where a memory location can be reached only by following pointers, and not by direct addressing, 
our algorithm is indeed optimal also for : it takes  time to even access an arbitrary element of the input.
Since every element of the input is the answer to at least one range query (e.g. the query whose range contains only this element), the bound follows.
An interesting question is whether a lower bound  could be shown in stronger models. However, note that any comparison-based lower bound (as the one in  \cite{DBLP:conf/esa/Har-PeledM08}) cannot be higher than : With  comparisons, an algorithm can determine the permutation of the array elements, which suffices to answer any query without further element comparisons.
Therefore, one would need to consider stronger models (e.g. the ``cell-probe'' model), in which proving
lower bounds is significantly more difficult.








\section{A Linear Space RAM Implementation}\label{s:RAM}


\begin{algorithm}\caption{Query()} \label{alg:linearquery}
\dontprintsemicolon
{\bf Input:} range select data structure , query range  and desired rank  \;
\lIf{}{\Return } \;
\uIf{ is undefined}{
  Compute median  value of the values in  \;
  .lowbits := BitVector \;
  .low\hspace*{1.5mm}    :=  \;
  .high    :=  \;
  deallocate the value array of  itself
}
 .lowbits.rank  \;
 .lowbits.rank  \;
\\
\lIf{  }{  \Return Query.low, , , }\;  \lElse{\hspace*{1.75cm}  \Return Query.high, , , } \end{algorithm}

Our starting point for a more space efficient implementation of
Algorithm~\ref{alg:query} is the observation that we do not actually
need all the information available in the arrays stored at the
interior nodes of our data structure. All we need is support for the
operation \Id{Find} that counts the number of elements  in
 that have index .  This information can already be
obtained from a bit-vector where a -bit indicates whether an
element of the original array  is in . For this bit-vector, the operation
corresponding to \Id{Find} is called \Id{rank}.  In the RAM model, there are data
structures that need space  bits,  can be constructed in
linear time and support \Id{rank} in constant time (e.g.,
\cite{Cla88,OkaSad06}\footnote{Indeed, since we only need the rank operation, there are very simple and efficient implementations: store a table with ranks for
indices that are a multiple of . General ranks are then the sum of the next smaller table entry and the number of -bits in the bit array between this rounded position and
the query position. Some processors have a POPCNT instruction for this purpose. Otherwise we can use lookup tables.}).  Unfortunately, this idea alone is not enough
since we would need to store  bit arrays consisting of 
position each on level .  Summed over all levels, this would still
need  bits of space even if optimally compressed data
structures were used.  This problem is solved using an additional
idea: for a node  of our data structure  with value array , we do not store a bit
array with  possible positions but only with  possible
positions, i.e., bits represent positions in  rather than in the
original input array. This way, we have  positions on every
\emph{level} leading to a total space consumption of  
bits.  For this idea to work, we need to be able to transform the
query range in the recursive call in such a way that \Id{rank}
operations in the contracted bit arrays are meaningful. Fortunately,
this is easy because the rank information we compute also defines the
query range in the contracted arrays. Algorithm~\ref{alg:linearquery}
gives pseudocode specifying the details.  Note that the algorithm is
largely analogous to Algorithm~\ref{alg:query}.  In some sense, the
algorithm becomes simpler because the distinction between query
positions and array positions for counting disappears (If we still
want to report the positions of the median values in the input, we can
store this information at the leaves of the data structure using
linear space). Using an analysis analogous to the analysis of
Algorithm~\ref{alg:query}, we obtain the following theorem:

\begin{theorem}
  The online range median problem (RMP) on an array with  elements
  and  range queries can be solved in time  and
  space  words on the RAM model.
\end{theorem}

By doing all the proprocessing up front, we obtain an algorithm
with preprocessing time  using  space and query time .
This improves the space consumption compared to \cite{DBLP:journals/njc/KrizancMS05} by a factor . 



\section{Higher Dimensions}\label{s:highd}

Since our algorithm decomposes the values rather than the positions of
elements, it can be naturally generalized to higher dimensional point
sets. We obtain an algorithm that needs  preprocessing
time plus the time for supporting range counting queries on each
level.  The amortized query time is the time for  range
counting queries.  Note that query ranges can be specified in any way
we wish: (hyper)-rectangles, circles, etc., without affecting the way
we handle values.  For example, using the
data structure for 2D range counting from \cite{DBLP:conf/isaac/JaJaMS04} we obtain a
data structure for the 2D rectangular range median problem that needs
 preprocessing time,  space, and
 query time. This not only applies to 2D arrays consisting
of  input points put to arbitrary two-dimensional point sets with
 elements.  

Unfortunately, further improvements, e.g. to logarithmic query time
seem difficult. Although the query range is the same at all levels of
recursion, Fractional Cascading becomes less effective when the result of a
rectangular range counting query is defined by more than a constant
number of positions within the data structure because we would
have to follow many forwarding pointers.  Also, the
array contraction trick that allowed us to use 
dense bit arrays
in Section~\ref{s:RAM} does not work anymore because an array with
half the number of bits need not contain any empty rows or columns.

Another indication that logarithmic query time in two dimensions might
be difficult to achieve is that there has been intensive work on the
more specialized median-filtering problem in image processing where we
ask for \emph{all} range medians with query ranges that are squares of
size  in an image with  pixels. The best previous algorithms known here need
time \cite{GilWerman} unless the range of values is very small
\cite{2Dmedian1,2Dmedian2}. Our result above improves this by a factor  
(by applying the general algorithm to input pieces of size ) but this
seems to be of theoretical interest only.


\section{Dynamic Range Medians}
\label{s:dynamic}


In this section, we consider a dynamic variant of the RMP,
where we have a linked list instead of an array,
and elements can be deleted or inserted arbitrarily.
In this setting, we still want to answer median queries, whose range is given by two pointers to the first and the last
element in the query range. 

In the following, we sketch a solution which allows  inserts and deletes   in  amortized time each, and range median queries in  worst case time. 
The basic idea is to use a BB() tree \cite{DBLP:conf/stoc/NievergeltR72} as a primary structure, in which all elements are ordered by their \emph{value}.
With each inner node, we associate a secondary structure, which contains all the elements of the node's subtree, ordered by their position in the input list.
More precisely, we store these elements in a balanced binary search tree, where nodes are augmented with a field indicating the size of their subtree, see e.g. \cite{DBLP:conf/icalp/Roura01}. 
This data structure permits to answer a range query by a simple adaptation of Algorithm~\ref{alg:query}: starting at the root,
we determine the number of elements within the query range which are in the left subtree, and depending on the result continue the search for the median in the left or in the right subtree. The required counting in each secondary structure takes  time, and we need to perform at most  such searches for any query.
When an element is inserted or deleted, we follow the search path in the BB() tree according to its value, and update all the  secondary structures of the visited nodes. 
The main difficulty arises when a rotation in the BB() tree is required: in this case, the secondary structures are rebuilt from scratch, which costs  time if the subtree which is rotated contains  nodes. 
However, as shown in \cite{mehlhornsbook,luekerwillard}, such rotations are required so rarely that 
the amortized time of such an event is only .


We note that using this dynamic data structure for the one-dimensional RMP, we can implement a two-dimensional median filter, by scanning over the image, maintaining all the pixels in a strip of width . In this way, we obtain a running time of  per pixel, which matches the state-of-the-art solution for this problem \cite{GilWerman}.
This indicates that obtaining a solution with  time for all operations could be difficult.












\section{Conclusion}\label{s:conclusions}



We have presented improved upper bounds for the range median problem. 

The query time of  our solution is asymptotically optimal for , or when all preprocessing has to be done up front.
For larger values of , our solution is at most a factor  from optimal. In a very restricted model where no arrays are allowed, our solution is optimal for all . Moreover, in the RAM model, our data structure requires only  space, which is clearly optimal. Making the data structure dynamic adds a factor   to the query time.
Using  space, it is trivial to precompute all medians of a given array so that the query time
becomes constant. However, it is open whether the term  in the query time could be reduced towards  in the RAM model when .

Given the simplicity of our data structure, a practical implementation would be easily possible.
To avoid the large constants involved when computing medians for recursively splitting the array,
one could use a pivot chosen uniformly at random. This should work  well in expectation.

It would be interesting to find faster solutions for the dynamic RMP or the two-dimensional (static) RMP:
Either would lead to a faster median filter for images, which is a basic tool in image processing.



\bibliographystyle{alpha}
\bibliography{references}

\end{document}