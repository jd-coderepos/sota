\documentclass{article}

\usepackage[final,nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{colortbl}
\usepackage{color}
\usepackage{pifont}
\usepackage{array}           \usepackage{tabu}           \usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{xcolor}     \usepackage{color}
\definecolor{ggreen}{HTML}{58a55c}
\usepackage[pagebackref=true,
   breaklinks=true,
   colorlinks = true,
   linkcolor = red,
   urlcolor = blue,
   citecolor = ggreen,
   anchorcolor = blue]{hyperref}

\newcommand\cb[1]{\color{blue} #1}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\definecolor{Gray}{gray}{0.95}
\definecolor{Gray7}{gray}{0.75}
\newcommand\grey[1]{\color{Gray7} #1}
\newcommand\bk[1]{\color{black} #1}


\begin{document}

\title{P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting}


\author{
    Ziyi Wang\thanks{Equal contribution. ~\textsuperscript{\dag}Corresponding author.} ~~~~
  	Xumin Yu ~~
	Yongming Rao \\
	\textbf{Jie Zhou ~~~
	Jiwen Lu}      \\
    Department of Automation, Tsinghua University, China\\
    {\tt\small \{wziyi22, yuxm20\}@mails.tsinghua.edu.cn;} \\{\tt\small raoyongming95@gmail.com; } \\
    {\tt\small \{lujiwen, jzhou\}@tsinghua.edu.cn} \\
}


\maketitle


\begin{abstract}
Nowadays, pre-training big models on large-scale datasets has become a crucial topic in deep learning. The pre-trained models with high representation ability and transferability achieve a great success and dominate many downstream tasks in natural language processing and 2D vision. However, it is non-trivial to promote such a pretraining-tuning paradigm to the 3D vision, given the limited training data that are relatively inconvenient to collect. In this paper, we provide a new perspective of leveraging pre-trained 2D knowledge in 3D domain to tackle this problem, tuning pre-trained image models with the novel \textit{Point-to-Pixel prompting} for point cloud analysis at a minor parameter cost. Following the principle of prompting engineering, we transform point clouds into colorful images with geometry-preserved projection and geometry-aware coloring to adapt to pre-trained image models, whose weights are kept frozen during the end-to-end optimization of point cloud analysis tasks. 
We conduct extensive experiments to demonstrate that cooperating with our proposed Point-to-Pixel Prompting, better pre-trained image model will lead to consistently better performance in 3D vision. Enjoying prosperous development from image pre-training field, our method attains 89.3\% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters. Our framework also exhibits very competitive performance on ModelNet classification and ShapeNet Part Segmentation. Code is available at \url{https://github.com/wangzy22/P2P}.
\end{abstract}

\section{Introduction}

With the rapid development of deep learning and computing hardware, neural networks are experiencing explosive growth in model size and representation capacity. Nowadays, pre-training big models has become an important research topic in both natural language processing~\cite{devlin2018bert,radford2019language,brown2020gpt3,smith2022MTNLG} and computer vision~\cite{radford2021clip,ramesh2021zero,he2022masked,riquelme2021scaling}, and has achieved a great success when transferred to downstream tasks with fine-tuning~\cite{he2020moco,chen2020mocov2,chen2020simclr,bao2021beit,he2022masked} or prompt-tuning~\cite{radford2021clip,petroni2019lama,wallace2019advtrigger,shin2020autoprompt,lester2021prompttuning,liu2021ptuning} strategies. Fine-tuning is a traditional tuning strategy that requires a large amount of trainable parameters, while prompt tuning is a recently emerged lightweight scheme to convert downstream tasks into the similar form as the pre-training task.  However, such prevalence of the pretraining-tuning pipeline cannot be obtained without the support of numerous training data in pre-training stage. Language pre-training leading work Megatron-Turing NLG~\cite{smith2022MTNLG} with 530 billion parameters is trained on 15 datasets containing over 338 billion tokens, while Vision MoE~\cite{riquelme2021scaling} with 14.7 billion parameters is trained on JFT-300M dataset~\cite{sun2017JFT} including 305 million training images.

Unfortunately, the aforementioned convention of pre-training big models on large-scale datasets and tuning on downstream tasks has encountered obstacles in 3D vision. 3D visual perception is gaining more and more attention given its superiority in many emerging research fields including autonomous driving~\cite{lang2019pointpillars,yin2021center}, robotics vision~\cite{chao2021dexycb, yang2020human2robot} and virtual reality~\cite{ma2021avatars, wei2019vr}. However, obtaining abundant 3D data such as point clouds from LiDAR is neither convenient nor inexpensive. For example, the widely used object-level point cloud dataset ShapeNet~\cite{chang2015shapenet} only contains 50 thousand synthetic samples. Therefore, pre-training fundamental 3D models with limited data remains an open question. There are some previous literature~\cite{xie2020pointcontrast,wang2021occo,yu2022pointbert} that attempts to develop specific pre-training strategies on point clouds with limited training data, such as Point Contrast~\cite{xie2020pointcontrast}, OcCo~\cite{wang2021occo} and Point-BERT~\cite{yu2022pointbert}. Although they prove that the pretraining-finetuning pipeline also works well in the 3D domain, the imbalance between numerous trainable parameters and limited training data may lead to insufficient optimization or overfitting problems.

\begin{figure*}[tb]
	\centering
	\includegraphics[width=1\linewidth]{examples.pdf}
	\caption{\small \textbf{Images produced by our Point-to-Pixel Prompting.} We show the original point clouds (top line) and the projected colorful images produced by our P2P of synthetic objects from ModelNet40 (left five columns) and real-world objects from ScanObjectNN (right three columns) from two different projection views.}
	\label{fig:example2}
\end{figure*}

Different from the previous methods that directly pre-train models on 3D data, we propose to transfer the pre-trained knowledge from 2D domain to 3D domain with appropriate prompting engineering, since images and point clouds display the same visual world and share some common knowledge.
In this way, we address the data-starvation problem in the 3D domain, given that the pre-training strategy is well-studied in the 2D field with abundant training data and that prompt-tuning on 3D tasks does not require much 3D training data. To the best of our knowledge, we are the first work to transfer knowledge in pre-trained image models to 3D vision with a novel prompting approach. More specifically, we propose an innovative Point-to-Pixel Prompting mechanism that transforms point clouds into colorful images with geometry-preserved projection and geometry-aware coloring. 
Examples of produced colorful images are shown in Figure~\ref{fig:example2}.
Then the colorful images are fed into the pre-trained image model with frozen weights to extract representative features, which are further deployed to downstream task-specific heads. 
The conversion from point clouds to colorful images and the end-to-end optimization pipeline promote the bidirectional knowledge flow between points and pixels. The geometric information from point clouds is mostly retained in projected images via our geometry-preserved projection, while the color information of natural images from the pre-trained image model is transmitted back to colorless point clouds via the cooperation between the geometry-aware coloring module and the fixed pre-trained image model.

We conduct extensive experiments to demonstrate that with our Point-to-Pixel Prompting, enlarging the scale of the same image model will result in higher point cloud classification performance, which is consistent with the observations in image classification. This suggests that we can take advantage of the successful researches in pre-training big image model, opening up a new avenue for point cloud analysis. With much fewer trainable parameters, we achieve comparable results with the best object classification methods on both synthetic ModelNet40~\cite{wu2015modelnet} and real-world ScanObjectNN~\cite{uy2019scanobjectnn}. We also demonstrate the potential of our method to perform dense predictions like part segmentation on ShapeNetPart~\cite{yi2016shapenetpart}. In conclusion, our Point-to-Pixel Prompting (P2P) framework explores the feasibility and ascendancy of transferring image pre-trained knowledge to the point cloud domain, promoting a new pre-training paradigm in 3D point cloud analysis.

\section{Related Work}

\subsection{Visual Pre-training}
Pre-training visual models has been studied thoroughly in the image domain. Supervised pre-training~\cite{dosovitskiy2020vit,DBLP:journals/corr/abs-2106-04560,carreira2017quo} on classification task with large-scale dataset is a traditional practice and is stimulated by the boosting development of the ever-growing fundamental vision models~\cite{he2016resnet,huang2017densenet,dosovitskiy2020vit,liu2021swin}. Weakly-supervised pre-training methods~\cite{tarvainen2017mean,berthelot2019mixmatch,xie2020self,pham2021meta} use less annotations while unsupervised pre-training approaches~\cite{he2020moco,chen2020mocov2,chen2020simclr,bao2021beit,he2022masked,grill2020bootstrap} introduces no task-related bias and brings higher transferability to various downstream tasks. 

Different from the prosperity of pre-training image models, pre-training 3D models is still under development. Many researches have developed self-supervised learning mechanisms with various pretext tasks such as solving jigsaw puzzles~\cite{sauder2019jigsaw}, orientation estimation~\cite{poursaeed2020orientation}, and deformation reconstruction~\cite{achituve2021deformation}. Inspired by pre-training strategies in image domain, Point Contrast~\cite{xie2020pointcontrast} adopts contrastive learning principle while OcCo~\cite{wang2021occo}, Point-BERT~\cite{yu2022pointbert} and Point-M2AE~\cite{zhang2022pointm2ae} introduce reconstruction pretext tasks for better representation learning. However, the data limitation in 3D domain remains a large obstacle in developing better pre-training strategies.

\subsection{Prompt Tuning}
\label{sec:prompt}

Prompt tuning is an important mechanism whose principle is to adapt downstream tasks with limited annotated data to the original pre-training task at a minimum cost, thus exploiting the pre-trained knowledge to solve downstream problems. It is first proposed in the natural language processing community~\cite{liu2021promptsurvey}, and has been leveraged in many vision-language models. At first, hand-crafted prompting methods~\cite{petroni2019lama, brown2020gpt3} are promoted and their followers~\cite{wallace2019advtrigger, shin2020autoprompt} develop an automated searching algorithm to select discrete prompt tokens within a large corpus. Recently, continuous prompting methods~\cite{li2021prefixtuning, lester2021prompttuning, liu2021ptuning, liu2021ptuningv2} are becoming the mainstream given their flexibility and high performance.

On the contrary, the development in prompting visual pre-training models lags behind. L2P~\cite{wang2021l2p} proposes a prompt pool for continual learning problem while VPT~\cite{jia2022vpt} first introduces continuous prompt tuning framework inspired by P-Tuning~\cite{liu2021ptuning, liu2021ptuningv2}. As far as we are concerned, there is no previous work like this paper to discuss tuning pre-trained image models for point cloud analysis with an appropriate prompting mechanism.

\subsection{Object-level Point Cloud Analysis}

Given the unordered data structure of point clouds, early literature has developed voxel-based and point-based methods to construct structural representations for point cloud object analysis. Voxel-based methods~\cite{maturana2015voxnet, klokov2017kdnet, riegler2017octnet} partition the 3D space into ordered voxels and perform 3D convolutions for feature extraction. 
Point-based methods~\cite{qi2017pointnet, qi2017pointnet++, ma2022pointmlp, cheng2021pranet, li2018pointcnn, wu2019pointconv, thomas2019kpconv, wang2019dgcnn, li2018adagraph} directly process unordered points and introduce various approaches to aggregate local information. Recently, attention-based Transformer~\cite{vaswani201transformer,guo2021pct,zhao2021pointtransformer} architecture has prevailed over other frameworks in vision community and achieved competitive performance in point cloud object analysis. 

Besides the aforementioned methods that perform representation learning in the 3D space, there are projection-based methods\cite{su2015mvcnn,kanezaki2018rotationnet,wei2020viewgcn,hamdi2021mvtn,feng2018gvcnn, zhang2022pointclip} that leverage multi-view images to represent 3D objects. Recently, MVTN~\cite{hamdi2021mvtn} introduces the differentiable rendering technique to build an end-to-end learning pipeline, rendering images online and regressing the optimal projection view. Different from theirs, our work designs a novel prompting engineering scheme, utilizing 2D color knowledge from pre-trained image models that is absent in colorless point clouds. Moreover, our framework is implemented in a faster single-view pattern, as we only select one random projection view during training and don't develop any aggregation strategy to explicitly fuse multi-view knowledge.

\section{Approach}

\begin{figure*}[tb]
	\centering
	\includegraphics[width=1\linewidth]{pipeline.pdf}
	\caption{\small \textbf{The pipeline of our proposed P2P framework.} Taking a point cloud  as the input, we first encode the geometry information for each point. Then we sample a projection view and rearrange the point-wise features into an image-style layout to obtain the pixel-wise features with \textit{Geometry-preserved Projection}. The colorless projection will be enriched to produce a colorful image  with the color information via a learnable \textit{Coloring Module}. Our P2P framework can be easily transferred to several downstream tasks with a task-specific head with the help of the transferable visual knowledge from the pre-trained image model. We take the classical Vision Transformer~\cite{dosovitskiy2020vit} as our pre-trained image model for illustration in this pipeline.}
	\label{fig:pipeline}
\end{figure*}


\subsection{Overview}

The overall framework of our P2P framework is illustrated in Figure~\ref{fig:pipeline}. The network architecture consists of four components: 1) a geometry encoder to extract point-level geometric features from the input point clouds, 2) a Point-to-Pixel Prompting module to produce colorful images based on geometric features, 3) a pre-trained image model to leverage pre-trained knowledge from image domain, and 4) a task-specific head to perform various kinds of point cloud tasks. We will introduce the geometry encoder, the Point-to-Pixel Prompting module and task-specific heads in detail in the following sections. As for the choice of the pre-trained image model, we investigate both convolution-based and attention-based architectures in Section~\ref{sec:exp_cls}.

With the proposed architecture that can be optimized in an end-to-end manner, we are able to exploit 2D pre-trained knowledge for point cloud analysis from two perspectives. In the forward process, the point clouds are projected into images with preserved geometry information and the resulting images can be recognized and handled by the pre-trained image model. In the backward optimization, the frozen pre-trained weights of the image model act as an anchor and guide the learnable coloring module to learn extra color knowledge for colorless point clouds, without explicit manual interference and only under the indirect supervision from the overall target functions of downstream tasks. Therefore, the resulting colorful images are expected to mimic patterns in 2D images and to be distinguishable for the pre-trained image model in downstream tasks.

\subsection{Point Cloud Feature Encoding}

One of the most significant advantages of 3D point clouds over 2D images is that point clouds contain more spatial and geometric information that is compressed or even lost in flat images. Therefore, we first extract geometry features from point clouds for better spatial comprehension, implementing a lightweight DGCNN~\cite{wang2019dgcnn} to extract local features of each point. 

Given an input point cloud  with  points, we first locate -nearest neighbors  of each point. Then for each local region, we implement a small neural network  to encode the relative position relations between the central point  and the local neighbor points . Then we can obtain geometric features  with dimension :

where  are coordinates of  respectively,  and  stand for max-pooling and concatenation within all points  in local neighbor region  respectively. 

\subsection{Point-to-Pixel Prompting}
\label{sec:nrp}

Following the principle of prompt tuning mechanism introduced in Section~\ref{sec:prompt}, we propose Point-to-Pixel Prompting to adapt point cloud analysis to image representation learning, on which the image model is initially pre-trained. As illustrated in Figure~\ref{fig:pipeline}, we first introduce geometry-preserved projection to transform the 3D point cloud into 2D images, rearranging 3D geometric features according to the projection correspondences. Then we propose a geometry-aware coloring module to dye projected images, transferring 2D color knowledge in the pre-trained image model to the colorless point cloud and obtaining more distinguishable images that can be better recognized by the pre-trained image model. 

\subsubsection{Geometry-Preserved Projection}
\label{sec:proj}

Once obtaining geometric features  of the input point cloud , we further rearrange them into an image-style layout  to prepare for producing colorful image , where  are height and width of the target image. We elaborately design a geometry-preserved projection to avoid information loss when casting 3D point clouds to 2D images.

The first step is to find spatial correspondence between point coordinates  and image pixel coordinates . Since there is a dimensional diminishing during the projection process, we randomly select a projection view during training to construct a stereoscopic space with flat image components. Equivalently, we rotate the input point cloud with rotation matrix  to get 3D coordinates  after rotation: . The rotation matrix  is constructed through two steps: first rotating around the axis  by angle , then rotating around the axis  by angle , where  and  are random rotation angles during training and fix-selected angles during inference. Then we just omit the final dimension  and evenly split the first two dimensions into 2D grids: , where  denotes point index,  denotes coordinate dimension,  denotes grid size at dimension . 

The second step is to rearrange per-point geometric features  into per-pixel  according to coordinates correspondence between  and . If there are multiple points  falling in the same pixel at , which is a common situation, we add the features of these points altogether to produce the pixel-level feature: . The summation operation brings two advantages related to geometry-preserved design. On the one hand, we consider all points in one pixel instead of keeping the foremost point according to depth and occlusion relation. Therefore, we are able to represent and optimize all points in one image and produce images containing semitransparent objects with richer geometric information as shown in Figure~\ref{fig:example2}. On the other hand, we conduct a summation operation instead of taking the average, resulting in larger feature values when there are more points in one pixel. Such design maintains the spatial density information of point clouds during the projection process, which is lacked in image representations and is critical in preserving geometry knowledge.

In conclusion, the geometry-preserved projection produces geometry-aware image features that contain plentiful spatial knowledge of the object. Note that we only use one projection view during training and do not explicitly design any aggregation functions for multi-view feature fusion. Therefore, we follow a more efficient single-view projection pipeline than its multi-view counterpart.


\subsubsection{Geometry-Aware Coloring}
\label{sec:color}

Despite that 3D point cloud contains richer geometric knowledge than 2D images, colorful pictures embrace more texture and color information than colorless point clouds, which is also decisive in visual comprehension. The frozen image model pre-trained on abundant images learns to perceive the visual world not only based on object shape and outlines, but also heavily relied on discriminative colors and textures. Therefore, the image feature map  that contains only geometric knowledge and lacks color information is not most suitable for the pre-trained image model to understand and process. In order to better leverage pre-trained 2D knowledge of the frozen image model, we propose to predict colors for each pixel, explicitly encouraging the network to migrate color knowledge in the pre-trained image model to  via the end-to-end optimization. Since the input  contains rich geometry information that will heavily affect the coloring process, the resulting images are expected to display different colors on different geometry parts, which has been verified in Figure~\ref{fig:example2}. 


More specifically, we design a lightweight 2D neural network  to predict RGB colors  for each pixel : . We implement several  convolutions in  for image smoothing, as the initial projected image feature  are relatively discontinuous due to the sparsity of the original point cloud. Therefore, the smoothing operation is critical in producing more realistic images that the pre-trained image model can recognize.
The resulting colorful images are then prepared for further image-level feature extraction through the pre-trained image model.


\subsection{Optimization on Downstream Tasks}

Take ViT as the pre-trained image model for example. The outputs from the pre-trained image model are image token features  and one class token feature , where  is the number of image patches and  is the token feature dimension. For different downstream tasks, we design different task-specific heads and optimization strategies.

\paragraph{Object Classification}

For object classification, we follow the common protocol in image Transformer models to utilize the class token  as the input to the classifier  implemented as only one linear layer: . We use the CrossEntropy loss as the optimization target.

\paragraph{Part Segmentation}

We rearrange the token features  into image layouts and upsample them to . Then we design a lightweight 2D segmentation head  based on SemanticFPN~\cite{kirillov2019semanticfpn} or UPerNet~\cite{xiao2018uper} to predict per-pixel segmentation logits: . 
Given that multiple points may correspond to one pixel and that we train the network in a single view pattern, projecting per-pixel predictions back to 3D points will cause supervision conflict. Instead, we project 3D labels into 2D image-style labels, exactly as how the point cloud is projected. Then we implement a per-pixel multi-label CE loss as there may be points from multiple classes projected to the same pixel: . The values of multi-hot 2D label  are assigned according to projection correspondences, satisfying . Supervision in 2D domain speeds up the training procedure without much information loss, since we keep all features of points in one pixel and the optimization target is accordingly based on their category distributions. During inference, we select multiple projection views and re-project 2D per-pixel segmentation results back to 3D points, fusing multi-view predictions. Therefore, the per-point segmentation is decided by the most evident predictions from the most distinguishable projection directions.


\section{Experiments}

\subsection{Datasets and Experiment Settings}
\label{sec:exp_setting}

\paragraph{Datasets.} We conduct classification on ModelNet40~\cite{wu2015modelnet} and ScanObjectNN~\cite{uy2019scanobjectnn}, while ShapeNetPart~\cite{uy2019scanobjectnn} is utilized for part segmentation. 
\textbf{ModelNet40} is a synthetic 3D dataset containing 12,311 CAD models from 40 categories. 
\textbf{ScanObjectNN} samples from real-world scans with background and occlusions. It contains 2,902 samples from 15 categories, and we conduct experiments on the perturbed (PB-T50-RS) variant. 
\textbf{ShapeNetPart} samples 16,881 objects covering 16 shape categories from the synthetic ShapeNet and annotates each object with part-level labels from 50 classes. 

\paragraph{Implementation Details.} We utilize AdamW~\cite{loshchilov2017adamw} optimizer and CosineAnnealing scheduler~\cite{loshchilov2016sgdr}, with learning rate  and weight decay . We freeze the weights of the pre-trained image model except for normalization layers. The model is trained for 300 epochs with a batch size of 64. During training, the rotation angle  are randomly selected from  and  to keep the objects standing upright. During inference, we evenly choose  values of  and  values of  to produce  views for majority voting. Please refer to the supplementary for architectural details.


\subsection{Object Classification}

\subsubsection{Results}
\label{sec:exp_cls}

\begin{table}[t]
\caption{\small \textbf{Classification results on ModelNet40 and ScanObjectNN.} For different image models, we report the image classification performance (IN Acc.) on ImageNet-1k~\cite{deng2009imagenet} dataset. After migrating them to point cloud analysis with Point-to-Pixel Prompting, we report the number of trainable parameters (Tr. Param.), performance on ModelNet40 dataset (MN Acc.) and performance on ScanObjectNN dataset (SN Acc.).} 
\label{tab:exp_trend}
\centering
\vspace{-5pt}
\begin{minipage}{.5\textwidth}
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{\footnotesize ResNet~\cite{he2016resnet}.}
    \vspace{-5pt}
    \label{tab:res}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][3pt]}gg@{\hskip 3pt}}
    \toprule
        Image Model     & IN Acc.  & Tr. Param.     & MN Acc.  & SN Acc.\\
    \midrule
        ResNet-18       &  69.8       & 109 K         &  91.6 &  82.6 \\
        ResNet-50       &  76.1       & 206 K         &  92.5 &  85.8 \\
        ResNet-101      & 77.4        & 257 K         &  93.1 &  87.4 \\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{\footnotesize Vision Transformer~\cite{dosovitskiy2020vit}.}
    \vspace{-5pt}
    \label{tab:vit}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][3pt]}gg@{\hskip 3pt}}
    \toprule
        Image Model   & IN Acc.  & Tr. Param.     & MN Acc.  & SN Acc. \\
    \midrule
        ViT-T    & 72.2       & 99 K         & 91.5  & 79.7 \\
        ViT-S    & 79.8      & 116 K         & 91.8  & 81.6 \\
        ViT-B    & 81.8       & 150 K         & 92.7  & 83.4 \\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{\footnotesize Swin Transformer~\cite{liu2021swin}.}
    \vspace{-5pt}
    \label{tab:swin}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][3pt]}gg@{\hskip 3pt}}
    \toprule
        Image Model   & IN Acc.  & Tr. Param.     & MN Acc.  & SN Acc. \\
    \midrule
        Swin-T      & 81.3       & 136 K         &  92.1  & 82.9  \\
        Swin-S      & 83.0       & 154 K         &  92.5  &  83.8 \\
        Swin-B      & 83.5         & 178 K         & 92.6  & 84.6  \\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{\footnotesize ConvNeXt~\cite{liu2022convnext}.}
    \vspace{-5pt}
    \label{tab:convnext}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][3pt]}gg@{\hskip 3pt}}
    \toprule
        Image Model   & IN Acc.  & Tr. Param.     & MN Acc. & SN Acc. \\
    \midrule
        ConvNeXt-T    & 82.1       & 126 K         & 92.6 & 84.9\\
        ConvNeXt-S    & 83.1        & 140 K         & 92.8  & 85.3\\
        ConvNeXt-B    & 83.8        & 159 K        & 93.0  & 85.7\\
        ConvNeXt-L    & 84.3        & 198 K         & 93.2  & 86.2\\
    \bottomrule
    \end{tabular}
    
    }
\end{subtable}
\end{minipage}
\hfill
\begin{minipage}{.42\textwidth}
\vspace{8pt}
 \begin{subfigure}[b]{\textwidth}
     \centering
     \caption{\footnotesize Accuracy on point cloud classification datasets vs. ImageNet-val for different models.}
     \vspace{-5pt}
     \label{fig:curve}
     \includegraphics[width=0.9\textwidth]{curve.pdf}
 \end{subfigure}
\end{minipage}
\end{table}

\paragraph{Main Results.} We implement our P2P framework with different image models of different scales, ranging from convolution-based ResNet~\cite{he2016resnet} and ConvNeXt~\cite{liu2022convnext} to attention-based Vision Transformer~\cite{dosovitskiy2020vit} and Swin Transformer~\cite{liu2021swinv2}. These image models are pre-trained on ImageNet-1k~\cite{deng2009imagenet} with supervised classification. We report the image classification performance of the original image model, the number of trainable parameters after Point-to-Pixel Prompting, and the classification accuracy on ModelNet40 and ScanObjectNN datasets, as shown in Table~\ref{tab:exp_trend}.

From the quantitative results and accuracy curve, we can conclude that enlarging the scale of the same image model will result in higher classification performance, which is consistent with the observations in image classification. Therefore, our proposed P2P prompting can benefit 3D domain tasks by leveraging the prosperous development of 2D visual domain, including abundant training data, various pre-training strategies and superior fundamental architectures.

\paragraph{Comparisons with Previous Methods.} 

Comparisons with previous methods on the ModelNet40 and ScanobjectNN are shown in Table~\ref{tab:exp_cls}. For baseline comparisons, we select methods~\cite{qi2017pointnet++, thomas2019kpconv, wang2019dgcnn, ma2022pointmlp, hamdi2021mvtn, ran2022repsurf, qian2022pointnext} that focus on developing 3D architectures and do not involve any pre-training strategies. We also select traditional pre-training work~\cite{yu2022pointbert, wang2021occo, pang2022pointmae} in 3D domain. For our P2P framework, we show results of two versions: (1) baseline version with ResNet-101 as the image model, (2) advanced version with HorNet-L~\cite{rao2022hornet} pre-trained on ImageNet-22k dataset~\cite{deng2009imagenet} as the image model, additionally replacing the linear head with a multi-layer perceptron (MLP) as the classifier.  

From the results we can draw three conclusions. Firstly, P2P outperforms traditional 3D pre-training methods. This suggests that the pre-trained knowledge from 2D domain is useful for solving 3D recognition problems and is better than directly pre-training on 3D datasets with limited data. Secondly, we achieve state-of-the-art performance on ScanObjectNN. Therefore, our P2P framework fully exploits the potential of pre-training knowledge from image domain and opens a new avenue for point cloud analysis. Finally, P2P performs relatively better on real-world ScanObjectNN than synthetic ModelNet. This may be caused by the data distribution of ScanObjectNN being more similar to the pre-trained ImageNet dataset, as they both contain visualizations of objects from the natural world. This prosperity reveals the potential of P2P in real-world applications.

\paragraph{Visualization Analysis.}

The visualizations of our projected colorful images are shown in Figure~\ref{fig:example2}. The first line shows point cloud samples, the second and third lines illustrate the colorful images from different projection views. Our geometry-preserved projection design maintains most spatial information, resulting in images of semitransparent objects that avoid occlusion problems, such as the chair leg in the second row  column. 

\begin{table}[t]
\caption{\small \textbf{Comparisons on classification accuracy (Acc.) with previous literature on point cloud datasets.} We report the pre-training modality (Pre-train) and trainable parameters number (Tr. Param.) of each method.} 
\label{tab:exp_cls}
\centering
\vspace{-5pt}
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize ModelNet40.}
  \vspace{-5pt}
  \label{tab:modelnet}
  \newcolumntype{g}{>{\columncolor{Gray}}c}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{@{\hskip 4pt}>{\columncolor{white}[4pt][\tabcolsep]}l c r  >{\columncolor{Gray}[\tabcolsep][4pt]}g@{\hskip 4pt}}
    \toprule
    Method      & Pre-train    & Tr. Param.  & Acc.(\%)  \\
    \midrule
    PointNet++~\cite{qi2017pointnet++}      
         & N/A           & 1.4 M              & 90.7      \\
    KPConv~\cite{thomas2019kpconv}
            & N/A         & 15.2 M             & 92.9      \\
    DGCNN~\cite{wang2019dgcnn}           
           & N/A          & 1.8 M              & 92.9      \\
    PointMLP-elite~\cite{ma2022pointmlp}
           & N/A          & 0.68 M             & 93.6      \\
    PointNeXt~\cite{qian2022pointnext}
          & N/A           & 1.4 M             & 94.0      \\
    PointMLP~\cite{ma2022pointmlp}        
           & N/A          & 12.6 M             & 94.1      \\
    RepSurf-U~\cite{ran2022repsurf}
          & N/A           & 1.5 M             & \textbf{94.7}      \\
    \midrule
    DGCNN-OcCo~\cite{wang2021occo} & 3D & 1.8M & 93.0 \\     
    Point-BERT~\cite{yu2022pointbert}      
          & 3D          & 21.1 M             & 93.2      \\
    Point-MAE~\cite{pang2022pointmae}
          & 3D          &     21.1 M               & 93.8  \\
    \midrule
    P2P (ResNet-101) & 2D   & 0.25 M  & 93.1      \\ 
    P2P (HorNet-L-22k-mlp) & 2D & 1.2 M & 94.0 \\
    \bottomrule
  \end{tabular}}
 \end{subtable}
  \hfill
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize ScanObjectNN.}
  \vspace{-5pt}
  \label{tab:scanobjectnn}
  \newcolumntype{g}{>{\columncolor{Gray}}c}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{@{\hskip 4pt}>{\columncolor{white}[4pt][\tabcolsep]}l c r  >{\columncolor{Gray}[\tabcolsep][4pt]}g@{\hskip 4pt}}
    \toprule
    Method    & Pre-train    & Tr. Param. & Acc.(\%)  \\
    \midrule
    PointNet++~\cite{qi2017pointnet++}
             & N/A        &  1.4 M   & 77.9      \\
    DGCNN~\cite{wang2019dgcnn}           
             & N/A        &  1.8 M   & 78.1      \\
    PRANet~\cite{cheng2021pranet}          
              & N/A       &  2.3 M   & 82.1      \\
    MVTN~\cite{hamdi2021mvtn}            
             & N/A        & 14.0 M   & 82.8      \\
    PointMLP-elite~\cite{ma2022pointmlp}  
               & N/A      & 0.68 M   & 83.8      \\
    PointMLP~\cite{ma2022pointmlp}        
              & N/A       & 12.6 M   & 85.4      \\
    RepSurf-U(2x)~\cite{ran2022repsurf}
              & N/A       & 6.8 M   & 86.1      \\
    PointNeXt~\cite{qian2022pointnext}
              & N/A       & 1.4 M   & 88.2      \\
    \midrule
    Point-BERT~\cite{yu2022pointbert}      
                & 3D     & 21.1 M    & 83.1      \\
    Point-MAE~\cite{pang2022pointmae}
                & 3D     &    21.1 M        & 85.2  \\
    \midrule
    P2P (ResNet-101) & 2D   & 0.25 M  & 87.4      \\ 
    P2P (HorNet-L-22k-mlp)   & 2D  & 1.2 M   &  \textbf{89.3}  \\
    \bottomrule
  \end{tabular}}
\end{subtable}
\vspace{-10pt}
\end{table}

\subsubsection{Ablation Studies}

To investigate the architecture design and training strategy of our proposed framework, we conduct extensive ablation studies on ModelNet40 classification. Except for further notice, we use the base version of Vision Transformer (ViT-B-1k) that is pre-trained on ImageNet-1k dataset as our image model. Illustrations of our ablation settings can be found in Figure~\ref{fig:ablations}.

\paragraph{Advantages of P2P Prompting over Other Tuning Methods.} We conduct extensive ablation studies to demonstrate the advantages of our proposed P2P Prompting over vanilla fine-tuning and other prompting methods, shown in Table~\ref{tab:abl_vpt}. As a baseline (Model A), we directly append classification head to the geometry encoder without the pre-trained image model. Then we incrementally insert pre-trained ViT blocks to process point tokens from the geometry encoder, and discuss different fine-tuning strategies including fixing all ViT weights (Model B), fine-tuning normalization parameters (Model B) and fine-tuning all ViT weights(Model B). We also implement Vision Prompt Tuning (VPT)~\cite{jia2022vpt} to Model B with shallow (Model C) and deep (Model C) variants.

From the comparisons between Model A and others, we can inspect the contribution of pre-trained knowledge from 2D to 3D classification. However, neither vanilla fine-tuning nor previously prompting mechanism VPT fully exploits the pre-trained image knowledge. Our Point-to-Pixel prompting is the best choice to migrate 2D pre-trained knowledge to 3D domain at a low trainable parameter cost.   

\paragraph{Point-to-Pixel Prompting Designs.}

After confirming that P2P is the most suitable tuning mechanism, we discuss the design choices of the P2P module in detail. In Point-to-Pixel Prompting, we produce colorful images to adapt to the pre-trained image model, whose advantages have been discussed in Section~\ref{sec:color}. Here we further prove the statement via ablation studies in Table~\ref{tab:abl_p2p}. Model D processes per-pixel feature  from Section~\ref{sec:proj} to directly generate image tokens and feed them to ViT blocks. In this variant, we bypass the explicit image generation process and directly adopt patch embedding layers on feature map . Model E generates binary black-and-white images according to the geometric projection from the point cloud, without predicting pixel colors as in P2P.

According to the results, Model D introduces much more trainable parameters due to the trainable patch embedding projection convolution layer with kernel size 16, while producing inferior classification results than P2P. On the other hand, even though Model E requires fewer trainable parameters, its performance lags far behind. Therefore, producing colorful images as the prompting mechanism can best communicate knowledge between the image domain and point cloud domain, fully exploiting pre-trained image knowledge from the frozen ViT model.

\begin{figure*}[t]
	\centering
	\includegraphics[width=1\linewidth]{ablations.pdf}
	\caption{\small \textbf{Ablations illustration.}  shows the pipeline of the overall P2P framework. Part (a) displays ablations on replacing P2P prompting with vanilla fine-tuning or visual prompt tuning (VPT)~\cite{jia2022vpt}. Part (b) illustrates ablations on Point-to-Pixel Prompting designs. Part (c) shows different tuning strategies on the pre-trained image model in our P2P framework. Gray letters on top of each model correspond to the Model column in Table~\ref{tab:ablation}.}
	\label{fig:ablations}
\end{figure*}

\begin{table}[t]
\caption{\small \textbf{Ablation studies on ModelNet40 classification.} We select ViT-B that is pre-trained on ImageNet-1k as our image model. We report trainable parameters (Tr. Param.) and accuracy (Acc.). (a) shows effects of different tuning strategies, including point-based network without the image model (A), fine-tuning the pre-trained image model to different extents (B,B,B), prompt tuning the pre-trained image model with different variants of VPT (C,C) and with our proposed Point-to-Pixel prompting (P2P). (b) shows different Point-to-Pixel Prompting types, discussing whether to explicitly produce images (D) and whether to predict pixel colors (E). (c) shows ablations on tuning settings of the pre-trained image model when training our P2P framework. (d) shows the effect of different pre-training strategies of the image model, where IN Acc. with  and  represent the linear probing and fine-tuning accuracy on ImageNet-1k dataset respectively. * denotes that we implement CoOp to report the zero-shot classification accuracy of the CLIP pre-trained model on ImageNet-1k. Illustrations of ablations (a,b,c) are shown in Figure~\ref{fig:ablations}.} \label{tab:ablation}
\centering
\begin{minipage}{.52\textwidth}
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \caption{\footnotesize Fine-tuning and Prompting Methods.}
    \vspace{-5pt}
    \centering
    \label{tab:abl_vpt}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}c|ccc|c >{\columncolor{Gray}[\tabcolsep][3pt]}g@{\hskip 3pt}}
\toprule
        Model   & Image Model   & VPT   & P2P   & Tr. Param.      & Acc.(\%) \\
    \midrule
        A       & \xmark   & \xmark & \xmark    & 7.76 K       & 88.5~\cb{(-4.2)} \\
    \midrule
        B       & Fixed & \xmark & \xmark       & 0.08 M        & 90.0~\cb{(-2.7)} \\
        B       & Finetune Norm & \xmark & \xmark    & 0.12 M    & 90.1~\cb{(-2.6)} \\
        B       & Finetune All & \xmark & \xmark    & 81.2 M         & 90.6~\cb{(-2.1)} \\
    \midrule
        C       & Prompt Tune   & Shallow & \xmark   & 0.31 M         & 90.2~\cb{(-2.5)} \\
        C       & Prompt Tune   & Deep & \xmark      & 0.50 M         & 90.0~\cb{(-2.7)} \\
    \midrule
        P2P     & Prompt Tune  & \xmark & \cmark    & 0.15 M        & 92.7 \\ 
    \bottomrule
    \end{tabular}
    }
\end{subtable}\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{6.5pt}
    \caption{\footnotesize Point-to-Pixel Prompting.}
    \vspace{-5pt}
    \label{tab:abl_p2p}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
\begin{tabular}{@{\hskip 6.5pt}>{\columncolor{white}[6.5pt][\tabcolsep]}c|cc|c >{\columncolor{Gray}[\tabcolsep][6.5pt]}g@{\hskip 6.5pt}}
    \toprule
        Model   & P2P Type   & Color  & Tr. Param.     & Acc.(\%) \\
    \midrule
        D       & Feature   & \xmark        & 12.1 M         & 90.8~\cb{(-1.9)}        \\
        E       & Image     & \xmark        & 0.07 M         & 89.8~\cb{(-2.9)}        \\
    \midrule
        P2P       & Image     & \cmark        & 0.15 M         & 92.7        \\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\end{minipage} \hfill
\begin{minipage}{.46\textwidth}
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{3pt}
    \caption{\footnotesize Tuning settings.}
    \vspace{-5pt}
    \label{tab:abl_tune}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}c|cc|c >{\columncolor{Gray}[\tabcolsep][3pt]}g@{\hskip 3pt}}
    \toprule
        Model   & Pre-train & Tune Param.  & Tr. Param.     & Acc.(\%) \\
    \midrule
        F       & \xmark    & All      & 81.9 M         & 86.3~\cb{(-6.4)}        \\
        G       & \cmark    & All      & 81.9 M         & 91.7~\cb{(-1.0)}        \\
        H       & \cmark    & Bias     & 0.21 M         & 92.3~\cb{(-0.4)}        \\
        I       & \cmark    & N/A      & 0.11 M         & 92.2~\cb{(-0.5)}        \\
    \midrule
        P2P     & \cmark    & Norm     & 0.15 M         & 92.7        \\
    \bottomrule
    \end{tabular}
    }
\end{subtable}\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{4pt}
    \caption{\footnotesize Different Pre-training Strategies.}
    \vspace{-5pt}
    \label{tab:abl_pretrain}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=\textwidth}{
    \begin{tabular}{@{\hskip 4pt}>{\columncolor{white}[4pt][\tabcolsep]}cl|lc >{\columncolor{Gray}[\tabcolsep][4pt]}g@{\hskip 4pt}}
    \toprule
        Model   & Pre-train   & IN Acc.(\%)  & Tr. Param.     & Acc.(\%) \\
    \midrule
        J       & ~~MAE      & ~~~~~68.0        & 0.15 M           & 91.6       \\
        K       & ~~CLIP      & ~~~~~71.7*                & 0.12 M           & 91.8        \\
        L       & ~~MoCo      & ~~~~~76.7        & 0.15 M          & 92.3        \\
        M       & ~~DINO      & ~~~~~78.2        & 0.15 M          & 92.8        \\
    \midrule
        N       & ~~IN 1k      & ~~~~~81.8        & 0.15 M         & 92.7        \\
        O       & ~~IN 22k     & ~~~~~84.0        & 0.15 M         & 92.9        \\
        
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\end{minipage}
\end{table}

\paragraph{Influence of Tuning Strategies.} 

After fixing the architecture of our P2P framework, we investigate the best tuning strategy, adjusting the tuning extent of the pre-trained image model: (1) Model F: training the image model from scratch without loading pre-trained weights. (2) Model G: tuning all ViT parameters. (3) P2P: tuning only normalization parameters. (4) Model H: tuning only bias parameters. (5) Model I: fix all ViT parameters without any tuning. 

According to the results in Table~\ref{tab:abl_tune}, tuning normalization parameters is the most suitable solution, avoiding 2D information lost during massive tuning (model G). Tuning normalization parameters also adapts the model to point cloud data distribution, which model H and I variant fail to accomplish. Additionally, quantitative comparisons between Model F and others demonstrate that the pre-trained knowledge from 2D domain is crucial in our P2P framework, since the limited data in 3D domain is insufficient for optimizing a large ViT model from scratch with numerous trainable parameters. 

\paragraph{Effects of Different Pre-training Strategies.}

In Table~\ref{tab:abl_pretrain}, we show the effects of different strategies for pre-training image models. For supervised pre-training, we load pre-trained weights on ImageNet-1k and ImageNet-22k datasets. For unsupervised pre-training, we select four most representative methods: CLIP~\cite{radford2021clip}, DINO~\cite{caron2021dino}, MoCo~\cite{chen2021mocov3} and MAE~\cite{he2022masked}. We report the linear probing and fine-tuning results on ImageNet-1k dataset of each pre-training strategy in IN Acc. column with  and  respectively. Note that we implement CoOp~\cite{zhou2021coop} to report the zero-shot classification accuracy (denoting with ) of the CLIP pre-trained model.

From the experiment results, we can conclude that supervised pre-trained image models obtain relatively better results than unsupervised pre-trained ones. This may because the objective of 3D classification is consistent with that in 2D domain, thus the supervised pre-training weight is more suitable to migrate to point cloud classification task. However, unsupervised approach with strong transferability such as DINO also achieves competitive performance.
Secondly, comparing among unsupervised pre-training methods, the one that achieves higher performance with linear probing on 2D classification produces better results in 3D classification. This suggests that the transferability of a pre-trained image model is consistent when migrating to 2D and 3D downstream tasks.

\subsection{Part Segmentation}
\label{sec:seg}

\begin{table*}[t]
\caption{\small \textbf{Part segmentation results on the ShapeNetPart dataset}. We report the mean IoU across all part categories mIoU (\%) and the mean IoU across all instance mIoU  (\%) , and the IoU (\%) for each category.}   

\label{tab:shapenetpart}
\centering
\setlength{\tabcolsep}{1.5pt}{
\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{adjustbox}{width=\textwidth} \normalsize
\begin{tabular}{@{\hskip 1pt}>{\columncolor{white}[1pt][\tabcolsep]}l|cc|cccccccccccccccc@{\hskip 1pt}}
\toprule
Model& mIoU & mIoU & \makecell[c]{aero\\plane}  & bag   & cap   & car   & chair & \makecell[c]{ear\\phone} & guitar & knife & lamp  & laptop & \makecell[c]{motor\\bike} & mug   & pistol & rocket & \makecell[c]{skate\\board}  & table \\
\midrule
PointNet~\cite{qi2017pointnet} & 80.4 & 83.7  & 83.4  & 78.7  & 82.5  & 74.9  & 89.6  & 73.0    & 91.5  & 85.9  & 80.8  & 95.3  & 65.2  & 93.0  & 81.2  & 57.9  & 72.8  & 80.6 \\
PointNet++~\cite{qi2017pointnet++} & 81.9 & 85.1  & 82.4  & 79.0  & 87.7  & 77.3  & 90.8  & 71.8  & 91.0  & 85.9  & 83.7  & 95.3  & 71.6  & 94.1  & 81.3  & 58.7  & 76.4  & 82.6 \\
DGCNN~\cite{wang2019dgcnn} & 82.3 & 85.2  & 84.0    & 83.4  & 86.7  & 77.8  & 90.6  & 74.7  & 91.2  & 87.5  & 82.8  & 95.7 & 66.3  & 94.9  & 81.1  & 63.5  & 74.5  & 82.6 \\
Point-BERT~\cite{yu2022pointbert} & 84.1 & 85.6  & 84.3 & 84.8  & 88.0    & 79.8  & 91.0 & 81.7 & 91.6  & 87.9  & 85.2 & 95.6  & 75.6  & 94.7  & 84.3    & 63.4 & 76.3 & 81.5 \\
PointMLP~\cite{ma2022pointmlp} & 84.6 & 86.1  & 83.5 & 83.4  & 87.5    & 80.5  & 90.3 & 78.2 & 92.2  & 88.1  & 82.6 & 96.2  & 77.5  & 95.8  & 85.4    & 64.6 & 83.3 & 84.3 \\
KPConv~\cite{thomas2019kpconv} & \textbf{85.1} & 86.4  & 84.6 & 86.3  & 87.2    & 81.1  & 91.1 & 77.8 & 92.6  & 88.4  & 82.7 & 96.2  & 78.1  & 95.8  & 85.4    & 69.0 & 82.0 & 83.6 \\
\midrule

P2P (CN-B-SFPN) & 82.5 & 85.7 & 83.2 & 84.1 & 85.9 & 78.0 & 91.0 & 80.2 & 91.7 & 87.2 & 85.4 & 95.4 & 69.6 & 93.5 & 79.4 & 57.0 & 73.0 & 83.6   \\
P2P (CN-L-UPer) & 84.1 & \textbf{86.5} & 84.3 & 85.1 & 88.3 & 80.4 & 91.6 & 80.8 & 92.1 & 87.9 & 85.6 & 95.9 & 76.1 & 94.2 & 82.4 & 62.7 & 74.7 & 83.7  \\
\bottomrule
\end{tabular}
\end{adjustbox}}
\end{table*}
\vspace{-5pt}

The quantitative part segmentation results on ShapeNetPart dataset are shown in Table~\ref{tab:shapenetpart}. We implement the base version of ConvNeXt~\cite{liu2022convnext} as image model and SemanticFPN~\cite{kirillov2019semanticfpn} as 2D segmentation head for baseline comparison. We further implement the large version of ConvNeXt as the image model and more complex UPerNet~\cite{xiao2018uper} as 2D segmentation head to obtain better results.  Our P2P framework can achieve better performance than classical point-based methods, which demonstrates its potential in performing 3D dense prediction tasks based on 2D pre-trained image models. We leave it for future work to develop advanced segmentation heads and supervision strategies to better leverage pre-trained 2D knowledge in object-level or even scene-level point cloud segmentation. 

\subsection{Limitations}

While P2P shows outstanding classification performance and a promising scaling-up trend, we think that P2P may have difficulty in performing 3D tasks that concentrates on modality-dependent geometry analysis like completion, reconstruction, or upsampling. This is because P2P exploits and transfers the shared visual semantic knowledge between 2D and 3D domains, but these low-level tasks focus more on 3D domain-specific information. Apart from that, even though our P2P framework only requires a few trainable parameters to leverage pre-trained 2D knowledge and obtain high performance, its overall training parameters and FLOPs are still large when the image model is large. We will investigate these problems in future works.

\section{Conclusion}

In this paper, we propose a point-to-pixel prompting method to tune pre-trained image models for point cloud analysis. The pre-trained knowledge in image domain can be efficaciously adapted to 3D tasks at a low trainable parameter cost and achieve competitive performance compared with state-of-the-art point-based methods, mitigating the data-starvation problem in point cloud field that has been an obstacle for massive 3D pre-training researches.  The proposed Point-to-Pixel Prompting builds a bridge between 2D and 3D domains, preserving the geometry information of point clouds in projected images while transferring color information from the pre-trained image model back to the colorless point cloud. Experimental results on object classification and part segmentation demonstrate the superiority and potential of our proposed P2P framework.

\begin{ack}
This work was supported in part by the National Key Research and Development Program of China
under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under
Grant 62125603 and Grant U1813218, in part by a grant from the Beijing Academy of Artificial
Intelligence (BAAI).
\end{ack}

\appendix

\section{More Experimental Results}

\subsection{Experiments on Different Pre-trained Image Models}

We conduct more experiments on point cloud classification tasks with different image models of different scales, ranging from convolution-based ConvNeXt to attention-based Vision Transformer to Swin Transformer. The image model is pre-trained on ImageNet-22k~\cite{deng2009imagenet} dataset. We report the image classification performance of the original image model finetuned on ImageNet-1k dataset, the number of trainable parameters after Point-to-Pixel Prompting, and the classification accuracy on ModelNet40~\cite{wu2015modelnet} and ScanObjectNN~\cite{uy2019scanobjectnn} datasets.

From the quantitative results and accuracy curve in Table~\ref{tab:exp}, we can conclude that enlarging the scale of the same image model will result in higher classification performance, which is consistent with the observations in image classification. 

\begin{table}[h]
\caption{\small \textbf{More results on ModelNet40 and ScanObjectNN.} We report the image classification performance (IN Acc.) on ImageNet dataset of different image models. After migrating them to point cloud analysis with Point-to-Pixel Prompting, we report the number of trainable parameters (Tr. Param.), performance on ModelNet40 dataset (MN Acc.) and performance on ScanObjectNN dataset (SN Acc.).} 
\label{tab:exp}
\centering
\vspace{-5pt}
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{5pt}
    \caption{\footnotesize Vision Transformer.~\cite{dosovitskiy2020vit}}
    \vspace{-5pt}
    \centering
    \label{tab:mn:vit}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=0.7\textwidth}{
    \begin{tabular}{@{\hskip 5pt}>{\columncolor{white}[5pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][5pt]}gg@{\hskip 5pt}}
    \toprule
        Image Model   & IN Acc.(\%)  & Tr. Param.     & MN Acc.(\%)  & SN Acc.(\%)\\

    \midrule
        ViT-T    & --       & 0.10 M         & 91.3  & 79.9 \\
        ViT-S    & --       & 0.12 M         & 91.9  & 82.6\\
        ViT-B    & 84.0       & 0.15 M         & 92.4  & 84.1\\
        ViT-L    & 85.2       & 0.22 M         & 93.2  & 85.0\\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{5pt}
    \caption{\footnotesize Swin Transformer.~\cite{liu2021swinv2}}
    \vspace{-5pt}
    \centering
    \label{tab:mn:swin}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=0.7\textwidth}{
    \begin{tabular}{@{\hskip 5pt}>{\columncolor{white}[5pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][5pt]}gg@{\hskip 5pt}}
    \toprule
        Image Model   & IN Acc.(\%)  & Tr. Param.     & MN Acc.(\%)  & SN Acc.(\%)\\
    \midrule
        Swin-T    & 80.9       & 0.13 M         & 92.5  & 84.2 \\
        Swin-S    & 83.2       & 0.15 M         & 92.8  & 85.6\\
        Swin-B    & 85.2       & 0.17 M         & 93.2  & 85.8\\
        Swin-L    & 86.3      & 0.22 M         & 93.4  & 86.7\\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\hfill \\
\begin{subtable}{\textwidth}
    \setlength\tabcolsep{4pt}
    \caption{\footnotesize ConvNeXt.~\cite{liu2022convnext}}
    \vspace{-5pt}
    \centering
    \label{tab:mn:convnext}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=0.7\textwidth}{
    \begin{tabular}{@{\hskip 5pt}>{\columncolor{white}[5pt][\tabcolsep]}l|cc| >{\columncolor{Gray}[\tabcolsep][5pt]}gg@{\hskip 5pt}}
    \toprule
        Image Model   & IN Acc.(\%)  & Tr. Param.     & MN Acc.(\%)  & SN Acc.(\%)\\
    \midrule
        ConvNeXt-T    & 82.9       & 0.12 M         & 92.5  & 84.1\\
        ConvNeXt-S    & 84.6       & 0.14 M         & 92.7  & 86.2\\
        ConvNeXt-B    & 85.8       & 0.16 M         & 93.2 & 86.5\\
        ConvNeXt-L    & 86.6       & 0.19 M         & 93.4  & 87.1\\
    \bottomrule
    \end{tabular}
    }
\end{subtable}
\end{table}

\subsection{Ablation Studies on Test View Choices}

During training, the rotation angle  is randomly selected from  and  is randomly selected from  to keep the objects standing upright in the images. During inference, we evenly divide the range of  and  into several segments and combine them into multiple views for majority voting. We conduct ablations on the number of views on ModelNet40 dataset with ViT pre-trained on ImageNet-1k dataset as the image model. From the ablation results in Table~\ref{tab:abl_view}, we choose  values of  and  values of  to produce  views for majority voting. 

\begin{table}[t]
\caption{\small \textbf{Ablation studies on test view choices.} We evenly divide  and  into multiple segments. We report the classification accuracy on ModelNet40 dataset with ViT-B pre-trained on ImageNet-1k dataset as the image model.} 
\label{tab:abl_view}
\centering
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize Choices of . We choose  segments of .}
  \label{tab:abl_viewtheta}
  \adjustbox{width=0.9\textwidth}{
  \begin{tabular}{c|cccccc}
    \toprule
        
            & 2     & 4     & 6     & 8     & 10        & 12    \\
    \midrule
      & 90.2  & 92.2  & 92.5  & 92.5  &\bf{92.7} & 92.7  \\
    \bottomrule
 \end{tabular}}
 \end{subtable}
  \hfill
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize Choices of . We choose  segments of .}
  \label{tab:abl_viewphi}
  \adjustbox{width=0.82\textwidth}{
  \begin{tabular}{c|ccccc}
    \toprule
        
            & 2     & 3     & 4     & 5     & 6     \\
    \midrule
          & 92.4  & 92.6  & \bf{92.7} & 92.6  & 92.6 \\
    \bottomrule
 \end{tabular}}
 \end{subtable}
\end{table}

\begin{table}[t]
\caption{\small \textbf{Ablation studies on projection pooling strategy.} For classification experiment, we report the accuracy on ModelNet40 dataset with ViT-B pre-trained on ImageNet-1k dataset as the image model. For segmentation experiment, we report the instance average IoU on ShapeNetPart dataset with ConvNeXt-B as the image model and SemanticFPN as the segmentation head.} 
\label{tab:abl_pool}
\centering
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize Classification Ablations.}
  \label{tab:abl_pool_cls}
  \adjustbox{width=0.62\textwidth}{
  \begin{tabular}{c|ccc}
    \toprule
    Method    & max   & mean  & sum    \\
    \midrule
    Accuracy  & 92.2  & 92.3  & \textbf{92.7}   \\
    \bottomrule
 \end{tabular}}
 \end{subtable}
  \hfill
\begin{subtable}{0.48\textwidth}
  \setlength\tabcolsep{4pt}
  \centering
  \caption{\footnotesize Segmentation Ablations.}
  \label{tab:abl_pool_seg}
  \adjustbox{width=0.6\textwidth}{
  \begin{tabular}{c|ccc}
    \toprule
    Method    & max   & mean  & sum    \\
    \midrule
      & 85.7 & 85.7 & 85.7   \\
    \bottomrule
 \end{tabular}}
 \end{subtable}
\end{table}

\subsection{Ablation Studies on Projection Pooling Strategy}

During the geometry-preserved projection, several points may fall in the same pixel. In P2P, we propose to \textit{add} the features of these points altogether for better optimization and keeping geometry density information. Here we conduct ablations on the pooling strategy in Table~\ref{tab:abl_pool}, including max-pooling, mean-pooling and summation. For classification experiment, we report the accuracy on ModelNet40 dataset with ViT-B pre-trained on ImageNet-1k dataset as the image model. For segmentation experiment, we report the instance average IoU on ShapeNetPart dataset with ConvNeXt-B as the image model and SemanticFPN~\cite{kirillov2019semanticfpn} as the segmentation head.

From the classification ablation results, summation is better than max-pooling and mean-pooling. On the one hand, the max-pooling operation drops much geometric information in one pixel. On the other hand, the mean-pooling operation neglects the density information from 3D domain, which also undermines the geometrical knowledge in projected images.

However, in segmentation experiments, the aforementioned three pooling strategies produce the same part segmentation performance. This may be because the multi-hot 2D labels in dense prediction provide extra geometrical guidance that makes up for the gap among different pooling strategies. 



\subsection{Visualization of Feature Distributions}

Figure~\ref{fig:tsne} shows feature distributions of ModelNet40 and ScanObjectNN datasets in t-SNE visualization. We can conclude that with our proposed Point-to-Pixel Prompting, the pre-trained image model can extract discriminative features from projected colorful images for point cloud analysis.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tsne-modelnet.png}
         \caption{Feature distribution on ModelNet40.}
         \label{fig:tsne_modelnet}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tsne-scanobjectnn.png}
         \caption{Feature distribution on ScanObjectNN.}
         \label{fig:tsne_scanobjectnn}
     \end{subfigure}
     \caption{Visualization of feature distributions in t-SNE representations. Best view in colors.}
     \label{fig:tsne}
\end{figure}

\section{Network Architecture}

\subsection{Point-to-Pixel Prompting}

The geometry encoder is implemented as a one-layer DGCNN~\cite{wang2019dgcnn} edge convolution. The input points coordinates are first embedded into 8-dim features  with a channel-wise convolution. Then we use the k-nearest-neighbor (kNN) algorithm to locate  neighbors  of each point , and concat the central point feature  with the relative feature  between each point  and neighboring points . Then the concatenated features are processed by a 2D convolution with kernel size 1 followed by a max-pooling layer within all points in , resulting in a geometry feature  of  dims.

In the geometry-preserved projection module, we first calculate the coordinate range  of the input point cloud. Then we calculate the grid size  so that the projected object can be fit in the image  with .

The coloring module consists of a basic block from ResNet~\cite{he2016resnet} architecture design with 33 convolutions and a final 2D convolution with kernel size 1, smoothing the pixel-level feature distribution and predicting RGB channels of image .

\begin{table}
\caption{Architecture details and experiment settings of our framework.  denotes the embedding dimension of image features extracted by pre-trained image models.} \label{tab:detail}
\vspace{5pt}

\begin{minipage}{.52\textwidth}
\begin{subtable}{\textwidth}
\caption{\footnotesize Architecture of Classification Model.}
  \label{tab:scanobjnn}
  \vspace{2pt}
  \newcolumntype{g}{>{\columncolor{Gray}}c}
  \small
  \setlength\tabcolsep{2pt}
  \adjustbox{width=0.95\linewidth}{
    \begin{tabular}{c|c|c c|c c}
    \toprule
    Module      & Block   &     &      & Kernel     & kNN \\
    \midrule
    \multirow{3}{*}{Geometry Encoder} 
                & Conv1d    & 3     & 8     & 1     &       \\
                & DGCNN     & 8     & 64    &       & 32    \\
                & Conv1d    & 64    & 64    & 1     &       \\
    \midrule
    \multirow{3}{*}{Image Coloring}
                & Basic Block& 64   & 64    & 3     &       \\
                & Conv2d    & 64    & 64    & 1     &       \\
                & Conv2d    & 64    & 3     & 1     &       \\
    \midrule
    \midrule
    \multirow{1}{*}{CLS Head}
                & Linear    &    & 40    &       &       \\
    \bottomrule
  \end{tabular}}
\end{subtable}
\hfill
\begin{subtable}{\linewidth}
\setlength\tabcolsep{5pt}
\caption{\footnotesize Architecture of Segmentation Model.}
  \label{tab:arc_modelnet}
  \vspace{2pt}
  \newcolumntype{g}{>{\columncolor{Gray}}c}
  \small
  \setlength\tabcolsep{2pt}
  \adjustbox{width=0.95\linewidth}{
  \begin{tabular}{c|c|c c|c c}
    \toprule
    Module      & Block   &     &      & Kernel     & kNN \\
    \midrule
    \multirow{4}{*}{Geometry Encoder} 
                & Conv1d    & 3     & 8     & 1     &       \\
                & DGCNN     & 8     & 64    &       & 32    \\
                & DGCNN     & 64    & 128   &       & 32    \\
                & Conv1d    & 128   & 64    & 1     &       \\
    \midrule
    \multirow{3}{*}{Image Coloring}
                & Basic Block& 64   & 64    & 3     &       \\
                & Conv2d    & 64    & 64    & 1     &       \\
                & Conv2d    & 64    & 3     & 1     &       \\
    \midrule
    \midrule
    \multirow{1}{*}{SEG Head}
                & Semantic FPN  &    & 50    &       &       \\
    \bottomrule
  \end{tabular}}
\vspace{5pt}
\end{subtable}
\end{minipage}\begin{minipage}{.44\textwidth}
\vspace{5pt}
\begin{subtable}{\textwidth}
  \setlength\tabcolsep{5pt}
  \caption{\footnotesize Experiment Settings for Classification.}
  \label{tab:setting}
  \vspace{2pt}
  \newcolumntype{g}{>{\columncolor{Gray}}c}
  \small
  \setlength\tabcolsep{2pt}
  \adjustbox{width=0.95\linewidth}{
    \begin{tabular}{l|c}
    \toprule
    Config          & Value \\
    \midrule
    optimizer       & AdamW~\cite{loshchilov2017adamw} \\
    learning rate   & 5e-4  \\
    weight decay    & 5e-2  \\
    learning rate scheduler & cosine~\cite{loshchilov2016sgdr}    \\
    training epochs & 300   \\
    batch size      & 64    \\
    GPU device      & RTX 3090 Ti   \\
    image size      & 224224    \\
    patch size     & 16 \\
    drop path rate & 0.1 \\
    image normalization & ImageNet style\\
    \midrule
    number of points& \makecell[l]{4096 (ModelNet) \\ 2048 (ScanObjectNN)} \\
    \midrule
    augmentation    & \makecell[c]{scale  \\ trans }    \\
    \midrule
    rotation angle  & \makecell[c]{ \\ } \\
    \bottomrule
  \end{tabular}}
\end{subtable}
\end{minipage}
\end{table}

\section{Implementation Details}

The implementation details of architectural design and experimental settings are shown in Table~\ref{tab:detail}, where  denotes the embedding dimension of image features extracted by pre-trained image models. We use slightly different architectures for classification and part segmentation. We use 4096 points for ModelNet40 to produce projected images that are relatively smoother, while too few points may lead to sparse and discontinuous pixel distribution in projected images that prevent them from being similar to real 2D images. 


\bibliographystyle{plain}
\bibliography{ref}


\end{document}