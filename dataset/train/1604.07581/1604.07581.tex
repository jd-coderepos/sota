\documentclass{article}

\usepackage{fullpage}
\usepackage{amsthm,amssymb,amsmath}  
\usepackage{comment,xspace,enumerate}
\usepackage[capitalise]{cleveref}
\usepackage[utf8]{inputenc}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{authblk}


\newcommand{\myparagraph}[1]{\paragraph{#1.}}

 
  \theoremstyle{plain}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{lemma}[theorem]{Lemma}  
  \newtheorem{corollary}[theorem]{Corollary}  
  \newtheorem{fact}[theorem]{Fact}
  \newtheorem{observation}[theorem]{Observation}
  \theoremstyle{definition}
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem{example}[definition]{Example}
  \newtheorem{remark}[definition]{Remark}
 
  \newtheorem*{claim}{Claim}
 
  
  \title{Pattern Matching and Consensus Problems on Weighted Sequences and Profiles\footnote{Work supported by the Polish Ministry of Science and Higher Education under the `Iuventus Plus' program in 2015-2016 grant no 0392/IP3/2015/73.}}

\author[1]{Tomasz Kociumaka}
\author[2]{Solon P. Pissis}
\author[1,2]{Jakub Radoszewski\footnote{The author is a Newton International Fellow.}}

\affil[1]{Institute of Informatics, University of Warsaw, Warsaw, Poland\\
    \texttt{[kociumaka,jrad]@mimuw.edu.pl}}
\affil[2]{Department of Informatics, King's College London, London, UK\\
    \texttt{solon.pissis@kcl.ac.uk}
}


\date{\vspace{-5ex}}

 
  \usepackage{amsmath,amsfonts}
  \usepackage{amsthm}
  \usepackage{tikz}
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{comment}
  \usepackage{multirow}
  \usepackage{todonotes}
  \usepackage[noend, noline, ruled]{algorithm2e}
  \usepackage[capitalise]{cleveref}
      
  \usetikzlibrary{decorations.pathreplacing,calc}

\setlength{\fboxsep}{5pt}
\newsavebox{\mybox}
\newenvironment{dsproblem}[1]
{\begin{center}\begin{lrbox}{\mybox}\begin{minipage}{0.96\columnwidth}#1 \textsc{Problem}\\}
{\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}\end{center}}


  \newcommand{\defdsproblem}[3]{
  \begin{dsproblem}{#1}
\textbf{Input:} #2\\
\textbf{Output:} #3
  \end{dsproblem}
  }

  \newcommand{\defdsproblembin}[3]{
  \begin{dsproblem}{#1}
\textbf{Input:} #2\\
\textbf{Question:} #3
  \end{dsproblem}
  }

\newcommand{\defdsproblempar}[4]{
  \begin{dsproblem}{#1}
\textbf{Input:} #2\\
\textbf{Question:} #3\\
\textbf{Parameters:} #4
  \end{dsproblem}
  }

  \newcommand{\defdsproblemoutpar}[4]{
  \begin{dsproblem}{#1}
\textbf{Input:} #2\\
\textbf{Output:} #3\\
\textbf{Parameters:} #4
  \end{dsproblem}
  }

  \newcommand{\MK}{\textsc{Multichoice Knapsack}\xspace}
  \newcommand{\PM}{\textsc{Profile Matching}\xspace}
  \newcommand{\WPM}{\textsc{Weighted Pattern Matching}\xspace}
  \newcommand{\GWPM}{\textsc{GWPM}\xspace}
  \newcommand{\GWPMFull}{\textsc{General Weighted Pattern Matching}\xspace}
  \newcommand{\WC}{\textsc{Weighted Consensus}\xspace}
  \newcommand{\SDWCFull}{\textsc{Short Dissimilar Weighted Consensus}\xspace}
  \newcommand{\SDWC}{\textsc{SDWC}\xspace}
  \newcommand{\PC}{\textsc{Profile Consensus}\xspace}
  \newcommand{\Knapsack}{\textsc{Knapsack}\xspace}
  \newcommand{\SubsetSum}{\textsc{Subset Sum}\xspace}
  \newcommand{\Sum}{\textsc{Sum}\xspace}
    



  \newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
  \newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
  \newcommand{\Oh}{\mathcal{O}}
  \newcommand{\Ohtilde}{\tilde{\mathcal{O}}}
    \newcommand{\Ohstar}{\mathcal{O}^*}
 \newcommand{\poly}{\mathit{poly}}
  \newcommand{\sub}{\subseteq}

  
  \newcommand{\Occ}{\mathit{Occ}}
  \newcommand{\lcp}{\mathit{lcp}}
  \newcommand{\ptr}{\mathit{ptr}}
  \newcommand{\str}{\mathit{str}}

  \renewcommand{\H}{\mathcal{H}}
  \newcommand{\C}{\mathcal{C}}
  \newcommand{\B}{\mathcal{B}}
  \newcommand{\F}{\mathcal{F}}
  \renewcommand{\L}{\mathcal{L}}
  \newcommand{\R}{\mathcal{R}}
  \renewcommand{\P}{\mathcal{P}}
  \newcommand{\Ls}{\mathcal{L}^*}
  \newcommand{\Rs}{\mathcal{R}^*}

  \newcommand{\WP}{\mathit{WP}}
  \newcommand{\PP}{\mathit{PP}}

  \newcommand{\fr}{\ensuremath{\frac1z}}
  \newcommand{\frsq}{}
  \newcommand{\match}{\approx_{\frac1z}}
  \newcommand{\matchsq}{\approx_{\frac{1}{\sqrt{z}}}}
  \newcommand{\mayqed}{}
  \newcommand{\mmid}{\mathrm{mid}}
  
  \DeclareMathOperator*{\rank}{rank}
  
  \newcommand{\HammingDistance}{d_H}
  \newcommand{\Score}{\mathrm{Score}}
  \newcommand{\NumStrings}{\mathrm{NumStrings}}


\begin{document}
  \maketitle
\begin{abstract}
  We study pattern matching problems on two major representations of uncertain sequences used in molecular biology:
  weighted sequences (also known as position weight matrices, PWM) and profiles (i.e., scoring matrices).
  In the simple version, in which only the pattern or only the text is uncertain, 
  we obtain efficient algorithms with theoretically-provable running times using a variation of the lookahead scoring technique.
  We also consider a general variant of the pattern matching problems in which both the pattern and the text are uncertain.
  Central to our solution is a special case where the sequences have equal length, called the consensus problem.
  We propose algorithms for the consensus problem parameterized by the number of strings that match one of the sequences.
  As our basic approach, a careful adaptation of the classic meet-in-the-middle algorithm for the knapsack problem is used.
  On the lower bound side, we prove that our dependence on the parameter is optimal up to lower-order terms
  conditioned on the optimality of the original algorithm for the knapsack problem.
\end{abstract}

  \section{Introduction}
  We study two well-known representations of uncertain texts: \emph{weighted sequences} and \emph{profiles}.
  A \emph{weighted sequence} (also known as uncertain sequence or position weight matrix, PWM)
  for every position and every letter of the alphabet specifies the probability of occurrence of this letter at this position;
  see \cref{table:weighted_sequence} for an example.
  A weighted sequence represents many different strings, each with the probability
  of occurrence equal to the product of probabilities of its letters at subsequent positions of the weighted sequence.
  Usually a threshold \fr\ is specified, and one considers only strings that match the weighted sequence with probability at least \fr.
  A \emph{scoring matrix} (or a profile) of length  is an  matrix.
  The \emph{score} of a string of length  is the sum of scores in the scoring matrix of the subsequent
  letters of the string at the respective positions.
  A string is said to match a scoring matrix if its matching score is above a specified threshold .

      \begin{figure}[htpb]
      \renewcommand*{\arraystretch}{1.2}
      \begin{center}
      \begin{tabular}{|c|c|c|c|}
        \hline
         &  &  &  \\
        \hline
        \  \ & \  \ & \  \ & \  \ \\
        \  \ & \  \ & \  \ & \  \ \\
        \hline
      \end{tabular}
      \end{center}
      \caption{A weighted sequence  of length 4 over the alphabet }\label{table:weighted_sequence}
      \end{figure}

  \subparagraph*{\WPM and \PM}
  First of all, we study the standard variants of pattern matching problems on weighted sequences and profiles,
  in which only the pattern or the text is an uncertain sequence.
  In the best-known formulation of the \WPM problem, we are given a weighted sequence of length , called a text,
  a solid (standard) string of length , called a pattern, both over an alphabet of size ,
  and a \emph{threshold probability} \fr.
  We are asked to find all positions in the text where the fragment
  of length  represents the pattern with probability at least \fr.
  Each such position is called an \emph{occurrence} of the pattern in the text;
  we also say that the fragment of the text and the pattern \emph{match}.
  The \WPM problem can be solved in  time via the Fast Fourier Transform~\cite{KCL_publication}.
  In a more general indexing variant of the problem, considered in
  \cite{amir_weighted_property_matching_j,costas_weighted_suffix_tree_j}, one can preprocess a weighted text
  in  time to report all  occurrences of a given solid pattern of length  in  time.
  (A similar indexing data structure, which assumes , was presented in~\cite{DBLP:conf/edbt/BiswasPTS16}.)
  Very recently, the index construction time was reduced to  for constant-sized alphabets \cite{CPM2016}.

  In the classic \PM problem, the pattern is an  profile, the text is a solid string of length , and
  our task is to find all positions in the text where the fragment of length  has a score above a specified
  threshold .
  A naive approach to the \PM problem works in  time.
  A broad spectrum of heuristics improving this algorithm in practice is known; for a survey see~\cite{DBLP:journals/tcs/PizziU08}.
  One of the principal techniques, coming in different flavours, is \emph{lookahead scoring} that consists in checking if a partial match
  could possibly be completed by the following highest scoring letters in the scoring matrix and, if not, pruning the
  naive search.
  The \PM problem can also be solved in  time via the Fast Fourier Transform~\cite{DBLP:journals/jcb/RajasekaranJS02}.
  
  \subparagraph*{\WC and \PC}
  As our most involved contribution, we study a general variant of pattern matching on weighted sequences
  and the consensus problems on uncertain sequences, which are closely related to the \MK problem.
  In the \WC problem, given two weighted sequences of the same length, we are to check if there is
  a string that matches each of them with probability at least \fr.
  A routine to compare user-entered weighted sequences with existing weighted sequences in the database is used,
  e.g., in JASPAR, a well-known database of PWMs \cite{JASPAR}.
  In the \GWPMFull (\GWPM) problem, both the pattern and the text are weighted.
  In the most common definition of the problem (see \cite{DBLP:conf/cwords/BartonP15,costas_weighted_suffix_tree_j}),
  we are to find all fragments of the text that give a positive answer to the \WC problem with the pattern.
  The authors of~\cite{DBLP:conf/cwords/BartonP15} proposed an algorithm for the \GWPM problem based on the weighted prefix table
  that works in  time.

  In an analogous way to the \WC problem, we define the \PC problem.
  Here we are to check for the existence of a string that matches both the scoring matrices above threshold .
  The \PC problem is actually a special case of the well-known (especially in practice) \MK problem
  (also known as the \textsc{Multiple Choice Knapsack} problem).
  In this problem, we are given  classes  of at most  items each--- items in total---each item  characterized by a value  and a weight .
  The goal is to select one item from each class so that the sums of values and of weights of the items are
  below two specified thresholds,  and .
  (In the more intuitive formulation of the problem, we require the sum of values to be \emph{above} a specified threshold,
  but here we consider an equivalent variant in which both parameters are symmetric.)
  The \MK problem is widely used in practice, but most research concerns approximation or heuristic solutions;
  see \cite{DBLP:books/daglib/0010031} and references therein.
  As far as exact solutions are concerned, the classic meet-in-the middle approach by Horowitz and Sahni~\cite{DBLP:journals/jacm/HorowitzS74},
  originally designed for the (binary) \Knapsack problem, immediately generalizes to 
  an -time\footnote{The  notation suppresses factors polynomial with respect to the instance size (encoded in binary). } solution for \MK. 
  


  Several important problems can be expressed as special cases of the \MK problem using folklore reductions (see~\cite{DBLP:books/daglib/0010031}).
  This includes the \SubsetSum problem, which for a set of  integers asks whether there is a subset summing up to a given integer ,
  and the -\Sum problem which, for  classes of  integers, asks to choose one element from each
  class so that the selected integers sum up to zero. 
  These reductions give immediate hardness results for the \MK problem, and they can be adjusted to yield the same consequences for \PC.
  For the \SubsetSum problem, as shown in \cite{DBLP:conf/mfcs/EtscheidKMR15,DBLP:books/daglib/0069796}, the existence of an -time solution for every 
  would violate the Exponential Time Hypothesis (ETH)~\cite{DBLP:journals/jcss/ImpagliazzoP01,ETHsurvey}.
  Moreover, the  running time, achieved in \cite{DBLP:journals/jacm/HorowitzS74}, has not been improved yet despite much effort.
  The 3-\Sum conjecture \cite{DBLP:journals/comgeo/GajentaanO95} and the more general -\Sum conjecture state that the 3-\Sum and -\Sum problems cannot be solved in
   time and  time, respectively, for any .



  \subparagraph*{Our Results}
  As the first result, we show how the lookahead scoring technique combined with a data structure
  for answering longest common prefix queries in a string can be applied to obtain simple and efficient
  algorithms for the standard pattern matching problems on uncertain sequences.
  For a weighted sequence, by  we denote the size of its list representation, and by  the
  maximal number of letters with score at least  at a single position (thus ).
  In the \PM problem, we set  as the number of strings that match the scoring matrix with score above .
  In general , however, we may assume that for practical data this number is actually much smaller.
  We obtain the following running times:
  \begin{itemize}
    \item  for \PM;
    \item  deterministic and  randomized (Las Vegas, failure with probability 
    for any given constant ) for \WPM.
  \end{itemize}
  
  The more complex part of our study is related to the consensus problems and to the \GWPM problem.
  Instead of considering \PC, we study the more general \MK.
  We introduce parameters based on the number of solutions with \emph{feasible} weight or value:
  , that is,
  the number of choices of one element from each class that satisfy the value threshold;
  ;
  , and .
  We obtain algorithms with the following complexities:
  \begin{itemize}
    \item  for \MK;
    \item  for \WC and  for \GWPMFull.
  \end{itemize}
  
  Since , our running time for \MK in the worst case matches (up to lower order terms) the time complexities of the fastest known solutions
  for both \SubsetSum (also binary \Knapsack) and 3-\Sum. 
  The main novel part of our algorithm for \MK is an appropriate (yet intuitive) notion of ranks of partial solutions.
  We also provide a simple reduction from \MK to \WC, which lets us transfer the negative results to the \GWPM problem.
  \begin{itemize}
    \item The existence of an -time solution for \WC for every   would violate the Exponential Time Hypothesis.
    \item For every , an -time solution for \WC would imply an -time algorithm for \SubsetSum.
    \item For every , an -time\footnote{
      The  notation ignores factors polylogarithmic with respect to the input size.
    } solution for \WC would imply an -time algorithm for 3-\Sum.
  \end{itemize}
  For the higher-order terms our complexities match the conditional lower bounds;
  therefore, we put significant effort to keep the lower order terms of the complexities
  as small as possible.

  \subparagraph*{Model of Computations}
  For problems on weighted sequences, we assume the word RAM model with word size  and .
We consider the log-probability model of representations of weighted sequences, that is, we assume that
  probabilities in the weighted sequences and the threshold probability \fr\ are all of the form ,
  where  and  are constants and  is an integer that fits in a constant number of machine words.
  Additionally, the probability 0 has a special representation.
  The only operations on probabilities in our algorithms are multiplications and divisions, which can be
  performed exactly in  time in this model.
  Our solutions to the \MK problem only assume the word RAM model with word size ,
  where  is the sum of integers in the input instance; this does not affect the  running time.

  \subparagraph*{Structure of the Paper}
  We start with Preliminaries, where we formally introduce the problems and the main notions used throughout the paper.
  The following three sections describe our algorithms: in \cref{sec:EWPM} for \PM and \WPM;
  in \cref{sec:MK} for \PC; and in \cref{sec:GWPMReduction} for \WC and \GWPMFull.
  A tailor-made, yet more efficient algorithm for \GWPMFull is presented in \cref{app:SDWC}.
  We conclude with \cref{app:fast}, where we introduce faster algorithms and matching lower bounds
  for \MK and \GWPM in the case that  is large.

  \section{Preliminaries}\label{sec:Preliminaries}
    Let  be an alphabet of size .
    A \emph{string}  over  is a finite sequence of letters from .
    We denote the length of  by  and, for ,
    the -th letter of  by .
    By  we denote the string  called a \emph{factor} of 
    (if , then the factor is an empty string).
    A factor is called a \emph{prefix} if  and a \emph{suffix} if .
    For two strings  and , we denote their concatenation by  ( in short).

    For a string  of length , by  we denote the length of the longest common prefix of factors  and .
    The following fact specifies a known efficient data structure answering such queries.
    It consists of the suffix array with its inverse, the LCP table and a data structure for range minimum queries on
    the LCP table; see \cite{AlgorithmsOnStrings} for details.

    \begin{fact}\label{fct:ver}
      Let  be a string of length  over alphabet of size .
      After -time preprocessing, given indices  and  () one can compute  in  time.
    \end{fact}

    The \emph{Hamming distance} between two strings  and  of the same length,
    denoted by , is the number of positions where the strings have different letters.

    \subsection{Profiles}
    In the \PM problem, we consider a \emph{scoring matrix} (a profile)  of size .
    For  and , we denote the integer score of the letter 
    at the position~ by .
    The \emph{matching score} of a string  of length  with the matrix  is
    
    If  for an integer \emph{threshold} , then we say that the string  \emph{matches the matrix 
    with threshold }.
    We denote the number of strings  that math  with threshold  by .
    
    For a string  and a scoring matrix , we say that  \emph{occurs in  at position 
    with threshold } if  matches  with threshold .
    We denote the set of all positions where  occurs in  by .
    These notions let us define the \PM problem:

    \defdsproblemoutpar{\PM}{
      A string  of length , a scoring matrix  of size , and
      a threshold .
    }{
      The set .
    }{
      .
    }

    \subsection{Weighted Sequences}
    A \emph{weighted sequence}  of length  over alphabet 
    is a sequence of sets of pairs of the form:
    
    Here,  is the occurrence probability of the letter  at the position .
    These values are non-negative and sum up to 1 for a given .
 
    For all our algorithms, it is sufficient that the probabilities sum up to \emph{at most} 1 for each position.
    Also, the algorithms sometimes produce auxiliary weighted sequences with sum of probabilities being smaller than 1 on some positions.

    We denote the maximum number of letters occurring at a single position of the weighted sequence
    (with non-zero probability) by 
    and the total size of the representation of a weighted sequence by .
    The standard representation consists of  lists with up to  elements each,
    so .   However, the lists can be shorter in general.
    Also, if the threshold probability  is specified, at each position of a weighted sequence
    it suffices to store letters with probability at least , and clearly
    there are at most  such letters for each position. This reduction can be performed in linear time, so we shall always assume
    that .
        


    The \emph{probability of matching} of a string  with a weighted sequence , , is 
    
    We say that a string  \emph{matches} a weighted sequence 
    with probability at least \fr, denoted by , if .
    Given a weighted sequence , by  we denote weighted sequence,
    called a \emph{factor} of , equal to  (if , then the factor is empty).
    We then say that a string  \emph{occurs} in  at position  if  matches the factor .
    We also say that  is a \emph{\fr-solid factor} of  at position  (a \emph{\fr-solid prefix} if 
    and a \emph{\fr-solid suffix} if ).
    We denote the set of all positions where  occurs in  by .

    \defdsproblem{\WPM}{
      A string  of length  and a weighted sequence  of length  with at most  letters at each position
      and  in total, and a threshold probability \fr.
    }{
      The set .
    }




  \section{Profile Matching and Weighted Pattern Matching}\label{sec:EWPM}
    In this section we present a solution to the \PM problem.
    Afterwards, we show that it can be applied for \WPM as well.

    For a scoring matrix , the \emph{heavy string} of , denoted ,
    is constructed by choosing at each position the heaviest letter, that is,
    the letter with the maximum score (breaking ties arbitrarily).
    Intuitively,  is a string that matches  with the maximum score.

    \begin{observation}\label{obs:crucial_profile}
      If we have  for a string  of length  and an  scoring matrix ,
      then  where .
    \end{observation}
    \begin{proof}
      Let .
      We can construct  strings of length  that match  with a score above 
      by taking either of the letters  or  at each position  such that .
      Hence, , which concludes the proof.
    \mayqed\end{proof}

    Our solution for the \PM problem works as follows.
    We first construct  and the data structure for finding lcp values between suffixes of  and .
    Let the variable  store the matching score of .
    In the -th step, we calculate the matching score of  by iterating through
    subsequent mismatches between  and  and making adequate updates in the matching score , which
    starts at .
    The mismatches are found using lcp-queries.
    This process terminates when the score  drops below  or when all the mismatches have been found.
    In the end, we include  in  if .
    A pseudocode of this approach is given below for completeness.

    \begin{theorem}
      \PM problem can be solved in  time.
    \end{theorem}
    \begin{proof}
      Let us bound the time complexity of the presented algorithm.
      The heavy string  can be computed in  time.
      The data structure for -queries in  can be constructed in  time by \cref{fct:ver}.
      Each query for  can then be answered in constant time by a corresponding -query
      in , potentially truncated to the end of .
      Finally, for each position  in the text  we will consider at most  mismatches between  and ,
      as afterwards the score  drops below  due to \cref{obs:crucial_profile}.
    \mayqed\end{proof}

      \begin{procedure}[htpb]
      ;\ \ ;\ \ \;
      \;
      Compute the data structure for -queries in \;
      \;
      \For{ \KwSty{to} }{
        ;\ \  ;\ \ \;
        \While{ \KwSty{and} }{
          \;
          ;\ \ \;
          \If{}{
            ;
          }
        }
        \lIf{}{insert  to }
      }
      \Return{}\;
      \caption{ProfileMatching(, , )}
    \end{procedure}
    
    Basically the same approach can be used for \WPM.
    In a natural way, we extend the notion of a heavy string to weighted sequences.
    Now we can restate \cref{obs:crucial_profile} in the language of probabilities instead of scores:
    \begin{observation}\label{obs:crucial}
      If a string  matches a weighted sequence  of the same length with probability at least \fr,
      then .
    \end{observation}

    Comparing to the solution to \PM, we compute the heavy string of the text instead
    of the pattern.
    An auxiliary variable  stores the matching probability between a factor
    of  and the corresponding factor of ; it needs to be updated when we move to the next position
    of the text.
    In the implementation, we perform the following operations on a weighted sequence:
    \begin{itemize}
      \item computing the probability of a given letter at a given position,
      \item finding the letter with the maximum probability at a given position.
    \end{itemize}

      \begin{procedure}[h]
      \caption{WeightedPatternMatching(, , \fr)}
      ;\ \ ;\ \ \;
      \;
      Compute the data structure for -queries in \;
      \;
      \For{ \KwSty{to} }{
        ;\ \ ;\ \ \;
        \While{ \KwSty{and} }{
          \;
          ;\ \ \;
          \If{}{
            ;
          }
          }
        \lIf{}{insert  to }
        \If{}{
          \;
        }
      }
      \Return{}\;
    \end{procedure}

    In the standard list representation, the latter can be performed on a single weighted sequence
    in  time after -time preprocessing.
    We can perform the former in constant time if, in addition to the list representation,
    we store the letter probabilities in a dictionary implemented using perfect hashing \cite{DBLP:journals/jacm/FredmanKS84}.
    This way, we can implement the algorithm in  time w.h.p.
    Alternatively, a deterministic dictionary~\cite{DBLP:conf/icalp/Ruzic08} can be used to obtain a deterministic solution in  time.
    We arrive at the following result.

    \begin{theorem}\label{thm:wpm}
      \WPM can be solved in  time with high probability by a Las-Vegas algorithm
      or in  time deterministically.
    \end{theorem}

    \begin{remark}
      In the same complexity one can solve the \GWPM problem with a solid text.
    \end{remark}
    
    
 \section{Profile Consensus as Multichoice Knapsack}\label{sec:MK}
 Let us start with a precise statement of the \MK problem.
    \defdsproblempar{\MK}{
      A set  of  items partitioned into  disjoint classes , each of size at most ,
      two integers  and  for each item , and two thresholds  and .
    }{
      Does there exist a \emph{choice}  (a set  such that  for each )
      satisfying both
       and 
      ?
    }{
       and : the number of choices  satisfying 
      and , respectively, as well as  and .
    }

  Indeed we see that the \PC problem reduces to the \MK problem.
  For two  scoring matrices, we construct  classes of  items each,
  with values equal to the scores of the letters in the first matrix and weights equal to the scores in the second matrix;
  both thresholds  and  are equal to .
    
    For a fixed instance of \MK, we say that  is a \emph{partial choice} if  for each class.
    The set  is called its \emph{domain}.
    For a partial choice , we define  and .    
    
    
     The classic -time solution to the \Knapsack problem~\cite{DBLP:journals/jacm/HorowitzS74}
 partitions  into two domains  of size roughly ,
 and for each  it generates all partial choices  ordered by .
 Hence, it reduces the problem to an instance of \MK with  classes.
 It is solved using the following lemma, proved below for completeness.
  \begin{lemma}\label{lem:knap2}
    The \MK problem can be solved in  time if  and
    the elements  of  and  are sorted by .
  \end{lemma}
    \begin{proof}
      Since the items of  and  are sorted by , 
      a single scan through these items lets us remove all irrelevant elements, that is, elements dominated by other elements in their class.
      Next, for each  we compute  such that  but otherwise  is largest possible.
      As we have removed irrelevant elements from , this item also minimizes  among all elements satisfying .
      Hence, if there is a feasible solution containing , then  is feasible. 
      If we process elements  by non-decreasing values , the values  do not increase, and
      thus the items  can be computed in  time in total.
    \end{proof} 

  The same approach generalizes to \MK. 
  The partition is chosen to balance the number of partial choices
  in each domain, and the worst-case time complexity is , where  is the number of choices.
  
  Our aim in this section is to replace  with the parameter  (which never exceeds ).
  The overall running time is going to be  : an overhead  of  appears.


  Two challenges arise once we adapt the meet-in-the-middle approach:
  how to restrict the set of partial choices to be generated so that a feasible solution is not missed,
  and how to define a partition  to balance the number of partial choices generated for  and .
  A natural idea to deal with the first issue is to consider only partial choices with small values  or .
  This is close to our actual solution, which is based on the notion of \emph{ranks} of partial choices.
  Our approach to the second problem is to consider multiple partitions: those of the form  for .
  This results in an extra  factor in the time complexity.
  However, in \cref{ss} we introduce preprocessing that lets us assume that .
  While dealing with these two issues, some further effort is required to avoid few other extra terms in the running time.
  In case of our algorithm, this is only , which stems from the fact that we need to keep partial solutions ordered by . 
  
  
    For a partial choice , we define  as the number of partial choices  with the same domain for which . 
    We symmetrically define .
    Ranks are introduced as an analogue of probabilities in weighted sequences.
    Probabilities are multiplicative, while for ranks we have submultiplicativity:
      
    \begin{fact}\label{fct:comb}
      Assume that  is a decomposition of a partial choice  into two disjoint subsets.
      Then  (and same for ).
    \end{fact}
    \begin{proof}
      Let  and  be the domains of  and , respectively.
      For every partial choices  over  and  over  such that  and , we have
      .
      Hence,  must be counted while determining .
    \end{proof}
  
  
      
   For , let  be the list of partial choices with domain  ordered by value ,
   and for  let  be the value  of -th element of  ( if ).
  Analogously, for , we define  as the list of partial choices over  ordered by ,
  and for ,  as the value of the -th element of  ( if ).
  
The following two observations yield a decomposition of each choice into a single item and two partial solutions of a small rank.
  In particular, we do not need to know  in order to check if the ranks are sufficiently large.
  
  \begin{lemma}\label{lem:decomp}
  Let  and  be positive integers such that  for each .
  For every choice  with , there is an index  and a decomposition 
  such that , , and .
  \end{lemma}
  \begin{proof}
    Let  with  and, for , let .
    If , we set , , and , satisfying the claimed conditions.
    
    Otherwise, we define  as the smallest index  such that ,
    and we set , , and .
    The definition of  implies  and .
    Moreover, we have 
    and thus . \end{proof}
  
  \begin{fact}\label{fct:bound}
  Let .
  If  for some , then .
  \end{fact}
  \begin{proof}
  Let  and  be the -th and -th entry in  and , respectively.
  Note that  implies
   by definition of~.
  Moreover,  and 
  (the equalities may be sharp due to draws).
  Now, \cref{fct:comb} yields the claimed bound.
  \end{proof}

   Note that  can be obtained by interleaving  copies of , where each copy corresponds
  to extending the choices from  with a different item.
  If we were to construct  having access to the whole , we could proceed as follows.
  For each , we maintain an \emph{iterator} on 
  pointing to the first element  on  for which  has not yet been added to .
  The associated \emph{value} is .
  All iterators initially point at the first element of . 
  Then the next element to append to  is always  corresponding to the iterator with minimum value. 
  Having processed this partial choice, we advance the pointer (or remove it, once it has already scanned the whole ).
  This process can be implemented using a binary heap  as a priority queue,
  so that initialization requires  time and outputting a single element takes  time.

  For all , let  be the prefix of  of length  and  be the prefix of  of length .
  A technical transformation of the procedure stated above
  leads to an online algorithm that constructs the prefixes  and .
  Along with each reported partial choice , the algorithm also computes .
  

  \begin{lemma}\label{lem:generate}
    After -time initialization, one can construct 
    online for , spending  time per each step.
    Symmetrically, one can construct  in the same time complexity.
  \end{lemma}
   \begin{proof}
      Our online algorithm is going to use the same approach as the offline computation of lists .
      The order of computations is going to be changed, though.
      
      At each step, for  to  we shall extend lists  with a single element 
      (unless the whole  has been already generated) from the top of the heap .
      Note that this way each iterator in  always points to an element 
      that is already in  or to the first element that has not been yet added to ,
      which is represented by the top of the heap . 
      
      We initialize the heaps as follows: we introduce  which represents the empty choice  with .
      Next, for  we build the heap  representing  iterators initially pointing to the top of .
      The initialization takes  time in total since a binary heap can be constructed in time linear in its size.
      
      At each step, the lists  are extended for consecutive values  from  to . 
      Since  is extended before , all iterators in  point to the elements of  while we compute .
      We take the top of  and move it to .
      Next, we advance the corresponding iterator and update its position in the heap .
      After this operation, the iterator might point to the top of .
      If  is empty, this means that the whole list  has already been generated and traversed by the iterator.
      In this case, we remove the iterator.
      
      It is not hard to see that this way we indeed simulate the previous offline solution.
      A single phase makes  operations on each heap .
      The running time is bounded by  at each step of the algorithm.
    \end{proof}

  
  
  The reduction of the following lemma is presented in \cref{ss}.
  Note that we may always assume that .
  Indeed, if we order the items  according to , 
  then only the first  of them might belong to a choice  with . 

\begin{lemma}\label{lem:knapred2}
    Given an instance  of the \MK problem, one can compute in  time
    an equivalent instance  with , ,
    , and .
    \end{lemma}

  \begin{theorem}\label{thm:knap}
    \MK can be solved in  time.
  \end{theorem}
  \begin{proof}
  Below, we give an algorithm in  time. 
  The final solution runs it in parallel on the original instance and on the instance with  and  swapped with  and~,
   waiting until at least one of them terminates.
    
    We increment an integer  starting from~, maintaining  and the lists  and  for , as long as  for some  (or until all the lists have been completely generated).
    By \cref{fct:bound}, we stop at .
    \cref{lem:knapred2} lets us assume that , so  
    the running time of this phase is  due to \cref{lem:generate}.
    
    
    Due to \cref{lem:decomp}, every feasible solution  admits a decomposition 
    with , , and  for some index . 
    We consider all possibilities for . For each of them we will reduce searching for  to an instance of the \MK
    problem with  and . By \cref{lem:knap2}, these instances can be solved in   time in total.
   
    The items of the -th instance are going to belong to classes  and ,
    where .
    The set  can be constructed by merging  sorted lists each of size ,
    i.e., in  time.
    Summing up over all indices , this gives  time.
       
    Clearly, each feasible solution of the constructed instances represents a feasible solution of the initial instance,
    and by \cref{lem:decomp}, every feasible solution of the initial instance has its counterpart in one of the constructed instances. 
  \end{proof}
 
  \subsection{Proof of Lemma~\ref{lem:knapred2}}\label{ss}
  Our reduction consists of two steps.   Its implementation uses the following notions:
  For each class , let . Also, let ; note that  is the smallest possible value  of a choice .
  We symmetrically define  and .
  First, we make sure that .
  \begin{lemma}\label{lem:knapred}
    Given an instance  of the \MK problem, one can compute in linear time an equivalent instance  with , , , , and .
  \end{lemma}
  \begin{proof}   
    Observe that if some class  contains a single item  for which both 
    and , then we can greedily include it in the solution .  
    Hence, we can remove such a class, setting  and .
    We execute this reduction rule exhaustively, which clearly takes  time in total
    and may only decrease the parameters  and .
    After the reduction, each class contains at least two items.
    
    
    We shall prove that now we can either find out that  or 
    that we are dealing with a NO-instance.
    To decide which case holds, let us define  as the difference between the second smallest
    value in the multiset  and .
    We set  as the sum of the  smallest values 
    for ; analogously we define .
   
    \begin{claim}
      If , then ;
      if , then ;
      otherwise, we are dealing with a NO-instance.
    \end{claim}
    \begin{proof}
      First, assume that .
      This means that there is a choice  with  containing at least  items  such that .
      \cref{fct:comb} yields  and consequently , as claimed.
      Symmetrically, if , then .
   
      Now, suppose that there is a feasible solution .
      As no class contains a single item minimizing both  and , there are at least 
      classes for which  contains an item not minimizing , or at least  classes
      for which  contains an item not minimizing . Without loss of generality, we assume that the former holds.
      Let  be the set of at least  classes  satisfying the condition. 
      If  does not minimize , then . 
      Consequently, . 
      However, observe that ,
      so , as claimed.
    \end{proof}
   
    The conditions from the claim can be verified in  time using a linear-time selection algorithm to compute  and .
    If any of the first two conditions holds, we return the instance obtained using our reduction.
    Otherwise, we output a dummy NO-instance.
  \end{proof}
  
    
    Before we proceed with the second reduction,   let us introduce an auxiliary notion.
    An item  is \emph{irrelevant} if there is another item  that \emph{dominates} , i.e., 
    such that  and .
    Removing irrelevant items leads to an equivalent instance of the \MK problem, and it may only decrease the parameters.
    
   \begin{lemma}\label{lem:redstep}
   Consider a class of items in an instance of the \MK problem.
   In linear time, we can remove some irrelevant items from the class so that the resulting class 
   satisfies  for each item .
   \end{lemma}
   \begin{proof}
   First, note that using a linear-time selection algorithm, we can determine for each item 
   whether  and whether . 
   If there is no item satisfying both conditions, we keep  unaltered.
   Otherwise, we have an item which dominates at least 
   other items. We scan through all items in  and remove those dominated by .
   Next, we repeat the algorithm.
   The running time of a single phase is clearly linear, and since  decreases geometrically,
   the total running time is also linear.
   \end{proof}    
   
   A straightforward way to decrease the number of classes is to replace two distinct classes , 
   with their Cartesian product , 
   assuming that the weight of a pair  is the sum of weights of  and . 
   This clearly leads to an equivalent instance of the \MK problem, does not alter the parameters , , and decreases . 
   On the other hand  and  may increase; the latter happens only if .
      
   These two reduction rules let us implement our reduction procedure which constitutes the proof of Lemma~\ref{lem:knapred2}.
    \begin{proof}
    First, we apply \cref{lem:knapred} to make sure that  and .
    We may now assume that , as otherwise we already have .
     
    Throughout the algorithm, whenever there are distinct classes of size at most , 
    we shall replace them with their Cartesian product.
    This may happen only  times, and a single execution takes  time,
    so the total running time needed for this part is .
    
    Furthermore, for every class that we get in the input instance or obtain as a Cartesian product,
    we apply \cref{lem:redstep}.  The total running time spent on this is also .
    
    Having exhaustively applied these reduction rules, we are guaranteed that
     for items  from all but one class. 
    Without loss of generality, we assume that the classes satisfying this condition are .
    
    Recall that  and  are defined as minimum values and weights of items in class 
    and that  and  are their sums over all classes.
    For , we define  as the difference between the 
    -th smallest value in the multiset   and .
    Next, we define  as the sum of the  smallest values .
    Symmetrically, we define  and .
    We shall prove a claim analogous to that in the proof of \cref{lem:knapred}.
    
    \begin{claim}
    If , then ; if , then ;  otherwise, we are dealing with a NO-instance. 
    \end{claim}
    \begin{proof}
    First, suppose that .
    This means that there is a choice  with  
    which contains at least  items  with . 
    By \cref{fct:comb}, the rank of this choice is at least , so , as claimed.
    The proof of the second case is analogous.
    
    Now, suppose that there is a feasible solution . For , we have 
    or . Consequently,  holds for at least 
    classes or  holds for at least  classes.
    Without loss of generality, we assume that the former holds. Let  be the set of (at least ) classes  satisfying the condition.
    For each , we clearly have , while for each , we have .
    Consequently, .
    Hence, , which concludes the proof.
    \end{proof}
       
    The condition from the claim can be verified using a linear-time selection algorithm: 
    first, we apply it for each class to compute  and , 
    and then, globally, to determine  and .
    If one of the first two conditions hold, we return the instance obtained through the reduction.
    It satisfies , i.e., .
    Otherwise, we construct a dummy NO-instance.
    \end{proof}

  
 

  \section{Weighted Consensus and General Weighted Pattern Matching}\label{sec:GWPMReduction}
    The \WC problem is formally defined as follows.
    \defdsproblem{\WC}{
      Two weighted sequences  and  of length  with at most  letters at each position and  in total,
      and a threshold probability \fr.
    }{
      A string  such that  and  or NONE if no such string exists.
    }

    If two weighted sequences satisfy the consensus, we write  and say that  \emph{matches} 
    with probability \fr.
    With this definition of a match, we extend the notion of an occurrence and the notation  to arbitrary weighted sequences.

    \defdsproblem{\GWPMFull (\GWPM)}{
      Two weighted sequences  and  of length  and , respectively, with at most  letters at each position
      and  in total, and a threshold probability \fr.
    }{
      The set .
    }

    In the case of the \GWPM problem, it is more useful to provide an \emph{oracle} that finds solid factors that correspond to particular occurrences of the pattern.
    Such an oracle, given , computes a string that matches both  and .




    We say that a string  is a \emph{maximal \fr-solid prefix} of a weighted sequence 
    if  is a \fr-solid prefix of  and no string , for , is a \fr-solid prefix of .
    Our algorithms rely on the following simple combinatorial observation, originally due to Amir et al.\ \cite{amir_weighted_property_matching_j}.

    \begin{fact}[\cite{amir_weighted_property_matching_j}]\label{fct:maxprefixes}
      A weighted sequence has at most  different maximal \fr-solid prefixes.
    \end{fact}


    The \WC problem is actually a special case of \MK.
    Namely, given an instance of the former, we can create an instance of the latter with  classes ,
    each containing an item  for every letter  which has non-zero probability at position  in both  and .
    We set  and  for this item,
    whereas the thresholds are . It is easy to see that this reduction indeed yields an equivalent instance
    and that it can be implemented in linear time.
    By \cref{fct:maxprefixes}, we have  for this instance, so \cref{thm:knap} yields the following result:
    \begin{corollary}\label{cor:red_simple}
    \WC problem can be solved in  time.
    \end{corollary}
    
    The \GWPM problem can be clearly reduced to  instances of \WC. 
    This leads to a naive -time algorithm.
    Below, we remove the first term in this complexity. 
    Our solution applies the approach used in \cref{sec:EWPM} for \WPM and uses an observation
    analogous to \cref{obs:crucial}.
    
    \begin{observation}\label{obs:crucial2}
      If  and  are weighted sequences that match with threshold ,
      then .
      Moreover there exists a consensus string  such that  unless .
    \end{observation}
 
    Our algorithm starts by computing  and  and the data structure for -queries in .
    We try to match  with every factor  of the text.
    Following \cref{obs:crucial2}, we check if 
    If not, then we know that no match is possible.
    Otherwise, let  be the set of positions of mismatches between  and .
    Assume that we store
     and 
    Then, in  time we can compute  and
    .
Now, we only need to check what happens at the positions in .

    If , then it suffices to check if  and .
    Otherwise, we construct two weighted sequences  and  by selecting only the positions from  in  and in .
    We multiply the probabilities of all letters at the first position in  by  and in  by .
    It is clear that  if and only if .
    
    Thus, we reduced the \GWPM problem to at most  instances of the \WC problem for strings of length . 
    By \cref{cor:red_simple}, solving such an instance takes  time.   
    Our reduction requires  time to preprocess the input (as in \cref{thm:wpm}),
    but this is dominated by the  total time of solving the \WC instances.
    If we memorize the solutions to all those instances
    together with the sets of mismatches  that lead to those instances, then we can also implement the oracle for the \GWPM problem with -time queries.
     In \cref{app:SDWC}, we design a tailor-made solution to replace the generic algorithm for the \MK problem,
     which lets us improve the  factor to . 
        
    The following reduction from \MK to \WC immediately yields that any significant improvement in the dependence
    on  and  in the running time of our algorithm
    would lead to breaking long-standing barriers for special cases of \MK.
   \begin{lemma}\label{lem:red}
    Given an instance  of the \MK problem with  classes of size , in linear time one can construct
    an equivalent instance of the \WC problem with  and sequences of length  over alphabet of size .
  \end{lemma}
    \begin{proof}
    We construct a pair of weighted sequences  of length 
    over alphabet . 
    Intuitively, choosing letter  at position  will correspond to taking the -th element of  to the solution ,
    which we denote as .
    Without loss of generality, we assume that weights and values are non-negative. Otherwise, we may subtract  from 
    and  from  for each item , as well  from  and  from .
    
    We set  to the smallest power of two such that .
    Let  and  for .
    For , we set:
    
    Clearly,  and .
    Moreover,
    we set 
    
    By the choice of , we have . 
    
    This way, for a string  of length , we have 
    
    and 
    
    
    Thus,  is a solution to the constructed instance of the \WC problem with two threshold probabilities,  and ,
    if and only if  is a solution to the underlying instance of the \MK problem.
    To have a single threshold , we append an additional position  with symbol 1 only, 
    with  and  provided that ,
    and symmetrically otherwise.
    
    If one wants to make sure that the probabilities at each position sum up to exactly one, two further letters can be introduced,
    one of which gathers the remaining probability in  and has probability 0 in , and the other gathers
    the remaining probability in , and has probability 0 in . 
   \end{proof} 

  \begin{theorem}\label{thm:lb}
  \WC problem is NP-hard and cannot be solved in:
  \begin{enumerate}
   \item  time unless the Exponential Time Hypothesis (ETH) fails;
   \item  time for some , unless there is an -time algorithm for the \SubsetSum problem;
    \item  time for some  and for , unless
    there is an -time algorithm for 3-\Sum.
    \end{enumerate}
  \end{theorem}
    \begin{proof}
    We use \cref{lem:red} to derive algorithms for the \MK problem based on hypothetical solutions for \WC.
    \SubsetSum is a special case of \MK with , i.e., . Hence,
    an -time solution for \WC would yield an -time algorithm for \SubsetSum,
    which contradicts ETH by the results of Etscheid et al.~\cite{DBLP:conf/mfcs/EtscheidKMR15} and Gurari~\cite{DBLP:books/daglib/0069796}.
    Similarly, an -time solution for \WC would yield an -time algorithm for \SubsetSum. 
    Moreover, -\Sum is a special case of \MK with , i.e., .
    Hence, an -time solution for \WC yields
    an -time algorithm for 3-\Sum.
    \end{proof}

    Nevertheless, it might still be possible to improve the dependence on  in the \GWPM problem.
    For example, one may hope to achieve  time for .






  
  







 
   
    
  
  \section{Faster \GWPM via Short Dissimilar Weighted Consensus}\label{app:SDWC}
  This section provides a faster solution for the \GWPMFull problem.
  The key ingredient is an improved solution for the following \SDWCFull problem:
     \defdsproblem{\SDWCFull (\SDWC)}{
      A  threshold probability \fr\ and two weighted sequences  and  of length  with at most  letters at each position
      and such that   and  are \emph{dissimilar}, i.e.,  for each position~.
    }{
      A string  such that  and  or NONE if no such string exists.
    }
    
    Note that the instances of the \WC problem produced by the reduction of \cref{sec:GWPMReduction} are actually instances of the \SDWC problem.
    Our tailor-made solution for the \SDWC problem works in  time.    
    It assumes that the letters at each position of the weighted sequences are sorted according to probabilities
    (in addition to storing the dictionary of letters and probabilities).
    This can be achieved in  time for each position.
    We have just proved:
    
   
    \begin{lemma}\label{lem:sdwc}
      The \GWPM problem and the computation of its oracle can be reduced
      in  time to at most  instances of the \SDWC problem.
    \end{lemma}     
   
   \subsection{Combinatorial Prerequisites}
    Our improvement upon the algorithm of \cref{thm:knap} is based on \cref{fct:maxprefixes}, 
    whose analogue does not hold for \MK in general.
    Technically, instead of the notion of maximal \fr-solid prefixes, the algorithm
    relies on \emph{light \fr-solid prefixes} defined as follows:
    We say that a string  of length  is a light \fr-solid prefix of a weighted sequence  if  or
     is a \fr-solid prefix of  such that . 
    We symmetrically define \emph{light \fr-solid suffixes} of .
    \cref{fct:maxprefixes} lets us bound the number of light solid prefixes.

    \begin{fact}\label{fct:lightprefixes}
      A weighted sequence has at most  different light \fr-solid prefixes.
    \end{fact}
    \begin{proof}
      We show a pair of inverse mappings between the set of maximal \fr-solid prefixes of a weighted sequence  and the
      set of light \fr-solid prefixes of .
      If  is a maximal \fr-solid prefix of , then we obtain a light \fr-solid prefix 
      by removing all trailing letters of  that are heavy letters at the corresponding positions in . 
      For the inverse mapping, we extend each light \fr-solid prefix by heavy letters as long as the prefix is \fr-solid.
    \mayqed\end{proof} 
   
    
    We use the notions of light \fr-solid prefixes and light \fr-solid suffixes to express a result 
    that we will use instead of \cref{lem:decomp,fct:bound}.
    
     \begin{lemma}\label{fct:key}
     Consider an instance of the \SDWC problem,
     and let  be real numbers such that . 
      Every consensus string 
      can be decomposed into  such that the following conditions hold for some :
      \begin{itemize}
        \item  is a light -solid prefix of ,
        \item  is a single letter,
        \item all characters of  are heavy in ,
        \item  is a light -solid suffix of .
        \end{itemize}
    \end{lemma}
      \begin{proof}
        We set  as the longest proper prefix of  which is a -solid prefix of both  and ,
        and we define .  Note that  is a light -solid prefix of  or , because  and 
        are dissimilar. If , we conclude the proof setting  and  to empty strings.
        
        Otherwise, we have  for  or .
        Since  and , this implies
        , i.e., that  is a -solid suffix of .
        We set  as the longest prefix of  composed of letters heavy in .
        This way  is clearly a light -solid suffix of .
      \end{proof}
      
      
      \subsection{Computing Light Solid Prefixes}
    
    We say that a string  is a common \fr-solid prefix (suffix) of  and 
    if it is a \fr-solid prefix (suffix) of both  and .
    A \emph{standard representation} of a common \fr-solid prefix  of length  of  and  is a triple 
    such that  and  are the probabilities  and .
    The string  is written using variable-length encoding so that a letter that occurs at
    a given position with probability  in  or  has a representation that consists of  bits. For every position , the encoding can be constructed as follows: we sort letters  according to  and assign subsequent integer identifiers according to this order.
     This lets us store a \fr-solid factor using  bits: 
     we concatenate the variable-length representations of its letters and we store a bit mask of size  
     that stores the delimiters between the representations of single letters.
    An analogous representation can be applied also to common \fr-solid suffixes.
    Our assumptions on the model of computations imply that the standard representation takes constant space.
    Moreover, constant time is sufficient to extend a common \fr-solid prefix by a given letter.


      The following observation describes longer light solid prefixes in terms of shorter ones.
      \begin{observation}\label{obs:light_step}
        Let  be a non-empty light \fr-solid prefix of .
        If one removes its last letter and then removes all the trailing letters which are heavy at the respective
        positions in , then a shorter light \fr-solid prefix of  is obtained.
      \end{observation}
	We build upon \cref{obs:light_step} to derive an efficient algorithm constructing light solid prefixes.\begin{lemma}\label{lem:lightprefixes_algo}
      Let  be an instance of the \SDWC problem and let .
      All common \fr-solid prefixes of  and  being light -solid prefixes of ,
      sorted first by their length and then by the probabilities in ,
      can be generated in  time.
    \end{lemma}
      \begin{proof}
        For , 
        let  be a list of the requested solid prefixes of length  sorted by their probabilities  in .
        \cref{fct:lightprefixes} guarantees that .
        
        We compute the lists  for subsequent lengths .
        We start with  containing the empty string with its probabilities .
        To compute  for , we use \cref{obs:light_step}.
        We consider all candidates  for the length of the shorter light -solid prefix,
        and then all letters  to put at position  of the new  light -solid prefix.
        
        For a given , we iterate over all elements  of  ordered by the non-increasing probabilities , 
        and try to extend each of them by the heavy letters in  at positions  and by the letter  at position .
        We process the letters  ordered by , ignoring the first one () and stopping as soon as we do not get a -solid prefix of . 
                
        More precisely, with , we compute
        
        check if  and , and, if so, insert
         at the beginning of a new list , indexed both by the letter 
        and by the length  of the shorter light -solid prefix.      
        When we encounter an element  of  and a letter  for which , we proceed to the next element of .
        If this happens for the heaviest letter , we stop considering the current list  and proceed to .
        The final step consists in merging all the  lists  in the order of probabilities in ;
        the result is .

        Let us analyse the time complexity of the -th step of the algorithm.
        If an element  and letter  that we consider satisfy , this accounts for a new light -solid prefix of .
        Hence, in total (over all steps) we consider  such elements.
        Note that some of these elements may be discarded due to the condition on .
        
        For each inspected element , we also consider at most one letter  for which  is not sufficiently large.
        If this is not the only letter considered for this element, such candidates can be charged to the previous class.
        The opposite situation may happen once for each list , which may give  additional operations in the -th step,
         in total.
        
        Thanks to the order in which the lists are considered, the products of probabilities
        ,  and factors  can be stored
        so that the representation of each subsequent light \frsq-solid prefix of length  is computed in  time.
        Finally, the merging step in the -th phase takes  time
        if a binary heap is used.

        The time complexity of the whole algorithm is .
        By the already mentioned \cref{fct:lightprefixes}, this is .
      \mayqed\end{proof}

    \subsection{Merge-in-the-Middle Implementation}
    In this section we apply \cref{fct:key} to solve the \SDWC problem.
    We use \cref{lem:lightprefixes_algo} to generate all candidates for  and ,
    and we apply a divide-and-conquer procedure to fill this with .
    Our procedure works for fixed ; the algorithm repeats it for all four choices.
    
      Let  denote a list of all common \fr-solid prefixes of  and  obtained by extending
      a light -solid prefix of  of length  by a single letter  at position ,
      and let  denote a list of all common -solid suffixes of  and  of length  that are light -solid suffixes of .  We assume that the lists  and  are sorted according to the probabilities in  and , respectively.


   \begin{lemma}\label{lem:L_R}
      The lists  and  for  can be computed in  time.
      Their total size is .
    \end{lemma}
      \begin{proof}
        -time computation of the lists  is directly due to \cref{lem:lightprefixes_algo}.
        As for the lists , we first compute in  time the lists of all light -solid prefixes of , sorted by the lengths of strings and then by the probabilities in , again using \cref{lem:lightprefixes_algo}.
        Then for each length  and for each letter  at the -th position, we extend all these prefixes by a single letter.
        This way we obtain  lists for a given  that can be merged according to the probabilities in  to form the list~.
        Generation of the auxiliary lists takes  time in total,
        and merging them using a binary heap takes  time.
        This way we obtain an -time algorithm.
      \mayqed\end{proof}

      Let  be a list of common \fr-solid prefixes of  and  of length 
      obtained by taking a common \fr-solid prefix from  for some 
      and extending it by  letters that are heavy at the respective positions in .
      Similarly,  is a list of common \fr-solid suffixes of length 
      obtained by taking a common \fr-solid suffix from  for some 
      and prepending it by  letters that are heavy in .
      Again, we assume that each of the lists  and  is sorted according to the probabilities in  and , respectively.
      
      A \emph{basic interval} is an interval  represented by its endpoints  such that
       and  for some integer  called the \emph{layer} of the interval.
      For every , there are  basic intervals and they are pairwise disjoint.
      
      \begin{example}
        For , the basic intervals are .
      \end{example}

      \begin{lemma}\label{lem:Ls_Rs}
        The lists  and  for all basic intervals  use  space
        and can be constructed in  time.
      \end{lemma}
      \begin{proof}
        We compute all the lists  and  for consecutive layers  of basic intervals .
        For , we have  and .
        Suppose that we wish to compute  for  at layer  (the computation of  is symmetric).
        Take .
        Let us iterate through all the elements  of the list , extend each string  by ,
        and multiply the probabilities  and  by
        
        respectively.
        If a common \fr-solid prefix is obtained, it is inserted at the end of an auxiliary list .
        The resulting list  is merged with  according to the probabilities in ; the result is .

        Thus, we can compute  in time proportional to the sum of lengths of  and .
        (Note that the necessary products of probabilities can be computed in  total time.)
        For every , the total length of the lists from the -th layer
        does not exceed the total length of the lists from the -th layer.
        By \cref{lem:L_R}, the lists at the -th layer have size .
        The conclusion follows from the fact that .
      \mayqed\end{proof}

 	  Next, we provide an analogue of \cref{lem:knap2}.
      \begin{lemma}\label{lem:meet}
        Let  and  be lists containing, for some , standard representations of common \fr-solid prefixes of length  and
        common \fr-solid suffixes of length  of  and .
        If the elements of each list are sorted according to non-decreasing probabilities in  or ,
        one can check in  time whether the concatenation of any \fr-solid prefix from 
        and \fr-solid suffix from  yields a string  such that  and .
      \end{lemma}
      \begin{proof}
        First, we filter out dominated elements of the lists, i.e., elements  such that there exists
        another element  with  and . 
        After this operation,  we make sure that both lists are sorted with respect to the non-decreasing probabilities in ;
        this might require reversing the list.
        
        
      
        For every element  of , we compute the first (leftmost) element  of 
        such that . This element maximizes  among all elements satisfying the latter condition.
        Hence, it suffices to check if , and if so, report the result .
        As the lists are ordered by  and , respectively, all such elements
        can be computed in  total time.
      \mayqed\end{proof}

      Finally, we are ready to apply a divide-and-conquer approach to the \SDWC problem:

      \begin{lemma}\label{lem:DWM_hard}
        The \SDWC problem can be solved in  time.
      \end{lemma}
      \begin{proof}
        The algorithm goes along \cref{fct:key}, considering all choices of  and .
        For each of them, we proceed as follows:
        
        First, we compute the lists ,  and ,  for all basic intervals.
        By \cref{lem:L_R,lem:Ls_Rs}, this takes  time.
        In order to find out if there is a feasible solution, it suffices to attempt joining
        a common \fr-solid prefix from  with a common \fr-solid suffix from  for some indices 
        by heavy letters of  at positions .

        We use a recursive routine to find such a pair of indices ,  in a basic interval 
        which has positive length and therefore can be decomposed into two basic subintervals  and .
        Then either , or both indices ,  belong to the same interval  or .
        To check the former case, we apply the algorithm of \cref{lem:meet} to  and .
        The two latter cases are solved by recursive calls for the subintervals.

        The recursive routine is called first for the basic interval .
        The computations performed by the routine for the basic intervals at the -th level
        take at most the time proportional to the total size of lists ,  at the -th level.
        \cref{lem:Ls_Rs} shows that the total size of the lists at all levels is .
        Consequently, the whole procedure works in  time.
      \mayqed\end{proof}
      \cref{lem:DWM_hard} combined with \cref{lem:sdwc} provides an efficient implementation of the \GWPMFull.

    \begin{theorem}\label{lem:gwpm}
    The \GWPM problem can be solved in  time.
     An oracle for the \GWPM problem using  space and supporting queries in  time can be
        computed within the same time complexity.
    \end{theorem}

 


    
    \section{Faster Algorithms for Large }\label{app:fast}
    In this section we analyse the running times of algorithms for the \MK problem expressed
    as   for some function  monotone with respect to both arguments.
    The algorithm of \cref{thm:knap} proves that achieving  is possible. 
    On the other hand, if we assume that \SubsetSum does not admit an -time solution,
    then we immediately get that we cannot have  for any .
    Similarly, the 3-\Sum conjecture implies that  is impossible.
    While this already refutes the possibility of having 
    across all arguments , such a bound may still hold for some special cases covering an infinite number of arguments.
    For example, we may potentially achieve  for .
    
    Before we prove that this is indeed possible, let us see the consequences of the hardness of 3-\Sum and, in general, -\Sum.   
    For a non-negative integer , the -\Sum conjecture refutes .
    By monotonicity of  with respect to the first argument, we conclude that 
    is impossible for . 
    On the other hand, monotonicity with respect to the second argument shows that
    
    is impossible for . The lower bounds following from -\Sum and -\Sum
    turn out to meet at ; see \cref{fig:graph}.
    
    
       \begin{figure}[hb]
   \begin{center}
    \begin{tikzpicture}
  \draw[->] (0.5,0.5) -- (7.5,0.5) node[right] {};
  \draw[->] (0.5,0.5) -- (0.5,4.5);
  \draw (1,1) -- (1.5, 1) -- (3,2) -- (10/3,2) -- (5,3) -- (5.25,3) -- (7,4);
  \draw[thin, dotted] (1,1) -- (7,4);
  \foreach \x in {1,...,7} {
  	\draw (\x,0.6) -- (\x, .4) node[below] {\tiny };
  }
  \foreach \x in {1,...,4} {
  	\draw (0.6,\x) -- (.4,\x) node[left] {\tiny };
  }
  
\end{tikzpicture}
\end{center}
\caption{Illustration of the upper bound (dotted) and lower bound (solid) on .}\label{fig:graph}
\end{figure}

  Consequently, we have some room between the lower and the upper bound of .
	In the aforementioned case of , the upper bound is ,
	compared to the lower bound of .
	Below, we show that the upper bound can be improved to meet the lower bound.
	More precisely, we show an algorithm whose running time is 
	for every positive integer .
	Note that , so for  the running time indeed matches the lower bounds up to the  term.
	
	
    Due to \cref{lem:knapred2}, the extra  term reduces to ,
    and if we measure the running time using  instead of , it becomes a constant ().
    In particular, this lets us prove that the \GWPM problem can be solved in  time
    for any integer , improving upon the solution of \cref{sec:GWPMReduction}
    unless  or  for an odd integer .
    

    \subsection{Algorithm for Multichoice Knapsack}\label{app:fastmk}    
	Let us start by discussing the bottleneck of the algorithm of \cref{thm:knap} for large .
	The problem is that the size of the classes does not let us partition every choice  into a prefix  and a suffix 
	with ranks both . \cref{lem:decomp} leaves us with an extra letter  between  and ,
	and in the algorithm we append it to the prefix (while generating ).

    We provide a workaround based on reordering of classes.
    Our goal is to make sure that items with large rank appear only in a few leftmost classes.
    For this, we guess the classes of the  items with largest rank (in a feasible solution) and move them to the front.
    Since this depends on the sought feasible solution, we shall actually verify all  possibilities.
    
    Now, our solution considers two cases:
    For , the reordering lets us assume , so we do not need
    to consider all items from .
    For , on the other hand, we exploit the fact that ,
    which at most .
         
    Combinatorial foundation of this intuition is formalized as a variant of \cref{lem:decomp}:
     
  \begin{lemma}\label{lem:decomp2}
  Let  and  be positive integers such that   for every .
  Let  and suppose that  is a choice with  such that  for .
    There is an index  and a decomposition 
  such that , , , and either  or .
  \end{lemma}
  \begin{proof}
  We claim that the decomposition constructed in the proof of \cref{lem:decomp} satisfies the extra 
  condition if . Let  and .
  Obviously  for  and, by the extra assumption,  for . Hence, \cref{fct:comb} yields . Simultaneously, we have ,
  so . Combining these inequalities, we immediately get the claimed bound.
  \end{proof}
    
   
   \begin{theorem}\label{thm:knap3}
    For every positive integer , the \MK problem can be solved in  time.
  \end{theorem}
  \begin{proof}
  As in the proof of \cref{thm:knap}, we actually provide an algorithm whose running time depends on  rather than .
  Moreover, \cref{lem:knapred2} lets us assume that .   
    
    We first guess the  positions where items with largest ranks  are present in the solution  and move these positions to the front. This gives  possible selections. For each of them, we proceed as follows.
    
    
    We increment an integer  starting from , maintaining  and all the lists  and  for ,
    as long as  for some .
    By \cref{fct:bound}, we stop with  and
    thus the total time of this phase is  due to the online procedure of \cref{lem:generate}.
    
    
    By \cref{lem:decomp2}, every feasible solution   admits a decomposition  for some ;
    we shall consider all possibilities for .
    For each of them, we reduce searching for  to an instance of the \MK  problem with  and . 
    By \cref{lem:knap2}, these instances can be solved in 
    time in total.
   
    For , the items of the -th instance are going to belong to classes  and ,
    where .
    The set  can be sorted by merging  sorted lists of size at most  each,
    i.e., in  time.
    On the other hand, for , we take 
     and .
    The former set can be constructed by merging  sorted lists of size  each,
    i.e., in  time. 
    
    Summing up over all indices , this gives  time
    for a single selection of the  positions with largest ranks,
    and  in total.
       
    Clearly, each solution of the constructed instances represents a solution of the initial instance,
    and by \cref{lem:decomp2}, every feasible solution of the initial instance has its counterpart in one of the constructed instances. 
  
      Before we conclude the proof, let us note that the optimal  does not need to be known in advance. 
    To deal with this issue, we try consecutive integers  and stop the procedure if
    \cref{fct:bound} yields that , i.e., if  is incremented beyond .
    If the same happens for the other instance of the algorithm (operating on  instead of ), we conclude that ,
    and thus we shall better use larger . 
    The running time until this point is  due to \cref{lem:generate}. 
    On the other hand, if , the algorithm behaves as if , i.e., runs in  time.
     This workaround (considering all smaller values ) adds extra 
    to the time complexity for the optimal value , which is less than the upper bound on the running time we have for this value .
      \end{proof}
    
  
    If we are to bound the complexity in terms of  only, the running time becomes
    
    Assumptions that  and  let us get rid of the  term, which can be bounded by .
    If one of these assumptions is not satisfied, we can improve the bound on the running time anyway:
    using \cref{thm:knap3} with increased  if , and using \cref{thm:knap} if .
    
    \begin{corollary}
    Let  be a positive integer such that . The \MK problem can be solved in  time.
    \end{corollary}
    
    This leads to the following results for weighted pattern matching:
    
   \begin{theorem}\label{thm:fast}
	Suppose that  for some positive integer .
	Then the \SDWC problem can be solved in  time,
	and the \GWPM problem can be solved in  time.
	\end{theorem}	
    
    As we noted at the beginning of this section, \cref{lem:red} implies that
    any improvement of the dependence of the running time on  or  by  (equivalently, by )
    would contradict the -\Sum conjecture.
	

  \section{Final Remarks}\label{sec:fr}
  In \cref{sec:MK}, we gave an -time algorithm for the \MK problem.
  Improvement of either exponent to  would result in a breakthrough for the \SubsetSum and 3-\Sum problems,
  respectively. 
  Nevertheless, this does not refute the existence of faster algorithms for some particular values 
  other than those emerging from instances of \SubsetSum or 3-\Sum.
  In \cref{app:fast}, we show an algorithm that is superior if  is a constant other than an odd integer.
  We also prove it to be optimal (up to lower order terms) for every constant 
  unless the -\Sum conjecture fails.

  \bibliographystyle{plain}
  \bibliography{wpm}

\end{document}
