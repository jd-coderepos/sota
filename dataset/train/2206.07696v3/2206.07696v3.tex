
\documentclass[10pt]{article} \usepackage[accepted]{tmlr}


\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[capitalise]{cleveref}
\usepackage[font=small]{caption}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{textcomp}

\usepackage{multirow}


\usepackage{mathtools}
\usepackage{enumitem}

\newcommand{\E}{\mathbb{E}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\given}{\,|\,}
\newcommand{\pdata}{p_{\mathrm{data}}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\textapproxx}{\raisebox{0.5ex}{\texttildelow}}

\title{Diffusion Models for Video Prediction and Infilling}



\author{\name Tobias H\"oppe \email tobihoeppe@gmail.com \\
      \addr KTH Stockholm
      \AND
      \name Arash Mehrjou\thanks{Equal advising.} \\
      \addr MPI for Intelligent Systems  \&  ETH Z\"urich
      \AND
      \name Stefan Bauer\footnotemark[1] \\
      \addr KTH Stockholm 
      \AND
      \name Didrik Nielsen\footnotemark[1] \\
      \addr Norwegian Computing Center 
      \AND
      \name Andrea Dittadi\footnotemark[1] \email adit@dtu.dk \\
      \addr  Technical University of Denmark \& MPI for Intelligent Systems 
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{11}  \def\year{2022} \def\openreview{\url{https://openreview.net/forum?id=lf0lr4AYM6}} 


\begin{document}


\maketitle

\begin{abstract}
Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain.
We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training.
By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at \url{https://sites.google.com/view/video-diffusion-prediction}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Videos contain rich information about the world, and a vast amount of diverse video data is available. 
Training models that understand this data can be crucial for developing agents that interact with the surrounding world effectively. 
In particular, video prediction plays an increasingly important role: autonomous driving \citep{hu_2020}, anticipating events \citep{zeng_2017}, planning \citep{finn_2017} and reinforcement learning \citep{pmlr-v97-hafner19a} are applications which can benefit from increasing performance of prediction models. 
On the other hand, video infilling---i.e., observing a part of a video and generating missing frames---can be used for example in planning, estimating trajectories, and video processing.
In addition, video models can be valuable for downstream tasks such as action recognition~\citep{kong_2018_survey} and pose estimation \citep{sahin_2020_review}. However, there has not been extensive research on video infilling and most research is focusing on generation or prediction.

Most recent approaches to video prediction are based on variational autoencoders \citep{babaeizadeh2021fitvid, saxena2021clockwork} or GANs \citep{clark_2019_dvd_gan, Luc_2020}.
Diffusion models \citep{pmlr-v37-sohl-dickstein15,ho_2020, pmlr-v139-nichol21a, song2021scorebased, abstreiter2021diffusion, mittal2022points,dockhorn2021score} have recently seen tremendous progress on static visual data, even outperforming GANs in image synthesis \citep{dhariwal2021diffusion}, but have not yet been extensively studied for videos.
Considering their impressive performance on images, it is reasonable to believe that diffusion models may also be useful for tasks in the video domain.


In this paper, we extend diffusion models to the video domain via several technical contributions. We use 3D convolutions and a new conditioning procedure incorporating randomness. Our model is not only able to predict future frames of a video but also fill in missing frames at arbitrary positions in the sequence (see \cref{fig:kinetics_infilling}). Therefore, our Random-Mask Video Diffusion (RaMViD) can be used for several video completion tasks. We summarize our technical contributions as follows:\footnote{Code is available at \url{https://github.com/Tobi-r9/RaMViD}.}
\begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt,parsep=\parskip]
    \item A novel diffusion-based architecture for video prediction and infilling. 
    \item Competitive performance with recent approaches across multiple datasets.
    \item Introduce a schedule for the random masking.
\end{itemize}

The remainder of this paper is organized as follows: In \cref{sec:related_work}, we provide the necessary background on diffusion models and video prediction and outline relevant related work. \cref{sec:masking} describes Random-Mask Video Diffusion (RaMViD). In \cref{sec:experiments}, we present and discuss extensive experiments on several benchmark datasets. We finally conclude with a discussion in \cref{sec:conclusion}. 

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{c|cccccc|cc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_6.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/upsample/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{The first two and last two frames of a video are given and our model does fill in the missing frames very accurate and with much detail.}
    \label{fig:kinetics_infilling}
\end{figure}


\section{Background and related work}
\label{sec:related_work}

\paragraph{Diffusion models.} Diffusion-based models generally refer to the class of machine learning algorithms that consist of gradually transforming a complex distribution into unstructured noise and learning to reverse this process to recover the data generating distribution. They have attracted a great deal of attention after being successfully applied to a diverse range of tasks such as image generation \citep{song2021scorebased, niu2020permutation}, audio \citep{chen2021wavegrad}, graph and shape generation \citep{cai2020learning}. The essence of these models is two stochastic (diffusion) processes implemented by Stochastic Differential Equations (SDEs), a forward and a backward one. We explain the formulation in the abstract domain here and specialize it later according to the application of this work. 

Let  be a sample from the empirical data distribution, i.e.,  and  be the data dimension. The forward diffusion process takes  as the starting point and creates the random trajectory  from  to the final time . The forward process is designed such that  has a simple unstructured distribution. One example of such SDEs is

where  is the Brownian motion. A desirable property of this process is the fact that the conditional distribution  takes a simple analytical form:

Upon learning the gradient of  for each , one can reverse the above process and obtain the complex data distribution from pure noise as

where  is a Brownian motion independent of the one in the forward direction. Hence, generating samples from the data distribution boils down to learning . 

The original score matching objective \citep{hyvarinen2005estimation}:

is the most intuitive way to learn the score function, but is unfortunately intractable. Denoising Score Matching (DSM) provides a tractable alternative objective function:

whose equivalence with the original score matching objective was shown by \citet{Vincent2011} and used to train energy models by \citet{saremi2018deep}. Similarly to many recent works, we use the DSM formulation of score matching to learn the score function.


\paragraph{Video prediction and infilling.} Research in video prediction has received more attention in the previous years, as the ability to predict videos can be used for several downstream tasks \citep{opera_2020_review}. Video prediction can be modeled in a deterministic or stochastic form. Deterministic modeling \citep{walker_2015, vondrick_2017, terwilliger_2019, sun_2019} tries to predict the most likely future, but this often leads to averaging the future states \citep{li_2019}. Due to the stochastic nature of the future, generative models have lately shown to be more successful in capturing the underlying dynamics. For this approach, variational models are often used by modeling the stochastic content in a latent variable \citep{babaeizadeh2018stochastic, saxena2021clockwork, pmlr-v80-denton18a, wu_2021_ghvae}. However, this often leads to blurry predictions due to underfitting, and \citet{babaeizadeh2021fitvid} have overcome this problem via architectural novelties. Blurry prediction is a less serious problem in GANs and promising results have been achieved especially on large datasets \citep{clark_2019_dvd_gan, Luc_2020}. 
On the other hand, the body of research on video infilling is significantly more scarce, with most works in this area focusing on frame interpolation \citep{jiang_2017_slomo}. However, \citet{xu_2018} have shown interesting results in infilling, by modeling the video as a stochastic generation process.

\paragraph{Concurrent work.}
\citet{yang_2022} is the only work so far that has used diffusion models for autoregressive video prediction, by modeling residuals for a predicted frame. However, since their evaluation procedure and datasets are different, a comparison with their work is not possible.   
A few concurrent works have recently considered diffusion models for video generation. \citet{ho_2022} focus on unconditional video generation, \citet{harvey_2022} use diffusion models to predict long videos, and \citet{voleti_2022}, the most similar to our work, also consider video prediction and infilling.

\section{Random-Mask Video Diffusion}
\label{sec:masking}
Our method, Random-Mask Video Diffusion (RaMViD), consists of two main features. First, the way we introduce conditional information is different from prior work. Second, by randomizing the mask, we can directly use the same approach for video prediction and video completion (infilling). In the following, we detail each of these aspects of RaMViD.

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{c|cccccccc}
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/uncond/frame_14.png} & \\
    
          &  &  &  &  &  &  &  \\
    \end{tabular}
    \caption{An unconditionally trained model is used to predict 15 frames given one frame. Even with re-sampling we can see, that objects in the background are not harmonized between the predicted and conditioned frames.}
    \label{fig:Bair_re}
\end{figure}

\subsection{Conditional training}
\label{sec:conditional_training}
Let  be a video with length . We partition the video  into two parts: the unknown frames  and the conditioning frames , where  and  are sets of indices such that  and . We write  with the following definition for :

where the superscript  indicates tensor indexing and in our case corresponds to selecting a frame from a video.
Here,  indicates the diffusion step, with  corresponding to the data and  to the prior Gaussian distribution.


If we use an unconditionally trained model, we find that the predicted unknown frames  do not harmonize well with the conditioning frames , as shown in \cref{fig:Bair_re}. One solution for this would be re-sampling, as proposed by \citet{Lugmayr_2022}. 
In re-sampling, we take one step in the learned reversed diffusion (denoising) process and then go back by taking a step in the forward diffusion process (i.e., adding noise again). This is repeated several times for each diffusion step, to make sure the predicted and conditioning frames are harmonized. However, this becomes computationally too expensive for videos, especially when using very few conditioning frames, as the number of re-sampling steps needs to be increased. To mitigate this issue, we propose to train the model conditionally with \emph{randomized masking}.

Conditional diffusion models usually optimize

where  is typically given as a separate input through an additional layer \citep{chen2021wavegrad} or it is concatenated with the input \citep{Saharia_2021, batzolis_2021, Saharia_2021_2}. On the other hand, we feed the entire sequence to the network  but only add noise to the unmasked frames: . The input to the network is then a video where some frames are noisy and some are clean:  (see \cref{fig:Bair_input}). The loss is computed only with respect to :

where  represents the output of the model with indices in . Note that the score function  has the same dimension as , whereas in \cref{eq:std_loss} it has the dimension of the entire video~. This leads to the forward diffusion process:

and the reversed diffusion process then becomes:

Similarly, \citet{tashiro2021csdi} compute the loss only on the unknown input. However, they also use concatenation and zero-padding to bring  and  to the same dimension. For a more detailed schematic, see \cref{sec:implementation details}. In our implementation, we used a discrete diffusion process with .

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{ccccccccc}
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/input/frame_14.png} & \\
    
          &  &  &  &  &  &  &  \\
    \end{tabular}
    \caption{Example input of the network with .}
    \label{fig:Bair_input}
\end{figure}


\subsection{Randomization}
As previously mentioned, the proposed model is able to perform several tasks. We achieve this by sampling  at random. At each training step, we first choose the number of conditioning frames , where  is a chosen hyperparameter. Then we define  by selecting  random indices from , and we refrain from applying the diffusion process to the corresponding frames. Since the videos now consist of original and noisy frames in varying positions, the model has to learn to distinguish between them in order to use the frames  as information for the reversed diffusion process.
After training, we can use RaMViD by fixing  to the set of indices of the known frames ( can be any arbitrary subset of ) and generating the unknown frames (those with indices in ).

Our approach allows us to use the exact same architecture of unconditionally trained models, thus enabling \textit{mixed training}, where we train the model conditionally and unconditionally at the same time. We set  (i.e., the model does not have any conditional information ) with probability , which is a fixed hyperparameter. If , our objective in \cref{eq:ramvid_objective} becomes the same as the objective in \cref{eq:dsm} used for unconditional training. The pseudocode for RaMViD is shown in \cref{alg:ramvid}.

\begin{algorithm}
\caption{RaMViD.}\label{alg:ramvid}
\begin{algorithmic}
\State Initialize model 
\State  Number of diffusion steps
\State  Max number of frames to condition on
\State  Length of the video
\While{not converged} 
\State  \State 
\State 
\If{}
\State 
\Else
\State 
\State  
\EndIf
\State 
\State 
\State 
\State Take a gradient step on:  
\EndWhile
\end{algorithmic}
\end{algorithm}



\section{Experiments}
\label{sec:experiments}

\subsection{Experimental setup}
\label{sec:exp_setup}

\paragraph{Implementation details.}
Our implementation relies on the official code of \citet{pmlr-v139-nichol21a},\footnote{\url{https://github.com/openai/improved-diffusion}} adapted to video data by using 3D convolutions. Even though most previous work uses the cosine noise schedule, we found that the linear noise schedule works better when training the model conditionally. Therefore, we use a linear diffusion schedule for our experiments. For the architecture, we also use the same as proposed by \citet{pmlr-v139-nichol21a}: a U-Net with self-attention at the resolutions 16 and 8. We do not encode the time dimension. We use two ResNet blocks per resolution for the BAIR dataset, and three blocks for Kinetics-600 and UCF-101. We set the learning rate for all our experiments to 2e-5, use a batch size of 32 for BAIR and 64 for Kinetics-600 and UCF-101, and fix . We found, especially on the more diverse datasets like Kinetics-600 and UCF-101, that larger batch sizes produce better results. Therefore, to increase the batch size, we use gradient accumulation by computing the gradients for micro-batches of size 2 and accumulate for several steps before doing back-propagation.

\paragraph{Datasets and evaluations.} 
To compare our model to prior work, we train it on the BAIR robot pushing dataset \citep{ebert_2017}. The dataset consists of short videos, with  resolution, of a robot arm manipulating different objects. For evaluation, we use the same setting as \citet{Rakhimov_2020}, which is to predict the next 15 frames given one observed frame. We train on videos of length 20.

Additionally, we evaluate our model on the Kinetics-600 dataset \citep{Carreira_2018}, which consists of roughly 500,000 10-second YouTube clips, also at 64  64 resolution, from 600 classes. The size and the diversity of this dataset make it a perfect task to investigate if the model captures the underlying real-world dynamics. For downloading and preprocessing we use the dataset's public repository.\footnote{\url{https://github.com/cvdfoundation/kinetics-dataset}} On Kinetics-600, we compare our model to concurrent work by predicting 11 frames when conditioned on 5 frames \citep{Luc_2020}. We additionally perform several ablation studies on video completion. We train on 16 frames and choose again .

To quantitatively evaluate the unconditional generation performance when using , we also train on UCF-101 \citep{soomro_2021_ucf}, a common benchmark for unconditional video generation. It consists of 13,320 videos from 101 human action classes. We also rescale this dataset to  and train with .  

To quantitatively evaluate prediction, we use the Fr\'echet Video Distance (FVD) \citep{Unterthiner_2018},\footnote{\url{https://github.com/google-research/google-research/tree/master/frechet_video_distance}} which captures semantic similarity and temporal coherence between videos by comparing statistics in the latent space of a Inflated 3D ConvNet (I3D) trained on Kinetics-400. 
To evaluate unconditional generation, we use the Inception Score (IS) \citep{salimans_improved_2016} to measure the quality and diversity of the generated videos. As we have to adapt the score to videos, we use the public repository from \citet{Saito_2020}.\footnote{\url{https://github.com/pfnet-research/tgan2}}


\subsection{BAIR}
We train four models on the BAIR dataset with  respectively. The models are trained for 250,000 iterations with a batch size of 32 on 8 GPUs. 

First, we test our method with the typical evaluation protocol for BAIR (predicting 15 frames, given one conditional frame). With all values of , we can achieve state-of-the-art performance, as shown in \cref{tab:bair}. By using , we can even increase the performance of our method. However, it seems that there is a tipping point after which the increasing unconditional rate hurts the prediction performance of the model. Interestingly, we find that also the model trained with  overcomes the harmonization problem described in \cref{sec:conditional_training}. We have trained more models with  but varying . When training with  we found a slight drop in performance but with  a slight increase.
Furthermore, we experiment with a task-specific model for prediction (i.e. setting  for all training steps), which we will call \emph{RaMViD fixed}. However, we find that this does not improve performance and does not appear to work reliably on other video completion tasks.

\begin{table}[h]
    \centering
    \caption{Prediction performance on BAIR. The values are taken from \citet{babaeizadeh2021fitvid} after inquiring about the evaluation procedure. Parameter counts were obtained either directly from the papers or by contacting the authors. Since our computational constraints did not allow us to do several runs for each method, we only give error bars for RaMViD ()}
    \begin{tabular}{ccc}
        \toprule
        \textbf{Method} &  \textbf{FVD} () & \textbf{\# parameters} \\
        \midrule
        Latent Video Transformer \citep{Rakhimov_2020} & 125.8 \\
        SAVP \citep{lee_2018_savp} & 116.4 \\
        DVD-GAN-FP \citep{clark_2019_dvd_gan} & 109.8 \\
        TrIVD-GAN-FP \citep{Luc_2020} & 103.3 \\
        VideoGPT \citep{yan_2021} & 103.3 & 40M \\
        Video Transfomer \citep{Weissenborn2020Scaling} & 94.0 & 373M \\
        FitVid \citep{babaeizadeh2021fitvid} & 93.6 & 302M \\
        MCVD \citep{voleti_2022}& 89.5 & 251.2M \\
        N\" UWA \citep{wu_2021_nuwa} & 86.9 \\
        \midrule 
        RaMViD () & 86.41 & 235M\\
        RaMViD () & 85.3  1.8 & 235M \\
        RaMViD () & 85.03 & 235M \\
        RaMViD () & 86.05 & 235M \\
        RaMViD () & 87.39 & 235M \\
        RaMViD () & \textbf{82.64} & 235M \\
        RaMViD (, \emph{fixed}) & 89.01 & 235M \\
        \bottomrule
    \end{tabular}
    
    \label{tab:bair}
\end{table}

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{ccc|cccccc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/surfing/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/gym/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Prediction of 11 frames given the first 5 frames on Kinetics-600 with RaMViD ().}
    \label{fig:kinetics_pre}
\end{figure}


Since we train with randomized masking, we can also perform video infilling with the same models, without retraining. We condition on the first and last frame (i.e., set  for sampling) and compute the FVD of the 14 generated frames. Again we find that the performance is very similar for different values of  (see \cref{tab:bair_infilling}), and only a slight change in results when using different values for .
Interestingly, RaMViD is also able to perform unconditional generation on BAIR for all considered values of , as shown in \cref{sec:extra_on_bair}.

\begin{table}
    \centering
    \caption{Infilling performance on BAIR.}
    \begin{tabular}{cc}
        \toprule
        \textbf{Method} &  \textbf{FVD} () \\
        \midrule
        RaMViD () & 85.68 \\
        RaMViD () & 85.02 \\
        RaMViD () & 87.04 \\
        RaMViD () & 87.85 \\
        RaMViD () & \textbf{83.83} \\
        RaMViD () & 87.11 \\
        RaMViD (, \emph{fixed}) & 119.76 \\
        \bottomrule
    \end{tabular}
    \label{tab:bair_infilling}
\end{table}

So far, we have shown that our method works very well for prediction and infilling. However, since the BAIR dataset is arguably rather simple and not very diverse, we will now evaluate RaMViD on the significantly more complex Kinetics-600 dataset. Since using  appears to lead to stable performance on all tasks we evaluated our model on, we will use  for our experiments on Kinetics-600 and UCF-101. 


\subsection{Kinetics-600}

For the Kinetics-600 dataset, we increase the batch size to 64 and train for 500,000 iterations on 8 GPUs. First, we evaluate the model on prediction with the setting described in \cref{sec:exp_setup} (predict 11 frames given 5 frames). When comparing our models to concurrent work, we find that RaMViD achieves state-of-the-art results by a significant margin (see \cref{tab:kinetics}). In \cref{fig:kinetics_pre}, we can see that the model produces temporally coherent outputs and is able to model details, especially in the background, such as clouds and patterns in the water. Nevertheless, it struggles with fast movements: objects moving quickly often get deformed, as can be observed in \cref{sec:kinetics-600_extra}. Similar to what we have seen in \cref{tab:bair}, having an unconditional rate  increases the performance up to a tipping point. However, differently from the model trained on BAIR, the FVD score now drops significantly with . We conjecture that this drop in performance is due to the complexity of the data distribution. In BAIR, the conditional and unconditional distributions are rather similar, while this is not true for Kinetics-600.

\begin{table}
    \centering
    \setlength{\tabcolsep}{6pt}
     \caption{Prediction performance on Kinetics-600. Values are taken from \citet{moing2021ccvs} after inquiring about the evaluation procedure. Parameter counts were obtained either directly from the papers or by contacting the authors. Since our computational constraints did not allow us to do several runs for each method, we only give error bars for RaMViD ()}
    \begin{tabular}{ccc}
        \toprule 
        \textbf{Method} &  \textbf{FVD} () & \textbf{\# parameters} \\
        \midrule

        Video Transfomer \citep{Weissenborn2020Scaling} & 170  5 & 373M \\
        DVD-GAN-FP \citep{clark_2019_dvd_gan} & 69  1 \\
        CCVS \citep{moing2021ccvs} & 55  1 & 366M \\
        TrIVD-GAN-FP \citep{Luc_2020} & 26  1 \\
        \midrule 
        RaMViD () & 18.69  & 308M \\ 
        RaMViD () & \textbf{17.53}  1.07  & 308M \\ 
        RaMViD () & 17.61 & 308M \\ 
        RaMViD () & 27.64 & 308M \\  
        \bottomrule
    \end{tabular}
    \label{tab:kinetics}
\end{table} 

We also evaluate RaMViD on two video completion tasks on Kinetics-600. The first task is to fill in a video given the two first and last frames (i.e., ): the challenge here is to harmonize the observed movement at the beginning with the movement observed at the end. 
In the second task, the conditioning frames are distributed evenly over the sequence (i.e., ), hence the model has to infer the movement from the static frames and harmonize them into one realistic video. RaMViD excels on both tasks, as shown quantitatively in \cref{tab:kinetics_completion} and qualitatively in \cref{fig:kinetics_infilling_2,fig:kinetics_completion}. Especially when setting  RaMViD is able to fill the missing frames with very high quality and coherence. 
This setting can be easily applied to upsampling by training a model on high-FPS videos and then sampling a sequence conditioned on a low-FPS video.

\begin{table}
    \centering
    \caption{Performance of RaMViD on Kinetics-600, when conditioning on different frames.}
    \begin{tabular}{ccc}
        \toprule
        \textbf{Method} &  \textbf{} &  \textbf{}\\
        \midrule
        RaMViD () & \textbf{10.68} & 6.28 \\ 
        RaMViD () & 10.85 & \textbf{4.91}  \\ 
        RaMViD () & 10.86 & 5.90  \\ 
        RaMViD () & 17.33 & 7.29 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:kinetics_completion}
\end{table}

We find that only RaMViD () and RaMViD () can generate unconditional videos on Kinetics-600. To quantify RaMViD's unconditional generation, we will evaluate these models on the UCF-101 dataset and compare it to other work. 


\subsection{UCF-101}
We train RaMViD on UCF-101 with the same setting as used for Kinetics-600 but for 450,000 iterations. \cref{tab:ucf_is} shows that our model achieves competitive performance on unconditional video generation, although it does not reach state-of-the-art. The trained models can successfully generate scenes with a static background and a human performing an action in the foreground, consistent with the training dataset (see \cref{fig:ucf_gen,fig:ucf_gen_2}). However, the actions are not always coherent and moving objects can deform over time. Note that UCF-101 is a very small dataset given its complexity. Therefore we do observe some overfitting. Since for each action we only have around 25 different settings, our model does not learn to combine those but generates very similar videos to the training set. Due to the characteristics of this dataset we think with more extensive hyperparameter tuning, one can achieve better results with RaMViD in unconditional generation. But our focus does not lie on this. 

\begin{table}
    \centering
    \caption{Generative performance of RaMViD on UCF-101. Note that the methods TGAN-F, VideoGPT and DVD-GAN in \cref{tab:ucf_is} are trained with 128  128 resolution, which gives them a slight advantage, as the IS score is computed with 112  112 resolution.}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Method} &  \textbf{IS} () & \textbf{\# parameters} & \textbf{resolution} \\
        \midrule
        VGAN \citep{vondrick_2016_vgan} & 8.31  0.09 & & 64  64 \\
        MoCoGAN \citep{Tulyakov_2018_CVPR} & 12.42  0.03 & 3.3M & 64  64\\
        TGAN-F \citep{KAHEMBWE2020506} & 13.62  0.06 & 17.5M & 64  64\\
        progressive VGAN \citep{acharya_2018_prgan} & 14.56  0.05 & & 64  64\\
        TGAN-F \citep{KAHEMBWE2020506} & 22.91  0.19 & 70M & 128  128\\
        VideoGPT \citep{yan_2021}& 24.69  0.3 & 200M & 128  128\\
        TGANv2 \citep{Saito_2020}& 26.60  0.47 & 200M & 64  64\\
        DVD-GAN \citep{clark_2019_dvd_gan} & \textbf{32.97}  1.7 & & 128  128\\
        \midrule 
        RaMViD () & 20.84  0.08 & 308M & 64  64 \\
        RaMViD () & 21.71  0.21 & 308M & 64  64\\
        \bottomrule
    \end{tabular}
    \label{tab:ucf_is}
\end{table}

\begin{figure}
\centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccccc}
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v5/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v6/frame_14.png} & \\
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Unconditional generation on the UCF-101 dataset. The first generation does not have much movement and is generated very realistically. In the second video, we the background is generated properly, but we see that the fast-moving people are unrealistically deformed.}
    \label{fig:ucf_gen}
\end{figure}


\subsection{Autoregressive sampling}
While we train our models only on 16 (Kinetics-600) or 20 (BAIR) frames, it is still possible to sample longer sequences autoregressively. By conditioning on the latest sampled frames, one can sample the next sequence and therefore generate arbitrarily long videos. In \cref{fig:kinetics_long}, we show examples of this autoregressive sampling with RaMViD () trained on Kinetics-600. However, we found that this is rather challenging because, at each autoregressive step, the quality of the generated sequence slightly deteriorates. This amplifies over time, often resulting in poor quality after about 30 frames.

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{c|ccccccccc}
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_0.png} & 
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_6.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_12.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_18.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_24.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_30.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_36.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_42.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v1/frame_48.png} &\\
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_0.png} & 
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_6.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_12.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_18.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_24.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_30.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_36.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_42.png} &
         \includegraphics[width=0.1\linewidth]{images/kinetics/long/v2/frame_48.png} &\\
         &  &  &  &  &  &  &  &  \\
    \end{tabular}
    \caption{Autoregressive prediction of 45 frames conditioned on 5 frames with RaMViD  trained on Kinetics-600.}
    \label{fig:kinetics_long}
\end{figure}



\section{Conclusion}
\label{sec:conclusion}
We have shown that diffusion models, which have been demonstrated to be remarkably powerful for image generation, can be extended to videos and used for several video completion tasks.
The way we introduce conditioning information is novel, simple, and does not require any major modification to the architecture of existing diffusion models, but it is nonetheless surprisingly effective. 
Although the proposed method targets conditional video generation, we also introduce an alternative masking schedule in an attempt to improve the unconditional generation performance without sacrificing performance on conditional generation tasks.

Since we have observed varying performance in different tasks using different masking schemes, an interesting direction for future research is to investigate which masking schedules are more suitable for each task.
It would also be interesting to explore in future work whether our conditioning technique is also effective for completion on other data domains. 
Finally, the focus of this work has been on the diffusion-based algorithm for videos rather than on optimizing the quality of each frame. It has been shown in concurrent works that including super-resolution modules helps create high-resolution videos. Adding a super-resolution module to RaMViD would be a relevant direction for future work.

\subsubsection*{Broader impact statement}
Generative models have been a cornerstone of AI research for many years. While some of these models have been used for the benefit of society e.g., in wildlife conservation, they can likewise be used with malicious intent such as for creating deepfakes. Similarly to other generative models, the capabilities of our approach for filling and predicting videos can be used for the benefit of all as well as unfortunately the opposite. In this work, we can not see any specific negative impact beyond the general possibility of malicious users of AI algorithms. In addition, our models require large video datasets for training. For some of these curated datasets, the underlying distribution of samples across various groups might not be uniform and before deploying any machine learning model trained on these datasets these aspects need to be carefully evaluated.  

\subsubsection*{Acknowledgments}
This project was enabled by the Berzelius cluster at the Swedish National Supercomputer Center (NSC).
We thank our anonymous reviewers for their constructive feedback.


\bibliography{main}
\bibliographystyle{tmlr}

\clearpage

\appendix
\section{Implementation details}
\label{sec:implementation details}

\cref{fig:model} presents a sketch of RaMViD's architecture. Thanks to the way we introduce conditioning frames, the architecture does not need to be different from the one in unconditional models.

\begin{figure}[bh]
    \centering
    \includegraphics[width=0.8\linewidth]{images/model/Network.drawio-cropped.pdf}
    \caption{Sketch of our method. In the last step we only compute the loss with respect to the frames that were corrupted with noise. The number of channels  is 128, and  is the video length.}
    \label{fig:model}
\end{figure}


As mentioned, we use the linear noise schedule and the score-based ("simple") objective for all experiments. All models are trained with 1000 diffusion steps, for sampling we used 750 steps on BAIR, and 500 on Kinetics-600 and UCF-101. 

As mentioned in \cref{sec:experiments}, we use the code base from \citet{pmlr-v139-nichol21a} (MIT license) and their proposed architecture, except that we use 3  3  3 convolution kernels.
In the encoder, we downsample only the spatial dimensions down to  in three steps. We use 128 channels for the first block and increase it by 128 for each downsampling step. As mentioned in \cref{sec:experiments}, we use multi-head self-attention at the resolutions 16 and 8, each with 4 attention heads. For sampling, we found it to be more beneficial to sample from the exponential moving average (EMA) of the weights \citep{pmlr-v139-nichol21a}. We set the EMA rate to .


\section{Additional results}

\subsection{Results on BAIR}
\label{sec:extra_on_bair}
\cref{fig:bair_gen} shows that all of the models are also able to do unconditional video generation (even RaMViD (), we assume that this is due to the low diversity of the dataset). Qualitatively, we can see in \cref{fig:bair_gen} that videos generated by models with higher  are better in generating details. While all models can generate the moving robot arm, only the models with  can properly generate the different objects in the box. However, we have no quantitative results on unconditional generation on BAIR.

\begin{figure}
\centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccccc}
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/0/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/25/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/50/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/generation_v2/75/frame_14.png} & \\
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Unconditional generation on the BAIR dataset sampled from RaMViD for  (first row) until  (last row). Due to the low complexity of the dataset, we can generate reasonable unconditional videos even with . However, the quality of details increases with increasing .}
    \label{fig:bair_gen}
\end{figure}

\subsection{Results on Kinetics-600}
\label{sec:kinetics-600_extra}
Kinetics-600 in practice appears to be the most difficult dataset among those considered here. While our results are state-of-the-art (see \cref{tab:kinetics}), we do observe failure cases. One of the most common failure cases is fast movement. In that case we often see a deformation of the moving object (see \cref{fig:kinetics_fail}). 

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{ccc|cccccc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/jump/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/prediction/karate/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Prediction of 11 frames given the first 5 frames on Kinetics-600 when conditioning on fast-moving objects. We can see that the background does get preserved well, while the object itself gets unrealistically deformed.}
    \label{fig:kinetics_fail}
\end{figure}

Fast camera movement can also be a problem for infilling. If an object is placed very different between the first and last frames, the model does not generate a harmonized movement but makes the object disappear in the first and appear in the last frames \cref{fig:kinetics_infilling_fail}. 

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{c|cccccc|cc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/fail/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Infilling on Kinetics. The conditioned frames are . The people in frames 0 and 1 are placed quite differently than in 14 and 15. The model is not able to generate the necessary camera movement and does simply interpolate between the frames.}
    \label{fig:kinetics_infilling_fail}
\end{figure}

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{c|cccccc|cc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_6.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v6/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_6.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v5/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_6.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v4/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_6.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/filling/v1/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Video infilling  on Kinetics-600 with RaMViD (.}
    \label{fig:kinetics_infilling_2}
\end{figure}

\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{cc|c|cc|c|ccc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_1.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_3.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_5.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_7.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_9.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v1/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_1.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_3.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_5.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_7.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_9.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v2/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_1.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_3.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_5.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_7.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_9.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v3/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_1.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_3.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_5.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_7.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_9.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/completion/v4/frame_14.png} & \\
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Video completion  on Kinetics-600 with RaMViD (.}
    \label{fig:kinetics_completion}
\end{figure}


\begin{figure}
\centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccccc}
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v8/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v7/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v4/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/ucf/v9/frame_14.png} & \\
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Qualitative results of unconditional generation on UCF-101. Scenes with less movement are generated well, but are often close to the training set.}
    \label{fig:ucf_gen_2}
\end{figure}

\section{Qualitative comparison}
The main qualitative improvement of RaMViD compared to other methods is the decrease in occurence of deformed objects in the predictions on Kinetics-600 (see \cref{fig:ccvs}). While the predictions of several other methods suffer from object deformations, RaMViD only suffers from this with fast-moving objects. On BAIR, on the other hand, our qualitative results are similar to the ones of recent methods, although RaMViD can predict details in the interactions in more detail (see \cref{fig:bair_gpt}).
\begin{figure}
\centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccccc}
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/95/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/95/frame_14.png} & \\
         & & & & & & & & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/11/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/11/frame_14.png} & \\
          & & & & & & & & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/ramvid/247/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/Bair/comparison/gpt/247/frame_14.png} & \\
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{A comparison between RaMViD () and VideoGPT \citep{yan_2021}. The first row shows our predictions and the second VideoGPT respectively. Visually, the difference is not significant, but we found that VideoGPT produces artefacts, especially when there are interacting objects.}
    \label{fig:bair_gpt}
\end{figure}


\begin{figure}
    \setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{ccc|cccccc}
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/101/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/101/frame_14.png} & \\
         & & & & & & & & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/103/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/103/frame_14.png} & \\
         & & & & & & & & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ramvid/8/frame_14.png} & \\
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_0.png} & 
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_2.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_4.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_6.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_8.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_10.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_12.png} &
         \includegraphics[width=0.11\linewidth]{images/kinetics/comparison/ccvs/8/frame_14.png} & \\
    
          &  &  &  &  &  &  & 
    \end{tabular}
    \caption{Predicted videos on Kinetics-600 from RaMViD () in the top row, and CCVS in the bottom row. We found that our model is better in modelling motion, whereas for more static videos, the difference is visually not significant. Nevertheless, we also found that both models perform equally bad for fast object/camera movement.}
    \label{fig:ccvs}
\end{figure}

\section{Compute}
\label{sec:compute}
Each model is trained on 8 NVIDIA A100 GPUs with 40 GB of memory. The models on BAIR are trained with a batch size of 32 and a micro-batch size of 16 for 250k iterations (\textapproxx 3 days). All other models on Kinetics-600 and UCF-101 are trained with a batch size of 64 and micro-batch size of 16. The models are trained for 500k iterations on Kinetics-600 (\textapproxx 10 days) and for 450k iterations on UCF-101 (\textapproxx 9 days).



\section{Datasets}
\label{sec:datasets}
The videos in all datasets have more frames than we train on. Therefore we choose random sub-sequences of the desired length during training.

\paragraph{BAIR robot pushing.} The BAIR robot pushing dataset can be used under an MIT license. We use the low resolution dataset (64  64). Since the data is already in the correct size, no prepossessing is necessary. For evaluation we predict one sequence for each of the 256 test videos and compare the FVD to the ground truth. To get a proper evaluation score, we do this 100 times and the final FVD score is the average over all 100 runs. We train on a sequence length of 20. 

\paragraph{Kinetics-600.} The Kinetics-600 dataset has a Creative Commons Attribution 4.0 International License. The videos have different resolutions, which is why we reshape and center crop them to a 64  64 resolution. For evaluation we take 50,000 videos from the test set and predict a sequence for each of the videos. We then compute the statistics for the ground truth and the predicted videos to obtain the FVD score. We train on a sequence length of 16.

\paragraph{UCF-101.} We could not find a license for the UCF-101 dataset. The original frames have a resolution of 160  120, therefore we resize and center crop the videos to a 64  64 resolution. We train on the entire dataset of 13,320 videos. To evaluate the generative performance, we sample 10000 videos unconditionally and compute the Inception Score (IS).\footnote{\url{https://github.com/pfnet-research/tgan2}} This is repeated three times.

\section{Sampling Speed}
 Since sampling 10,000 videos takes about 9 hours with the large model trained on Kinetics-600 and UCF-101, and 7 hours with the smaller model trained on BAIR using 500 sampling steps, it is crucial to know how many sampling steps are necessary to achieve satisfactory performance. We found that by using only 250 sampling steps for RaMViD (p = 0.25) trained on BAIR, our results drop significantly (to 101.54). However, when using 500 sampling steps, we achieve an FVD of 85.07 which is very similar to using 750 steps (84.20). We observe the same behaviour on Kinetics-600, where we achieve an FVD of 14 / 16 with 750 / 500 sampling steps but only an FVD of 49 with 250 steps. Therefore, when using the DDPM sampler with RaMViD, we recommend using a minimum of 500 sampling steps for video generation.

\section{Concurrent work}
As mentioned in \cref{sec:related_work}, three concurrent works on diffusion models for videos were recently made public. Only \citet{ho_2022} and \citet{voleti_2022} consider similar tasks as we do. \citet{ho_2022} appears to outperform RaMViD on unconditional video generation on UCF-101, which is not surprising, as we train with the mixed method and therefore the models are mostly trained for conditional generation. \citet{voleti_2022} evaluate their method on BAIR with the same procedure we used, and the results reported in their publication suggest that RaMViD outperforms their proposed method.
\end{document}
