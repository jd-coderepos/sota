\documentclass{article}



\usepackage[preprint]{neurips_2020}






\usepackage{amsmath,amssymb} \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hidelinks]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{array}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\title{Noisy Differentiable Architecture Search}
\newcommand{\citec}[1]{(\cite{#1})}


\author{
Xiangxiang Chu \\
Xiaomi AI Lab \\
\texttt{chuxiangxiang@xiaomi.com}\\
\And
Bo Zhang  \\
Xiaomi AI Lab \\
\texttt{zhangbo11@xiaomi.com}\\
\And
Xudong Li \thanks{Work done as an intern. Equal Contribution.} \\
University of Chinese Academy of Sciences\\
\texttt{lixudong16@mails.ucas.edu.cn}
}

\begin{document}

\maketitle

\begin{abstract}
  \emph{Simplicity is the ultimate sophistication}. Differentiable Architecture Search (DARTS) has now become one of the mainstream paradigms of neural architecture search. However, it largely suffers from several disturbing factors of the optimization process whose results are unstable to reproduce. \cite{chu2019fair} point out that skip connections natively have an unfair advantage in exclusive competition which primarily leads to dramatic performance collapse. While FairDARTS \citec{chu2019fair} turns the unfair competition into a collaborative one, we instead impede such unfair advantage by injecting unbiased random noise into skip operations' output. In effect, the optimizer should perceive this difficulty at each training step and refrain from overshooting on skip connections, but in a long run, it still converges to the right solution area since no bias is added to the gradient in terms of expectation. We name this novel approach as NoisyDARTS.
Our experiments on CIFAR-10 and ImageNet attest that it can effectively break the skip connections' unfair advantage and yield better performance. It generates a series of models that achieve state-of-the-art results on both datasets. Code will be made available here\footnote{\url{https://github.com/xiaomi-automl/NoisyDARTS}}.
\end{abstract}

\section{Introduction}
Performance collapse from an excessive number of skip connections in the inferred model is a  fatal drawback for the differentiable architecture search approaches \citec{liu2018darts,chen2019progressive,zela2020understanding,chu2019fair}. Quite an amount of previous research has focused on addressing this issue. FairDARTS \citec{chu2019fair} concludes the cause of this collapse to be an unfair advantage in an exclusively competitive environment. Under this perspective, they summarize several current effective approaches as different ways of avoiding the unfair advantage.  Inspired by their empirical observations, we adopt quite a different and straightforward approach by injecting unbiased noise to skip connections' output. The underlying philosophy is simple:  the injected noise would bring perturbations into the gradient flow via skip connections so that its unfair advantage doesn't comfortably take effect.


Our contributions can be summarized in the following aspects.

\begin{itemize}
	\item We propose a simple but effective approach to address the performance collapse issue in the differentiable architecture search: injecting noise into the gradient flow of skip connections.
	\item We dive into the requirements of the desired noise, which should be of small variance as well as unbiased for the gradient flow in terms of expectation. Particularly, we found that Gaussian noise with zero mean and small variance is a handy solution.
	\item Our research corroborates the root cause analysis of performance collapse in FairDARTS \citec{chu2019fair} since suppressing the unfairness from skip connections does generate substantial results.
	
\item We performed extensive experiments on two widely used search spaces and two standard datasets to verify the effectiveness and robustness of the proposed approach. With efficient search, we achieve state-of-the-art results on both CIFAR-10 (\textbf{97.61\%}) and ImageNet (\textbf{77.9\%}). Good transferability is also verified on object detection tasks. 
\end{itemize}

\section{Related Work}

\textbf{Performance collapse in DARTS.}
The notorious performance collapse of DARTS is unanimously confirmed by many \citec{chen2019progressive,zela2020understanding,chu2019fair}. To remedy this failure, \cite{chen2019progressive} carefully set a hard constraint to limit the number of skip connections. This is a strong prior since architectures within this regularized search space generally perform well, as indicated by \cite{chu2019fair}. Meantime, \cite{zela2020understanding} devised several search spaces to prove that DARTS leads to degenerate models where skip connections are dominant. To robustify the searching process, they proposed to monitor the sharpness of validation loss curvature, which correlates with the induced model's performance. This, however, adds too much extra computation, as it needs to calculate the eigenspectrum of the Hessian matrix. \cite{chu2019fair} instead relaxes the search space to avoid exclusive competition. They allow each operation to have independent architecture weight by using \emph{sigmoid} to mimic a multi-hot encoding other than the original \emph{softmax} for a one-hot encoding. This modification enlarges the search space as it allows multiple choices between every two nodes, whereas the intrinsic collapse in the original DARTS search space still calls for a better solution.

\textbf{Noise tricks.}
Noise injection is a common and effective trick in many deep learning scenarios. \citet{vincent2008extracting} add noises into auto-encoders to extract robust features.  \citet{fortunato2018noisy} propose NoisyNet which utilizes random noise to perform exploration in reinforcement learning. \citet{chen2015net2net} make use of noises to handle network expansion on knowledge transfer tasks. Meanwhile, 
Injecting noise to gradients has been proved to greatly facilitate the training of deep neural networks \citec{neelakantan2015adding,zhang2019calibrated}. Most recently, a noisy student is also devised to progressively manipulate unlabelled data \citec{xie2019self}.



\section{Noisy DARTS}
\subsection{Motivation}
FairDARTS \citec{chu2019fair} proves that skip connections usually overperform during searching under the competitive environment, resulting in a degenerate model where they contribute less. In view of this, \cite{chu2019fair} convert the competition into collaboration by giving each operation an independent coefficient (applying \emph{sigmoid} instead of \emph{softmax} over architectural parameter ) so that each  obtains gain not at the cost of suppressing others, but profiting from the collaborative work according to its proper share. Enlightened by their perspective, we propose a more natural approach to suppress skip connection from overshooting. Straightforwardly, injecting noise into the skip connection operations would simply do. The added noise will play a role of troubling the overwhelming gradient flow via skip connections so that the unfair advantage is directly weakened. The remaining problem, however, is to study what type of noise will be more effective.
\subsection{Requirements for the Injected Noise}\label{theory}

We let  be the noise injected into the skip operation, and  be the corresponding architectural weight. The loss of skip operation can then be written as,

where  is the validation loss function and  is the softmax operation for . 
Approximately, when the noise is much smaller than the output features, we also have

In the noisy scenario, the gradient of the architectural parameters via the skip connection operation becomes,



As random noise  brings uncertainty to the gradient update, skip connections have to overcome this difficulty in order to win over other operations. Their unfair advantage is then much weakened. However, not all types of noise are equally effective in this regard. A basic requirement is that, while assuring  suppression on the unfair advantage, it shouldn't bring in any bias to the gradient in terms of its expectation. 
Formally,  the expectation of the gradient can be written as,

Based on the premise stated in Equation~\ref{eq:approximate}, we take  out of the expectation in Equation~\ref{eq:gradient} to make an approximation. As there is still an extra  in the gradient of skip connection, to keep the gradient unbiased,  must be 0. It's thus natural to introduce small and unbiased noise, i.e. with \textbf{zero mean and small variance}. For simplicity, we just use Gaussian noise and other options should work as well.




\subsection{Stepping out of the Performance Collapse by Noise}
Based on the above analysis, we propose NoisyDARTS to step out of the performance collapse of DARTS. In practice, we inject Gaussian noise  into skip connections to weaken the unfair advantage. 

Formally, the edge  from node  to  in each cell operates on th input feature  and its output is denoted as . The intermediate node  gathers all inputs from the incoming edges:

Let  be the set of  candidate operations on edge . Specially, let  be the skip connection . NoisyDARTS injects the additive noise  into skip operation  to get a mixed output,


To ensure the gradient of a skip connection is unbiased and the noise  is small enough, we set  and , where  is a positive coefficient. That is to say, the standard deviation of the noise changes accordingly with a mini-batch of samples , i.e.,  times that of . Setting a low , we meet  as required by Equation~\ref{eq:approximate}. 

Compared with DARTS, the only modification is  injecting the noise into the skip connections. The architecture search problem remains the same as the original DARTS, which is to interleavely learn  and network weights  that minimize the validation loss , as shown in Equation~\ref*{minimize}. 


The modified optimization process codenamed NoisyDARTS is shown in Algorithm~\ref{alg:noisy-darts}. 

\begin{algorithm}[th]
	\caption{NoisyDARTS-Noisy Differentiable Architecture Search}
	\label{alg:noisy-darts}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} Architecture parameters , network weights  , noise control parameter , .
		\WHILE{\emph{not reach }}
		\STATE 		 Inject random Gaussian noise  into the skip connections' output.
		\STATE Update weights  by  
		\STATE Update architecture parameters   by  
		\ENDWHILE
		\STATE Derive the final architecture according to .
	\end{algorithmic}
\end{algorithm}



\section{Experiments}

\subsection{Search Space}
To verify the validity of our method, we adopt two widely used search spaces: the DARTS search space \cite{liu2018darts} and MobileNetV2's search space as in \cite{cai2018proxylessnas}. The former consists of a stack of duplicate normal cells and reduction cells, which are represented by a DAG of 7 nodes with each edge among intermediate nodes having 7 possible operations (max pooling, average pooling, skip connection, separable convolution 33 and 55, dilation convolution 33 and 55). The latter comprises 19 layers and each contains 7 choices: inverted bottleneck blocks denoted as Ex\_Ky (expansion rate , kernel size ) and a skip connection. 

\subsection{Searching on CIFAR-10}
In the search phase, we use similar hyperparameters and tricks as \cite{liu2018darts}. All experiments are done on a Tesla V100 with PyTorch 1.0 \citec{paszke2019pytorch}. The search phase takes about 7 GPU hours, which is less than the previously reported cost (12 GPU hours) due to a better implementation. We only use the first-order approach for optimization since it is more effective. The best models are selected under the noise with a zero mean and . An example of the evolution of the architectural weights during the search phase is exhibited in Figure~\ref{fig:alpha-evolution-best}.

\begin{figure}[ht]
       \centering
       \subfigure[Normal cell]{
               \includegraphics[width=0.98\textwidth]{v2_8_10-normal-alpha.pdf}
        }
       \subfigure[Reduction cell]{
               \includegraphics[width=0.98\textwidth]{v2_8_10-reduce-alpha.pdf}
       }
       \caption{Evolution of architectural weights during the NoisyDARTS searching phase on CIFAR-10. Skip connections in normal cells are largely suppressed.}
       \label{fig:alpha-evolution-best}
\end{figure}

For training a single model, we use the same strategy and data processing tricks as \cite{chen2019progressive,liu2018darts}, and it takes about 16 GPU hours. 
The results are shown in Table~\ref{tab:comparison-cifar10}. The best NoisyDARTS model achieves a new state-of-the-art result of 97.61\% with only 534M FLOPS and 3.25M parameters. The searched cells are shown in Figure~\ref{fig:the-searched-cells-a} and Table~\ref{table:genotypes} (supplementary). It's interesting to see that this model chooses as many as 4 skip connections for reduction cells, it still obtains highly competitive result. However, we can attribute it to the elimination of the unfair advantage \citec{chu2019fair}. In other words, the unfair advantage is suppressed to a level that the activated skip connections indeed contribute to the performance of the selected model.


\begin{table}
	\begin{center}
		\caption{Results on CIFAR-10. : MultAdds computed using the genotypes provided by the authors. : Averaged on training the best model for several times. GD: Gradient-based, TF: Transferred from ImageNet.} 
		\label{tab:comparison-cifar10}
		\begin{footnotesize}
			\begin{tabular}{*{6}{l}} 			
				\toprule
				Models  & Params (M) &  (M) & Top-1 (\%) & Type  \\
\midrule
				NASNet-A (\cite{zoph2017learning})  & 3.3 & 608  &  97.35 & RL \\
				ENAS (\cite{pham2018efficient}) & 4.6 & 626 & 97.11 & RL     \\	
				MdeNAS (\cite{zheng2019multinomial}) & 3.6 & 599 & 97.45 & MDL \\
				\midrule
				DARTS(first order)(\cite{liu2018darts}) & 3.3 & 528 & 97.00 & GD \\ 
SNAS( \cite{xie2018snas})  & 2.8 & 422 & 97.15 & GD\\
				GDAS (\cite{dong2019searching}) & 3.37 & 519 & 97.07 &GD \\
				SGAS (Cri.2 avg.) (\cite{li2019sgas}) & 3.9 & - & 97.33 & GD\\
				P-DARTS (\cite{chen2019progressive}) & 3.4 & 532 & 97.5 & GD \\ 
				PC-DARTS (\cite{xu2020pcdarts}) & 3.6 & 558 & 97.43 & GD \\ 
				RDARTS (\cite{zela2020understanding}) &-& -& 97.05 & GD \\
				FairDARTS (\cite{chu2019fair}) & 3.320.46  & 45861 & 97.46 & GD \\
NoisyDARTS-a (Ours) & 3.25 & 534 & 97.61 & GD \\
				NoisyDARTS-b (Ours) & 3.01 & 494 & 97.53 & GD \\
				NoisyDARTS-A-t (Ours) & 4.3 & 447 & 98.28 & TF \\
				\bottomrule
			\end{tabular}
		\end{footnotesize}
	\end{center}
\end{table}

\begin{figure}[t]
	\centering
	\subfigure[Normal cell]{
		\includegraphics[width=0.48\textwidth,scale=0.8]{v2_8_10_normal.pdf}
	}
	\subfigure[Reduction cell]{
		\includegraphics[width=0.48\textwidth,scale=0.8]{v2_8_10_reduction.pdf}
	}
	\caption{NoisyDARTS-a cells searched on CIFAR-10.}
	\label{fig:the-searched-cells-a}
\end{figure} 
\begin{figure}[ht]
	\centering
	\subfigure[Normal cell]{
		\includegraphics[width=0.55\textwidth,scale=0.8]{v2_8_7_normal.pdf}
	}
	\subfigure[Reduction cell]{
		\includegraphics[width=0.35\textwidth,scale=0.8]{v2_8_7_reduction.pdf}
	}
	\caption{NoisyDARTS-b cells searched on CIFAR-10 with .}
	\label{fig:the-searched-cells-b}
\end{figure}




\subsection{Searching Proxylessly on ImageNet}

\begin{figure}[ht]
	\centering		\includegraphics[width=0.7\textwidth,scale=0.8]{darts-vs-noisy-darts-imagenet-dominant-ops.pdf}
	\caption{Stacked plot of dominant operations during searching on ImageNet. The inferred model of DARTS (left) obtains 66.4\% top-1 accuracy on ImageNet validation dataset. The inferred model of NoisyDARTS (right) obtains 76.1\% top-1 accuracy. }
	\label{fig:stacked-plot-dominant-imagent}
\end{figure}

For ImageNet experiments, we use exactly the same space as \cite{cai2018proxylessnas,chu2019scarletnas}.
In the search phase, the Gaussian noise with zero mean and a variance of 0.2 is injected to the skip connection operations. We split the original training set into two datasets with equal capacity to act as our training and validation dataset. The original validation set is treated as the test set. We use the SGD optimizer with a batch size of 768. The learning rate for the network weights is initialized as 0.045 and it decays to 0 within 30 epochs following the cosine decay strategy.  Besides, we utilize Adam optimizer () and a constant learning rate of 0.001. This stage takes about 12 GPU days on Tesla V100 machines. In the training phase for inferred standalone models, we use similar training tricks as EfficientNet \citec{tan2019efficientnet}. Similar to \cite{liu2018darts,liu2018progressive}, the objective of this experiment is to find the best model without considering other constraints on FLOPS or the number of parameters.  The evolution of dominating operations during the search is illustrated in Figure~\ref{fig:stacked-plot-dominant-imagent}. Compared with DARTS,  the injected noise in NoisyDARTS successfully eliminates the unfair advantage.


The ImageNet classification results are shown in Table \ref{tab:comparison-imagenet}. Our model NoisyDARTS-A obtains the new state of the art results:  top-1 accuracy on ImageNet validation dataset with 4.9M number of parameters. After being equipped with more tricks as in EfficientNet, namely, squeeze and excitation \citec{hu2018squeeze}, Swish activation, and AutoAugment \citec{cubuk2018autoaugment}, it obtains   top-1 accuracy  with 5.5M parameters. The searched architecture is shown in Figure~\ref{fig:imagenet-architecture}.

\begin{table}
	\begin{center}
		\caption{Classification results on ImageNet. : Based on its published code. : Searched on CIFAR-10. : Searched on CIFAR-100. : Searched on ImageNet. : w/ SE and Swish} 
		\label{tab:comparison-imagenet}
		\begin{footnotesize}
			\begin{tabular}{*{6}{l}} 			
				\toprule
				Models &  (M)  &Params (M) & Top-1 (\%) & Top-5 (\%)  \\
\midrule 
				MobileNetV2(1.4) \citec{sandler2018mobilenetv2}   & 585 & 6.9 & 74.7 & 92.2 \\
				\midrule
				NASNet-A \citec{zoph2017learning}  & 564 & 5.3 &74.0 & 91.6\\
				AmoebaNet-A\citec{real2018regularized} & 555  & 5.1 & 74.5 &92.0\\
				MnasNet-92 \citec{tan2018mnasnet}  & 388 & 3.9 & 74.79 & 92.1 \\ 	
				MdeNAS\citec{zheng2019multinomial} & - & 6.1 & 74.5 & 92.1  \\
				\midrule
				DARTS \citec{liu2018darts} & 574 & 4.7 & 73.3 & 91.3\\
				SNAS \citec{xie2018snas}  &522&4.3&72.7&90.8 \\
				GDAS \citec{dong2019searching}  &581&5.3&74.0&91.5 \\
				PNAS \citec{liu2018progressive} & 588 & 5.1 & 74.2& 91.9 \\ 
				FBNet-C \citec{wu2018fbnet}   & 375 & 5.5 &  74.9 & 92.3  \\ 
FairNAS-C  \citec{chu2019fairnas} &321 & 4.4 & 74.7 &92.1  \\
				P-DARTS \citec{chen2019progressive}& 577 & 5.1 & 74.9 & 92.3  \\  FairDARTS-B \citec{chu2019fair}& 541 & 4.8 &75.1 & 92.5 \\
				NoisyDARTS-A (Ours) &446&4.9&76.1& 93.0\\
\midrule
				MobileNetV3 \citec{howard2019searching} & 219 & 5.4 &75.2 & 92.2 \\
				MoGA-A \citec{chumoga} & 304 & 5.1 & 75.9 & 92.8\\
				MixNet-M \citec{tan2020mixconv} &360 & 5.0 & 77.0 & 93.3\\
				EfficientNet B0 \citec{tan2019efficientnet} &390 & 5.3  & 76.3 & 93.2  \\
				NoisyDARTS-A (Ours) &449 & 5.5 & 77.9 & 94.0\\


\bottomrule
			\end{tabular}
		\end{footnotesize}
	\end{center}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.13]{imagenet-architecture.pdf}
	\caption{NoisyDARTS-A searched on ImageNet. Colors represent different stages.}
	\label{fig:imagenet-architecture}
\end{figure}

\subsection{Transferring to CIFAR-10}
We transferred our NoisyDARTS-A model searched on ImageNet to CIFAR-10. The model is trained for 200 epochs with a batch size of 256 and a learning rate of 0.05. We set the weight decay to be 0.0, a dropout rate of 0.1 and a drop connect rate of 0.1. In addition, we also use AutoAugment to avoid overfitting. Specifically, the transferred model NoisyDARTS-A-t achieved  top-1 accuracy with only 447M FLOPS, as shown in Table~\ref{tab:comparison-cifar10}.

\subsection{Transferred Results on Object Detection}
We further evaluate the transferability of our searched models on the COCO objection task \citec{lin2014coco}. Particularly, we utilize a drop-in replacement for the backbone  based on the Retina framework \citec{lin2017focal}. Besides, we use the MMDetection tool box since it provides a good implementation for various detection algorithms \citec{chen2019mmdetection}. Following the same training setting as \cite{lin2017focal}, all models in Table~\ref{table:fairnas-coco-retina} are trained and evaluated on the COCO dataset for 12 epochs. The learning rate is initialized as 0.01 and decayed by 0.1 at epoch 8 and 11. As shown in Table~\ref{table:fairnas-coco-retina}, our model obtains the best transferability than other models under the mobile settings.
\begin{table}
\small
	\begin{center}
		\caption{Object detection of various drop-in backbones. : w/ SE and Swish}
		\label{table:fairnas-coco-retina}
		\begin{tabular}{*{2}{l}H*{7}{l}}
			\toprule
			Backbones &   & Params (M) &Acc    & AP & AP & AP & AP & AP & AP \\
& (M) & (M) & (\%) &(\%) & (\%)& (\%)&(\%) &(\%) &(\%) \\
			\midrule
			MobileNetV2 \citec{sandler2018mobilenetv2} & 300 & 3.4& 72.0 & 28.3 & 46.7 & 29.3 & 14.8 & 30.7 & 38.1\\
			SingPath NAS \citec{stamoulis2019single} & 365 & 4.3 & 75.0 & 30.7 & 49.8 & 32.2 & 15.4 &33.9 & 41.6\\
			MnasNet-A2 \citec{tan2018mnasnet} & 340& 4.8 & 75.6 & 30.5 & 50.2 & 32.0 & 16.6 & 34.1 & 41.1\\
MobileNetV3 \citec{howard2019searching} & 219 & & 75.2& 29.9 & 49.3 & 30.8 & 14.9 & 33.3 & 41.1\\
MixNet-M \citec{tan2020mixconv} & 360 & 5.0 & 77.0 & 31.3& 51.7 & 32.4& 17.0 & 35.0 & 41.9   \\
			\textbf{NoisyDARTS-A (Ours)} & 449 & 5.5& 77.9& 33.1& 53.4 & 34.8& 18.5 & 36.6 & 44.4  \\
\bottomrule
		\end{tabular}
	\end{center}
\end{table}
\subsection{Ablation Study}

\subsubsection{With vs Without Noise}
We compare the searched models with and without noises on two commonly used search spaces in Table~\ref{table:ablation-with-noise}.  NoisyDARTS  robustly escape from the performance collapse across different search spaces and datasets. Note that without noise,  such differentiable approach performs severely worse and obtains only  top-1 on the ImageNet classification task. In constract, our simple yet effective method can find a state-of-the-art model with  top-1 accuracy. 
\begin{table}
	\begin{center}
		\caption{NoisyDARTS can robustly escape from the performance collapse across different search spaces and datasets.}
		\label{table:ablation-with-noise}
		\begin{tabular}{*{3}{l}}
			\toprule
			Type & Dataset & Top-1 (\%)\\
			\midrule
			w/ Noise & CIFAR-10 & 97.6 \\
			w/o Noise & CIFAR-10 & 97.0 \\
			w/ Noise & ImageNet &  76.1 \\
			w/o Noise &ImageNet & 66.4\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Zero-mean (unbiased) Noise  vs. Biased Noise}
According to Equation~\ref{eq:gradient}, good noise should interfere little with overall gradient flow in terms of global expectation but prevent skip connections from benefiting too much locally. Our ablation experiments confirm this hypothesis, as shown in Table~\ref{table:ablation-exps}. The searched cells are shown in Table~\ref{table:genotypes-baised-noise} (supplementary). When a non-zero mean noise is injected, it brings in a deterministic bias that overshoots the gradient and misguides the whole optimization process. Figure~\ref{fig:the-searched-cells-b} visualizes the NoisyDARTS-b cells.



\begin{table}[ht]
	\begin{center}
		\caption{Ablation experiments on Gaussian noise of different standard deviations.}
		\label{table:ablation-exps}
		\begin{tabular}{*{5}{l}}
		\toprule
		Noise Type &   &  & Avg. Top-1(\%)  & Best Top-1 (\%)\\
		\midrule
		Gaussian & 0.0 &  0.1 & 97.210.21 & \textbf{97.53} \\ Gaussian & 0.5 &  0.1  & 97.020.21  & 97.28 \\ Gaussian & 1.0 &  0.1 &  96.890.26 & 97.21 \\ \midrule
		Gaussian & 0.0 &  0.2 &97.300.23 & \textbf{97.61} \\ Gaussian & 0.5 &  0.2 & 97.160.15& 97.49 \\ Gaussian & 1.0 &  0.2 & 96.820.57 & 97.35 \\ \bottomrule
		\end{tabular}
	\end{center}
\end{table}


\subsubsection{Gaussian Noise  vs. Uniform Noise}

According to the analysis of Section~\ref{theory}, Gaussian noise is an appropriate choice. It can be unbiased and satisfies Equation~\ref*{eq:gradient}. In the same vein, unbiased uniform noise should be equally useful. We compare Gaussian noise with uniform noise in terms of searching effectiveness in Table~\ref{table:ablation-noise-type}. Both have improved performance while Gaussian is slightly better. We run each group of experiments eight independent times.

\begin{table}[ht]
	\begin{center}
		\caption{Ablation experiments on different types of noise and mixing operations. :  for multiplicative noise.}
		\label{table:ablation-noise-type}
		\begin{tabular}{*{4}{l}}
		\toprule
		Noise Type &   &   & Avg. Top-1 (\%)\\
		\midrule
		Gaussian & 0.0 &  0.1 &  97.210.21 \\ Uniform & 0.0 &  0.1 & 97.120.15  \\ \midrule
		Gaussian &  0.0  & 0.2 &  97.300.23 \\ Uniform &  0.0 & 0.2  & 97.150.23 \\ \bottomrule
		\end{tabular}
		\quad
		\begin{tabular}{*{4}{l}}
		\toprule
		Noise Mixture &   &   & Top-1 (\%)\\
		\midrule
		Additive &  0.0 & 0.1  & 97.210.21  \\ Multiplicative & 1.0 & 0.1  & 97.150.23 \\ \midrule
		Additive  & 0.0 & 0.2 & 97.300.23 \\Multiplicative & 1.0 & 0.2 & 97.220.23  \\ \bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Additive Noise vs. Multiplicative Noise}
To further testify that NoisyDARTS is robust regardless of noise mixture, we blend the noise by multiplying it with the output  of skip connection, which should be approximately effective as additive noise. We also give a similar theoretical proof in Section~\ref*{theory} that multiplicative noise can  tackle with performance collapse, see Appendix~\ref{supp:multi-noise}. Experiment results are shown in Table~\ref{table:ablation-noise-type}.
\section{Conclusion}


In this paper, we proposed a novel differentiable architecture search approach, NoisyDARTS. By injecting unbiased Gaussian noise into skip connections' output, we successfully let the optimization process be perceptible about the disturbed gradient flow. In such a way, the unfair advantage is largely attenuated. Experiments show that NoisyDARTS can work both effectively and robustly. The searched models achieved state-of-the-art results on CIFAR-10 and ImageNet. NoisyDARTS-a and NoisyDARTS-b also confirm that our proposed method can allow many skip connections as long as they do substantially contribute to the performance of the derived model.



\bibliographystyle{unsrtnat} \bibliography{egbib}

\clearpage

\appendix
\section{Analysis of Multiplicative Noise}\label{supp:multi-noise}
We set the output of skip connection with  multiplicative noise is , where  is sampled from a certain distribution. Similar to Section~\ref{theory}, the expectation of the gradient under multiplicative noise can be written as:

Again notice that taking   out of the expectation in Equation~\ref{eq:supp-gradient} requires Equation~\ref{eq:approximate} be satisfied. 
 To keep the gradient unbiased,  should be close to 1. We use Gaussian distribution  with a small .



\section{NoisyDARTS Architectures}

Following tables give the genotypes of best models searched under noise of various settings.

\begin{table}[ht]
	\begin{center}
		\caption{Best NoisyDARTS architecture genotypes searched on CIFAR-10 with unbiased noise.}
		\label{table:genotypes}
		\begin{tabular}{lp{300pt}}
			\toprule
			Model &  Architecture Genotype \\
			\midrule
			NoisyDARTS-a & Genotype(normal=[('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 0), ('skip\_connect', 0), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 3), ('sep\_conv\_3x3', 1), ('dil\_conv\_5x5', 4), ('dil\_conv\_3x3', 3)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 0), ('dil\_conv\_5x5', 1), ('skip\_connect', 2), ('max\_pool\_3x3', 0), ('skip\_connect', 2), ('skip\_connect', 3), ('skip\_connect', 2), ('dil\_conv\_5x5', 4)], reduce\_concat=range(2, 6))
			 \\
			NoisyDARTS-b & Genotype(normal=[('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 0), ('skip\_connect', 2), ('sep\_conv\_3x3', 0), ('dil\_conv\_3x3', 3), ('skip\_connect', 0), ('dil\_conv\_3x3', 3), ('dil\_conv\_3x3', 4)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 0), ('skip\_connect', 1), ('max\_pool\_3x3', 0), ('skip\_connect', 2), ('skip\_connect', 2), ('max\_pool\_3x3', 0), ('skip\_connect', 2), ('avg\_pool\_3x3', 0)], reduce\_concat=range(2, 6))
			 \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[ht]
	\begin{center}
		\caption{Best NosiyDARTS architecture genotypes searched on CIFAR-10 under biased noise.}
		\label{table:genotypes-baised-noise}
		\begin{tabular}{lp{330pt}}
			\toprule
			() &  Architecture Genotype \\
			\midrule
			(0.5, 0.1) &  Genotype(normal=[('sep\_conv\_3x3', 0), ('sep\_conv\_3x3', 1), ('skip\_connect', 0), ('dil\_conv\_3x3', 2), ('dil\_conv\_5x5', 3), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 4), ('dil\_conv\_3x3', 3)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 1), ('skip\_connect', 0), ('skip\_connect', 2), ('avg\_pool\_3x3', 1), ('skip\_connect', 2), ('avg\_pool\_3x3', 1), ('dil\_conv\_5x5', 3), ('skip\_connect', 4)], reduce\_concat=range(2, 6)) \\ (1.0, 0.1) &   Genotype(normal=[('sep\_conv\_3x3', 0), ('sep\_conv\_3x3', 1), ('skip\_connect', 1), ('skip\_connect', 0), ('dil\_conv\_3x3', 3), ('dil\_conv\_5x5', 2), ('dil\_conv\_5x5', 4), ('dil\_conv\_3x3', 3)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 0), ('sep\_conv\_3x3', 1), ('max\_pool\_3x3', 0), ('dil\_conv\_5x5', 2), ('skip\_connect', 3), ('skip\_connect', 2), ('skip\_connect', 3), ('skip\_connect', 4)], reduce\_concat=range(2, 6)) \\ (0.5, 0.2) &  Genotype(normal=[('sep\_conv\_3x3', 0), ('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 1), ('skip\_connect', 0), ('dil\_conv\_5x5', 3), ('dil\_conv\_5x5', 2), ('dil\_conv\_5x5', 4), ('dil\_conv\_3x3', 1)], normal\_concat=range(2, 6), reduce=[('avg\_pool\_3x3', 0), ('skip\_connect', 1), ('avg\_pool\_3x3', 0), ('skip\_connect', 2), ('avg\_pool\_3x3', 0), ('dil\_conv\_3x3', 3), ('dil\_conv\_5x5', 4), ('avg\_pool\_3x3', 0)], reduce\_concat=range(2, 6)) \\ (1.0, 0.2) &  Genotype(normal=[('sep\_conv\_3x3', 1), ('skip\_connect', 0), ('dil\_conv\_5x5', 2), ('sep\_conv\_3x3', 1), ('dil\_conv\_3x3', 3), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 4), ('dil\_conv\_5x5', 3)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 0), ('max\_pool\_3x3', 1), ('max\_pool\_3x3', 0), ('max\_pool\_3x3', 1), ('dil\_conv\_5x5', 3), ('max\_pool\_3x3', 0), ('dil\_conv\_5x5', 3), ('avg\_pool\_3x3', 0)], reduce\_concat=range(2, 6)) \\ \bottomrule
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[ht]
	\begin{center}
		\caption{Best NosiyDARTS architecture genotypes searched on CIFAR-10 with uniform noise.}
		\label{table:genotypes-uniform-noise}
		\begin{tabular}{lp{330pt}}
			\toprule
			() &  Architecture Genotype \\
			\midrule
			(0.0, 0.1) & Genotype(normal=[('skip\_connect', 0), ('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 2), ('sep\_conv\_3x3', 1), ('dil\_conv\_3x3', 3), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 3)], normal\_concat=range(2, 6), reduce=[('avg\_pool\_3x3', 0), ('max\_pool\_3x3', 1), ('skip\_connect', 2), ('max\_pool\_3x3', 0), ('skip\_connect', 3), ('skip\_connect', 2), ('skip\_connect', 3), ('avg\_pool\_3x3', 0)], reduce\_concat=range(2, 6)) \\ (0.0, 0.2) & Genotype(normal=[('sep\_conv\_3x3', 0), ('sep\_conv\_5x5', 1), ('skip\_connect', 2), ('skip\_connect', 0), ('dil\_conv\_5x5', 3), ('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 1), ('dil\_conv\_3x3', 4)], normal\_concat=range(2, 6), reduce=[('avg\_pool\_3x3', 0), ('sep\_conv\_5x5', 1), ('avg\_pool\_3x3', 0), ('skip\_connect', 2), ('dil\_conv\_5x5', 3), ('avg\_pool\_3x3', 0), ('avg\_pool\_3x3', 0), ('skip\_connect', 3)], reduce\_concat=range(2, 6)) \\ \bottomrule
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[ht]
	\begin{center}
		\caption{Best NosiyDARTS architecture genotypes searched on CIFAR-10 with multiplicative noise.}
		\label{table:genotypes-multi-noise}
		\begin{tabular}{lp{330pt}}
			\toprule
			() &  Architecture Genotype \\
			\midrule
			(1.0, 0.1) & Genotype(normal=[('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 0), ('skip\_connect', 0), ('skip\_connect', 2), ('skip\_connect', 0), ('dil\_conv\_3x3', 2), ('dil\_conv\_3x3', 4), ('dil\_conv\_5x5', 3)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 1), ('max\_pool\_3x3', 0), ('skip\_connect', 2), ('max\_pool\_3x3', 0), ('dil\_conv\_5x5', 3), ('skip\_connect', 2), ('skip\_connect', 2), ('skip\_connect', 4)], reduce\_concat=range(2, 6)) \\ (1.0, 0.2) & Genotype(normal=[('sep\_conv\_3x3', 1), ('sep\_conv\_3x3', 0), ('sep\_conv\_5x5', 1), ('skip\_connect', 0), ('dil\_conv\_3x3', 3), ('sep\_conv\_3x3', 0), ('dil\_conv\_5x5', 4), ('skip\_connect', 2)], normal\_concat=range(2, 6), reduce=[('max\_pool\_3x3', 0), ('max\_pool\_3x3', 1), ('skip\_connect', 2), ('avg\_pool\_3x3', 0), ('skip\_connect', 3), ('skip\_connect', 2), ('skip\_connect', 3), ('skip\_connect', 2)], reduce\_concat=range(2, 6)) \\\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\end{document}
