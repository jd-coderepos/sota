\documentclass{article}
\usepackage{fullpage}
\usepackage{latexsym}
\usepackage[dvips]{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pstricks}
\usepackage{pst-grad}
\usepackage{placeins}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[american]{babel}

\usepackage{times}
\usepackage{mathptmx}
\usepackage{mathrsfs}

\usepackage{CLRmacros}



\newlength{\savearraycolsep}
\newcommand{\narrowarray}[1]{\setlength{\savearraycolsep}{\arraycolsep}\setlength{\arraycolsep}{#1\arraycolsep}}
\newcommand{\normalarray}{\setlength{\arraycolsep}{\savearraycolsep}}


\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\title{Solving Cyclic Longest Common Subsequence in Quadratic Time}
\author{Andy Nguyen\thanks{Department of Computer Science, Stanford University}}


\begin{document}
\bibliographystyle{acm}
\maketitle

\begin{abstract}
We present a practical algorithm for the cyclic longest common subsequence () problem that runs in  time, where  and  are the lengths of the two input strings.  While this is not necessarily an asymptotic improvement over the existing record, it is far simpler to understand and to implement.
\end{abstract}

\section{Introduction}
The longest common subsequence () problem is a classic problem whose myriad extensions are applicable to many fields ranging from string matching for DNA sequence alignment to dynamic time warping for shape distance.  While the original problem considers strings with a beginning and an end as input, a natural extension is to consider cyclic strings, where the starting point is not fixed on either string.  This extension is useful for matching such objects as DNA plasmids and closed curves.  In this paper we present a practical algorithm for the cyclic longest common subsequence () problem that runs in  time, where  and  are the lengths of the two input strings.

\section{Related Work}
The state-of-the-art algorithm for regular  comes from~\cite{MP80}, who apply the standard dynamic programming algorithm with the ``Four Russians'' speedup~\cite{ADKF70} to achieve a running time of .  There are also algorithms (cite) that achieve better runtimes depending on the input, but all of these algorithms are asymptotically slower in the worst case.

The cyclic version of  has seen more recent improvements.  The first major improvement comes from~\cite{M90}, which uses a divide-and-conquer strategy to solve the problem in  runtime.  Since then, this approach has been applied to generalizations to the problem, and practical optimizations have been discovered; see~\cite{SFC07} for example.  We describe the setup used in these papers in our preliminaries, though we build a different approach on top of this setup to arrive at our new algorithm. \cite{LMS98} provides a solution that runs in  time on arbitrary inputs, with asymptotically better performance on similar inputs; however, this solution requires considerable amounts of machinery both to understand and to implement.

\section{Preliminaries}

Given a string  of length  and an integer , , we can define  = .  For example, , while .  We can extend this definition to any integer  by defining  when  or .

Let us define the following:

\begin{definition}
Let  and  be strings of length  and  respectively, where .  A \emph{cyclic longest common subsequence} of  and   is a string that satisfies the following properties:
\begin{enumerate}
\item  for some integers  and .
\item  for all integers  and .
\end{enumerate}
\end{definition}

We can first observe that  is no easier than .  To see this, we observe that we can solve  using  by prepending  and  with  \verb|x|'s, where \verb|x| is a symbol not found in either string.  This forces  to keep  and  aligned with each other to match all the \verb|x|'s, allowing us to extract  from the result.

On the other hand, we can also show that  is not much harder than .  We could solve  by solving  instances of , corresponding to each possible pair of cuts of the strings  and .  This implies that we have a solution to CLCS that runs in  time.  In fact, we can do even better:

\begin{lemma}
There is some cyclic longest common subsequence of  and   that satisfies the following properties:
\begin{enumerate}
\item  for some integer .
\item  for all integers  and .
\end{enumerate}
\end{lemma}
\begin{proof}
Given in~\cite{M90}.
\end{proof}

This lemma allows us to run only  instances of , giving us a runtime of .  Now let's see if we can do better.  To do so, let's take a short detour to look at regular .  Recall that we can solve  in  time with the use of a two-dimensional dynamic programming (DP) table that stores lengths  and parent pointers .  Normally we look at this table as a two-dimensional array; however, we can also look at this table as a directed grid graph  as follows (Figure~\ref{fig:tabletograph}):

\begin{figure}[htb]
\centering
\includegraphics[width=.95\linewidth]{ppfig1.png}
\caption{Visualizing the DP table as a grid graph.}
\label{fig:tabletograph}
\end{figure}

\begin{itemize}
\item For each entry  in the table we have a single node .
\item For each adjacent pair of entries  and  in the table where  is considered when computing , we have a directed edge from  to .  Note that all edges point down and/or to the right, so the graph is acyclic.  We see that all of the horizontal and vertical edges are present in the graph, and in addition we have diagonal edges wherever the corresponding characters in the two strings match.
\end{itemize}

For our convenience, we will fix the orientation of the graph as shown in Figure~\ref{fig:tabletograph}, namely, that  is the top left and  is the bottom right.  In this orientation, the first coordinate denotes the row, and the second coordinate denotes the column.

\begin{lemma}
Finding an  is equivalent to finding a shortest path from  to  in .
\end{lemma}
\begin{proof}
Given in~\cite{M90}.
\end{proof}

\begin{corollary}
The set of parent pointers computed by any DP solution to  (when treated as undirected edges) forms a shortest path tree of  rooted at .
\end{corollary}

Note that this holds true regardless of the choice of parent pointer in case of ties.

This means that we can solve  by solving a shortest path problem on a directed acyclic graph with unit edge lengths.

\section{Algorithm}

According to Lemma 2, we can solve  by solving  shortest-path problems, one for each cut of .  These shortest-path problems are closely related, as we see when we construct the graph  (Figure~\ref{fig:biggraph}).  Then the shortest paths we want to find are the ones from  to , from  to , , and from  to .  Now, we haven't done anything that will save us computation time; it will still cost us  time to find these  shortest paths.  However, the hope is that since these paths all live on the same graph, we can reuse some work between shortest path computations.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\linewidth]{ppfig2.png}
  \caption{Constructing the graph .}
  \label{fig:biggraph}
\end{figure}

As we have fixed the orientation of the graph, we can call one edge \emph{lower} than another with respect to this orientation, and we can do so similarly for paths.  We then define the following:

\begin{definition}
A \emph{lowest shortest path tree} of  is a shortest path tree where for every node , the path from the root to  contained in the shortest path tree is lower than any other shortest path from the root to the node.
\end{definition}

First, we show how to compute a lowest shortest path tree efficiently.

\begin{lemma}
The set of parent pointers computed by the DP solution that favors   first, then , and finally  in case of ties forms a lowest shortest path tree of  rooted at .
\end{lemma}
\begin{proof}
Suppose for contradiction that there is some node  where there is a lower shortest path  from  to  than the one  contained in the shortest path tree.  Beginning at  and tracing back towards ,  and  must eventually diverge at some node .  This divergence means that  and  follow two different valid parent pointers.  But since  follows the parent pointers that favor   first, then , and finally , it follows that  cannot be lower than .
\end{proof}

This lemma implies that maintaining the DP tiebreaking policy locally will yield a lowest shortest path tree globally.  We'll use this to our advantage to bound the work we need to do to extract all  shortest paths.

If we compute the lowest shortest path tree of , we can immediately read off the shortest path from  to .  Before we can read the shortest path from  to , we need to re-root the lowest shortest path tree at .  We can see that this is equivalent to removing the top row entirely, and then fixing the remainder of the tree to restore its properties.

\begin{lemma}
If we remove the top row of nodes from the lowest shortest path tree rooted at , we are left with at most two subtrees  and (possibly) .
\end{lemma}
\begin{proof}
We show this by showing there are at most two edges between the top row and the rest of the tree.  The vertical edge on the far left, from  to , must clearly be in the tree, and we denote the subtree rooted at  to be .  There cannot be any other vertical edges, since the vertical edge on the far left provides us with a lower path to any node in the second row.  Now consider the first diagonal edge, scanning from left to right, from  to .  We denote the subtree rooted at  to be .  By similar reasoning, there cannot be any other diagonal edges to the right of this one.
\end{proof}

Note that because of the tiebreaking rule we use for the lowest shortest path tree, any node in  that is adjacent to a node in  (to the left or diagonally up and left; we call these nodes in  \emph{boundary nodes}) chose its parent to be in  because that choice was \emph{strictly} better than the parent option in .

We next note the following about the length entries in the DP table corresponding to  (recall that these are lengths of the common subsequence, not lengths of the paths in the graph):
\begin{lemma}
For any entry  in the table, .
\end{lemma}
\begin{proof}
Clearly .  Now suppose for contradiction that .  We trace back along the parent pointers starting from  until we enter column .  The moment we do so, we find an entry where the length is either  if we followed a  or  if we followed a .  Either way, this entry is above  in the same column, yet strictly larger, so we could have filled this entry straight down to , which violates the optimality of .
\end{proof}

This lemma combined with our earlier observation about  and  leads us to the following key fact:
\begin{corollary}
Let  be such that  but .  Then .
\end{corollary}

Now, if we were to reconnect the two subtrees by setting , we would lower the length values of all nodes in  by exactly  (since we lose the diagonal of ).  But since every boundary node's old parent choice was strictly better than the option in , lowering the length values of all nodes in  does not result in any inconsistencies in the length table, which means we have a shortest path tree.  This is not necessarily a lowest shortest path tree; however, Corollary 2 tells us that after this reconnection, every boundary node  satisfies , which means in a lowest shortest path tree, .  Furthermore, every node in  has its length unchanged, and none of its potential parents gained length, so its parent pointer is unchanged.  Similarly, every non-boundary node in  loses exactly one unit of length, as do all of its potential parents, so its parent pointer is also unchanged.

This means if we set the parent of every boundary node to be , we now have a lowest shortest path tree, rooted at .  Note that we can walk along the graph to find all boundary nodes by moving down or right (or both), which means we can perform this rerooting process in  time.  The pseudocode is given below, where we assume that the input tree is rooted at .

\codebox{
\< \\
\li\>   \\
\li\>   \\
\li\>  \While  and  \\
\li\>\>   \\
\li\>  \If  \\
\li\>\>  \Return \\
\li\>   \\
\li\>  \While  and  \\
\li\>\>  \If  ==  \\
\li\>\>\>   \\
\li\>\>\>   \\
\li\>\>  \ElseIf  ==  \\
\li\>\>\>   \\
\li\>\>\>   \\
\li\>\>\>   \\
\li\>  \Else \\
\li\>\>\>   \\
\li\>  \While  and  ==  \\
\li\>\>   \\
\li\>\>   \\
}

With this process in place, we can now extract the  shortest paths from  by computing the lowest shortest path tree of , and then alternating between reading the shortest path from the root to the appropriate end node and re-rooting the lowest shortest path tree one node lower.  We only need to do so  times, which means the entire process runs in  time.  The pseudocode is given below.

\codebox{
\< \\
\li\>   \\
\li\>   \\
\li\>  let  and  be new tables \\
\li\>   \\
\li\>   \\
\li\>  \For  \To  \\
\li\>\>   \\
\li\>\>   \\
\li\>\>  \If  \\
\li\>\>\>   \\
\li\>  \Return  \\
}

\section{Implementation}

If we need to return the subsequence string, we must follow the pseudocode above.  If we only need to return the string's length, we can skip the traceback subroutine and instead simply read off the entry in the length table corresponding to the start of the traceback.  In this case, we need to update the far edge of the length table after each reroot, decrementing the lengths by 1.  In addition to simplifying the code, this change reduces the cache inefficiency of the algorithm, resulting in faster runtimes in practice.

It is worth noting that implementing this algorithm requires far less effort than either the  algorithm given in~\cite{M90} or the  algorithm given in~\cite{LMS98}.

\section{Conclusions}

We have presented a practical algorithm that solves  in  time.  Note that the proof of correctness relies heavily on the fact that all edges in the graph have weight 1.  This means that this algorithm does not naturally apply to the usual extensions of the problem, such as edit distance and dynamic time warping.  It may be worth investigating whether the algorithm can be adapted to certain classes of edge weights.

\bibliography{writeup}

\end{document}