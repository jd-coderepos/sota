\section{Experiments}
\label{sec:exp}

In this section, we describe experimental settings in detail.
We validate the effectiveness of our design compared to various SOTA methods.
Visualizations are also provided.

\subsection{Protocols}
\label{sec:exp-protocal}

\subsubsection{Implementation Details}

\textbf{Feature Encoding.}
We adopt a ResNet-50~\cite{he2016resnet}, which is pre-trained on ImageNet~\cite{deng2009imagenet}, with an FPN~\cite{lin2017feature} to obtain multi-scale image features. 
Following previous works~\cite{zhu2020deformabledetr,li2022bevformer}, the output features are from stage $S_{8\times}$, $S_{16\times}$ and $S_{32\times}$ of ResNet-50, where the subscripts $n\times$ indicates the downsampling factor.
In the FPN module, the features are transformed into a four-level output with an additional $S_{64\times}$ level. 
The number of output channels of each level is set to 256. 
Then we adopt a simplified view transformer with 3 encoder layers proposed in BEVFormer~\cite{li2022bevformer}. 
Note that we do not use temporal information, and thus the temporal self-attention layer in the BEVFormer encoder is replaced by a deformable attention~\cite{zhu2020deformabledetr} layer.
The size of BEV grids is set to $200 \times 100$, with four different height levels of $\{-1.5m, -0.5m, +0.5m, +1.5m\}$ relative to the ground. 



\smallskip
\noindent
\textbf{Deformable Decoder.}
For the decoder, we utilize the decoder layer in Deformable DETR~\cite{zhu2020deformabledetr} that each decoder layer contains three layers: 
a self-attention layer with 8 attention heads, 
a deformable attention layer with 8 attention heads and 4 offset points,
and a two-layer feed-forward network with 512 channels in the middle. 
After each operation, a dropout layer with a ratio of 0.1 and a layer normalization is applied. 
The dimension of initial queries $q=[q_p, q_o] \in Q$ is set to 256, where $q_p$ is utilized to generate the initial reference point, and $q_o$ is the initial object query.
Notably, the reference points will remain unchanged across different layers.
The query number for centerlines and traffic elements is set to 200 and 100 respectively.


\smallskip
\noindent
\textbf{Scene Graph Neural Network.}
We utilize a simplified version of Graph Convolutional Network (GCN)~\cite{kipf2016semi} as our GNN layer. 
Given an input matrix $Q^{i} \in \mathbb{R}^{N \times C}$, with $N$ representing the number of nodes and $C$ denoting the number of channels, the output of the operation is:
\begin{equation}
    Q^{i'} = \sigma \Big( T^i Q^{i} \mathbf{W}^{i} \Big),
\end{equation}
where $\mathbf{W}^i \in \mathbb{R}^{C \times C}$ is the learnable weight matrix, $T^i \in \mathbb{R}^{N' \times N}$ describes the adjacency matrix with $N'$ output nodes, and $\sigma(\cdot)$ is the activation function. 
Note that the matrix $T$ is inferred without gradients during training.
For the traffic element branch, an embedding network is employed before each GNN layer. The embedding network is a two-layer MLP, in which the output channels are 512 and 256.
In between the MLP, a ReLU activation function and a dropout layer are included. $\beta_{ll}$ and $\beta_{lt}$ are set to 0.6.

\smallskip
\noindent
\textbf{Prediction Heads.}
The prediction head for perception comprises a classification head and a regression head.
For the traffic element branch, the classification head is a single-layer MLP, which outputs the sigmoid probability of each class. 
The regression head is a three-layer MLP with ReLU as the activation function in between, which predicts the normalized coordinates of 2D bounding boxes in the form of $\{cx, cy, width, height\}$.
For centerline, the classification head consists of a three-layer MLP with LayerNorm and ReLU in between, which predicts the confidence score. 
The regression head is a three-layer MLP with ReLU, which predicts the normalized point set of $11 \times 3$ for a centerline.
To predict topology relationships, relationship heads are applied.
Given the instance queries $\widetilde{Q}_{a}$ and $\widetilde{Q}_{b}$ with 256 feature channels, the topology head first applies a three-layer MLP:
\begin{equation}
    {\widetilde{Q}'_{a}} = \texttt{MLP}_{\texttt{a}} (\widetilde{Q}_{a}), \ {\widetilde{Q}'_{b}} = \texttt{MLP}_{\texttt{b}} (\widetilde{Q}_{b}),
\end{equation}
where the number of output channels is 128.
For each pair of queries $\tilde{q}'_{a} \in \widetilde{Q}'_a$ and $\tilde{q}'_{b} \in \widetilde{Q}'_b$, the output is the confidence of the relationship:
\begin{equation}
    conf. = \texttt{sigmoid} \Big( \texttt{MLP}_{\texttt{top}} \big( \texttt{concat}(\tilde{q}'_{a}, \tilde{q}'_{b}) \big) \Big),
\end{equation}
where the $\texttt{MLP}$ is independent in different types of relationships.

\smallskip
\noindent
\textbf{Loss.}
The $\mathcal{L}_{det_{\text{TE}}}$ is decomposed into a classification, a regression, and an IoU loss:
\begin{equation}
    \mathcal{L}_{det_{\text{TE}}} = \lambda_{cls} \cdot \mathcal{L}_{cls} + \lambda_{reg} \cdot \mathcal{L}_{reg} + \lambda_{iou} \cdot \mathcal{L}_{iou},
\end{equation}
where $\lambda_{cls}$, $\lambda_{reg}$, and $\lambda_{iou}$ is set to 1.0, 2.5, and 1.0 respectively. 
The classification loss $\mathcal{L}_{cls}$ is a Focal loss.
Note that the regression loss $\mathcal{L}_{reg}$ is an L1 Loss calculated on a normalized format of $\{cx, cy, width, height\}$, while the IoU loss $\mathcal{L}_{iou}$ is a GIoU loss computed on the denormalized coordinates.
For centerline detection, the $\mathcal{L}_{det_{\text{LC}}}$ comprises a classification and a regression loss:
\begin{equation}
    \mathcal{L}_{det_{\text{LC}}} = \lambda_{cls} \cdot \mathcal{L}_{cls} + \lambda_{reg} \cdot \mathcal{L}_{reg},
\end{equation}
where $\lambda_{cls}$ and $\lambda_{reg}$ is set to $1.5$ and $0.025$ respectively. 
Note that the regression loss is calculated on the denormalized 3D coordinates.
For topology reasoning, we adopt the same Focal loss but different weights on different types of relationships:
\begin{equation}
    \mathcal{L}_{top} = \lambda_{top_{ll}} \cdot \mathcal{L}_{top_{ll}} + \lambda_{top_{lt}} \cdot \mathcal{L}_{top_{lt}},
\end{equation}
where both $\lambda_{top_{ll}}$ and $\lambda_{top_{lt}}$ are set to $5.0$. 



\smallskip
\noindent
\textbf{Training.}
The resolution of input images is 2048 $\times$ 1550, except for the front-view image, which is in the size of 1550 $\times$ 2048 and is cropped into 1550 $\times$ 1550.
For data augmentation, resizing with a factor of 0.5 and color jitter is used by default.
We adopt the AdamW optimizer~\cite{adamw} and a cosine annealing schedule with an initial learning rate of $1\,\times\,10^{-4}$. \algname is trained for 24 epochs with a batch size of 8 with 8 NVIDIA Tesla A100 GPUs.


\subsubsection{Re-implementation of SOTA Methods}

\begin{table}[t!]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{c|l|ccccc}
    		\toprule
      
                Data & Method
                & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$
                & DET$_{t}$$\uparrow$ 
                & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                
            \midrule
            
                \multirow{5}{*}{\rotatebox[origin=c]{90}{\parbox[t]{1.5cm}{\centering $subset\_A$}}}
                & STSU~\cite{can2021stsu}  & 12.7  & 0.5& 43.0 &  \underline{15.1} &  25.4\\
                & VectorMapNet~\cite{liu2022vectormapnet}  & 11.1  & 0.4& 41.7 &  5.9 & 20.8\\
                & MapTR~\cite{liao2023maptr} & 8.3&  0.2  & \underline{43.5} & 5.9 & 20.0 \\
                & MapTR*~\cite{liao2023maptr} &  \underline{17.7}&  \underline{1.1}&  \underline{43.5}&  10.4& \underline{26.0} \\
& \textbf{\algname} (Ours) & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\
                
            \midrule
            
                \multirow{5}{*}{\rotatebox[origin=c]{90}{\parbox[t]{1.5cm}{\centering $subset\_B$}}}
                & STSU~\cite{can2021stsu}  & 8.2 &  0.0 & 43.9 & \underline{9.4} & 21.2\\
                & VectorMapNet~\cite{liu2022vectormapnet}  & 3.5  & 0.0& 49.1 &  1.4 & 16.3\\
                & MapTR~\cite{liao2023maptr}  & 8.3& 0.1 &  \underline{54.0}  & 3.7 & 21.1 \\
                & MapTR*~\cite{liao2023maptr} &  \underline{15.2}&  \underline{0.5}&  \underline{54.0}&  6.1& \underline{25.2} \\
                & \textbf{\algname} (Ours)  & \textbf{24.3} & \textbf{2.5} & \textbf{55.0} & \textbf{14.2} & \textbf{33.2}\\

            \bottomrule
    	\end{tabular}
    }
    \caption{
        \textbf{Comparison with state-of-the-art methods} on the \datasetname benchmark.
         \algname outperforms all previous works by a wide margin, especially in directed centerline perception and topology reasoning. 
        *: Topology reasoning evaluation is based on matching results on Chamfer distance.
The highest score is bolded, while the second one is underlined.
    }
    \label{tab:ex:main}
    \vspace{-5px}
\end{table}


Since there are no prior methods for the task of driving scene understanding, we adapt three state-of-the-art algorithms which are initially designed for lane graph estimation or map learning: STSU~\cite{can2021stsu}, VectorMapNet~\cite{liu2022vectormapnet}, and MapTR~\cite{liao2023maptr}.
For a fair comparison, we utilize the same image backbone and adopt the same TE head as in \algname. As for topology reasoning, we treat it differently based on their own modeling concepts.
Specifically, STSU predicts centerlines as Bezier curves and their relationships. Thus most of its designs are retained, while other irrelevant modules are removed. Additionally, we append a topology head for predicting correspondence between centerlines and traffic elements.
The goal of VectorMapNet and MapTR is to predict vectorized map elements such as lanelines and pedestrian crossings. VectorMapNet utilizes a DETR-like decoder to estimate key points and then introduces an auto-regressive module to generate detailed graphical information for a laneline instance. MapTR directly predicts polylines with a fixed number of points using a Transformer decoder. 
To adapt them to our task, we supervise VectorMapNet and MapTR with centerline labels and use the element queries in their Transformer decoders as the instance queries to produce the driving scene topology.



\smallskip
\noindent
\textbf{STSU.}
The original model utilizes EfficientNet-B0~\cite{tan2019efficientnet} as the backbone and detects centerlines and vehicles under the monocular setting. 
It employs a BEV positional embedding and a DETR head to predict three Bezier control points for each centerline, and uses object queries in the decoder to predict the connectivity of centerlines.
To adapt to the multi-view inputs, we re-implement STSU by replacing the original backbone with ResNet-50 and aligning the image input resolution with our model. 
Subsequently, we compute and concatenate the BEV embedding of images from different views. 
The concatenated embedding is then fed into the DETR encoder. 
We retain the original DETR decoder to predict the Bezier control points, which are interpolated into 11 equidistant points as outputs. 
The lane-lane relationship prediction head of STSU is preserved as well. 
For the newly added traffic element detection branch and lane-traffic element topology head, we adopt the same design as in \algname to ensure a fair comparison.



\smallskip
\noindent
\textbf{VectorMapNet} utilizes a DETR-like decoder to estimate key points and an auto-regressive module to generate detailed graphical information for a laneline instance.
It uses a smaller input image size with a shorter edge length of 256.
We increase the length to 775 while maintaining the aspect ratio.
The backbone setting is aligned with \algname.
The perception range is defined as $\pm30\textit{m} \times \pm15\textit{m}$ in the original setting, but we expand it to
$\pm50\textit{m} \times \pm25\textit{m}$.
The centerline outputs of VectorMapNet are interpolated to fit our setting during the prediction process.
For topology prediction, we use the key point object queries in the VectorMapNet decoder as instance queries of centerlines. 
We implement the modification on the given codebase of VectorMapNet while retaining other settings.
However, due to their lack of support for 3D centerlines, we only predict 2D centerlines in the BEV space and ignore the height dimension.


\smallskip
\noindent
\textbf{MapTR.}
We align the original backbone setting with \algname. 
The perception region is expanded to $\pm50\textit{m} \times \pm25\textit{m}$.
We adopt MapTR to predict 11 points for each centerline. 
For topology prediction, we use the average of point queries of an instance in the MapTR decoder as the object query of a centerline. The traffic element head and the topology head are with the same setting as in \algname. 
The implementation is also done on the open-source codebase of MapTR.
Due to the lack of support for 3D centerlines, we only predict 2D centerlines in BEV and ignore the height dimension during inference.



\begin{table}[t!]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{lc|cc|c|c}
    		\toprule

                Method  & Topology 
                & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$
                & DET$_{l, chamfer}$$\uparrow$ 
                & FPS
                \\
                
            \midrule
            
                STSU~\cite{can2021stsu} & \ding{51} & 14.2 & 0.6 & 13.8 & \textbf{12.8} \\
                VectorMapNet~\cite{liu2022vectormapnet}  &  \ding{55} & 12.7 & - & 10.3 & 1.0 \\
                MapTR~\cite{liao2023maptr}  & \ding{55}  & 10.0 & - & 21.7 & 11.5 \\

            \midrule
            
\textbf{\algname}  & \ding{51} & \textbf{27.7} & \textbf{4.6} & \textbf{27.4} & 10.1 \\

            \bottomrule
    	\end{tabular}
    }
    \caption{
        \textbf{Comparison on centerline perception with a ResNet-50 backbone.}
        ``Topology'' denotes that the network is trained with topology supervision.
    }
    \label{tab:ex:unifiedbackbone}
\end{table}

\begin{table}[t!]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{l|c}
        \toprule
            Method & mIoU$\uparrow$ 
            \\
            \midrule
            HDMapNet~\cite{li2021hdmapnet} & 18.3 \\
            STSU~\cite{can2021stsu} & 31.1  \\
            VectorMapNet~\cite{liu2022vectormapnet} & 25.0  \\
            MapTR~\cite{liao2023maptr} & 35.7  \\
            \textbf{\algname} (Ours) & \textbf{39.0}  \\
            \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Comparison on BEV segmentation.} When rendering centerlines on the BEV grids, \algname also outperforms the previous approach.}
    \label{tab:sota-seg}
    \vspace{-5px}
\end{table}

\subsubsection{Dataset and Metrics}

We conduct experiments on the \datasetname benchmark~\cite{openlanev2}, which covers complex urban scenarios. 
The dataset contains topological structures in the driving scenes, and raises huge challenges for algorithms to perceive and reason about the environment accurately.
Ablation studies are conducted on the $subset\_A$ of \datasetname.

\smallskip
\noindent
\textbf{Dataset.}
Built on top of the Argoverse 2~\citep{wilson2av2} and nuScenes~\citep{caesar2020nuscenes} datasets, the \datasetname benchmark includes images from 2,000 scenes collected worldwide under different environments.
The dataset is split into two subsets, namely $subset\_A$ and $subset\_B$.
Each subset contains 1,000 scenes with multi-view images and annotations at 2$Hz$.
All lanes within $[-50m, +50m]$ along the x-axis and $[-25m, +25m]$ along the y-axis are annotated in the 3D space.
Centerlines are described in the form of lists of points.
Each list is ordered and comprises 201 points in 3D space.
Statistically, about 90\% of frames have more than 10 centerlines while about 10\% have more than 40.
Traffic elements follow the typical labeling style in 2D detection that objects are labeled as 2D bounding boxes on the front-view images.
Each element is denoted as a 2D bounding box on the front view image, with its attribute.
There are 13 types of attributes, including \textit{unknown}, \textit{red}, \textit{green}, \textit{yellow}, \textit{go\_straight}, \textit{turn\_left}, \textit{turn\_right}, \textit{no\_left\_turn}, \textit{no\_right\_turn}, \textit{u\_turn}, \textit{no\_u\_turn}, \textit{slight\_left}, and \textit{slight\_right}.
The topology relationships are provided in the form of adjacency matrices based on the ordering of centerlines and traffic elements.
In the adjacency matrices, an entry $(i, j)$ is positive (\ie, 1) if and only if the elements at $i$ and $j$ are connected.
Statistically, most of the lanes have one predecessor and successor. 
The number of connections is up to 7, as a lane leads to many directions in a complex intersection.
For the correspondence between centerlines and traffic elements, the majority have less than 5 correspondence, while a tiny number of elements have up to 12.

\smallskip
\noindent
\textbf{Perception Metrics.}
The $\text{DET}$ score is the typical mean average precision (mAP) for measuring instance-level perception performance.
Based on the Fr\'echet distances~\cite{eiter1994frechet}, the $\text{DET}_{l}$ score is averaged over match thresholds of $\mathbb{T} = \{1.0, 2.0, 3.0\}$:
\begin{equation}
    \text{DET}_{l} = \frac{1}{|\mathbb{T}|} \sum_{t \in \mathbb{T}} AP_t.
\end{equation}
Note that the defined BEV range is relatively large compared to other lane detection datasets, so accurate perception of lanes in the distance is hard. 
As a result, thresholds $\mathbb{T}$ are relaxed based on the distance between the lane and the ego car.
The $\text{DET}_{t}$ uses IoU as the similarity measure and is averaged over different attributes $\mathbb{A}$ of traffic elements:
\begin{equation}
    \text{DET}_{t} = \frac{1}{|\mathbb{A}|} \sum_{a \in \mathbb{A}} AP_a.
\end{equation}


\smallskip
\noindent
\textbf{Reasoning Metrics.}
The $\text{TOP}$ score is an mAP metric adapted from the graph domain.
Specifically, given a ground truth graph $G = (V, E)$ and a predicted one $\hat{G} = (\hat{V}, \hat{E})$, it builds a projection on the vertices such that $V = \hat{V}' \subseteq \hat{V}$, where the Fr\'{e}chet and IoU distances are utilized for similarity measure among lane centerlines and traffic elements respectively.
Inside the predicted $ \hat{V}'$, two vertices are regarded as connected if the confidence of the edge is greater than $0.5$.
Then the TOP score is the averaged vertice mAP between $(V, E)$ and $(\hat{V}', \hat{E}')$ over all vertices:
\begin{equation}
    \text{TOP} = \frac{1}{|V|} \sum_{v \in V} \frac{\sum_{\hat{n}' \in \hat{N}'(v)} P(\hat{n}') \mathbbm{1}(\hat{n}' \in N(v))}{|N(v)|},
\end{equation}
where $N(v)$ denotes the ordered list of neighbors of vertex $v$ ranked by confidence and $P(v)$ is the precision of the $i$-th vertex $v$ in the ordered list.
The $\text{TOP}_{ll}$ is for topology among centerlines on graph $(V_{l}, E_{ll})$, and the $\text{TOP}_{lt}$ for topology between lane centerlines and traffic elements on graph $(V_{l} \cup V_{t}, E_{lt})$.


\begin{table}[t!]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{c|l|ccccc}
    		\toprule
      
                Method
                & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$
                & DET$_{t}$$\uparrow$ 
                & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                
            \midrule
            
                Baseline & 25.7 & 4.0 & 47.2 & 20.6 & 34.6 \\
                + SG & 27.7 & 3.7 & 48.0 & 20.1 & 35.0 \\
                + SKG & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\

            \bottomrule
    	\end{tabular}
    }
    \caption{
        \textbf{Ablation on the design of scene graph neural network}. 
``SG'' represents the vanilla scene graph, and ``SKG'' is the enhanced SGNN with the proposed scene knowledge graph.
    }
    \label{tab:ab:gnn}
    \vspace{-5px}
\end{table}


\begin{table}[t!]
    \centering
    \scalebox{0.85}{
    	\begin{tabular}{l|ccccc}
    		\toprule
             Method & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$ & DET$_{t}$$\uparrow$ & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                \midrule
                 LL only & 27.9  & 3.8 & 47.8 & 20.3 & 35.1 \\
                 LT only & 27.8  & 3.9 & 47.5 & 20.5 & 35.1\\
                 \algname & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\
                \bottomrule
    	\end{tabular}
    }
    \caption{\textbf{Ablation on feature propagation} in the scene knowledge graph.
    ``LL only'' denotes that SGNN only aggregates the spatial information from lane-lane connectivity, and ``LT only'' indicates that only semantic information from the lane-traffic element relationship is included in SGNN.
    }
    \label{tab:ab:featurepropagation}
\end{table} 


\begin{table}[t!]
    \centering
    \scalebox{0.85}{
    	\begin{tabular}{l|cccccc}
    		\toprule
                Method & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$ & DET$_{t}$$\uparrow$ & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                \midrule
                w/o embedding & 28.4  & \textbf{4.1} & 46.9 & 20.5 & 35.2 \\
                \algname & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\
                \bottomrule
    	\end{tabular}
     }
    \caption{\textbf{Ablation on traffic element embedding.} TE embedding is necessary to deal with inconsistency in the feature space of different queries.}
    \label{tab:ablation-embedding}
    \vspace{-5px}
\end{table} 



\smallskip
\noindent
\textbf{Overall Metrics.}
The primary task of the dataset is scene structure perception and reasoning, which requires the model to recognize the dynamic drivable states of lanes in the surrounding environment.
The OpenLane-V2 Score (OLS) summarizes metrics covering different aspects of the primary task:
\begin{equation}
    \text{OLS} = \frac{1}{4} \bigg[ \text{DET}_{l} + \text{DET}_{t} + f(\text{TOP}_{ll}) + f(\text{TOP}_{lt}) \bigg],
\end{equation}
where DET and TOP describe performance on perception and reasoning respectively, and $f$ is the square root function. 


\subsection{Main Results}
\label{sec:exp-results}

\begin{figure*}[t!]
  \centering
       \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{Figs/vis_subset_a.pdf}
         \vspace{-15px}
        \caption{Qualitative results on $subset\_A$ of the \datasetname dataset.}
        \vspace{3px}
     \end{subfigure}
     
          \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{Figs/vis_subset_b.pdf}
         \vspace{-17px}
        \caption{Qualitative results on $subset\_B$ of the \datasetname dataset.}
     \end{subfigure}
      \vspace{-15pt}
  \caption{
  \textbf{Qualitative results} of \algname and other algorithms. While driving in complex scenarios, \algname achieves superior lane graph prediction performance compared to other SOTA methods. It also successfully builds all connections between traffic elements and lanes (top right, and correspondingly colored lines in BEV).
Colors denote categories of traffic elements.
  }
  \label{fig:vis}
\end{figure*}

We compare the proposed \algname to several state-of-the-art methods in \Cref{tab:ex:main}. 
\algname outperforms all previous algorithms by a large margin.
As the SOTA map learning method MapTR ignores the direction of centerlines with the permutation-equivalent modeling, we additionally evaluate MapTR based on Chamfer distance matching. 
However, its performance on DET$_l$, as well as topology metrics, significantly degenerates. 
The performance of centerline queries without directional information indicates that understanding the complex scenario and perceiving presented instances are two totally different stories.
All methods achieve similar DET$_t$, since we adopt the same traffic element detection branch.
However, \algname still has a better performance on LC-TE topology reasoning, indicating the necessity of the proposed SGNN module, in which different entities are modeled differently.


\smallskip
\noindent
\textbf{Comparison on Centerline Perception.} 
To have a fair comparison, we use a unified backbone architecture and PV-to-BEV transformation module for various SOTA methods on centerline perception task.
We keep the topology supervision for STSU, as it is originally designed for detecting centerlines and their topology relationship.
Since VectorMapNet and MapTR are for the task of laneline detection where there is no relationship between visible lanelines, we alter the supervision from laneline to centerline and ignore topology supervision to preserve their design choice.
As shown in \Cref{tab:ex:unifiedbackbone}, \algname outperforms other methods, indicating the effectiveness of the model design.
To better align with previous works~\cite{liu2022vectormapnet, liao2023maptr}, we also provide DET$_{l,\text{chamfer}}$ with the Chamfer distance as the similarity measure.
It does not take the lane direction into account and is thresholded on \{0.5, 1.0, 1.5\}.
Under this metric, \algname also shows a prevailing performance compared to other methods.
Besides, we measure the runtime of each model on an A100 bare machine.
The FPS of \algname is 10.1. Compared to other methods on the same machine with aligned input size $512\!\times676$, our method has comparable online efficiency but higher performance.


\smallskip
\noindent
\textbf{Comparison on BEV Segmentation.} 
The prediction results of our method are rendered to BEV with a fixed line width of 0.75$m$ aligned with HDMapNet~\cite{li2021hdmapnet}, and the mIoU metric is adopted following common segmentation tasks. 
We reproduce HDMapNet with an enlarged BEV perception range of $\pm50\textit{m} \times \pm25\textit{m}$ to get aligned with \algname.
As shown in \Cref{tab:sota-seg}, \algname significantly surpasses other methods in terms of mIoU, verifying the advance of our framework.


\begin{figure*}[t!]
  \centering
    \includegraphics[width=\linewidth]{Figs/sup_vis_final.pdf}
     \vspace{-15pt}
  \caption{
  \textbf{Failure case under large-area occlusion.} \algname fails to predict centerlines and the lane graph in the intersection with a large bus colluding in front. Note that the relationship between the left lane and the red light is an incorrect annotation where our algorithm reasons about the direction of the left lane and avoids the false positive prediction.
  }
  \label{fig:supp-vis-occlusion}
\end{figure*}


\begin{table}[t!]
    \centering
    \scalebox{0.85}{
    	\begin{tabular}{c|cccccc}
    		\toprule
                \# GNN & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$ & DET$_{t}$$\uparrow$ & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                \midrule
                1 & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\
                2 & 27.9 & 4.0 & 47.5 & 20.9 & 35.3 \\
                3 & 20.4 & 0.5 & 46.1 & 15.7 & 28.3 \\
                4 & 12.2 & 0.0 & 48.5 & 8.2 & 22.6 \\
                \bottomrule
    	\end{tabular}
    
    }
    \caption{\textbf{Ablation on the number of GNN layers} in the scene knowledge graph. Model performance drops as the number of SGNN layers increases.}
    \label{tab:ab:gnnlayer}
    \vspace{-5px}
\end{table} 



\subsection{Ablation Study}
\label{sec:exp-ablation}


\noindent
\textbf{Effect of Design in Scene Graph Neural Network.}
For ablations on the proposed SGNN, we alternate the proposed network into a baseline without feature propagation by downgrading the SGNN module to an MLP and the supervision of topology reasoning only occurs at the final decoder layer. The concatenation and down-sampling operations, as well as the traffic element embedding, are also removed. As illustrated in \Cref{tab:ab:gnn}, the proposed SKG outperforms models in other settings, demonstrating its effectiveness for topology understanding.
Compared to the SG version, the scene knowledge graph provides an additional improvement of 0.8\% for centerline perception, owning to the predefined semantic prior encoded in the categories of traffic elements.
The improvement of traffic element detection and topology reasoning is also consistent.


\smallskip
\noindent
\textbf{Effect on Feature Propagation.}
In the ``LL only'' setting, we set the $\beta_{lt}$ parameter to 0. Similar to the baseline, we remove the concatenation and down-sampling operations, as well as the traffic element embedding.
For ``LT only'', we set the $\beta_{ll}$ parameter to 0, while other modules remain intact. 
Results are reported in \Cref{tab:ab:featurepropagation}.
In the ``LL only'' setting, the drop on $\text{TOP}_{lt}$ demonstrates the importance of the graph $G_{lt}$.
With the ``LT only'' design, $\text{DET}_l$ degenerates when removing the graph $G_{ll}$, showing the importance of feature propagation between centerline queries.
These experiments show that both branches are necessary for achieving satisfactory model performance on the primary task.


\begin{table}[t!]
    \centering
    \scalebox{0.85}{
    	\begin{tabular}{c|cccccc}
    		\toprule
                Edge Weight & DET$_{l}$$\uparrow$ 
                & TOP$_{ll}$$\uparrow$ & DET$_{t}$$\uparrow$ & TOP$_{lt}$$\uparrow$ 
                & OLS$\uparrow$ 
                \\
                \midrule
                0.5 & 28.4  & 4.0 & 47.7 & \textbf{20.8} & 35.4 \\
                0.6 & \textbf{28.5} & \textbf{4.1} & \textbf{48.1} & \textbf{20.8} & \textbf{35.6}  \\
                0.7 & 27.3 & \textbf{4.1} & 47.7 & 20.7 & 35.1\\
                \bottomrule
    	\end{tabular}
    }
\caption{\textbf{Ablation on edge weight} in the scene knowledge graph.}
    \label{tab:ab:weight}
    \vspace{-5px}
\end{table} 

\smallskip
\noindent
\textbf{Effect on Traffic Element Embedding.}
In the ``w/o embedding'' setting, we remove the traffic element embedding network and use $Q_t$ as the input of SGNN directly.
As shown in \Cref{tab:ablation-embedding}, removing the embedding results in a 1.2\% performance drop in traffic element recognition.
The reason is that TE queries contain a large amount of spatial information in the PV space due to the 2D detection supervision signals, resulting in significant inconsistencies in the feature spaces.
In all, the experiments demonstrate that TE embedding effectively filters out irrelevant spatial information and extracts high-level semantic knowledge to help centerline detection and lane topology reasoning.




\smallskip
\noindent
\textbf{Effect on the Number of GNN Layers.}
Though GNN is beneficial for propagating features in the knowledge graph, raising the number of GNN layers leads to degenerated performance. As shown in \Cref{tab:ab:gnnlayer}, SGNN with a single GNN layer achieves the best performance. The reason is that a GNN layer increases the similarity of adjacent vertices. When stacking multiple GNN layers, features of all vertices become less discriminative.






\smallskip
\noindent
\textbf{Effect on Edge Weight.}
Edge weight in the scene knowledge graph represents how much information is propagated through the SGNN layers. In \Cref{tab:ab:weight}, 0.6 corresponds to the most appropriate ratio and is used as the default value.


\subsection{Qualitative Analysis}


We provide a qualitative comparison in \cref{fig:vis}. \algname predicts most centerlines correctly and constructs a lane graph in BEV. Yet, prior works fail to output all entities or get confused about their connectivity. 
\cref{fig:supp-vis-occlusion} shows a case where a bus occludes the intersection in the front view image. 
\algname fails to predict lanes and the topology, especially those in the left half of the crossing. 
A large-scale dataset and learning techniques, such as active learning, would solve such failure cases in a real-world deployment.
