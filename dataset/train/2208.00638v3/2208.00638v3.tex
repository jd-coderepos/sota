\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{emnlp2022}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{inconsolata}

\usepackage{bm}

\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\useunder{\uline}{\ul}{}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{lscape}
\usepackage{bbding}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{siunitx}
\newcommand{\hzt}[1]{\textcolor{red}{[hzt: #1]}}
\newcommand{\lgy}[1]{\textcolor{blue}{[lgy: #1]}}
\newcommand{\yzc}[1]{\textcolor{green}{[yzc: #1]}}

\newcommand{\tenc}[1]{\textcolor{blue}{#1}}
\newcommand{\senc}[1]{\textcolor{red}{#1}}
\newcommand{\sss}{\textcolor{red}{\faTimes\ }}
\newcommand{\ttt}{\textcolor{blue}{\faTimes\ }}
\newcommand{\fff}{\textcolor{gray}{\faTimes\ }}


\title{Composable Text Controls in Latent Space with ODEs}

\makeatletter
\renewcommand{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother


\author{Guangyi Liu ,~~
Zeyu Feng,~~
Yuan Gao,~~
Zichao Yang,~~
Xiaodan Liang,~~\\
{\bf Junwei Bao,~~
Xiaodong He,~~
Shuguang Cui,~~
 Zhen Li,~~
Zhiting Hu}\\
FNii, CUHK-Shenzhen,~~ UC San Diego, ~~MBZUAI, \\Carnegie Mellon University, ~~ DarkMatter AI Research,~~ JD AI Research\\
{\small \tt guangyi.liu@mbzuai.ac.ae, lizhen@cuhk.edu.cn, zhh019@ucsd.edu}
}




\begin{document}
\maketitle


\begin{abstract}
Real-world text applications often involve \emph{composing} a  wide range of text control operations, such as editing the text \emph{w.r.t.} an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact \emph{latent} space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.\footnote{Code: \url{https://github.com/guangyliu/LatentOps}}
\end{abstract}


%
 

\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth,page=1]{fig/intro.pdf}
    \vspace{-8pt}
    \caption{Examples of different composition of text operations, such as editing a text in terms of different attributes sequentially (top) or at the same time (middle), or generating a new text of target properties (bottom). The proposed \textsc{LatentOps} enables a single LM (e.g., an adapted GPT-2) to perform arbitrary text operation composition in the latent space.}
    \label{fig:intro}
    \vspace{-15pt}
\end{figure}

Many text problems involve a diverse set of text control operations, such as editing different attributes (e.g., sentiment, formality) of the text, inserting or changing the keywords, generating new text of diverse properties, and so forth. In particular, different \emph{composition} of those operations are often required in various real-world applications (Figure~\ref{fig:intro}).  

Conventional approaches typically build a conditional model (e.g., by finetuning pretrained language models) for each specific combination of operations~\cite{hu2017toward,keskarCTRL2019,DBLP:journals/corr/abs-1909-08593}, which is unscalable given the combinatorially many possible compositions and the lack of supervised data. Most recent research thus has started to explore plug-and-play solutions. Given a pretrained language model (LM), those approaches plug in arbitrary constraints to guide the production of desired text sequences \cite{Dathathri2020Plug,DBLP:journals/corr/abs-2104-05218, DBLP:conf/nips/KumarMST21, DBLP:conf/emnlp/KrauseGMKJSR21,DBLP:journals/corr/abs-2203-13299,QinCOLD}. 
The approaches, however, typically rely on search or optimization in the complex text \emph{sequence space}. The discrete nature of text makes the search/optimization extremely difficult. Though some recent work introduces continuous approximations to the discrete tokens \citep{qin2020back,QinCOLD,DBLP:conf/nips/KumarMST21}, the high dimensionality and complexity of the sequence space still renders it inefficient to find the accurate high-quality text.


In this paper, we develop \textsc{LatentOps}, a new efficient approach that performs composable control operations in the compact and continuous \emph{latent space} of text. \textsc{LatentOps} permits plugging in arbitrary operators (e.g., attribute classifiers) applied on text latent vectors, to form an energy-based distribution on the low-dimensional latent space. We then develop  
an efficient sampler based on ordinary differential equations (ODEs) \cite{DBLP:conf/iclr/0011SKKEP21,nie2021controllable,Vahdat_LSGM} to draw latent vector samples that bear the desired attributes. 


A key challenge after getting the latent vector is to decode it into the target text sequence. To this end, we connect the latent space to pretrained LM decoders (e.g., GPT-2) by efficiently adapting a small subset of the LM parameters in a variational auto-encoding (VAE) manner~\cite{DBLP:journals/corr/KingmaW13,DBLP:conf/conll/BowmanVVDJB16}. 

Previous attempts of editing text in latent space have often been limited to single attribute and small-scale models, due to the incompatibility of the latent space with the existing transformer-based pretrained LMs \cite{DBLP:conf/nips/WangH019,DBLP:conf/aaai/LiuFZPL20, DBLP:conf/icml/ShenMBJ20,DBLP:conf/acl/DuanXPHL20,DBLP:conf/emnlp/MaiPMSH20}. \textsc{LatentOps} overcomes the difficulties and enables a single large LM to perform arbitrary composable text controls.

We conduct experiments on three challenging settings, including sequential editing of text \emph{w.r.t.} a series of attributes, editing compositional attributes simultaneously, and generating new text given various attributes. Results show that composing operators within our method manages to generate or edit high-quality text, substantially improving over respective baselines in terms of quality and efficiency.

















 \label{sec:intro}

\section{Background}


\subsection{Energy-based Models and ODE Sampling}
\label{sec:bg_ebms}
Given an arbitrary energy function , energy-based models (EBMs) define a Boltzmann distribution:

where  is the normalization term (the summation is replaced by integration if  is a continuous variable). 
EBMs are flexible to incorporate any functions or constraints into the energy function . Recent work has explored text-based EBMs (where  is a text sequence) for controllable text generation \cite{DBLP:conf/nips/HuYSQLDX18,DBLP:conf/iclr/DengBOSR20,DBLP:journals/corr/abs-2106-15078,DBLP:conf/iclr/KhalifaED21,DBLP:journals/corr/abs-2203-13299,QinCOLD}.
Despite the flexibility, sampling from EBMs is rather challenging due to the intractable . The text-based EBMs
face with even more difficult sampling due to the extremely large and complex (discrete or soft) text space.



Langevin dynamics \cite[LD,][]{DBLP:conf/icml/WellingT11,DBLP:journals/corr/abs-1811-08413} is a gradient-based Markov chain Monte Carlo (MCMC) approach often used for sampling from EBMs \citep{DBLP:conf/nips/DuM19,DBLP:conf/nips/SongE19,DBLP:journals/corr/abs-2004-06030,QinCOLD}. It is considered as a more efficient way compared to other gradient-free alternatives (e.g., Gibbs sampling \citep{bishop2006pattern}). However, due to several critical hyperparameters (e.g., step size, number of steps, noise scale), LD tends to be sensitive and unrobust in practice \citep{nie2021controllable,DBLP:journals/corr/abs-1903-08689,DBLP:conf/iclr/GrathwohlWJD0S20}. 

On the other hand, stochastic/ordinary differential equations (SDEs/ODEs) \cite{anderson1982reverse} offer another sampling technique recently applied in image generation \citep{DBLP:conf/iclr/0011SKKEP21,nie2021controllable}. An SDE characterizes a \emph{diffusion process} that maps real data to random noise in continuous time . Specifically, let  be the value of the process following , indexed by time . At start time ,  which is the data distribution, and at the end ,  which is the noise distribution (e.g., standard Gaussian). The \emph{reverse} SDE instead generates a real sample from the noise by working backwards in time (from  to ). More formally, consider a {\it variance-preserving} SDE \citep{DBLP:conf/iclr/0011SKKEP21} whose reverse is written as

where d is an infinitesimal negative time step;  is a standard Wiener process when time flows backwards from  to ; and the scalar  is a time-variant coefficient linear \emph{w.r.t.} time . Given a noise , solving the above reverse SDE returns a  that is a sample from the desired distribution . One could use different numerical solvers to this end.
\cite{burrage2000numerical, higham2001algorithmic,rossler2009second}. 
The SDE sampler sometimes need to combine with an additional corrector to improve the sample quality \citep{DBLP:conf/iclr/0011SKKEP21}.

Further, as shown in \citep{DBLP:conf/iclr/0011SKKEP21,DBLP:journals/entropy/MaoutsaRO20}, each (reverse) SDE has a corresponding ODE, solving which leads to samples following the same distribution. The ODE is written as (see Appendix~\ref{app:derivation_ode} for the derivations):

Solving the ODE with relevant numerical methods~\cite{euler1824institutionum,calvo1990fifth,engstler1997mur8} corresponds to an sampling approach that is more efficient and robust \citep{DBLP:conf/iclr/0011SKKEP21,nie2021controllable}.

In this work, we adapt the ODE sampling for our approach. Crucially, we overcome the text control and sampling difficulties in the aforementioned sequence-space methods, by defining the text control operations in a compact latent space, handled by a latent-space EBMs with the ODE solver for efficient sampling.
























\subsection{Latent Text Modeling with Variational Auto-Encoders}
\label{sec:bg_vae}
Variational auto-encoders (VAEs)~\cite{DBLP:journals/corr/KingmaW13,DBLP:conf/icml/RezendeMW14} have been used to model text with a low-dimensional continuous latent space with certain regularities \citep{DBLP:conf/conll/BowmanVVDJB16,hu2017toward}. An VAE connects the text sequence space  and the latent space  with an encoder  that maps text  into latent vector , and a decoder  that maps a  into text. Previous work usually learns text VAEs from scratch, optimizing the encoder and decoder parameters with the following objective:

where  is a standard Gaussian distribution as the prior, and  is the Kullback-Leibler divergence that pushes  to be close to the prior. The first term encourages  to encode relevant information for reconstructing the observed text , while the second term adds regularity so that any  can be decoded into high-quality text in the text sequence space .
Recent work \citep{li-etal-2020-optimus,hu2021causal} scales up VAE by initializing the encoder and decoder with pretrained LMs (e.g., BERT~\cite{bert} and GPT-2~\cite{gpt2}, respectively). However, they still require costly finetuning of the whole model on the target corpus. 

In comparison, our work converts a given pretrained LM (e.g., GPT-2) into a latent-space model efficiently by tuning only a small subset of parameters, as detailed more in \S\ref{sec:vae_training}.








 \begin{figure*}
    \centering
    \vspace{-15pt}
    \includegraphics[width=0.85\textwidth]{fig/model_v4.pdf}
\caption{Overview of \textsc{LatentOps}. 
    (Left): We equip pretrained LMs (e.g., GPT-2) with the compact continuous latent space through parameter-efficient adaptation (\S\ref{sec:vae_training}).
    (Right): One could plug in arbitrary operators (e.g., attribute classifiers) to obtain the latent-space EBM (\S\ref{sec:latent_ebms}). We then sample desired latent vectors efficiently by solving the ODE which works backwards through the diffusion process from time  to . The resulting sample  is fed to the decoder (adapted GPT-2) to generate the desired text sequence.
    }
    \label{fig:model}
    \vspace{-10pt}
\end{figure*}

\section{Composable Text Latent Operations} 
We develop our approach \textsc{LatentOps} that quickly adapts a given pretrained LM (e.g., GPT-2) to enable composable text latent operations. The approach consists of two components, namely a VAE based on the pretrained LM that connects the text space with a compact continuous latent space, and EBMs on the latent space that permits arbitrary attribute composition and efficient sampling. 

More specifically, the VAE decoder  offers a way to map any given latent vector  into the corresponding text sequence. Therefore, text control (e.g., editing a text or generating a new one) boils down to finding the desired vector  that bears the desired attributes and characteristics. To this end, one could plug in any relevant attribute operators (e.g., classifiers), resulting in a latent-space EBM that characterizes the distribution of  with the desired attributes. We could then draw the  samples of interest, performed efficiently with an ODE solver. Figure~\ref{fig:model} gives an illustration of the approach.

\textsc{LatentOps} thus avoids the difficult optimization or sampling in the complex text sequence space as compared to the previous plug-and-play methods \cite[e.g.,][]{DBLP:journals/corr/abs-2104-05218,Dathathri2020Plug,QinCOLD}. Our approach is also compatible with the powerful pretrained LMs, requiring only minimal adaptation to equip the LMs with a latent space, rather than costly retraining from scratch as in the recent diffusion LM \cite{li2022diffusion}. 


In the following, we first present the latent-space EBM formulation (\S\ref{sec:latent_ebms}) for composable operations, and derive the efficient ODE sampler (\S\ref{sec:ode_sampler}); we discuss the parameter-efficient adaptation of pretrained LMs for the latent space (\S\ref{sec:vae_training});  we then discuss the implementation details (\S\ref{sec:implement}).






\subsection{Composable Latent-Space EBMs}
\label{sec:latent_ebms}

We aim to formulate the latent-space EBMs such that one can easily plug in arbitrary attribute operators to define the latent distribution of interest. Besides, as we want to obtain fluent text with the VAE decoder  described in \S\ref{sec:vae_training}, the latent distribution over  should match the structure of the VAE latent space. 

Formally, let  be a vector of desired attribute values, where each  (e.g., positive sentiment, or informal writing style). Note that  does not have a prefixed length as one can plug in any number of attributes to control on the fly. In general, to assess if a vector  bears the desired attribute , we could use any function  that takes in  and , and outputs a score measuring how well  is carried in . 
For a categorical attribute (e.g., sentiment, either positive or negative), one of the common choices is to use a trained attribute classifier, where  is the output logit vector and  is the logit of the particular class  of interest. For clarity of presentation, we focus on categorical attributes and classifiers in the rest of the paper, and assume the attributes are independent with each others. 

We are now ready to formulate the latent-space EBMs by plugging in the attribute classifiers. Specifically, we define the joint distribution:

where  is the Gaussian prior distribution of VAE (\S\ref{sec:bg_vae}), and  is formulated with energy function  to encode the different target attributes. Such a decomposition of  results in two key desirable properties: (1)
The marginal distribution over  equals the VAE prior, i.e., . This facilitates the VAE decoder to generate fluent text;
(2) the energy function in  enables the combination of arbitrary attributes, with . Each  is the balance weight, and  is the defined as the negative log probability (i.e., the normalized logit) of  to make sure the different attribute classifiers have outputs at the same scale for combination:





















\subsection{Efficient Sampling with ODEs}
\label{sec:ode_sampler}
Once we have the desired distribution  over the latent space and attributes, we would like to draw samples  given the target attribute values . The samples can then be fed to the VAE decoder (\S\ref{sec:vae_training}) to obtain the desired text. As discussed in \S\ref{sec:bg_ebms} and also shown in our ablation study in \S\ref{app:compare_sde}, sampling with ODEs has the benefits of robustness compared to Langevin dynamics that is sensitive to hyperparameters, and efficiency compared to SDEs that require additional correction. 

We now derive the ODE sampling in the latent space. Specifically, we adapt the ODE from Eq.\eqref{eq:ode_x} into our latent-space setting, which gives:

For , notice that at ,  is the VAE prior distribution  as defined in Eq.\eqref{eq:joint_dist}, which is the same as  (i.e., the Gaussian noise distribution after diffusion). This means that in the diffusion process, we always have  that is time-invariant \citep{nie2021controllable}. Similarly, for , since the input  follows the time-invariant distribution and the classifiers  are fixed, the  is also time-invariant. Plugging the definitions of those components, we obtain the simple ODE formulation:

We can then easily create latent samples conditioning on the given attribute values, by drawing  and solving the Eq.\eqref{eq:final_ode} with a differentiable neural ODE solver\footnote{\url{https://github.com/rtqichen/torchdiffeq}}~\cite{chen2018neuralode,chen2021eventfn} to obtain . In \S\ref{sec:implement}, we discuss more implementation details with approximated starting point  for text editing and better empirical performance.






\subsection{Adapting Pretrained LMs for Latent Space}
\label{sec:vae_training}
To decode the  samples into text sequences, we equip pretrained LMs (e.g., GPT-2) with the latent space through parameter-efficient adaptation. More specifically, we adapt the autoregressive LM into a text latent model within the VAE framework (\S\ref{sec:bg_vae}). Differing from the previous VAE work that trains from scratch or finetunes the full parameters of pretrained LMs \citep{li-etal-2020-optimus,hu2021causal,hu2017toward}, we show that it is sufficient to only update a small portion of the LM parameters to connect the LM with the latent space, while keeping the LM capability of generating fluent coherent text. Specifically, we 
augment the autoregressive LM with small MLP layers that pass the latent vector  to the LM, and insert an additional transformer layer in between the LM embedding layer and the original first layer. The resulting model then serves as the decoder in the VAE objective (Eq.\ref{eq:vae_loss}), for which we only optimize the MLP layers, the embedding layer, and the inserted transformer layer, while keeping all other parameters frozen. For the encoder, we use a BERT-small model \cite{bert,DBLP:journals/corr/abs-1908-08962} and finetune it in the VAE framework. As discussed later in \S\ref{sec:implement}, the tuned encoder can be used to produce the initial  values in the ODE sampler for text editing.








\subsection{Implementation Details}\label{sec:implement}

We discuss more implementation details of the method. Overall, given an arbitrary text corpus (e.g., a set of text from any domain of interest), we first build the VAE by adapting the pretrained LMs as described in \S\ref{sec:vae_training}. Once the latent space is established, we keep it (including all the VAE components) fixed, and perform compositional text operations in the latent space on the fly.

\paragraph{Acquisition of attribute classifiers}
We can acquire attribute classifiers  on the frozen latent space by training using arbitrary datasets with annotations. Specifically, we encode the input text into the latent space with the VAE encoder, and then train the classifier to predict the attribute label given the latent vector. Each classifier, as is built on the semantic latent space, can be trained efficiently with only a small number of examples (e.g., 200 per class). This allows us to acquire a large diversity of classifiers (e.g., sentiment, formality, different keywords) in our experiments (\S\ref{sec:exp}) using readily-available data from different domains, and flexibly compose them together to perform operations on text in the domain of interest. 


\paragraph{Initialization of ODE sampling}

To sample  with the ODE solver (\S\ref{sec:ode_sampler}), we need to specify the initial . For text editing operations (e.g., transferring sentiment from positive to negative) that start with a given text sequence, we initialize  to the latent vector of the given text by the VAE encoder. We show in our experiments that the resulting  samples as the solution of the ODEs can preserve the relevant information in the original text while obtaining the desired target attributes.

For generating new text of target attributes, the normal way is to sample  from the prior Gaussian distribution . However, due to the inevitable gap between the prior distribution and the learned VAE posterior on , such a Gaussian noise sample does not always lead to coherent text outputs. We thus follow \citep{li-etal-2020-optimus,hu2021causal} to learn a small (single-layer) GAN \citep{goodfellow2014generative}  that simulates the VAE posterior distribution, using all encoded  of real text as the training data. We then generate the initial  from the .






\paragraph{Sample selection}
The compact latent space learned by VAE allows us to conveniently create multiple semantically-close variants of a sampled  and pick the best one in terms of certain task criteria. Specifically, we add random Gaussian noise perturbation (with a small variance) to  to get a set of vectors close to  in the latent space and select one from the set. We found the sample perturbation and selection is most useful for operations related to the text content. For example, in text editing (\S\ref{sec:tst_exp}), we pick a vector based on the content preservation (e.g., BLEU with the original text) and attribute accuracy. More details are provided in \S\ref{app:sample_selection}.









 
\section{Experiments}\label{sec:exp}
We conduct extensive experiments of composable text controls to show the flexibility and efficiency of \textsc{LatentOps}, including generating new text of compositional attributes (\S\ref{sec:cg_exp}) and editing existing text in terms of desired attributes sequentially or simultaneously (\S\ref{sec:tst_exp}). 
All code will be released upon acceptance.
\begin{table}[t]
\vspace{-15pt}
\setlength\tabcolsep{3.4pt}
\scriptsize
\centering
\begin{tabular}{llcccccc}
\toprule
\multirow{3}{*}{Attributes} &\multirow{3}{*}{Methods} & \multicolumn{4}{c}{Accuracy} &  Fluency & Diversity\\\cmidrule(r){3-6}\cmidrule(r){7-7}\cmidrule(r){8-8}
& &S & T & F & G-M &PPL & sBL    \\
\midrule
\multirow{5}{*}{S}
                 & GPT2-FT& 0.98 & -&-&0.98 & 10.6 & 23.8\\\cmidrule{2-8}
                 & PPLM &0.86&-&-&0.86&11.8&31.0 \\
                & FUDGE&0.77&-&-&0.77&\textbf{10.3}&27.2\\
                 & Ours &\textbf{0.99} &-&-&\textbf{0.99} & 30.4 & \textbf{13.0} \\
                 \midrule
\multirow{5}{*}{S+T} 
                 & GPT2-FT   &0.98&0.95&-&0.969&9.0&36.8 \\\cmidrule{2-8}
                 & PPLM&0.81&0.59&-&0.677&15.7&28.7 \\
                & FUDGE&0.67&0.63&-&0.565&\textbf{11.0}&35.9\\
                 & Ours &\textbf{0.98}& \textbf{0.93}& -&\textbf{0.951}& 25.2& \textbf{19.7}\\
                 \midrule
\multirow{5}{*}{S+T+F} 
                 & GPT2-FT  &0.97&0.92&0.87&0.919&10.3&36.8 \\\cmidrule{2-8}
                 & PPLM &0.82&0.57&0.56&0.598&17.5&30.5  \\
                &FUDGE&0.67&0.64&0.62&0.556&\textbf{11.5}&35.9\\
                 & Ours &\textbf{0.97}& \textbf{0.92}& \textbf{0.93}       &\textbf{0.937}   & 25.8            & \textbf{21.1}\\
                 \midrule
\end{tabular}
\caption{Results of generation with compositional attributes. S, T and F stand for sentiment, tense and formality, respectively.  G-M is the geometric mean of all accuracy. 
  For reference, the PPL of test data and human-annotated data is 15.9 and 24.5. Since GPT2-FT is a fully-supervised model for reference, we mark the best result \textbf{bold} except GPT2-FT.
}
\label{tab:multi_cg}
\vspace{-10pt}
\end{table}
\paragraph{Setup}
We evaluate in two domains, including the Yelp review  \cite{DBLP:conf/nips/ShenLBJ17} preprocessed by \citet{DBLP:conf/naacl/LiJHL18} and the Amazon comment corpus \cite{DBLP:conf/www/HeM16}. 
For each domain, we quickly adapt the GPT2-large to equip with a latent space as described in \S\ref{sec:vae_training}. 
The resulting VAE models then serve as the base model, on which we plug in various attribute classifiers for generation and editing. 
We consider the attributes of \emph{sentiment} (positive, negative), \emph{formality} (formal, informal), and \emph{tense} (pase, present, future). 
(We also study other attributes related to diverse \emph{keywords}, which we present in \S\ref{app:generation_keyword}).
The sentiment/tense classifiers are quickly acquired by training on a small subset of Yelp and Amazon instances (200 labels per class), where the sentiment labels were readily available in the corpus and the tense labels are automatically parsed (\S\ref{app:setup}). 
There is no formality information in the Yelp/Amazon corpora, yet the flexibility of \textsc{LatentOps} allows us to acquire the formality classifier using a separate dataset GYAFC \cite{DBLP:conf/naacl/RaoT18}. 
\S\ref{app:setup} gives more details of the setup.

\subsection{Generation with Compositional Attributes}
\label{sec:cg_exp}

We apply \textsc{LatentOps} to generate new text of arbitrary desired attributes on Yelp domain.










\paragraph{Baselines}
We compare with the previous plug-and-play text control approaches {\bf PPLM}~\cite{Dathathri2020Plug} and {\bf FUDGE}~\cite{DBLP:journals/corr/abs-2104-05218}. 
As mentioned earlier, both approaches apply attribute classifiers on the complex sequence space, with an autoregressive LM as a base model. 
We obtain the base model by finetuning GPT2-large on the above domain corpus (e.g., Yelp). We further compare with an expensive supervised method {\bf GPT2-FT} which finetunes a GPT2-large for \emph{each} combination of attributes. 
To get the supervised data (\S\ref{app:generation_baselines}), we automatically annotate the domain corpus for formality and tense labels with a trained classifier and tagger, respectively. 
\paragraph{Metrics}
Attribute accuracy is given by a BERT classifier to evaluate the success rate.
Perplexity (PPL) is calculated by a GPT2 finetuned on the corresponding domain to measure fluency. 
We calculate self-BLEU (sBL) to evaluate the diversity.
For each case, we sample 150 sequencs to evaluate. 

\subsubsection{Experimental Results}
\label{sec:generation_composable}
We list the average results of each combination in Table~\ref{tab:multi_cg}. \textsc{LatentOps} achieves observably higher accuracy and diversity, even compared with the fully-supervised method (i.e., GPT2-FT). For fluency, the perplexity of our \textsc{LatentOps} is within a regular interval (the perplexity of human-annotated data is 24.5). However, the baselines obtain excessive perplexity at the expense of diversity.
\begin{table}[t]
\centering
\scriptsize
\vspace{-15pt}
\begin{tabular}{m{0.45\textwidth}}
\toprule
\textbf{Negative + Future + Formal}\\
\midrule
GPT2-FT: \\
	\quad i will not be back.\\
	\quad would not recommend this location to anyone. \senc{[No Subject]}\\
	\quad would not recommend them for any jewelry or service. \senc{[No Subject]}\\
	\quad if i could give this place zero stars, i would.\\
	\midrule

PPLM:\\
	\quad i \senc{could} not recommend them at all.\\
	\quad i \senc{could not} believe this \senc{was not good}!\\
	\quad this \senc{was a big deal}, because the food \senc{was great}. \\
	\quad i \senc{could} not recommend them.\\
	\midrule

FUDGE:\\
    \quad not a great pizza to get a great pie! \senc{[No Tense]}\\
    \quad however, this place \senc{is pretty good}. \\
    \quad i \senc{have never} seen anything like these. \\
    \quad will definitely return. \senc{[No Subject]}\\
	\midrule

Ours:\\
	\quad i would not believe them to stay .\\
	\quad i will never be back .\\
	\quad i would not recommend her to anyone in the network .\\
	\quad they will not think to contact me for any reason .\\
	\bottomrule
\end{tabular}
\caption{Examples of generation with compositional attributes. 
We mark failed spans in \senc{red}.
}
\label{tab:examples_compositional}
\vspace{-10pt}
\end{table}

Table~\ref{tab:examples_compositional} shows some generated samples. Ours yields fluent sentences that mostly satisfy the controls. Moreover, GPT2-FT performs similar, although it misses the subject in the second and the third examples. PPLM may fail due to the lack of global concern, e.g., the double negation leads to positive sentiment in the second example. Both PPLM and FUDGE could hardly succeed in all the controls simultaneously since it operates on the sequence space of an autoregressive LM, which is arduous to coordinate the controls.
Refer to \S\ref{app:generation_compositional} for more generated examples and analysis.




\begin{comment}
\subsubsection{Generation with Compositional Attributes and Keyword}
\label{sec:generation_keyword}
\hzt{Move this experiment completely to appendix.}
We regard keyword as an attribute of the text sequence. To prepare the data, we extract all verbs, nouns and the variants appeared in the Yelp review dataset, filter out the sentiment-related words, and construct the training data (Appendix~\ref{app:generation_keyword}).

We conduct the experiments of both single keyword and keyword combining with other attributes (sentiment and tense). 


We first give the automatic evaluation results in Table~\ref{tab:lexical_cg}. We list the average results of each combination of keywords, sentiment and tense. All success rate, diversity and fluency are at a high level. To make the results more intuitive, we also give some generated examples shown in Table~\ref{tab:lexical_1} in Appendix~\ref{app:generation_keyword}.
\begin{table}[ht]
\setlength\tabcolsep{3.1pt}
\scriptsize
\centering
\vspace{-8pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Attributes}& \multicolumn{4}{c}{Accuracy} &  Fluency & Diversity\\
\cmidrule(r){2-5}\cmidrule(r){6-6}\cmidrule(r){7-7}
&Keyword & Sentiment & Tense & G-M &PPL & sBL    \\
\midrule
{Keyword}  &{0.98} &-&-&{0.98} & 21.7 & {10.6} \\
                 \cmidrule{2-7}
{+ Sentiment}   &{0.94}& {0.96}& -& {0.95}& 21.3& {10.8}\\
                 \cmidrule{2-7}
{\ \ + Tense}   &{0.93}& {0.9}& {0.93}  &{0.92}   & 19.7            & 10.9\\
                 \bottomrule
\end{tabular}
\vspace{-5pt}
\caption{Results of generation with compositional attributes and keyword. 
}
\label{tab:lexical_cg}
\vspace{-13pt}
\end{table}
\end{comment}

\subsubsection{Runtime Efficiency}
\label{sec:runtime}
To quantify the computational cost of each method, we evaluate the consumed time for generating 150 examples.
We start timing after the models are loaded and before the generation starts. And we end timing right after 150 sentences are generated. We run five times for each method and average the results as final results, shown in Table~\ref{tab:time_comsumed}. Since we sample in the low-dimensional compact latent space, our method is 6.6 faster than FUDGE and 578 faster than PPLM. 
\begin{table}[t]
\small
    \centering
    \vspace{-20pt}
    \begin{tabular}{cccc}
    \toprule
    Methods & PPLM & FUDGE & Ours \\\midrule
    Time (s) & 3182 (578) & 36.1 (6.6) & 5.5 (1)\\\bottomrule
    \end{tabular}
\caption{Results of generation time of each method.}
    \label{tab:time_comsumed}
    \vspace{-10pt}
\end{table}

\subsection{Text Editing}
\label{sec:tst_exp}
We evaluate our model's text editing ability on both Yelp and Amazon domains, i.e, changing sentences' sentiment, tense and formality attributes sequentially (\S\ref{sec:sequential_edit}) or altogether (\S\ref{sec:text_edit_compositional}). 

\paragraph{Baselines}
Since few previous works can handle the sequential and compositional attributes editing task, we mainly compare with  FUDGE~\cite{DBLP:journals/corr/abs-2104-05218}. 
Moreover, we train three Style Transformer~\cite{DBLP:conf/acl/DaiLQH19} models (for sentiment, tense, and formality, respectively) to sequentially edit the source sentences as a baseline of sequential editing. 
To show the superiority of our \textsc{LatentOps}, we also conduct text editing with single attribute and compare with several recent state-of-the-art methods (\S\ref{app:baseline_text_edit}).
We adopt the same setting (few-shot) as in \S\ref{sec:cg_exp} for FUDGE and our \textsc{LatentOps}.
It is noteworthy that \textsc{LatentOps} is precisely the same model as in \S\ref{sec:cg_exp}, so it does not require further training. 

\paragraph{Metrics}
Besides success rate and fluency mentioned in \S\ref{sec:cg_exp}, we evaluate the ability of content preservation. Since it is a critical measure lying in the field of text editing,
we utilize two metrics: input-BLEU (iBL, BLEU between input and output)
and CTC score~\cite{ctc_score} (bi-directional information alignment between input and output).
For single attribute setting, we also evaluate reference-BLEU (rBL, BLEU between human-annotated ground truth and output) and perform human evaluations (\S\ref{app:example_text_edit_single}). 


\subsubsection{Sequential Editing}
\label{sec:sequential_edit}
In this section, we give the results of sequential editing, whose goal is to edit the given text by changing an attribute each time and keep the main content consistent. We consider the situation that source sentences are with formal manner, positive sentiment and present tense (selected by external classifiers in Yelp), and the goal is to transfer the source sentences to informal manner, negative sentiment and past tense, separately and sequentially. Potential entanglements exist among these attributes, and it is hard to control each attribute independently.

 The automatic evaluation results are listed in Table~\ref{tab:seq_edit_auto}. 
\textsc{LatentOps} performs the best on acquiring desired controls and maintaining others and achieves a balanced trade-off among accuracy, content alignment, and fluency. 
FUDGE fails to introduce the informal manner, while it achieves better formality controls after introducing negative sentiment, showing its deficiency of ability of disentanglement. Furthermore, although FUDGE preserves the most content, it mistakes the core and puts the cart (content) before the horse (accuracy). STrans performs plain overall and cannot guarantee fluency well. 

\begin{table}[t]
    \centering
    \scriptsize
    \vspace{-20pt}
    \setlength\tabcolsep{4.5pt}
    \begin{tabular}{llcccccc}
    \toprule
    \multirow{3}{*}{Attributes}&\multirow{3}{*}{Methods}&\multicolumn{3}{c}{Accuracy}&\multicolumn{2}{c}{Content}&Fluency\\ \cmidrule(r){3-5}  \cmidrule(r){6-7}  \cmidrule(r){8-8} 
     &    & F & S & T & iBL & CTC  &PPL\\\midrule
        \multirow{3}{*}{Informal}& FUDGE & 0.04& 0.06& 0.0 &\textbf{99.4}&0.479&\textbf{19.3}  \\
        & STrans & 0.45      & 0.14      & {0.06}  & {65.4}  & 0.470& 36.0 \\
    & Ours   & \textbf{0.85}      & {0.07}      & 0.07  & 64.2  & \textbf{0.482}& {20.2} \\\midrule
    \multirow{3}{*}{+ Negative}&  FUDGE &   0.49    & 0.35     &  0.10 & \textbf{48.6} & 0.451 & {35.0} \\
    &STrans & 0.38      & 0.82      & 0.10   & {42.4} & 0.457 & 39.9 \\
    & Ours   & \textbf{0.75}      & \textbf{0.92}      & {0.07}  & 42.1 & \textbf{0.468}& \textbf{28.7}  \\\midrule
    \multirow{3}{*}{\ \ + Present} & FUDGE &  0.48    & 0.35      &  0.10 & \textbf{49.3} & {0.452}& \textbf{30.7}  \\
    & STrans & 0.36      & 0.81      & 0.50   & {25.6}  & 0.453& 45.4 \\
    & Ours   & \textbf{0.61}      & \textbf{0.83}      & \textbf{0.74}  & 20.7 & \textbf{0.461}&{31.5}\\\bottomrule
    \end{tabular}
\caption{Automatic evaluations of sequential editing on Yelp review dataset. F, S and T stand for the accuracy of formality (to informal), sentiment (to negative) and tense (to present), respectively.
    }
    \label{tab:seq_edit_auto}
    \vspace{-10pt}
\end{table}
We provide some examples in Table~\ref{tab:seq_edit_part}. The formality control of FUDGE makes no effect. Besides, FUDGE would introduce some irrelevant information, e.g., \textit{garlic pizza} and \textit{thing's}. A similar situation exists in STrans, e.g., \textit{ate} and \textit{korean food}.  
More examples and analysis are in \S\ref{app:example_seq_edit}.

\subsubsection{Text Editing with Compositional Attributes}
\label{sec:text_edit_compositional}
We give the results of text editing with compositional attributes on Yelp, aiming to edit attributes of sentiment and tense of the source sentences. 
The automatic evaluation results are listed in Table~\ref{tab:tst_multiple}. 
\textsc{LatentOps} achieves a higher success rate and content alignment (CTC). 
FUDGE performs better on iBL and worse on CTC. As demonstrated by \citet{ctc_score}, the two-way approach (CTC) is more effective and exhibits a higher correlation than single-directional alignment (e.g., BLEU), which is consistent with our observation: FUDGE prefers to generate long sentences that contain the spans in source (raise iBL), but it will also introduce irrelevant information (lower CTC). We give some examples in \S\ref{app:example_text_edit_compositional} to support the claim. 
\subsection{Ablation Study}
To clarify the advantage of sampling from ODE, we compare different sampling methods, including Stochastic Gradient Langevin Dynamics (SGLD) and Predictor-Corrector sampler with SDE in \S\ref{app:compare_sde}.


\begin{table}[t]
    \centering
    \vspace{-15pt}
    \setlength\tabcolsep{2.pt}
    \scriptsize
    \begin{tabular}{ll}
    \toprule
         Source & the flowers and prices were great . \\
         \midrule
           FUDGE:&\\
         + informal&the flowers and prices were great. \senc{[Formal]}\\
         \quad + negative& \senc{garlic pizza} and prices were \senc{great}.\\
         \quad\quad + present&\senc{garlic pizza} and prices \senc{were great}.\\
          STans:&\\
         + informal&the flowers and prices were great ?\\
         \quad+ negative&the \senc{ate} and prices were terrible ?\\
         \quad\quad+ present&the \senc{ate} and prices are terrible ?\\
          Ours:& \\
         + informal& and the flowers and prices were great ! \\
         \quad+ negative& and the flowers and prices were terrible !\\
         \quad\quad+ present& and the flowers and prices are terrible !\\\midrule
         Source & best korean food on this side of town .\\\midrule
         FUDGE:&\\
         + informal&best korean food on this side of town. \senc{[Formal]}\\
         \quad+ negative&\senc{thing's best} korean food on this side of town.\\
         \quad\quad+ present &\senc{thing's best} korean food on this side of town. \senc{[No Tense]}\\
          STans:&\\
         + informal&best korean food on this side of town \senc{korean food} . \senc{[Formal]}\\
         \quad+ negative&only korean food on this side of town \senc{korean food} .\\
         \quad\quad+ present&only korean food on this side of town \senc{korean food} . \senc{[No Tense]}\\
          Ours:& \\
          + informal&best korean food on this side of town ! \\
         \quad + negative& worst korean food on this side of town ! \\
          \quad\quad+ present& this is worst korean food on this side of town !\\\bottomrule
    \end{tabular}
\caption{Some examples of sequential editing. We mark failed spans in \senc{red}. }
    \label{tab:seq_edit_part}
    \vspace{-10pt}
\end{table}
 \section{Related Work}
Recent works on text generation can be divided into two categories. One generates desirable texts by directly modifying the text sequence space. The other operates on the latent space to obtain a representation that can be decoded into sequence with desired attributes. More detailed discussions can be found in Section \S\ref{app:lace}.
\subsection{Text Control in Sequence Space}
Pretrained LM has shown tremendous success in text generation, and many have studied large autoregressive LMs such as GPT-2 on conditional generation by performing operations on the sequence space of the language models. For example, \citet{Dathathri2020Plug} proposes a plug-and-play framework that utilizes gradients of attribute classifiers to modify the hidden states of the pretrained LM at every step, named PPLM. 
FUDGE~\cite{DBLP:journals/corr/abs-2104-05218} follows a similar architecture but incorporates classifiers that predict the conditional probability of a complete sentence given prefixes to adjust the vocabulary probability distribution given by LM. Differing from these two approaches with left-to-right decoding, MUCOCO~\cite{DBLP:conf/nips/KumarMST21} formulates the decoding process as a multi-objective continuous optimization that combines loss of pretrained LM and attributes classifiers. The optimization gradient is applied directly to the soft representation consisting of each token's vocabulary distribution. COLD~\cite{QinCOLD} adopts the exact soft representation but uses an energy-based model with attribute constraints and Langevin Dynamics to sample. 
\begin{table}[t]
\setlength\tabcolsep{5.5pt}
    \scriptsize
    \centering
    \vspace{-15pt}
    \begin{tabular}{lccccc}
    \toprule
    \multirow{3}{*}{Methods} & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Content}  & Fluency\\\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-6}
    & Sentiment & Tense & iBL  &  CTC &PPL   \\
    \midrule
     FUDGE & 0.36  & 0.56  & \textbf{56.5}  & 0.450 & \textbf{17.3}\\
    Ours  & \textbf{0.95}  & \textbf{0.95}  & {37.1}& \textbf{0.465} & 30.1 \\\bottomrule
    \end{tabular}
\caption{Automatic evaluation results of text editing with compositional attributes on Yelp review dataset.}
    \label{tab:tst_multiple}
    \vspace{-10pt}
\end{table}
\subsection{Text Control in Latent Space}
Another common approach to control text generation is modifying text representation in the latent space. Some methods~\cite{DBLP:conf/icml/MuellerGJ17, DBLP:conf/aaai/LiuFZPL20} utilize a VAE to encode the input sequence into  in the latent space and then use attribute networks that are jointly trained with the VAE to obtain  that can be decoded into the desired sequence. 
PPVAE~\cite{DBLP:conf/acl/DuanXPHL20} uses an unconditional Pre-train VAE and a conditional Plugin-VAE to achieve the goal.
Plug and Play \cite{mai-etal-2020-plug} follows a similar framework but replaces the VAE with an Auto-encoder and the Plugin-VAE with an MLP to obtain a desired vector . Some methods use an attribute classifier to edit the latent representation  with Fast-Gradient-Iterative-Modification \cite{DBLP:conf/nips/WangH019}. Because of the recent success of diffusion models, LDEBM \cite{yulatent} proposes a diffusion process in the latent space whose reverse process is constructed with a sequence of EBMs for text generation. 













%
 \section{Conclusions}
We have developed a new efficient approach that performs composable control operations in the compact latent space of text, named \textsc{LatentOps}. 
The proposed method permits combining arbitrary operators applied on a latent vector, resulting in an energy-based distribution on the low-dimensional continuous latent space.
We develop an efficient and robust sampler based on ODEs that effectively samples from the distribution guided by gradients.
We connect the latent space to popular pretrained LM by efficient adaptation without finetuning the whole model.
We showcase its compositionality, flexibility and firm performance on several distinct tasks.
In future work, we can explore the control of more complicated texts.
 \section*{Ethical Considerations}
The contributions of this paper mostly focus around the fundamental challenges in designing an efficient approach for composable text operations in the compact latent space of text, and the proposed method is examined on commonly used public datasets. This work has applications in conditional text generation, text style transfer, data augmentation, and few-shot learning.

VAEs, the framework of our latent model, are trained to mimic the training data distribution, and , bias introduced in data collection will make VAEs generate samples with a similar bias. Additional bias could be introduced during model design or training. However, such techniques could be misused to produce fake or misleading information, and researchers should be aware of these risks and explore the techniques responsibly.
\section*{Limitations}
The primary focus of this paper is the analysis of single sentences. Our objective has been to deeply understand the potential of the proposed method, with a particular emphasis on its controllability and compositionality. Analyzing single sentences offers a relatively controlled setting, making it easier to derive clear insights and manage data complexities.

However, it's important to recognize that our findings, while based on single sentences, may not directly translate to longer textual content. Lengthier texts bring with them complex structures, dependencies, and nuanced contexts that might affect the performance of our methods. Adapting to these challenges may require further refinements.



Considering the scope of our current investigation, there exists significant opportunity for future research. This includes not only adapting our methodology to handle more intricate textual scenarios but also contrasting its performance with other potential approaches. Such explorations remain promising directions for forthcoming studies.
%
 \bibliography{anthology}
\bibliographystyle{acl_natbib}


\clearpage
\appendix

\onecolumn
\section{Derivation of ODE Formulation}
\label{app:derivation_ode}
\newcommand{\td}{\text d}
\newcommand{\gxt}{\bm{G}(\bm{x},t)}
\newcommand{\bmx}{\bm x}
\newcommand{\fxt}{\bm f(\bm x,t)}
\newcommand{\barw}{\Bar{\bm w}}
\subsection{General Form}
Let's consider the general diffusion process defined by SDEs in the following form (see more details in Appendix A and D.1 of \citet{DBLP:conf/iclr/0011SKKEP21}):

where  and . The corresponding reverse-time SDE is derived by~\citet{anderson1982reverse}:

where we refer  for a matrix-valued function , and  is the Jacobian matrix of .
Then the ODE corresponding to Eq.~\ref{eq:general_forward_sde} has the following form:

\subsection{Derivation of Our ODE}
In this work, we adopt the Variance Preserving (VP) SDE~\cite{DBLP:conf/iclr/0011SKKEP21} to define the forward diffusion process:

where the coefficient functions of Eq.~\ref{eq:general_forward_sde} are  and , independent of . Following Eq.~\ref{eq:general_reverse_sde}, the corresponding reverse-time SDE is derived as:

which infers to the Eq.~\ref{eq:sde}. Then, we derive the deterministic process (ODE) on the basis of Eq.~\ref{eq:general_ode}:

which gives the derivation of Eq.~\ref{eq:ode_x}.
\newpage
\section{Evaluation of Sample Selection Strategy}
\label{app:sample_selection}
As we stated in \S\ref{sec:implement}, we adopt a sample selection strategy for content-related generation tasks (text editing and generation with keywords).
Previous works also have similar strategies to improve the generation quality (i.e., PPLM~\cite{Dathathri2020Plug} and FUDGE~\cite{DBLP:journals/corr/abs-2104-05218}).

Since our latent model is trained by VAE objective, a sample  corresponds to a distribution  in . Thus, we can search for better output by expanding the search space through sampling , where , and pick the best. 
Specifically, from ODE sampling,  acts as the mean, and the variance  is predefined. We generate  by sampling  from standard Gaussian:


We decode each  and pick the best one according to the criterion of the task. We prefer the output that conforms to the desired attribute and achieves a high BLEU score with the source text for the text editing task. We want the output that contains the desired keyword or its variants for the generation with keywords. 

In our experiments (text editing and generation with keywords), we set  as the default. To better demonstrate the strategy's improvement, we provide the quantitative and qualitative results towards different .

We follow the same setting of text editing with single attribute on Yelp (\S\ref{app:example_text_edit_single}). The automatic evaluation results are shown in Table~\ref{tab:sample_selection}. As  increases, all the metrics get improved. To reflect the trend of change in accuracy and content preservation, we plot Figure~\ref{fig:sample_selection}, which indicates that large  gives better accuracy and better input-BLEU.


\begin{figure}[ht]
\begin{minipage}[ht]{0.5\textwidth}
\centering
\small
\begin{tabular}{cccccc}
\toprule


\multirow{2}{*}{}&Accuracy&\multicolumn{3}{c}{Content}&Fluency\\\cmidrule(r){2-2}\cmidrule(r){3-5}\cmidrule(r){6-6}
& Sentiment & iBL & rBL & CTC & PPL\\\midrule
2&0.75&51.1&21.4&0.4737&26.3\\
4&0.82&50.6&22.0&0.4729&26.7\\
6&0.89&49.6&22.3&0.4729&26.2\\
8&0.9&50.5&22.2&0.4732&25.9\\
10&0.92&50.8&23.1&0.4730&26.2\\
12&0.93&51.4&23.2&0.4733&26.1\\
14&\underline{0.94}&51.4&23.0&0.4732&26.9\\
16&\underline{0.94}&52.4&23.4&0.4737&\underline{25.9}\\
18&\textbf{0.95}&\underline{52.6}&\underline{23.6}&\underline{0.4739}&\textbf{25.8}\\
20&\textbf{0.95}&\textbf{54.0}&2\textbf{4.2}&\textbf{0.4743}&\underline{25.9}\\
\bottomrule
\end{tabular}
  \captionof{table}{Automatic evaluation results towards to different  on Yelp review dataset. 
  We mark the best \textbf{bold} and the second best \underline{underline}.}
  \label{tab:sample_selection}
\end{minipage}
\hfill
\begin{minipage}[ht]{0.48\textwidth}
\centering
\includegraphics[width=0.9\textwidth,page=1]{fig/sample_selection_fig.pdf}
\captionof{figure}{The trend of change of accuracy and input-BLEU as  increases. The digit below each data point represents the corresponding .}
\label{fig:sample_selection}
\end{minipage}
\end{figure}
We also provide some examples in Table~\ref{tab:exmaple_sample_selection}. One observation is that all the outputs from the same source sequence describe similar scenarios but slightly differ in expression.
Thus, we can select the most suitable expression based on predefined rules. 

\begin{table}[ht]
\centering
    \footnotesize
    \vspace{-19pt}
    \begin{tabular}{l|l}
    \toprule
Source&there is definitely not enough room in that part of the venue .\\
Target&there is so much room in that part of the venue\\\midrule
&there is definitely plenty of room in that perfect location .\\
&there is definitely no room enough in that venue to be the best part .\\
&there is definitely plenty of room right in that venue .\\
&there is definitely plenty of room right in the venue that needs .\\
&there is definitely plenty of room right in that venue .\\
&there is definitely enough room that can be right in the venue .\\
&there is definitely nothing better in room for that type of venue .\\
&there is definitely plenty of room in the right venue for that level .\\
&there is definitely nothing better in that room style of place .\\
&there is definitely a good room inside that best of all need in space .\\
&there is definitely plenty of room in the right level that is appropriate .\\
&there is definitely enough room in that right part of the venue .\\
&there is definitely plenty of room right in the deck that is needed .\\
&there is definitely enough room in that good atmosphere .\\
&there is definitely plenty of room in the right area , which is comfortable .\\
&there is definitely plenty of room in that perfect state of the place .\\
&there is definitely plenty of room that ideal in the location .\\
&there is definitely enough room in that perfect venue to all .\\
&there is definitely plenty of room in the right venue as well .\\
&there is definitely plenty of room available in the overall venue , too .\\
\midrule[1pt]
Source&it is n't terrible , but it is n't very good either .\\
Target&it is n't perfect , but it is very good .\\\midrule
&it is n't terrible , but it is very good also !\\
&it is very good , but it does n't even look great !\\
&it is n't terrible , but it is very good and definitely is good !\\
&it is n't great , but it is definitely very good !\\
&it is n't terrible , it is good and the menu is definitely great !\\
&it is n't terrible , but it is n't very good either .\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is very good also !\\
&it is n't terrible , but it is definitely very good !\\
&it is very good , and it is n't terrible either .\\
&it is n't terrible , but it is very good and well made !\\
&it is very good , but it 's not really great either .\\
&it is n't terrible , but it is very good and well worth it .\\
&it is n't terrible , but it is definitely very good and good !\\
&it is n't terrible , but it is very good also !\\
&it is n't terrible , but it is very good and definitely is great !\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is n't very good either .\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is very good and always great !\\
\midrule[1pt]
Source&the food was pretty bad , i would not go there again .\\
Target&the food was great, i would go there again.\\\midrule
&he food was pretty good , i would go there again .\\
&the food was pretty good , i would def go there again !\\
&the food was pretty good , i would go again !\\
&the food was pretty good , i would go there again !\\
&the food was pretty good , i would definitely go there again .\\
&the food was pretty good , i would go back there again .\\
&the food was pretty good , i would definitely go back again .\\
&the food was pretty good , i would definitely go there again !\\
&the food was pretty good , i would definitely go there again .\\
&the food was pretty good , i would always go there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would not go there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would go back there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would definitely go there again !\\
&the food was pretty good , i would not go there again .\\
&the food was pretty good , i would definitely go there again .\\
&the food was pretty good , i would definitely go back again .\\
&the food was pretty good , i would go here again .\\
\bottomrule
    \end{tabular}
    \vspace{-7pt}
    \caption{Examples of sample selection strategy (). 
}
    \label{tab:exmaple_sample_selection}
\end{table}
\clearpage
\section{Distinguishing from Other Works}
\label{app:lace}
In this section, we outline the foundational and methodological differences that set \textsc{LatentOps} apart from models like LACE~\citep{nie2021controllable}.
The underlying motivation for our approach is fundamentally different. Text, unlike images, is characterized by discrete values and varying lengths, making it inherently more challenging to model. Given these complexities, there are only a handful of works exploring text operations within a condensed latent space. If we can fully comprehend this text latent space, we can align various textual tasks, such as generation and text editing, with operations in the latent domain.

LACE~\citep{nie2021controllable}, for instance, builds its foundation on pre-trained GANs. In order to train their classifiers, class labels for latent vectors are essential. This necessitates the use of external classifiers to retrieve the class labels of the latent vectors, with human intervention required to filter out subpar samples. Our approach, on the other hand, effectively adapts large pre-trained LMs to the VAE framework. For specific datasets, the VAE is not only efficient but also expeditious in its training. Thanks to the bi-directional mapping between the latent space and text space, we can directly train the classifier using the marginal distribution.

Furthermore, LACE establishes a joint distribution in the image space and then transitions to the latent space using the reparameterization trick. Contrarily, our model defines its joint distribution directly within the text latent space.

Additionally, the architecture of LACE, built upon the GAN latent space, necessitates certain specialized regularization terms in its energy function for different tasks to optimize performance. Our model benefits from the more structured latent space provided by the VAE, allowing our energy function to remain straightforward and consistently aligned with our practical definitions.

\section{More Details and Results of Experiments}
In this section, we provide more details and results of the experiments (\S\ref{sec:exp}).
\subsection{Setup}
\label{app:setup}
The Yelp dataset and Amazon dataset contain 443K/4K/1K and 555K/2K/1K sentences as train/dev/test sets, respectively.
Since Yelp and Amazon datasets\footnote{\url{https://github.com/lijuncen/Sentiment-and-Style-Transfer}}\footnote{The datasets are distributed under CC BY-SA 4.0 license.} are mainly developed for sentiment usage, we annotate them with a POS tagger to get the tense attribute
to test the ability of our model that can be extended to an arbitrary number of attributes. 
Besides, we also use GYAFC dataset~\cite{DBLP:conf/naacl/RaoT18} to include the formality attribute. Note that the GYAFC dataset
has somewhat different domains from Yelp/Amazon, which can be used to test our model's out-of-domain generalization ability. All the datasets are in English.

We adopt BERT-small\footnote{The BERT model follows the Apache 2.0 License.} and GPT2-large\footnote{The GPT2 model follows the MIT License.}  as the encoder and decoder of our latent model, respectively. The training paradigm follows \S\ref{sec:implement}, and some training tricks~\cite{li-etal-2020-optimus} (i.e., cyclical schedule for KL weight and KL thresholding scheme) are applied to stabilize the training of the latent model. All the attributes are listed in Table~\ref{tab:operators}. 
All the models are trained and tested on a single Tesla V100 DGXS with 32 GB memory.
Input-BLEU, reference-BLEU and self-BLEU are implemented by nltk~\cite{DBLP:books/daglib/0022921} package. 



We employ a BERT classifier to determine attribute accuracy, serving as a metric for the evaluation of the success rate. More precisely, we finetune BERT-base models dedicated to classification tasks using the respective dataset. For instance, when evaluating sentiment, the classifier tailored for the Yelp dataset registers accuracies of 97.1\% and 97.3\% on the dev and test sets, respectively. Meanwhile, for the Amazon dataset, the sentiment classifier records accuracies of 86.9\% and 85.7\% on the dev and test sets.

In our experiments of generation with single attribute, we also incorporate the MAUVE metric~\citep{pillutla2021mauve}, an automatic measure of the gap between neural text and human text for text generation. In alignment with the official recommendations associated with MAUVE, we select a random subset of 10,000 sentences from the training set to serve as reference sentences.

For the operator (classifier) , we adopt a four-layer MLP as the network architecture as shown in Table~\ref{tab:arc_classifier}. Since the number of trainable parameters of the classifier is small, it is rapid to train and sample. 
\begin{table}[ht]
    \centering
\begin{tabular}{l|l |l}
    \toprule
        Style & Attributes  & Dataset\\\midrule
         Sentiment& Positive / Negative  &Yelp, Amazon\\Tense& Future / Present / Past & Yelp \\
Keywords& Existence / No Existence & Yelp\\Formality & Formal / Informal & GYAFC \\
\bottomrule
    \end{tabular}
    \caption{All attributes and the corresponding dataset are used in our experiments.} \label{tab:operators}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lllll}
        \toprule
        Input & Layer 1 & Layer 2 & Layer 3 & Layer 4\\\midrule
         &
         Linear 43, LeakyReLU&
         Linear 22, LeakyReLU&
         Linear 2, LeakyReLU&
         Linear \#logits\\\bottomrule
    \end{tabular}
    \caption{The architecture of the attribute classifier.}
    \label{tab:arc_classifier}
\end{table}
\paragraph{Observations on Scalability}
In our experiments with different encoder and decoder scales, several observations emerged. Firstly, while we evaluated various encoder models, including BERT, RoBERTa, and other pre-trained language models (PLMs) of different scales, the distinctions in performance were minimal. In this context, BERT-small proved sufficiently robust, serving as an effective encoder for the VAE framework. Secondly, the scale of the decoder was observed to significantly influence performance. We examined a spectrum of models, ranging from GPT2-base to GPT2-xl. Through these tests, GPT2-base and GPT2-large emerged as the optimal choices, providing a harmonious blend of performance results and computational efficiency.
\paragraph{Stability of VAE training}
The stability of VAE training has benefited from recent innovations, as evidenced by works like \citet{li-etal-2020-optimus}. Our empirical observations, corroborated by subsequent research such as \citet{hu2021causal}, attest to these advancements. In addition, our approach's constrained parameter training further enhances this stability. As a result, training the VAE has become less of a challenge or bottleneck than before.
\subsection{Generation with Compositional Attributes}
The section is a supplement of \S\ref{sec:cg_exp}, we give more details of experimental configuration, generated examples and discussion.
\subsubsection{More Details of Baselines}
\label{app:generation_baselines}
We compare our method with PPLM~\cite{Dathathri2020Plug}, FUDGE~\cite{DBLP:journals/corr/abs-2104-05218}, and a finetuned GPT2-large~\cite{gpt2}. PPLM and FUDGE are plug-and-play controllable generation approaches on top of an autoregressive LM as the base model. For fair comparison (\S\ref{sec:vae_training}), we obtain the base model by finetuning the embedding layer and the first transformer layer of pretrained GPT2-large on the Yelp review dataset with unlabeled data. All the classifiers/discriminators of PPLM, FUDGE and our \textsc{LatentOps} are trained by a small subset of the original dataset (200 labeled data instances per class). 
\paragraph{PPLM}  requires a discriminator attribute model (or bag-of-words attribute models) learned from a pretrained LM's top-level hidden layer. At decoding, PPLM modifies the states toward the increasing probability of the desired attribute via gradient ascent. We only consider the discriminator attribute model, which is consistent with other baselines and ours. We follow the default setting of PPLM, and for each attribute, we train a single layer MLP as the discriminator.  
\paragraph{FUDGE} has a discriminator that takes in a prefix sequence and predicts whether the generated sequence would meet the conditions. FUDGE could control text generation by directly modifying the probabilities of the pretrained LM by the discriminator output. 
We follow the architecture of FUDGE and train a discriminator for each attribute.
Furthermore, we tune the  parameter of FUDGE which is a weight that controls how much the probabilities of the pretrained LM are adjusted by the discriminator, and we find =10 yields the best results. We follow the default setting of FUDGE, and for each attribute, we train a three-layer LSTM followed by a Linear as the discriminator.
\paragraph{GPT2-FT} is a finetuned GPT2-large model that is a conditional language model, not plug-and-play. Specifically, we train an external classifier for the out-of-domain attribute (i.e., formality) to annotate all the data in Yelp. For tense, we use POS tagging to annotate the data automatically. Then we finetune the embedding layer and the first layer of GPT2-large by the labeled data. Since GPT2-FT is fully-supervised and not plug-and-play, it is not comparable with other baselines and ours, and we only use it for reference.
\subsubsection{More Discussion of Generation with Compositional Attributes}
\label{app:generation_compositional}
\paragraph{Discussion of Quantitative Results}
As we state in \S\ref{sec:generation_composable}, our method is superior to baselines. We want to discuss the results in Table~\ref{tab:multi_cg}.

For success rate, our method dramatically outperforms FUDGE and PPLM as expected since both control the text by modifying the outputs (hidden states and probabilities) of PLM, which includes the token-level feature and lacks the sentence-level semantic feature. On the contrary, our method controls the attributes by operating the sentence-level latent vector, which is more suitable. 

For diversity, since our method bilaterally connects the discrete data space with continuous latent space, which is more flexible to sample, ours gains obvious superiority in diversity. Conversely, PLMs like GPT2, which is the basis of PPLM and FUDGE, are naturally short of the ability to generate diverse texts. They generate diverse texts by adopting other decoding methods (like top-k), which results in the low diversity of the baselines. 

For fluency, we calculate the perplexity given by a finetuned GPT2, which processes the same architecture and training data of PPLM and FUDGE, so naturally, they can achieve better perplexity even compared to the perplexity of test data and human-annotated data. Moreover, our method only requires an Extra Adapter to guide the fixed GPT2, and our fluency is in a regular interval, a little higher than the perplexity of human-annotated data.

Since GPT2-FT is trained with full joint labels (all the data has all three attribute labels), it can achieve a reasonable success rate, and ours is comparable. Moreover, consistent with PPLM and FUDGE, GPT2-FT can achieve good perplexity but poor diversity due to the sampling method.

\paragraph{Discussion of Qualitative Results}
We provide some generated examples in Table~\ref{tab:cg_examples} to raise a more direct comparison. Consistent with the quantitative results, it is difficult for FUDGE to control all the desired attributes successfully, although GPT2-FT and ours perform well. For diversity, it is evident that FUDGE and GPT2-FT prefer to generate short sentences containing very little information. Some words appear highly, yet ours gives a more diverse description. Regarding fluency, since FUDGE and GPT2-FT tend to generate simple sentences, they can obtain better perplexity readily. However, ours is inclined to generate more informative sentences. In conclusion, there is a trade-off between diversity and fluency. It can be handled well by ours, but for the baselines, they pursue fluency too much and lose diversity.



















\begin{table}[ht]
\centering
\scriptsize
\vspace{-15pt}
 \setlength\tabcolsep{1.5pt}
\begin{tabular}{m{0.4\textwidth}m{0.4\textwidth}}
\toprule
\textbf{Positive + Present + Formal}&\textbf{Negative + Past + Inormal}  \\\midrule
 \multicolumn{1}{l}{GPT2-FT:}&\multicolumn{1}{l}{GPT2-FT:}\\
    \quad the staff is friendly and helpful.&\quad didn't bother with the food and just walked out.\\
    \quad i love it \senc{here}. \senc{[Informal]}&\quad just not a good place for me. \senc{[No Tense]}\\
    \quad this {is} the place to go for {traditional} chinese food.&\quad not a fan of this place. \senc{[No Tense]}\\
    \quad highly recommend them. \senc{[Informal]}&\quad just not good. \senc{[No Tense]}\\
    \quad the menu {is} small but {very nice}.&\quad horrible! \senc{[No Tense]}\\
    \quad it{'s} a {great} place.&\quad oh and the cake was way too salty.\\
    \quad i {highly {recommend}} this place.&\quad but we didn't even finish it.\\
    \midrule
 \multicolumn{1}{l}{PPLM:}&\multicolumn{1}{l}{PPLM:}\\
    \quad i love this store and the service is always friendly and courteous.&\quad i ordered delivery... what?\\
    \quad the staff was so friendly \senc{\&} helpful!\senc{[Informal]}&\quad \senc{great} service. \senc{[No Tense]}\\
    \quad the place is clean.&\quad this place was terrible!\\
    \quad the best french bakery i have ever been to in las vegas!&\quad the service was horrible horrible horrible!\\
    \quad this place \senc{was} a gem!&\quad i ordered the ribs and brisket tacos and it was very bland. \senc{[Formal]}\\
    \quad she does love to make suggestions and i appreciate that.&   \quad the staff was very apologetic \senc{and apologetic} and \senc{refund} my \\uparrow\downarrow\downarrow\uparrow\downarrow\downarrow\downarrow\uparrow\lambda\lambda\sim\uparrow\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\uparrow\downarrow\downarrow$ \\
\midrule
\multirow{3}{*}{Sentiment}
                 & SGLD &0.64 &-&-&0.64 &\textbf{2.0} & 96.6\\
                 & SDE & \underline{0.82} & -&-& \underline{0.82}& 63.8 & \textbf{6.3} \\
                 & ODE &\textbf{0.99} &-&-&\textbf{0.99} & \underline{30.4} & \underline{13.0} \\
                 \cmidrule{2-8}
\multirow{3}{*}{\quad + Tense} 
                 & SGLD &0.61 &\underline{0.68}&-& 0.644&\textbf{1.9} & 97.8\\
                 & SDE & \underline{0.79} & 0.61&-& \underline{0.692}& 60.6 & \textbf{6.8} \\
                 & ODE &\textbf{0.98}& \textbf{0.93}& -&\textbf{0.951}& \underline{25.2}& \underline{19.7}\\
                 \cmidrule{2-8}
\multirow{3}{*}{\quad +Formality} 
                 & SGLD &0.52 &0.44&\underline{0.82}& 0.573&\textbf{2.3} & 96.8\\
                 & SDE & \underline{0.77} & \underline{0.60}&0.67& \underline{0.675}& 62.5 & \textbf{6.7} \\
                 & ODE &\textbf{0.97}& \textbf{0.92}& \textbf{0.93}&\textbf{0.937}   & \underline{25.8}& \underline{21.1}\\
                 \midrule
\end{tabular}
\caption{Comparison of different sampling method.}
\label{tab:camparison_sde}
\end{table*}

SGLD could generate high quality sentences, but all the sentences contain the similar content, for example: "\texttt{awesome food is great as always !}", "\texttt{great food is awesome as always !}", "\texttt{great food is awesome and always good !}", "\texttt{great place for your haircut .}" and "\texttt{great place with typically no bacon .}". Therefore, it performs the worst in the perspective of diversity. Also, the success rate is at a low level because of the sensitivity and instability of LD (\S\ref{sec:bg_ebms}).

Contrary to SGLD, the SDE sampler cannot guarantee the fluency of the generated sentences, although diversity is good.

We also compute the generation time of different sampling methods as shown in Table~\ref{tab:time_comsumed_sampler}. Combining the automatic evaluation results, sampling by ODE sampler gives the best trade-off among various aspects.

\begin{table}[ht]
    \centering
    \begin{tabular}{cccc}
    \toprule
    Samplers & SGLD & SDE & Ours \\\midrule
    Time & 5.1s (0.93x) & 15.6s (2.85x) & 5.5s (1x)\\\bottomrule
    \end{tabular}
    \caption{Results of generation time of different samplers.}
    \label{tab:time_comsumed_sampler}
\end{table}

\begin{table}
\small
 \setlength\tabcolsep{3pt}
    \centering
    \begin{tabular}{p{0.05\linewidth}p{0.85\linewidth}}
    \toprule
    Initial & Keywords\\
    \midrule
    a & accommodate add afternoon agree airport ambiance ambience amount animal answer anyone anything apartment apologize apology appetizer appointment area arizona arrive art ask atmosphere attention attitude auto average avoid az \\
    \midrule
    b & baby back bacon bag bagel bakery bar bartender base bathroom bbq bean beat become bed beef beer begin believe bell bike bill birthday biscuit bit bite book bottle bowl box boy boyfriend bread breakfast bring brunch buck buffet building bun burger burrito business butter buy \\
    \midrule
    c & cab cafe cake call car card care carry case cash cashier center chain chair chance change charge charlotte check cheese chef chicken child chili chip chocolate choice choose city class cleaning close club cocktail coffee color combo come company condition consider contact continue cook cooky corn cost counter couple coupon course cover crab crave cream credit crew crispy crowd crust cup curry customer cut \\
    \midrule
    d & date daughter day deal dealership decide decor deli deliver delivery dentist department deserve desk dessert detail diner dining dinner dip discount dish do doctor dog dollar donut door downtown dress dressing drink drive driver drop \\
    \midrule
    e & eat egg employee enchilada end entree environment establishment evening event everyone everything expect expectation experience explain eye \\
    \midrule
    f & face facility fact family fan fee feel feeling felt fill find finish fish fit fix flavor flight floor flower folk follow food foot forget friday friend front fruit fry furniture future \\
    \midrule
    g & game garden get gift girl give glass go god grab greet grill grocery ground group guess guest guy gym gyro \\
    \midrule
    h & hair haircut half hand handle happen have head hear heart help hit hold hole home homemade honey hope hospital hostess hotel hour house husband \\
    \midrule
    i & ice idea include ingredient inside item \\
    \midrule
    j & job joint juicy \\
    \midrule
    k & keep kid kind kitchen know \\
    \midrule
    l & lady leave let lettuce level life light line list listen live lobster location look lot lunch \\
    \midrule
    m & mac machine madison make mall man management manager manicure manner margarita mark market massage matter meal mean meat meatball medium meet melt member mention menu mile min mind mine minute mix mom money month morning mouth move movie mushroom music \\
    \midrule
    n & nail name need neighborhood night none noodle notch nothing notice number nurse \\
    \midrule
    o & occasion offer office oil ok okay omelet one onion online open opinion option orange order organize others overcook overprice own owner \\
    \midrule
    p & pack pad pancake park parking part party pass pasta patio pay pedicure people pepper person pet phoenix phone pick picture pie piece pittsburgh pizza place plan plate play please plenty point pool pork portion potato practice prepare price pricing process produce product provide purchase put \\
    \midrule
    q & quality question quick quote \\
    \midrule
    r & ranch rate rating read reason receive refill relax remember rent repair replace request reservation resort rest restaurant result return review rib rice ride ring rock roll room run rush \\
    \midrule
    s & salad sale salmon salon salsa salt salty sandwich saturday sauce sausage save saw say schedule school scottsdale seafood season seat seating section see seem selection sell send sense serve server service set share shoe shop shopping shot show shrimp side sign sit size slice soda someone something son sound soup space speak special spend spice spicy spinach sport spot spring staff stand standard star starbucks start state station stay steak step stick stock stop store story street strip stuff style stylist sub suggest summer sunday suppose surprise sushi \\
    \midrule
    t & table taco take talk taste tasty tea team tech tell thai thanks theater thing think throw time tip tire toast today tomato ton tonight topping tortilla touch town treat trip try tuna turn tv type \\
    \midrule
    u & understand update use \\
    \midrule
    v & valley value vega vegetable veggie vehicle venue vet vibe view visit \\
    \midrule
    w & waffle wait waiter waitress walk wall want wash watch water way wedding week weekend while wife window wine wing wish woman word worker world wrap write \\
\midrule
    y & year yelp yesterday yummy \\
\bottomrule
    \end{tabular}
    \caption{All keywords. Sort in alphabetical order.}
    \label{tab:all_keywords}
\end{table}
 


\end{document}
