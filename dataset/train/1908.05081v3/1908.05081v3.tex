
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color,xcolor}
\usepackage{amsmath,bm}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{makecell}
\usepackage{stfloats} 
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,bm}
\usepackage{setspace}
\usepackage{amsthm}
\newtheorem{defn}{Definition}
\usepackage{wrapfig}
\usepackage{miniplot}
\newtheorem{prop}{Proposition}

\title{AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models}



\author{Ke Sun \\
Zhejiang Lab\\
Key Lab. of Machine Perception (MoE), School of EECS, Peking University\\
\texttt{ajksunke@pku.edu.cn} 
\And
Zhanxing Zhu\textsuperscript{*} \\
Beijing Institute of Big Data Research, Beijing, China \\
\texttt{zhanxing.zhu@pku.edu.cn}\\
\AND
Zhouchen Lin\thanks{Corresponding author.} \\
Key Lab. of Machine Perception (MoE), School of EECS, Peking University \\
Pazhou Lab, Guangzhou, China\\
\texttt{zlin@pku.edu.cn} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle



\begin{abstract}
The design of deep graph models still remains to be investigated and the crucial part is how to explore and exploit the knowledge from different hops of neighbors in an efficient way. In this paper, we propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network; and the proposed graph convolutional network called AdaGCN~(Adaboosting Graph Convolutional Network) has the ability to efficiently extract knowledge from high-order neighbors of current nodes and then integrates knowledge from different hops of neighbors into the network in an Adaboost way. Different from other graph neural networks that directly stack many graph convolution layers, AdaGCN shares the same base neural network architecture among all ``layers'' and is recursively optimized, which is similar to an RNN. Besides, We also theoretically established the connection between AdaGCN and existing graph convolutional methods, presenting the benefits of our proposal. Finally, extensive experiments demonstrate the consistent state-of-the-art prediction performance on graphs across different label rates and the computational advantage of our approach AdaGCN~\footnote{Code is available at \url{https://github.com/datake/AdaGCN}.}.
\end{abstract}

\section{Introduction}
Recently, research related to learning on graph structural data has gained considerable attention in machine learning community. Graph neural networks~\citep{gori2005new, hamilton2017inductive, velivckovic2017graph}, particularly graph convolutional networks~\citep{kipf2016semi,defferrard2016convolutional,bruna2013spectral} have demonstrated their remarkable ability on node classification~\citep{kipf2016semi}, link prediction~\citep{zhu2016max} and clustering tasks~\citep{fortunato2010community}. Despite their enormous success, almost all of these models have \emph{shallow model architectures} with only two or three layers. The shallow design of GCN appears counterintuitive as deep versions of these models, in principle, have access to more information, but perform worse.  Oversmoothing~\citep{li2018deeper} has been proposed to explain why deep GCN fails, showing that by repeatedly applying Laplacian smoothing, GCN may mix the node features from different clusters and makes them indistinguishable. This also indicates that by stacking too many graph convolutional layers, the embedding of each node in GCN is inclined to converge to certain value~\citep{li2018deeper}, making it harder for classification. These shallow model  architectures restricted by oversmoothing issue limit their ability to extract the knowledge from high-order neighbors, \textit{i.e., features from remote hops of neighbors for current nodes}. Therefore, it is crucial to design deep graph models such that high-order information can be aggregated in an effective way for better predictions.

There are some works~\citep{xu2018representation, liao2019lanczosnet, klicpera2018predict, li2019can,liu2020towards} that tried to address this issue partially, and the discussion can refer to Appendix~\ref{appendix:deep graph}. By contrast, we argue that a key direction of constructing deep graph models lies in the efficient exploration and effective combination of information from different orders of neighbors. Due to the apparent \textit{sequential relationship} between different orders of neighbors, it is a natural choice to incorporate boosting algorithm into the design of deep graph models. As an important realization of boosting theory, AdaBoost~\citep{freund1999short} is extremely easy to implement and keeps competitive in terms of both practical performance and computational cost~\citep{hastie2009multi}. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision~\citep{huang2017learning} and AdaGAN~\citep{tolstikhin2017adagan} has already successfully incorporated boosting algorithm into the training of GAN~\citep{goodfellow2014generative}. 

In this work, we focus on incorporating AdaBoost into the design of deep graph convolutional networks in a non-trivial way. Firstly, in pursuit of the introduction of AdaBoost framework, we refine the type of graph convolutions and thus obtain a novel RNN-like GCN architecture called AdaGCN. Our approach can efficiently extract knowledge from different orders of neighbors and then combine these information in an AdaBoost manner \emph{with iterative updating of the node weights}. Also, we compare our AdaGCN with existing methods from the perspective of both architectural difference and feature representation power to show the benefits of our method. Finally, we conduct  extensive experiments to demonstrate the consistent state-of-the-art performance of our approach across different label rates and computational advantage over other alternatives. 

\section{Our Approach: AdaGCN}

\subsection{Establishment of AdaGCN} \label{approach_establish}
Consider an undirected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with $N$ nodes $v_i \in \mathcal{V}$, edges $(v_i, v_j) \in \mathcal{E}$. $A \in \mathbb{R}^{N \times N}$ is the adjacency matrix with corresponding degree matrix $D_{ii}=\sum_j A_{ij}$. In the vanilla GCN model~\citep{kipf2016semi} for semi-supervised node classification, the graph embedding of nodes with two convolutional layers is formulated as:
\begin{equation} 	\label{eq_GCN}
\begin{aligned} 
Z=\hat{A} \ \text{ReLU}(\hat{A}XW^{(0)}) W^{(1)}
\end{aligned} \end{equation}
where $Z \in \mathbb{R}^{N \times K}$ is the final embedding matrix~(output logits) of nodes before softmax and $K$ is the number of classes. $X \in \mathbb{R}^{N \times C}$ denotes the feature matrix where $C$ is the input dimension. $\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ where $\tilde{A}=A+I$ and $\tilde{D}$ is the degree matrix of $\tilde{A}$. In addition, $W^{(0)}\in \mathbb{R}^{C\times H}$ is the input-to-hidden weight matrix for a hidden layer with $H$ feature maps and $W^{(1)}\in \mathbb{R}^{H\times K}$ is the hidden-to-output weight matrix. 

Our key motivation of constructing deep graph models is to efficiently explore information of high-order neighbors and then combine these messages from different orders of neighbors in an AdaBoost way. Nevertheless, if we naively extract information from high-order neighbors based on GCN, we are faced with stacking $l$ layers' parameter matrix $W^{(i)}, i=0,...,l-1$, which is definitely costly in computation. Besides, Multi-Scale Deep Graph Convolutional Networks~\citep{luan2019break} also theoretically demonstrated that the output can only contain the stationary information of graph structure and loses all the local information in nodes for being smoothed if we simply deepen GCN. Intuitively, the desirable representation of node features does not necessarily need too many nonlinear transformation $f$ applied on them. This is simply due to the fact that the feature of each node is normally one-dimensional sparse vector rather than multi-dimensional data structures, e.g., images, that intuitively need deep convolution network to extract high-level representation for vision tasks. This insight has been empirically demonstrated in many recent works~\citep{wu2019simplifying, klicpera2018predict, xu2018powerful}, showing that a two-layer fully-connected neural networks is a better choice in the implementation. Similarly, our AdaGCN also follows this direction by choosing an appropriate $f$ in each layer rather than directly deepen GCN layers.

Thus, we propose to remove ReLU to avoid the expensive joint optimization of multiple parameter matrices. Similarly, Simplified Graph Convolution~(SGC)~\citep{wu2019simplifying} also adopted this practice, arguing that nonlinearity between GCN layers is not crucial and the majority of the benefits arises from local weighting of neighboring features. Then the simplified graph convolution is:
\begin{equation} \begin{aligned} \label{eq:SGC}
Z=\hat{A}^l X W^{(0)} W^{(1)} \cdots W^{(l-1)} = \hat{A}^lX \tilde{W},
\end{aligned} \end{equation}
where we collapse $W^{(0)} W^{(1)} \cdots W^{(l-1)}$ as $\tilde{W}$ and $\hat{A}^l$ denotes $\hat{A}$ to the $l$-th power. In particular, one crucial impact of ReLU in GCN is to accelerate the convergence of matrix multiplication since the ReLU is a contraction mapping intuitively. Thus, the removal of ReLU operation could also alleviate the oversmoothing issue, i.e. slowering the convergence of node embedding to indistinguishable ones~\citep{li2018deeper}. Additionally, without ReLU this simplified graph convolution is also able to avoid the aforementioned joint optimization over multiple parameter matrices, resulting in computational benefits. Nevertheless, we find that this type of stacked linear transformation from graph convolution has \textit{insufficient power in representing information of high-order neighbors}, which is revealed in our experiment described in Appendix~\ref{sec:AdaSGC}. Therefore, we propose to utilize an appropriate \textit{nonlinear function} $f_\theta$, e.g., a two-layer fully-connected neural network, to replace the linear transformation $\tilde{W}$ in Eq.~\ref{eq:SGC} and enhance the representation ability of \textit{each base classifier} in AdaGCN as follows:
\begin{equation} 
\begin{aligned} 
Z^{(l)}=f_{\theta}(\hat{A}^l X),
\end{aligned} 
\end{equation}
where $Z^{(l)}$ represents the final embedding matrix~(output logits before Softmax) after the $l$-th base classifier in AdaGCN. This formulation also implies that the $l$-th base classifier in AdaGCN is extracting knowledge from features of current nodes and their $l$-th hop of neighbors. Due to the fact that the function of $l$-th base classifier in AdaGCN is similar to that of the $l$-th layer in other traditional GCN-based methods that directly stack many graph convolutional layers, \emph{we regard the whole part of $l$-th base classifier as the $l$-th layers in AdaGCN}. As for the realization of Multi-class AdaBoost, we apply SAMME~(Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm~\citep{hastie2009multi}, a natural and clean multi-class extension of the two-class AdaBoost  adaptively combining weak classifiers. 

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.8\textwidth,trim=70 200 80 85,clip]{0network.pdf}
	\caption{The RNN-like architecture of AdaGCN with each base classifier $f_{\theta}^{(l)}$ sharing the same neural network architecture $f_{\theta}$. $w^l$ and $\theta_l$ denote node weights and parameters computed after the $l$-th base classifier, respectively.}
	\label{figure_network}
\end{figure}

As illustrated in Figure~\ref{figure_network}, we apply base classifier $f_{\theta}^{(l)}$ to extract knowledge from current node feature and $l$-th hop of neighbors by minimizing current weighted loss. Then we directly compute the weighted error rate $err^{(l)}$ and corresponding weight $\alpha^{(l)}$ of current base classifier $f_{\theta}^{(l)}$ as follows:
\begin{equation} 
\label{eq_err}
\begin{aligned} 
err^{(l)}&=\sum_{i=1}^{n} w_{i} \mathbb{I}\left(c_{i} \neq f_\theta^{(l)}\left(x_{i}\right)\right) / \sum_{i=1}^{n} w_{i}\\
\alpha^{(l)}&=\log \frac{1-err^{(l)}}{err^{(l)}}+\log (K-1),
\end{aligned} 
\end{equation}
where $w_i$ denotes the weight of $i$-th node and $c_i$ represents the category of current $i$-th node. To attain a positive $\alpha^{(l)}$, we only need $(1-err^{(l)})>1/K$, i.e., the accuracy of each weak classifier should be better than random guess~\citep{hastie2009multi}. This can be met easily to guarantee the weights to be updated in the right direction. Then we adjust nodes' weights by increasing weights on incorrectly classified ones:
\begin{equation} 
\begin{aligned}
w_{i} \leftarrow w_{i} \cdot \exp \left(\alpha^{(l)} \cdot \mathbb{I}\left(c_{i} \neq f_{\theta}^{(l)}\left(x_{i}\right)\right)\right), i=1, \ldots, n 
\end{aligned} 
\end{equation}	
After re-normalizing the weights, we then compute $\hat{A}^{l+1}X=\hat{A} \cdot (\hat{A}^l X)$ to sequentially extract knowledge from $l$+1-th hop of neighbors in the following base classifier $f_{\theta}^{(l+1)}$. One crucial point of AdaGCN is that different from traditional AdaBoost, we only define one $f_{\theta}$, e.g. a two-layer fully connected neural network, \emph{which in practice is recursively optimized in each base classifier just similar to a recurrent neural network}. This also indicates that the parameters from last base classifier are leveraged as the initialization of next base classifier, which coincides with our intuition that $l+1$-th hop of neighbors are directly connected from $l$-th hop of neighbors. The efficacy of this kind of layer-wise training has been similarly verified in~\citep{belilovsky2018greedy} recently. Further, we combine the predictions from different orders of neighbors in an Adaboost way to obtain the final prediction $C(A,X)$:
\begin{equation} 
\begin{aligned}
C(A, X)=\arg \max _{k} \sum_{l=0}^{L} \alpha^{(l)} f_\theta^{(l)}(\hat{A}^{l}X)
\end{aligned} 
\end{equation}
Finally, we obtain the concise form of AdaGCN in the following:
\begin{equation} 
\label{eq_AdaGCN}
\begin{aligned} 
&\hat{A}^{l}X=\hat{A} \cdot (\hat{A}^{l-1}X)\\
&Z^{(l)}=f_{\theta}^{(l)}(\hat{A}^l X)\\
&      Z=\text{AdaBoost}(Z^{(l)})
\end{aligned} 
\end{equation}

Note that $f_\theta$ is non-linear, rather than linear in SGC~\citep{wu2019simplifying}, to guarantee the representation power. As shown in Figure~\ref{figure_network}, the architecture of AdaGCN is a variant of RNN with synchronous sequence input and output. \textit{Although the same classifier architecture is adopted for $f_{\theta}^{(l)}$, their parameters are different, which is different from vanilla RNN}. We provide a detailed description of the our algorithm in Section~\ref{alg:algorithm_AdaGCN}.		

\subsection{Comparison with Existing Methods}
\begin{wrapfigure}[16]{r}{0.52\textwidth}
	\includegraphics[width=0.5\textwidth,trim=50 90 100 135,clip]{1Connection.pdf}
	\caption{Comparison of the graph model architectures. $f_a$ in JK network  denotes one aggregation layer with aggregation function such as concatenation or max pooling.}
	\label{figure_connection}
\end{wrapfigure}

\textbf{Architectural Difference.} As illustrated in Figure~\ref{figure_network} and \ref{figure_connection}, there is an apparent difference among the architectures of GCN~\citep{kipf2016semi}, SGC~\citep{wu2019simplifying}, Jumping Knowledge~(JK)~\citep{xu2018representation} and AdaGCN. Compared with these existing graph convolutional approaches that sequentially convey intermediate result $Z^{(l)}$ to compute final prediction, our AdaGCN transmits weights of nodes $w^i$, aggregated features of different hops of neighbors $\hat{A}^{l}X$. More importantly, in AdaGCN the embedding $Z^{(l)}$ is independent of the flow of computation in the network  and the sparse adjacent matrix $\hat{A}$ is also not directly involved in the computation of individual network because we compute $\hat{A}^{(l+1)}X$ in advance and then feed it instead of $\hat{A}$ into the classifier $f_{\theta}^{(l+1)}$, thus yielding significant computation reduction, which will be discussed further in  Section~\ref{alg:algorithm_AdaGCN}.

\textbf{Connection with PPNP and APPNP.} We also established a strong connection between AdaGCN and previous state-of-the-art Personalized Propagation of Neural Predictions~(PPNP) and Approximate PPNP~(APPNP)~\citep{klicpera2018predict} method that leverages personalized pagerank to reconstruct graph convolutions in order to use information from a large and adjustable neighborhood. The analysis can be summarized in the following Proposition~\ref{theorem:PPNP}. Proof can refer to Appendix~\ref{sec: proof_theorem1}.

\begin{prop}\label{theorem:PPNP}
	Suppose that $\gamma$ is the teleport factor. Let matrix sequence $\{Z^{(l)}\}$ be from the output of each layer $l$ in AdaGCN, then PPNP is equivalent to the Exponential Moving Average~(EMA) with exponentially decreasing factor $\gamma$ on $\{Z^{(l)}\}$ in a sharing parameters version, and its approximate version APPNP can be viewed as the approximated form of EMA with a limited number of terms.
\end{prop}

Proposition~\ref{theorem:PPNP} illustrates that AdaGCN can be viewed as an \textit{adaptive form} of APPNP, formulated as:
\begin{equation}  
\label{eq_AdaGCN_equivalent}
\begin{aligned} 
Z=\sum_{l=0}^{L}\alpha^{(l)}f_{\theta}^{(l)}(\hat{A}^l X)
\end{aligned} 
\end{equation}
Specifically, the first discrepancy between AdaGCN and APPNP lies in the adaptive coefficient $\alpha^{(l)}$ in AdaGCN determined by the error of $l$-th base classifier $f_{\theta}^{(l)}$ rather than fixed exponentially decreased weights in APPNP. In addition, AdaGCN employs classifier $f_{\theta}^{(l)}$ with different parameters to learn the embedding of different orders of neighbors, while APPNP shares these parameters in its form. We verified this benefit of our approach in our experiments shown in Section~\ref{experiment_performance}.

\textbf{Comparison with MixHop} MixHop~\citep{abu2019mixhop} applied the similar way of graph convolution by repeatedly mixing feature representations of neighbors at various distance. Proposition~\ref{theorem:mixhop} proves that both AdaGCN and MixHop are able to represent feature differences among neighbors while previous GCNs-based methods cannot. Proof can refer to Appendix~\ref{sec: proof_theorem2}. Recap the definition of general layer-wise Neighborhood Mixing~\citep{abu2019mixhop} as follows:
\begin{defn}{\underline{General layer-wise Neighborhood Mixing:}}
	\label{defn:mixhop}
	A graph convolution network has the ability to represent the layer-wise neighborhood mixing if for any $b_0, b_1, ..., b_L$, there exists an injective mapping $f$ with a setting of its parameters, such that the output of this graph convolution network can express the following formula:
	\begin{equation}  
	\label{eq_MixHop}
	\begin{aligned} 
	f\left(\sum_{l=0}^{L} b_{l} \sigma\left(\hat{A}^{l} X\right)\right)
	\end{aligned} 
	\end{equation}
\end{defn}

\begin{prop}\label{theorem:mixhop}
	AdaGCNs defined by our proposed approach~(Eq.~\eqref{eq_AdaGCN}) are capable of representing general layer-wise neighborhood mixing, i.e., can meet the Definition~\ref{defn:mixhop}.
\end{prop}

\begin{algorithm}[t!]
	\setstretch{0.01}
	\caption{AdaGCN based on SAMME.R Algorithm}
	\textbf{Input}: Features Matrix $X$, normalized adjacent matrix $\hat{A}$, a two-layer fully connected network $f_{\theta}$, number of layers $L$ and number of classes $K$.\\
	
	\textbf{Output}: Final combined prediction $C(A, X)$.
	
	\begin{algorithmic}[1] \STATE Initialize the node weights $w_i = 1/n, i = 1,2,...,n$ on training set, neighbors feature matrix $\hat{X}^{(0)}=X$ and classifier $f_{\theta}^{(-1)}$. 
		\FOR{$l$ = 0 to L}
		\STATE Fit the graph convolutional classifier $f_{\theta}^{(l)}$ on neighbor feature matrix $\hat{X}^{(l)}$ based on $f_{\theta}^{(l-1)}$ by minimizing current weighted loss.
		\STATE Obtain the weighted probability estimates $p^{(l)}(\hat{X}^{(l)})$ for $f_{\theta}^{(l)}$:
		$$p_{k}^{(l)}(\hat{X}^{(l)})=\text{Softmax}(f_{\theta}^{(l)}(c=k | \hat{X}^{(l)})), k=1, \ldots, K$$
		
		\STATE Compute the individual prediction $h_{k}^{(l)}(x)$ for the current graph convolutional classifier $f_{\theta}^{(l)}$:
		$$h_{k}^{(l)}(\hat{X}^{(l)}) \leftarrow(K-1)\left(\log p_{k}^{(l)}(\hat{X}^{(l)})-\frac{1}{K} \sum_{k^{\prime}} \log p_{k^{\prime}}^{(l)}(\hat{X}^{(l)})\right)$$
		where $k=1, \ldots, K$.
		
		\STATE Adjust the node weights $w_i$ for each node $x_i$ with label $y_{i}$ on training set:
		$$w_{i} \leftarrow w_{i} \cdot \exp \left(-\frac{K-1}{K} y_{i}^{\top} \log p^{(l)}\left(x_{i}\right)\right), i=1, \ldots, n$$
		
		\STATE Re-normalize all weights $w_i$.
		\STATE Update $l$+1-hop neighbor feature matrix $\hat{X}^{(l+1)}$:
		$$\hat{X}^{(l+1)}=\hat{A} \hat{X}^{(l)}$$
		\ENDFOR
		\STATE Combine all predictions $h_{k}^{(l)}(\hat{X}^{(l)})$ for $l=0,...,L$.
		$$C(A, X)=\arg \max _{k} \sum_{l=0}^{L} h_{k}^{(l)}(\hat{X}^{(l)})$$
		
		\STATE \textbf{return} Final combined prediction $C(A, X)$.
	\end{algorithmic}
	\label{alg:adagcn}
\end{algorithm}


Albeit the similarity, AdaGCN distinguishes from MixHop in many aspects. Firstly, MixHop concatenates all outputs from each order of neighbors while we combines these predictions in an Adaboost way, which has theoretical generalization guarantee based on boosting theory~\cite{hastie2009multi}. \cite{oono2020optimization} have recently derived the optimization and generalization guarantees of multi-scale GNNs, serving as the theoretical backbone of AdaGCN. Meantime, MixHop allows full linear mixing of different orders of neighboring features, while AdaGCN utilizes different non-linear transformation $f_{\theta}^{(l)}$ among all layers, enjoying stronger expressive power.

\section{Algorithm}\label{alg:algorithm_AdaGCN}
In practice, we employ SAMME.R~\citep{hastie2009multi}, the soft version of SAMME, in AdaGCN. SAMME.R~(R for Real) algorithm~\citep{hastie2009multi} leverages real-valued confidence-rated predictions, i.e., weighted probability estimates, rather than predicted hard labels in SAMME,  in the prediction combination, which has demonstrated a better generalization and faster convergence than SAMME. We elaborate the final version of AdaGCN in Algorithm~\ref{alg:adagcn}. We provide the analysis on the choice of model depth $L$ in Appendix~\ref{sec:vc_dimension}, and then we elaborate the computational advantage of AdaGCN in the following.

\textbf{Analysis of Computational Advantage.} Due to the similarity of graph convolution in MixHop~\citep{abu2019mixhop}, AdaGCN also requires no additional memory or computational complexity compared with previous GCN models. Meanwhile, our approach enjoys huge computational advantage compared with GCN-based models, e.g., PPNP and APPNP,  stemming from excluding the additional computation involved in sparse tensors, such as the sparse tensor multiplication between $\hat{A}$ and other dense tensors, \textit{in the forward and backward propagation of the neural network}. Specifically, there are only $L$ times sparse tensor operations for an AdaGCN model with $L$ layers, i.e., $\hat{A}^{l}X=\hat{A} \cdot (\hat{A}^{l-1}X)$ for each layer $l$. This operation in each layer yields a \textit{dense tensor} $B^{l} = \hat{A}^{l}X$ for the $l$-th layer, which is then fed into the computation in a two-layer fully-connected network, i.e., $f_{\theta}^{(l)}(B^{l})=\text{ReLU}(B^{l} W^{(0)})W^{(1)}$. Due to the fact that dense tensor $B^{l}$ has been computed in advance, there is no other computation related to sparse tensors in the multiple forward and backward propagation procedures while training the neural network. By contrast, this multiple computation involved in sparse tensors in the GCN-based models, e.g., GCN: $\hat{A} \ \text{ReLU}(\hat{A}XW^{(0)}) W^{(1)}$, is highly expensive. AdaGCN avoids these additional sparse tensor operations in the neural network and then attains huge computational efficiency. We demonstrate this viewpoint in the Section~\ref{sec:experiment_computation}.

\section{Experiments}


\begin{table}[b!]
\label{table:dataset}
	\centering
	\scalebox{0.8}{\begin{tabular}{llrrrr}  
			\hline
			\textbf{Dateset}  & \textbf{Nodes} & \textbf{Edges} & \textbf{Classes} & \textbf{Features} & \textbf{Label Rate} \\
			\hline
			CiteSeer &  3,327&    4,732&    6&    3,703&  3.6\%\\
			Cora     &  2,708&    5,429&    7&    1,433&  5.2\%\\
			PubMed   & 19,717&   44,338&    3&     500&  0.3\%\\
			MS Academic&18,333&   81,894&    15&     6,805&  1.6\%\\
			Reddit   & 232,965&   11,606,919&    41&     602&  65.9\%\\
			\hline
		\end{tabular} 
	}
	\caption{Dateset statistics}
\end{table}

\textbf{Experimental Setup.} We select five commonly used graphs: CiteSeer, Cora-ML~\citep{bojchevski2017deep,mccallum2000automating}, PubMed~\citep{sen2008collective}, MS-Academic~\citep{shchur2018pitfalls} and Reddit. Dateset statistics are summarized in Table 1. Recent graph neural networks suffer from overfitting to a single splitting of training, validation and test datasets~\citep{klicpera2018predict}. To address this problem, inspired by~\citep{klicpera2018predict}, we test all approaches on multiple random splits and initialization to conduct a rigorous study. Detailed dataset splittings are provided in Appendix~\ref{sec:experiments}.



\begin{figure}[t!]
	\centering
	\includegraphics[width=0.8\textwidth,trim=100 20 80 50,clip]{1Deep.pdf}
	\caption{Comparison of test accuracy of different models as the layer increases. We regard the $l$-th base classifier as the $l$-th layer in AdaGCN as both of them are leveraged to exploit the information from $l$-th order of neighbors for current nodes.}
	\label{figure_deep}
\end{figure}

\textbf{Basic Setting of Baselines and AdaGCN.} We compare AdaGCN with GCN~\citep{kipf2016semi} and Simple Graph Convolution~(SGC)~\citep{wu2019simplifying} in Figure~\ref{figure_deep}. In Table~\ref{table_performance}, we employ the same baselines as~\citep{klicpera2018predict}:  V.GCN~(vanilla GCN)~\citep{kipf2016semi} and GCN with our early stopping, N-GCN~(network of GCN)~\citep{abu2018n}, GAT~(Graph Attention Networks)~\citep{velivckovic2017graph}, BT.FP~(bootstrapped feature propagation)~\citep{buchnik2018bootstrapped} and JK~(jumping knowledge networks with concatenation)~\citep{xu2018representation}. In the computation part, we additionally compare AdaGCN with FastGCN~\citep{chen2018fastgcn} and GraphSAGE~\citep{hamilton2017inductive}. We refer to the result of baselines from~\citep{klicpera2018predict} and the implementation of AdaGCN is adapted from APPNP. For AdaGCN, after the line search on hyper-parameters, we set $h=5000$ hidden units for the first four datasets except Ms-academic with $h=3000$, and 15, 12, 20 and 5 layers respectively due to the different graph structures. In addition, we set dropout rate to 0 for Citeseer and Cora-ML datasets and 0.2 for the other datasets and $5\times10^{-3} L_2$ regularization on the first linear layer. We set weight decay as $1\times 10^{-3}$ for Citeseer while $1\times 10^{-4}$ for others. More detailed model parameters and analysis about our early stopping mechanism can be referred from Appendix~\ref{sec:experiments}.

\begin{table}[b!]
\centering
	\scalebox{0.8}{
		\begin{tabular}{lcccc}
			\hline
			\textbf{Model} & \textbf{Citeseer} & \textbf{Cora-ML} & \textbf{Pubmed} & \textbf{MS Academic}\\
			\hline
			V.GCN &73.51$\pm$0.48&82.30$\pm$0.34&77.65$\pm$0.40&91.65$\pm$0.09\\
			GCN   &75.40$\pm$0.30&83.41$\pm$0.39&78.68$\pm$0.38&92.10$\pm$0.08\\
			N-GCN &74.25$\pm$0.40&82.25$\pm$0.30&77.43$\pm$0.42&92.86$\pm$0.11\\
			GAT   &75.39$\pm$0.27&84.37$\pm$0.24&77.76$\pm$0.44&91.22$\pm$0.07\\
			JK    &73.03$\pm$0.47&82.69$\pm$0.35&77.88$\pm$0.38&91.71$\pm$0.10\\
			BT.FP &73.55$\pm$0.57&80.84$\pm$0.97&72.94$\pm$1.00&91.61$\pm$0.24\\
			PPNP  &75.83$\pm$0.27&85.29$\pm$0.25&OOM&OOM\\
			APPNP &75.73$\pm$0.30&85.09$\pm$0.25&79.73$\pm$0.31&93.27$\pm$0.08\\
			\hline
			PPNP~(ours) &75.53$\pm$0.32&84.39$\pm$0.28&OOM&OOM\\
			APPNP~(ours) &75.41$\pm$0.35&84.28$\pm$0.28&79.41$\pm$0.34&92.98$\pm$0.07\\
			AdaGCN&\bf{76.68$\pm$0.20}&\bf{85.97$\pm$0.20}&\bf{79.95$\pm$0.21}&\bf{93.17$\pm$0.07}\\
			\hline
			P value&1.8$\times10^{-15}$&2.2$\times10^{-16}$&1.1$\times10^{-5}$&2.1$\times10^{-9}$\\
			\hline
		\end{tabular}
	}
	\caption{Average accuracy under 100 runs with uncertainties showing the 95 \% confidence level calculated by bootstrapping. OOM denotes ``out of memory''. ``(ours)'' denotes the results based on our implementation, which are slight lower than numbers above from original literature~\citep{klicpera2018predict}. P values of paired t test between APPNP~(ours) and AdaGCN are provided in the last row.}
	\label{table_performance}
\end{table}

\subsection{Design of Deep Graph Models to Circumvent Oversmoothing Effect}\label{experiment_deep}

It is well-known that GCN suffers from oversmoothing~\citep{li2018deeper} with the stacking of more graph convolutions. However, combination of knowledge from each layer to design deep graph models is a reasonable method to circumvent oversmoothing issue. In our experiment, we aim to explore the prediction performance of GCN, GCN with residual connection~\citep{kipf2016semi}, SGC and our AdaGCN with a growing number of layers. 



From Figure~\ref{figure_deep}, it can be easily observed that oversmoothing leads to the rapid decreasing of accuracy for GCN~(blue line) as the layer increases. In contrast, the speed of smoothing~(green line) of SGC is much slower than GCN due to the lack of ReLU analyzed in Section~\ref{approach_establish}. Similarly, GCN with residual connection~(yellow line) partially mitigates the oversmoothing effect of original GCN but fails to take advantage of information from different orders of neighbors to improve the prediction performance constantly. Remarkably, AdaGCN~(red line) is able to consistently enhance the performance with the increasing of layers across the three datasets. This implies that AdaGCN can efficiently incorporate knowledge from different orders of neighbors and circumvent oversmoothing of original GCN in the process of constructing deep graph models. In addition, the fluctuation of performance for AdaGCN is much lower than GCN especially when the number of layer is large.



\begin{table}[t!]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{lcccc}
			\hline
			& \textbf{Citeseer} & \textbf{Cora-ML} & \textbf{Pubmed} & \textbf{MS Academic}\\
			\textbf{Label Rates} &1.0\% \ \ / \ \ 2.0\% &2.0\% \ \ / \ \ 4.0\%&0.1\% \ \ / \ \ 0.2\%&0.6\% \ \ / \ \ 1.2\%\\
			\hline
			V.GCN      &67.6$\pm$1.4/70.8$\pm$1.4&76.4$\pm$1.3/81.7$\pm$0.8&70.1$\pm$1.4/74.6$\pm$1.6&89.7$\pm$0.4/91.1$\pm$0.2\\
			GCN      &70.3$\pm$0.9/72.7$\pm$1.1&80.0$\pm$0.7/82.8$\pm$0.9&71.1$\pm$1.1/75.2$\pm$1.0&89.8$\pm$0.4/91.2$\pm$0.3\\
			PPNP     &72.5$\pm$0.9/74.7$\pm$0.7&80.1$\pm$0.7/83.0$\pm$0.6&OOM&OOM\\
			APPNP    &72.2$\pm$1.3/74.2$\pm$1.1&80.1$\pm$0.7/83.2$\pm$0.6&74.0$\pm$1.5/77.2$\pm$1.2&91.7$\pm$0.2/92.6$\pm$0.2\\
			\hline
			AdaGCN   &\bf{74.2$\pm$0.3}/\bf{75.5$\pm$0.3} &\bf{83.7$\pm$0.3}/\bf{85.3$\pm$0.2} & \bf{77.1$\pm$0.5}/\bf{79.3$\pm$0.3} & \bf{92.1$\pm$0.1}/92.7$\pm$0.1\\
			\hline
		\end{tabular}
	}
	\caption{Average accuracy across different label rates with 20 splittings of datasets under 100 runs.}
	\label{table_labelrate}
\end{table}

\subsection{Prediction Performance}\label{experiment_performance}

We conduct a rigorous study of AdaGCN on four datasets under multiple splittings of dataset. The results from Table~\ref{table_performance} suggest the state-of-the-art performance of our approach and the improvement compared with APPNP validates the benefit of adaptive form for our AdaGCN. More rigorously, p values under paired t test demonstrate the significance of improvement for our method.

In the realistic setting, graphs usually have different labeled nodes and thus it is necessary to investigate the robust performance of methods on different number of labeled nodes. Here we utilize label rates to measure the different numbers of labeled nodes and then sample corresponding labeled nodes per class on graphs respectively. Table~\ref{table_labelrate} presents the consistent state-of-the-art performance of AdaGCN under different label rates. An interesting manifestation from Table~\ref{table_labelrate} is that AdaGCN yields more improvement on fewer label rates compared with APPNP, showing more efficiency on graphs with few labeled nodes. Inspired by the Layer Effect on graphs~\citep{sun2019multi}, we argue that the increase of layers in AdaGCN can result in more benefits on the efficient propagation of label signals especially on graphs with limited labeled nodes.

\begin{wraptable}[9]{r}{0.5\textwidth}
	
	\centering
	\scalebox{0.9}{
		\begin{tabular}{lrr}
			\hline
			\textbf{Reddit} & \textbf{F1-Score} & \textbf{Per-epoch training time}\\
			\hline
			V.GCN      &94.46$\pm$0.06&5627.46ms\\
			PPNP     &OOM&OOM\\
			APPNP    &95.04$\pm$0.07&29489.81ms\\
			\hline
			AdaGCN   &\bf{95.39$\pm$0.13}&\bf{32.29ms}\\
			\hline
		\end{tabular}
	}
	\caption{Average F1-scores and per-epoch training time of typical methods on Reddit dataset under 5 runs.}
	\label{table_reddit}
\end{wraptable}

More rigorously, we additionally conduct the comparison on a larger dataset, i.e., Reddit. We choose the best layer as 4 due to the fact that AdaGCN with larger number of layers tends to suffer from overfitting on this relatively simple dataset~(with high label rate 65.9\%). Table~\ref{table_reddit} suggests that AdaGCN can still outperform other typical baselines, including V.GCN, PPNP and APPNP. More experimental details can be referred from Appendix~\ref{sec:experiments}.

\subsection{Computational Efficiency} \label{sec:experiment_computation}

Without the additional computational cost involved in sparse tensors in the propagation of the neural network, AdaGCN presents huge computational efficiency. From the left part of Figure~\ref{figure_time}, it exhibits that AdaGCN has the fastest speed of per-epoch training time in comparison with other methods except the comparative performance with FastGCN in Pubmed. In addition, there is a somewhat inconsistency in computation of FastGCN, with fastest speed in Pubmed but slower than GCN on Cora-ML and MS-Academic datasets. Furthermore, with multiple power iterations involved in sparse tensors, APPNP unfortunately has relatively expensive computation cost. It should be noted that this computational advantage of AdaGCN is more significant when it comes to large datasets, e.g., Reddit. Table~\ref{table_reddit} demonstrates AdaGCN has the potential to perform much faster on larger datasets.

Besides, we explore the computational cost of ReLU and sparse adjacency tensor with respect to the number of layers in the right part of Figure~\ref{figure_time}. We focus on comparing AdaGCN with SGC and GCN as other GCN-based methods, such as GraphSAGE and APPNP, behave similarly with GCN. Particularly, we can easily observe that both SGC~(green line) and GCN~(red line) show a linear increasing tendency and GCN yields a larger slope arises from ReLU and more parameters. For SGC, stacking more layers directly is undesirable regarding the computation. Thus, a limited number of SGC layers is preferable with more advanced optimization techniques~\cite{wu2019simplifying}. It also shows that the computational cost involved sparse matrices in neural networks plays a dominant role in all the cost especially when the layer is large enough. In contrast, our AdaGCN~(pink line) displays an almost constant trend as the layer increases simply because it excludes the extra computation involved in sparse tensors $\hat{A}$, such as $\cdots \hat{A} \ \text{ReLU}(\hat{A}XW^{(0)}) W^{(1)} \cdots$, in the process of training neural networks. AdaGCN maintains the updating of parameters in the $f_{\theta}^{(l)}$ \textit{with a fixed architecture} in each layer while the layer-wise optimization, therefore displaying a nearly constant computation cost within each epoch although more epochs are normally needed in the entire layer-wise training. We leave the analysis of exact time and memory complexity of AdaGCN as future works, but boosting-based algorithms including AdaGCN is memory-efficient~\citep{oono2020optimization}.

\begin{figure}[t!]
\centering
	\includegraphics[width=0.7\textwidth,trim=100 25 100 50,clip]{2Time.pdf}
	\caption{Left: Per-epoch training time of AdaGCN vs other methods under 5 runs on four datasets. Right: Per-epoch training time of AdaGCN compared with GCN and SGC with the increasing of layers and the digit after ``$k=$'' denotes the slope in a fitted linear regression.}
	\label{figure_time}
\end{figure}

\section{Discussions and Conclusion} 


One potential concern is that AdaBoost~\citep{hastie2009multi,freund1999short} is established on i.i.d. hypothesis while graphs have inherent data-dependent property. Fortunately, the statistical convergence and consistency of boosting~\citep{lugosi2001bayes,mannor2003greedy} can still be preserved when the samples are weakly dependent~\citep{lozano2013convergence}. More discussion can refer to  Appendix~\ref{sec: proof_boosting}. In this paper, we propose a novel RNN-like deep graph neural network architecture called AdaGCNs. With the delicate architecture design, our approach AdaGCN can effectively explore and exploit knowledge from different orders of neighbors in an Adaboost way. Our work paves a way towards better combining different-order neighbors to design deep graph models rather than only stacking on specific type of graph convolution.


\subsubsection*{Acknowledgments}
Z. Lin is supported by NSF China (grant no.s 61625301 and 61731018), Major Scientific Research Project of Zhejiang Lab (grant no.s 2019KB0AC01 and 2019KB0AB02), Beijing Academy of Artificial Intelligence, and Qualcomm. 


\bibliography{AdaGCN}
\bibliographystyle{iclr2021_conference}

\clearpage

\small
\appendix

\section{Appendix}
\subsection{Related Works on Deep Graph Models}\label{appendix:deep graph}

A straightforward solution~\citep{kipf2016semi,xu2018representation} inspired by ResNets~\citep{he2016deep} was by adding residual connections, but this practice was unsatisfactory both in prediction performance and computational efficiency towards building deep graph models, as shown in our experiments in Section~\ref{experiment_deep} and \ref{sec:experiment_computation}. More recently, JK~(Jumping Knowledge Networks~\citep{xu2018representation}) introduced jumping connections into final aggregation mechanism in order to extract knowledge from different layers of graph convolutions. However, this straightforward change of GCN architecture exhibited inconsistent empirical performance for different aggregation operators, which cannot demonstrate the successful construction of deep layers. In addition, Graph powering-based method~\citep{jin2019power} implicitly leveraged more spatial information by extending classical spectral graph theory to robust graph theory, but they concentrated on defending adversarial attacks rather than model depth. LanczosNet~\citep{liao2019lanczosnet} utilized Lanczos algorithm to construct low rank approximations of the graph Laplacian and then can exploit multi-scale information. Moreover, APPNP~(Approximate Personalized Propagation of Neural Predictions,~\citep{klicpera2018predict}) leveraged the relationship between GCN and personalized PageRank to derive an improved global propagation scheme. Beyond these, DeepGCNs~\citep{li2019can} directly adapted residual, dense connection and dilated convolutions to GCN architecture, but it mainly focused on the task of point cloud semantic segmentation and has not demonstrated its effectiveness in typical graph tasks. Similar to our work,  Deep Adaptive Graph Neural Network~(DAGNN)~\citep{liu2020towards} also focused on incorporating information from large receptive fields through the entanglement of representation transformation and propagation, while our work efficiently ensembles knowledge from large receptive fields in an Adaboost manner. Other related works based on global attention models~\citep{puny2020graph} and sample-based methods~\citep{zeng2019graphsaint}  are also helpful to construct deep graph models.

\subsection{Insufficient Representation Power of AdaSGC}
\label{sec:AdaSGC}
As illustrated in Figure~\ref{figure_AdaSGC}, with the increasing of layers, AdaSGC with only linear transformation has insufficient representation power both in extracting knowledge from high-order neighbors and combining information from different orders of neighbors while AdaGCN exhibits a consistent improvement of performance as the layer increases.

\begin{figure}[htbp]
	\centering
	\centering\includegraphics[width=0.8\textwidth,trim=100 30 100 40,clip]{5AdaSGC.pdf}
	\caption{AdaSGC vs AdaGCN.}
	\label{figure_AdaSGC}
\end{figure}


\subsection{Proof of Proposition 1} \label{sec: proof_theorem1}
Firstly, we further elaborate the Proposition 1 as follows, then we provide the proof.

Suppose that $\gamma$ is the teleport factor. Consider the output $Z_{\text{PPNP}}=\gamma (\mathbb{I} - (1-\gamma)\hat{A})^{-1}f_{\theta}(X)$ in PPNP and $Z_{\text{APPNP}}$ from its approxminated version APPNP. Let matrix sequence $\{Z^{(l)}\}$ be from the output of each layer $l$ in AdaGCN, then PPNP is equivalent to the Exponential Moving Average~(EMA) with exponentially decreasing factor $\gamma$, a first-order infinite impulse response filter, on $\{Z^{(l)}\}$ in a sharing parameters version, i.e., $f_{\theta}^{(l)} \equiv f_{\theta}$. In addition, APPNP, which we reformulate in Eq.~\ref{eq_APPNP}, can be viewed as the approximated form of EMA with a limited number of terms. 
\begin{equation} 
\label{eq_APPNP}
\begin{aligned} 
Z_{\text{APPNP}}= (\gamma \sum_{l=0}^{L-1}(1-\gamma)^l \hat{A}^l + (1-\gamma)^L \hat{A}^L)f_{\theta}(X)
\end{aligned} 
\end{equation}

\begin{proof}
	According to Neumann Theorem, $Z_{\text{PPNP}}$ can be expanded as a Neumann series:
	\begin{equation}
	\begin{aligned}
	Z_{\text{PPNP}}&=\gamma (\mathbb{I} - (1-\gamma)\hat{A})^{-1}f_{\theta}(X)\\
	&=\gamma \sum_{l=0}^{\infty}(1-\gamma)^{\l}\hat{A}^l f_{\theta}(X), \nonumber
	\end{aligned}
	\end{equation}
	where feature embedding matrix sequence $\{Z^{(l)}\}$ for each order of neighbors share the same parameters $f_{\theta}$. If we relax this sharing nature to the adaptive form with respect to the layer and put $\hat{A^l}$ into $f_{\theta}$, then the output $Z$ can be approximately formulated as:
	\begin{equation}
	\begin{aligned}
	Z_{\text{PPNP}} \approx \gamma \sum_{l=0}^{\infty}(1-\gamma)^{\l}f_{\theta}^{(l)}(\hat{A}^l X) \nonumber
	\end{aligned} 
	\end{equation}
	This relaxed version from PPNP is the Exponential Moving Average form of matrix sequence $\{Z^{(l)}\}$ with exponential decreasing factor $\gamma$. Moreover, if we approximate the EMA by truncating it after $L-1$ items, then the weight omitted by stopping after $L-1$ items is $(1-\gamma)^L$. Thus, the approximated EMA is exactly the APPNP form:
	\begin{equation}
	\begin{aligned}
	Z_{\text{APPNP}}= (\gamma \sum_{l=0}^{L-1}(1-\gamma)^l \hat{A}^l + (1-\gamma)^L \hat{A}^L)f_{\theta}(X) \nonumber
	\end{aligned}
	\end{equation}
\end{proof}

\subsection{Proof of Proposition 2} \label{sec: proof_theorem2}

\begin{proof}
	We consider a two layers fully-connected neural network as $f$ in Eq.~\ref{eq_AdaGCN_equivalent}, then the output of AdaGCN can be formulated as:
	\begin{equation}  
	\begin{aligned} 
	Z=\sum_{l=0}^{L}\alpha^{(l)}\sigma(\hat{A}^l X W^{(0)}) W^{(1)} \nonumber
	\end{aligned} 
	\end{equation}
	Particularly, we set $W^{(0)}=\frac{b_l}{\text{sign}(b_l) \alpha^{(l)}} \mathbb{I}$ and $W^{(1)}=\text{sign}(b_l)\mathbb{I}$ where $\text{sign}(b_l)$ is the signed incidence scalar w.r.t $b_l$. Then the output of AdaGCN can be presented as:
	\begin{equation}  
	\begin{aligned} 
	Z&=\sum_{l=0}^{L}\alpha^{(l)}\sigma(\hat{A}^l X \frac{b_l}{\text{sign}(b_l) \alpha^{(l)}} \mathbb{I}) \text{sign}(b_l)\mathbb{I} \\ 
	&=\sum_{l=0}^{L}\alpha^{(l)}\sigma(\hat{A}^l X)\frac{b_l}{\text{sign}(b_l) \alpha^{(l)}} \text{sign}(b_l)\\
	&=\sum_{l=0}^{L} b_{l} \sigma\left(\hat{A}^{l} X\right) \nonumber 
	\end{aligned} 
	\end{equation}
	The proof that GCNs-based methods are not capable of representing general layer-wise neighborhood mixing has been demonstrated in MixHop~\citep{abu2019mixhop}. Proposition~\ref{theorem:mixhop} proved.
\end{proof}

\subsection{Explanation about Consistency of Boosting on Dependent Data} \label{sec: proof_boosting}

\begin{defn}
	($\beta$-mixing sequences.) Let $ \sigma_i^j= \sigma(W)=\sigma(W_i, W_{i+1}, ... , W_j)$ be the $\sigma$-field generated by a strictly stationary sequence of random variables $W = (W_i, W_{i+1}, ... , W_j)$. The $\beta$-mixing coefficient is defined by:
	\begin{equation}
	\begin{aligned}
	\beta_{W}(n)=\sup _{k} \mathbb{E} \sup \left\{\left|\mathbb{P}\left(A | \sigma_{1}^{k}\right)-\mathbb{P}(A)\right| : A \in \sigma_{k+n}^{\infty}\right\} \nonumber
	\end{aligned}
	\end{equation}
	Then a sequence $W$ is called $\beta$-mixing if $\text{lim}_{n\rightarrow \infty} \beta_W(n) = 0$. Further, it is algebraically $\beta$-mixing if there is a positive constant $r_{\beta}$ such that $\beta_{W}(n)=\mathcal{O}(n^{-r_{\beta}})$.
\end{defn}

\begin{defn}
	(Consistency) A classification rule is consistent for a certain distribution $P$ if $E(L(h_n)) = P\{h_n (X) = Y\}\rightarrow a$ as $n\rightarrow \infty$ where $a$ is a constant. It is strongly Bayes-risk consistent if $\text{lim}_{n\rightarrow \infty} L(h_n) = a$ almost surely.
\end{defn}

Under these definitions, the convergence and consistence of regularized boosting method on stationary $\beta$-mixing sequences can be proved under mild assumptions. More details can be referred from~\citep{lozano2013convergence}.

\subsection{Experimental Details}\label{sec:experiments}

\textbf{Early Stopping on AdaGCN.} We apply the same early stopping mechanism across all the methods as~\citep{klicpera2018predict} for fair comparison. Furthermore, boosting theory also has the capacity to perfectly incorporate early stopping and it has been shown that for several boosting algorithms including AdaBoost, this regularization via early stopping can provide guarantees of consistency~\citep{zhang2005boosting,jiang2004process,buhlmann2003boosting}.

\textbf{Dataset Splitting.} We choose a training set of a fixed nodes per class, an early stopping set of 500 nodes and test set of remained nodes. Each experiment is run with 5 random initialization on each data split, leading to a total
of 100 runs per experiment. On a standard setting, we randomly select 20 nodes per class. For the two different label rates on each graph, we select 6, 11 nodes per class on citeseer, 8, 16 nodes per class on Cora-ML, 7, 14 nodes per class on Pubmed and 8, 15 nodes per class on MS-Academic dataset.

\textbf{Model parameters.} For all GCN-based approaches, we use the same hyper-parameters in the original paper: learning rate of 0.01, 0.5 dropout rate, $5 \times 10^{-4} \ L_2$ regularization weight, and 16 hidden units. For FastGCN, we adopt the officially released code to conduct our experiments. PPNP and APPNP are adapted with best setting: $K=10$ power iteration steps for APPNP, teleport probability $\gamma=0.1$ on Cora-ML, Citeseer and Pubmed, $\gamma=0.2$ on Ms-Academic. In addition, we use two layers
with $h = 64$ hidden units and apply L2 regularization with $\lambda=5 \times 10^{-3}$ on the weights of the first layer and use dropout with dropout rate $d = 0.5$ on both layers and the adjacency matrix. The early stopping criterion uses a patience of $p = 100$ and an (unreachably high) maximum of $n = 10 000$ epochs.The implementation of AdaGCN is adapted from PPNP and APPNP. Corresponding patience $p=300$ and $n=500$ in the early stopping of AdaGCN. Moreover, SGC is re-implemented in a straightforward way without incorporating advanced optimization for better illustration and comparison. Other baselines are adopted the same parameters described in PPNP and APPNP.

\textbf{Settings on Reddit dataset.} By repeatedly tuning the parameters of these typical methods on Reddit, we finally choose weight decay rate as $10^{-4}$, hidden layer size 100 and epoch 20000 for AdaGCN. For APPNP, we opt weight decay rate as $10^{-5}$, dropout rate as 0 and epoch 500. V.GCN applies the same parameters in \citep{kipf2016semi} and we choose epoch as 500. All approaches have not deployed early stopping due to the expensive computational cost on the large Reddit dataset, which is also a fair comparison.

\subsection{Choice of the Number of Layers} \label{sec:vc_dimension}
Different from the ``forcible'' behaviors in CNNs that directly stack many convolution layers, in our AdaGCN there is a theoretical guidance on the choice of model depth $L$, i.e., the number of base classifiers or layers, derived from boosting theory. Specifically, according to the boosting theory, the increasing of $L$ can exponentially decreases the empirical loss, however, from the perspective of VC-dimension, an overly large $L$ can yield overfitting of AdaGCN. It should be noted that the deeper graph convolution layers in AdaGCN are not always better, which indeed heavily depends on the the complexity of data. In practice, $L$ can be determined via cross-validation. Specifically, we start a VC-dimension-based analysis to illustrate that too large $L$ can yield overfitting of AdaGCN. For $L$ layers of AdaGCN, its hypothesis set is
\begin{equation} 
\label{eq_set}
\begin{aligned} 
\mathcal{F}_{L}=\left\{\arg\max_k \left(\sum_{l=1}^{L} \alpha^{(l)} f_{\theta}^{(l)} \right) : \alpha^{(l)} \in \mathbb{R}, l \in[1, L]\right\}
\end{aligned} 
\end{equation}
Then the VC-dimension of $\mathcal{F}_T$ can be bounded as follows in terms of the VC-dimension $d$ of the family of base hypothesis:
\begin{equation} 
\label{eq_VC}
\begin{aligned} 
\text{VCdim}\left(\mathcal{F}_{L}\right) \leq 2(d+1)(L+1) \log _{2}((L+1) e),
\end{aligned} 
\end{equation}
where $e$ is a constant and the upper bounds grows as $L$ increases. Combined with VC-dimension generalization bounds, these results imply that larger values of $L$ can lead to overfitting of AdaBoost. This situation also happens in AdaGCN, which inspires us that there is no need to stack too many layers on AdaGCN in order to avoid overfitting. In practice, $L$ is typically determined via cross-validation.

\end{document}
