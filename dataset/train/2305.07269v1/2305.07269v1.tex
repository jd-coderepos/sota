\section{Experiments and Discussion}
\label{experiments}
\begin{table*}[h]
\begin{center}
  \caption{\textbf{Generalizability with different scene variety.} We compare single-stage meta-learning (only prior learning) and supervised learning. ConvNeXt-Base backbone is used.  means training on - and testing on -dataset. Replica and HM3D respectively hold lower and higher scene variety for training. 
   Meta-Learning has much larger improvements especially trained on low scene-variety Replica.}
  \vspace{-1pt}
\footnotesize
  \label{table:limited}
  \begin{tabular}[c]
  {
  p{3.3cm}<{\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}|
  p{1.4cm}<{\centering\arraybackslash}}
  \hlineB{2}
  \hline
     & \multicolumn{3}{c}{\cellcolor[HTML]{99CCFF}Replica  VA} & \multicolumn{3}{c}{\cellcolor[HTML]{F7BCDC}HM3D  VA} \\
      Method & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE\\
    \hline
      Direct supervised learning & 0.718 & 0.538 & 1.078 & 0.544 & 0.456 & 0.715 \\
      Meta-Learning & \textbf{0.548} & \textbf{0.430} & \textbf{0.761} & \textbf{0.427} & \textbf{0.369} & \textbf{0.603} \\
      Improvement & \textit{-23.6\%} & \textit{-20.1\%} & \textit{-29.4\%} & \textit{-21.5\%} & \textit{-19.1\%} & \textit{-15.7\%} \\
    \hline
    \hlineB{2}
  \end{tabular}
  \vspace{-8pt}
\end{center}
\end{table*}

\textbf{Aims}. We validate our meta-initialization with five questions. \textbf{Q1} Can meta-learning improve learning image-to-depth mapping on limited scene-variety datasets? (Sec.~\ref{experiments:scene-fitting}) \textbf{Q2} What improvements can meta-initialization bring compared with the most popular ImageNet-initialization? (Sec.~\ref{experiments:intra-dataset}) \textbf{Q3} How does meta-initialization help zero-shot cross-dataset generalization? (Sec.~\ref{experiments:cross-dataset}) \textbf{Q4} How does more accurate depth help learn better 3D representation? (Sec.~\ref{experiments:nerf}) \textbf{Q5} How is the proposed fine-grained task related to other meta-learning findings? (Supp Sec. D)



\textbf{Datasets:} Exemplar data are illustrated in Supp Sec. B.
\begin{itemize}[leftmargin=*,topsep=-3pt,itemsep=-0.8ex]
  \item Hypersim \cite{roberts2020hypersim} has high scene variety with 470 synthetic indoor environments, from small rooms to large open spaces, with about 67K training and 7.7K testing images.
  \item HM3D \cite{ramakrishnan2021habitat} and Replica \cite{straub2019replica} are associated with 200K and 40K images that are taken from SimSIN \cite{wu2022toward}. HM3D has 800 scenes with \textit{much higher scene variety} than Replica, which only has 18 overlapping scenes. 
\item NYUv2 \cite{silberman2012indoor} contains real 654 testing images. It uses older camera models with high imaging noise and limited camera viewing direction.
  \item VA \cite{wu2022toward} consists of 3.5K photorealistic renderings for testing on challenging lighting conditions and arbitrary camera viewing directions.
\end{itemize}
\vspace{4pt}

\textbf{Training Settings.} We use ResNet \cite{he2016deep}, ConvNeXt \cite{liu2022convnet}, and their variants as the network architecture to extract bottleneck features. Then we build a depth regression head following \cite{Godard_2019_ICCV} containing 5 convolution blocks with skip connection from the encoder. Each convolution block contains a 3x3 convolution, an ELU activation, and a bilinear 2 upsampling layer to recover the input size at the end output. Channel-size of each convolution block is . Last, a 3x3 convolution for 1-channel output with a sigmoid activation are used to get inverse depth, which is then converted to depth \cite{Dijk_2019_ICCV}. We set , , ,  for ResNet, and  for ConvNeXt. At the supervised learning stage, we train models for 15 epochs with a learning rate of , optimized by AdamW \cite{loshchilov2018decoupled} with a weight decay of 0.01. Input size to the network is 256256.   loss is used as . 





\textbf{Metrics.} We adopt common monocular depth estimation evaluation metrics. Error metrics (in meters, the lower the better): Mean Absolute Error (MAE), Absolute Relative Error (AbsRel), Root Mean Square Error (RMSE). Threshold Accuracy:  (in , percentage of correct pixel depth. Higher percentage implies more structured depth hence better). Correctness: ratio between prediction and groundtruth is within .

\subsection{Meta-Learning on Limited Scene Variety} 
\label{experiments:scene-fitting}
We first show how a single-stage meta-learning (only prior stage) performs. We train =15 epochs of meta-learning and compare with 15 epochs of direct supervised learning where both training pieces converge already. 
The other hyperparameters are the same as given in Training Settings. 
Replica Dataset of limited scene variety is used to verify gain on limited sources.
Fig. \ref{fitting} show fitting to training scenes. 
From the figure, meta-learning is capable of identifying near and far fields without irregularity that direct supervised learning struggles with. Under limited training scenes, meta-learning induces a better image-to-depth mapping that delineates object shapes, separates depth-relevant/-irrelevant cues, and shows flat planes where rich depth-irrelevant textures exist. The observation follows the explanation in Sec.~\ref{explanation}. See Supp Sec. C for more analysis.


We next numerically examine generalizability to unseen scenes when training on different level scene-variety data. HM3D (high scene variety) and Replica (low scene variety) are used as training sets and VA is used for testing. Table \ref{table:limited} shows that models trained by single-stage meta-learning substantially outperform direct supervised learning with - improvements. The advantage is more evident when trained on lower scene-variety Replica. 



\begin{table}[tb!]
\label{table:intra}
\begin{center}
  \caption{\textbf{Effects of Meta-Initialization on intra-dataset evaluation.} We train and test meta-initialization (full Algorithm \ref{meta-algo}) on the same dataset. Hypersim and NYUv2 of higher scene variety are used. Using the same architecture, meta-initialization (+Meta) consistently outperforms ImageNet-initialization (no marks). Both error (in orange) and accuracy (in green) are reported.}
  \vspace{-8pt}
\footnotesize
  \label{table:intra-dataset}
  \begin{tabular}[c]
  {
  p{2.7cm}<{\arraybackslash}
  p{0.5cm}<{\centering\arraybackslash}
  p{0.65cm}<{\centering\arraybackslash}
  p{0.6cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}}
  \hlineB{2}
  
      \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim}  & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } \\
    \hline
      ResNet50     & 1.288 & 0.248 & 1.775 & 64.8 & 87.1 & 94.7 \\
      ResNet50 + Meta & \textbf{1.205} & \textbf{0.239} & \textbf{1.680} & \textbf{66.7} & \textbf{87.9} & \textbf{95.0} \\
      \hline
      ResNet101    & 1.197 & 0.234 & 1.671 & 67.4 & 88.5 & 95.3 \\
      ResNet101 + Meta   & \textbf{1.158} & \textbf{0.220} & \textbf{1.595} & \textbf{68.0} & \textbf{89.0} & \textbf{95.4}\\
      \hline
      ConvNeXt-base   & 1.073 & 0.201 & 1.534 & 73.6 & 91.1 & 96.3 \\
      ConvNeXt-base + Meta  & \textbf{0.994} & \textbf{0.188} & \textbf{1.425} & \textbf{74.9} & \textbf{91.7} & \textbf{96.5} \\ 
    \hlineB{2}
    \hline
  \end{tabular}

  \begin{tabular}[c]
  {
  p{2.7cm}<{\arraybackslash}
  p{0.5cm}<{\centering\arraybackslash}
  p{0.65cm}<{\centering\arraybackslash}
  p{0.6cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}
  p{0.4cm}<{\centering\arraybackslash}}
  \hlineB{2}
  
     \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} NYUv2}  & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } &  \multicolumn{1}{|c|}{\cellcolor[HTML]{D5F5E3} } \\
    \hline
      ResNet50    & 0.345 & 0.131 & 0.480 & 83.6 & 96.4 & 99.0 \\
      ResNet50 +Meta   & \textbf{0.325} & \textbf{0.122} & \textbf{0.454} & \textbf{85.4} & \textbf{96.8} & \textbf{99.3} \\
      \hline
      ResNet101    & 0.318 & 0.120 & 0.448 & 85.6 & 97.1 & 99.3 \\
      ResNet101 +Meta   & \textbf{0.303} & \textbf{0.112} & \textbf{0.420} & \textbf{86.7} & \textbf{97.4} & \textbf{99.4} \\
      \hline
      ConvNeXt-base   & 0.273 & 0.101 & 0.394 & 89.4 & 97.9 & \textbf{99.5} \\ 
      ConvNeXt-base+Meta   & \textbf{0.266} & \textbf{0.099} & \textbf{0.387} & \textbf{89.8} & \textbf{98.1} & \textbf{99.5} \\ 
      \hline
    \hlineB{2}
    \hline
  \end{tabular}
  \vspace{-23pt}
\end{center}
\end{table}


\begin{figure*}[bt!]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.88\linewidth]{figures/qualitative.png}
    \vspace{-4pt}
    \caption{\textbf{Depth map qualitative comparison.} Results of our meta-initialization have better object shapes with clearer boundaries. Depth-irrelevant textures are suppressed, and flat planes are predicted, as shown in Hypersim- Row 2 ceiling and 3 textured wall examples.}
    \vspace{-10pt}
    \label{intra-qualitative}
\end{figure*}



\subsection{Meta-Initialization v.s. ImageNet-Initialization} 
\label{experiments:intra-dataset}



Sec.~\ref{experiments:scene-fitting} shows single-stage meta-learning induces much better depth regression, but the depth is yet detailed. We next train full Algorithm \ref{meta-algo}, using meta-learned weights as initialization for following supervised learning. We \textbf{go beyond limited sources} and train on higher scene-variety datasets. 
Intuitively, higher scene variety helps supervised learning attain better depth prediction and might diminish meta-learning's advantages of few-shot and low-source learning.
However, such studies are practical for validating meta-learning in real-world applications.
Comparison is drawn with baselines of direct supervised learning without meta-initialization that begins from ImageNet-initialization instead. 




Table \ref{table:intra-dataset} shows intra-dataset evaluation that trains/ tests on each official data splits.
For Hypersim evaluation is capped for depth at 20m and 10m for NYUv2. 
Uniquely, using meta-initialization attains \textbf{consistently} lower errors and higher accuracy than the baselines, especially AbsRel (averagely +6.5\%) and  (averagely +1.69 points) that indicates accurate depth structure is predicted. We further display qualitative comparison for depth and 3D point cloud view in Fig. \ref{intra-qualitative} and \ref{pointcloud}. 
The gain simply comes from better training schema without more data or constraints, advanced loss, or model design. More results are in Supp Sec. G.


\subsection{Zero-Shot Cross-Dataset Evaluation} 
\label{experiments:cross-dataset}

To faithfully validate a trained model in the wild, we design protocols for zero-shot cross-dataset inference. High scene-variety and larger-size synthetic datasets, Hypersim and HM3D, are used as training sets. VA, Replica, and NYUv2 serve as testing, and their evaluations are capped at 10m. We median-scale prediction to groundtruth in the protocol to compensate for different camera intrinsic. In Table \ref{table:cross-dataset-hm3d}, compared with ImageNet-initialization, meta-initialization \textbf{consistently} improves in nearly all the metrics, especially  (averagely +1.97 points). 
The gain comes from that meta-prior attains a better image-to-depth mapping with coarse but smooth and reasonable depth. Conditioned on the initialization, the learning better calibrates to open-world image-to-depth relation hence generalizes better to unseen scenes. 





\begin{table}[tb!]
\begin{center}
  \caption{\textbf{Zero-Shot cross-dataset evaluation using meta-initialization (Algorithm \ref{meta-algo}).} Comparison is drawn between without meta-initialization (no marks, ImageNet-initialization) and with our meta-initialization (Meta) using different sizes ConvNeXt. Results of "+Meta" are consistently better.}
  \vspace{-4pt}
\footnotesize
  \label{table:cross-dataset-hm3d}
  
  \begin{tabular}[c]
  {
  p{2.75cm}<{\arraybackslash}|
  p{0.5cm}<{\centering\arraybackslash}|
  p{0.65cm}<{\centering\arraybackslash}|
  p{0.6cm}<{\centering\arraybackslash}|
  p{0.4cm}<{\centering\arraybackslash}|
  p{0.4cm}<{\centering\arraybackslash}|
  p{0.4cm}<{\centering\arraybackslash}}
  \hlineB{2}
  
       \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} HM3D  VA} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small & 0.267 & 0.180 & 0.389 & 74.6 & 91.0 & 96.1 \\
      ConvNeXt-small + Meta & \textbf{0.233} & \textbf{0.162} & \textbf{0.345} & \textbf{77.8} & \textbf{93.1} & \textbf{97.3} \\
      ConvNeXt-base  & 0.258 & 0.176 & 0.385 & 76.1 & 91.1 & 95.4 \\ 
      ConvNeXt-base + Meta  & \textbf{0.238} & \textbf{0.163} & \textbf{0.356} & \textbf{78.0} & \textbf{92.5} & \textbf{96.7} \\
      ConvNeXt-large  & 0.242 & 0.170 & 0.357 & 78.1 & 91.5 & 95.7 \\
      ConvNeXt-large + Meta & \textbf{0.226} & \textbf{0.160} & \textbf{0.330} & \textbf{78.9} & \textbf{92.2} & \textbf{96.3} \\
    \hlineB{2}
    


       \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} HM3D  NYUv2}  & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small  & 0.540 & 0.213 & 0.728 & 69.2 & 88.7 & 95.8 \\
      ConvNeXt-small + Meta & \textbf{0.527} & \textbf{0.206} & \textbf{0.710} & \textbf{70.7} & \textbf{89.0} & \textbf{95.9} \\
      ConvNeXt-base  & 0.529 & 0.208 & 0.717 & 70.1 & 89.4 & 96.0 \\
      ConvNeXt-base + Meta & \textbf{0.505} & \textbf{0.199} & \textbf{0.691} & \textbf{71.6} & \textbf{89.8} & \textbf{96.3} \\
      ConvNeXt-large & 0.501 & 0.192 & 0.690 & 72.0 & 90.4 & 96.4 \\ 
      ConvNeXt-large +Meta & \textbf{0.481} & \textbf{0.190} & \textbf{0.660} & \textbf{73.2} & \textbf{90.6} & \textbf{96.6} \\ 
    \hlineB{2}
    
    \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} HM3D  Replica} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small & 0.222 & 0.138 & 0.321 & 84.5 & 93.9 & 96.6 \\
      ConvNeXt-small + Meta & \textbf{0.200} & \textbf{0.126} & \textbf{0.287} & \textbf{85.6} & \textbf{95.7} & \textbf{98.1} \\
      ConvNeXt-base & 0.217 & 0.134 & 0.316 & 84.6 & 94.2 & 96.6 \\ 
      ConvNeXt-base + Meta & \textbf{0.192} & \textbf{0.117} & \textbf{0.277} & \textbf{87.1} & \textbf{96.4} & \textbf{98.5} \\
      ConvNeXt-large & 0.214 & 0.137 & 0.307 & 84.3 & 94.0 & 96.6 \\
      ConvNeXt-large + Meta  & \textbf{0.191} & \textbf{0.117} & \textbf{0.275} & \textbf{87.1} & \textbf{96.5} & \textbf{98.5} \\
    \hlineB{2}
    \hline


\multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim  VA}  & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small & 0.291 & 0.215 & 0.404 & 68.5 & 90.8 & 96.7 \\
      ConvNeXt-small + Meta & \textbf{0.280} & \textbf{0.207} & \textbf{0.398} & \textbf{70.4} & \textbf{91.3} & \textbf{97.0} \\
      ConvNeXt-base  & 0.275 & 0.201 & 0.393 & 71.3 & 91.8 & 97.3 \\
      ConvNeXt-base + Meta & \textbf{0.259} & \textbf{0.194} & \textbf{0.365} & \textbf{72.8} & \textbf{92.8} & \textbf{97.8} \\ 
      ConvNeXt-large  & 0.263 & 0.198 & 0.369 & 73.0 & 92.0 & 97.1 \\
      ConvNeXt-large + Meta  & \textbf{0.248} & \textbf{0.183} & \textbf{0.355} & \textbf{74.6} & \textbf{93.5} & \textbf{97.8} \\
    \hlineB{2}
    
    \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim  NYUv2} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small & 0.434 & 0.165 & 0.598 & 75.7 & 94.3 & 98.5 \\
      ConvNeXt-small + Meta & \textbf{0.415} & \textbf{0.155} & \textbf{0.575} & \textbf{77.8} & \textbf{95.1} & \textbf{98.8} \\
      ConvNeXt-base & 0.396 & 0.150 & 0.549 & 79.6 & 95.6 & 98.9 \\
      ConvNeXt-base + Meta & \textbf{0.386} & \textbf{0.141} & \textbf{0.524} & \textbf{80.3} & \textbf{96.0} & \textbf{99.0} \\
      ConvNeXt-large & 0.389 & 0.149 & 0.542 & 79.8 & 95.6 & 98.8 \\
      ConvNeXt-large + Meta & \textbf{0.375} & \textbf{0.140} & \textbf{0.517} & \textbf{81.2} & \textbf{96.2} & \textbf{99.1} \\
    \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim  Replica} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
      ConvNeXt-small & 0.307 & 0.189 & 0.417 & 72.4 & 92.1 & \textbf{97.5} \\
      ConvNeXt-small + Meta & \textbf{0.294} & \textbf{0.178} & \textbf{0.404} & \textbf{74.5} & \textbf{92.7} & \textbf{97.5} \\
      ConvNeXt-base  & 0.312 & 0.185 & 0.429 & 74.1 & 92.6 & 97.4 \\ 
      ConvNeXt-base + Meta & \textbf{0.288} & \textbf{0.173} & \textbf{0.399} & \textbf{75.6} & \textbf{93.3} & \textbf{97.9} \\ 
      ConvNeXt-large & 0.285 & 0.172 & 0.394 & 75.8 & 93.2 & 97.7 \\
      ConvNeXt-large + Meta & \textbf{0.273} & \textbf{0.165} & \textbf{0.380} & \textbf{77.0} & \textbf{94.0} & \textbf{98.1} \\
    \hlineB{2}
    \hline
  \end{tabular}
  \vspace{-20pt}
\end{center}
\end{table}


We further experiment on recent high-performing architecture dedicated to depth estimation, including BTS \cite{lee2019big}, DPT (hybrid and large size) \cite{Ranftl2021}, and DepthFormer \cite{li2022depthformer}. Table \ref{table:cross-dataset-dedicated} displays the comparison and shows that meta-initialization consistently improves performance for zero-shot cross-dataset inference using the dedicated architectures, gearing higher generalizability for the existing models. 




\begin{table}[tb!]
\begin{center}
  \caption{\textbf{Zero-Shot cross-dataset evaluation on dedicated depth estimation architecture.} Plugging our meta-initialization (Algorithm \ref{meta-algo}) into the those frameworks can stably improve results.}
  \vspace{-4pt}
\footnotesize
  \label{table:cross-dataset-dedicated}
  
  \begin{tabular}[c]
  {
  p{3.15cm}<{\arraybackslash}|
  p{0.5cm}<{\centering\arraybackslash}|
  p{0.65cm}<{\centering\arraybackslash}|
  p{0.55cm}<{\centering\arraybackslash}|
  p{0.35cm}<{\centering\arraybackslash}|
  p{0.35cm}<{\centering\arraybackslash}|
  p{0.35cm}<{\centering\arraybackslash}}
  \hlineB{2}
  
       \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim  Replica} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
    BTS-ResNet50 \cite{lee2019big} & 0.417 & 0.226& 0.588 & 69.4 & 87.2 & 94.0 \\
    BTS-ResNet50+Meta \cite{lee2019big} & \textbf{0.386} & \textbf{0.208} & \textbf{0.565} & \textbf{70.6} & \textbf{88.4} & \textbf{95.3}\\
    BTS-ResNet101 \cite{lee2019big} & 0.406 & 0.217 & 0.570 & 70.0 & 87.8& 94.4\\
    BTS-ResNet101+Meta \cite{lee2019big} & \textbf{0.382} & \textbf{0.206} & \textbf{0.559} & \textbf{70.8} & \textbf{88.5}& \textbf{95.4}\\
    \hline
    DepthFormer \cite{li2022depthformer} & 0.355 & 0.192 & 0.525 & 72.9 & 90.8 & 96.4 \\
      DepthFormer + our Meta  & \textbf{0.339} & \textbf{0.181} & \textbf{0.499} & \textbf{74.4} & \textbf{92.1} & \textbf{96.9} \\
      \hline
      DPT-hybrid \cite{Ranftl2021} & 0.370 & 0.197 & 0.549 & 71.3 & 89.0 & 95.7 \\ 
      DPT-hybrid + our Meta & \textbf{0.324} & \textbf{0.169} & \textbf{0.487} & \textbf{75.6} & \textbf{92.8} & \textbf{97.8} \\
      DPT-large \cite{Ranftl2021} & 0.331 & 0.172 & 0.499 & 75.4 & 91.6 & 97.1 \\
      DPT-large + our Meta & \textbf{0.314} & \textbf{0.164} & \textbf{0.474} & \textbf{77.2} & \textbf{92.7} & \textbf{97.5} \\
      
\hlineB{2}
    \hline
  
   \multicolumn{1}{|c|}{\cellcolor[HTML]{99CCFF} Hypersim  NYUv2} & \cellcolor[HTML]{FAE5D3} MAE & \cellcolor[HTML]{FAE5D3} AbsRel & \cellcolor[HTML]{FAE5D3} RMSE & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  & \cellcolor[HTML]{D5F5E3}  \\
    \hline
    BTS-ResNet50 \cite{lee2019big} & 0.487 & 0.196&0.654 & 71.8 &90.4 &95.6 \\
    BTS-ResNet50+Meta \cite{lee2019big} & \textbf{0.455}& \textbf{0.178} & \textbf{0.628} & \textbf{73.9} & \textbf{92.4} & \textbf{97.3}\\
    BTS-ResNet101 \cite{lee2019big} & 0.468 & 0.187 & 0.641 & 72.3 & 90.8 & 95.8 \\
    BTS-ResNet101+Meta \cite{lee2019big} & \textbf{0.450} & \textbf{0.175} & \textbf{0.623} & \textbf{74.2} & \textbf{92.6} & \textbf{97.5}\\
    \hline
      DepthFormer \cite{li2022depthformer} & 0.442 & 0.169 & 0.608 & 75.1 & 93.9 & \textbf{98.2} \\
      DepthFormer + our Meta  & \textbf{0.416} & \textbf{0.157} & \textbf{0.580} & \textbf{77.8} & \textbf{94.3} & \textbf{98.2} \\
    \hline
      DPT-hybrid \cite{Ranftl2021} & 0.409 & 0.149 & 0.580 & 78.9 & 94.8 & 98.3 \\ 
      DPT-hybrid + our Meta & \textbf{0.395} & \textbf{0.140} & \textbf{0.559} & \textbf{81.0} & \textbf{96.4} & \textbf{99.1} \\
      DPT-large \cite{Ranftl2021} & 0.373 & 0.136 & 0.534 & 82.3 & 96.2 & 98.8 \\
      DPT-large + our Meta & \textbf{0.364} & \textbf{0.131} & \textbf{0.520} & \textbf{83.2} & \textbf{96.6} & \textbf{99.1} \\
      
    \hlineB{2}
    \hline
  \end{tabular}
  \vspace{-25pt}
\end{center}
\end{table}


\subsection{Depth Supervision in NeRF}
\label{experiments:nerf}
\vspace{-2pt}
We show that more accurate depth from meta-initialization can better supervise the distance  a ray travels in NeRF.  is determined by the volumetric rendering rule \cite{deng2022depth}. Additional to pixel color loss, we use monocular predicted distance map , converted from depth, to supervise the training by . The experiment is conducted on Replica's office-0 environment with 180 training views. After 30K training steps, we obtain NeRF-rendered views and calculate commonly-used image quality metrics (PSNR and SSIM, the higher the better). We use ConvNeXt-base to predict . The comparison is made between using (A) without meta-initialization and (B) with meta-initialization.
Results: (A): (38.67, 0.9629), (B): (39.29, 0.9680). The results show better image quality is attained induced by better 3D representation in depth from meta-initialization. 
See more results in Supp Sec. F.






















%
