\documentclass[preprint,12pt]{elsarticle}







\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{pdfauthor={Hossein Amirkhani}}
\usepackage{xpatch}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\newcommand{\fa}[1]{{\faFont{#1}}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\setcitestyle{square}
\makeatletter






\begin{document}

\begin{frontmatter}





\title{FarsTail: A Persian Natural Language Inference Dataset}



\author{Hossein Amirkhani} \ead{amirkhani@qom.ac.ir}
\author{Mohammad Azari Jafari}
\author{Azadeh Amirak\corref{eq}}
\author{\\Zohreh Pourjafari\corref{eq}} 
\author{Soroush Faridan Jahromi\corref{eq}}
\author{Zeinab Kouhkan\corref{eq}}
\cortext[eq]{Equal contribution}
\address{Computer Engineering and IT Department, University of Qom, Iran}


\begin{abstract}
Natural language inference (NLI) is known as one of the central tasks in natural language processing (NLP) which encapsulates many fundamental aspects of language understanding. With the considerable achievements of data-hungry deep learning methods in NLP tasks, a great amount of effort has been devoted to develop more diverse datasets for different languages. In this paper, we present a new dataset for the NLI task in the Persian language, also known as Farsi, which is one of the dominant languages in the Middle East. This dataset, named FarsTail, includes 10,367 samples which are provided in both the Persian language as well as the indexed format to be useful for non-Persian researchers. The samples are generated from 3,539 multiple-choice questions with the least amount of annotator interventions in a way similar to the SciTail dataset. A carefully designed multi-step process is adopted to ensure the quality of the dataset. We also present the results of traditional and state-of-the-art methods on FarsTail including different embedding methods such as word2vec, fastText, ELMo, BERT, and LASER, as well as different modeling approaches such as DecompAtt, ESIM, HBMP, ULMFiT, and cross-lingual transfer approach to provide a solid baseline for the future research. The best obtained test accuracy is 78.13\% which shows that there is a big room for improving the current methods to be useful for real-world NLP applications in different languages. The dataset is available at \url{https://github.com/dml-qom/FarsTail}. 
\end{abstract}





\begin{keyword}
Natural language processing \sep Natural language inference \sep Persian language \sep Farsi dataset \sep Deep learning \sep Benchmark




\end{keyword}


\end{frontmatter}



\section{Introduction}
\label{sec:introduction}
Natural Language Processing (NLP) deals with the development of automatic methods for processing, analyzing, and generating human languages. It consists of a vast number of problems, ranging from low-level to high-level tasks such as named entity recognition~\citep{yadav2019survey}, sentiment analysis~\citep{keramatfar2019bibliometrics}, machine translation~\citep{yang2020survey}, and machine reading comprehension~\citep{baradaran2020survey}. One important task in NLP is Natural Language Inference (NLI) which is believed to be a stringent test for language understanding, since a system with the ability to identify the implications of natural language sentences should have a good level of language understanding~\citep{maccartney2009natural}.

The goal of NLI is to determine the inference relationship between a premise  and a hypothesis . It is a three-class problem, where each pair  is assigned to one of these classes: \textit{entailment} if the hypothesis can be inferred from the premise, \textit{contradiction} if the hypothesis contradicts with the premise, and \textit{neutral} if none of the other conditions hold. To determine the hypothesis status, some prior knowledge is considered besides the premise. This includes the knowledge that typical speakers of that language know, such as the commonsense facts and general semantic knowledge. For example, the typical English speakers know that “USA” refers to “the United States of America”.

After substantial success of deep learning (DL) based methods in different artificial intelligence tasks, the NLP researchers also started to develop DL-based models to learn the patterns in available natural language data generated by humans~\citep{otter2020survey}.  The percentage of deep learning papers nearly doubled in a six-year period from 2012 in the major NLP conferences~\citep{young2018recent}. Since these methods need a large amount of training data to let the model learn the general pattern for the particular task without overfitting to the available data, different research groups started to gather and publish large datasets. For the NLI task, the development of Stanford NLI dataset (SNLI) caused a considerable progress in developing DL-based models for NLI task~\citep{bowman2015large}.

In DL-based NLI literature, there has been a considerable amount of researches on languages with a large amount of training data, such as English, but little attention has been paid to data-poor languages. Despite some efforts in developing NLI datasets for other languages by translation or transferring knowledge obtained from learning on one language to other languages~\citep{conneau2018xnli}, presenting native datasets for other languages help develop models with more comprehensive language understanding capabilities. In addition, these datasets can be used to evaluate the proposed learning architectures and methods for a broader range of languages. 

The focus of this paper is on Persian (Farsi) language which is a pluricentric language spoken and used by around 110 million people in countries such as Iran, Afghanistan, and Tajikistan. It has had a considerable influence on its neighboring languages such as Turkic, Armenian, Georgian, and Indo-Aryan languages. Its alphabet includes 32 characters written right to left. Table~\ref{tab:farsi-difficulties} shows some features of Persian language which make its processing different from other languages.

\begin{table}[!t]
\caption{Some features of Persian language which make its processing different from other languages.}
    \label{tab:farsi-difficulties}
    \centering
    \includegraphics[width=\textwidth]{Images/persian.pdf}


\end{table}

In this paper, we present, to the best of our knowledge, the first relatively large-scale Persian corpus for NLI task, called FarsTail. We tried to reduce the amount of annotation interventions to provide realistic samples which are naturally occurring in real-world applications instead of task-specific synthesized examples. A protocol similar to the SciTail dataset is followed~\citep{khot2018scitail} where the sentences are either generated from multiple-choice questions with the least amount of intervention or selected from natural sentences that already exist independently “in the wild”. 



Each person generates three data examples from a multiple-choice question, one for each class, with the same premises but different hypotheses. The \textit{entailment} hypothesis is formed by substituting the correct answer in the question. Then, a text snippet is extracted from web that the generated hypothesis can be inferred from. The \textit{contradiction} hypothesis is formed by substituting one wrong answer in the question. Finally, the \textit{neutral} hypothesis is extracted from web such that it is similar to the question but with an unknown status based on the premise. In the next phase, each sample is relabeled by four other persons and the samples with at least 4 out of 5 agreements are preserved. The rejected samples undergo a new modification and relabeling phase. 

A total of 10,367 samples are generated from a collection of 3,539 multiple-choice questions. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances, respectively. We ensure that the instances with the same premises are in the same set. The developed dataset can also be used in other tasks such as question answering, summarization, semantic search, and machine translation. The developed dataset (as raw texts for Persian researchers and indexed data for non-Persian researchers) has been released for non-commercial usages. 

We evaluate different traditional and state-of-the-art methods on FarsTail, including different embedding methods such as word2vec~\citep{mikolov2013distributed}, fastText~\citep{bojanowski2017enriching}, ELMo~\citep{peters2018deep}, BERT~\citep{devlin2018bert}, and LASER~\citep{artetxe2019massively}, as well as different modeling methods such as DecompAtt~\citep{parikh2016decomposable}, ESIM~\citep{chen2016enhanced}, HBMP~\citep{talman2019sentence}, and ULMFiT~\citep{howard2018universal}. We also investigate the cross-lingual transfer learning approach by translating the train/test datasets~\citep{conneau2018xnli}. The best obtained accuracy on test set is 78.13\% which shows that there are many rooms to improve the models trained on this dataset. 

The merits of the proposed dataset over existing non-English datasets such as XNLI~\citep{conneau2018xnli} are:
\begin{itemize}
    \item In FarsTail, task-specific human-generated texts are kept as low as possible to provide texts which are naturally occurring in real-world applications. To this end, FarsTail is collected by a protocol similar to SciTail dataset~\citep{khot2018scitail}; however, in contrast to SciTail which only contains the neutral and entailment classes, we also include contradiction examples in the dataset. 
    \item FarsTail is not generated by translating from other languages, so it contains first-hand native examples without translation clues. In addition, it does not suffer from the risk of removing some semantic relations by translation because of cultural differences. 
    \item Since FarsTail is based on real textual contents, where the sentences are constructed from real questions or selected from web, a model trained on this dataset can be used in other NLP tasks such as question answering and machine translation. 
\end{itemize}

The rest of this paper is organized as follows. In Section~\ref{sec:literature}, the available English and non-English NLI datasets are reviewed. Section~\ref{sec:farstail} presents the FarsTail development process as well as its statistics. The experimental results are presented in Section~\ref{sec:experiments}, and the paper concludes in the last section. 

\section{Related work}
\label{sec:literature}
In this section, we review some available English and non-English NLI datasets. 

\subsection{English NLI datasets}
\begin{itemize}
    \item SICK~\citep{marelli2014semeval}: As one of the first attempts to introduce relatively large-scale datasets for NLI task, this dataset was introduced as a task in SemEval-2014. It consists of about 10k English sentence pairs annotated for two different tasks, relatedness in meaning and entailment. The original sentence pairs are randomly selected from 8k ImageFlickr dataset and the SemEval 2012 STS MSR-Video Description dataset. Some rule-based syntactic and lexical transformations are applied to each sentence to obtain sentences with similar, contradictory, and different meanings. Its partly automated construction introduced some spurious patterns into the data~\citep{bowman2015large}. 
    \item SNLI~\citep{bowman2015large}: The Stanford NLI dataset has been developed to alleviate the lack of large-scale annotated data for the NLI problem. It includes 570k labeled instances (550k training, 10k validation, and 10k test examples) gathered using the Amazon Mechanical Turk. An image caption was presented to each turker as the premise and they were asked to generate three sentences as hypothesis, one for each class (entailment, contradiction, and neutral). In the relabeling phase, if at least three out of four new labelers agreed with the main label, this instance was kept in the dataset. This dataset played a considerable role in developing and enhancing deep learning-based NLI systems.  
    \item MultiNLI~\citep{williams2017broad}: Compared to SNLI, MultiNLI covers 10 different genres of spoken and written text. With 433k instances, its scale is comparable to SNLI. The test set consists of two parts: matched set which includes the same genres in the training set and mismatched set which includes genres not available in the training set. This allows for cross-genre generalization evaluation. 
    \item MedNLI~\citep{romanov2018lessons}: This dataset was generated by the same approach as SNLI, adjusted for the clinical domain. The MIMIC-III v1.3~\citep{johnson2016mimic}, with de-identified records of 38,597 patients, was used as the premise source. The hypothesis sentences were generated by clinicians. Four clinicians worked on a total of 4,683 premises over a period of six weeks, which resulted in 14,049 unique sentence pairs. 
    \item SciTail~\citep{khot2018scitail}: This is the first NLI dataset which is collected using the available texts without authoring the sentences. This makes the dataset more realistic, since it consists of natural texts instead of task-specific synthesized sentences. SciTail is the most similar dataset to the dataset presented in this paper. The hypotheses were created from science questions and their corresponding answers, and premises were gathered from the relevant web sentences. It contains 1,834 questions with 10,101 entailment instances and 16,925 neutral ones. This dataset does not contain the contradiction label. 
    \item QA-NLI~\citep{demszky2018transforming}: This dataset is similar to SciTail, except that it was fully automatically constructed. The authors proposed a method to derive NLI datasets from the question answering datasets. This was done by introducing the QA2D task to derive a declarative sentence from a question-answer pair. The generated sentence  along with the corresponding passage  forms an NLI example as . For the correct, incorrect, and unknown answers, the pairs were labeled as entailment, contradiction, and neutral, respectively. Note that incorrect answers are available in QA datasets with multiple answers, and unknowns are also available in some datasets such as SQuAD 2.0~\citep{rajpurkar2018know}. 
\end{itemize}

\subsection{Non-English NLI datasets}
\begin{itemize}
    \item Evalita~\citep{bos2009textual}: This dataset was constructed to infer the entailment relationship between short Italian sentence pairs. It contains 800 pairs, constructed on the basis of Wikipedia revision histories. 
    \item ArbTEDS~\citep{alabbas2013dataset}: This dataset contains 600 Arabic pairs annotated as either inferable or non-inferable. A semi-automatic tool was used to extract the candidate pairs from web, using the Arabic news headlines as the hypothesis and one paragraph returned by the Google-API for this headline as the premise. The pairs were then annotated by eight annotators. 
    \item German emails~\citep{eichler2014analysis}:  This dataset was constructed from the customer emails of a multimedia software company to its support center as premises and the categories descriptions as the hypotheses. The matching and non-matching categories were considered as entailment and non-entailment hypotheses, respectively. It contains 638 entailment and 24,143 non-entailment pairs. 
    \item ASSIN~\citep{fonseca2016overview}: This dataset contains 10,000 pairs, half in Brazilian Portuguese and half in European Portuguese. It is a two-class problem with the entailment and not-entailment classes. 
    \item XNLI~\citep{conneau2018xnli}: This dataset was developed for evaluating the cross-lingual understanding capabilities of models. The same crowdsourcing-based procedure used for MultiNLI dataset~\citep{williams2017broad} was followed to collect and validate 750 examples from each of ten text sources resulted in a total of 7,500 examples. These examples were then translated into 14 different languages by professional translators. The total 112,500 annotated pairs are in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili, and Urdu languages. For a discussion of the advantages of our proposed FarsTail dataset over XNLI refer to Section~\ref{sec:introduction}.
\end{itemize}

\section{FarsTail dataset}
\label{sec:farstail}
In this section, we present the process of developing FarsTail dataset as well as its statistics. FarsTail has been developed with a process similar to the SciTail dataset~\citep{khot2018scitail} with some modifications. A group of five persons (called annotators herein) with a background in NLI worked under the supervision of an NLP expert to develop FarsTail. The taken steps are depicted in Fig.~\ref{fig:farstail-steps} which include generating NLI instances from multiple-choice questions, relabeling, and data cleaning. The details of these steps are given in Sections~\ref{subsec:generate}~and~\ref{subsec:relabel}, and the dataset statistics are presented in Section~\ref{subsec:statistics}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{Images/steps.pdf}
    \caption{The FarsTail dataset development steps.}
    \label{fig:farstail-steps}
\end{figure}

\subsection{Generating NLI instances from questions}
\label{subsec:generate}
A collection of 3,539 multiple-choice questions was gathered from Iranian university exams in different topics including religion, history, constitution of Iran, history of literature, and Islamic revolution. For each multiple-choice question, an annotator followed the following steps to generate three different pairs, one for each class (entailment, contradiction, and neutral):
\begin{enumerate}
    \item The correct answer is inserted into the question to generate a sentence called . 
    \item The web is searched to find a text portion  where   has entailment relation. We use the available texts on the web instead of generating the premises to provide real-world, naturally occurring texts instead of task-specific synthesized examples. 
    \item An incorrect answer is inserted into the question to generate a sentence called  such that  has contradiction relation. The annotator is asked to generate  similar to  in length, but different in structure and words. 
    \item From the web, a related sentence  is found with a similar length to  and  such that its entailment or contradiction relation cannot be inferred from . The pair  is considered as a neutral instance.
\end{enumerate}
Fig.~\ref{fig:generation} shows an example of the sample generation process in FarsTail. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{Images/sample-generation.pdf}
    \caption{An example of generating NLI instances from questions in FarsTail.}
    \label{fig:generation}
\end{figure}

\subsection{Relabeling and data cleaning}
\label{subsec:relabel}
After the sample generation phase, each sample was relabeled by the other four annotators retaining the samples with an agreement of at least 80\% among five labelers. The samples were presented to the annotators in a random order to reduce annotation bias caused by presenting the samples with the same premise in succession. To give the rejected samples one more chance, they were revised by their original annotator and relabeled again. The samples which could not obtain a 80\% label agreement in any of these two relabeling phases were removed. Among all 10,617 samples , 190 samples were removed in this phase resulting in 10,427 instances.

The retained samples were investigated one more time for spelling and writing mistakes emphasizing on avoiding probable label change caused by cleaning. Finally, to reduce the unwanted repetition in the data, 60 more samples were removed including the instances generated from different questions which both their premises and hypotheses had a cosine similarity higher than 0.8. The total number of samples in the dataset is therefore 10,367. 

The instances were randomly divided into training, validation, and test sets such that the samples generated from the same question were in the same subset. In addition, to avoid information leak, the samples generated from different questions which either their premises or hypotheses had a cosine similarity higher than 0.9 were included in the same subset. The training, validation, and test sets percentages are nearly 70/15/15 with 7,266, 1,537, and 1,564 samples, respectively.

The dataset is presented in two formats, raw and indexed. The raw data includes the Persian sentences, while the indexed data is a tokenized version of sentences where each sentence is encoded as a list of word indexes (integers)\footnote{Hazm python library was used for tokenization (\url{https://github.com/sobhe/hazm})}. The final dataset as well as an API for accessing data and the trained models have been released for non-commercial usages\footnote{\url{https://github.com/dml-qom/FarsTail}}. 

\subsection{FarsTail statistics}
\label{subsec:statistics}
The statistics of FarsTail dataset is presented in Table~\ref{tab:statistics}. To provide the possibility for comparing different subsets, there is one section for each of train, validation, and test sets. For each of these sets, beside the total statistics, the statistics for different classes are also shown separately where E, C, and N stand for \textit{entailment}, \textit{contradiction}, and \textit{neutral} classes, respectively.

\begin{table}[t!]
    \centering
    \caption{Statistics of the FarsTail dataset.}
    \label{tab:statistics}
    \begin{tabular}{c c|P{1.2cm} P{1.2cm} P{1.2cm} P{1.2cm} P{1.2cm} P{1.2cm} P{1.2cm}}
         subset & class & \parbox{1.2cm}{\centering samples} & \parbox{1.2cm}{\centering prem.\\ tokens} & \parbox{1.2cm}{\centering hyp.\\ tokens} & \parbox{1.2cm}{\centering prem.\\ proc.\\ tokens} & \parbox{1.2cm}{\centering hyp.\\ proc.\\ tokens} & overlap & \parbox{1.2cm}{\centering proc.\\ overlap} \\
         \hline
         \multirow{4}{*}{Train} & E & 2,429 & 40.50 & 15.53 & 19.35 & 8.42 & 0.67 & 0.68\\
         & C & 2,389 & 40.23 & 15.61 & 19.20 & 8.30 & 0.57 & 0.54\\
         & N & 2,448 & 40.52 & 15.62 & 19.31 & 8.26 & 0.40 & 0.30\\
         & Total & 7,266 & 40.42 & 15.59 & 19.29 & 8.33 & 0.55 & 0.51\\
         \hline
         \multirow{4}{*}{Val} & E & 515 & 39.70 & 14.85 & 19.13 & 8.27 & 0.67 & 0.66\\
         & C & 499 & 39.58 & 15.09 & 19.17 & 8.11 & 0.58 & 0.54\\
         & N & 523 & 39.71 & 14.95 & 19.16 & 8.06 & 0.39 & 0.29\\
         & Total & 1,537 & 39.67 & 14.96 & 19.15 & 8.14 & 0.54 & 0.50\\
         \hline
         \multirow{4}{*}{Test} & E & 519 & 39.57 & 15.48 & 18.84 & 8.39 & 0.68 & 0.68\\
         & C & 510 & 39.44 & 15.81 & 18.86 & 8.38 & 0.57 & 0.52\\
         & N & 535 & 39.23 & 16.02 & 18.73 & 8.36 & 0.38 & 0.27\\
         & Total & 1,564 & 39.41 & 15.78 & 18.81 & 8.38 & 0.54 & 0.49\\
    \end{tabular}
\end{table}

The column ``samples" of Table~\ref{tab:statistics} shows the number of samples in each subset. As mentioned in Section~\ref{subsec:relabel}, 70/15/15\% of data go to the train, validation, and test sets, respectively. It can be seen that this is a balanced dataset without any meaningful differences between the number of samples in different classes. 

The next column (premise tokens) presents the average number of tokens in the premises obtained by the Hazm python library's tokenizer. The next column (hypothesis tokens) shows the same values for hypothesis sentences. To provide a more meaningful length statistic, the next two columns (premise processed tokens and hypothesis processed tokens) report the number of unique tokens ignoring stopwords\footnote{A stoplist with 389 words was used from Hazm library.} as well as one-character tokens including punctuations. It is worth mentioning that there are a total of 20,973 tokens in FarsTail dataset where 467 tokens are stopwords or one-character tokens. 

According to these four ``tokens" columns, there is not any significant difference between the average number of tokens in train, validation, and test sets. More importantly, the average number of tokens in different classes are almost the same which shows that the length of premises and hypotheses cannot be exploited as a feature to find clues about the class of the given inputs. 

One more point to consider about the ``tokens" columns is that the premises in FarsTail are longer than the premises in SciTail dataset~\citep{khot2018scitail}. The reported average premise length for \textit{entail} and \textit{neutral} samples in SciTail training set are 10.79 and 10.28, respectively, while these numbers are 19.35 and 19.31 in FarsTail. Regarding hypotheses, the average length for \textit{entail} and \textit{neutral} samples are respectively 6.69 and 7.01 which are almost the same as FarsTail (8.42 and 8.26). These longer premises are due to the FarsTail's sample generation process where we insisted on finding exact web text portions which the hypothesis could be inferred from. Anyway, this makes FarsTail a more challenging dataset since it seeks more reasoning to connect the facts presented in longer premises. 

Finally, the last two columns show the average proportion of the hypothesis tokens that overlap with the premise. Both columns treat the sentences as a set of tokens ignoring the word repetition, but the second column also ignore the stopwords and one-character tokens. As expected, the most and the least overlap between premise and hypothesis are in the \textit{entailment} and \textit{neutral} samples, respectively. This shows that there are some superficial clues in the samples which can be exploited to estimate the relationship between two sentences without truly understanding them. In the next section, we show that the mere similarity between premise and hypothesis can be used in a simple baseline model which obtains an accuracy higher than random; however, this accuracy is far from that obtained by more advanced deep models.

\section{Experiments}
\label{sec:experiments}
In this section, we present the results of different traditional and deep learning-based methods on the FarsTail dataset to provide a baseline for future researches. In Section~\ref{subsec:models}, we introduce the evaluated models, and in Section 4.2, the results are presented and discussed. 

\subsection{Models}
\label{subsec:models}
We used different methods for representing the input sentences ranging from traditional TF-IDF to more recent word embedding methods such as word2vec\footnote{\url{http://vectors.nlpl.eu/repository}}~\citep{mikolov2013distributed}, fastText\footnote{\url{https://fasttext.cc/docs/en/crawl-vectors.html}}~\citep{bojanowski2017enriching}, ELMo\footnote{\url{https://github.com/HIT-SCIR/ELMoForManyLangs}}~\citep{peters2018deep}, and BERT\footnote{\url{https://github.com/imgarylai/bert-embedding}}~\citep{devlin2018bert}. Beside these representations, to investigate the ability of a model to determine the relationship between a given premise and hypothesis just using their word-level similarity, we also considered the simple method of representing a given pair by the cosine similarity between their bag-of-word vectors. 

As the classifier, we exploited different general models including Support Vector Machine (SVM), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) along with three models developed specially for the NLI task including DecompAtt~\citep{parikh2016decomposable}, ESIM~\citep{chen2016enhanced}, and HBMP~\citep{talman2019sentence}. 

One popular approach in learning with small labeled training NLP datasets is to train a language model (LM) on a large unlabeled corpus and fine-tune it on the downstream task. We used ULMFiT~\citep{howard2018universal} which is one of the popular methods in this line with three steps: LM pre-training, LM fine-tuning, and classifier fine-tuning. In the first step, a language model is trained on a general-domain corpus. We used the Persian Wikipedia for this purpose. Then, the trained LM is fine-tuned on the target task texts without considering their labels. Finally, the pre-trained language model is augmented with additional layers which are trained on the labeled dataset of the target task. 

As another strategy, we evaluated the cross-lingual transfer approach as is investigated for XNLI dataset~\citep{conneau2018xnli}. We adopted two simple translation-based baselines, \textit{Translate-Source} and \textit{Translate-Target}. In \textit{Translate-Source} approach, we translated the training data of MultiNLI dataset to Persian language and trained an ESIM model on the union of this translated set and the FarsTail's training set. Then, the FarsTail's training set was used one more time to fine-tune the trained model. The Google Translate service was exploited for translation. In the \textit{Translate-Target} approach, we trained an ESIM model on the union of original MultiNLI training set and the English-translated version of FarsTail's training set, which was then fine-tuned on the FarsTail's English training set. For estimating the label of a given Persian input, its English translation is fed into the trained model. 

We also tested the LASER embedding\footnote{\url{https://github.com/facebookresearch/LASER}}~\citep{artetxe2019massively} as a space which is shared between multiple languages. Since LASER provides sentence embeddings rather than word embeddings, a simple deep model was trained on the computed representations. 



\subsection{Results and discussion}
\label{subsec:results}

Table~\ref{tab:general-classifiers} shows the results obtained from training general classifiers on FarsTail's training set. These classifiers include SVM, LSTM, and BiGRU. For each classifier, different representation methods are investigated. In the \textit{cosine} representation, we use the simple cosine similarity between the count vectors of the premise and hypothesis. This is a simple baseline to investigate the model which just exploits the similarity between the premise and hypothesis to decide about their inference relationship. The obtained 57.54\% test accuracy shows that the mere similarity between premise and hypothesis can be used to obtain an accuracy higher than random, but far better accuracies can be obtained by more advanced techniques as is elaborated in the rest of this section. According to Fig.~\ref{fig:confusion} (SVM + Cosine), this simple baseline obtains a good performance in distinguishing the \textit{neutral} from the other two classes which is compatible with the overlap statistics presented in Table~\ref{tab:statistics}, where the overlap between premises and hypotheses in the \textit{neutral} class is clearly different from that in the other two classes. On the other hand, the worst performance of this simple similarity-based baseline is in the \textit{contradiction} class where the model is nearly random. This is because contradiction needs a higher level of inference to be determined. 

For each of the other representations in Table~\ref{tab:general-classifiers}, we use two different versions, \textit{separate} and \textit{concat}. In the \textit{separate} approach, the premise and hypothesis are represented separately and the obtained representations are then concatenated; while in the \textit{concat} method, they are concatenated by a unique character in the middle before representation calculation. Note that the LASER and tf-idf representations are just used with the SVM classifier because they deliver sentence-level representations which cannot be used with the word-level methods like LSTM and BiGRU. On the other hand, to feed the SVM classifier with the word-level representations including word2vec, fastText, ELMo, and BERT, we compute a tf-idf-weighted average of these word representations for each sentence.

In almost all rows in Table~\ref{tab:general-classifiers}, the \textit{separate} approach obtains a better performance than the \textit{concat} one, except the BERT embedding where the \textit{concat} representation obtains far better results than the \textit{separate} approach. This is due to the way BERT is pre-trained with a special separator token added between sentence pairs. The best test accuracy in this table is obtained using the concatenated version of BERT representation using the LSTM classifier which shows the power of BERT embedding as is known in the community. The corresponding confusion matrix is presented in Fig.~\ref{fig:confusion} showing the success of this method in improving the accuracy in all classes specially the \textit{contradiction} class.

\begin{table}[p]
    \centering
    \caption{Validation and test set accuracy of general classifiers trained on FarsTail's training set using different sentence representations.}
    \label{tab:general-classifiers}
    \begin{tabular}{P{3cm} c c c}
        Classifier & Representation & Val Accuracy & Test Accuracy\\
        \hline
         \multirow{13}{*}{SVM} & cosine & 0.5485 & 0.5754\\
         & LASER (separate) & 0.5459 & 0.5198\\
         & LASER (concat) & 0.4938 & 0.4808 \\
         & tf-idf (separate) & 0.5303 & 0.5301\\
         & tf-idf (concat) & 0.4502 & 0.4495\\
         & word2vec (separate) & 0.5120 & 0.5448\\
         & word2vec (concat) & 0.3975 & 0.4201\\
         & fastText (separate) & 0.5296 & 0.5371\\
         & fastText (concat) & 0.4112 & 0.4175\\
         & ELMo (separate) & 0.5621 & 0.5710\\
         & ELMo (concat) & 0.4457 & 0.4604\\
         & BERT (separate) & 0.5745 & 0.5575\\
         & BERT (concat) & \textbf{0.6532} & \textbf{0.6752}\\
         \hline
         \multirow{8}{*}{LSTM} & word2vec (separate) & 0.5172 & 0.5243\\
         & word2vec (concat) & 0.4932 & 0.5006\\
         & fastText (separate) & 0.5205 & 0.5192\\
         & fastText (concat) & 0.5068 & 0.5147\\
         & ELMo (separate) & 0.5478 & 0.5505\\
         & ELMo (concat) & 0.5407 & 0.5428\\
         & BERT (separate) & 0.5394 & 0.5249\\
         & BERT (concat) & \textbf{0.7534} & \textbf{0.7583}\\
         \hline
         \multirow{8}{*}{BiGRU} & word2vec (separate) & 0.5192 & 0.5224\\
         & word2vec (concat) & 0.4951 & 0.4942\\
         & fastText (separate) & 0.5211 & 0.5243\\
         & fastText (concat) & 0.5062 & 0.5166\\
         & ELMo (separate) & 0.5582 & 0.5428\\
         & ELMo (concat) & 0.5368 & 0.5454\\
         & BERT (separate) & 0.5348 & 0.5301\\
         & BERT (concat) & \textbf{0.7625} & \textbf{0.7558}\\
    \end{tabular}
\end{table}

In Table~\ref{tab:nli-specific}, the results of DecompAtt~\citep{parikh2016decomposable}, ESIM~\citep{chen2016enhanced}, and HBMP~\citep{talman2019sentence} methods trained on the FarsTail's training set are presented. These methods are specifically published for the NLI task. For each of these methods, the confusion matrix of the best model is also depicted in Fig.~\ref{fig:confusion}. 
For the ESIM and HBMP methods, all representations obtain almost similar accuracies; while for the DecompAtt method, word2vec outperforms other embeddings by a large margin. 

\begin{table}[t!]
    \centering
    \caption{Validation and test set accuracy of NLI-specific models trained on FarsTail's training set using different sentence representations.}
    \label{tab:nli-specific}
    \begin{tabular}{P{3cm} c c c}
        Model & Representation & Val Accuracy & Test Accuracy\\
        \hline
         \multirow{4}{*}{DecompAtt} & word2vec & \textbf{0.6597} & \textbf{0.6566}\\
         & fastText & 0.6051 & 0.5831\\
         & ELMo & 0.5719 & 0.5505\\
         & BERT & 0.5999 & 0.5722\\
         \hline
         \multirow{4}{*}{ESIM} & word2vec & 0.7028 & 0.7110\\
         & fastText & 0.7033 & \textbf{0.7136}\\
         & ELMo & 0.6903 & 0.6873\\
         & BERT & \textbf{0.7189} & \textbf{0.7136}\\
         \hline
         \multirow{4}{*}{HBMP} & word2vec & \textbf{0.6617} & \textbf{0.6604}\\
         & fastText & 0.6584 & 0.6521\\
         & ELMo & 0.6467 & 0.6349\\
         & BERT & 0.6526 & 0.6432\\
    \end{tabular}
\end{table}

Finally, Table~\ref{tab:advanced} shows the results obtained by language modeling and cross-lingual transfer approaches. First, the ULMFiT method~\citep{howard2018universal} is applied on the FarsTail's training data, which is a language modeling trained on the Persian Wikipedia and fine-tuned on the target task (details are presented in Section~\ref{subsec:models}). Then, the ESIM model~\citep{chen2016enhanced}, as a NLI-specific method, is trained using the BERT embeddings of not just the FarsTail data but also the MultiNLI samples~\citep{williams2017broad}. Note that the BERT used in our experiments is a multilingual embedding including a shared space for different languages. The usage of the vast number of samples available in the English MultiNLI dataset makes a clear improvement in the obtained accuracy, where the test accuracy jumps from 0.7136 (Table~\ref{tab:nli-specific}) to 0.7462 (Table~\ref{tab:advanced}). 

We also report the results obtained by two simple translation-based approaches, i.e., \textit{Translate-Source} and \textit{Translate-Target}. In \textit{Translate-Source}, the union of FarsTail's training data and Persian-translated MultiNLI training set is used to train an ESIM model; while in \textit{Translate-Target}, the FarsTail's training data is translated to English to be used along with the original MultiNLI data (for more details refer to Section~\ref{subsec:models}). The results show that the \textit{Translate-Source} approach, which uses the target language (Persian) as the model's native language, is much more successful than \textit{Translate-Target}. This is due to the fact that translating the training data from the original Persian language to other languages, as is done in \textit{Translate-Target}, removes some useful target-specific clues; while in \textit{Translate-Target}, we use the available data in other languages but preserve our valuable samples in the target language without any manipulation. This is specially true in our experiments where the source and target domains are different; while in XNLI experiments~\citep{conneau2018xnli} with similar target and source domains, \textit{Translate-Target} obtains better results. 

The results in Table~\ref{tab:advanced} demonstrate the usefulness of transfer learning in data-poor languages. 
Note that our best overall result is obtained using the \textit{Translate-Source} approach with fastText embeddings. Anyway, this 78.13\% test accuracy shows that there is a big room for improving the current methods to be useful for real-world NLP applications in different languages.

\begin{table}[t!]
    \centering
    \caption{Validation and test set accuracy obtained by language modeling and cross-lingual transfer approaches applied to FarsTail's training set.}
    \label{tab:advanced}
    \begin{tabular}{P{3cm} P{4cm} c c}
        Method & Representation & Val Accuracy & Test Accuracy\\
        \hline
         ULMFiT & Learned & 0.7281 & 0.7244 \\
         ESIM & \small {BERT (FarsTail + MultiNLI)} & 0.7419 & 0.7462\\
         \hline
         \multirow{3}{*}{Translate-Source} & word2vec & 0.7534 & 0.7653\\
         & fastText & \textbf{0.7778} & \textbf{0.7813}\\
         & BERT & 0.7358 & 0.7519\\
         \hline
         \multirow{3}{*}{Translate-Target} & word2vec & 0.6714 & 0.6822\\
         & fastText & 0.7024 & 0.7046\\
         & BERT & 0.6802 & 0.6899\\
    \end{tabular}
\end{table}

\begin{figure}[t!]
    \begin{subfigure}{0.40\textwidth}
        \includegraphics[width=\textwidth]{Images/SVM-Cosine.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/LSTM-BERT.pdf}
    \end{subfigure}
    
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/DecompAtt-word2vec.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/hbmp-word2vec.pdf}
    \end{subfigure}
    
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/ESIM-BERT.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/ESIM-BERT-Multi.pdf}
    \end{subfigure}
    
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/ULMFiT.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{Images/TranslateSource-fastText.pdf}
    \end{subfigure}
    \caption{Confusion matrices of different models for FarsTail test set.}
    \label{fig:confusion}
\end{figure}

\section{Conclusion}
In this paper, we introduced, to the best of our knowledge, the first relatively large-scale NLI dataset for Persian language. We presented the details of the FarsTail development process, which is carefully designed to ensure the data quality. We also presented the dataset statistics as well as the results of some traditional and state-of-the-art methods on it. FarsTail is freely available for non-commercial purposes for both Persian researchers as well as non-Persian ones, since we have presented an indexed version of the dataset along with the raw samples. 

Due to the usage of multiple-choice questions in developing the FarsTail dataset, these questions along with their corresponding premises can also be exploited in the machine reading comprehension task. In the future, we plan to present this MRC dataset as a byproduct of FarsTail. 
Finally, since the best obtained result on the FarsTail's test set, even using SOTA methods, was 78.13\%, we hope it invokes more research on developing methods which are applicable to real-world NLP tasks in different languages, specially data-poor ones. 

\bibliographystyle{elsarticle-num} 
\bibliography{references}

\end{document}
