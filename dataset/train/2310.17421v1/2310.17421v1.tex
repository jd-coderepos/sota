\documentclass{fcs}

\usepackage{bm}
\usepackage{verbatim} 
\usepackage{url} 
\usepackage[table]{xcolor}
\usepackage[normalem]{ulem}

\usepackage[top=2cm,left=2cm,right=2cm]{geometry}

\volumn{ }
\doi{ }
\articletype{RESEARCH~ARTICLE}
\copynote{{\copyright} PREPRINT}
\ratime{}
\email{fronchetti at lidi.info.unlp.edu.ar}
\title{\
\label{delta} 
\difj = \aifjp - \aifj, \quad f=1,\dots,F-1

\label{window} 
\wif = \left[ \dif, \difp, \dots, \difw \right], \quad f=1,\dots,\fw

\label{histogram}
\hl = \frac{ | \, \{ \wf \, | \, BMU(\wf) = \cl  \} \, | }{ \fw }, \quad l=1,\dots,K

\label{factorization}
P(c | \ac) = P(c | \cl) P( \cl | \ac)

\label{classactionmodel}
P(c | \ac) \propto \sum_{l=1}^{K} P(c | \cl) P( \cl | \ac) = \sum_{l=1}^{K} P(c | \cl) * \hl

\label{classwdfmodel}
P(c | \cl) = \frac{ | assigned(\cl) \cap ofClass(c) |}{|assigned(\cl)| }

\label{assigned}
assigned(\cl)= \{ \wif | BMU(\wif)=\cl, \\
\, i=1,\dots,N, \, f=1,\dots,\fw \}

\label{ofClass}
ofClass(c)= \{ \wif | c_i=c, \, f=1,\dots,\fw \}

c^{*} = \max_c P(c | \ac), \quad c=1,\dots,C


where  is the number of classes,  is defined as in eq. \ref{classactionmodel}, and  and  are estimated as in eqs. \ref{classwdfmodel} and \ref{histogram} respectively.

 
\section{Experiments}
\label{experiments}


We evaluated the descriptor on two well known and publicly available datasets, MSRC12 \cite{fothergill2012instructing} and MSR Action3D \cite{li2010action}, both acquired using a Kinect Sensor, which captures 20 joints of the human body (Figure \ref{kinect_joints}). \footnote{A detailed description of how we prepared the datasets and which action instances were corrected/left out, scripts to read the files, and code to run the experiments can be found in \url{http://facundoq.github.io/publications/dam/action_descriptor/}} 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{img/joints-circle-contrast.pdf}
\caption{Joints detected by Kinect}
\label{kinect_joints}
\end{figure}


For both datasets, we tested various values for , the window size, to evaluate the importance of including temporal information in the descriptor and , the number of clusters, to establish if the classifier was robust in a wide variety of settings of this parameter. The amount of frames the actions are resampled to, , was set to 25 for all experiments with Action3D.  For the MSRC12 dataset, since the actions are easier to separate by means of joint activation patterns instead of trajectory information, the actions were resampled to  frames without losing accuracy. After computing the WDFs, they were rescaled to euclidean norm  so as to provide scale invariance to the descriptor.

\subsection{MSR Action3D Dataset}
\label{action3d}
\begin{figure*}[t]
\doublerulesep 0.1pt
\begin{footnotesize}
\begin{tabular}{ c c c } 
\includegraphics[height=50mm]{figures/acc_action3D_AS1_graph.pdf} &
\includegraphics[height=50mm]{figures/acc_action3D_AS2_graph.pdf} &
\includegraphics[height=50mm]{figures/acc_action3D_AS3_graph.pdf} \\
AS1 & AS2 & AS3 \\
\end{tabular}
\centering
\wrappingcaption{Results obtained with our descriptor for MSR-Action3D, with different values for , the window size, and , the amount of clusters. Each result is the average of 30 independent runs.}
\label{figure_acc_Action3D}
\end{footnotesize}
\end{figure*}

We used the MSR Action3D dataset \cite{li2010action} which consists of action sequences of 20 different types of actions performed by 10 subjects. Each sequence consists of an action, segmented so that the start of the sequence roughly corresponds to that of the action, and the same for the end. For each sequence the dataset contains the position of the 20 joints for all the frames captured, which are time-labeled.

There are 567 sequences in total, of which we used 526. The original authors had already left out 10 sequences out of the 567; we excluded 31 more that we found heavily corrupted and corrected 48 in which some joints positions were exchanged\cite{li2010action}. 

\subsubsection{Experimental setup}
We employed the typical experimental setup on this dataset \cite{li2010action}, which divides the action classes into three action sets (AS1, AS2 and AS3), each composed of the actions of 8 classes, with some overlap between action sets.


We performed experiments on each set separately, following a cross subject protocol with a 50/50 subject split \cite{li2010action}. We performed 30 cross validation runs for each parameter configuration, and computed the average classification rate for each over the 30 runs. 

Our best results were obtained with a window size  and  clusters obtained after training a  SOM network.



\subsubsection{Results}

\begin{table}
\begin{footnotesize}

\wrappingcaption{Comparison of results on the MSR-Action 3D dataset.}
\begin{tabular}{ | p{5cm} | c | } 
	 \hline
	 \rowcolor{lightgray}
	 \centering \textbf{Method} & \textbf{ Accuracy (\%)}  \\ 
	 \hline
	 Ellis (logistic regression) \cite{Ellis2013} & 65.70 \\ 
	 \hline
 	 Li (Action Graph) \cite{li2010action} & 74.70 \\ 
	 \hline
	 Wang (Random Occupancy Pattern)  \cite{wang2012robust} & 86.50 \\ 
	 \hline
	 Wang (Actionlets Ensemble) \cite{wang2012actionlet} & 88.20 \\ 
	 \hline
	 Hussein (Cov3DJ) \cite{hussein2013human} & 90.53\\ 
	 \hline	 
	 \textbf{Proposed descriptor} & \textbf{94.07}  \\ 
	 \hline
\end{tabular}

\label{table_acc_action3D_comparativa}
\end{footnotesize}
\end{table}

Table \ref{table_acc_action3D_comparativa} compares the results obtained with the proposed method against other state-of-the-art models. Our descriptor achieves an accuracy of 94.07\%, reducing the error in  with respect to the best method. 




Figure \ref{figure_acc_Action3D} displays the different accuracies obtained by varying the parameters of the method for each subset of the dataset. It is evident that the effect of the windowing scheme is beneficial in almost all cases. On the other hand, the effect of the number of clusters in the model is mostly insignificant.


Figure \ref{figure_acc_AVG_Action3D} summarizes the results showing the average accuracy over the three datasets for all the parameter configurations evaluated . Lastly, table \ref{table_acc_action3D_SOM25} shows the results obtained in the each subset of the dataset separately with our best configuration  ( neurons in the SOM network,  window size). Note that results for our descriptor with  and  also outperform the second best known method listed in table \ref{table_acc_action3D_comparativa}.
 


 
\begin{figure}
\includegraphics[width=0.4\textwidth]{figures/acc_AVG_action3D.pdf}
\centering
\caption{ Average accuracy (\textit{y axis}) on MSR-Action3D for different window sizes  (\textit{x axis}) and number of clusters,  (colors). The average is computed over the accuracies of the 3 subsets (AS1, AS2, AS3).}
\label{figure_acc_AVG_Action3D}
\end{figure} 
 


\begin{table}
\begin{footnotesize}
\wrappingcaption{Accuracy of the model on each subset of the MSR-Action 3D dataset, for different values of  and a fixed number of clusters . Standard deviation shown in parenthesis.}
\begin{tabular}{ | c | c | c | c | c | } 
\hline
\rowcolor{lightgray}	
\textbf{Window}	& 1 & 3 & 5 & 7 \\
\hline
\textbf{AS1}	& 89.4 (0.9) &	92.6 (1.4)&	92.0 (1.5)&	91.2 (1.5)\\
\hline
\textbf{AS2}	& 76.8 (1.9)&	92.2 (1.6)&	91.1 (1.3)&	90.7 (1.0)\\
\hline
\textbf{AS3}	& 94.8 (0.5)&	97.4 (0.7)&	97.9 (1.0)&	98.9 (0.7)\\
\hline
\hline
\textbf{Mean} & \textbf{87.02} &	\textbf{94.07} &	\textbf{93.68} &	\textbf{93.61} \\
\hline
\end{tabular}
\label{table_acc_action3D_SOM25}
\end{footnotesize}
\end{table}


\begin{comment}
\doublerulesep 0.1pt
\begin{table}[h]
\begin{footnotesize}
\caption{Action sets for MSR Action3D} 
\label{action_sets}
\begin{tabular}{p{2cm}p{1cm}p{0.5cm}p{0.5cm}p{0.5cm}}
\hline\hline\noalign{\smallskip}
    Action & Sequences & AS1 & AS2 & AS3 \\
\noalign{\smallskip} \hline
    Tennis Swing & 20 & No & Yes & No\\
    Tennis Swing & 20 & No & Yes & No\\
    Tennis Swing & 20 & No & Yes & No\\
\hline\hline
\end{tabular}
\end{footnotesize}
\end{table}
\end{comment}

\subsubsection{Discussion}





In this section we perform a detailed analysis of the proposed method, exploring the results in each subset of the dataset separately.



Figure \ref{figure_acc_Action3D} shows a significant difference in accuracy between subsets. The method consistently reaches very high accuracies on Subset AS3, as high as  in occasions. On the other hand, the method achieves much worse accuracies on AS2, in which most configurations are below .

Figure \ref{figure_confMatrix_AS2} shows the confusion matrix for a single run of the experiment on AS2, from which it can be deduced which actions affect the recognition rate the most. For example, actions  \textit{high arm wave} and \textit{hand catch} are performed with the same set of joints and are very similar in their motions, thus making our model less likely to discriminate between them. Other actions such as \textit{forward kick} or \textit{two hand wave} are naturally easier to distinguish since they involve the use of different joints altogether. This pattern in the difficulty of the dataset was already described in previous works \cite{wang2012actionlet}\cite{wang2012robust}.




\begin{figure}
\includegraphics[width=0.45\textwidth]{figures/confusion_matrix_AS2.pdf}
\centering
\caption{Confusion matrix for subset AS2 of MSR-Action3D.}
\label{figure_confMatrix_AS2}
\end{figure} 

\begin{figure}
\includegraphics[width=0.45\textwidth]{figures/confusion_matrix_AS2_prob.pdf}
\centering
\caption{ Average probability of assigning an action of class  (rows) to every other class (columns) for subset AS2 of MSR-Action3D.}
\label{figure_probMatrix_AS2}
\end{figure} 

Figure \ref{figure_probMatrix_AS2} shows the probability matrix employed by the ProbSOM classification rule for the testing actions. The matrix shows the average estimated probabilities that an action of a certain class  belongs to another class  (where possibly ) as computed by our classification rule. It is remarkable that for some actions (\textit{forward kick},\textit{two hand wave}) not only the class prediction was perfect (\textit{forward kick},\textit{two hand wave}) but also that the probability of the action  belonging to a class  was  for classes other than  (the true class of the action ), ie,  if . However, some actions which can be readily identified as similar from just their general trajectory did not have such a perfect probability assignment.








 
\subsection{MSRC12 Dataset}
\label{msrc12}


The MSRC12 dataset is described in \cite{fothergill2012instructing} by Fothergill et al. and consists of 594 sequences of 12 different types of actions performed by 30 subjects. Each sequence contains the recorded positions of several instances or performances of the same action by the same person. The action types are described as: \textit{Crouch or hide, Shoot a pistol, Throw an object, Change weapon, Kick, Put on night vision goggles, Start Music/Raise Volume (of music), Navigate to next menu, Wind up the music, Take a Bow to end music session, Protest the music,} and \textit{Move up the tempo of the song}. The data was captured using a Kinect device. 

\subsubsection{Experimental setup}

Following the experimental setup by Hussein et al \cite{hussein2013human}, we segmented the sequences to isolate each action instance using their annotation, obtaining a set of 6244 action instances. 

By inspecting the actions visually we were able to detect instances in which the subject did not follow the instructions as intended by the experimenters. For example, subject 10 performs \textit{Kick} with his left leg, instead of the right one; for the action \textit{Wind it up}, subjects must trace a circle with both hands in front of them, but subject 9 makes it with only one hand (alternating right and left), and subject 21 traces only half of the circle. This was indeed expected by the experimenters, given that the database was generated precisely to measure how different instruction modalities (video, pictures, text) affect the performance of the actions, and so those actions performed incorrectly were not excluded from the dataset \cite{fothergill2012instructing}. The differences in execution are so wide that this outliers can be readily considered as instances of other types of action classes, and we have therefore excluded those instances from our experiments \footnote{The site https://sites.google.com/site/damdescriptor contains a full list of left out and corrected actions.}. 


With the filtered dataset, we performed cross subject experiments with a 50/50 subject split, and 30 cross validation runs for each parameter configuration. We report the average classification rate for each configuration over the 30 runs. The window size for WDFs was set to  and  clusters where used in a  topography SOM network. We also performed leave-one-subject-out cross validation tests following the protocol described in \cite{hussein2013human}.

\subsubsection{Results}


\doublerulesep 0.1pt
\begin{table}
\begin{footnotesize}
\wrappingcaption{Comparison of results on the MSRC-12  
dataset with cross-subject validation.}
\begin{tabular}{ | p{6cm} | c | } 
	 \hline
	 \rowcolor{lightgray}
	 \centering \textbf{Method} &\textbf{ Acc. (\%)}  \\ 
	 \hline
 	 Ellis (Logistic reg.)\cite{Ellis2013} &  88.7\\ 
	 \hline
	 Hussein (Cov3DJ) \cite{hussein2013human} & 91.7 \\ 
	 \hline	 
	 \textbf{Proposed descriptor} & \textbf{91.7}  \\ 
	 \hline
\end{tabular}
\label{table_acc_MSRC-12_comparativa}
\end{footnotesize}

\end{table}

\doublerulesep 0.1pt
\begin{table}
\begin{footnotesize}
\wrappingcaption{Comparison of results on the MSRC-12 dataset with leave-one-subject-out cross-validation. Note that in the case of Jiang et al's Hierarchical Model, a traditional leave-one-out cross-validation scheme was employed, training and testing with the same subject. }
\centering
\begin{tabular}{ | p{3.99cm} | p{1.9cm} | c | } 
	 \hline
	 \rowcolor{lightgray}
	 \centering \textbf{Method} &  \centering \textbf{Test Validation} & \textbf{ Acc. (\%)}  \\ 
	 \hline
	 Hussein et al. (Cov3DJ) \cite{hussein2013human}& leave-one-subject-out & 93.6 \\ 
	 \hline
	 Negin et al. (Decision Forest)\cite{negin2013decision}& leave-one-subject-out & 93.2 \\ 
	 \hline	 
	 Jiang et al. (Hierarchical Model) \cite{Jiang2013}& leave-one-out & 94.6 \\
	 \hline
	 \textbf{Proposed descriptor}& leave-one-subject-out & 93.1  \\ 
	 \hline
\end{tabular}
\label{table_acc_MSRC-12_leave-one-out}
\end{footnotesize}
\end{table}

\begin{figure}
\includegraphics[width=0.47\textwidth]{figures/acc_subjects_MSRC12_bars.pdf}
\centering
\caption{Accuracy by subject on the MSRC-12 dataset for leave-one-subject-out cross-validation }
\label{figure_acc_MSRC12_subjects}
\end{figure}





Table \ref{table_acc_MSRC-12_comparativa} shows the results obtained for MSRC12 with the proposed method, compared with the most recent ones of the state of the art. The experiments employ a cross-subject validation scheme, following \cite{hussein2013human}, but with subjects chosen randomly for each subject split. The results show the average accuracy on the dataset over 30 independent runs. 




Table \ref{table_acc_MSRC-12_leave-one-out} shows results for a leave-one-subject-out validation scheme (with 30 runs), which allows to evaluate the descriptor's performance by simulating a classic scenario where a new subject tries to use a system trained by other people. In this setting, our method obtains an accuracy of 93\%, a similar result to those obtained by Negin et al., Hussein et al. and Jiang et al.


In both experimental settings, the variances for the average accuracy of our classification method were low (), but standard hypothesis tests for the differences in accuracies between classifiers could not be performed because neither the variances for the classification accuracy nor the code for performing the tests was available for any of the other methods considered. Nonetheless, the variances obtained seem appropriate given that the test results depend highly on the subjects chosen for testing, as explained in the next paragraph. 





Figure \ref{figure_acc_MSRC12_subjects} shows the accuracy obtained for each subject separately in the leave-one-subject-out cross validation experiments. The graph shows that there is a high amount of variation in the accuracies for each subject, which can be attributed to the different methods the subjects were instructed with (video, pictures, text). For example, subject 28 achieves an accuracy of nearly 100\%, while on the other hand for subject 2 it is only 76\%. This data suggests that the observed differences in classification accuracy between our descriptor and other models should not be statistically significant.





\begin{figure}
\includegraphics[width=0.48\textwidth]{figures/acc_MSRC12_bars.pdf}
\centering
\caption{Accuracy on the MSRC-12 dataset for different window and cluster sizes.  window sizes,  = Method Accuracy}
\label{figure_acc_MSRC12_param}
\end{figure} 

Figure \ref{figure_acc_MSRC12_param} shows the behaviour of the method for different window and cluster sizes. As with MSRAction3D (subsection \ref{action3d}), the bar graphs clearly show the advantage of using WDFs instead of simple direction frames. On the other hand, the difference in accuracy from varying the number of clusters is much less significant. Given that the success of the ProbSOM classification rule is ultimately determined by how well the estimation of the distribution of WDF for each class was performed, and that estimation on the diversity in performing a certain action, the amount of clusters actually depends highly on the amount of samples in the dataset.





\subsubsection{Discussion}  

\begin{figure}
\includegraphics[width=0.48\textwidth]{figures/confusion_matrix_MSRC12_prob.pdf}
\centering
\caption{Average probability of assigning an action of class  to every other class for MSRC-12 dataset with leave-one-subject-out validation.}
\label{figure_confMatrix_MSRC12_prob}
\end{figure} 

\begin{figure}
\includegraphics[width=0.48\textwidth]{figures/confusion_matrix_MSRC12.pdf}
\centering
\caption{Confusion matrix for MSRC-12 dataset with leave-one-subject-out validation. }
\label{figure_confMatrix_MSRC12}
\end{figure} 




The method compares favorably with other state-of-the-art classification techniques for the MSRC12 dataset using traditional cross-subject validation. 

Although the accuracy our method achieves is the same as Hussein's \cite{hussein2013human}, their descriptor has a very high dimensionality; the best results they obtain are with a descriptor with  overlapping levels, for which a skeleton represented with 20 joints results in a descriptor of length 7320 \cite{hussein2013human}.  Our descriptor is of length at most 900 in the case of a  SOM network. Coupled with the fact that the computation of the descriptor involves only simple arithmetic operations, it should provide better real-time performance in terms of efficiency and frame rate.


The method also shows state-of-the art performance when tested with a leave-one-subject-out cross-validation scheme, making it clear that new users can expect a robust response when using a human action recognition system that employs the descriptor.






As with the Action3D dataset, figure \ref{figure_confMatrix_MSRC12_prob} shows the distribution of probabilities used by the method to classify actions in the MSRC12 dataset. This is the average of the probabilities for all actions, with the average for each class computed with all the test actions of the class. We also present a confusion matrix for the same dataset and testing scheme in figure \ref{figure_confMatrix_MSRC12}. 

Some actions(i.e \textit{Duck} or \textit{Push Right}) were classified almost perfectly, with a low entropy distribution, which presents a strong case for the robustness of the model. Certain actions, like \textit{Lift outstretched arms} or \textit{Shoot} present more difficulties to the descriptor, presumably because they employ similar joints and direction movements. However, the confusion matrix in figure \ref{figure_confMatrix_MSRC12} also shows that the misclassification errors mostly involve two or three classes at a time, and so a second layer of classifiers could be trained to separate those specific classes which when together raise the classification error.

%
 
  
\section{Conclusion}
\label{conclusion}

We have defined a descriptor for human action recognition based on the distribution of action movements (DAM descriptor). 

The descriptor is computed as a histogram over a set of canonical or representative movements windowed directions, which are in turn obtained by performing a clustering with a SOM network,  over the set of all windowed directions in the training set. We believe that a key step in the generation of the descriptor is the windowing scheme that, unlike other authors' approaches, is included as an intermediate step and not with a pyramidal scheme.

The descriptor is both scale and translation invariant.  If desired,  direction invariance of the trajectory of an action can be achieved by inverting the direction of each joint in the direction frames. Rotation invariance can be incorporated as well by rotating the user to a canonical rotation using his position and pose to set the axes of the joint positions instead of the camera's. 

The descriptor shows state-of-the-art performance on standard cross subjects tests on two well known datasets: MSRC12 and MSRAction3D achieving a classification rate of 91.7\% and 94\% respectively. Additional leave-one-subject-out tests on MSRC12 have also shown state-of-the-art results with an accuracy of 93\%. 

Our future work will concentrate on including more datasets in our tests to evaluate the generalization performance of the method with other classes, further testing with pyramidal schemes to better include temporal information about the action, and evaluating its performance in online classification tasks. 





\bibliographystyle{unsrt}
\bibliography{tex/ref}



\Biography{figures/franco.pdf}{Franco RONCHETTI is an advanced PhD student in Computer Science at the School of computer Science of the UNLP. Teaching Assistant for Grade Courses at the National University of La Plata. Between 2008-2011, recipient of a scholarship granted by the Ministry of Education of the Nation for the strengthening of human resources in the area of Information and Communication Technologies. Carrying out research activities since 2010 at the Institute of Research in Computer Science III-LIDI in areas related to soft-computing and intelligent systems, with focus on dynamic problems such as signal processing and pattern recognition, with various publications in the field.}

\Biography{figures/facu2.png}{Facundo QUIROGA received his BS in Computer Science in 2014 from the Faculty of Informatics of UNLP, Argentina. He is currently a PhD Student in the same Faculty, doing research work at the III-LIDI Institute. His main research fields comprise machine learning and signal processing, with applications in action recognition and biomedical data analysis. He is also interested in computational neuroscience and the interplay between artificial neural networks and biological ones.
}
\pagebreak

\Biography{figures/laura.pdf}{Laura LANZARINI is a full-Time Head Professor since 2001 at the School of Computer Science of the UNLP, teaching in Data Mining topics. Guest Head Professor at the National University of Tierra del Fuego. Guest Professor of the Post-Grade Program at the University of Buenos Aires. Distinguished by the UNLP in December 2010 with the Award to Scientific, Technological, and Artistic Work for Senior Researchers. Director of the group "Intelligent Systems," part of the Institute of Research in Computer Science III-LIDI. Author of numerous publications in journals, congress proceedings and scientific workshops with international referees in subjects pertaining to the design and development of adaptive application based on neural networks, swarm intelligence, and other metaheuristics applicable to Data Mining and Text Mining problems.}


\Biography{figures/cesar.pdf}{Cesar ESTREBOU is a PhD student in Computer Vision, Image Processing and Graphic Computing at the School of Computer Science of the UNLP. Since 2008 is Teaching Assistant (Assignments Supervisor) for Grade Courses at the National University of La Plata. He has been carrying out research activities since 2005 at the Institute of Research in Computer Science III-LIDI in areas related to intelligent systems, real-time systems and software engenieering, with publications in journals, congress proceedings and scientific workshops with international referees in those fields.}

 
\end{document}
