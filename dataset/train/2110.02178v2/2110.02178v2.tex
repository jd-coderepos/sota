
\documentclass[preprint]{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{xcolor}
\usepackage{algorithm2e}
\usetikzlibrary{decorations.pathreplacing,calligraphy, arrows}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{floatrow}
\usepackage{pifont}
\usepackage{wrapfig}

\hypersetup{
     colorlinks=true,
     linkcolor=red,
     filecolor=blue,
     citecolor=black,      
     urlcolor=cyan
     }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\definecolor{cmarkColor}{RGB}{51,160,44}
\definecolor{xmarkColor}{RGB}{228,26,28}

\definecolor{gArrow}{RGB}{244,165,130}
\definecolor{centerPix}{RGB}{202,0,32}
\definecolor{patchPix}{RGB}{5,113,176}

\definecolor{stdSampler}{RGB}{31,120,180}
\definecolor{mscSampler}{RGB}{215, 25, 28}

\newcommand{\cmark}{\textcolor{cmarkColor}{\ding{51}}}\newcommand{\xmark}{\textcolor{xmarkColor}{\ding{55}}}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\tiny,
    breakatwhitespace=false,         
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{}\space},
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    linewidth=\textwidth,
    stepnumber=2
}
\lstset{style=mystyle}

\newcommand{\arch}{MobileViT}


\title{\arch: Light-weight, General-purpose, and Mobile-friendly Vision Transformer}



\author{Sachin Mehta \\
Apple
\And
Mohammad Rastegari\\
Apple
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\providecommand{\mohammad}[1]{{\protect\color{blue}{\bf [Mohammad: #1]}}}
\providecommand{\sachin}[1]{{\protect\color{black}{#1}}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: \emph{is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks?} Towards this end, we introduce \arch, a light-weight and general-purpose vision transformer for mobile devices. \arch~presents a different perspective for the global processing of information with transformers. Our results show that \arch~significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, \arch~achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, \arch~is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: \url{https://github.com/apple/ml-cvnets}.
\end{abstract}

\section{Introduction}
Self-attention-based models, especially vision transformers \citep[ViTs; Figure \ref{fig:vit};][]{dosovitskiy2020image}, are an alternative to convolutional neural networks (CNNs) to learn visual representations. Briefly, ViT divides an image into a sequence of non-overlapping patches and then learns inter-patch representations using multi-headed self-attention in transformers \citep{vaswani2017attention}. The general trend is to increase the number of parameters in ViT networks to improve the performance \citep[e.g.,][]{touvron2021training, graham2021levit, wu2021cvt}. However, these performance improvements come at the cost of model size (network parameters) and latency. Many real-world applications (e.g., augmented reality and autonomous wheelchairs) require visual recognition tasks (e.g., object detection and semantic segmentation) to run on resource-constrained mobile devices in a timely fashion. To be effective, ViT models for such tasks should be light-weight and fast. Even if the model size of ViT models is reduced to match the resource constraints of mobile devices, their performance is significantly worse than light-weight CNNs. For instance, for a parameter budget of about 5-6 million, DeIT \citep{touvron2021training} is 3\% less accurate than MobileNetv3 \citep{howard2019searching}. Therefore, the need to design light-weight ViT models is imperative.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \resizebox{0.65\columnwidth}{!}{
            \hspace{-85pt}


\definecolor{patch0}{RGB}{127,201,127}
\definecolor{patch1}{RGB}{190,174,212}
\definecolor{patch2}{RGB}{253,192,134}
\definecolor{patch3}{RGB}{255,255,153}

\definecolor{patch4}{RGB}{56,108,176}
\definecolor{patch5}{RGB}{240,2,127}
\definecolor{patch6}{RGB}{191,91,23}
\definecolor{patch7}{RGB}{158,1,66}

\definecolor{patch8}{RGB}{228,26,28}
\definecolor{patch9}{RGB}{55,126,184}
\definecolor{patch10}{RGB}{77,175,74}
\definecolor{patch11}{RGB}{152,78,163}

\definecolor{patch12}{RGB}{255,127,0}
\definecolor{patch13}{RGB}{255,255,51}
\definecolor{patch14}{RGB}{166,86,40}
\definecolor{patch15}{RGB}{247,129,191}


\newcommand{\drawCube}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, fill=#4, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeDiv}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
       
       \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-1) -- ++(0,-\cubey,0) -- ++(0,0,1) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-1) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeEmpty}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=white, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\cubeWithPatches}{
    \begin{tikzpicture}
\node (patch_0) at (-1.0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch0}{0.75}};
        \node (patch_1) at (-0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch1}{0.75}};
        \node (patch_2) at (0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch2}{0.75}};
        \node (patch_3) at (0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch3}{0.75}};
\node (patch_4) at (-1.0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch4}{0.75}};
        \node (patch_5) at (-0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch5}{0.75}};
        \node (patch_6) at (0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch6}{0.75}};
        \node (patch_7) at (0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch7}{0.75}};
\node (patch_8) at (-1.0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch8}{0.95}};
        \node (patch_9) at (-0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch11}{0.95}};
        \node (patch_10) at (0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch10}{0.95}};
        \node (patch_11) at (0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch9}{0.95}};
\node (patch_12) at (-1.0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch12}{0.5}};
        \node (patch_13) at (-0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch13}{0.5}};
        \node (patch_14) at (0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch14}{0.5}};
        \node (patch_15) at (0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch15}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatches}{
    \begin{tikzpicture}
        \node (patch_0) {\drawCube{1.5}{0.15}{2}{patch3}{0.75}};
        \node[above=-0.99cm of patch_0] (patch_1) {\drawCube{1.5}{0.15}{2}{patch2}{0.75}};
        \node[above=-0.99cm of patch_1] (patch_2) {\drawCube{1.5}{0.15}{2}{patch1}{0.75}};
        \node[above=-0.99cm of patch_2] (patch_3) {\drawCube{1.5}{0.15}{2}{patch0}{0.75}};
\node[above=-0.99cm of patch_3] (patch_4) {\drawCube{1.5}{0.15}{2}{patch7}{0.75}};
        \node[above=-0.99cm of patch_4] (patch_5) {\drawCube{1.5}{0.15}{2}{patch6}{0.75}};
        \node[above=-0.99cm of patch_5] (patch_6) {\drawCube{1.5}{0.15}{2}{patch5}{0.75}};
        \node[above=-0.99cm of patch_6] (patch_7) {\drawCube{1.5}{0.15}{2}{patch4}{0.75}};
\node[above=-0.99cm of patch_7] (patch_8) {\drawCube{1.5}{0.15}{2}{patch11}{0.95}};
        \node[above=-0.99cm of patch_8] (patch_9) {\drawCube{1.5}{0.15}{2}{patch10}{0.95}};
        \node[above=-0.99cm of patch_9] (patch_10) {\drawCube{1.5}{0.15}{2}{patch9}{0.95}};
\node[above=-0.99cm of patch_10] (patch_11){\drawCube{1.5}{0.15}{2}{patch8}{0.95}};
\node[above=-0.99cm of patch_11] (patch_12){\drawCube{1.5}{0.15}{2}{patch15}{0.5}};
        \node[above=-0.99cm of patch_12] (patch_13){\drawCube{1.5}{0.15}{2}{patch14}{0.5}};
        \node[above=-0.99cm of patch_13] (patch_14){\drawCube{1.5}{0.15}{2}{patch13}{0.5}};
        \node[above=-0.99cm of patch_14] (patch_15){\drawCube{1.5}{0.15}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViT}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.05}{0.05}{2}{patch3}{0.75}};
        \node (patch_1) at (0, 0.05) {\drawCube{0.05}{0.05}{2}{patch2}{0.75}};
        \node (patch_2) at (0, 0.1) {\drawCube{0.05}{0.05}{2}{patch1}{0.75}};
        \node (patch_3) at (0, 0.15) {\drawCube{0.05}{0.05}{2}{patch0}{0.75}};
\node (patch_4)  at (0, 0.2) {\drawCube{0.05}{0.05}{2}{patch7}{0.75}};
        \node (patch_5)  at (0, 0.25) {\drawCube{0.05}{0.05}{2}{patch6}{0.75}};
        \node (patch_6) at (0, 0.3) {\drawCube{0.05}{0.05}{2}{patch5}{0.75}};
        \node (patch_7) at (0, 0.35) {\drawCube{0.05}{0.05}{2}{patch4}{0.75}};
\node (patch_8) at (0, 0.4) {\drawCube{0.05}{0.05}{2}{patch11}{0.95}};
        \node (patch_9) at (0, 0.45) {\drawCube{0.05}{0.05}{2}{patch10}{0.95}};
        \node (patch_10) at (0, 0.5) {\drawCube{0.05}{0.05}{2}{patch9}{0.95}};
        \node (patch_11) at (0, 0.55) {\drawCube{0.05}{0.05}{2}{patch8}{0.95}};
\node (patch_12) at (0, 0.6) {\drawCube{0.05}{0.05}{2}{patch15}{0.5}};
        \node (patch_13) at (0, 0.65) {\drawCube{0.05}{0.05}{2}{patch14}{0.5}};
        \node (patch_14) at (0, 0.7) {\drawCube{0.05}{0.05}{2}{patch13}{0.5}};
        \node (patch_15) at (0, 0.75) {\drawCube{0.05}{0.05}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViTSmall}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.2}{0.2}{3}{patch3}{0.75}};
        \node (patch_1) at (0, 0.2) {\drawCube{0.2}{0.2}{3}{patch2}{0.75}};
        \node (patch_2) at (0, 0.4) {\drawCube{0.2}{0.2}{3}{patch1}{0.75}};
        \node (patch_3) at (0, 0.6) {\drawCube{0.2}{0.2}{3}{patch0}{0.75}};
\node (patch_4)  at (0, 0.8) {\drawCube{0.2}{0.2}{3}{patch7}{0.75}};
        \node (patch_5)  at (0, 1.0) {\drawCube{0.2}{0.2}{3}{patch6}{0.75}};
        \node (patch_6) at (0, 1.2) {\drawCube{0.2}{0.2}{3}{patch5}{0.75}};
        \node (patch_7) at (0, 1.4) {\drawCube{0.2}{0.2}{3}{patch4}{0.75}};
\node (patch_8) at (0, 1.6) {\drawCube{0.2}{0.2}{3}{patch11}{0.95}};
        \node (patch_9) at (0, 1.8) {\drawCube{0.2}{0.2}{3}{patch10}{0.95}};
        \node (patch_10) at (0, 2.0) {\drawCube{0.2}{0.2}{3}{patch9}{0.95}};
        \node (patch_11) at (0, 2.2) {\drawCube{0.2}{0.2}{3}{patch8}{0.95}};
\node (patch_12) at (0, 2.4) {\drawCube{0.2}{0.2}{3}{patch15}{0.5}};
        \node (patch_13) at (0, 2.6) {\drawCube{0.2}{0.2}{3}{patch14}{0.5}};
        \node (patch_14) at (0, 2.8) {\drawCube{0.2}{0.2}{3}{patch13}{0.5}};
        \node (patch_15) at (0, 3.0) {\drawCube{0.2}{0.2}{3}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\mvtBlock}{
    \begin{tikzpicture}
    
        \draw [line width=0.75mm, fill=green!20, rounded corners] (-2.5, 2.5) -- (40.5, 2.5) -- (40.5, -7.25) -- (-2.5, -7.25) --cycle;
        \node [align=center] (temp) at (19, 2.0) {\scalebox{2.5}{\textcolor{red}{\bfseries \arch~block}}};
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, 0.25) -- (25.5, 0.25) -- (25.5, -7) -- (6.5, -7) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26.5, 1.75) -- (37.0, 1.75) -- (37.0, -5.5) -- (26.5, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{blue}{\bf Local}} \\ \scalebox{2}{\textcolor{blue}{\bf representations}}};
\node[align=center] (cap_img) at (16, -0.4) {\scalebox{2}{\textcolor{blue}{\bf Transformers as Convolutions}}};
        \node[align=center] (cap_img_a) at (16, -1.1) {\scalebox{1.75}{\textcolor{blue}{\bf (global representations)}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{blue}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.6cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.7cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.6cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, >=latex, line width=1.25mm, color=red] (tensor) edge[in=175, out=5] (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.8) -- (14, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.8) -- (25.25, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\mvtBlockOld}{
    \begin{tikzpicture}
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, -1.25) -- (25.5, -1.25) -- (25.5, -6.5) -- (6.5, -6.5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26, 1.75) -- (36.5, 1.75) -- (36.4, -5.5) -- (26, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{red}{\bf Local}} \\ \scalebox{2}{\textcolor{red}{\bf representations}}};
\node[align=center] (cap_img) at (16, -1.75) {\scalebox{2}{\textcolor{red}{\bf Global representations}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{red}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.35cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-3x3}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.75mm] (tensor) -- (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.25) -- (14, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.25) -- (25.25, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\tikzset{
    cross/.pic = {
    \draw[rotate = 45] (-#1,0) -- (#1,0);
    \draw[rotate = 45] (0,-#1) -- (0, #1);
    }
}

\newcommand{\vitC}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node [right=1.25cm of img, draw, fill=white, rounded corners, minimum height=2cm, minimum width=5cm, line width=1mm, align=center] (conv_stack) {\scalebox{2}{\{Conv- \} } \\ \scalebox{2}{} \\ \scalebox{2}{Reshape \& Flatten}};
\node [right=1.25cm of conv_stack, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.9)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node[right=1.00cm of flatten, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.2,-0.85)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=2cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=2cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (conv_stack);
        \draw [->, line width=1mm] (conv_stack) -- (flatten);
\draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}


\newcommand{\mvtBlockA}{
    \begin{tikzpicture}
        \node[inner sep=0pt, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [right=0.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=0.75cm of conv, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=above:{}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (patch_im) {\cubeWithPatches};
\node [right=0.75cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};
        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}] (proj_tensor) {\drawCube{2}{2}{1}{white}{1.0}};
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Concat} \\ \scalebox{1.5}{} \\ \scalebox{1.5}{Conv-3x3}};
\node [right=0.75cm of concat, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.5mm] (tensor) -- (conv);
        \draw [->, line width=0.5mm] (conv) -- (img);
        \draw [->, line width=0.5mm] (img) -- (flatten);
        \draw [->, line width=0.5mm] (flatten) -- (transformer);
        \draw [->, line width=0.5mm] (transformer) -- (trans_out);
        \draw [->, line width=0.5mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.5mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.5mm] (conv_proj) -- (proj_tensor);
        \draw [->, line width=0.5mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.5mm] (tensor) -- (dummY_tensor.center) -| (concat.north);
        \draw [->, line width=0.5mm] (concat) -- (out);
        


        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(5, -2) -- (12, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(16.5, -2) -- (23.5, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\vit}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
        \begin{scope}[shift={(img.south west)},x={(img.south east)},y={(img.north west)}]
            \draw[help lines, xstep=.2, ystep=.2, color=black, line width=0.75mm] (0,0) grid (1.0, 1.0);
        \end{scope}
        
\node [right=1.25cm of img, label={[shift={(-1,-3.3)}]\scalebox{2}{}}, label={[shift={(-0.25,-1.1)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(-1, -3) -- (5, -3) node [black,midway, below=8pt] {\scalebox{2}{Flatten image patches}};
        
        \node[right=1.25cm of flatten, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.2,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.95)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=1cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=1.5cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (flatten);
        \draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}

\newcommand{\stdDDP}{
    \begin{tikzpicture}
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-3}}] (tensor_a1) at (0, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-2}}] (tensor_a2) at (0, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-1}}] (tensor_a3) at (0, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_c) at (3.1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-1.4, -1.4) -- (4, -1.4);
        \node at (3.5, -1.9)  {\scalebox{2.5}{time}};
        
        \node[above=0.75cm of tensor_a3, align=center] (b){\scalebox{3}{\textcolor{stdSampler}{\bfseries Standard sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\mscDDP}{
    \begin{tikzpicture}
    
        \node[label={[shift={(-3.6,-1.75)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0.92, 0, 3) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-2.25)}]\scalebox{2.5}{GPU-3}}] at (-0.08, 2, 2.5) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-3.0)}]\scalebox{2.5}{GPU-2}}] at (-0.57, 4.01, 1.5) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node[label={[shift={(-3.85,-1.0)}]\scalebox{2.5}{GPU-1}}] at (1.5, 6.1, 4) {\drawCube{4}{1}{2}{red!20}{1.0}};
        
        \node at (3.2-0.5, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (marked) at (3.2-0.5, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node at (5.90-0.95, 0.5, 2.5) {\drawCube{4}{1}{2}{red!20}{1.0}};
        \node at (4.44-0.95, 2.02, 1.6) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node at (5.92-0.95, 4.01, 3.05) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node at (5.45-0.95, 6.55, 3.95) {\drawCube{2}{2}{2}{red!20}{1.0}};
        
        \node at (8.2-1.4, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-2.45, -2.35) -- (7, -2.35);
        
        \node at (6.5, -2.75)  {\scalebox{2.5}{time}};
        \node[above=0.75cm of marked, align=center] (b) {\scalebox{3}{\textcolor{mscSampler}{\bfseries Multi-scale sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\tensorNotations}{
    \begin{tikzpicture}
        \node[label={[shift={(-1.75,-2.5)}]\rotatebox{90}{ \scalebox{1.35}{Height}}}, label={[shift={(-1.2,-0.9)}]\rotatebox{50}{\scalebox{1.35}{Batch}}}, label={[shift={(-0.5,-3.5)}] \scalebox{1.35}{Width}}] (tensor_c) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[right=0.1cm of tensor_c] (a) {\scalebox{1.75}{Tensor}};
        
        \node[below=1cm of tensor_c] (tensor_d) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node[right=0.1cm of tensor_d] (b) {\scalebox{1.75}{Gradient sync.}};
    \end{tikzpicture}
}



%
 \vit
        }
        \caption{\textbf{Standard visual transformer (ViT)}}
        \label{fig:vit}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \resizebox{0.8\columnwidth}{!}{
            \hspace{-85pt}


\definecolor{patch0}{RGB}{127,201,127}
\definecolor{patch1}{RGB}{190,174,212}
\definecolor{patch2}{RGB}{253,192,134}
\definecolor{patch3}{RGB}{255,255,153}

\definecolor{patch4}{RGB}{56,108,176}
\definecolor{patch5}{RGB}{240,2,127}
\definecolor{patch6}{RGB}{191,91,23}
\definecolor{patch7}{RGB}{158,1,66}

\definecolor{patch8}{RGB}{228,26,28}
\definecolor{patch9}{RGB}{55,126,184}
\definecolor{patch10}{RGB}{77,175,74}
\definecolor{patch11}{RGB}{152,78,163}

\definecolor{patch12}{RGB}{255,127,0}
\definecolor{patch13}{RGB}{255,255,51}
\definecolor{patch14}{RGB}{166,86,40}
\definecolor{patch15}{RGB}{247,129,191}


\newcommand{\drawCube}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, fill=#4, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeDiv}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
       
       \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-1) -- ++(0,-\cubey,0) -- ++(0,0,1) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-1) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeEmpty}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=white, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\cubeWithPatches}{
    \begin{tikzpicture}
\node (patch_0) at (-1.0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch0}{0.75}};
        \node (patch_1) at (-0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch1}{0.75}};
        \node (patch_2) at (0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch2}{0.75}};
        \node (patch_3) at (0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch3}{0.75}};
\node (patch_4) at (-1.0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch4}{0.75}};
        \node (patch_5) at (-0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch5}{0.75}};
        \node (patch_6) at (0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch6}{0.75}};
        \node (patch_7) at (0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch7}{0.75}};
\node (patch_8) at (-1.0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch8}{0.95}};
        \node (patch_9) at (-0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch11}{0.95}};
        \node (patch_10) at (0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch10}{0.95}};
        \node (patch_11) at (0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch9}{0.95}};
\node (patch_12) at (-1.0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch12}{0.5}};
        \node (patch_13) at (-0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch13}{0.5}};
        \node (patch_14) at (0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch14}{0.5}};
        \node (patch_15) at (0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch15}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatches}{
    \begin{tikzpicture}
        \node (patch_0) {\drawCube{1.5}{0.15}{2}{patch3}{0.75}};
        \node[above=-0.99cm of patch_0] (patch_1) {\drawCube{1.5}{0.15}{2}{patch2}{0.75}};
        \node[above=-0.99cm of patch_1] (patch_2) {\drawCube{1.5}{0.15}{2}{patch1}{0.75}};
        \node[above=-0.99cm of patch_2] (patch_3) {\drawCube{1.5}{0.15}{2}{patch0}{0.75}};
\node[above=-0.99cm of patch_3] (patch_4) {\drawCube{1.5}{0.15}{2}{patch7}{0.75}};
        \node[above=-0.99cm of patch_4] (patch_5) {\drawCube{1.5}{0.15}{2}{patch6}{0.75}};
        \node[above=-0.99cm of patch_5] (patch_6) {\drawCube{1.5}{0.15}{2}{patch5}{0.75}};
        \node[above=-0.99cm of patch_6] (patch_7) {\drawCube{1.5}{0.15}{2}{patch4}{0.75}};
\node[above=-0.99cm of patch_7] (patch_8) {\drawCube{1.5}{0.15}{2}{patch11}{0.95}};
        \node[above=-0.99cm of patch_8] (patch_9) {\drawCube{1.5}{0.15}{2}{patch10}{0.95}};
        \node[above=-0.99cm of patch_9] (patch_10) {\drawCube{1.5}{0.15}{2}{patch9}{0.95}};
\node[above=-0.99cm of patch_10] (patch_11){\drawCube{1.5}{0.15}{2}{patch8}{0.95}};
\node[above=-0.99cm of patch_11] (patch_12){\drawCube{1.5}{0.15}{2}{patch15}{0.5}};
        \node[above=-0.99cm of patch_12] (patch_13){\drawCube{1.5}{0.15}{2}{patch14}{0.5}};
        \node[above=-0.99cm of patch_13] (patch_14){\drawCube{1.5}{0.15}{2}{patch13}{0.5}};
        \node[above=-0.99cm of patch_14] (patch_15){\drawCube{1.5}{0.15}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViT}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.05}{0.05}{2}{patch3}{0.75}};
        \node (patch_1) at (0, 0.05) {\drawCube{0.05}{0.05}{2}{patch2}{0.75}};
        \node (patch_2) at (0, 0.1) {\drawCube{0.05}{0.05}{2}{patch1}{0.75}};
        \node (patch_3) at (0, 0.15) {\drawCube{0.05}{0.05}{2}{patch0}{0.75}};
\node (patch_4)  at (0, 0.2) {\drawCube{0.05}{0.05}{2}{patch7}{0.75}};
        \node (patch_5)  at (0, 0.25) {\drawCube{0.05}{0.05}{2}{patch6}{0.75}};
        \node (patch_6) at (0, 0.3) {\drawCube{0.05}{0.05}{2}{patch5}{0.75}};
        \node (patch_7) at (0, 0.35) {\drawCube{0.05}{0.05}{2}{patch4}{0.75}};
\node (patch_8) at (0, 0.4) {\drawCube{0.05}{0.05}{2}{patch11}{0.95}};
        \node (patch_9) at (0, 0.45) {\drawCube{0.05}{0.05}{2}{patch10}{0.95}};
        \node (patch_10) at (0, 0.5) {\drawCube{0.05}{0.05}{2}{patch9}{0.95}};
        \node (patch_11) at (0, 0.55) {\drawCube{0.05}{0.05}{2}{patch8}{0.95}};
\node (patch_12) at (0, 0.6) {\drawCube{0.05}{0.05}{2}{patch15}{0.5}};
        \node (patch_13) at (0, 0.65) {\drawCube{0.05}{0.05}{2}{patch14}{0.5}};
        \node (patch_14) at (0, 0.7) {\drawCube{0.05}{0.05}{2}{patch13}{0.5}};
        \node (patch_15) at (0, 0.75) {\drawCube{0.05}{0.05}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViTSmall}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.2}{0.2}{3}{patch3}{0.75}};
        \node (patch_1) at (0, 0.2) {\drawCube{0.2}{0.2}{3}{patch2}{0.75}};
        \node (patch_2) at (0, 0.4) {\drawCube{0.2}{0.2}{3}{patch1}{0.75}};
        \node (patch_3) at (0, 0.6) {\drawCube{0.2}{0.2}{3}{patch0}{0.75}};
\node (patch_4)  at (0, 0.8) {\drawCube{0.2}{0.2}{3}{patch7}{0.75}};
        \node (patch_5)  at (0, 1.0) {\drawCube{0.2}{0.2}{3}{patch6}{0.75}};
        \node (patch_6) at (0, 1.2) {\drawCube{0.2}{0.2}{3}{patch5}{0.75}};
        \node (patch_7) at (0, 1.4) {\drawCube{0.2}{0.2}{3}{patch4}{0.75}};
\node (patch_8) at (0, 1.6) {\drawCube{0.2}{0.2}{3}{patch11}{0.95}};
        \node (patch_9) at (0, 1.8) {\drawCube{0.2}{0.2}{3}{patch10}{0.95}};
        \node (patch_10) at (0, 2.0) {\drawCube{0.2}{0.2}{3}{patch9}{0.95}};
        \node (patch_11) at (0, 2.2) {\drawCube{0.2}{0.2}{3}{patch8}{0.95}};
\node (patch_12) at (0, 2.4) {\drawCube{0.2}{0.2}{3}{patch15}{0.5}};
        \node (patch_13) at (0, 2.6) {\drawCube{0.2}{0.2}{3}{patch14}{0.5}};
        \node (patch_14) at (0, 2.8) {\drawCube{0.2}{0.2}{3}{patch13}{0.5}};
        \node (patch_15) at (0, 3.0) {\drawCube{0.2}{0.2}{3}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\mvtBlock}{
    \begin{tikzpicture}
    
        \draw [line width=0.75mm, fill=green!20, rounded corners] (-2.5, 2.5) -- (40.5, 2.5) -- (40.5, -7.25) -- (-2.5, -7.25) --cycle;
        \node [align=center] (temp) at (19, 2.0) {\scalebox{2.5}{\textcolor{red}{\bfseries \arch~block}}};
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, 0.25) -- (25.5, 0.25) -- (25.5, -7) -- (6.5, -7) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26.5, 1.75) -- (37.0, 1.75) -- (37.0, -5.5) -- (26.5, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{blue}{\bf Local}} \\ \scalebox{2}{\textcolor{blue}{\bf representations}}};
\node[align=center] (cap_img) at (16, -0.4) {\scalebox{2}{\textcolor{blue}{\bf Transformers as Convolutions}}};
        \node[align=center] (cap_img_a) at (16, -1.1) {\scalebox{1.75}{\textcolor{blue}{\bf (global representations)}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{blue}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.6cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.7cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.6cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, >=latex, line width=1.25mm, color=red] (tensor) edge[in=175, out=5] (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.8) -- (14, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.8) -- (25.25, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\mvtBlockOld}{
    \begin{tikzpicture}
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, -1.25) -- (25.5, -1.25) -- (25.5, -6.5) -- (6.5, -6.5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26, 1.75) -- (36.5, 1.75) -- (36.4, -5.5) -- (26, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{red}{\bf Local}} \\ \scalebox{2}{\textcolor{red}{\bf representations}}};
\node[align=center] (cap_img) at (16, -1.75) {\scalebox{2}{\textcolor{red}{\bf Global representations}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{red}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.35cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-3x3}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.75mm] (tensor) -- (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.25) -- (14, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.25) -- (25.25, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\tikzset{
    cross/.pic = {
    \draw[rotate = 45] (-#1,0) -- (#1,0);
    \draw[rotate = 45] (0,-#1) -- (0, #1);
    }
}

\newcommand{\vitC}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node [right=1.25cm of img, draw, fill=white, rounded corners, minimum height=2cm, minimum width=5cm, line width=1mm, align=center] (conv_stack) {\scalebox{2}{\{Conv- \} } \\ \scalebox{2}{} \\ \scalebox{2}{Reshape \& Flatten}};
\node [right=1.25cm of conv_stack, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.9)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node[right=1.00cm of flatten, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.2,-0.85)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=2cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=2cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (conv_stack);
        \draw [->, line width=1mm] (conv_stack) -- (flatten);
\draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}


\newcommand{\mvtBlockA}{
    \begin{tikzpicture}
        \node[inner sep=0pt, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [right=0.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=0.75cm of conv, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=above:{}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (patch_im) {\cubeWithPatches};
\node [right=0.75cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};
        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}] (proj_tensor) {\drawCube{2}{2}{1}{white}{1.0}};
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Concat} \\ \scalebox{1.5}{} \\ \scalebox{1.5}{Conv-3x3}};
\node [right=0.75cm of concat, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.5mm] (tensor) -- (conv);
        \draw [->, line width=0.5mm] (conv) -- (img);
        \draw [->, line width=0.5mm] (img) -- (flatten);
        \draw [->, line width=0.5mm] (flatten) -- (transformer);
        \draw [->, line width=0.5mm] (transformer) -- (trans_out);
        \draw [->, line width=0.5mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.5mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.5mm] (conv_proj) -- (proj_tensor);
        \draw [->, line width=0.5mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.5mm] (tensor) -- (dummY_tensor.center) -| (concat.north);
        \draw [->, line width=0.5mm] (concat) -- (out);
        


        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(5, -2) -- (12, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(16.5, -2) -- (23.5, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\vit}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
        \begin{scope}[shift={(img.south west)},x={(img.south east)},y={(img.north west)}]
            \draw[help lines, xstep=.2, ystep=.2, color=black, line width=0.75mm] (0,0) grid (1.0, 1.0);
        \end{scope}
        
\node [right=1.25cm of img, label={[shift={(-1,-3.3)}]\scalebox{2}{}}, label={[shift={(-0.25,-1.1)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(-1, -3) -- (5, -3) node [black,midway, below=8pt] {\scalebox{2}{Flatten image patches}};
        
        \node[right=1.25cm of flatten, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.2,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.95)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=1cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=1.5cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (flatten);
        \draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}

\newcommand{\stdDDP}{
    \begin{tikzpicture}
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-3}}] (tensor_a1) at (0, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-2}}] (tensor_a2) at (0, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-1}}] (tensor_a3) at (0, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_c) at (3.1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-1.4, -1.4) -- (4, -1.4);
        \node at (3.5, -1.9)  {\scalebox{2.5}{time}};
        
        \node[above=0.75cm of tensor_a3, align=center] (b){\scalebox{3}{\textcolor{stdSampler}{\bfseries Standard sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\mscDDP}{
    \begin{tikzpicture}
    
        \node[label={[shift={(-3.6,-1.75)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0.92, 0, 3) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-2.25)}]\scalebox{2.5}{GPU-3}}] at (-0.08, 2, 2.5) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-3.0)}]\scalebox{2.5}{GPU-2}}] at (-0.57, 4.01, 1.5) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node[label={[shift={(-3.85,-1.0)}]\scalebox{2.5}{GPU-1}}] at (1.5, 6.1, 4) {\drawCube{4}{1}{2}{red!20}{1.0}};
        
        \node at (3.2-0.5, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (marked) at (3.2-0.5, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node at (5.90-0.95, 0.5, 2.5) {\drawCube{4}{1}{2}{red!20}{1.0}};
        \node at (4.44-0.95, 2.02, 1.6) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node at (5.92-0.95, 4.01, 3.05) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node at (5.45-0.95, 6.55, 3.95) {\drawCube{2}{2}{2}{red!20}{1.0}};
        
        \node at (8.2-1.4, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-2.45, -2.35) -- (7, -2.35);
        
        \node at (6.5, -2.75)  {\scalebox{2.5}{time}};
        \node[above=0.75cm of marked, align=center] (b) {\scalebox{3}{\textcolor{mscSampler}{\bfseries Multi-scale sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\tensorNotations}{
    \begin{tikzpicture}
        \node[label={[shift={(-1.75,-2.5)}]\rotatebox{90}{ \scalebox{1.35}{Height}}}, label={[shift={(-1.2,-0.9)}]\rotatebox{50}{\scalebox{1.35}{Batch}}}, label={[shift={(-0.5,-3.5)}] \scalebox{1.35}{Width}}] (tensor_c) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[right=0.1cm of tensor_c] (a) {\scalebox{1.75}{Tensor}};
        
        \node[below=1cm of tensor_c] (tensor_d) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node[right=0.1cm of tensor_d] (b) {\scalebox{1.75}{Gradient sync.}};
    \end{tikzpicture}
}



%
 \mvtBlock
        }
        \resizebox{\columnwidth}{!}{
            \hspace{-15pt}
\newcommand{\gap}{0.5cm}

\newcommand{\mvtArch}{
    \begin{tikzpicture}
    \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node[right=\gap of img, draw, fill=yellow!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l1) {Conv- \\  2};
\node[right=\gap of l1, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l2) {MV2};
\node[right=\gap of l2, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l3) {MV2 \\  2};
\node[right=\gap of l3, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center, label=above:{}] (l4) {MV2};
\node[right=\gap of l4, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l5) {MV2 \\  2};
\node[right=\gap of l5, draw, fill=green!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center, label=above:{}, label=below:{}] (l6) {\arch \\ block};
\node[right=\gap of l6, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l7) {MV2 \\  2};
\node[right=\gap of l7, draw, fill=green!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center, label=above:{\scalebox{1.0}{}}, label=below:{}] (l8) {\arch \\ block};
\node[right=\gap of l8, draw, fill=red!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l9) {MV2 \\  2};
\node[right=\gap of l9, draw, fill=green!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center, label=above:{}, label=below:{}] (l10) {\arch \\ block};
\node[right=\gap of l10, draw, fill=yellow!20, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l11) {Conv-};
\node[right=\gap of l11, draw, fill=white, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l12) {Global pool \\  Linear};
\node[right=\gap of l12, fill=white, rounded corners, minimum height=1.25cm, line width=0.5mm, align=center] (l13) {Logits};
\draw [->, line width=0.5mm] (img) -- (l1);
    \draw [->, line width=0.5mm] (l1) -- (l2);
    \draw [->, line width=0.5mm] (l2) -- (l3);
    \draw [->, line width=0.5mm] (l3) -- (l4);
    \draw [->, line width=0.5mm] (l4) -- (l5);
    \draw [->, line width=0.5mm] (l5) -- (l6);
    \draw [->, line width=0.5mm] (l6) -- (l7);
    \draw [->, line width=0.5mm] (l7) -- (l8);
    \draw [->, line width=0.5mm] (l8) -- (l9);
    \draw [->, line width=0.5mm] (l9) -- (l10);
    \draw [->, line width=0.5mm] (l10) -- (l11);
    \draw [->, line width=0.5mm] (l11) -- (l12);
    \draw [->, line width=0.5mm] (l12) -- (l13);
\node[below=0.2cm of img, align=center] (cap_img) {\scalebox{1.25}{Output spatial } \\ \scalebox{1.25}{dimensions}};
    \node[below=0.75cm of l1, align=center] (cap_l1) {\scalebox{1.25}{}};
    \node[below=0.75cm of l3, align=center] (cap_l3) {\scalebox{1.25}{}};
    \node[below=0.75cm of l5, align=center] (cap_l3) {\scalebox{1.25}{}};
    \node[below=0.75cm of l7, align=center] (cap_l3) {\scalebox{1.25}{}};
    \node[below=0.75cm of l9, align=center] (cap_l3) {\scalebox{1.25}{}};
    \node[below=0.75cm of l12, align=center] (cap_l3) {\scalebox{1.25}{}};
    
    \node[above=0.5cm of l6] (temp) {\scalebox{3}{\textcolor{green!80}{}}};
\end{tikzpicture}
} \mvtArch
        }
        \caption{{\bfseries \arch}. Here, \colorbox{yellow!20}{Conv-} in the \arch~block represents a standard  convolution and \colorbox{red!20}{MV2} refers to MobileNetv2 block. Blocks that perform down-sampling are marked with .}
        \label{fig:mvt_arch}
    \end{subfigure}
    \caption{\textbf{Visual transformers vs. \arch}}
    \label{fig:comparison_vit_mvt}
\end{figure}
\begin{figure}[t!]
    \centering
    \floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,center},capbesidewidth=8cm}}]{figure}[\FBwidth]{
    \caption{\textbf{\arch~shows better task-level generalization properties} as compared to light-weight CNN models. The network parameters are listed for SSDLite network with different feature extractors (MobileNetv1 \citep{howard2017mobilenets}, MobileNetv2 \citep{sandler2018mobilenetv2}, MobileNetv3 \citep{howard2019searching}, MNASNet \citep{tan2019mnasnet}, MixNet \citep{tan2019mixconv}, and \arch~(Ours)) on the MS-COCO dataset.}  
    \label{fig:task_generalizability}  }{\includegraphics[height=90px]{figures/task_generalization.pdf}}
\end{figure}

Light-weight CNNs have powered many mobile vision tasks. However, ViT-based networks are still far from being used on such devices. Unlike light-weight CNNs that are easy to optimize and integrate with task-specific networks, ViTs are heavy-weight (e.g., ViT-B/16 vs. MobileNetv3: 86 vs. 7.5 million parameters), harder to optimize \citep{xiao2021early}, need extensive data augmentation and L2 regularization to prevent over-fitting \citep{touvron2021training, wang2021pyramid}, and require expensive decoders for down-stream tasks, especially for dense prediction tasks. For instance, a ViT-based segmentation network \citep{ranftl2021vision} learns about 345 million parameters and achieves similar performance as the  CNN-based network, DeepLabv3 \citep{chen2017rethinking}, with 59 million parameters. The need for more parameters in ViT-based models is likely because they lack image-specific inductive bias, which is inherent in CNNs \citep{xiao2021early}. To build robust and high-performing ViT models, hybrid approaches that combine convolutions and transformers are gaining interest \citep{xiao2021early, d2021convit, chen2021mobile}. However, these hybrid models are still heavy-weight and are sensitive to data augmentation. For example, removing CutMix \citep{zhong2020random} and DeIT-style \citep{touvron2021training} data augmentation causes a significant drop in ImageNet accuracy (78.1\% to 72.4\%) of  \citet{heo2021rethinking}.

It remains an open question to combine the strengths of CNNs and transformers to build ViT models for mobile vision tasks. Mobile vision tasks require light-weight, low latency, and accurate models that satisfy the device's resource constraints, and are general-purpose so that they can be applied to different tasks (e.g., segmentation and detection). Note that floating-point operations (FLOPs) are not sufficient for low latency on mobile devices because FLOPs ignore several important inference-related factors such as memory access, degree of parallelism, and platform characteristics \citep{ma2018shufflenet}. For example, the ViT-based method of \citet{heo2021rethinking}, PiT, has  fewer FLOPs than DeIT \citep{touvron2021training} but has a similar inference speed on a mobile device (DeIT vs. PiT on iPhone-12: 10.99 ms vs. 10.56 ms). Therefore, instead of optimizing for FLOPs\footnote{\arch~FLOPs can be further reduced using existing methods (e.g., DynamicViT \citep{rao2021dynamicvit}).}, this paper focuses on designing a \textbf{light-weight} (\textsection \ref{sec:light_wt_transformer}), \textbf{general-purpose} (\textsection \ref{ssec:imagenet_results} \& \textsection \ref{sec:general_purpose}), and \textbf{low latency} (\textsection \ref{sec:mobile_friendly}) network for mobile vision tasks. We achieve this goal with \arch~that combines the benefits of CNNs (e.g., spatial inductive biases and less sensitivity to data augmentation) and ViTs (e.g., input-adaptive weighting and global processing). Specifically, we introduce the \arch~block that encodes both local and global information in a tensor effectively (Figure \ref{fig:mvt_arch}). Unlike ViT and its variants (with and without convolutions), \arch~presents a different perspective to learn global representations. Standard convolution involves three operations: unfolding, local processing, and folding. \arch~block replaces local processing in convolutions with global processing using transformers. This allows \arch~block to have CNN- and ViT-like properties, which helps it learn better representations with fewer parameters and simple training recipes (e.g., basic augmentation). To the best of our knowledge, this is the first work that shows that light-weight ViTs can achieve light-weight CNN-level performance with simple training recipes across different mobile vision tasks. For a parameter budget of about 5-6 million, \arch~achieves a top-1 accuracy of 78.4\% on the ImageNet-1k dataset \citep{russakovsky2015imagenet}, which is 3.2\% more accurate than MobileNetv3 and has a simple training recipe (\arch~vs. MobileNetv3: 300 vs. 600 epochs; 1024 vs. 4096 batch size). We also observe significant gains in performance when \arch~is used as a feature backbone in highly optimized mobile vision task-specific architectures. Replacing MNASNet \citep{tan2019mnasnet} with \arch~as a feature backbone in SSDLite \citep{sandler2018mobilenetv2} resulted in a better (+1.8\% mAP) and smaller () detection network (Figure \ref{fig:task_generalizability}).

\section{Related Work}
\label{sec:related_work}

\textbf{Light-weight CNNs.} The basic building layer in CNNs is a standard convolutional layer. Because this layer is computationally expensive, several factorization-based methods have been proposed to make it light-weight and mobile-friendly \citep[e.g.,][]{jin2014flattened, chollet2017xception, mehta2020dicenet}. Of these, separable convolutions of \citet{chollet2017xception} have gained interest, and are widely used across state-of-the-art light-weight CNNs for mobile vision tasks, including MobileNets \citep{howard2017mobilenets, sandler2018mobilenetv2, howard2019searching}, ShuffleNetv2 \citep{ma2018shufflenet}, ESPNetv2 \citep{mehta2019espnetv2}, MixNet \citep{tan2019mixconv}, and MNASNet \citep{tan2019mnasnet}. These light-weight CNNs are versatile and easy to train. For example, these networks can easily replace the heavy-weight backbones (e.g., ResNet \citep{he2016deep}) in existing task-specific models (e.g., DeepLabv3) to reduce the network size and improve latency. Despite these benefits, one major drawback of these methods is that they are spatially local. This work views transformers as convolutions; allowing to leverage the merits of both convolutions (e.g., versatile and simple training) and transformers (e.g., global processing) to build light-weight (\textsection \ref{sec:light_wt_transformer}) and general-purpose (\textsection \ref{ssec:imagenet_results} and \textsection \ref{sec:general_purpose}) ViTs.

\textbf{Vision transformers.} \citet{dosovitskiy2020image} apply transformers of \citet{vaswani2017attention} for large-scale image recognition and showed that with extremely large-scale datasets (e.g., JFT-300M), ViTs can achieve CNN-level accuracy without image-specific inductive bias. With extensive data augmentation, heavy L2 regularization, and distillation, ViTs can be trained on the ImageNet dataset to achieve CNN-level performance \citep{touvron2021training, touvron2021going, zhou2021deepvit}. However, unlike CNNs, ViTs show substandard optimizability and are difficult to train. Subsequent works \citep[e.g.,][]{graham2021levit, dai2021coatnet, liu2021swin, wang2021pyramid, yuan2021tokens, chen2021mobile} shows that this substandard optimizability is due to the lack of spatial inductive biases in ViTs. Incorporating such biases using convolutions in ViTs improves their stability and performance. Different designs have been explored to reap the benefits of convolutions and transformers. For instance, ViT-C of \citet{xiao2021early} adds an early convolutional stem to ViT. CvT \citep{wu2021cvt} modifies the multi-head attention in transformers and uses depth-wise separable convolutions instead of linear projections. BoTNet \citep{srinivas2021bottleneck} replaces the standard  convolution in the bottleneck unit of ResNet with multi-head attention. ConViT \citep{d2021convit} incorporates soft convolutional inductive biases using a gated positional self-attention. PiT \citep{heo2021rethinking} extends ViT with depth-wise convolution-based pooling layer. Though these models can achieve competitive performance to CNNs with extensive augmentation, the majority of these models are heavy-weight. For instance, PiT and CvT learns  and  more parameters than EfficientNet \citep{tan2019efficientnet} and achieves similar performance (top-1 accuracy of about 81.6\%) on ImageNet-1k dataset, respectively. Also, when these models are scaled down to build light-weight ViT models, their performance is significantly worse than light-weight CNNs. For a parameter budget of about 6 million, ImageNet-1k accuracy of PiT is 2.2\% less than MobileNetv3.

\textbf{Discussion.} Combining convolutions and transformers results in robust and high-performing ViTs as compared to vanilla ViTs. However, an open question here is: \emph{how to  combine the strengths of convolutions and transformers to build light-weight networks for mobile vision tasks?} This paper focuses on designing light-weight ViT models that outperform state-of-the-art models with simple training recipes. Towards this end, we introduce \arch~that combines the strengths of CNNs and ViTs to build a light-weight, general-purpose, and mobile-friendly network. \arch~brings several novel observations. (i) \textbf{Better performance:} For a given parameter budget, \arch~models achieve better performance as compared to existing light-weight CNNs across different mobile vision tasks (\textsection \ref{ssec:imagenet_results} and \textsection \ref{sec:general_purpose}). (ii) \textbf{Generalization capability:} Generalization capability refers to the gap between training and evaluation metrics. For two models with similar training metrics, the model with better evaluation metrics is more generalizable because it can predict better on an unseen dataset. Unlike previous ViT variants (with and without convolutions) which show poor generalization capability even with extensive data augmentation as compared to CNNs \citep{dai2021coatnet}, \arch~shows better generalization capability (Figure \ref{fig:imagenet_training}). (iii) \textbf{Robust:} A good model should be robust to hyper-parameters (e.g., data augmentation and L2 regularization) because tuning these hyper-parameters is time- and resource-consuming.  Unlike most ViT-based models, \arch~models train with basic augmentation and are less sensitive to L2 regularization (\textsection \ref{sec:appendix_ablations}).

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.35\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_training.pdf}
        \caption{Training error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_validation.pdf}
        \caption{Validation error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.28\columnwidth}
        \centering
        \begin{subfigure}[b]{\columnwidth}
            \centering
            \resizebox{0.8\columnwidth}{!}{
            \Huge
            \begin{tabular}{lrrr}
                \toprule[1.5pt]
                \textbf{Model} & \textbf{\# Params.} & \textbf{Top-1} & \sachin{\textbf{Top-5}} \\
                \midrule 
                 \arch-XXS & 1.3 M & 69.0 & \sachin{88.9} \\
                 \arch-XS & 2.3 M & 74.8 & \sachin{92.3} \\
                 \arch-S & 5.6 M & 78.4 & \sachin{94.1} \\
                 \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{Validation accuracy}
        \label{fig:model_sizes}
        \end{subfigure}
        \vfill
        \begin{subfigure}[b]{\columnwidth}
            \centering
            \includegraphics[width=0.8\columnwidth]{figures/param_dist.pdf}
            \caption{Parameter distribution}
            \label{fig:parameter_dist}
        \end{subfigure}
    \end{subfigure}
    \caption{\sachin{\textbf{\arch~shows similar generalization capabilities as CNNs}. Final training and validation errors of MobileNetv2 and ResNet-50 are marked with \scalebox{1.25}{} and \scalebox{1.25}{}, respectively (\textsection \ref{sec:appendix_msc}).}}
    \label{fig:imagenet_training}
\end{figure}

\section{\arch: A Light-weight Transformer}
\label{sec:light_wt_transformer}

A standard ViT model, shown in Figure \ref{fig:vit}, reshapes the input  into a sequence of flattened patches , projects it into a fixed -dimensional space , and then learn inter-patch representations using a stack of  transformer blocks. The computational cost of self-attention in vision transformers is . Here, , , and  represent the channels, height, and width of the tensor respectively, and  is number of pixels in the patch with height  and width , and  is the number of patches. Because these models ignore the spatial inductive bias that is inherent in CNNs, they require more parameters to learn visual representations. For instance, DPT \citep{dosovitskiy2020image}, a ViT-based network, learns  more parameters as compared to DeepLabv3 \citep{chen2017rethinking}, a CNN-based network, to deliver similar segmentation performance (DPT vs. DeepLabv3: 345 M vs. 59 M). Also, in comparison to CNNs, these models exhibit sub-standard optimizability. These models are sensitive to L2 regularization and require extensive data augmentation to prevent overfitting \citep{touvron2021training, xiao2021early}. 

This paper introduces a light-weight ViT model, \arch. The core idea is to learn global representations with transformers as convolutions. This allows us to implicitly incorporate convolution-like properties (e.g., spatial bias) in the network, learn representations with simple training recipes (e.g., basic augmentation), and easily integrate \arch~with downstream architectures (e.g., DeepLabv3 for segmentation).

\subsection{\arch~Architecture}
\label{ssec:mobilevit_arch}

\textbf{\arch~block.} The \arch~block, shown in Figure \ref{fig:mvt_arch}, aims to model the local and global information in an input tensor with fewer parameters. Formally, for a given input tensor , \arch~applies a  standard convolutional layer followed by a point-wise (or ) convolutional layer to produce . The  convolutional layer encodes local spatial information while the point-wise convolution projects the tensor to a high-dimensional space (or -dimensional, where ) by learning linear combinations of the input channels. 

With \arch, we want to model long-range non-local dependencies while having an effective receptive field of . One of the widely studied methods to model long-range dependencies is dilated convolutions. However, such approaches require careful selection of dilation rates. Otherwise, weights are applied to padded zeros instead of the valid spatial region \citep{yu2015multi, chen2017rethinking, mehta2018espnet}. Another promising solution is self-attention \citep{wang2018non, ramachandran2019stand, bello2019attention, dosovitskiy2020image}. Among self-attention methods, vision transformers (ViTs) with multi-head self-attention are shown to be effective for visual recognition tasks. However, ViTs are heavy-weight and exhibit sub-standard optimizability. This is because ViTs lack spatial inductive bias \citep{xiao2021early, graham2021levit}. 

To enable \arch~to learn global representations with spatial inductive bias, we unfold  into  non-overlapping flattened patches . Here, ,  is the number of patches, and  and  are height and width of a patch respectively. For each , inter-patch relationships are encoded by applying transformers to obtain  as:

Unlike ViTs that lose the spatial order of pixels, \arch~neither loses the patch order nor the spatial order of pixels within each patch (Figure \ref{fig:mvt_arch}). Therefore, we can fold  to obtain . \sachin{ is then projected to low -dimensional space using a point-wise convolution and combined with  via concatenation operation. Another  convolutional layer is then used to fuse these concatenated features.} Note that because  encodes local information from  region using convolutions and  encodes global information across  patches for the -th location, each pixel in  can encode information from all pixels in , as shown in Figure \ref{fig:rep_learning}. Thus, the overall effective receptive field of \arch~is .

\begin{figure}[t!]
    \centering
    \includegraphics[height=70px]{figures/fig_each_pixel.pdf}
    \caption{\textbf{Every pixel sees every other pixel in the \arch~block.} In this example, the \textcolor{centerPix}{\bf red} pixel attends to \textcolor{patchPix}{\bf blue} pixels (pixels at the corresponding location in other patches) using transformers. Because \textcolor{patchPix}{\bf blue} pixels have already encoded information about the neighboring pixels using convolutions, this allows the \textcolor{centerPix}{\bf red} pixel to encode information from all pixels in an image. Here, each cell in \textcolor{black}{\bf black} and \textcolor{gray}{\bf gray} grids represents a patch and a pixel, respectively.}
    \label{fig:rep_learning}
\end{figure}


\textbf{Relationship to convolutions.} Standard convolutions can be viewed as a stack of three sequential operations: (1) unfolding, (2) matrix multiplication (to learn local representations), and (3) folding. \arch~block is similar to convolutions in the sense that it  also leverages the same building blocks. \arch~block replaces the local processing (matrix multiplication) in convolutions with deeper global processing (a stack of transformer layers). As a consequence, \arch~has convolution-like properties (e.g., spatial bias). Hence, the \arch~block can be viewed as \emph{transformers as convolutions}. An advantage of our intentionally simple design is that low-level efficient implementations of convolutions and transformers can be used out-of-the-box; allowing us to use \arch~on different devices without any extra effort.

\textbf{Light-weight.} \arch~block uses standard convolutions and transformers to learn local and global representations respectively. Because previous works \citep[e.g.,][]{howard2017mobilenets, mehta2021delight} have shown that networks designed using these layers are heavy-weight, a natural question arises: Why \arch~is light-weight? We believe that the issues lie primarily in learning global representations with transformers. For a given patch, previous works \citep[e.g.,][]{touvron2021training, graham2021levit} convert the spatial information into latent by learning a linear combination of pixels (Figure \ref{fig:vit}). The global information is then encoded by learning inter-patch information using transformers. As a result, these models lose image-specific inductive bias, which is inherent in CNNs. Therefore, they require more capacity to learn visual representations. Hence, they are deep and wide. Unlike these models, \arch~uses convolutions and transformers in a way that the resultant \arch~block has convolution-like properties while simultaneously allowing for global processing. This modeling capability allows us to design shallow and narrow \arch~models, which in turn are light-weight. Compared to the ViT-based model DeIT that uses =12 and =192,  \arch~model uses = \{2, 4, 3\} and =\{96, 120, 144\} at spatial levels , , and , respectively. The resulting \arch~network is faster (), smaller (), and better () than DeIT network (Table \ref{tab:compare_latency}; \textsection \ref{sec:mobile_friendly}).

\textbf{Computational cost.} The computational cost of multi-headed self-attention in \arch~and ViTs (Figure \ref{fig:vit}) is  and , respectively. In theory, \arch~is inefficient as compared to ViTs. However, in practice, \arch~is more efficient than ViTs. \sachin{\arch~has  fewer FLOPs and delivers  better accuracy than DeIT on the ImageNet-1K dataset} (Table \ref{tab:compare_latency}; \textsection \ref{sec:mobile_friendly}). We believe that this is because of similar reasons as for the light-weight design (discussed above).

\textbf{\arch~architecture.} Our networks are inspired by the philosophy of light-weight CNNs. We train \arch~models at three different network sizes (S: small, XS: extra small, and XXS: extra extra small) that are typically used for mobile vision tasks (Figure \ref{fig:model_sizes}). The initial layer in \arch~is a strided  standard convolution, followed by MobileNetv2 (or MV2) blocks and \arch~blocks (Figure \ref{fig:mvt_arch} and \textsection \ref{sec:ablation_arch}). We use Swish \citep{elfwing2018sigmoid} as an activation function. Following CNN models, we use  in the \arch~block. The spatial dimensions of feature maps are usually multiples of 2 and . Therefore, we set  at all spatial levels (see \textsection \ref{sec:appendix_ablations} for more results). The MV2 blocks in \arch~network are mainly responsible for down-sampling. Therefore, these blocks are shallow and narrow in \arch~network. Spatial-level-wise parameter distribution of \arch~in Figure \ref{fig:parameter_dist} further shows that the contribution of MV2 blocks towards total network parameters is very small across different network configurations.

\subsection{Multi-scale Sampler for Training Efficiency} 
\label{ssec:multi_scale_sampler}

A standard approach in ViT-based models to learn multi-scale representations is \emph{fine-tuning}. For instance, \citet{touvron2021training} fine-tunes the DeIT model trained at a spatial resolution of  on varying sizes independently. Such an approach for learning multi-scale representations is preferable for ViTs because positional embeddings need to be interpolated based on the input size, and the network's performance is subjective to interpolation methods. Similar to CNNs, \arch~does not require any positional embeddings and it may benefit from multi-scale inputs during training. 
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \resizebox{!}{60px}{
            \hspace{-125pt}


\definecolor{patch0}{RGB}{127,201,127}
\definecolor{patch1}{RGB}{190,174,212}
\definecolor{patch2}{RGB}{253,192,134}
\definecolor{patch3}{RGB}{255,255,153}

\definecolor{patch4}{RGB}{56,108,176}
\definecolor{patch5}{RGB}{240,2,127}
\definecolor{patch6}{RGB}{191,91,23}
\definecolor{patch7}{RGB}{158,1,66}

\definecolor{patch8}{RGB}{228,26,28}
\definecolor{patch9}{RGB}{55,126,184}
\definecolor{patch10}{RGB}{77,175,74}
\definecolor{patch11}{RGB}{152,78,163}

\definecolor{patch12}{RGB}{255,127,0}
\definecolor{patch13}{RGB}{255,255,51}
\definecolor{patch14}{RGB}{166,86,40}
\definecolor{patch15}{RGB}{247,129,191}


\newcommand{\drawCube}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, fill=#4, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeDiv}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
       
       \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-1) -- ++(0,-\cubey,0) -- ++(0,0,1) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-1) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeEmpty}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=white, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\cubeWithPatches}{
    \begin{tikzpicture}
\node (patch_0) at (-1.0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch0}{0.75}};
        \node (patch_1) at (-0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch1}{0.75}};
        \node (patch_2) at (0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch2}{0.75}};
        \node (patch_3) at (0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch3}{0.75}};
\node (patch_4) at (-1.0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch4}{0.75}};
        \node (patch_5) at (-0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch5}{0.75}};
        \node (patch_6) at (0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch6}{0.75}};
        \node (patch_7) at (0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch7}{0.75}};
\node (patch_8) at (-1.0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch8}{0.95}};
        \node (patch_9) at (-0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch11}{0.95}};
        \node (patch_10) at (0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch10}{0.95}};
        \node (patch_11) at (0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch9}{0.95}};
\node (patch_12) at (-1.0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch12}{0.5}};
        \node (patch_13) at (-0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch13}{0.5}};
        \node (patch_14) at (0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch14}{0.5}};
        \node (patch_15) at (0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch15}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatches}{
    \begin{tikzpicture}
        \node (patch_0) {\drawCube{1.5}{0.15}{2}{patch3}{0.75}};
        \node[above=-0.99cm of patch_0] (patch_1) {\drawCube{1.5}{0.15}{2}{patch2}{0.75}};
        \node[above=-0.99cm of patch_1] (patch_2) {\drawCube{1.5}{0.15}{2}{patch1}{0.75}};
        \node[above=-0.99cm of patch_2] (patch_3) {\drawCube{1.5}{0.15}{2}{patch0}{0.75}};
\node[above=-0.99cm of patch_3] (patch_4) {\drawCube{1.5}{0.15}{2}{patch7}{0.75}};
        \node[above=-0.99cm of patch_4] (patch_5) {\drawCube{1.5}{0.15}{2}{patch6}{0.75}};
        \node[above=-0.99cm of patch_5] (patch_6) {\drawCube{1.5}{0.15}{2}{patch5}{0.75}};
        \node[above=-0.99cm of patch_6] (patch_7) {\drawCube{1.5}{0.15}{2}{patch4}{0.75}};
\node[above=-0.99cm of patch_7] (patch_8) {\drawCube{1.5}{0.15}{2}{patch11}{0.95}};
        \node[above=-0.99cm of patch_8] (patch_9) {\drawCube{1.5}{0.15}{2}{patch10}{0.95}};
        \node[above=-0.99cm of patch_9] (patch_10) {\drawCube{1.5}{0.15}{2}{patch9}{0.95}};
\node[above=-0.99cm of patch_10] (patch_11){\drawCube{1.5}{0.15}{2}{patch8}{0.95}};
\node[above=-0.99cm of patch_11] (patch_12){\drawCube{1.5}{0.15}{2}{patch15}{0.5}};
        \node[above=-0.99cm of patch_12] (patch_13){\drawCube{1.5}{0.15}{2}{patch14}{0.5}};
        \node[above=-0.99cm of patch_13] (patch_14){\drawCube{1.5}{0.15}{2}{patch13}{0.5}};
        \node[above=-0.99cm of patch_14] (patch_15){\drawCube{1.5}{0.15}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViT}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.05}{0.05}{2}{patch3}{0.75}};
        \node (patch_1) at (0, 0.05) {\drawCube{0.05}{0.05}{2}{patch2}{0.75}};
        \node (patch_2) at (0, 0.1) {\drawCube{0.05}{0.05}{2}{patch1}{0.75}};
        \node (patch_3) at (0, 0.15) {\drawCube{0.05}{0.05}{2}{patch0}{0.75}};
\node (patch_4)  at (0, 0.2) {\drawCube{0.05}{0.05}{2}{patch7}{0.75}};
        \node (patch_5)  at (0, 0.25) {\drawCube{0.05}{0.05}{2}{patch6}{0.75}};
        \node (patch_6) at (0, 0.3) {\drawCube{0.05}{0.05}{2}{patch5}{0.75}};
        \node (patch_7) at (0, 0.35) {\drawCube{0.05}{0.05}{2}{patch4}{0.75}};
\node (patch_8) at (0, 0.4) {\drawCube{0.05}{0.05}{2}{patch11}{0.95}};
        \node (patch_9) at (0, 0.45) {\drawCube{0.05}{0.05}{2}{patch10}{0.95}};
        \node (patch_10) at (0, 0.5) {\drawCube{0.05}{0.05}{2}{patch9}{0.95}};
        \node (patch_11) at (0, 0.55) {\drawCube{0.05}{0.05}{2}{patch8}{0.95}};
\node (patch_12) at (0, 0.6) {\drawCube{0.05}{0.05}{2}{patch15}{0.5}};
        \node (patch_13) at (0, 0.65) {\drawCube{0.05}{0.05}{2}{patch14}{0.5}};
        \node (patch_14) at (0, 0.7) {\drawCube{0.05}{0.05}{2}{patch13}{0.5}};
        \node (patch_15) at (0, 0.75) {\drawCube{0.05}{0.05}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViTSmall}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.2}{0.2}{3}{patch3}{0.75}};
        \node (patch_1) at (0, 0.2) {\drawCube{0.2}{0.2}{3}{patch2}{0.75}};
        \node (patch_2) at (0, 0.4) {\drawCube{0.2}{0.2}{3}{patch1}{0.75}};
        \node (patch_3) at (0, 0.6) {\drawCube{0.2}{0.2}{3}{patch0}{0.75}};
\node (patch_4)  at (0, 0.8) {\drawCube{0.2}{0.2}{3}{patch7}{0.75}};
        \node (patch_5)  at (0, 1.0) {\drawCube{0.2}{0.2}{3}{patch6}{0.75}};
        \node (patch_6) at (0, 1.2) {\drawCube{0.2}{0.2}{3}{patch5}{0.75}};
        \node (patch_7) at (0, 1.4) {\drawCube{0.2}{0.2}{3}{patch4}{0.75}};
\node (patch_8) at (0, 1.6) {\drawCube{0.2}{0.2}{3}{patch11}{0.95}};
        \node (patch_9) at (0, 1.8) {\drawCube{0.2}{0.2}{3}{patch10}{0.95}};
        \node (patch_10) at (0, 2.0) {\drawCube{0.2}{0.2}{3}{patch9}{0.95}};
        \node (patch_11) at (0, 2.2) {\drawCube{0.2}{0.2}{3}{patch8}{0.95}};
\node (patch_12) at (0, 2.4) {\drawCube{0.2}{0.2}{3}{patch15}{0.5}};
        \node (patch_13) at (0, 2.6) {\drawCube{0.2}{0.2}{3}{patch14}{0.5}};
        \node (patch_14) at (0, 2.8) {\drawCube{0.2}{0.2}{3}{patch13}{0.5}};
        \node (patch_15) at (0, 3.0) {\drawCube{0.2}{0.2}{3}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\mvtBlock}{
    \begin{tikzpicture}
    
        \draw [line width=0.75mm, fill=green!20, rounded corners] (-2.5, 2.5) -- (40.5, 2.5) -- (40.5, -7.25) -- (-2.5, -7.25) --cycle;
        \node [align=center] (temp) at (19, 2.0) {\scalebox{2.5}{\textcolor{red}{\bfseries \arch~block}}};
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, 0.25) -- (25.5, 0.25) -- (25.5, -7) -- (6.5, -7) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26.5, 1.75) -- (37.0, 1.75) -- (37.0, -5.5) -- (26.5, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{blue}{\bf Local}} \\ \scalebox{2}{\textcolor{blue}{\bf representations}}};
\node[align=center] (cap_img) at (16, -0.4) {\scalebox{2}{\textcolor{blue}{\bf Transformers as Convolutions}}};
        \node[align=center] (cap_img_a) at (16, -1.1) {\scalebox{1.75}{\textcolor{blue}{\bf (global representations)}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{blue}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.6cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.7cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.6cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, >=latex, line width=1.25mm, color=red] (tensor) edge[in=175, out=5] (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.8) -- (14, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.8) -- (25.25, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\mvtBlockOld}{
    \begin{tikzpicture}
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, -1.25) -- (25.5, -1.25) -- (25.5, -6.5) -- (6.5, -6.5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26, 1.75) -- (36.5, 1.75) -- (36.4, -5.5) -- (26, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{red}{\bf Local}} \\ \scalebox{2}{\textcolor{red}{\bf representations}}};
\node[align=center] (cap_img) at (16, -1.75) {\scalebox{2}{\textcolor{red}{\bf Global representations}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{red}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.35cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-3x3}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.75mm] (tensor) -- (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.25) -- (14, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.25) -- (25.25, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\tikzset{
    cross/.pic = {
    \draw[rotate = 45] (-#1,0) -- (#1,0);
    \draw[rotate = 45] (0,-#1) -- (0, #1);
    }
}

\newcommand{\vitC}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node [right=1.25cm of img, draw, fill=white, rounded corners, minimum height=2cm, minimum width=5cm, line width=1mm, align=center] (conv_stack) {\scalebox{2}{\{Conv- \} } \\ \scalebox{2}{} \\ \scalebox{2}{Reshape \& Flatten}};
\node [right=1.25cm of conv_stack, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.9)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node[right=1.00cm of flatten, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.2,-0.85)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=2cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=2cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (conv_stack);
        \draw [->, line width=1mm] (conv_stack) -- (flatten);
\draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}


\newcommand{\mvtBlockA}{
    \begin{tikzpicture}
        \node[inner sep=0pt, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [right=0.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=0.75cm of conv, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=above:{}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (patch_im) {\cubeWithPatches};
\node [right=0.75cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};
        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}] (proj_tensor) {\drawCube{2}{2}{1}{white}{1.0}};
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Concat} \\ \scalebox{1.5}{} \\ \scalebox{1.5}{Conv-3x3}};
\node [right=0.75cm of concat, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.5mm] (tensor) -- (conv);
        \draw [->, line width=0.5mm] (conv) -- (img);
        \draw [->, line width=0.5mm] (img) -- (flatten);
        \draw [->, line width=0.5mm] (flatten) -- (transformer);
        \draw [->, line width=0.5mm] (transformer) -- (trans_out);
        \draw [->, line width=0.5mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.5mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.5mm] (conv_proj) -- (proj_tensor);
        \draw [->, line width=0.5mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.5mm] (tensor) -- (dummY_tensor.center) -| (concat.north);
        \draw [->, line width=0.5mm] (concat) -- (out);
        


        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(5, -2) -- (12, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(16.5, -2) -- (23.5, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\vit}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
        \begin{scope}[shift={(img.south west)},x={(img.south east)},y={(img.north west)}]
            \draw[help lines, xstep=.2, ystep=.2, color=black, line width=0.75mm] (0,0) grid (1.0, 1.0);
        \end{scope}
        
\node [right=1.25cm of img, label={[shift={(-1,-3.3)}]\scalebox{2}{}}, label={[shift={(-0.25,-1.1)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(-1, -3) -- (5, -3) node [black,midway, below=8pt] {\scalebox{2}{Flatten image patches}};
        
        \node[right=1.25cm of flatten, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.2,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.95)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=1cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=1.5cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (flatten);
        \draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}

\newcommand{\stdDDP}{
    \begin{tikzpicture}
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-3}}] (tensor_a1) at (0, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-2}}] (tensor_a2) at (0, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-1}}] (tensor_a3) at (0, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_c) at (3.1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-1.4, -1.4) -- (4, -1.4);
        \node at (3.5, -1.9)  {\scalebox{2.5}{time}};
        
        \node[above=0.75cm of tensor_a3, align=center] (b){\scalebox{3}{\textcolor{stdSampler}{\bfseries Standard sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\mscDDP}{
    \begin{tikzpicture}
    
        \node[label={[shift={(-3.6,-1.75)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0.92, 0, 3) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-2.25)}]\scalebox{2.5}{GPU-3}}] at (-0.08, 2, 2.5) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-3.0)}]\scalebox{2.5}{GPU-2}}] at (-0.57, 4.01, 1.5) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node[label={[shift={(-3.85,-1.0)}]\scalebox{2.5}{GPU-1}}] at (1.5, 6.1, 4) {\drawCube{4}{1}{2}{red!20}{1.0}};
        
        \node at (3.2-0.5, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (marked) at (3.2-0.5, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node at (5.90-0.95, 0.5, 2.5) {\drawCube{4}{1}{2}{red!20}{1.0}};
        \node at (4.44-0.95, 2.02, 1.6) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node at (5.92-0.95, 4.01, 3.05) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node at (5.45-0.95, 6.55, 3.95) {\drawCube{2}{2}{2}{red!20}{1.0}};
        
        \node at (8.2-1.4, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-2.45, -2.35) -- (7, -2.35);
        
        \node at (6.5, -2.75)  {\scalebox{2.5}{time}};
        \node[above=0.75cm of marked, align=center] (b) {\scalebox{3}{\textcolor{mscSampler}{\bfseries Multi-scale sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\tensorNotations}{
    \begin{tikzpicture}
        \node[label={[shift={(-1.75,-2.5)}]\rotatebox{90}{ \scalebox{1.35}{Height}}}, label={[shift={(-1.2,-0.9)}]\rotatebox{50}{\scalebox{1.35}{Batch}}}, label={[shift={(-0.5,-3.5)}] \scalebox{1.35}{Width}}] (tensor_c) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[right=0.1cm of tensor_c] (a) {\scalebox{1.75}{Tensor}};
        
        \node[below=1cm of tensor_c] (tensor_d) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node[right=0.1cm of tensor_d] (b) {\scalebox{1.75}{Gradient sync.}};
    \end{tikzpicture}
}



%
 \stdDDP
        }
        \resizebox{!}{60px}{
            \hspace{-55pt}


\definecolor{patch0}{RGB}{127,201,127}
\definecolor{patch1}{RGB}{190,174,212}
\definecolor{patch2}{RGB}{253,192,134}
\definecolor{patch3}{RGB}{255,255,153}

\definecolor{patch4}{RGB}{56,108,176}
\definecolor{patch5}{RGB}{240,2,127}
\definecolor{patch6}{RGB}{191,91,23}
\definecolor{patch7}{RGB}{158,1,66}

\definecolor{patch8}{RGB}{228,26,28}
\definecolor{patch9}{RGB}{55,126,184}
\definecolor{patch10}{RGB}{77,175,74}
\definecolor{patch11}{RGB}{152,78,163}

\definecolor{patch12}{RGB}{255,127,0}
\definecolor{patch13}{RGB}{255,255,51}
\definecolor{patch14}{RGB}{166,86,40}
\definecolor{patch15}{RGB}{247,129,191}


\newcommand{\drawCube}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, fill=#4, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeDiv}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
       
       \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-1) -- ++(0,-\cubey,0) -- ++(0,0,1) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-1) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeEmpty}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=white, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\cubeWithPatches}{
    \begin{tikzpicture}
\node (patch_0) at (-1.0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch0}{0.75}};
        \node (patch_1) at (-0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch1}{0.75}};
        \node (patch_2) at (0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch2}{0.75}};
        \node (patch_3) at (0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch3}{0.75}};
\node (patch_4) at (-1.0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch4}{0.75}};
        \node (patch_5) at (-0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch5}{0.75}};
        \node (patch_6) at (0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch6}{0.75}};
        \node (patch_7) at (0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch7}{0.75}};
\node (patch_8) at (-1.0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch8}{0.95}};
        \node (patch_9) at (-0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch11}{0.95}};
        \node (patch_10) at (0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch10}{0.95}};
        \node (patch_11) at (0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch9}{0.95}};
\node (patch_12) at (-1.0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch12}{0.5}};
        \node (patch_13) at (-0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch13}{0.5}};
        \node (patch_14) at (0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch14}{0.5}};
        \node (patch_15) at (0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch15}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatches}{
    \begin{tikzpicture}
        \node (patch_0) {\drawCube{1.5}{0.15}{2}{patch3}{0.75}};
        \node[above=-0.99cm of patch_0] (patch_1) {\drawCube{1.5}{0.15}{2}{patch2}{0.75}};
        \node[above=-0.99cm of patch_1] (patch_2) {\drawCube{1.5}{0.15}{2}{patch1}{0.75}};
        \node[above=-0.99cm of patch_2] (patch_3) {\drawCube{1.5}{0.15}{2}{patch0}{0.75}};
\node[above=-0.99cm of patch_3] (patch_4) {\drawCube{1.5}{0.15}{2}{patch7}{0.75}};
        \node[above=-0.99cm of patch_4] (patch_5) {\drawCube{1.5}{0.15}{2}{patch6}{0.75}};
        \node[above=-0.99cm of patch_5] (patch_6) {\drawCube{1.5}{0.15}{2}{patch5}{0.75}};
        \node[above=-0.99cm of patch_6] (patch_7) {\drawCube{1.5}{0.15}{2}{patch4}{0.75}};
\node[above=-0.99cm of patch_7] (patch_8) {\drawCube{1.5}{0.15}{2}{patch11}{0.95}};
        \node[above=-0.99cm of patch_8] (patch_9) {\drawCube{1.5}{0.15}{2}{patch10}{0.95}};
        \node[above=-0.99cm of patch_9] (patch_10) {\drawCube{1.5}{0.15}{2}{patch9}{0.95}};
\node[above=-0.99cm of patch_10] (patch_11){\drawCube{1.5}{0.15}{2}{patch8}{0.95}};
\node[above=-0.99cm of patch_11] (patch_12){\drawCube{1.5}{0.15}{2}{patch15}{0.5}};
        \node[above=-0.99cm of patch_12] (patch_13){\drawCube{1.5}{0.15}{2}{patch14}{0.5}};
        \node[above=-0.99cm of patch_13] (patch_14){\drawCube{1.5}{0.15}{2}{patch13}{0.5}};
        \node[above=-0.99cm of patch_14] (patch_15){\drawCube{1.5}{0.15}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViT}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.05}{0.05}{2}{patch3}{0.75}};
        \node (patch_1) at (0, 0.05) {\drawCube{0.05}{0.05}{2}{patch2}{0.75}};
        \node (patch_2) at (0, 0.1) {\drawCube{0.05}{0.05}{2}{patch1}{0.75}};
        \node (patch_3) at (0, 0.15) {\drawCube{0.05}{0.05}{2}{patch0}{0.75}};
\node (patch_4)  at (0, 0.2) {\drawCube{0.05}{0.05}{2}{patch7}{0.75}};
        \node (patch_5)  at (0, 0.25) {\drawCube{0.05}{0.05}{2}{patch6}{0.75}};
        \node (patch_6) at (0, 0.3) {\drawCube{0.05}{0.05}{2}{patch5}{0.75}};
        \node (patch_7) at (0, 0.35) {\drawCube{0.05}{0.05}{2}{patch4}{0.75}};
\node (patch_8) at (0, 0.4) {\drawCube{0.05}{0.05}{2}{patch11}{0.95}};
        \node (patch_9) at (0, 0.45) {\drawCube{0.05}{0.05}{2}{patch10}{0.95}};
        \node (patch_10) at (0, 0.5) {\drawCube{0.05}{0.05}{2}{patch9}{0.95}};
        \node (patch_11) at (0, 0.55) {\drawCube{0.05}{0.05}{2}{patch8}{0.95}};
\node (patch_12) at (0, 0.6) {\drawCube{0.05}{0.05}{2}{patch15}{0.5}};
        \node (patch_13) at (0, 0.65) {\drawCube{0.05}{0.05}{2}{patch14}{0.5}};
        \node (patch_14) at (0, 0.7) {\drawCube{0.05}{0.05}{2}{patch13}{0.5}};
        \node (patch_15) at (0, 0.75) {\drawCube{0.05}{0.05}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViTSmall}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.2}{0.2}{3}{patch3}{0.75}};
        \node (patch_1) at (0, 0.2) {\drawCube{0.2}{0.2}{3}{patch2}{0.75}};
        \node (patch_2) at (0, 0.4) {\drawCube{0.2}{0.2}{3}{patch1}{0.75}};
        \node (patch_3) at (0, 0.6) {\drawCube{0.2}{0.2}{3}{patch0}{0.75}};
\node (patch_4)  at (0, 0.8) {\drawCube{0.2}{0.2}{3}{patch7}{0.75}};
        \node (patch_5)  at (0, 1.0) {\drawCube{0.2}{0.2}{3}{patch6}{0.75}};
        \node (patch_6) at (0, 1.2) {\drawCube{0.2}{0.2}{3}{patch5}{0.75}};
        \node (patch_7) at (0, 1.4) {\drawCube{0.2}{0.2}{3}{patch4}{0.75}};
\node (patch_8) at (0, 1.6) {\drawCube{0.2}{0.2}{3}{patch11}{0.95}};
        \node (patch_9) at (0, 1.8) {\drawCube{0.2}{0.2}{3}{patch10}{0.95}};
        \node (patch_10) at (0, 2.0) {\drawCube{0.2}{0.2}{3}{patch9}{0.95}};
        \node (patch_11) at (0, 2.2) {\drawCube{0.2}{0.2}{3}{patch8}{0.95}};
\node (patch_12) at (0, 2.4) {\drawCube{0.2}{0.2}{3}{patch15}{0.5}};
        \node (patch_13) at (0, 2.6) {\drawCube{0.2}{0.2}{3}{patch14}{0.5}};
        \node (patch_14) at (0, 2.8) {\drawCube{0.2}{0.2}{3}{patch13}{0.5}};
        \node (patch_15) at (0, 3.0) {\drawCube{0.2}{0.2}{3}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\mvtBlock}{
    \begin{tikzpicture}
    
        \draw [line width=0.75mm, fill=green!20, rounded corners] (-2.5, 2.5) -- (40.5, 2.5) -- (40.5, -7.25) -- (-2.5, -7.25) --cycle;
        \node [align=center] (temp) at (19, 2.0) {\scalebox{2.5}{\textcolor{red}{\bfseries \arch~block}}};
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, 0.25) -- (25.5, 0.25) -- (25.5, -7) -- (6.5, -7) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26.5, 1.75) -- (37.0, 1.75) -- (37.0, -5.5) -- (26.5, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{blue}{\bf Local}} \\ \scalebox{2}{\textcolor{blue}{\bf representations}}};
\node[align=center] (cap_img) at (16, -0.4) {\scalebox{2}{\textcolor{blue}{\bf Transformers as Convolutions}}};
        \node[align=center] (cap_img_a) at (16, -1.1) {\scalebox{1.75}{\textcolor{blue}{\bf (global representations)}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{blue}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.6cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.7cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.6cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, >=latex, line width=1.25mm, color=red] (tensor) edge[in=175, out=5] (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.8) -- (14, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.8) -- (25.25, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\mvtBlockOld}{
    \begin{tikzpicture}
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, -1.25) -- (25.5, -1.25) -- (25.5, -6.5) -- (6.5, -6.5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26, 1.75) -- (36.5, 1.75) -- (36.4, -5.5) -- (26, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{red}{\bf Local}} \\ \scalebox{2}{\textcolor{red}{\bf representations}}};
\node[align=center] (cap_img) at (16, -1.75) {\scalebox{2}{\textcolor{red}{\bf Global representations}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{red}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.35cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-3x3}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.75mm] (tensor) -- (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.25) -- (14, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.25) -- (25.25, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\tikzset{
    cross/.pic = {
    \draw[rotate = 45] (-#1,0) -- (#1,0);
    \draw[rotate = 45] (0,-#1) -- (0, #1);
    }
}

\newcommand{\vitC}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node [right=1.25cm of img, draw, fill=white, rounded corners, minimum height=2cm, minimum width=5cm, line width=1mm, align=center] (conv_stack) {\scalebox{2}{\{Conv- \} } \\ \scalebox{2}{} \\ \scalebox{2}{Reshape \& Flatten}};
\node [right=1.25cm of conv_stack, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.9)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node[right=1.00cm of flatten, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.2,-0.85)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=2cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=2cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (conv_stack);
        \draw [->, line width=1mm] (conv_stack) -- (flatten);
\draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}


\newcommand{\mvtBlockA}{
    \begin{tikzpicture}
        \node[inner sep=0pt, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [right=0.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=0.75cm of conv, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=above:{}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (patch_im) {\cubeWithPatches};
\node [right=0.75cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};
        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}] (proj_tensor) {\drawCube{2}{2}{1}{white}{1.0}};
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Concat} \\ \scalebox{1.5}{} \\ \scalebox{1.5}{Conv-3x3}};
\node [right=0.75cm of concat, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.5mm] (tensor) -- (conv);
        \draw [->, line width=0.5mm] (conv) -- (img);
        \draw [->, line width=0.5mm] (img) -- (flatten);
        \draw [->, line width=0.5mm] (flatten) -- (transformer);
        \draw [->, line width=0.5mm] (transformer) -- (trans_out);
        \draw [->, line width=0.5mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.5mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.5mm] (conv_proj) -- (proj_tensor);
        \draw [->, line width=0.5mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.5mm] (tensor) -- (dummY_tensor.center) -| (concat.north);
        \draw [->, line width=0.5mm] (concat) -- (out);
        


        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(5, -2) -- (12, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(16.5, -2) -- (23.5, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\vit}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
        \begin{scope}[shift={(img.south west)},x={(img.south east)},y={(img.north west)}]
            \draw[help lines, xstep=.2, ystep=.2, color=black, line width=0.75mm] (0,0) grid (1.0, 1.0);
        \end{scope}
        
\node [right=1.25cm of img, label={[shift={(-1,-3.3)}]\scalebox{2}{}}, label={[shift={(-0.25,-1.1)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(-1, -3) -- (5, -3) node [black,midway, below=8pt] {\scalebox{2}{Flatten image patches}};
        
        \node[right=1.25cm of flatten, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.2,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.95)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=1cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=1.5cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (flatten);
        \draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}

\newcommand{\stdDDP}{
    \begin{tikzpicture}
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-3}}] (tensor_a1) at (0, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-2}}] (tensor_a2) at (0, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-1}}] (tensor_a3) at (0, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_c) at (3.1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-1.4, -1.4) -- (4, -1.4);
        \node at (3.5, -1.9)  {\scalebox{2.5}{time}};
        
        \node[above=0.75cm of tensor_a3, align=center] (b){\scalebox{3}{\textcolor{stdSampler}{\bfseries Standard sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\mscDDP}{
    \begin{tikzpicture}
    
        \node[label={[shift={(-3.6,-1.75)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0.92, 0, 3) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-2.25)}]\scalebox{2.5}{GPU-3}}] at (-0.08, 2, 2.5) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-3.0)}]\scalebox{2.5}{GPU-2}}] at (-0.57, 4.01, 1.5) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node[label={[shift={(-3.85,-1.0)}]\scalebox{2.5}{GPU-1}}] at (1.5, 6.1, 4) {\drawCube{4}{1}{2}{red!20}{1.0}};
        
        \node at (3.2-0.5, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (marked) at (3.2-0.5, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node at (5.90-0.95, 0.5, 2.5) {\drawCube{4}{1}{2}{red!20}{1.0}};
        \node at (4.44-0.95, 2.02, 1.6) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node at (5.92-0.95, 4.01, 3.05) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node at (5.45-0.95, 6.55, 3.95) {\drawCube{2}{2}{2}{red!20}{1.0}};
        
        \node at (8.2-1.4, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-2.45, -2.35) -- (7, -2.35);
        
        \node at (6.5, -2.75)  {\scalebox{2.5}{time}};
        \node[above=0.75cm of marked, align=center] (b) {\scalebox{3}{\textcolor{mscSampler}{\bfseries Multi-scale sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\tensorNotations}{
    \begin{tikzpicture}
        \node[label={[shift={(-1.75,-2.5)}]\rotatebox{90}{ \scalebox{1.35}{Height}}}, label={[shift={(-1.2,-0.9)}]\rotatebox{50}{\scalebox{1.35}{Batch}}}, label={[shift={(-0.5,-3.5)}] \scalebox{1.35}{Width}}] (tensor_c) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[right=0.1cm of tensor_c] (a) {\scalebox{1.75}{Tensor}};
        
        \node[below=1cm of tensor_c] (tensor_d) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node[right=0.1cm of tensor_d] (b) {\scalebox{1.75}{Gradient sync.}};
    \end{tikzpicture}
}



%
 \mscDDP
        }
        \resizebox{!}{60px}{
            \hspace{-60pt}


\definecolor{patch0}{RGB}{127,201,127}
\definecolor{patch1}{RGB}{190,174,212}
\definecolor{patch2}{RGB}{253,192,134}
\definecolor{patch3}{RGB}{255,255,153}

\definecolor{patch4}{RGB}{56,108,176}
\definecolor{patch5}{RGB}{240,2,127}
\definecolor{patch6}{RGB}{191,91,23}
\definecolor{patch7}{RGB}{158,1,66}

\definecolor{patch8}{RGB}{228,26,28}
\definecolor{patch9}{RGB}{55,126,184}
\definecolor{patch10}{RGB}{77,175,74}
\definecolor{patch11}{RGB}{152,78,163}

\definecolor{patch12}{RGB}{255,127,0}
\definecolor{patch13}{RGB}{255,255,51}
\definecolor{patch14}{RGB}{166,86,40}
\definecolor{patch15}{RGB}{247,129,191}


\newcommand{\drawCube}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, fill=#4, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeDiv}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
       
       \draw[draw=black, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-1) -- ++(0,-\cubey,0) -- ++(0,0,1) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-1) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\drawCubeEmpty}[5]{
    \begin{tikzpicture}[line join=round]
      \pgfmathsetmacro{\cubex}{#1}
      \pgfmathsetmacro{\cubey}{#2}
      \pgfmathsetmacro{\cubez}{#3}
      \draw[draw=white, opacity=1] (0,0,0)-- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle
       (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle
       (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{tikzpicture}
}

\newcommand{\cubeWithPatches}{
    \begin{tikzpicture}
\node (patch_0) at (-1.0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch0}{0.75}};
        \node (patch_1) at (-0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch1}{0.75}};
        \node (patch_2) at (0, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch2}{0.75}};
        \node (patch_3) at (0.5, -1, -0.65) {\drawCube{0.5}{0.5}{2}{patch3}{0.75}};
\node (patch_4) at (-1.0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch4}{0.75}};
        \node (patch_5) at (-0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch5}{0.75}};
        \node (patch_6) at (0, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch6}{0.75}};
        \node (patch_7) at (0.5, -0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch7}{0.75}};
\node (patch_8) at (-1.0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch8}{0.95}};
        \node (patch_9) at (-0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch11}{0.95}};
        \node (patch_10) at (0, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch10}{0.95}};
        \node (patch_11) at (0.5, 0, -0.65) {\drawCube{0.5}{0.5}{2}{patch9}{0.95}};
\node (patch_12) at (-1.0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch12}{0.5}};
        \node (patch_13) at (-0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch13}{0.5}};
        \node (patch_14) at (0, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch14}{0.5}};
        \node (patch_15) at (0.5, 0.5, -0.65) {\drawCube{0.5}{0.5}{2}{patch15}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatches}{
    \begin{tikzpicture}
        \node (patch_0) {\drawCube{1.5}{0.15}{2}{patch3}{0.75}};
        \node[above=-0.99cm of patch_0] (patch_1) {\drawCube{1.5}{0.15}{2}{patch2}{0.75}};
        \node[above=-0.99cm of patch_1] (patch_2) {\drawCube{1.5}{0.15}{2}{patch1}{0.75}};
        \node[above=-0.99cm of patch_2] (patch_3) {\drawCube{1.5}{0.15}{2}{patch0}{0.75}};
\node[above=-0.99cm of patch_3] (patch_4) {\drawCube{1.5}{0.15}{2}{patch7}{0.75}};
        \node[above=-0.99cm of patch_4] (patch_5) {\drawCube{1.5}{0.15}{2}{patch6}{0.75}};
        \node[above=-0.99cm of patch_5] (patch_6) {\drawCube{1.5}{0.15}{2}{patch5}{0.75}};
        \node[above=-0.99cm of patch_6] (patch_7) {\drawCube{1.5}{0.15}{2}{patch4}{0.75}};
\node[above=-0.99cm of patch_7] (patch_8) {\drawCube{1.5}{0.15}{2}{patch11}{0.95}};
        \node[above=-0.99cm of patch_8] (patch_9) {\drawCube{1.5}{0.15}{2}{patch10}{0.95}};
        \node[above=-0.99cm of patch_9] (patch_10) {\drawCube{1.5}{0.15}{2}{patch9}{0.95}};
\node[above=-0.99cm of patch_10] (patch_11){\drawCube{1.5}{0.15}{2}{patch8}{0.95}};
\node[above=-0.99cm of patch_11] (patch_12){\drawCube{1.5}{0.15}{2}{patch15}{0.5}};
        \node[above=-0.99cm of patch_12] (patch_13){\drawCube{1.5}{0.15}{2}{patch14}{0.5}};
        \node[above=-0.99cm of patch_13] (patch_14){\drawCube{1.5}{0.15}{2}{patch13}{0.5}};
        \node[above=-0.99cm of patch_14] (patch_15){\drawCube{1.5}{0.15}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViT}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.05}{0.05}{2}{patch3}{0.75}};
        \node (patch_1) at (0, 0.05) {\drawCube{0.05}{0.05}{2}{patch2}{0.75}};
        \node (patch_2) at (0, 0.1) {\drawCube{0.05}{0.05}{2}{patch1}{0.75}};
        \node (patch_3) at (0, 0.15) {\drawCube{0.05}{0.05}{2}{patch0}{0.75}};
\node (patch_4)  at (0, 0.2) {\drawCube{0.05}{0.05}{2}{patch7}{0.75}};
        \node (patch_5)  at (0, 0.25) {\drawCube{0.05}{0.05}{2}{patch6}{0.75}};
        \node (patch_6) at (0, 0.3) {\drawCube{0.05}{0.05}{2}{patch5}{0.75}};
        \node (patch_7) at (0, 0.35) {\drawCube{0.05}{0.05}{2}{patch4}{0.75}};
\node (patch_8) at (0, 0.4) {\drawCube{0.05}{0.05}{2}{patch11}{0.95}};
        \node (patch_9) at (0, 0.45) {\drawCube{0.05}{0.05}{2}{patch10}{0.95}};
        \node (patch_10) at (0, 0.5) {\drawCube{0.05}{0.05}{2}{patch9}{0.95}};
        \node (patch_11) at (0, 0.55) {\drawCube{0.05}{0.05}{2}{patch8}{0.95}};
\node (patch_12) at (0, 0.6) {\drawCube{0.05}{0.05}{2}{patch15}{0.5}};
        \node (patch_13) at (0, 0.65) {\drawCube{0.05}{0.05}{2}{patch14}{0.5}};
        \node (patch_14) at (0, 0.7) {\drawCube{0.05}{0.05}{2}{patch13}{0.5}};
        \node (patch_15) at (0, 0.75) {\drawCube{0.05}{0.05}{2}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\cubeWithFlattenPatchesViTSmall}{
    \begin{tikzpicture}
        \node (patch_0) at (0, 0) {\drawCube{0.2}{0.2}{3}{patch3}{0.75}};
        \node (patch_1) at (0, 0.2) {\drawCube{0.2}{0.2}{3}{patch2}{0.75}};
        \node (patch_2) at (0, 0.4) {\drawCube{0.2}{0.2}{3}{patch1}{0.75}};
        \node (patch_3) at (0, 0.6) {\drawCube{0.2}{0.2}{3}{patch0}{0.75}};
\node (patch_4)  at (0, 0.8) {\drawCube{0.2}{0.2}{3}{patch7}{0.75}};
        \node (patch_5)  at (0, 1.0) {\drawCube{0.2}{0.2}{3}{patch6}{0.75}};
        \node (patch_6) at (0, 1.2) {\drawCube{0.2}{0.2}{3}{patch5}{0.75}};
        \node (patch_7) at (0, 1.4) {\drawCube{0.2}{0.2}{3}{patch4}{0.75}};
\node (patch_8) at (0, 1.6) {\drawCube{0.2}{0.2}{3}{patch11}{0.95}};
        \node (patch_9) at (0, 1.8) {\drawCube{0.2}{0.2}{3}{patch10}{0.95}};
        \node (patch_10) at (0, 2.0) {\drawCube{0.2}{0.2}{3}{patch9}{0.95}};
        \node (patch_11) at (0, 2.2) {\drawCube{0.2}{0.2}{3}{patch8}{0.95}};
\node (patch_12) at (0, 2.4) {\drawCube{0.2}{0.2}{3}{patch15}{0.5}};
        \node (patch_13) at (0, 2.6) {\drawCube{0.2}{0.2}{3}{patch14}{0.5}};
        \node (patch_14) at (0, 2.8) {\drawCube{0.2}{0.2}{3}{patch13}{0.5}};
        \node (patch_15) at (0, 3.0) {\drawCube{0.2}{0.2}{3}{patch12}{0.5}};
\end{tikzpicture}
}

\newcommand{\mvtBlock}{
    \begin{tikzpicture}
    
        \draw [line width=0.75mm, fill=green!20, rounded corners] (-2.5, 2.5) -- (40.5, 2.5) -- (40.5, -7.25) -- (-2.5, -7.25) --cycle;
        \node [align=center] (temp) at (19, 2.0) {\scalebox{2.5}{\textcolor{red}{\bfseries \arch~block}}};
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, 0.25) -- (25.5, 0.25) -- (25.5, -7) -- (6.5, -7) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26.5, 1.75) -- (37.0, 1.75) -- (37.0, -5.5) -- (26.5, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{blue}{\bf Local}} \\ \scalebox{2}{\textcolor{blue}{\bf representations}}};
\node[align=center] (cap_img) at (16, -0.4) {\scalebox{2}{\textcolor{blue}{\bf Transformers as Convolutions}}};
        \node[align=center] (cap_img_a) at (16, -1.1) {\scalebox{1.75}{\textcolor{blue}{\bf (global representations)}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{blue}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.6cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-4.7)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-3)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.7cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.6cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, >=latex, line width=1.25mm, color=red] (tensor) edge[in=175, out=5] (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.8) -- (14, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.8) -- (25.25, -5.8) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\mvtBlockOld}{
    \begin{tikzpicture}
    
        \draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (1.4, -0.75) -- (6.1, -0.75) -- (6.1, -5) -- (1.4, -5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (6.5, -1.25) -- (25.5, -1.25) -- (25.5, -6.5) -- (6.5, -6.5) -- cycle;
\draw [dashed, line width=0.25mm, fill=cyan!5, rounded corners] (26, 1.75) -- (36.5, 1.75) -- (36.4, -5.5) -- (26, -5.5) -- cycle;
\node[align=center] (cap_img) at (3.75, -1.45) {\scalebox{2}{\textcolor{red}{\bf Local}} \\ \scalebox{2}{\textcolor{red}{\bf representations}}};
\node[align=center] (cap_img) at (16, -1.75) {\scalebox{2}{\textcolor{red}{\bf Global representations}}};
\node[align=center] (cap_img) at (35, -5) {\scalebox{2}{\textcolor{red}{\bf Fusion}}};
\node[inner sep=0pt, label={[shift={(-0.35,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [below right=1.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=2.25cm of conv, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=below:{\scalebox{1.5}{}}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.45)}]\scalebox{1.5}{}}, label={[shift={(-1.45,-1.75)}]\scalebox{1.5}{}}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.65)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.5)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{\scalebox{1.5}{}}}] (patch_im) {\cubeWithPatches};
\node [right=1.5cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};


        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor_a) {\drawCube{2}{2}{1}{white}{1.0}};
        
        \node [right=28.35cm of tensor, fill=white, label={[shift={(-0.25,-2.9)}]\scalebox{1.5}{}}, label={[shift={(-1.65,-2.25)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.8)}] \rotatebox{45}{\scalebox{1.5}{}}}] (proj_tensor) {\drawCubeDiv{2}{2}{2}{white}{1.0}};
        
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Conv-3x3}};
\node [right=1.25cm of concat, fill=white, label={[shift={(-0.25,-3.1)}]\scalebox{1.5}{}}, label={[shift={(-1.5,-2.0)}]\scalebox{1.5}{}}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{\scalebox{1.5}{}}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.75mm] (tensor) |- (conv);
        \draw [->, line width=0.75mm] (conv) -- (img);
        \draw [->, line width=0.75mm] (img) -- (flatten);
        \draw [->, line width=0.75mm] (flatten) -- (transformer);
        \draw [->, line width=0.75mm] (transformer) -- (trans_out);
        \draw [->, line width=0.75mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.75mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.75mm] (conv_proj) -- (proj_tensor_a);
        \draw [->, line width=0.75mm] (proj_tensor_a) -- (proj_tensor);
        \draw [->, line width=0.75mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.75mm] (tensor) -- (proj_tensor);
        \draw [->, line width=0.75mm] (concat) -- (out);
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (7, -5.25) -- (14, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm] (18.5, -5.25) -- (25.25, -5.25) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\tikzset{
    cross/.pic = {
    \draw[rotate = 45] (-#1,0) -- (#1,0);
    \draw[rotate = 45] (0,-#1) -- (0, #1);
    }
}

\newcommand{\vitC}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
\node [right=1.25cm of img, draw, fill=white, rounded corners, minimum height=2cm, minimum width=5cm, line width=1mm, align=center] (conv_stack) {\scalebox{2}{\{Conv- \} } \\ \scalebox{2}{} \\ \scalebox{2}{Reshape \& Flatten}};
\node [right=1.25cm of conv_stack, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.9)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node[right=1.00cm of flatten, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.1,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.2,-0.85)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=2cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=2cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=2cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (conv_stack);
        \draw [->, line width=1mm] (conv_stack) -- (flatten);
\draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}


\newcommand{\mvtBlockA}{
    \begin{tikzpicture}
        \node[inner sep=0pt, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (tensor) at (0,0) {\drawCube{2}{2}{1}{white}{1.0}};
\node [above=0.5cm of tensor] (dummY_tensor) {};
        \node [right=0.75cm of tensor, draw, fill=white, rounded corners, text width=2.25cm, minimum height=1.5cm, fill=gray!10, line width=0.5mm, align=center] (conv) {\scalebox{1.5}{Conv-3x3} \\ \scalebox{2}{} \\ \scalebox{1.5}{Conv-1x1}};
\node [right=0.75cm of conv, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (img) {\cubeWithPatches};
\node [right=0.75cm of img, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (flatten) {\cubeWithFlattenPatches};
\node [right=0.75cm of flatten, draw, fill=white, rounded corners, label=above:{}, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (transformer) {\scalebox{1.5}{Transformer}};
\node [right=0.75cm of transformer, label={[shift={(-0.45,-2.35)}]}, label={[shift={(-1.35,-1.75)}]}, label={[shift={(-1.0,-0.75)}] \rotatebox{45}{}}] (trans_out) {\cubeWithFlattenPatches};
\node [right=0.75cm of trans_out, label={[shift={(-0.25,-3.5)}]}, label={[shift={(-1.65,-2.5)}]}, label={[shift={(-1.25,-0.75)}] \rotatebox{45}{}}] (patch_im) {\cubeWithPatches};
\node [right=0.75cm of patch_im, draw, fill=white, rounded corners, minimum height=1.5cm, fill=gray!10, line width=0.5mm] (conv_proj) {\scalebox{1.5}{Conv-1x1}};
        \node [right=0.75cm of conv_proj, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}] (proj_tensor) {\drawCube{2}{2}{1}{white}{1.0}};
\node [right=0.75cm of proj_tensor, draw, fill=white, rounded corners, text width=2.25cm, line width=0.5mm, minimum height=1.5cm, fill=gray!10, align=center] (concat) {\scalebox{1.5}{Concat} \\ \scalebox{1.5}{} \\ \scalebox{1.5}{Conv-3x3}};
\node [right=0.75cm of concat, fill=white, label={[shift={(-0.25,-3.0)}]}, label={[shift={(-1.5,-2.0)}]}, label={[shift={(-1.25,-0.45)}] \rotatebox{45}{}}, label={[shift={(-0.25,-1.75)}]\scalebox{2}{}}] (out) {\drawCube{2}{2}{1}{white}{1.0}};
\draw [->, line width=0.5mm] (tensor) -- (conv);
        \draw [->, line width=0.5mm] (conv) -- (img);
        \draw [->, line width=0.5mm] (img) -- (flatten);
        \draw [->, line width=0.5mm] (flatten) -- (transformer);
        \draw [->, line width=0.5mm] (transformer) -- (trans_out);
        \draw [->, line width=0.5mm] (trans_out) -- (patch_im);
        \draw [->, line width=0.5mm] (patch_im) -- (conv_proj);
        \draw [->, line width=0.5mm] (conv_proj) -- (proj_tensor);
        \draw [->, line width=0.5mm] (proj_tensor) -- (concat);
        \draw [->, line width=0.5mm] (tensor) -- (dummY_tensor.center) -| (concat.north);
        \draw [->, line width=0.5mm] (concat) -- (out);
        


        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(5, -2) -- (12, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Unfold}};

        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(16.5, -2) -- (23.5, -2) node [black,midway, below=8pt] {\scalebox{1.75}{Fold}};
    \end{tikzpicture}
}

\newcommand{\vit}{
    \begin{tikzpicture}
    
        \node[inner sep=0pt] (img) at (0,0)  {\includegraphics[width=75px]{sample.jpg}};
        \begin{scope}[shift={(img.south west)},x={(img.south east)},y={(img.north west)}]
            \draw[help lines, xstep=.2, ystep=.2, color=black, line width=0.75mm] (0,0) grid (1.0, 1.0);
        \end{scope}
        
\node [right=1.25cm of img, label={[shift={(-1,-3.3)}]\scalebox{2}{}}, label={[shift={(-0.25,-1.1)}] \rotatebox{45}{\scalebox{2}{}}}] (flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \draw [decorate,decoration={calligraphic brace,amplitude=10pt,raise=0pt, mirror},yshift=0pt, line width=0.75mm]
(-1, -3) -- (5, -3) node [black,midway, below=8pt] {\scalebox{2}{Flatten image patches}};
        
        \node[right=1.25cm of flatten, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (p_patches) {\scalebox{2}{Linear}};
        
        \node [right=1.25cm of p_patches, label={[shift={(-1.2,-3.5)}]\scalebox{2}{}}, label={[shift={(-0.25,-0.95)}] \rotatebox{45}{\scalebox{2}{}}}] (proj_flatten) {\cubeWithFlattenPatchesViTSmall};
        
        \node [right=0.75 cm of proj_flatten] (add) {\scalebox{6}{}};
        
        \node [below=1.2cm of add, circle, draw, radius=1cm, label=below:{\scalebox{2}{Positional encoding}}, line width=1mm] (pos_encoding) {\tikz \draw[scale=0.1,domain=-3.141:3.141,smooth,variable=\t, line width=1mm] plot (\t,{sin(\t r)});};
        
        \node [right=1.25cm of add, draw, fill=white, rounded corners, label=above:{\scalebox{2}{}}, minimum height=1.5cm, line width=1mm] (transformer) {\scalebox{2}{Transformer}};
        
        \node[right=1.25cm of transformer, draw, fill=white, rounded corners, minimum height=1.5cm, line width=1mm, align=center] (l12) {\scalebox{2}{Linear}};
        
        \node[right=1.25cm of l12, fill=white, rounded corners, minimum height=1.5cm, line width=0.5mm, align=center] (l13) {\scalebox{2}{Logits}};
        
        \draw [->, line width=1mm] (img) -- (flatten);
        \draw [->, line width=1mm] (flatten) -- (p_patches);
        \draw [->, line width=1mm] (p_patches) -- (proj_flatten);
        \draw [->, line width=1mm] (proj_flatten) -- (add);
        \draw [->, line width=1mm] (pos_encoding) -- (add);
        \draw [->, line width=1mm] (add) -- (transformer);
        \draw [->, line width=1mm] (transformer) -- (l12);
        \draw [->, line width=1mm] (l12) -- (l13);
        
    \end{tikzpicture}
}

\newcommand{\stdDDP}{
    \begin{tikzpicture}
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 0, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 0, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-3}}] (tensor_a1) at (0, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 2, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 2, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-2}}] (tensor_a2) at (0, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 4, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (3.1, 4, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node[label={[shift={(-2.8,-2.2)}]\scalebox{2.5}{GPU-1}}] (tensor_a3) at (0, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_b) at (1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (tensor_b) at (2.05, 6, 0) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node (tensor_c) at (3.1, 6, 0) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-1.4, -1.4) -- (4, -1.4);
        \node at (3.5, -1.9)  {\scalebox{2.5}{time}};
        
        \node[above=0.75cm of tensor_a3, align=center] (b){\scalebox{3}{\textcolor{stdSampler}{\bfseries Standard sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\mscDDP}{
    \begin{tikzpicture}
    
        \node[label={[shift={(-3.6,-1.75)}]\scalebox{2.5}{GPU-4}}] (tensor_a) at (0.92, 0, 3) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-2.25)}]\scalebox{2.5}{GPU-3}}] at (-0.08, 2, 2.5) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[label={[shift={(-2.85,-3.0)}]\scalebox{2.5}{GPU-2}}] at (-0.57, 4.01, 1.5) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node[label={[shift={(-3.85,-1.0)}]\scalebox{2.5}{GPU-1}}] at (1.5, 6.1, 4) {\drawCube{4}{1}{2}{red!20}{1.0}};
        
        \node at (3.2-0.5, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (3.2-0.5, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node (marked) at (3.2-0.5, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \node at (5.90-0.95, 0.5, 2.5) {\drawCube{4}{1}{2}{red!20}{1.0}};
        \node at (4.44-0.95, 2.02, 1.6) {\drawCube{1}{2}{4}{red!20}{1.0}};
        \node at (5.92-0.95, 4.01, 3.05) {\drawCube{4}{2}{1}{red!20}{1.0}};
        \node at (5.45-0.95, 6.55, 3.95) {\drawCube{2}{2}{2}{red!20}{1.0}};
        
        \node at (8.2-1.4, -0.2, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 1.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 3.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node at (8.2-1.4, 5.8, 2) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        
        \draw[->, line width=0.5mm] (-2.45, -2.35) -- (7, -2.35);
        
        \node at (6.5, -2.75)  {\scalebox{2.5}{time}};
        \node[above=0.75cm of marked, align=center] (b) {\scalebox{3}{\textcolor{mscSampler}{\bfseries Multi-scale sampler}}};
    
    \end{tikzpicture}
}


\newcommand{\tensorNotations}{
    \begin{tikzpicture}
        \node[label={[shift={(-1.75,-2.5)}]\rotatebox{90}{ \scalebox{1.35}{Height}}}, label={[shift={(-1.2,-0.9)}]\rotatebox{50}{\scalebox{1.35}{Batch}}}, label={[shift={(-0.5,-3.5)}] \scalebox{1.35}{Width}}] (tensor_c) {\drawCube{2}{2}{2}{red!20}{1.0}};
        \node[right=0.1cm of tensor_c] (a) {\scalebox{1.75}{Tensor}};
        
        \node[below=1cm of tensor_c] (tensor_d) {\drawCube{0.1}{2}{2}{yellow!20}{1.0}};
        \node[right=0.1cm of tensor_d] (b) {\scalebox{1.75}{Gradient sync.}};
    \end{tikzpicture}
}



%
 \tensorNotations
        }
        \caption{Standard vs. multi-scale sampler illustration}
        \label{fig:sampler_illustration}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.38\columnwidth}
        \centering
        \resizebox{0.85\columnwidth}{!}{
            \begin{tabular}{lrr}
                \toprule[1.5pt]
                \textbf{Sampler} & \textbf{\# Updates} & \textbf{Epoch time}  \\
                \midrule[1pt]
                \textcolor{stdSampler}{\bfseries Standard} & 375 k & 380 sec \\
                \textcolor{mscSampler}{\bfseries Multi-scale (ours)} & \textbf{232 k} & \textbf{270 sec} \\
                \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{Training efficiency. \small{Here, \textcolor{stdSampler}{\bfseries standard} sampler refers to PyTorch's DistributedDataParallel sampler.}}
        \label{fig:train_eff}
    \end{subfigure}
    \caption{\textbf{Multi-scale vs. standard sampler.}}\label{fig:sampler_blocks}
\end{figure}


Previous CNN-based works \citep[e.g.,][]{redmon2017yolo9000, mehta2021evrnet} have shown that multi-scale training is effective. However, most of these works sample a new spatial resolution after a fixed number of iterations. For example, YOLOv2 \citep{redmon2017yolo9000} samples a new spatial resolution from a pre-defined set at every 10-th iteration and uses the same resolution across different GPUs during training. This leads to GPU under-utilization and slower training because the same batch size (determined using the maximum spatial resolution in the pre-defined set) is used across all resolutions. 
To facilitate \arch~learn multi-scale representations without fine-tuning and to further improve training efficiency (i.e., fewer optimization updates), we extend the multi-scale training method to variably-sized batch sizes. Given a sorted set of spatial resolutions  and a batch size  for a maximum spatial resolution of , we randomly sample a spatial resolution  at -th training iteration on each GPU and  compute the batch size for -th iteration as: . As a result, larger batch sizes are used for smaller spatial resolutions. This reduces optimizer updates per epoch and helps in faster training. 

Figure \ref{fig:sampler_blocks} compares standard and multi-scale samplers. Here, we refer to DistributedDataParallel in PyTorch as the standard sampler. Overall, the multi-scale sampler (i) reduces the training time as it requires fewer optimizer updates with variably-sized batches (Figure \ref{fig:train_eff}), (ii) improves performance by about 0.5\% (Figure \ref{fig:acc_compare_sampler}; \textsection \ref{sec:appendix_msc}), and (iii) forces the network to learn better multi-scale representations (\textsection \ref{sec:appendix_msc}), i.e., the same network when evaluated at different spatial resolutions yields better performance as compared to the one trained with the standard sampler. In \textsection \ref{sec:appendix_msc}, we also show that the multi-scale sampler is generic and improves the performance of CNNs (e.g., MobileNetv2).

\section{Experimental Results}
In this section, we first evaluate \arch s performance on the ImageNet-1k dataset and show that \arch~delivers better performance than state-of-the-art networks (\textsection \ref{ssec:imagenet_results}). In \textsection \ref{sec:general_purpose} and \textsection \ref{sec:mobile_friendly}, we show \arch s are general-purpose and mobile-friendly, respectively.

\subsection{Image Classification on the ImageNet-1k Dataset}
\label{ssec:imagenet_results}

\textbf{Implementation details.} We train \arch~models from \emph{scratch} on the ImageNet-1k classification dataset \citep{russakovsky2015imagenet}. The dataset provides 1.28 million  and 50 thousand images for training and validation, respectively. The \arch~networks are trained using PyTorch for 300 epochs on 8 NVIDIA GPUs with an effective batch size of 1,024 images using AdamW optimizer \citep{loshchilov2017decoupled}, label smoothing cross-entropy loss (smoothing=0.1), and multi-scale sampler (). The learning rate is increased from 0.0002 to 0.002 for the first 3k iterations and then annealed to 0.0002 using a cosine schedule \citep{loshchilov2016sgdr}. We use L2 weight decay of 0.01. We use basic data augmentation (i.e., random resized cropping and horizontal flipping) and evaluate the performance using a single crop top-1 accuracy. For inference, an exponential moving average of model weights is used.
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.42\columnwidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/cnn_comparison.pdf}
        \caption{Comparison with light-weight CNNs}
        \label{fig:compare_imagenet_cnn_graph}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.57\columnwidth}
        \centering
        \begin{subfigure}[b]{\columnwidth}
            \centering
            \resizebox{0.55\columnwidth}{!}{
                \Huge
                \begin{tabular}{lrr}
                    \toprule[1.5pt]
                    \textbf{Model} & \textbf{\# Params.}  & \textbf{Top-1}  \\
                    \midrule[1pt]
                    MobileNetv1 & 2.6 M & 68.4 \\
                    MobileNetv2 & 2.6 M & 69.8 \\
                    MobileNetv3 & 2.5 M & 67.4 \\
                    ShuffleNetv2 & 2.3 M & 69.4 \\
                    ESPNetv2 & 2.3 M & 69.2 \\
                    \arch-XS (Ours) & 2.3 M & \textbf{74.8} \\
                    \bottomrule
                \end{tabular}
            }
            \caption{Comparison with light-weight CNNs (similar parameters)}
            \label{fig:compare_imagenet_small_models_cnn}
        \end{subfigure}
        \vfill
        \begin{subfigure}[b]{\columnwidth}
            \centering
            \resizebox{0.55\columnwidth}{!}{
                \Huge
                \begin{tabular}{lrr}
                    \toprule[1.5pt]
                    \textbf{Model} & \textbf{\# Params.}  & \textbf{Top-1}  \\
                    \midrule[1pt]
                    DenseNet-169 & 14 M & 76.2 \\
                    EfficientNet-B0 & 5.3 M & 76.3 \\
                    ResNet-101 & 44.5 M & 77.4 \\
                    ResNet-101-SE & 49.3 M & 77.6 \\
                    \arch-S (Ours) & 5.6 M & \textbf{78.4} \\
                    \bottomrule
                \end{tabular}
            }
            \caption{Comparison with heavy-weight CNNs}
            \label{fig:compare_imagenet_heavy_cnn}
        \end{subfigure}
    \end{subfigure}
    \caption{\textbf{\arch~vs. CNNs} on ImageNet-1k validation set. All models use basic augmentation.}
    \label{fig:compare_imagenet_cnn}
\end{figure}

\vspace{-4mm}
\textbf{Comparison with CNNs.} Figure \ref{fig:compare_imagenet_cnn_graph} shows that \arch~outperforms \emph{light-weight} CNNs across different network sizes (MobileNetv1 \citep{howard2017mobilenets}, MobileNetv2 \citep{sandler2018mobilenetv2}, ShuffleNetv2 \citep{ma2018shufflenet}, ESPNetv2 \citep{mehta2019espnetv2}, and MobileNetv3 \citep{howard2019searching}). For instance, for a model size of about 2.5 million parameters (Figure \ref{fig:compare_imagenet_small_models_cnn}), \arch~outperforms MobileNetv2 by 5\%, ShuffleNetv2 by 5.4\%, and MobileNetv3 by 7.4\% on the ImageNet-1k validation set. Figure \ref{fig:compare_imagenet_heavy_cnn} further shows that \arch~delivers better performance than  \emph{heavy-weight} CNNs (ResNet \citep{he2016deep}, DenseNet \citep{huang2017densely}, ResNet-SE \citep{hu2018squeeze}, and EfficientNet \citep{tan2019efficientnet}). For instance, \arch~is 2.1\% more accurate than EfficentNet for a similar number of parameters.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.4\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/transformer_comparison.pdf}
        \caption{}
        \label{fig:compare_imagenet_trans_fig}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.55\columnwidth}
        \centering
        \resizebox{0.8\columnwidth}{!}{
            \Huge
            \begin{tabular}{llrrr}
                \toprule[1.5pt]
                \textbf{Row \#} & \textbf{Model} & \textbf{Augmentation} & \textbf{\# Params.}  & \textbf{Top-1}  \\
                \midrule[1pt]
                R1 & DeIT & \textcolor{codegreen}{\bf Basic} & 5.7 M & 68.7 \\
                R2 & T2T & \textcolor{red}{\bf Advanced} & 4.3 M & 71.7 \\
                R3 & DeIT & \textcolor{red}{\bf Advanced} & 5.7 M & 72.2 \\
                R4 & PiT & \textcolor{codegreen}{\bf Basic} & 10.6 M & 72.4 \\
                R5 & Mobile-former & \textcolor{red}{\bf Advanced} & 4.6 M & 72.8 \\
                R6 & PiT & \textcolor{red}{\bf Advanced} & 4.9 M & 73.0 \\
                R7 & CrossViT & \textcolor{red}{\bf Advanced} & 6.9 M & 73.4 \\
                R8 & \arch-XS (Ours) & \textcolor{codegreen}{\bf Basic} & 2.3 M & \textbf{74.8} \\
                \midrule
                R9 & CeiT & \textcolor{red}{\bf Advanced} & 6.4 M & 76.4 \\
                R10 & DeIT & \textcolor{red}{\bf Advanced} & 10 M & 75.9 \\
                R11 & T2T & \textcolor{red}{\bf Advanced} & 6.9 M & 76.5 \\
                R12 & ViL & \textcolor{red}{\bf Advanced} & 6.7 M & 76.7 \\
                R13 & LocalVit & \textcolor{red}{\bf Advanced} & 7.7 M & 76.1 \\
                R14 & Mobile-former & \textcolor{red}{\bf Advanced} & 9.4 M & 76.7 \\
                R15 & PVT & \textcolor{red}{\bf Advanced} & 13.2 M & 75.1 \\
                R16 & ConViT & \textcolor{red}{\bf Advanced} & 10 M & 76.7 \\
                R17 & PiT & \textcolor{red}{\bf Advanced} & 10.6 M & 78.1 \\
                \sachin{R18} & \sachin{BoTNet} & \textcolor{codegreen}{\bf Basic} & \sachin{20.8 M} & \sachin{77.0} \\
                R19 & BoTNet & \textcolor{red}{\bf Advanced} & 20.8 M & 78.3 \\
                R20 & \arch-S (Ours) & \textcolor{codegreen}{\bf Basic} & 5.6 M & \textbf{78.4} \\
                \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{}
        \label{fig:compare_imagenet_trans_table}
    \end{subfigure}
    \caption{\sachin{\textbf{\arch~vs. ViTs} on ImageNet-1k validation set. Here, \textcolor{codegreen}{\bfseries basic} means ResNet-style augmentation while \textcolor{red}{\bfseries advanced} means a combination of augmentation methods with basic (e.g., MixUp \citep{zhang2018mixup}, RandAugmentation \citep{cubuk2019autoaugment}, and CutMix \citep{zhong2020random}).}}
    \label{fig:compare_imagenet_transformers}
\end{figure}

\vspace{-4mm}
\paragraph{Comparison with ViTs.} Figure \ref{fig:compare_imagenet_transformers} compares \arch~with ViT variants that are trained from \emph{scratch} on the ImageNet-1k dataset without distillation  (DeIT \citep{touvron2021training}, T2T \citep{yuan2021tokens}, PVT \citep{wang2021pyramid}, CAIT \citep{touvron2021going}, DeepViT \citep{zhou2021deepvit}, CeiT \citep{yuan2021incorporating}, CrossViT \citep{chen2021crossvit}, LocalViT \citep{li2021localvit}, PiT \citep{heo2021rethinking}, ConViT \citep{d2021convit}, ViL \citep{zhang2021multi}, BoTNet \citep{srinivas2021bottleneck}, and Mobile-former \citep{chen2021mobile}). Unlike ViT variants that benefit significantly from \textcolor{red}{\textbf{advanced}} augmentation (e.g., PiT w/ \textcolor{codegreen}{\textbf{basic}}  vs. \textcolor{red}{\textbf{advanced}}: 72.4 (R4) vs. 78.1 (R17); Figure \ref{fig:compare_imagenet_trans_table}), \arch~achieves better performance with fewer parameters and \textcolor{codegreen}{\textbf{basic}} augmentation. For instance, \arch~is  smaller and  better than DeIT (R3 vs. R8 in Figure \ref{fig:compare_imagenet_trans_table}). 

Overall, these results show that, similar to CNNs, \arch s are easy and robust to optimize. Therefore, they can be easily applied to new tasks and datasets. 

\subsection{\arch~as a General-purpose Backbone}
\label{sec:general_purpose}
To evaluate the general-purpose nature of \arch, we benchmark \arch~on two widely studied mobile vision tasks: (1) object detection (\textsection  \ref{ssec:object_detection}) and (2) semantic segmentation (\textsection  \ref{ssec:semantic_segmentation}). 

\begin{wraptable}[15]{r}{5.25cm}
   \begin{subtable}[b]{\columnwidth}
        \centering
            \resizebox{0.85\columnwidth}{!}{
            \Huge
            \begin{tabular}{lrr}
                \toprule[1.5pt]
                \textbf{Feature backbone} & \textbf{\# Params.}  & \textbf{mAP} \\
                \midrule[1pt]
                MobileNetv3 & 4.9 M & 22.0 \\
                MobileNetv2 & 4.3 M & 22.1 \\
                MobileNetv1 & 5.1 M & 22.2 \\
                MixNet & 4.5 M & 22.3 \\
                MNASNet & 4.9 M & 23.0 \\
                \arch-XS (Ours) & \textbf{2.7 M} & 24.8 \\
                \arch-S (Ours) & 5.7 M & \textbf{27.7} \\
                \bottomrule
            \end{tabular}
        }
            \caption{Comparison w/ light-weight CNNs}
            \label{tab:ms_coco_light_wt}
        \end{subtable}
        \vfill
        \begin{subtable}[b]{\columnwidth}
                \centering
                \resizebox{0.75\columnwidth}{!}{
                \Huge
                \begin{tabular}{lrr}
                    \toprule[1.5pt]
                    \textbf{Feature backbone} & \textbf{\# Params.}  & \textbf{mAP}  \\
                    \midrule[1pt]
                    VGG & 35.6 M & 25.1 \\
                    ResNet50 & 22.9 M & 25.2 \\
                    \arch-S~(Ours) & \textbf{5.7 M} & \textbf{27.7} \\
                    \bottomrule
                \end{tabular}
            }
            \caption{Comparison w/ heavy-weight CNNs}
            \label{tab:ms_coco_heavy_wt}
        \end{subtable}
    \caption{\textbf{Detection w/ SSDLite.}}
    \label{tab:ms_coco_results}
\end{wraptable} 

\vspace{-3mm}
\subsubsection{Mobile Object Detection}
\label{ssec:object_detection}

\textbf{Implementation details.} We integrate \arch~with a single shot object detection backbone \citep[SSD;][]{liu2016ssd}. Following light-weight CNNs (e.g., MobileNets), we replace standard convolutions in the SSD head with separable convolutions and call the resultant network as SSDLite. We finetune \arch, pre-trained on the ImageNet-1k dataset, at an input resolution of  using AdamW on the MS-COCO dataset \citep{lin2014microsoft} that contains 117k training and 5k validation images. We use smooth L1 and cross-entropy losses for object localization and classification, respectively. The performance is evaluated on the validation set using mAP@IoU of 0.50:0.05:0.95. For other hyper-parameters, see \textsection \ref{sec:appendix_training_details}. 

\textbf{Results.} Table \ref{tab:ms_coco_light_wt} shows that, for the same input resolution of , SSDLite with \arch~outperforms SSDLite with other light-weight CNN models (MobileNetv1/v2/v3, MNASNet, and MixNet). For instance, SSDLite's performance improves by 1.8\%, and its model size reduces by  when \arch~is used as a backbone instead of MNASNet. Further, SSDLite with \arch~outperforms standard SSD-300 with heavy-weight backbones while learning significantly fewer parameters (Table \ref{tab:ms_coco_heavy_wt}). \sachin{Also, qualitative results in \textsection \ref{sec:app_od_res} confirms \arch's ability to detect variety of objects.}

\vspace{-3mm}
\subsubsection{Mobile Semantic Segmentation}
\label{ssec:semantic_segmentation}

\begin{wraptable}[8]{r}{6.5cm}
    \centering
    \resizebox{!}{30px}{
        \Huge
        \begin{tabular}{lrr}
            \toprule[1.5pt]
            \textbf{Feature backbone} & \textbf{\# Params.}  & \textbf{mIOU} \\
            \midrule[1pt]
            MobileNetv1 & 11.2 M & 75.3 \\
            MobileNetv2 & 4.5 M & 75.7 \\
            \arch-XXS~(Ours) & 1.9 M & 73.6 \\
            \arch-XS~(Ours) & 2.9 M & \textbf{77.1} \\
            \midrule
            ResNet-101 & 58.2 M & \textbf{80.5} \\
            \arch-S~(Ours) & 6.4 M & 79.1 \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{Segmentation w/ DeepLabv3.}}
    \label{tab:semantic_segmentation_deeplabv3}
\end{wraptable}

\textbf{Implementation details.} We integrate \arch~with DeepLabv3 \citep{chen2017rethinking}. We finetune \arch~using AdamW with cross-entropy loss on the PASCAL VOC 2012 dataset \citep{everingham2015pascal}. Following a standard training practice \citep[e.g.,][]{chen2017rethinking, mehta2019espnetv2}, we also use extra annotations and data from  \citet{hariharan2011semantic} and \citet{lin2014microsoft}, respectively. The performance is evaluated on the validation set using mean intersection over union (mIOU). For other hyper-parameters, see \textsection \ref{sec:appendix_training_details}. 

\textbf{Results.} Table \ref{tab:semantic_segmentation_deeplabv3} shows that DeepLabv3 with \arch~is smaller and better. The performance of DeepLabv3 is improved by 1.4\%, and its size is reduced by  when \arch~is used as a backbone instead of MobileNetv2. Also, \arch~gives competitive performance to model with ResNet-101 while requiring  fewer parameters; suggesting \arch~is a powerful backbone. \sachin{Also, results in \textsection \ref{sec:app_sem_seg} shows that \arch~learns generalizable representations of the objects and perform well on an \emph{unseen} dataset.}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=0.7\columnwidth]{figures/mvt_latency.pdf}
        \caption{Classification @ }
        \label{fig:imagenet_latency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=0.7\columnwidth]{figures/mvt_ssd_latency.pdf}
        \caption{Detection @ }
        \label{fig:ssd_latency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=0.7\columnwidth]{figures/mvt_deeplabv3_latency.pdf}
        \caption{Segmentation @ }
        \label{fig:deeplab_latency}
    \end{subfigure}
    \caption{\textbf{Inference time of \arch~models on different tasks.} Here, dots in green color region represents that these models runs in real-time (inference time  33 ms).}
    \vspace{-2mm}
    \label{fig:latency_graphs}
\end{figure}

\vspace{-2mm}
\subsection{Performance on Mobile Devices}
\label{sec:mobile_friendly}
\vspace{-2mm}

Light-weight and low latency networks are important for enabling mobile vision applications. To demonstrate the effectiveness of \arch~for such applications, pre-trained full-precision \arch~models are converted to CoreML using publicly available \citet{coreml2021}. Their inference time is then measured (average over 100 iterations) on a mobile device, i.e., iPhone 12.

\textbf{Mobile-friendly.} Figure \ref{fig:latency_graphs} shows the inference time of \arch~networks with two patch size settings (Config-A:  and Config-B: ) on three different tasks. Here  in Config-X denotes the height  (width ) of a patch at an output stride\footnote{Output stride: Ratio of the spatial dimension of the input to the feature map.} of 8, 16, and 32, respectively. The models with smaller patch sizes (Config-A) are more accurate as compared to larger patches (Config-B). This is because, unlike Config-A models, Config-B models are not able to encode the information from all pixels (Figure \ref{fig:ablation_rep_learning} and \textsection \ref{sec:appendix_ablations}). On the other hand, for a given parameter budget, Config-B models are faster than Config-A even though the theoretical complexity of self-attention in both configurations is the same, i.e., . With larger patch sizes (e.g., ==), we have fewer number of patches  as compared to smaller patch sizes (e.g., ==). As a result, the computation cost of self-attention is relatively less. Also, Config-B models offer a higher degree of parallelism as compared to Config-A because self-attention can be computed simultaneously for more pixels in a larger patch (=) as compared to a smaller patch (=). Hence, Config-B models are faster than Config-A. To further improve \arch's latency, linear self-attention \citep{wang2020linformer} can be used. Regardless, all models in both configurations run in real-time (inference speed  30 FPS) on a mobile device except for \arch-S models for the segmentation task. This is expected as these models process larger inputs () as compared to classification () and detection () networks.

\begin{wraptable}[8]{r}{5.5cm}
    \centering
    \resizebox{\columnwidth}{!}{
        \Huge
        \begin{tabular}{lccrc}
        \toprule[1.5pt]
        \textbf{Model} & \textbf{\# Params.}  & \textbf{FLOPs}  & \textbf{Time}  & \textbf{Top-1}   \\
        \midrule
         MobileNetv2 & 3.5 M & \textbf{\sachin{0.3 G}}  & \textbf{0.92 ms} & 73.3  \\
         DeIT & 5.7 M & \sachin{1.3 G}  &  10.99 ms & 72.2  \\
         PiT & 4.9 M & \sachin{0.7 G} & 10.56 ms & 73.0  \\
         \arch~(Ours) & \textbf{2.3 M} & \sachin{0.7 G} & 7.28 ms & \textbf{74.8} \\
         \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{ViTs are slower than CNNs}. Results with multi-scale sampler (\textsection \ref{sec:appendix_msc}).}}
    \label{tab:compare_latency}
\end{wraptable}
\textbf{Discussion.} We observe that \arch~and other ViT-based networks (e.g., DeIT and PiT) are slower as compared to MobileNetv2 on mobile devices (Table \ref{tab:compare_latency}). This observation contradicts previous works which show that ViTs are more scalable as compared to CNNs \citep{dosovitskiy2020image}. This difference is primarily because of two reasons. First, dedicated CUDA kernels exist for transformers on GPUs, which are used out-of-the-box in ViTs to improve their scalability and efficiency on GPUs \citep[e.g.,][]{shoeybi2019megatron, lepikhin2020gshard}. Second, CNNs benefit from several device-level optimizations, including batch normalization fusion with convolutional layers \citep{jacob2018quantization}. These optimizations improve latency and memory access. However, such dedicated and optimized operations for transformers are currently not available for mobile devices. Hence, the resultant inference graph of \arch~and ViT-based networks for mobile devices is sub-optimal. We believe that similar to CNNs, the inference speed of \arch~and ViTs will further improve with dedicated device-level operations in the future.

\clearpage

\section{Acknowledgements}
We are grateful to Ali Farhadi, Peter Zatloukal, Oncel Tuzel, Ashish Shrivastava, Frank Sun, Max Horton, Anurag Ranjan, and anonymous reviewers for their helpful comments. We are also thankful to Apple's infrastructure and open-source teams for their help with training infrastructure and open-source release of the code and pre-trained models.

\bibliography{main}
\bibliographystyle{iclr2022_conference}

\clearpage

\appendix

\section{\arch~Architecture}
\label{sec:ablation_arch}

\arch's are inspired by the philosophy of light-weight CNNs and the overall architecture of \arch~at different parameter budgets is given in Table \ref{tab:app_arch}.  The initial layer in \arch~is a strided  standard convolution, followed by MobileNetv2 (or MV2) blocks and \arch~blocks. We use Swish \citep{elfwing2018sigmoid} as an activation function. Following CNN models, we use  in the \arch~block. The spatial dimensions of feature maps are usually multiples of 2 and . Therefore, we set  at all spatial levels. The MV2 blocks in \arch~network are mainly responsible for down-sampling. Therefore, in these blocks, we use an expansion factor of four, except for \arch-XXS where we use an expansion factor of 2. The transformer layer in \arch~takes a -dimensional input, as shown in Figure \ref{fig:mvt_arch}. We set the output dimension of the first feed-forward layer in a transformer layer as  instead of , a default value in the standard transformer block of \citet{vaswani2017attention}.

\begin{table}[b!]
    \centering
    \resizebox{0.95\columnwidth}{!}{
        \begin{tabular}{lccclll}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Layer}} & \multirow{2}{*}{\textbf{Output size}} & \multirow{2}{*}{\textbf{Output stride}} & \multirow{2}{*}{\textbf{Repeat}} & \multicolumn{3}{c}{\textbf{Output channels}} \\
            \cmidrule[1.25pt]{5-7}
            & & & & XXS & XS & S \\
            \midrule[1pt]
            Image &  & 1 & \multicolumn{4}{c}{} \\
            \midrule
            Conv-,  & \multirow{2}{*}{} & \multirow{2}{*}{2} & 1 & 16 & 16 & 16 \\
            MV2 & & & 1 & 16 & 32 & 32 \\
            \midrule
            MV2,  & \multirow{2}{*}{} & \multirow{2}{*}{4} & 1 & 24 & 48 & 64 \\
            MV2 & &  & 2 & 24 & 48 & 64 \\
            \midrule
            MV2,  & \multirow{2}{*}{} & \multirow{2}{*}{8} & 1 & 48 & 64 & 96 \\
            \arch~block () & &  & 1 & 48 () & 64 () & 96 () \\
            \midrule
            MV2,  & \multirow{2}{*}{} & \multirow{2}{*}{16} & 1 & 64 & 80 & 128 \\
            \arch~block () & &  & 1 & 64 () & 80 () & 128 () \\
            \midrule
            MV2,  & \multirow{3}{*}{} & \multirow{3}{*}{32} & 1 & 80 & 96 & 160 \\
            \arch~block () & &  & 1 & 80 () & 96 () & 160 () \\
            Conv- & & & 1 & 320 & 384 & 640 \\
            \midrule
            Global pool & \multirow{2}{*}{} & \multirow{2}{*}{256} & \multirow{2}{*}{1} & \multicolumn{3}{c}{} \\
            Linear &  &  &  & 1000 & 1000 & 1000 \\
            \midrule
            \midrule
            \multicolumn{4}{l}{\bfseries Network Parameters} & 1.3 M & 2.3 M & 5.6 M \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{\arch~architecture.} Here,  represents dimensionality of the input to the transformer layer in \arch~block (Figure \ref{fig:mvt_arch}). By default, in \arch~block, we set kernel size  as three and spatial dimensions of patch (height  and width ) in \arch~block as two.}
    \label{tab:app_arch}
\end{table}


\section{Multi-scale sampler}
\label{sec:appendix_msc}

\textbf{Multi-scale sampler reduces generalization gap.} Generalization capability refers to the  gap  between  training  and  evaluation  metrics.   For  two  models  with  similar  training  metrics,the model with better evaluation metrics is more generalizable because it can predict better on an unseen dataset. Figure \ref{fig:train_error_sampler} and Figure \ref{fig:val_error_sampler} compares the training and validation error of the \arch-S model trained with standard and multi-scale samplers. The training error of \arch-S with multi-scale sampler is higher than standard sampler while validation error is lower. Also, the gap between training error and validation error of \arch-S with multi-scale sampler is close to zero. This suggests that a multi-scale sampler improves generalization capability. Also, when \arch-S trained independently with standard and multi-scale sampler is evaluated at different input resolutions (Figure \ref{fig:inp_res_impact}), we observe that \arch-S trained with multi-scale sampler is more robust as compared to the one trained with the standard sampler. We also observe that multi-scale sampler improves the performance of \arch~models at different model sizes by about 0.5\% (Figure \ref{fig:acc_compare_sampler}). These observations in conjunction with impact on training efficiency (Figure \ref{fig:train_eff}) suggests that a multi-scale sampler is effective. Pytorch implementation of multi-scale sampler is provided in Listing \ref{list:mscSamplerCode}.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[height=85px]{figures/std_vs_msc_training.pdf}
        \caption{Training error}
        \label{fig:train_error_sampler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[height=85px]{figures/std_vs_msc_validation.pdf}
        \caption{Validation error}
        \label{fig:val_error_sampler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.38\columnwidth}
        \centering
        \includegraphics[height=85px]{figures/inp_res_impact.pdf}
        \caption{Validation acc. vs. input resolution}
        \label{fig:inp_res_impact}
    \end{subfigure}
    \caption{\textbf{\arch-S learns better representations with multi-scale sampler on ImageNet-1k.}}
    \label{fig:app_sampler_results}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=90px]{figures/std_vs_msc_top1_acc.pdf}
    \caption{\textbf{\arch's performance on ImageNet-1k with standard and multi-scale sampler.}}
    \label{fig:acc_compare_sampler}
\end{figure}

\textbf{Multi-scale sampler is generic.} We train a heavy-weight (ResNet-50) and a light-weight (MobileNetv2-1.0) CNN with the multi-scale sampler to demonstrate its generic nature. Results in Table \ref{tab:msc_sampler_generic} show that a multi-scale sampler improves the performance as well as training efficiency. For instance, a multi-scale sampler improves the performance of MobileNetv2-1.0 by about 1.4\% while decreasing the training time by 14\%.

\begin{table}[t!]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lrrrcrlcrr}
        \toprule[1.5pt]
        \textbf{Model} & \textbf{\# Params} & \textbf{\# Epochs} & \textbf{\# Updates}  && \multicolumn{2}{c}{\textbf{Top-1 accuracy} } && \multicolumn{2}{c}{\textbf{Training time} } \\
        \midrule[1pt]
         ResNet-50 w/ standard sampler (PyTorch) & 25 M & -- & -- && 76.2 & (0.0\%) && -- & -- \\
         ResNet-50 w/ standard sampler (our repro.) & 25 M & 150 & 187 k && 77.1 & (+0.9\%) && 54 k sec. & (1.35) \\
         ResNet-50 w/ multi-scale sampler (Ours) & 25 M & 150 & 116 k && \textbf{78.6} & (+2.4\%) && 40 k sec. & (1) \\
         \midrule[1.25pt]
         MobileNetv2-1.0 w/ standard sampler (PyTorch) & 3.5 M & -- & --  && 71.9 & (0.0\%) && -- & (--) \\
         MobileNetv2-1.0 w/ standard sampler (our repro.) & 3.5 M & 300 & 375 k && 72.1 & (+0.2\%) && 78 k sec. & (1.16)\\
         MobileNetv2-1.0 w/ multi-scale sampler (Ours) & 3.5 M & 300 & 232 k && \textbf{73.3} & (+1.4\%) && 67 k sec. & (1) \\
         \bottomrule[1.5pt]
    \end{tabular}
    }
    \caption{\textbf{Multi-scale sampler is generic.} All models are trained with basic augmentation on the ImageNet-1k. Results are with exponential moving average.}
    \label{tab:msc_sampler_generic}
\end{table}

\section{Ablations}
\label{sec:appendix_ablations}

\textbf{Impact of weight decay.} A good model should be insensitive or less sensitive to L2 regularization (or weight decay) because tuning it for each task and dataset is time- and resource-consuming. Unlike CNNs, ViT models are sensitive to weight decay \citep{dosovitskiy2020image, touvron2021training, xiao2021early}. To study if \arch~models are sensitive to weight decay or not, we train the \arch-S model by varying the value of weight decay from  to .  Results are shown in Figure \ref{fig:wt_decay}. With an exception to the \arch~model trained with a weight decay of , all other models converged to a similar solution. This shows that \arch~models are robust to weight decay. In our experiments, we use the value of weight decay as 0.01. Note that  is the widely used value of weight decay in most CNN-based models, such as ResNet and DenseNet. Even at this value of weight decay, \arch~outperforms CNNs on the ImageNet-1k dataset (e.g., DenseNet vs. \arch: 76.2 with 14 M parameters vs. 77.4 with 5.7 M parameters).

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth} 
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_l2reg_training.pdf}
        \caption{Training error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_l2reg_validation.pdf}
        \caption{Validation error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \resizebox{0.75\columnwidth}{!}{
        \begin{tabular}{lr}
        \toprule[1.5pt]
          \textbf{Weight decay} & \textbf{Top-1}  \\
          \midrule
            0.1 & 76.5 \\0.01 & \textbf{78.4} \\0.001 & 77.6 \\0.0001 & 77.4 \\\bottomrule[1.5pt]
        \end{tabular}
        }
        \caption{Validation accuracy}
    \end{subfigure}
    \caption{\textbf{Impact of weight decay}. Here, results are shown for \arch-S model (5.7 M parameters) on the ImageNet-1k dataset. Results in \textbf{(c)} are with exponential moving average.}
    \label{fig:wt_decay}
\end{figure}

\textbf{Impact of skip-connection.} Figure \ref{fig:skip_conn} studies the impact of skip-connection in the \arch~block (\textcolor{red}{\bf red} arrow in Figure \ref{fig:mvt_arch}). With this connection, the performance of \arch-S improves by 0.5\% on the ImageNet dataset. Note that even without this skip-connection, \arch-S delivers similar or better performance than state-of-the-art CNN- (Figure \ref{fig:compare_imagenet_cnn}) and ViT-based (Figure \ref{fig:compare_imagenet_trans_table}) models, that too with basic data augmentation.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth} 
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_skip_training.pdf}
        \caption{Training error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/mvt_skip_validation.pdf}
        \caption{Validation error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \resizebox{0.75\columnwidth}{!}{
        \begin{tabular}{cr}
        \toprule[1.5pt]
          \textbf{Skip-connection} & \textbf{Top-1}  \\
          \midrule
            \xmark & 77.9 \\
            \cmark & \textbf{78.4} \\
        \bottomrule[1.5pt]
        \end{tabular}
        }
        \caption{Validation accuracy}
    \end{subfigure}
    \caption{\textbf{Impact of skip connection}. Here, results are shown for \arch-S model (5.7 M parameters) on the ImageNet-1k dataset. Results in \textbf{(c)} are with exponential moving average.}
    \label{fig:skip_conn}
\end{figure}

\textbf{Impact of patch sizes.} \arch~combines convolutions and transformers to learn local and global representations effectively. Because convolutions are applied on  regions and self-attention is computed over patches with spatial dimensions of  and , it is essential to establish a good relationship between , , and . Following previous works on CNN designs, we set  and then vary  and . Specifically, we study four configurations: (i)  at all spatial levels (Figure \ref{fig:ablation_rep_learning_a}). In this case,  and would allow each pixel to encode information from every other pixel using \arch. (ii)  at all spatial levels (Figure \ref{fig:ablation_rep_learning_b}). In this case, . Similar to (i), this configuration would also allow each pixel to encode information from every other pixel using \arch. (iii)  at all spatial levels (Figure \ref{fig:ablation_rep_learning_c}). In this case,  and would not allow each pixel to aggregate information from other pixels in the tensor. (iv) , , and  at spatial level of , , and , respectively. Unlike (i), (ii), and (iii), the number of patches  is the same across different spatial resolutions in (iv). Also,  only for a spatial level of  where . Note that all these models have the same number of network parameters and the same computational cost of self-attention, i.e., . Here,  is the number of patches,  is the number of pixels in a patch with height  and width , and  is the model dimension. 

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[height=95px]{figures/illustration_1a.pdf}
        \caption{}
        \label{fig:ablation_rep_learning_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[height=95px]{figures/fig_each_pixel.pdf}
        \caption{}
        \label{fig:ablation_rep_learning_b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[height=95px]{figures/illustration_2b.pdf}
        \caption{}
        \label{fig:ablation_rep_learning_c}
    \end{subfigure}
    \caption{\textbf{Relationship between kernel size () for convolutions and patch size () for folding and unfolding in \arch.} In \textbf{a} and \textbf{b}, the \textcolor{centerPix}{\bfseries red} pixel is able to aggregate information from all pixels using local (\textcolor{cyan}{\bfseries cyan} colored arrows) and global (\textcolor{gArrow}{\bfseries orange} colored arrows) information while in \textbf{(c)}, every pixel is not able to aggregate local information using convolutions with kernel size of   from  patch region. Here, each cell in \textcolor{black}{\bf black} and \textcolor{gray}{\bf gray} grids represents a patch and pixel, respectively.}
    \label{fig:ablation_rep_learning}
\end{figure}

\begin{table}[t!]
    \centering
        \resizebox{0.4\columnwidth}{!}{
        \begin{tabular}{crrr}
        \toprule[1.5pt]
          \textbf{Patch sizes} & \textbf{\# Params.} & \textbf{Time}  & \textbf{Top-1}  \\
          \midrule
            2,2,2 & 5.7 M & 9.85 ms & 78.4 \\
            3,3,3 & 5.7 M & 14.69 ms & \textbf{78.5} \\
            4,4,4 & 5.7 M & 8.23 ms & 77.6 \\
            8,4,2 & 5.7 M & 8.20 ms & 77.3 \\
        \bottomrule[1.5pt]
        \end{tabular}
        }
    \caption{\textbf{Impact of patch sizes}. Here, the patch sizes are for spatial levels at , , and , respectively.  Also, results are shown for \arch-S model on the ImageNet-1k dataset. Results are with exponential moving average.  Spatial dimensions of feature map are not multiple of patch dimensions. Therefore, we use bilinear interpolation in folding and unfolding operations to resize the feature map.}
    \label{fig:patch_sizes}
\end{table}

Results are shown in Table \ref{fig:patch_sizes}. We can see that when , \arch~can aggregate information more effectively, which helps improve performance. In our experiments, we used  instead of  because spatial dimensions of feature maps are multiples of 2, and using  requires additional operations. For folding and unfolding, we need to either pad or resize. In the case of padding, we need to mask the padded pixels in self-attention in transformers. These additional operations result in latency, as shown in Table \ref{fig:patch_sizes}. To avoid these extra operations, we choose  in our experiments, which also provides a good trade-off between latency and accuracy. 

\sachin{\textbf{Impact of exponential moving average and label smoothing.} Exponential moving average (EMA) and label smoothing (LS) are two standard training methods that are used to improve CNN- and Transformer-based models performance \citep{sandler2018mobilenetv2, howard2019searching, tan2019mnasnet, touvron2021training, dai2021coatnet, xiao2021early}. Table \ref{tab:app_ls_ema} shows that LS marginally improves the performance of \arch-S while EMA has little or no effect on model's performance on the ImageNet-1k dataset. Because previous works have shown these methods to be effective in reducing stochastic noise and prevent network from becoming over-confident, we use these methods to train \arch~models.} 

\begin{table}[t!]
    \centering
    \resizebox{0.25\columnwidth}{!}{
        \begin{tabular}{ccc}
            \toprule[1.5pt]
            \textbf{LS} & \textbf{EMA} & \textbf{Top-1}  \\
            \midrule[1pt]
            \xmark & \xmark &  78.0 \\
            \cmark & \xmark & 78.3 \\
            \cmark & \cmark & \textbf{78.4} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{Effect of label smoothing (LS) and exponential moving average (EMA) on the performance of \arch-S on the ImageNet-1k dataset.} First row results are with cross-entropy.}}
    \label{tab:app_ls_ema}
\end{table}

\section{Training Details for SSDLite and DeepLabv3}
\label{sec:appendix_training_details}

All SSDLite-\arch~and DeepLabv3-\arch~networks are trained for 200 and 50 epochs with a standard sampler on 4 NVIDIA GPUs and with an effective batch size of 128 images, respectively. The learning rate is increased from 0.00009 to 0.0009 in the first 500 iterations and then annealed to 0.00009 using a cosine learning rate scheduler. We use L2 weight decay of 0.01. We change the stride of MV2 block from two to one at an output stride of 32 in Table \ref{tab:app_arch} to obtain DeepLabv3-\arch~ models at an output stride of 16.

For these models, we do not use a multi-scale sampler. This is because these task-specific networks are resolution-dependent. For example, DeepLabv3 uses an atrous (or dilation) rate of 6, 12, and 18 at an output stride of 16 to learn multi-scale representations. If we use a lower resolution (say ) than , then the atrous kernel weights will be applied to padded zeros; making multi-scale learning ineffective.

\section{Extended Discussion}
\label{sec:app_ext_discuss}

\paragraph{Memory footprint.} \sachin{A light-weight network running on mobile devices should be memory efficient. Similar to MobileNetv2, we measure the memory that needs to be materialized at each spatial level (Table \ref{tab:app_mem_foot}). At lower spatial levels (i.e., an output stride of 8, 16, and 32) where MobileViT blocks are employed, required memory is lesser or comparable to light-weight CNNs. Therefore, similar to light-weight CNNs, \arch~networks are also memory efficient.}

\begin{table}[b!]
    \centering
    \resizebox{0.35\columnwidth}{!}{
        \begin{tabular}{ccc}
        \toprule[1.5pt]
          \textbf{OS} & \textbf{MobileNetv2-1.0} & \textbf{\arch-XS} \\
        \midrule[1.25pt]
            2 & 400 & 784 \\
            4 &  200 & 294 \\
            8 &  100 & 98 \\
            16 &  62 & 31 \\
            32 &  32 & 37 \\
        \midrule
        \textbf{Top-1} & 73.3 & 74.8 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{Comparison between MobileNetv2 and \arch~ in terms of maximum memory (in kb) that needs to be materialized at each spatial resolution in the network.} The top-1 accuracy is measured on the ImageNet-1k validation set. Here, OS (output stride) is the ratio of spatial dimensions of the input to the feature map.}}
    \label{tab:app_mem_foot}
\end{table}

\paragraph{FLOPs.} \sachin{Floating point operations (FLOPs) is another metric that is widely used to measure the efficiency of a neural network. Table \ref{tab:app_vit_flops} compare FLOPs of \arch~with different ViT-based networks on the ImageNet-1k dataset. For similar number of FLOPs, \arch~is faster, smaller, and better. For instance, PiT and \arch~has the same number of FLOPs, but \arch~is  faster,  smaller, and  better (R2 vs. R4 in Table \ref{tab:app_vit_flops}). It is important to note that FLOPs for networks in R2-R4 are the same, but their latency and performance are different. This shows that FLOPs is not a sufficient metric for network efficiency as it does not account for inference-related factors such as memory access, degree of parallelism, and platform characteristics.}

\begin{table}[b!]
    \centering
    \resizebox{0.6\columnwidth}{!}{
        \begin{tabular}{lccrc}
        \toprule[1.5pt]
        \textbf{Model} & \textbf{\# Params.}  & \textbf{FLOPs}  & \textbf{Time}  & \textbf{Top-1}   \\
        \midrule
         (R1) DeIT & 5.7 M & 1.3 G  &  10.99 ms & 72.2  \\
         (R2) PiT & 4.9 M & \textbf{0.7 G} & 10.56 ms & 73.0  \\
         (R3) \arch-XS (Ours; 8,4,2) & \textbf{2.3 M} & \textbf{0.7 G} & \textbf{5.93 ms} & 73.8 \\
         (R4) \arch-XS (Ours; 2,2,2) & \textbf{2.3 M} & \textbf{0.7 G} & 7.28 ms & \textbf{74.8} \\
         \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{Comparison of different ViT-based networks}. The performance of \arch-XS model is reported at two different patch-size settings. See \textsection \ref{sec:ablation_arch} for details.}}
    \label{tab:app_vit_flops}
\end{table}

\sachin{The ImageNet-1k pre-training helps in performance improvement in down-stream tasks such as object detection and semantic segmentation \citep{long2015fully, chen2017rethinking, redmon2017yolo9000}. Because such tasks are used in real-world applications and often uses higher image inputs as compared to the ImageNet-1k classification task, it is important to compare the FLOPs of a network on down-stream tasks. Towards this end, we compare the FLOPs of \arch~with MobileNetv2 on three tasks, i.e., classification, detection, and segmentation. Results are shown in Table \ref{tab:app_mb2_mvit}. We can observe that (1) the gap between MobileNetv2 and \arch~FLOPs reduces as the input resolution increases. For instance, MobileNetv2 has  fewer FLOPs as compared to \arch on the ImageNet-1k classification task, but on the semantic segmentation, they have similar FLOPs (Table \ref{tab:imagenet_flops_app} vs. Table \ref{tab:seg_flops_app}) and (2) MobileNetv2 models are significantly faster but less accurate than \arch~models across different tasks. The low-latency of MobileNetv2 models is likely because of dedicated and optimized hardware-level operations on iPhone. We believe that (1) the inference speed of \arch~will further improve with such dedicated operations and (2) our results will inspire future research in the area of hardware design and optimization.}

\begin{table}[t!]
    \centering
    \begin{subtable}[b]{0.48\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{
            \begin{tabular}{lccrc}
            \toprule[1.5pt]
            \textbf{Model} & \textbf{\# Params.}  & \textbf{FLOPs}  & \textbf{Time}  & \textbf{Top-1}   \\
            \midrule
             MobileNetv2 & 3.5 M & \textbf{0.3 G}  &  \textbf{0.92 ms} & 73.3  \\
             \arch-XS (Ours; 8,4,2) & \textbf{2.3 M} & 0.7 G & 5.93 ms & 73.8 \\
             \arch-XS (Ours; 2,2,2) & \textbf{2.3 M} & 0.7 G & 7.28 ms & \textbf{74.8} \\
             \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{ImageNet-1k classification}
        \label{tab:imagenet_flops_app}
    \end{subtable}
    \hfill
    \begin{subtable}[b]{0.48\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{
            \begin{tabular}{lccrc}
            \toprule[1.5pt]
            \textbf{Backbone} & \textbf{\# Params.}  & \textbf{FLOPs}  & \textbf{Time}  & \textbf{mAP}   \\
            \midrule
             MobileNetv2 & 4.3 M & \textbf{0.8 G}  &  \textbf{2.3 ms} & 22.1  \\
             \arch-XS (Ours; 8,4,2) & \textbf{2.7 M} & 1.6 G & 10.7 ms & 23.1 \\
             \arch-XS (Ours;2,2,2) & \textbf{2.7 M} & 1.6 G & 12.6 ms & \textbf{24.8} \\
             \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{Object detection w/ SSDLite.}
        \label{tab:coco_flops_app}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{\columnwidth}
        \centering
        \resizebox{0.48\columnwidth}{!}{
            \begin{tabular}{lccrc}
            \toprule[1.5pt]
            \textbf{Backbone} & \textbf{\# Params.}  & \textbf{FLOPs}  & \textbf{Time}  & \textbf{mIOU}   \\
            \midrule
             MobileNetv2 & 4.3 M & 5.8 G  &  \textbf{6.5 ms} & 75.7  \\
             \arch-XS (Ours) & \textbf{2.9 M} & \textbf{5.7 G} & 25.1 ms & 75.4 \\
             \arch-XS (Ours) & \textbf{2.9 M} & \textbf{5.7 G} & 32.3 ms & \textbf{77.1} \\
             \bottomrule[1.5pt]
            \end{tabular}
        }
        \caption{Semantic segmentation w/ DeepLabv3.}
        \label{tab:seg_flops_app}
    \end{subtable}
    \caption{\sachin{\textbf{\arch~vs. MobileNetv2 on different tasks}. The FLOPs and inference time in (a), (b) and (c) are measured at , , and  respectively with an exception to \arch-XS model in (a) which uses  as an input resolution for measuring inference time on iPhone 12 neural engine. Here, the performance of \arch-XS models is reported at two different patch-size settings. See \textsection \ref{sec:ablation_arch} for details.}}
    \label{tab:app_mb2_mvit}
\end{table}

\paragraph{Inference time on different devices.} \sachin{Table \ref{tab:app_inference_diff_device} compares the inference time of different models on three different devices, i.e., iPhone12 CPU, iPhone12 neural engine, and NVIDIA V100 GPU. MobileNetv2 is the fastest network across all devices. On iPhone (both CPU and neural engine), \arch~delivers better performance as compared to DeIT and PiT. However, on GPU, DeIT and PiT are faster than \arch. This is likely because \arch~models (1) are shallow and narrow, (2) run at higher spatial resolution ( instead of ), and (2) did not use GPU- accelerated operations for folding and unfolding as they are not supported on mobile devices. However, when we replaced our \emph{unoptimized} fold and unfold operations with PyTorchs Unfold and Fold operations, the latency of \arch~model is improved from 0.62 ms to 0.47 ms.}

\sachin{Overall, our findings suggest that they are opportunities for optimizing ViT-based models, including MobileViT, for different accelerators. We believe that our work will inspire future research in building more efficient networks.}

\begin{table}[t!]
    \centering
    \resizebox{0.9\columnwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\# Params} } & \multirow{2}{*}{\textbf{FLOPs} } & \multirow{2}{*}{\textbf{Top-1} } & \multicolumn{3}{c}{\textbf{Inference time}  }\\
            \cmidrule[1pt]{5-7}
            & & & & \textbf{iPhone12 - CPU} & \textbf{iPhone12 - Neural Engine} & \textbf{NVIDIA V100 GPU} \\
            \midrule[1.25pt]
            MobileNetv2 & 3.5 M & \textbf{0.3 G} & 73.3 & \textbf{7.50 ms} & \textbf{0.92 ms} & \textbf{0.31 ms} \\
            DeIT & 5.7 M & 1.3 G & 72.2 & 28.15 ms & 10.99 ms & 0.43 ms \\
            PiT & 4.9 M & 0.7 G & 73.0 & 24.03 ms & 10.56 ms & 0.46 ms \\
            \arch~(Ours) & \textbf{2.3 M} & 0.7 G & \textbf{74.8} & 17.86 ms & 7.28 ms & 0.62 ms/0.47 ms \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{Inference time on different devices.} The run time of \arch~is measured at  while for other networks, it is measured at . For GPU, inference time is measured for a batch of 32 images while for other devices, we use a batch size of one. Here,  represents that \arch~model uses PyTorch's Unfold and Fold operations. Also, patch sizes for \arch~model at an output stride of 8, 16, and 32 are set to two.}}
    \label{tab:app_inference_diff_device}
\end{table}

\section{Qualitative Results on the Task of Object Detection}
\label{sec:app_od_res}
\sachin{Figures \ref{fig:app_od_res_a}, \ref{fig:app_od_res_b}, and \ref{fig:app_od_res_c} shows that SSDLite with \arch-S can detect different objects under different settings, including changes in illumination and viewpoint, different backgrounds, and non-rigid deformations.}

\begin{figure}[b!]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.49\columnwidth]{visualizations/detect/000000017029.jpg} & 
        \includegraphics[width=0.49\columnwidth]{visualizations/detect/000000030504.jpg} \\
        \includegraphics[width=0.5\columnwidth]{visualizations/detect/000000059598.jpg} & 
        \includegraphics[width=0.45\columnwidth]{visualizations/detect/000000060855.jpg} \\
        \includegraphics[width=0.5\columnwidth]{visualizations/detect/000000061658.jpg} & 
        \includegraphics[width=0.45\columnwidth]{visualizations/detect/000000066926.jpg} \\
    \end{tabular}
    \caption{\textbf{Object detection results} of SSDLite-\arch-S on the MS-COCO validation set.}
    \label{fig:app_od_res_b}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.495\columnwidth]{visualizations/detect/000000002006.jpg}  & \includegraphics[width=0.495\columnwidth]{visualizations/detect/000000003845.jpg} \\
        \includegraphics[width=0.4\columnwidth]{visualizations/detect/000000007977.jpg} & \includegraphics[width=0.4\columnwidth]{visualizations/detect/000000002431.jpg} \\
        \includegraphics[width=0.4\columnwidth]{visualizations/detect/000000012576.jpg} & \includegraphics[width=0.4\columnwidth]{visualizations/detect/000000012639.jpg}
    \end{tabular}
    \caption{\textbf{Object detection results} of SSDLite-\arch-S on the MS-COCO validation set.}
    \label{fig:app_od_res_a}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.45\columnwidth]{visualizations/detect/000000134112.jpg} & 
        \includegraphics[width=0.45\columnwidth]{visualizations/detect/000000128654.jpg} \\
        \includegraphics[width=0.5\columnwidth]{visualizations/detect/000000151629.jpg} & 
        \includegraphics[width=0.45\columnwidth]{visualizations/detect/000000074733.jpg} \\
        \includegraphics[width=0.6\columnwidth]{visualizations/detect/000000105264.jpg} & 
        \includegraphics[width=0.384\columnwidth]{visualizations/detect/000000094871.jpg} \\
    \end{tabular}
    \caption{\textbf{Object detection results} of SSDLite-\arch-S on the MS-COCO validation set.}
    \label{fig:app_od_res_c}
\end{figure}

\clearpage

\section{Semantic Segmentation Results on an Unseen Dataset}
\label{sec:app_sem_seg}

\sachin{To demonstrate that \arch~learns good generalizable representations of objects, we evaluate the DeepLabv3-\arch~model in Section \ref{ssec:semantic_segmentation} on the MS-COCO validation set that contains 5k images. Following official torchvision segmentation models \citep{pytorch2021Seg}, object classes in the MS-COCO dataset are mapped to the object classes in the PASCAL VOC dataset and models are evaluated in terms of mIOU. Note that the MS-COCO validation set is an \emph{unseen} test set for DeepLabv3-\arch~models because these images are neither part of the training nor the validation set used for training DeepLabv3-\arch~models.}

\sachin{Table \ref{tab:app_seg_res} compares the performance of DeepLabv3-\arch~models with MobileNetv3-Large that was trained with three different segmentation backbones (LR-ASPP \citep{howard2019searching}, DeepLabv3, and FCN \citep{long2015fully}). For the same segmentation model, i.e., DeepLabv3, \arch~is a more effective backbone than MobileNetv3. DeepLabv3-\arch-S model is  smaller and 5.1\% more accurate than DeepLabv3-MobileNetv3-Large model. Furthermore, qualitative results in Figure \ref{fig:app_seg_res_a} and Figure \ref{fig:app_seg_res_b} further demonstrates that \arch~learns good generalizable representations of the objects and perform well in the wild.}

\begin{table}[h!]
    \centering
    \resizebox{0.5\columnwidth}{!}{
        \begin{tabular}{lcc}
            \toprule[1.5pt]
            \textbf{Model} & \textbf{\# Params}  & \textbf{mIOU}  \\
            \midrule[1pt]
            LR-ASPP w/ MobileNetV3-Large & 3.2 M &  57.9 \\
            FCN w/ MobileNetV3-Large & 5.1 M & 57.8 \\
            \midrule
            DeepLabv3 w/ MobileNetV3-Large & 11.0 M & 60.3 \\
            DeepLabv3 w/ \arch-XXS (Ours) & \textbf{1.9 M} & 46.7 \\
            DeepLabv3 w/ \arch-XS (Ours) & 2.9 M & 57.4 \\
            DeepLabv3 w/ \arch-S (Ours) & 6.4 M & \textbf{65.4} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\sachin{\textbf{Semantic segmentation on the MS-COCO validation set.} MobileNetv3-Large results are from official torchvision segmentation models \citep{pytorch2021Seg}.}}
    \label{tab:app_seg_res}
\end{table}

\begin{figure}[b!]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.25\columnwidth]{visualizations/seg/rgb/000000017029.jpg} &  \includegraphics[width=0.25\columnwidth]{visualizations/seg/pred/000000017029.png} & \includegraphics[width=0.25\columnwidth]{visualizations/seg/overlay/000000017029.png} \\
        \includegraphics[width=0.25\columnwidth]{visualizations/seg/rgb/000000018833.jpg} &  \includegraphics[width=0.25\columnwidth]{visualizations/seg/pred/000000018833.png} & \includegraphics[width=0.25\columnwidth]{visualizations/seg/overlay/000000018833.png} \\
        \includegraphics[width=0.25\columnwidth]{visualizations/seg/rgb/000000094871.jpg} &  \includegraphics[width=0.25\columnwidth]{visualizations/seg/pred/000000094871.png} & \includegraphics[width=0.25\columnwidth]{visualizations/seg/overlay/000000094871.png} \\
        \includegraphics[width=0.25\columnwidth]{visualizations/seg/rgb/000000121242.jpg} &  \includegraphics[width=0.25\columnwidth]{visualizations/seg/pred/000000121242.png} & \includegraphics[width=0.25\columnwidth]{visualizations/seg/overlay/000000121242.png} \\
        \includegraphics[width=0.25\columnwidth]{visualizations/seg/rgb/000000136355.jpg} &  \includegraphics[width=0.25\columnwidth]{visualizations/seg/pred/000000136355.png} & \includegraphics[width=0.25\columnwidth]{visualizations/seg/overlay/000000136355.png} \\
        \toprule[1.5pt]
        \multicolumn{3}{c}{\includegraphics[width=0.75\columnwidth]{visualizations/seg/cmap.png}} 
    \end{tabular}
    \caption{\textbf{Semantic segmentation results} of Deeplabv3-\arch-S model on the \emph{unseen} MS-COCO validation set (\textbf{left:} input RGB image, \textbf{middle:} predicted segmentation mask, and \textbf{right:} Segmentation mask overlayed on RGB image). Color encoding for different objects in the PASCAL VOC dataset is shown in the last row.}
    \label{fig:app_seg_res_a}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.3\columnwidth]{visualizations/seg/rgb/000000145781.jpg} &  \includegraphics[width=0.3\columnwidth]{visualizations/seg/pred/000000145781.png} & \includegraphics[width=0.3\columnwidth]{visualizations/seg/overlay/000000145781.png} \\
        \includegraphics[width=0.3\columnwidth]{visualizations/seg/rgb/000000148730.jpg} &  \includegraphics[width=0.3\columnwidth]{visualizations/seg/pred/000000148730.png} & \includegraphics[width=0.3\columnwidth]{visualizations/seg/overlay/000000148730.png} \\
        \includegraphics[width=0.3\columnwidth]{visualizations/seg/rgb/000000149622.jpg} &  \includegraphics[width=0.3\columnwidth]{visualizations/seg/pred/000000149622.png} & \includegraphics[width=0.3\columnwidth]{visualizations/seg/overlay/000000149622.png} \\
        \includegraphics[width=0.3\columnwidth]{visualizations/seg/rgb/000000190756.jpg} &  \includegraphics[width=0.3\columnwidth]{visualizations/seg/pred/000000190756.png} & \includegraphics[width=0.3\columnwidth]{visualizations/seg/overlay/000000190756.png} \\
        \includegraphics[width=0.3\columnwidth]{visualizations/seg/rgb/000000549220.jpg} &  \includegraphics[width=0.3\columnwidth]{visualizations/seg/pred/000000549220.png} & \includegraphics[width=0.3\columnwidth]{visualizations/seg/overlay/000000549220.png} \\
        \toprule[1.5pt]
        \multicolumn{3}{c}{\includegraphics[width=0.75\columnwidth]{visualizations/seg/cmap.png}} 
    \end{tabular}
    \caption{\textbf{Semantic segmentation results} of Deeplabv3-\arch-S model on the \emph{unseen} MS-COCO validation set (\textbf{left:} input RGB image, \textbf{middle:} predicted segmentation mask, and \textbf{right:} Segmentation mask overlayed on RGB image). Color encoding for different objects in the PASCAL VOC dataset is shown in the last row. }
    \label{fig:app_seg_res_b}
\end{figure}


\clearpage

\lstinputlisting[label=list:mscSamplerCode,language=Python, caption=PyTorch implementation of multi-scale sampler]{code/msc_sampler.py}


\end{document}
