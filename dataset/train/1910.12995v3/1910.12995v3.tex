Task-oriented dialog systems have attracted more and more attention in recent years, because they allow for natural interactions with users to help them achieve simple tasks such as flight booking or restaurant reservation. Dialog state tracking (DST) is an important component of task-oriented dialog systems \cite{Young2013POMDPBasedSS}. Its purpose is to keep track of the state of the conversation from past user inputs and system outputs. Based on this estimated dialog state, the dialog system then plans the next action and responds to the user. In a \textit{slot-based} dialog system, a state in DST is often expressed as a set of slot-value pairs. The set of slots and their possible values are typically domain-specific, defined in a domain ontology.


With the renaissance of deep learning, many neural network based approaches have been proposed for the task of DST \cite{Mrksic2016NeuralBT,Liu2017AnET,zhong-etal-2018-global,Ren2018TowardsUD,nouri2018toward,mrkvsic2018fully,Korpusik2019DialogueST,chao2019bert}. These methods achieve highly competitive performance on standard DST datasets such as DSTC-2 \cite{Henderson2014TheSD} or WoZ 2.0 \cite{Wen2016ANE}. However, most of these methods still have some limitations. First, many approaches require training a separate model for each slot type in the domain ontology \cite{Mrksic2016NeuralBT,zhong-etal-2018-global,mrkvsic2018fully}. Therefore, the number of parameters is proportional to the number of slot types, making the scalability of these approaches a significant issue. Second, some methods only operate on a fixed domain ontology \cite{Liu2017AnET,zhong-etal-2018-global}. The slot types and possible values need to be defined in advance and must not change dynamically. Finally, state-of-the-art neural architectures for DST are typically heavily-engineered and conceptually complex \cite{zhong-etal-2018-global,Ren2018TowardsUD,nouri2018toward,mrkvsic2018fully}. Each of these models consists of a number of different kinds of sub-components. In general, complicated deep learning models are difficult to implement, debug, and maintain in a production setting.

Recently, several pretrained language models, such as ELMo \cite{Peters:2018} and BERT \cite{Devlin2019BERTPO}, were used to achieve state-of-the-art results on many NLP tasks. In this paper, we show that by finetuning a pretrained BERT model, we can build a conceptually simple but effective model for DST. Given a dialog context and a candidate slot-value pair, the model outputs a score indicating the relevance of the candidate. Because the model shares parameters across all slot types, the number of parameters does not grow with the ontology size. Furthermore, because each candidate slot-value pair is simply treated as a sequence of words, the model can be directly applied to new types of slot-value pairs not seen during training. We do not need to retrain the model every time the domain ontology changes. Empirical results show that our proposed model outperforms prior work on the standard WoZ 2.0 dataset. However, a drawback of the model is that it is too large for resource-limited systems such as mobile devices.

To make the model less computationally demanding, we propose a compression strategy based on the knowledge distillation framework \cite{Hinton2015DistillingTK}. Our final compressed model achieves results comparable to that of the full model, but it is around 8 times smaller and performs inference about 7 times faster on a single CPU.

%
