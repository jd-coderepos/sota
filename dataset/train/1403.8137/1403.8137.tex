\subsection{Adversarial Model}
In our C-CBPS  model there are 3 parties --  the broker, the publisher
and the subscriber.  The publisher and subscriber have a shared random
string $r$ known only to the  two of them. Meta-data $\bbm$ is private
to the  publisher and predicate  $\bbP$ is private to  the subscriber.
The publisher and subscriber each  have a separate and private channel
to the  broker. They are to each  send a single message  to the broker
from  which the  broker  should  be able  to  correctly, securely  and
efficiently  compute the  value of  $\bbP(\bbm)$ but  learn absolutely
nothing  else. We  work  in the  information-theoretic security  model
where we don't make  any constraining assumptions on the computational
resources  available  to  the  broker.   However,  the  publisher  and
subscriber are  restricted to be  polynomial-time probabilistic Turing
machines. In fact, given  the shared randomness they are deterministic
polynomial-time  Turing  machines. And  in  keeping with  Kerckhoffs's
principle \cite{Kerckhoff1883} it is assumed that the broker knows the
details  of   the  algorithm/protocol  being  used,   i.e.,  it  knows
everything except for $\bbP$,  $\bbm$ and $r$.  This adversarial model
is captured in Figure \ref{fig:adversarialmodel}

\begin{figure}
\centering
\includegraphics[width=3.4in, height=3in]{AdversarialModel.pdf}
\vspace{-.5in}
\caption{Adversarial model}
\label{fig:adversarialmodel}
\end{figure}

\subsection{Measure}
We denote the  length of the meta-data $\bbm$ as $n  = |\bbm|$. In the
case of the predicate $\bbP$ the  relevant measure is the depth and we
parameterize it as a multiple of  $\lg n$, to be precise we denote the
depth  of $\bbP$  by $\kappa\lg  n$ where  $n$ is  the  parameter that
asymptotically  goes to  infinity while  $\kappa$ is  a  constant. The
reason   for  parameterizing  in   this  way   is  that   the  obvious
parameterization of the  {\em size} of $\bbP$, in  terms of the number
of gates,  is not relevant  as it is  still an open problem  to handle
arbitrary  polynomial-sized  predicate  circuits  in  the  information
theoretic setting. We remind the reader that the defining contribution
of  this  paper   is  showing  how  to  handle   circuits  in  \NCone,
i.e. logarithmic-depth circuits.

As we will see from the  subsequent sections of this paper, the broker
will get two (sub)sequences of  group elements each from the publisher
and subscriber that he will  interlace and multiply together to obtain
$\bbP(\bbm)$. We  denote by  $L$ the length  of the sequence  that the
broker composes from the  shares he receives. The efficiency question,
thus, becomes  given an $n$  and a $\kappa$  what is the  smallest $L$
that a given  protocol achieves. In what follows we  will see that, in
the  case  of  the  protocol  based  on  Valiant's  universal  circuit
\cite{V76}, $L = n^{\Omega(\kappa\lg n)}$ which is non-polynomial.  In
general  the goal of  this paper  is to  achieve $L$  which will  be a
polynomial in $n$ but the lower  the degree of the polynomial the more
efficient  the protocol.  We will  show that  UGP-Match achieves  $L =
n^{12\kappa+1}\lg  n$.   With  FSGP-Match   we  bring  this   down  to
$L=4n^{2\kappa+2})$ and then finally with OFGSP-Match we bring it down
to $L=2n^{2\kappa+1}$. We point out that these upper bounds are exact,
i.e. we can analyze these constructions down to the exact constant and
so  do not need  to employ  the big-oh  notation.  For  convenience we
present our results in the form of a table.

\begin{table}[h]
\centering
\begin{tabular}{||c|cc||}
\hline \hline
Protocol & Complexity(L) & In words\\ \hline \hline
Universal Circuit & $n^{\Omega(\kappa\lg n)}$ & (super-poly-time)\\ \hline
UGP-Match & $\tilde{O}(n^{20\kappa+2})$ & (poly-time)\\ \hline
FSGP-Match & $4n^{2\kappa+2}$ & '' \\ \hline
OFSGP-Match & $2n^{2\kappa+1}$ & ''\\
\hline \hline
\end{tabular}
\caption{This table shows the complexity of the various protocols.}
\label{tab:complexity}
\end{table}


\subsection{Terminology and Propositions}
\label{subsec:terminology}
We set up some terminology and  definitions for use in the rest of the
paper. We  also state and/or  prove some basic propositions  that will
set the ground for the results to come later.  

For the sake of completeness we define the C-CBPS model formally.
\begin{definition}[C-CBPS]
The 3  parties in the C-CBPS  model and their states  of knowledge are
captured in Figure \ref{fig:adversarialmodel}. There is a single round
of  communication  where the  publisher  and  subscriber  each send  a
private  message ($E_r(\bbm)$  and $E_r(\bbP)$,  respectively)  to the
broker who is then able to efficiently compute $\bbP(\bbm)$ such that

\noindent{\bf Correctness}  $\forall \bbm,  \bbP$, and $r$,  given the
encrypted   messages  $E_r(\bbm),   E_r(\bbP)$  the   broker  computes
$\bbP(\bbm)$ correctly all the time.

\noindent{\bf  Security}  $\forall \bbm,  \bbP$,  and  $r$, given  the
encrypted  messages $E_r(\bbm), E_r(\bbP)$  the broker  learns nothing
whatsoever about $\bbm, \bbP$ (other than the value of $\bbP(\bbm)$).

\end{definition}


We will use the language  of multiplicative groups. For our purposes a
\emph{group} is a  just a set of elements with  a binary operation and
inverses. We  let G  denote a generic  multiplicative group.   We will
need $G$ to be an unsolvable group (for reasons to be explained later)
and so we can take it  to be $S_5$ the symmetric group of permutations
of  a  $5$-element set.   By  default we  will  use  the standard  and
implicit  one-line notation  derived from  Cauchy's  two-line notation
\cite{WikiPermNotation} to represent a permutation.
\begin{definition} [Cycle]
A permutation  $g \in S_5$ is said  to be a \emph{cycle}  if its graph
consists of  exactly one cycle of length  $5$. For example, $(5  3 4 1
2)$  is a  cycle  because its  graph  is the  cycle  $1 \rightarrow  5
\rightarrow 2 \rightarrow 3 \rightarrow 4 \rightarrow 1$.
\end{definition}

When  we use the  product-of-cycles notation  then we  will explicitly
have a subscript ``cycle'' at the end.   For example $(2 4 5 1 3) = (1
2 4)(3 5)_{\mbox{cycle}}$ represents the permutation $1 \rightarrow 2,
2 \rightarrow  4, 3 \rightarrow 5,  4 \rightarrow 1,  5 \rightarrow 3$
with graph $1 \rightarrow 2 \rightarrow 4 \rightarrow 1, 3 \rightarrow
5 \rightarrow 3$.

For the rest of this paper we  will use $\alpha \in S_5$ to denote the
cycle $(23451)$. And, the identity in the group is $1_{S_5} = (1 2 3 4
5)$.

Our  protocols  will involve  group  programs  which  in turn  involve
sequences  of group elements  and their  products. This  motivates the
following:

\begin{definition}[Value of a sequence]
Given a sequence $S$ of  group elements $g_1$, $g_2$, $\ldots$, $g_L$,
the {\em value} of the sequence $\mbox{Value}(S)$ is defined to be the
product of the sequence elements in order, i.e.
\[ \mbox{Value}(S) = \prod_{i=1}^L g_i = g_1g_2\ldots g_L. \]
\end{definition}

\begin{definition}[Blinding]
Given a sequence $S$ of  group elements $g_1$, $g_2$, $\ldots$, $g_L$,
$\mathcal{BS}(S)$ is used to denote the distribution over sequences of
the form
\[ g1\cdot r_1, r_1^{-1}\cdot g_2\cdot r_2,\ldots,  r_L^{-1}\cdot g_L, \] 
generated by choosing each $r_i, \forall_i 1\leq i \leq L-1$ uniformly
and independently from $G$.  We overload the term $\mathcal{BS}(S)$ to
also  refer   to  a  specific  sequence  selected   according  to  the
distribution $\mathcal{BS}(S)$ and the context should be sufficient to
resolve any  ambiguity.  The $r_i$  are referred to as  {\em blinders}
and  {\em blinding}  $S$ refers  to the  act of  selecting  a sequence
according to the distribution $\mathcal{BS}(S)$.
\end{definition}

\begin{lemma}[The Blinding Lemma]
\label{lemma:blindinglemma}
Given a  sequence of group elements,  $S$, of length  $L$, the blinded
sequence $\mathcal{BS}(S)$ has the following two properties:

\noindent{\bf Preserves value}:  $Value(\mathcal{BS}(S)) = Value(S)$

\noindent{\bf  Uniform distribution}:  $\mathcal{BS}(S)$  is uniformly
distributed  over the  space of  all  sequences of  group elements  of
length $L$  with the  same value,  i.e. for any  sequence $S'  = g'_1,
g'_2\ldots g'_L$ we have that
\[ \mbox{Pr}(\mathcal{BS}(S) = S'| \mbox{Value}(S') = \mbox{Value}(S)) = \frac{1}{|G|^{L-1}} \] 
where  the probability  is measured  over  the random  choices of  the
blinders.
\end{lemma}

\begin{proof}
It is  easy to see that  blinding preserves the value  of the sequence
because the blinders,  the $r_i$, cancel out when  the elements of the
blinded sequence are multiplied together.

Now, we need to show the uniform distribution property. First, observe
that  the space  of all  sequences $S'$  such that  $\mbox{Value}(S) =
\mbox{Value}(S')$ has  size exactly  $|G|^{L-1}$.  This is  because we
can pick  the first $L-1$  elements, $g'_1, g'_2,\ldots,  g'_{L-1}$ of
$S'$ arbitrarily from the group  (in $|G|^{L-1}$ ways) but having done
that then  (because these elements are  chosen from a  group) there is
exactly   one   value   for    the   $n$-th   element   $g_L$   namely
$g_{L-1}^{-1}\cdot            g_{L-2}^{-1}            \cdot\ldots\cdot
g_1^{-1}\cdot\mbox{Value}(S)$    such    that   $\mbox{Value}(S')    =
\mbox{Value}(S)$.

Hence, to  compute $\mbox{Pr}(\mathcal{BS}(S) =  S')$ we just  need to
compute  the probability  that the  first  $L-1$ elements  of the  two
sequences  ($\mathcal{BS}(S)$ and $S'$)  match because  the condition,
that  $\mbox{Value}(S')  =  \mbox{Value}(S)$  along with  the  already
proven  fact that  $\mbox{Value}(\mathcal{BS}(S))  = \mbox{Value}(S')$
implies that the $L$-th elements must automatically match if the first
$L-1$ match.

Let $\mathcal{BS}(S) = b_1, b_2,\ldots, b_L$. Then what we have argued
is that
\begin{align*}
  & \mbox{Pr}((b_1 = s'_1) \wedge (b_2 = s'_2) \wedge \dots \wedge (b_L = s'_L)|(\mbox{Value}(S') = \mbox{Value}(S)))\\
  & =\mbox{Pr}((b_1 = s'_1) \wedge (b_2 = s'_2) \wedge \dots \wedge (b_{L-1} = s'_{L-1})| (b_L = s'_L))\\
  & =\mbox{Pr}((b_1 = s'_1) \wedge (b_2 = s'_2) \wedge \dots \wedge (b_{L-1} = s'_{L-1})) \\
  & =\mbox{Pr}(b_1 = s'_1)  \times \mbox{Pr}((b_2 = s'_2)|(b_1 = s'_1))\\ 
  & \times\mbox{Pr}((b_3 = s'_3)|(b_1 = s'_1) \wedge (b_2 = s'_2)) \\ 
  & \times \dots \\
  & \times \mbox{Pr}((b_{n-1} = s'_{n-1})|  (b_1 = s'_1) \wedge (b_2 =
  s'_2) \wedge \ldots \\
  & \hspace{3cm} \ldots \wedge (b_{L-1} = s'_{L-1}))
\end{align*}
But, by the definition of blinding, the above is 
\begin{align*}
  & \mbox{Pr}(r_1 = s_1^{-1}\cdot s'_1) \times \mbox{Pr}(r_2 = s_2^{-1}\cdot s_1^{-1}\cdot s'_1 \cdot s'_2) \\
  & \times \dots \times \mbox{Pr}(r_{L-1} = \prod_{i=1}^{L-1}s_i^{-1}\prod_{i=}^{L-1}s'_i) \\
  & ==\frac{1}{|G|} \times \frac{1}{|G|} \times\dots\times \frac{1}{|G|}\\
  & =\frac{1}{|G|^{L-1}}.
\end{align*}

But this  means that  $\mathcal{BS}(S)$ is uniformly  distributed over
the space of all sequences of group elements with the same value since
the space of all such sequences is of size exactly $|G|^{L-1}$, as has
been argued earlier.

\end{proof}

The following definition, of a group program, is central to this paper
and is presented in visual form in Figure \ref{fig:groupprogram}.

\begin{definition}[Group Program]
Let  $\alpha$  be an  element  of $S_5$.  A  group  program of  length
\emph{$L$}   is  $(g_{1}^{0}$,  $\ldots$,   $g_L^{0})$,  $(g_{1}^{1}$,
$\ldots$ ,$g_L^{1})$, $(k_{1}$, $\ldots$,  $k_L)$ where for any $i,j$:
$g_{i}^{j}  \in S_{5}$ and  $k_{i} \in  \{1,\ldots,n\}$.  We  say that
this program $\alpha$-computes  $f:\{0,1\}^{n} \rightarrow \{0,1\}$ if
$\forall x$,
\[ f(x)=1 \Rightarrow \prod ^{\ell} _{i=1} g_{i}^{x_{k_i}} = \alpha \]
\[ f(x)=0 \Rightarrow \prod ^{\ell} _{i=1} g_{i}^{x_{k_i}} = 1_{S_5}; \]
which  we can write  compactly as  $\forall x  : \prod  ^{\ell} _{i=1}
g_{i}^{x_{k_i}} = \alpha^{f(x)}$.
\end{definition}

\begin{figure}
\centering
\includegraphics[width=3.4in, height=3in]{GroupProgram.pdf}
\vspace{-0.8in}
\caption{The form of a Group Program}
\label{fig:groupprogram}
\end{figure}

We will  consider predicates  represented as $n$-input,  single output
(bounded  fan-in)  circuits   of  AND($\wedge$  two-input),  OR($\vee$
two-input) and  NOT($\neg$ single input) gates. Our  definition of the
depth  of  a  circuit  is  slightly non-standard  in  that  we  ignore
NOT($\neg$) gates. This is because  of the Barrington Transform (to be
elaborated below) which transforms a circuit into a group progam whose
length  depends  only  on  the  depth  of  the  circuit  in  terms  of
AND($\wedge$) and OR($\vee$) gates.

\begin{definition}[Depth of a Circuit]
The depth of a circuit is defined to be the number of AND and OR gates
in the  longest path from  an input to  the output. (NOT gates  do not
count towards depth).
\end{definition}

In his seminal paper \cite{B89}, on the way to showing that \NCone~ is
computable by  fixed-width branching programs,  Barrington showed that
any   logarithmic-depth   circuit    can   be   transformed   into   a
polynomial-length  group  program  -  a  transformation  we  term  the
Barrington Transform.

\begin{theorem}[Barrington Transform] 
\label{thm:barrington-transform}
Any circuit  of depth $d$ can be
  transformed   into  a   group   program  of   length  $4^{d}$   that
  $\alpha$-computes the same function as the circuit.
\end{theorem}

For  the  sake  of  completeness  we present  the  proof  in  Appendix
\ref{appendix:barrington-transform}.   The  proof,  presented  as  a  series  of
lemmas, details  the Barrington Transform  showing how to  transform a
circuit  into a  group program.  This  treatment is  directly based  on
\cite{Viola09}.

It  follows  from   Theorem  \ref{thm:barrington-transform}  that  the
Barrington Transform transforms any $n$-input single output circuit of
depth $\kappa\lg n$ into a group program of length $n^{2\kappa}$ where
$\kappa$ is a  constant.  

We will  be using an extension  of the notion  of randomized encodings
\cite{IK00,  AIK05,  A11}  that  played  a  significant  role  in  the
breakthrough  showing  the  feasibility  of cryptography  in  \NCzero~
\cite{AIK04}.  First, we give the definition of randomized encodings.

\begin{definition}[Randomized encoding] 
A   function  $f(x)$   is   said  to   have   a  randomized   encoding
$\hat{f}(x,r)$,  where $r$  is a  random  string, if  there exist  two
efficiently computable (deterministic, polynomial-time) algorithms REC
and SIM such that

\noindent{\bf  Correctness} $\forall x,  r$, given  $\hat{f}(x,r)$ REC
recovers $f(x)$, i.e. REC$(\hat{f}(x,r)) = f(x)$ .

\noindent{\bf  Security}  $\forall  x$,  given $x$  and  $r'$  (random
coins), SIM produces a distribution identical to $\hat{f}(x,r)$, i.e.,
the distribution of  SIM$(x, r')$ is identical to  the distribution of
$\hat{f}(x,r)$.

\end{definition}

The above definition will make  it easier to comprehend the definition
of a {\em 2-decomposable randomized encoding}. We believe that, though
natural  in the  context of  2-party secure  computation, this  is the
first   time  this   definition   is  appearing   explicitly  in   the
literature. A stronger definition of decomposable randomized encodings
has  appeared   before  \cite{IKOS08,   IKOS09}  and  the   notion  of
2-decomposable  randomized  encodings  is  implicit  in  prior  works,
including \cite{FKN94}.

\begin{definition}[2-decomposable Randomized encoding] 
\label{def:2-decomp}
A  function  $f(x,y)$ is  said  to  have  a 2-decomposable  randomized
encoding $\langle\hat{f}_1(x,r),  \hat{f}_2(y,r)\rangle$, where $r$ is
a  ({\em  shared})  random  string,  if there  exist  two  efficiently
computable  (deterministic, polynomial-time)  algorithms  REC and  SIM
such that

\noindent{\bf    Correctness}    $\forall     x,    y,    r$,    given
$\langle\hat{f}_1(x,r), \hat{f}_2(y,r)\rangle$  REC recovers $f(x,y)$,
i.e., REC$(\langle\hat{f}_1(x,r), \hat{f}_2(y,r)\rangle) = f(x,y)$.

\noindent{\bf Security} $\forall x, y$,  given $x, y$ and $r'$ (random
coins),     SIM    produces     a     distribution    identical     to
$\langle\hat{f}_1(x,r), \hat{f}_2(y,r)\rangle$, i.e., the distribution
of   SIM$(x,   y,  r')$   is   identical   to   the  distribution   of
$\langle\hat{f}_1(x,r), \hat{f}_2(y,r)\rangle$.
\end{definition}

The schemes  presented in this paper  rely crucially on  the model and
construction presented  in \cite{FKN94}. We  define the FKN  model and
show how protocols for it are essentially equivalent to 2-decomposable
randomized encodings.

\begin{definition}[FKN model]
\label{def:FKNModel}
The 3  parties in the FKN  model and their states  of knowledge are
captured  in Figure  \ref{fig:FKNModel}. There  is a  single  round of
communication  where  Alice  and  Bob  each  send  a  private  message
($E_r(x)$ and  $E_r(y)$, respectively)  to Carol who  is then  able to
efficiently compute the publicly known function $f(x,y)$ such that

\noindent{\bf  Correctness}  $\forall  x,   y$,  and  $r$,  given  the
encrypted messages $E_r(x),  E_r(y)$ Carol computes $f(x,y)$ correctly
all the time.

\noindent{\bf Security}  $\forall x, y$, and $r$,  given the encrypted
messages $E_r(x), E_r(y)$ Carol learns nothing whatsoever about $x, y$
(other than the value of $f(x,y)$).
\end{definition}

\begin{figure}
\centering
\includegraphics[width=3.4in, height=3in]{FKNModel.pdf}
\vspace{-.5in}
\caption{FKN model}
\label{fig:FKNModel}
\end{figure}

\begin{lemma}[Equivalence   of  FKN   and   2-decomposable  randomized
  encoding]
\label{lemma:FKN2decomp}
There is  a 1-1  isomorphism between protocols  for the FKN  model and
2-decomposable randomized encodings.
\end{lemma}
\begin{proof}
  We will first  see that a 2-decomposable randomized  encoding of the
  function $f(x,y)$  gives rise to a  protocol for the  FKN model. Let
  Alice compute and send  $\hat{f}_1(x,r)$ to Carol while Bob computes
  and send $\hat{f}_2(y,r)$ to Carol. From the Correctness property it
  follows    that    Carol    can   run    REC$(\langle\hat{f}_1(x,r),
  \hat{f}_2(y,r)\rangle)$  to  compute   $f(x,y)$  correctly  all  the
  time.  And from  the  Security  property we  get  that Carol  learns
  nothing but the value $f(x,y)$.

  Similarly,  given a  protocol for  the FKN  model that  computes the
  publicly  known  function  $f(x,y)$  it  is easy  to  see  that  the
  Correctness and Security properties  carry over by setting $E_r(x) =
  \hat{f}_1(x,r)$ and $E_r(y) = \hat{f}_2(x,r)$.
\end{proof}

We present  the definition of  a Universal Function below.  

\begin{definition}[Universal Function]
A Universal Function $\mathcal{UF}$ has  two inputs $x$ and  $y$ where
$y$ is  the encoding of a function  for which $x$ is  a suitable input.
The output  of $\mathcal{UF}(x,y)$ is  defined to be $y(x)$.
\end{definition}

One  can similarly  define  Universal Circuits  which  are similar  to
Universal Functions  except that they  work with encoding  of circuits
and simulate circuits.
\begin{definition}[Universal Circuit]
A Universal Circuit $\mathcal{UC}$ has  two inputs $x$ and  $y$ where
$y$ is  the encoding of a circuit  for which $x$ is  a suitable input.
The output  of $\mathcal{UC}(x,y)$ is  defined to be $y(x)$,  in other
words  the   Universal  Circuit  outputs  the   result  obtained  from
simulating  function $y$  on  input $x$.  (Universal  Circuits are  the
circuit equivalent of Universal Turing Machines).
\end{definition}

The central idea  of this paper is to bypass  Universal Circuits so we
will not  be utilizing this  definition except to explain  (in Section
\ref{sec:overview})  how it  is that  we pick  up efficiency  gains by
avoiding them.


One  can naturally  apply the  notion of  a  2-decomposable randomized
encoding to a universal function $f(x,y) = y(x)$.

And   analogous    to   the   correspondence    expressed   in   Lemma
\ref{lemma:FKN2decomp} we have the following correspondence.

\begin{lemma}[Equivalence  of  C-CBPS  and  2-decomposable  randomized
  encoding of universal functions]
\label{lemma:ccbps-2decompuniv}
There is a 1-1 isomorphism  between protocols for the C-CBPS model and
2-decomposable randomized encodings of universal functions.
\end{lemma}
\begin{proof}
The proof is very similar  to that of Lemma \ref{lemma:FKN2decomp} and
so  we provide  only  the key  aspects  of the  correspondence as  the
details  are  straightforward.   $x,  y$ correspond  to  $\bbm,  \bbP$
respectively.     $\hat{f}_1(x,r),   \hat{f}_2(y,r)$    correspond   to
$E_r(\bbm), E_r(\bbP)$ respectively.  And the Correctness and Security
properties carry over in a direct way.
\end{proof} 

\begin{figure}
\centering
\includegraphics[width=3.4in, height=3in]{AlphaOneGroupProgram.pdf}
\vspace{-.8in}
\caption{The form of a $(\alpha, 1)$-preserving Group Program}
\label{fig:alphaone}
\end{figure}


We now  present some definitions that clarify  the ``fixed structure''
trick  alluded  to  earlier,  and  how it  relates  to  2-decomposable
randomized encodings of universal functions.  The following definition
of a $(\alpha,1)$-preserving group program is presented in visual form
in Figure \ref{fig:alphaone}.

\begin{definition}[$(\alpha,1)$-preserving group programs]
  An  $(\alpha,1)$-preserving group  program of  length $L$  is $(g_1,
  g_2,\ldots, g_{L+1})$,  $(k_1, k_2,\ldots, k_L)$ where  for any $i$:
  $g_{i} \in S_{5}$ and $k_{i}  \in \{1,\ldots,n\}$.  We say that this
  $(\alpha,1)$-preserving      group     program     $\alpha$-computes
  $f:\{0,1\}^{n} \rightarrow \{0,1\}$ if $\forall x$,
  \[      f(x)=1      \Rightarrow      (\prod      ^{\ell}      _{i=1}
  g_{i}\cdot\alpha^{x_{k_i}})g_{L+1} = \alpha \]
\[       f(x)=0      \Rightarrow      (\prod       ^{\ell}      _{i=1}
g_{i}\cdot\alpha^{x_{k_i}})g_{L+1}  =  1_{S_5}; \]  which  we can  write
compactly    as     $\forall    x    :     (\prod    ^{\ell}    _{i=1}
g_{i}\cdot\alpha^{x_{k_i}})g_{L+1} = \alpha^{f(x)}$.
\end{definition}

\begin{definition}[Index sequence  of an $(\alpha,1)$-preserving group
  program]
  Given  $(g_1, g_2,\ldots,  g_{L+1})$, $(k_1,  k_2,\ldots,  k_L)$, an
  $(\alpha,1)$-preserving  group  program  of  length  $L$,  its  index
  sequence is $(k_1, k_2,\ldots, k_L)$.
\end{definition}

Analogous   to   Theorem   \ref{thm:barrington-transform} we have

\begin{theorem}[$(\alpha,1)$-preserving Barrington Transform] 
\label{thm:alpha-one-barrington-transform}
Any   circuit    of   depth   $d$   can   be    transformed   into   a
$(\alpha,1)$-preserving   group  program   of   length  $4^{d}$   that
$\alpha$-computes the same function as the circuit.
\end{theorem}

For  the  sake  of  completeness  we present  the  proof  in  Appendix
\ref{appendix:alpha-one-barrington}. 

The  definition of  a  fixed structure group  program  given below  is
crucial to  improving the  efficiency of the  ideas presented  in this
paper and making them practical.

\begin{definition}[Fixed structure group program]
\label{def:fixed-structure}
  If a class  of functions (say, the class  of $n$-input single output
  functions computable by circuits of  depth $\kappa\lg n$) can all be
  transformed  into $(\alpha,1)$-preserving  group  programs with  the
  exact same index sequence (and hence the exact same length) then the
  resulting  class  of  group  programs   is  said  to  have  a  fixed
  structure.  In a  slight abuse  of language  we refer  to  the class
  itself as a fixed structure group program.
\end{definition}