
\documentclass[11pt,table]{article}
\usepackage[in]{fullpage}           \usepackage[sc]{titlesec}           \usepackage{microtype}
\usepackage{xurl}                   \usepackage{xcolor}
\usepackage{amsmath,amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\algnewcommand{\LineComment}[1]{\State 
 \textcolor{gray}{\# #1}}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{titlesec}
\usepackage[numbers,sort]{natbib}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{amsthm}

\usepackage[outline]{contour}
\usepackage{ulem}
\normalem
\newcommand \myul[4]{\begingroup \renewcommand \ULdepth {#1}\renewcommand \ULthickness {#2}\contourlength{#3}\mbox{\uline{\phantom{#4}}}\llap{\contour{white}{#4}}\endgroup }

\usepackage[breaklinks]{hyperref}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator{\x}{\mathbf{x}}
\DeclareMathOperator{\X}{\mathcal{X}}
\DeclareMathOperator{\y}{\mathbf{y}}
\DeclareMathOperator{\vv}{\mathbf{v}}
\DeclareMathOperator{\Y}{\mathcal{Y}}
\renewcommand{\k}{\mathbf{k}}
\DeclareMathOperator{\e}{\mathbf{e}}
\DeclareMathOperator{\m}{\boldsymbol{\mu}}
\newcommand{\pt}[1]{\rho_{#1}}
\newcommand{\mt}[1]{\boldsymbol{\mu}_{#1}}
\newcommand{\kl}[2]{D_{KL}\left(#1 \parallel #2\right)}
\newcommand{\N}[2]{\mathcal{N}\left(#1 , #2\right)}
\newcommand{\bc}[1]{#1_c}
\DeclareMathOperator{\R}{\mathbb{R}}
\newcommand{\I}[1]{\boldsymbol{I}}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newcommand{\tidx}[2]{#1_{#2}}
\newcommand{\didx}[2]{#1^{(#2)}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\pars}{\theta}
\newcommand{\parsn}{\vec{\pars}}
\newcommand{\parst}[1]{\tidx{\pars}{#1}}
\newcommand{\parsnt}[1]{\tidx{\parsn}{#1}}
\newcommand{\alphat}[1]{\tidx{\alpha}{#1}}
\newcommand{\yt}[1]{\tidx{\y}{#1}}
\newcommand{\constvec}[2]{\vec{#1}}
\newcommand{\0}[1]{\constvec{0}{#1}}
\newcommand{\1}[1]{\constvec{1}{#1}}
\newcommand{\yd}{y}
\newcommand{\ydd}[1]{\didx{\yd}{#1}}
\newcommand{\xdd}[1]{\didx{x}{#1}}
\newcommand{\parsdd}[1]{\didx{\pars}{#1}}
\newcommand{\oh}[2]{\mathbf{e}_{#1}}
\newcommand{\ds}[1]{\{1,#1\}}
\newcommand{\dsd}[2]{\ds{#1}^{#2}}
\newcommand{\ui}[1]{U\ds{#1}}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\def\net{\Psi\xspace}
\newcommand{\sender}[2]{p_{_S}\left(#1 \mid #2\right)}
\newcommand{\out}{p_{_O}}
\newcommand{\outn}{\vec{p}_{_O}}
\newcommand{\rec}{p_{_R}}
\newcommand{\inp}{p_{_I}}
\newcommand{\flow}{p_{_F}}
\newcommand{\update}{p_{_U}}
\newcommand{\pred}[1]{\hat{#1}}
\newcommand{\eps}{\vec{\pred{\epsilon}}}


\begin{document}
\title{\textsc{Bayesian Flow Networks}}
\author{Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez}
\date{
\vspace{-6pt}
\texttt{\{alex,rupesh,timothy,tino\}@nnaisense.com}\\
\vspace{6pt}
NNAISENSE
}
\maketitle

\begin{abstract}
This paper introduces \emph{Bayesian Flow Networks} (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution.
Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required.
Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures.
Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling.
The loss function directly optimises data compression and places no restrictions on the network architecture.
In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task\footnote{Code and trained models can be found at \url{https://github.com/nnaisense/bayesian-flow-networks}}.
\end{abstract}
\section{Introduction}
Large-scale neural networks have revolutionised generative modelling over the last few years, with an unprecedented ability to capture complex relationships among many variables. 
Building a convincing joint model of all the pixels in a high resolution image, for example, was impossible before the advent of modern generative networks.

Key to the expressive power of most of these networks --- including autoregressive models e.g.~\citep{sutskever2011generating,graves2013generating}, flow-based models~\citep{rezende2015variational}, deep VAEs~\citep{vahdat2020nvae} and diffusion models~\citep{sohl2015deep} --- is that the joint distribution they encode is broken down into a series of steps, thereby eluding the ``curse of dimensionality'' that would doom any effort to explicitly define all the interactions among so many variables.
In colloquial terms they solve a hard problem by splitting it into easy pieces.

A general way to view such distributions is as an exchange of messages between a sender, Alice, who has access to some data, and her friend Bob, who wishes to receive it in as few bits as possible. 
At each step Alice sends a message to Bob that reveals something about the data.
Bob attempts to guess what the message is: the better his guess the fewer bits are needed to transmit it.
After receiving the message, Bob uses the information he has just gained to improve his guess for the next message.
The loss function is the total number of bits required for all the messages.

In an autoregressive language model, for example, the messages are the word-pieces the text is divided into. 
The distribution encoding Bob’s prediction for the first message is of necessity uninformed: a zero-gram prior based on the relative frequencies of different word-pieces. 
The transmission cost is the negative log-probability under this prior. 
Bob then uses the first word-piece to predict the second; on average, the second prediction will be slightly more informed than the first, and the expected transmission cost will be slightly lower. 
The process repeats with the predictions improving at each step. 
The sum of the transmission costs is the negative log-probability of the complete text sequence, which is the loss function minimised by maximum likelihood training.  
It is also the minimum number of bits that would be required for Alice to transmit the pieces to Bob using arithmetic coding~\citep{witten1987arithmetic}. 
There is therefore a direct correspondence between fitting an autoregressive model with maximum likelihood and training it for data compression.

Autoregressive networks are currently state-of-the-art for language modelling~\citep{openai2023gpt4}, and in general perform well on discrete data where a natural ordering exists.
However they have proved less effective in domains such as image generation, where the data is continuous and no natural order exists among variables (e.g. there is no reason to generate one pixel before another).
They also have the drawback that generating samples requires as many network updates as there are variables in the data.

Diffusion models are an alternative framework that has proved particularly effective for image generation~\cite{dhariwal2021diffusion,rombach2022high}.
In this case the transmission procedure is a little more complex\footnote{We are here describing the reverse process of diffusion models.}.
Each message Bob receives is a noisy version of the message before, where the noise is designed so that in expectation the messages approach the data.
The transmission cost at each step is the Kullback-Leibler divergence between the distribution from which Alice draws the message and Bob's prediction of that distribution (which is a reparameterisation of his prediction of the data, and which is therefore improved by the information he gained from the previous message).
The sum of the KL divergences is the \emph{evidence lower bound} minimised by diffusion training~\citep{sohl2015deep}; it is also the expected number of bits needed to transmit the data using an efficient bits-back coding scheme~\citep{Wallace1991ClassificationBM,hinton1993keeping}. 
Once again there is an exact equivalence between the loss function used to train the model and the model’s ability to compress data, as elucidated by previous authors~\citep{townsend2019practical}.

We posit that the superiority of diffusion over autoregression for image generation lies in the way diffusion progresses from coarse to fine image details as the level of noise decreases --- a more natural way to construct an image than one dot at a time.
However diffusion has yet to match autoregression for discrete data, which is unfortunate, as diffusion models have the advantage of decoupling the number of generation steps from the number of variables.
A fundamental challenge is that when the data is discrete, the noise in the diffusion process is also discrete, and therefore discontinuous.
To return to the transmission metaphor, if the data is a piece of text, then Bob begins the process with a totally garbled text, every symbol of which is either randomly altered or left unchanged by each of Alice's messages.
A key motivation for this work was our belief that a fully continuous transmission process --- where Alice's messages smoothly alter Bob's beliefs --- would be more effective for discrete data.
Moreover this should open the door to gradient-based sample guidance~\citep{dhariwal2021diffusion} and few-step generation techniques~\citep{salimans2022progressive,watson2022learning,song2023consistency}, similar to those that have been developed for continuous diffusion.

\begin{figure}[t!] 
    \includegraphics[width=\textwidth]{figures/bayesian_flow_overview.pdf}
\caption{\textbf{System Overview}. The figure represents one step of the modelling process of a Bayesian Flow Network. The data in this example is a ternary symbol sequence, of which the first two variables (`B' and `A') are shown.  At each step the network emits the parameters of the output distribution based on the parameters of the previous input distribution.  The sender and receiver distributions (both of which are continuous, even when the data is discrete) are created by adding random noise to the data and the output distribution respectively.  A sample from the sender distribution is then used to update the parameters of the input distribution, following the rules of Bayesian inference. Conceptually, this is the message sent by Alice to Bob, and its contribution to the loss function is the KL divergence from the receiver to the sender distribution.}
\label{fig:overview}
\end{figure}

\emph{Bayesian Flow Networks} (BFNs), the model introduced in this paper, differ from diffusion models in that the network operates on the parameters of a data distribution, rather than on a noisy version of the data itself.
This ensures that the generative process is fully continuous and differentiable, even when the data is discrete.
BFNs can be summarised by the following transmission scheme (Figure~\ref{fig:overview}).
Bob has an ``input distribution'' which is initially a simple prior: a standard normal for continuous data, a uniform categorical for discrete data.
At each transmission step he feeds the parameters of the input distribution (e.g. the mean of a normal distribution, the probabilities of a categorical distribution) into a neural network.
The network outputs the parameters of a second distribution referred to as the ``output distribution''.
Alice then creates a ``sender distribution'' by adding noise to the data according to a predefined schedule, and Bob creates a ``receiver distribution'' by convolving the output distribution with the same noise distribution used by Alice: intuitively, for every value the data could take on, Bob constructs the sender distribution Alice would have used if that value was correct, then sums over all these hypothetical sender distributions, weighted by the probability of the corresponding value under the output distribution.
Alice picks a sample from the sender distribution and sends it to Bob at a cost equal to the KL divergence from receiver to sender.
Bob then uses the sample to update his input distribution, following the rules of Bayesian inference.
Usefully, the Bayesian updates are available in closed-form as long as the input distribution models all the variables in the data independently.
Once the update is complete, Bob again feeds the parameters of the input distribution to the network which returns the parameters of the output distribution.
The process repeats for  steps, at which point Bob can predict the data accurately enough that Alice can send it to him without any noise.

Note the key difference between the input and output distributions: the input distribution receives information about each variable in the data independently (via the Bayesian updates), and is therefore unable to exploit contextual information, such as neighbouring pixels in an image or related words in a text; the output distribution, on the other hand, is produced by a neural network that jointly processes all the parameters in the input distribution, giving it access to all available context.
Intuitively, the combination of the input and output distributions represents a division of labour between Bayesian inference and deep learning that plays to both of their strengths: the former provides a mathematically optimal and finely controllable way to collect and summarise information about individual variables, while the latter excels at integrating information over many interrelated variables.

The above transmission process defines an -step loss function that can be generalised to continuous time by sending  to .
In continuous time the Bayesian updates become a \emph{Bayesian flow} of information from the data to the network.
As well as removing the need to predefine the number of steps during training, the continuous-time loss function is mathematically simpler and easier to compute than the discrete-time loss.
A BFN trained with continuous-time loss can be run for any number of discrete steps during inference and sampling, with performance improving as the number of steps increases.

The rest of the paper is structured as follows. 
A short summary of related work is given in Section~\ref{sec:related}.
The basic framework of BFNs, along with a general derivation of the discrete and continuous time loss functions is provided in Section~\ref{sec:bfn}.
Specialisations of the framework to continuous, discretised and discrete data are provided in Sections~\ref{sec:cts}--\ref{sec:discrete}, along with pseudocode for training, evaluating and sampling from the network.
Experimental results on the CIFAR-10, dynamically binarized MNIST and text8 datasets are provided in Section~\ref{sec:experiments} and concluding remarks are given in Section~\ref{sec:conclusion}.

\section{Related Work}\label{sec:related}
Of existing methods, Bayesian Flow Networks are most closely related to diffusion models.
However the two differ in some crucial aspects.
Most obviously BFNs embody a function from one distribution to another --- rather than from data to a distribution, like diffusion models and most other probabilistic networks. 
One advantage of this approach is that, because the parameters of a categorical distribution are real-valued probabilities, the inputs to the network are continuous even when the data is discrete. 
This contrasts with discrete diffusion, which natively uses discrete samples as input~\citep{sohl2015deep,hoogeboom2021,austin2021d3pm}.

Numerous authors have proposed continuous variants of discrete diffusion.
Typically these rely either on mapping to and from a continuous embedding space~\citep{strudel2022self,li2022diffusionlm,dieleman2022continuous,chen2022analog}, or on restricting continuous diffusion to the probability simplex~\citep{richemond2022categorical,mahabadi2023tess,lou2023reflected}.
While we do not directly compare against the above methods, we note that continuity is an inherent property of the Bayesian Flow framework (the network inputs automatically lie on the probability simplex by virtue of being the parameters of a categorical distribution), rather than a constraint added to an existing system.
As well as reducing the number of free parameters and design choices (e.g. the continuous embedding space, the mapping functions), this ensures that BFNs directly optimise the negative log-likelihood of discrete data, unlike continuous diffusion methods for discrete data, which typically require either simplified loss functions~\citep{mahabadi2023tess} or auxiliary loss terms~\citep{li2022diffusionlm} to make learning stable.

For continuous data, BFNs are most closely related to variational diffusion models~\citep{kingma2021variational}, with a very similar continuous-time loss function.
The main difference in this case is that the network inputs are considerably less noisy in BFNs than in variational diffusion and other continuous diffusion models. 
This is because the generative process of BFNs begins with the parameters of a fixed prior, whereas that of diffusion models begins with pure noise.
We hypothesise that the reduction in noise could lead to faster learning on large datasets where the model underfits; however we have yet to test this hypothesis experimentally.

Another key difference from diffusion models is that there is no need to define and invert a forward process for BFNs, which arguably makes it easier to adapt them to different distributions and data types.
We showcase this flexibility by adapting BFNs to continuous, discretised and discrete data, with minimal changes to the training procedure.
This contrasts with e.g.\ discretised diffusion, which requires carefully defined transition matrices~\citep{austin2021d3pm}.
\section{Bayesian Flow Networks}\label{sec:bfn}
This section covers the basic mathematical formalism of Bayesian Flow Networks, laying out the structure of the various functions and distributions required by the model, along with the discrete and continuous-time loss functions used for training.
Specific instantiations of the general framework for continuous, discretised and discrete data are given in Sections~\ref{sec:cts}--\ref{sec:discrete}. 
\subsection{Input and Sender Distributions}
Given -dimensional data , let  be the parameters of a factorised \emph{input distribution} , with

For example,  may consist of the probabilities of a categorical distribution. 
Let  be a similarly factorised \emph{sender distribution} with  and

where  is an \emph{accuracy} parameter defined such that when , the sender samples are entirely uninformative about  and as  increases the samples become progressively more informative.
\subsection{Output Distribution \texorpdfstring{}{}}
During the data transmission process, the input parameters  are passed along with the process time  as input to a neural network .
The network then emits an output vector  which is used to parameterise an \textit{output distribution} factorised in the same way as the input and sender distributions:

As discussed in the introduction, the key difference between the input and output distributions is that while each  depends only on information gathered via  about , each  depends (via the network) on all of  and hence all of .
The output distribution, unlike the input distribution, can therefore exploit context information, such as surrounding pixels in an image or related words in a text.
\subsection{Receiver Distribution \texorpdfstring{}{}}
Given sender distribution  and output distribution  the \emph{receiver distribution} over  is defined as

Intuitively this can be understood as a receiver who knows the form of the sender distribution  but does not know , and therefore integrates over all , and hence all possible sender distributions, weighted by the probability  given to  by the output distribution .
The receiver distribution therefore combines two sources of uncertainty: the ``known unknown'' of the sender distribution entropy (which is a function of ), and the ``unknown unknown'' of the output distribution entropy.
\subsection{Bayesian Updates}
Given parameters  and sender sample  drawn with accuracy  the \emph{Bayesian update function}  is derived by applying the rules of Bayesian inference to compute the updated parameters : 
 
The \emph{Bayesian update distribution}  is then defined by marginalizing out :

where  is the multivariate Dirac delta distribution centred on the vector .
In Sections~\ref{sec:cts_additive} and \ref{sec:disc_additive} we will prove that both forms of  considered in this paper have the following property: the accuracies are additive in the sense that if  then

It follows from this property that given prior input parameters , the probability of observing parameters  after drawing a sequence of  sender samples  with accuracies  is

\subsection{Accuracy Schedule \texorpdfstring{}{}}
By performing an infinite number of transmission steps, the Bayesian update process can be generalized to continuous time.
Let  be the process \textit{time} and let  be the \emph{accuracy rate} at time .
Now define the \emph{accuracy schedule}  as 

It follows from the above definitions that  is a monotonically increasing function of , that , and that .

Specific forms of  for continuous and discrete data are provided in Sections~\ref{sec:cts_beta} and \ref{sec:disc_beta}. 
Both are derived using simple heuristics, with a deeper investigation left for future work.
\subsection{Bayesian Flow Distribution \texorpdfstring{}{}}
Given prior parameters , Bayesian update distribution  and accuracy schedule , the \emph{Bayesian flow distribution}  is the marginal distribution over input parameters at time , defined by

\subsection{Loss Function \texorpdfstring{}{}}
Given prior parameters  and accuracy schedule , consider a sequence of  sender samples  sampled at times  where . The sender distribution at step  is 
where

the receiver distribution at step  is , 
and the input parameter sequence  is recursively calculated from

Define the -step \textit{discrete-time loss}  as the expected number of nats required to first transmit , and the \textit{reconstruction loss}  as the expected number of nats required to then transmit .
Since --- using a bits-back coding scheme~\citep{hinton1993keeping, duda2009asymmetric} --- it requires  nats to transmit a sample from  to a receiver with ,

where

and since the number of nats needed to transmit  using an arithmetic coding scheme~\citep{witten1987arithmetic} based on  is , and the marginal probability of  is given by ,

Note that  is not directly optimised in this paper; however it is indirectly trained by optimising  since both are minimised by matching the output distribution to the data.
Furthermore, as long as  is high enough, the input distribution at  will be very close to , making it trivial for the network to fit .

The loss function  is defined as the total number of nats required to transmit the data, which is the sum of the n-step and reconstruction losses:

Alternatively   can be derived as the loss function of a variational autoencoder (VAE;~\citep{kingma2013auto}). Consider the sequence  as a latent code with posterior probability given by

and autoregressive prior probability given by

Then, noting that the decoder probability , the complete transmission process defines a VAE with loss function given by the negative variational lower bound (VLB)

\subsection{Discrete-Time Loss \texorpdfstring{}{}}
Eq.~\ref{disc_t_loss_n_step} can be rewritten as

where  is the uniform distribution over the integers from 1 to .
Furthermore, it follows from Eqs.~\ref{updateseq} and ~\ref{param_flow_dist} that

and hence

which allows us approximate  via Monte-Carlo sampling without computing the -step sum.
\subsection{Continuous-Time Loss \texorpdfstring{}{}}
Eq.~\ref{disc_t_loss_exp} can be used to train the network directly.
However this presupposes that  is fixed during training.
Furthermore, for discrete and discretised data the KL terms do not have analytic solutions, leading to noisy gradient estimates.

Inspired by Variational Diffusion Models~\cite{kingma2021variational} we derive a continuous-time loss function  by taking the limit of  as .
This turns out to be mathematically simpler than the discrete-time loss, as well as removing both the noisy gradients for the discrete and discretised KL terms and the need to fix  during training.

Let

Then, from the definition of  in Eq.~\ref{disc_t_loss_exp},

where  is the continuous uniform distribution over the interval .
As we will see, for all the sender, receiver distribution pairs in this paper,

where  is a function from data space to sender space,  is a distribution over  with finite expectation and variance,  denotes the convolution of two probability distributions and  is a scalar constant.

The following proposition is now required:
\begin{proposition}\label{proposition}
For a continuous univariate probability distribution  with finite expectation  and variance , the convolution  as .
\end{proposition}

\begin{proof}
Let  be some variance in the interval  and consider the sequence of random variables  where  and  for . Define

It follows from the definition of convolution that . 
Since  as , and , the result is proved if it can be shown that as ,  or equivalently .

\sloppy The Lyapunov central limit theorem~\citep{georgii2008stochastics} states that if there exists  such that  then .
First note that  as .
Hence if  then .
Now set  and observe that for ,  is the third moment of the half-normal distribution, which is .
Our choice of  therefore ensures that  for .
Also note that  and, since  and  are finite,  for some constant .
Hence

\end{proof}
It follows from the continuity of  and Eq.~\ref{deltat} that  as .
Therefore, Proposition \ref{proposition} can be applied to Eq.~\ref{convkl} to yield

where 

Therefore,

Substituting from Eq.~\ref{deltat},

and hence

\subsection{Sample Generation}
Given prior parameters , accuracies  and corresponding times , the n-step sampling procedure recursively generates  by sampling  from , 
 from  (meaning that  --- see Eq.~\ref{r_dist}), then setting 
.
Given  the network is run one more time and the final sample is drawn from .
\section{Continuous Data}\label{sec:cts}
For continuous data  and hence .
In our experiments,  is normalised to lie in  to ensure that the network inputs remain in a reasonable range; however this is not essential for the mathematical framework.
\subsection{Input Distribution \texorpdfstring{}{}}\label{sec:cts_input}
The input distribution for continuous data is a diagonal normal:

where  is the  identity matrix. 
We define the prior parameters as

where  is the length  vectors of zeros.
Hence the input prior is a standard multivariate normal:

The usual Bayesian approach would be to fit the prior mean and variance to the training data. 
However we found that a standard prior worked better in practice, as well as simplifying the equations. 
It is important to remember that the distributions  are never used directly to make predictions, but rather to inform the network's predictions. 
All that matters is that the parameters fed into the network accurately and accessibly encode the information received so far about . 
The network can easily learn the empirical prior of the training set and use that to correct its predictions.
\subsection{Bayesian Update Function \texorpdfstring{}{}}
Given a univariate Gaussian prior  over some unknown data  it can be shown~\citep{murphy2007conjugate} that the Bayesian posterior after observing a noisy sample  from a normal distribution  with known precision  is , where

Since both  and  distributions are normal with diagonal covariance,  Eqs.~\ref{alpha_update} and \ref{mean_update} can be applied to obtain the following Bayesian update function for parameters  and sender sample  drawn from :

with

\begin{figure}[t!]
\includegraphics[width=\textwidth]{figures/bayesian_updates_cts}
\caption{\textbf{Bayesian updates for continuous data}. For univariate data , the initial input distribution parameters  are updated to , ,  by iterating Eqs.~\ref{cts_precision_y_update} and \ref{cts_mean_y_update} with sender samples , ,  drawn with accuracies , ,  respectively. Note how the input mean (, , ) stochastically approaches the data, while the input precision smoothly increases.}
\end{figure}
\subsection{Bayesian Update Distribution \texorpdfstring{}{}}
Eq.~\ref{cts_mean_y_update} computes  given a single sample  from the sender distribution.
To marginalise over  as defined in Eq.~\ref{param_update_dist}, the following standard identity for normal distributions can be applied:

Substituting , , ,  and , Eq.~\ref{cts_mean_y_update} gives:

and therefore (since  is the only random part of )

\begin{figure}[t]
\includegraphics[width=\textwidth]{figures/bayesian_update_dist_cts}
\caption{\textbf{Bayesian update distribution for continuous data}. For , the plot shows the distribution  over input mean  from Eq.~\ref{cts_input_mean_distribution} given initial parameters  and 11  values spaced log-linearly between  and . Note how the distribution is tightly concentrated around  for very low alpha, then smoothly progresses to a tight concentration around  for high alpha.}
\end{figure}
\subsection{Additive Accuracies}\label{sec:cts_additive}
We can check that the sender accuracies are additive in the sense required by Eq.~\ref{additive} by first observing that if  is drawn from  then

Define

and apply Identity~\ref{normal_identity_1} with  and  to see that

Now observe that if  is drawn from  then

and hence

where

Another standard identity for Gaussian variables can now be applied:

to see that 

and hence

as required. 
\subsection{Accuracy Schedule \texorpdfstring{}{}}\label{sec:cts_beta}
We derive  for continuous data by requiring that the expected entropy of the input distribution linearly decreases with .
Intuitively, this means that information flows into the input distribution at a constant rate.
Define

Then if  linearly decreases with ,

Define  to be the standard deviation of the input distribution at . 
We will choose  empirically to minimise the loss; in general it should be small enough to ensure that the reconstruction loss is low, but not so small as to create unnecessary transmission costs.
Recalling that the precision  at time  is , we see that

Therefore

\subsection{Bayesian Flow Distribution \texorpdfstring{}{}}
Recall from Eq.~\ref{param_flow_dist} that

Therefore, setting  and  in Eq.~\ref{cts_update_dist}, and recalling that ,

where

\begin{figure}[t!]
\includegraphics[width=\textwidth]{figures/bayesian_flow_cts}
\caption{\textbf{Bayesian flow for continuous data}. For ,  and  defined as in Eqn.~\ref{cts_gamma_t}, the plot shows stochastic parameter trajectories for the input distribution mean  (white lines) superimposed on a log-scale heatmap of the Bayesian flow distribution . Note how the trajectories all begin at  then fan out before converging on .}
\label{fig:cts_param_flow}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/input_variance}
\caption{\textbf{Input variance for Bayesian Flow Networks and diffusion models}. For  and  defined as in Eqn.~\ref{cts_gamma_t}, the blue line shows the variance  of the distribution over the input mean  as a function of  (see Eq.~\ref{cts_param_flow_dist}). Note that the variance is 0 at  (since the input prior  is deterministic) and becomes small again as  approaches 1 and  becomes increasingly concentrated around the data. The green and red lines show the equivalent network input variance for two different noise schedules from the literature (linear~\citep{ ho2020denoising} and cosine~\citep{ nichol2021improved}) during the reverse process of a diffusion model (note that  is reversed relative to diffusion convention). The input variance is much lower for Bayesian Flow Networks.}
\end{figure}
\subsection{Output Distribution \texorpdfstring{}{}}\label{sec:cts_output}
Following standard practice for diffusion models~\citep{song2020score}, the output distribution is defined by reparameterising a prediction of the Gaussian noise vector  used to generate the mean  passed as input to the network.
Recall from Eq.~\ref{cts_param_flow_dist} that

and hence

The network outputs an estimate  of  and this is transformed into an estimate  of  by 

Given  the output distribution is

Note that , making the transformation from  to  undefined at . 
We therefore set  for  under some small threshold .
Also,   is clipped to lie within the allowed range  for .
In our experiments  and .
\subsection{Sender Distribution \texorpdfstring{}{}}\label{sec:cts_sender}
The sender space  for continuous data, and the sender distribution is normal with precision :

\subsection{Receiver Distribution \texorpdfstring{}{}}
Substituting Eqs.~\ref{cts_p_dist} and  \ref{cts_q_dist} into Eq.~\ref{r_dist},

\begin{figure}[t!]
\includegraphics[width=\textwidth]{figures/sender_output_receiver_cts}
\caption{\textbf{Sender, output and receiver distributions for continuous data}. Note that the sender and receiver distributions have identical variance and the output distribution is a Dirac delta distribution centred on the network prediction .}
\end{figure}
\subsection{Reconstruction Loss \texorpdfstring{}{}}\label{sec:cts_reconstruction}
Truly continuous data requires infinite precision to reconstruct, which makes the reconstruction loss problematic.
However it would be reasonable to assume that either the data is finely discretised (as all information is on a digital computer), or that it contains some noise.
The reconstruction loss for discretised data is presented in Section~\ref{sec:discd_reconstruction}. 
Alternatively, if we assume the presence of normally distributed measurement noise on , with fixed isotropic variance , then a noisy version of the reconstruction loss can be defined as the expected KL divergence between  and the output distribution at :

The noise does not directly affect training, as the reconstruction loss is not optimised. 
However the value of  places a natural upper limit on the value that should be chosen for : there is no point transmitting the data to greater precision than it was originally measured.
Empirically, we find that when  the reconstruction loss is very small.
\subsection{Discrete-Time Loss \texorpdfstring{}{}}\label{sec:cts_disc_t_loss}
From Eqs.~\ref{cts_q_dist} and \ref{ctsrecdist},

and from Eqs.~\ref{alpha_i} and \ref{cts_beta_t},

Therefore, substituting into Eq.~\ref{disc_t_loss_exp},

where .
\subsection{Continuous-time Loss \texorpdfstring{}{}}\label{sec:ctsctstloss}
Eq.~\ref{convkl} claimed that

for some embedding function , constant  and distribution  over  with finite mean and variance.
If  is the identity function,  and 

then  has finite mean and variance and

so the claim is true and the continuous-time loss from Eq~\ref{cts_t_loss} applies, with 
and  as defined in Eq~\ref{ctsalphat}, yielding

\subsection{Pseudocode}
Pseudocode for evaluating the -step loss  and continuous-time loss  for continuous data is presented in Algorithms~\ref{alg:n_step_loss_cts} and \ref{alg:cts_t_loss_cts}, while the sample generation procedure is presented in Algorithm~\ref{alg:samp_gen_cts}.
\begin{algorithm}[H]
\begin{algorithmic}
\LineComment{Note that , but  is fully determined by }
\LineComment{For our experiments , }
\Function{\lstinline{cts_output_prediction}}{, , }
\If{}
\State 
\Else
\State Input  to network, receive  as output
\State 
\State clip  to 
\EndIf
\State \textbf{Return} 
\EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Discrete-Time Loss  for Continuous Data}\label{alg:n_step_loss_cts}
\begin{algorithmic}
\State \textbf{Require:} , number of steps 
\State \textbf{Input:} continuous data 
\State 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Continuous-Time Loss  for Continuous Data}\label{alg:cts_t_loss_cts}
\begin{algorithmic}
\State \textbf{Require:} 
\State \textbf{Input:} continuous data 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Sample Generation for Continuous Data}\label{alg:samp_gen_cts}
\begin{algorithmic}
\State \textbf{Require:} , number of steps 
\State 
\State 
\For{ to } 
    \State 
    \State 
    \State 
    \State 
    \State 
    \State 
\EndFor
\State 
\State \textbf{Return} 
\end{algorithmic}
\end{algorithm}
\section{Discretised Data}\label{sec:discretised}
This section considers continuous data that has been discretised into  bins.
For example, 8-bit images are discretised into 256 bins, 16-bit audio is discretised in  bins.
This data is represented by tiling  into  intervals, each of length .
Let ,  and  denote respectively the left, centre and right of interval , and let  denote the set of integers from 1 to .
Then for ,
 
Let  be the vector of the indices of the bins occupied by , and let ,  and  be the corresponding vectors of left edges, centres and right edges of the bins.
If the data has not already been discretised, we set .
For example if the red channel in an 8-bit RGB image has index 110, it will be represented by the number .
Note that each  therefore lies in the range  and not .

The input distribution , prior parameters , sender distribution , Bayesian update function , Bayesian update distribution , Bayesian flow distribution  and accuracy schedule  are all identical to the continuous case described in Section~\ref{sec:cts}. 
It may surprise the reader that the output distribution is discretised while the input, sender and receiver distributions are not.
We made this choice partly for mathematical convenience (Bayesian updates are considerably more complex for discretised distributions;~\citep{austin2021d3pm}) and partly because we suspected that it would easier for the network to interpret continuous means than discrete probabilities as input.
In a similar vein to our argument for standard priors in Sec.~\ref{sec:cts_input}, we remind the reader that the input distribution only serves to inform the network and not directly to model the data; all that matters is that the input parameters contain enough information to allow the network to make accurate predictions.

Section~\ref{sec:cts_disc_t_loss} noted that the level of measurement noise assumed for continuous data should inform the choice of standard deviation  for the input distribution at  (which in turn defines the accuracy schedule ).
For discretised data a similar role is played by the width of the discretisation bins, as these place a natural limit on how precisely the data needs to be transmitted.
For example, for -bit data with 256 bins and hence a bin width of , setting  corresponds to a final input distribution with standard deviation roughly one eighth of the width of the bin, which should be precise enough for the network to identify the correct bin with very high probability.

One caveat with discretisation is that calculating the loss has  computational cost, which may be prohibitive for very finely discretised data. 
In any case, the benefits of discretisation tend to decrease as the number of bins increases, as we will see in our experiments.
\begin{figure}[t!] 
\includegraphics[width=\textwidth]{figures/output_discretised}
\caption{\textbf{Output distribution for discretised data}. For univariate data  discretised into  bins, the green line shows the continuous distribution  that is discretised to yield the output distribution , as described in Section~\ref{sec:discd_output}. Bin boundaries are marked with vertical grey lines. The heights of the green bars represent the probabilities assigned to the respective bins by . For ease of visualisation these heights are rescaled relative to the probability density, as indicated on the right axis. Note the clipping at : the area under the dotted green line to the left of  is added to the probability of the first bin, the area under the dotted green line to the right of  is added to the probability of the last bin.}
\label{fig:discd_p}
\end{figure}
\subsection{Output Distribution \texorpdfstring{}{}}\label{sec:discd_output}
Discretised continuous distributions offer a natural and expressive way to model discretised data with neural networks~\cite{salimans2017pixel}. 
As in Section~\ref{sec:cts_output}, the network outputs  are not used to predict  directly, but rather to model the Gaussian noise vector  used to generate the mean sample  passed as input to the network.

First  is split into two length  vectors,  and .
Then these are transformed to  and  using

For each , define the following univariate Gaussian cdf

and clip at  to obtain

Then, for ,

and hence

\subsection{Receiver Distribution \texorpdfstring{}{}}
Substituting Eq.~\ref{discd_p_dist} and Eq. \ref{cts_q_dist} into Eq.~\ref{r_dist} gives

\begin{figure}[t!]
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{figures/sender_output_receiver_discretised_1}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{figures/sender_output_receiver_discretised_2}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{figures/sender_output_receiver_discretised_3}
\end{subfigure}
\caption{\textbf{Sender, output and receiver distributions for discretised data}. For data  discretised into 8 bins, the three plots depict the sender distribution (red line), the discretised output distribution (green bars; heights reflect the probabilities assigned to each bin, rescaled as in Figure~\ref{fig:discd_p}) and receiver distribution (blue line) for progressively increasing values of , and for progressively more accurate predictions of  (both of which typically happen as  increases). Also shown are the continuous distribution  (dotted green line) which is discretized to create the output distribution and the continuous receiver distribution from Section~\ref{sec:cts} (dashed orange line). Bin boundaries are marked with vertical grey lines. Note the KL divergences printed in the top right: taking discretisation into account leads to a lower KL due to the density ``bumps'' at the bin centres where  could be. The advantage of discretisation becomes more pronounced as the prediction gets closer to  and more of the probability mass is concentrated in the correct bin.}
\end{figure}
\subsection{Reconstruction Loss \texorpdfstring{}{}}\label{sec:discd_reconstruction}
The reconstruction loss for discretised data is

\subsection{Discrete-time Loss \texorpdfstring{}{}}
From Eqs.~\ref{cts_q_dist} and \ref{discd_r_dist_1},

which cannot be calculated in closed form, but can be estimated with Monte-Carlo sampling.
Substituting into Eq.~\ref{disc_t_loss_exp},

\subsection{Continuous-time Loss \texorpdfstring{}{}}
Justifying the claim made in Eq.~\ref{convkl} follows almost the same reasoning here as in Section~\ref{sec:ctsctstloss}, with  and  the identity function. 
The only difference is that

which clearly has finite variance and mean.
Since

the claim holds and the continuous time loss from Eq~\ref{cts_t_loss} can be applied with

and  as defined in Eq~\ref{ctsalphat}, yielding

Note that  is a function of the complete discretised distribution , hence  depends on both  and , and not only on , as for continuous data. This also means that calculating  has  computational cost for discretised data.
\subsection{Pseudocode}
Pseudocode for evaluating the discrete-time loss  and continuous-time loss  for discretised data is presented in Algorithms~\ref{alg:n_step_loss_discd} and \ref{alg:cts_t_loss_discd}, while sample generation is presented in Algorithm~\ref{alg:samp_gen_discd}.
\begin{algorithm}[H]
\begin{algorithmic}
\Function{\lstinline{discretised_cdf}}{}
\State 
\State 
\State \textbf{Return} 
\EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\begin{algorithmic}
\LineComment{For our experiments }
\LineComment{, }
\Function{\lstinline{discretised_output_distribution}}{, }.
\If{}
\State 
\State 
\Else
\State Input  to network, receive  as output
\State 
\State 
\EndIf
\For{, }
\State 
\EndFor
\State \textbf{Return} 
\EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Discrete-Time Loss  for Discretised Data}\label{alg:n_step_loss_discd}
\begin{algorithmic}
\LineComment{}
\State \textbf{Require:} , number of steps , number of bins 
\State \textbf{Input:} discretised data 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Continuous-Time Loss  for Discretised Data}\label{alg:cts_t_loss_discd}
\begin{algorithmic}
\State \textbf{Require:} , number of bins 
\State \textbf{Input:} discretised data 
\State 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Sample Generation for Discretised Data}\label{alg:samp_gen_discd}
\begin{algorithmic}
\LineComment{}
\State \textbf{Require:} , number of steps , number of bins 
\State 
\State 
\For{ to } 
    \State 
    \State 
    \State 
    \State 
    \State 
    \State 
\EndFor
\State 
\State \textbf{Return} 
\end{algorithmic}
\end{algorithm}
\section{Discrete Data}\label{sec:discrete}
We now consider discrete data in which no meaningful order or distance exists between the classes, unlike the discretised continuous data covered in the previous section.
Some obvious examples are text characters, classification labels or any binary data. 
In this context the data is represented as a  dimensional vector of class indices:  , where  is the set of integers from  to .
\subsection{Input Distribution \texorpdfstring{}{}}\label{sec:disc_input}
For discrete data, the input distribution is a factorised categorical over the class indices.
Let  with , where  is the probability assigned to class  for variable . 
Then

The input prior is uniform with

where  is the length  vector whose entries are all .
We chose a uniform prior---rather than an empirical prior fit to the training data---for the same reasons we chose a standard normal prior for continuous data: it's mathematically simpler, and the disparity between the true prior and the simple prior can easily be corrected by the network.
\subsection{Output Distribution \texorpdfstring{}{}}\label{sec:disc_output}
Given data , network inputs  and corresponding network outputs , the output distribution for discrete data is as follows:

Note that for binary data only the probability  that  is fed into the network, on the grounds that the probability of  can easily be inferred from .
The output distribution for binary data is determined by applying the logistic sigmoid function elementwise to the length  output vector to get the probability for :

where

then inferring the probabilities for  from

In principle one class could also be removed from the inputs and outputs when  and inferred from the others.
However this would require the network to internalise a slightly more sophisticated inference procedure that could potentially slow down learning.
We therefore followed deep-learning convention and included a redundant input and output unit for .

All probabilities are rescaled to the range  by multiplying by two then subtracting one before feeding them into the network.
\subsection{Sender Distribution \texorpdfstring{}{}}\label{sec:disc_sender}
Given , and a vector of  class indices , let

where  is the Kronecker delta function.
Clearly  and , so the vector

defines a valid distribution over  classes.
To simplify notation we will from now on drop the superscripts and refer to  as ,  as  and so on, except where necessary to remove ambiguity.

Consider a vector of integer counts , corresponding to the number of times each of the  classes is observed among  independent draws from .
Then the probability of observing  is given by the following multinomial distribution:

Now consider the fraction  of observations of class  in .
Clearly

meaning that for any finite  it would be possible to deduce from  what the value of  is if  is sufficiently large.
However as  shrinks,  becomes closer to uniform, meaning that a larger  is required to unambigously identify  from . 
By defining the accuracy  and sending  (and hence  for any finite ),  can therefore be used to define a continuous-valued sender distribution that smoothly varies from totally uninformative at  to totally informative as , like the sender distribution for continuous data.

It can be proved from the central limit theorem that for any set of discrete probabilities , where  , that if  then in the limit  the following result holds~\cite{georgii2008stochastics}:

where  is the  identity matrix.
Therefore

Now define

And the length  sender sample  as

Note that , unlike , is continuous (), and that  measures the number of times each class is observed, minus the average number of observations per class.
Intuitively,  provides information about the relative concentration of the classes among the counts, with (since ) positive values for classes observed more frequently than the mean and negative values for those observed less frequently than the mean. 
As  grows the concentration increases around the true class, and hence  become more informative about .

Rearranging Eq.~\ref{y_def},

which we can use for the following change of variables:

where we have used the fact that  and hence . 
Recall that  and hence ,
 which can be substituted into the above to yield

Substituting from Eq.~\ref{q_def},

and hence

Applying the identity  for   to  it can be seen that

and hence

Furthermore, it follows directly from Eq.~\ref{q_def} that

Now define

Plugging Eq.~\ref{gamma_limit} and \ref{q_limit} into Eq.~\ref{p_y_i_omega},

Restoring the superscript,

where  is a vector of ones,  is the identity matrix and  is the projection from the class index  to the length  one-hot vector defined by , and therefore

where .
\subsection{Receiver Distribution \texorpdfstring{}{}}
Substituting Eq.~\ref{disc_pred_dist} and Eq. \ref{disc_q_dist} into Eq.~\ref{r_dist} gives the following receiver distribution for dimension :

\subsection{Bayesian Update Function \texorpdfstring{}{}}
Recall from Section~\ref{sec:disc_input} that  is the probability assigned to  by .
Dropping the superscript and returning to the count distribution  defined in Eq.~\ref{multi_def}, the posterior probability that  after observing  is

Substituting Eq.~\ref{count_dist} into Eq.~\ref{disc_bayes} and cancelling terms in the enumerator and denominator,

Now define

Substituting the definition of  from Eq.~\ref{y_def} into the definition of  from Eq.~\ref{disc_update_param_def},

and hence, from Eq.~\ref{post_prob},

Therefore in the limit  with , the stochastic parameter update from  to  induced by drawing  from  can be sampled by first drawing  from  then setting .
Hence the Bayesian update function is 

where the redundant parameter  has been included for consistency with the update function for continuous data.
\subsection{Bayesian Update Distribution \texorpdfstring{}{}}
Substituting Eqs.~\ref{disc_q_dist} and \ref{disc_param_update_function} into Eq.~\ref{param_update_dist},

\subsection{Additive Accuracies}\label{sec:disc_additive}
It follows from the definition of the update distribution that if  is drawn from  then  is drawn from .
Furthermore, if  is drawn from  then  is drawn from .
Substituting the definition of  from Eqn~\ref{disc_update_param_def},

From Eqn.~\ref{disc_q_def_uni}

and hence, from Identity~\ref{normal_identity_2}

Therefore, if  is drawn from  and  then  is drawn from\\  and

as required. 
\subsection{Accuracy Schedule \texorpdfstring{}{}}\label{sec:disc_beta}
As with continuous data, the guiding heuristic for  was to decrease the expected entropy of the input distribution linearly with . In the continuous case, where the entropy is a deterministic function of , applying the heuristic was straightforward; in the discrete case an explicit computation of  would be needed.
We were unable to derive an analytic expression for this term, but found that

was a reasonable approximation, with  determined empirically for each experiment.
Therefore


\begin{figure}[t!]
\begin{centering}
\includegraphics[width=0.6\textwidth]{figures/k_sqrt_beta_h}
\caption{\textbf{Accuracy schedule vs. expected entropy for discrete data}. The surface plot shows the expectation over the parameter distribution  of the entropy of the categorical input distribution  for  to  and  to . The red and cyan lines highlight the entropy curves for 2 and 27 classes, the two values that occur in our experiments. The red and cyan stars show the corresponding values we chose for .}
\end{centering}
\label{fig:disc_acc_vs_entropy}
\end{figure}
\subsection{Bayesian Flow Distribution \texorpdfstring{}{}}
Substituting Eq.~\ref{disc_par_update_def} into Eq.~\ref{param_flow_dist},

Since the prior is uniform with , this reduces to

which can be sampled by drawing  from  then setting .

The sender distribution for discrete data can therefore be interpreted as a source of softmax logits for the Bayesian flow distribution; the higher the sender accuracy  is, the larger in expectation the logits corresponding to  will be in , hence the closer  will be to  and the more information the network will gain about .
\begin{figure}[t!]
\includegraphics[width=\textwidth]{figures/bayesian_flow_discrete}
\caption{\textbf{Bayesian flow for discrete data}. For , the input distribution parameters  can be visualised as points on the 2-simplex, with the data  corresponding to the bottom left corner. For the accuracy schedule  from Eq.~\ref{disc_beta_t}, the white line shows a single input parameter trajectory starting from  and evolving under the Bayesian update distribution  from Eq.~\ref{disc_par_update_def}, superimposed on log-scale heatmaps of the Bayesian flow distribution  from Eq.~\ref{disc_param_flow}, plotted at regular intervals from  to .}
\label{fig:bayes_flow_disc}
\end{figure}
\begin{figure}[t!]
\includegraphics[width=\textwidth]{figures/bayesian_flow_binary}
\caption{\textbf{Bayesian flow for binary data}.
For the input probability  of class one, the plot shows several parameter trajectories starting from  at  and evolving under the Bayesian update distribution to , superimposed on a log-scale heatmap of the Bayesian flow distribution.
 in this plot.
Note that both here and in Figure~\ref{fig:bayes_flow_disc} the convergence towards the data appears slower and noisier than the equivalent trajectories for continuous data in Figure~\ref{fig:cts_param_flow}. 
This is a fundamental consequence of discreteness: since all points in  are equidistant the input distributions cannot concentrate on values close to  as the trajectories progress.}
\label{fig:bayes_flow_bin}
\end{figure}
\subsection{Reconstruction Loss \texorpdfstring{}{}}\label{sec:disc_reconstruction}
The reconstruction loss for discrete data is

\subsection{Discrete-time Loss \texorpdfstring{}{}}
From Eqs.~\ref{disc_q_def_uni} and \ref{disc_r_dist_uni},

Therefore, substituting into Eq.~\ref{disc_t_loss_exp},

where, from Eq.~\ref{disc_beta_t},

\subsection{Continuous-time Loss \texorpdfstring{}{}}
Let

and apply Identity~\ref{normal_identity_1} to see that if

then

and similarly if

then 

The Kullback-Leibler divergence is invariant under affine transformations of variables, hence

Now set ,  and

which has finite variance and the following finite expectation

where

The conditions in Eq.~\ref{convkl} are therefore satisfied and Eqs.~\ref{disc_p_expectation} and \ref{disc_alpha_t} can be substituted into Eq.~\ref{cts_t_loss} to yield

where

\subsection{Pseudocode}
Pseudocode for evaluating the discrete-time loss  and continuous-time loss  for discrete data is presented in Algorithms~\ref{alg:n_step_loss_disc} and \ref{alg:cts_t_loss_disc}, while sample generation is presented in Algorithm~\ref{alg:samp_gen_disc}.
\begin{algorithm}[H]
\begin{algorithmic}
\Function{\lstinline{discrete_output_distribution}}{}
\State Input  to network, receive  as output
\For{}
\If{}
\State 
\State 
\Else
\State 
\EndIf
\EndFor
\State \textbf{Return} 
\EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Discrete-Time Loss  for Discrete Data}\label{alg:n_step_loss_disc}
\begin{algorithmic}
\State \textbf{Require:} , number of steps , number of classes 
\State \textbf{Input:} discrete data 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Continuous-Time Loss  for Discrete Data}\label{alg:cts_t_loss_disc}
\begin{algorithmic}
\State \textbf{Require:} , number of classes 
\State \textbf{Input:} discrete data 
\State 
\State 
\State 
\State 
\State 
\State 
\State 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Sample Generation for Discrete Data}\label{alg:samp_gen_disc}
\begin{algorithmic}
\State \textbf{Require:} , number of steps , number of classes 
\State 
\For{ to } 
    \State 
    \State 
    \State 
    \State 
    \State 
    \State 
\EndFor
\State 
\State \textbf{Return} 
\end{algorithmic}
\end{algorithm}
\section{Experiments}\label{sec:experiments}
We evaluated Bayesian Flow Networks (BFNs) on the following generative benchmarks: CIFAR-10 (3232 8-bit color images), dynamically binarized MNIST (2828 binarized images of handwritten digits) and text8 (length 256 character sequences with a size 27 alphabet).
The continuous (Sec.~\ref{sec:cts}) and discretised (Sec.~\ref{sec:discretised}) versions of the system were compared on CIFAR-10, while the discrete version (Sec.~\ref{sec:discrete}) was applied to the other datasets.
In all cases, the network was trained using the continuous-time loss , with the discrete-time loss  evaluated for testing only, with various values of .
Standard network architectures and training algorithms were used throughout to allow for direct comparison with existing methods.
Because the focus of this paper is on probabilistic modelling rather than image generation, FID scores were not calculated. However, examples of generated data are provided for all experiments.

\begin{table}[t!]
\centering
\begin{tabular}{@{}llc@{}}
\toprule
Model                                                               & \multicolumn{1}{c}{Dynamically Binarized MNIST} & CIFAR-10 \\ \midrule
Improved DDPM \citep{nichol2021improved}                            &                                     & 2.94     \\
NVAE \citep{vahdat2020nvae}                                         & \multicolumn{1}{c}{78.01}           & 2.91     \\
PixelVAE++\textsuperscript{\dag} \citep{sadeghi2019pixelvae++}        & \multicolumn{1}{c}{78.00}           & 2.90     \\
Locally Masked PixelCNN\textsuperscript{\dag} \citep{jain2020locally} & \multicolumn{1}{c}{77.58}           & 2.89     \\
Image Transformer\textsuperscript{\dag} \citep{parmar2018image}       &                                     & 2.89     \\
DDPM++ \citep{kim2021soft}                                          &                                     & 2.88     \\
LSGM \citep{vahdat2021score}                                        &                                     & 2.87     \\
VDVAE \citep{child2020very}                                         & \multicolumn{1}{c}{}                & 2.87     \\
Sparse Transformer\textsuperscript{\dag} \citep{child2019generating}  &                                     & 2.80     \\
Reflected Diffusion \citep{lou2023reflected}                        &                                     & 2.68     \\
VDM \citep{kingma2021variational}                                   &                                     & 2.65\\    
ARDM-Upscale 4 \citep{hoogeboom2021autoregressive}                  &                                     & 2.64
\\ \midrule
\textbf{BFN}                                                        & \multicolumn{1}{c}{77.87}                &      2.66    \\ 
\midrule
CR-NVAE* \citep{sinha2021consistency}                                & \multicolumn{1}{c}{76.93}          & 2.51    \\
VDM* \citep{kingma2021variational}                                   & \multicolumn{1}{c}{}                & 2.49    \\ \bottomrule
\end{tabular}
\caption{\textbf{Comparison of dynamically binarized MNIST and CIFAR-10 results with other methods}. The best published results for both datasets (*) use data augmentation for regularization. Results for models marked with (\textsuperscript{\dag}) are exact values; all other results are upper bounds.}
\label{tab:mnist-cifar-results}
\end{table}
\subsection{Dynamically Binarized MNIST}
\begin{table}[t!]
\centering
\begin{tabular}{cccccccc}
\toprule
-steps & 10 & 25 & 50 & 100 & 784 & 1000 & \\ 
\midrule
NPI &  &  &  &  &  &  &  \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Dynamically binarized MNIST results}. NPI is nats per image  averaged over 2,000 passes through the test set with  or  sampled once per test image per pass. The reconstruction loss  (included in NPI) was . 784 is the total number of pixels per image, hence the number of steps required to generate an image with an autoregressive model.}
\label{tab:mnist_results}
\end{table}

\textbf{Data.}\quad 
The binarized MNIST benchmark data was originally created from the MNIST dataset of handwritten images \citep{lecun-mnisthandwrittendigit-2010} by treating the grayscale pixel intensities as Bernoulli probabilities and sampling a particular binarization \citep{salakhutdinov2008quantitative} which is held fixed during training.
In recent years, a variant of the same benchmark has become more popular, with a new binarization sampled from the probabilities for every training batch.
The two are not comparable, as the latter, which we refer to as dynamically binarized MNIST, effectively has a larger training set and hence gives better test set performance.
All our experiments and the results referenced from the literature use dynamically binarized MNIST.
\\

\begin{figure}[t!]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/bin_mnist_test}
  \caption{Test Data}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/bin_mnist_samp}
  \caption{Generated Data}
\end{subfigure}
\caption{\textbf{MNIST real and generated data}. Samples generated with 100 steps.}
\end{figure}

\noindent\textbf{Setup.}\quad The network architecture was based on a U-Net introduced for diffusion models~\citep{nichol2021improved}.
Starting from the hyperparameters used for the CIFAR-10 dataset (see Appendix A in the above reference), we made the following modifications: the number of resblocks was reduced from three to two and the layer widths were reduced from  to  with .
Finally, the input and output of the standard network were concatenated and projected back to the output size.
600 randomly selected training images (1\% of the training set) were used as a validation set.
The optimiser was AdamW~\citep{loshchilov2017decoupled} with learning rate , weight decay 0.01 and .
Dropout was used with probability 0.5, the training batch size was 512, and  was set to  (see Sec.~\ref{sec:disc_beta}).
The network was trained for  weight updates until early stopping.
An exponential moving average of model parameters with a decay rate of 0.9999 was used for evaluation and sample generation.
The total number of learnable parameters was approximately 25M.
\\

\begin{figure}[t!]
\centering
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/bin_mnist_input}
    \caption{Input Distribution}
\end{subfigure}
\par\bigskip
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/bin_mnist_output}
    \caption{Output Distribution}
\end{subfigure}

\caption{\textbf{MNIST Input and output distributions}. For two test set images the figure shows the white pixel probability at 20 steps evenly spaced between  and . Note how the input probabilities are initially uniform whereas the output distribution initially predicts a superposition of multiple digits, closely matching the per-pixel marginal prior over the training set: this supports our belief that the network learns to correct for the uniform prior in the input distribution. Also note that the output distribution is much less noisy than the input distribution, and that it changes more dramatically as new information is received (e.g. the network appears to switch from predicting a  to a  to a  for the first image). This highlights the network's use of context to resolve ambiguity and noise in the input distribution.}
\end{figure}

\noindent\textbf{Results.}\quad As can be seen from Table~\ref{tab:mnist-cifar-results}, BFN is close to state-of-the-art for this task with no data augmentation. 
Table~\ref{tab:mnist_results} shows the expected inverse relationship between loss and number of steps.
Direct optimisation of the -step loss would likely lead to reduced loss for low values of ; however we leave that for future work.
One issue is that the reconstruction loss was relatively high at 0.46 nats per image. 
The obvious way to decrease this would be to increase , but we found that doing so led to slower learning and worse performance.
Along with the loss curves in Figure~\ref{fig:bin_mnist_loss}, this suggests that the accuracy schedule is suboptimal for binary data.

\begin{figure}[t!]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/bin_mnist_cts_t_loss}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/bin_mnist_all_loss}
\end{subfigure}
\caption{\textbf{MNIST losses against time}. The left plot shows the mean over the test set of the cts. time loss  used for training for transmission time  between 0 and 1. The right plot shows the average cumulative value of  up to , along with the reconstruction loss  evaluated at  and the sum of these two losses, which would be the total loss if the transmission process halted at .
Note the unevenness of  against : we speculate that rescaling  to make the loss curve more uniform could improve performance.}
\label{fig:bin_mnist_loss}
\end{figure}
\subsection{CIFAR-10}
\begin{table}[t!]
\centering
\begin{tabular}{ccccc}
\toprule
-steps & Cts. (256 bins) & Discd. (256 bins) & Cts. (16 bins) & Discd. (16 bins)\\ 
\midrule
10  & 6.18 & 3.91 & 1.42 & 1.16\\ 
25  & 3.65 & 3.16 & 1.11 & 1.02\\ 
50  & 3.10 & 2.93 & 1.03 & 0.98\\ 
100 & 2.86 & 2.81 & 0.99 & 0.96 \\ 
250 & 2.73 & 2.73 & 0.97 & 0.94\\ 
500 & 2.69 & 2.71 & 0.96 & 0.94\\ 
1000& 2.67 & 2.70 & 0.96 & 0.94\\ 
\midrule
 &  2.66 & 2.68 & 0.96 & 0.94\\ 
\bottomrule
\toprule
 &  0.001 & 0.003 & 0.073 & 0.070\\ 
\midrule
Updates & 5M & 5M & 250K & 1M \\ 
\bottomrule
\end{tabular}
\caption{\textbf{CIFAR-10 results}. All losses are bits per dimension (BPD) averaged over 100 passes through the test set with  or  sampled once per test image per pass. The reconstruction losses  (included in BPD) and the number of training updates for each network are shown below.}
\label{tab:cifar_results}
\end{table}

\textbf{Data.}\quad Two sets of generative modelling experiments were conducted on the CIFAR-10 database~\citep{Krizhevsky09learningmultiple}, one at the standard bit-depth of 8, corresponding to 256 discretised bins per colour channel, and one at a reduced bit-depth of 4, corresponding to  bins per channel.
In both cases the bins evenly partitioned the interval  and the data was pre-processed by assigning each channel intensity to the nearest bin centre, as described in Section~\ref{sec:discretised}.
The purpose of comparing 16 and 256 bin discretisation was twofold: (1) to test the hypothesis that the advantage of training with the discretised loss from Section~\ref{sec:discretised} rather than the continuous loss from Section~\ref{sec:cts} would be greater when the number of bins was lower, and (2) to test whether modelling the data at lower precision would lead to improved perceptual quality.
No data augmentation, such as horizontal flips or random crops, was used on the training set.
\\

\begin{figure}[t!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cifar_test_256_bins}
  \caption{Test Data (256 bins)}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cifar_samp_256_bins}
  \caption{Generated Data (256 bins)}
\end{subfigure}
\par\bigskip
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cifar_test_16_bins}
  \caption{Test Data (16 bins)}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering 
  \includegraphics[width=0.9\linewidth]{figures/cifar_samp_16_bins}
  \caption{Generated Data (16 bins)}
\end{subfigure}
\caption{\textbf{CIFAR-10 real and generated data}. Samples generated with 4,000 steps, using networks trained with discretised loss. The same random seed was used for both sets of samples. Note the improved image quality of the 16 bin samples compared to the 256 bin samples.}
\label{fig:cifar_samples}
\end{figure}

\noindent\textbf{Setup.}\quad 
The network architecture was essentially the same as that used for Variational Diffusion Models (VDMs~\citep{kingma2021variational}), including the Fourier feature inputs.
The only modification was an extra input-output connection similar to the network for MNIST.
In total there were approximately 31M learnable parameters.
The following hyperparameters were used for all CIFAR-10 experiments:
a validation set of 500 randomly selected training images (1\% of the training set),
the  AdamW~\citep{loshchilov2017decoupled} optmizer with weight decay 0.01, learning rate  and ,
dropout with probability 0.1,
training batch size of 128,
,
, and
an exponential moving average of model parameters with a decay rate of 0.9999 for evaluation and sample generation.
For the 256 bin experiments , while for the 16 bin experiments .
For the networks trained with continuous loss, the reconstruction loss was measured using the discretised version of  from Section~\ref{sec:discd_reconstruction} rather than the continuous version from Section~\ref{sec:cts_reconstruction}, using a discretised Gaussian with mean equal to  and std.\ deviation chosen empirically to be  for 256 bins and  for 16 bins.
This ensured the results were comparable between continuous and discretised training, and consistent with the literature.
\\

\begin{figure}[t!]
\centering
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/cifar_input}
    \caption{Input Mean}
\end{subfigure}
\par\bigskip
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/cifar_output}
    \caption{Output Mean}
\end{subfigure}
\caption{\textbf{CIFAR-10 Input and output distributions}. For two test set images the figure shows the means of the input and output distributions at steps evenly spaced between  and . }
\end{figure}

\noindent\textbf{Results.}\quad Table~\ref{tab:mnist-cifar-results} shows that the best performing BFN gives 2.66 BPD for the 256 bin data, which is close to the state-of-the-art at 2.64 BPD.
The most obvious performance benchmark (given the shared network architecture and similarity in loss function) is the VDM result at 2.65 BPD~\citep{kingma2021variational}.
However this took 10M weight updates to achieve, and due to time constraints we were only able to train BFNs for 5M updates.
Validation performance was still improving after 5M updates, and it remains unclear how much performance would improve with 10M updates.

Table~\ref{tab:cifar_results} shows that discretised loss gave better performance than continuous loss for 16 bins, as well as much faster training time (250K updates vs. 1M).
This supports the hypothesis that training with discretised loss is most beneficial when the number of bins is relatively low.
Furthermore, for both 16 and 256 bins, discretised training gave much better results when the number of steps  was low (e.g. 10 or 25).
However continuous loss gave better performance than discretised loss on 256 bins (2.66 BPC vs 2.68); more investigation would be needed to understand why.

Figure~\ref{fig:cifar_samples} shows that discretised training with 16 bins gives better sample quality than training with 256 bins.
This is presumably because the loss function of the former is restricted to the first four bits of the data in which --- as can be seen by comparing the test data at 16 and 256 bins --- most of the perceptually relevant information is contained.
An interesting direction for future work would be to train one BFN to model the lower bits of an image, and a second BFN to conditionally upscale to higher bits, as has previously been explored for autoregressive models~\citep{menick2018generating,hoogeboom2021autoregressive}.

\begin{figure}[t!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cifar_cts_t_loss}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cifar_all_loss}
\end{subfigure}
\caption{\textbf{CIFAR-10 losses against time}. The plot was made using the network trained with discretised loss on 256 bins. Note the high loss at the very start of the process, which we did not observe with discrete data.}
\end{figure}

\begin{table}[t!]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
                                                 & Model                                             & BPC                    \\ \midrule
\multirow{3}{*}{Flow-based models}               & IAF/SCF\textsuperscript{\dag} \citep{ziegler2019}                       & 1.88                   \\
                                                 & Argmax Coupling Flow\textsuperscript{\dag} \citep{hoogeboom2021}        & 1.80                   \\
                                                 & Discrete Flow\textsuperscript{\dag} \citep{tran2019}                    & 1.23                   \\ \midrule
\multirow{3}{*}{Order-agnostic Models}           & OA-ARDM \citep{hoogeboom2021autoregressive}       & 1.43  0.001 \\
                                                 & MAC \citep{shih2022training}           & 1.40 \\
                                                 \midrule
\multirow{3}{*}{Diffusion models}                & Multinomial Diffusion \citep{hoogeboom2021}                   & 1.72 \\
& D3PM uniform \citep{austin2021d3pm}                   & 1.61  0.02 \\
                                                 & D3PM NN \citep{austin2021d3pm}                        & 1.59  0.03 \\
                                                 & D3PM mask \citep{austin2021d3pm}                      & 1.45  0.02 \\ \midrule
                                                 & \textbf{BFN}   & \textbf{1.41}                       \\ \midrule
Autoregressive baseline                          & Transformer\textsuperscript{\dag} \citep{austin2021d3pm}                    & 1.23                   \\
Best result*                & Adaptive Span Transformer\textsuperscript{\dag} \citep{sukhbaatar2019} & 1.07                   \\ \bottomrule
\end{tabular}
\caption{\textbf{Comparison of text8 results with other methods}. The best published model on this dataset (*) was trained on sequences of length 512. Rest of the above models were trained on sequences of length 256. Results for models marked with (\textsuperscript{\dag}) are exact values; all other results are upper bounds.
}
\label{tab:text8_comparison}
\end{table}
\subsection{text8}
\begin{table}[t!]
\centering
\begin{tabular}{cccccccc}
\toprule
-steps & 10 & 25 & 50 & 100 & 256 & 1000 & \\ 
\midrule
BPC & 1.70 & 1.52 & 1.47 & 1.43 & 1.42 & 1.41 & 1.41 \\ 
\bottomrule
\end{tabular}
\caption{\textbf{text8 results}. BPC is bits per character averaged over 1M randomly cropped sequences from the test set with  or  sampled once per crop. The reconstruction loss  (included in BPC) was .}
\label{tab:text8_results}
\end{table}

\noindent\textbf{Data.}\quad The text8 dataset~\citep{mahoney09ltcb} was derived from a subset of the enwik9 Wikipedia dataset by removing punctuation and restricting the text to lowercase Latin letters and spaces, giving an alphabet of size 27.
For clarity, we represent the space character with an underscore in figures.
\\

\begin{figure}[t!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/text8_test}
  \caption{Test Data}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/text8_samp}
  \caption{Generated Data}
\end{subfigure}
\caption{\textbf{text8 real and generated data.} Samples generated with 1000 steps.}
\end{figure}

\noindent\textbf{Setup.}\quad The network architecture was a Transformer similar to the small model () used by \citet{radford2019language} except that it uses the GELU activation function \citep{hendrycks2016gaussian} and the depth was increased to 24 layers.
The input and output of the Transformer were concatenated and then projected back to the output size to produce the final output.
The standard training/validation/test split of 90M/5M/5M consecutive characters was used, and
the network was trained with a batch size of 3328 sequences of length 256, randomly cropped from the training set, for 1.2\,M weight updates using the AdamW optimizer\citep{loshchilov2017decoupled}. 
The learning rate was set to , weight decay to 0.1 and  to .
An exponential moving average of model parameters with a decay rate of 0.9999 was used for evaluation and sample generation.
Dropout was not used, but overfitting was observed towards the end of training indicating that regularization may further improve results.
 was 0.75.
The total number of learnable parameters was approximately 170M.
Note that the  batch size and number of layers were larger than prior results from diffusion models. 
The first choice increases model capacity while the second tends to make overfitting more likely.
These choices were made to maximize the utilization of available resources while achieving results in reasonable time.
\\

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/text8_in_out}
\caption{\textbf{text8 Input and Output Distributions}. The heatmaps show the character probability distributions across part of a test sequence at various times during the flow process. Whereas the expected entropy for each letter decreases independently in the input distribution, the entropy of the output distribution tends to chunk into words and phrases --- e.g. the date ``one\_five\_six\_one'' is confidently predicted early in the process.}
\end{figure}

\noindent\textbf{Results.}\quad
Table~\ref{tab:text8_comparison} shows that BFN yielded a  1.41 BPC on the text8 test set, which is better than all discrete diffusion models we found in the literature, and close to the best order-agnostic model, MAC at 1.40 BPC.
We note however that both a standard autoregressive baseline and a discrete flow model perform substantially better at 1.23 BPC.
Table~\ref{tab:text8_results} shows that performance is reasonably robust to decreased , with only 100 steps required to reach 1.43 BPC.
This result could probably be improved by training with the discrete-time loss.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/text8_in_out_chars}
\caption{\textbf{text8 Input and Output Distributions}. An alternative visualisation with the character sizes scaled in proportion to their probability.}
\end{figure}
\section{Conclusion}\label{sec:conclusion}
This paper introduced Bayesian Flow Networks, a new class of generative model that combines Bayesian inference with neural networks in an iterative modelling process.
Discrete and continuous-time loss functions were derived along with sampling procedures, and the model was succesfully applied to continuous, discretised and discrete data.
We hope this work will inspire fresh perspectives and new directions for generative modelling research.
\section*{Ackowledgements}\label{sec:acknowledgements}
We would like to thank Vojtech Micka for his invaluable engineering and infrastructure support.
\bibliographystyle{plainnat}
\bibliography{bibliography}
\end{document}
