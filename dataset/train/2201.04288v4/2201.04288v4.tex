\section{Experiments}
\subsection{Experimental setup}

\paragraph{Model variants} For the backbone of each view, we consider five ViT variants, ``Tiny'', ``Small'', ``Base'', ``Large'', and ``Huge''. Their settings strictly follow the ones defined in BERT~\cite{devlin_naacl_2019} and ViT~\cite{dosovitskiy2020image, steiner2021train}, \ie number of transformer layers, number of attention heads, hidden dimensions.
See Appendix \ref{sec: model_configs} for the detailed settings.
For convenience, each model variant is denoted with the following abbreviations indicating the backbone size and tubelet length.
For example, B/2+S/4+Ti/8 denotes a three-view model, where a ``Base'', ``Small'', and ``Tiny'' encoders are used to processes tokens from the views with tubelets of sizes , , and , respectively.
Note that we omit 16 in our model abbreviations because all our models use  as the spatial tubelet size except for the ``Huge'' model, which uses , following ViT~\cite{dosovitskiy2020image}.
All model variants use the same global encoder which follows the ``Base'' architecture, except that the number of heads is set to 8 instead of 12.
The reason is that the hidden dimension of the tokens should be divisible by the number of heads for multi-head attention, and the number of hidden dimensions across all standard transformer architectures (from ``Tiny'' to ``Huge''~\cite{steiner2021train, dosovitskiy2020image}) is divisible by 8.



\begin{table*}[t]
    \vspace{-0.5\baselineskip}
	\begin{subtable}[t]{.31\linewidth}
		\centering
		\caption{Effects of different model-view assignments.
		}
		\vspace{-0.1\baselineskip}
    	\setlength{\tabcolsep}{4pt} \renewcommand*{\arraystretch}{1.11}  \scriptsize{
			\begin{tabular}{  l c c c }
			    \toprule
                Model variants & GFLOPs & MParams & Top-1 \\
                \midrule
                B/8+Ti/2 & 81 & 161 &77.3  \\ B/2+Ti/8 & 337 & 221 & 81.3 \\ \midrule
                B/8+S/4+Ti/2 & 202 & 250 & 78.5 \\ 
                B/2+S/4+Ti/8 & 384 & 310 & 81.8 \\ B/4+S/8+Ti/16 & 195 & 314 & 81.1  \\ \bottomrule
             \end{tabular}
		}
		\label{tab:model_view_assignment}
		\vspace{1.\baselineskip}
		\centering
		\caption{Effects of the same model applied to different views.}
		\vspace{-0.3\baselineskip}
		\scriptsize{
			\begin{tabular}{  l c c c }
			    \toprule
                Model variants & GFLOPs & MParams & Top-1 \\
                \midrule
                B/4+S/8+Ti/16 & 195 & 314 & 81.1 \\ B/4+B/8+B/16 & 324 & 759 & 81.1\\ \midrule
                B/2+Ti/8 & 337 & 221 & 81.3 \\ B/2+B/8 & 448 & 465 & 81.5\\ \midrule
                B/2+S/4+Ti/8 & 384 & 310 & 81.8 \\ B/2+B/4+B/8 & 637 & 751 & 81.7 \\ \bottomrule
             \end{tabular}
		}
		\label{tab:same_model_different_views}
	\end{subtable}
  	\hfill
  	\begin{subtable}[t]{.37\linewidth}
		\centering
  		\caption{Comparison of different cross-view fusion methods.}
  		\vspace{-0.1\baselineskip}
  		\setlength{\tabcolsep}{4pt} \scriptsize{
	  		\begin{tabular}{  l  c  c  c  l }
	  		    \toprule
                Model variants & Method & GFLOPs & MParams & Top-1 \\
                \midrule
                B/4 & \multirow{3}{*}{N/A} & 145 & 173 & 78.3 \\ S/8 &  & 20 & 60 & 74.1 \\ Ti/16 &  & 3 & 13 & 67.6 \\ \midrule
                \multirow{5}{*}{B/4+S/8+Ti/16} & Ensemble & 168 & 246 & 77.7   \\ & Late fusion & 187 & 306 & 80.6  \\ & MLP & 202 & 323 & 80.6  \\ & Bottleneck & 188 & 306 & 81.0  \\ 
                & CVA & 195 & 314 & \textbf{81.1}  \\ \bottomrule
    \end{tabular}
	\label{tab:fusion_methods}
  	}
    \vspace{1.05\baselineskip}
    \centering
	\scriptsize{
	\caption{Comparison to SlowFast multi-resolution method. }
	\vspace{-0.1\baselineskip}
    \begin{tabular}{  l  c  c  c }
      \toprule
      Model variants & GFLOPs & MParams & Top-1 \\
      \midrule
      \multicolumn{3}{l}{\textit{SlowFast (transformer backbone)}} \\
      Slow-only (B) & 79 & 87 & 78.0 \\ Fast-only (Ti) & 63 & 6 & 74.6 \\ Slowfast (B+Ti) & 202 & 105 & 79.7 \\ \midrule B/4+Ti/16 (ours) & 168 & 224  & \textbf{80.8} \\ \bottomrule
    \end{tabular}}
    \label{tab:slowfast}
  	\end{subtable}
  	\hfill
  	\begin{subtable}[t]{.3\linewidth}
  		\setlength{\tabcolsep}{6pt} \centering
  		\caption{Effects of increasing number of views.}
  		\vspace{-0.1\baselineskip}
  		\scriptsize{
  			\begin{tabular}{  l  c  l }
  			    \toprule
                Model variants & GFLOPs & Top-1 \\
                \midrule
                B/4 & 145 & 78.3  \\ B/4+Ti/16 & 168 & 80.8 (\textcolor{green}{+2.5}) \\ B/4+S/8+Ti/16 & 195 & 81.1 (\textcolor{green}{+2.8}) \\ \midrule
                B/4 (14) & 168 &  78.1 (\textcolor{red}{-0.2}) \\ B/4 (17) & 203 & 78.4 (\textcolor{green}{+0.1}) \\ \bottomrule
               \end{tabular}
  			\label{tab:num_views}
  		}
  		\vspace{1.4\baselineskip}
  		\setlength{\tabcolsep}{4pt} \centering
	    \scriptsize{
	    \caption{Effects of applying CVA at different layers.} \vspace{-0.1\baselineskip}
        \begin{tabular}{  c  c  c  c }
        \toprule
        Fusion layers & GFLOPs & MParams & Top-1 \\
        \midrule
        0 & \multirow{3}{*}{195} & \multirow{3}{*}{314} &  80.96 \\
        5 & &  & 81.08 \\
        11 & &  & 81.00 \\ \midrule
        0, 1 & \multirow{4}{*}{203} & \multirow{4}{*}{323} & 80.91 \\
        5, 6 &  &  & 80.96 \\
        10, 11 &  &  & 80.81 \\
        5, 11 &  & & \textbf{81.14} \\
        \midrule
        0, 5, 11 & 210 & 331 & 80.95 \\
        \bottomrule
       \end{tabular}
       \label{tab:cva_layers}
       }
	\end{subtable}
	\vspace{-0.5\baselineskip}
	\caption{Ablation studies of our method.
	(a) Assigning larger models to smaller tubelet sizes achieves the highest accuracy.
	(b) We apply the same ``Base'' encoder to all views, and show that there is minimal accuracy difference to the alternatives from (a), but a large increase in computation.
	(c) A comparison of different cross-view fusion methods, shows that Cross-View Attention (CVA) is the best.
	The ``Ensemble'' and ``late fusion'' baselines are detailed in the text.
	(d) We compare our approach to the alternate temporal multi-resolution method of \cite{feichtenhofer_iccv_2019}, implemented in the context of transformers, and show signficant improvements.
	(e) We achieve substantial accuracy by adding more views, and this improvement is larger than that obtained by adding more layers to a single encoder.
	(f) The optimal fusion layers are at the middle and late stages of the network. }
	\label{tab:ablation}
	\vspace{-\baselineskip}
\end{table*}
 \paragraph{Training and inference} 
We follow the training settings of ViViT reported in the paper and public code~\cite{arnab2021vivit}, unless otherwise stated. 
Namely, all models are trained on 32 frames with a temporal stride of 2.
We train our model using synchronous SGD with momentum of 0.9 following a cosine learning rate schedule with a linear warm up.
The input frame resolution is set to be  in both training and inference. We follow~\cite{arnab2021vivit} and apply the same data augmentation and regularization schemes~\cite{huang_stochasticdepth_eccv_2016,zhang_mixup_iclr_2018,cubuk_arxiv_2019,szegedy_cvpr_2016}, which were used by~\cite{touvron2021training} to train vision transformers more effectively.
During inference, we adopt the standard evaluation protocol by averaging over multiple spatial and temporal crops.
The number of crops is given in the results tables.
For reproducibility, we include exhaustive details in Appendix \ref{sec: hparams}.



\paragraph{Initialization}
Following previous works~\cite{arnab2021vivit, bertasius_arxiv_2021,patrick2021keeping}, we initialize our model from a corresponding ViT model pretrained on large-scale image datasets~\cite{deng2009imagenet, 300m_iccv17} obtained from the public code of~\cite{dosovitskiy2020image}.
The initial tubelet embedding operator, , and positional embeddings, , have different shapes in the pretrained model and we use the same technique as~\cite{arnab2021vivit} to adapt them to initialize each view of our multiview encoder (Sec.~\ref{sec:method_multiview_encoder}).
The final global encoder (Sec.~\ref{sec:method_global_encoder}) is randomly initialized.



\paragraph{Datasets} We report the performance of our proposed models on a diverse set of video classification datasets:

\emph{Kinetics}~\cite{kay_arxiv_2017} is a collection of large-scale, high-quality datasets of 10s video clips focusing on human actions. We report results on Kinetics 400, 600, and 700, with 400, 600, and 700 classes, respectively.

\emph{Moments in Time}~\cite{monfort_pami_2019} is a collection of 800,000 labeled 3~second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene.

\emph{Epic-Kitchens-100}~\cite{damen2021rescaling} consists of 90,000 egocentric videos, totaling 100 hours, recorded in kitchens. Each video is labeled with a ``noun'' and a ``verb'' and therefore we predict both categories using a single network with two ``heads''. Three accuracy scores (``noun'', ``verb'', and ``action'') are commonly reported for this dataset with action accuracy being the primary metric. The “action” label is formed by selecting the top-scoring noun and verb pair. 

\emph{Something-Something V2}~\cite{goyal_iccv_2017} consists of more than 220,000 short video clips that show humans interacting with everyday objects. Similar objects and backgrounds appear in videos across different classes. Therefore, in contrast to other datasets, this one challenges a model’s capability to distinguish classes from motion cues. 


\subsection{Ablation study} \label{sec:ablations}

We conduct ablation studies on the Kinetics 400 dataset.
In all cases, the largest backbone in the multiview encoder is ``Base'' for faster experimentation.
We report accuracies when averaging predictions across multiple spatio-temporal crops, as standard practice~\cite{arnab2021vivit, bertasius_arxiv_2021, feichtenhofer_iccv_2019,Carreira_2017_CVPR}.
In particular, we use  crops, that is 4 temporal crops, with 3 spatial crops for each temporal crop.
We used a learning rate of 0.1 for all experiments for 30 epochs, and used no additional regularization as done by~\cite{arnab2021vivit}.


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{mtv_fe_vs_vivit_fe.pdf}
         \caption{Accuracy[\%] - GFLOPs comparison between MTV and ViViT-FE.}
         \label{fig:flops}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{speed_mtv_fe_vs_vivit_fe.pdf}
        \caption{Accuracy[\%] - Throughput comparison between MTV and ViViT-FE.}
         \label{fig:latency}
     \end{subfigure}
        \caption{
        Accuracy/computation trade-off between ViViT-FE~\cite{arnab2021vivit} (blue) and our MTV (red). Figure~\ref{fig:flops} shows that MTV is consistently better and requires less FLOPs than ViViT-FE to achieve higher accuracy across different model scales (shown by the dotted green arrows pointing upper-left). With additional FLOPs, MTV shows larger accuracy gains (shown by the dotted green arrows pointing upper-right).
        Similarly, Fig.~\ref{fig:latency} shows that MTV can have higher throughput than ViVIT-FE, whilst still improving its accuracy, across all model scales.
        All speed comparisons are measured with the same hardware (Cloud TPU-v4), whilst the accuracy is computed from  view testing.
        }
        \label{fig:accuracy/complexity tradeoff}
    \label{fig:comparison_to_vivit}
    \vspace{-\baselineskip}
\end{figure*}



\iffalse
\begin{figure}[t]
    \centering
\includegraphics[width=0.96\columnwidth]{mtv_fe_vs_vivit_fe.pdf}
    \vspace{-0.5\baselineskip}
    \caption{Accuracy/complexity trade-off between ViViT-FE~\cite{arnab2021vivit} (blue) and our MTV (red).
    MTV is consistently better and requires less FLOPs than ViViT-FE to achieve higher accuracy across different model scales
    (indicated by the dotted green arrows pointing upper-left).
    With additional FLOPs, MTV shows larger accuracy gains (shown by the dotted green arrows pointing upper-right).
    }
	\label{fig:comparison_to_vivit}
\end{figure} 
\fi 
\paragraph{Model-view assignments}
Recall that a view is a video representation in terms of tubelets, and that a larger view equates to larger tubelets (and hence fewer transformer tokens) and smaller views correspond to smaller tubelets (and thus more tokens).

We considered two model-view assignment strategies: larger models for larger views (\eg, B/8+Ti/2, the larger ``Base'' model is used to encode  tubelets and the smaller ``Tiny'' model encodes  tubelets) and smaller models for larger views (\eg, B/2+Ti/8).
Table~\ref{tab:model_view_assignment} shows that assigning a larger model to smaller views is superior. For example, B/2+S/4+Ti/8 scores 81.8\% while B/8+S/4+Ti/2 only scores 78.5\%. One may argue that this is due to the increase of the FLOPs but B/4+S/8+Ti/16 still outperforms B/8+S/4+Ti/2 by a large margin under similar FLOPs. Our explanation is that larger views capture the gist of the scene, which requires less complexity to learn while the details of the scene are encapsulated by smaller views so a larger-capacity model is needed.

Another strategy is to assign the same model to all views. Table~\ref{tab:same_model_different_views} shows that in all three examples there is little difference between assigning a ``Base'' model and assigning a ``Small'' or ``Tiny'' model to larger views.
This result is surprising yet beneficial since we can reduce the complexity of the model at almost no cost of accuracy.


\paragraph{What is the best cross-view fusion method?\eatpunct}
Table~\ref{tab:fusion_methods} shows the comparison of different fusion methods on a three-view model.
We use one late fusion and an ensemble approach as the baselines.
``Ensemble'' simply sums the probabilities produced from each view, where the models from each view are trained separately. We also tried summing up the logits and majority voting but both obtained worse results. This method actually decreases the performance compared to the B/4 model since ``Small'' and ``Tiny'' models perform not comparably well. ``Late fusion'' concatenates the final embeddings produced by the transformer encoder from each view without any cross-view operations before feeding it into the global encoder. It improves the B/4 model from 78.3\% to 80.6\%.
All of our fusion methods except MLP outperform the baselines while CVA is the best overall.
Based on this observation, we choose CVA as the fusion method for all subsequent experiments. MLP fusion is the worst performing method of the three and we think it is because concatenation in the MLP blocks introduces additional channels that have to be randomly initialized, making model optimization more difficult. 


\paragraph{Effect of the number of views}
Table~\ref{tab:num_views} shows performance on Kinetics-400 as we increase the number of views.
With two views we achieve a \textbf{+2.5}\% in Top-1 accuracy over the baseline B/4 model.
As we increase to three views, the improvement widens to \textbf{2.8}\%.
Furthermore, we show that such improvement is non-trivial. For example, we also train a 14-layer and a 17-layer variants of the ``Base'' model. They share similar FLOPs with our two-view and three-view counterparts but their performance remains similar to that of the baseline.


\paragraph{Which layers to apply cross-view fusion?}
Motivated by Tab.~\ref{tab:fusion_methods}, we fix the fusion method to CVA, and vary the locations and number of layers where we apply CVA, when using a three-view B+S+Ti model (each encoder thus has 12 layers) in Tab.~\ref{tab:cva_layers}.
The choices are in the early-, mid-, and late-stages of the transformer encoders and the number of fusion layers is set to be one and two.
When using one fusion layer, the best location for fusion is mid followed by late, then early.
Adding more fusion layers in the same stage does not improve the performance but combining mid and late fusion improves the performance.
For example, fusion at 5th and 11th layers achieve the best result.
Based on this observation, we set the fusion layers to be \{11, 23\} for L+B+S+Ti and \{11, 23, 31\} for H+B+S+Ti model variants, respectively, in subsequent experiments.


\paragraph{Comparison to SlowFast}
SlowFast~\cite{feichtenhofer_iccv_2019} proposes a two-stream CNN architecture that takes frames sampled at two different frame rates. The ``Slow’’ pathway, built with a larger encoder, processes the low frame rate stream to capture the semantics of the scene while the ``Fast’’ pathway that takes in high frame rate inputs is used to capture motion information. 
To make a fair comparison, we implement~\cite{feichtenhofer_iccv_2019} in the context of transformers where we use ``Base'' and ``Tiny'' models as the encoders for the Slow and Fast paths respectively and use CVA for lateral connections. The Slow path takes four frames as inputs sampled with a temporal stride of 16 and the Fast path takes 16 frames sampled with a stride of 4.
As SlowFast captures multiscale temporal information by varying the frame rate to the two streams, the temporal duration for the tubelets is set to 1 in this case.
Table~\ref{tab:slowfast} shows that our method is significantly more accurate than the SlowFast method whilst also using fewer FLOPs.


\subsection{Comparison to the state of the art}
\label{sec:sota}

\begin{table*}[t]
    \vspace{-0.5\baselineskip}
	\caption{Comparisons to state-of-the-art. For ``views'',  denotes  temporal views and  spatial views. We report the total TFLOPs to process all spatio-temporal views.
	We use shorter notation, MTV-B, L, H to denote variants, B/2+S/4+Ti/8, L/2+B/4+S/8+Ti/16, and H/2+B/4+S/8+Ti/16, respectively. Models use a spatial resolution of , unless explicitly stated by MTV (p), which refers to a spatial resolution of . 
	Models are pretrained on ImageNet-21K unless explicitly stated in parenthesis.
	}
	\vspace{-0.5\baselineskip}
	\begin{subtable}[t]{.39\linewidth}
		\centering
		\caption{Kinetics 400}
    	\setlength{\tabcolsep}{3pt} \renewcommand*{\arraystretch}{1.10}  

		\vspace{-0.3\baselineskip}
		\scriptsize{
			\begin{tabular}{lcccc}
				\toprule
				Method 																			 & Top 1                & Top 5         & Views 	& TFLOPs \\
				\midrule
				TEA~\cite{li_tea_cvpr_2020}												& 76.1					& 92.5 &  & 2.10 \\  TSM-ResNeXt-101~\cite{lin_tsm_cvpr_2019}						  & 76.3				 & -- &  -- & -- \\I3D NL~\cite{wang2018nonlocal}										 & 77.7                  & 93.3         		 &   & 10.77     \\ 
				VidTR-L~\cite{zhang2021vidtr} & 79.1 & 93.9 &  & 10.53 \\
				LGD-3D R101~\cite{qiu2019learning}							&  79.4				    & 94.4			 		&  --	& --					\\  
				SlowFast R101-NL~\cite{feichtenhofer_iccv_2019}       		&  79.8                 &  93.9                   &     & 7.02   \\  X3D-XXL~\cite{feichtenhofer_cvpr_2020}      					&  80.4					&  94.6			  		&    & 5.82   \\
				OmniSource~\cite{duan2020omni} & 80.5 & 94.4 & -- & -- \\
				TimeSformer-L~\cite{bertasius_arxiv_2021} & 80.7				& 94.7					&  & 7.14 \\
				MFormer-HR~\cite{patrick2021keeping} & 81.1 & 95.2 &  & 28.76 \\
				MViT-B~\cite{fan2021multiscale}					  	& 81.2				& 95.1					&  & 4.10 \\
				MoViNet-A6~\cite{kondratyuk2021movinets} & 81.5 	&  \textbf{95.3}  &  & 0.39  \\  ViViT-L FE~\cite{arnab2021vivit} & 81.7 	&  93.8  &  &  11.94 \\  \textbf{MTV-B} & \textbf{81.8}  & 95.0 	&  & 4.79 \\ \textbf{MTV-B} (320p) & \textbf{82.4}  & 95.2 	&  & 11.16 \\ \midrule
				\multicolumn{4}{l}{\textit{Methods with web-scale pretraining}}                                \\ 
				VATT-L~\cite{akbari2021vatt} (HowTo100M)  &  82.1 &  95.5 	&  & 29.80 \\  ip-CSN-152~\cite{tran_iccv_2019} (IG) 			  &  82.5					& 95.3			&   & 3.27	  \\
				R3D-RS (WTS)~\cite{du2021revisiting} & 83.5 & -- &  & 9.21  \\
				OmniSource~\cite{duan2020omni} (IG) & 83.6 & 96.0 & -- & -- \\
				ViViT-H~\cite{arnab2021vivit} (JFT) 														&  84.9 &  95.8 	&  & 47.77 \\  TokenLearner-L/10~\cite{ryoo2021tokenlearner} (JFT) 														&  85.4 &  96.3 	&  & 48.91 \\  Florence~\cite{yuan2021florence} (FLD-900M) & 86.5 & 97.3 &  & -- \\
				CoVeR (JFT-3B)~\cite{zhang2021co} & 87.2 & -- &  & -- \\
				\textbf{MTV-L} (JFT) 														&  84.3 &  96.3 	&  & 18.05 \\  \textbf{MTV-H} (JFT) 														&  85.8 &  96.6 	&  & 44.47 \\ \textbf{MTV-H} (WTS) 														&  \textbf{89.1} &  \textbf{98.2} 	&  & 44.47 \\ \textbf{MTV-H} (WTS 280p) 														&  \textbf{89.9} &  \textbf{98.3} 	&  & 73.57 \\ \bottomrule
			\end{tabular}
		}
		\label{tab:sota_kinetics400}
	\end{subtable}
  	\hfill
  	\begin{subtable}[t]{.29\linewidth}
		\centering
  		\caption{Kinetics 600}
  		\setlength{\tabcolsep}{4pt} \vspace{-0.3\baselineskip}
  		\scriptsize{
	  		\begin{tabular}{lcc}
	  			\toprule
	  			Method 																			 & Top 1                & Top 5      \\ \midrule
SlowFast R101-NL~\cite{feichtenhofer_iccv_2019}       		&  81.8                     &  95.1   \\ X3D-XL~\cite{feichtenhofer_cvpr_2020}      						 &  81.9					&  95.5		\\ TimeSformer-L~\cite{bertasius_arxiv_2021}				  & 82.2				& 95.6		\\ MFormer-HR~\cite{patrick2021keeping} & 82.7 & 96.1  \\
	  			ViViT-L FE~\cite{arnab2021vivit} & 82.9 	&  94.6 \\  MViT-B~\cite{fan2021multiscale}					  & 83.8				& 96.3		\\ MoViNet-A6~\cite{kondratyuk2021movinets} & \textbf{84.8} 	&  \textbf{96.5} \\
	  			\textbf{MTV-B} & 83.6  & 96.1 \\ \textbf{MTV-B} (320p) & 84.0  & 96.2 \\ \midrule
  				R3D-RS (WTS)~\cite{du2021revisiting} & 84.3 & --  \\
	  			ViViT-H~\cite{arnab2021vivit} (JFT) & 85.8 & 96.5 \\ TokenLearner-L/10~\cite{ryoo2021tokenlearner} (JFT) &  86.3 &  97.0 \\  Florence~\cite{yuan2021florence} (FLD-900M) & 87.8 & 97.8 \\
	  			CoVeR (JFT-3B)~\cite{zhang2021co} & 87.9 & -- \\
	  			\textbf{MTV-L} (JFT) &  85.4 &  96.7 \\ \textbf{MTV-H} (JFT) &  86.5 &  97.3 \\ \textbf{MTV-H} (WTS) &  \textbf{89.6} &  \textbf{98.3} \\ \textbf{MTV-H} (WTS 280p) &  \textbf{90.3} &  \textbf{98.5} \\ \bottomrule
	  		\end{tabular}
	  		\label{tab:sota_kinetics600}
  		}
  		\vspace{0.6\baselineskip} \centering
		\caption{Something-Something v2}
		\vspace{-0.1\baselineskip}
		\setlength{\tabcolsep}{6pt} \scriptsize{
			\begin{tabular}{lcc}
				\toprule
				Method 												  & Top 1                & Top 5   \\
				\midrule
				SlowFast R50~\cite{feichtenhofer_iccv_2019,wu_multigrid_cvpr_2020}		& 61.7 & --    \\
				TimeSformer-HR~\cite{bertasius_arxiv_2021}					& 62.5 & -- \\
				VidTR~\cite{zhang2021vidtr} & 63.0 & -- \\
				ViViT-L FE~\cite{arnab2021vivit}		& 65.9  & 89.9 \\ MViT~\cite{fan2021multiscale} & 67.7 & 90.9 \\
				MFormer-L~\cite{patrick2021keeping} & 68.1 & \textbf{91.2}  \\
				\textbf{MTV-B} & 67.6 & 90.1 \\ \textbf{MTV-B} (320p) & \textbf{68.5} & 90.4 \\ \bottomrule
			\end{tabular}
			\label{tab:sota_ssv2}
		}
  	\end{subtable}
  	\hfill
  	\begin{subtable}[t]{.28\linewidth}
  		\setlength{\tabcolsep}{4pt} \centering
  		\caption{Kinetics 700}
  		\vspace{-0.3\baselineskip}
  		\setlength{\tabcolsep}{6pt} \scriptsize{
  			\begin{tabular}{lcc}
  				\toprule
  				& Top 1 & Top 5 \\ 
  				\midrule
  				VidTR-L~\cite{zhang2021vidtr} & 70.2 & -- \\
  				SlowFast R101~\cite{feichtenhofer_iccv_2019} 	&  71.0 & 89.6       \\
  				MoViNet-A6~\cite{kondratyuk2021movinets} 	&  72.3     &  --     \\
  				\textbf{MTV-L} & \textbf{75.2} & \textbf{91.7} \\  \midrule
	  			CoVeR (JFT-3B)~\cite{zhang2021co} & 79.8 & -- \\
  				\textbf{MTV-H} (JFT) & 78.0 & 93.3 \\  \textbf{MTV-H} (WTS) & \textbf{82.2} & \textbf{95.7} \\  \textbf{MTV-H} (WTS 280p) & \textbf{83.4} & \textbf{96.2} \\  \bottomrule
  			\end{tabular}
  			\label{tab:sota_kinetics700}
  		}
  		\vspace{0.18\baselineskip} \centering
  		\caption{Epic-Kitchens-100 Top 1 accuracy}
  		\vspace{-0.2\baselineskip}
  		\setlength{\tabcolsep}{4pt} \scriptsize{
			\begin{tabular}{lccc}
				\toprule
				Method 													 & Action & Verb  & Noun  \\
				\midrule
ViViT-L FE~\cite{arnab2021vivit} & 44.0 & 66.4 & 56.8 \\ MFormer-HR~\cite{patrick2021keeping} & 44.5 & 67.0 & 58.5  \\
				MoViNet-A6~\cite{kondratyuk2021movinets} & 47.7 & \textbf{72.2} & 57.3 \\ \textbf{MTV-B} & 46.7 & 67.8 & \textbf{60.5} \\ \textbf{MTV-B} (320p) & \textbf{48.6} & 68.0 & \textbf{63.1} \\ \midrule
			    \textbf{MTV-B} (WTS 280p) & \textbf{50.5} & 69.9 & \textbf{63.9} \\ \bottomrule
			\end{tabular}
		}
		\label{tab:sota_epic_kitchens}
\centering
  		\caption{Moments in Time}
  		\vspace{-0.1\baselineskip}
  		\setlength{\tabcolsep}{6pt} \scriptsize{
  			\begin{tabular}{lcc}
  				\toprule
  				& Top 1 & Top 5 \\ 
  				\midrule
  				AssembleNet-101~\cite{ryoo2019assemblenet} 	&  34.3     &  62.7     \\
  				ViViT-L FE~\cite{arnab2021vivit} 	& 38.5 & 64.1 \\  MoViNet-A6~\cite{kondratyuk2021movinets} 	&  40.2     &  --     \\
  				\textbf{MTV-L}	& \textbf{41.7} & \textbf{69.7} \\  \midrule
  				VATT-L (HT100M)~\cite{akbari2021vatt}	&  41.1     &  67.7     \\
  				\textbf{MTV-H} (JFT)	& \textbf{44.0} & \textbf{70.2} \\  \textbf{MTV-H} (WTS)	& \textbf{45.6} & \textbf{74.7} \\  \textbf{MTV-H} (WTS 280p)	&
  				\textbf{47.2} & \textbf{75.7} \\  \bottomrule
  			\end{tabular}
  			\label{tab:sota_moments_in_time}
  		}

	\end{subtable}
	\label{tab:sota}
	\vspace{-\baselineskip}
\end{table*}

 We compare to the state-of-the-art across six different datasets.
We evaluate models with four temporal- and three spatial-views per video clip, following~\cite{arnab2021vivit}. 
To make the notation more concise, we now use MTV-B to refer to B/2+S/4+Ti/8, MTV-L to refer to L/2+B/4+S/8+Ti/16 and MTV-H to refer to H/2+B/4+S/8+Ti/16.
Except for Kinetics, all our models start from a Kinetics 400 checkpoint and then are fine-tuned on the target datasets following~\cite{arnab2021vivit,patrick2021keeping,fan2021multiscale}.


\paragraph{Accuracy/computation trade-offs}
Figure~\ref{fig:comparison_to_vivit} compares our proposed MTV to its single-view counterpart, ViViT Factorized Encoder (FE)~\cite{arnab2021vivit} at every model scale on Kinetics 400.
We compare to ViViT-FE using tubelets with a temporal dimension of , as the authors obtained the best performance with this.

We can control the complexity of MTV by increasing or decreasing  used in each view.
For example, increasing  from 2 to 4 for the smallest view (and proportionally increasing  for all other views) will roughly reduce the input tokens by half for each view, and thus halve the total FLOPs for processing each input.
Our method with  for the smallest view consistently achieves higher accuracy than ViViT-FE at every complexity level while using fewer FLOPs, indicated by the green arrows pointing to the upper-left in Fig.~\ref{fig:flops}.
This further validates that processing more views in parallel enables us to achieve larger accuracy improvements than increasing the number of input tokens.
If we set  as in ViViT-FE, we use additional FLOPs, but increase significantly in accuracy too, as indicated by the green arrow pointing to the upper-right in Fig.~\ref{fig:flops}.

Furthermore, note how our B/2 model (transformer depth of 12 layers) outperforms ViViT-L/2 (24 layers), whilst using less FLOPs.
Similarly, our L/2 model outperforms ViViT-H/2.
This shows that we can achieve greater accuracy improvements by processing multiple views in parallel than by increasing the depth for processing a single view.

Finally, note that Fig.~\ref{fig:latency} shows that our conclusions are also consistent when using the inference time to measure our model's efficiency.
Appendix~\ref{sec: comparison_to_vivit} also shows that these trends also hold when using an unfactorized~\cite{arnab2021vivit} backbone architecture of ViViT and MTV.


\paragraph{Kinetics}
We compare to methods that are pretrained on ImageNet-1K, ImageNet-21K~\cite{deng2009imagenet} and those that do not utilize pretraining at all in the first part of Tab.~\ref{tab:sota}.
In the second part of the tables, we compare to methods that are pretrained on web-scale datasets such as Instagram 65M~\cite{ghadiyaram2019large}, JFT-300M~\cite{300m_iccv17}, JFT-3B~\cite{zhai2021scaling}, WTS~\cite{stroud2020learning}, Florence~\cite{yuan2021florence} or HowTo100M~\cite{miech2019howto100m}.
Observe that we achieve state-of-the-art results both with and without web-scale pretraining.

On Kinetics 400, our ImageNet-21K pretrained ``Base'' model improves the ``Large'' ViViT-FE model~\cite{arnab2021vivit}, which corresponds to a  deeper, single-view equivalent of our model by 0.1\% and 1.2\% in Top-1 and Top-5 accuracy, whilst using 40\% of the total FLOPs.
Our higher resolution version improves further by 0.7\% and 1.4\% while still using slightly fewer FLOPs.
On Kinetics 600, our ``Base'' model scores second to~\cite{kondratyuk2021movinets} whose model structure is derived using architecture search on Kinetics 600 itself.
We show significant improvements over~\cite{kondratyuk2021movinets} on both Kinetics 400 and 700 for which the architecture of~\cite{kondratyuk2021movinets} was not directly optimized for.

When using additional JFT-300M pretraining, our ``Huge'' model outperforms other recent transformer models using the same pretraining dataset~\cite{arnab2021vivit, ryoo2021tokenlearner}.
And when we utilize the Weak Textual Supervision (WTS) dataset of~\cite{stroud2020learning} for pre-training, we substantially advance the best reported results on Kinetics:
On Kinetics 400, we achieve a Top-1 accuracy of 89.9\%, which improves upon the previous highest result (CoVeR~\cite{zhang2021co}) by 2.7\%.
Similarly, on Kinetics 600, we achieve a Top-1 of 90.3\%, which is an absolute improvement of 2.4\% on~\cite{zhang2021co}.
On Kinetics 700, we achieve 83.4\%, which improves even further by 3.6\% over~\cite{zhang2021co}.
We also improve upon R3D-RS~\cite{du2021revisiting}, which also used WTS pretraining, by 6.4\% and 6.0\% on Kinetics-400 and -600. 



\paragraph{Epic-Kitchens-100}
Following the standard protocol \cite{damen2021rescaling}, we report Top-1 action-, verb- and noun-accuracies with action accuracy being the primary metric.
Our results are averaged over  crops as additional spatial crops did not help.
Both our MTV-B and MTV-B(320p) significantly improve the previous state-of-the-art on noun classes, and MTV-B(320p) achieves a new state-of-the-art of 48.6\% on actions. With WTS pretraining and increasing resolution, we improved the results to 50.5\%. 
We found that additional data augmentation (detailed in Appendix \ref{sec: hparams}) has to be used to achieve good performance (as also observed by~\cite{arnab2021vivit, patrick2021keeping}) as this is the smallest dataset of all six with 67,000 training examples. 


\paragraph{Something-Something V2}
This dataset consists of class labels such as ``move to left'' and ``pointing to right''~\cite{goyal_iccv_2017}.
As the model needs to explicitly reason about direction, we do not perform random horizontal or vertical flipping as data augmentation on this dataset as also done by~\cite{fan2021multiscale}.
We improve substantially over ViViT-L-FE~\cite{arnab2021vivit}, which corresponds to a deeper single-view equivalent of our model by 2.6\%, and also improve upon MFormer~\cite{patrick2021keeping} by 0.4\%.


\paragraph{Moments in Time} 
Our MTV-L model significantly improves over the previous state-of-the-art~\cite{kondratyuk2021movinets} by 1.5\% in Top-1 accuracy.
Moreover, our model with ImageNet-21K pretraining even outperforms VATT~\cite{akbari2021vatt}, which was pretrained on HowTo100M~\cite{miech2019howto100m}, a dataset consisting of around 100M video clips.
When using WTS pre-training, we improve our accuracy even further, achieving 47.2\%.

