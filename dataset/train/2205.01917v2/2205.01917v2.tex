\section{Experiments}
In this section, we first describe the details of our experimental setup. The main results are presented next organized as visual recognition tasks, crossmodal alignment tasks, image captioning and multimodal understanding tasks. Our main results are conducted under three categories for downstream tasks: zero-shot transfer, frozen-feature evaluation and finetuning.
We also present ablation experiments including training objectives and architecture designs.

\subsection{Training Setup}

\textbf{Data.}
As discussed in Section~\ref{secs:method}, CoCa is pretrained from scratch in a single stage on both web-scale alt-text data and annotated images by treating all labels simply as texts.
We use the JFT-3B dataset~\cite{zhai2021scaling} with label names as the paired texts,
and the ALIGN dataset~\citep{jia2021scaling} with noisy alt-texts.
Similar to~\cite{pham2021combined},
we randomly shuffle and concatenate label names of each image in JFT together with a prompt sampled from~\cite{radford2021learning}. An example of the resulting text label of a JFT image would look like ``a photo of the {cat, animal}''.
Unlike prior models~\cite{zhai2021lit, pham2021combined} that also use the combination of these two datasets,
we train all model parameters from scratch at the same time without pretraining an image encoder with supervised cross-entropy loss for simplicity and pretraining efficiency.
To ensure fair evaluation,
we follow the strict de-duplication procedures introduced in \citep{zhai2021lit, jia2021scaling} to filter all near-domain examples (3.6M images are removed in total) to our downstream tasks. To tokenize text input,
we use a sentence-piece model~\cite{sennrich2015neural, kudo2018subword} with a vocabulary size of 64k trained on the sampled pretraining dataset.

\textbf{Optimization.} Our models are implemented in the Lingvo framework~\citep{shen2019lingvo} with GSPMD~\citep{huang2019gpipe, xu2020automatic, lepikhin2020gshard, xu2021gspmd} for scaling performance. 
Following \citep{pham2021combined},
we use a batch size of 65,536 image-text pairs, where half of each batch comes from JFT and ALIGN, respectively.
All models are trained on the combined contrastive and captioning objectives in Eq.(\ref{eq:coca}) for 500k steps, roughly corresponding to 5 epochs on JFT and 10 epochs on ALIGN.
As shown later in our studies, we find a larger captioning loss weight is better and thus  and .
Following \citep{jia2021scaling},
we apply a contrastive loss with a trainable temperature  with an initial value of 0.07.
For memory efficiency,
we use the Adafactor~\cite{shazeer2018adafactor} optimizer with  and decoupled weight decay ratio of 0.01.
We warm up the learning rate for the first 2\% of training steps to a peak value of , and linearly decay it afterwards. Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips.
Following \cite{radford2021learning, jia2021scaling, yuan2021florence},
we continue pretraining for one epoch on a higher resolution of 576576.
For finetuning evaluation,
we mainly follow simple protocols and directly train CoCa on downstream tasks without further metric-specific tuning like CIDEr scores (details in Appendix \ref{secs:detailed_ae_ft} and \ref{secs:detailed_vl}).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/radar.png}
\caption{Comparison of CoCa with other image-text foundation models (without task-specific customization) and multiple state-of-the-art task-specialized models.}
\label{figs:key_results}
\end{figure}

\begin{table}[t]
\resizebox{1.0\textwidth}{!}{\parbox{.35\linewidth}{
\small
\centering
\begin{tabular}{@{}lc@{}}
    \toprule
    Model & ImageNet \\
    \midrule
    ALIGN~\cite{jia2021scaling} & 88.6 \\
    Florence~\cite{yuan2021florence} & 90.1 \\
    MetaPseudoLabels~\cite{pham2021meta} & 90.2 \\
    CoAtNet~\cite{dai2021coatnet} & 90.9 \\
    ViT-G~\cite{zhai2021scaling} & 90.5 \\
    \ \ + Model Soups~\cite{wortsman2022model} & 90.9 \\
    \midrule
    CoCa (frozen) & 90.6 \\
    CoCa (finetuned) & \textbf{91.0} \\
    \bottomrule
\end{tabular}
}
\hfill
\parbox{.65\linewidth}{
\small
\centering
\begin{tabular}{@{}lcccc@{}}
    \toprule
    Model &  K-400 & K-600 & K-700 & Moments-in-Time \\
    \midrule
    ViViT~\cite{arnab2021vivit} &  84.8 & 84.3 & - & 38.0 \\
    MoViNet~\cite{kondratyuk2021movinets} & 81.5 & 84.8 & 79.4 & 40.2\\
    VATT~\cite{akbari2021vatt} & 82.1 & 83.6 & - & 41.1 \\
    Florence~\cite{yuan2021florence} & 86.8 & 88.0 & - & - \\
    MaskFeat~\cite{wei2021masked} & 87.0 & 88.3 & 80.4 & \\
    CoVeR~\cite{zhang2021co} & 87.2 & 87.9 & 78.5 & 46.1 \\
    \midrule
    CoCa (frozen) & 88.0 & 88.5 & 81.1 & 47.4 \\
    CoCa (finetuned) & \textbf{88.9} & \textbf{89.4} & \textbf{82.7} & \textbf{49.0} \\
    \bottomrule
\end{tabular}
}
}
\caption{\label{tabs:image_video_ae_ft} Image classification and video action recognition with frozen encoder or finetuned encoder.}
\end{table} 
\subsection{Main Results}
\label{secs:main_results}
We extensively evaluate the capabilities of CoCa models on a wide range of downstream tasks as a pretrained foundation model.
We mainly consider core tasks of three categories that examine (1) visual recognition, (2) crossmodal alignment, and (3) image captioning and multimodal understanding capabilities.
Since CoCa produces both aligned unimodal representations and fused multimodal embeddings at the same time,
it is easily transferable to all three task groups with minimal adaption.
Figure~\ref{figs:key_results} summarizes the performance on key benchmarks of CoCa compared to other dual-encoder and encoder-decoder foundation models and state-of-the-art task-specialized methods. CoCa sets new state-of-the-art results on tasks of all three categories with a single pretrained checkpoint.


\subsubsection{Visual Recognition Tasks}
\label{secs:visual_recognition}
Our visual recognition experiments are conducted on ImageNet~\cite{deng2009imagenet} as image recognition benchmark, and multiple video datasets including Kinetics-400~\cite{kay2017kinetics}, Kinetics-600~\cite{carreira2018short}, Kinetics-700~\cite{carreira2019short}, Moments-in-Time~\cite{monfort2019moments} as test-beds for video action recognition; it is noteworthy that CoCa pretrains on image data only, without accessing any extra video datasets. We apply the CoCa encoder on video frames individually (Section~\ref{secs:downstream}) without early fusion of temporal information, yet the resulting CoCa-for-Video model performs better than many spatio-temporal early-fused video models. 

\textbf{Frozen-feature.} We apply a pretrained frozen CoCa model on both image classification and video action recognition. The encoder is used for both tasks while the decoder is discarded. As discussed in Section~\ref{secs:downstream}, an attentional pooling is learned together with a softmax cross-entropy loss layer on top of the embedding outputs from CoCa encoder. For video classification, a single query-token is learned to weight outputs of all tokens of spatial patches  temporal frames. We set a learning rate of  on both attentional pooler and softmax, batch size of 128, and a cosine learning rate schedule (details in Appendix \ref{secs:detailed_ae_ft}). For video action recognition, we compare CoCa with other approaches on the same setup (\ie, without extra supervised video data and without audio signals as model inputs).
As shown in Table~\ref{tabs:image_video_ae_ft}, without finetuning full encoder,
CoCa already achieves competitive Top-1 classification accuracies compared to specialized image and outperforms prior state-of-the-art specialized methods on video tasks.


\textbf{Finetuning.} Based on the architecture of frozen-feature evaluation, we further finetune CoCa encoders on image and video datasets individually with a smaller learning rate of . More experimental details are summarized in the Appendix~\ref{secs:detailed_ae_ft}.
The finetuned CoCa has improved performance across these tasks.
Notably, CoCa obtains new state-of-the-art 91.0\% Top-1 accuracy on ImageNet, as well as better video action recognition results compared with recent video approaches.
More importantly, CoCa models use much less parameters than other methods in the visual encoder as shown in Figure \ref{fig:ft_imagenet}.
These results suggest the proposed framework efficiently combines text training signals and thus is able to learn high-quality visual representation better than the classical single-encoder approach.


\begin{figure}\centering
\subfloat[Finetuned ImageNet Top-1 Accuracy.\label{fig:ft_imagenet}]{\includegraphics[width=0.45\linewidth]{figures/ft_imagenet.png}}\qquad
\subfloat[Zero-Shot ImageNet Top-1 Accuracy.\label{fig:zs_imagenet}]{\includegraphics[width=0.45\linewidth]{figures/zs_imagenet.png}}
\caption{Image classification scaling performance of model sizes.}
\end{figure}

  

\begin{table}[t]
\centering
\vspace{-1em}
\resizebox{1.0\textwidth}{!}{\begin{tabular}{@{}lcccccccccccc@{}}
    \toprule 
    & \multicolumn{6}{c}{Flickr30K (1K test set)} & \multicolumn{6}{c}{MSCOCO (5K test set)} \\
    & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
    Model & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
    \midrule
    CLIP~\cite{radford2021learning}  & 88.0 & 98.7 & 99.4 & 68.7 & 90.6 & 95.2 & 58.4 & 81.5 & 88.1 & 37.8 & 62.4 & 72.2 \\
    ALIGN~\cite{jia2021scaling} & 88.6 & 98.7 & 99.7 & 75.7 & 93.8 & 96.8 & 58.6 & 83.0 & 89.7 & 45.6 & 69.8 & 78.6 \\
    FLAVA~\cite{singh2021flava} & 67.7 & 94.0 & - & 65.2 & 89.4 & - & 42.7 & 76.8 & - & 38.4 & 67.5 & - \\
    FILIP~\cite{yao2021filip} & 89.8 & 99.2 & 99.8 & 75.0 & 93.4 & 96.3 & 61.3 & 84.3 & 90.4 & 45.9 & 70.6 & 79.3 \\
    Florence~\cite{yuan2021florence} & 90.9 & 99.1 & - & 76.7 & 93.6 & - & 64.7 & 85.9 & - & 47.2 & 71.4 & - \\
    \midrule
    CoCa-Base & 89.8 & 98.8 & 99.8 & 76.8 & 93.7 & 96.8 & 63.8 & 84.7 & 90.7 & 47.5 & 72.4 & 80.9 \\
    CoCa-Large & 91.4 & 99.2 & 99.9 & 79.0 & 95.1 & 97.4 & 65.4 & 85.6 & 91.4 & 50.1 & 73.8 & 81.8 \\
    CoCa & \bf 92.5 & \bf 99.5 & \bf 99.9 & \bf 80.4 & \bf 95.7 & \bf 97.7 & \bf 66.3 & \bf 86.2 & \bf 91.8 & \bf 51.2 & \bf 74.2 & \bf 82.0 \\
    \bottomrule
  
\end{tabular}
}
\caption{\label{tabs:zs-it-rt} Zero-shot image-text retrieval results on Flickr30K~\cite{plummer2015flickr30k} and MSCOCO~\cite{chen2015microsoft} datasets.}
\vspace{-1em}
\end{table} \begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{\begin{tabular}{@{}lccccccc@{}}
    \toprule
    Model &  ImageNet & ImageNet-A & ImageNet-R & ImageNet-V2 & ImageNet-Sketch & ObjectNet & Average\\
    \midrule
    CLIP~\cite{radford2021learning} & 76.2 & 77.2 & 88.9 & 70.1 & 60.2 & 72.3 & 74.3\\
    ALIGN~\cite{jia2021scaling} & 76.4 & 75.8 & 92.2 & 70.1 & 64.8 & 72.2 & 74.5\\
    FILIP~\cite{yao2021filip} & 78.3 & - & - & - & - & - & - \\
    Florence~\cite{yuan2021florence} & 83.7 & - & - & - & - & - & - \\
    LiT~\cite{zhai2021lit} & 84.5 & 79.4 & 93.9 & 78.7 & - & 81.1 & - \\
    BASIC~\cite{pham2021combined} & 85.7 & 85.6 & 95.7 & 80.6 & 76.1 & 78.9 & 83.7\\
    \midrule
    CoCa-Base & 82.6 & 76.4 & 93.2 & 76.5 & 71.7 & 71.6 & 78.7 \\
    CoCa-Large & 84.8 & 85.7 & 95.6 & 79.6 & 75.7 & 78.6 & 83.3 \\
    CoCa & \bf 86.3 & \bf 90.2 & \bf 96.5 & \bf 80.7 & \bf 77.6 & \bf 82.7 & \bf 85.7 \\
    \bottomrule
\end{tabular}
}
\caption{\label{tabs:zs-image} Zero-shot image classification results on ImageNet~\cite{deng2009imagenet}, ImageNet-A~\cite{hendrycks2021natural}, ImageNet-R~\cite{hendrycks2021many}, ImageNet-V2~\cite{recht2019imagenet}, ImageNet-Sketch~\cite{wang2019learning} and ObjectNet~\cite{barbu2019objectnet}.}
\vspace{-1em}
\end{table} 

    
 

    
 


\begin{table}[t]
\centering
\small
\resizebox{0.7\textwidth}{!}{\begin{tabular}{@{}lcccccc@{}}
    \toprule 
    & \multicolumn{6}{c}{MSR-VTT Full} \\ 
    & \multicolumn{3}{c}{Text  Video} & \multicolumn{3}{c}{Video  Text} \\ 
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    Method & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
    \midrule
    CLIP~\cite{portillo2021clip} & 21.4 & 41.1 & 50.4 & 40.3 & 69.7 & 79.2 \\
    Socratic Models~\cite{zeng2022socratic} & - & - & - & 44.7 & 71.2 & 80.0 \\
    \midrule
    CLIP~\cite{portillo2021clip} (subset) & 23.3 & 44.2 & 53.6 & 43.3 & 73.3 & \bf 81.8 \\
    Socratic Models~\cite{zeng2022socratic} (subset) & - & - & - & 46.9 & \bf 73.5 & 81.3 \\
    CoCa (subset) & \bf 30.0 & \bf 52.4 & \bf 61.6 & \bf 49.9 & 73.4 & 81.4 \\
    
    \bottomrule
 
\end{tabular}
}
\caption{\label{tabs:msrvtt} Zero-shot Video-Text Retrieval on MSR-VTT Full test set.}
\end{table} 
\subsubsection{Crossmodal Alignment Tasks}
\label{secs:cross_modal_alignment}

Unlike other fusion-based foundation methods \cite{wang2021simvlm,singh2021flava,wang2022unifying},
CoCa is naturally applicable to crossmodal alignment tasks since it generates aligned image and text unimodal embeddings.
In particular, we are interested in the zero-shot setting where all parameters are frozen after pretraining and directly used to extract embeddings.
Here, we use the same embeddings used for contrastive loss during pretraining, and thus the multimodal text decoder is not used.


\textbf{Zero-Shot Image-Text Retrieval.}
We evaluate CoCa on the two standard image-text retrieval benchmarks: MSCOCO~\cite{chen2015microsoft} and Flickr30K~\cite{plummer2015flickr30k}. 
Following the CLIP setting \cite{radford2021learning},
we first independently feed each image/text to the corresponding encoder and obtain embeddings for all image/text in the test set.
We then retrieve based on cosine similarity scores over the whole test set.
As shown in Table \ref{tabs:zs-it-rt},
CoCa significantly improves over prior methods on both image-to-text and text-to-image retrievals on all metrics.
In addition,
our model is parameter-efficient,
with CoCa-Base already outperforming strong baselines (CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling}) and CoCa-Large outperforming Florence \cite{yuan2021florence} (which contains a parameter count comparable to ViT-Huge). 
This shows that CoCa learns good unimodal representations \emph{and} aligns them well across modalities.

\textbf{Zero-Shot Image Classification.}
Following prior work \cite{radford2021learning,jia2021scaling},
we use the aligned image/text embeddings to perform zero-shot image classification by matching images with label names without finetuning.
We follow the exact setup in \cite{radford2021learning} and apply the same set of prompts used for label class names.
As shown in Table \ref{tabs:zs-image},
CoCa sets new state-of-the-art zero-shot classification results on ImageNet.
Notably, CoCa uses fewer parameters than prior best model \cite{pham2021combined} while smaller CoCa variants already outperform strong baselines \cite{radford2021learning,yuan2021florence}, as shown in Figure \ref{fig:zs_imagenet}.
In addition,
our model demonstrates effective generalization under zero-shot evaluation,
consistent with prior findings \cite{radford2021learning,jia2021scaling},
with CoCa improving on all six datasets considered.
Lastly,
while prior models \cite{zhai2021lit,pham2021combined} found sequentially pretraining with single-encoder and dual-encoder methods in multiple stages 
is crucial to performance gains,
our results show it is possible to attain strong performance by unifying training objectives and datasets in a single-stage framework.


\textbf{Zero-Shot Video Retrieval.} 
We evaluate video-text retrieval using CoCa on MSR-VTT~\cite{xu2016msr} using the full split. 
Table~\ref{tabs:msrvtt} shows that CoCa produces the highest retrieval metrics for both text-to-video and video-to-text retrieval. 
It is important to note that MSR-VTT videos are sourced from YouTube, and we require the original videos to compute our embeddings. 
Many of the videos have been made explicitly unavailable~\cite{smaira2020short},
hence we compute retrieval over the subset of data that is publicly available at the time of evaluation.  
Using code\footnote{\url{https://socraticmodels.github.io}} provided by the authors of Socratic Models~\cite{zeng2022socratic},
we re-computed metrics on the available subset for those methods, indicated by ``(subset)'' for fairest comparison.




\begin{table}[t]
\centering
\vspace{-2em}
\resizebox{0.7\textwidth}{!}{\begin{tabular}{@{}lcccccc@{}}
    \toprule 
    & \multicolumn{2}{c}{VQA} & \multicolumn{2}{c}{SNLI-VE} & \multicolumn{2}{c}{NLVR2} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    Model & test-dev & test-std & dev & test & dev & test-p \\
    \midrule
    UNITER~\cite{chen2020uniter} & 73.8 & 74.0 & 79.4 & 79.4 & 79.1 & 80.0 \\
    VinVL~\cite{Zhang_2021_CVPR} & 76.6 & 76.6 & - & - & 82.7 & 84.0 \\
    CLIP-ViL~\cite{shen2021much} & 76.5 & 76.7 & 80.6 & 80.2 & - & - \\
    ALBEF~\cite{li2021align} & 75.8 & 76.0 & 80.8 & 80.9 & 82.6 & 83.1 \\
    BLIP~\cite{li2022blip} & 78.3 & 78.3 & - & - & 82.2 & 82.2 \\
    OFA~\cite{wang2022unifying} & 79.9 & 80.0 & \textcolor{lightgray}{90.3} & \textcolor{lightgray}{90.2} & - & - \\
    VLMo~\cite{wang2021vlmo} & 79.9 & 80.0 & - & - & 85.6 & 86.9 \\
    SimVLM~\cite{wang2021simvlm} & 80.0 & 80.3 & 86.2 & 86.3 & 84.5 & 85.2 \\
    Florence~\cite{yuan2021florence} & 80.2 & 80.4 & - & - & - & - \\
    METER~\cite{dou2021empirical} & 80.3 & 80.5 & - & - & - & - \\
    \midrule
    CoCa & \bf 82.3 & \bf 82.3 & \bf 87.0 & \bf 87.1 & \bf 86.1 & \bf 87.0 \\
    \bottomrule
\end{tabular}
}
\caption{\label{tabs:it-understanding} Multimodal understanding results comparing vision-language pretraining methods. OFA uses both image and text premises as inputs while other models utilize the image only.}
\end{table} \subsubsection{Image Captioning and Multimodal Understanding Tasks}

Another key advantage of CoCa is its ability to process multimodal embeddings as an encoder-decoder model trained with the generative objective.
Therefore,
CoCa can perform both image captioning and multimodal understanding downstream tasks without any further fusion adaptation \cite{shen2021much,dou2021empirical}.
Overall,
experimental results suggest CoCa reaps the benefit of a encoder-decoder model to obtain strong multimodal understanding and generation capabilities,
in addition to the vision and retrieval capabilities as a dual-encoder method.


\textbf{Multimodal Understanding.} 
As shown in \cite{wang2021simvlm},
the output of encoder-decoder models can jointly encode image and text inputs, and can be used for tasks that require reasoning over both modalities.
We consider three popular multimodal understaning benchmarks: visual question answering (VQA v2 \citep{goyal2017making}), visual entailment (SNLI-VE \citep{xie2019visual}), and visual reasoning (NLVR2 \citep{suhr2018corpus}).
We mainly follow the settings in \cite{wang2021simvlm} and train linear classifiers on top of the decoder outputs to predict answers (more details in Appendix \ref{secs:detailed_vl}).
Our results in Table \ref{tabs:it-understanding} suggest that CoCa outperforms strong vision-language pretraining (VLP) baselines and obtains state-of-the-art performance on all three tasks.
While prior dual-encoder models \cite{radford2021learning,yuan2021florence} do not contain fusion layers and thus require an additional VL pretraining stage for downstream multimodal understanding tasks,
CoCa subsumes the three pretraining paradigms and obtains better performance on VL tasks with lightweight finetuning.

\textbf{Image Captioning.}
In addition to multimodal classification tasks,
CoCa is also directly applicable to image captioning tasks as an encoder-decoder model.
We finetune CoCa with the captioning loss  only on MSCOCO \cite{chen2015microsoft} captioning task and evaluate on both MSCOCO Karpathy-test split and NoCaps \cite{agrawal2019nocaps} online evaluation.
As shown by experiments in Table \ref{tabs:caption},
CoCa outperforms strong baselines trained with cross-entropy loss on MSCOCO,
and achieves results comparable to methods with CIDEr metric-specific optimization~\cite{rennie2017self}.
It is noteworthy that we do not use CIDEr-specific optimization~\cite{rennie2017self} for simplicity.
On the challenging NoCaps benchmark,
CoCa obtains new state-of-the-art on both validation and test splits (generated examples shown in Figure~\ref{figs:caption}).
These results showcase the generative capability of CoCa as an image-text foundation model.
\begin{table}[t]
\centering
\vspace{-1em}
\begin{tabular}{@{}lcccccccc@{}}
    \toprule 
    & \multicolumn{4}{c}{MSCOCO} & \multicolumn{4}{c}{NoCaps} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    &  &  &  &  & \multicolumn{2}{c}{Valid} & \multicolumn{2}{c}{Test} \\
    & B@4 & M & C & S & C & S & C & S \\
    \midrule
    CLIP-ViL~\cite{shen2021much} & 40.2 & 29.7 & 134.2 & 23.8 & - & - & - & - \\
    BLIP~\cite{li2022blip} & 40.4 & - & 136.7 & - & 113.2 & 14.8 & - & - \\
    VinVL\cite{Zhang_2021_CVPR} & 41.0 & 31.1 & 140.9 & 25.4 & 105.1 & 14.4 & 103.7 & 14.4 \\
    SimVLM~\cite{wang2021simvlm} & 40.6 & 33.7 & 143.3 & \bf 25.4 & 112.2 & - & 110.3 & 14.5 \\
    LEMON~\cite{hu2021scaling} & \bf 41.5 & 30.8 & 139.1 & 24.1 & 117.3 & 15.0 & 114.3 & 14.9 \\
    \textcolor{lightgray}{LEMON~\cite{hu2021scaling}} & \textcolor{lightgray}{42.6} & \textcolor{lightgray}{31.4} & \textcolor{lightgray}{145.5} & \textcolor{lightgray}{25.5} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-}\\
    \textcolor{lightgray}{OFA~\cite{wang2022unifying}} & \textcolor{lightgray}{43.5} & \textcolor{lightgray}{31.9} & \textcolor{lightgray}{149.6} & \textcolor{lightgray}{26.1} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-} & \textcolor{lightgray}{-}\\
    \midrule
    CoCa & 40.9 & \bf 33.9 & \bf 143.6 & 24.7 & \bf 122.4 & \bf 15.5 & \bf 120.6 & \bf 15.5\\
    \bottomrule
\end{tabular}
\caption{\label{tabs:caption} Image captioning results on MSCOCO and NoCaps (B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE). Models finetuned with CIDEr optimization. }
\end{table} \begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/captions}
\caption{Curated samples of text captions generated by CoCa with NoCaps images as input.}
\label{figs:caption}
\end{figure}


\subsection{Ablation Analysis}
We extensively ablate the properties of CoCa on a smaller model variant.
Specifically,
we train CoCa-Base with a reduced 12 decoder layers and a total batch size of 4,096.
We mainly evaluate using zero-shot image classification and VQA,
since the former covers both visual representation quality and crossmodal alignment,
while the later is representative for multimodal reasoning.

\textbf{Captioning \vs \ Classification.}
We first examine the effectiveness of captioning loss on image annotation datasets.
To do this,
we train a naive encoder-decoder model using  on the JFT-3B dataset,
and compare with a standard ViT-Base single-encoder model trained with  in Table \ref{tab:single_tower}.
We find encoder-decoder models to perform on par with single-encoder pretraining on both linear evaluation and finetuned results.
This suggests that the generative pretraining subsumes classification pretraining,
consistent with our intuition that  is a special case of  when text vocabulary is the set of all possible class names.
Thus, our CoCa model can be interpreted as an effective unification of the three paradigms.
This explains why CoCa does not need a pretrained visual encoder to perform well.

\begin{table*}[t]
\vspace{-.2em}
\centering
\subfloat[
Encoder-decoder \vs\ single-encoder models (trained on JFT).
\label{tab:single_tower}
]{
\centering
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{lcc}
    \toprule 
    loss & LE & FT \\
    \midrule
     & 81.0 & 85.1 \\
     & 82.1 & 84.9 \\
    \bottomrule
    & \\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{1em}
\subfloat[
Training objectives ablation.
\label{tab:loss_ablation}
]{
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{lccc}
    \toprule 
    loss & ZS & VQA & TPU cost \\
    \midrule
     & 70.7 & 59.2 & 1\\
     & - & 68.9 & 1.17\\
    \textbf{} & \textbf{71.6} & \textbf{69.0} & \textbf{1.18}\\
    \bottomrule
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{1em}
\subfloat[
Training objectives weights.
\label{tab:loss_ratio}
]{
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{1pt}{1.05}
\begin{tabular}{lcc}
    \toprule 
     & ZS & VQA \\
    \midrule
    1:1 & 71.5 & 68.6 \\
    1:2 & 71.0 & 68.1 \\
    \textbf{2:1} & \textbf{71.6} & \textbf{69.0} \\
    \bottomrule
\end{tabular}
\end{center}}\end{minipage}
}
\\
\centering
\vspace{.3em}
\subfloat[
Unimodal decoder layers.
\label{tab:decoder_layers}
]{
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{6pt}{1.05}
\begin{tabular}{lcc}
    \toprule 
     & ZS & VQA \\
    \midrule
    3 & 70.2 & 69.0 \\
    \textbf{6} & \textbf{71.6} & \textbf{69.0} \\
    9 & 71.4 & 68.8 \\
    \bottomrule
    & \\
    & \\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{1em}
\subfloat[
Contrastive text embedding design ablation.
\label{tab:text_pooler}
]{
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{1pt}{1.05}
\begin{tabular}{lcc}
    \toprule 
    variant & AE & MSCOCO \\
    \midrule
    \textbf{1 [CLS]} & \textbf{80.7} & \textbf{41.4} \\
    \ \ + text tokens & 80.3 & 40.2 \\
    8 [CLS] & 80.3 & 36.9 \\
    \ \ + text tokens & 80.4 & 40.3 \\
    \bottomrule
    & \\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{1em}
\subfloat[
Attentional pooler design ablation.
\label{tab:img_pooler}
]{
\centering
\begin{minipage}{0.29\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{lcc}
    \toprule 
    variant & ZS & VQA \\
    \midrule
    parallel & 71.2 & 68.7 \\
    \textbf{cascade} & \textbf{71.6} & \textbf{69.0} \\
     & 71.5 & 69.0 \\
     & 69.3 & 64.4 \\
     & 71.2 & 68.2 \\
    \bottomrule
\end{tabular}
\end{center}}\end{minipage}
}
\vspace{-.1em}
\caption{CoCa ablation experiments. On ImageNet classification, we report top-1 accuracy for: zero-shot (ZS), linear evaluation (LE), attentional evaluation (AE) using pooler on frozen feature, and finetuning (FT). On MSCOCO retrieval, we report the average of image-to-text and text-to-image R@1. On VQA, we report the dev-set vqa score. The default CoCa setting is \textbf{bold}.}
\label{tab:ablations} \vspace{-.5em}
\end{table*}
 
\textbf{Training Objectives.}
We study the effects of the two training objectives and
compare CoCa with single-objective variants in Table \ref{tab:loss_ablation}.
Compared to the contrastive-only model,
CoCa significantly improves both zero-shot alignment and VQA (notice that the contrastive-only model requires additional fusion for VQA).
CoCa performs on par with the captioning-only model on VQA while it additionally enables retrieval-style tasks such as zero-shot classification.
Table \ref{tab:loss_ratio} further studies loss ratios and suggests that the captioning loss not only improves VQA but also zero-shot alignment between modalities.
We hypothesize that generative objectives learn fine-grained text representations that further improve text understanding.
Finally, we compare training costs in Table \ref{tab:loss_ablation} (measured in TPUv3-core-days; larger is slower) and find CoCa to be as efficient as the captioning-only model (\aka{naive encoder-decoder}) due to the sharing of compute between two objectives.
These suggest combining the two losses induces new capabilities and better performance with minimal extra cost.

\textbf{Unimodal and Multimodal Decoders.}
CoCa introduces a novel decoder design and we ablate its components.
In Table \ref{tab:decoder_layers},
we vary the number of unimodal decoder layers (while keeping the total number of layers the same).
Intuitively,
fewer unimodal text layers leads to worse zero-shot classification due to lack of capacity for good unimodal text understanding,
while fewer multimodal layers reduces the model's power to reason over multimodal inputs such as VQA.
Overall, we find decoupling the decoder in half maintains a good balance.
One possibility is that global text representation for retrieval doesn't require deep modules \cite{pham2021combined} while early fusion for shallow layers may also be unnecessary for multimodal understanding.
Next, we explore various options to extract unimodal text embeddings.
In particular, 
we experiment with the number of learnable \verb+[CLS]+ tokens as well as the aggregation design.
For the later,
we aggregate over either the \verb+[CLS]+ tokens only or the concatenation of \verb+[CLS]+ and the original input sentence.
Interestingly, in Table \ref{tab:text_pooler}
we find training a single \verb+[CLS]+ token without the original input
is preferred for both vision-only and crossmodal retrieval tasks.
This indicates that learning an additional simple sentence representation mitigates interference between contrastive and captioning loss,
and is powerful enough for strong generalization.


\textbf{Attentional Poolers.}
CoCa exploits attentional poolers in its design both for different pretraining objectives and objective-specific downstream task adaptations.
In pretraining, we compare a few design variants on using poolers for contrastive loss and generative loss: (1) the ``parallel'' design which extracts both contrastive and generative losses at the same time on Vision Transformer encoder outputs as shown in Figure~\ref{figs:coca_details}, and (2) the ``cascade'' design which applies the contrastive pooler on top of the outputs of the generative pooler. Table \ref{tab:img_pooler} shows the results of these variants.
Empirically,
we find at small scale the ``cascade'' version (contrastive pooler on top of the generative pooler) performs better and is used by default in all CoCa models.
We also study the effect of number of queries where  means no generative pooler is used (thus all ViT output tokens are used for decoder cross-attention).
Results show that both tasks prefer longer sequences of detailed image tokens at a cost of slightly more computation and parameters.
As a result,
we use a generative pooler of length 256 to improve multimodal understanding benchmarks while still maintaining the strong frozen-feature capability.