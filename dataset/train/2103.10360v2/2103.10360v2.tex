\begin{table*}[t]
    \caption{Results on the SuperGLUE dev set.} \label{tab:superglue}
    \centering
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{ll*{10}{r}}
        \toprule
        &Model &  \makecell{ReCoRD\\F1/Acc.} & \makecell{COPA\\Acc.} & \makecell{WSC\\Acc.} & \makecell{RTE\\Acc.} & \makecell{BoolQ\\Acc.} &  \makecell{WiC\\Acc.} & \makecell{CB\\F1/Acc.}  & \makecell{MultiRC\\F1a/EM} & Avg \\
        \midrule\midrule
        \multicolumn{5}{l}{\textit{Pretrained on BookCorpus and Wikipedia}}\0.02in]
        &T5\base & 76.2 / 75.4 & 73.0 & 79.8 & 78.3 & 80.8 & 67.9& 94.8 / 92.9 & 76.4 / 40.0 & 76.0 \\
        &T5\largem & 85.7 / 85.0 & 78.0 & \textbf{84.6} & 84.8 & 84.3 & 71.6 & 96.4 / 98.2 & 80.9 / 46.6 & 81.2 \\
        &BART\largem & 88.3 / 87.8 & 60.0 & 65.4 & 84.5 & 84.3 & 69.0 & 90.5 / 92.9 & 81.8 / 48.0 & 76.0 \\
        &RoBERTa\largem & 89.0 / 88.4 & \textbf{90.0} & 63.5 & 87.0 & \textbf{86.1} & \textbf{72.6} & 96.1 / 94.6 & \textbf{84.4 / 52.9} & 81.5 \\
        &\modelx\roberta & \textbf{89.6 / 89.0} & 82.0 & 83.7 & \textbf{87.7} & 84.7 & 71.2& \textbf{98.7 / 98.2} & 82.4 / 50.1 & \textbf{82.9}\\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\section{Experiments}
\label{sec:experiment}

We now describe our pretraining setup and the evaluation of downstream tasks.



\subsection{Pretraining Setup}
\label{subsec:data}
For a fair comparison with BERT~\cite{devlinBERT2019}, we use BooksCorpus~\cite{BookCorpusZhu5} and English Wikipedia as our pretraining data. We use the uncased wordpiece tokenizer of BERT with 30k vocabulary. 
We train \modelx\base and \modelx\largem with the same architectures as BERT\base and BERT\largem, containing 110M and 340M parameters respectively. 

For multi-task pretraining, we train two Large-sized models with a mixture of the blank infilling objective and the document-level or sentence-level objective, denoted as \docmodel and \sentmodel.  Additionally, we train two larger \model models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M (30 layers, hidden size 1152, and 18 attention heads) parameters with document-level multi-task pretraining, denoted as \modelx and \modelx.




To compare with SOTA models, we also train a Large-sized model with the same data, tokenization, and hyperparameters as RoBERTa~\cite{RoBERTa}, denoted as \modelx\roberta.
Due to resource limitations, we only pretrain the model for 250,000 steps, which are half of RoBERTa and BART's training steps and close to T5 in the number of trained tokens. 
More experiment details can be found in \Cref{sec:appendix_pretrain}.

\begin{table*}
    \caption{Results of abstractive summarization on the CNN/DailyMail and XSum test sets.}
    \label{tab:cnndm_xsum}
    \centering
\small
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \multirowthead{2}{Model}& \multicolumn{3}{c}{CNN/DailyMail} & \multicolumn{3}{c}{XSum}\\
        & RG-1 & RG-2 & RG-L & RG-1 & RG-2 & RG-L\\
        \midrule
        BERTSumAbs~\cite{BERTSum2019} & 41.7 & 19.4 & 38.8 & 38.8 & 16.3 & 31.2 \\
        UniLMv2\base~\cite{UniLMv220} & 43.2 & 20.4 & 40.1 & 44.0 & 21.1 & 36.1 \\
        T5\largem~\cite{raffelT52020} & 42.5 & 20.7 & 39.8 & 40.9 & 17.3 & 33.0 \\
        BART\largem~\cite{lewisBART2019} & \textbf{44.2} & \textbf{21.3} & \textbf{40.9} & 45.1 & 22.3 & \textbf{37.3} \\
        \midrule
        \modelx\roberta & 43.8 & 21.0 & 40.5 & \textbf{45.5} & \textbf{23.5} & \textbf{37.3}\\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\subsection{SuperGLUE}
\label{subsec:single_obj}

To evaluate our pretrained \model models, we conduct experiments on the SuperGLUE benchmark~\cite{SuperGLUE2019} and report the standard metrics. SuperGLUE consists of 8 challenging NLU tasks.
We reformulate the classification tasks as blank infilling with human-crafted cloze questions, following PET~\cite{PET2:2009-07118}. Then we finetune the pretrained \model models on each task as described in \Cref{subsec:finetune}. The cloze questions and other details can be found in \Cref{subsec:appendix_superglue}.


For a fair comparison with \modelx\base and \modelx\largem, we choose BERT\base and BERT\largem as our baselines, which are pretrained on the same corpus and for a similar amount of time. We report the performance of standard finetuning (i.e. classification on the [CLS] token representation). The performance of BERT with cloze questions is reported in \Cref{subsec:ablation}. To compare with \modelx\roberta, we choose T5, BART\largem, and RoBERTa\largem as our baselines. T5 has no direct match in the number of parameters for BERT\largem, so we present the results of both T5\base (220M parameters) and T5\largem (770M parameters). All the other baselines are of similar size to BERT\largem.

\Cref{tab:superglue} shows the results. With the same amount of training data, \model consistently outperforms BERT on most tasks with either base or large architecture. The only exception is WiC (word sense disambiguation). On average, \modelx\base scores 4.6\% higher than BERT\base, and \modelx\largem scores 5.0\% higher than BERT\largem. It clearly demonstrates the advantage of our method in NLU tasks. In the setting of RoBERTa\largem, \modelx\roberta can still achieve improvements over the baselines, but with a smaller margin. Specifically, \modelx\roberta outperforms T5\largem but is only half its size. We also find that BART does not perform well on the challenging SuperGLUE benchmark. We conjecture this can be attributed to the low parameter efficiency of the encoder-decoder architecture and the denoising sequence-to-sequence objective. 

\subsection{Multi-Task Pretraining}
\label{subsec:multi_obj}
Then we evaluate the \model's performance in a multi-task setting (\Cref{subsec:objective}). Within one training batch, we sample short spans and longer spans (document-level or sentence-level) with equal chances. We evaluate the multi-task model for NLU, seq2seq, blank infilling, and zero-shot language modeling.

\textbf{SuperGLUE.}
For NLU tasks, we evaluate models on the SuperGLUE benchmark. The results are also shown in \Cref{tab:superglue}. We observe that with multi-task pretraining, \docmodel and \sentmodel perform slightly worse than \modelx\largem, but still outperform BERT\largem and UniLM\largem. Among multi-task models, \sentmodel outperforms \docmodel by 1.1\% on average. Increasing \docmodel's parameters to 410M (1.25BERT\largem) leads to better performance than \modelx\largem. \model with 515M parameters (1.5BERT\largem) can perform even better.



\begin{table}[t]
\centering
  \small
    \caption{Results on Gigaword summarization.}
     \label{tab:gigaword}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lccc}
        \toprule
Model & RG-1 & RG-2 & RG-L\\
        \midrule
        MASS & 37.7 & 18.5 & 34.9 \\
        UniLM\largem & 38.5 & 19.5 & 35.8 \\
        \midrule
        \modelx\largem & 38.6 & 19.7 & 36.0 \\
        \modelx\largemulti & 38.5 & 19.4 & 35.8\\
        \modelx & 38.9 & 20.0 & \textbf{36.3}\\
        \modelx & \textbf{38.9} & \textbf{20.0} & 36.2 \\
\bottomrule
    \end{tabular}
    }
\vspace{0.2in}
   \centering
    \caption{Results on SQuAD question generation.}
        \label{tab:squad_qg}
    \resizebox{0.88\linewidth}{!}{
     \begin{tabular}{lccc}
        \toprule
        Model &  BLEU-4 & MTR & RG-L\\
        \midrule
        SemQG & 18.4 & 22.7 & 46.7\\
        UniLM\largem & 22.1 & 25.1 & \textbf{51.1}\\
        \midrule
        \modelx\largem  & 22.4 & 25.2 & 50.4\\
        \modelx\largemulti & 22.3 & 25.0 & 50.2\\
        \modelx & 22.6 & 25.4 & 50.4 \\
        \modelx & \textbf{22.9} & \textbf{25.6} & 50.5\\
        \bottomrule
    \end{tabular}
    }
\vspace{0.2in}
    \caption{BLEU scores on Yahoo text infilling.  indicates the results from \cite{BLM2020}.}
    \label{tab:yahoo}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        Mask ratio         & 10\%&	20\%&	30\%&	40\%&	50\% \\\midrule
        BERT     &	82.8&	66.3&	50.3&	37.4&	26.2 \\
        BLM     &    86.5&	73.2&	59.6&	46.8&	34.8 \\
        \modelx\largem      &	\textbf{87.8}&	\textbf{76.7}&	\textbf{64.2}&	\textbf{48.9}&	\textbf{38.7} \\
        \modelx\largemulti  &	87.5&	76.0&	63.2&	47.9&	37.6 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{Sequence-to-Sequence.}
\label{sec:seq2seq}
Considering the available baseline results, we use the Gigaword dataset~\cite{GigawordRushCW15} for abstractive summarization and the SQuAD 1.1 dataset~\cite{SQuAD:RajpurkarZLL16} for question generation~\cite{NQG:du2017} as the benchmarks for models pretrained on BookCorpus and Wikipedia. Additionally, we use the CNN/DailyMail~\cite{GetPointSummarization2017} and XSum~\cite{XSum2018} datasets for abstractive summarization as the benchmarks for models pretrained on larger corpora. 


The results for models trained on BookCorpus and Wikipedia are shown in \Cref{tab:gigaword,tab:squad_qg}. We observe that \modelx\largem can achieve performance matching the other pretraining models on the two generation tasks.
\sentmodel can perform better than \largemodel, while
\docmodel performs slightly worse than \modelx\largem. This indicates that the document-level objective, which teaches the model to extend the given contexts, is less helpful to conditional generation, which aims to extract useful information from the context. Increasing \docmodel's parameters to 410M leads to the best performance on both tasks.
The results for models trained on larger corpora are shown in \Cref{tab:cnndm_xsum}. \modelx\roberta can achieve performance matching the seq2seq BART model, and outperform T5 and UniLMv2.

 
\textbf{Text Infilling.}
Text infilling is the task of predicting missing spans of text which are consistent with the surrounding context \cite{TextInfill2019,Fillin2020,BLM2020}. \model is trained with an autoregressive blank infilling objective, thus can straightforwardly solve this task. We evaluate \model on the Yahoo Answers dataset~\cite{YahooAnswers} and compare it with Blank Language Model (BLM) \cite{BLM2020}, which is a specifically designed model for text infilling. From the results in \Cref{tab:yahoo}, \model outperforms previous methods by large margins (1.3 to 3.9 BLEU) and achieves the state-of-the-art result on this dataset. We notice that \modelx\largemulti slightly underperforms \modelx\largem, which is consistent with our observations in the seq2seq experiments.








 
\textbf{Language Modeling.}
Most language modeling datasets such as WikiText103 are constructed from Wikipedia documents, which our pretraining dataset already contains.
Therefore, we evaluate the language modeling perplexity on a held-out test set of our pretraining dataset, which contains about 20M tokens, denoted as BookWiki. We also evaluate GLM on the LAMBADA dataset~\cite{LAMBADADataset2016}, which tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of a passage. As the baseline, we train a GPT\largem model~\cite{GPT2-2018,GPT3-2020} with the same data and tokenization as \modelx\largem. 

The results are shown in \Cref{tab:language_model}. All the models are evaluated in the zero-shot setting. Since \model learns the bidirectional attention, we also evaluate \model under the setting in which the contexts are encoded with bidirectional attention. Without generative objective during pretraining, \modelx\largem cannot complete the language modeling tasks, with perplexity larger than 100. With the same amount of parameters, \modelx\largemulti performs worse than GPT\largem. This is expected since \docmodel also optimizes the blank infilling objective. Increasing the model's parameters to 410M (1.25 of GPT\largem) leads to a performance close to GPT\largem. \modelx (1.5 of GPT\largem) can further outperform GPT\largem. With the same amount of parameters, encoding the context with bidirectional attention can improve the performance of language modeling. Under this setting, \modelx outperforms GPT\largem. This is the advantage of \model over unidirectional GPT. We also study the contribution of 2D positional encoding to long text generation. We find that removing the 2D positional encoding leads to lower accuracy and higher perplexity in language modeling.



\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/zslm2.pdf}
    \vspace{-0.1in}
    \caption{Zero-shot language modeling results.}
\label{tab:language_model}
\end{figure}

\begin{table*}
    \caption{Ablation study on the SuperGLUE dev set. (T5  GLM -- shuffle spans + sentinel tokens.)}
    \label{tab:classifier}
\centering
    \small
    \begin{tabular}{l*{10}{r}}
        \toprule
        Model &  \makecell{ReCoRD\\F1/Acc.} & \makecell{COPA\\Acc.} & \makecell{WSC\\Acc.} & \makecell{RTE\\Acc.} & \makecell{BoolQ\\Acc.} &  \makecell{WiC\\Acc.} & \makecell{CB\\F1/Acc.}  & \makecell{MultiRC\\F1a/EM} & Avg \\
        \midrule
BERT\largem & 76.3 / 75.6 & 69.0 & 64.4 & 73.6 & 80.1 & \textbf{71.0} & 94.8 / 92.9 & 71.9 / 24.1 & 72.0 \\
        BERT\largem (reproduced) & \textbf{82.1 / 81.5} & 63.0 & 63.5 & 72.2 & 80.8 & 68.7 & 80.9 / 85.7 & 77.0 / 35.2 & 71.2 \\
        BERT\largem (cloze) & 70.0 / 69.4 & \textbf{80.0} & 76.0 & 72.6 & 78.1 & 70.5 & 93.5 / 91.1 & 70.0 / 23.1 & 73.2 \\
        \modelx\largem & 81.7 / 81.1 & 76.0 & \textbf{81.7} & 74.0 & \textbf{82.1} & 68.5 & \textbf{96.1 / 94.6} & 77.1 / 36.3 & \textbf{77.0}\\
        \hspace{1em}-- cloze finetune & 81.3 / 80.6 & 62.0 & 63.5 & 66.8 & 80.5 & 65.0 & 89.2 / 91.1 & 72.3 / 27.9 & 70.0\\
        \hspace{1em}-- shuffle spans & 82.0 / 81.4 & 61.0 & 79.8 & 54.5 & 65.8 & 56.3 & 90.5 / 92.9 & 76.7 / 37.6 & 68.5\\
        \hspace{1em}+ sentinel tokens & 81.8 / 81.3 & 69.0 & 78.8 & \textbf{77.3} & 81.2 & 68.0 & 93.7 / 94.6 & \textbf{77.5 / 37.7} & 76.0\\
        \bottomrule
    \end{tabular}
\end{table*}


\textbf{Summary.} 
Above all, we conclude that \model effectively shares model parameters across natural language understanding and generation tasks, achieving better performance than a standalone BERT, encoder-decoder, or GPT model. 


\subsection{Ablation Study}
\label{subsec:ablation}

\Cref{tab:classifier} shows our ablation analysis for \model. First, to provide an apple-to-apple comparison with BERT, we train a BERT\largem model  with our implementation, data, and hyperparameters (row 2). The performance is slightly worse than the official BERT\largem and significantly worse than GLM\largem. It confirms the superiority of \model over Masked LM pretraining on NLU tasks. Second, 
we show the SuperGLUE performance of \model finetuned as sequence classifiers (row 5) and BERT with cloze-style finetuning (row 3). Compared to BERT with cloze-style finetuning, \model benefits from the autoregressive pretraining. Especially on ReCoRD and WSC, where the verbalizer consists of multiple tokens, \model consistently outperforms BERT. This demonstrates \model's advantage in handling variable-length blank. Another observation is that the cloze formulation is critical for \model's performance on NLU tasks. For the large model, cloze-style finetuning can improve the performance by 7 points. Finally, we compare \model variants with different pretraining designs to understand their importance. Row 6 shows that removing the span shuffling (always predicting the masked spans from left to right) leads to a severe performance drop on SuperGLUE. Row 7 uses different sentinel tokens instead of a single  token to represent different masked spans. The model performs worse than the standard \model. We hypothesize that it wastes some modeling capacity to learn the different sentinel tokens which are not used in downstream tasks with only one blank. In \Cref{tab:language_model}, we show that removing the second dimension of 2D positional encoding hurts the performance of long text generation.

We note that T5 is pretrained with a similar blank infilling objective. \model differs in three aspects: (1) \model consists of a single encoder, (2) \model shuffles the masked spans, and (3) \model uses a single [MASK] instead of multiple sentinel tokens. While we cannot directly compare \model with T5 due to the differences in training data and the number of parameters, the results in \Cref{tab:superglue,tab:classifier} have demonstrated the advantage of \model.