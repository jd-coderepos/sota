\subsection{$\omega$ choices}


Algorithms \ref{algo::add} and \ref{algo::tdadd} rely on relaxation
parameters which have severe impact on the efficiency and stability of the
resulting multigrid solver.
Proper choices deliver several well-established multigrid flavours. 
While the semantics of the Jacobi relaxation $\omega_S$ in Algorithm 
\ref{algo::add} is well-understood and can be studied in terms of local Fourier
analysis, Algorithm \ref{algo::add} also introduces $\omega_{cg}$-scaling of the
coarse grid contribution.
Multiple valid choices for this parameter do exist with
different properties \cite{Bastian:98:AdditiveVsMultiplicativeMG}.
In Algorithm \ref{algo::tdadd}, we refrain from distinguishing $\omega_{S}$ and
$\omega_{cg}$ in the formula but instead introduce a vertex-dependent
relaxation $\omega_\ell$, i.e.~each vertex may have its individual relaxation
factor.
As a vertex is unique due to its spatial position plus its level, this 
facilitates level-dependent $\omega$ choices.


Let $succ(v) \in \{0,\ldots,\ell _{max}-\ell _{min}\}$ be an integer
variable per vertex $v$ with
\[
  succ(v) = \left\{ 
    \begin{array}{ll}
      0 & \mbox{if $v$ is an unrefined vertex, or} \\
      \min _{i} (succ(v_i))+1 & \mbox{for all children $v_i$ of $v$
      otherwise.}
    \end{array}
  \right.
\]
A child vertex of a parent vertex is a vertex with at least one adjacent cell
whose parent in turn is adjacent to the parent vertex.
This property deduces from the parent-child relation on the tree.
Furthermore, we define the predicate $cPoint$ that holds for any
vertex whose spatial position coincides with a vertex position on the next coarser levels.
The predicate distinguishes c-points from f-points in the multigrid terminology.
We obtain various smoother variants:

\begin{center}
 \begin{tabular}{l|p{7cm}}
   Relaxation parameter & Description \\
   \hline
   \\[-0.2cm]
   $\omega_\ell(v)=\left\{  
   \begin{array}{lcl}
     \omega_{S}<1 & \mbox{if} & succ(v)=0 \ \mbox{and} \\
     0           && \mbox{otherwise.}
   \end{array}
   \right.$
   & Relaxed Jacobi on the dynamically adaptive
   grid as the {\it coarse relaxation parameter equals zero}. 
   \\ 
   \hline
   \\[-0.2cm]
   $\omega_\ell(v)= \omega_{S} < 1$
   & 
   {\it Undamped coarse grid correction}.
   \\
   \hline
   \\[-0.2cm]
  $\omega_\ell (v)=\left\{  
    \begin{array}{lcl}
      \omega_{S}<1 & \mbox{if} & succ(v)\leq L  \  \mbox{and} \\
      0           && \mbox{otherwise}
    \end{array}
  \right.$ 
  &
  {\it Undamped $L$-grid} scheme on adaptive grids
  \\
   \hline
   \\[-0.2cm]
  $\omega_\ell (v)= \left( \omega_{S} \right)^{succ(v)+1}\ \ \omega_S<1$
  &
  Classic additive multigrid from \cite{Bastian:98:AdditiveVsMultiplicativeMG} where
  coarse grid updates contribute to the fine grid solution with an {\em
  exponential damping}.
  \\
   \hline
   \\[-0.2cm]
  $\omega_\ell (v)= \left( \omega_{S} \right) ^{ (1-1/n) \cdot
  (succ(v)+1)}$
  &
  {\em Transition} relaxation. $n$ is the iteration counter.
  \\
  \hline
 \end{tabular}
\end{center}




\noindent
Schemes that use the same $\omega$ on each and every level (undamped coarse grid
correction) become unstable \cite{Bastian:98:AdditiveVsMultiplicativeMG} for
setups with many levels.
They tend to overshoot.
If only one or two grids are used for the multigrid scheme ($L\in\{2,3\}$), this
overshooting is not that significant and solvers are more robust.
However, we loose multigrid efficiency.
Exponential damping is thus used by most codes.
The coarser the grid the smaller its influence on the actual solution.
This renders an exact coarse grid solve unnecessary.
With the relaxation factors from above all the schemes apply straightforwardly
to dynamically adaptive grids.
Empirically, we observe that undamped schemes outperform their stable
counterparts in the first few iterations. 
The overshooting is not dominant yet. 
We therefore propose a hybrid smoother choice that transitions from an undamped
coarse grid correction into exponential damping. 
  


\subsection{Hierarchical basis and BPX-type solvers}


\begin{figure}[htb]
  \begin{center} 
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/transition-05.png}
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/hb-05.png}
    \\
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/transition-08.png}
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/hb-08.png}
    \\
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/transition-18.png}
    \includegraphics[width=0.35\textwidth]{experiments/illustrations/hb-vs-bpx/hb-18.png}
  \end{center}
  \vspace{-0.4cm}
  \caption{
    Snapshot of the solution of $-\Delta u + 1000\ u = \chi$ with the transition
    scheme of the plain additive multigrid (left) and the hierarchical basis
    approach (right) after 5, 8 and 18 iterations. $\chi$ is a load of one within a circle around the
    domain's centre, i.e.~a characteristic function which can be modelled via a
    Heavyside operator.
    For $-\phi \approx 4000$, the plain additive scheme
    left becomes unstable due to overshooting already visible here.
    The \texttt{hb}-scheme updates per level only additional fine grid points
    compared to the coarser grids and thus is less sensitive to overshooting. 
    Both visualisations set hanging nodes to value zero. 
    The rough components in the pictures thus are visualisation artefacts; no
    real high frequency contributions.
   \label{fig::add-vs-hb}
  }
\end{figure}


While additive multigrid with exponential damping or transition is robust for
the Poisson equation, it runs into instabilities if we encounter a non-zero
$\phi<0$ term in \eqref{eq::pHelmholtz}; despite the fact that the problem
remains well-posed positive-definite on all levels due to the additional minus sign in front of $\phi$.
The solver is sensitive to the reaction term.
Robustness with respect to a reaction term however is mandatory prior to
tackling any ill-definiteness. 

We find the shift $\phi<0$ make the additive multigrid
overshoot on coarser levels, pollute the approximation and introduce a non-local
oscillation in the follow-up iteration.
The overshooting/oscillation typically grows per iteration if the diffusive
operator is not dominating (Figure \ref{fig::add-vs-hb}).
A straightforward fix to this instability is the switch from a
hierarchical generating system into a hierarchical basis
\cite{Griebel:90:HTMM,Griebel:94:Multilevel}.
It is identified by an \texttt{hb-}
prefix from here on.
Following \cite{Bastian:98:AdditiveVsMultiplicativeMG}, such a switch results
from a modification of the generic relaxation parameter into
\begin{equation}
  \omega_\ell (v) \gets \left\{ 
    \begin{array}{rcl}
      0 & \mbox{if} & cPoint(v) \qquad \mbox{and} \\
      \omega_\ell (v) & & \mbox{otherwise.} 
    \end{array}
  \right..
  \label{equation:hb-omega}
\end{equation}

\noindent
We mask out c-points.
Such a modification unfolds a variety of reasonable
and unreasonable smoothing schemes due to the various choices of
$\omega_\ell(v)$ on the right-hand side.
Our numerical results detail this.


While \eqref{equation:hb-omega} with its localised updates---vertices coinciding spatially are updated solely on the
coarsest level---prevents the additive scheme from overshooting too
significantly for $\phi<0$, it comes at the price of a deteriorating convergence
speed. 
It continues to assume a uniform smooth geometric multiscale behaviour of the solution, as 
any unknown update is determined by the update in the point plus the c-point
updates of surrounding vertices.
Increasing absolute values of $\phi$ in combination with non-smooth right-hand
sides however decreases the smoothness of the solution along jumps of
the latter.
This becomes apparent immediately at hands of a gedankenexperiment with a Heavyside $\chi$.
A \texttt{hb-}solver locally overshoots where $\chi$ changes and the overall
approximation starts to creep towards the correct solution due
to local oscillations while non-local oscillations are eliminated.

One fix to this challenge adds an additional $-PI\omega _\ell
S(u_\ell, b_\ell)$ term to all smoothing updates.
This is known as BPX
\cite{Bastian:98:AdditiveVsMultiplicativeMG,Bramble:90:BPX}.
Reiterating through our data dependency analysis, we find that the BPX operator
can not be implemented straightforwardly within Algorithm
\ref{algo::tdadd}---even with the pipelining variables in place---as 
c-point impacts spread to their surrounding through the coarser grids while they
are not altered themselves.

These considerations lead to a BPX FAS in Algorithm
\ref{algo::tdbpx}.
The key idea is to keep $sf$ and $sc$ and to introduce another helper variable
$si$ holding the injected value of a smoother update without any c-point
distinction. 
It is set as soon as we determine the smoother impact.
This impact is discarded for c-points due to
\eqref{equation:hb-omega}.
Finally, the one-sweep realisation modifies the prolongation by adding an
additional $-Psi$ term. 
\eqref{equation:hb-omega} in combination with this term ensures the BPX
inter-level correlation as we have $(id - PI)=0$ on vertices for
which $cPoint$ holds.

\begin{algorithm}[htb]
 \SetAlgoNoLine
    \begin{algorithmic}[1]
       \Function{tdBPX}{$\ell $}
         \State $sc_\ell \leftarrow sc_\ell + P sc_{\ell -1}$ 
         \State \If{not $cPoint(v)$}
         {
           \State \phantom{xx} $sc_{\ell} \leftarrow sc_{\ell} -
           Psi_{\ell-1}$
           \Comment BPX-type modification of fine grid correction
           \label{line:tdbpx:pmodification}
         }         
         \State $u_\ell \leftarrow u_\ell + sc_{\ell} + sf_{\ell}$
         \State $\hat u \leftarrow u_\ell - Pu_{\ell -1}$
         \State
         \If{$\ell < \ell _{max}$} {
           \State \phantom{xx} \Call{tdBPX}{$\ell +1$}
         }
         \State $r_\ell \leftarrow b_\ell - H_\ell u_\ell$
         \State $\hat r_\ell \leftarrow b_\ell - H_\ell \hat u_\ell$
        \State
		\eIf{$cPoint(v)$} {
			\State \phantom{xx}  $sc_\ell (v) \leftarrow 0$
			\Comment Realise \eqref{equation:hb-omega}, i.e.~cancel out
			update}{
			\State \phantom{xx}  $sc_\ell \leftarrow \omega _{\ell} S(u_\ell,b_\ell)$
			\Comment{Anticipate coarse correction}
		}
		\State
         \If{$\ell > \ell _{min}$}{
           \State \phantom{xx} $si_{\ell-1} \leftarrow I\omega _{\ell}
           S(u_\ell,b_\ell)$
             \Comment{Memorise dropped fine grid update}
           \State \phantom{xx} $b_{\ell -1} \leftarrow R \hat r_\ell$
           \State \phantom{xx} $sf_{\ell -1} \leftarrow I\left( sf_\ell +
           sc_\ell \right)$
         }
      \EndFunction
  \end{algorithmic}
  \caption{
    Single-sweep BPX variant realisation incorporating FAS. Invoked by
     \textsc{tdBPX}($\ell _{min}$). We do not rely on
     \eqref{equation:hb-omega} here, i.e.~$\omega $ is $cPoint$-agnostic, as we
     realise the case distinction within the multilevel code.
    \label{algo::tdbpx}
  }
\end{algorithm} 
 

We emphasise that \eqref{equation:hb-omega} follows the same pipelining idea we
introduced for the additive scheme and at the same time renders the storage of a
fine grid correction $sf$ unnecessary.
We could add any fine smoothing impact directly onto $sc$ and at the same time
skip the injection of $sf_\ell + sc_\ell$.
Such a BPX realisation uses the same data layout as the additive multigrid.
No $sf$ is to be held, but we need an additional $si$.
This preserves the number of variables.
The reason for this possibility results from the fact that the coarsest
vertex in $\Omega_h$ holds the valid nodal representation of the
solution in Algorithm
\ref{algo::tdbpx}.
All finer vertices at the same location are copies.
We preserve $sf$ in the presented code to emphasise the closeness to the
additive scheme.
While this wastes one entry per vertex, it might
make sense to preserve the fine grid injection and thus to allow
BPX's fine grid update to change $cPoints$ as well:
in applications with non-trivial boundary conditions, those sometimes are
simpler to evaluate in a nodal setting rather than a hierarchical basis.
The injection then automatically reconstructs the data consistency on all
levels.


\subsection{Feature-based dynamic adaptivity}

All algorithmic ingredients introduced are well-suited for any arbitrary 
adaptivity.
Throughout the top down steps, we may add any number of vertices as
long as we initialise their hierarchical surplus with 0 and prolong the solution
$p$-linearly.
They then seamlessly integrate into the solver's workflow. 
For faster convergence, higher order interpolation might be advantageous.
Discarding vertices is permitted throughout the backtracking, i.e.~the steps up
in the grid hierarchy.
The FAS ensures that all solution information is already available on the
coarsened mesh.
Multilevel meta information such as $cPoint$ or $succ(v)$ can be computed
on-the-fly throughout the tree traversal's backtracking.
It then automatically adopts to updated refinement patterns.


In the present paper, we stick to simple feature-based refinement and specify
both regular grids and adaptive grids through a maximal and
minimal mesh size $h_{max}$ and $h_{min}$.
We start from a grid satisfying $h_{max}$ and, in parallel to
the smoothing steps, measure the value $s = \max _{d \in \{1,\ldots,p\}} 
|\Delta _d u|$ per vertex on each grid level.
Per step, we refine the 10\% of the vertices with the highest $s$ value, while
we erase the 2\% vertices with the smallest $s$ value.
These values are shots from the hip but empirically show reasonable
grid refinement structures.
They yield a grid that adopts itself to solution characteristics.
We realise feature-based adaptivity.
More sophisticated schemes with proper error estimators are out of scope.


To avoid global sorting, we split up the whole span of $s$
values into 20 subranges and bin vertices into these ranges.
All vertices fitting into a fixed number of bins holding the largest $s$
values are refined.
This fixed number is selected such that the 10\% goal is met as close as
possible.
Erasing works analogously with the bins with the smallest $s$ values.
Refining and erasing are vetoed in two cases:
if maximal or minimal mesh constraints would be violated;
or if residual divided by diagonal element exceeds $10^{-2}$.
In the latter case, the vertex is still subject to major updates, i.e.~has not
`converged', and we postpone a refinement or coarsening.


The interplay of the feature-based refinement with the creation of a FMG cycle
is detailed in Remark \ref{remark:fmg:cycle}.
We note that our criterion yields different grid refinement patterns
for different solvers as we integrate refinement into the solve (Figure
\ref{fig::add-vs-hb}).
This advocates for better criteria and renders the present experiments 
feasibility studies.
