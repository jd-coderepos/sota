\subsection{ choices}


Algorithms \ref{algo::add} and \ref{algo::tdadd} rely on relaxation
parameters which have severe impact on the efficiency and stability of the
resulting multigrid solver.
Proper choices deliver several well-established multigrid flavours. 
While the semantics of the Jacobi relaxation  in Algorithm 
\ref{algo::add} is well-understood and can be studied in terms of local Fourier
analysis, Algorithm \ref{algo::add} also introduces -scaling of the
coarse grid contribution.
Multiple valid choices for this parameter do exist with
different properties \cite{Bastian:98:AdditiveVsMultiplicativeMG}.
In Algorithm \ref{algo::tdadd}, we refrain from distinguishing  and
 in the formula but instead introduce a vertex-dependent
relaxation , i.e.~each vertex may have its individual relaxation
factor.
As a vertex is unique due to its spatial position plus its level, this 
facilitates level-dependent  choices.


Let  be an integer
variable per vertex  with

A child vertex of a parent vertex is a vertex with at least one adjacent cell
whose parent in turn is adjacent to the parent vertex.
This property deduces from the parent-child relation on the tree.
Furthermore, we define the predicate  that holds for any
vertex whose spatial position coincides with a vertex position on the next coarser levels.
The predicate distinguishes c-points from f-points in the multigrid terminology.
We obtain various smoother variants:

\begin{center}
 \begin{tabular}{l|p{7cm}}
   Relaxation parameter & Description \\
   \hline
   \-0.2cm]
   
   & 
   {\it Undamped coarse grid correction}.
   \\
   \hline
   \-0.2cm]
  
  &
  Classic additive multigrid from \cite{Bastian:98:AdditiveVsMultiplicativeMG} where
  coarse grid updates contribute to the fine grid solution with an {\em
  exponential damping}.
  \\
   \hline
   \
  \omega_\ell (v) \gets \left\{ 
    \begin{array}{rcl}
      0 & \mbox{if} & cPoint(v) \qquad \mbox{and} \\
      \omega_\ell (v) & & \mbox{otherwise.} 
    \end{array}
  \right..
  \label{equation:hb-omega}


\noindent
We mask out c-points.
Such a modification unfolds a variety of reasonable
and unreasonable smoothing schemes due to the various choices of
 on the right-hand side.
Our numerical results detail this.


While \eqref{equation:hb-omega} with its localised updates---vertices coinciding spatially are updated solely on the
coarsest level---prevents the additive scheme from overshooting too
significantly for , it comes at the price of a deteriorating convergence
speed. 
It continues to assume a uniform smooth geometric multiscale behaviour of the solution, as 
any unknown update is determined by the update in the point plus the c-point
updates of surrounding vertices.
Increasing absolute values of  in combination with non-smooth right-hand
sides however decreases the smoothness of the solution along jumps of
the latter.
This becomes apparent immediately at hands of a gedankenexperiment with a Heavyside .
A \texttt{hb-}solver locally overshoots where  changes and the overall
approximation starts to creep towards the correct solution due
to local oscillations while non-local oscillations are eliminated.

One fix to this challenge adds an additional  term to all smoothing updates.
This is known as BPX
\cite{Bastian:98:AdditiveVsMultiplicativeMG,Bramble:90:BPX}.
Reiterating through our data dependency analysis, we find that the BPX operator
can not be implemented straightforwardly within Algorithm
\ref{algo::tdadd}---even with the pipelining variables in place---as 
c-point impacts spread to their surrounding through the coarser grids while they
are not altered themselves.

These considerations lead to a BPX FAS in Algorithm
\ref{algo::tdbpx}.
The key idea is to keep  and  and to introduce another helper variable
 holding the injected value of a smoother update without any c-point
distinction. 
It is set as soon as we determine the smoother impact.
This impact is discarded for c-points due to
\eqref{equation:hb-omega}.
Finally, the one-sweep realisation modifies the prolongation by adding an
additional  term. 
\eqref{equation:hb-omega} in combination with this term ensures the BPX
inter-level correlation as we have  on vertices for
which  holds.

\begin{algorithm}[htb]
 \SetAlgoNoLine
    \begin{algorithmic}[1]
       \Function{tdBPX}{}
         \State  
         \State \If{not }
         {
           \State \phantom{xx} 
           \Comment BPX-type modification of fine grid correction
           \label{line:tdbpx:pmodification}
         }         
         \State 
         \State 
         \State
         \If{} {
           \State \phantom{xx} \Call{tdBPX}{}
         }
         \State 
         \State 
        \State
		\eIf{} {
			\State \phantom{xx}  
			\Comment Realise \eqref{equation:hb-omega}, i.e.~cancel out
			update}{
			\State \phantom{xx}  
			\Comment{Anticipate coarse correction}
		}
		\State
         \If{}{
           \State \phantom{xx} 
             \Comment{Memorise dropped fine grid update}
           \State \phantom{xx} 
           \State \phantom{xx} 
         }
      \EndFunction
  \end{algorithmic}
  \caption{
    Single-sweep BPX variant realisation incorporating FAS. Invoked by
     \textsc{tdBPX}(). We do not rely on
     \eqref{equation:hb-omega} here, i.e.~ is -agnostic, as we
     realise the case distinction within the multilevel code.
    \label{algo::tdbpx}
  }
\end{algorithm} 
 

We emphasise that \eqref{equation:hb-omega} follows the same pipelining idea we
introduced for the additive scheme and at the same time renders the storage of a
fine grid correction  unnecessary.
We could add any fine smoothing impact directly onto  and at the same time
skip the injection of .
Such a BPX realisation uses the same data layout as the additive multigrid.
No  is to be held, but we need an additional .
This preserves the number of variables.
The reason for this possibility results from the fact that the coarsest
vertex in  holds the valid nodal representation of the
solution in Algorithm
\ref{algo::tdbpx}.
All finer vertices at the same location are copies.
We preserve  in the presented code to emphasise the closeness to the
additive scheme.
While this wastes one entry per vertex, it might
make sense to preserve the fine grid injection and thus to allow
BPX's fine grid update to change  as well:
in applications with non-trivial boundary conditions, those sometimes are
simpler to evaluate in a nodal setting rather than a hierarchical basis.
The injection then automatically reconstructs the data consistency on all
levels.


\subsection{Feature-based dynamic adaptivity}

All algorithmic ingredients introduced are well-suited for any arbitrary 
adaptivity.
Throughout the top down steps, we may add any number of vertices as
long as we initialise their hierarchical surplus with 0 and prolong the solution
-linearly.
They then seamlessly integrate into the solver's workflow. 
For faster convergence, higher order interpolation might be advantageous.
Discarding vertices is permitted throughout the backtracking, i.e.~the steps up
in the grid hierarchy.
The FAS ensures that all solution information is already available on the
coarsened mesh.
Multilevel meta information such as  or  can be computed
on-the-fly throughout the tree traversal's backtracking.
It then automatically adopts to updated refinement patterns.


In the present paper, we stick to simple feature-based refinement and specify
both regular grids and adaptive grids through a maximal and
minimal mesh size  and .
We start from a grid satisfying  and, in parallel to
the smoothing steps, measure the value  per vertex on each grid level.
Per step, we refine the 10\% of the vertices with the highest  value, while
we erase the 2\% vertices with the smallest  value.
These values are shots from the hip but empirically show reasonable
grid refinement structures.
They yield a grid that adopts itself to solution characteristics.
We realise feature-based adaptivity.
More sophisticated schemes with proper error estimators are out of scope.


To avoid global sorting, we split up the whole span of 
values into 20 subranges and bin vertices into these ranges.
All vertices fitting into a fixed number of bins holding the largest 
values are refined.
This fixed number is selected such that the 10\% goal is met as close as
possible.
Erasing works analogously with the bins with the smallest  values.
Refining and erasing are vetoed in two cases:
if maximal or minimal mesh constraints would be violated;
or if residual divided by diagonal element exceeds .
In the latter case, the vertex is still subject to major updates, i.e.~has not
`converged', and we postpone a refinement or coarsening.


The interplay of the feature-based refinement with the creation of a FMG cycle
is detailed in Remark \ref{remark:fmg:cycle}.
We note that our criterion yields different grid refinement patterns
for different solvers as we integrate refinement into the solve (Figure
\ref{fig::add-vs-hb}).
This advocates for better criteria and renders the present experiments 
feasibility studies.
