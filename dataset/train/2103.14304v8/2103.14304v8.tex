\documentclass[journal]{IEEEtran}

\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage[]{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage[colorlinks]{hyperref}
\usepackage{cite}

\hyphenation{}

\begin{document}

\title{Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation}

\author{
Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang
\thanks{ Corresponding author.}
\thanks{W. Li, H. Liu, and R. Ding are with Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University, Beijing 100871, China. 
E-mail: \{wenhaoli, hongliu, dingrunwei\}@pku.edu.cn. 
M. Liu is with School of Intelligent Systems Engineering, Sun Yat-sen University, China. E-mail: nkliuyifang@gmail.com. 
P. Wang is with Alibaba Group, Bellevue, WA, 98004, USA. 
E-mail: pichao.wang@alibaba-inc.com. 
W. Yang is with the Shenzhen Key Lab of Information Science and Technology, Shenzhen Engineering Lab of
IS\&DRM, Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Shenzhen 518055, China. 
E-mail: yang.wenming@sz.tsinghua.edu.cn

This work is supported by National Key R\&D Program of China (No. 2020AAA0108904), Basic and Applied Basic Research Foundation of Guangdong (No. 2020A1515110370), Science and Technology Plan of Shenzhen (Nos. JCYJ20190808182209321, JCYJ20200109140410340).}
}

\markboth{}
{Li \MakeLowercase{\textit{et al.}}: 
Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation}

\maketitle

\begin{abstract}
Despite the great progress in 3D human pose estimation from videos, it is still an open problem to take full advantage of a redundant 2D pose sequence to learn representative representations for generating one 3D pose. To this end, we propose an improved Transformer-based architecture, called Strided Transformer, which simply and effectively lifts a long sequence of 2D joint locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce the redundancy of the sequence, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively shrink the sequence length and aggregate information from local contexts. The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE. STE not only effectively aggregates long-range information to a single-vector representation in a hierarchical global and local fashion, but also significantly reduces the computation cost. Furthermore, a full-to-single supervision scheme is designed at both full sequence and single target frame scales applied to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision and hence helps produce smoother and more accurate 3D poses. The proposed Strided Transformer is evaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and achieves state-of-the-art results with fewer parameters. Code and models are available at \url{https://github.com/Vegetebird/StridedTransformer-Pose3D}. 
\end{abstract}

\begin{IEEEkeywords}
3D human pose estimation, Transformer, Strided convolution.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{3}{D} human pose estimation is a classic computer vision task that aims to estimate 3D joint locations of a human body from images or videos. 
This task has drawn tremendous attention in the past decades~\cite{radwan2013monocular,li20143d,zhao20183,hu20213dbodynet} since it plays a significant role in wide applications, such as clinic~\cite{kadkhodamohammadi2021generalizable}, computer animation~\cite{pullen2002motion}, action recognition~\cite{wang2018depth,liu2017robust,liu2018recognizing,wei2019learning,song2021constructing,chen2021learning,li2021memory,yang2021unik,zhang2017action,chen2017multi}, and human-robot interaction~\cite{garcia2019human,gui2018teaching}. 
Many state-of-the-art approaches adopt a two-stage pipeline (\emph{i.e.}, 2D-to-3D lifting method)~\cite{martinez2017simple,pavllo20193d,hua2021weakly}, which first estimates 2D keypoints and then lifts them to 3D space. 
Although the 2D-to-3D lifting method benefits from the reliable performance of 2D pose detectors, it is still a highly ill-posed problem due to the inherent ambiguity in depth, since multiple 3D interpretations can be projected to the same 2D pose in the image space. 

To alleviate this problem, temporal context information has been investigated by many researchers. 
Some methods~\cite{lee2018propagating,rayat2018exploiting,cai2019exploiting} leverage past and future data in the sequence to predict the 3D pose of the target frame. 
For instance, Cai \emph{et al.}~\cite{cai2019exploiting} presented a local-to-global graph convolutional network to exploit spatio-temporal relations to estimate 3D keypoints from a 2D pose sequence.
However, these approaches have small temporal receptive fields and limited temporal correlation windows, thus suffering from modeling long-range dependencies. 

\begin{figure}[t]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/moti.pdf}
   \caption
   {
      Our Strided Transformer Encoder (STE) takes the outputs of Vanilla Transformer Encoder (VTE) as input (yellow) and generates a 3D pose for the target frame as output (top). 
      The self-attention mechanism (blue) concentrates on global context and the strided convolution (green) aggregates information from local contexts. 
   }
   \label{fig:moti}
\end{figure}

\begin{figure*}[t]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/2d_pose.pdf}
   \caption
   {
      Example of 2D pose sequences of 27 consecutive frames (520 ms) on Human3.6M dataset (captured from 50 Hz cameras). 
      It contains huge redundant information as nearby poses are same. 
      The rectangle denotes the center frame. 
   }
   \label{fig:2d_pose}
\end{figure*}

Vanilla Transformer~\cite{Attention} is developed for exploiting long-range dependencies and achieves tremendous success in natural language processing~\cite{tay2020efficient,zihang2020funnel-transformer} and computer vision~\cite{han2020survey,he2021transreid,li2021trear,han2020exploiting,li2021transformer}. 
It consists of a self-attention module and a position-wise feed-forward network (FFN). 
The self-attention module computes pairwise dot-product among all input elements to capture global-context information, and the FFN acts as pattern detectors over the input across all layers~\cite{geva2020transformer}. 
Such a design looks like a good choice for the 2D-to-3D pose lifting method to capture long-range dependencies. 
However, there are several shortcomings in the Vanilla Transformer Encoder (VTE)~\cite{Attention}:
(i) The full-length sequence in the forward pass across all layers contains significant redundancy for video-based pose estimation as nearby poses are quite similar, as illustrated in Fig.~\ref{fig:2d_pose}. 
(ii) The time and memory complexity of the attention operation grows quadratically with the input length, making it very expensive to process long sequences. 
Thus, the receptive field may be forced to decrease in real-time applications, whereas a large receptive field is important to enhance the estimation consistency~\cite{liu2020attention}. 
(iii) The VTE architecture is less capable to extract fine-grained local feature patterns, which is well-known to be crucial for computer vision tasks. 
To mitigate these issues, we propose to gradually merge nearby poses to shrink the sequence length until one representation of the target pose is acquired. 
An alternative is to perform pooling operation after the FFN~\cite{zihang2020funnel-transformer}. 
However, lots of valuable information will be lost using pooling operation, and the local information can not be well exploited. 
Motivated by the previous methods~\cite{pavllo20193d,liu2020attention} that are able to elegantly handle variable-length sequences via temporal convolutions, we propose to replace fully-connected layers in FFN with strided convolutions to progressively reduce the sequence length. 
The modified Transformer is dubbed Strided Transformer Encoder (STE), as shown in Fig.~\ref{fig:moti}. 
With the proposed STE, we can model both global and local information in a hierarchical architecture, and the computation in FFN can be traded off for constructing a deeper model to boost the model capacity. 

Although the STE can aggregate long-range information to a single-pose representation, it remains a question whether this single representation is enough to represent a long sequence and how to make this representation work in improving the performance. 
We observe that directly supervising the model at a single target frame scale always breaks temporal smoothness among video frames, while only supervising at a full sequence scale cannot explicitly learn a specific representation for the target frame. 
These observations encourage us to develop a method that can effectively embed both scales into a learnable framework. 
Therefore, based on the outputs of VTE and STE, a full-to-single supervision scheme is designed at both full and single scales, which can impose extra temporal smoothness constraints at the full sequence scale and refine the estimation at the single target frame scale. 
This scheme brings great benefits in producing smoother and more accurate 3D poses. 

The proposed architecture is called Strided Transformer, as shown in Fig.~\ref{fig:pipline}. 
Extensive experiments are conducted on two standard 3D human pose estimation datasets, \emph{i.e.}, Human3.6M~\cite{ionescu2013human3} and HumanEva-I~\cite{sigal2010humaneva}. 
Experimental results show that the proposed method achieves state-of-the-art performance. 

Our contributions are summarized as follows:
\begin{itemize}
   \item We propose a new Transformer-based architecture for 3D human pose estimation called Strided Transformer, which can simply and effectively lift a long 2D pose sequence to a single 3D pose. 

   \item To reduce the sequence redundancy and computation cost, Strided Transformer Encoder (STE) is introduced to gradually reduce the temporal dimensionality and aggregate long-range information into a single-vector representation of pose sequences in a hierarchical global and local fashion. 
   \item A full-to-single supervision scheme is designed to impose extra temporal smoothness constraints during training at the full sequence scale and further refine the estimation at the single target frame scale. 

   \item State-of-the-art results are achieved with fewer parameters on two commonly used benchmark datasets, making our method a strong baseline for Transformer-based 3D pose estimation. 
\end{itemize}

\section{Related Work}
At the early stage of applying deep neural networks on 3D pose estimation task, many methods~\cite{pavlakos2017coarse,sun2018integral,zhao2019semantic,liu2019feature} learned the direct mapping from RGB images to 3D poses (\emph{i.e.}, one-stage pose estimation).
However, these methods require sophisticated architectures with high computation costs, which are impractical in realistic applications.  

\begin{figure*}[t]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/pipline.pdf}
   \caption
   {
      Overview of our proposed Strided Transformer for predicting the 3D joint locations of the target frame (center frame) from the estimated 2D pose sequences. 
      It mainly consists of a Vanilla Transformer Encoder (VTE) and a Strided Transformer Encoder (STE). 
      The network first models long-range information via VTE and then aggregates the information into one target pose representation from the proposed STE. 
      The model is trained end-to-end at both full sequence and single target frame scales. 
   }
   \label{fig:pipline}
\end{figure*}

\textbf{Two-stage pose estimation.}
Two-stage methods formulate the problem of 3D human pose estimation as 2D keypoint detection followed by 2D-to-3D lifting estimation~\cite{martinez2017simple,fang2018learning,xu2021graph}. 
Recent works show that 3D locations of body joints can be efficiently and effectively recovered using detected 2D poses from state-of-the-art 2D pose detectors, and this 2D-to-3D pose lifting method outperforms one-stage approaches.  
For example, 
Martinez \emph{et al.}~\cite{martinez2017simple} lifted 2D joint locations to 3D space via a fully-connected residual network. 
Fang \emph{et al.}~\cite{fang2018learning} proposed a pose grammar model to encode the human body configuration of human poses from 2D space to 3D space. 
To improve the generalization of the trained 2D-to-3D pose estimator, Gong~\cite{gong2021poseaug}
introduced a pose augmentation framework that is differentiable. 
We also follow this two-stage pipeline because it is widely adopted among the state-of-the-art methods in this domain. 

\textbf{Video pose estimation.}
Recently, many approaches tried to exploit temporal information~\cite{rayat2018exploiting,pavllo20193d,cai2019exploiting,wang2020motion} to improve the accuracy and the smoothness of the estimated 3D pose sequence. 
To predict temporally consistent 3D poses, Hossain \emph{et al.}~\cite{rayat2018exploiting} designed a sequence-to-sequence network with LSTM. 
Pavllo \emph{et al.}~\cite{pavllo20193d} introduced a fully convolutional model based on dilated temporal convolutions. 
Cai \emph{et al.}~\cite{cai2019exploiting} directly chose the 3D pose of the target frame from the outputs of the proposed graph-based method and then fed it to a refinement model. 
To produce smoother 3D sequences, Wang \emph{et al.}~\cite{wang2020motion} designed an U-shaped graph convolutional network and involved motion modeling into learning. 
However, the temporal connectivity of these architectures is inherently limited and is mainly constrained to simple sequential correlations. 
Different from most existing works that employed LSTM-based~\cite{rayat2018exploiting}, graph-based~\cite{cai2019exploiting,wang2020motion}, or temporal convolutional networks~\cite{pavllo20193d,liu2020attention,chen2021anatomy} to exploit temporal information, we propose a Transformer-based architecture to capture long-range dependencies from input 2D pose sequences. 
Furthermore, compared with previous methods~\cite{cai2019exploiting,wang2020motion} that either utilize a refinement model or use a motion loss to improve estimations, we design a full-to-single supervision scheme that refines the intermediate predictions to produce smoother and more accurate estimations. 

\textbf{Visual Transformers.}
Transformer models first proposed in~\cite{Attention} are commonly used in various language tasks. 
Recently, Transformers have shown promising performance in many computer vision tasks, such as object detection~\cite{carion2020end,zhu2020deformable} and image classification~\cite{dosovitskiy2020image,yuan2021tokens}. 
DETR~\cite{carion2020end} presented a new Transformer-based design for object detection systems. 
ViT~\cite{dosovitskiy2020image} proposed to apply a standard Transformer architecture directly to sequential image patches for image classification. 
METRO~\cite{lin2020end} introduced a Transformer framework to reconstruct 3D human pose and mesh from a single image. 
However, METRO focused on the one-stage pose estimation and ignores the temporal information across frames. 
Unlike DETR~\cite{carion2020end}, ViT~\cite{dosovitskiy2020image}, or METRO~\cite{lin2020end} that directly apply Transformer to images, we utilize a Transformer-based architecture to effectively map 2D keypoints to 3D poses. 
Additionally, efficient strided convolutions are incorporated into Transformer models to address the redundancy problem for the video-based 3D pose estimation task. 

\section{Method}
In this section, we first present an overview of the proposed Strided Transformer for 3D human pose estimation from a 2D video stream, and then show how our Transformer-based architecture learns a representative single-pose representation from redundant sequences resulting in an enhanced estimation. 
Finally, the complexity analysis of our network is presented. 

\subsection{Overview}
The overall framework of our proposed method is illustrated in Fig.~\ref{fig:pipline}. 
Given a sequence of the estimated 2D poses
 from videos, we aim at reconstructing 3D joint locations  for a target frame (center frame), where  denotes the 2D joint locations at frame ,  is the number of video frames, and  is the number of joints. 
The network contains a Vanilla Transformer Encoder (VTE) followed by a Strided Transformer Encoder (STE), which is trained in a full-to-single prediction scheme at both full sequence and single target frame scales. 
Specifically, VTE is first used to model long-range information and is supervised by the full sequence scale to enforce temporal smoothness. 
Then, the proposed STE aggregates the information to generate one target pose representation and is supervised by the single target frame scale to produce more accurate estimations.  

\subsection{Strided Transformer Encoder}
\label{sec:STE}
Despite the substantial performance gains achieved by Transformers~\cite{Attention} in many computer vision tasks, the full-length token representation makes it unsuitable for many video-based vision tasks that only require a single-vector representation of a sequence. 
To this end, STE is proposed to gradually compress the sequence of hidden states and model both global and local information in a hierarchical architecture. 
Each layer of the proposed STE consists of a multi-head self-attention (MSA) and a convolutional feed-forward network (CFFN). 

\subsubsection{Multi-head self-attention}
The core mechanism of the Transformer model is MSA~\cite{Attention}. 
Suppose there are a set of queries (), keys (), and values () of dimension . 
Then the MSA can be computed as:

where  and , and  are parameter matrices. 
The hyperparameter  is the number of multi-attention heads,  is the dimension of the model, and  in our implementation. 

\subsubsection{Convolutional feed-forward network}
In the existing fully-connected (FC) layers in the FFN of VTE (Eq. (\ref{equ:ffn})), it always maintains a full-length sequence of hidden representations across all layers with a high computation cost. 
It contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. 
However, to reconstruct more accurate 3D body joints of the target frame, crucial information should be extracted from the entire pose sequences. 
Therefore, it requires selectively aggregating useful information. 

To tackle this issue, inspired by the previous works~\cite{pavllo20193d,liu2020attention} that employ temporal convolutions to effectively shrink the sequence length, we make modifications to the generic FFN. 
Given the input feature vector  with  sequences and  channels to generate an output of  features, the operation performed by FC in FFN can be formulated as: 


If 1D convolution is considered with kernel size  and strided factor , a strided convolution in CFFN can be computed as:


In this way, fully-connected layers in FFN of VTE are replaced with strided convolutions. 
The modified VTE is termed as Strided Transformer Encoder (STE), which can be represented as:

where  denotes the layer normalization,  denotes the max pooling operation, and  is the index of STE layers. 

The STE is a hierarchical global and local architecture, where the self-attention mechanism models global context and the strided convolution helps capture local contexts, as presented in Fig.~\ref{fig:network} (right). 
It gradually merges the nearby poses to a short sequence length representation, as illustrated in Fig.~\ref{fig:architecture}. 
Importantly, through such a hierarchical design, the redundancy of the sequence and the computation cost can be reduced. 

\begin{figure}[t]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/network.pdf}
   \caption
   {
      The network architecture of our proposed Strided Transformer. 
      The left is the VTE and the right is the STE. 
      Here,  and  denote the number of layers of the two modules, respectively. 
      The hyperparameters , ,  and  are the kernel size, the strided factor, the dimension, and the number of hidden units. 
      The max pooling operation is applied to the residuals to match the temporal dimensions. 
   }
   \label{fig:network}
\end{figure}

\subsection{Network Architecture}
\label{sec:transpose}
In this section, we describe how to use the proposed Transformer-based network architecture to estimate 3D human poses from a sequence of 2D poses. 
As shown in Fig.~\ref{fig:architecture}, the proposed network is composed of four components: a pose embedding, a Vanilla Transformer Encoder (VTE), a Strided Transformer Encoder (STE), and a regression head. 

\subsubsection{Pose embedding}
Given a sequence of the estimated 2D poses , the pose embedding first concatenates  coordinates of the  joints for each frame to tokens , and then embeds each token to a high dimensional feature  using a 1D convolutional layer with  channels, followed by batch normalization, dropout, and a ReLU activation. 

\begin{figure*}[t]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/architecture.pdf}
   \caption
   {
      An instantiation of the proposed Strided Transformer network.  
      It reconstructs the target 3D body joints by progressively reducing the sequence length. 
      The input consists of 2D keypoints for a receptive field of 27 frames with  joints. 
      Convolutional feed-forward networks are in blue where  denotes kernels of size 3 with strided factor 3 and 256 output channels. 
      The tensor sizes are shown in parentheses, \emph{e.g.},  denotes 27 frames and 34 channels. 
      Due to strided convolutions, the max pooling operation is applied to the residuals to match the shape of subsequent tensors.
   }
   \label{fig:architecture}
\end{figure*}

\subsubsection{Vanilla Transformer Encoder}
Suppose that the VTE consists of  layers, the learnable position embedding  is used before the first layer of VTE, which can be formulated as follows: 


Then, given the embedded feature , the VTE layers can be represented as: 

where  is the index of VTE layers. 
It can be expressed by using a function of a VTE layer : 


\subsubsection{Strided Transformer Encoder}
For the STE, it is built upon the outputs of VTE and takes the  as input. 
The learnable position embeddings  with strided factor  are used for every layer of STE due to the different sequence lengths. 
Then, the STE layers can be represented as follows:

where  is the index of STE layers, , and  denotes the function of an STE layer whose details can be found in Eq. (\ref{equ:ste_1}) and Eq. (\ref{equ:ste_2}). 

\subsubsection{Regression head}
In order to perform the regression, a batch normalization and a 1D convolutional layer are applied to the outputs of VTE and STE,  and , respectively. 
Finally, the outputs of 3D pose prediction are  and , where  and  are predictions of the 3D pose sequence and the 3D joint locations of the target frame, respectively. 

\subsection{Full-to-Single Prediction}
The iterative refinement scheme, aimed at producing predictions in multiple processing stages, is effective for 3D pose estimation~\cite{pavlakos2017coarse,cai2019exploiting}. 
Motivated by the success of such iterative processing, we also consider a refinement scheme. 
A full-to-single scheme is proposed to incorporate both full sequence and single target frame scales constraints into the framework. 
This scheme further refines the intermediate predictions to produce more accurate estimations rather than using a single component with a single output. 
More precisely, the full sequence scale can enforce temporal smoothness and the single target frame scale helps learn a specific representation for the target frame. 

\subsubsection{Full sequence scale}
The first step is to supervise at full sequence scale by imposing extra temporal smoothness constraints during training from the output of VTE followed by a regression head. 
A sequence loss  is used to improve upon single frame predictions for temporal consistency over a sequence. 
This loss ensures that the estimated 3D pose sequences  coincide with the ground truth 3D joint sequences : 

where  and  represent the sequence of estimated 3D poses and ground truth 3D joint locations of joint  at frame , respectively. 

\subsubsection{Single target frame scale}
In the second step, the supervision is adopted on the output of STE followed by a regression head. 
A single-frame loss  is used to refine the estimation at the single target frame scale. 
It minimizes the distance between the estimated 3D pose  and the target ground truth 3D joint annotation : 

where  and  represent the target frame's estimated 3D pose and ground truth 3D joint locations of joint , respectively. 

\subsubsection{Loss function}
In our implementation, the model is supervised at both full sequence scale and single target frame scale. 
We train the entire network in an end-to-end manner with the total loss:

where  and  are weighting factors. 

\subsection{Complexity Analysis}
In this section, we use floating-point operations (FLOPs) to measure the computational cost and analyze the compression ratio of our proposed Strided Transformer network. 
Given the sequence length , dimension , strided factor , and kernel size , the FLOPs of a VTE layer  and an STE layer  can be computed by: 


where , , and  are the FLOPs of the MSA, FFN, and CFFN, respectively. 

Then if we consider  layers of VTE and STE with input sequence length , dimension , strided factor , and kernel size , the encoder-wise FLOPs of VTE  can be formulated as:

the encoder-wise FLOPs of STE  can be formulated as:
0.5mm] 
      &= \sum_{n=1}^{N}\left[(\frac{6+2KS^{-1}}{S^{n-1}}) T D ^{2} + \frac{2}{S^{2(n-1)}}T^{2} D\right]. 
   \end{aligned}

   \alpha = \frac{2\mathcal{F}_{VTE}}{\mathcal{F}_{VTE}+\mathcal{F}_{STE}} =\frac{2}{1+\beta}, 

   \beta = \frac{\mathcal{F}_{STE}}{\mathcal{F}_{VTE}} = \frac{468D+91T}{972D+243T}.


We have  with a fixed . 
Thus, the compression ratio  of our 27-frame Strided Transformer is 1.35. 

\begin{table*}[t]
   \centering
   \caption
   {
      Quantitative comparisons on Human3.6M under protocol \#1 and protocol \#2,
      where  indicates the temporal information used in each method. 
      Best in bold, second-best underlined.
   }
   \resizebox{\textwidth}{!}{
   \begin{tabular}{@{}l|ccccccccccccccc|c@{}}
      \toprule[1pt]
      \textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
      \midrule[0.5pt]
      
Martinez \emph{et al.}~\cite{martinez2017simple} ICCV'17  &51.8 &56.2 &58.1 &59.0 &69.5 &78.4 &55.2 &58.1 &74.0 &94.6 &62.3 &59.1 &65.1 &49.5 &  52.4 &62.9\\

Fang \emph{et al.}~\cite{fang2018learning} AAAI'18  & 50.1& 54.3& 57.0& 57.1& 66.6& 73.3& 53.4& 55.7& 72.8& 88.6& 60.3& 57.7& 62.7& 47.5& 50.6& 60.4 \\

Lee \emph{et al.}~\cite{lee2018propagating} ECCV'18  &\underline{40.2} &49.2 &47.8 &52.6 &50.1 &75.0 &50.2 &43.0 &55.8 &73.9 &54.1 &55.6 &58.2 &43.3 &43.3 &52.8 \\

Xu \emph{et al.}~\cite{xu2021graph} CVPR'21 &45.2 &49.9 &47.5 &50.9 &54.9 &66.1 &48.5 &46.3 &59.7 &71.5 &51.4 &48.6 &53.9 &39.9 &44.1 &51.9 \\

Gong \emph{et al.}~\cite{gong2021poseaug} CVPR'21  &- &- &- &- &- &- &- &- &- &- &- &- &- &- &- &50.2 \\
      
Cai \emph{et al.}~\cite{cai2019exploiting} ICCV'19  &44.6 &47.4 &45.6 &48.8 &50.8 &59.0 &47.2 &43.9&57.9 &61.9 &49.7 &46.6 &51.3 &37.1 &39.4 &48.8 \\
      
Pavllo \emph{et al.}~\cite{pavllo20193d} CVPR'19  & 45.2 & 46.7 & 43.3 & 45.6 & 48.1 & 55.1 & 44.6 & 44.3 & 57.3 & 65.8 & 47.1 & 44.0 & 49.0 & 32.8 & 33.9 & 46.8 \\

Lin \emph{et al.}~\cite{lin2019trajectory} BMVC'19   &42.5 &44.8 &42.6 &44.2 &48.5 &57.1 &42.6 &41.4 &56.5 &64.5 &47.4 &43.0 &48.1 &33.0 &35.1 &46.6 \\

Xu \emph{et al.}~\cite{xu2020deep} CVPR'20 & \textbf{37.4} & {43.5}& 42.7& {42.7}& 46.6& 59.7& \textbf{41.3} &45.1 &\textbf{52.7} &\textbf{60.2} & 45.8& 43.1& 47.7& 33.7& 37.1& 45.6 \\
   
Liu \emph{et al.}~\cite{liu2020attention} CVPR'20   &41.8 &44.8 &{41.1} &44.9 &47.4 &54.1 &43.4 &42.2 &56.2 &63.6 &\underline{45.3} &43.5 &{45.3} &\underline{31.3} &32.2 &45.1 \\

Zeng \emph{et al.}~\cite{zeng2020srnet} ECCV'20  & 46.6& 47.1& 43.9& \underline{41.6}& \underline{45.8} & \textbf{49.6} & 46.5& \textbf{40.0}&\underline{53.4} & 61.1& 46.1& 42.6& \textbf{43.1}& 31.5& 32.6& 44.8 \\

Wang \emph{et al.}~\cite{wang2020motion} ECCV'20   &\underline{40.2} &\textbf{42.5} &42.6 &\textbf{41.1} &46.7 &56.7 &\underline{41.4} &42.3 &56.2 &\underline{60.4} &46.3 &\underline{42.2} &46.2 &31.7 &\underline{31.0} &44.5 \\

Chen \emph{et al.}~\cite{chen2021anatomy} TCSVT'21   &41.4 &{43.5} &\textbf{40.1} &42.9 &46.6 &\underline{51.9} &41.7 &42.3 &53.9 &\textbf{60.2} &45.4 &\textbf{41.7} &46.0 &31.5 &32.7 &\underline{44.1} \\
      
      \midrule[0.5pt]

      Ours  &{40.3} &\underline{43.3} &\underline{40.2} &{42.3} &\textbf{45.6} &{52.3} &{41.8} &\underline{40.5} &{55.9} &{60.6} &\textbf{44.2} &{43.0} &\underline{44.2} &\textbf{30.0} &\textbf{30.2} &\textbf{43.7} \\

      \toprule[1pt]

      \textbf{Protocol \#2} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
      \midrule[0.5pt]

Martinez \emph{et al.}~\cite{martinez2017simple} ICCV'17  &39.5 &43.2 &46.4 &47.0 &51.0 &56.0 &41.4 &40.6 &56.5 &69.4 &49.2 &45.0 &49.5 &38.0 &43.1 &47.7 \\

Pavlakos \emph{et al.}~\cite{pavlakos2018ordinal} CVPR'18  &34.7 &39.8 &41.8 &38.6 &42.5 &47.5 &38.0 &36.6 &50.7 &56.8 &42.6 &39.6 &43.9 &32.1 &36.5 &41.8 \\

Liu \emph{et al.}~\cite{liu2020comprehensive} ECCV'20  &35.9 &40.0 &38.0 &41.5 &42.5 &51.4 &37.8 &36.0 &48.6 &56.6 &41.8 &38.3 &42.7 &31.7 &36.2 &41.2 \\

Gong \emph{et al.}~\cite{gong2021poseaug} CVPR'21  &- &- &- &- &- &- &- &- &- &- &- &- &- &- &-  &39.1 \\

Cai \emph{et al.}~\cite{cai2019exploiting} ICCV'19   &35.7 &37.8 &36.9 &40.7 &39.6 &45.2 &37.4 &34.5 &46.9 &50.1 &40.5 &36.1 &41.0 &29.6 &33.2 &39.0 \\

Lin \emph{et al.}~\cite{lin2019trajectory} BMVC'19   &32.5 &35.3 &34.3 &36.2 &37.8 &43.0 &33.0 &\underline{32.2} &45.7 &51.8 &38.4 &\underline{32.8} &37.5 &25.8 &28.9 &36.8 \\

Pavllo \emph{et al.}~\cite{pavllo20193d} CVPR'19  &34.1 &36.1 &{34.4} &37.2 &{36.4} &42.2 &34.4 &33.6 &45.0 &52.5 &37.4 &33.8 &37.8 &25.6 &27.3 &36.5\\

Xu \emph{et al.}~\cite{xu2020deep} CVPR'20  &\textbf{31.0} &\textbf{34.8} &34.7 &\textbf{34.4} &\underline{36.2} &43.9 &\underline{31.6} &33.5 &\textbf{42.3} &\textbf{49.0} &37.1 &33.0 &39.1 &26.9 &31.9 &36.2 \\

Liu~et al.~\cite{liu2020attention} CVPR'20  &\underline{32.3} &\underline{35.2} &\underline{33.3} &35.8 &\textbf{35.9} &\textbf{41.5} &33.2 &{32.7} &\underline{44.6} &50.9 &\underline{37.0} &\textbf{32.4} &37.0 &\underline{25.2} &27.2 &35.6 \\

Wang \emph{et al.}~\cite{wang2020motion} ECCV'20   &32.9 &\underline{35.2} &35.6 &\textbf{34.4} &36.4 &42.7 &\textbf{31.2} &{32.5} &45.6 &50.2 &37.3 &\underline{32.8} &\underline{36.3} &26.0 &\textbf{23.9} &\underline{35.5} \\

      \midrule[0.5pt]

      Ours  &{32.7} &{35.5} &\textbf{32.5} &\underline{35.4} &\textbf{35.9} &\underline{41.6} &{33.0} &\textbf{31.9} &{45.1} &\underline{50.1} &\textbf{36.3} &{33.5} &\textbf{35.1} &\textbf{23.9} &\underline{25.0}  &\textbf{35.2} \\

      \toprule[1pt]
      \end{tabular}
   }
   \label{table:h36m}
\end{table*}

\begin{table}[ht]
   \centering
   \caption
   { 
      Quantitative comparisons with state-of-the-art methods in different receptive fields on Human3.6M. 
      The computational complexity, MPJPE, and frame per second (FPS) are reported. 
      FPS is computed on a single GeForce GTX 2080 Ti GPU.
   }  
   \setlength{\tabcolsep}{1.10mm} 
   \begin{tabular}{cccccc}
   \toprule [1pt]
   Model & &Param (M) &FLOPs (G) &MPJPE (mm) &FPS \\
   \midrule [0.5pt]  
   Pavllo \emph{et al.}~\cite{pavllo20193d} &27 &8.56 &0.017 &48.8 &1492 \\
   Pavllo \emph{et al.}~\cite{pavllo20193d} &81 &12.75 &0.025 &47.7 &1121 \\
   Pavllo \emph{et al.}~\cite{pavllo20193d} &243 &16.95 &0.033 &46.8 &863 \\
   Chen \emph{et al.}~\cite{chen2021anatomy} &27 &31.88 &0.061 &45.3 &410 \\
   Chen \emph{et al.}~\cite{chen2021anatomy} &81 &45.53 &0.088  &44.6 &315 \\
   Chen \emph{et al.}~\cite{chen2021anatomy} &243 &59.18 &0.116 &44.1 &264 \\
   \midrule [0.5pt]

   Ours (27 frames) &27 &4.01 &0.128 &46.9 &118 \\
   Ours (81 frames) &81 &4.06 &0.392 &45.4 &112 \\
   Ours (243 frames) &243 &4.23 &1.372 &44.0 &108 \\
   Ours (351 frames) &351 &4.34 &2.142 &\textbf{43.7} &105 \\

   \toprule [1pt]
   \end{tabular}
   \label{table:compare}
\end{table}

\section{Experiments}
\subsection{Datasets and Evaluation}
The proposed method is evaluated on two challenging benchmark datasets, \emph{i.e.}, Human3.6M~\cite{ionescu2013human3} and HumanEva-I~\cite{sigal2010humaneva}. 
Human3.6M dataset is the largest publicly available dataset for 3D human pose estimation, which consists of 3.6 million images captured from 4 synchronized cameras with 50 Hz. 
There are 7 professional subjects performing 15 daily activities such as “Waiting”, “Smoking”, and “Posing”.
Following the standard protocol in prior works~\cite{chen2019weakly,tome2018rethinking,pavllo20193d}, 5 subjects (S1, S5, S6, S7, S8) are used for training and 2 subjects (S9 and S11) are used for evaluation. 
The frames from all views are trained by a single model for all actions. 
HumanEva-I is a much smaller dataset with fewer subjects and actions compared to Human3.6M. 
Following~\cite{pavllo20193d,lee2018propagating}, our model is trained for all subjects (S1, S2, S3) and all actions (Walk, Jog, Box). 

\begin{table*}[htb]
   \centering
   \caption
   {
      Quantitative comparisons of MPJPE in millimeter on Human3.6M under protocol \#1, using ground truth 2D joint locations as input. 
       means the method utilizing temporal information. 
      Best in bold. 
   }
   \resizebox{\textwidth}{!}{
   \begin{tabular}{@{}l|ccccccccccccccc|c@{}}
   \toprule[1pt]
   \textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
   \midrule[0.5pt]

Martinez \emph{et al.}~\cite{martinez2017simple} ICCV'17 &37.7 &44.4 &40.3 &42.1 &48.2 &54.9 &44.4 &42.1 &54.6 &58.0 &45.1 &46.4 &47.6 &36.4 &40.4 &45.5 \\

Lee \emph{et al.}~\cite{lee2018propagating} ECCV'18   &32.1 &36.6 &34.3 &37.8 &44.5 &49.9 &40.9 &36.2 &44.1 &45.6 &35.3 &35.9 &30.3 &37.6 &35.5 &38.4 \\

Pavllo \emph{et al.}~\cite{pavllo20193d} CVPR'19   &35.2 &40.2 &32.7 &35.7 &38.2 &45.5 &40.6 &36.1 &48.8 &47.3 &37.8 &39.7 &38.7 &27.8 & 29.5 &37.8 \\

Cai \emph{et al.}~\cite{cai2019exploiting} ICCV'19   &32.9 &38.7 &32.9 &37.0 &37.3 &44.8 &38.7 &36.1 &41.0 &45.6 &36.8 &37.7 &37.7 &29.5 &31.6 &37.2 \\

Xu \emph{et al.}~\cite{xu2021graph} CVPR'21 &35.8 &38.1 &31.0 &35.3 &35.8 &43.2 &37.3 &31.7 &38.4 &45.5 &35.4 &36.7 &36.8 &27.9 &30.7 &35.8 \\

Liu \emph{et al.}~\cite{liu2020attention} CVPR'20  &34.5 &37.1 &33.6 &34.2 &32.9 &37.1 &39.6 &35.8 &40.7 &41.4 &33.0 &33.8 &33.0 &26.6 &26.9 &34.7 \\

Chen \emph{et al.}~\cite{chen2021anatomy} TCSVT'21   &- &- &- &- &- &- &- &- &- &- &- &- &- &- &- &32.3 \\
   
Zeng \emph{et al.}~\cite{zeng2020srnet} ECCV'20   &34.8 &32.1 &28.5 &30.7 &31.4 &36.9 &35.6 &30.5 &38.9 &40.5 &32.5 &31.0 &29.9 &22.5 &24.5 &32.0 \\

   \midrule[0.5pt]
   Ours &\textbf{27.1} &\textbf{29.4} &\textbf{26.5} &\textbf{27.1} &\textbf{28.6} &\textbf{33.0} &\textbf{30.7} &\textbf{26.8} &\textbf{38.2} &\textbf{34.7} &\textbf{29.1} &\textbf{29.8} &\textbf{26.8} &\textbf{19.1} &\textbf{19.8} &\textbf{28.5} \\

   \toprule[1pt]
   \end{tabular}
   }
   \label{table:h36m_gt}
\end{table*}

\begin{table*}[htb]
   \centering
   \caption
   {
      Results show the velocity error (MPJPV) of our methods and other state-of-the-arts on Human3.6M. 
      Here,  denotes our result without the supervision of full sequence scale. 
      Best in bold. 
   }
   \resizebox{\textwidth}{!}{
   \begin{tabular}{@{}l|ccccccccccccccc|c@{}}
   \toprule[1pt]
   \textbf{MPJPV} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
   \midrule[0.5pt]
Pavllo \emph{et al.}~\cite{pavllo20193d} CVPR'19   &3.0 &3.1 &2.2 &3.4 &2.3 &2.7 &2.7 &3.1 &2.1 &2.9 &2.3 &2.4 &3.7 &3.1 &2.8 &2.8 \\

Lin \emph{et al.}~\cite{lin2019trajectory} BMVC'19   &2.7 &2.8 &2.1 &3.1 &2.0 &2.5 &2.5 &2.9 &1.8 &2.6 &2.1 &2.3 &3.7 &2.7 &3.1 &2.7 \\

Chen \emph{et al.}~\cite{chen2021anatomy} TCSVT'21 &2.7 &2.8 &2.0 &3.1 &2.0 &2.4 &2.4 &2.8 &1.8 &2.4 &2.0 &2.1 &3.4 &2.7 &2.4 &2.5 \\

Wang \emph{et al.}~\cite{wang2020motion} ECCV'20 &\textbf{2.3} &\textbf{2.5} &2.0 &\textbf{2.7} &2.0 &2.3 &\textbf{2.2} &\textbf{2.5} &1.8 &2.7 &1.9 &2.0 &\textbf{3.1} &\textbf{2.2} &2.5 &2.3 \\
   
   \midrule[0.5pt]
   Ours  &2.8 &2.8 &2.1 &3.2 &2.2 &2.5 &2.6 &2.8 &1.8 &2.4 &2.1 &2.3 &3.5 &3.0 &2.6 &2.6 \\

   Ours &{2.4} &\textbf{2.5} &\textbf{1.8} &{2.8} &\textbf{1.8} &\textbf{2.2} &\textbf{2.2} &\textbf{2.5} &\textbf{1.5} &\textbf{2.0} &\textbf{1.8} &\textbf{1.9} &{3.2} &{2.5} &\textbf{2.1} &\textbf{2.2} \\

   \toprule[1pt]
   \end{tabular}
   }
   \label{table:mpjpv}
\end{table*}

\begin{table}[ht]
   \centering
   \scriptsize
   \caption
   {
      Quantitative results on HumanEva-I dataset under protocol \#2. 
      Best in bold, second-best underlined.
      (MRCNN) - Mask-RCNN; 
      (GT) - 2D ground truth. 
   }
   \setlength{\tabcolsep}{0.95mm} 
   \begin{tabular}{@{}l|ccc|ccc|ccc|c@{}}
   \toprule[1pt]
   & \multicolumn{3}{c}{Walk} & \multicolumn{3}{c}{Jog} &
   \multicolumn{3}{c}{Box} \\
   & S1 & S2 & S3 & S1 & S2 & S3 & S1 & S2 & S3 & Avg. \\
   \midrule[0.5pt]

   Martinez \emph{et al.}~\cite{martinez2017simple} &19.7 &17.4 &46.8 &26.9 &18.2 &18.6 &- &- &- &-\\

   Pavlakos \emph{et al.}~\cite{pavlakos2017coarse} &22.3 &19.5 &\textbf{29.7} &28.9 &21.9 &23.8 &- &- &- &-\\

   Lee \emph{et al.}~\cite{lee2018propagating} &18.6 &19.9 &\underline{30.5} &25.7 &16.8 &17.7 &42.8 &48.1 &53.4 &30.3 \\

   Pavllo \emph{et al.}~\cite{pavllo20193d} &\textbf{13.9} &\underline{10.2} &46.6 &\underline{20.9} &\textbf{13.1} &\textbf{13.8} &\underline{23.8} &\underline{33.7} &\underline{32.0} &\underline{23.1} \\

   \midrule[0.5pt]
   Ours ( MRCNN) &\underline{14.0} &\textbf{10.0} &32.8 &\textbf{19.5} &\underline{13.6} &\underline{14.2} &\textbf{22.4} &\textbf{21.6} &\textbf{22.5} &\textbf{18.9} \\
   \midrule[0.5pt]

   Ours ( GT) &9.7 &7.6 &15.8 &12.3 &9.4 &11.2 &14.8 &12.9 &16.5 &12.2 \\
   \toprule[1pt]
   \end{tabular}
   \label{table:humaneva_eval}
\end{table} 

Three standard evaluation protocols are used in the experiments. 
The mean per joint position error (MPJPE) is the average Euclidean distance between the ground truth and predicted positions of the joints, which is referred to as protocol \#1 in many works~\cite{fang2018learning,kocabas2019self}. 
Procrustes analysis MPJPE (P-MPJPE) is adopted, where the estimated 3D pose is aligned to the ground truth in translation, rotation, and scale. 
This protocol is referred to as protocol \#2~\cite{martinez2017simple,rayat2018exploiting}.
Following~\cite{pavllo20193d,chen2021anatomy,wang2020motion}, we also report the mean per joint velocity error (MPJVE) corresponding to the MPJPE of the first derivative of the 3D pose sequences. 
This metric measures the smoothness of predictions over time and is vital for video-based 3D pose estimation.

\subsection{Implementation Details}
In our experiments, the proposed Strided Transformer contains  encoder layers,  attention heads,  dimensions, and  hidden units for both VTE and STE. 
The kernel sizes  and  are set to 1 and 3 in all STE layers, respectively. 
The strided factor  is set to 1, and  is set to  for the receptive field of 27 frames,  for 81,  for 243, and  for 351. 
The weighting factors  and  are set to 1. 

All experiments are conducted on the PyTorch framework with one GeForce GTX 3090 GPU.
The network is trained using Amsgrad optimizer. 
An initial learning rate of 0.001 is used with a shrink factor of 0.95  applied after each epoch. 
The same refine module as~\cite{cai2019exploiting,wang2020motion} is adopted. 
We only apply horizontal flip augmentation during training/test stages. 
The 2D poses can be obtained by performing any classic 2D pose detections or directly using the 2D ground truth. 
Following~\cite{pavllo20193d,cheng2019occlusion}, the cascaded pyramid network (CPN)~\cite{chen2018cascaded} is used for Human3.6M and Mask R-CNN~\cite{he2017mask} is adopted for HumanEva-I to obtain 2D poses for a fair comparison.

\subsection{Comparison with State-of-the-art Results}
Our method is compared with previous state-of-the-art approaches on Human3.6M dataset. 
The performance of our 351-frame model with CPN input is reported in Table~\ref{table:h36m}. 
Our method outperforms the state-of-the-art methods on Human3.6M under all metrics (43.7 mm on protocol \#1 and 35.2 mm on protocol \#2). 

Table~\ref{table:compare} compares the computational complexity, MPJPE, and frame per second (FPS) with several state-of-the-art methods in different receptive fields on Human3.6M. 
Our model is lightweight and the number of parameters hardly increases with the increased receptive fields, which is practical for real-time applications. 
Compared with temporal convolutional networks~\cite{pavllo20193d,chen2021anatomy}, our proposed Transformer-based network requires fewer total parameters with competitive performance for 3D pose estimation in videos. 
Besides, even though the inference speed of the proposed model is lower than~\cite{pavllo20193d,chen2021anatomy}, it still has an acceptable FPS for real-time inference. 
Fig.~\ref{fig:results} shows some qualitative comparisons with state-of-the-art methods~\cite{pavllo20193d,liu2020attention}, which indicates that our methods can produce more accurate 3D predictions. 

To further explore the upper bound of our method, the results from 2D ground truth inputs are reported in Table~\ref{table:h36m_gt}. 
It can be seen that our method achieves the best result (28.5 mm in MPJPE), outperforming all other methods. 
This demonstrates if a more robust 2D pose detection is available, our Strided Transformer can produce more accurate 3D poses. 

As shown in Table~\ref{table:mpjpv}, with the supervision of full sequence scale, our method reduces the MPJVE by 15.4\% (from 2.6 mm to 2.2 mm), achieving smoother predictions with lower MPJVE than other models.
It indicates that the full-to-single supervision scheme can enhance temporal smoothness and produce vastly smoother poses. 

To evaluate the generalizability of our model to smaller datasets, experiments are conducted on HumanEva-I based on Mask R-CNN 2D detections and 2D ground truth. 
The results in Table~\ref{table:humaneva_eval} demonstrate that our method achieves promising results on all kinds of actions. 

\subsection{Ablation Studies}
\textbf{Input sequence length.}
The MPJPE results of our model with different sequence lengths (between 1 and 351) on Human3.6M are shown in Fig.~\ref{fig:frames and 2D} (a). 
It can be seen that our proposed method obtains larger gains under both 2D pose inputs (CPN and GT) with more input frames used for predictions, but the error saturates past a certain point. 
This is expected since directly lifting 3D poses from disjointed 2D poses leads to temporally incoherent outputs~\cite{dabral2018learning}. 
It is worth mentioning that our method gets a better result with  (43.7 mm) than  (44.0 mm), while the performance decreases with longer inputs () in~\cite{liu2020attention}. 
This indicates that our method equipped with the global self-attention mechanism is powerful in modeling long-range dependencies. 
Meanwhile, with the help of STE, our method can learn the representative representation from long sequences. 
Next, we choose  on Human3.6M in the following ablation experiments as a compromise between the accuracy and computational complexity. 

\textbf{2D detections.}
For the 2D-to-3D pose lifting task, the accuracy of the 2D detections directly influences the results of 3D pose estimation~\cite{martinez2017simple}. 
To show the effectiveness of our method on different 2D pose detectors, we carry out experiments with the detections from Stack Hourglass (SH)~\cite{newell2016stacked}, Detectron~\cite{pavllo20193d}, and CPN~\cite{chen2018cascaded}. 
Moreover, to test the tolerance of our method to different levels of noise, we also train our network by 2D ground truth (GT) with various levels of additive Gaussian noises. 
The results are shown in Fig.~\ref{fig:frames and 2D} (b). 
It can be observed that the MPJPE of 3D poses increases linearly with the two-norm errors of 2D poses. 
Besides, our method performs well on different 2D inputs, indicating the effectiveness and robustness of our method.

\textbf{Model hyperparameters.}
As shown in Table~\ref{table:hyperparameters}, we first analyze the effect of the number of VTE layers. 
Empirically, it can be found that the performance cannot be improved when naively stacking multiple standard Transformer encoder layers. 
Notably, our model equipped with STE is more accurate at the same number of Transformer encoder layers and comparable model parameters. 
For example, our method ( and ) has better performance and fewer FLOPs than the model of  at the same  and  (46.9 mm vs. 47.9 mm, 0.128G vs. 0.174G). 
In addition, our STE (, 0.041G) also has fewer FLOPs than standard Transformer encoder (, 0.087G) with similar parameters, which achieves  less computation. 
It verifies the effectiveness of our proposed STE in reducing computation cost and boosting performance. 
Then, we investigate the influence of various hyperparameters combinations to find the optimal network architecture. 
It can be observed that using 3 encoder layers of both VTE and STE modules, 256 dimensions, and 512 hidden units achieves the best performance. 

\begin{figure}[tb]
   \centering
   \begin{subfigure}[htb]{0.241\textwidth}
      \includegraphics[width=\textwidth]{figure/frames.pdf}
      \caption{}
      \label{fig:frames}
   \end{subfigure}
   \begin{subfigure}[htb]{0.241\textwidth}
      \includegraphics[width=\textwidth]{figure/2D_detections.pdf}
      \caption{}
      \label{fig:2D_detections}
   \end{subfigure}
   \caption
   {
      (a) Ablation studies on different sequence lengths of our method on Human3.6M with the MPJPE metric. 
      (b) The impact of 2D detections on Human3.6M. 
      Here,  represents the Gaussian noise with mean zero and  is the standard deviation. 
      (CPN) - Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) - 2D ground truth.  
   }
   \label{fig:frames and 2D}
\end{figure}

\begin{figure*}[htb]
   \centering
   \includegraphics[width=1.00 \linewidth]{figure/results.pdf}
   \caption
   {
      Qualitative comparisons with the previous state-of-the-art methods, TCN~\cite{pavllo20193d} and ATTN-TCN~\cite{liu2020attention} on Human3.6M dataset. 
      Wrong estimations are highlighted by red circles. 
   }
   \label{fig:results}
\end{figure*}

\textbf{Strided factor.}
We observe that the strided factor of STE used in our Strided Transformer has an impact on the estimation performance. 
Here, we study the influence of using different design choices of strided factor of STE. 
The experimental results are depicted in Table~\ref{table:Strided}.
It shows that using a strided factor  has the best performance. 
This demonstrates the benefit of gradually reducing the temporal dimensionality with a small strided factor. 

\begin{table}[tb]
   \centering
   \caption
   {
      Ablation study on the hyperparameters of our model on Human3.6M under protocol \#1. 
       and  are the number of VTE and STE layers, respectively. 
       and  are the dimensions and the number of hidden units. 
   }
   \setlength{\tabcolsep}{1.8mm} 
   \begin{tabular}{ccccccc}
   \toprule  [1pt]
    & & & &Param (M) &FLOPs (G)  &MPJPE (mm)\\
   \midrule  [0.5pt]
   2 &- &512 &2048 &6.36 &0.342  &47.9 \\
   3 &- &512 &2048 &9.51 &0.514  &47.8 \\
   4 &- &512 &2048 &12.66 &0.685 &48.0 \\
   5 &- &512 &2048 &15.82 &0.856 &48.4 \\
   6 &- &512 &2048 &18.97 &1.028 &49.3 \\
   \midrule [0.5pt]  

   2 &- &256 &512 &1.08 &0.058 &47.8 \\
   3 &- &256 &512 &1.61 &0.087 &47.6 \\
   4 &- &256 &512 &2.13 &0.116 &47.8 \\
   5 &- &256 &512 &2.66 &0.145 &47.7 \\
   6 &- &256 &512 &3.19 &0.174 &47.9 \\
   \midrule [0.5pt]  

   - &3 &256 &512 &2.42 &0.041 &48.0 \\
   2 &3 &256 &512 &3.48 &0.099 &47.4 \\
   3 &3 &256 &512 &4.01 &0.128 &\textbf{46.9} \\
   2 &3 &512 &2048 &22.18 &0.589 &47.4 \\
   3 &3 &512 &2048 &25.33 &0.761 &47.3 \\
   \toprule  [1.0pt] 
   \end{tabular}
\label{table:hyperparameters}
\end{table}

\begin{table}[tb]
   \caption
   {
      Ablation study on the strided factor of STE with the receptive field . 
      The evaluation is performed on Human3.6M under protocol \#1.
   }
   \centering  
   \setlength{\tabcolsep}{7.50mm} 
   \begin{tabular}{ccc}
   \toprule  [1pt]
   Layers &Strided factor &MPJPE (mm)  \\
   \midrule  [0.5pt]  
   3 & &\textbf{46.9} \\
   3 & &47.5  \\
   3 & &47.3 \\
   2 & &47.2  \\
   2 & &47.1  \\
   1 & &47.7 \\
   \toprule [1pt]
   \end{tabular}
   \label{table:Strided}
\end{table}

\begin{table}[tb]
   \caption
   {
      Ablation study on different prediction schemes. 
      The evaluation is performed on Human3.6M under protocol \#1.
       represents the performance gap between the methods and ours.
   }
   \centering  
   \setlength{\tabcolsep}{7.30mm} 
   \begin{tabular}{lcc}
   \toprule  [1pt]
   Prediction scheme &MPJPE (mm) & \\
   \midrule [0.5pt]
   Full &47.9 &1.0 \\
   Single &48.3 &1.4 \\
   Full-to-full &47.4 &0.5 \\
   Single-to-single &48.5 &1.6 \\
   Full-to-single &\textbf{46.9} &- \\
   \toprule [1pt]
   \end{tabular}
   \label{table:prediction}
\end{table}

\begin{table}[!htb]
   \centering
   \caption
   {
      Ablation study on each component of our network architecture on Human3.6M under protocol \#1.
   }
   \setlength{\tabcolsep}{8.35mm} 
   
   \begin{tabular}{lc}
   \toprule [1.0pt] 
   Method& MPJPE (mm) \\
   \midrule [0.5pt] 
   Ours, proposed &\textbf{46.9} \\
   Ours, intermediate predictions &48.1 \\
   Ours, Pooling Transformer &47.3 \\
   \midrule [0.5pt]

   w/o VTE &48.0 \\
   w/o STE &47.6 \\
   \toprule [1.0pt] 
   \end{tabular}
   \label{table:ablation_method}
\end{table}

\textbf{Prediction scheme.}
We further examine the proposed prediction scheme of full sequence scale and single target frame scale by using five different designs: 
(i) Full: the STE of our proposed method is replaced with VTE, and the new architecture is only supervised by the full sequence scale (the sequence loss). 
(ii) Single: the proposed method is only supervised by the single target frame scale (single-frame loss). 
(iii) Full-to-full: the architecture consists of six VTE layers, whose first three layers and final three layers are both supervised by the sequence loss. 
(iv) Single-to-single: VTE and STE of the proposed method are both supervised by the single-frame loss.
(v) Full-to-single: our proposed method. 
In Table~\ref{table:prediction}, it can be observed that the schemes of considering only one prediction manner (i, ii, iii, iv) decay performance, and our full-to-single prediction scheme (v) is the best. 
The empirical results indicate that our proposed full-to-single mechanism is crucial for performance improvement. 

\textbf{Model components.}
As shown in Table~\ref{table:ablation_method}, an ablation study is performed to assess the effectiveness of different components of our method. 
We select the center frame of intermediate predictions from VTE as final results, which increases the 
MPJPE by 1.2 mm (from 46.9 mm to 48.1 mm). 
It proves that the scheme of intermediate supervision can further improve estimation accuracy. 
Next, we perform pooling operation after FFN of VTE following~\cite{zihang2020funnel-transformer} and then replace STE of our proposed method with it. 
The new architecture is termed as Pooling Transformer, and its error increases by 0.4 mm, which highlights that our STE can preserve more valuable information than Pooling Transformer by exploiting local contexts to aggregate information. 
Removing VTE (only trained with single-frame loss) leads to a 1.1 mm increase in MPJPE error. 
Besides, removing STE (only trained with sequence loss) increases the MPJPE to 47.6 mm. 
These results validate the importance of both VTE and STE modules in our Strided Transformer, where VTE mainly models long-range information and STE focuses on aggregating information in a hierarchical global and local fashion. 

\begin{figure*}[htb]
   \centering
   \begin{subfigure}[htb]{0.497\textwidth}
      \includegraphics[width=\textwidth]{figure/243_3_no_VTE.png}
      \caption{VTE}
      \label{fig:attetion_VTE}
   \end{subfigure}
   \begin{subfigure}[htb]{0.497\textwidth}
      \includegraphics[width=\textwidth]{figure/243_3_no_STE.png}
      \caption{STE}
      \label{fig:attetion_STE}
   \end{subfigure}
   \caption
   {
      Multi-head attention maps () from VTE and STE of our 243-frame model. 
      It illustrates that the self-attention mechanism systematically assigns a weight distribution to frames, all of which might benefit the inference. 
      Brighter color indicates higher attention score. 
   }
   \label{fig:attention}
\end{figure*}

\begin{figure}[htb]
   \centering
   \includegraphics[width=1.0 \linewidth]{figure/wild_compare.pdf}
   \caption
   {
      Qualitative comparisons on challenging in-the-wild videos with previous state-of-the-art methods, ATTN-TCN~\cite{liu2020attention}, TCN~\cite{pavllo20193d}, and GCN~\cite{cai2019exploiting}.  
      The last row shows the failure case, where the 2D detector has failed badly. 
   }
   \label{fig:wild_compare}
\end{figure}

\subsection{Qualitative Results}
\textbf{Attention visualization.}
Our method is easily interpretable through visualizing the attention score across frames to explain what the target frame relies on. 
Visualization results of the multi-head attention maps of the first attention layers from VTE and STE (243-frame model) are shown in Fig.~\ref{fig:attention}. 
The left map shows strong attention close to the input frames~\cite{Wu2020LiteTransformer,jiang2020convbert}, while the right map mainly pays strong attention to the center frame across all the sequences. 
This is expected since the proposed full-to-single strategy enables the VTE and STE modules to learn different representations: 
(i) VTE selectively identifies important sequences that are close to the input frames and enforces temporal consistency across frames. 
(ii) STE learns a specific representation from the input sequences using both past and future data, improving the representation ability of features to reach an optimal inference for the target frame. 
Note that a few attention head maps are sparse due to the different temporal patterns or semantics. 

\textbf{3D reconstruction visualization.}
We further evaluate our method on challenging in-the-wild videos from YouTube. 
Fig.~\ref{fig:wild_compare} shows the qualitative comparisons with the previous state-of-the-art methods~\cite{liu2020attention,pavllo20193d,cai2019exploiting}. 
We use the same 2D detector (cascaded pyramid network~\cite{chen2018cascaded}) to obtain 2D poses and then feed them to the models for a fair comparison. 
Despite the challenging samples with
complex actions and fast movements, the proposed method can produce realistic and structurally plausible 3D predictions outperforming previous works. 
This demonstrates our method is robust to partial occlusions and tolerant to depth ambiguity. 
The last row shows the failure case caused by a big 2D detection error.

\section{Conclusion} 
In this work, we investigate the suitableness of applying a Transformer-based network to the task of video-based 3D human pose estimation. 
From the proposed Strided Transformer with Strided Transformer Encoder (STE) and full-to-single supervision scheme, we show how the representative single-pose representation can be learned from redundant sequences. 
The key is to reasonably use strided convolutions in the Transformer architecture to aggregate long-range information into a single-vector pose in a hierarchical global and local fashion. 
Meanwhile, the computation cost can be reduced significantly. 
Moreover, our full-to-single supervision scheme enhances temporal smoothness and further refines the representation for the target frame. 
Comprehensive experiments on two benchmark datasets demonstrate that our method achieves superior performance compared with state-of-the-art methods. 

Although our method can reduce the computation cost of Transformers, the computational complexity and runtime cost of our method are still larger than temporal convolutional networks~\cite{pavllo20193d,chen2021anatomy}, indicated in Table~\ref{table:compare}. 
It is well acknowledged that the strong performance of Transformers comes at high computational costs. 
Note that the scope of this paper only targets improving FFN in the Transformer model. 
Future works may include designing a more efficient self-attention mechanism and extending our Strided Transformer to solve multi-view 3D human pose estimation. 
In addition, we hope that our approach would bring inspiration to the field of skeleton-based representation learning, \emph{e.g.}, action recognition, motion prediction, pose tracking, and so on. 

\bibliographystyle{IEEEtran}
\bibliography{ref.bib}

\newpage
\section{Appendix} 

\begin{figure*}[!hb]
	\centering
	\includegraphics[width=0.62 \linewidth]{figure/dataset.pdf}
	\caption
	{
    Visual results of our proposed method on Human3.6M dataset (first 3 rows) and HumanEva-I dataset (last 2 rows). 
	}
	\label{fig:dataset}
\end{figure*}

\begin{figure*}[!hb]
	\centering
	\includegraphics[width=0.86 \linewidth]{figure/wild.pdf}
	\caption
	{
    Qualitative results on challenging wild videos. 
    The number is the frame index of input videos.
	}
	\label{fig:wild}
\end{figure*}

\end{document}
