\newif\ifplain
\plaintrue

\ifplain
\documentclass[10pt,a4paper]{article}
\fi

\usepackage[usenames,dvipsnames]{xcolor} 
\usepackage[linesnumbered,ruled]{algorithm2e}
	\definecolor{comment}{rgb}{0,0.25,0}
	\SetAlgorithmName{Algorithm}{Algorithm}{List of Algorithms}
	\SetAlgoProcName{Procedure}{Procedure}
	\SetAlgoFuncName{Function}{Function}
	\SetAlFnt{\small}
	\SetAlCapFnt{\small}
	\SetAlCapNameFnt{\small}
	\SetAlCapHSkip{0pt}
	\SetProcFnt{\small}
	\SetProcNameFnt{\small}
	\SetProcArgFnt{\small}
\renewcommand{\CommentSty}[1]{\textnormal{\color{comment} #1}}
	\SetProcNameSty{textsc}
	\SetKw{KwAnd}{and}
	\SetKw{KwBreak}{break}
	\SetKw{KwDownTo}{downto}
	\SetKw{KwExit}{exit}
	\SetKw{KwOr}{or}
	\SetKw{KwReturn}{return}
	\SetKwFor{ForAll}{for all}{do}{end forall}
	\SetKwFor{ForAllPar}{for all}{do in parallel}{end forallpar}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{cite}

\usepackage{graphicx}

\usepackage[hidelinks]{hyperref}

\usepackage{mathtools}
	\mathtoolsset{showonlyrefs=true} 

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{
	tick label style = {font=\sansmath\sffamily\small},
	every axis label = {font=\sansmath\sffamily\small},
	legend style = {font=\sffamily\small},
	label style = {font=\sansmath\sffamily\small},
	title style = {font=\sansmath\sffamily\small},
}
\pgfplotscreateplotcyclelist{cyclelist1}{darkgray,mark=*,solid\\blue!50!cyan,mark=square*,solid\\purple,every mark/.append style={scale=1.3},mark=triangle*,solid\\orange,every mark/.append style={scale=1.3},mark=diamond*,solid\\black!50!blue,mark=*,dashed\\red,mark=square*,dashed\\black!50!green,every mark/.append style={scale=1.3},mark=triangle*,dashed\\magenta,every mark/.append style={scale=1.3},mark=diamond*,dashed\\}
\pgfplotscreateplotcyclelist{cyclelist11}{darkgray,thick,mark=*,solid\\blue!50!cyan,thick,mark=square*,dashed\\purple,thick,every mark/.append style={scale=1.3},mark=triangle*,dashdotted\\black!20!orange,thick,every mark/.append style={scale=1.3},mark=diamond*,dashdotdotted\\}
\pgfplotscreateplotcyclelist{cyclelist2}{black,mark=*,solid\\blue,mark=square*,solid\\red,every mark/.append style={scale=1.3},mark=triangle*,solid\\orange,every mark/.append style={scale=1.3},mark=diamond*,solid\\}
\usepgfplotslibrary{external}
\tikzexternalize
\tikzsetexternalprefix{images/tikz/}
\tikzsetfigurename{fig}
\expandafter\gdef\csname c@tikzext@no@\pgfkeysvalueof{/tikz/external/figure name}\endcsname{1}

\providecommand{\extendboundingbox}{\path
	([shift={(-5\pgflinewidth,-5\pgflinewidth)}]current bounding box.south west)
	([shift={( 5\pgflinewidth, 5\pgflinewidth)}]current bounding box.north east);
}

\usepackage{pgfplotstable}
\usepackage{sansmath}

\usepackage[final,notref,notcite,color]{showkeys}
\definecolor{labelkey}{rgb}{1,0,1}
\renewcommand*\showkeyslabelformat[1]{\fbox{\normalfont\footnotesize\ttfamily#1}} 

\usepackage{subfig}
\usepackage{xspace}

\ifplain
\usepackage[margin=25mm,includefoot]{geometry}
\usepackage[shortlabels]{enumitem}
\setlist{itemsep=0.3ex,parsep=0ex}
\fi


\providecommand{\nnz}{\ensuremath{\mathit{nnz}}\xspace}
\providecommand{\var}[1]{\ensuremath{\mathit{#1}}\xspace}

\begin{document}

\title{Loading Large Sparse Matrices Stored in Files in the Adaptive-Blocking Hierarchical Storage Format}
\author{Daniel Langr\footnote{E-mail: \href{mailto:langrd@fit.cvut.cz}{langrd@fit.cvut.cz}} \ and Ivan \v{S}ime\v{c}ek \ and Pavel Tvrd{\' \i}k \\
\small Czech Technical University in Prague \\
\small Department of Computer Systems \\
\small Faculty of Information Technology \\
\small Th\'{a}kurova 9, 160 00, Praha, Czech Republic \\
}


\maketitle

\begin{abstract}

The parallel algorithm for loading large sparse matrices from files into distributed memories of high performance computing (HPC) systems is presented. This algorithm was designed specially for matrices stored in files in the space-efficient adaptive-blocking hierarchical storage format (ABHSF). The algorithm can be used even if matrix storing and loading procedures use a different number of processes, different matrix-processes mapping, or different in-memory storage format. The file format based on the utilization of the HDF5 library is described as well. Finally, the presented experimental study evaluates the proposed algorithm empirically.

\textbf{Keywords:} adaptive-blocking hierarchical storage format, high performance computing, hierarchical data format, loading algorihtm, parallel I/O, sparse matrix.

\end{abstract}

\section{Introduction}
\label{sec:intro}


Large sparse matrices emerge frequently in high performance computing (HPC) applications. Sometimes, these matrices need to be stored to a file system and then loaded back, e.g., when the checkpointing-restart (C/R) resiliency technique is applied. The runtime of the store/load process is generally proportional to the amount of data processed by the I/O subsystem. Common in-memory sparse matrix storage formats used for computations are typically not much space efficient. We have shown that when matrices need to be saved to a file system, it pays off to convert them into some highly space-efficient format, such as the ABHSF~\cite{Langr:2013a}. However, this approach brings some runtime overhead into the I/O processes since matrices need to be converted between the in-memory format and the ABHSF on the fly. 
We have previously introduced and experimentally evaluated algorithms for conversion of sparse matrices to the ABHSF from the most commonly used formats---the \emph{coordinate} (COO) and \emph{compressed sparse rows} (CSR) formats~\cite{Saad:2003,Barrett:1994}. In this article, we focus on the successive step of loading matrices back from a file system to memory. 

Modern HPC systems are based on hybrid distributed/shared memory architectures. They consists of shared-memory nodes connected by fast networks. Matrices emerging in these systems thus need to be mapped to separate address spaces of different application (typically MPI) processes. Each process then takes care of some portion of matrix nonzero elements, which are in its address space present in memory in a sparse storage format. The conditions under which are matrices treated are given by 
\begin{enumerate}
\item the number of application processes,
\item the particular mapping of matrix nonzero elements to these processes,
\item the sparse storage format used for storing the to-process mapped elements in its address space.
\end{enumerate}
We further call these conditions simply the \emph{configuration}.

When matrices need to be loaded from a file system to memory, the configuration can be either same or different compared to the configuration used during the storage process. We consider both these options within this text and study their influence to the performance of the matrix loading processes.

\section{Data and File Structures}

Let  be an  matrix with \nnz nonzero elements. Let further  be stored in HDF5-based files in the ABHSF sparse storage format. The process of creating these files (matrix storage) including the storage algorithms was described by Langr et al.~\cite{Langr:2013a}. For all file operations, we use the HDF5 library~\cite{HDFGroup:2013} for both serial and parallel I/O; it is a de-facto standard for intensive I/O operations in HPC. Due to microbenchmarking performed on various modern HPC system, we chose a single-file-per-process strategy for storing matrices (in contrast to shared-file strategy where a single file is shared by all processes); it generally provided higher I/O performance in our measurements. This strategy means that during matrix storage, each process stores its local nonzero elements into a separate file independent of files accessed by other processes. All these files are stored in the directory called \texttt{matrix} in a HDF5-based file called \texttt{matrix-}\texttt{.h5spm}, where  denotes a process number (MPI process rank).

Assume that prior to the matrix storage,  was treated by  processes denoted by . Let  denote the set of nonzero elements of  stored in the address space of process . The nonzero elements of  are distributed among processes such that all the nonzero elements treated by process  falls to a submatrix of  that starts at row  and column  and has the size . Thus,

Then, for all the nonzero elements  treated by process  holds  and . In most general case, ,  and  might hold for each process . However, in practice, one- or two-dimensional partitioning schemes are most commonly used for matrix-processes mapping problem due to optimization of communication needed during sparse matrix-vector multiplication (SpMV) operation; see, e.g., \cite{Catalyurek:2010} for a survey of these schemes.

In contrary of the common mathematical notation used above, we further consider 0-based indexing for data structures and algorithm pseudocodes. The ABHSF is based on partitioning of the local (per-process) submatrix to a fixed blocks of sizes ; this format is described in detail by Langr et al.~\cite{Langr:2012b}. The structure of the \texttt{matrix-}\texttt{.h5spm} file is as follows:
\begin{tabbing}
\hspace*{1em} \= \hspace*{8em} \= \kill
\textbf{structure} \var{abhsf}  \\
\> \var{m}:           \> number of rows ; \\
\> \var{n}:           \> number of columns ; \\
\> \var{z}:           \> number of nonzero elements \nnz; \\
\> \var{m\_local}:    \> number of local rows ; \\
\> \var{n\_local}:    \> number of local columns ; \\
\> \var{z\_local}:    \> number of local nonzero elements ; \\
\> \var{m\_offset}:   \> first row of local submatrix ; \\
\> \var{n\_offset}:   \> first column of local submatrix ; \\
\> \var{block\_size}: \> block size ; \\
\> \var{blocks}:      \> number of nonzero blocks of local submatrix; \\
\> \var{schemes[]}:   \> scheme tags for nonzero blocks (COO, CSR, bitmap, dense); \\
\> \var{zetas[]}:     \> number of nonzero elements of nonzero blocks; \\
\> \var{brows[]}:     \> block row indexes of nonzero blocks; \\
\> \var{bcols[]}:     \> block column indexes of nonzero blocks; \\
\> \var{coo\_lrows[]}:     \> in-block row indexes of nonzero elements for COO blocks; \\
\> \var{coo\_lcols[]}:     \> in-block column indexes of nonzero elements for COO blocks; \\
\> \var{coo\_vals[]}:      \> in-block values of nonzero elements for COO blocks; \\
\> \var{csr\_lcolinds[]}:  \> in-block column indexes of nonzero elements for CSR blocks; \\
\> \var{csr\_rowptrs[]}:   \> in-block offsets of rows data for CSR blocks; \\
\> \var{csr\_vals[]}:      \> in-block values of nonzero elements for CSR blocks; \\
\> \var{bitmap\_bitmap[]}: \> bitmap structure of nonzero elements for bitmap blocks; \\
\> \var{bitmap\_vals[]}:   \> in-block values of nonzero elements for bitmap blocks; \\
\> \var{dense\_vals[]}:    \> in-block values of all elements for dense blocks; \\
.
\end{tabbing}

The data name accompanied with  denote HDF5 datasets (generally arrays). Other data names denote HDF5 attributes (generally simple variables). 

Let \var{csr} be a data structure that represents a matrix in the CSR storage format in a computer memory of process  defined as follows:
\begin{tabbing}
\hspace*{1em} \= \hspace*{6em} \= \kill
\textbf{structure} \var{csr}  \\
\> \var{m}:           \> number of rows ; \\
\> \var{n}:           \> number of columns ; \\
\> \var{z}:           \> number of nonzero elements \nnz; \\
\> \var{m\_local}:    \> number of local rows ; \\
\> \var{n\_local}:    \> number of local columns ; \\
\> \var{z\_local}:    \> number of local nonzero elements ; \\
\> \var{m\_offset}:   \> first row of local submatrix ; \\
\> \var{n\_offset}:   \> first column of local submatrix ; \\
\> \var{vals[]}:      \> values of nonzero elements; \\
\> \var{colinds[]}:   \> column indexes of nonzero elements; \\
\> \var{rowptrs[]}:   \> indexes of data for individual rows; \\
.
\end{tabbing}

Let \var{element\_t} be a auxiliary data structure representing a single matrix nonzero element defined as follows:
\begin{tabbing}
\hspace*{1em} \= \hspace*{4em} \= \kill
\textbf{structure} \var{element\_t}  \\
\> \var{row}:	\> row index; \\
\> \var{col}:	\> column index; \\
\> \var{val}:   \> value; \\
.
\end{tabbing}

\section{Algorithms}

The pseudocode for loading matrices from files stored in the ABHSF into computer memory is presented by \autoref{alg:direct}--\ref{alg:loadblockdense}. In memory, the loaded local matrix nonzero elements are as output stored in the CSR format. The algorithms can be easily adapted for the COO format as well; COO is simpler and more generic than CSR (another option is to convert elements from CSR to COO afterwards, such conversion is straightforward). 

Due to the length of the pseudocode, the algorithm is recursively split into several procedures. However, we assume that all variables and arrays have a global scope, i.e., they are directly accessible inside procedures as well (without passing them as parameters).

The presented pseudocode works for the same configuration that was used for matrix storage. However, it can be adapted even for situations where different store/load configuration is needed. Let  be the id/rank of a process that should have, in its local memory, a matrix elements  after the loading procedure. The algorithm for different storing and loading configurations differ from \autoref{alg:direct}--\ref{alg:loadblockdense} only slightly, thus we do not present it as a complete pseudocode. The changes consists of the following steps:
\begin{enumerate}
\item The presented algorithm need to be encapsulated with the outer loop, in which \emph{all} processes read \emph{all} stored files.
\item The read nonzero elements are stored into memory of process  only if .
\end{enumerate}

This adaption covers an arbitrary mapping of matrix nonzero elements to processes (given by the \emph{mapping function} ). Moreover, it also covers situations, where a different numbers of processes are used during storing and loading procedures. When a different in-memory storage format is finally required, the most straightforward way is to store elements in COO, sort them accordingly, and finally convert into the desired format. Since there many such formats exists in practice, such conversion is beyond the scope of this text.

\begin{algorithm}[p]
\DontPrintSemicolon
\KwIn{\var{abhsf}}
\KwOut{\var{csr}}
\KwData{, , \var{elements}, \var{zeta}, \var{brow}, \var{bcol}, \var{last\_brow}, , \var{row}, }
\BlankLine
 \;
 \;
 \;
 \;
 \;
 \;
 \;
 \;
 \;
 \;
 empty dynamic array of \var{element\_t} types \; 
\tcp{initial read of datasets entries:}
 \;
 \;
 \;
 \;
 \;
\For{ \KwTo }{
  call \textsc{LoadBlock} \;
  \If{}{
    \tcp{next block:}
     \;
     \;
     \;
     \;
    \tcp{process block row if needed:}
    \If{ \KwAnd }{
      \lIf{\var{elements} }{sort \var{elements} lexicographically} 
      \If{\var{elements} }{
         \;
        \For{ \KwTo }{
          \While{}{
            append  to  \;
             \;
          }
          append  to  \;
          append  to  \;
          \While{}{
            append size of \var{elements} to  \;
             \;
          }
          empty \var{elements} array \;
        }
         \;
      }
    }
  }
}
\caption{Loading of matrices from files in the ABHSF into memory.}
\label{alg:direct}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\uIf{}{
  call \textsc{LoadBlockCOO} \;
}
\uElseIf{}{
  call \textsc{LoadBlockCSR} \;
}
\uElseIf{}{
  call \textsc{LoadBlockBitmap} \;
}
\uElseIf{}{
  call \textsc{LoadBlockDense} \;
}
\Else{
  raise error (wrong scheme tag) \;
}
\caption{Procedure \textsc{LoadBlock}}
\label{alg:loadblock}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\KwData{, \var{lrow}, \var{lcol}, \var{element}}
\BlankLine
\For{ \KwTo }{
   next value from \var{abhsf.coo\_lrows[]} \;
   next value from \var{abhsf.coo\_lcols[]} \;
   variable of \var{element\_t} type \;
   \;
   \;
   next value from \var{abhsf.coo\_vals[]} \;
  append \var{element} into \var{elements} array \;
}
\caption{Procedure \textsc{LoadBlockCOO}}
\label{alg:loadblockcoo}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\KwData{\var{rowptrs\_1}, \var{lrow}, \var{rowptrs\_2}, \var{rowptr}, \var{lcol}, \var{element}}
\BlankLine
 next value from \var{abhsf.csr\_rowptrs[]} \;
\For{ \KwTo }{
   next value from \var{abhsf.csr\_rowptrs[]} \;
  \For{ \KwTo }{
     next value from \var{abhsf.csr\_lcolinds[]} \;
     variable of \var{element\_t} type \;
     \;
     \;
     next value from \var{abhsf.csr\_vals[]} \;
    append \var{element} into \var{elements} array \;
  }
   \;
}
\caption{Procedure \textsc{LoadBlockCSR}}
\label{alg:loadblockcsr}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\KwData{\var{bit}, \var{lrow}, \var{lcol}, \var{byte}, \var{element}}
\BlankLine
 \;
\For{ \KwTo }{
  \For{ \KwTo }{
    \If{}{
       next value from \var{abhsf.bitmap\_bitmap[]} \;
       \;
    }
    \If{}{
       variable of \var{element\_t} type \;
       \;
       \;
       next value from \var{abhsf.bitmap\_vals[]} \;
      append \var{element} into \var{elements} array \;
    }
     shifted right of 1 bit \;
     \;
  }
}
\caption{Procedure \textsc{LoadBlockBitmap}}
\label{alg:loadblockbitmap}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\KwData{\var{lrow}, \var{lcol}, \var{val}, \var{element}}
\BlankLine
\For{ \KwTo }{
  \For{ \KwTo }{
     next value from \var{abhsf.dense\_vals[]} \;
    \If{}{
       variable of \var{element\_t} type \;
       \;
       \;
       \;
      append \var{element} into \var{elements} array \;
    }
  }
}
\caption{Procedure \textsc{LoadBlockDense}}
\label{alg:loadblockdense}
\end{algorithm}

\section{Experiments}

We have performed experiments with the experimental MPI/C++ implementation of the proposed algorithm using the Anselm HPC system operated by IT4Innovations, located in Ostrava, Czech Republic. This system is a Bullx Linux cluster consisting of 3.3k Intel Sandy Bridge cores, 15 TB of memory, an Infiniband interconnect and a Lustre parallel file system.

As a source of sparse matrices, we used the scalable parallel generator of matrices based on enlargement of small ``seed'' matrices by a Kronecker product operation, see~\cite{Langr:2013b} for details. As a seed matrix, we used the real unsymmetric square matrix \textsf{cage12} with 130k or rows and columns and 2M of nonzero elements. This matrix was enlarged so that the local per-process matrix part occupied 256 GB of memory using the COO format, double precision representation of element values, and 32 bit row and column indexes. 

As for configuration, the enlarged matrix was mapped to processes in a row-wise manner, i.e, each process took care or a contiguous chunk of rows such that the amortized number of nonzero elements treated by each process was the same. For matrix storage, we used 60 MPI processes.

We measured the loading times of the following cases:
\begin{enumerate}
\item the same loading configuration was used as within storage procedure (i.e., 60 MPI processes and balanced row-wise mapping);
\item a different number of processes and a regular column-wise mapping (same amortized number of columns per process) was used.
\end{enumerate}
The second test case (different configurations) were measured for two different HDF5 parallel I/O strategies: \emph{indepedent} and \emph{collective}; see~\cite{HDFGroup:2013} for details. Recall that in this case, all processes read all files, thus all processes access each file at once.

The measured results are shown in \autoref{fig:results}. We can clearly see that when the storing and loading configurations match, the loading time is the lowest. Such a result was expected, since the overall amount of data processed by the I/O subsystem is the lowest as well. As for different configurations, the independent HDF5 strategy resulted in considerably lower loading times than the collective strategy. Moreover, these loading times were almost independent of the number of reading processes. We can also observe that these times are much lower than the loading time for the same configurations multiplied by the number of processes, which is proportional to the amount of processed data.

\pgfplotstableread{
processes collective independent direct 
30 1766.39 394.961 0
50 2058.73 286.18 0
55 2144.03 284.86 0
58 2225.29 289.376 0
59 2245.54 286.73 0
60 2244.9 284.733 20.0173                             
61 2233.36 285.016 0
62 2991.84 275.145 0
65 2411.73 270.195 0
70 3084.63 271.736 0
120 3470.52 255.111 0
}{\results}
\begin{figure}[t]
\centering
\includegraphics{fig1}
\caption{Measured time of the process of loading matrices from the file system to memory for different configurations.}
\label{fig:results}
\end{figure}

\section{Conclusions}

We have presented an algorithm for loading sparse matrices that were stored in files in the ABHSF. The algorithm works in cases where both the same or different configurations for storing and loading procedures were used. The presented method for different configurations is general. It can be used when a different number of processes, different matrix-processes mapping, and/or a different in-memory storage format is used. Due to this approach, the algorithm reads all stored files by all processes, which results in lower loading times (when compared with the same configuration case). When the configurations are different but determined, it might be possible to develop algorithms adapted especially for such configuration pairs. Such a development represents subjects for future research.

\subsection*{Acknowledgements}

This work was supported by the Czech Science Foundation under Grant No. P202/12/2011.
This work was supported by the IT4Innovations Centre of Excellence project (CZ.1.05/1.1.00/02.0070), funded by the European Regional Development Fund and the national budget of the Czech Republic via the Research and Development for Innovations Operational Programme, as well as Czech Ministry of Education, Youth and Sports via the project Large Research, Development and Innovations Infrastructures (LM2011033).

\bibliographystyle{abbrv}
\bibliography{langr}

\end{document}
