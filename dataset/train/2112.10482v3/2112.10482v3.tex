\begin{table*}[t]
\begin{center}
	\footnotesize\begin{tabular}{lcccccccccc}
        \toprule
Model                & Top-1 EM & Top-10 EM & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr & SPICE \\
\midrule
\textbf{Valid} \\
W2V NODET 	 &  18.78 & 46.25 & 25.23 & 16.74 & 12.74 & 9.40 & 29.26 & 11.18 & 55.10 & 9.71 \\
VoteNet+MCAN 	 &  17.33 & 45.54 & 28.09 & 16.72 & 10.75 & 6.24 & 29.84 & 11.41 & 54.68 & 10.65 \\
ScanRefer+MCAN (end-to-end) 	 &  18.59 & 46.76 & 26.93 & 16.59 & 11.59 & 7.87 & 30.03 & 11.52 & 55.41 & 11.28 \\
ScanQA 	 &  20.28 & 50.01 & 29.47 & 19.84 & 14.65 & 9.55 & 32.37 & 12.60 & 61.66 & 11.86 \\
\midrule
\textbf{Test w/ objects} \\
W2V NODET 	 &  21.64 & 52.81 & 27.42 & 18.30 & 13.92 & 9.90 & 31.26 & 12.08 & 59.61 & 9.51 \\
VoteNet+MCAN 	 &  19.71 & 50.76 & 29.46 & 17.23 & 10.33 & 6.08 & 30.97 & 12.07 & 58.23 & 10.44 \\
ScanRefer+MCAN (end-to-end) 	 &  20.56 & 52.35 & 27.85 & 17.27 & 11.88 & 7.46 & 30.68 & 11.97 & 57.36 & 10.58 \\
ScanQA 	 &  23.45 & 56.51 & 31.56 & 21.39 & 15.87 & 12.04 & 34.34 & 13.55 & 67.29 & 11.99 \\
\midrule
\textbf{Test w/o objects} \\
W2V NODET 	 &  20.05 & 50.43 & 27.24 & 19.07 & 15.53 & 11.06 & 28.92 & 11.45 & 55.33 & 9.75 \\
VoteNet+MCAN 	 &  18.15 & 48.56 & 29.63 & 17.80 & 11.57 & 7.10 & 29.12 & 11.68 & 53.34 & 10.36 \\
ScanRefer+MCAN (end-to-end) 	 &  19.04 & 49.70 & 26.98 & 16.17 & 11.28 & 7.82 & 28.61 & 11.38 & 53.41 & 10.63 \\
ScanQA 	 &  20.90 & 54.11 & 30.68 & 21.20 & 15.81 & 10.75 & 31.09 & 12.59 & 60.24 & 11.29 \\
\bottomrule
	\end{tabular}
    \caption{
        Performance comparison for question answering with image captioning metrics.
    }
\label{table:performance2}
\end{center}
\end{table*}

\begin{table*}[t]
\begin{center}
	\footnotesize\begin{tabular}{ccccccccccccc}
        \toprule

ANS & OBJ & LOC & Top-1 EM & Top-10 EM & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr & SPICE \\

\midrule
\multicolumn{3}{l}{\textbf{Valid}} \\
\Checkmark &  &  & 10.16 & 36.60 & 11.78 & 5.49 & 0.12 & 0.02 & 16.51 & 6.11 & 26.44 & 4.41 \\
\Checkmark &  \Checkmark & & 14.44 & 43.06 & 19.24 & 13.04 & 10.38 & 0.00 & 23.00 & 8.77 & 42.22 & 6.78 \\
\Checkmark &  & \Checkmark & 16.86 & 45.13 & 22.95 & 15.67 & 12.63 & 9.21 & 26.72 & 10.22 & 49.21 & 8.67 \\
\Checkmark &  \Checkmark & \Checkmark & 20.28 & 50.01 & 29.47 & 19.84 & 14.65 & 9.55 & 32.37 & 12.60 & 61.66 & 11.86 \\
\midrule
\multicolumn{3}{l}{\textbf{Test w objects}} \\
\Checkmark &  & & 12.16 & 42.77 & 12.86 & 5.45 & 0.12 & 0.02 & 17.55 & 6.71 & 29.17 & 4.05 \\
\Checkmark &  \Checkmark & & 18.31 & 49.18 & 22.32 & 14.53 & 11.15 & 7.92 & 26.37 & 9.94 & 49.10 & 6.36 \\
\Checkmark &  & \Checkmark &  20.46 & 51.67 & 25.06 & 16.91 & 14.06 & 11.37 & 29.22 & 11.13 & 55.17 & 8.21 \\
\Checkmark & \Checkmark & \Checkmark &  23.45 & 56.51 & 31.56 & 21.39 & 15.87 & 12.04 & 34.34 & 13.55 & 67.29 & 11.99 \\
\midrule
\multicolumn{3}{l}{\textbf{Test w/o objects}} \\
\Checkmark & & & 10.78 & 39.44 & 11.94 & 5.02 & 0.12 & 0.02 & 15.34 & 5.91 & 25.51 & 3.51 \\
\Checkmark &  \Checkmark & & 16.23 & 46.30 & 21.37 & 13.49 & 10.71 & 7.64 & 23.63 & 9.10 & 43.21 & 6.13 \\
\Checkmark &  & \Checkmark &  18.12 & 49.60 & 25.12 & 17.58 & 14.68 & 10.23 & 26.50 & 10.43 & 49.93 & 8.16 \\
\Checkmark & \Checkmark & \Checkmark &  20.90 & 54.11 & 30.68 & 21.20 & 15.81 & 10.75 & 31.09 & 12.59 & 60.24 & 11.29 \\
\bottomrule
	\end{tabular}
    \caption{
        Performance comparison.
    }
\label{table:performance1}
\end{center}
\end{table*}

\begin{table*}[t]
\begin{center}
	\footnotesize\begin{tabular}{lcccccccccc}
        \toprule
Model                & Top-1 EM & Top-10 EM & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr & SPICE \\
\midrule
\textbf{valid} \\
ScanQA (xyz)  	 &  20.34 & 49.71 & 29.46 & 19.64 & 13.98 & 9.06 & 32.55 & 12.69 & 62.25 & 12.43 \\
ScanQA (xyz+rgb)  	 &  20.13 & 50.44 & 29.17 & 19.52 & 13.93 & 9.01 & 32.07 & 12.63 & 61.22 & 12.01 \\
ScanQA (xyz+rgb+normal)   	 &  20.00 & 49.11 & 30.00 & 19.25 & 12.91 & 7.47 & 32.54 & 12.55 & 60.83 & 12.45 \\
ScanQA (xyz+multiview)  	 &  20.32 & 50.65 & 30.19 & 19.59 & 14.00 & 9.53 & 32.78 & 12.74 & 62.43 & 12.64 \\
ScanQA (xyz+multiview+normal) 	 &  20.28 & 50.01 & 29.47 & 19.84 & 14.65 & 9.55 & 32.37 & 12.60 & 61.66 & 11.86 \\
\midrule
\textbf{Test w/ objects} \\
ScanQA (xyz)  	 &  24.02 & 55.67 & 32.14 & 21.89 & 16.76 & 12.75 & 34.95 & 13.92 & 68.36 & 11.95 \\
ScanQA (xyz+rgb)  	 &  23.59 & 55.43 & 31.80 & 21.86 & 16.77 & 13.06 & 34.66 & 13.83 & 68.23 & 12.06 \\
ScanQA (xyz+rgb+normal)   	 &  23.49 & 54.50 & 32.11 & 21.87 & 15.92 & 11.29 & 34.40 & 13.67 & 67.94 & 11.72 \\
ScanQA (xyz+multiview)  	 &  23.57 & 55.51 & 32.29 & 21.61 & 16.11 & 12.41 & 34.64 & 13.77 & 67.62 & 12.25 \\
ScanQA (xyz+multiview+normal) 	 &  23.45 & 56.51 & 31.56 & 21.39 & 15.87 & 12.04 & 34.34 & 13.55 & 67.29 & 11.99 \\
\midrule
\textbf{Test w/o objects} \\
ScanQA (xyz)  	 &  20.70 & 53.96 & 31.68 & 22.32 & 17.42 & 12.93 & 31.59 & 12.99 & 61.33 & 12.19 \\
ScanQA (xyz+rgb)  	 &  21.34 & 54.12 & 31.64 & 22.00 & 16.39 & 11.66 & 31.79 & 12.98 & 62.24 & 11.87 \\
ScanQA (xyz+rgb+normal)   	 &  20.90 & 52.72 & 31.48 & 21.28 & 15.76 & 11.04 & 31.56 & 12.84 & 60.73 & 11.66 \\
ScanQA (xyz+multiview)  	 &  21.35 & 53.70 & 31.55 & 21.63 & 15.95 & 11.17 & 31.85 & 12.89 & 61.86 & 11.95 \\
ScanQA (xyz+multiview+normal) 	 &  20.90 & 54.11 & 30.68 & 21.20 & 15.81 & 10.75 & 31.09 & 12.59 & 60.24 & 11.29 \\
\bottomrule
	\end{tabular}
    \caption{
        Performance comparison.
    }
\label{table:performance2}
\end{center}
\end{table*}



\section{Experiments}


\subsection{Experimental Setup}
\noindent \textbf{Data augmentation.} In this experimental setup, we refer to the experimental setup of existing studies on scene understanding of ScanNet~\cite{dai2017scannet}
through languages such as ScanRefer and Scan2Cap~\cite{chen2020scanrefer,chen2021scan2cap}.
We apply data augmentation to our training data and apply rotation about all three axes by a random angle in $[-5\textdegree, 5\textdegree]$and randomly translate the point cloud within 0.5 meters in all directions. Since the ground alignment in ScanNet is incomplete, we rotate it on all axes (not just the top).




\noindent \textbf{Training.}
For training ScanQA model, we used Adam~\cite{kingma2014adam}, batch size $16$, and an initial learning rate of $5\mathrm{e}{-4}$.
We train the model for $30$ epochs and decrease the learning rate by by $0.2$ times after $15$ epochs.
To mitigate fitting exactly against its training data, we set the weight decay factor to $1\mathrm{e}{-5}$.

\noindent \textbf{Evaluation.}
Our ScanQA takes as input both point clouds of the entire scene and a given question and generates object proposals.
For answering a question, our model selects the answer from candidates based on the features of the object proposal corresponding to the question words.
The goal of our system is to return an accurate answer using 3D-VQA models.
To evaluate QA performance, we used accuracy at 1 and 10 (Acc@1, Acc@10, respectively) as the evaluation metric.
When using the detection module (VoteNet), we average the results of three evaluation runs with different random seeds.


\noindent \textbf{Baselines.}
To valid the effects of 3D-VQA model, we prepared several baselines.
We conducted our empirical investigation using the following models.

\noindent \textit{\textbf{RandomgImage+2D-VQA}}
First, we prepared several 2D-VQA models as baselines to show how our 3D-VQA models outperform 2D-based VQA models for understanding 3D scenes. 
We use {MCAN} ~\cite{Yu_2019_CVPR} for 2D-VQA models.
MCAN is a transformer network~\cite{NIPS2017_3f5ee243} that uses the cross attention mechanism to represent the relationship between question words and features of objects in the image. 
The proposed method uses some of the modules used in mcan, such as self-attention and cross-attention, to create 3D-VQA. By comparing the two, we can confirm the importance of creating a model specialized for 3D-VQA.
Since 2D models cannot be directly applied to a 3D environment, images are randomly extracted from the video taken to build the ScanNet dataset. We used a bottom-up attention model~\cite{Anderson_2018_CVPR} for extracting objects' appearance features. Then we applied pre-trained 2D-VQA models to these images and compute the answer score for each image. Finally, we select the most probable answer according to the averaged all answer scores of these images.


\noindent \textit{\textbf{OracleImage+2D-VQA}}
To investigate the upper bound of the performance of 2D-VQA for questions in 3D space, we used images around target objects associated with question-answer pairs. 
The camera position is set based on the coordinates of the bounding box of the correct object, and the image is captured from the direction and distance that the bounding box reflects the most. Depending on the position of the camera, the object may not be visible, so multiple images were used.
We applied 2D-VQA models to these images similar to RandomImage+2D-VQA.
Note that it is difficult to obtain such images in actual question and answer situations.
By looking at the performance of this method, we can learn the difficulty in solving this 3D-VQA task.

\noindent \textit{\textbf{VoteNet+MCAN}}
VoteNet is a 3D object detection method about locating and recognizing objects in a 3D scene.
This method detect objects in 3D space and extracts their features fed in to the standard VQA model.
Unlike our method,
this method does not take into account where the target object is located in the 3D space, what the target object is, etc.



\noindent \textit{\textbf{ScanRefer+MCAN (pipeline)}}
ScanRefer is a 3D object localization method for localizing a given linguistic description to a corresponding target object in 3D space.
ScanRefer internally uses VoteNet to detect objects in the room, and estimates the object corresponding to the description from among the candidate objects.
Even thought ScanRefer cannot be used directly for question answering, we used ScanRefer to identify the object corresponding to the question and applied 2D-VQA (MCAN) to the images around the object localized by ScanRefer.
Since both ScanRefer and MCAN are performed separately, it is not possible to learn them end-to-end.

\noindent \textit{\textbf{ScanRefer+MCAN (end-to-end)}}
This method is more sophisticated and closer to the proposed method.
While ScanRefer+MCAN (pipeline) conducts the localization of the object corresponding to the question and question answering separately, in this method, this method learn localization and QA modules simultaneously.
Specifically, the method takes as input the object proposal feature of ScanRefer for VQA models and then 
predict answers based on 3D object features and question content.
Unlike this method, the proposed method shares the same modules for localization and QA and learns the common parts of both simultaneously like multi-task learning~\cite{Zhan_2021_TKDE}.



\subsection{Quantitative Analysis}
\noindent \textbf{Overall results.} 
We show the 3D-QA performance on ScanQA dataset in Table~\ref{table:performance1} with image captioning metrics.
The best result per column is marked in bold typeface.
We compared our ScanQA with competitive baselines: VoteNet+MCAN, ScanRefer+MCAN (pipeline) and ScanRefer+MCAN (end-to-end).
These baselines share some of the components of the proposed method and can be used to identify which elements are important for 3D-QA.
The results show that our ScanQA method significantly outperformed all baselines across all data splits over all evaluation metrics. In particular, 
ScanQA significantly outperformed ScanRefer+MCAN (end-to-end), which learns QA and object localization separately over all baselines across all data splits over all evaluation metrics, indicating that ScanQA have succeeded in synergistic learning by sharing object localization and question module.
ScanQA also outperformed VoteNet+MCAN with large margins. This result suggests using localizing objects in 3D space and predicting object categories related questions are important for 3D-QA. We will clarify this point in the ablation study.
Interestingly, VoteNet+MCAN, ScanRefer+MCAN (end-to-end), ScanQA markedly outperformed ScanRefer+MCAN (pipeline), 
which detect target objects related to a question using a pretrained ScanRefer and then apply 2D-QA to surrounding target images indicating that end-to-end training with 3D and language information is important to solve 3D-QA model problems.



\subsection{Ablation Studies}
We conduct an ablation study on our model to examine what components and point cloud features contribute to the 3D-QA performance. Table~\ref{}

\subsection{Qualitative Analysis}

Finally, we demonstrate how multi-step spatiotemporal reasoning works by visualizing ex- amples. Fig. 4 shows some typical examples from the reasoning process of the TS-STMAC network. We selected the frames based on a score, which is the product of the temporal at- tention to a frame and the top five spatial attentions on the regions at each reasoning step. We also show words that receive more attention from the controller unit. The results show the cell tends to identify relevant frames and regions through multi-step reasoning, suggesting that our method effectively incorporated the spatial and temporal features as well as textual information into VideoQA.