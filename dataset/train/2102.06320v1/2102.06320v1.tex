\documentclass{article}
\usepackage[letterpaper, margin=0.7in]{geometry}

\usepackage[utf8]{inputenc}

\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}

\usepackage{booktabs}

\usepackage[inline]{enumitem}

\usepackage{tablefootnote}
\usepackage{multirow}

\usepackage{adjustbox}

\usepackage{comment}

\usepackage{graphicx}
\usepackage{framed}

\begin{document}

\title{On Automatic Parsing of Log Records}

\author{ Jared Rand and Andriy Miranskyy \\
Department of Computer Science, Ryerson University, Toronto, Canada \\
jrand@ryerson.ca, avm@ryerson.ca
}
\date{}

\maketitle

\begin{abstract}
Software log analysis helps to maintain the health of software solutions and ensure compliance and security. Existing software systems consist of heterogeneous components emitting logs in various formats. A typical solution is to unify the logs using manually built parsers, which is laborious.

Instead, we explore the possibility of automating the parsing task by employing machine translation (MT). We create a tool that generates synthetic Apache log records which we used to train recurrent-neural-network-based MT models. Models' evaluation on real-world logs shows that the models can learn Apache log format and parse individual log records. The median relative edit distance between an actual real-world log record and the MT prediction is less than or equal to 28\%. Thus, we show that log parsing using an MT approach is promising.

\end{abstract}

\section{Introduction}
Modern software solutions consist of a stack of hardware and software components~\cite{DBLP:conf/acsac/YenOOLRJK13,DBLP:journals/software/MiranskyyHCL16}. A failure of a component may lead to an outage, affecting the solution’s customers. It is crucial to quickly detect such a failure and identify the potential root cause of the problem to create a fix. Detection of the root cause is a daunting task, taking up to 40\% of the time needed to fix the problem~\cite{DBLP:journals/jss/MurtazaHMG14}. Finding the root causes is done by examining the logs emitted by the solution’s components~\cite{DBLP:journals/jss/MurtazaHMG14,DBLP:conf/sigsoft/MiranskyyMGDWG07,DBLP:conf/icse/BeschastnikhBEK14,DBLP:journals/tse/MarianiPS17}. 
Additionally, these logs may be used in the detection of cyberattacks~\cite{DBLP:conf/acsac/YenOOLRJK13} or auditing compliance with policies~\cite{DBLP:conf/acsac/YenOOLRJK13,DBLP:conf/IEEEares/DernaikaCCR19}.

There exist tools that do automatic log examinations and speed up diagnostics~\cite{DBLP:journals/jss/MurtazaHMG14,DBLP:conf/sigsoft/MiranskyyMGDWG07,DBLP:conf/icse/BeschastnikhBEK14,DBLP:journals/tse/MarianiPS17}. Unfortunately, components have individual log formats, while analysis tools prefer the logs in a unified format~\cite{DBLP:journals/software/MiranskyyHCL16}. Thus, maintainers need to manually create individual converters for every log format~\cite{DBLP:conf/icws/HeZZL17}. Given that a solution may have thousands of components, this becomes laborious~\cite{DBLP:journals/software/MiranskyyHCL16,DBLP:conf/iwpc/MessaoudiPBBS18}. If one can create a tool that parses logs automatically,  this tool will speed up defect detection and repair, allowing developers to focus on creating new functionality and reducing maintenance efforts. 

There exists a significant body of literature on recognizing formats. Researchers worked on detecting field type\footnote{Note that it is relatively easy to detect some patterns, e.g., dates, using basic RegEx. But a basic RegEx may have challenges understanding if a date belongs to the log record's timestamp field or if the date is part of a log message itself, which implies that developers will have to code up additional rules. } in a structured record set~\cite{DBLP:conf/kdd/HulsebosHBZSKDH19}, developing parsers rapidly and incrementally~\cite{DBLP:conf/csmr/NierstraszKGLB07}, extracting specific information from a log string~\cite{DBLP:conf/icdm/FuLWL09,DBLP:journals/tkde/MakanjuZM12,DBLP:conf/icws/HeZZL17,DBLP:conf/IEEEares/DernaikaCCR19}, and detecting log formats by comparing multiple similar log strings~\cite{DBLP:conf/icdm/Du016,DBLP:conf/iwpc/MessaoudiPBBS18,DBLP:journals/infsof/El-MasriPGHB20}. \textit{But can we individually parse a complete raw log string  (so that we can convert it to a universal format) without comparing it to other log records emitted by the same component? }

This leads us to the following research question (RQ): \textit{How can we automatically parse an individual raw log string?} To answer our RQ, we reduce parsing to a machine translation (MT) task. That is, we need to create an Oracle that, given a raw log string as input, will produce a string of tokens, showing which particular field a character in the input string belongs to. A toy example of the input and output is given in Figure~\ref{fig:toy}.

How should an Oracle look? In the field of MT for natural languages,  the best models are currently based on deep neural networks (DNNs) following an encoder-decoder architecture that implements sequence-to-sequence learning.  We will discuss the details of the models in Section~\ref{sec:models}.

DNNs require large volumes of data for training. Thus, we create a tool to generate synthetic logs mimicking real ones. We will further discuss synthetic data generation and real logs (used for validation of the models) in Section~\ref{sec:data}. 
We will then discuss the setup of our experiments in Section~\ref{sec:setup}, followed by a comparison of the performance of our models in Section~\ref{sec:results} and a summary of our findings in Section~\ref{sec:summary}. Finally, we will provide details for accessing the tool and the logs in Section~\ref{sec:access}.


\begin{figure}[bt]
    \centering
    \begin{framed}
    \small
    \texttt{2020-09-20 jane ERROR file not found} \\
    \texttt{tttttttttt\_uuuu\_lllll\_iiiiiiiiiiiiii} 
    \end{framed}
    \caption{A toy example of log translation. The top line represents an input string of the raw log. The characters in the bottom line represent the field type that a given input character is mapped to. For example, all the characters in the substring \texttt{2020-09-20} belong to a time field, denoted by \texttt{t}; substring \texttt{jane} represents a user field denoted by \texttt{u};  substring \texttt{ERROR} is a log record type field denoted by \texttt{l}, and  substring \texttt{file not found} is log message details denoted by \texttt{i}. Finally, \texttt{\_} denotes a separator between the fields. }
    \label{fig:toy}
\end{figure}

\section{Models} \label{sec:models}
We chose DNN architectures, popular for machine translation of natural language. First, we compared recurrent neural networks using gated recurrent units  (GRU)~\cite{DBLP:conf/emnlp/ChoMGBBSB14} against those using long short-term memory (LSTM)~\cite{DBLP:journals/neco/HochreiterS97} cells. Our preliminary experiments showed that LSTM-based translators outperformed the GRU-based ones (hinting that extra memory-related gates present in the LSTM cell but absent in the GRU cell may be necessary for our task). Thus, we focused on LSTM-based translators and converged on three architectures. 

The first one, deemed , is based on the classic neural MT architecture akin to~\cite{DBLP:conf/nips/SutskeverVL14} with LSTM-based encoder and decoder layers. The second one, deemed , is similar to  but added the attention layer as per Bahdanau et al.~\cite{DBLP:journals/corr/BahdanauCB14}. The third one, deemed , is based on the seq2seq architecture~\cite{DBLP:journals/corr/BritzGLL17} with the LSTM layers and  Luong et al. attention mechanism~\cite{DBLP:conf/emnlp/LuongPM15}. During our initial tests we evaluated  with and without beam search decoding~\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}. We ended up using  with regular inference as initial tests found beam search evaluation to be inferior for our datasets.


\section{Logs under study} \label{sec:data}
In this work, we focus on log formats of the  Apache HTTP server~\cite{apachemodlog:online}. The ubiquitousness of this product makes it a good candidate for our tests.

We will discuss our generator tool, designed to create synthetic Apache log records in Section~\ref{sec:generator}. Then we will discuss the real-world datasets in Section~\ref{sec:real_data}, followed by the description of the synthetic datasets used for training the Oracle in Section~\ref{sec:synth_data}.  

\subsection{Synthetic logs' generator} \label{sec:generator}
As discussed above, to train DNNs, we need large volumes of data. There exist tools for generating synthetic logs, e.g.,~\cite{faker:online,kiritbas44:online}. However, for the training of the DNNs, we need not only the raw log strings, which will serve as input to a DNN, but also the information about the log strings' format that will be used as output from the model. 

Most of the generators use the Apache Common Log Format (hereby referred to as CLF) or Combined Log Format (an extended version of CLF hereby referred to as ELF). Explanations of what these formats look like can be found in Table~\ref{tab:formats} (for details, see Apache manual~\cite{apachecombinedlogformat:online}).

While CLF and ELF are the most common forms of Apache logs, we did not feel confident that a DNN trained solely on CLF and ELF could accomplish our goal of recognizing the fields of any Apache log. Additionally, the existing fake log generators did not vary much in their generated data, often using the current date of generation as the date for each log record and using a small subset of fake websites and sub-pages for which it would create requests. Furthermore, in order to train the DNNs, we needed `ground truth' output strings, which would be easier to generate alongside the log strings rather than after the fact.

Thus, we created our own Python-based generator, which extends and combines the Faker~\cite{faker:online} and fake-useragent~\cite{fakeuser:online} libraries. These libraries were used to realistically generate specifically-formatted fields, such as the user agent field and IPv6 addresses. 

Our generator creates synthetic log records based on 15 fields\footnote{There are  permutations of log formats that can be generated from these 15 fields. For efficiency, we only implemented these 15 fields out of 43 total possible Apache fields~\cite{apachemodlog:online}. The generator can be easily extended with additional fields, if required. } and one separator field listed in Table~\ref{tab:fields}. The user should specify the total number of the log records to produce and the type of format (random or fixed). 
The tool then generates raw log records (input) and the associated translations (outputs). An example of the generated data is shown in Figure~\ref{fig:synth_record}.

\begin{table}[bt]
\caption{ Apache log formats, see Table~\ref{tab:fields} for fields' description.}
\label{tab:formats}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Name                             & Fields (acronyms)       \\ 
\midrule
Common Log Format (CLF)          & \texttt{h l u t "r" s b}         \\
Combined Log Format (ELF) & \texttt{h l u t "r" s b "R" "i"} \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[bt]
\caption{A list of fields that our synthetic log generator can produce. For details about the fields, see~\cite{apachemodlog:online}.}
\label{tab:fields}
\centering
\begin{tabular}{@{}p{0.09\columnwidth}p{0.87\columnwidth}@{}}
\toprule
Field & Field           \\                                                         
acronym & description   \\ \midrule
\texttt{h}             & IP address of the client host. Can be IPv4 or IPv6. \\
\texttt{l}             & The remote logname. We were unable to find a good example of what   kinds of values are returned by Apache servers, thus for this  paper we only supplied the commonly-given value ‘-’. This field could not be omitted as it is present in both the ELF and CLF formats.  \\
\texttt{u}             & The remote username. Can be empty (‘-’) \\
\texttt{t}             & The datetime of the request, presented in the default {[}day/month/year:hour:minute:second zone{]} format.  \\
\texttt{r}             & The request line from the client. Made up of the method, path and   querystring, and protocol. \\
\texttt{s}             & The status of the request. \\
\texttt{b}             & The number of bytes sent.   \\
\texttt{m}             & The request method.   \\
\texttt{U}             & The requested URI path.  \\
\texttt{H}             & The request protocol.  \\
\texttt{q}             & The request querystring.   \\
\texttt{v}             & The canonical servername of the server servicing the request.  \\
\texttt{V}             & The servername according to UseCanonical. In our generator, this   field is identical to the \texttt{v}   field.   \\
\texttt{i}            & The user agent of the request\tablefootnote{\label{extrafieldnote} In a real Apache HTTP server deployment, this field is extracted from the \texttt{\%i}  log parameter, see~\cite{apachemodlog:online} for details.}. \\
\texttt{R}             & The referrer of the request\textsuperscript{\ref{extrafieldnote}}. \\   
\texttt{\_}             & Represents a separator between log fields. \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
    \centering
    \tiny
    \begin{framed}
    \texttt{1: 41.193.93.229 - - 10/Jul/7983:05:08:49 +0100 "GET explore/category/home.html HTTP/2" 302 65953} \\
    \texttt{2: hhhhhhhhhhhhh\_l\_u\_tttttttttttttttttttttttttt\_rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\_sss\_bbbbb} \\
    \texttt{3: 192.168.4.25 - - [22/Dec/2016:16:11:41 +0300] "POST /DVWA/login.php HTTP/1.1" 200 1532 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; w3af.sf.net"} \\
    \texttt{4: hhhhhhhhhhhh\_l\_u\_[tttttttttttttttttttttttttt]\_"rrrrrrrrrrrrrrrrrrrrrrrrrrrrr"\_sss\_bbbb\_"R"\_"iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"} \\
    \texttt{5: PUT ycgmvjc.com HTTP/2 - 102186 - \url{d8idf~gyck.html}} \\
    \texttt{6: mmm\_VVVVVVVVVVV\_HHHHHH\_l\_bbbbbb\_F\_UUUUUUUUUUUUUUU}
    \end{framed}
    \caption{An example of a record for translation generated by our log generator.  Lines 1, 3, and 5 are examples of logs in CLF, ELF, and random format, respectively. The characters in lines 2, 4, and 6 represent mapping to their equivalent field type (described in Table~\ref{tab:fields}).  }
    \label{fig:synth_record}
\end{figure*}


\subsection{Real logs} \label{sec:real_data}

To validate the generalizability of the DNNs trained on synthetic log records produced by the generator discussed in Section~\ref{sec:generator}, we take three publicly available log files and deem the validation datasets ~\cite{apacheht89:online}, ~\cite{apacheht41:online}, and ~\cite{examples55:online}. The first two logs capture execution results of two web vulnerability scanners, namely, Netsparker and Acunetix~\cite{BASSEYYAR201828}. The third one is a sample Apache web server log file that can be processed by Elastic software. The summary statistics of the log files is shown in Table~\ref{tab:datasets}.

The log format of the validation logs is (or is extremely similar to) ELF\footnote{The log records of  and  were wrapped in quotes (\texttt{"}), which makes them a slightly different format from ELF. While this is likely a product of processing and these quotes could be removed, we opted to keep them for training completeness.}. We have parsed the log records from these three files using custom scripts to produce the correct character-to-field mapping strings. 

\subsection{Synthetic datasets} \label{sec:synth_data}

Using our log generator (covered in Section~\ref{sec:generator}), we create synthetic logs for training and testing, while varying the number of log records and their formats. We create five synthetic datasets that may help us uncover various challenges of the ``log translation'' task. The datasets summary statistics are given in Table~\ref{tab:datasets}; their details are as follows.

The first dataset, deemed , contains \textit{trivial}-to-learn data. 
The triviality of this dataset refers to the ability of models trained on this set to learn the fields and correctly interpret the validation datasets.  consists of 100K ELF mock logs. The models trained on this dataset should be able to recognize the ELF-based log records from our validation datasets very well. However, the generalizability to other formats would be mediocre.

The second dataset, deemed , has \textit{easy}-to-learn data. 
It contains 20K lines using a mix of the formats: ELF, CLF, and randomly generated records, which are generated based on all the fields in Table~\ref{tab:fields}. The random and CLF records are added to assess the generalizability of the model. Hypothetically, models trained on  and validated on  datasets will have worse performance than those trained on , because it has fewer log records and the model has to learn of multiple formats.

The third dataset, deemed , contains \textit{moderately}-difficult-to-learn data.
 contains a mix of formats:  of 100K records are in the CLF format and  have randomly generated formats. Based on  Table~\ref{tab:formats},  CLF is similar to ELF, but two of the fields (\texttt{R} and \texttt{i}) are missing. Thus, it will be harder for the model to recognize ELF records in . 

The fourth dataset, deemed , is a shorter version of  containing only 20K records. The more observations a dataset has, the more information a machine learning model will obtain for training. However, this comes at a cost of increased training time. Thus, we want to see how the change in observations affects models' performance.
  will help us assess how much data are needed for training. 

The fifth dataset, deemed , carries \textit{hard}-to-learn data. 
This dataset contains 100K randomly generated records, and none of them look like CLF or ELF. Thus, models trained on this dataset must learn the nature of the fields well in order to properly parse log records in .


\begin{table*}[bt]
\caption{ Datasets' description.}
\label{tab:datasets}
\centering
\begin{tabular}{@{}lrrrrp{0.53\textwidth}@{}}
\toprule
Dataset & Log records&  \multicolumn{3}{c}{Log records length}   & Log records' format description \\
  \cmidrule{3-5}
 & count & min & median & max  &   \\
\midrule
  & 100,000 & 136 & 272 & 1173              &  ELF  \\
  & 20,000 & 4 & 250 & 1173              &  of the ELF format,  of the CLF format, and  of randomly drawn and reshuffled fields shown in Table~\ref{tab:fields}.  The random strings have  to  records in them.  \\
  & 100,000 & 4 & 161 & 1294              &  of the CLF format and   of the randomly generated records using the same approach as in the  case. The random strings have  to  fields in them.    \\
  & 20,000 & 4 & 162 & 1527              &  Identical to the one of .  \\
  & 100,000 & 4 & 291 & 1528              &  of the randomly generated records using the same approach as in the  case. The random strings have  to  fields in them.    \\
\midrule
  & 7,314 & 194 & 238 & 602 & 100\% ELF                                \\
  & 6,539 & 79 & 238 & 4398 & 100\% ELF                                \\
  & 10,000 & 81 & 231 & 1363  & 100\% ELF                                \\ 
\bottomrule
\end{tabular}
\end{table*}

\section{Experimental Setup} \label{sec:setup}
We want to assess the generalizability of our approach. To do this, we create three groups of experiments where a model is trained on Training Datasets  (described in Section~\ref{sec:synth_data}). We train the neural networks for up to 300 epochs with a mini-batch size of 64 log records. Optimization is done using the Adam algorithm\footnote{We used learning rate of , , , and .}~\cite{DBLP:journals/corr/KingmaB14}. Each dataset is randomly split into 90\% training data and 10\% validation data during training.  The best model, preserved for further evaluation, is the one with the lowest cross-entropy loss for  and  and sparse categorical cross-entropy loss for . Translation is done at the character-level, i.e., we do not do any log record prepossessing.

We have tuned the models' hyper-parameters. For  and , we have experimented with different number of cells in these layers, namely . This was done to assess the degree of `freedom' required to recognize different log formats. We also varied the dropout rates in the encoder and decoders layer, setting them to . This was done to regularize the model and reduce overtraining.

We evaluate the performance of the models on three real-world log files (discussed in Section~\ref{sec:real_data}). The performance is measured using the Levenshtein (a.k.a. the edit) distance~\cite{levenshtein1966binary}, which computes the number of changes needed to transform a string returned by the Oracle into an expected (ground truth) string. 
We report the absolute and relative Levenshtein distances, deemed  and , respectively. For a given log record,  is computed by dividing  by the log record length.  and  lie within  range: the closer to ~--- the better. 

\section{Results} \label{sec:results}

\subsection{Comparison approach}
It is easy to measure  and  for individual strings. However, how to aggregate the measurements for a particular  dataset? Let us look at the distributions of the  and  values. For the sake of brevity, we show the distributions only for the models trained on  in Figure~\ref{fig:distr}. As we can see, the distributions have a long right tail: although the median values of  are between 51 and 61, the max values go up to 4283. This is due to the presence in  and  of a small number of long log records (as shown in Table~\ref{tab:datasets}). Namely, less than  0.2\% of log records are longer than 1000 characters, while the median length of the log records is between 231 and 238 characters for all three . 
Once we normalize the results using the  , we can see that the amount of changes is proportional to the string’s length, with the median  between ~21 and 25\%.

To preserve space and allow quantitative comparison, we present the summary statistics for the remaining evaluations by computing min, max, mean, and various quantiles of the distributions. The results for the top models, based on the lowest values of  and  for a given quantile, are shown in Table~\ref{tab:results}.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{figs/T_M_ed_sup.pdf}
    \caption{Distribution of  and  values (left and right pane, respectively) for the best model trained on the  dataset and validated on , , and  datasets.}
    \label{fig:distr}
\end{figure}

\setlength{\tabcolsep}{2.6pt}
\begin{table*}[tb]
\caption{ Summary statistics for the distributions of  and  for the best performing models. All the models have 512 cells in the LSTM layers;  denotes the dropout rate and is either  or .  denotes the sample quantile function, where  is the probability value; e.g., when , the  returns the median of a distribution. For example, for a model trained on dataset , when validated on , the maximum value of  is  (as shown in the bottom-right corner). }
\label{tab:results}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}ll|rrrrrrrr|rrrrrrrr|rrrrrrrr@{}}
\toprule
 \multicolumn{2}{c|}{Trained}     & \multicolumn{8}{c|}{}                                                 & \multicolumn{8}{c|}{}                                                 & \multicolumn{8}{c}{}                                                 \\
  \multicolumn{2}{c|}{on}            & min  & avg  &  &  &  &  &  & max  & min  & avg  &  &  &  &  &  & max  & min  & avg  &  &  &  &  &  & max  \\
\midrule
\multirow{6}{*}{} &  & 7 & 33 & 13 & 21 & 101 & 119 & 224 & 298 & 7 & 22 & 14 & 14 & 17 & 33 & 105 & 4150 & 2 & 22 & 9 & 17 & 78 & 83 & 169 & 419 \\
&         & 13 & 43 & 22 & 32 & 121 & 151 & 277 & 426 & 11 & 30 & 23 & 23 & 25 & 51 & 140 & 3566 & 7  & 23 & 18 & 24 & 33  & 59  & 77  & 1050 \\
&         & 10 & 35 & 20 & 25 & 59  & 122 & 305 & 491 & 10 & 29 & 19 & 20 & 22 & 45 & 136 & 4145 & 6  & 22 & 13 & 21 & 37  & 102 & 174 & 853  \\
&  & 21 & 63 & 56 & 78 & 100 & 122 & 305 & 477 & 22 & 61 & 58 & 58 & 65 & 78 & 204 & 4242 & 17 & 70 & 70 & 91 & 113 & 116 & 184 & 1281 \\
&         & 17 & 55 & 59 & 71 & 86  & 92  & 203 & 240 & 16 & 51 & 51 & 53 & 57 & 59 & 124 & 4283 & 15 & 56 & 61 & 80 & 88  & 93  & 108 & 1146 \\
&         & 15 & 69 & 68 & 88 & 113 & 135 & 233 & 476 & 16 & 60 & 59 & 61 & 67 & 97 & 201 & 4285 & 15 & 66 & 66 & 85 & 105 & 110 & 202 & 1241 \\
\midrule
\multirow{6}{*}{} &             & 0.03 & 0.11 & 0.06 & 0.08 & 0.33 & 0.36 & 0.50 & 0.59 & 0.03 & 0.06 & 0.06 & 0.06 & 0.06 & 0.11 & 0.34 & 2.18 & 0.01 & 0.12 & 0.04 & 0.09 & 0.27 & 0.50 & 1.75 & 2.02 \\
&             & 0.05 & 0.15 & 0.08     & 0.10     & 0.39     & 0.46     & 0.61     & 0.71 & 0.05 & 0.10 & 0.10     & 0.10     & 0.10     & 0.17     & 0.43     & 0.82 & 0.03 & 0.10 & 0.08     & 0.09     & 0.15     & 0.35     & 0.47     & 0.77 \\
&             & 0.05 & 0.12 & 0.08     & 0.09     & 0.20     & 0.40     & 0.69     & 0.82 & 0.04 & 0.09 & 0.08     & 0.08     & 0.10     & 0.15     & 0.46     & 0.94 & 0.03 & 0.11 & 0.05     & 0.10     & 0.20     & 0.51     & 0.63     & 0.75 \\
&            & 0.09 & 0.23 & 0.23     & 0.26     & 0.33     & 0.39     & 0.70     & 0.80 & 0.09 & 0.22 & 0.24     & 0.24     & 0.29     & 0.29     & 0.61     & 0.97 & 0.09 & 0.30 & 0.29     & 0.36     & 0.43     & 0.59     & 0.69     & 0.94 \\
&          & 0.07 & 0.20 & 0.23     & 0.28     & 0.30     & 0.33     & 0.42     & 0.58 & 0.06 & 0.18 & 0.21     & 0.22     & 0.25     & 0.26     & 0.37     & 0.97 & 0.04 & 0.24 & 0.25     & 0.31     & 0.36     & 0.42     & 0.48     & 0.84 \\
&  & 0.07 & 0.25 & 0.28 & 0.31 & 0.40 & 0.45 & 0.63 & 0.79 & 0.06 & 0.21 & 0.25 & 0.25 & 0.26 & 0.35 & 0.61 & 0.97 & 0.06 & 0.28 & 0.27 & 0.32 & 0.43 & 0.63 & 0.64 & 0.91 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Discussion}

\subsubsection{Architecture comparison}
The models with the attention mechanisms ( and ) underperformed and are not shown in Table~\ref{tab:results}; the -based models are the winners. The poor performance may suggest that the attention mechanisms, which work well for natural language translation, are less suited for our problem.

\subsubsection{Hyperparameters}
Table~\ref{tab:results} shows that the models with 512 cells prevailed, which may imply that we do need some degree of freedom to learn the patterns but not too much to overwhelm the optimizer.

The dropout rate  is the best in all cases, except for models trained on . In this particular case, a dropout of  and  provide similar results. We expected the dropout to be positive as our goal is to generalize the learning process, and these expectations were met. 


\subsubsection{Dataset size}
Comparison of results for  and  in Table~\ref{tab:results} suggests that the larger number of observations improves performances of the model. However,  we can observe the diminishing returns: increasing the dataset size by a factor of five (as in the case of  and ) leads to incremental improvements in the  and  values. 

Moreover, compare the results for models trained on  and : the model trained on 20K of simpler observations from  performs better than the model trained on 100K harder records from . This implies that the quality of the input matters more than the quantity. 


\subsubsection{Training time}
We used Nvidia Tesla P100 and Titan X Pascal GPUs for the training. The training time per epoch (for datasets with 100K log records) ranged between one and three hours, depending on the models and inputs. Thus, training the model for hundreds of epochs is time-consuming. We observed that the validation loss would often reach the lowest (best) values between 60 and 150 epochs during models' training, which enabled us to perform early stopping and save time.

\subsubsection{Training datasets vs. models' performance} As expected, the resulting models' performance degrades as the difficulty of the training dataset increases. The -based model came up first (with the exception of a few high quantiles): it had to learn only the fields' beginning and end while the fields' order stayed the same. The -based model came last: not only did the model have to understand the beginning and ends of the fields, but also it had to learn the field's order while never been able to see a single example of the ELF record. To our pleasant surprise, the -based model did not fall behind their competitors significantly and was quite close to the -based model, with the median  between  and  , and the 90-th quantile of  between  and  . This implies that the -based model was able to learn the concept of fields relatively well.



\section{Summary} \label{sec:summary}
Our RQ was: \textit{How can we automatically parse an individual raw log string?} To answer the RQ, we reformulated parsing as an MT task and explored three MT architectures. We showed that the models based on the classic LSTM MT architecture are the most promising. 
Moreover, the model trained on randomly generated log records was able to partially parse the log records in a format that it did not see in training: 50\% of log records per evaluation dataset were parsed with the  and 90\% of records with the .  This implies that parsing of the individual strings using MT is possible.

This work is of interest to researchers as it provides novel insights into the log parsing field. The quality of our models’ translation is not adequate for the practitioner at this stage, but we hope that this work will inspire others to explore various MT approaches and improve upon our results. 

To aid these researchers, we created a tool for generating synthetic logs in Apache format used for creating training data for the models. We also found and parsed three real-world validation logs for models’ evaluation. 

\section{Data Availability}\label{sec:access}
The tool source code repository is~\cite{dat:github}. The version of the tool used in this paper is stored at~\cite{code:zenodo}. The training and validation datasets,  and , are available at~\cite{dat:zenodo}.
\bibliographystyle{IEEEtran}
\bibliography{references} 

\end{document}
