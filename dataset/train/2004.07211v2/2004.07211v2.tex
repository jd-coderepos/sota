\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}






\usepackage[final]{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{soul}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsbsy}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{enumitem, amssymb}
\usepackage{pifont}
\usepackage{setspace}
\usepackage[normalem]{ulem}
\usepackage{comment}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{cellspace}
\usepackage{marvosym}
\usepackage{bigstrut}
\setlength\bigstrutjot{3pt}
\usepackage{float}
\usepackage{epigraph}

\usepackage{twoopt}
\usepackage{tikz}
\usetikzlibrary{fit}



\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argsoftmax}{\operatornamewithlimits{softmax}}
\newcommand{\KL}{\operatornamewithlimits{\textit{D}_{\textit{KL}}}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\textbf{\textendash}}
\newcommand{\tmark}{}
\newcommand{\dmark}{}

\usepackage[first=0, last=9]{lcg}
\newcommand{\random}{\rand\arabic{rand}}
\newcommand{\fkrs}{\textcolor{blue}{\tiny{0.\random\random}}}
\newcommand{\tmprs}[1]{\textcolor{magenta}{#1}}

\newcommand*\ruleline[1]{\par\noindent\raisebox{.8ex}{\makebox[\linewidth]{\hrulefill\hspace{1ex}\raisebox{-.8ex}{#1}\hspace{1ex}\hrulefill}}}

\newcommand{\quotationmarks}[1]{``#1''}
 


\title{Dark Experience for General Continual Learning:\\a Strong, Simple Baseline}

\author{
	\hspace{-0.34cm} Pietro Buzzega \hspace{0.35cm} Matteo Boschini \hspace{0.35cm} Angelo Porrello \hspace{0.35cm} Davide Abati \hspace{0.35cm} Simone Calderara\\
	\vspace{0.2cm} \\
	AImageLab - University of Modena and Reggio Emilia, Modena, Italy \\
	\texttt{name.surname@unimore.it} \\
}

\begin{document}
\maketitle
\begin{abstract}
Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards \textit{General Continual Learning} (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, \textit{Dark Experience Replay}, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance.\\Code is available at \href{https://github.com/aimagelab/mammoth}{https://github.com/aimagelab/mammoth}.
\end{abstract} \section{Introduction} \label{sec:introduction}
Practical applications of neural networks may require to go beyond the classical setting where all data are available at once: when new classes or tasks emerge, such models should acquire new knowledge on-the-fly, incorporating it with the current one. However, if the learning focuses on the current set of examples solely, a sudden performance deterioration occurs on the old data, referred to as catastrophic forgetting~\cite{mccloskey1989catastrophic}. As a trivial workaround, one could store all incoming examples and re-train from scratch when needed, but this is impracticable in terms of required resources. Continual Learning (CL) methods aim at training a neural network from a stream of non i.i.d.\ samples, relieving catastrophic forgetting while limiting computational costs and memory footprint~\cite{rebuffi2017icarl}.

It is not always easy to have a clear picture of the merits of these works: due to subtle differences in the way methods are evaluated, many state-of-the-art approaches only stand out in the setting where they were originally conceived. Several recent papers~\cite{de2019continual, farquhar2018towards, hsu2018re, van2019three} address this issue and conduct a critical review of existing evaluation settings, leading to the formalization of three main experimental settings~\cite{hsu2018re, van2019three}. By conducting an extensive comparison on them, we surprisingly observe that a simple Experience Replay baseline (\textit{i.e.}\ interleaving old examples with ones from the current task) consistently outperforms cutting-edge methods in the considered settings.

Also, the majority of the compared methods are unsuited for real-world applications, where memory is bounded and tasks intertwine and overlap. Recently,~\cite{de2019continual}
introduced a series of guidelines that CL methods should realize to be applicable in practice:
i) \textbf{no task boundaries}: do not rely on boundaries between tasks during training;
ii) \textbf{no test time oracle}: do not require task identifiers at inference time;
iii) \textbf{constant memory}: have a bounded memory footprint throughout the entire training phase. 

These requirements outline the General Continual Learning (GCL), of which Continual Learning is a relaxation. As reported in Table~\ref{tab:gcl_requirements}, ER also stands out being one of the few methods that are fully compliant with GCL. MER~\cite{riemer2018learning} and GSS~\cite{aljundi2019gradient} fulfill the requirements as well, but they suffer from a very long running time which hinders their applicability to non-trivial datasets.
\begin{table}[t]
\setlength{\tabcolsep}{2.5pt}
    \centering
\resizebox{\textwidth}{!}{\begin{tabular}{ccccccccccccccccc}  
\toprule
\multirow{2}{*}{\textbf{Methods}} & PNN & PackNet & HAT & ER & MER & GSS & GEM & A-GEM & HAL & iCaRL & FDR & LwF & SI & oEWC & \textbf{DER} & \textbf{DER++}\\
& \cite{rusu2016progressive}&\cite{mallya2018packnet}&\cite{serra2018overcoming}&\cite{ratcliff1990connectionist, riemer2018learning}&\cite{riemer2018learning}&\cite{aljundi2019gradient}&\cite{lopez2017gradient}&\cite{chaudhry2018efficient}&\cite{chaudhry2020using}&\cite{rebuffi2017icarl}&\cite{benjamin2018measuring}&\cite{li2017learning}&\cite{zenke2017continual}&\cite{kirkpatrick2017overcoming}&\textbf{(ours)}&\textbf{(ours)}\\
\midrule
\textbf{Constant} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} \\
\textbf{memory} & & & & & & & & & & & & & & & & \\ 
\midrule
\textbf{No task} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} \\
\textbf{boundaries} & & & & & & & & & & & & & & & & \\ 
\midrule
\textbf{No test} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark}\\
\textbf{time oracle} & & & & & & & & & & & & & & & & \\ 
\bottomrule
\end{tabular}}
\caption{Continual learning approaches and their compatibility with the General Continual Learning major requirements~\cite{de2019continual}. For an exhaustive discussion, please refer to supplementary materials.}
    \vspace{-1.3em}
    \label{tab:gcl_requirements}
\end{table}

In this work, we propose a novel CL baseline that improves on ER while maintaining a very simple formulation. We call it \textbf{Dark Experience Replay (DER)} as it relies on \textit{dark knowledge}~\cite{hinton2014dark} for distilling past \textit{experiences}, sampled over the entire training trajectory. Our proposal satisfies the GCL guidelines and outperforms the current state-of-the-art approaches in the standard CL experiments we conduct. With respect to ER, we empirically show that our baseline exhibits remarkable qualities: it converges to flatter minima, achieves better model calibration at the cost of a limited memory and training time overhead. Eventually, we propose a novel GCL setting (MNIST-360); it displays MNIST digits sequentially and subject to a smooth increasing rotation, thus generating both sudden and gradual changes in their distribution. By evaluating the few GCL-compatible methods on MNIST-360, we show that DER also qualifies as a state-of-the-art baseline for future studies on this setting. \section{Related Work} \label{sec:related}
\textit{\textbf{Rehearsal-based methods}} tackle catastrophic forgetting by replaying a subset of the training data stored in a memory buffer. Early works~\cite{ratcliff1990connectionist,robins1995catastrophic} proposed \textbf{Experience Replay (ER)}, that is interleaving old samples with current data in training batches. Several recent studies directly expand on this idea: \textbf{Meta-Experience Replay (MER)}~\cite{riemer2018learning} casts replay as a meta-learning problem to maximize transfer from past tasks while minimizing interference; \textbf{Gradient based Sample Selection (GSS)}~\cite{aljundi2019gradient} introduces a variation on ER to store optimally chosen examples in the memory buffer; \textbf{Hindsight Anchor Learning (HAL)}~\cite{chaudhry2020using} complements replay with an additional objective to limit forgetting on pivotal learned data-points. On the other hand, \textbf{Gradient Episodic Memory (GEM)}~\cite{lopez2017gradient} and its lightweight counterpart \textbf{Averaged-GEM (A-GEM)}~\cite{chaudhry2018efficient} leverage old training data to build optimization constraints to be satisfied by the current update step. These works show improvements over ER when confining the learning to a small portion of the training set (\textit{e.g.}, 1k examples per task). However, we believe that this setting rewards \textit{sample efficiency} -- \textit{i.e.}, making good use of the few shown examples -- which represents a potential confounding factor for assessing catastrophic forgetting. Indeed, Section~\ref{sec:experiments} reveals that the above-mentioned approaches are not consistently superior to ER when lifting these restrictions, which motivates our research in this kind of methods.

\textit{\textbf{Knowledge Distillation}}. Several approaches exploit Knowledge Distillation~\cite{hinton2015distilling} to mitigate forgetting by appointing a past version of the model as a teacher. \textbf{Learning Without Forgetting (LwF)}~\cite{li2017learning} computes a smoothed version of the current responses for the new examples at the beginning of each task, minimizing their drift during training. A combination of replay and distillation can be found in \textbf{iCaRL}~\cite{rebuffi2017icarl}, which employs a buffer as a training set for a \textit{nearest-mean-of-exemplars} classifier while preventing the representation from deteriorating in later tasks via a self-distillation loss term. 

\textit{\textbf{Other Approaches.}} Regularization-based methods extend the loss function with a term that prevents network weights from changing, as done by \textbf{Elastic Weight Consolidation (EWC)}~\cite{kirkpatrick2017overcoming}, \textbf{online EWC (oEWC)}~\cite{schwarz2018progress}, \textbf{Synaptic Intelligence (SI)}~\cite{zenke2017continual} and \textbf{Riemmanian Walk (RW)}~\cite{chaudhry2018riemannian}. Architectural methods, on the other hand, devote distinguished sets of parameters to distinct tasks. Among these, \textbf{Progressive Neural Networks (PNN)}~\cite{rusu2016progressive} instantiates new networks incrementally as novel tasks occur, resulting in a linearly growing memory requirement. To mitigate this issue, \textbf{PackNet}~\cite{mallya2018packnet} and \textbf{Hard Attention to the Task (HAT)}~\cite{serra2018overcoming} share the same architecture for subsequent tasks, employing a heuristic strategy to prevent intransigence by allocating additional units when needed. \section{Dark Experience Replay}
\label{sec:model}
Formally, a CL classification problem is split in  tasks; during each task  input samples  and their corresponding ground truth labels  are drawn from an i.i.d.\ distribution . A function , with parameters , is optimized on one task at a time in a sequential manner. We indicate the output logits with  and the corresponding probability distribution over the classes with . The goal is to learn how to correctly classify, at any given point in training, examples from any of the observed tasks up to the current one :

This is especially challenging as data from previous tasks are assumed to be unavailable, meaning that the best configuration of  w.r.t.\  must be sought without  for . Ideally, we look for parameters that fit the current task well while approximating the behavior observed in the old ones: effectively, we encourage the network to mimic its original responses for past samples. To preserve the knowledge about previous tasks, we seek to minimize the following objective:

where  is the optimal set of parameters at the end of task , and  is a hyper-parameter balancing the trade-off between the terms. This objective, which resembles the teacher-student approach, would require the availability of  for previous tasks. To overcome such a limitation, we introduce a replay buffer  holding past \textit{experiences} for task . Differently from other rehearsal-based methods~\cite{aljundi2019gradient, chaudhry2020using, riemer2018learning}, we retain the network's logits , instead of the ground truth labels .

As we focus on General Continual Learning, we intentionally avoid relying on task boundaries to populate the buffer as the training progresses. Therefore, in place of the common task-stratified sampling strategy, we adopt \textit{reservoir} sampling~\cite{vitter1985random}: this way, we select  random samples from the input stream, guaranteeing that they have the same probability  of being stored in the buffer, without knowing the length of the stream  in advance. We can rewrite Eq.~\ref{eq:obj_3} as follows:

Such a strategy implies picking logits  during the optimization trajectory, so potentially different from the ones that can be observed at the task's local optimum.
Even if counter-intuitive, we empirically observed that this strategy does not hurt performance, while still being suitable without task boundaries. Furthermore, we observe that the replay of sub-optimal logits has beneficial effects in terms of flatness of the attained minima and calibration (see Section~\ref{sec:ablation}). 


Under mild assumptions~\cite{hinton2015distilling}, the optimization of the KL divergence in Eq.~\ref{eq:obj_4} is equivalent to minimizing the Euclidean distance between the corresponding pre-softmax responses (\textit{i.e.} logits). In this work we opt for matching logits, as it avoids the information loss occurring in probability space due to the squashing function (e.g., softmax)~\cite{liu2018improving}. With these considerations in hands, Dark Experience Replay (DER, algorithm~\ref{alg:der}) optimizes the following objective:

We approximate the expectation by computing gradients on batches sampled from the replay buffer.

\textbf{\textit{Dark Experience Replay++.}}~It is worth noting that the \textit{reservoir} strategy may weaken DER under some specific circumstances. Namely, when a sudden distribution shift occurs in the input stream, logits that are highly biased by the training on previous tasks might be sampled for later replay: leveraging the ground truth labels as well -- as done by ER -- could mitigate such a shortcoming. On these grounds, we also propose \textbf{Dark Experience Replay++ (DER++, algorithm~\ref{alg:derpp})}, which equips the objective of Eq.~\ref{eq:obj_5} with an additional term on buffer datapoints, promoting higher conditional likelihood w.r.t.\ their ground truth labels with a minimal memory overhead:

where  is an additional coefficient balancing the last term\footnote{The model is not overly sensitive to  and : setting them both to  yields stable performance.} (DER++ collapses to DER when ).
\begin{figure}
\centering
\begin{minipage}[t]{0.485\textwidth}
\begin{algorithm}[H]
\small
\caption{- Dark Experience Replay}
\label{alg:der}
\begin{algorithmic}
  \STATE {\bfseries Input:} dataset , parameters , scalar ,
  \STATE \hspace{3.1em}learning rate 
  \vspace{0.55em}
  \STATE 
      \FOR{\texttt{} \textbf{in}
          \texttt{}}
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
      \ENDFOR
    \vspace{0.55em}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.485\textwidth}
\begin{algorithm}[H]
\small
\caption{- Dark Experience Replay ++}
\label{alg:derpp}
\begin{algorithmic}
  \STATE {\bfseries Input:} dataset , parameters , scalars  and ,
  \STATE \hspace{3.1em}learning rate 
  \STATE 
      \FOR{\texttt{} \textbf{in}
          \texttt{}}
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
          \STATE 
      \ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}

\subsection{Relation with previous works}
While both our proposal and LWF~\cite{li2017learning} leverage knowledge distillation in Continual Learning, they adopt remarkably different approaches. The latter does not replay past examples, so it only encourages the similarity between teacher and student responses w.r.t.\ to data points of the current task. Alternatively, iCaRL~\cite{rebuffi2017icarl} distills knowledge for past outputs w.r.t.\ past exemplars, which is more akin to our proposal.
However, the former exploits the network appointed at the end of each task as the sole teaching signal. On the contrary, our methods store logits sampled throughout the optimization trajectory, which resembles having several different teacher parametrizations.

A close proposal to ours is given by \textbf{Function Distance Regularization (FDR)} for combatting catastrophic forgetting (Sec.~3.1 of~\cite{benjamin2018measuring}). Like FDR, we use past exemplars and network outputs to align past and current outputs. However, similarly to the iCaRL discussion above, FDR stores network responses at task boundaries and thus cannot be employed in a GCL setting. Instead, the experimental analysis we present in Sec.~\ref{sec:ablation} reveals that the need of task boundaries can be relaxed through \textit{reservoir} without experiencing a drop in performance; on the contrary we empirically observe that DER and DER++ achieve significantly superior results and remarkable properties. We finally highlight that the motivation behind~\cite{benjamin2018measuring} lies chiefly in studying how the training trajectory of NNs can be characterized in a functional  Hilbert space, whereas the potential of function-space regularization for Continual Learning problems is only coarsely addressed with a single experiment on MNIST. In this respect, we present extensive experiments on multiple CL settings as well as a detailed analysis (Sec.~\ref{sec:ablation}) providing a deeper understanding on the effectiveness of this kind of regularization.
 \section{Experiments}
\label{sec:experiments}
We adhere to~\cite{hsu2018re,van2019three} and model the sequence of tasks according to the following three settings: 

\textbf{Task Incremental Learning (Task-IL)} and \textbf{Class Incremental Learning (Class-IL)} split the training samples into partitions of classes (tasks). Although similar, the former provides task identities to select the relevant classifier for each example, whereas the latter does not; this difference makes Task-IL and Class-IL the easiest and hardest scenarios among the three~\cite{van2019three}. In practice, we follow~\cite{de2019continual, zenke2017continual} by splitting CIFAR-10~\cite{krizhevsky2009learning} and Tiny ImageNet~\cite{tinyimgnet} in  and  tasks, each of which introduces  and  classes respectively. We show all the classes in the same fixed order across different runs. 

\textbf{Domain Incremental Learning (Domain-IL)} feeds all classes to the network during each task, but applies a task-dependent transformation to the input; task identities remain unknown at test time. For this setting, we leverage two common protocols built upon the MNIST dataset~\cite{lecun1998gradient}, namely \textbf{Permuted MNIST}~\cite{kirkpatrick2017overcoming} and \textbf{Rotated MNIST}~\cite{lopez2017gradient}. They both require the learner to classify all MNIST digits for  subsequent tasks, but the former applies a random permutation to the pixels, whereas the latter rotates the images by a random angle in the interval .

As done in previous works~\cite{farquhar2018towards,rebuffi2017icarl,van2019three,wu2019large}, we provide task boundaries to the competitors demanding them at training time (\textit{e.g.}\ oEWC or LwF). This choice is meant to ensure a fair comparison between our proposal -- which does not need boundaries -- and a broader class of methods in literature.
\subsection{Evaluation Protocol}
\label{subsec:cl_eval_prot}
\textit{\textbf{Architecture.}}~For tests we conducted on variants of the MNIST dataset, we follow~\cite{lopez2017gradient, riemer2018learning} by employing a fully-connected network with two hidden layers, each one comprising of  ReLU units. For CIFAR-10 and Tiny ImageNet, we follow~\cite{rebuffi2017icarl} and rely on ResNet18~\cite{he2016deep} (not pre-trained). 

\textit{\textbf{Augmentation.}}~For CIFAR-10 and Tiny ImageNet, we apply random crops and horizontal flips to both stream and buffer examples. We propagate this choice to competitors for fairness. It is worth noting that combining data augmentation with our regularization objective enforces an implicit consistency loss~\cite{bachman2014learning, becker1992self}, which aligns predictions for the same example subjected to small data transformations.

\textit{\textbf{Hyperparameter selection.}}~We select hyperparameters by performing a grid-search on a validation set, the latter obtained by sampling  of the training set. For the Domain-IL scenario, we make use of the final average accuracy as the selection criterion. Differently, we perform a combined grid-search for Class-IL and Task-IL, choosing the configuration that achieves the highest final accuracy averaged on the two settings. Please refer to the supplementary materials for a detailed characterization of the hyperparameter grids we explored along with the chosen configurations.

\textit{\textbf{Training.}}~To provide a fair comparison among CL methods, we train all the networks using the Stochastic Gradient Descent (SGD) optimizer. Despite being interested in an online scenario, with no additional passages on the data, we reckon it is necessary to set the number of epochs per task in relation to the dataset complexity. Indeed, if even the pure-SGD baseline fails at fitting a single task with adequate accuracy, we could not properly disentangle the effects of catastrophic forgetting from those linked to underfitting --- we refer the reader to the supplementary material for an experimental discussion regarding this issue. For MNIST-based settings, one epoch per task is sufficient. Conversely, we increase the number of epochs to  for Sequential CIFAR-10 and  for Sequential Tiny ImageNet respectively, as commonly done by works that test on harder datasets~\cite{rebuffi2017icarl, wu2019large, zenke2017continual}. We deliberately hold batch size and minibatch size out from the hyperparameter space, thus avoiding the flaw of a variable number of update steps for different methods.
\subsection{Experimental Results}
\label{subsec:cl_exps}
\begin{table*}[t]
\centering
{
\setlength{\tabcolsep}{2.0pt}
\small
\begin{tabular}{clcccccccc}
\toprule
 \multirow{2}{*}{\textbf{Buffer}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{S-CIFAR-10}} & \multicolumn{2}{c}{\textbf{S-Tiny-ImageNet}} & \textbf{P-MNIST} & \textbf{R-MNIST}\\
\addlinespace[0.35ex]
 & & \textit{Class-IL} & \textit{Task-IL} & \textit{Class-IL} & \textit{Task-IL} & \textit{Domain-IL} & \textit{Domain-IL}\\
\midrule
\multirow{2}{*}{\xmark} & JOINT          & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
& SGD             & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
     \midrule
     

     & oEWC~\cite{schwarz2018progress}             & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
\multirow{2}{*}{\xmark} & SI~\cite{zenke2017continual}               & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & LwF~\cite{li2017learning}              & \tiny{} & \tiny{} & \tiny{} & \tiny{} & - & - \\
     & PNN~\cite{rusu2016progressive}              & - & \tiny{} & - & \tiny{} & - & - \\
\midrule
    & ER~\cite{riemer2018learning} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & GEM~\cite{lopez2017gradient} & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & A-GEM~\cite{chaudhry2018efficient} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & iCaRL~\cite{rebuffi2017icarl} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & -     & -     \\
200 & FDR~\cite{benjamin2018measuring}         & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & GSS~\cite{aljundi2019gradient}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & HAL~\cite{chaudhry2020using}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & \textbf{DER (ours)} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & \textbf{DER++ (ours)} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
\midrule
    & ER~\cite{riemer2018learning} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & GEM~\cite{lopez2017gradient} & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & A-GEM~\cite{chaudhry2018efficient} & \tiny{}  & \tiny{}  & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & iCaRL~\cite{rebuffi2017icarl} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & -     & -     \\
500 & FDR~\cite{benjamin2018measuring}          & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & GSS~\cite{aljundi2019gradient}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & HAL~\cite{chaudhry2020using}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
    & \textbf{DER (ours)} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
    & \textbf{DER++ (ours)} & \tiny{} & \tiny{} &  \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
\midrule
     & ER~\cite{riemer2018learning} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
     & GEM~\cite{lopez2017gradient} & \tiny{} &  \tiny{} & - & - & \tiny{} & \tiny{}\\
     & A-GEM~\cite{chaudhry2018efficient} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\              
     & iCaRL~\cite{rebuffi2017icarl} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & -     & -     \\
5120 & FDR~\cite{benjamin2018measuring}          & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
     & GSS~\cite{aljundi2019gradient}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
     & HAL~\cite{chaudhry2020using}          & \tiny{} & \tiny{} & - & - & \tiny{} & \tiny{} \\
     & \textbf{DER (ours)} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
     & \textbf{DER++ (ours)} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} & \tiny{} \\
\bottomrule
\end{tabular}
}
\caption{Classification results for standard CL benchmarks, averaged across  runs. `-' indicates experiments we were unable to run, because of compatibility issues (\textit{e.g.}\ between PNN, iCaRL and LwF in Domain-IL) or intractable training time (\textit{e.g.}\ GEM, HAL or GSS on Tiny ImageNet).}
\label{tab:cl_acc}
\end{table*} In this section, we compare DER and DER++ against two regularization-based methods (\textit{oEWC}, \textit{SI}), two methods leveraging Knowledge Distillation (\textit{iCaRL}, \textit{LwF}\footnote{In Class-IL, we adopted a multi-class implementation as done in~\cite{rebuffi2017icarl}.}), one architectural method (\textit{PNN}) and six rehearsal-based methods (\textit{ER}, \textit{GEM}, \textit{A-GEM}, \textit{GSS}, \textit{FDR}~\cite{benjamin2018measuring}, \textit{HAL})\footnote{We omit MER as we experienced an intractable training time on these benchmarks (e.g. while DER takes approximately  hours on Seq. CIFAR-10, MER takes  hours -- see Sec.~\ref{sec:ablation} for further comparisons). }
We further provide a lower bound, consisting of \textit{SGD} without any countermeasure to forgetting and an upper bound given by training all tasks jointly (JOINT). Table~\ref{tab:cl_acc} reports performance in terms of average accuracy at the end of all tasks (we refer the reader to supplementary materials for other metrics as forward and backward transfer~\cite{lopez2017gradient}). Results are averaged across ten runs, each one involving a different initialization. 

DER and DER++ achieve state-of-the-art performance in almost all settings. When compared to oEWC and SI, the gap appears unbridgeable, suggesting that regularization towards old sets of parameters does not suffice to prevent forgetting. We argue that this is due to local information modeling weights importance: as it is computed in earlier tasks, it could become untrustworthy in later ones. While being computationally more efficient, LWF performs worse than SI and oEWC on average. PNN, which achieves the strongest results among non-rehearsal methods, attains lower accuracy than replay-based ones despite its memory footprint being much higher at any buffer size.

When compared to rehearsal methods, DER and DER++ show strong performance in the majority of benchmarks, especially in the Domain-IL scenario. For these problems, a shift occurs within the input domain, but not within the classes: hence, the relations among them also likely persist. As an example, if it is true that during the first task number 2's visually look like 3's, this still holds when applying rotations or permutations, as it is done in the following tasks. We argue that leveraging soft-targets in place of hard ones (ER) carries more valuable information~\cite{hinton2015distilling}, exploited by DER and DER++ to preserve the similarity structure through the data-stream. Additionally, we observe that methods resorting to gradients (GEM, A-GEM, GSS) seem to be less effective in this setting.

The gap in performance we observe in Domain-IL is also found in the Class-IL setting, as DER is remarkably capable of learning how classes from different tasks are related to each other. This is not so relevant in Task-IL, where DER performs on par with ER on average. In it, classes only need to be compared in exclusive subsets, and maintaining an overall vision is not especially rewarding. In such a scenario, DER++ manages to effectively combine the strengths of both methods, resulting in generally better accuracy. Interestingly, iCaRL appears valid when using a small buffer; we believe that this is due to its helpful \textit{herding} strategy, ensuring that all classes are equally represented in memory. As a side note, other ER-based methods (HAL and GSS) show weaker results than ER itself on such challenging datasets.
\subsection{MNIST-360}
\label{subsec:gcl_exp_setup}
To address the General Continual Learning desiderata, we propose a novel protocol: MNIST-360. It models a stream of data presenting batches of two consecutive MNIST digits at a time (\textit{e.g.} , ,  etc.), as depicted in Fig.~\ref{fig:mnist360}. We rotate each example of the stream by an increasing angle and, after a fixed number of steps, switch the lesser of the two digits with the following one. As it is impossible to distinguish 6's and 9's upon rotation, we do not use 9's in MNIST-360. The stream visits the nine possible couples of classes three times, allowing the model to leverage positive transfer when revisiting a previous task. In the implementation, we guarantee that: i) each example is shown once during the overall training; ii) two digits of the same class are never observed under the same rotation. We provide a detailed description of training and test sets in supplementary materials.

\begin{figure*}[t]
    \centering
    {
    \resizebox{\textwidth}{!}{
    \setlength\tabcolsep{5pt}
    \begin{tabular}{ccccccccccc}
        \includegraphics{imgs/mnist360/00.png} & 
        \includegraphics{imgs/mnist360/01.png} & 
        \includegraphics{imgs/mnist360/02.png} & 
        \includegraphics{imgs/mnist360/03.png} & 
        \includegraphics{imgs/mnist360/04.png} & 
        \includegraphics{imgs/mnist360/05.png} & 
        \includegraphics{imgs/mnist360/06.png} & 
        \includegraphics{imgs/mnist360/07.png} & 
        \includegraphics{imgs/mnist360/08.png} &
        \includegraphics{imgs/mnist360/09.png} &
        \includegraphics{imgs/mnist360/10.png} \\
        \includegraphics{imgs/mnist360/11.png} & 
        \includegraphics{imgs/mnist360/12.png} & 
        \includegraphics{imgs/mnist360/13.png} & 
        \includegraphics{imgs/mnist360/14.png} & 
        \includegraphics{imgs/mnist360/15.png} & 
        \includegraphics{imgs/mnist360/16.png} & 
        \includegraphics{imgs/mnist360/17.png} & 
        \includegraphics{imgs/mnist360/18.png} & 
        \includegraphics{imgs/mnist360/19.png} &
        \includegraphics{imgs/mnist360/10.png} &
        \includegraphics{imgs/mnist360/21.png} \\
    \end{tabular}
    }
    }
    \caption{Example batches of the MNIST-360 stream.}
\label{fig:mnist360}
\end{figure*}
\begin{table}[t]
\setlength{\tabcolsep}{5pt}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cc|c|cccccc}  
    \toprule
    JOINT & SGD & \textbf{Buffer} & ER~\cite{riemer2018learning} & MER~\cite{riemer2018learning} & A-GEM-R~\cite{chaudhry2018efficient} & GSS~\cite{aljundi2019gradient} & \textbf{DER (ours)} & \textbf{DER++ (ours)}\\
    \midrule
     & &  &  \tiny{}  & \tiny{}  & \tiny{}  & \tiny{} & \tiny{} & \tiny{}         \\                  
     \tiny{}& \tiny{} & 500  & \tiny{}  & \tiny{}  & \tiny{}  & \tiny{} & \tiny{}                          & \tiny{} \\  
    &  & 1000  & \tiny{}  & \tiny{}  & \tiny{}  & \tiny{} & \tiny{}                          & \tiny{} \\
    \bottomrule
    \end{tabular}
    }
\caption{Accuracy on the test set for MNIST-360.}
\label{tab:gcl_results}
\end{table}

It is worth noting that such a setting presents both sharp (change in class) and smooth (rotation) distribution shifts; therefore, for the algorithms that rely on explicit boundaries, it would be hard to identify them. As outlined in Section~\ref{sec:introduction}, just ER, MER, and GSS are suitable for GCL. However, we also explore a variant of A-GEM equipped with a reservoir memory buffer (A-GEM-R). We compare these approaches with DER and DER++, reporting the results in Table~\ref{tab:gcl_results} (we keep the same fully-connected network we used on MNIST-based datasets). As can be seen, DER and DER++ outperform other approaches in such a challenging scenario, supporting the effectiveness of the proposed baselines against alternative replay methods. Due to space constraints, we refer the reader to supplementary materials for an additional evaluation regarding the memory footprint.
\begin{figure}[t]
\centering
    {
    \setlength\tabcolsep{0pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccc}
        \includegraphics[height=20em]{imgs/other_plots/perturbation_loss.pdf} &
        \includegraphics[height=20em]{imgs/other_plots/hessian_eigs.pdf} & 
        \includegraphics[height=20em]{imgs/other_plots/calibration_tasks.pdf}
        \\
        \includegraphics[height=20em]{imgs/other_plots/calibration_bars.pdf} &
        \includegraphics[height=20em]{imgs/other_plots/fewshot.pdf} &
        \includegraphics[height=20em]{imgs/other_plots/times.pdf} 
         \\
    \end{tabular}
    }
    }
    \caption{Results for the model analysis. [] higher is better, [] lower is better \textit{(best seen in color)}.}
\vspace{-1em}
    \label{fig:plot_ablation}
\end{figure}

\section{Model Analysis}
\label{sec:ablation}
In this section, we provide an in depth analysis of DER and DER++ by comparing them against FDR and ER.
By so doing, we gather insights on the employment of logits sampled throughout the optimization trajectory, as opposed to ones at task boundaries and ground truth labels.

\textbf{\textit{DER converges to flatter minima.}}~Recent studies~\cite{chaudhari2017entropy, jastrzkebski2018three, keskar2017large} link Deep Network generalization to the geometry of the loss function, namely the flatness of the attained minimum. While these works link flat minima to good train-test generalization, here we are interested in examining their weight in Continual Learning. 
Let us suppose that the optimization converges to a sharp minimum w.r.t.\  (Eq.~\ref{eq:obj}): in that case, the tolerance towards local perturbations is quite low. As a side effect, the drift we will observe in parameter space (due to the optimization of  for ) will intuitively lead to an even more serious drop in performance.

On the contrary, reaching a flat minimum for  could give more room for exploring neighbouring regions of the parameter space, where one may find a new optimum for task  without experiencing a severe failure on tasks . We conjecture that the effectiveness of the proposed baseline is linked to its ability to attain flatter and robust minima, which generalizes better to unseen data and, additionally, favors adaptability to incoming tasks. To validate this hypothesis, we compare the flatness of the training minima of FDR, ER, DER and DER++ utilizing two distinct metrics.

Firstly, as done in~\cite{zhang2019your, zhang2018deep}, we consider the model at the end of training and add independent Gaussian noise with growing  to each parameter. This allows us to evaluate its effect on the average loss across all training examples. As shown in Fig.~\ref{fig:plot_ablation}(a) (S-CIFAR-10, buffer size 500), ER and especially FDR reveal higher sensitivity to perturbations than DER and DER++.
Furthermore, \cite{chaudhari2017entropy, jastrzkebski2018three, keskar2017large} propose measuring flatness by evaluating the eigenvalues of : sharper minima correspond to larger Hessian eigenvalues. At the end of training on S-CIFAR-10, we compute the empirical Fisher Information Matrix  w.r.t.\ the whole training set (as an approximation of the intractable Hessian~\cite{chaudhari2017entropy, kirkpatrick2017overcoming}). Fig.~\ref{fig:plot_ablation}(b) reports the sum of its eigenvalues : as one can see, DER and especially DER++ produce the lowest eigenvalues, which translates into flatter minima following our intuitions. It is worth noting that FDR's large  for buffer size  could be linked to its failure case in S-CIFAR-10, Class-IL.

\textbf{\textit{DER converges to more calibrated networks.}}~Calibration is a desirable property for a learner, measuring how much the confidence of its predictions corresponds to its accuracy. Ideally, we expect output distributions whose shapes mirror the probability of being correct, thus quantifying how much one can trust a specific prediction. Recent works find out that modern Deep Networks -- despite largely outperforming the ones from a decade ago -- are less calibrated~\cite{guo2017calibration}, as they tend to yield overconfident predictions~\cite{kull2019beyond}. In real-world applications, AI tools should support decisions in a continuous and online fashion (\textit{e.g.}\ weather forecasting~\cite{brocker2009reliability} or econometric analysis~\cite{gneiting2007probabilistic}); therefore, calibration represents an appealing property for any CL system aiming for employment outside of a laboratory environment.  

Fig.~\ref{fig:plot_ablation} shows, for TinyImageNet, the value of the Expected Calibration Error (ECE)~\cite{naeini2015obtaining} during the training and the reliability diagram at the end of it respectively.
It can be seen that DER and DER++ achieve a lower ECE than ER and FDR without further application of \textit{a posteriori} calibration methods (\textit{e.g.}, Temperature Scaling, Dirichlet Calibration, ...). This means that models trained using Dark Experience are less overconfident and, therefore, easier to interpret. As a final remark, \textit{Liu et al.}\ link this property to the capability to generalize to novel classes in a zero-shot scenario~\cite{liu2018generalized}, which could translate into an advantageous starting point for the subsequent tasks for DER and DER++.

\textbf{\textit{On the informativeness of DER's buffer.}}~Network responses provide a rich description of the corresponding data point. Following this intuition, we posit that the merits of DER also result from the knowledge inherent in its memory buffer: when compared to the one built by ER, the former represents a more informative summary of the overall (full) CL problem. If that were the case, a new learner trained only on the buffer would yield an accuracy that is closer to the one given by jointly training on all data. To validate this idea, we train a network from scratch using the memory buffer as the training set: we can hence compare how memories produced by DER, ER, and FDR summarize well the underlying distribution. Fig.~\ref{fig:plot_ablation}(e) shows the accuracy on the test set: as can be seen, DER delivers the highest performance, surpassing ER, and FDR. This is particularly evident for smaller buffer sizes, indicating that DER's buffer should be especially preferred in scenarios with severe memory constraints.

Further than its pure performance, we assess whether a model trained on the buffer can be specialized to an already seen task: this would be the case of new examples from an old distribution becoming available on the stream. We simulate it by sampling  samples per class from the test set and then fine-tuning on them with no regularization; Fig.~\ref{fig:plot_ablation} reports the average accuracy on the remainder of the test set of each task: here too, DER's buffer yields better performance than ER and FDR, thus providing additional insight regarding its representation capabilities.

\textbf{\textit{On training time.}}~When facing up with a data-stream, one often cares about reducing the overall processing time: otherwise, training would not keep up with the rate at which data are made available to the stream. In this regard, we assess the performance of both DER and DER++ and other rehearsal methods in terms of wall-clock time (seconds) at the end of the last task. To guarantee a fair comparison, we conduct all tests under the same conditions, running each benchmark on a Desktop Computer equipped with an NVIDIA Titan X GPU and an Intel i7-6850K CPU. Fig.~\ref{fig:plot_ablation}(f) reports the execution time we measured on S-CIFAR10, indicating the time necessary for each of  tasks. We draw the following remarks: i) DER has a comparable running time w.r.t.\ other replay methods such as ER, FDR, and A-GEM; ii) the time complexity for GEM grows linearly w.r.t.\ the number of previously seen tasks;  iii) GSS is extremely slow ( examples per second on average, while DER++ processes  examples per second), making it hardly viable in practical scenarios. \section{Conclusions}
In this paper, we introduce Dark Experience Replay: a simple baseline for Continual Learning, which leverages Knowledge Distillation for retaining past experience and therefore avoiding catastrophic forgetting. We show the effectiveness of our proposal through an extensive experimental analysis, carried out on top of standard benchmarks. Also, we argue that the recently formalized General Continual Learning provides the foundation for advances in diverse applications; for this reason, we propose MNIST-360 as an experimental protocol for this setting. We recommend DER as a starting point for future studies on both CL and GCL in light of its strong results on all evaluated settings and of the properties observed in Sec.~\ref{sec:ablation}. \section*{Broader Impact}

We hope that this work will prove useful to the Continual Learning (CL) scientific community as it is fully reproducible and includes:
\begin{itemize}
    \item a clear and extensive comparison of the state of the art on multiple datasets;
    \item Dark Experience Replay (DER), a simple baseline that outperforms all other methods while maintaining a limited memory footprint.
\end{itemize}
As revealed by the analysis in Section~\ref{sec:ablation}, DER also proves to be better calibrated than a simple Experience Replay baseline, which means that it could represent a useful starting point for the study of CL decision-making applications where an overconfident model would be detrimental.

We especially hope that the community will benefit from the introduction of MNIST-360, the first evaluation protocol adhering to the General Continual Learning scenario. The latter has been recently proposed to describe the requirement of a CL system that can be applied to real-world problems. Widespread adoption of our protocol (or new ones of similar design) can close the gap between the current CL studies and practical AI systems. Due to the abstract nature of MNIST-360 (it only contains digits), we believe that ethical and bias concerns are not applicable.

\begin{ack}
This research was funded by the Artificial Intelligence Research and Innovation Center (AIRI) within the University of Modena and Reggio Emilia. We also acknowledge Panasonic
Silicon Valley Lab, NVIDIA Corporation and Facebook Artificial Intelligence Research for their contribution to the hardware infrastructure we used for our experiments.
\end{ack}

\bibliographystyle{plain}
\small {
\bibliography{references}
}

\normalsize
\appendix
\clearpage{}\section {Justification of Table~\ref{tab:gcl_requirements}}
\label{app:justifications}

Below we provide a justification for each mark of Table~\ref{tab:gcl_requirements}:
\subsection{Constant Memory}
    \begin{itemize}
        \item \textit{Distillation methods} need to accommodate a teacher model along with the current learner, at a fixed memory cost. While iCaRL maintains a snapshot of the network as teacher, LWF stores teacher responses to new task data at the beginning of each task.
        \item \textit{Rehearsal methods} need to store a memory buffer of a fixed size. This also affects iCaRL.
        \item \textit{Architectural methods} increase the model size linearly with respect to the number of tasks. In more detail, PackNet and HAT need a Boolean and float mask respectively, while PNN devotes a whole new network to each task.
        \item \textit{Regularization methods} usually require to store up to two parameters sets, thus respecting the constant memory constraint.
    \end{itemize}
\subsection{No Task Boundaries}
\begin{itemize}
        \item \textit{Distillation methods} depend on task boundaries to appoint a new teacher. iCaRL also depends on them to update its memory buffer, in accordance with the \textit{herding} sampling strategy.
        \item \textit{Architectural methods} require to know exactly when the task finishes to update the model. PackNet also re-trains the network at this time.
        \item \textit{Regularization methods} exploit the task change to take a snapshot of the network, using it to constrain drastic changes for the most important weights (oEWC, SI). Online EWC also needs to pass over the whole training set to compute the weights importance.
\item \textit{Rehearsal methods} can operate in the absence of task boundaries if their memory buffer exploits to the \textit{reservoir} sampling strategy. This applies to ER, GSS, MER, DER and DER++ and can easily be extended to A-GEM (by replacing \textit{ring} sampling with \textit{reservoir} as discussed in Sec.~\ref{subsec:gcl_exp_setup}). Other rehearsal approaches, however, rely on boundaries to perform specific steps: HAL hallucinates new anchors that synthesize the task it just completed, whereas FDR needs them to record converged logits to replay.
    \end{itemize}
    GEM does not strictly depend on task boundaries, but rather on task identities to associate every memorized input with its original task (as described in Sec.~ of~\cite{lopez2017gradient}). This is meant to let GEM set up a separate QP for each past task (notice that this is instead unnecessary for A-GEM, which only solves one generic constraint w.r.t.\ the average gradient of all buffer items). We acknowledge that reliance on task boundaries and reliance on task identities are \textit{logically equivalent}: indeed, i) the availability of task identities straightforwardly allows any method to recognize and exploit task boundaries; ii) \textit{vice versa}, by relying on task boundaries and maintaining a task counter, one can easily associate task identities to incoming input points (under the assumption that tasks are always shown in a sequence without repetitions). This explains why Table~\ref{tab:gcl_requirements} indicates that GEM depends on task boundaries. This is also in line with what argued by the authors of~\cite{aljundi2019gradient}.
    
    \subsection{No test time oracle}
\begin{itemize}
        \item \textit{Architectural methods} need to know the task label to modify the model accordingly before they make any prediction.
        \item LWF is designed as a multi-head method, which means that its prediction head must be chosen in accordance with the task label.
        \item \textit{Regularization methods}, \textit{rehearsal methods} and iCaRL can perform inference with no information about the task.
    \end{itemize}
\newpage
\section{Details on the Implementation of MNIST-360}
\label{app:mnist360}

MNIST-360 presents the evaluated method with a sequence of MNIST digits from  to  shown at increasing angles.

\subsection{Training}

For Training purposes, we build batches using exemplars that belong to two consequent classes at a time, meaning that  pairs of classes are possibly encountered: , , , , , , , , and . Each pair is shown in this order in  rounds ( in our experiments) at changing rotations. This means that MNIST-360 consists of  pseudo-tasks, whose boundaries are not signaled to the tested method. We indicate them with  where  is the round number and  are digits forming one of the pairs listed above.

As every MNIST digit  appears in  pseudo-tasks, we randomly split its example images evenly in  groups  where . The set of exemplars that are shown in  is given as , where  is an integer division.

At the beginning of , we initialize two counters  and  to keep track of how many exemplars of  and  are shown respectively. Given batch size  ( in our experiments), each batch is made up of  samples from  and  samples from , where:





This allows us to produce balanced batches, in which the proportion of exemplars of  and  is maintained the same. Pseudo-task  ends when the entirety of  has been shown, which does not necessarily happen after a fixed number of batches.

Each digit  is also associated with a counter  that is never reset during training and is increased every time an exemplar of  is shown to the evaluated method. Before its showing, every exemplar is rotated by 



where  is the number of total examples of digit  in the training set and  is a digit-specific angular offset, whose value for the  digit is given by  (, , , etc.). By so doing, every digit's exemplars are shown with an increasing rotation spanning an entire  angle throughout the entire procedure. Rotation changes within each pseudo-task, resulting into a gradually changing distribution. Fig.~\ref{fig:mnist360} in the main paper shows the first batch of the initial  pseudo-tasks with .

\subsection{Test}

As no task boundaries are provided, evaluation on MNIST-360 can only be carried out after the training is complete. For test purposes, digits are still shown with an increasing rotation as per Eq.~\ref{eq:mnist360_rot}, with  referring to the test-set digit cardinality and no offset applied ().

The order with which digits are shown is irrelevant, therefore no specific batching strategy is necessary and we simply show one digit at a time.

\section{Accuracy vs.\ Memory Occupation}
In Fig.~\ref{fig:performance_vs_memory_suppl}, we show how the accuracy results for the experiments in Section~\ref{subsec:cl_exps} and~\ref{subsec:smnist} relate to the total memory usage of the evaluated methods. We maintain that having a reduced memory footprint is especially important for a CL method. This is usually fairly easy to assess for rehearsal-based methods, as they clearly specify the number of items that must be saved in the memory buffer. While this could lead to the belief that they have higher memory requirements than other classes of solutions~\cite{chaudhry2018efficient}, it should be noted that architectural, distillation- and regularization-based methods can instead be characterized by non-negligible fixed overheads, making them less efficient and harder to scale.

\vspace{1em}
\begin{figure}[H]
\centering
{
\setlength\tabcolsep{5pt}
\begin{tabular}{cc}
  \includegraphics[width=.475\textwidth]{imgs/perf_mem/perf_mem_legend.pdf} & \includegraphics[width=.475\textwidth]{imgs/perf_mem/mnist_360_general_continual_learning.pdf} \\ 
 \end{tabular}
}
\end{figure}
\vspace{1em}
\begin{figure}[H]
\centering
{
\setlength\tabcolsep{5pt}
\begin{tabular}{cc}
  \includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_cifar_10_class_il.pdf} & \includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_cifar_10_task_il.pdf} \\
 \end{tabular}
}
\end{figure}
\vspace{1em}
\begin{figure}[H]
\centering
{
\setlength\tabcolsep{5pt}
\begin{tabular}{cc}
  \includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_tiny_imagenet_class_il.pdf} & \includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_tiny_imagenet_task_il.pdf} \\
\end{tabular}
}
\end{figure}

\newpage
\begin{figure}[H]
\centering
{
\setlength\tabcolsep{5pt}
\begin{tabular}{cc}
  \includegraphics[width=.475\textwidth]{imgs/perf_mem/permuted_mnist_domain_il.pdf} &
  \includegraphics[width=.475\textwidth]{imgs/perf_mem/rotated_mnist_domain_il.pdf} \\
\includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_mnist_class_il.pdf} & \includegraphics[width=.475\textwidth]{imgs/perf_mem/sequential_mnist_task_il.pdf} \\
\end{tabular}
}
\caption{Performance vs.\ memory allocation for the experiments of Section~\ref{sec:experiments} and~\ref{subsec:smnist}. Successive points of the same method indicate increasing buffer size. Methods with lower accuracy or excessive memory consumption may be omitted \textit{(best viewed in color).}}
\label{fig:performance_vs_memory_suppl}
\end{figure}

\section{Reservoir Sampling Algorithm}
In the following, we provide the buffer insertion algorithm (\ref{alg:reservoir}) for the Reservoir Sampling strategy~\cite{vitter1985random}.
\begin{figure}[H]
    \centering
    \begin{algorithm}[H]
    \caption{- Reservoir Sampling}
    \label{alg:reservoir}
    \begin{algorithmic}
      \STATE {\bfseries Input:} memory buffer , number of seen examples , example , label .
      \IF{} \STATE  \ELSE 
      \STATE  
      \IF{  }
      \STATE 
      \ENDIF
      \ENDIF
      \STATE \textbf{return} 
    \end{algorithmic}
    \end{algorithm}
\end{figure}

\section{Details on the Implementation of iCaRL}
Although iCaRL~\cite{rebuffi2017icarl} was initially proposed for the Class-IL setting, we make it possible to use it for Task-IL as well by introducing a modification of its classification rule. Let  be the average feature vector of the exemplars for class  and  be the feature vector computed on example , iCaRL predicts a label  as

Instead, given the tensor of average feature vectors for all classes , we formulate iCaRL's network response  as 

Considering the argmax for , without masking (Class-IL setting), results in the same prediction as Eq.~\ref{eq:icarl_classification}.

It is also worth noting that iCaRL exploits a weight-decay regularization term (\textit{wd\_reg}), as suggested in~\cite{rebuffi2017icarl}, in order to make its performance competitive with the other proposed approaches.

\section {Additional Results}

\subsection{Sequential-MNIST}
\label{subsec:smnist}

Similarly to Sequential CIFAR-10, the Sequential MNIST protocol split the whole training set of the MNIST Digits dataset in  tasks, each of which introduces two new digits.
\begin{table}[H]
\centering
{
\setlength{\tabcolsep}{2.0pt}
\begin{tabular}{lccccccc}
\toprule
\textbf{Model}          & \multicolumn{3}{c}{\textit{Class-IL}} &~~~~~&   \multicolumn{3}{c}{\textit{Task-IL}}  \\
\midrule
JOINT                   &                         & \tiny{} &                         &      &                         & \tiny{} &                       \\
SGD                     &                         & \tiny{} &                         &      &                         & \tiny{} &                       \\
\midrule
oEWC                    &                         & \tiny{} &                         &      &                         & \tiny{} &                       \\
SI                      &                         & \tiny{} &                         &      &                         & \tiny{} &                       \\
LwF                     &                         & \tiny{} &                         &      &                         & \tiny{} &                       \\
PNN                     &                         & -                       &                         &      &                         & \tiny{} &                       \\
\midrule
\textbf{Buffer}         & 200   & 500   & 5120 &                & 200   & 500   & 5120                  \\
\midrule
ER                      & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
MER                     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
GEM                     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
A-GEM                   & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
iCaRL                   & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
FDR                     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
GSS                     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
HAL                     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
\textbf{DER (ours)}     & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
\textbf{DER++ (ours)}   & \tiny{} & \tiny{} & \tiny{} &      & \tiny{} & \tiny{} & \tiny{} \\
\bottomrule
\end{tabular}
}
\vspace{0.5em}
\caption{Results for the Sequential-MNIST dataset.}
\end{table}


%
 
\subsection{Additional Comparisons with Experience Replay}
\label{subsec:chaudhry_mnist}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{others/imgs/readyciodri.pdf}
\caption{A comparison between our proposal (DER++) and the variants of Experience Replay presented in~\cite{chaudhry2019tiny}.}
\label{fig:ciodri}
\end{figure}

In the main paper we already draw a thorough comparison with Experience Replay (ER), showing that DER and DER++ often result in better performance and more remarkable capabilities. It is worth noting that the ER we compared with was equipped with the \textit{reservoir} strategy; therefore, it would be interesting to see whether the same experimental conclusions also hold for other variants of naive replay (\textit{e.g.} ER with ring-buffer). For this reason, Fig.~\ref{fig:ciodri} provides further analysis in the setting of~\cite{chaudhry2019tiny}, which investigates what happens when varying the number of samples that are retained for later replay. Interestingly, while \textit{reservoir} weakens ER when very few past memories are available, it does not bring DER++ to the same flaw. In the low-memory regime, indeed, the probability of leaving a class out from the buffer increases: while ER would not have any chance to retain the knowledge underlying these \quotationmarks{ghost} classes, we conjecture that DER++ could recover this information from the non-argmax outputs of the past predicted distribution.

\subsection{Single-Epoch Setting}
\label{subsec:single_ep}

\begin{table}[H]
    
    \centering
    \begin{tabular}{ccccccc}
    \toprule
& Buffer & ER & FDR & DER++ & JOINT & JOINT\\ \midrule
    \textbf{\#epochs} & & 1 & 1 & 1 & 1 & 50/100\\ \midrule
    \multirow{3}{*}{\shortstack[c]{Seq.\\CIFAR-10}} &   &  &  &  & & \\ &   &  &  &  &  &  \\ &  &  &  &  & & \\ \midrule
    \multirow{3}{*}{\shortstack[c]{Seq.\\Tiny ImageNet}} &    &  &  &   & & \\ &    &  &  &   &  &  \\ &  &  &  &  & & \\ \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \caption{Single-epoch evaluation setting (Class-IL).}
    \label{tab:singleepoch}
\end{table}

Several Continual Learning works present experiments even on fairly complex datasets (\textit{e.g.:} CIFAR-10, CIFAR-100, Mini ImageNet) in which the model is only trained for one epoch for each task~\cite{aljundi2019gradient, chaudhry2020using, chaudhry2018efficient, lopez2017gradient}. As showing the model each example only once could be deemed closer to real-world CL scenarios, this is a very compelling setting and somewhat close in spirit to the reasons why we focus on General Continual Learning.

However, we see that committing to just one epoch (hence, few gradient steps) makes it difficult to disentangle the effects of catastrophic forgetting (the focus of our work) from those of underfitting. This is especially relevant when dealing with complex datasets and deserves further investigation: for this reason, we conduct a single-epoch experiment on Seq. CIFAR-10 and Seq. Tiny ImageNet. We include in Tab.~\ref{tab:singleepoch} the performance of different rehearsal methods; additionally, we report the results of joint training when limiting the number of epochs to one and, \textit{vice versa}, when such limitation is removed (see last two columns). While the multi-epoch joint training learns to classify with a satisfactory accuracy, the single-epoch counterpart (which is the upper bound to all other methods in this experiment) yields a much lower accuracy and underfits dramatically. In light of this, it is hard to evaluate the merits of other CL methods, whose evaluation is severely undermined by this confounding factor. Although DER++ proves reliable even in this difficult setting, we feel that future CL works should strive for realism by designing experimental settings which are fully in line with the guidelines of GCL~\cite{de2019continual} rather than adopting the single-epoch protocol.

\subsection{Forward and Backward Transfer}
\label{subsec:fwbwt}

In this section, we present additional results for the experiments presented in Sec.~\ref{subsec:cl_exps} and~\ref{subsec:smnist}, reporting \textit{Forward Transfer} (FWT), \textit{Backward Transfer} (BWT)~\cite{lopez2017gradient} and \textit{Forgetting} (FRG)~\cite{chaudhry2018riemannian}. The first one assesses whether a model is capable of improving on unseen tasks w.r.t.\ random guessing, whereas the second and third ones measure the performance degradation in subsequent tasks. Despite their popularity in recent CL works~\cite{chaudhry2018riemannian, chaudhry2020using, de2019continual,lopez2017gradient}, we did not report them in the main paper because we believe that the average accuracy represents an already exhaustive measure of CL performance.

FWT is computed as the difference between the accuracy just before starting training on a given task and the one of the random-initialized network; it is averaged across all tasks. While one can argue that learning to classify unseen classes is desirable, the meaning of such a measure is highly dependent on the setting. Indeed, Class-IL and Task-IL show distinct classes in distinct tasks, which makes transfer impossible. On the contrary, FWT can be relevant for Domain-IL scenarios, provided that the input transformation is not disruptive (as it is the case with Permuted-MNIST). In conclusion, as CL settings sooner or later show all classes to the network, we are primarily interested in the accuracy at the end of the training, not the one before seeing any example.

FRG and BWT compute the difference between the current accuracy and its best value for each task. It is worth noting that any method that restrains the learning of the current task could exhibit high backward transfer but low final accuracy. This is as easy as increasing the weight of the regularization term: this way, the past knowledge is well-preserved but the current task is not learned properly. Moreover, BWT makes the assumption that the highest value of the accuracy on a task is the one yielded at the end of it. This is not always true, as rehearsal-based methods can exploit the memory buffer in a subsequent task, even enhancing their performance on a previous one if they start from low accuracy.

\begin{table}[H]
\centering
{
\setlength{\tabcolsep}{2.0pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccccc}
\toprule
\multicolumn{8}{c}{\textbf{FORWARD TRANSFER}} \vspace{0.2em}\\
\toprule
 \multirow{2}{*}{\textbf{Buffer}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{S-MNIST}} & \multicolumn{2}{c}{\textbf{S-CIFAR-10}} & \textbf{P-MNIST} & \textbf{R-MNIST}\\
\addlinespace[0.35ex]
 & & \textit{Class-IL} & \textit{Task-IL} & \textit{Class-IL} & \textit{Task-IL} & \textit{Domain-IL} & \textit{Domain-IL}\\
\midrule
\xmark                  & SGD                   & \tiny{}                          &  \tiny{}                           &  \tiny{}                          & \tiny{}                           &  \tiny{}                           & \tiny{} \\
\midrule                            
                        & oEWC                  &  \tiny{} & \tiny{}& \tiny{}                          & \tiny{}                           &  \tiny{}                           & \tiny{} \\
\multirow{2}{*}{\xmark} & SI                    &  \tiny{}                          & \tiny{}                           & \tiny{}                          & \tiny{}                           &  \tiny{} & \tiny{} \\
                        & LwF                   & \tiny{}                          &  \tiny{}                           & \tiny{} & \tiny{} & -                                                 & -                       \\
                        & PNN                   & -                                                 & N/A                                               & -                                                 & N/A                                               & -                                                 & -                       \\
\midrule                                                        
                        & ER                    & \tiny{}                          & \tiny{}                           & \tiny{}                          &  \tiny{}                           &  \tiny{} & \tiny{} \\
                        & MER                   & \tiny{}                          & \tiny{}                           & -                                                 & -                                                 & -                                                 & -                       \\
                        & GEM                   & \tiny{}                          & \tiny{} &  \tiny{}&  \tiny{}                           &  \tiny{}                           & \tiny{}                           \\
                        & A-GEM                 & \tiny{} &  \tiny{}                          & \tiny{}                          & \tiny{}                           &  \tiny{}                           & \tiny{}\\
\multirow{2}{*}{200}    & iCaRL                 & N/A                                               & N/A                                               & N/A                                               & N/A                                               & -                                                 & -                       \\
                        & FDR                   & \tiny{}                          & \tiny{}                           & \tiny{}                          & \tiny{}                           & \tiny{}                           & \tiny{} \\
                        & GSS                   & \tiny{}                          &  \tiny{}                           & \tiny{}                          & \tiny{}  &  \tiny{}                           & \tiny{} \\
                        & HAL                   & \tiny{}                          & \tiny{}                           & \tiny{}                          & \tiny{}                           & \tiny{}                           & \tiny{}  \\
                        & \textbf{DER (ours)}   & \tiny{}                          &  \tiny{} & \tiny{}                          &  \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & \textbf{DER++ (ours)} & \tiny{}                          & \tiny{}                           &  \tiny{}&  \tiny{}                           &  \tiny{}                           & \tiny{} \\
\midrule                            
                        & ER                    & \tiny{}                          &  \tiny{}                           &  \tiny{}                          & \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & MER                   & \tiny{}                          &  \tiny{}                           & -                                                 & -                                                 & -                                                 & -                       \\
                        & GEM                   & \tiny{}                          &  \tiny{}                           & \tiny{}                          &  \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & A-GEM                 &  \tiny{}                          &  \tiny{}                           &  \tiny{}                          &  \tiny{} &  \tiny{}                           & \tiny{} \\
\multirow{2}{*}{500}    & iCaRL                 & N/A                                               & N/A                                               & N/A                                               & N/A                                               & -                                                 & -                       \\
                        & FDR                   &  \tiny{}                          &  \tiny{}                           &  \tiny{}&  \tiny{}                           & \tiny{}                           & \tiny{} \\
                        & GSS                   & \tiny{}                          &  \tiny{}                           &  \tiny{}                          &  \tiny{}                           &  \tiny{} & \tiny{} \\
                        & HAL                   & \tiny{}                           & \tiny{}                            &  \tiny{}                          & \tiny{}                            & \tiny{}                           & \tiny{} \\
                        & \textbf{DER (ours)}   &  \tiny{}&  \tiny{} & \tiny{}                          & \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & \textbf{DER++ (ours)} & \tiny{}                          & \tiny{}                           &  \tiny{}                          & \tiny{}                           & \tiny{}                           & \tiny{} \\
\midrule                            
                        & ER                    & \tiny{}                          &  \tiny{}                           & \tiny{}                          & \tiny{}                           &  \tiny{} & \tiny{} \\
                        & MER                   & \tiny{}                          & \tiny{}                           & -                                                 & -                                                 & -                                                 & -                       \\
                        & GEM                   &  \tiny{}                          & \tiny{}                           & \tiny{}                           & \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & A-GEM                 & \tiny{}                          &  \tiny{} & \tiny{}                           & \tiny{}                           &  \tiny{}                           & \tiny{} \\              
\multirow{2}{*}{5120}   & iCaRL                 & N/A                                               & N/A                                               & N/A                                               & N/A                                               & -                                                 & -                       \\
                        & FDR                   &  \tiny{}& \tiny{}                           & \tiny{}                           & \tiny{}                           & \tiny{}                           & \tiny{} \\
                        & GSS                   & \tiny{}                          & \tiny{}                           & \tiny{}                           & \tiny{}                                             &  \tiny{}                           & \tiny{} \\
                        & HAL                   & \tiny{}                          & \tiny{}                            & \tiny{}                          &  \tiny{}                           & \tiny{}                          & \tiny{} \\
                        & \textbf{DER (ours)}   & \tiny{}                          & \tiny{}                           & \tiny{} &  \tiny{}                           &  \tiny{}                           & \tiny{} \\
                        & \textbf{DER++ (ours)} & \tiny{}                          &  \tiny{}                           & \tiny{}                          &  \tiny{} &  \tiny{}                           & \tiny{}\\
\bottomrule
\end{tabular}
}
\vspace{0.5em}
\caption{Forward Transfer results for the Experiments of Sec.~\ref{subsec:cl_exps} and~\ref{subsec:smnist}.}
}
\end{table} \begin{table}[H]
\centering
{
\setlength{\tabcolsep}{2.0pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccccc}
\toprule
 \multicolumn{8}{c}{\textbf{BACKWARD TRANSFER}} \vspace{0.2em}\\
\toprule
 \multirow{2}{*}{\textbf{Buffer}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{S-MNIST}} & \multicolumn{2}{c}{\textbf{S-CIFAR-10}} & \textbf{P-MNIST} & \textbf{R-MNIST}\\
\addlinespace[0.35ex]
 & & \textit{Class-IL} & \textit{Task-IL} & \textit{Class-IL} & \textit{Task-IL} & \textit{Domain-IL} & \textit{Domain-IL}\\
\midrule
\xmark                  & SGD                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                         & \tiny{}                      & \tiny{} \\
                        \midrule
                        & oEWC                  &   \tiny{}  &  \tiny{}                          &\tiny{}& \tiny{}                       & \tiny{}                      & \tiny{} \\
\multirow{2}{*}{\xmark} & SI                    &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                         & \tiny{}& \tiny{} \\
                        & LwF                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                         & -                                             & -                        \\
                        & PNN                   & -                                                     &  \tiny{} & -                                              &  \tiny{}& -                                             & -                        \\
\midrule                                                                                
                        & ER                    &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      &  \tiny{} \\
                        & MER                   &   \tiny{}                            &   \tiny{}                         & -                                              & -                                                & -                                             & -                       \\
                        & GEM                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & A-GEM                 &   \tiny{}                            &  \tiny{}& \tiny{}                       & \tiny{}                         & \tiny{}                      & \tiny{} \\
\multirow{2}{*}{200}    & iCaRL                 &   \tiny{}  &   \tiny{}                     & \tiny{}&  \tiny{}& -                                             & -                        \\
                        & FDR                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & GSS                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & HAL                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & \textbf{DER (ours)}   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      &  \tiny{} \\
                        & \textbf{DER++ (ours)} &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}&  \tiny{} \\
\midrule                                                                                    
                        & ER                    &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      &  \tiny{} \\
                        & MER                   &   \tiny{}                            &   \tiny{}                         & -                                              & -                                                & -                                             & -                       \\
                        & GEM                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      &  \tiny{} \\
                        & A-GEM                 &  \tiny{}                             &   \tiny{}                         & \tiny{}                       & \tiny{}                         & \tiny{}                      & \tiny{} \\
\multirow{2}{*}{500}    & iCaRL                 &   \tiny{}                            &  \tiny{}& \tiny{}                      &  \tiny{}& -                                             & -                        \\
                        & FDR                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      &  \tiny{} \\
                        & GSS                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & HAL                   &  \tiny{}                              &   \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                      & \tiny{} \\
                        & \textbf{DER (ours)}   &    \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         &  \tiny{}                      &  \tiny{} \\
                        & \textbf{DER++ (ours)} &    \tiny{}  &   \tiny{}                         & \tiny{}&  \tiny{}                      &  \tiny{}&  \tiny{} \\
\midrule                                                                                    
                        & ER                    &    \tiny{}                            &    \tiny{}                         & \tiny{}                       &  \tiny{}&  \tiny{}                      &  \tiny{} \\
                        & MER                   &    \tiny{}  &    \tiny{}                         & -                                              & -                                                & -                                             & -                       \\
                        & GEM                   &    \tiny{}                            &    \tiny{}                         & \tiny{}                       & \tiny{}                          &  \tiny{}                      &  \tiny{} \\
                        & A-GEM                 &  \tiny{}                            &   \tiny{}& \tiny{}                       & \tiny{}                          & \tiny{}                      & \tiny{} \\              
\multirow{2}{*}{5120}   & iCaRL                 &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                          & -                                             & -                        \\
                        & FDR                   &   \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                          &  \tiny{}                      &  \tiny{} \\
                        & GSS                   &    \tiny{}                            &   \tiny{}                         & \tiny{}                       & \tiny{}                          & \tiny{}                      & \tiny{} \\
                        & HAL                   &   \tiny{}                             &    \tiny{}                         & \tiny{}                       &  \tiny{}                         & \tiny{}                       & \tiny{}  \\
                        & \textbf{DER (ours)}   &    \tiny{}                            &   \tiny{}                         & \tiny{}                       &  \tiny{}                         &  \tiny{}                      &  \tiny{} \\
                        & \textbf{DER++ (ours)} &    \tiny{}                            &   \tiny{}                         & \tiny{}&  \tiny{}                       &  \tiny{}&  \tiny{} \\
\bottomrule
\end{tabular}
}
\vspace{0.5em}
\caption{Backward Transfer results for the Experiments of Sec.~\ref{subsec:cl_exps} and~\ref{subsec:smnist}.}
}
\end{table} \begin{table}[H]
\centering
{
\setlength{\tabcolsep}{2.0pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccccc}
\toprule
 \multicolumn{8}{c}{\textbf{FORGETTING}} \vspace{0.2em}\\
\toprule
 \multirow{2}{*}{\textbf{Buffer}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{S-MNIST}} & \multicolumn{2}{c}{\textbf{S-CIFAR-10}} & \textbf{P-MNIST} & \textbf{R-MNIST}\\
\addlinespace[0.35ex]
 & & \textit{Class-IL} & \textit{Task-IL} & \textit{Class-IL} & \textit{Task-IL} & \textit{Domain-IL} & \textit{Domain-IL}\\
\midrule
\xmark                  & SGD                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\
\midrule                                                                                            
                        & oEWC                  &  \tiny{}        &   \tiny{}                                  & \tiny{}                 &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\
\multirow{2}{*}{\xmark} & SI                    &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                     & \tiny{} \\
                        & LwF                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & -                                                                     & -                       \\
                        & PNN                   & -                                                         &   \tiny{}        & -                                                                 &  \tiny{} &                                          -                       &                                               -                       \\
\midrule                                                                                            
                        & ER                    &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               &  \tiny{} \\
                        & MER                   &  \tiny{}                                  &   \tiny{}                                  & -                                                                  & -                                                                & -                                                                     & -                       \\
                        & GEM                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & A-GEM                 &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\
\multirow{2}{*}{200}    & iCaRL                 &  \tiny{}        &   \tiny{}        & \tiny{}                 &   \tiny{}                                          & -                                                                     & -                       \\
                        & FDR                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & GSS                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & HAL                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & \textbf{DER (ours)}   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               &  \tiny{} \\
                        & \textbf{DER++ (ours)} &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                     &  \tiny{} \\
\midrule                                                                                            
                        & ER                    &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               &  \tiny{} \\
                        & MER                   &  \tiny{}                                  &   \tiny{}                                  & -                                                                  & -                                                                & -                                                                     & -                       \\
                        & GEM                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               &  \tiny{} \\
                        & A-GEM                 &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\
\multirow{2}{*}{500}    & iCaRL                 &  \tiny{}                                  &   \tiny{}        & \tiny{}                                           &   \tiny{}                & -                                                                     & -                       \\
                        & FDR                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               &  \tiny{} \\
                        & GSS                   &  \tiny{}                                  &   \tiny{}                                  &  \tiny{}                                          &   \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & HAL                   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               & \tiny{}  \\
                        & \textbf{DER (ours)}   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          &  \tiny{}                                               &  \tiny{} \\
                        & \textbf{DER++ (ours)} &   \tiny{}        &   \tiny{}                                  & \tiny{}                 &   \tiny{}                                          &  \tiny{}                     &  \tiny{} \\
\midrule                                                                                            
                        & ER                    &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          &  \tiny{}                                               &  \tiny{} \\
                        & MER                   &   \tiny{}        &   \tiny{}        & -                                                                  & -                                                                & -                                                                     & -                       \\
                        & GEM                   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          &  \tiny{}                                               &  \tiny{} \\
                        & A-GEM                 & \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &  \tiny{}                                          & \tiny{}                                               & \tiny{} \\              
\multirow{2}{*}{5120}   & iCaRL                 &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & -                                                                     & -                       \\
                        & FDR                   &  \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          &  \tiny{}                                               &  \tiny{} \\
                        & GSS                   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                            & \tiny{}                                             & \tiny{}                                                &   \tiny{} \\
                        & HAL                   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          & \tiny{}                                               & \tiny{} \\
                        & \textbf{DER (ours)}   &   \tiny{}                                  &   \tiny{}                                  & \tiny{}                                           &   \tiny{}                                          &  \tiny{}                                               &  \tiny{} \\
                        & \textbf{DER++ (ours)} &   \tiny{}                                  &   \tiny{}                                  &  \tiny{}                 &   \tiny{}                &  \tiny{}                     &  \tiny{} \\
\bottomrule
\end{tabular}
}
\caption{Forgetting results for the Experiments of Sec.~\ref{subsec:cl_exps} and~\ref{subsec:smnist}.}
}
\end{table} 
\section{Hyperparameter Search}
\subsection{Best values}
In Table~\ref{tab:hyperparams}, we show the best hyperparameter combination that we chose for each method for the experiments in the main paper, according to the criteria outlined in Section~\ref{subsec:cl_eval_prot}. We denote the learning rate with \textit{lr}, the batch size with \textit{bs} and the minibatch size (i.e. the size of the batches drawn from the buffer in rehearsal-based methods) with \textit{mbs}, while other symbols refer to the respective methods. We hold \textit{batch size} and \textit{minibatch size} out of the hyperparameter search space for all Continual Learning benchmarks. Their values are fixed as follows: Sequential MNIST: ; Sequential CIFAR-10, Sequential Tiny ImageNet: ; Permuted MNIST, Rotated MNIST: .

Conversely, \textit{batch size} and \textit{minibatch size} belong to the hyperparameter search space for experiments on the novel MNIST-360 dataset. It must be noted that MER does not depend on \textit{batch size}, as it internally always adopts a single-example forward pass.
\setlength\cellspacetoplimit{3.5pt}
\setlength\cellspacebottomlimit{1.5pt}

\begin{table}[H]
\centering\begin{tabular}{| Sl | Sc | Sl | Sc | Sl |}
\hline
\textit{Method} & \textit{Buffer} & \textbf{Permuted MNIST} & \textit{Buffer} & \textbf{Rotated MNIST}\\ 
\hline
SGD     & \xmark & \textit{lr:}  & \xmark & \textit{lr:}  \\
oEWC    & \xmark & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  & \xmark & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  \\
SI      & \xmark & \textit{lr:}  ~~ \textit{c:}  ~~ \textit{:}  & \xmark & \textit{lr:}  ~~ \textit{c:}  ~~ \textit{:}  \\
ER      & 200    & \textit{lr:}        & 200  &\textit{lr:}    \\
        & 500    & \textit{lr:}        & 500  &\textit{lr:}     \\
        & 5120   & \textit{lr:}        & 5120 &\textit{lr:}    \\
GEM     & 200    & \textit{lr:}  ~~ \textit{:}  & 200  &\textit{lr:}  ~~  \textit{:}  \\
        & 500    & \textit{lr:}  ~~ \textit{:}  & 500  &\textit{lr:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}  ~~ \textit{:}  & 5120 &\textit{lr:}  ~~ \textit{:}  \\
A-GEM   & 200    & \textit{lr:}        & 200  &\textit{lr:}    \\
        & 500    & \textit{lr:}        & 500  &\textit{lr:}    \\
        & 5120   & \textit{lr:}        & 5120 &\textit{lr:}    \\
FDR     & 200    & \textit{lr:}   ~~ \textit{:}  & 200  & \textit{lr:}   ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  \\
GSS     & 200 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 200 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 500 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
HAL     & 200    & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}   & 200  & \textit{lr:}   ~ \textit{:} ~~ \textit{:}  ~ \textit{:}   \\
        & 500    & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}    & 500  & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}   \\
        & 5120   & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}    & 5120 & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}   \\
DER     & 200    & \textit{lr:}   ~~ \textit{:}  & 200  & \textit{lr:}   ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  \\
DER++   & 200    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 200  & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & & & & \\
\hline
\textit{Method} & \textit{Buffer} & \textbf{Sequential MNIST} & \textit{Buffer} & \textbf{Sequential CIFAR-10}  \\
\hline
SGD & \xmark & \textit{lr:}   ~~ & \xmark & \textit{lr:}  \\
oEWC    & \xmark & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  & \xmark & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  \\
SI      & \xmark & \textit{lr:}  ~~ \textit{c:}  ~~ \textit{:}  & \xmark & \textit{lr:}  ~~ \textit{c:}  ~~ \textit{:}  \\
LwF     & \xmark & \textit{lr:}  ~ \textit{: } ~ \textit{T:}  ~ \textit{wd:}  & \xmark & \textit{lr:}  ~~ \textit{: } ~~ \textit{T:}  \\
PNN     & \xmark & \textit{lr:}   ~~ & \xmark & \textit{lr:}  \\
ER      & 200    & \textit{lr:}    & 200  &\textit{lr:}   \\
        & 500    & \textit{lr:}     & 500  &\textit{lr:}   \\
        & 5120   & \textit{lr:}    & 5120 &\textit{lr:}   \\
        & & & & \\
MER     & 200    & \textit{lr:}    ~ \textit{:}  ~ \textit{:}  ~ \textit{nb:}  ~ \textit{bs:}  & & \\
        & 500    & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{nb:}  ~ \textit{bs:}  & & \\
        & 5120   & \textit{lr:}    ~ \textit{:}  ~ \textit{:}  ~ \textit{nb:}  ~ \textit{bs:}  & & \\
GEM     & 200    & \textit{lr:}   ~~  \textit{:}  & 200  &\textit{lr:}  ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~  \textit{:}  & 500  &\textit{lr:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~  \textit{:}  & 5120 &\textit{lr:}  ~~ \textit{:}  \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering\begin{tabular}{| Sl | Sc | Sl | Sc | Sl |}
\hline
\textit{Method} & \textit{Buffer} & \textbf{Sequential MNIST} & \textit{Buffer} & \textbf{Sequential CIFAR-10} \\
\hline
A-GEM   & 200    & \textit{lr:}    & 200  & \textit{lr:}   \\
        & 500    & \textit{lr:}    & 500  & \textit{lr:}   \\
        & 5120   & \textit{lr:}    & 5120 & \textit{lr:}   \\
iCaRL   & 200    & \textit{lr:}  ~ \textit{wd:}  & 200  & \textit{lr:}  ~~ \textit{wd:}  \\
        & 500    & \textit{lr:}  ~~ \textit{wd:}  & 500  & \textit{lr:}  ~~ \textit{wd:} \\
        & 5120   & \textit{lr:}  ~~ \textit{wd:}  & 5120 & \textit{lr:}  ~~ \textit{wd:} \\
FDR     & 200    & \textit{lr:}    ~~ \textit{:}  & 200  & \textit{lr:}   ~~ \textit{:}  \\
        & 500    & \textit{lr:}    ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  \\
        & 5120   & \textit{lr:}    ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  \\
GSS     & 200    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 200  & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:} \\
        & 500    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:} \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:} \\
HAL     & 200    & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}   & 200  & \textit{lr:}  ~~            \textit{:}  ~~ \textit{:}  \\
        & & & & \textit{:}  \\
        & 500    & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}   & 500  & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & & & & \textit{:}  \\
        & 5120   & \textit{lr:}   ~ \textit{:}  ~ \textit{:}  ~ \textit{:}  & 5120 & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  \\
        & & & & \textit{:}  \\
DER     & 200    & \textit{lr:}    ~~ \textit{:}  & 200  & \textit{lr:}   ~~ \textit{:}  \\
        & 500    & \textit{lr:}    ~~ \textit{:}  & 500  & \textit{lr:}   ~~ \textit{:}  \\
        & 5120   & \textit{lr:}    ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  \\
DER++   & 200    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 200 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 500    & \textit{lr:}     ~~ \textit{:}  ~~ \textit{:}  & 500 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}     ~~ \textit{:}  ~~ \textit{:}  & 5120 & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  \\
        & & & & \\
\hline
\textit{Method} & \textit{Buffer} & \textbf{Sequential Tiny ImageNet} & \textit{Buffer} & \textbf{MNIST-360} \\
\hline
SGD & \xmark & \textit{lr:}  & \xmark & \textit{lr:}  ~~ \textit{bs:}  \\
oEWC    & \xmark & \textit{lr:}  ~~ \textit{:}  ~~ \textit{:}  & & \\
SI      & \xmark & \textit{lr:}  ~~ \textit{c:}  ~~ \textit{:}  & & \\
LwF     & \xmark & \textit{lr:}  ~~ \textit{: } ~~ \textit{T:}  & & \\
PNN     & \xmark & \textit{lr:}  & & \\
ER      & 200    & \textit{lr:}   & 200    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & 500    & \textit{lr:}    & 500    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & 5120   & \textit{lr:}   & 1000   & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
MER     & & & 200  & \textit{lr:}   ~~  \textit{mbs:}  ~~ \textit{:}  \\
        & & & & \textit{:}  ~~ \textit{nb:}  \\
        & & & 500  & \textit{lr:}   ~~  \textit{mbs:}  ~~ \textit{:}  \\
        & & & & \textit{:}  ~~ \textit{nb:} \\
        & & & 1000 & \textit{lr:}  ~~  \textit{mbs:}  ~~ \textit{:}  \\
        & & & & \textit{:}  ~~ \textit{nb:} \\
A-GEM  & 200    & \textit{lr:}   &
200    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & 500    & \textit{lr:}   &
500    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & 5120   & \textit{lr:}   &
1000    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:} \\
iCaRL   & 200    & \textit{lr:}  ~~ \textit{wd:}  & & \\
        & 500    & \textit{lr:}  ~~ \textit{wd:}  & & \\
        & 5120   & \textit{lr:}  ~~ \textit{wd:}  & & \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering\begin{tabular}{| Sl | Sc | Sl | Sc | Sl |}
\hline
\textit{Method} & \textit{Buffer} & \textbf{Sequential Tiny Imagenet} & \textit{Buffer} & \textbf{MNIST-360} \\
\hline
FDR      & 200   & \textit{lr:}  ~~ \textit{:}  & & \\
        & 500    & \textit{lr:}   ~~ \textit{:}  & &\\
        & 5120   & \textit{lr:}   ~~ \textit{:}  & & \\
GSS      & & & 200    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & & & 500    & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
        & & & 1000   & \textit{lr:}  ~~ \textit{bs:}  ~~ \textit{mbs:}  \\
DER      & 200   & \textit{lr:}  ~~ \textit{:}  & 200  & \textit{lr:}  ~~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~~ \textit{:} \\
        & 500    & \textit{lr:}   ~~ \textit{:}  & 500  & \textit{lr:}  ~~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~~ \textit{:} \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  & 1000 & \textit{lr:}  ~~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~~ \textit{:} \\
DER++   & 200    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 200    & \textit{lr:}  ~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~~ \textit{:}  ~~ \textit{:}  \\
        & 500    & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 500    & \textit{lr:}  ~~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~ \textit{:}  ~~ \textit{:}  \\
        & 5120   & \textit{lr:}   ~~ \textit{:}  ~~ \textit{:}  & 1000   & \textit{lr:}  ~~ \textit{bs:}  \\
        & & & & \textit{mbs:}  ~~ \textit{:}  ~~ \textit{:}  \\
\hline
\end{tabular}
\vspace{0.8em}
\caption{Hyperparameters selected for our experiments.}
\label{tab:hyperparams}
\end{table} 
\subsection{All values}
In the following, we provide a list of all the parameter combinations that were considered (Table~\ref{tab:hyperparam_space}). Note that the same parameters are searched for all examined buffer sizes.

{
\begin{table}[H]
    \begin{multicols}{3}
        \setlength{\tabcolsep}{2.5pt}
        \scriptsize
        \begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{\textbf{Permuted MNIST}}\\
\textit{Method} &  \textit{Par} &
\textit{Values} \\
\midrule
SGD/JOINT  & \textit{lr} & [0.03, 0.1, 0.2] \\
\midrule
oEWC & \textit{lr} & [0.1, 0.01] \\
& 	 \textit{} & [0.1, 1, 10, 30, 90,100] \\
& 	 \textit{} & [0.9, 1.0] \\
\midrule
SI  & \textit{lr} & [0.01, 0.1] \\
& 	 \textit{c} & [0.5, 1.0] \\
& 	 \textit{} & [0.9, 1.0] \\
\midrule
ER & \textit{lr} & [0.03, 0.1, 0.2] \\
\midrule
GEM & \textit{lr} & [0.01, 0.1, 0.3] \\
& \textit{} & [0.1, 0.5, 1]\\
\midrule
A-GEM  & \textit{lr} & [0.01, 0.1, 0.3] \\
\midrule
GSS  & \textit{lr} & [0.03, 0.1, 0.2] \\
& 	 \textit{gmbs} & [10, 64, 128] \\
& 	 \textit{nb} & [1] \\
\midrule
HAL  & \textit{lr} & [0.03, 0.1, 0.3] \\
& 	 \textit{} & [0.1, 0.2] \\
& 	 \textit{} & [0.3, 0.5] \\
& 	 \textit{} & [0.1] \\
\midrule
DER  & \textit{lr} & [0.1, 0.2] \\
& 	 \textit{} & [0.5, 1.0] \\
\midrule
DER++  & \textit{lr} & [0.1, 0.2] \\
& 	 \textit{} & [0.5, 1.0] \\
& 	 \textit{} & [0.5, 1.0] \\
\bottomrule
\end{tabular}

\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{\textbf{Rotated MNIST}}\\
\textit{Method} & \textit{Par} & \textit{Values} \\
\midrule
SGD   & \textit{lr} & [0.03, 0.1, 0.2] \\
\midrule
oEWC  & \textit{lr} & [0.01, 0.1] \\
& 	  \textit{} & [0.1, 0.7, 1, 10, 30, 90,100] \\
& 	  \textit{} & [0.9, 1.0] \\
\midrule
SI  & \textit{lr} & [0.01, 0.1] \\
& 	  \textit{c} & [0.5, 1.0] \\
& 	  \textit{} & [0.9, 1.0] \\
\midrule
ER  & \textit{lr} & [0.1, 0.2] \\
\midrule
GEM  & \textit{lr} & [0.01, 0.3, 0.1] \\
&  \textit{} & [0.1, 0.5, 1]\\
\midrule
A-GEM & \textit{lr} & [0.01, 0.1, 0.3] \\
\midrule
GSS  & \textit{lr} & [0.03, 0.1, 0.2] \\
& 	 \textit{gmbs} & [10, 64, 128] \\
& 	 \textit{nb} & [1] \\
\midrule
HAL  & \textit{lr} & [0.03, 0.1, 0.3] \\
& 	 \textit{} & [0.1, 0.2] \\
& 	 \textit{} & [0.3, 0.5] \\
& 	 \textit{} & [0.1] \\
\midrule
DER & \textit{lr} & [0.1, 0.2] \\
& 	 \textit{} & [0.5, 1.0] \\
\midrule
DER++ & \textit{lr} & [0.1, 0.2] \\
& 	  \textit{} & [0.5, 1.0] \\
& 	  \textit{} & [0.5, 1.0] \\
\bottomrule
\end{tabular}

\begin{tabular}{cccc}
\toprule
\multicolumn{3}{c}{\textbf{Sequential Tiny ImageNet}}\\
\midrule
\textit{Method} & \textit{Par} &
\textit{Values} \\
\midrule
SGD  & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
oEWC & \textit{lr} & [0.01, 0.03] \\
& \textit{} & [10, 25, 30, 90, 100] \\
& \textit{} & [0.9, 0.95, 1.0] \\
\midrule
SI   & \textit{lr} & [0.01, 0.03] \\
& \textit{c} & [0.5] \\
& \textit{} & [1.0] \\
\midrule
LwF  & \textit{lr} & [0.01, 0.03] \\
& \textit{} & [0.3, 1, 3] \\
& \textit{T} & [2.0, 4.0] \\
& \textit{wd} & [0.00005, 0.00001] \\
\midrule
PNN  & \textit{lr} & [0.03, 0.1] \\
\midrule
ER  & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
A-GEM  & \textit{lr} & [0.003, 0.01] \\
\midrule
iCaRL & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{wd} & [0.00005, 0.00001] \\
\midrule
FDR  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.03, 0.1, 0.3, 1.0, 3.0] \\
\midrule
DER  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.1, 0.5, 1.0] \\
\midrule
DER++ & \textit{lr} & [0.01, 0.03] \\
& \textit{} & [0.1, 0.2, 0.5, 1.0] \\
& \textit{} & [0.5, 1.0] \\
\bottomrule
\end{tabular}     \end{multicols}
\end{table}
\newpage
\begin{table}[H]
    \begin{multicols}{3}
        \setlength{\tabcolsep}{3.5pt}
        \scriptsize
        \begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{\textbf{Sequential MNIST}}\\
\textit{Method}  & \textit{Par} &
\textit{Values} \\
\midrule
SGD   & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
oEWC  & \textit{lr} & [0.01, 0.03, 0.1] \\
& 	  \textit{} & [10, 25, 30, 90, 100] \\
& 	  \textit{} & [0.9, 1.0] \\
\midrule
SI  & \textit{lr} & [0.01, 0.03, 0.1] \\
& 	  \textit{c} & [0.3, 0.5, 0.7, 1.0] \\
& 	  \textit{} & [0.9, 1.0] \\
\midrule
LwF   & \textit{lr} & [0.01, 0.03, 0.1] \\
& 	  \textit{} & [0.3, 0.5, 0.7, 1.0] \\
& 	  \textit{T} & [2.0, 4.0] \\
\midrule
PNN   & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
ER  & \textit{lr} & [0.03, 0.01, 0.1] \\
\midrule
MER  & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{} & [1.0] \\
& 	  \textit{nb} & [1] \\
\midrule
GEM & \textit{lr} & [0.01, 0.03, 0.1] \\
&  \textit{} & [0.5, 1]\\
\midrule
A-GEM  & 	  \textit{lr} & [0.03, 0.1] \\
\midrule
iCaRL & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{wd} & [0.0001, 0.0005] \\
\midrule
FDR   & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{} & [0.2, 0.5, 1.0] \\
\midrule
GSS   & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{gmbs} & [10] \\
& 	  \textit{nb} & [1] \\
\midrule
HAL   & \textit{lr} & [0.03, 0.1, 0.2] \\
& 	  \textit{} & [0.1, 0.5] \\
& 	  \textit{} & [0.2, 0.5, 0.7] \\
& 	  \textit{} & [0.1, 0.5] \\
\midrule
DER   & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{} & [0.2, 0.5, 1.0] \\
\midrule
DER++ & \textit{lr} & [0.03, 0.1] \\
& 	  \textit{} & [0.2, 0.5, 1.0] \\
& 	  \textit{} & [0.2, 0.5, 1.0] \\
\bottomrule
\end{tabular}


\begin{tabular}{cccc}
\toprule
\multicolumn{3}{c}{\textbf{Sequential CIFAR-10}}\\
\textit{Method} & \textit{Par} &
\textit{Values} \\
\midrule
SGD  & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
oEWC & \textit{lr} & [0.03, 0.1] \\
& \textit{} & [10, 25, 30, 90, 100]\\
& \textit{} & [0.9, 1.0] \\
\midrule
SI   & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{c} & [0.5, 0.1] \\
& \textit{} & [1.0] \\
\midrule
LwF  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.3, 1, 3, 10] \\
& \textit{T} & [2.0] \\
\midrule
PNN  & \textit{lr} & [0.03, 0.1] \\
\midrule
ER  & \textit{lr} & [0.01, 0.03, 0.1] \\
\midrule
GEM  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.5, 1]\\
\midrule
A-GEM & \textit{lr} & [0.03, 0.1] \\
\midrule
iCaRL & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{wd} & [0.0001, 0.0005] \\
\midrule
FDR  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.03, 0.1, 0.3, 1.0, 3.0] \\
\midrule
GSS  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{gmbs} & [32] \\
& \textit{nb} & [1] \\
\midrule
HAL   & \textit{lr} & [0.01, 0.03, 0.1] \\
& 	  \textit{} & [0.1, 0.5] \\
& 	  \textit{} & [0.2, 0.3, 0.5] \\
& 	  \textit{} & [0.1] \\
\midrule
DER  & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.2, 0.5, 1.0] \\
\midrule
DER++ & \textit{lr} & [0.01, 0.03, 0.1] \\
& \textit{} & [0.1, 0.2, 0.5] \\
& \textit{} & [0.5, 1.0] \\
\bottomrule
\end{tabular}


\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{\textbf{MNIST-360}}\\
\textit{Method}  & \textit{Par} &
\textit{Values} \\
\midrule
SGD   & \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 8, 16] \\
\midrule
ER  & \textit{mbs} & [16, 64, 128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 8, 16] \\
\midrule
MER   & \textit{mbs} & [128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{} & [1.0] \\
& 	  \textit{} & [1.0] \\
& 	  \textit{nb} & [1] \\
\midrule
A-GEM-R  & \textit{mbs} & [16, 64, 128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 16] \\
\midrule
GSS   & \textit{mbs} & [16, 64, 128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 8, 16] \\
& 	  \textit{gmbs} & [16, 64, 128] \\
&     \textit{nb} & [1] \\
\midrule
DER   & \textit{mbs} & [16, 64, 128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 8, 16] \\
& 	  \textit{} & [0.5, 1.0] \\
\midrule
DER++ & \textit{mbs} & [16, 64, 128] \\
& 	  \textit{lr} & [0.1, 0.2] \\
& 	  \textit{bs} & [1, 4, 8, 16] \\
& 	  \textit{} & [0.2, 0.5] \\
& 	  \textit{} & [0.5, 1.0] \\
\bottomrule
\end{tabular}
     \end{multicols}
\caption{Hyperparameter space for Grid-Search}
    \label{tab:hyperparam_space}
\end{table}
}\clearpage{}

\end{document}
