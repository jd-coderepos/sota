
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\title{KGRefiner: Knowledge Graph Refinement for Improving Accuracy of Translational Link Prediction Methods}

\author{Mohammad Javad Saeedizade, Najmeh Torabian \& Behrouz Minaei-Bidgoli  \\
Iran University of Science and Technology \\
School of Computer Engineering\\
Data Mining Lab\\
\texttt{\{m\char`_saeedizade,b\char`_minaei\}@iust.ac.ir} \\
\texttt{Najmeh.torabian@gmail.com} \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
The Link Prediction is the task of predicting missing relations between entities of the knowledge graph. Recent work in link prediction has attempted to provide a model for increasing link prediction accuracy by using more layers in neural network architecture. In this paper, we propose a novel method of refining the knowledge graph so that link prediction operation can be performed more accurately using relatively fast translational models. Translational link prediction models, such as TransE, TransH, TransD, have less complexity than deep learning approaches. Our method uses the hierarchy of relationships and entities in the knowledge graph to add the entity information as auxiliary nodes to the graph and connect them to the nodes which contain this information in their hierarchy. Our experiments show that our method can significantly increase the performance of translational link prediction methods in H@10, MR, MRR.
\end{abstract}

\section{Introduction}
Knowledge graphs represent a set of interconnected descriptions of entities, including objects, events, or concepts. These graphs are structures by which knowledge is stored in triples. These triples include the three parts head, relation, and tail. The relation determines the type of relationship between head and tail. These graphs are becoming a popular approach to display and model different information in the world. Additionally, knowledge graphs have several applications, for example, question answering systems \citep{qa_kg_1,qa_kg_2}, recommendation systems \citep{kb-recommender}, search engines \citep{xiong2017explicit}, relationship extraction \citep{distant_supervision2009}, etc. 

\indent Despite many efforts to build knowledge graphs, they are not complete yet. For example, in the Freebase \citep{freebase}, over 70\% of people do not have their place of birth in the graph. This incompleteness of knowledge graphs has motivated researchers to add information to the graph and complete it.

\indent One of the developing fields in completing the knowledge graph is knowledge graph embedding (KGE). The task of KGE is to embed entities and relationships in a small continuous vector space. One application of these embedding is to predict missing links in the knowledge graph.

Translational link prediction models use the sum of the head and relation vectors to predict the tail. These models started with TransE \citep{bordes2013translating}, and after that, TransH \citep{transh}, TransR \citep{lin2015learning}, TransD \citep{ji2015knowledge}, RotatE \citep{rotate}, etc., tried to improve it in the following years. The advantages of translational methods over deep learning techniques are that they are robust, and their score function is considerably faster. Therefore, in this work, we tried to improve these translational methods.

\indent There is a lot of information in knowledge graphs. The hierarchy of entities and relationships is part of it. Paris, for example, its hierarchy is ``entity$\,\to\,$physical\_entity$\,\to\,$object$\,\to\,$location$\,\to\,$region$\,\to\,$area$\,\to\,$center$\,\to\,$seat$\,\to\,$capital$\,\to\,$national\_capital''. This hierarchy is not given enough attention in link prediction methods, and we intend to use this information in this paper.

\indent SACN \citep{sacn_paper} added some nodes and relationships to the graph to use the graph structure information but did not justify adding these nodes and edges, so it is not generalizable for other graphs. In addition, SACN added this information only to FB15K237 and did not provide a method for WN18RR. In this paper, we added a much smaller number of relationships and fewer nodes to the graph training section by interpreting them. HRS \citep{zhang2018knowledge} used relation clusters and sub-relations to use this information. Nevertheless, like SACN, this can not be generalized well.

The \citep{moon2017learning} considered that if two entities are embedded closely in the embedding space, they are similar and assigned entities' classes based on closeness. Still, we assumed that if two entities use the same relation in the graph or have common elements in their hierarchies, they are related. 

When link prediction models learned the relation between Paris and France, previous link prediction methods did not notice that Paris is a city and France is a country. To use this information, we added auxiliary nodes to the graph that included the classes of entities and connected them to related entities. For example, we added an extra node for countries to the knowledge graph and connected it to all the knowledge graph countries. Our contributions are as follows:
\begin{enumerate}
	\item[$\bullet$]  We presented a method for refining the knowledge graph, which is independent of the structure of the link prediction model and adds triples to the knowledge graph. These triples increase the accuracy of link prediction with the same time and space complexity of translational models.
	\item[$\bullet$]  We evaluated our proposed method on two FB15K237 and WN18RR datasets with successful translational models. The results showed that accuracy in link prediction was significantly increased on H@10, MRR, and MR.
  \end{enumerate}
\section{Related Work}
Knowledge graph embedding is an active and developing field to embed the entities and relations of the knowledge graph. These embeddings are used in link prediction, question answering systems, relation extraction, etc. Knowledge graph embedding starts with TransE \citep{bordes2013translating}, which is the first translational link prediction method. It interprets relation as a transition from head entity to tail in the graph. Some drawbacks of the TransE model are its inability to model N-1, 1-N, and N-N relationships. In the following years, some other translational approaches, such as TransH \citep{transh}, TransD \citep{ji2015knowledge}, and TransR \citep{lin2015learning}, were inspired by the initial idea of TransE \citep{bordes2013translating} and tried to improve it. These translational models have much more speed against deep learning models such as ConvE \citep{conve}, ConvKB \citep{convkb}, SACN \citep{sacn_paper}, and HAKE \citep{zhang2020learning}, but their accuracy is slightly lower than these models. Therefore, we proposed a method to increase the accuracy of these translational models.
\\
\indent
Knowledge graph refinement is a field of correcting or improving the knowledge graph. BioKG \citep{zhao2020biomedical}, which worked on medical graphs, has tried to provide a method for removing the wrong information in these graphs. Other works in the refinement of the knowledge graphs try to add information. SACN \citep{sacn_paper} has also added attributes to the knowledge graph, like our work. SACN proposed FB15k237\_Attr; this method for constructing this dataset has three major issues. First, it only worked for FB15k237, but our proposed method can be applied on WN18RR as well. Second, it has brought the number of FB15k237 relations from 237 to 484; therefore, it has more time complexity than ours. However, we only proposed two new relations for FB15k237 and only one relation for WN18RR. Third, these new relations and entities are not interpretable in SACN; It does not provide a reason for adding these attributes. So it can not be generalized on other graphs. \\
\indent
HRS \citep{zhang2018knowledge} tried to use sub-relation and relation-cluster to make better predictions. It used the hierarchy of relations as a sub-relationship, and it created a relation cluster to use these as two additional parts of the transition in the translational models. Because links in Wordnet do not have information about entities, HRS sub-relation and relation-cluster on Wordnet are meaningless.
\section{Background}
\label{gen_inst}
\noindent Suppose E as the collection of all entities of knowledge graph and R set of all its relationships. The ($e_s$, $r$, $e_o$) is called a triple. The $e_s \sim$ E is the head, and $e_o \sim E$ is the tail of a triple. Finally, $r \sim E$ represents the relation between $e_s$ and $e_o$.
\subsection{Link Prediction}
Link prediction is the task of predicting the missing link of a knowledge graph by inferring from existing facts on it. The score function of link prediction methods is $\psi(e_o, r, e_s)$, which evaluates triple's accuracy. Our goal in teaching a model that has the highest estimation for the missing triplets of the graph and the lowest prediction for false triples.
\subsection{Translational Link Prediction Models}
Translational link prediction methods consider the relation as a transition from head to tail. For example (Paris, Capital of, France), the relation ``Capital of'' is a transition from Paris to France. TransE \citep{bordes2013translating} is the first translational link prediction model. In TransE, embeddings for correct triples are learned as $e_s + r \sim e_o$. It means that the sum of the head's embedding and relation's embedding must be close to the tail; primarily, the distance measure is the L2 norm.
Here are some translational link predictions:\\

\noindent \textbf{TransE:}
For factual triple ($e_s$,$r$,$e_o$), adding embeddings of head and relation should be closed to the tail embedding, and on the other hand, for corrupted ones ($e_s$,$r$,$e_o\prime$),  $e_s +  r$ should have a distance with $e_o\prime$. The score function of TransE is as follow: 
\begin{align*}
    \psi(e_o, r, e_s) &= -|| h + r - t ||^2_2
\end{align*}
\noindent \textbf{TransH} \citep{transh}\textbf{:}
To improve modelling of N-1, 1-N and N-N, TransH defined a hyperplane for each relations, and translation property should be established on that hyperplane.
\begin{align*}
    h_ \bot = w^ \bot_r hw_r \,\, &,\,\, t_ \bot = w^ \bot_r tw_r \\
    \psi(e_o, r, e_s) &= -|| h_ \bot + r - t_ \bot ||^2_2
\end{align*}

\noindent \textbf{TransD} \citep{ji2015knowledge} \textbf{:}
It creates a dynamic matrix for all entity-relation pairs and maps the head and tail into M1 and M2, respectively. The transition from head to tail is as follow:
\begin{align*}
    M^1_r  =  w_r w^ \bot_h + I \,\, &,\,\, M^2_r  =  w_r w^ \bot_t + I \\
    h_ \bot = M^1_r h \,\, &,\,\, t_ \bot = M^2_r t \\
    \psi(e_o, r, e_s) = -|&| h_ \bot + r - t_ \bot ||^2_2
\end{align*}
\noindent \textbf{TransR} \citep{lin2015learning} \textbf{:}
It considers that entities may have multiple aspects, and various relations focus on different aspects of entities. It projects entities into relation space by projection matrix M.
\begin{align*}
h_ \bot = M_r h \,\, &,\,\, h_ \bot = M_r t  \\
    \psi(e_o, r, e_s) = -|&| h_ \bot + r - t_ \bot ||^2_2
\end{align*}
\noindent \textbf{RotatE} \citep{rotate} \textbf{:}
RotatE deals with relation as a rotation to complex space. This rotation brings the source entity to the target entity in the complex space. The relation applies to the head entity by  Hadamard product. Then it uses the L1 norm to measure the distance from the tail entity in the score function.
\begin{align*}
    \psi(e_o, r, e_s) &= -|| h_ \circ r - t_ \bot ||^2
\end{align*}


\subsection{Knowledge Graph Refinement}
The knowledge graph refinement follows two main objectives: (A) adding information to the knowledge graph, which is a subcategory of the knowledge graph completion. (B) Detecting incorrect information and remove those triplets from the knowledge graph to increase the correctness of the knowledge graph.




\section{KGRefiner}
\label{headings}
\begin{figure*}[ht!]
	\centering
			\includegraphics[width=1\textwidth]{Untitled.png}
		\label{fig:verticalcell}
		\caption{Simple illustration of changes in embedding space. The right side graph shows the effect of adding auxiliary nodes to the graph, which translational models bring all countries together and cities together in vector space.}
\end{figure*}
In this work, we propose a method to add information to the graph, which refines the knowledge graph and increases link prediction accuracy. In FB15k237,  we do this refinement by using relation hierarchies, and in WN18RR, we use hierarchies of entities. We add this information to the graph as a new node; these nodes are auxiliary nodes. We introduce several new relations to connect these new nodes to graph nodes, and we add these triples to the graph.\\ 
Translational link prediction methods such as TransE \citep{bordes2013translating}, TransH \citep{transh}, TransD \citep{ji2015knowledge}, etc., create transition property in their embeddings. For example, in TransE, embeddings are made as follow: 
\begin{align}
	e_s + r &\approx e_o
 \end{align}
 This means in embedding space; the tail entity should be close to the sum of head and relation. For example, let's consider these triples: 
 \begin{align}
	Paris + capital of &\approx France \\
Tehran + capital of &\approx Iran
 \end{align}
Link prediction model is not aware of both tails entities are country. If we add new node as ``country'' to the graph and connect it to all graph's countries with a new relation ``RelatedTo'' then these triples are added to graph: 
\begin{align}
France + RelatedTo &\approx country \\
Iran + RelatedTo &\approx country 
\end{align}
Equations 4 and 5, which are similar, bring closer the embeddings of France and Iran, which are semantically identical. Figure 1 gives an illustration of what changes KGrefiner brings for the embedding space. This closeness in evaluating Equation 2 causes the model to search between countries when asked where France’s capital is.
\subsection{Refinement of FB15k237}
In FB15k237, graph relations contain information about entities. For example, the ``entity$\,\to\,$physical\_entity$\,\to\,$object$\,\to\,$location$\,\to\,$region$\,\to\,$area$\,\to\,$center$\,\to\,$seat$\,\to\,$capital$\,\to\,$national\_capital'' is a relationship between countries and cities, and nodes on one side of relationships can be considered similar. Higher levels usually have more general information about objects in the hierarchy, and lower levels have more specific, so we extracted the last three levels of hierarchies from each relation in this graph to use this information. Then, for each sub-relation, we counted the number of repetitions in the graph training section. We removed those components with less than 100 repetitions in the graph to reduce the number of these sub-relations, and the number 100 is arbitrary. Finally, 285 sub-relations remained, which we added to the set of entities in this graph (as new nodes). We call these auxiliary nodes relation-nodes. We defined two new relations, ``RelatedTo'' and ``HasAttribute'', to connect these relation-nodes to the graph. For each triple, if the entity is the triple's head, we linked it with relation-node by ``RelatedTo'', and if it is the tail of the triple, we use ``HasAttribute'' to establish these connections. For example, to refine relation between Paris and France, (Paris,``entity$\,\to\,$physical\char`_entity$\,\to\,$object$\,\to\,$location$\,\to\,$region$\,\to\,$area$\,\to\,$center$\,\to\,$seat$\,\to\,$capital$\,\to\,$national\char`_capital'',France), ``capital'' has repetition over 100, so the following triples were added to the graph:
\begin{align*}
	France + HasAttribute &\approx capital \\
	Paris + RelatedTo &\approx capital 	
\end{align*}
\subsection{Refinement of WN18RR}
To refine this graph, we use the hierarchy of entities. In Freebase, we used relationships, but relationships do not give us information about entities in Wordnet. France, for example, has a hierarchy of ``existence $\,\to\,$ place $\,\to\,$ region $\,\to\,$ region $\,\to\,$ administrative region $\,\to\,$ country $\,\to\,$ France''. This hierarchy gives us good information about France. Except for the last level, we extract the other last three levels of entities.  Among these levels, we hold those with more than an arbitrary number of 50 repetitions among entities to reduce these levels. As a result, 207 levels remained. We add these levels as new nodes to the graph training section and connect them to the entities with these levels in their hierarchy with a new type of connection. In this graph, we define a new relation and name it ``HasAttribute''. For example, France and Iran have a ``country'' in their hierarchical structure. Then, the following triples were added to the training section of the graph:
\begin{align*}
	France + HasAttribute &\approx country \\
	Iran + HasAttribute &\approx country 	
\end{align*}
\begin{table*}[h]
  \begin{center}
\begin{tabular}{l|c|c|c|c} 
      \textbf{Dataset} & \textbf{FB15k237} & \textbf{FB15k237-Refined}& \textbf{WN18RR} & \textbf{WN18RR-Refined}\\
      \hline
      Entities & 14541 & 14826& 40943& 41150\\
      Relations & 237 & 239& 11& 12\\
      Train Edges & 272115 & 550998& 86835& 230135\\
      Val. Edges & 17535 & 17535& 3034& 3034\\
      Test Edges & 20466 & 20466& 31134& 31134\\
    \end{tabular}
    \caption{Statistics of the experimental datasets. The refined version represents that graph has some auxiliary nodes.}
\bigskip
\begin{tabular}{|c|ccc|}
\hline
Baseline                    & H@10                & MR                 & MRR                 \\ \hline
TransE                      & 45.6                & 347                & \textbf{29.4}                \\
\textbf{TransE + KGRefiner} & \textbf{47}         & \textbf{203}       & 29.1                \\ \hline
TransD                      & \textbf{45.3}       & 256                & \textbf{28.6}       \\
\textbf{TransD + KGRefiner} & 43.7                & \textbf{227}       & 24                  \\ \hline
RotatE                      & \textbf{47.4}       & {\ul \textbf{185}} & \textbf{29.7}       \\
\textbf{RotatE + KGRefiner} & 43.9                & 226                & 27.9                \\ \hline
TransH                      & 36.6                & 311                & 21.1                \\
\textbf{TransH + KGRefiner} & {\ul \textbf{48.9}} & \textbf{221}       & {\ul \textbf{30.2}} \\ \hline
\end{tabular}
 \caption{Link prediction results on FB15K237 and its refined version. Results of TransE is taken from \protect\citep{convkb}, TransH and TransD from \protect\citep{zhang2018knowledge}, but for RotatE we used \protect\citep{han2018openke} to produce scores.}
\begin{tabular}{|c|ccc|}
\hline
Baseline                    & H@10                & MR                 & MRR                 \\ \hline
TransE                      & 50.1                & 3384               & \textbf{22.6}       \\
\textbf{TransE + KGRefiner} & \textbf{53.7}       & \textbf{1125}      & 22.2                \\ \hline
TransH                      & 42.4                & 5875               & 18.6                \\ \textbf{TransH + KGRefiner} & \textbf{51.4}       & \textbf{1534}      & \textbf{20.8}       \\ \hline
TransD                      & 42.8                & 5482               & 18.5                \\
\textbf{TransD + KGRefiner} & \textbf{52.3}       & \textbf{1348}      & \textbf{21.4}       \\ \hline
RotatE                      & 54.7                & 4274               & {\ul \textbf{47.3}} \\
\textbf{RotatE + KGRefiner} & {\ul \textbf{57.0}} & {\ul \textbf{683}} & 44.8                \\ \hline
\end{tabular}


  \caption{Link prediction results on WN18RR and its refined version. Results of TransE is taken from \protect\citep{convkb}, TransH and TransD from \protect\citep{zhang2018knowledge}, for RotatE we used \protect\citep{han2018openke} to produce scores. For other results, we used \protect\citep{han2018openke} to produce them.}

\end{center}
\end{table*}

\section{Exprement}
\subsection{Datasets}
We evaluated our work on popular benchmarks: FB15K237 and WN18RR; these datasets are respectively refined from real knowledge graphs: WordNet \citep{wordnet} and Freebase \citep{freebase}. In addition, we built two other datasets with KGRefiner: FB15K237-Refined and WN18RR-Refined, respectively, from FB15K237  and WN18RR. The details of the datasets are shown in Table 1.


\subsection{Baselines}
To demonstrate the effectiveness of our models, we compare results with the original translational models TransE \citep{bordes2013translating}, TransH \citep{transh}, TransD \citep{ji2015knowledge}, and the last translational model, RotatE \citep{rotate}.

\subsection{Experimental Settings}
We used implementation of baselines by OpenKE \citep{han2018openke}. We used an embedding dimension of 200 for all models. Also, we removed self adversarial negative sampling from TransE and RotatE to have a fair comparison. We tried \{200, 500, 1000, 2000\} epochs, and we picked the best epoch according to MRR on the validation set. Other hyperparameters of the models are those mentioned in OpenKE. Hyperparameters for FB15K237 and FB15K237-Refined and also WN18RR and WN18RR-Refined are the same.

\subsection{Experimental Results}
Table 2 and 3 compares the experimental results of our KGRefiner plus translational models and with previously published results. Results in bold font are the best results in the group, and the underlined results denote the best results in the column. KGRefiner with TransH obtains the highest H@10 and MRR on FB15k237, and also KGRefiner with RotatE reached the best MR and H@10 in WN18RR.
\begin{center}
\begin{table}[]
  \centering
  \begin{tabular}{l|c}
  Model                                    & \begin{tabular}[c]{@{}c@{}}Training Time \\ for single epoch\end{tabular} \\ \hline
  TransE \citep{bordes2013translating} $[\oplus]$                    & 2.8 s                                                                                \\ \hline
  TransH \citep{transh}  $[\oplus]$                   & 5.2 s                                                                                \\ \hline
  TransD \citep{ji2015knowledge}  $[\oplus]$          & 5.2 s                                                                                \\ \hline
  RotatE \citep{rotate}    $[\oplus]$                 & 5 s                                                                                  \\ \hline
  ConvE \\ \citep{conve}     $[\ominus]$                  & 279 s                                                                                \\ \hline
  ConvKB \\ \citep{convkb}   $[\ominus]$                  & 40 s   
                                                                             
  \end{tabular}
  \caption{Comparison between translational technique and deep learning methods in training time. [$\oplus$]: These models are implemented by OpenKE \protect\citep{han2018openke} and [$\ominus$] are produced by their original implementations.}

  \end{table}
\end{center}
\subsection{Speed of Models}
The training time of translational models is much less than deep learning approaches such as ConvE, SACN, ConvKB, etc. The complexity in scoring function and neural network layers in their architecture reduces training speed in deep learning methods. Table 4 compares the time that each model needs to be trained for one epoch on FB15k237. We ran models on Nvidia K80. For fair comparison embedding dimension for all models is 200. These models usually need 1000 epochs, so the runtime difference between TransE and RotatE is around 35000s for FB15k237.




\section{Conclusion}
In this paper, we propose KGRefiner, a novel knowledge graph refinement method that alleviates the limitations of translational models by capturing additional information in knowledge graph hierarchies. We used hierarchy components as new nodes, and by connecting these nodes to proper entities in the knowledge graph, we have a more informative graph. Our experimental results show that our KGRefiner outperforms other state-of-the-art translational models on two benchmark datasets WN18RR and FB15k237. Furthermore, it is the first augmentation method that works with both Wordnet and Freebase, while old methods only perform only on one dataset.
\indent In future works, we will expand our work on datasets that can be formulated on the triple structure. For example, recommender system datasets can be formed on graph schema, and KGRefiner can be applied.
 

\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}


\end{document}
