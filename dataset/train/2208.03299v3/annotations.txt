[{'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '46.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '47.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '11'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '38.8'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '54.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '52.8'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '64.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '60.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '45.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '42.4'}}]
