\section{Experiments}
In this section, the proposed Stripformer is evaluated. 
We first describe the datasets and implementation details. Then we compare our method with the state-of-the-arts quantitatively and qualitatively. 
At last, the ablation studies are provided to demonstrate the effectiveness of the Stripformer design. 
\subsection{Datasets and Implementation Details}
For comparison, we adopt the widely used GoPro dataset \cite{Nah_2017_CVPR} which includes  blurred and sharp pairs for training and  pairs for testing. 
The HIDE dataset~\cite{HAdeblur} with  images is included only for testing. 
To address real-world blurs, we evaluate our method on the RealBlur dataset \cite{rim_2020_ECCV}, which has  blurred and sharp pairs for training and  pairs for testing.\footnoteonlytext{The authors from the universities in Taiwan completed the experiments.}


We train our network with a batch size of  on the GoPro dataset. 
Adam optimizer is used with the initial learning rate of  that is steadily decayed to  by the cosine annealing strategy. 
We adopt random cropping, flipping, and rotation for data augmentation, like~\cite{MT_2020_ECCV,Yuan_2020_CVPR}. We train Stripformer on the GoPro training set and evaluate it on the GoPro testing set and HIDE dataset. For the RealBlur dataset, we use the RealBlur training set to train the model and evaluate it on the RealBlur testing set. We test our method on the full-size images using an NVIDIA 3090 GPU. 

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{3mm}
\caption{Evaluation results on the benchmark GoPro testing set. The best two scores in each column are highlighted in bold and underlined, respectively.  represents the work did not release the code or pre-trained weight. Params and Time are calculated in (M) and (ms), respectively.}
\begin{tabular}{l|c|c|c|c}\hline\hline
{Method}         & {PSNR\,}  & {SSIM\,} & {Params\,} &{Time\,} \\ \hline
\multicolumn{4}{c}{CNN-based} 
\\\hline
{DeblurGAN-v2~\cite{Kupyn_2019_ICCV}} & {29.55} & {0.934} & {68} & {60}\\
{EDSD\dag~\cite{Yuan_2020_CVPR}} & {29.81} & {0.937} & \bf{3} & \bf{10}\\
{SRN~\cite{tao2018srndeblur}} & {30.25} & {0.934} & \underline{7} & {650}  \\
{PyNAS\dag~\cite{Hu_2021_ICCV}} & {30.62} & {0.941} & 9 & \underline{17}  \\
{DSD~\cite{gao2019dynamic}} & {30.96} & {0.942} & {\bf{3}} & {1300}\\
{DBGAN\dag~\cite{Zhang_2020_CVPR}} & {31.10} & {0.942} & {-} & {-} \\
{MTRNN~\cite{MT_2020_ECCV}} & {31.13} & {0.944} & {\bf3} & {30} \\
{DMPHN~\cite{Zhang_2019_CVPR}} & {31.20} & {0.945} & {22} & 303\\
{SimpleNet\dag~\cite{Li_2021_ICCV}} & {31.52} & {0.950} & {25} & 376\\
{RADN\dag~\cite{RADN_2020_ECCV}} & {31.85} & {0.953} & {-} & 38 \\
{SAPHN\dag~\cite{SAPN2020}} & {32.02} & {0.953} & {-} & 770 \\
{SPAIR\dag~\cite{Purohit_2021_ICCV}} & {32.06} & {0.953} & {-} & - \\
{MIMO~\cite{MIMO}} & {32.45} & {0.957} & {16} & 31  \\
{TTFA\dag~\cite{Chi_2021_CVPR}} & {32.50} & {0.958} & {-} & -  \\
{MPRNet~\cite{Zamir2021MPRNet}} & {32.66} & {0.959} & {20} & 148\\ \hline
\multicolumn{4}{c}{Transformer-based}
\\\hline
{IPT~\cite{IPT}} & {32.58} & {-} & {114} & {-}      \\
{Stripformer} & {\bf{33.08}} & {\bf{0.962}} & {20} & 52  \\
\hline\hline
\end{tabular}
\label{Tab:GoPro_eval}
\end{table}

\subsection{Experimental Results}
\noindent\textbf{Quantitative Analysis.}
We compare our model on the GoPro testing set with several existing SOTA methods~\cite{IPT,Chi_2021_CVPR,MIMO,gao2019dynamic,Hu_2021_ICCV,Kupyn_2019_ICCV,Li_2021_ICCV,MT_2020_ECCV,RADN_2020_ECCV,Purohit_2021_ICCV,SAPN2020,tao2018srndeblur,Yuan_2020_CVPR,Zamir2021MPRNet,Zhang_2019_CVPR,Zhang_2020_CVPR}.
In Table~\ref{Tab:GoPro_eval}, all of the compared methods utilize CNN-based architectures to build the deblurring networks except for IPT~\cite{IPT} and our network, where transformers serve as the backbone for deblurring.
As shown in Table~\ref{Tab:GoPro_eval}, Stripformer performs favorably against all competing methods in both PSNR and SSIM on the GoPro test set.
It is worth mentioning that Stripformer can achieve state-of-the-art performance by only using the GoPro training set. It has exceeded the expectation that transformer-based architectures tend to have suboptimal performance compared to most of the CNN-based methods without using a large amount of training data~\cite{IPT,dosovitskiy2020vit}.
That is, a transformer-based model typically requires a large dataset, \eg more than one million annotated data, for pre-training to compete with a CNN-based model in vision tasks.
For example, IPT fine-tunes the models with pre-training on ImageNet for deraining, denoising, and super-resolution tasks to achieve competitive performance.
We attribute Stripformer's success on deblurring to being able to better leverage the local and global information with our intra-strip and inter-strip attention design.
In addition, Stripformer can run efficiently without using recurrent architectures compared to the~\cite{SAPN2020,Zamir2021MPRNet,Zhang_2019_CVPR}.
Table~\ref{Tab:HIDE_eval} and Table~\ref{Tab:Realblur_eval} report more results on the HIDE and RealBlur datasets, respectively. As can be seen, Stripformer again achieves the best deblurring performance among the SOTA methods for both synthetic and real-world blur datasets.

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{3mm}
\caption{Evaluation results on the benchmark HIDE dataset. Note that all the models are trained on the GoPro training set. The best two scores in each column are highlighted in bold and underlined, respectively. Params and Time are calculated in (M) and (ms), respectively.}
\begin{tabular}{l|c|c|c|c}\hline\hline
{Method} & {PSNR\,}  & {SSIM\,} & {Params\,} &{Time\,} 
\\\hline
{DeblurGAN-v2~\cite{Kupyn_2019_ICCV}}  & {27.40} & {0.882} & 68 &  57  \\
{SRN~\cite{tao2018srndeblur}}        & {28.36} & {0.904} & \underline7 &  424      
\\
{HAdeblur~\cite{HAdeblur}}        & {28.87} & {0.930} & - &  -  
\\
{DSD~\cite{gao2019dynamic}}        & {29.01} & {0.913}  & \bf3 &  1200  
\\
{DMPHN~\cite{Zhang_2019_CVPR}}        & {29.10} & {0.918}  & 22 & 310  
\\
{MTRNN~\cite{MT_2020_ECCV}}        & {29.15} & {0.918} & 22 & \underline{40}  
\\
{SAPHN~\cite{SAPN2020}}        & {29.98} & {0.930} & - &  -
\\
{MIMO~\cite{MIMO}}  & {30.00} & {0.930} & 16 &  \bf30  
\\ 
{TTFA~\cite{Chi_2021_CVPR}} & {30.55} & {0.935}  & - &  -
\\
{MPRNet~\cite{Zamir2021MPRNet}}        & {\underline{30.96}} & {\underline{0.939}}
& 20 & 140 
\\
\noalign{\hrule height 1.0pt}
{Stripformer}      & \bf{31.03} & \bf{0.940}   
& 20 &  43
\\
\hline\hline
\end{tabular}
\label{Tab:HIDE_eval}
\end{table}

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{0.5mm}
\caption{Evaluation results on the RealBlur testing set. The best and the second scores in each column are highlighted in bold and underlined, respectively. Params and Time are calculated in (M) and (ms), respectively.
}
\begin{tabular}{l|cc|cc|cc}
\hline\hline
            & \multicolumn{2}{c|}{RealBlur-J} & \multicolumn{2}{c|}{RealBlur-R} & \multicolumn{2}{c}{RealBlur}\\
Model      & PSNR\, & SSIM\, & PSNR\, & SSIM\, & {Params\,} &{Time\,}  \\ \hline
DeblurGANv2~\cite{Kupyn_2019_ICCV}  & 29.69          & 0.870  & 36.44  & 0.935 & 68 & 60         \\ 
SRN~\cite{tao2018srndeblur} & 31.38  & 0.909          & 38.65  & 0.965 & \bf7 & 412 \\          
MPRNet~\cite{Zamir2021MPRNet}  & 31.76          &  \underline{0.922}          & \underline{39.31}           & \underline{0.972}  & 20 & 113       \\
{SPAIR~\cite{Purohit_2021_ICCV}} & {31.82} & {-} & {-} & - & -  & - \\
MIMO~\cite{MIMO}  & \underline{31.92}   & 0.919          & -          & -    & \underline{16} & \bf39      \\ 
\noalign{\hrule height 1.0pt}
Stripformer        &    \bf32.48      &   \bf0.929       &     \bf39.84       &     \bf0.974 &  20 & \underline{42}    \\
\hline\hline
\end{tabular}
\label{Tab:Realblur_eval}
\end{table}
\noindent\textbf{Qualitative Analysis.}
Figure~\ref{fig:Visualization_Result_GoPro} and Figure~\ref{fig:Visualization_Result_HIDE} show the qualitative comparisons on the GoPro test set and the HIDE dataset among our method and those in \cite{MIMO,gao2019dynamic,MT_2020_ECCV,Zamir2021MPRNet,Zhang_2019_CVPR}.
They indicate that our method can better restore images, especially on highly textured regions such as texts and vehicles.
It can also restore fine-grained information like facial expressions on the HIDE dataset. 
In Figure~\ref{fig:Visualization_result_Realblur}, we show the qualitative comparisons on the RealBlur test set among our method and those in \cite{MIMO,Kupyn_2019_ICCV,tao2018srndeblur,Zamir2021MPRNet}. 
This dataset contains images in low-light environments where motion blurs usually occur. 
As can be observed, our model can better restore these regions than the competing works.
In Figure~\ref{fig:Visualization_result_RWBI}, we show the qualitative comparisons on the RWBI~\cite{Zhang_2020_CVPR} dataset, which contains real images without ground truth. As shown, our model produces sharper deblurring results than the other methods~\cite{MIMO,Kupyn_2019_ICCV,tao2018srndeblur,Zamir2021MPRNet}.
Overall, the qualitative results demonstrate that Stripformer works well on blurred images in both synthetic and real-world scenes.

\begin{figure}[t!]
\centering
\includegraphics[width=1\columnwidth]{Figures/gopro_result.pdf}
\caption{Qualitative comparisons on the GoPro testing set. The deblurred results from left to right are produced by DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MTRNN~\cite{MT_2020_ECCV}, MIMO~\cite{MIMO}, MPRNet~\cite{Zamir2021MPRNet}, and our method, respectively.}
\label{fig:Visualization_Result_GoPro}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=1\columnwidth]{Figures/HIDE_result.pdf}
\caption{Qualitative comparisons on the HIDE dataset. The deblurred results from left to right are produced by DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MTRNN~\cite{MT_2020_ECCV}, MIMO~\cite{MIMO}, MPRNet~\cite{Zamir2021MPRNet} and our method, respectively.}
\label{fig:Visualization_Result_HIDE}
\end{figure}

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{4.5mm}
\caption{Component analysis of Stripformer trained and tested on the GoPro training and test sets.}
\begin{tabular}{cccc|c}
\hline
Intra-SA & Inter-SA & CPE &  & PSNR \\
\hline
 &       &     &       & 32.84 \\
      &  &     &       & 32.88 \\
 &  &     &       & 33.00 \\
 &  &  &       & 33.03 \\
 &  &  &  & 33.08 \\
\hline
\end{tabular}
\label{tab:ablation}
\end{table}

\subsection{Ablation Studies}
Here, We conduct ablation studies to analyze the proposed design, including component and computational analyses and comparisons against various modern attention mechanisms.


\noindent\textbf{Component Analysis.} 
Stripformer utilizes intra-strip and inter-strip attention blocks in horizontal and vertical directions to address various blur patterns with diverse magnitudes and orientations.
Table~\ref{tab:ablation} reports the contributions of individual components of Stripformer.
The first two rows of Table~\ref{tab:ablation} show the performance of using either the intra-strip or inter-strip attention blocks only, respectively. 
The two types of attention blocks are synergistic since combining them results in better performance, as given in the third row.
It reveals that Stripformer encoders feature both pixel-wise and region-wise dependency, more suitable to solve the regional-specific blurred artifacts. 
In the fourth row, the conditional positional encoding (CPE)~\cite{chu2021conditional} is included for positional encoding and boosts the performance, which shows it works better for arbitrary input sizes compared to the fixed, learnable positional encoding.  
The last row shows that the contrastive loss can further improve deblurring performance.

\begin{figure*}[t!]
\centering
\includegraphics[width=1\columnwidth]{Figures/RealBlur_result.pdf}
\caption{Qualitative comparisons on the RealBlur dataset. The deblurred results from left to right are produced by DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur}, MPRNet~\cite{Zamir2021MPRNet}, MIMO~\cite{MIMO} and our method.}
\label{fig:Visualization_result_Realblur}
\end{figure*}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{1mm}
\caption{Comparison among various attention mechanisms. FLOPs are calculated the same way as IPT. The inference time (Time) is measured based on the GoPro and HIDE datasets on average for full-resolution (1280x720) images.}
\begin{tabular}{c|ccccc}
\hline
Method &  IPT~\cite{IPT} & Swin~\cite{liu2021Swin} & CCNet~\cite{huang2019ccnet} & Twins~\cite{chu2021Twins} & Ours \\
\hline
Params (M) & 114 &  20   &   20    &   20   &   20   \\
FLOPs (G) &  32 &  6.7   &   7.4  &   6.5   &    6.9  \\
Time (ms) & -- &   48  &   47    &   43   &   48   \\
GoPro (PSNR) & 32.58 &  32.39   &  32.73 &  32.89  &  \bf33.08\\
HIDE (PSNR) & -- &  30.19  & 30.61  & 30.82   &  \bf31.03\\
\hline
\end{tabular}
\label{tab:transformer}
\end{table}

\noindent\textbf{Analysis on Transformer-based Architectures and Attention Mechanisms.}
We compare Stripformer against efficient transformer-based architectures, including Swin~\cite{liu2021Swin} and Twins~\cite{chu2021Twins}, and the attention mechanism CCNet~\cite{huang2019ccnet}. 
Note that these three compared methods are designed for efficient attention computation rather than deblurring.
For fair comparisons, we replace our attention mechanism in Stripformer with theirs using a similar parameter size and apply the resultant models to deblurring.
As reported in Table~\ref{tab:transformer}, the proposed intra-strip and inter-strip attention mechanism in Stripformer works better in PSNR than all the other competing attention mechanisms. 
The reason is that Stripformer takes the inductive bias of image deblurring into account to design the intra-strip and inter-strip tokens and attention mechanism.
It strikes a good balance among the image restorability, model size, and computational efficiency for image deblurring.
Besides, SWin works with a shifted windowing scheme, restricting its self-attention computed locally. Thus, it is not enough to obtain sufficient global information for deblurring like our intra- and inter-strip attention design. CCNet extracts pixel correlations horizontally and vertically in a criss-cross manner. Twins uses local window attention and global sub-sampled attention (down to the size of ). Both of them consider more global information, working better than SWin. Our strip-wise attention design can better harvest local and global blur information to remove short-range and long-range blur artifacts, performing favorably against all these compared attention mechanisms.

\begin{figure*}[t!]
\centering
\includegraphics[width=1\columnwidth]{Figures/RWBI_result.pdf}
\caption{Qualitative comparisons on the RWBI~\cite{Zhang_2020_CVPR}. The deblurred results from left to right are produced by MPRNet~\cite{Zamir2021MPRNet}, MIMO~\cite{MIMO}, DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur}, and our method.}
\label{fig:Visualization_result_RWBI}
\end{figure*}





