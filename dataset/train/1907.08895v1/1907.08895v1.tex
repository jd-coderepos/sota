\documentclass{bmvc2k}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{wrapfig}
\usepackage{enumitem} 





\title{An Efficient 3D CNN for Action/Object Segmentation in Video}

\addauthor{Rui Hou}{houray@gmail.com}{1}
\addauthor{Chen Chen}{chen.chen@uncc.edu}{2}
\addauthor{Rahul Sukthankar}{sukthankar@google.com}{3}
\addauthor{Mubarak Shah}{shah@crcv.ucf.edu}{4}

\addinstitution{Niantic Labs \\ Sunnyvale, CA, USA}
\addinstitution{University of North Carolina at Charlotte \\ Charlotte, NC, USA}
\addinstitution{Google Research \\ Mountain View, CA, USA}
\addinstitution{University of Central Florida \\ Orlando, FL, USA}

\runninghead{Hou \etal}{An Efficient 3D CNN for Action/Object Segmentation in Video}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}
\def\ie{\emph{i.e}\bmvaOneDot}
\begin{document}

\maketitle

\begin{abstract}
 
Convolutional Neural Network (CNN) based image segmentation has made great progress in recent years. However, video object segmentation remains a challenging task due to its high computational complexity. Most of the previous methods employ a two-stream CNN framework to handle spatial and motion features separately. In this paper, we propose an end-to-end encoder-decoder style 3D CNN to aggregate spatial and temporal information simultaneously for video object segmentation. 
To efficiently process video, we propose 3D separable convolution for the pyramid pooling module and decoder, which dramatically reduces the number of operations while maintaining the performance. Moreover, we also extend our framework to video action segmentation by adding an extra classifier to predict the action label for actors in videos.
Extensive experiments on several video datasets demonstrate the superior performance of the proposed approach for action and object segmentation compared to the state-of-the-art. 

\end{abstract}

\section{Introduction}
\label{sec:introduction}
Video object segmentation is a fundamental task in video content analysis. It aims to assign a foreground/background label for each pixel in a video frame. Compared to image segmentation, there are two major differences in video object segmentation.
First, the amount of data to be processed in video can be orders of magnitude greater than in image segmentation, placing greater constraints in terms of computational resources.
Second, the temporal domain provides additional information about object  motion that can judiciously be exploited to improve segmentation performance.

Video object segmentation approaches can be divided into two categories -- semi-supervised and unsupervised. The semi-supervised approaches \cite{Maninis2018Video,Voigtlaender2017Online} assume the foreground object in the first frame of test video is provided and the task is to segment the specified object in the following frames. On the other hand, the unsupervised approaches \cite{khoreva2016learning,caelles2017one,Song_2018_ECCV,tokmakov2017learning} segment foreground objects without any prior knowledge, which is more suited for practical use.  

In this paper, we approach the video object segmentation in the \textbf{unsupervised setting}. We propose an end-to-end encoder-decoder style 3D CNN based method to solve the video object segmentation problem efficiently. Its encoder is composed of a R2plus1D (R2P1D) network \cite{r2plus1d_cvpr18} and a 3D pyramid pooling module. Its decoder is designed to recover both spatial and temporal dimensions to generate an output of the same size as the input clip. Instead of the popular two-stream framework, we adopt 3D CNN to aggregate spatial and temporal information. To efficiently process video, we propose 3D separable convolution for the pyramid pooling module and decoder, which dramatically reduces the number of operations while maintaining the performance. Additionally, we also extend our framework to video action segmentation by adding an extra classifier to predict the action label for actions in videos.
The main contributions of this paper are three-fold:
\begin{enumerate}\item We propose a simple yet efficient 3D CNN framework for action/object segmentation in videos. To the best of our knowledge, this is the first time 3D CNN has been explored for video object segmentation to simultaneously model the spatial and temporal information in a video.
\item We evaluate our method on video object segmentation and action segmentation benchmarks and demonstrate state-of-the-art performance.
\item We conduct a detailed ablation study to identify the relative contributions of the individual components.
\end{enumerate}

\section{Related Work}
\label{sec:related_work}
Convolutional neural networks have been demonstrated to achieve excellent results in video action understanding \cite{lecun2015deep, karpathy2014large, lstm_ng}. Video should not be treated as a set of independent frames, since the connection between frames provides extra temporal information for understanding. Simonyan \etal \cite{2stream_cnn_simonyan_2014two} propose the two-stream CNN approach for action recognition, which consists of two CNNs taking image and optical flow as input respectively. To avoid computing optical flow separately, Tran \etal \cite{c3d} propose 3D CNN for large scale action recognition. Hara \etal \cite{hara2018can} apply 3D convolution on ResNet structure. Carreira \etal \cite{carreira2017quo} propose I3D by extending Inception network from 2D to 3D and including an extra optical flow stream. Tran \etal \cite{r2plus1d_cvpr18} and Xie \etal \cite{xie2018rethinking} factorize 3D CNN to treat spatial and temporal information separately to reduce the computational cost while keeping the performance. However, to the best of our knowledge, we are the first ones to exploit 3D CNN for video object segmentation.


\textbf{CNN-based segmentation.} The success of CNN-based approaches for image classification \cite{krizhevsky2012imagenet, he2016deep} have led to dramatic advances in image segmentation \cite{long2015fully, noh2015learning}. Many of the segmentation approaches leverage recognition models trained on ImageNet and replace the fully-connected layers with 11 kernel convolutions to generate dense (pixel-wise) labels. Recently, the encoder-decoder style network architecture, such as SegNet \cite{badrinarayanan2015segnet} and U-Net \cite{ronneberger2015u}, has been the main stream design for semantic segmentation. Moreover, pyramid pooling \cite{zhao2017pyramid, chen2017rethinking} and dilated convolution \cite{yu2015multi, chen2017rethinking, chen2018deeplab} are effective techniques to improve the segmentation accuracy. Our 3D CNN also builds upon the encoder-decoder structure for video object segmentation. 

\textbf{Video object segmentation.} Video object segmentation \cite{ke2007event, yan2008learning} aims to delineate the foreground object(s) from the background in each frame. Semi-supervised segmentation pipelines \cite{Maninis2018Video,Voigtlaender2017Online} assume that the segmentation mask of the first frame in the sequence during testing is given, 
and exploit temporal consistency in video sequences to propagate the initial segmentation mask to subsequent frames.

In the more challenging unsupervised setting, as we address in this paper, no object mask is provided as initialization during the test phase.
Unsupervised segmentation has been addressed by several variants of CNN-based models, such as the two-stream architecture \cite{khoreva2016learning,caelles2017one}, recurrent neural networks \cite{Song_2018_ECCV, tokmakov2017learning} and multi-scale feature fusion \cite{tokmakov2016learning, Song_2018_ECCV}. These approaches generally perform much better than traditional clustering-based pipelines \cite{Chang2013A}. The core idea behind such approaches involves leveraging motion cues explicitly (via optical flow) using a two-steam network \cite{jain2017fusionseg,tokmakov2016learning,tokmakov2017learning}, and/or employing a memory module to capture the evolution of object appearance over time \cite{Song_2018_ECCV,tokmakov2017learning}.


\textbf{Action segmentation.} Action segmentation provides pixel-level localization for actions (\ie action segmentation maps), which are more accurate than bounding boxes for action localization. Lu \etal \cite{lu2015human} propose supervoxel hierarchy to enforce the consistency of the human segmentation in video. Gavrilyuk \etal \cite{gavrilyuk2018actor} infer pixel-level segmentation of an actor and its action in video from a
natural language input sentence. 


While we take inspiration from these works, we are the first to present a 3D CNN based deep framework for video object segmentation in a fully automatic manner. Moreover, our proposed method is designed with computational efficiency in mind to enable practical applications for video segmentation.




\section{Generalizing CNN for Dense Prediction from 2D to 3D}
\label{sec:method}
Generalizing CNN framework from  images (2D) to videos (spatio-temporal 3D) involves more work than simply adding one more dimension. A key challenge is due to the asymmetry between space and time. To address the change in apparent size of an object due to perspective, image pipelines crop and reshape images to a fixed size. One cannot do the same with videos since  input videos and the duration of an object track or action in an untrimmed video can vary widely in the temporal dimension. Since an entire video cannot be rescaled to  a fixed size, approaches typically process video as sequences of short (\eg 8-frame) clips.
In this section, we delve into the details of our network design to address these challenges.




\subsection{3D separable convolution with dilation}
\label{sec:3d_separable_conv}
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{3d_conv_compare.pdf}
\caption{Comparison between standard 3D convolution, R2plus1D and 3D separable convolution. R2plus1D factorizes 3D convolution into spatial and temporal convolutions. 3D separable convolution is composed of two convolution modules -- channel-wise convolution and point-wise convolution. 
}
\label{fig:3d_conv_compare}
\end{figure}
3D CNN training is much less efficient than its 2D counterpart, since 3D kernels and feature maps have more parameters. To address pixel-level prediction in video more efficiently, we propose to use 3D separable convolution in lieu of the standard 3D convolution.
3D separable convolution factorizes a standard 3D convolution into a channel-wise 3D convolution and a point-wise 3D convolution with  kernel. The channel-wise convolution applies a specific filter to each input channel, then point-wise convolution combines the outputs of the channel-wise convolution via a  convolution. This factorization drastically reduces computation and model size.

A standard 3D convolutional operator takes a  feature map  as input and produces a  feature map , where ,  and  are the height, width and number of frames (duration) in a clip, respectively.  and  are the number of input/output channels. 
3D convolution kernel has shape , 
where ,  and  are the spatial and temporal dimensions of the kernel. 
The computation complexity of a standard 3D convolution is:

On the other hand, 3D separable convolution is a two-step process. First channel-wise convolution generates the intermediate feature map by applying a specific filter to each input channel ( channels in total).  The computational complexity of channel-wise convolution is:

where the kernel of channel-wise convolution has size . Then point-wise convolution projects intermediate feature map to the final output with computational complexity:


The computational cost of the standard 3D convolution is the multiplication of the feature map dimension (), input channel dimension , output channel dimension  and kernel dimension (); while in 3D separable convolution, channel-wise convolution is irrelevant to output filters and point-wise convolution is isolated from kernel dimension. 

Therefore, the computational cost reduction of 3D separable convolution is:


For example, a 3D separable convolution with  kernel dimension and  input/output size only needs about  of computational resource compared to that of a standard 3D convolution, leading to significant inference computation reduction.

Meanwhile, in R2plus1D convolution, the standard 3D convolution is factorized into spacial and temporal convolutions. The input feature map with  channels passes through spatial convolution with kernel shape  and generates a  channels intermediate feature map. Then, the intermediate result goes through the temporal convolution with kernel shape  and generates the final output. Compared to standard 3D convolution, the computational cost reduction of R2plus1D convolution is:

The computational reduction is depends on the number of intermediate filters . According to \cite{r2plus1d_cvpr18}, the number of intermediate filters is set as  to keep the number of parameters the same as standard convolution.

\textbf{Dilated convolution.} Dilation rate  is an attribute of convolution operation. It has the ability to increase the receptive field size, while maintaining the computational cost by skipping  entities per valid one in one of dimensions (, , ). Adding dilation rates helps capture multi-scale feature representations. Specifically in 3D separable convolution, the dilation rate is only applied to channel-wise convolution, since kernel size of point-wise convolution is fixed at . Moreover, due to the asymmetry between space and time in video, dilation rate in 3D convolution can be divided into two parts -- spatial rate  and temporal rate . By using the dilation rate, the channel-wise convolution can be expressed as follows:

As shown in Eq.~\ref{eq:dilated_conv}, the computational cost does not change by adding the dilation rate. 



\subsection{Network architecture}
\label{sec:network}


\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{segmentation-pipeline_v5.pdf}
\caption{The network architecture of our method for video object segmentation. It has three components: an encoder (feature extractor), a pyramid pooling module and a decoder to recover the spatial-temporal details gradually. The encoder, shown in the top-left, takes pixel values as input, extracts features layer by layer, and generates rich contextual feature as the final output. Moreover, its intermediate results are merged with the decoder module. The pyramid pooling module connects the encoder and decoder modules. It varies the receptive field size by modifying the spatial stride. ``S. 3D Conv'' indicates the 3D separable convolution. ``Frame level features'' block is composed by a reduced average pooling and duplication to generate the frame level features with the same dimension as the other branches.
}
\label{fig:segmentation}
\end{figure}


The proposed 3D CNN architecture for action/object segmentation in video is illustrated in Figure~\ref{fig:segmentation}.
The network builds upon an encoder-decoder structure for image semantic segmentation. 
A video is divided into 8-frame clips as input to the network. In the encoder module, 3D convolution are performed. To capture higher level information, the spatial and temporal sizes are reduced. In order to generate the pixel-wise segmentation map for each frame in the original size, 3D upsampling is used in the decoder module to increase the resolution of feature maps. To capture spatial and temporal information at different scales, a concatenation with the corresponding feature maps from the encoder module is employed after each 3D upsampling layer. Finally, a softmax cross entropy loss layer is used for pixel-wise prediction (\ie~background or object foreground) for each frame in a clip.

In the encoder, we adopt R2plus1D~\cite{r2plus1d_cvpr18} as feature extractor to leverage its pre-trained model on large scale action recognition dataset. We modify the last three groups of convolutional layers by adjusting the dilation rate to keep temporal stride as .
We also insert a 3D pyramid pooling before the decoder. 
In the decoder, the up-sampling layer group is composed of a tri-linear interpolation layer, a 3D separable convolution layer as well as a feature concatenation layer incorporating encoder feature. 


\textbf{3D Pyramid Pooling.}
Compared to the 2D counterpart \cite{chen2018deeplab}, pyramid pooling in 3D is more challenging. In spatial domain, multi-scale information should be captured. In temporal domain, detailed motion information should be preserved as well.
Therefore, our 3D pyramid pooling has 5 branches. The details are listed as follows:
\begin{enumerate}\item 3D separable convolutions with kernel size  and different spatial dilation rates . The convolutions with multiple spatial dilation rates are able to capture multi-scale information. 

\item 3D separable convolution with kernel size  -- a default layer for dense prediction.

\item There are two steps to get the frame feature. First, input is average pooled with kernel . For the average pooling output, its spatial dimension is , while its temporal length and filter size are the same as those of the input. Then the average pooling output is upscaled to the input size.
\end{enumerate}

By concatenating the  branches together, and applying a  convolution, the final output is obtained and fed into the decoder module.


\section{Experiments}
\label{sec:experiments}
\textbf{Implementation details.} 
To verify the effectiveness of the proposed 3D CNN framework for action/object segmentation in videos, we evaluate our approach on two video object segmentation datasets -- DAVIS'16 \cite{Perazzi2016} and Segtrack-v2 \cite{li2013video}, and a video action segmentation dataset -- J-HMDB~\cite{Jhuang:ICCV:2013}. We adopt R2plus1d encoder pretrained on Kinetics dataset~\cite{zisserman2017kinetics} and leverage the Adam optimizer with initial learning rate , exponential decay rate  and decay step  epoch. The model is trained for  epochs with exponential decay.
In 3D spatial pyramid pooling module, a large crop size is required to ensure the convolution kernel with spatial dilation rate is effective; otherwise, the filter weights with large dilation rate are mostly applied to the padded zero region. Therefore, We employ a spatial crop size of  during both training and testing.
During training, we randomly scale the resolution of input clips with ratio  and apply a random horizontal flip.

\textbf{Experiments on DAVIS'16.}
Densely Annotated Video Segmentation 2016 (DAVIS'16) dataset is a benchmark dataset for video object segmentation. It consists of 50 videos with 3455 annotated frames. Consistent with most prior work, we conduct experiments on the 480p videos with a resolution of  pixels. 30 videos are used for training and 20 for validation.
We adopt the same evaluation setting in \cite{Perazzi2016}. There are three parts. \textbf{Region Similarity} , which is obtained by IoU (Intersection over Union) between the prediction and the ground-truth segmentation map. \textbf{Contour Accuracy}  measures the contours accuracy. \textbf{Temporal Stability}  tracks the temporal consistency in a video. For the first two evaluation, we report the mean, recall and decay. For the third one, we report the average.

\begin{table}[!hbp]

\centering
\footnotesize
\begin{tabular}{c|c|cccccccccc}

\hline

\multicolumn{2}{c|}{\multirow{2}{*}{Measure}}    & MotAdapt  & LSMO  & PDB   & ARP  & FSEG   & LMP & FST    & CUT  & NLC & Ours \\
\multicolumn{2}{c|}{}           & \cite{siam2018video}  & \cite{tokmakov2019learning} & \cite{Song_2018_ECCV} & \cite{koh2017primary} & \cite{jain2017fusionseg}  & \cite{tokmakov2016learning}  & \cite{keuper2015motion}   & \cite{faktor2014video}  & \cite{fragkiadaki2012video} & \\ 
\hline
\multirow{3}{*}{}  
& Mean                & 77.2  & 78.2  & 77.2  & 76.2	& 70.7	
                                & 70.0	& 55.8	& 55.2	& 55.1  & \textbf{78.3}  \\
& Recall              & 87.8  & 89.1  & 90.1  & \textbf{91.1}	& 83.5	
                                & 85.0	& 64.9	& 57.5  & 55.8  & \textbf{91.1} \\
& Decay             & 5.0   & 4.1   & 0.9   & 7.0	& 1.5	
                                & 1.3	& \textbf{0.0}	& 2.2   & 12.6  & 2.3 \\
\hline
\multirow{3}{*}{}  
& Mean                & \textbf{77.4}  & 75.9  & 74.5  & 70.6	& 65.3	
                                & 65.9	& 51.1	& 55.2	& 52.3  & 77.2 \\
& Recall              & 84.4  & \textbf{84.7}  & 84.4  & 83.5  & 73.8	
                                & 79.2	& 51.6	& 61.0	& 51.9  & \textbf{84.7} \\
& Decay             & 3.3   & 3.5   & \textbf{-0.2}  & 7.9   & 1.8	
                                & 2.5	& 2.9	& 3.4	& 11.4  & 4.9 \\
\hline

& Mean              & 27.9  & \textbf{21.2}  & 29.1  & 39.3	& 32.8	
                                & 57.2	& 36.6	& 27.7	& 42.5  & 22.0\\
\hline
\end{tabular}
\caption{Overall results of region similarity (), contour accuracy () and temporal stability () for different approaches.  means the higher the better, and  means the lower the better.}
\label{tab:davis_overall}
\end{table}
We compare our results with several unsupervised approaches, since our approach does not require any manual annotation or prior information about the object to be segmented.
We cannot compare directly to semi-supervised approaches that require the ground truth segmentation map in the first frame of each test video to be given. Table~\ref{tab:davis_overall} summarizes the performance of our method against the state-of-the-art unsupervised approaches on DAVIS'16. Our method achieves the best performance in all performance metrics. Compared to ARP \cite{koh2017primary}, the previous state-of-the-art unsupervised approach, our method achieves 5\% gain in contour accuracy () and 15\% gain in temporal stability (), demonstrating that 3D CNN can effectively take advantage of the temporal information in video frames to achieve temporal segmentation consistency. We also present the qualitative results on four video sequences in Figure~\ref{fig:mtcnn_example}.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\linewidth]{seg_example.pdf}
\caption{Qualitative results of the proposed (Ours) approach (red), ARP (yellow), LVO (cyan) and FSEG (magenta) on selected frames from DAVIS dataset.}
\label{fig:mtcnn_example}
\end{figure}

\textbf{Experiments on Segtrack-v2.} SegTrack-v2 \cite{li2013video} contains  video clips with 24 objects and 947 frames. Pixel-level object mask on each frame is provided. We adopt the same evaluation setting in \cite{khoreva2016learning} and report the mean Intersection over Union. For videos with multiple instances with individual ground-truth segmentation mask, we group them as a single foreground for evaluation. Compared with other unsupervised video object segmentation methods, our proposed approach outperforms all of them (see table \ref {tab:segtrack}).
\begin{table}[]
\centering
\small
\begin{tabular}{l|ccccc}
\hline
Method      & FST \cite{papazoglou2013fast}  & KEY \cite{lee2011key}  & LSMO \cite{tokmakov2017learning} & FSEG \cite{jain2017fusionseg} & Ours \\
\hline
mean IoU    & 53.5  & 57.3  & 53.7  & 61.4  & {\bf 62.1} \\   
\hline
\end{tabular}
\caption{Video object segmentation results on Segtrack-v2 dataset. We compare the performance of our approach with other state-of-the-art unsupervised approaches.}
\label{tab:segtrack}
\end{table}


\textbf{Experiments on J-HMDB.}
The J-HMDB dataset consists of 928 videos with 21 different actions. There are three train-test splits and the evaluation is done on the average results over the three splits. We leverage the mask annotation provided from J-HMDB dataset and train our semantic segmentation pipeline to segment the foreground action. Then the region with maximum area is selected through connected component \cite{he2009fast}. On the final feature map of each frame, we crop a box to tightly surround the selected foreground region and resize the box into a fixed shape. Finally, the cropped feature map goes through a classifier to predict action classes. We use softmax cross entropy loss to back propagate gradients for all the weights. For evaluation, we followed the metrics in \cite{gavrilyuk2018actor}. Mean IoU is computed as the average over the IoU of each test sample. In addition, frame-level mean Average Precision (mAP) is evaluated as well. Since bounding boxes detection can be obtained by selecting the tightest rectangle region enclosing the segmentation mask, we are also able to compare with the state-of-the-art action detection approaches. 
\begin{table}[!htbp]
\centering
\small
\begin{tabular}{lccc}
\hline
                                                & mean IoU      & frame-mAP ()\\
\hline
Lu \etal~\cite{lu2015human}                     & 48.8          & -- \\
Gavrilyuk \etal~\cite{gavrilyuk2018actor}       & 54.2          & -- \\
\hline
Kalogeiton \etal \cite{kalogeiton2017action}    & --            & 65.7 \\
Duarte \etal \cite{duarte2018videocapsulenet}   & --            & 64.6 \\
Hou \etal \cite{hou2017tube}                    & --            & 61.3 \\
\hline
\textbf{Ours}                                   & \textbf{68.1} & \textbf{68.9} \\
\hline
\end{tabular}
\caption{Comparison with the state-of-the-art action segmentation and detection approaches on J-HMDB. The first part of the table shows the mean Intersection-over-Union (IoU) of action segmentation approaches and the second part shows the frame level mean Average Precision (mAP) of CNN based action detection approaches.}
\label{tab:jhmdb-segmentation}
\end{table}

Table~\ref{tab:jhmdb-segmentation} reports the action segmentation/detection results of our method and the state-of-the-art approaches. It is evident that our method outperforms these methods considerably in evaluation metrics. Figure~\ref{fig:jhmdb_seg_results} presents both action segmentation (in red) and bounding boxes detection (in yellow) results on several video sequences from J-HMDB.


\begin{figure}[!t]
\centering
\includegraphics[width=0.98\linewidth]{jhmdb_segmentation.pdf}
\caption{Action segmentation and detection results obtained by our method on the J-HMDB dataset. Red pixel-wise segmentation maps show the predictions, and the yellow boxes show the bounding boxes generated from the segmentation maps.}
\label{fig:jhmdb_seg_results}
\end{figure}

\iffalse
Figure~\ref{fig:seg_ind} shows the quantitative results per video sequence of our approach and the next three top performing methods on DAVIS dataset: ARP \cite{koh2017primary}, LVO \cite{tokmakov2017learning} and FSEG \cite{jain2017fusionseg}. Our approach performs the best on low contrast videos including Blackswan, Car-Roundabout and Scooter-Black and achieves competitive results on other videos. Figure~\ref{fig:mtcnn_example} presents the qualitative results on four video sequences. In the first row, our results are the most accurate. Our method is the only one which can detect the rims. In the second row, ARP performs the best in suppressing background. However, only our approach detects both legs. The third row shows that only our method is able to accurately segment the tail of the camel. The last row is a very challenging video because of the smoke and small initial size of the car. ARP misses part of the car, while LVO and FSEG mis-classify part of background as moving object. However, our method segments out the car completely and accurately from the background smoke in the scene. 
\fi



\iffalse
\begin{figure*}[!bt]
\footnotesize
\centering

\begin{tikzpicture}
\begin{axis}[
ybar,
enlargelimits=0.015,
bar width=2pt,
width=0.9\linewidth,
height=0.24\linewidth,
x tick label style={rotate=30, anchor=east},
xticklabels={Blackswan, Bmx-Trees, Breakdance, Camel,
Car-Roundabout, Car-Shadow, Cows, Dance-Twirl, Dog,
Drift-Chicane, Drift-Straight, Goat, Horsejump-High,
Kite-Surf, Libby, Motocross-Jump, Paragliding-Launch,
Parkour, Scooter-Black, Soapbox},
xtick=data,
ylabel=Mean Jaccard index (),
legend style={at={(0.76,1.04)}, anchor=north,legend columns=-1},
]

\addplot coordinates {(0, 92.3) (1, 52.9) (2, 66.9) (3, 86.8) (4, 92.1) (5, 91.6) (6, 89.9) (7, 72.2) (8, 86.3) (9, 77.5) (10, 81.6) (11, 79.3) (12, 74.0) (13, 65.3) (14, 66.8) (15, 79.6) (16, 53.2) (17, 78.1) (18, 80.8) (19, 85.0)};
\addplot coordinates {(0, 88.1) (1, 49.9) (2, 76.2) (3, 90.3) (4, 81.6) (5, 73.6) (6, 90.8) (7, 79.8) (8, 71.8) (9, 79.7) (10, 71.5) (11, 77.6) (12, 83.8) (13, 59.1) (14, 65.4) (15, 82.3) (16, 60.1) (17, 82.8) (18, 74.6) (19, 84.6)};
\addplot coordinates {(0, 74.1) (1, 43.3) (2, 37.1) (3, 88.1) (4, 88.6) (5, 92.0) (6, 90.2) (7, 81.0) (8, 88.7) (9, 63.9) (10, 84.9) (11, 82.3) (12, 82.4) (13, 64.6) (14, 69.0) (15, 80.5) (16, 62.2) (17, 84.9) (18, 71.8) (19, 81.3)};
\addplot coordinates {(0, 81.2) (1, 52.2) (2, 51.2) (3, 83.6) (4, 90.7) (5, 89.6) (6, 86.9) (7, 70.4) (8, 88.9) (9, 59.6) (10, 81.1) (11, 83.0) (12, 65.2) (13, 39.2) (14, 58.4) (15, 77.5) (16, 57.1) (17, 76.0) (18, 68.8) (19, 62.4)};

\legend{Ours,ARP \cite{koh2017primary}, LVO \cite{tokmakov2017learning},FSEG \cite{jain2017fusionseg}}
\end{axis}
\end{tikzpicture}
\caption{Comparison of Mean Jaccard index () of different approaches on each of the sequences independently.}
\label{fig:seg_ind}
\end{figure*}
\fi






\subsection{Ablation Study}
To better understand the contribution of each component in our proposed approach, we conduct video object segmentation on DAVIS'16 with different settings, summarized in Table~\ref{tab:ablation} and Table~\ref{tab:var_convs}.

\textbf{Dilation rate.} We adopt R2Plus1D pre-trained model in our approach, and increase the feature map's temporal size of last two convolution layer group by specifying the dilation rate. 
As shown in first section of Table~\ref{tab:ablation}, the performance is boosted by increasing the number of frames in the final feature map. However, too large feature map (8 frames) will cause out of memory error, that is the reason we stop at 4 frames.



\textbf{3D pyramid pooling.} As shown in the second section of Table~\ref{tab:ablation}, inserting 3D pyramid pooling module improves the segmentation accuracy by over 5\%. We experiment multiple branches with different spatial dilation rates (as noted in the bracket). According to the experimental results, including more branches with larger receptive field ( branches) or larger temporal stride is not helpful. When the receptive field size is close to or larger than the feature map size, most of the filters cannot capture any useful information, since they only cover padded zeros instead of valid area. Segmentation accuracy further improves when frame-level features are added.



\begin{table}[!htbp]
\centering
\begin{tabular}{c|c|c}
\hline
Final feature map dim.  & 3D Pyramid Pooling    & -Mean \\
\hline
     & --                    & 64.5 \\
     & --                    & 69.1 \\
     & --                    & 71.3 \\
\hline
     & (6, 12, 18)           & 78.1 \\
     & (6, 12, 18) + FF      & 78.3 \\
     & (6, 12, 18, 24)       & 77.9 \\
     & (8, 16, 24)           & 74.9 \\
\hline
\end{tabular}
\caption{Ablation study of our method for video object segmentation on DAVIS-16. 
In the first horizontal section of the Table, we investigate various temporal size of the final feature map by increasing the temporal dilation rate in the last two convolutional layer groups. In the second section of the Table, we fix feature map dimensions (including temporal dimension) and explore different settings in 3D pyramid pooling module (shown in the second column), which includes various receptive fields and whether frame-level features (FF) are included or not. Specifically, the numbers in the parentheses () indicate spatial dilation rates of branches. We compare  performance with different sizes of feature maps,  
with or without 3D pyramid pooling layer. 
Mean IoU is used as evaluation metric as shown in third column. 
}
\label{tab:ablation}
\end{table}

\textbf{3D separable convolution}. The proposed pipeline leverages 3D separable convolution in pyramid pooling and decoder. We perform an experiment by replacing all the 3D separable convolutions with R2plus1D convolutions and standard 3D convolutions.
The comparison of performance and computational cost is shown in Table~\ref{tab:var_convs}. All the experiments are carried out on a workstation with a NVIDIA Titan XP GPU and PyTorch. We only take 3D pyramid pooling and decoder part during inference into computation cost, since all of them share the same pre-trained model. ``Operations'' counts the total number of additions and multiplications. ``GPU Mem.'' is the size of allocated GPU memory. With the same input (a  frames clip with resolution ), 3D separable convolution greatly reduces the computational cost without sacrificing the performance.


\begin{table}[!htbp]
\centering
\begin{tabular}{l|ccc}
\hline
Conv. Type          & Operations    & GPU Mem.  & -Mean \\
\hline
Standard 3D Conv.   & 136 Billion   & 255 MB    & 77.1\\
R2plus1D Conv.      & 136 Billion   & 256 MB    & 77.6\\
3D Separable Conv.  & 6 Billion     & 11 MB     & 77.4\\
\hline
\end{tabular}
\caption{The comparison of performance and computational cost of different 3D convolutions. 3D separable convolution is able to reach the similar accuracy with only 5\% of parameters. In R2plus1D, we adopt the settings in \cite{r2plus1d_cvpr18}, which sets the number of intermediate filters to be  the same as the number of parameters of the standard 3D convolution.} \label{tab:var_convs}
\end{table}


\section{Conclusion}
\label{sec:conclusion}
This paper proposes a fully convolutional 3D CNN pipeline for action/object segmentation in video. The approach leverages a model pre-trained on large-scale action recognition task as an encoder to enable us to perform unsupervised video object segmentation (\ie generate pixel-level object masks without initialization). We also use separable filters to significantly reduce the computational burden of the standard 3D convolutions. Extensive experiments on several benchmark datasets demonstrate the strength of our approach for spatio-temporal action segmentation as well as video object segmentation compared with the state-of-the-art approaches.

\section{Acknowledgements}
This research is based upon work supported in part by the National Science Foundation under Grants No.\ 1741431 and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R\&D Contract No.\ D17PC00345. The views, findings, opinions, and conclusions or recommendations contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, ODNI, IARPA, or the U.S.\ Government. The U.S.\ Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.



\bibliographystyle{abbrv}
\bibliography{egbib}
\end{document}