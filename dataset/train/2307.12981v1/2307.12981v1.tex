
\appendix
\setcounter{section}{0}


\section{3D-Language Data}
\subsection{Prompts}
In figure \ref{fig:prompt1} and \ref{fig:prompt2}, we show two exemplar prompts for generating task decomposition data and 3D-assisted dialog data. Specifically, they are generated using the boxes-demonstration-instruction method described in the paper. For each sample in the few shot samples, the "content" has the bounding boxes of the scenes, and the "response" refers to human-written responses for demonstration. For query, it consists of the bounding boxes of scenes that we query the ChatGPT to give us responses. 
\begin{figure}[htbp]
    \centering
    \vspace{-2em}
    \includegraphics[width=0.55\linewidth]{figures/task.pdf}
    \caption{Prompts on generating task decomposition data}
    \label{fig:prompt1}
    \vspace{-2em}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/chat.pdf}
    \caption{Prompts on generating 3D-assisted dialog data}
    \label{fig:prompt2}
\end{figure}

\subsection{Data Distribution}
In figure \ref{fig:data}, we show the distribution of our data.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/statistics.png}
    \caption{Data distribution on different types for our 3D-language data}
    \label{fig:data}
\end{figure}

\section{Experiments}
\subsection{Implementation Details}
Using Pretrained BLIP-2 as backbones, we train 3D-LLMs for 100K steps, and validate every 1K step. We run the models on 8 nodes, where each node has 8 V100s. The batch size is 16 for each node.  The AdamW optimizer is used, with  = 0.9,
 = 0.999, and a weight decay of 0.05. Additionally, we apply a linear warmup of the learning
rate during the initial 1K steps, increasing from  to , followed by a cosine decay with a minimum learning rate of 0.

3D-LLMs based on pretrained flamingo are trained using the AdamW optimizer with global norm
clipping of 1, no weight decay for the perceiver resampler and weight decay of 0.1 for the other
trainable parameters. The learning rate is increased linearly from 0 to  up over the first 5000 steps
then held constant for the duration of training. The model is trained on 8 A100s. The batch size is 16. We use Distributed Data Parallel (DDP) to train the model.
\subsection{Held-Out Evaluation}
\subsubsection{3DMV-VQA}
We finetune our pretrained 3D-LLMs on the 3DMV-VQA dataset and compare with baseline models.

\textbf{Baselines \& Evaluation Metrics} We include representative baseline models on the benchmark. 
\textbf{NS-VQA} is the neuro-symbolic method that first extracts object proposals and then perform neuro-symoblic reasoning. \textbf{3D-Feature + LSTM} is a baseline that extracts 3D features first and then concatenates with LSTM to output final answers. \textbf{3D-CLR} is the state-of-the-art method that extracts 3D feature fields first, and then perform neuro-symbolic reasoning. In addition to these baselines, we also add 2D VLMs baselines like we did for ScanQA.


\begin{table*}[htbp]
	\begin{center}\small
 
	\begin{tabular}{lccccc}
	\hline 
      Methods   & Concept & Counting & Relation& Comparison & Overall\\ 
     
    \hline 
NS-VQA* &59.8&21.5&33.4&61.6 &38.0\\ 
        3D-Feature+LSTM  &61.2 &22.4 & 49.9 & 61.3 &48.2\\ 
        3D-CLR*  & 66.1 & \textbf{41.3} & 57.6&  \textbf{72.3} & 57.7\\ 
    \hline 
       flamingo-SingleImage &58.7 & 18.5 & 38.4 &60.1 & 40.3\\
       flamingo-MultiView &60.0 & 18.3 & 40.2 & 61.4 & 41.6\\
       BLIP-SingleImage & 58.0 & 20.4 & 42.3 & 62.3 & 43.1\\
       BLIP-MultiView & 61.9 & 21.1 & 48.0 & 62.3 & 47.1\\
    \hline 
        3D-LLM (flamingo) & \textbf{68.9} & 32.4 & \textbf{61.6} & 68.3 & \textbf{58.6}\\
        3D-LLM (BLIP5-opt) & 63.4 & 30.7 & 57.6 & 65.2 & 54.9\\
        3D-LLM (BLIP2-flanT5)  & 68.1 & 31.4 & 55.1 & 69.7 & 54.6\\
    \hline 
	\end{tabular}
	\end{center}
        \vspace{-1em}
	\caption{Experimental results on 3DMV-VQA dataset. * denotes using explicit object representations and neuro-symbolic reasoning. }
    \label{tab:3dmv-vqa}
\end{table*}

\textbf{Result Analysis} Table \ref{tab:3dmv-vqa} shows the performances on 3DMV-VQA. We can see that 3D-LLMs outperform state-of-the-art baseline model in the question types of concept and relation, and also in the overall performance. Our model also outperforms 3D-Feature+LSTM, demonstrating the power of LLMs over vanilla language models with similar 3D features as inputs. Overall, 3D-based methods outshine 2D-based versions of the methods. Our 3D-LLMs outperform their corresponding 2D VLMs with image input, further demonstrating the importance of 3D representations for 3D-LLMs.

\subsubsection{3D Grounding (Referring) on ScanRefer}
    In order to examine 3D-LLMs' 3D localization abilities, we carry out a held-out experiment on ScanRefer benchmark. Specifically, ScanRefer benchmark requires the models to output object locations given a referring sentence of the objects. We finetune 3D-LLMs on ScanRefer training set and report the results on ScanRefer validation sets.

    \textbf{Baselines} We include the baseline models in the original ScanRefer paper. Specifically, \textbf{OCRand(OracleCatRand)} use an oracle with ground truth bounding boxes of objects, and predict the box
by simply selecting a random box that matches the object category. \textbf{Vote+Rand(VoteNetRand)} uses the predicted object proposals of the
VoteNet \cite{Qi2019DeepHV} backbone and selects a box randomly with the correct semantic class label. \textbf{SCRC \& One-stage} are 2D image baselines for referring expression comprehension by extending SCRC \cite{hu2016natural} and One-stage \cite{yang2019fast} to 3D using back-projection.
Since 2D referring expression methods operate on a single image frame, we construct a 2D training set by using the recorded camera pose associated with each
annotation to retrieve the frame from the scan video with the closest camera
pose. At inference time, we sample frames from the scans (using every 20th
frame) and predict the target 2D bounding boxes in each frame. We then select
the 2D bounding box with the highest confidence score from the bounding box
candidates and project it to 3D using the depth map for that frame. \textbf{ScanRefer} uses a pretrained VoteNet
backbone with a trained GRU for selecting a matching bounding box.

    \textbf{Evaluation Metrics} To evaluate the performance of our method, we measure the thresholded
accuracy where the positive predictions have higher intersection over union (IoU)
with the ground truths than the thresholds. Similar to work with 2D images, we
use ACC@kIoU as our metric, where the threshold value k for IoU is set to 0.25. In addition to that, we also report the Average IoUs of our 3D-LLMs. Since our model focuses on 3D localization of objects, we also report the distances from the centers of predicted bounding boxes to the ground-truth bounding boxes.
    
    \textbf{Result Analysis} In Table \ref{tab:grounding}, we show the results on ScanRefer. As we can see, our 3D-LLMs can have decent performances on grounding and referring, and outperform most of the baselines, showing that 3D-LLMs have the ability of 3D localization. 
    Notably, the baseline models use ground-truth bounding boxes, or a pre-trained object detector to propose bounding boxes and classes for object proposals. Then, they use scoring modules to vote for the most likely candidate. Our method does not use any explicit object proposal module or ground truth bounding boxes, but outputs the locations of the bounding boxes directly using LLM losses for predicting tokens, while still outperforming most of the baselines. We could also see from the Avg. Dist metric the bounding boxes we predict is very close to the ground-truth brounding boxes. 

\begin{table}[!ht]
    \setlength{\tabcolsep}{0.17em}
    \centering
    \small
    \begin{tabular}{l|lllllllll}
        \hline
        ~ & OCRand & Vote+Rand & SCRC & One-stage & ScanRefer & 3D-LLM  & 3D-LLM & 3D-LLM  \\ 
        ~ &  &  &  &  &  & (flamingo) & (BLIP2-opt) & (BLIP2-flant5) \\ \hline
        ACC@0.25 & 29.9 & 10.0 & 18.7 & 20.4  & 41.2 & 21.2 & 29.6 & 30.3 \\ 
        Avg. IoU & / & / & / & / & / & 19.9 & 23.1 & 24.9 \\ 
        Avg. Dist. & / & / &/ & /  & / & 1.1 & 1.07 & 1.03 \\ 
        \hline
    \end{tabular}
    \caption{Experimental Results on ScanRefer}
    \label{tab:grounding}
\end{table}

\subsubsection{Object Navigation}
We show the ability of our 3D-LLM to progressively understand the environment and navigate to a target object. We formulate the navigation process as a conversation. At each time step, we online build a 3D feature from the partially observed scene. We feed this feature, current agent location, and history location to the 3D-LLM for predicting a 3D waypoint the agent should go for. We then use an off-the-shelf local policy~\cite{wijmans2019dd} to determine a low-level action (\textit{e.g.}, go forward, turn left or right) for navigating to the waypoint. The 3D-LLM predicts ``stop'' if it believes the agent has reached the target object. 

In Figure~\ref{fig:objnav}, we visualize a conversation process and its corresponding navigation trajectory. At the beginning when the target object is not observed, the 3D-LLM predicts a waypoint that leads the agent to explore the area most likely containing the target object. When the agent observes the target object (\textit{i.e.}, red box in the partially observed scene), the 3D-LLM predicts a waypoint leading the agent to it. The example episode is performed on the HM3D dataset~\cite{ramakrishnan2021habitat} using Habitat simulator~\cite{savva2019habitat}.



\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/objnav.pdf}
\caption{\textbf{Visualization of an object navigation episode.}}
    \label{fig:objnav}
\end{figure}




\subsection{Held-In Evaluation}
\subsubsection{3D Dense Captioning}
In Table \ref{tab:dense}, we show the results of 3D dense captioning. Specifically, given a 3D bounding box, models are expected to output the caption describing what's in that region. We can see that our 3D-LLMs outperform image-based baselines. 
\begin{table}[!ht]
    \centering
    \small
    \begin{tabular}{lllllll}
    \hline
         ~ & BLEU-1 & BLEU-2 & BLEU-3 & BLEU4 & METEOR & ROUGH-L  \\ \hline
        flamingo-SingleImage & 21.5 & 10.5 & 6.9 & 4.1 &11.1 & 23.4\\
        flamingo-MultiView & 24.4 & 12.3 & 7.1 & 4.6 & 11.9 & 25.8\\
        BLIP-SingleImage & 23.0 & 11.7 & 7.7 & 4.6 & 11.3 & 23.8\\
        BLIP-MultiView & 25.3 & 14.1 & 9.0 & 5.6 & 12.5 & 24.9\\
        3D-LLM (flamingo) & 29.6 & 16.8 & 10.6 &  5.9 & 
        11.4 & 29.9 \\
        3D-LLM (BLIP2-opt) & 32.5 & 18.7 & 11.9 & 6.5 & 11.7 & 31.5 \\
        3D-LLM (BLIP2-flant5) & 34.3 & 20.5 & 13.2 & 8.1 & 13.1 & 33.2  \\ 
\hline
    \end{tabular}
\caption{Experimental Results on Held-In 3D Dense Captioning Dataset.}
    \label{tab:dense}
\end{table}




\subsection{More Ablative Studies}
\subsubsection{Ablative Studies on Flamingo Perceiver}
We first examine how the perceiver resampler of Flamingo benefits the training. We carry out an ablative experiment where we take out the perceiver of the flamingo model. Table \ref{tab:perceiver} shows the results. From the table, we can see that the perceiver module is indeed beneficial for the training 
of 3D-LLM.
\begin{table}[!ht]
    \centering
       \small
    \begin{tabular}{l|llllllll}
    \hline
        ~ & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGH\_L & CIDER & EM \\ \hline
        wo/ perceiver & 29.2 & 17.2 & 11.2 & 7.4 & 11.4 & 30.4 & 58.9 & 20.6 \\ \hline
         w/ perceiver & 30.3 & 17.8 & 12 & 7.2 & 12.2 & 32.3 & 59.2 & 20.4 \\ \hline
    \end{tabular}
    \caption{Ablative Study on the Perceiver of Flamingo Model.}
    \label{tab:perceiver}
\end{table}







\subsection{More Qualitative Examples}
We show more qualitative examples in Figure \ref{fig:caption}, \ref{fig:qualitative2}, \ref{fig:qualitative3}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative-caption.pdf}
    \caption{Qualitative Examples on 3D Captioning}
    \label{fig:caption}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/qualitative-ongoing.pdf}
    \caption{Qualitative Examples on 3D-Assisted Dialog, 3D Dense Captioning and Question Answering. }
    \label{fig:qualitative2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative-supp03.pdf}
    \caption{Qualitative Examples on Task Decomposition and Grounding (Referring). }
    \label{fig:qualitative3}
\end{figure}
