\pdfoutput=1




\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{graphicx}

\renewcommand{\UrlFont}{\ttfamily\small}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}   
\usepackage{bbm}
\newcommand{\sign}{\text{sign}}

\newcommand{\micomment}[1]{\textcolor{red}{\bf \small [ #1 --MI]}}
\newcommand{\Negin}[1]{\textcolor{violet}{\bf \small [ #1 --Negin]}}
\newcommand{\andrewd}[1]{\textcolor{blue}{\bf \small [ #1 --AndrewD]}}
\newcommand{\shufan}[1]{\textcolor{orange}{\bf \small [ #1 --Shufan]}}

\usepackage{array,multirow,colortbl}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

\usepackage{microtype}

\def\aclpaperid{1887} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{You can't pick your neighbors, or can you? \\ When and how to rely on retrieval in the NN-LM}

\author{Andrew Drozdov\thanks{~~Corresponding author: adrozdov@cs.umass.edu},
  ~Shufan Wang,
  ~Razieh Rahimi,\\
  {\bf ~Andrew McCallum,
  ~Hamed Zamani,
  ~and~Mohit Iyyer}\\
  \AND\ \label{eq:knn_prob}
    P_{KNN}(w_t | q_t) &\propto \sum_{(k_i, v_i)} \mathbbm{1}_{w_t = v_i} \exp(-d(k_i, q_t))
 P^{'}(w_t | q_t) &= \lambda P_{KNN}(w_t | q_t) + (1-\lambda) P(w_t | q_t)
 P^{'}(w_t | .) &= \lambda_q P_{KNN}(w_t | .) + (1-\lambda_q) P(w_t | . )

where  is a function of both the query and its retrieved documents rather than constant for all queries. This is highly similar to the formulation in \citet{He2021EfficientNN}, except theirs ignores retrieved items when deciding the coefficient.

Using the same  for all examples is limiting and does not leverage retrieval well if neighboring keys are clearly relevant (like shown in Figure \ref{fig:adaptive_coeff}). Of course, the critical decision here is how to map semantic similarity to an appropriate value for the coefficient.
We find it convenient and effective to use a piecewise function based on semantic similarity, following the bucketing described in \S\ref{sec:analysis_vecdist}.
We use the validation data for tuning, sorting by semantic similarity with the topic retrieved item then dividing all the queries into  equally sized buckets. For each bucket we perform the same hyperparameter search over coefficients as in NN-LM.\footnote{See \citealt{khandelwal20generalization} Figure 5.} 

Example coefficient assignments for different numbers of buckets () are shown in Figure~\ref{fig:bucket_coeff}.

\section{Experiments and Results}

To measure the importance of retrieval quality in the NN-LM, we evaluate our approach (\S\ref{sec:adaptive_coeff}) on two English language modeling datasets. The first is the Wikitext-103 corpus \cite{merity2016pointer} used by \citet{khandelwal20generalization}. The second is PG-19 \cite{rae2020compressive}, which we include because it consists of books and is thematically distinct from the encyclopedic documents in Wikitext-103.

\subsection{Experimental Setup and Pre-processing}

\paragraph{Wikitext-103} The data is split 103M/217K/245K tokens for training, validation, and test. We use the pretrained model from \citet{khandelwal20generalization}, and associated 267K word-level vocab.

\paragraph{PG-19} To understand when adapting the coefficient to retrieval quality is desirable compared with a static coefficient, we include PG-19 in our experiments. PG-19 consists of books and is thematically distinct form the encyclopedic douments in the Wikitext-103 data. We sample 2,000 books from the training corpus, which gives approximately 150M tokens and is close in size to Wikitext-103. We use the standard validation split (50 books) and test split (100 books). We use word-level tokenization with a 300K vocabulary derived from our constructed training split. We train our own model using the same architecture and hyperparameters from \citet{khandelwal20generalization}.

\paragraph{Baselines} We choose these baselines to isolate the effect of retrieval quality on the performance of the NN-LM: the self-attentive adaptive input representation from \citet{baevski2018adaptive} as the base model, the original NN-LM \cite{khandelwal20generalization}, and the continuous cache model \cite{Grave2017ImprovingNL} which retrieves from both the datastore and local context. As described in \S\ref{sec:knnlm_approach}, the datastore is built by encoding a large text corpus, in this case the training set. Although we use approximate neighbors, we compute the next word probability with \textit{exact} distance as this substantially boosts performance \cite{khandelwal20generalization}.\footnote{\citet{He2021EfficientNN} and \citet{Alon2022NeuroSymbolicLM} present efficient extensions of NN-LM. We exclude these as baselines since they're designed with approximate vector distance in mind.}


\begin{table}[t!]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ r | c | c | c }
\toprule
 & Dev & Dev & Dev \\
\midrule
1 & 17.091 & 14.989 & 16.091 \\
2 & 16.909 & 14.854 & 15.933 \\
4 & 16.763 & 14.767 & 15.815 \\
8 & 16.665 & 14.727 & 15.743 \\
16 & 16.637 & 14.722 & 15.727 \\
32 & 16.629 & 14.722 & 15.721 \\
64 & 16.622 & 14.724 & 15.719 \\
128 & 16.619 & 14.724 & 15.715 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Validation perplexity on Wikitext-103. Used for hyperparameter tuning.}
\label{tab:tune}
\end{table}

\subsection{Tuning NN-LM Hyperparameters}

For the original formulation of NN-LM there are two hyperparameters to tune: the number of items to retrieve () and the interpolation coefficient (). These are tuned on the validation set. We introduce an important hyperparameter for the number of buckets to use () and tune a new interpolation coefficient () separately for each bucket. Since each bucket is assigned its own coefficient, the total number of hyperparameters grows with the number of buckets. Even so, our approach has about the same speed as the original NN-LM both for parameter tuning and during inference. We make hyperparameter tuning efficient by caching expensive computation (see \S\ref{sec:tuning} for more details). At test time, selecting the coefficient is an  lookup based on the semantic similarity of the top neighbor.

To select the number of buckets (), we use the first half of the validation data (Dev) to define partition boundaries, and find the best performing interpolation coefficient for each partition separately. Then we measure perplexity on the second half of the validation data (Dev) using those partition boundaries and coefficients. The choice of  that gives the best perplexity on Dev is the one we ultimately use. With  chosen, we then re-compute the partition boundaries and corresponding coefficients using the full validation data (Dev), which is used to evaluate against the test data.

An example of tuning for  on Wikitext-103 is shown in Table \ref{tab:tune}. Increasing  always leads to better perplexity on Dev, albeit with diminishing returns. Since the partition boundaries and coefficients are chosen using Dev, it is not guaranteed that increasing  improves perplexity on the held-out data (Dev). Although, tuning the partition boundaries and coefficients on the validation data does not guarantee improvement on the test data, in our experiments we find our adaptive coefficient is always as effective as the original static one.


\subsubsection{Computational Cost of Tuning}

\label{sec:tuning}

Our approach is nearly the same speed as the original NN-LM both at test time and for hyperparameter tuning. This is the case even though our hyperparameter count scales with  and is more than an order of magnitude more than what is used for the NN-LM. We accomplish this by effectively caching query vectors, retrieved items, and associated vector distances. The initial time to compute these values takes hours and is the same as with NN-LM, but after computed it takes less than 5 minutes to perform the hyperparameter search for the adaptive coefficient on the Wikitext-103 data.\footnote{All experiments are run on a single Titan X GPU with 256GB CPU memory.} Our implementation with caching is available here: \href{https://github.com/iesl/knnlm-retrieval-quality}{github.com/iesl/knnlm-retrieval-quality}.


\begin{table}[t!]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ l | c | c | c | c | c }
\toprule
 &  &  &  & Dev & Test \\
\midrule
Base LM & -    & - & -     &  17.96 & 18.65 \\
NN-LM  & 0.25 & 1 &  1024 &  16.06 & 16.12 \\
+CCache & 0.25 & 1 &  1024 &  15.81 & 15.79 \\
\midrule
Ours (TFIDF) &  & 32 & 1024 & 15.76  & 15.54 \\
Ours         &  & 32 & 1024 & 15.72  & \textbf{15.50} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Test and validation perplexity on Wikitext-103. This is our main result and demonstrates that our new formulation with adaptive coefficient () substantially improves over NN-LM.}
\label{tab:results_ppl}
\end{table}

\subsection{Perplexity on WikiText-103}

Table \ref{tab:results_ppl} reports the perplexity from our approach and various baselines on the Wikitext-103 validation and test sets. Our approach scores 15.50 perplexity on the test set. This is a 16.9\% improvement over the base language model and a 3.8\% improvement over the original NN-LM formulation.

For the number of buckets () we found 32 to work best (see Table \ref{tab:tune}), and the set of coefficients are the same as shown in Figure~\ref{fig:bucket_coeff}. Our search space includes  and .

\citet{khandelwal20generalization} find that retrieving from recent history using the continuous cache model (CCache; \citealt{Grave2017ImprovingNL}) is complementary to retrieving from the datastore, improving perplexity when combined with NN-LM. This type of caching is out of scope of this paper, and our approach already outperforms their combined model.


\subsection{Perplexity on PG-19}

To further understand how lexical overlap influences NN-LM performance we evaluate using the PG-19 dataset. Compared to Wikipedia, text across books has much less repetition, so text retrieved from the datastore is less likely to overlap with -grams in the evaluation data.

We train our own model using the same architecture and hyperparams for Wikitext-103, and report perplexity in Table \ref{tab:overlap_ppl}. We found  works best. Despite the challenging properties of the book data, NN-LM is still effective. Our re-formulation is marginally beneficial here.

\subsection{Filtering -grams from the Datastore}

Thus far, our analysis indicates that lexical overlap is important for strong NN-LM performance. To test this directly for our adaptive coefficient, we follow the procedure described in \S\ref{sec:analysis_bow} to rebuild the datastore but remove from the index large -grams () and their surrounding tokens that also appear in the evaluation data.

The results for this experiment on both Wikitext-103 and PG-19 are shown in Table \ref{tab:overlap_ppl}. Most of NN-LM's improvements on Wikitext-103 come from retrieving contexts with overlapping -grams,\footnote{As others have previously noted, Wikitext-103 contains considerable amounts of duplicate text \cite{McCoy2021HowMD}. Deduplicating the training data can be helpful for language modeling \cite{lee-etal-2022-deduplicating,dedup2022kandpal}, and sometimes other tasks \cite{schofield-etal-2017-quantifying}, but we completely remove text that overlaps with the evaluation data.} which could motivate simpler and faster retrieval functions. On the other hand, the cases in which -gram overlap does not play a major role require further investigation. 

\section{Discussion}

In previous sections we use observations of NN-LM to motivate our new approach that adapts the interpolation coefficient to retrieval quality. Here we analyze results with our new method to see how they compare with baselines and deepen our understand of retrieval-enhanced language modeling.


\begin{table}[t!]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ l | c | r | r | c }
\toprule
 &  &  &  & Dev \\
\midrule
Dense & 0.25        &  1 & 1024 & 16.06 \\
Dense &  & 32 & 1024 & 15.72 \\
TFIDF &  & 32 & 1024 & 15.76 \\
\midrule
Dense & 0.05  & 1 & 1   & 17.10 \\
Dense & 0.15  & 1 & 8   & 16.66 \\
Dense & 0.25  & 1 & 64  & 16.31 \\
\midrule
Dense &  & 16  & 1   & 16.63     \\
Dense &  & 128 & 8   & 16.19    \\
Dense &  & 16  & 64  & 15.90   \\
\midrule
TFIDF &  & 32  & 1   & 16.38     \\
TFIDF &  & 64  & 8   & 16.06    \\
TFIDF &  & 16  & 64  & 15.87   \\
\bottomrule
\end{tabular}
\end{center}
\caption{Validation perplexity on Wikitext-103 used for ablation analysis. The NN-LM uses a single static value for the interpolation coefficient (), our method uses an adaptive coefficient (). This table includes our approach when using the semantic similarity (Dense) or bag-of-words representation (TFIDF). Based on how many items are retrieved (), our approach works best with a different amount of buckets ().}
\label{tab:compare_tfidf}
\end{table}

\subsection{Can we adapt to lexical similarity?}

The original NN-LM has similar performance when its results are stratified by either semantic or lexical similarity (\S\ref{sec:analysis_vecdist}), but in our new formulation we adaptive the coefficient only according to semantic similarity. What if we use lexical similarity instead? We explore this possible alternative and report the results for Wikitext-103 in Table \ref{tab:compare_tfidf}.


In general, we find that both semantic and lexical similarity\footnote{To measure lexical similarity we use TFIDF vectors.} yield similar results when used to bucket queries. For the best setting, when , the learned vectors work better, reflecting recent findings that dense vectors outperform sparse representations for various retrieval-related tasks \cite{lee-etal-2019-latent,gao-etal-2021-coil}. Hence, throughout this paper we adapt the coefficient using semantic similarity and  unless otherwise specified. Interestingly, for lower values of  the bag-of-words representation has an edge over semantic similarity. Perhaps this suggests lexical similarity is more precise, and if retrieving many items is costly, then adapting the coefficient according to lexical similarity might be particularly helpful. 

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{more_figs/base_ppl_part_of_speech_combined.scatter.pdf}
\includegraphics[width=\linewidth]{more_figs/part_of_speech_diff_wiki.scatter.pdf}
\includegraphics[width=\linewidth]{more_figs/part_of_speech_diff_pg19.scatter.pdf}
\caption{Perplexity of the base language model (top), grouped by part-of-speech. Relative perplexity improvement by NN-LM approches on Wikitext-103 (center) and PG-19 (bottom). The lines corresponding NN-LM match Figure \ref{fig:analysis_pos_wiki_knnlm} --- they are included here to emphasize the difference to our new formulation.}
\label{fig:pos_ppl_combined}
\end{figure}

\begin{table*}
\setlength\tabcolsep{4pt}
{\small
\begin{center}
\begin{tabular}{l l l}
\toprule[1.5pt]
Book & Context &  \\
\midrule[1pt] {\small \textit{The Unbearable Bassington}, Saki (1912)} &
My dear Francesca , he said soothingly , laying his hand \textbf{affectionately} &
{\small }
\\
\midrule[0.1pt]
{\small \textit{FLORA}, A.L.O.E. (1860)} &
My dear madam , said Mr. Ward earnestly , laying his hand \textit{on} &
{\small } 
\\
{\small \textit{Peter}, Smith (1908)} &
this young man 's uncle , said Peter , laying his hand \textbf{affectionately} &
{\small }
\\
\midrule[1pt] {\small \textit{Life of Napoleon Bonaparte}, Sloane (1896)} &
during the worst periods of terror , were thronged from pit to \textbf{gallery} &
{\small }
\\
\midrule[0.1pt]
{\small \textit{Sketches of Reforms\textemdash}, Stanton (1849)} &
For weeks , that theater was crowded from pit to \textit{dome} &
{\small }
\\
{\small \textit{Farquharson of Glune}, Bateman (1908)} &
The storm of feeling swept alike from stall to \textbf{gallery} &
{\small }
\\
\midrule[1pt] {\small \textit{Walking}, Thoreau (1851)} &
like a dream of the Middle Ages . I floated down its historic \textbf{stream} &
{\small }
\\
\midrule[0.1pt]
{\small \textit{The Automobilist Abroad}, Mansfield (1907)} &
 France is a pleasure , a voyage up a picturesque and historic \textit{French} &
{\small }
\\
{\small \textit{Canadian Notabilities}, Dent (1880)} &
two small sailing craft slowly making their way up the majestic \textbf{stream} &
{\small }
\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
}
\caption{Examples from PG-19 where relevant contexts are found even with large -grams removed from the datastore. There can be overlap in small -grams (top), local structure (center), or semantics (bottom). The contexts are shown with their corresponding book. Rank () is shown except for queries (). Values are bolded or italicized.}
\label{tab:data_examples}
\end{table*}

\subsection{Do syntactic trends hold across domains?}

We repeat the syntactic analysis from \S\ref{sec:syntactic_analysis} using our adaptive coefficient and include PG-19 as an additional dataset.\footnote{We only include the first 500K tokens from PG-19 validation data, as this is already more than twice the size of Wikitext-103 validation data.} The corresponding plots are shown in Figure \ref{fig:pos_ppl_combined}.

In both domains, the base model has a similar pattern of perplexity for part-of-speech tags, but there are some differences when comparing NN-LM across domains. For instance, NN-LM is especially helpful for adjectives in wikipedia text, but much less so for the book data. It's satisfying to see our new formulation of the NN-LM has a similar impact in many cases for both domains, e.g. improving performance on adjectives nearly 5\% despite the aforementioned differences. Also, our formulation and NN-LM provide consistent benefits even in the relatively more challenging book domain. Besides being potentially stylistically and syntactically distinct, we imagine encyclopedic text has more repetition than book data, which would likely influence the amount of lexical overlap between the train and evaluation data. We explore the effect of deliberately limiting lexical overlap in the next subsection, providing insights for the different cases when retrieval is helpful.

\subsection{What use is the restricted datastore?}


As we established in \S\ref{sec:analysis_bow}, the lexical overlap between a query and a retrieved context is a reasonable proxy for relevance. In Table \ref{tab:overlap_ppl}, we report the perplexity of our adaptive coefficient when ignoring large -grams that overlap with the evaluation data when building the index, yielding a restricted less effective datastore. With these highly \textit{relevant} contexts removed, we observe that the NN-LM shows substantially worse test perplexity on Wikitext-103, 18.05 instead of 16.12. PG-19 exhibits different behavior, and the change in perplexity is minimal. This suggests that NN-LM can be helpful even when there are not large overlapping -grams between the datastore and evaluation corpus --- such cases occur frequently in PG-19, and we visualize examples in Table \ref{tab:data_examples}.

With the restricted datastore, the benefit from adapting the coefficient is substantially diminished for Wikitext-103, but less so for PG-19. This suggests the partitions capture qualities besides lexical similarity. Alternatively, it could be that short -grams are helpful in Wikitext-103, despite \citet{khandelwal20generalization} reporting that interpolating the base language model with an -gram model was not very effective.

It is worth noting that even when contexts with high lexical overlap are removed from the datstore, adapting the coefficient is robust and provides performance at least on par with NN-LM in the same setting. While NN-LM is weakened here, it does improve over the base language model. In future work, it could prove fruitful to explore alternate strategies besides semantic or lexical similarity.



\section{Related Work}


We extend the NN-LM by adapting the interpolation coefficient to retrieval quality (measured by semantic similarity).
AdaptRet \cite{He2021EfficientNN} models the interpolation coefficient as a function of the query. This is convenient, since one can skip retrieval if the coefficient is below a threshold, although requires training a separate adaptor network. Crucially, their coefficient predictions are based solely on query features, and does not take into account whether retrieval is successful. Our approach incorporates the quality of retrieval, and improves language modeling results. It is simple and effective, and only needs lightweight hyperparameter tuning without any additional training.

RetoMaton \cite{Alon2022NeuroSymbolicLM} provides an alternative means to bypass retrieval. They build a graph over the datastore, and at each time step they either retrieve like the original NN-LM or re-use the previously retrieved neighbors to traverse the graph. This is more efficient than AdaptRet, providing better results at lower cost. Both AdaptRet and RetoMaton are designed with efficiency in mind. They rely on approximate distance using product quantization and perform about as well as the exact distance version of the NN-LM. We improve upon NN-LM by about 4\% perplexity.


There are many recent works that use retrieval components for language tasks besides language modeling, such as question answering \citep{Godbole2019MultistepEI,Guu2020REALMRL, kassner-schutze-2020-bert}, dialogue generation \citep{Fan2021AugmentingTW}, conversational search \cite{Hashemi2020GuidedTL}, semantic parsing \citep{Gupta2021RETRONLURA}, data augmentation \cite{Du2021SelftrainingIP}, and machine translation \cite{khandelwal2021nearest,zheng-etal-2021-adaptive,chunkMT2022}. 

There are alternatives to NN-LM that incorporate document structure \cite{Xu2021CapturingSL}, but their experimental setup is not comparable with ours. In our baselines we only consider models matching the original NN-LM backbone, although alternative architectures show promise for retrieval-enhanced language modeling \cite{Yogatama2021AdaptiveSL,meng2022gnnlm,zhong2022training}. Scaling the datastore \cite{Borgeaud2021ImprovingLM} or model size \cite{Shoeybi2019MegatronLMTM} have shown to effectively improve language modeling. Alternatively, text generation may be improved through more advanced ranking \cite{min-etal-2021-joint} or decoding \cite{rankgen22} algorithms.


Researchers have explored fundamental extensions to NN that are agnostic to language data. \citet{locallyadaptive1993} spatially partition the datastore, adapting the value of  for each region. Keeping  fixed, \citet{discadapt1995knn} instead adapt the shape of the neighborhood based on local information. 

\section{Conclusion}

In this paper, we have  proposed a novel and effective re-formulation of the NN-LM. Our approach adapts the interpolation coefficient to the quality of retrieved documents measured by semantic similarity. We motivate our approach through extensive analysis, which also provides insights on the types of tokens and contexts NN-LM is most helpful for. Importantly, we empirically demonstrate the effectiveness of our approach through experiments on two domains, Wikitext-103 (encyclopedic text) and PG-19 (book data), and outperform the original NN-LM by 4\% test perplexity on the Wikitext-103 language modeling corpus.

\section*{Limitations}

The NN-LM leverages a datastore, and when populated with text relevant for the task domain, can be used to improve language modeling performance. The benefits of this procedure are data dependent and domain-specific, and the same applies to the adaptive coefficient technique that we introduce.

The adaptive coefficient requires many more tunable hyperparameters. To address this, we release an optimized codebase to perform this hyperparameter search in neglible time compared with the original NN-LM.

\section*{Ethical Concerns and Impact}

Even when used with the best intentions language models can produce malicious or harmful text, and guards are typically used to account for inherent bias or undesirable output. In our case, we do not generate text and simply use the model to evaluate perplexity on existing data, so effectiveness of safety guards and their limitations is not a relevant concern in this work.

\section*{Acknowledgements}

We are grateful to Fernando Diaz, Urvashi Khandelwal, Kalpesh Krishna, Simeng Sun, the UMass NLP group and IESL for several useful discussions during the course of the project. This work was supported in part by the Center for Intelligent Information Retrieval and the Center for Data Science; in part by the IBM Research AI through the AI Horizons Network; in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction; in part by the National Science Foundation (NSF) grant numbers  IIS-1922090, IIS-1955567, IIS-1763618, and IIS-2106391; in part by the Defense Advanced Research Projects Agency (DARPA) via Contract No. FA8750-17-C-0106 under Subaward No. 89341790 from the University of Southern California; and in part by the Office of Naval Research (ONR) via Contract No. N660011924032 under Subaward No. 123875727 from the University of Southern California. Any opinions, findings and conclusions or recommendations expressed in this material are of the authors and do not necessarily reflect those of the sponsor.

\bibliography{anthology,acl}
\bibliographystyle{acl_natbib}


\end{document}
