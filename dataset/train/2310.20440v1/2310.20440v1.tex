\PassOptionsToPackage{utf8}{inputenc}
\documentclass{bioinfo}
\copyrightyear{2023} \pubyear{2023}


\newenvironment{note}
{\par\color{black}\begin{quote}\textbf{Note:}\ }
{\end{quote}\par}

\newenvironment{example}
{\par\color{black}\begin{quote}\textbf{Example:}\ }
{\end{quote}\par}

\newenvironment{warning}
{\par\color{black}\begin{quote}\textbf{Warning:}\ }
{\end{quote}\par}

\usepackage{appendix}

\usepackage{natbib}
\bibliographystyle{natbib}




\begin{document}
\firstpage{1}

\subtitle{}

\title[SourceData-NLP]{The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models}
\author[Abreu-Vicente \textit{et~al}.]{
    Abreu-Vicente, Jorge\,; 
    Sonntag, Hannah\,; 
    Eidens, Thomas\,;
    Lemberger, Thomas\,
}
\address{EMBO, Meyerhofstr. 1, 69117 Heidelberg, Germany}
\corresp{To whom correspondence should be addressed: thomas.lemberger@embo.org}
\abstract{
\textbf{Introduction}: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.\\
\textbf{Results}: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.\\
\textbf{Conclusions}: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.\\
\textbf{Availability: }To facilitate researchers' access to SourceData-NLP, we provide open-source access to the source code for the raw data (\href{https://github.com/source-data/soda-data}{https://github.com/source-data/soda-data}), machine-learning-ready data (\href{https://huggingface.co/datasets/EMBO/SourceData}{https://huggingface.co/datasets/EMBO/SourceData}), and generated models (\href{https://github.com/source-data/soda-model}{https://github.com/source-data/soda-model}).
}

\maketitle

\clearpage

\section{Introduction}

Scientific productivity has reached unprecedented levels. As of June 2022, PubMed~\citep{pubmedhelp} alone has over 34 million articles.  Furthermore, millions of new articles are published every year in PubMed and preprint platforms such as  bioRxiv\footnote{https://www.biorxiv.org/}~\citep{biorxiv}. This productivity comes at a cost: the vast amount of literature makes it increasingly difficult for scientists to keep up with the latest research in their field. The "curse of specialization" further exacerbates this problem, as scientists become increasingly siloed in their respective areas of expertise. To address these challenges, the demand is growing for natural language processing (NLP) tools that automate the extraction of knowledge from the literature at scale.

By automatically analyzing text, NLP enables the computational processing of large volumes of literature. Named-entity recognition~\citep[NER][]{bikel-etal-1997-nymble} and named-entity linking~\citep[NEL][]{hoffart-etal-2011-robust} are important tasks for downstream analysis. NER identifies and categorizes entities within a text, while NEL assigns ontology links or standard identifiers to the entities. These NLP tasks enable the extraction of structured information from scientific literature and facilitate the identification of key concepts and relationships in the data.

The field of biocuration has made significant efforts to assemble large annotated datasets \citep[e.g.,][]{perera,zhao20-biomedical-curation,bigbio}. Integrating biocuration in the publishing process can complement these efforts by capturing and exposing structured information as soon as the papers are published, thus avoiding delay. It may also improve the reliability and rigor of reporting scientific findings by resolving potential ambiguities in terminologies or unclear concepts. An added benefit is access to the expertise of the authors, which can greatly assist the biocuration process.  Finally, leveraging the publication process provides a unique opportunity to accumulate over time large structured datasets for training NLP models. The extraction of structured data from the scientific literature will have significant positive impacts. First, it greatly facilitates the systematic maintenance of biological knowledge bases and ontologies. Second, it helps to make papers and the reported evidence easier to discover and to mine ~\citep{sourcedata}.

In this article, we present the SourceData-NLP dataset. A first-of-its-kind extensive NER/NEL dataset produced by the routine curation of papers during the publication process. The SourceData-NLP dataset is based on the SourceData annotation framework~\citep{sourcedata}, focused on the annotation and linking of biological entities mentioned in the figure legends. In the life sciences,  experimental results are mainly presented in figures, with captions providing natural language technical descriptions of the respective experiments. As such, figures can be considered as the foundation of a paper in which the evidence supporting the claims of a paper is presented. To leverage the information provided in figure captions, SourceData annotates eight types of biological entities of interest across biological organizational scales (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, diseases), their role in experimental design, and the kind of experimental assay. To highlight the utility of SourceData-NLP, we conduct a systematic analysis of the performance of the two best-performing transformers-based~\citep{transformers} models for biomedical NER (BioLinkBERT,~\citealp{biolinkbert}; PubmedBERT,~\citealp{pubmedbert}). 

To facilitate access for researchers to SourceData-NLP, we provide the source code to raw data\footnote{https://github.com/source-data/soda-data}, machine learning-ready data\footnote{https://huggingface.co/datasets/EMBO/SourceData}, and the trained models\footnote{https://github.com/source-data/soda-model}. 

\subsection{Related work}\label{sec:overview}

Significant efforts have been made over the years to create annotated datasets for training and benchmarking algorithms on NER and NEL tasks. A summary of these efforts is shown in Table~\ref{tab:dataset-overview},  while a more detailed summary can be found in the Biomedical Dataset Library ~\citep[BigBIO,][]{bigbio}. The properties of each dataset vary widely, from highly focused to broad scope. Among the former, there are datasets that focus on genes and proteins, such as BC2GM\citep{bc2gm}, NLM-Gene~\citep{islamaj2021nlm}, GENETAG~\citep{genetag}, and  GnormPlus~\citep{genenormplus}, diseases, such as NCBI-disease~\citep{ncbidisease} and BC5CDR~\citep{bc5chem}, chemical compounds, such as BC5CDR and BC4CHEMD~\citep{bc5chem}, or species, such as LINNEAEUS~\citep{linnaeus}. Datasets such as natEM~\citep{pyysalo2014anatomical}, BioNLP13CG~\citep{pyysalo-etal-2013-overview}, or MedMentions~\citep{medmentions} have a large number of entity classes encompassing a much broader scope than SourceData-NLP.

The recently released BioRED dataset~\citep{biored} has a similar scope to SourceData-NLP. It annotates genes, diseases, chemicals, genetic variants, species, and cell lines. However, BioRED is focused on the relation detection (RD) task, which reveals interactions between entities. BioRED has a set of more than 20,000 entities tagged with links to databases and includes a novelty classification label intended to highlight novel discoveries.

Previous studies have focused on accurately extracting entities (NER/NEL) and their relationships (RD) by annotating sentences, abstracts, or full texts. Annotation of figure captions that describe scientific results and annotation of the roles that biomedical entities play in specific experimental designs have, to the best of our knowledge, not been described in previous work. Both are useful for building data models that capture important features of scientific experiments in biomedical research, which is the focus of SourceData-NLP.


\begin{table*}[]
\processtable{\label{tab:dataset-overview}Overview of NER/NEL available datasets.} {
\begin{tabular}{lcccclllrl}\toprule
\textbf{Dataset} &
  \textbf{NER} &
  \textbf{NEL} &
  \textbf{RE} &
  \textbf{ER} &
  \textbf{Novel} &
  \textbf{Corpora size} &
  \textbf{Entity type} &
  \textbf{\# Mentions} &
  \textbf{References} \\\midrule
SourceData-NLP   & \checkmark & \checkmark &   & \checkmark &   & 18,689 figure captions of 3,223 papers                            & GP, CH, C, T, SC,  O, EA,  D           & 623,162 &  This work                     \\
BC2GM        & \checkmark &   &   &   &   & 20,000 sentences           & G                                      & 44,500  & \citealp{bc2gm}                 \\
BC5DR        & \checkmark & \checkmark & \checkmark &   &   & 1,000 PubMed Articles      & CH, D                                  & 29,271  & \citealp{bc5chem}               \\
EBM PICO     & \checkmark &   &   &   &   & 4,993 Abstracts            & PICO                                   & 88,106  & \citealp{ebm-pico}              \\
JNLPBA       & \checkmark &   &   &   &   & 2,404  PubMed Abstracts    & P, DNA, RNA, C line, C type            & 59,963  & \citealp{jnlpba-a}                \\
NCBI-disease & \checkmark & \checkmark &   &   &   & 793 PubMed Abstracts        & D                                      & 6,892   & \citealp{ncbidisease}          \\
AnatEM       & \checkmark & \checkmark &   &   &   & 1,212 documents            & 12+ entities                           & 13,000  & \citealp{pyysalo2014anatomical} \\
BC4CHEMD     & \checkmark &   &   &   &   & 10,000 PubMed Abstracts    & CH                                     & 84,355  & \citealp{KrallingerCHEMDNER}    \\
BioNLP09     & \checkmark & \checkmark &   &   &   & 1,210 Abstracts            & P, P regions, C locations              & 15,497  & \citealp{kim-etal-2009-overview} \\
BioNLP11ID   & \checkmark & \checkmark & \checkmark &   &   &  316 Abstracts              & P, O, CH, two-component system         &  12,788 &  \citealp{pyysalo-etal-2011-overview} \\
BioNLP13CG   & \checkmark & \checkmark & \checkmark &   &   &  600 Abstracts              & 20+ entities                           &  21,654 &  \citealp{pyysalo-etal-2013-overview} \\
BioNLP13GE   & \checkmark & \checkmark & \checkmark &   &   & 776 Abstracts               & P, anaphora, binding, entity           & 12,726  & \citealp{kim-etal-2013-genia}   \\
BioNLP13PC   & \checkmark & \checkmark & \checkmark &   &   &  525 Abstracts              &  CH, G or GP,  complex, SC             &  15,901 &  \citealp{ohta-etal-2013-overview} \\
NLM-Gene     & \checkmark & \checkmark &   &   &   & 550 Titles and abstracts    & G                                      & 15,553  & \citealp{islamaj2021nlm}        \\
LINNAEUS     & \checkmark &   &   &   &   & 100 PMC Articles            & O                                      & 4,259   & \citealp{linnaeus}              \\
BioRED       & \checkmark & \checkmark & \checkmark &   & \checkmark & 600 PMC Articles            & G, D, CH, variant, O, C line           & 20,419  & \citealp{biored}                \\
GENETAG      & \checkmark &   &   &   &   & 15,000 Sentences           & G                                      & 17,866  & \citealp{genetag}               \\
GNormPlus    & \checkmark & \checkmark &   &   &   & 694 Abstracts               & G, G Family and P Domain               & 9,257   & \citealp{genenormplus}             \\
MedMentions  & \checkmark & \checkmark &   &   &   & 4,392 Titles and abstracts & More than 20 different entities        & 385,098 & \citealp{medmentions}           \\\botrule
\end{tabular}}{\textbf{ER: }Experimental roles, \textbf{GP: }Gene products, \textbf{G: }Genes, \textbf{P: }Proteins, \textbf{C: }Cells,
\textbf{T: }Tissue, \textbf{SC: }Subcellular, \textbf{O: } Organism or species, \textbf{EA: }Experimental assay,
\textbf{D: }Disease, \textbf{CH: }Chemicals or small molecules, \textbf{PICO: }Participants, interventions, outcomes
}
\end{table*}

\begin{methods}
\section{Materials and methods}

\subsection{The SourceData-NLP dataset}\label{sec:soda_dataset}

In this section, we outline the principles underlying the SourceData-NLP annotation process. Since the annotation is integrated within the publication workflow it is strongly constrained by the nature of the editorial process in a scientific journal. Consequently, the annotation framework is designed to balance simplicity and expressiveness,  ensuring feasibility while reflecting key elements of the experimental approach \citep{sourcedata}. Our workflow includes a curation step, which is followed by a validation step by the authors as needed. To keep concepts as simple as possible, the annotation focuses mainly on bioentities that are linked to identifiers from a select number of ontologies and knowledge bases. Complex concepts such as biological processes, pathways, or detailed entity attributes are intentionally excluded to prioritize speed and efficiency.

\subsubsection{Scope and annotations}\label{sec:scope}

SourceData-NLP describes the data presented in scientific figures published in the field of cell 
and molecular biology. The annotations focus on biological entities that are relevant to the
scientific meaning of the data and the underlying experimental design.
The annotation process is composed of the following steps:
\begin{enumerate}
    \item Splitting of composite figures into coherent panels ("panel segmentation").
    \item Tagging of biological entities.
    \item Linking of tagged entities to external identifiers.
    \item Categorization of entities into the role they play in the specific experimental design.
\end{enumerate}

Annotation guidelines are provided in the Appendix~\ref{app:guidelines}\footnote{The
guidelines are also maintained at https://sourcedata.embo.org/documentation/}.

\paragraph{Panel segmentation:} 
In the life sciences, figures commonly include multiple panels that present results obtained through various approaches. For efficient curation and representation of the experimental design, annotation is performed at the level of individual panels. The figure panels serve as a coherent unit of research, often describing results from a single experimental assay and a specific experimental system. The tagging procedures detailed below are applied to all panels, except those that display schematics, computational simulation results, overviews, or workflows.

\paragraph{Tagging biological terms (entities, diseases, and experimental assays):}
Specific biochemical terms within a panel legend are tagged and classified into eight mutually exclusive classes: small molecules, gene products (genes and proteins), subcellular components, cell types, cell lines, tissues, organisms, and diseases. These classes span various organizational scales in biological organisms, from small molecules to species. Generic references to these classes, such as "proteins," "genes," and "cells," are not included in the annotation process. Post-translational modifications, mutations, and other attributes of an entity are not tagged, focusing solely on the base terms. To enable the description of the empirical design, we also tag the experimental assays as an additional class.

\paragraph{Entity linking:} 
Biological entities and experimental assays are normalized and linked to their corresponding identifiers in ontologies. Normalization ensures the unambiguous identification of entities. In cases where a single identifier cannot be assigned definitively, data curators are instructed to assign multiple possible identifiers to the entity. The ontologies used in SourceData-NLP, listed in Table~\ref{tab:kb}, are carefully selected to facilitate the annotation process, with preference given to curated entries.

\paragraph{Entity roles:} 
The entities from the previous step are further categorized based on the experimental design. SourceData-NLP utilizes "roles" to represent causal hypotheses that are experimentally tested. There are six defined experimental roles, as described in~\citet{sourcedata}. Here, we provide a brief overview of the two main "role" categories. More details can be found in Appendix A.

The first category is the "measured variable," which refers to a component related to the measurement reported in a figure and performed through a specific experimental assay. These concepts can be mapped to the concepts of "measuredVariable" and "measurementTechnique", respectively, in schema.org. For example, specific proteins detected on a Western blot are the measured variables in this assay.

The second main "role" category is assigned to entities that are targets of experimental interventions and are labeled as "controlled variables". These entities are subjected to targeted and controlled perturbations as an experimental approach to test potential cause-and-effect relationships. For instance, an experiment measuring the effect of the targeted genetic disruption of gene X (the controlled variable) on the abundance of a protein Y (the measured variable) is an empirical test of whether a change in X causes a change in Y.

Note that entities mentioned in figure captions cannot always be interpreted as having either the "measured variable" or the "controlled variable" role, depending on the design of the experiment or when they are mentioned merely to provide contextual information. In such cases, these entities are assigned the default generic "biological component" role to indicate the absence of a clear causal interpretation.

\begin{table}[!t]
\processtable{\label{tab:kb}Ontologies to which the SourceData-NLP tagged entities are normalized.} {

    \begin{tabular}{@{}l|ll@{}}\toprule 
    \textbf{Entity Type}               & \textbf{Primary Resource}           & \textbf{Secondary Resource}      \\\midrule
    \textbf{Small molecules}           & ChEBI                           & PubChem                   \\
    \textbf{Genes}                     & NCBI Gene                       & Rfam                      \\
    \textbf{Proteins}                  & UniprotKB/Swiss-Prot            &                                  \\
    \textbf{Subcellular components}       & Gene Ontology               &                                  \\
    \textbf{Cell types and cell lines} & Cellosaurus                     & Cell Ontology             \\
    \textbf{Tissues \& organs}         & Uberon                          &                                  \\
    \textbf{Organisms \& species}      & NCBI Taxonomy               &                                  \\
    \textbf{Diseases}                  & Disese Ontology              & MeSH                      \\
    \textbf{Experimental assays}       & BAO, OBI              &                                  \\\botrule
    \end{tabular}}{
    ~\citealp{chebi}, ~\citealp{ncbigene}, ~\citealp{uniprot}, 
    ~\citealp{geneont1}, ~\citealp{genont2}, ~\citealp{cellosaurus}, 
    ~\citealp{uberon}, ~\citealp{ncbitaxonomy1}, ~\citealp{ncbitaxonomy2}, 
    ~\citealp{bao}, ~\citealp{obi}, ~\citealp{pubchem}, ~\citealp{rfam}, ~\citealp{cellont}, ~\citealp{do}, ~\citealp{mesh}
    }
\end{table}

\subsubsection{Data acquisition}

The curation workflow takes place on the SourceData platform~\citep{sourcedata}. Curation is carried out by professional curators from Molecular Connections\footnote{https://molecularconnections.com/}. When curators are uncertain, or in some systematically predefined cases, additional information or identifier validation is requested from the authors (see policies in App~\ref{app:guidelines}). In the EMBO Press editorial process, authors are requested to provide their source data for figures showing relevant experimental results in their papers. These figures are then sent to the curation team, to be annotated following the guidelines described in Section~\ref{sec:scope} and Appendix~\ref{app:guidelines}. 

\subsubsection{Dataset summary}

The SourceData-NLP dataset consists of 18,689 figures, manually divided into 62,543 annotated panels from 3,223 articles published in 25 journals (Table~\ref{tab:journals}). The dataset is split into train, evaluation, and test sets in an 80-10-10 ratio.  A total of  661,862 entities are provided, including 623,162 entities linked to identifiers in external databases. The distribution of the entities across the annotation classes can be found in Table~\ref{tab:mentions}. To our knowledge, SourceData-NLP represents one of the most extensive biomedical annotated datasets currently available for NER and NEL. The dataset can be accessed at HuggingFace~\citep{huggingface}\footnote{https://huggingface.co/datasets/EMBO/sd-nlp-non-tokenized}. 

\begin{table}[!t]
\processtable{\label{tab:journals}Distribution of annotated articles per journal.} {

    \begin{tabular}{@{}l|r@{}}\toprule 
    \textbf{Journal}                    & \textbf{\# articles}\\\midrule
    \textbf{The EMBO Journal}           & 1193\\
    \textbf{EMBO Reports}               & 935 \\
    \textbf{EMBO Molecular Medicine}    & 594 \\
    \textbf{Molecular Systems Biology}  & 169 \\
    \textbf{Nature Cell Biology }       & 51 \\
    \textbf{Nature}                     & 43 \\
    \textbf{The Journal of Cell Biology}& 40 \\
    \textbf{Nature Communications}      & 40 \\
    \textbf{BioRXiv}                    & 38\\
    \textbf{PLOS Pathogens}             & 28 \\
    \textbf{Others}                     & 102\\\botrule
    \end{tabular}}{If less than 20 articles have been annotated, the journal
    is included in "others".}
\end{table}


\begin{table*}[]
\processtable{Entity mentions, unique entity mentions, and the uniqueness ratio of the mentions in the dataset.\label{tab:mentions}}{
\begin{tabular}{@{}ll|rrr|rrr|rrr|r@{}}\toprule
  \textbf{Task} &
  \textbf{Class} &
  \textbf{Train} &
  \textbf{Uniq.} &
  \textbf{(\%)} &
  \textbf{Test} &
  \textbf{Uniq.} &
  \textbf{(\%)} &
  \textbf{Valid.} &
  \textbf{Uniq.} &
  \textbf{(\%)} &
  \textbf{Total}\\ \midrule
                                 & Small mol.      & 58,696  & 6,555  & 11.2  & 6,935  & 1,293 & 18.6  & 7,898  & 1,498 & 19.0 & 73,529     \\
                                 & Gene products   & 201,524 & 19,022 & 9.4   & 26,341 & 3,617 & 13.7  & 28,260 & 3,934 & 13.9  & 256,125    \\
                                 & Subcellular     & 30,807  & 3,229  & 10.5  & 4,674  & 734    & 15.7  & 3,901  & 736    & 18.9  & 39,382     \\
                                 & Cell type       & 21,644  & 2,014  & 9.3   & 2,872  & 465    & 16.2  & 2,829  & 408    & 14.4  & 27,345     \\
NER                              & Tissue          & 27,988  & 3,600  & 12.9  & 3,851  & 776    & 20.2  & 3,808  & 786    & 20.6  & 35,647     \\
                                 & Organism        & 33,709  & 1,746  & 5.2   & 4,422 & 344    & 8.1   & 4,959  & 385    & 7.8   & 42,892     \\
                                 & Exp. assay      & 92,245  & 10,520 & 11.4  & 11,197 & 2,137 & 19.1  & 13,098 & 2,367 & 18.1  & 116,540    \\ 
                                 & Disease         & 5,140   & 947     & 18.4  & 602     & 128    & 21.2  & 633     & 159    & 25.1  & 6,375     \\
                                 & Cell line       & 19,870  & 1,174  & 5.9   & 2,367  & 254    & 10.7  & 3,090  & 260    & 8.4   & 25,327     \\
                                 & \textbf{Total}  & \textbf{491,623}	& \textbf{48,807} & \textbf{9.9} &	\textbf{63,261} & \textbf{9,748} & \textbf{15.4} & \textbf{68,476} &	\textbf{10,533} & \textbf{15.4} & \textbf{623,162}\\ \hline
Gene-product                     & Controlled      & 62,064  & 8,899  & 14.3  & 8,206    & 1,474 & 18.0  & 8,732  & 1,533 & 17.6  & 79,002 \\
roles                            & Measured        & 84,358  & 11,741 & 13.9  & 10,792   & 2,228 & 20.6  & 12,235  & 2,538 & 20.7  & 107,385 \\ 
                                 & \textbf{Total}  & \textbf{146,422}	& \textbf{20,640} & \textbf{14.1} &	\textbf{18,998} & \textbf{3,702} & \textbf{19.5} & \textbf{20,967} & \textbf{4,071} & \textbf{19.4} & \textbf{186,387}\\ \hline
Small mol.                       & Controlled      & 28,932  & 3,600  & 12.4  & 3,511  & 689    & 19.6  & 4,181     & 832    & 19.9  & 36,624 \\
roles                            & Measured        & 8,526   & 1,489  & 17.5  & 912     & 251    & 27.5  & 1,099     & 274    & 24.9  & 10,537 \\ 
                                 & \textbf{Total}  & \textbf{37,458}	& \textbf{5,089} & \textbf{13.6} &	\textbf{4,423} & \textbf{940} & \textbf{21.3} & \textbf{5,280} &	\textbf{1,006} & \textbf{19.01} & \textbf{47,161}\\ \botrule

\end{tabular}}{
}
\end{table*}

\subsection{Benchmarking of NLP tasks using SourceData-NLP}\label{sec:nlp-benchmark}

To highlight the usefulness of SourceData-NLP, we conducted a set of experiments to train and benchmark the best existing language models across the different tasks supported by our dataset. Previous works have consistently shown that biomedical language models such as PubMedBERT~\citep{pubmedbert}, and BioLinkBERT ~\citep{biolinkbert} that use a biomedical vocabulary (i.e., where the tokenization model was trained only on biomedical texts) outperform both biomedical language models that use a general vocabulary (BioBERT~\citealp{biobert}; BioMegatron~\citealp{biomegatron}; BioMedRoBERTa~\citealp{biomedroberta}) and general domain language models (BERT~\citealp{bert}; RoBERTa~\citealp{roberta}). Accordingly, the experiments of this work were carried out with the PubMedBERT and BioLinkBERT models using both their large and base versions.

We perform the benchmarking on four tasks: splitting the caption into panels ("panel segmentation"), NER, and the novel semantic interpretation of empirical roles. The best-performing model for each task is available on the EMBO repository hosted on the HuggingFace transformers Hub\footnote{https://huggingface.co/EMBO}. We next describe the fine-tuning procedures in detail.

\subsubsection{Panel segmentation, NER, and semantic interpretation of empirical roles}\label{sec:token-class-experiment}

All the tasks are formulated as token classification problems. In this configuration, the language models are fine-tuned to assign labels to individual tokens. Labels are assigned based on the IOB schema~\citep{biotagging}, which denotes the (B)eginning tokens of entities, tokens (I)nside entities, and tokens (O)ut of entities.

For the panel segmentation task, the entire figure caption is used as input for the model. Tokens are labeled as {\verb'O'} or {\verb'B-PANEL_START'} to indicate the starting position of each panel.

The tasks of NER and semantic interpretation of empirical roles utilize the text of distinct panels as input. Subsequently, tokens are categorized into one of eight distinct and non-overlapping classes. These include \verb'O', signifying no biological entity of interest; \verb'B-GENEPROD' and \verb'I-GENEPROD', representing gene products; and additional classifications for other biological entities like subcellular components, cell types, cell lines, tissues, organisms, diseases, and experimental assays, as annotated in SourceData-NLP.

The task of assigning empirical roles to the entities is specifically applied to gene products as they are by far the most prevalent entities with these roles. We approach this task in three different ways. First, we make the semantic classification task purely dependent on context, masking the gene product entities with the special {\verb'[MASK]'} token and assigning the labels {\verb'CONTROLLED_VAR'}, {\verb'MEASURED_VAR'}, or {\verb'O'} to them. Secondly, we mark the position of entities with a special token without masking them. In this case, the models can learn both from context and from the identity of the entities. Finally, we train the models to recognize the roles of gene products without any extra information, forcing the models to perform both, NER and role interpretation on a single step.

To ensure reproducibility, we maintained the same fine-tuning hyperparameters for all tasks as summarized in Table~\ref{tab:hyperparam}.  Model parameters were optimized with AdamW with the cross-entropy loss. To avoid memory issues the large version of the models were trained using a batch size of 8 compared to 32 for the base model. To achieve comparable results between both model sizes, we used the same learning rate and 4 gradient accumulation steps for training large models. The reported results are the average F1 scores obtained from 5 consecutive runs of the experiments using different random seeds.

Training is carried out on an NVIDIA DGX Station with 4 Tesla V100 GPUs for 2 epochs. Each fine-tuning takes about 20 minutes for a base model and about an hour for a large model, which has approximately 3 times more parameters. 

\begin{table}[]
\processtable{\label{tab:hyperparam}Hyper-parameters used for fine-tuning. The numbers in parenthesis show different values for large (>300M parameters) models.} {\begin{tabular}{@{}ll@{}}\toprule
Parameter search space           & Fixed fine-tune param.\\\midrule
train batch size                 & 32 (8)               \\
eval batch size                  & 64                   \\
epochs                           & 2                    \\
gradient accumulation steps      & 1 (4)                \\
learning rate                    & 1e-4                 \\
lr scheduler                     & cosine               \\
adam                    & 0.9                  \\
adam                    & 0.999                \\
adam                   & 1e-10                \\
weight\_decay                    & 0.0                  \\
adafactor                        & True                 \\\botrule
\end{tabular}
}{}
\end{table}

\begin{table*}[]
\processtable{\label{tab:models}Comparison of the high-level properties of the benchmark models.} {\begin{tabular}{@{}lcclll@{}}\toprule \textbf{Model}              & \textbf{Param.}  & \textbf{Corpora}                                & \textbf{Tokenization}  & \textbf{Pretraining}         & \textbf{Ref.}\\\midrule
BioLinkBERT        & 108M                & BERT + PubMed                           & WP-bio            & PubMedBERT                      & \cite{biolinkbert}\\
BioLinkBERT       & 333M                & BERT + PubMed                           & WP-bio            & PubMedBERT                      & \cite{biolinkbert}\\
PubMedBERT     & 109M                & PubMed                                          & WP-bio            & -                               & \cite{pubmedbert}\\
PubMedBERT     & 333M                & PubMed                                          & WP-bio            & -                               & \cite{pubmedbert}\\\botrule
\end{tabular}}{}
\end{table*}

\end{methods}

\section{Results}

In this section, we first discuss data annotation inconsistencies to be corrected in future dataset releases. We then show the results of the experiments described in Section~\ref{sec:nlp-benchmark}. We discuss several approaches to the NER task in Section~\ref{sec:results-ner}, followed by the panel segmentation results in Section~\ref{sec:results-panelization}. We present the novel task of semantic interpretation of empirical roles in Section~\ref{sec:results-roles}.

\subsection{Data consistency report}\label{sec:data-consistency}

The SourceData annotation workflow includes a quality control step in which a second annotator performs spot checks on the annotations. This process helps identify and rectify inconsistencies in the labeling of biological entities that are inevitably introduced by human annotators during the workflow. Since the annotation process spanned several years, the guidelines were continuously refined to address systematic and common annotation errors.

To address systematic errors outside the workflow, we implemented the following corrections:
\begin{itemize}
    \item[1.] Exclusion of panels lacking at least one "measured variable" (the entity under study).
    \item[2.] Verification of entities with different IDs but identical text to ensure that differences were not due to annotation errors.
    \item[3.] Compilation of a list of overly generic terms (e.g., "image," "percent," or "cell" – see the complete list in Appendix~\ref{app:generic-terms}) that annotators commonly used, and removal of annotations associated with those terms.
\end{itemize}

These measures have improved the accuracy of our annotations. However, it's important to emphasize that the effectiveness of these corrections ultimately relies on the expertise and vigilance of our human annotators in identifying patterns and inconsistencies.
 
\subsection{NER}\label{sec:results-ner}

We evaluated the performance of two prominent biomedical language models, PubMedBERT and BioLinkBERT, in the context of the Named Entity Recognition (NER) task. We conducted a comparison between the base version (with 110 million parameters) and the large version (with 330 million parameters) of these models, followed by fine-tuning the pre-trained models for NER using the SourceData-NLP dataset.

In Sections~\ref{sec:multi-class} and~\ref{sec:single-class}, we discuss how the model's performance varies under multi-class training, where one model assigns mutually exclusive labels for all entity classes, and single-class training, which involves training separate models for binary classification of each entity category. Finally, in Section~\ref{sec:memo-vs-gen}, we analyze the model's memorization and generalization capabilities and explore how they are affected when introducing noise into the input data.

\subsubsection{Multi-class NER}\label{sec:multi-class}

In this setup, a single model is trained to assign mutually exclusive labels across all entity categories. For fine-tuning, we added a fully connected classification head and trained the model with the hyperparameters shown in Table~\ref{tab:hyperparam} using the cross-entropy loss function.

The results of this experiment are summarized in Table~\ref{tab:ner} and Figure~\ref{fig:multi-vs-single}. The models performed very similarly. The large versions tend to perform slightly better, a tendency that is more marked for BioLinkBERT than for PubMedBERT. The disease class benefits the most from using larger language models.

Comparison across entity classes shows that the performance of the NER task varies significantly across different entity categories. The best performances are obtained with gene products, cell lines, organisms, and small molecules. On the opposite end, the performance is lowest for subcellular components, cell types, diseases, and experimental assays. There are several likely explanations for these differences. The entities that are easier to learn are also the most frequently annotated in our dataset. In addition, entities such as gene products and cell lines tend also to have a simpler structure and morphology (for example "Creb1" or "HeLa") than complex terminologies that are typical for subcellular structures, diseases, and cell types (for example "endoplasmic reticulum" or "hippocampal CA1 pyramidal neuron"). 

For gene products, cell lines, and organisms, our models' performance is comparable to that of previous studies ~\citep{biored,Sharma2019BioFLAIRPP,jnlpba-a, jnlpba-b,kebiolm}. However, for small molecules and diseases, our performance is lower than in prior work. For example, BioMegatron~\citep{biomegatron} obtains an F1 score of 88.5 for small molecules and SparkNLP~\citep{sparknlp} a 90.5 for diseases. The reason is likely that these entities appear less frequently in our dataset, while the mentioned works use datasets explicitly tailored for the mentioned entity types. We note that directly comparing the performance of these models is difficult, as they were trained and evaluated using different methodologies on datasets that differ in their complexity.

\begin{table}[]
\processtable{\label{tab:ner}Benchmarking base and large versions of 
PubMedBERT and BioLinkBERT on the NER task.
The values show the F1 scores for each entity category, highlighting the best in bold.
 shows the improvement of the large model compared to the base.} {
\begin{tabular}{@{}l|rrr|rrr|r@{}}\toprule
\textbf{Model} & \multicolumn{3}{c|}{\textbf{PubMedBERT}} & \multicolumn{3}{c|}{\textbf{BioLinkBERT}} & \multicolumn{1}{c}{}        \\
Model Size     & base     & large     &      & base     & large     &      & \multicolumn{1}{r}{Support} \\\midrule
Gene product           & 91.9 & 92.3          & 0.43 & 89.2          & \textbf{92.7} & 3.78  & 26,321 \\
Cell line              & 89.8 & 90.4          & 0.66 & 90.7          & \textbf{90.8} & 0.11  & 2,367  \\
Organism               & 86.7 & \textbf{87.8} & 1.25 & 87.7          & 87.5          & -0.23 & 4,222  \\
Small molecule         & 84.8 & 85.0          & 0.24 & 82.9          & \textbf{86.2} & 3.83  & 6,932  \\
Tissue                 & 82.6 & \textbf{83.5} & 1.08 & 82.9          & 83.4          & 0.60  & 3,851  \\
Subcellular            & 78.8 & 79.4          & 0.76 & \textbf{79.6} & 79.4          & -0.25 & 4,671  \\
Cell type              & 72.1 & \textbf{72.6} & 0.69 & 71.4          & 72.2          & 1.11  & 2,872  \\
Disease                & 66.6 & 68.6          & 2.92 & 66.1          & \textbf{69.5} & 4.89  & 602     \\
Exp. Assay             & 67.3 & 67.8          & 0.74 & 67.1          & \textbf{68.7} & 2.33  & 11,196 \\\midrule
\textbf{Micro avg.}    & 83.7 & 84.2          & 0.59 & 82.5          & \textbf{84.7} & 2.60  & 63,034 \\ 
\textbf{Macro avg.}    & 80.1 & 80.8          & 0.87 & 79.7          & \textbf{81.2} & 1.85  & 63,034 \\
\textbf{Weighted avg.} & 83.6 & 84.2          & 0.71 & 82.4          & \textbf{84.6} & 2.60  & 63,034  \\\botrule
\end{tabular}
}{}
\end{table}

\begin{figure}\centering
\includegraphics[width=1.\columnwidth]{figures/multi_vs_single_ner.eps}
\caption{F1 scores for individual entity categories are presented, as acquired through the application of multi-label models (depicted by boxes that include both base and large versions of PubMedBERT and BioLinkBERT) and single-label models (indicated by red dots from the BioLinkBERT large single-label models). The collective performances of the multi-label models are represented by solid lines for the micro average, dashed lines for the weighted average, and dotted lines demonstrating the macro average.}
\label{fig:multi-vs-single}
\end{figure}

\subsubsection{Single-class NER}\label{sec:single-class}

Prior work shows that models trained jointly on all entity classes to assign mutually exclusive labels can outperform ensembles of single-class models~\citep[e.g.,][]{mtbioner, Tong2021AMA}. We tested this hypothesis by training BioLinkBERT on each category individually and comparing it to our multi-class model. We used the same training hyperparameters shown in Table~\ref{tab:hyperparam}.

On average, the single- and multi-class models showed no significant difference (Table~\ref{tab:multi-vs-single} and Figure~\ref{fig:multi-vs-single}.).

In summary, our multi-class model achieved comparable performance to nine single-class models while being nine times more computationally efficient. We conclude that multi-class models are a superior strategy.

\begin{table}[!h]
\processtable{\label{tab:multi-vs-single}F1 scores of single- and
multi-class models for each entity category. We show the percentage 
difference between both ( for the mean multi-model 
and  for the best multi-model), estimated as 
. The best scores are highlighted in bold. 
The multi-class F1 scores are obtained as the macro-averages 
of the four models trained in Section~\ref{sec:multi-class}.} {\begin{tabular}{@{}lrrc|rc@{}}\toprule
               & Single         & Multi        &  & Multi & \\\midrule
Gene product   & 91.8           & 91.5              & -0.3     &  \textbf{92.7}  & 1.0\\
Cell line      & \textbf{91.3}  & 90.4              & -1.0     &  90.8           & -0.6\\
Organism       & 87.4           & 87.4              & 0.0      &  \textbf{87.8}  & 0.5\\
Small Molecule & 85.0           & 84.7              & -0.3     &  \textbf{86.2}  & 1.4\\
Tissue         & \textbf{83.5}  & 83.1              & -0.4     &  \textbf{83.5}  & 0.0\\
Subcellular    & 78.7           & 79.3              & 0.8      &  \textbf{79.6}  & 1.1\\
Cell type      & 72.4           & 72.1              & -0.4     &  \textbf{72.6}  & 0.3\\
Disease        & 67.4           & 67.7              & 0.4      &  \textbf{69.5}  & 3.0\\
Exp. Assay     & 68.2           & 67.7              & -0.7     &  \textbf{68.7}  & 0.7\\\hline
Macro average  & 80.6           & 80.5              & 12.4     &  \textbf{81.3}  & 0.8\\ \botrule
\end{tabular}}{}
\end{table}

\subsubsection{Memorization vs generalization}\label{sec:memo-vs-gen}

The relative contribution of memorization versus generalization in NER transformer models is not fully understood. Previous work by \cite{Kim2021HowDY-memory} illustrates that such models do not generalize well due to their propensity to take advantage of dataset biases or substandard naming conventions. In contrast, \cite{Tnzer2021MemorisationVG}  suggests that models can be robust to noisy data, assuming the patterns they learn are sufficiently frequent. To investigate these two aspects of the learning process, we evaluate the memorization and generalization capabilities of PubMedBERT and BioLinkBERT, both trained on the SourceData-NLP dataset.

Following \citep{Kim2021HowDY-memory}, we divided test set entities into "memorization" and "generalization" subsets. Entities were considered memorized if they were encountered during fine-tuning training or if they were present in the validation set. In contrast, the generalization subset is exclusively composed of entities that the model has never seen in either the training or the validation set. 

The results are shown in Fig.~\ref{fig:memo-vs-gen}. Both for PubMedBERT and BioLinkBERT the F1 scores are better on entities of the memorization subset than from the generalization subset. Both large versions perform better than the base versions. In addition, the BioLinkBERT models generalize better to novel entities than PubMedBERT. The results suggest that the initial training of BioLinkBERT has positively affected its generalization capacities.

Prior studies suggested that adding noise to AI datasets can improve generalization \citep[e.g.,][]{guozhong96,Ciresan2010DeepBS,Zur2009NoiseIF}. We hypothesized that adding noise to the dataset could boost the models' generalization capabilities while leaving its memorization essentially unchanged, thereby improving overall performance. To test this hypothesis, we use targeted masking to randomly mask a fraction of entities in the training set, thus encouraging the model to learn more from the context. Probabilistic masking was performed at the training stage on each batch so that the masking was different for every batch and epoch. We then test the model's performance on the test set using no masking at inference time and compare the results across several masking probabilities. 

As shown in Figure~\ref{fig:memo-vs-gen-masking}, no improvement in generalization was observed irrespective of models and model size or masking probability. These results also hold for every class separately. It might be that our datasets include already sufficient intrinsic noise (see Section~\ref{sec:data-consistency}) and therefore adding further masking generates a second noise source that further confuses the models. 

\begin{figure}\centering
\includegraphics[width=1.0\columnwidth]{figures/memo_vs_gen.eps}
\caption{The figure illustrates the contrast between memorization and generalization in language models by displaying the weighted average F1 scores of PubMedBERT (represented by circles) and BioLinkBERT (represented by squares). The base models are portrayed in black, and the large versions are rendered in red.}
\label{fig:memo-vs-gen}
\end{figure}

\begin{figure}\centering
\includegraphics[width=1.0\columnwidth]{figures/memo_vs_gen_masking.eps}
\caption{The performance in terms of general (depicted in red), memorization (in blue), and generalization (in green) is demonstrated through the F1 score of PubMedBERT base, measured against the masking probability.}
\label{fig:memo-vs-gen-masking}
\end{figure}

\subsection{Panel segmentation task}\label{sec:results-panelization}

The panel segmentation task splits figure captions into their constituent panels by assigning a \verb|B-PANEL_START| label to the first token of each panel. We fine-tuned BioLinkBERT and PubMedBERT to evaluate how well these models would perform. Both the models perform similarly as shown in Table~\ref{tab:panelization-results}. We also studied how well they could memorize versus generalize for this task. Panel separators in the biomedical literature are not very varied and tend to be consistent (e.g. A, a),  (A), (a)), so most examples were considered "memorized." "Generalized" examples were figures without explicit panel separators, where any word might indicate a new panel. This typically happens with content from publishers who do not include the panel separators in their XML content but only as style CSS classes on the live website. Some high-order panel separators (e.g. Z, O) also required generalization.

The models' overall F1 scores were similar for memorized and generalized examples, as expected given that memorized examples made up over 95\% of the data. However, for generalized examples, F1 scores were ~4 points higher on average. These encouraging results show that these language models can distinguish panels based on context alone.

\begin{table}[]
\processtable{\label{tab:panelization-results}Results of the panel segmentation task for overall,
memorized tokens and non-memorized (generalization) tokens.
The best-performing model for each case is shown in bold.} {
\begin{tabular}{@{}llrrr@{}}\toprule
\textbf{Pretrained model} & \textbf{Size} & \textbf{F1 overall} & \textbf{F1 memo.} & \textbf{F1 gen.} \\\midrule
PubMedBERT  & base  & 90.5          & 90.1          & 94.3          \\
PubMedBERT  & large & 91.5          & 91.2          & 95.2          \\
BioLinkBERT & base  & 90.8          & 89.3          & 95.7          \\
BioLinkBERT & large & \textbf{92.4} & \textbf{92.0} & \textbf{96.5} \\\botrule
\end{tabular}
}{}
\end{table}

\subsection{Semantic interpretation of empirical roles}\label{sec:results-roles}

In the context of the SourceData-NLP project, we introduced a novel NLP task to determine the empirical roles of entities within biomedical experiments (see section~\ref{sec:token-class-experiment} above). This task, which uses token classification, assigns the "measured variable" or "controlled variable" roles to entities based on context. In this task, entities that cannot be assigned one of these two roles are assigned the "none" category. This task is challenging, even for curators, as it involves the interpretation of the experimental design and is highly dependent on contextual information. We were therefore interested in investigating to which extent this task can be learned from the SourceData-NLP dataset.

Our experiments utilized both base and large versions of PubMedBERT. We primarily focused on gene products due to their balanced representation in the dataset (42\% as the "controlled variable" and 58\% as the "measured variable").

We explored three distinct approaches for training:
\begin{enumerate}
    \item\textbf{Context-only approach}: all gene products tagged in the SourceData-NLP dataset were first masked. The pre-trained models were then fine-tuned to classify these masked entities into "measured variable", "controlled variable" or "none". This approach guarantees that the models learn the classification task solely based on context, without the influence of the identity of the specific entities.
    \item \textbf{Marked-entity approach}: gene products were first flanked by a special token to explicitly mark their position in the text. The pre-trained models were then fine-tuned \textit{without} any entity masking. This allows the models to learn both from context and from the identity of the entities.
    \item \textbf{Single-step approach}: in contrast to the previous approaches, here entities were not masked and their position was not marked. The models were therefore trained to both recognize the entity and classify their role in a single step, thus combining NER with semantic role classification in a joint task.
\end{enumerate}

In the inference stage, to identify entity mentions in the text for both context-only and marked-entity tasks, a prior NER step is required. However, the single-step approach handles both tasks at the same time, eliminating the need for a multi-stage pipeline workflow.

The results of these experiments are detailed in Table~\ref{tab:roles-gp}. Since the first and second approaches require a prior NER step during inference, we also report a combined score for the pipeline by multiplying the F1 scores of the role classification step by the respective NER scores (Table~\ref{tab:ner}). The best approaches were the pipeline methods ("context-only" and "marked-entity" approach). The marked-entity approach, which provides information about the identity of the entity, was only slightly better than the masked-entity approach, suggesting that in the case of gene products, this task is largely dominated by contextual information.

\begin{table*}[]
\processtable{\label{tab:roles-gp} Micro averaged F1 scores of the empirical role of entities task for gene products for the context only, marked-entity, and single-step approaches. For the approaches that need NER on inference (context only and marked-entity), we also provide the corrected F1 score by multiplying the F1 scores of the role classification step by the respective NER score in Table~\ref{tab:ner}.} {


\begin{tabular}{clllll}\toprule
\textbf{Model}             & \multicolumn{5}{c}{Micro averaged F1}                                                 \\ \cline{2-6} 
\textbf{}                  & Context-only & Context only corr. & Marked-entity & Marked-entity corr. & Single-step \\\midrule
\textbf{PubMedBERT-base}   & 91.2         & 83.8               & 91.6          & 84.2                & 81.4        \\
\textbf{PubMedBERT-large}  & 91.0         & 84.0               & 92.3          & 85.2                & 82.5        \\
\end{tabular}

}{}

\end{table*}

\section{Discussion}

The SourceData-NLP is a unique dataset in the field of cell and molecular biology. It was created by embedding a systematic data curation process within the publishing workflow. The construction of the dataset is a annotation process, encompassing figure partitioning, biological entity tagging, external identifier linking, and categorization based on experimental roles. The curation process focused on the figures in the published papers, which are directly linked to the experimental evidence presented. The resulting annotations encapsulate key elements of the experimental design that the reported scientific data.

The merits of integrating curation within the publishing process are underscored by the dataset's scale and utility in training large language models. This sustained curation effort has enabled the production of one of the most extensive NER and NEL datasets currently available in the field of molecular biology and life sciences, emphasizing the potential of this approach. 

The SourceData-NLP contains more than 600,000 disambiguated entities present in 62,543 annotated figure panels from 3,223 papers, making it a useful resource for training NLP models. Its diverse representation of biomedical entities ensures that resultant models are adept at addressing challenges inherent to the field. To facilitate the access of researchers to SourceData-NLP and assist the advancement of biomedical NER and NEL methods, we provide the source code to the raw data\footnote{https://github.com/source-data/soda-data}, machine-learning ready data\footnote{https://huggingface.co/datasets/EMBO/SourceData}, and models generated\footnote{https://github.com/source-data/soda-model}. 

Additionally, we have delineated a novel NLP task centered on the interpretation of the role of entities in a given experimental design. We show that this task, which depends heavily on contextual information, can be efficiently learned to determine whether a gene product is measured (a "measured variable") or whether it is the target of an experimental perturbation (a "controlled variable"). Identifying these roles within a specific experiment allows us to infer the causal hypothesis that is being tested (for example, "Does the perturbation of gene product X influence the measurement of gene product Y?"). 

In biomedical research, testing causal hypotheses is a key experimental approach to investigating molecular mechanisms underlying biological processes and human diseases. The models trained on SourceData-NLP thus open the door to the large-scale extraction of such causal hypotheses from the literature and linking them in a traceable way to published scientific results.

\subsection{Limitations and future work}\label{sec:future} 

One of the limitations of the SourceData-NLP dataset is its inherent noise. The constraints of the publishing process did not allow curation by multiple annotators. Even though an independent validation step by authors and quality control by the editorial office was in place, all discrepancies could not be eliminated. While the SourceData-NLP is already a robust resource, we anticipate that the workflow will benefit from enhanced automated consistency checks.

The scope of the journals from which the dataset was generated is focused on cell and molecular biology. While it provides a rich resource for entities such as small molecules, genes, proteins, subcellular components, and cell lines, its coverage of other biomedical entities like cell types, tissues, organs, and diseases is sparser. A promising avenue to overcome such limitations is to use data augmentation strategies when training models. One possibility is using generative models by generating synthetic examples from an exhaustive dictionary of entities. These synthetic examples could considerably expand the dataset to improve the coverage and robustness of fine-tuned models~\citep[e.g., ][]{guo2023improving,whitehouse2023llmpowered,yuan2023large}. An alternative strategy is to use existing examples as templates in which a fraction of the annotated entities could be replaced by sampling terms of the appropriate type from dictionaries or controlled vocabularies in a process akin to ontology-guide data augmentation \citep{Abdollahi2020OntologyGuidedDA}. This approach would provide a richer and more varied set of examples, potentially enhancing the dataset's depth. A third approach to this problem could be to merge SourceData-NLP with other NER datasets dedicated to the different classes annotated in SourceData-NLP and train the models using partial annotation training~\citep{Ding2023PartialAL}.

While treating the tagging and characterization of biomedical entities as a sequential pipeline shows promising results (see section~\ref{sec:results-roles}), it's plagued by error propagation. The rise of large language models (LLMs) such as GPT and others, has demonstrated their capabilities in various NLP tasks when formulated as text-to-text tasks \citep{t5}. However, their performance on NER tasks has been reported as still remaining subpar~\citep{jimenez22}, especially in the biomedical field~\citep{deusser2023informed}. The rapid progress in LLMs suggests however that the text-to-text approach with generative models may improve in the future, in particular when combined with fine-tuning. 

An exciting potential application of SourceData-NLP is the construction of a large knowledge graph tailored for molecular biology. Knowledge graphs are structured representations of information, where entities and their interrelationships are depicted as nodes and edges, respectively. With the semantic roles obtained from our novel task, we can systematically identify the experimental roles of biological entities in scientific results published in the field of molecular biology literature. This enables the representation of the experimental design in terms of entities linked to the respective "controlled variable" and "measured variable". It is therefore possible to build a knowledge graph with entities as nodes and directional relationships that represent that causal hypothesis tested in the reported experiments. Such a knowledge graph would not only serve as a visual and interactive tool for researchers but also facilitate graph data mining, hypothesis generation, and predictive modeling.

\section*{Acknowledgements}

We thank David Kartchner from the Georgia Institute of Technology for his help with the dataset deposition to the BigBIO biomedical dataset library. We thank Robin Liechti and Lou Götz from the Swiss Institute of Bioinformatics for their help with the SourceData core database, and Alejandro Riera from EMBO for his support with the GPU computing infrastructure. The authors used AI writing assistants (chatGPT4, Claude, Grammarly) to correct and improve sentences or paragraphs in the main text; the entire text was subsequently proofread by the authors.





\bibliography{main}

\newpage
\appendix
\renewcommand{\thesection}{\Alph{section}}
\section{SourceData guidelines\label{app:guidelines}}
\subsection{Introduction}\label{app:introduction}

Experiments in cell and molecular biology involve the empirical manipulation, observation, and description of biological entities. Biological and chemical entities can be entire organisms, a subset of their constituents, or part of the experimental milieu.

\begin{note}
    In this document, the terms entity and component are used interchangeably.
\end{note}

\begin{example}
    Calcium, oligomycin, p53, mitochondria, liver, mus musculus, synapse, and HeLa cells are entities.
\end{example}

\begin{example}
    The cell cycle, apoptosis, wound healing, or type II diabetes are not entities.
\end{example}

SourceData description of the data presented in scientific figures specifies the entities that are relevant to the scientific meaning of the data. Annotation of attributes of such entities, biological processes, or diseases is not yet part of the SourceData specification described in this document.

In the following sections of this document, we define the key concepts used in the SourceData annotation process, including the partitioning of composite figures into coherent panels, the tagging of entities, their assignments to types and roles, and their normalization using external identifiers.

\subsection{Partitioning figures into panels}\label{app:partitioning-figures-into-panels}

Conventional figures are composed of multiple panels and are associated with a description, the figure legend (or figure caption), that explains the content of the figure. While figures tend to present a heterogeneous mixture of experimental designs and assays, individual panels are much more coherent. SourceData annotation is therefore carried out at the level of individual panels.

A panel should be defined as a subset of a full figure such that all of the data points/measurements/observations included in the panel are comparable to each other in a scientifically meaningful way. It is often possible to define a single common observational assay across all observations/data points presented in the same panel. In the majority of cases, panels correspond to the visual panels spontaneously delimited by authors.

Each panel must be associated with its specific panel legend.

\begin{note}
    It is often possible to generate a panel legend by including the appropriate textual fragments of the full figure legend. In some instances, multiple non-contiguous fragments need to be spliced together.
\end{note}

\subsection{Tagging entities}\label{app:tagging-entities}

The primary source of information for SourceData annotation is the text of the panel legend and the image of the figure. Relevant terms from the legend or from the image are attached to a tag that specifies their type and role and that can be further linked to identifiers from external biological databases.

\subsubsection{Tagging terms in figure legends}\label{app:tagging-terms}
To be tagged, a panel must report experimental data. In the text of a panel legend, terms that correspond to specific biological and chemical entities should all be tagged.

\begin{note}
    Panels that present schematics, computational simulation results, overviews, and workflows are not tagged.
\end{note}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-1.eps}
    \caption{Schemes like the one shown above do not need to be annotated.}
    \label{fig:scheme}
\end{figure*}

In general, generic terms referring to broad classes of biological components (e.g., 'proteins', 'cells', 'animals') should not be tagged unless they refer to the object of an assay.

Some terms such as those referring to proteins or genes can be appended with prefixes or suffixes that indicate a post-translational modification, a mutation or other variations of the actual base term. In such cases, pre- or suffixes should be left out and only the base term should be tagged. In other cases, a prefix is added to an entity to denote a species origin, in which case the prefix should be kept.

\begin{example}
    If the cancer-related mutant form of B-RAF is mentioned as \texttt{B-RAF(V600E)} in the text of the legend, the suffix \texttt{(V600E)} indicating the mutation should be ignored and only \texttt{B-RAF} should be tagged. Similarly, if \texttt{p-Akt1} is designating the phosphorylated form of Akt1, only \texttt{Akt1} should be tagged and the prefix 'p-' should be left out.
\end{example}

\begin{example}
    The protein dMyc refers to the Drosophila Myc protein homolog and should be tagged as \texttt{dMyc}.
\end{example}

\begin{note}
    Some components are engineered by assembling or fusing multiple sub-components, which should be tagged individually. For example, the term \texttt{RAS-GFP} referring to a fusion protein between GFP and RAS should be annotated with two tags: \texttt{RAS} and \texttt{GFP}.
\end{note}

\begin{note}
    In some instances, a fusion construct of multiple entities can be created and referred to within the text via a shorthand symbol generated by authors. In this case, floating tags should be created to refer to the individual components of the fusion construct rather than tagging the shorthand symbol as an entity.
\end{note}

\subsubsection{Adding terms missing from the figure legend}\label{app:adding-terms}

Terms can be added as floating tags to complement the description of an experiment with entities that are missing from the text of the legend. These entities typically appear in the image of the figure but not in the legend.

The use of floating tags should be restricted only to entities with role \texttt{controlled variable}, \texttt{measured variable}, or \texttt{experimental variable}. If one or several of the 3 elements are missing, they should be annotated as floating tags.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-2.eps}
    \caption{Key elements missing in the figure caption should be added as a floating tag: in this example, the measured variable component AGO2 is missing and was added as a floating tag.}
    \label{fig:floating_tag}
\end{figure*}

When a generic term is used, such as \texttt{cell} or \texttt{transcripts}, to refer to a specific entity, i.e., a specific cell line or a specific mRNA, a floating tag should be used to refer explicitly to the specific entity.

\subsection{Entity types \& normalization}\label{app:entity-types-normalization}

\subsubsection{Types}\label{app:types}

Entities are assigned to one of seven types spanning successive levels of biological organizations (Table~\ref{tab:kb}). Each type is mutually exclusive. If an entity is linked to an identifier from an external resource, it should use the resource associated with its type according to Table~\ref{tab:kb}.

\begin{example}
    \texttt{ATP} is a small molecule, \texttt{creb1} is a gene, \texttt{CREB1} is a protein, \texttt{the Golgi apparatus} is a cellular component, \texttt{HEK293} is a cell line, the \texttt{retina} is a tissue, and \texttt{Saccharomyces cerevisiae} and \texttt{PhiX174} are organisms.
\end{example}

If an entity does not fit any of the predefined types, the undefined type is assigned. By definition, undefined entities cannot be linked to any external resources.

\begin{note}
    Undefined entities are tagged as such to enable a retrospective analysis of whether additional resources and types should be added in a subsequent version of the SourceData model.
\end{note}

In omics experiments, the number of entities measured is too large to be all listed explicitly. It is then possible to represent the experiments by adding as a floating tag the reserved words \texttt{multiple components} and assigning the appropriate type.

\subsubsection{Linking to standard identifiers (normalization)}\label{app:linking}

In the normalization process, entities should be linked to one or several identifiers of the external resources corresponding to the entity's type (see Table~\ref{tab:kb}). If an entity is linked to multiple identifiers, it must mean that there is uncertainty about the exact identity of the entity.

\begin{example}
    If the term \texttt{Akt} is used to refer to the mouse protein Akt, it is unclear whether it refers to the Akt1, Akt2, or Akt3 isoform. As such, the term will be normalized to the external identifiers \texttt{Uniprot:P31750; Uniprot:Q60823; Uniprot:Q9WUA6} corresponding to Akt1, Akt2, and Akt3, respectively.
\end{example}

\begin{note}
    In the case of entities that are normalized to identifiers from ontologies and taxonomies (subcellular components, cell types, tissues, and organisms), uncertainty about the identity of the entity should be expressed by normalizing it to a sufficiently generic concept in the ontology/taxonomy. For example, the strain \texttt{HSV-1 (F)} does not have a specific entry in the NCBI Taxonomy database but can be normalized to the more generic taxon \texttt{HSV-1[NCBI Taxonomy:10304]}.
\end{note}

Linking reporter components or normalizing components to an external identifier is optional.

Identifiers pointing to curated records of external databases should be preferred over identifiers referring to non-curated records. If relevant records exist both in the primary and secondary resources listed in Table~\ref{tab:kb}, identifiers from the primary resource should be used.

\subsection{Entity roles}\label{app:entity-roles}

Biological components listed in the caption of a figure each play a different role in the experimental design: some components are altered in a controlled manner, others remain untouched by the experimenter, and some are directly or indirectly measured to perform measurements or observations. Accordingly, the following roles are defined:
\begin{itemize}
    \item Biological component
    \item Measured variable
    \item Controlled variable
    \item Reporter component
    \item Normalizing component
    \item Experimental variable
\end{itemize}

\begin{note}
    For all types of entities, if there are multiple instances of the same entity in the figure legend, all instances of the tag should be captured.
\end{note}

\subsubsection{Biological components}\label{app:biological-components}
A biological component is a generic category for any experimentally relevant component that does not fit any of the other defined roles within SourceData. Often it will contain the organism, the cell, or a generic treatment that is present across all conditions.

\subsubsection{Measured variables}\label{app:assayed-components}
A measured variable is the component that is measured or observed.

\begin{example}
    The proteins detected on a Western blot are the measured variables except the loading control, if any, which is considered as a normalizing component (see below).
\end{example}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-3.eps}
    \caption{In the example above, tubulin is the normalizing component.}
    \label{fig:normalizing_western}
\end{figure*}

\begin{note}
    If a molecular marker, for example, the protein \texttt{EEA1}, is measured to visualize a higher order structure, for example, endosomal vesicles, the marker (EEA1 in this example) is tagged as a measured variable. The higher-order structure (endosomal vesicles in this example) is tagged as a measured variable only if it is explicitly highlighted on the image or a property of the entity (such as number/localization) is mentioned in the text of the legend.
\end{note}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-4.eps}
    \caption{In this case, \texttt{HDEL} is an ER marker, so it is captured as a measured variable. In addition, because there is a specific reference to the ER in the legend, ER is also captured as a measured variable. Note that in this example there is a second marker, \texttt{CMAC}, which is, however, a reporter as is therefore captured as a reporter for vacuole, which is labeled as a measured variable.}
    \label{fig:markers}
\end{figure*}

\subsubsection{Controlled variables}\label{app:controlled-variables}
A controlled variable (also called perturbation, intervention, manipulation, alteration, or independent variable) is a component that is experimentally altered. A controlled variable must be targeted and must be controlled. This implies that the experiment must involve the same experimental system across experimental groups and must involve a comparison between several experimental groups to test whether the controlled variable causes an effect on the measured variable.

\begin{example}
    The function of the gene creb1 can be investigated by comparing creb1 wt (control group) to creb1-/- knockout (test group) mice; in this experiment, creb1 is the controlled variable. If, and only if, it is appropriately controlled, the purpose of such an experiment is to infer a cause-and-effect relationship, whether direct or indirect, between the controlled variable and the measured variable.
\end{example}

\begin{warning}
    If a drug (cycloheximide, for example) is applied across all experimental groups, it is not considered a controlled variable, since there is no control group to compare the effect of the drug across conditions. A controlled variable must be controlled. Accordingly, in such a context, the drug should be tagged as a biological component. Similarly, if a cell strain harboring the same genetic mutation is used across all experimental groups, the mutated gene is not a controlled variable but a generic biological component.
\end{warning}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-5.eps}
    \caption{\texttt{MOB1A} and \texttt{LATS2} are both overexpressed across all conditions in this experiment, i.e., they are not controlled for. The only entity that is differentially manipulated in this experiment is \texttt{TRIP6}. Thus, both \texttt{MOB1A} and \texttt{LATS2} are considered biological components and \texttt{TRIP6} is considered the controlled variable.}
    \label{fig:biological_component}
\end{figure*}

\begin{note}
    The target of an experimental manipulation is usually tagged as a controlled variable. Small molecules such as drugs, inhibitors, agonists, and other pharmacological compounds are usually considered as the controlled variable when their effects are compared across experimental groups. An exception is when a small molecule (for example, doxycycline, IPTG, arabinose) is used to manipulate the activity of an engineered circuit controlling the actual entity of interest (for example, a gene whose expression needs to be varied), in which case the entity of interest is considered as the controlled variable and the triggering compound (doxycycline, IPTG, arabinose) is captured as a biological component.
\end{note}

\begin{example}
    If cells are treated with different doses of the PKA inhibitor \texttt{H89}, \texttt{H89} is tagged as the controlled variable. 
\end{example}

\begin{note}
    In experiments that test the action of an entity over time, the entity is tagged as a controlled variable only if a control group is tested or if the \texttt{time = 0} is also shown as a point of comparison.
\end{note}

\begin{example}
    In a siRNA-mediated knockdown experiment, the gene targeted by the siRNA is tagged as a controlled variable.
\end{example}

\begin{note}
    A controlled variable \textit{must} involve controlled experimental conditions. It is therefore common that control experimental groups are treated with a neutral compound, for example, the solvent used to dissolve the administered drug. By convention, such components MUST be assigned the generic role of biological components.
    
    In transfection experiments for overexpression, the main entity of the construct used for transfection should be labeled as a controlled variable of type gene and if detected, the protein should be tagged as a measured variable of type protein.
\end{note}

\subsubsection{Reporter components}\label{app:reporter-components}
A reporter component is used as a proxy to measure or observe indirectly a measured variable of interest to which it is linked as part of a synthetic or engineered construct.

\begin{example}
    A RAS-GFP fusion protein includes the RAS protein as a measured variable and GFP as a reporter component.
\end{example}

\begin{example}
    The luciferase gene can be used as a reporter gene to monitor the transcriptional activity of a given gene promoter, which is the actual measured variable of interest.
\end{example}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-6.eps}
    \caption{In this luciferase experiment, what is being measured is the effect of miR-16-5p on the 3'UTR of Bdnf, so \texttt{Bdnf} is labeled as the measured variable.}
    \label{fig:luciferase}
\end{figure*}

Linking reporter components or normalizing components to an external identifier is optional.

\subsubsection{Normalizing components}\label{app:normalizing-components}
A normalizing component is a component that is assayed to provide baseline measurements from each experimental group so that the data can be normalized across groups.

\begin{example}
    The proteins beta-actin or GAPDH are often assayed to serve as loading control in Western blots and are then tagged as normalizing components.
\end{example}

Linking normalizing components to an external identifier is optional.

\subsubsection{Experimental variables}\label{app:experimental-variables}
When a component is used to compare multiple experimental groups but it is not possible to infer a cause-and-effect relationship between this component and the measured variables of the experiment, the component is said to be an experimental variable.

\begin{example}
    If the expression of a given gene is measured across tissues and cell lines, including liver, muscle, brain, HEK293, and HeLa cells, the tissues or cell types are tagged as experimental variables.
\end{example}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-7.eps}
    \caption{In this example, the expression of \texttt{ArfGAP1} is assayed in several cell types in parallel. This is the prototypical definition of an experimental variable.}
    \label{fig:experimental_variable}
\end{figure*}

\subsection{Tagging experimental assays}\label{app:tagging-experimental-assays}
In addition to entities, SourceData is also experimenting with the tagging of non-entity terms. In particular, the experimental assay used to observe or measure the measured variables of an experiment is tagged and normalized to identifiers either from the BioAssay Ontology (BAO) or from the Ontology for Biomedical Investigations (OBI).

\begin{note}
    At present, SourceData only captures experimental assays used to collect data, i.e., associated with the measured variable. The experimental assay used to induce the controlled variable should not be tagged. In addition, SourceData strives to capture the experimental assay itself and not necessarily the METHOD. If both the assay and the method are explicitly mentioned in the figure legend, SourceData captures both. If not, only the experimental assay is captured, either as a tag in the figure legend or as a floating tag if missing.
\end{note}

\begin{example}
    If a figure represents images from an immunostaining, it suffices to annotate \texttt{immunostaining} as the experimental assay and there is no need to add a floating tag for \texttt{microscopy} if this is not present in the figure legend.
\end{example}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-8.eps}
    \caption{In this example, the experimental assay is \texttt{gene expression measurement} and the method is qRT-PCR. Because qRT-PCR is mentioned in the figure legend, it is captured. However, \texttt{gene expression} is also added as a floating tag to capture the experimental assay for the experiment.}
    \label{fig:assay_method}
\end{figure*}

\subsection{Time-related variables}\label{app:time-related-variables}
Add a floating tag \texttt{time course} or \texttt{age} to indicate a comparison of a controlled variable or a measured variable over time within an experiment.

\subsection{Physical variables}\label{app:physical-variables}
A Physical variable refers to particular experimental conditions, e.g. \texttt{cold exposure}, \texttt{footshock}, etc. Add them only when explicitly mentioned in the figure legend.

\subsection{Special cases: Protein complexes, FACS, cell cycle phases, and DNA staining}\label{sec:special-cases}
Some experimental designs are unique. For the following cases, these guidelines should be observed:

\begin{itemize}
    \item Protein complexes: although the intuitive normalization for protein complexes would be of type protein, it is more adequate to assign them the type \texttt{subcellular component} because the Gene Ontology (GO) database contains normalized references for protein complexes. An example of this would be RNA Polymerase II, which is made up of a number of individual subunits.
    
\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-9.eps}
        \caption{Notice how in this example RNA-Pol-II is assigned the type \texttt{subcellular component}, which can be normalized to the GO database.}
        \label{fig:protein_complexes}
    \end{figure*}

    \item FACS experiments: for FACS experiments, SourceData captures both the cells and the DNA (or whichever element is stained and sorted) as measured variables.
    
\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-10.eps}
        \caption{In this example, HeLa cells are stained with AnnexinV to measure cell death. Both HeLa cells and AnnexinV are captured as measured variables.}
        \label{fig:FACS}
    \end{figure*}

    \item Cell cycle phases: at present, SourceData does not include cell cycle phases as time elements. Cell cycle phases may be added when appropriate to experimental variables or biological components as the class \texttt{unknown}.
    \item DNA staining: for DNA stains like BrdU, EdU, etc., SourceData captures DNA as the measured variable and \texttt{BrdU staining} as the experimental assay.
\end{itemize}

\subsection{Representing 'omics' experiments}\label{app:representing-omics-experiments}
For experiments performing a large number of measurements (>15-20), for example in metabolomics, genomics, transcriptomics, and proteomics, the measured variables cannot be listed individually. The following tentative guidelines are then followed:

\begin{itemize}
    \item The reserved expression \texttt{'multiple components'} should be included as a floating tag, with the relevant entity type, and with the role measured variable.
    
\begin{figure*}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/SoDa-paper-appendix-figure-11.eps}
        \caption{Note how individual metabolites are captured under the expression \texttt{multiple components} in this example.}
        \label{fig:multiple_components}
    \end{figure*}

    \item Both the measurement type (protein expression, protein-DNA interactions, protein-protein interactions, etc...) and the assay technology (experimental platform such as the sequencing platform, mass spec platform, etc...) as mentioned in Materials \& Methods should be tagged as experimental assay, if necessary as floating tag.
    \item The experimental system that is profiled should be added as a biological component, even if it requires adding a floating tag.
\end{itemize}

\begin{example}
    In proteomics, if mass spectrometry is used to measure protein-protein interactions in HeLa cell extracts, for example, the terms \texttt{mass spectrometry} (BAO:0000055) and \texttt{protein-protein interaction assay} (BAO:0002990) will both be added as experimental assay floating tags, as well as \texttt{HeLa} (CL/CVCL:CVCL\_0030) as a general biological component that specifies the experimental system.
\end{example}

\subsection{Guidelines for author queries}\label{app:guidelines-for-author-queries}
When in doubt about the normalization of an entity, authors can be queried via the validation interface. The following should be systematically queried to ensure accurate annotation: disease, cell lines, strains, cell types, and molecules if these are normalized to PubChem.

\section{Removal of generic terms from the annotations}\label{app:generic-terms}

During our examination of the assembled dataset, we noticed that numerous general terms —such as age, DNA, cells, and animals— had been tagged. We chose to omit a subset of these non-specific terms from the dataset. This decision was motivated because these terms are often too vague or encompassing. As such, they are not ideal for generating precise descriptions of experimental setups. These are explicitly marked to guide future annotation efforts by the dataset curators. Additionally, we have created a correction patch that eliminates all tags corresponding to these identified terms, thereby ensuring dataset consistency. A comprehensive, alphabetically ordered list of such terms is shown below. These changes will be available in the version 2.0.2 of SourceData-NLP.

\paragraph{List of omitted non-specific terms:} age, aggregates, all, amino acids, analysis, and, animal, animals, antibiotics, area, assay, bacteria, bacterial, based, body weight, cell death, cell, cells, cellular, chromatin, cleaved, complex, concentration, contralateral, core, count, counts, cytoplasmic, cytosolic, dark, distance, DNA, duct, ductal, ducts, embryo, embryos, female, females, fetal, fetus, fibers, fluorescence intensity, fluorescence, fluorescent, gas, gland, glands, hand, heatmap, IHC, images, images, individual, intensity, ipsilateral, laser, length, level, levels, light, line, littermates, lung tumor, male, males, membrane, micrograph, micrographs, morphology, muscle, muscles, nuclear, number, numbers, parasite, parasites, patient, patients, percent, percentage, percentages phenotype, phenotypes, photograph, photographs, plant,  plants, primary cell,  primary cells, protein, proteins, pulse, puncta, punctae, quantification, ratio, rest, RNA, SEM, sequence, size, stained, staining, stem, strain, tailed, TEM, temperature, test, tests, the, tumor, tumors, virus, viruses, weeks, weight, white.

\end{document}
