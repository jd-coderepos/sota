\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2021}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\setcitestyle{square}



\title{PolarStream: Streaming Lidar Object Detection and Segmentation with Polar Pillars}



\author{Qi Chen\thanks{work done while interning at Motional} \\
  Johns Hopkins University\\
  Baltimore, MD 21218 \\
  \texttt{qchen42@jhu.edu} \\
\And
   Sourabh Vora \\
   Motional \\
   Santa Monica, CA 90401 \\
   \texttt{sourabh.vora@motional.com} \\
   \AND
   Oscar Beijbom \\
   Motional \\
   Santa Monica, CA 90401 \\
   \texttt{oscar.beijbom@motional.com } \\
}

\begin{document}

\maketitle

\begin{abstract}
Recent works recognized lidars as an inherently streaming data source and showed that the end-to-end latency of lidar perception models can be reduced significantly by operating on wedge-shaped point cloud sectors rather then the full point cloud. However, due to use of cartesian coordinate systems these methods  represent the sectors as rectangular regions, wasting memory and compute. In this work we propose using a polar coordinate system and make two key improvements on this design. First, we increase the spatial context by using multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from the past scan. Second, we improve the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods. We also achieve comparable results to existing non-streaming methods but with lower latencies. The code and pretrained models are available at  \url{https://github.com/motional/polarstream}. 
\end{abstract}
\section{Introduction}\label{intro}
The ability to accurately perceive objects in dense urban environments still remains a challenging problem for self-driving cars. While such self-driving cars typically deploy a wide variety of sensors lidars play a key role due to the accurate range information provided. Driven in part by the availability of benchmark datasets \cite{geiger2013vision, caesar2019nuscenes, sun2020scalability}, the last decade has seen tremendous progress in lidar based 3D object detection \cite{zhou2018voxelnet,lang2019pointpillars,yang2018pixor,meyer2019lasernet,fan2021rangedet}. However, these methods all ignore the fact that most lidar sensors scan the scene sequentially as the lidar rotates around the z-axis. They instead wait for the rotational scan to complete (colloquially known as full sweep) before processing data, thereby introducing a large data capture latency (usually 50 to 100 ms). 

First, Han et al. \cite{han2020streaming} and then STROBE \cite{frossard2020strobe} recognized this problem and proposed solutions which processed lidar sectors (shown in Fig. \ref{fig:packets}) as soon as they arrived. They showed that a streaming based architecture can achieve significantly reduced latency over the traditional non-streaming baselines. Both of these methods encode the point clouds as an image in bird's-eye view (BEV) using cuboid-shaped voxels. In doing so, they ignore the natural polar representation formed by the lidar sectors. Using cuboid-shaped voxels restricts them to performing convolutions on the minimal rectangular 

\begin{figure}
  \centering 
\includegraphics[width=\linewidth]{images/comparison_updated.pdf} 
  \caption{Left: An illustration of streaming lidar point clouds on bird's eye view. Lidar point clouds arrive as wedge-shape sectors (shown in gray masks) as the scanner rotates. Previous methods, Han et al.\cite{han2020streaming} and STROBE\cite{frossard2020strobe}, represent the sectors using rectangular regions, wasting half of memory and computation for empty regions. Ours represents the sectors as wedge-shape regions using a polar grid. Right: Comparison of different streaming methods wrt. Panoptic Quality vs End-to-End Lantency as we slice the full sweep into  sectors using the NuScenes\cite{caesar2019nuscenes} val split. The end-to-end latency includes  ms for LiDAR scan and the total runtime of the algorithms.}\label{fig:packets}
\end{figure}\vspace{-3mm}

region enclosing the point cloud sector which wastes both computation and memory. As shown in Fig. \ref{fig:packets}, a large portion of the enclosed rectangular region remains empty. 

Another challenge associated with streaming perception models is the limited view of the scene observed by each sector. Objects close to the ego-vehicle can often be fragmented across multiple sectors as shown by the car highlighted in green in Fig.\ref{fig:packets}. Han et al.\cite{han2020streaming} proposes to increase the context available to the model by maintaining a recurrent memory across consecutive sectors. STROBE \cite{frossard2020strobe} also aggregates representations from the previous sectors by maintaining full-sweep feature maps across multiple scales. However, both these solutions add extra computation.


In this work, we propose to encode individual point cloud sectors using polar pillars. Polar pillars naturally address the inefficiency of existing streaming approaches by representing the point cloud sectors as more compact wedge-shaped regions as shown in Fig. \ref{fig:packets}. Further, we propose a simple minimal-latency approach to enhance the context available to the model by simply padding the representation of the neighboring sectors across multiple strides of the backbone. Using polar pillars allows us to pad features from the preceding sector of the current scan and/or the following sector from the previous scan, no matter how many sectors the full sweep is divided into.

The polar BEV representation has recently started gaining attention in the lidar perception literature primarily because it balances the points across grid cells. In fact, polar grid outperforms the cartesian grid on the lidar segmentation task \cite{zhang2020polarnet, zhou2020cylinder3d}. However, the detection peformance on a polar grid still lags the cartesian grid \cite{alsfasser2020exploiting, chen2020every, rapoport2020s}. This is because of the distortion the objects undergo when this representation is ultimately unfolded to a rectangular representation to enable the use of convolutional layers. The object represented by the green box in Fig. \ref{fig:net} shows an example of this distortion. Further, the distortion increases with range as the pillars progressively become larger. This makes a polar representation not compatible with the translation-invariance property of convolution.

In this work, we propose several techniques to address the distortion problem described above. We first propose a \textbf{Feature Undistortion} module which transforms the polar representation into a canonical Cartesian representation (as shown in Fig. \ref{fig:net}) for classification branch. Next, we propose using the \textbf{Range Stratified Convolution\&Normalization} layers on the regression branches of the detection head. These layers apply different convolution kernels and normalization based on range (Fig.\ref{fig:net}) to cater to the changing pillar sizes in a polar grid. Our proposed model closes the gap on 3D object detection models using cartesian representations without adding any significant latency. 



Finally, we train multitasking streaming models that do simultaneous 3D object detection, lidar segmentation and panoptic segmentation, for the first time in literature. Results on the nuScenes dataset show that our proposed model \textbf{PolarStream} outperforms all streaming methods in both panoptic quality and speed. PolarStream also stays competitive with the top-performing lidar perception methods on the nuScenes leaderboard while being at least twice as fast as the rest. We do several ablation studies and extensive analysis to show the effectiveness of PolarStream.

In summary, our contributions are:
\begin{itemize}[leftmargin=*]
\itemsep0em 
\vspace{-3mm}
\item An efficient streaming based lidar perception models using a polar grid.
\item Multi-scale context padding: an efficient approach to enhance the context of streaming lidar perception models
\item Several improvements to the core problem of applying convolutions on a polar grid: Feature Undistortion, Range Stratified Convolution\&Normalization all add minimal latency to our model.
\end{itemize}


\vspace{-4mm}\section{Related Works}\vspace{-3mm}
\label{related}
\subsection{Non-streaming lidar perception}\vspace{-2mm}

Most lidar perception architectures take inspiration from the image perception literature \cite{ren2016faster,liu2016ssd,lin2017focal}. Some single-stage methods typically convert the point cloud into a bird's-eye view image \cite{zhou2018voxelnet,lang2019pointpillars,yang2018pixor} or a range view image \cite{meyer2019lasernet,fan2021rangedet} and perform detection in those views. The most common paradigm is to convert the lidar point cloud into a BEV image as it offers several advantages like a lack of scale ambiguity, a near lack of occlusion, the ease of fusing HD maps \cite{hdnet} and performing simultaneous detection and trajectory predictions \cite{intentnet, fnf}. To convert the point clouds into a BEV representation, most existing models choose to group the points into voxels. The most commonly used voxels are cuboid-shaped based on Cartesian coordinates. VoxNet \cite{maturana2015voxnet}, MV3D \cite{mv3d}, Pixor \cite{pixor}, Complex-YOLO \cite{complexyolo} represent the cuboid-shaped voxels as occupancy grids. To avoid quantization effects of occupancy grids and extract richer voxel features, VoxelNet \cite{zhou2018voxelnet} samples a fixed number of points within each voxel and applies a simple PointNet \cite{qi2017pointnet} to them. For efficiency, PointPillars \cite{lang2019pointpillars} discretizes the 3D space into pillars so there is only one voxel along the height dimension. 



\vspace{-1mm}
Some recent methods that operate on BEV start to explore polar voxels for point clouds. For 3D object detection, Alsfasser et al \cite{alsfasser2020exploiting} voxelizes points under the Cylindrical Coordinate System, MVF \cite{zhou2019end} adopts both cuboid-shaped voxels and spherical voxels, and CVCNet combines cylindrical and spherical coordinate system into one Hybrid-Cylindrical-Spherical (HCS) coordinate system to detect object from both bird's eye view and range view. On the other hand, the success of PolarNet \cite{zhang2020polarnet} and Cylinder3D \cite{zhou2020cylinder3d} shows the advantage of Cylindrical grids over Cartesian voxels in LiDAR semantic segmentation. Panoptic-PolarNet \cite{zhou2021panoptic} further extends PolarNet to the task of panoptic segmentation.

\vspace{-3mm}\subsection{Streaming lidar perception}\vspace{-2mm}

Streaming lidar perception is relatively new in literature and offers a compelling argument in reducing the end-to-end latency. Han et al \cite{han2020streaming} proposed a couple of enhancements to convert a 3D object detector to operate on streaming data: a) using an LSTM to accumulate features from preceding sectors and b) applying stateful NMS to suppress objects across multiple sectors. STROBE \cite{frossard2020strobe} accumulates features not only from the preceding sectors of the same scan but also from the previous scan by maintaining multi-scale memory feature maps. Features extracted from the current sector is concatenated and fused with the corresponding cropped region in the memory feature maps.

\vspace{-3mm}\section{PolarStream}
\label{methods}\vspace{-3mm}

In this section, we introduce PolarStream, a streaming model based on polar pillars. We introduce how we prepare lidar streaming data in Sec.\ref{input}, polar pillars as a representation for point clouds sectors in Sec.\ref{representation}, the simultaneous detection and segmentation model including techniques to improve detection on a polar grid in Sec.\ref{simultaneous}, and multi-scale context padding to enlarge context in Sec.\ref{sec:padding}. 

\vspace{-4mm}\subsection{Streaming LiDAR Inputs}\label{input}\vspace{-2mm}
Since there is no streaming lidar dataset available, we simulate a streaming system from the NuScenes dataset \cite{caesar2019nuscenes} by slicing the point clouds into n sectors according to their azimuth. As shown in Fig.\ref{fig:packets}, each sector is like a slice of a full pizza. We try  sectors in our experiments, where  means full sweep.  The dataset contains  scenes, comprising  scenes for training,  scenes for validation and  scenes for test. Each scene is of  duration, captured by 32-beam lidar.  frames are annotated in total, including 10 object categories such as cars, motorcycles and pedestrians and six stuff classes such as vegetation and drivable region. We consider 10 object classes for detection, 16 classes in total for semantic and panoptic segmentation.

\vspace{-2mm}\subsection{Polar Pillars}\label{representation}\vspace{-2mm}
The point clouds sector consists of  points, each represented by a vector of point feature , where  is its Cartesian coordinates.  is the polar coordinates.  is the reflection intensity and  is the timestamp when the lidar point is captured. Points are accumulated from 10 successive frames in total to obtain denser point clouds. The points from previous frames are motion-compensated and transformed to current frame. We group the points according to the cylindrical pillar resolution  where  so there is only one pillar along the height dimension. Following MVF \cite{zhou2019end}, we adopt dynamic voxelization to sample all points within each pillar, instead of randomly sampling a fixed number of points per pillar.

\vspace{-3mm}\subsection{Simultaneous Detection and Segmentation}\label{simultaneous}\vspace{-2mm}
We design PolarStream: a simultaneous object detection and segmentation network by extending PointPillars \cite{lang2019pointpillars}, one of the most widely used 3D object detectors balancing accuracy and speed. As shown in Fig.\ref{fig:net}, PolarStream consists of a Pillar Feature Encoder, followed by a 2D CNN backbone and a U-Net\cite{ronneberger2015u} like structure. On top are the detection and segmentation heads. 

\vspace{-3.5mm}\paragraph{Detection Heads}\label{det} We adopt CenterPoint \cite{yin2020center} heads with modifications to make it compatible with polar pillars. To assign targets to the 10-class heatmap to indicate the objects, the gaussian radius of the object center is computed using the span of range and azimuth of the object bounding box, instead of using length and width of the box. Following CenterPoint, we also regress the center offset as , the bounding box size  as , and predict the bounding box height . We regress the relative bounding box orientation  as  and relative velocity as  similar to \cite{rapoport2020s}. Unlike most methods, which use multi-group detection heads that partition object classes to several groups according to their size, we use single-group detection heads to balance accuracy and speed. A comparison against multi-group detection heads is shown in Supplementary. 
For streaming data with , we apply stateful-NMS proposed in Han et al.\cite{han2020streaming}.










\vspace{-3.5mm}\paragraph{Segmentation Head} To extend PointPillars for segmentation, we add a semantic segmentation head in parallel with the detection heads. The segmentation head is made of a single 1x1 convolution layer. The input for the segmentation head is concatenation of the outputs from pillar feature encoder and bilinearly upsampled features from the 2D backbone.

\vspace{-3.5mm}\paragraph{Panoptic Fusion}\label{fusion} Similar to Panoptic-PolarNet \cite{zhou2021panoptic}, for each point belonging to things, we predict the instance id as the box id whose category is the same and center is the nearest. For streaming data with , the panoptic segmentation task is not well defined. For example, the points in the  sector may belong to the box in the  sector if the majority of the box is in the  sector. However, when we are doing panoptic fusion for  sector, we do not have information from the  sector. Therefore we choose global panoptic fusion for streaming point clouds, i.e., we assign instance ids according to the boxes from all sectors of the same sweep. 







\begin{figure}
  \centering 
\includegraphics[width=\linewidth]{images/net_copy.pdf}
  \caption{Simultaneous LiDAR object detection and segmentation network with polar pillars. We adopt the same backbone as in PointPillars\cite{lang2019pointpillars}, and add a semantic segmentation head in parallel with the detection heads. The input wedge-shape pillars are unfolded into a rectangular feature map for convolution. The object (green box) is distorted because one end near the sensor looks bigger and the other end far from the sensor looks smaller. Feature Undistortion is applied to classification head to mimic bilinear sampling and interpolate cartesian pillar features from polar pillar features. Range Stratified Convolution\& Normalization is applied to center offset regression head. }\label{fig:net}
\end{figure}

\vspace{-3.5mm}\paragraph{Multi-Task Learning}

We adopt Focal Loss \cite{lin2017focal} for classification and L1 loss for bounding box regression, orientation and velocity estimation. For segmentation, we use the weighted cross-entropy loss and lovasz-softmax loss \cite{berman2018lovasz}. The total loss is the weighted sum of losses for each component. 

\vspace{-3.5mm}\paragraph{Feature Undistortion}\label{undistortion}
As mentioned in Sec.\ref{intro}, objects have distorted appearances with polar pillars, we propose Feature Undistortion to undistort the features. As shown on the top right of Fig.\ref{fig:net}, the idea of undistortion is to interpolate features at cartesian pillar locations from the original polar pillar locations so that the translation-invariant property of convolution applies. We find the connection of bilinear sampling to convolution and mimic bilinear sampling using convolution. For bilinear sampling, the interpolated features at point  can be sampled from its neighboring points :
\vspace{-1.5mm}\vspace{-1.5mm}
where  is a function of distance(, ). 

We find Equation \ref{bilinear} has the similar form to convolution, except that for convolution  is fixed because same kernel is slided through every location of the feature map. To make  distance-dependent, we tweak Equation \ref{bilinear} by adding a new parameter  so Equation \ref{bilinear} can be rewriten as:
\vspace{-1mm}
where  is conditioned on distance(, ). We model  as the output of a neural network. We build a standalone fully convolutional network  that takes position encodings at  and its neighboring points , i.e.  as input, and output . Simply put:
\vspace{-1.5mm}
To make it more general, we also add a bias term , and another standalone network  so that  and 
\vspace{-1mm}

 and  is trained together with our main network, and during inference  and  are fixed for each location  so it does not need extra runtime for  and . We apply feature undistortion in center heatmap prediction.

\vspace{-3.5mm}\paragraph{Range Stratified Convolution\&Normalization}\label{stratum}
Another challenge with polar pillars is that the center offset is dependent on range and azimuth so it has different statistics at different regions: suppose the heatmap center is at , and the target is at . The center offset is 
\vspace{-1mm}\vspace{-2mm}
For simplicity, assume , i.e. the center offset moves along a circle. Suppose  then 

where  is a small angle and  is the polar pillar angle size. Then 

Similarly, we can derive that  is also dependent on range and azimuth and observe that for Cartesian pillars center offset ranges from -1 to 1 and mean is 0.49 and std is 0.28, while polar pillars center offset ranges from -2 to 2, mean is 0 and std is 0.64. The polar std is much larger than that for Cartesian pillars. Hence it's more difficult to regress center offset based on polar pillars. Based on these observations, we propose Range Stratified Convolution\& Normalization instead of regular convolution and batch normalization\cite{ioffe2015batch}. As shown on bottom right of Fig.\ref{fig:net}, Range Stratified Convolution applies individual kernels at different ranges and Range Stratified Normalization only normalizes over individual regions within certain range instead of entire spatial dimension. We apply Range Stratified Convolution\&Normalization to center offset regression. We also apply Range Stratified Normalization to the shared convolution for detection heads.
\vspace{-2mm}\subsection{Multi-Scale Context Padding}\label{sec:padding}
 \begin{figure}
  \centering 
\includegraphics[width=\linewidth]{images/padding_v2.pdf}\vspace{-2mm}
  \caption{Multi-Scale Context Padding. We present both trailing-edge padding and bidirectional padding. Trailing-edge padding pads current sector with features from preceding sector. Bidirectional padding additionally pads current sector with features from `following' sector of past time frame. Full-sweep feature maps are merged for past time frame and warped to the coordinate system of current time by ego-motion compensation. Context Padding is applied to every convolution in the backbone.}\label{fig:padding}
\end{figure}

\vspace{-2mm}\paragraph{Trailing-Edge Context Padding}As shown in Fig.\ref{fig:padding}, the sector is unfolded to a rectangle feature map on - plane as input for convolution. The lidar sectors arrive one after another by increasing the angle the sensor scans so the unfolded feature map of a sector is spatially connected to its preceding sector along  dimension. This unique property of using polar pillars inspires us to, instead of zero-padding along  dimension, pad the features from preceding sector where it is spatially connected to current sector. The receptive field of a neuron increases as the neural network goes from bottom layer to top and the network encodes multi-scale representation of the input at different stages. This motivates us to pad context from preceding sector before every convolution of the 2D CNN backbone, as illustrated in trailing-edge padding of Fig.\ref{fig:padding}. Although we only pad a few columns to the feature map, the neural network is replenished with sufficient context from multiple ranges and multiple scales at different stages of the network. We keep zero-padding for  dimension and the other end of the  dimension, as the other end of  dimension points to the future sector.



\vspace{-3mm}\paragraph{Bidirectional Context Padding} With trailing-edge padding the current sector is padded with context from preceding sector. To provide further context we pad the leading-edge with warped features from the following sector of the \emph{previous} sweep. To do this we aggregate the full-sweep multi-scale feature maps from the previous sweep and warp the feature maps to the coordinate system of current sweep using ego-motion compensation. We then pad the leading edge of the current sector with the corresponding warped features spatially connected to the current sector.
\vspace{-3mm}

\section{Implementation}\label{implement}\vspace{-2mm}
\subsection{Network Details}\vspace{-2mm}
For polar pillars with  sectors per sweep,  range is m,  rad and m, the pillar size is . For Cartesian pillars, the pillar size is . When ,  range is m,  m and m, leaving same input size of  for both Cartesian pillars and polar pillars when . We find the minimal rectangular region to enclose the sectors when  for Cartesian Pillars. We set segmentation loss weight to 2 and classification loss to 1 for both polar pillars and Cartesian pillars. For Cartesian pillars the bounding box regression weight is 0.25. For polar pillars, since regression is harder, we set the loss weight to 0.5. We make sure they are the best configuration for each setting. For  and  in Feature Undistortion, they share the same architecture: a 3x3 conv followed by 1x1 conv with tanh as activation. We show the network architecture in Supplementary. All runtimes are measured on a single V100 GPU using Pytorch.

\vspace{-3mm}\subsection{Augmentation}\vspace{-2mm}
We adopt class-balanced sampling as proposed in CBGS \cite{zhu2019class}. Before slicing the point clouds into sectors, we conduct random flipping along  axes, scaling with a scale factor sampled from [0.95, 1.05], rotation around  axis between [-0.3925, 0.3925] rad and translation in range  m in  axis. Unlike most methods, we do not use database sampling\cite{yan2018second} for fast training.
\vspace{-3mm}\section{Experiments and Results}\vspace{-3mm}
\label{results}
\subsection{Evaluation}\vspace{-2mm}
We gather the predictions from individual sectors and evaluate PolarStream similar to full-sweep methods. We evaluate 3D detection and lidar semantic segmentation on the NuScenes benchmark \cite{caesar2019nuscenes}. The detection mean average precision (mAP) is based on the distance threshold (i.e.  and ). Additionally, we use nuScenes detection score (NDS) \cite{caesar2019nuscenes}, a weighted sum of mAP and precision on box location, scale, orientation, velocity and attributes. For semantic segmentation, we follow the standard mean intersection-over-union (mIoU) metric. Since nuScenes does not provide instance labels for panoptic segmentation, we follow Panoptic-PolarNet \cite{zhou2021panoptic} to generate labels and evaluate panoptic segmentation on validation split using the Panoptic Quality (PQ) metric.

\subsection{Comparison with other Streaming Methods}

\paragraph{Baselines}
Han et al.\cite{han2020streaming} and STROBE\cite{frossard2020strobe} did not release their code and in addition performed evaluation on two different datasets. To enable benchmarking we re-implemented their methods using the same backbone and input resolution as we use and evaluated on the nuScenes dataset. Specifically we re-implemented stateful-NMS and stateful-RNN of Han el al. and multi-scale memory module in STROBE. We did not implement the HD map branch in STROBE in order to ensure a fair comparison. We also apply stateful-NMS and global panoptic fusion to all the methods in comparison as they are just post-processing techniques. We extend both methods to the task of simultaneous object detection, semantic segmentation and panoptic segmentation. We also provide baselines that simply apply Cartesian pillars or polar pillars to individual point clouds sectors. We compare panoptic quality, segmentation mIoU, detection mAP, NDS with the baselines using  sectors (Tab.\ref{tab:streaming1}). We also show the comparison of our method to Han et al. and STROBE wrt. PQ vs. end-to-end latency (Fig. \ref{fig:packets}).




\begin{table}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \caption{Comparison of streaming methods on nuScenes Val split. CP: context padding; CP x1: trailing-edge padding; CP x2: bidirectional padding. }\label{tab:streaming1}
  \scalebox{0.88}{
  \begin{tabular}{p{2.55cm}|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{2.5cm}{Method} & \multicolumn{6}{c|}{Panoptic Quality (PQ)} & \multicolumn{6}{c}{Segmentation mIoU}\\
    \cline{2-13}
    & 1 & 2 & 4 & 8 & 16 & 32 & 1 & 2 & 4 & 8 & 16 & 32\\
    \hline
    Cartesian Pillars & 67.8 & 66.8 & 67.6 & 65.5 &64.1 & 59.8 & 72.1 & 71.5 & 70.9 &70.2 &68.6 &65.7\\ 
    Han et al.\cite{han2020streaming} & - & 66.3 & 67.4 & 66.2 & 64.6 & 61.6 &- &70.6 &71.5 &70.1 &69.2 &66.5 \\ 
    STROBE\cite{frossard2020strobe} & 63.8 & 65.2 & 66.2 & 65.2 &62.9 & 59.5 &69.2 &69.7 &69.8 &69.6 &67.6 &65.6 \\ \hline
    Polar Pillars & \textbf{68.7} & 68.6 & 67.7 & 66.2 & 63.9 & 60.3 &\textbf{73.4} &73.4 &72.5 &70.8 &69.5 &67.1 \\
Ours-PolarStream & - &\textbf{68.8} &\textbf{69.6} &\textbf{68.8} &\textbf{69.2} & \textbf{68.3} &- &\textbf{73.5}&\textbf{74.2}&\textbf{73.4} &\textbf{73.8} & \textbf{73.1}\\
\hline
    \hline
    \multirow{2}{2.5cm}{Method} & \multicolumn{6}{c|}{Detection mAP} & \multicolumn{6}{c}{ Detection NDS}\\
    \cline{2-13}
    & 1 & 2 & 4 & 8 & 16 & 32 & 1 & 2 & 4 & 8 & 16 & 32\\
    \hline
    Cartesian Pillars & \textbf{52.3} & 51.3 & \textbf{54.9} & 49.7 &52.4 & 47.6 & \textbf{60.7} & 60.3 & \textbf{62} &57.9 &59.5 &57.1\\ 
    Han et al.\cite{han2020streaming} & - & 50.9 & 52.9 & \textbf{53.8} & 52.7 & 50.6 &- &59.6 &60.3 &60.8 &60.3 &58 \\ 
    STROBE\cite{frossard2020strobe} & 46.9 & 48.7 & 49.4 & 47.9 & 45.4 & 42 &53.8 &54.6 &51.5 &48.9 &47.1 &44.8 \\ \hline
    Polar Pillars & 51.2 & 52.1 & 51.9 & 52.5 & 51.9 & 46.7 &60 &\textbf{61.4 }&61.4 &60.9 &60.3 &55.8 \\
Ours-PolarStream & - & 51.5 &53.2 &52.7 &\textbf{53.9}&\textbf{52.4}&-&60.3&61.2&\textbf{60.9}&\textbf{61.4} &\textbf{60}\\
\hline

  \end{tabular}
  }
\end{table}
\paragraph{Results} Tab. \ref{tab:streaming1} shows that PolarStream outperforms all previous streaming methods, including the Cartesian pillars baseline, in both PQ and Segmentation mIoU. When there are more than four sectors in a scene, previous methods as well as the Cartsian/polar pillar baselines show a trend of decreasing PQ and mIoU as the number of sectors increases. However, our PolarStream with bidirectional context padding does not show such a trend: the performance remains almost the same or even better than the full-sweep method. When , PolarStream got  and  improvement in PQ and segmentation mIoU compared to the Cartesian pillars baseline. When sectors become smaller and spatial context becomes limited, the improvement is more significant. When , our PolarStream with Bidirectional Context Padding outperforms all previous streaming methods by a large margin, with  and  improvements in PQ and segmentation mIoU. This shows that our bidiretional context padding makes better use of spatial context compared to previous methods. Our detection NDS is always the highest or at least on par with the highest among all streaming models. Interestingly, Cartesian pillars/Han et al.'s method show higher mAP than ours for 1, 4 and 8 sectors, when the sectors have plenty of spatial view and our context padding does not have many benefits. Our PolarStream outperform all previous streaming methods in detection mAP for 16 and 32 sectors, when spatial view is limited and Bidirectional Context Padding shows more advantages. In addition, the orientation and velocity error of Han et al.'s is on average 14.6\% and 12.9\% higher than ours, which will cause problems for the downstream tracking and prediction tasks. As shown in Fig.\ref{fig:packets}, our methods offers better operating points considering both accuracy and end-to-end latency. Detailed metrics including velocity error and per-class metrics are shown in Supplementary.



\subsection{Discussions on Streaming}
\paragraph{Full Sweep vs. Streaming}
Contrary to the findings of Han et al\cite{han2020streaming}, we saw improved detection performance using 2, 4, 8 and 16 sectors as compared to models trained on full-sweeps. We hypothesize this improvement to less variation in point coordinates within a sector since all sectors are first transformed to a canonical coordinate frame before processing. This suggests that simulating streaming lidars can also serve as an augmentation technique for full-sweep detection.

\paragraph{Diagnosis of Previous Streaming Methods}\label{discussion}

 \begin{figure}
  \centering 
\includegraphics[width=\linewidth]{images/diagnosis_clockwise.pdf}
  \caption{An illustration of how previous streaming methods, Han et al.\cite{han2020streaming} and STROBE\cite{frossard2020strobe} enlarge spatial context of current sector.  }\label{fig:diagnosis}
\end{figure}
We first analyze STROBE's low performance in Tab. \ref{tab:streaming1}. While all other methods aggregate points from past 10 sweeps after ego-motion compensation (\textit{Point Warp}), STROBE processes the points one sweep at a time, and aggregates information from the past sweeps by first transforming the features based on ego-motion (\textit{Feature Warp}) and then fusing them with the current frame. As shown in the full-sweep case in Tab.\ref{tab:streaming1}, all the metrics for STROBE are significantly lower than other methods. We thus find Feature Warp inferior to Point Warp for detection and especially for velocity estimation. The average velocity error (AVE)\cite{caesar2019nuscenes} of STROBE is 0.607 m/s, significantly higher compared to Cartesian Pillars with Point Warp (0.358 m/s). We speculate that this high velocity error is because the feature maps in STROBE don't encode time information on account of processing one sweep at a time as compared to the other methods which encode the time lag for each accumulated point from the past 10 sweeps. For 1, 2, 4 sectors, STROBE enjoys the lowest latency because it processes fewer points compared to other methods, also shown in Fig. \ref{fig:packets}. The pillar feature encoder runs faster. But this advantage disappears for more than 8 sectors because there are also only a smaller number of points processed by all other methods.


Compared to baseline Cartesian Pillars, the method of Han et al. only start to work when more than 8 sectors per sweep. For 2 and 4 sectors, it even hurts the accuracy especially in object detection. This can be explained in Fig. \ref{fig:diagnosis}. For 2 and 4 sectors, the feature map is fully occupied. Adding the pooled features from preceding sectors is like adding noise to current sector, resulting in worse accuracy. Starting from 8 sectors, there is an empty region in current sector so adding the pooled features from preceding sectors is like padding the empty region, and therefore enlarging the context.

\paragraph{How Streaming Models Enlarge Context}
 We further hypothesize not only our method and Han et al work by padding context from preceding sectors, but also STROBE works by padding. As shown in Fig. \ref{fig:diagnosis}, for 2 and 4 sectors when the feature map is fully occupied, fusing features from previous sweep is like densifying the features. Starting from 8 sectors, when there is an empty region, fusing features is like padding the empty region. We argue that all existing streaming methods work by padding, but in different format. STROBE and Han et al. are restricted by the shape of the sectors and require empty region as placeholder, and the padded features are added or fused to the placeholder. Our method pads along the edges of feature maps and is not constrained by the shape of the sector.

\paragraph{Further Thoughts about Context}We argue that context has two aspects. First is its feature values, for the texture information it carries. Second is its spatial relation to the object of interest. Since convolution is translation-invariant, convolutional neural networks alone do not encode spatial relation. The spatial relation is maintained in the spatial arrangement of neurons on the feature map. The stateful-RNN in Han et al. must work together with the empty region as placeholder to maintain the spatial arrangement, while stateful-RNN alone does not encode spatial relation. On the other hand, although our padding along the feature map edges seems simple, it is an effective solution to both add feature values and maintain spatial relation of context.


 \begin{figure}
  \centering 
\includegraphics[width=\linewidth]{images/benchmark.pdf}
  \caption{Comparison of our methods, full-sweep PolarStream (PLS1) and PolarStream with Bidirectional Context Padding using four sectors (PLS4), and other methods on the nuScenes dataset. We compare detection and semantic segmentation on the nuScenes benchmark, and panoptic segmentation on the nuScenes val split. We only compare with methods that report both accuracy and runtime. The methods in comparison are: CenterPoint\cite{yin2020center}(CP), HotSpotNet\cite{chen2020object}(H), CVCNet\cite{chen2020every}(CVC), PointPainting\cite{vora2020pointpainting}(PNT), PointPillars\cite{lang2019pointpillars}(PP),SAPNET\cite{ye2020sarpnet}(S), AF2S3Net\cite{cheng20212}(A), Cylinder3D\cite{zhou2020cylinder3d}(C3D), PolarNet\cite{zhang2020polarnet}(PLN),SPVNAS\cite{tang2020searching}(SPV), SalsaNext\cite{cortinhal2020salsanext}+CBGS\cite{zhu2019class}(S+C), PolarNet+CBGS(P+C) and PaP\cite{zhou2021panoptic}(PaP). We color our methods in green and other methods in red.}\label{fig:benchmark}

\end{figure}


\subsection{Comparison with other Full-Sweep Models}
As the full-sweep 3D object detection and LiDAR semantic segmentation have longer histories compared to streaming models, there are more full-sweep methods in the literature. We also compare with these methods. We present the results of our full-sweep PolarStream model with  (PLS1) and best performing PolarStream model with  (PLS4), with the same backbone as in PointPillars\cite{lang2019pointpillars}.  As shown in Fig. \ref{fig:benchmark}, our method maintains a good balance of runtime and accuracy compared to other methods on both the nuScenes detection and semantic segmentation benchmark. We achieve even faster runtime with PLS4 while preserving almost same accuracy as PLS1. The panoptic segmentation results on the nuScenes val split show that our methods outperform all existing methods in PQ with at least  less runtime. 

We also adopt a heavier 3D ResNet\cite{he2016deep} backbone as in CBGS\cite{zhu2019class} and compare with the state of the art methods for 3D object detection and semantic segmentation in Tab. \ref{tab:heavy}. Our PLS1-heavy is able to match/beat the state-of-the-art models for detection (CenterPoint\cite{yin2020center}) and segmentation (Cylinder3D\cite{zhou2020cylinder3d}) on the nuScenes validation set. In this work, we focus on onboard applications so we only choose the same backbone as in PointPillars\cite{lang2019pointpillars} for streaming.

\begin{table}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \caption{Comparison with state-of-the-art methods on nuScenes Val split.}\label{tab:heavy}
  \scalebox{0.88}{
  \begin{tabular}{p{2.55cm}|c|c|c|c|c|c}
    \hline
    \multirow{2}{2.5cm}{Method} & \multicolumn{3}{c|}{3D Detection} & \multicolumn{3}{c}{Semantic Segmentation}\\
    \cline{2-7}
    & mAP & runtime(Hz) & \#parameters(MB)  & mAP & runtime(Hz) & \#parameters(MB)\\
    \hline
    CenterPoint\cite{yin2020center} & 58.4 & 11 & 96 & - & - & -\\ 
    Cylinder3D\cite{zhou2020cylinder3d} &- & -&- & 76.1 &11 &215\\ 
    Ours-PLS1-heavy & 57.7 & 11 & 96 & 77.7 &11 & 197\\
    \hline
   
  \end{tabular}
  }
\end{table}

\subsection{Ablation Studies}\label{ablated}

\paragraph{The Effect of Multi-Scale Context Padding}
\begin{table}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \caption{Ablation Study of Multi-Scale Context Padding on nuScenes Val split. CP: context padding; CP x1: trailing-edge padding; CP x2: bidirectional padding. }\label{tab:padding}
  \scalebox{0.88}{
  \begin{tabular}{p{2.55cm}|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{2.5cm}{Method} & \multicolumn{6}{c|}{Panoptic Quality (PQ)} & \multicolumn{6}{c}{Segmentation mIoU}\\
    \cline{2-13}
    & 1 & 2 & 4 & 8 & 16 & 32 & 1 & 2 & 4 & 8 & 16 & 32\\
    \hline
    Polar Pillars & 68.7 & 68.6 & 67.7 & 66.2 & 63.9 & 60.3 &73.4 &73.4 &72.5 &70.8 &69.5 &67.1 \\ \hline
Ours-w/ CP x1 & - & 68.6 & 69 & 68.1 & 66.4 & 63.4 &-&73.3 &73.7 &72.6 &71.6 &70  \\
    Ours-w/ CP x2& - &\textbf{68.8} &\textbf{69.6} &\textbf{68.8} &\textbf{69.2} & \textbf{68.3} &- &\textbf{73.5}&\textbf{74.2}&\textbf{73.4} &\textbf{73.8} & \textbf{73.1}\\
   
    \hline
    \hline
    \multirow{2}{2.5cm}{Method} & \multicolumn{6}{c|}{Detection mAP} & \multicolumn{6}{c}{ Detection NDS}\\
    \cline{2-13}
    & 1 & 2 & 4 & 8 & 16 & 32 & 1 & 2 & 4 & 8 & 16 & 32\\
    \hline
    Polar Pillars & 51.2 & 52.1 & 51.9 & 52.5 & 51.9 & 46.7 &60 &\textbf{61.4 }&\textbf{61.4} &60.9 &60.3 &55.8 \\ \hline
 
    Ours-w/ CP x1 & - & \textbf{52.2} & \textbf{53.6} & 52.4 & 52.3 & 49.3 &-&60.3 &61.2 &60.8 &60.4 &58.8  \\
    Ours-w/ CPx2 & - & 51.5 &53.2 &\textbf{52.7} &\textbf{53.9}&\textbf{52.4}&-&60.3&61.2&\textbf{60.9}&\textbf{61.4} &\textbf{60}\\
    
    \hline

  \end{tabular}
  }
\end{table}
As shown in Tab.\ref{tab:padding}, the advantage of Multi-Scale Context Padding starts to show up for 8, 16 and 32 sectors, especially in segmentation. For 32 sectors, when the spatial view is the most restricted, we observe the largest gain in detection mAP and segmentation mIoU. Multi-Scale Context Padding does not improve or hurt the baseline polar pillars model for 2 and 4 sectors, because the network already sees enough spatial view. As we increase the amount of context, from trailing-edge padding to bidirectional padding, we observe more improvements. Surprisingly, we observe that with 2, 4, 8 and 16 sectors, our PolarStream with bidirectional padding even outperforms our full-sweep baseline of polar pillars in all metrics including PQ, detection mAP, NDS and segmentation IoU. It suggests that streaming models can be both faster and more accurate.

 \paragraph{The effect of Feature Undistortion and Range Stratified Convolution\& Normalization} 
 We do the ablation studies with , i.e., the full-sweep case. To show how we close the gap of detection accuracy between polar pillars and Cartesian Pillars, we also list the results of Cartesian pillars with the same architecture and input size. We find that polar pillars outperforms Cartesian pillars in semantic segmentation mIoU (73.2 vs 72.1), which is also found in prior arts\cite{zhang2020polarnet}, because points in the same polar pillar have less disagreement in the semantic label compared to those in a Cartesian pillar. However, polar pillars is less accurate in object detection due to the challenges we discussed. In Tab. \ref{tab:improve} we show either Range Stratified Convolution\& Normalization or Feature Undistortion helps to improve detection accuracy based on polar pillars (by 0.9 and 0.4 mAP respectively). With both techniques combined, we improve detection mAP from  to , narrowing the gap compared to Cartesian pillars (). We also apply both techniques to Cartesian pillars and they do not improve Cartesian pillars, showing they only address the specific challenges of polar pillars, instead of improving the performance by adding more parameters to the network. Our techniques do not add noticeable runtime (0.5ms). In addition, we find detection mAP can be improved when simultaneouly trained with semantic segmentation. The improvement is more significant for Cartesian pillars, but Cartesian pillars suffer from slight drop in segmentation mIoU.
\begin{table*}[h]
  \caption{Ablation Studies on the validation split of nuScenes.}
  \label{tab:improve}
  \centering
  \scalebox{0.81}{
  \begin{tabular}{c|cccc|c|cc}
  Method & seg & det & Stratified Norm\&Conv & Feature Undistortion  & Runtime (ms) & Det mAP & Seg mIoU\\
  \hline
    \multirow{2}{1.8cm}{Polar Pillars} &\checkmark&&    &  &29.5  & -& 73.2 \\
 & &\checkmark&    &  & 38.2 & 48.2 & -\\
  & &\checkmark& \checkmark   &  & 38.4 & 49.1 & - \\
   & &\checkmark&   & \checkmark &  38.4&48.6 & - \\
  & &\checkmark& \checkmark & \checkmark  & 38.7 & 50.3 & - \\
  &\checkmark&\checkmark &\checkmark &\checkmark  & 44.9&51.2  &73.4\\
   \hline
   \multirow{2}{2.3cm}{Cartesian Pillars} &\checkmark & &  &  &  29.8  &- &72.5 \\
  & &\checkmark &  &  & 38.1  &50.6 & -\\
& &\checkmark & \checkmark & \checkmark  & 38.5 & 50.6 & -\\
 &\checkmark & \checkmark&  &  &   44.5  &52.3 &72.1\\
  \hline
  \end{tabular}
  }
\end{table*}








\section{Conclusion}
In this work we propose a streaming model for simultaneous 3D object Detection, Lidar Segmentation and Panoptic Segmentation. Polar pillars is introduced as a more compact representation for lidar sectors compared to previous methods. Multi-scale context padding including trailing-edge padding and bidirectional padding is proposed to enhance spatial context of the streaming model with minimal latency. Additionally we make several improvements, Feature Undistortion and Range Stratified Convolution\& Normalization, to address the problem of applying convolutions on a polar grid. Our model showed significant improvements over previous streaming methods with lower latency.
\bibliographystyle{splncs04}
\bibliography{main.bib}























\end{document}
