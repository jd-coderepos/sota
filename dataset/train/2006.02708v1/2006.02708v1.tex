\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}




\usepackage[preprint]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{natbib}
\usepackage{xcolor}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{soul}

\usepackage{amsmath}
\usepackage{multirow}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\equref}[1]{Eqn.~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}

\setlength{\parskip}{1.5mm}

\graphicspath{{./images/}}


\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etal{et al.}
\def\vs{\emph{vs.}}
\def\wrt{\emph{w.r.t.~}}






\title{Unsupervised Depth Learning in Challenging Indoor Video: Weak Rectification to Rescue}





\author{Jia-Wang Bian, Huangying Zhan, Naiyan Wang, \And Tat-Jun Chin, Chunhua Shen, Ian Reid \\ \\
  University of Adelaide \quad Australian Centre for Robotic Vision \quad TuSimple \\
}

\begin{document}

\maketitle

\begin{abstract}

Single-view depth estimation using CNNs trained from unlabelled videos has shown significant promise. 
However, the excellent results have mostly been obtained in street-scene driving scenarios, 
and such methods often fail in other settings,
particularly indoor videos taken by handheld devices,
in which case the ego-motion is often degenerate, \ie, the rotation dominates the translation.
In this work, we establish that the degenerate camera motions exhibited in handheld settings are a critical obstacle for unsupervised depth learning.
A main contribution of our work is fundamental analysis which shows that the rotation behaves as noise during training, 
as opposed to the translation (baseline) which provides supervision signals.
To capitalise on our findings, we propose a novel data pre-processing method for effective training, \ie, we search for image pairs with modest translation and remove their rotation via the proposed weak image rectification.
With our pre-processing, existing unsupervised models can be trained well in challenging scenarios (\eg, NYUv2 dataset), 
and the results outperform the unsupervised SOTA by a large margin (0.147 \vs\  0.189 in the AbsRel error).

\end{abstract}

\section{Introduction}
\label{sec:intro}

Inferring 3D geometry from 2D images is a long-standing problem in robotics and computer vision.
Depending on the specific use case, it is usually solved by Structure-from-Motion~\cite{schonberger2016structure} or Visual SLAM~\cite{davison2007monoslam, newcombe2011dtam, mur2015orb}.
Underpinning these traditional pipelines is searching for correspondences~\cite{lowe2004distinctive,Bian2019Gms} across multiple images and triangulating them via epipolar geometry~\cite{Zhang1998,hartley2003multiple,bian2019bench} to obtain 3D points.
Following the growth of deep learning-based approaches, 
Eigen~\etal~\cite{eigen2014depth} show that the depth map can be inferred from a single color image by a CNN,
which is trained with the ground-truth depth supervisions captured by range sensors. 
Subsequently a series of supervised methods~\cite{liu2016learning, eigen2015predicting, chakrabarti2016depth, laina2016deeper, li2017two, fu2018deep, Yin2019enforcing} have been proposed and the accuracy of estimated depth is progressively improved.

Based on epipolar geometry~\cite{Zhang1998,hartley2003multiple,bian2019bench}, learning depth without requiring the ground-truth supervision has been explored.
Garg~\etal~\cite{garg2016unsupervised} show that the single-view depth CNN can be trained from stereo image pairs with known baseline via photometric loss.
Zhou~\etal~\cite{zhou2017unsupervised} further explored the unsupervised framework and proposed to train the depth CNN from unlabelled videos. 
They additionally introduced a Pose CNN to estimate the relative camera pose between consecutive frames, and they still use photometric loss for supervision.
Following that, a number of of unsupervised methods have been proposed, which can be categorised into stereo-based~\cite{godard2017unsupervised, zhan2018unsupervised, zhan2019self, watson2019self} and video-based~\cite{Wang2018CVPR, mahjourian2018unsupervised, yin2018geonet, zou2018df, ranjan2019cc, monodepth2, gordon2019depth, chen2019self, zhou2019unsupervised, bian2019depth}, according to the type of training data.
Our work follows the latter paradigm, since unlabelled videos are easier to obtain in real-world scenarios.

Unsupervised methods have shown promising results in driving scenes, \eg, KITTI~\cite{Geiger2013IJRR} and Cityscapes~\cite{Cordts2016Cityscapes}.
However, as reported in~\cite{Zhou_2019_ICCV}, they usually fail in generic scenarios such as the indoor scenes in NYUv2 dataset~\cite{silberman2012indoor}.
For example, GeoNet~\cite{yin2018geonet}, which achieves state-of-the-art performance in KITTI, is unable to obtain reasonable results in NYUv2.
To this end, \cite{Zhou_2019_ICCV} proposes to use optical flow as the supervision signal to train the depth CNN,
and very recent \cite{zhao2020towards} uses optical flow for estimating ego-motion to replace the Pose CNN.
However, the reported depth accuracy \cite{zhao2020towards} is still limited, \ie, 0.189 in terms of \emph{AbsRel}---see also qualitative results in \figref{fig:show}.

Our work investigates the fundamental reasons behind poor results of unsupervised depth learning in indoor scenes.
In addition to the usual challenges such as non-Lambertian surfaces and low-texture scenes, we identify the camera motion profile in the training videos as a critical factor that affects the training process. To develop this insight, we conduct an in-depth analysis of the effects of camera pose to current unsupervised depth learning framework. Our analysis shows that
(i) fundamentally the camera rotation behaves as noise to training, while the translation contributes effective gradients; (ii) the rotation component dominates the translation component in indoor videos captured using handheld cameras, while the opposite is true in autonomous driving scenarios.

To capitalise on our findings, we propose a novel data pre-processing method for unsupervised depth learning. Our analysis (described in \secref{sec:pose-effects}) indicates that image pairs with small relative camera rotation and moderate translation should be favoured.
Therefore, we search for image pairs that fall into our defined translation range, and we weakly rectify the selected pairs to remove their relative rotation. Note that the processing requires no ground truth depth and camera pose. With our proposed data pre-processing, we demonstrate that existing state-of-the-art (SOTA) unsupervised methods can be trained well in the challenging indoor NYUv2 dataset~\cite{silberman2012indoor}. The results outperform the unsupervised SOTA~\cite{zhao2020towards} by a large margin (0.147 \vs\ 0.189 in the AbsRel error).

To summarize, our main contributions are three-fold:
\begin{itemize}
    \item We theoretically analyze the effects of camera motion on current unsupervised frameworks for depth learning, and reveal that the camera rotation behaves as noise for training depth CNNs, while the translation contributes effective supervisions.
    \item We calculate the distribution of camera motions in different scenarios, which, along with the analysis above, helps to answer the question why it is challenging to train unsupervised depth CNNs from indoor videos captured using handheld cameras.
    \item We propose a novel method to select and weakly rectify image pairs for better training. It enables existing unsupervised methods to show competitive results with many supervised methods in the challenging NYUv2 dataset.
\end{itemize}

\section{Analysis}

We first overview the unsupervised framework for depth learning.
Then, we revisit the depth and camera pose based image warping and demonstrate the relationship between camera motion and depth network training.
Finally, we compare the statistics of camera motions in different datasets to verify the impact of camera motion on depth learning.

\subsection{Overview of video-based unsupervised depth learning framework}\label{sec:sc-overview}

Following SfMLearner~\cite{zhou2017unsupervised}, plenty of video-based unsupervised frameworks for depth estimation have been proposed. SC-SfMLearner~\cite{bian2019depth}, which is the current SOTA framework, additionally constrains the geometry consistency over \cite{zhou2017unsupervised},
leading to more accurate and scale-consistent results.
In this paper, we use SC-SfMLearner as our framework,
and overview its pipeline in \figref{fig:sc-overview}.

\paragraph{Forward.}
A training image pair (, ) is first passed into
a weight-shared depth CNN to obtain the depth maps (, ), respectively.
Then, the pose CNN takes the concatenation of two images as input and predicts their 6D relative camera pose .
With the predicted depth  and pose ,
the warping flow between two images is generated according to \secref{sec:proof}.


\paragraph{Loss.}
First, the main supervision signal is the photometric loss .
It calculates the color difference in each pixel between  with its warped position on  using a \emph{differentiable bilinear interpolation}~\cite{jaderberg2015stn}.
Second, depth maps are regularized by the geometric inconsistency loss , where it enforces the consistency of predicted depths between different frames.
Besides, a weighting mask  is derived from  to handle dynamics and occlusions, which is applied on  to obtain the weighted .
Third, depth maps are also regularized by a smoothness loss ,
which ensures that depth smoothness is guided by the edge of images.
Overall, the objective function is:

where , , and  are hyper-parameters to balance different losses. 


\begin{figure}[t]
\centering
\includegraphics[width=0.96\linewidth]{imgs/sc-figure.pdf} 
\caption{Overview of SC-SfMLearner~\cite{bian2019depth}.
Firstly, in the forward pass, training images (, ) are passed into the network to predict depth maps (, ) and relative camera pose .
With  and , we obtain the warping flow between two views according to \equref{eqn:full_tansformation}.
Secondly, given the warping flow, the photometric loss  and the geometry consistency loss  are computed.
Also, the weighting mask  is derived from  and applied over  to handle dynamics and occlusions.
Moreover, an edge-aware smoothness loss  is used to regularize the predicted depth map.
See \cite{bian2019depth} for more details.
}
\label{fig:sc-overview}
\end{figure}


\subsection{Depth and camera pose based image warping}\label{sec:proof}
The image warping builds the link between networks and losses during training,
\ie, the warping flow is generated by network predictions (depth and camera motion) in forward pass,
and the gradients are back-propagated from the losses via the warping flow to networks.
Therefore, we investigate the warping to analyze the camera pose effects on the depth learning, 
which avoids involving image content factors, such as illumination changes and low-texture scenes.


\paragraph{Full transformation.}
The camera pose is composed of rotation and translation components.
For one point (, ) in the first image that is warped to (, ) in the second image.
It satisfies:

where  is the depth of this point in two images and  is the 3x3 camera intrinsic matrix.
 is a 3x3 rotation matrix and  is a 3x1 translation vector.
We decompose the full warping flow and discuss each component below.
 
\paragraph{Pure-rotation transformation.}
If two images are related by a pure-rotation transformation (\ie, ),
based on \equref{eqn:full_tansformation},
the warping satisfies:

where  is as known as the \emph{homography matrix} ~\cite{hartley2003multiple},
and we have


where , standing for the depth relation between two views, is determined by the third row of the above equation,
\ie, .
It indicates that we can obtain (, ) without .
Specifically, solving the above equation, we have



This demonstrates that the rotational flow in image warping is independent to the depth,
and it is only determined by  and .
Consequently, the rotational motion in image pairs cannot contribute effective gradients to supervise the depth CNN during training,
even when it is correctly estimated.
More importantly, if the estimated rotation is inaccurate\footnote{Related work~\cite{zhou2017unsupervised,yin2018geonet,ranjan2019cc,monodepth2, bian2019depth} shows that the Pose CNN enables more accurate translation estimation than ORB-SALM~\cite{mur2015orb}, but its predicted rotation is much worse than the latter, as demonstrated in \cite{bian2019depth,zhan2019dfvo}.}, noisy gradients will arise and harm the depth CNN in backpropagation.
Therefore, we conclude that the rotational motion behaves as the noise to unsupervised depth learning.


\paragraph{Pure-translation transformation.}
A pure-translation transformation means that  is an identity matrix in \equref{eqn:full_tansformation}.
Then we have

where ( ) are camera focal lengths, and (, ) are principal point offsets.
Solving the above equation, we have

It shows that the translation vector  is coupled with the depth  during the warping from (, ) to (, ).
This builds the link between the depth CNN and the warping,
so that gradients from the photometric loss can flow to the depth CNN via the warping.
Therefore, we conclude that the translational motion provides effective supervision signals to depth learning.

\subsection{Distribution of decomposed camera motions in different scenarios}\label{sec:pose-effects}

\paragraph{Inter-frame camera motions and warping flows.}

\figref{fig:pose-statistics}(a) shows the camera motion statistics on KITTI~\cite{Geiger2013IJRR} and NYUv2~\cite{silberman2012indoor} datasets.
KITTI is pre-processed by removing static images, as in done~\cite{zhou2017unsupervised, bian2019depth}.
We pick one image of every 10 frames in NYUv2, which is denoted as \emph{Original NYUv2}.
Then we apply the proposed pre-processing (\secref{sec:method}) to obtain \emph{Rectified NYUv2}.
For all datasets, we compare the decomposed camera pose of their training image pairs \wrt the absolute magnitude and inter-frame warping flow\footnote{We first compute the rotational flow using \equref{eqn:homography}, and then we obtain the translational flow by subtracting the rotational flow from the overall warping flow. Here, the translational flow is also called \emph{residual parallax} in \cite{Li2020MannequinChallengeLT}, where it is used to compute depth from correspondences and relative camera poses.}.
Specifically, we compute the averaged warping flow of randomly sample points in the first image using the ground-truth depth and pose.
For each point (, ) that is warped to (, ), the flow magnitude is .
\figref{fig:pose-statistics}(a) shows that the rotational flow dominates the translational flow in \emph{Original NYUv2} but it is opposite in KITTI.
Along with the conclusion in \secref{sec:proof} that the depth is supervised by the translation while the rotation behaves as the noise,
this answers the question why unsupervised depth learning methods that obtain state-of-the-art results in driving scenes often fail in indoor videos.
Besides, the results on \emph{Rectified NYUv2} demonstrate that our proposed data pre-processing can address this issue.


\paragraph{Warping error sensitivity to depth error.}
Besides the above statistics, we investigate the relation between warping error and depth error.
As the network is supervised via the warping,
we expect the warping error (px) to be sensitive to depth errors.
For investigation, we manually generate wrong depths for randomly sampled points and then analyze their warping errors in all datasets.
\figref{fig:pose-statistics}(b) shows the results,
which shows that the warping error in \emph{Original NYUv2} is about  times smaller than that in KITTI when the sampled points have the same relative error.
This indicates another challenge in indoor videos against driving scenes.
Indeed, the issue is due to the fact that the sensitivity will be significantly decreased when the camera translation is small.
Formally, when  is close to , based on \equref{eqn:translation}, we have:

This causes the warping error hard to separate accurate/inaccurate depth estimates, confusing the depth CNN.
We address this issue by translation-based image pairing (see \secref{sec:select}).
The results on \emph{Rectified NYUv2} demonstrate that the efficacy of our proposed method.


\begin{figure}[t]
\centering
\footnotesize
\begin{tabular}{ccc}
KITTI~\cite{Geiger2013IJRR} & Original NYUv2~\cite{silberman2012indoor} & Rectified NYUv2 \\
R=, T= & R=, T= & R=, T=\\
\includegraphics[width=0.3\linewidth]{imgs/kitti_rt_flows.pdf} &
\includegraphics[width=0.3\linewidth]{imgs/nyu_10_rt_flows.pdf} &
\includegraphics[width=0.3\linewidth]{imgs/nyu_rect_rt_flows.pdf} \\
\multicolumn{3}{c}{(a) Inter-frame camera motion and warping flows} \\
\includegraphics[width=0.3\linewidth]{imgs/kitti_depth_flow_k1.pdf} &
\includegraphics[width=0.3\linewidth]{imgs/nyu_10_depth_flow.pdf} &
\includegraphics[width=0.3\linewidth]{imgs/nyu_rect_depth_flow.pdf} \\
\multicolumn{3}{c}{(b) Warping error with depth error} \\
\end{tabular}
\caption{Camera motion statistics (a) and warping error sensitivity investigation (b).
"Rectified" stands for the proposed pre-processing described in \secref{sec:method}.
In (a), the first row shows the averaged magnitude of camera poses, \ie, \textbf{R} for rotation and \textbf{T} for translation.
The plot shows the distribution of  decomposed warping flow magnitudes (px) over randomly sampled points.
In (b), we manually generate wrong depths for randomly sampled points using the ground-truth depths for investigating the warping errors. Note different scale in vertical axis.
}
\label{fig:pose-statistics}
\vspace{-2mm}
\end{figure}


\section{Proposed data processing}\label{sec:method}


The above analysis suggests that unsupervised depth learning frameworks favour image pairs those have small rotational and sufficient translational motions for training.
However, unlike driving sequences, videos captured by handheld cameras tend to have more rotational while less translational motions, as shown in \figref{fig:pose-statistics}.
In this section, we describe the proposed method to select image pairs with appropriate translation in \secref{sec:select},
and reduce the rotation of selected pairs in \secref{sec:rectify}.














\subsection{Translation-based image pairing}\label{sec:select}

For high frame rate videos (\eg, fps in NYUv2~\cite{silberman2012indoor}),
we first downsample the raw videos temporally to remove redundant images, \ie, extract one key frame from every  frames.
Here,  is used in NYUv2.
The resulting data is denoted as the \emph{Original NYUv2} in all experiments.
Then, instead of only considering adjacent frames as a pair, 
we pair up each image with its following  frames
We also let  in NYUv2~\cite{silberman2012indoor}.
For each image pair candidate, we compute the relative camera pose by searching for feature correspondences and using the epipolar geometry~\cite{hartley2003multiple,bian2019bench}.
As the estimated pose is up-to-scale~\cite{hartley2003multiple},
we use the \emph{translational flow} (\ie, as the same as in \figref{fig:pose-statistics}(a)) instead of absolute translation distance for pairing.
No ground-truth data is required in the proposed method.

First, we generate correspondences by using SIFT~\cite{lowe2004distinctive} features.
Then we apply the \emph{ratio test}~\cite{lowe2004distinctive} and GMS~\cite{bian2019depth} to find good ones.
Second, with the selected correspondences, we estimate the \emph{essential matrix} using the five-point algorithm~\cite{nister2004efficient} within a RANSAC~\cite{fischler1981random} framework,
and then we recover the relative camera pose.
Third, for each image pair candidate, we compute the averaged magnitude of translational flows overall all inlier correspondences, 
which is as the same as in \figref{fig:pose-statistics}(a).
Based on the distribution of warping flows on KITTI that is a good example for us,
we empirically set the expected range as  pixels.
The out-of-range pairs are removed.

Although running Struecture-from-Motion (\eg, COLMAP~\cite{schonberger2016structure}) or VSLAM (\eg, ORB-SLAM~\cite{mur2015orb}) to compute relative camera poses is also possible,
we argue that it is overkill for our problem.
More importantly, these pipelines are often brittle, especially when processing videos with pure rotational motions and low-texture contents \cite{parra2019visual}.
Compared with them, our method does not require a 3D map,
and hence avoiding issues such as incomplete reconstruction and tracking lost.


\subsection{3-DoF weak rectification}\label{sec:rectify}

In order to remove the rotational motion of selected pairs,
we propose a weak rectification method.
It warps two images to a common plane using the pre-computed rotation matrix .
Specifically, (i) we fist convert  to rotation vector  using Rodrigues formula~\cite{trucco1998introductory} to obtain half rotation vectors for two images (\ie,  and ),
and then we convert them back to rotation matrices  and .
(ii) Given , , and camera intrinsic , we warp images to a new common image plane according to \equref{eqn:homography}.
Then in the common plane, we crop their overlapped rectangular regions to obtain the weakly rectified pairs.
See the Matlab pseudo code in the supplementary material.

Compared with the standard rectification~\cite{fusiello2000compact},
our method only uses the rotation  for image warping and deliberately ignores the translation ,
so our weakly rectified pairs have 3-DoF translational motions,
while the rigorously rectified pairs have 1-DoF translational motions, \ie, corresponding points have identical vertical coordinates.
The reason is that we have different input settings (\ie, temporal frames from arbitrary-motion videos \vs\ left and right images from two horizontal cameras)
and different purposes (\ie, depth learning \vs\ stereo matching) with the latter.

On the one hand, due to the rigours 1-DoF requirement in stereo matching,
the standard rectification~\cite{fusiello2000compact} suffers in forward-motion pairs,
where the epipoles lie inside the image and cause heavy deformation, \eg, resulting in extremely large images~\cite{fusiello2000compact}.
Although polar rectification~\cite{pollefeys1999simple} can mitigate the issue to some extent,
the results are still deformed.
However, this issue is avoided in our 3-DoF weak rectification,
as we do not constrain the translational motion.
On the other hand, the rigorous 1-DoF rectification is indeed unnecessary for depth learning.
For example, related methods~\cite{zhou2017unsupervised,yin2018geonet,ranjan2019cc,monodepth2, bian2019depth} work well in KITTI videos,
where image pairs have 3-DoF translational motions,
and the results are comparable to methods those training on KITTI stereo pairs~\cite{garg2016unsupervised, godard2017unsupervised, zhan2018unsupervised}.
Moreover, these methods show that the Pose CNN predicted 3-DoF translation is quite accurate, which even outperforms ORB-SALM~\cite{mur2015orb} on short sequences (\ie, 5-frame segments).

Due to above reasons, we propose the 3-DoF weak rectification,
which reduces the rectification requirement and more suits the unsupervised depth learning problem.
In practice, we still let the Pose CNN to predict 6-DoF motions as all related works~\cite{zhou2017unsupervised,yin2018geonet,ranjan2019cc,monodepth2, bian2019depth},
where we use the predicted 3-DoF rotational motion to compensate the rotation residuals (see \figref{fig:pose-statistics}) caused by the imperfect rectification,
and use the predicted 3-DoF translational motion to help train the depth CNN.



\section{Experiments}
\label{sec:experiment}

\subsection{Method, dataset, and metrics}

\paragraph{Method.}
We use the updated SC-SfMLearner~\cite{bian2019depth}, publicly available on GitHub, as our unsupervised learning framework.
Compared with the original version, it replaces the encoder of depth and pose CNNs with a ResNet-18~\cite{he2016deep} backbone to enable training from the Imagenet~\cite{imagenet_cvpr09} pre-trained model.
Besides, to demonstrate that our proposed pre-processing is universal to different methods,
we also experiment with Monodepth2~\cite{monodepth2} (ResNet-18 backbone) in ablation studies.
For all methods, we use the default hyper-parameters, and train models for  epochs.

\paragraph{NYUv2 depth dataset.}
The NYUv2 depth dataset~\cite{silberman2012indoor} is composed of indoor video sequences recorded by a handheld Kinect RGB-D camera at  resolution.
The dataset contains 464 scenes taken from three cities.
We use the officially provided 654 densely labeled images for testing,
and use the rest  sequences (no overlap with testing scenes) for training () and validation ().
The raw training sequences contain  images.
It is first downsampled  times to remove redundant frames,
and then processed by using our proposed method, resulting in total  rectified image pairs.
The images are resized to  resolution for training.



\begin{table}[t]
\centering
    \caption{Single-view depth estimation results on NYUv2~\cite{silberman2012indoor}.
    As reported in \cite{Zhou_2019_ICCV}, unsupervised methods like GeoNet~\cite{yin2018geonet} fail to show reasonable results in this challenging dataset.
    }
    \label{tab:nyu}
    \scalebox{0.9}{
    \begin{tabular}{l c | c c c  | c c c}
     \toprule[2pt]
     \multirow{2}{*}{Methods} & \multirow{2}{*}{Supervision} & \multicolumn{3}{c|}{Error } & \multicolumn{3}{c}{Accuracy }  \\
     \cline{3-8}
      & & AbsRel & Log10 & RMS  &  &  &  \\
     \hline
     Make3D~\cite{saxena2006learning} & \cmark &  0.349 & - & 1.214 & 0.447 & 0.745 & 0.897 \\
     Depth Transfer~\cite{karsch2014depth} & \cmark & 0.349 & 0.131 & 1.21 & - & - & - \\
     Liu~\etal~\cite{liu2014discrete} & \cmark &  0.335 & 0.127 & 1.06 & - & - & - \\ 
     Ladicky~\etal~\cite{ladicky2014pulling} & \cmark & - & - & - & 0.542 & 0.829 & 0.941 \\
     Li~\etal~\cite{li2015depth} & \cmark & 0.232 & 0.094 & 0.821 & 0.621 & 0.886 & 0.968 \\
     Roy~\etal~\cite{roy2016monocular} & \cmark & 0.187 & 0.078 & 0.744 & - & - & - \\
     Liu~\etal~\cite{liu2016learning} & \cmark & 0.213 & 0.087 & 0.759 & 0.650 & 0.906 & 0.976 \\
     Wang~\etal~\cite{wang2015towards} & \cmark & 0.220 & 0.094 & 0.745 & 0.605 & 0.890 & 0.970 \\
     Eigen~\etal~\cite{eigen2015predicting} & \cmark & 0.158 & - & 0.641 & 0.769 & 0.950 & 0.988 \\
     Chakrabarti~\etal~\cite{chakrabarti2016depth} & \cmark & 0.149 & - & 0.620 & 0.806 & 0.958 & 0.987 \\
     Laina~\etal~\cite{laina2016deeper} & \cmark & 0.127 & 0.055 & 0.573 & 0.811 & 0.953 & 0.988 \\
     Li~\etal~\cite{li2017two} & \cmark & 0.143 & 0.063 & 0.635 & 0.788 & 0.958 & 0.991 \\
     DORN~\cite{fu2018deep} & \cmark & 0.115 & 0.051 & 0.509 & 0.828 & 0.965 & 0.992 \\
     VNL~\cite{Yin2019enforcing} & \cmark & \textbf{0.108} & \textbf{0.048} & \textbf{0.416} & \textbf{0.875} & \textbf{0.976} & \textbf{0.994} \\
     \hline
Zhou~\etal~\cite{Zhou_2019_ICCV} & \xmark & 0.208 & 0.086 & 0.712 & 0.674 & 0.900 & 0.968 \\
     Zhao~\etal~\cite{zhao2020towards} & \xmark & 0.189 & 0.079 & 0.686 & 0.701 & 0.912 & 0.978 \\
     Ours & \xmark &  \textbf{0.147}  &   \textbf{0.062}  &   \textbf{0.536}  &   \textbf{0.804}  &   \textbf{0.950}  &   \textbf{0.986}  \\
     \toprule[2pt]
    \end{tabular}
    }
\end{table}


\begin{table}[t]
\centering
    \caption{Ablation studies on NYUv2~\cite{silberman2012indoor}.
    \emph{Rectified} stands for the proposed data processing.
    Note that Monodepth2~\cite{monodepth2} models often collapse when training from scratch, especially on original data.
    Here, we report the results for their successful case.
    }
    \label{tab:ablation}
    \scalebox{0.8}{
    \begin{tabular}{l | c c | c c c | c c c }
     \toprule[2pt]
     Learning & Training & ImageNet & \multicolumn{3}{c|}{Error } & \multicolumn{3}{c}{Accuracy } \\
     \cline{4-9}
     Framework & Data & Pretraining & AbsRel & Log10 & RMS  &  &  &  \\
     \hline
     \multirow{4}{*}{SC-SfMLearner~\cite{bian2019depth}} & Original & \xmark &   0.188  &   0.079  &   0.666  &   0.712  &   0.918  &   0.973  \\
     & Original & \cmark & 0.166  &  0.071  &   0.621  &   0.755  &   0.934  &   0.981  \\
     & Rectified & \xmark  &   0.170  &   0.072  &   0.603  &   0.752  &   0.930  &   0.980  \\
     & Rectified & \cmark & \textbf{0.147}  &   \textbf{0.062}  &   \textbf{0.536}  &   \textbf{0.804}  &   \textbf{0.950}  &   \textbf{0.986}  \\
     \hline
     \multirow{4}{*}{Monodepth2~\cite{monodepth2}} & Original & \xmark & 0.213 & 0.088 & 0.713 & 0.662 & 0.902 & 0.972 \\  
     & Original & \cmark & 0.182 & 0.076 & 0.642 & 0.721 & 0.934 & 0.982\\
     & Rectified & \xmark & 0.181 & 0.075 & 0.637 & 0.741 & 0.926 & 0.976\\
     & Rectified & \cmark & \textbf{0.157} & \textbf{0.066} & \textbf{0.567} & \textbf{0.783} & \textbf{0.944} & \textbf{0.984} \\
     \toprule[2pt]
    \end{tabular}
    }
\end{table}




\begin{table}[!t]
\centering
    \caption{Single-view depth estimation results on 7 Scenes~\cite{shotton2013scene}.
    The model is pre-trained on NYUv2~\cite{silberman2012indoor}, and on each scene, we fine-tune models for three epochs. 
    As the training data is limited, the fine-tuning consumes less than  minutes.
    }
    \label{tab:7-scene}
    \scalebox{0.9}{
    \begin{tabular}{l c | c c |  c c}
     \toprule[2pt]
     \multirow{2}{*}{Scenes} & \multirow{2}{*}{Training pairs} &\multicolumn{2}{c|}{Before Fine-tuning} & \multicolumn{2}{c}{After Fine-tuning}  \\
     \cline{3-6}
     &  & AbsRel & Acc () & AbsRel & Acc () \\
     \hline
     Chess & 2.6k & 0.169 & 0.719 & \textbf{0.103} & \textbf{0.880} \\
     Fire & 1.5k & 0.158 & 0.758 & \textbf{0.089} & \textbf{0.916} \\
     Heads & 0.5k & 0.162 & 0.749 & \textbf{0.124} & \textbf{0.862}  \\
     Office & 3.1k & 0.132 & 0.833 & \textbf{0.096} & \textbf{0.912} \\
     Pumpkin & 2.3k & 0.117 & 0.857 & \textbf{0.083} & \textbf{0.946} \\
     RedKitchen & 4.9k & 0.151 & 0.780 & \textbf{0.101} & \textbf{0.896} \\
     Stairs & 1.6k & 0.162 & 0.765 & \textbf{0.106} & \textbf{0.855} \\
     \toprule[2pt]
    \end{tabular}
    }
\end{table}


\paragraph{RGB-D 7 Scenes dataset.}
The dataset~\cite{shotton2013scene} contains 7 scenes, 
and each scene contains several video sequences (- frames per sequence), 
which are captured by a Kinect camera at  resolution.
We follow the official train/test split for each scene.
For training, we use the proposed pre-processing,
and for testing, we simply extract one image from every  frames. We first pre-train the model on NYUv2 dataset, and then fine-tune the model on this dataset to demonstrate the universality of the proposed method.

\paragraph{Evaluation metrics.}
We follow previous methods~\cite{liu2016learning,laina2016deeper,fu2018deep, Yin2019enforcing} to evaluate depth estimators.
Specifically, we use the mean absolute relative error (AbsRel),
mean log10 error (Log10), root mean squared error (RMS),
and the accuracy under threshold ( < 
, ).
As unsupervised methods cannot recover the absolute scale, we multiply the predicted depth maps by a scalar that matches the median with the ground truth,
as done in~\cite{zhou2017unsupervised,bian2019depth,monodepth2}.

\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{imgs/vis1.pdf}
\caption{Qualitative comparison of single-view depth estimation on NYUv2~\cite{silberman2012indoor}.
More results are attached in the supplementary material.}
\label{fig:show}
\vspace{-2mm}
\end{figure}



\subsection{Results}\label{sec:results}

\paragraph{Comparing with the state-of-the-art (SOTA) methods.}

\tabref{tab:nyu} shows the results on NYUv2~\cite{silberman2012indoor}. It shows that our method outperforms previous unsupervised SOTA method~\cite{zhao2020towards} by a large margin.
\figref{fig:show} shows the qualitative depth results.
Note that NYUv2 dataset is so challenging that previous unsupervised methods such as GeoNet~\cite{yin2018geonet} is unable to get reasonable results, as reported in \cite{Zhou_2019_ICCV}.
Besides, our method also outperforms a series of fully supervised methods~\cite{liu2016learning, saxena2006learning, karsch2014depth, liu2014discrete, ladicky2014pulling, li2015depth, roy2016monocular, wang2015towards, eigen2015predicting, chakrabarti2016depth}.
However, it still has a gap between the SOTA supervised approach~\cite{Yin2019enforcing}.


\paragraph{Ablation studies.}

\tabref{tab:ablation} summarizes the results.
First, for both SC-SfMLearner~\cite{bian2019depth} and Monodepth2~\cite{monodepth2},
training on our rectified data leads to significantly better results than on original data.
It also demonstrates that the proposed pre-processing is independent to method chosen.
Besides, note that the training is easy to collapse in original data, especially when starting from scratch.
We here report the results for their successful case.



\paragraph{Generalization.}

\tabref{tab:7-scene} shows the depth estimation results on 7 Scenes dataset~\cite{shotton2013scene}.
It shows that our model can generalize to previously unseen data,
and fine-tuning on a few new data can boost the performance significantly.
This has huge potentials to real-world applications,
\eg, we can quickly adapt our pre-trained model to a new scene.







\paragraph{Timing.}
It takes  () hours to train SC-SfMLearner~\cite{bian2019depth} models for  epochs on rectified (original) data, measured in a single 16GB NVIDIA V100 GPU.
Learning curves are provided in the supplementary material,
which show that our pre-processing enables faster convergence.
The inference speed of models is about fps on  images in a NVIDIA RTX 2080 GPU.


\section{Conclusion}
In this paper, we investigate the degenerate motion in indoor videos,
and theoretically analyze its impact on the unsupervised monocular depth learning.
We conclude that (i) rotational motion dominates translational motion in videos taken by handheld devices,
and (ii) rotation behaves as noises while translation contributes effective signals to learning.
Moreover, we propose a novel data pre-processing method,
which searches for modestly translational pairs and remove their relative rotation for effective training.
Comprehensive results in different datasets and learning frameworks demonstrate the efficacy of proposed method,
and we establish a new unsupervised SOTA performance in challenging indoor NYUv2 dataset. 





\bibliographystyle{unsrt}
\bibliography{reference}


\clearpage

\section{Additional details}

\paragraph{Experimental details in Fig. 2.}
First, we follow \cite{zhou2017unsupervised,bian2019depth} to pre-process KITTI~\cite{Geiger2013IJRR} dataset,
where static frames that are manually labelled by Eigen~\etal~\cite{eigen2014depth} are removed from the raw video.
The images are resized to .
The accurate ground truth depth and camera poses are provided by a Velodyne laser scanner and a GPS localization system. 
Second, as the NYUv2~\cite{silberman2012indoor} dataset does not provide the ground truth camera pose, we use the ORB-SLAM2~\cite{mur2015orb} (RGB-D mode with the ground truth depth) to compute the camera trajectory.
The image resolution is .
We down-sample the raw videos by picking first image of every 10 times.
Third, we randomly select one long sequence from the dataset for analysis.
Given a sequence, we randomly sample  valid points (with a good depth range) per image and compute their projection magnitudes (Fig. 2(a)) and projection errors (Fig. 2(b)) using the ground truth.
For box visualization, we randomly sample  points that are collected from the entire sequence.


\paragraph{Implementation details in Sec. 3.}
First, for computing the feature correspondence,
we use the SIFT~\cite{lowe2004distinctive} implementation by VLFeat library.
The default parameters are used.
Second, we use the built-in function in Matlab library to compute the \emph{essential matrix} and relative camera pose.
The maximum RANSAC~\cite{fischler1981random} iterations are ,
and the inlier threshold is .
we use the following pseudo Matlab code to compute the weakly rectified images.

\begin{verbatim}
function [ImRect1, ImRect2] = WeakRectify(Im1, Im2, K, R)


[R1, R2] = computeHalfRotations(R);
    
    H1 = projective2d(K * R1 / K);
    H2 = projective2d(K * R2 / K);

imageSize = size(Im1);
    [xBounds, yBounds] = computeOutputBounds(imageSize, H1, H2);

ImRect1 = transformImage(Im1, H1, xBounds, yBounds);
    ImRect2 = transformImage(Im2, H2, xBounds, yBounds);
end

function [Rl, Rr] = computeHalfRotations(R)
r = rotationMatrixToVector(R);
    
Rr = rotationVectorToMatrix(r / -2);
    
Rl = Rr';
end
\end{verbatim}




\section{Additional results}

\paragraph{Learning curves.}
The following figure shows the validation loss when training on NYUv2~\cite{silberman2012indoor}.
"Rectified" stands for the proposed pre-processing, and "pt" stands for pre-training on ImageNet~\cite{imagenet_cvpr09}.
It demonstrates that training on our rectified data leads to better results and faster convergence, compared with the original dataset.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{imgs/validation-loss.pdf}
\end{figure}


\paragraph{Visualization of single-view depth estimation.}
\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{imgs/vis2.pdf} \\
\caption{More qualitative comparison of single-view depth estimation on NYUv2~\cite{silberman2012indoor}.}
\label{fig:show_supp}
\vspace{-2mm}
\end{figure}
\figref{fig:show_supp} shows more results on NYUv2~\cite{silberman2012indoor}.



\paragraph{Visualization of rectification and fine-tuning effects.}

\figref{fig:vis} shows results.
In NYUv2~\cite{silberman2012indoor}, we train models on both original data and our rectified data. In 7 Scenes~\cite{shotton2013scene}, we fine-tune the model that is pre-trained on NYUv2.
The qualitative evaluation results demonstrate the efficacy and universality of our proposed pre-processing,
and it demonstrates the generalization ability of pre-trained depth CNN in previously unseen scenes. 

\begin{figure}[ht]
\centering
\begin{tabular}{lc}
\rotatebox{90}{\hspace*{16mm} NYUv2} & \includegraphics[width=0.9\linewidth]{imgs/show1.pdf} \\
\rotatebox{90}{\hspace*{16mm} 7 Scenes} & \includegraphics[width=0.9\linewidth]{imgs/show2.pdf} \\
\end{tabular}
\caption{Qualitative results for ablation studies.
In NYUv2~\cite{silberman2012indoor}, we train models on both original data and our rectified data.
In 7 Scenes~\cite{shotton2013scene}, we fine-tune the model that is pre-trained on NYUv2.}
\label{fig:vis}
\end{figure}



\paragraph{Visualization of depth and converted point cloud.}
\figref{fig:demo} shows the video screenshot.
We predict depth using our trained model on one sequence (\ie, office) from 7 Scenes~\cite{shotton2013scene}.
The top shows the textured point cloud generated by the predicted depth map (bottom right) and source image (bottom left). The full video is attached along with this manuscript.


\begin{figure}[t]
\centering
 \includegraphics[width=0.9\linewidth]{imgs/office_02.pdf} \\
\caption{Depth and point cloud visualization on 7 Scenes~\cite{shotton2013scene}. The top shows the textured point cloud generated by the predicted depth map (bottom right) with the source image (bottom left). The full video is also attached.}
\label{fig:demo}
\end{figure}


\end{document}