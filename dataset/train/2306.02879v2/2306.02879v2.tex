
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       


\usepackage{xcolor}         \usepackage{lipsum}  
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\urlstyle{same}
\usepackage{abraces}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color, colortbl}
\usepackage{caption}



\usepackage{enumitem}
\usepackage{afterpage}
\usepackage{tocloft}
\usepackage{rotating}
\usepackage{titletoc}




\definecolor{BoldDelta}{HTML}{287EB8}
\definecolor{CiteBlue}{HTML}{2471a3}
\definecolor{LightGray}{HTML}{F0F1F2} \newcommand{\etall}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\bfparagraph}[1]{\paragraph{\textbf{#1}}}
\newcommand{\bfstart}[1]{\noindent\textbf{#1.}}
\newcommand{\yibing}[1]{{\color{blue} #1}}

\hypersetup{colorlinks,linkcolor={red},citecolor={CiteBlue},urlcolor={red}}  


\title{Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization}
\iclrfinalcopy
\author{Yibing Liu\textsuperscript{1}, Chris Xing Tian\textsuperscript{1}, Haoliang Li\textsuperscript{1}, Lei Ma\textsuperscript{2}, Shiqi Wang\textsuperscript{1} \\
City University of Hong Kong\textsuperscript{1} \&  The University of Tokyo\textsuperscript{2}\\
\texttt{lyibing112@gmail.com,xingtian4-c@my.cityu.edu.hk}\\
\texttt{\{haoliang.li,shiqiwang\}@cityu.edu.hk,ma.lei@acm.org}\\
}











\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle
\vspace{-2mm}
\definecolor{github}{HTML}{ec038a} \hypersetup{colorlinks,linkcolor={red},citecolor={CiteBlue},urlcolor={github}}  
\begin{abstract}




	
	
	


	











	The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, \ie, in-distribution (InD).
	In this paper, we study the OOD problem from a neuron activation view. 
	We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. 
	Then, to characterize the relationship between neurons and OOD issues, we introduce the \textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data.
	Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K).
	2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating model robustness.
	Compared to prevalent InD validation criteria, we show that NAC not only can select more robust models, but also has a stronger correlation with OOD test performance. Our code is available at: \url{https://github.com/BierOne/ood_coverage}.

\end{abstract}



\vspace{-1mm}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
\section{Introduction}
\vspace{-1mm}
\label{Sec:Intro}
Recent advances in machine learning systems hinge on an implicit assumption that the training and test data share the same distribution, known as in-distribution (InD)~\citep{tech:ViT,tech:conv,tech:ResNet,tech:deep_conv}.
However, this assumption 
\setlength\intextsep{2pt}
\begin{wrapfigure}[17]{r}{0cm}
	\centering
	\includegraphics[width=0.38\textwidth]{supplements/Intro_plots.pdf}
	\caption{OOD detection performance on CIFAR-100 and ImageNet. AUROC scores (\%) are averaged over the OOD datasets and backbones.}
	\label{Fig:Intro-OOD-AUROC}
\end{wrapfigure}
rarely holds in real-world scenarios due to the presence of out-of-distribution (OOD) data, \eg, samples 
from unseen classes~\citep{Setup:DG}. 
Such distribution shifts between OOD 
and InD often drastically challenge well-trained models, leading to significant performance drops~\citep{OOD_Problem:ImgNet,OOD_Problem:ML-State}.


Prior efforts tackling this OOD problem mainly arise from two avenues: 1) OOD detection and 2) OOD generalization. The former one targets at designing tools that differentiate between InD and OOD data inputs, thereby refraining from using unreliable model predictions~\citep{OOD_Detect:MSP,OOD_Detect:ODIN,OOD_Detect:Energy,OOD_Detect:GradNorm}. 
In contrast, OOD generalization focuses on developing robust networks to generalize unseen OOD data, relying solely on InD data for training~\citep{Setup:DG,Baseline:CORAL,Baseline:GroupDRO,CL&DG:SelfReg,Baseline:Fish}. 
Despite the emergence of numerous studies, it is shown that existing approaches are often arguable to provide insights into the fundamental cause and mitigation of OOD issues~\citep{OOD_Detect:ReAct,Setup:DomainBed}.








As suggested by~\cite{OOD_Detect:ReAct,OOD_Detect:LINe}, neurons could exhibit distinct activation patterns when exposed to data inputs from InD and OOD (See Figure~\ref{Fig:Intro-Neuron-OOD}). 
This reveals the potential of leveraging neuron behavior to characterize model status in terms of the OOD problem.
Yet, though several studies recognize this significance, they either choose to modify the neural networks~\citep{OOD_Detect:ReAct}, or lack the suitable definition of 
neuron activation states~\citep{OOD_Detect:LINe,NACT_DG:NeuronCoverage}. 
For instance, \citet{OOD_Detect:ReAct} proposes a neuron truncation strategy that clips neuron output to separate the InD and OOD data, improving OOD detection.
However, such truncation unexpectedly decrease the model classification ability~\citep{OOD_Detect:SimpleAct}. More recently, \citet{OOD_Detect:LINe} and \citet{NACT_DG:NeuronCoverage} employ a threshold to characterize neurons into binary states (\ie, activated or not) based on the neuron output. This characterization, however, discards valuable neuron distribution details. 
Unlike them, in this paper, we show that {by leveraging natural neuron activation states, a simple statistical property of neuron distribution could effectively facilitate the OOD solutions.}










We first propose to formulate the neuron activation state by considering both the neuron output and 
its influence on model decisions. Specifically, inspired by~\citet{OOD_Detect:GradNorm}, we model neuron 
\setlength\intextsep{1pt}
\begin{wrapfigure}[15]{r}{0cm}
	\centering
	\includegraphics[width=0.37\textwidth]{supplements/Intro_nac.pdf}
	\caption{NAC models \textit{coverage area} in neuron activation space using InD training data. Upon receiving OOD data, neurons tend to behave outside the expected coverage area, thus with lower coverage scores. }


\label{Fig:Intro-Neuron-Cov}
\end{wrapfigure}
influence as the gradients derived from Kullback-Leibler (KL) divergence~\citep{tech:KL} between network output and a uniform vector. 
Then, to characterize the relationship between neuron behavior and OOD issues, we draw insights from coverage analysis in system testing~\citep{NACT_System:DeepXplore,NACT_System:DeepGauge}, which reveals that \textit{rarely-activated (covered) neurons by a training set can potentially trigger undetected bugs, such as misclassifications, during the test stage}.
In this sense, we introduce the concept of \textit{neuron activation coverage} ({NAC}), 
which {quantifies the coverage degree of neuron states under InD training data} (See Figure~\ref{Fig:Intro-Neuron-Cov}).
In particular, if a neuron state is frequently activated by InD training inputs, NAC would assign it with a higher coverage score, indicating fewer underlying defects in this state. 
This paper applies NAC to two OOD tasks:










\bfstart{OOD detection} 
Since OOD data potentially trigger abnormal neuron activations, they should present smaller coverage scores compared to the InD test data (Figure~\ref{Fig:Intro-Neuron-Cov}). 
As such, we present \textbf{NAC} for \textbf{U}ncertainty \textbf{E}stimation (\texttt{NAC-UE}), which directly averages coverage scores over all neurons as data uncertainty. 
We evaluate \texttt{NAC-UE} over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1k), establishing new state-of-the-art performance over the 21 previous best OOD detection methods.
Notably, our \texttt{NAC-UE} achieves a 10.60\% improvement on FPR95 (with a 4.58\% gain on AUROC) over CIFAR-100 compared to the competitive ViM~\citep{OOD_Detect:ViM} (See Figure~\ref{Fig:Intro-OOD-AUROC}).





\bfstart{OOD generalization} 
Given that underlying defects can exist outside the coverage area~\citep{NACT_System:DeepXplore}, we hypothesize that the robustness of the network increases with a larger coverage area.
To this end, we employ \textbf{NAC} for \textbf{M}odel \textbf{E}valuation (\texttt{NAC-ME}), which measures model robustness by integrating the coverage distribution of all neurons.
Through experiments on DomainBed~\citep{Setup:DomainBed}, we find that a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets. 
Moreover, compared to InD validation criteria, \texttt{NAC-ME} not only selects more robust models, but also exhibits stronger correlation with OOD test performance.
For instance, on the Vit-b16~\citep{tech:ViT}, \texttt{NAC-ME} outperforms validation criteria by 11.61\% in terms of rank correlation with OOD test accuracy.











































































\section{NAC: Neuron Activation Coverage}
\vspace{-1mm}
\label{Sec:Method_Pre}
This paper studies OOD problems in multi-class classification, where  denotes the input space and  is the output space.
Let  be the training set, comprising \textit{i.i.d.} samples from the joint distribution . 
A neural network parameterized by , , is trained on samples drawn from , producing a logit vector for classification.


We illustrate our {NAC}-based approaches in Figure~\ref{Fig:Method}. In the following, we first formulate the neuron activation state (Section~\ref{Sec:Method_NState}), and then introduce the details of our NAC (Section~\ref{Sec:Method_NACFunc}). We finally show how to apply NAC to two OOD problems (Section~\ref{Sec:Method_App}): OOD detection and generalization.


\begin{figure*}
	[t]
	\centering \includegraphics[width=0.97\columnwidth]{supplements/method.pdf} \vspace{-2mm}
\caption{Illustration of our NAC-based methods. NAC is derived from the probability density function (PDF), which quantifies the coverage degree of neuron states under the InD training set .
Building upon NAC, we devise two approaches for tackling different OOD problems: OOD Detection (\texttt{NAC-UE}) and OOD Generalization (\texttt{NAC-ME}).
}
	\label{Fig:Method}
	\vspace{-2mm}
\end{figure*}



\vspace{-1mm}
\subsection{Formulation of Neuron Activation State} 
\vspace{-1mm}
\label{Sec:Method_NState}
Neuron outputs generally depend on the propagation from network input to the layer where the neuron resides. However, this does not consider the neuron influence in subsequent propagations.
{As such, we introduce gradients backpropagated from the KL divergence between network output and a uniform vector~\citep{OOD_Detect:GradNorm}, to model the neuron influence. }
Formally, we denote by  the output vector of a specific layer (Section~\ref{Sec:Exp_OOD_Detection} discusses this layer choice), where  is the number of neurons and  is the raw output of -th neuron in this layer.
By setting the uniform vector , the desired KL divergence can be given as:
\vspace{-2mm}

where , and  denotes -element in .  is a constant. 
By combining the KL gradient with neuron output, we then formulate \textit{neuron activation state} as, 
\vspace{-0.5mm}

where  is the sigmoid function with a steepness controller . 
In the rest of this paper, we will also use the notation  to represent the neuron state function.







\bfstart{Rationale of } Here, we further analyze the gradients from KL divergence to show how this part contributes to the neuron activation state . 
Without loss of generality, let the network be , where  is the predictor following . 
Since , we can rewrite the Eq.~(\ref{Eq:State}) as follows (more details are provided in Appendix~\ref{Appendix:Sec_KL_Details}):
\vspace{-1mm}

where 
(1)  corresponds the simple explanation method known as \textit{Input}  \textit{Gradient}~\citep{exp:inputXgrad}, which quantifies the contribution of neurons to the model prediction . 
It is also the general form of many prevalent explanation methods, such as -LRP~\citep{exp:LRP}, DeepLIFT~\citep{exp:DeepLIFT}, and IG~\citep{exp:IG}; 
(2)  measures the deviation of model predictions from a uniform distribution, thus denoting sample confidence~\citep{OOD_Detect:GradNorm}.
In this way, we builds  by considering both the significance of neurons on model predictions, and model confidence in input data.
Intuitively, if a neuron contributes less to the output (or the model lacks confidence in input data), the neuron would be considered less active.


























\vspace{-1mm}
\subsection{Neuron Activation Coverage (NAC)}
\vspace{-1mm}
\label{Sec:Method_NACFunc}
With the formulation of neuron activation state, we now introduce the \textit{neuron activation coverage} (NAC) to characterize neuron behaviors under InD and OOD data. 
Inspired by system testing~\citep{NACT_System:DeepXplore,NACT_System:DeepGauge,NACT_System:DeepHunter}, NAC aims to quantify the coverage degree of neuron states under InD training data. 
The intuition is that \textit{if a neuron state is rarely activated (covered) by any InD input, the chances of triggering bugs (\eg, misclassification) under this state would be high.}
Since NAC directly measures the statistical property (\ie, coverage) over neuron state distribution, we derive the NAC function from the probability density function (PDF).
Formally, given a state  of -th neuron, and its PDF  over an InD set , the function for NAC can be given as:
\vspace{-1mm}

where  is the probability density of  over the set , and  denotes the lower bound for 
achieving full coverage \textit{w.r.t.} state . 
In cases where the neuron state  is frequently activated by InD training data, the coverage score  would be 1, denoting fewer underlying defects in this state.
Notably, if  is too low, noisy activations would dominate the coverage, reducing the significance of coverage scores. 
Conversely, an excessively large value of  also makes the NAC function vulnerable to data biases. 
For example, given a homogeneous dataset comprising numerous similar samples, the coverage score of a neuron state  can be mischaracterized as abnormally high, marginalizing the effects of other meaningful states. We analyze the effect of  in Section~\ref{Sec:Exp_OOD_Detection}.















\subsection{Applications}
After modeling the NAC function over InD training data, we can directly apply it to tackle existing OOD problems. In the following, we illustrate two application scenarios.


\bfstart{Uncertainty estimation for OOD detection} 
Since OOD data often trigger abnormal neuron behaviors (See Figure~\ref{Fig:Intro-Neuron-OOD}), we employ \textbf{NAC} for \textbf{U}ncertainty \textbf{E}stimation (\texttt{NAC-UE}), 
which directly 
\setlength\intextsep{8pt}
\begin{wrapfigure}[17]{r}{0cm}
	\centering
	\includegraphics[width=0.36\textwidth]{supplements/Intro_neuron_plots_plain.pdf}
	\caption{OOD~vs.~InD neuron activation states. We employ PACS~\citep{Dataset:PACS} \textit{Photo} domain as InD and \textit{Sketch} as OOD. All neurons stem from the layer4 of ResNet-50. }
\label{Fig:Intro-Neuron-OOD}
\end{wrapfigure}
averages coverage scores over all neurons as the uncertainty of test samples. 
Formally, given a test data , the function for \texttt{NAC-UE} can be given as,
\label{Sec:Method_App}

where  is the number of neurons;  denotes the state of -th neuron;  is the controller of NAC function. If the neuron states triggered by  are frequently activated by InD training samples, the coverage score  would be high, suggesting that  is likely to come from InD distribution. 
By considering multiple layers in the network, we propose using \texttt{NAC-UE} for OOD detection following~\citet{OOD_Detect:Energy,OOD_Detect:GradNorm,OOD_Detect:ReAct}: 

where  is a threshold, and  denotes the neuron state function of layer . The test sample with an uncertainty score  less than  would be categorized as OOD; otherwise, InD.



















\bfstart{Model evaluation for OOD generalization} 
OOD data potentially trigger neuron states beyond the coverage area of InD data (Figure~\ref{Fig:Intro-Neuron-Cov} and Figure~\ref{Fig:Intro-Neuron-OOD}), thus leading to misclassifications. 
From this perspective, we hypothesize that the robustness of networks could positively correlate with the size of coverage area.
For instance, as coverage area narrows, larger inactive space would remain, increasing the chances of triggering underlying bugs.
Hence, we propose \textbf{NAC} for \textbf{M}odel \textbf{E}valuation (\texttt{NAC-ME}), which characterizes model generalization ability based on the integral of neuron coverage distribution. 
Formally, given an InD training set , \texttt{NAC-ME} measures the generalization ability of a model (parameterized by ) as the average of integral \textit{w.r.t.} NAC distribution:

where 
 is the number of neurons, and  is the controller of NAC function. 
Given the training set , if a neuron is consistently active throughout the activation space, we consider it to be well exercised by InD training data, thus with a lower probability of triggering bugs, i.e., favorable robustness.













\bfstart{Approximation} 
To enable efficient processing of large-scale datasets, we adopt a simple histogram-based approach for modeling the probability density function (PDF) function.  This approach divides the neuron activation space into  intervals, and naturally supports mini-batch approximation. We provide more details in  Appendix~\ref{Appendix:Approximation_Details}.
In addition, we efficiently calculate  using the Riemman approximation~\citep{Rieman}, 
\vspace{-1mm}







\begin{table*}
	\centering
	\adjustbox{max width=0.98\textwidth}{\begin{tabular}{l cc cc cc cc cc}
			\toprule
			\multirow{3}{*}{\parbox{1.6cm}{Method}}				&\multicolumn{2}{c}{MINIST}		&\multicolumn{2}{c}{SVHN}		&\multicolumn{2}{c}{Textures}		&\multicolumn{2}{c}{Places365}		&\multicolumn{2}{c}{Average}		\\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
			&FPR95	&AUROC	&FPR95	&AUROC		&FPR95		&AUROC				&FPR95		&AUROC					&FPR95		&AUROC					\\	
\midrule
			\multicolumn{11}{c}{\emph{CIFAR-10 Benchmark}} \\
OpenMax & 23.33{\tiny4.67} & 90.50{\tiny0.44} & 25.40{\tiny1.47} & 89.77{\tiny0.45} & 31.50{\tiny4.05} & 89.58{\tiny0.60} & 38.52{\tiny2.27} & 88.63{\tiny0.28} & 29.69{\tiny1.21} & 89.62{\tiny0.19} \\ 
ODIN & 23.83{\tiny12.34} & \underline{95.24}{\tiny1.96} & 68.61{\tiny0.52} & 84.58{\tiny0.77} & 67.70{\tiny11.06} & 86.94{\tiny2.26} & 70.36{\tiny6.96} & 85.07{\tiny1.24} & 57.62{\tiny4.24} & 87.96{\tiny0.61} \\ 
			MDS & 27.30{\tiny3.55} & 90.10{\tiny2.41} & 25.96{\tiny2.52} & 91.18{\tiny0.47} & 27.94{\tiny4.20} & 92.69{\tiny1.06} & 47.67{\tiny4.54} & 84.90{\tiny2.54} & 32.22{\tiny3.40} & 89.72{\tiny1.36} \\ 
			MDSEns & \textbf{1.30}{\tiny0.51} & \textbf{99.17}{\tiny0.41} & 74.34{\tiny1.04} & 66.56{\tiny0.58} & 76.07{\tiny0.17} & 77.40{\tiny0.28} & 94.16{\tiny0.33} & 52.47{\tiny0.15} & 61.47{\tiny0.48} & 73.90{\tiny0.27} \\ 
			RMDS & 21.49{\tiny2.32} & 93.22{\tiny0.80} & 23.46{\tiny1.48} & 91.84{\tiny0.26} & 25.25{\tiny0.53} & 92.23{\tiny0.23} & \underline{31.20}{\tiny0.28} & \underline{91.51}{\tiny0.11} & 25.35{\tiny0.73} & 92.20{\tiny0.21} \\ 
			Gram & 70.30{\tiny8.96} & 72.64{\tiny2.34} & 33.91{\tiny17.35} & 91.52{\tiny4.45} & 94.64{\tiny2.71} & 62.34{\tiny8.27} & 90.49{\tiny1.93} & 60.44{\tiny3.41} & 72.34{\tiny6.73} & 71.73{\tiny3.20} \\ 
ReAct & 33.77{\tiny18.00} & 92.81{\tiny3.03} & 50.23{\tiny15.98} & 89.12{\tiny3.19} & 51.42{\tiny11.42} & 89.38{\tiny1.49} & 44.20{\tiny3.35} & 90.35{\tiny0.78} & 44.90{\tiny8.37} & 90.42{\tiny1.41} \\ 
VIM & \underline{18.36}{\tiny1.42} & 94.76{\tiny0.38} & \underline{19.29}{\tiny0.41} & \underline{94.50}{\tiny0.48} & \underline{21.14}{\tiny1.83} & \underline{95.15}{\tiny0.34} & 41.43{\tiny2.17} & 89.49{\tiny0.39} & \underline{25.05}{\tiny0.52} & \underline{93.48}{\tiny0.24} \\ 
			KNN & 20.05{\tiny1.36} & 94.26{\tiny0.38} & \underline{22.60}{\tiny1.26} & \underline{92.67}{\tiny0.30} & \underline{24.06}{\tiny0.55} & \underline{93.16}{\tiny0.24} & \underline{30.38}{\tiny0.63} & \underline{91.77}{\tiny0.23} & \underline{24.27}{\tiny0.40} & \underline{92.96}{\tiny0.14} \\ 
ASH & 70.00{\tiny10.56} & 83.16{\tiny4.66} & 83.64{\tiny6.48} & 73.46{\tiny6.41} & 84.59{\tiny1.74} & 77.45{\tiny2.39} & 77.89{\tiny7.28} & 79.89{\tiny3.69} & 79.03{\tiny4.22} & 78.49{\tiny2.58} \\ 
			SHE & 42.22{\tiny20.59} & 90.43{\tiny4.76} & 62.74{\tiny4.01} & 86.38{\tiny1.32} & 84.60{\tiny5.30} & 81.57{\tiny1.21} & 76.36{\tiny5.32} & 82.89{\tiny1.22} & 66.48{\tiny5.98} & 85.32{\tiny1.43} \\ 
			GEN & 23.00{\tiny7.75} & 93.83{\tiny2.14} & 28.14{\tiny2.59} & 91.97{\tiny0.66} & 40.74{\tiny6.61} & 90.14{\tiny0.76} & 47.03{\tiny3.22} & 89.46{\tiny0.65} & 34.73{\tiny1.58} & 91.35{\tiny0.69} \\ 
			\rowcolor{LightGray}
			\textbf{NAC-UE} & \underline{15.14}{\tiny2.60}  & \underline{94.86}{\tiny1.36}  & \textbf{14.33}{\tiny1.24}  & \textbf{96.05}{\tiny0.47}  & \textbf{17.03}{\tiny0.59}  & \textbf{95.64}{\tiny0.44}  & \textbf{26.73}{\tiny0.80}  &  \textbf{91.85}{\tiny0.28}  & \textbf{18.31}{\tiny0.92} &  \textbf{94.60}{\tiny0.50} \\ 
			\midrule
			
			\multicolumn{11}{c}{\emph{CIFAR-100 Benchmark}} \\
OpenMax & 53.82{\tiny4.74} &  76.01{\tiny1.39} &  53.20{\tiny1.78} &  82.07{\tiny1.53} &  56.12{\tiny1.91} &  80.56{\tiny0.09} &  \underline{54.85}{\tiny1.42} &  \underline{79.29}{\tiny0.40} &  54.50{\tiny0.68} &  79.48{\tiny0.41 }\\ 
ODIN & \underline{45.94}{\tiny3.29} &  \underline{83.79}{\tiny1.31} &  67.41{\tiny3.88} &  74.54{\tiny0.76} &  62.37{\tiny2.96} &  79.33{\tiny1.08} &  59.71{\tiny0.92} &  79.45{\tiny0.26} &  58.86{\tiny0.79} &  79.28{\tiny0.21 }\\ 
			MDS & 71.72{\tiny2.94} &  67.47{\tiny0.81} &  67.21{\tiny6.09} &  70.68{\tiny6.40} &  70.49{\tiny2.48} &  76.26{\tiny0.69} &  79.61{\tiny0.34} &  63.15{\tiny0.49} &  72.26{\tiny1.56} &  69.39{\tiny1.39 }\\ 
			MDSEns & \textbf{2.83}{\tiny0.86} &  \textbf{98.21}{\tiny0.78} &  82.57{\tiny2.58} &  53.76{\tiny1.63} &  84.94{\tiny0.83} &  69.75{\tiny1.14} &  96.61{\tiny0.17} &  42.27{\tiny0.73} &  66.74{\tiny1.04} &  66.00{\tiny0.69 }\\ 
			RMDS & 52.05{\tiny6.28} &  79.74{\tiny2.49} &  51.65{\tiny3.68} &  84.89{\tiny1.10} &  53.99{\tiny1.06} &  83.65{\tiny0.51} &  \textbf{53.57}{\tiny0.43} &  \textbf{83.40}{\tiny0.46} &  \underline{52.81}{\tiny0.63} &  \underline{82.92}{\tiny0.42 }\\ 
			Gram & 53.53{\tiny7.45} &  80.71{\tiny4.15} &  \textbf{20.06}{\tiny1.96} &  \textbf{95.55}{\tiny0.60} &  89.51{\tiny2.54} &  70.79{\tiny1.32} &  94.67{\tiny0.60} &  46.38{\tiny1.21} &  64.44{\tiny2.37} &  73.36{\tiny1.08 }\\ 
ReAct & 56.04{\tiny5.66} &  78.37{\tiny1.59} &  50.41{\tiny2.02} &  83.01{\tiny0.97} &  55.04{\tiny0.82} &  80.15{\tiny0.46} &  \underline{55.30}{\tiny0.41} &  80.03{\tiny0.11} &  54.20{\tiny1.56} &  80.39{\tiny0.49 }\\ 
VIM & 48.32{\tiny1.07} &  81.89{\tiny1.02} &  46.22{\tiny5.46} &  83.14{\tiny3.71} &  \underline{46.86}{\tiny2.29} &  \underline{85.91}{\tiny0.78} &  61.57{\tiny0.77} &  75.85{\tiny0.37} &  \underline{50.74}{\tiny1.00} &  81.70{\tiny0.62 }\\ 
			KNN & 48.58{\tiny4.67} &  82.36{\tiny1.52} &  51.75{\tiny3.12} &  84.15{\tiny1.09} &  \underline{53.56}{\tiny2.32} &  \underline{83.66}{\tiny0.83} &  60.70{\tiny1.03} &  79.43{\tiny0.47} &  53.65{\tiny0.28} &  \underline{82.40}{\tiny0.17 }\\ 
ASH & 66.58{\tiny3.88} &  77.23{\tiny0.46} &  \underline{46.00}{\tiny2.67} &  \underline{85.60}{\tiny1.40} &  61.27{\tiny2.74} &  80.72{\tiny0.70} &  62.95{\tiny0.99} &  78.76{\tiny0.16} &  59.20{\tiny2.46} &  80.58{\tiny0.66 }\\ 
			SHE & 58.78{\tiny2.70} &  76.76{\tiny1.07} &  59.15{\tiny7.61} &  80.97{\tiny3.98} &  73.29{\tiny3.22} &  73.64{\tiny1.28} &  65.24{\tiny0.98} &  76.30{\tiny0.51} &  64.12{\tiny2.70} &  76.92{\tiny1.16 }\\ 
			GEN & 53.92{\tiny5.71} &  78.29{\tiny2.05} &  55.45{\tiny2.76} &  81.41{\tiny1.50} &  61.23{\tiny1.40} &  78.74{\tiny0.81} &  56.25{\tiny1.01} &  \underline{80.28}{\tiny0.27} &  56.71{\tiny1.59} &  79.68{\tiny0.75 }\\ 
			\rowcolor{LightGray}
			\textbf{NAC-UE} & \underline{21.97}{\tiny6.62 } &  \underline{93.15}{\tiny1.63 } &  \underline{24.39}{\tiny4.66 } &  \underline{92.40}{\tiny1.26 } &  \textbf{40.65}{\tiny1.94 } &  \textbf{89.32}{\tiny0.55 } &  73.57{\tiny1.16  } &  73.05{\tiny0.68 } &  \textbf{40.14}{\tiny1.86 } &  \textbf{86.98}{\tiny0.37 }\\ 
			
			\bottomrule
		\end{tabular}
	}
	\vspace{-1.5mm}
	\caption{OOD detection performance on CIFAR-10 and CIFAR-100 benchmarks. We format \textbf{first}, \underline{second}, and \underline{third} results. Full results for all baselines are provided in Table~\ref{Appendix:Tab:Full_OOD_Detection_CiFAR10} and Table~\ref{Appendix:Tab:Full_OOD_Detection_CiFAR100}.}
	\label{table:OOD_Detection_CIFAR}
	\vspace{-2mm}
\end{table*}










\vspace{-2mm}
\section{Experiments}
\vspace{-2mm}
\subsection{Case Study 1: OOD Detection}
\vspace{-2mm}
\label{Sec:Exp_OOD_Detection}
\bfstart{Setup} Our experimental settings align with the latest version of OpenOOD\footnote{https://github.com/Jingkang50/OpenOOD.}~\citep{Setup:OpenOOD,Setup:OpenOODv1.5}. 
We evaluate our \texttt{NAC-UE} on three benchmarks: CIFAR-10, CIFAR-100, and ImageNet-1k.
For CIFAR-10 and CIFAR-100, InD dataset corresponds to the respective CIFAR, and 4 OOD datasets are included: \texttt{MNIST}~\citep{OOD_Dataset:MNIST}, \texttt{SVHN}~\citep{OOD_Dataset:SVHN}, \texttt{Textures}~\citep{OOD_Dataset:Textures}, and \texttt{Places365}~\citep{OOD_Dataset:Places}.
For ImageNet experiments,  ImageNet-1k serves as InD, along with 3 OOD datasets: \texttt{iNaturalist}~\citep{OOD_Dataset:iNaturalist}, \texttt{Textures}~\citep{OOD_Dataset:Textures}, and \texttt{OpenImage-O}~\citep{OOD_Detect:ViM}. We use pretrained ResNet-50 and Vit-b16 for ImageNet experiments, and ResNet-18 for CIFAR. 
For all employed benchmarks, we compare our \texttt{NAC-UE} with 21 SoTA OOD detection methods. We provide more details in Appendix~\ref{Appendix:OOD_Detection_Details}.








\bfstart{Metrics} 
We utilize two threshold-free metrics in our evaluation: 1) FPR95: the false-positive-rate of OOD samples when the true positive rate of ID samples is at 95\%; 2) AUROC: the area under the receiver operating characteristic curve.
Throughout our implementations, all pretrained models are left unmodified, preserving their classification ability during the OOD detection phase.

\bfstart{Implementation details} 
We first build the NAC function using InD training data, utilizing 1,000 training images for ResNet-18 and ResNet-50, and 50,000 images for Vit-b16. 
Note that in this stage, we merely use training samples less than 5\% of the training set (See Appendix~\ref{Appendix:Ablation_TrainingSamples} for more analysis).
Next, we employ \texttt{NAC-UE} to calculate uncertainty scores during the test phase.
Following OpenOOD, we use the validation set to select hyperparameters and evaluate \texttt{NAC-UE} on the test set.



















\bfstart{Results} 
Table~\ref{table:OOD_Detection_CIFAR} and Table~\ref{table:OOD_Detection_ImgNet} mainly illustrate our results on CIFAR and ImageNet benchmarks, where we compare \texttt{NAC-UE} with 21 SoTA methods.
As can be seen, our \texttt{NAC-UE} consistently outperforms all of the SoTA methods on average performance, establishing record-breaking performance over 3 benchmarks. Specifically, \texttt{NAC-UE} reduces the FPR95 by 10.60\% and 5.96\% over the most competitive rival~\citep{OOD_Detect:ViM,OOD_Detect:KNN} on CIFAR-100 and CIFAR-10, respectively. 
On the large-scale ImageNet benchmark, \texttt{NAC-UE} also consistently improves AUROC scores 
across backbones and OOD datasets.
Besides, since \texttt{NAC-UE} performs in a \textit{post-hoc} fashion, it preserves model classification ability (\ie, InD accuracy) during the OOD detection phase. In contrast, advanced methods such as ReAct~\citep{OOD_Detect:ReAct} and ASH~\citep{OOD_Detect:SimpleAct} exhibit promising OOD detection results at the expense of InD performance~\citep{OOD_Detect:SimpleAct}.














\newcolumntype{g}{>{\columncolor{LightGray}}c}
\begin{table*}
	\centering
	\adjustbox{max width=0.98\textwidth}{\begin{tabular}{lc ccc ccc ccc g}
					\toprule		
\rowcolor{white}
					\parbox{1.9cm}{Dataset} & \parbox{1.7cm}{Backbone} & OpenMax & MDS & RMDS & ReAct & VIM & KNN & ASH & SHE & GEN & \textbf{NAC-UE} \\
					
					\midrule
					\multirow{3}{*}{iNaturalist}& ResNet-50 & 92.05 & 63.67 & 87.24 & {96.34} & 89.56 & 86.41 & {97.07} & 92.65 & 92.44 & {96.52} \\ 
					& Vit-b16 & {94.93} & {96.01} & {96.10} & 86.11 & 95.72 & 91.46 & 50.62 & 93.57 & 93.54 & 93.72 \\ 
					& Average & {93.49} & 79.84 & 91.67 & 91.23 & 92.64 & 88.94 & 73.85 & {93.11} & 92.99 & \textbf{95.12} \\ 
					
					\midrule
					\multirow{3}{*}{OpenImage-O} & ResNet-50 & 87.62 & 69.27 & 85.84 & {91.87} & 90.50 & 87.04 & {93.26} & 86.52 & 89.26 & {91.45} \\ 
					& Vit-b16 & 87.36 & {92.38} & {92.32} & 84.29 & {92.18} & 89.86 & 55.51 & 91.04 & 90.27 & 91.58 \\ 
					& Average & 87.49 & 80.83 & 89.08 & 88.08 & {91.34} & 88.45 & 74.39 & 88.78 & {89.77} & \textbf{91.52} \\ 
					
					\midrule
					\multirow{3}{*}{Textures}& ResNet-50 & 88.10 & 89.80 & 86.08 & 92.79 & {97.97} & {97.09} & 96.90 & 93.60 & 87.59 & {97.9} \\ 
					& Vit-b16 & 85.52 & 89.41 & 89.38 & 86.66 & 90.61 & {91.12} & 48.53 & {92.65} & 90.23 & {94.17} \\ 
					& Average & 86.81 & 89.61 & 87.73 & 89.73 & {94.29} & {94.11} & 72.72 & 93.13 & 88.91 & \textbf{96.04} \\ 
					
					\bottomrule
				\end{tabular}
		}
	\vspace{-1mm}
	\caption{OOD detection performance (AUROC) on ImageNet. See Table~\ref{Appendix:Tab:Full_OOD_Detection_ImageNet} for full results.}
	\label{table:OOD_Detection_ImgNet}
	\vspace{-3mm}
\end{table*}












\bfstart{NAC-UE with training methods} 
Training-time regularization is one of the potential directions in 
\setlength\intextsep{3pt}
\begin{wraptable}[10]{r}{0cm}
	\resizebox{0.41\textwidth}{!}{
		\centering
		\newcolumntype{g}{>{\columncolor{LightGray}}c}
		\begin{tabular}{l g gg}
			\toprule
			\rowcolor{white}
			Training & Method & FPR95 & AUROC \\ 
			\midrule
			\rowcolor{white}
			\multirow{2}{*}{ConfBranch} 
			& Baseline & 50.98 & 83.94 \\ 
			& NAC-UE & \textbf{31.04} & \textbf{93.90} \\ 
			\midrule
			\rowcolor{white}
			\multirow{2}{*}{RotPred} 
			& Baseline & 36.67 & 90.00 \\ 
			& NAC-UE & \textbf{30.24} & \textbf{93.28} \\ 
			\midrule
			\rowcolor{white}
			\multirow{2}{*}{GODIN} 
			& Baseline & 50.87 & 85.51 \\ 
			& NAC-UE & \textbf{26.86} & \textbf{94.61} \\ 
			\bottomrule
		\end{tabular}
	}
	\caption{ImageNet results of NAC-UE with different training methods. }
	\label{table:OOD_Detection_PLUG}
\end{wraptable}
OOD detection. Here, we further show that \texttt{NAC-UE} is pluggable to existing training methods. 
Table~\ref{table:OOD_Detection_PLUG} illustrates our results using three training schemes: ConfBranch~\citep{OOD_Detect:Train:ConfBranch}, RotPred~\citep{OOD_Detect:Train:RotPred}, and GODIN~\citep{OOD_Detect:Train:Godin}, where we compare NAC-UE with the detection method employed in the original paper, \ie, \textit{Baseline} in Table~\ref{table:OOD_Detection_PLUG}. 
Notably, \texttt{NAC-UE} 
significantly improves upon the baseline method across all three training approaches, which highlights its effectiveness for OOD detection again.













\noindent\textbf{Where to apply NAC-UE?} 
Since \texttt{NAC-UE} performs based on neurons in a network, we further investigate its effect when using neurons from different layers. Table~\ref{table:OOD_Layer} exhibits the results, where the ResNet is utilized as the backbone for analysis.
It can be drawn that 
(1) the performance of \texttt{NAC-UE} positively correlates with the number of employed layers. 
This is intuitive, as including more layers enables a greater number of neurons to be considered, thereby enhancing the accuracy of \texttt{NAC-UE} in estimating the model status;
(2) even with a single layer of neurons, \texttt{NAC-UE} is able to achieve favorable performance. For instance, by employing \textit{layer4}, \texttt{NAC-UE} already achieves 23.50\% FPR95, which outperforms the previous best method KNN on CIFAR-10.





\vspace{2mm}
\begin{table*}[!hb]
	\centering
	\resizebox{0.9\columnwidth}{!}{
		\begin{tabular}{cccc cc cc cc}
			\toprule
			\multicolumn{4}{c}{Layer Combinations} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet}  \\ 
			\cmidrule(lr){1-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
			Layer4 & Layer3 & Layer2 & Layer1 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC  \\
			\midrule 
 &  &  &  & 23.50 & 93.21 & 85.84 & 58.37 & 26.89 & 94.57  \\ 
			 &  &  &  & 21.32 & 94.35 & 44.92 & 85.25 & 23.51 & 95.05  \\ 
			 &  &  &  & 18.50 & 94.46 & \textbf{39.96} & 86.94 & 22.69 & 95.23  \\ 
			\rowcolor{LightGray}
			 &  &  &  & \textbf{18.31} & \textbf{94.60} & 40.14 & \textbf{86.98} & \textbf{22.49} & \textbf{95.29}  \\ 
			\bottomrule
		\end{tabular}
	}
	\vspace{-1.5mm}
	\caption{Performance of \texttt{NAC-UE} with different layer choices. }
	\label{table:OOD_Layer}
\end{table*}
\vspace{1.5mm}


















\bfstart{The superiority of neuron activation state } Section~\ref{Sec:Method_NState} formulates the neuron activation state  by combining the neuron output  with its KL gradients . Here, we ablate this formulation to examine the superiority of . 
In particular, we analyze the neuron behaviors \textit{w.r.t.} 1) raw neuron output: , 2) KL gradients of neuron output: , and 3) ours neuron state: . 

Figure~\ref{Fig:Detection_Distr} illustrates the results, where we visualize the InD and OOD distribution of different neurons in the ImageNet benchmark. 
As can be seen, under the form of , neurons tend to present distinct activation patterns when exposed to InD and OOD data. 
This distinctiveness greatly facilitates the separability between InD and OOD, thereby leading to the best OOD detection performance with \texttt{NAC-UE}, \eg, 16.58\% FPR95 () \textit{vs}. 35.72\% FPR95 () on \textit{layer4}.
Contrary to that, when considering the vanilla form of  and , the neuron behaviors under InD and OOD are largely overlapped, which further spotlights the unique characteristic of our .  More detailed analysis can be found in Appendix~\ref{Appendix:Ablation_NeuronState}.



























\begin{figure*}
	[t]
	\centering \includegraphics[width=0.99\columnwidth]{supplements/state_ablation_fpr.pdf} \vspace{-2mm}
	\caption{Ablation studies on the neuron activation state. We visualize InD (ImageNet) and OOD (iNaturalist) distributions \textit{w.r.t.} (a) neuron output, ; (b) KL gradients of neuron output, ; (c) our defined neuron state, . All states are normalized via the sigmoid function.}
	\label{Fig:Detection_Distr}
	\vspace{-2mm}
\end{figure*}


\begin{table*}[t]
\begin{minipage}{.32\linewidth}
		\centering
		\newcolumntype{g}{>{\columncolor{LightGray}}c}
		\resizebox{0.97\columnwidth}{!}{
			\begin{tabular}{lcc}
			\toprule  
\multirow{2}{*}{\parbox{2cm}{Sigmoid \\Steepness ()}}		&\multirow{2}{*}{FPR95}		&\multirow{2}{*}{AUROC}	\\
			& &\\
			\midrule
			  	&40.07 & 85.48  \\ 
			  	&25.64 & 92.11  \\ 
			\rowcolor{LightGray}
			  	&\textbf{23.50} & \textbf{93.21}  \\ 
			  	&48.99 & 86.00  \\ 
			  	&92.69 & 54.69  \\ 
			\bottomrule
		\end{tabular}
		}
	\vspace{-1mm}
		\caption{\texttt{NAC-UE} \textit{w.r.t} different  over CIFAR-10.}
		\label{Tab:OOD_Detection_Sig}
	\end{minipage}\hfill
	\begin{minipage}{.305\linewidth}
		\centering
		\resizebox{0.97\columnwidth}{!}{
			\begin{tabular}{lcc}
				\toprule  


\multirow{2}{*}{\parbox{1.7cm}{Lower \\Bound ()}}		&\multirow{2}{*}{FPR95}		&\multirow{2}{*}{AUROC}	\\
				& &\\
				\midrule
				 	&27.10	&91.51\\  
				 	&24.16	&92.79\\  
				\rowcolor{LightGray}
				 	&\textbf{23.50}	&\textbf{93.21}\\ 
				 	&28.35	&92.17\\ 
				 	&36.70	&90.38\\  
				\bottomrule
			\end{tabular}
		}
	\vspace{-1mm}
		\caption{\texttt{NAC-UE} \textit{w.r.t} different  over CIFAR-10.}
		\label{Tab:OOD_Detection_R}
	\end{minipage}\hfill
	\begin{minipage}{.32\linewidth}
		\centering
		\resizebox{0.97\columnwidth}{!}{
			\begin{tabular}{lcc}
				\toprule  
\multirow{2}{*}{\parbox{2cm}{No. of \\Intervals ()}}		&\multirow{2}{*}{FPR95}		&\multirow{2}{*}{AUROC}	\\
				& &\\
				\midrule
				 		&25.19	&91.80\\ 
				\rowcolor{LightGray}
				 		&\textbf{23.50}	&\textbf{93.21}\\ 
				  	&24.23	&93.09\\ 
				  	&33.87	&91.11\\ 
				  	&40.36	&89.69\\ 
				\bottomrule
			\end{tabular}
		}
	\vspace{-1mm}
		\caption{\texttt{NAC-UE} \textit{w.r.t} different  over CIFAR-10.}
		\label{Tab:OOD_Detection_M}
	\end{minipage}
\vspace{-4mm}
\end{table*}










\bfstart{Paramter analysis} Table~\ref{Tab:OOD_Detection_Sig}-\ref{Tab:OOD_Detection_M} presents a systematically analysis of the effect of sigmoid steepness (), lower bound () for full coverage, and the number of intervals () for PDF approximation. The following observations can be noted: 
1) A relatively steep sigmoid function could make \texttt{NAC-UE} perform better. We conjecture this is due to that neuron activation states often distribute in a small range, thus requiring a steeper function to distinguish their finer variations;
2) \texttt{NAC-UE} is sensitive to the choice of . 
As previously discussed, a small r would allows noisy activations to dominate NAC, thus diminishing the effect of coverage scores. Also, a large  makes the NAC vulnerable to data biases, \eg, in datasets with numerous similar samples, a neuron state can be inaccurately characterized with a high coverage score, disregarding other meaningful neuron states.
3) \texttt{NAC-UE} works better with a moderate .
This is intuitive as a lower  may not sufficiently approximate the PDF function, while a higher  can easily lead to overfitting on the utilized training samples.



























\vspace{-1mm}
\subsection{Case Study 2: OOD Generalization}
\vspace{-1mm}
\label{Sec:Exp_OOD_Generalization}
\bfstart{Setup} Our experimental settings follow the Domainbed benchmark~\citep{Setup:DomainBed}. Without employing digital images, we adopt four datasets: \texttt{VLCS}~\citep{Dataset:VLCS} (4 domains, 10,729 images) , \texttt{PACS}~\citep{Dataset:PACS} (4 domains, 9,991 images), \texttt{OfficeHome}~\citep{Dataset:OfficeHome} (4 domains, 15,588 images), and \texttt{TerraInc}~\citep{Dataset:TerraIncognita} (4 domains, 24,788 images). 
For all datasets, we report the \textit{leave-one-out} test accuracy following~\citep{Setup:DomainBed}, whereby results are averaged over cases that use a single domain for test and the others for training. 
For all employed backbones, we utilize the hyperparameters suggested by~\citep{tech:swad} to fine-tune them. 
The training strategy is ERM~\citep{Baseline:ERM}, unless stated otherwise. 
We set the total training steps as 5000, and the evaluation frequency as 300 steps for all models. 
We use the validation set to select hyperparameters of \texttt{NAC-ME}. See Appendix~\ref{Appendix:OOD_Generalization_Details} for more details.






\bfstart{Model evaluation criteria} 
Since OOD data is assumed unavailable during model training, existing methods commonly resort to InD validation accuracy to evaluate a model~\citep{Baseline:Fishr,CL&DG:PCL,Baseline:Fish,CL&DG:SelfReg}. 
Thus, we mainly compare \texttt{NAC-ME} with the prevalent \textit{validation criterion}~\citep{Setup:DomainBed}. 
We also leverage the \textit{oracle criterion}~\citep{Setup:DomainBed} as the upper bound, which directly utilizes OOD test data for model evaluation. 








\bfstart{Metrics} 
Here, we utilize two metrics: 1) Spearman Rank Correlation (RC) between OOD test accuracy and the model evaluation scores (\ie, InD validation accuracy or \texttt{NAC-ME} scores), which are sampled at regular evaluation intervals (\ie, every 300 steps) during the training process; 
2) OOD Test Accuracy (ACC) of the best model selected by the criterion within a single run of training. 
















\begin{table*}
	\centering
	\adjustbox{max width=0.94\textwidth}{\newcolumntype{g}{>{\columncolor{LightGray}}c}
		\begin{tabular}{c g gg gg gg gg gg}
\toprule
			\rowcolor{white}
			\multirow{2}{*}{Bakbone}		&\multirow{2}{*}{\parbox{2cm}{~~~~~Method}}		&\multicolumn{2}{c}{VLCS}		&\multicolumn{2}{c}{PACS}		&\multicolumn{2}{c}{OfficeHome}		&\multicolumn{2}{c}{TerraInc}		&\multicolumn{2}{c}{Average}		\\
			\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \rowcolor{white}
			&								&RC			&ACC						&RC			&ACC					&RC			&ACC					&RC			&ACC				&RC		&ACC		\\
\midrule \rowcolor{white}
        	& Oracle & - & 77.67 & - & 80.51 & - & 56.18 & - & 44.51 & - & 64.72  \\ \cmidrule(lr){3-12} \rowcolor{white}
			& Validation & 34.27 & 75.12 & 68.71 & \textbf{79.01} & 83.50 & 55.60 & 39.58 & 37.36 & 56.52 & 61.77  \\ 
			& NAC-ME & \textbf{50.29} & \textbf{75.83} & \textbf{74.16} & 78.85 & \textbf{84.91} & \textbf{55.76} & \textbf{40.42} & \textbf{39.45} & \textbf{62.45} & \textbf{62.47}  \\ 
			\multirow{-4}[1]{*}{\parbox{2cm}{~~~ResNet-18}} 
			&  & \textcolor{BoldDelta}{(+16.02)} & \textcolor{BoldDelta}{(+0.71)}& \textcolor{BoldDelta}{(+5.45)}& \textcolor{gray}{(-0.16)} & \textcolor{BoldDelta}{(+1.41)}& \textcolor{BoldDelta}{(+0.16)}& \textcolor{BoldDelta}{(+0.84)}& \textcolor{BoldDelta}{(+2.09)}& \textcolor{BoldDelta}{(+5.93)}& \textcolor{BoldDelta}{(+0.70)} \\ 
			
			\midrule	\rowcolor{white}	
			& Oracle & - & 79.79 & - & 86.10 & - & 65.95 & - & 50.76 & - & 70.65  \\ \cmidrule(lr){3-12} \rowcolor{white}
			& Validation & \textbf{31.43} & \textbf{77.70} & 58.54 & 84.57 & 67.93 & 65.04 & 37.07 & 46.07 & 48.74 & 68.34  \\ 
			& NAC-ME & 28.68 & 76.41 & \textbf{62.07} & \textbf{85.28} & \textbf{69.16} & \textbf{65.23} & \textbf{40.16} & \textbf{47.10} & \textbf{50.02} & \textbf{68.51}  \\ 
			\multirow{-4}[1]{*}{ResNet-50}
			&  & \textcolor{gray}{(-2.75)} & \textcolor{gray}{(-1.29)} & \textcolor{BoldDelta}{(+3.53)}& \textcolor{BoldDelta}{(+0.71)}& \textcolor{BoldDelta}{(+1.23)}& \textcolor{BoldDelta}{(+0.19)}& \textcolor{BoldDelta}{(+3.09)}& \textcolor{BoldDelta}{(+1.03)}& \textcolor{BoldDelta}{(+1.28)}& \textcolor{BoldDelta}{(+0.17)} \\ 
			
			\midrule	\rowcolor{white}	
			& Oracle & - & 79.11 & - & 71.99 & - & 61.44 & - & 41.29 & - & 63.46  \\ \cmidrule(lr){3-12} \rowcolor{white}
			& Validation & 37.95 & 77.43 & 89.34 & 69.83 & 98.71 & 61.22 & 22.71 & 36.28 & 62.18 & 61.19  \\ 
			& NAC-ME & \textbf{49.59} & \textbf{77.97} & \textbf{90.67} & \textbf{70.99} & \textbf{99.14} & \textbf{61.26} & \textbf{23.26} & \textbf{36.69} & \textbf{65.67} & \textbf{61.73}  \\ 
			\multirow{-4}[1]{*}{Vit-t16} 					
			&  & \textcolor{BoldDelta}{(+11.64)} & \textcolor{BoldDelta}{(+0.54)}& \textcolor{BoldDelta}{(+1.33)}& \textcolor{BoldDelta}{(+1.16)}& \textcolor{BoldDelta}{(+0.43)}& \textcolor{BoldDelta}{(+0.04)}& \textcolor{BoldDelta}{(+0.55)}& \textcolor{BoldDelta}{(+0.41)}& \textcolor{BoldDelta}{(+3.49)}& \textcolor{BoldDelta}{(+0.54)} \\ 
			
			\midrule	\rowcolor{white}	
			& Oracle & - & 80.96 & - & 90.23 & - & 81.23 & - & 52.23 & - & 76.16  \\ \cmidrule(lr){3-12} \rowcolor{white}
			& Validation & 18.81 & 78.70 & 41.38 & 87.80 & 58.29 & 80.11 & 0.92 & 45.49 & 29.85 & 73.03  \\ 
			& NAC-ME & \textbf{37.42} & \textbf{79.20} & \textbf{45.04} & \textbf{88.83} & \textbf{63.17} & \textbf{80.52} & \textbf{20.22} & \textbf{47.86} & \textbf{41.46} & \textbf{74.10}  \\ 
			\multirow{-4}[1]{*}{Vit-b16} 					
			&  & \textcolor{BoldDelta}{(+18.61)} & \textcolor{BoldDelta}{(+0.50)}& \textcolor{BoldDelta}{(+3.66)}& \textcolor{BoldDelta}{(+1.03)}& \textcolor{BoldDelta}{(+4.88)}& \textcolor{BoldDelta}{(+0.41)}& \textcolor{BoldDelta}{(+19.30)} & \textcolor{BoldDelta}{(+2.37)}& \textcolor{BoldDelta}{(+11.61)} & \textcolor{BoldDelta}{(+1.07)} \\
			
			\bottomrule
		\end{tabular}
	}
	\vspace{-1.5mm}
	\caption{OOD generalization results on DomainBed. \textit{Oracle} denotes the upper bound, which uses OOD test data to evaluate models.  denotes the improvement of \texttt{NAC-ME} over the validation criterion. All scores are averaged over 3 random trials. Full results are provided in Appendix~\ref{Appendix:Sec_DomainBed_Results}.}
	\label{table:OOD_Selection_Main}
	\vspace{-3.5mm}
\end{table*}




\bfstart{Results} As illustrated in Table~\ref{table:OOD_Selection_Main}, we mainly compare our \texttt{NAC-ME} with the typical validation criterion over four backbones: ResNet-18, ResNet-50, Vit-t16, and Vit-b16. 
We provide the main observations in the following:
1) The positive correlation (\ie, RC  0) between the \texttt{NAC-ME} and OOD test performance consistently holds across architectures and datasets; 
2) By comparison with the validation criterion, \texttt{NAC-ME} not only selects more robust models (with higher OOD accuracy), but also exhibits stronger correlation with OOD test performance. For instance, on the TerraInc dataset, \texttt{NAC-ME} achieves a rank correlation of 20.22\% with OOD test accuracy, surpassing validation criterion by 19.30\% on Vit-b16. 
Similarly, on the VLCS dataset, \texttt{NAC-ME} also shows a rank correlation of 52.29\%, outperforming the validation criterion by 16.02\% on ResNet-18. 
Such results highlight the potential of \texttt{NAC-ME} in evaluating model generalization ability.















\setlength\intextsep{0pt}
\begin{wraptable}[10]{r}{0cm}
	\resizebox{0.37\textwidth}{!}{
		\centering
		\newcolumntype{g}{>{\columncolor{LightGray}}c}
		\begin{tabular}{c g gg}
			\toprule
			\rowcolor{white}
			Algorithm			&Method		&RC			&ACC		\\
			\midrule
			\rowcolor{white}
\rowcolor{white}
			&Validation						&61.76		&80.66					\\	
			
			&{NAC-ME}						&\textbf{66.85} 		&\textbf{80.92}						\\
\multirow{-3}[1]{*}{\parbox{2cm}{\centering SelfReg}} &						&\textcolor{BoldDelta}{(+5.09)}	&\textcolor{BoldDelta}{(+0.26)}					\\
			\midrule
			\rowcolor{white}
\rowcolor{white}
			&Validation						&70.06 		&80.68					\\
			&{NAC-ME}						&\textbf{76.55} 		&\textbf{81.54}						\\
\multirow{-3}[1]{*}{\parbox{2cm}{\centering CORAL}}
			&						&\textcolor{BoldDelta}{(+6.49)}	&\textcolor{BoldDelta}{(+0.86)}					\\
			\bottomrule
				\vspace{-5mm}
		\end{tabular}
	}
	\caption{OOD generalization results on \texttt{PACS}~\citep{Dataset:PACS}, averaged over 3 trials. Backbone: ResNet-18.}
	\label{Tab:OOD_Selection_SoTA}
\end{wraptable}

\bfstart{NAC-ME can co-work with SoTA learning algorithms}
Recent literature has suggested numerous learning algorithms to enhance the model robustness~\citep{Baseline:DANN,Baseline:Fish,Baseline:Fishr}. 
In this sense, we further investigate the potential of \texttt{NAC-ME} by implementing it with two recent SoTA algorithms: CORAL~\citep{Baseline:CORAL} and SelfReg~\citep{CL&DG:SelfReg}. 
The results are shown in Table~\ref{Tab:OOD_Selection_SoTA}. We can see that \texttt{NAC-ME} as an evaluation criterion still presents better performance compared with the validation criterion, which spotlights its effectiveness again.





\setlength\intextsep{3pt}
\begin{wrapfigure}[15]{r}{0cm}
	\centering
	\includegraphics[width=0.37\textwidth]{supplements/rc_wilds.pdf}
	\captionof{figure}{The positive relationship between RC and the volume of OOD test data. Dataset: \texttt{iWildCAM}~\citep{Dataset:Wilds}. Backbone: ResNet-50.}
	\label{Fig:rc_wilds}
\end{wrapfigure}
\noindent\textbf{Does the volume of OOD test data hinder the Rank Correlation (RC)?}
As illustrated in Table~\ref{table:OOD_Selection_Main}, while in most cases \texttt{NAC-ME} outperforms the validation criterion on model selection, 
we can find 
that the Rank Correlation (RC) still falls short of its maximum value, \eg, on the 
VLCS dataset using ResNet-18, RC only reaches 50\% compared to the maximum of 100\%.
Given that Domainbed only provides 6 OOD domains at most, we hypothesize that the volume/variance of OOD test data may be the reason: insufficient OOD test data may be unreliable to reflect model generalization ability, thereby hindering the validity of RC. 
To this end, we conduct additional experiments on the iWildCam dataset~\citep{Dataset:Wilds}, which includes 323 domains and 203,029 images in total. 
Figure~\ref{Fig:rc_wilds} illustrates the results, where we analyze the relationship between RC and the volume of OOD test data by randomly sampling different ratios of OOD data for RC calculation.
As can be seen, an increase in the ratio of test data also leads to an improvement in the RC, which confirms our hypothesis regarding the effect of OOD data. 
Furthermore, we can observe that in most cases, \texttt{NAC-ME} could still outperform the validation criterion. 
These observations spotlight the capability of our NAC again.
























\section{Related Work}
\vspace{-1mm}
\bfstart{Neuron coverage in system testing} 
Traditional system testing commonly leverages coverage criteria to uncover defects in software programs~\citep{NACT_System:Traditional_Testing}. These criteria measure the degree to which certain codes or components have been exercised, thereby revealing areas with potential defects.
To simulate such program testing in neural networks, \citet{NACT_System:DeepXplore} first introduced neuron coverage, which measures the proportion of activated neurons within a given input set.
The underlying idea is that if a network performs with larger neuron coverage during testing, it is likely to have fewer undetected bugs, \eg, misclassification. 
In line with this, \citet{NACT_System:DeepGauge} extended neuron coverage with fine-grained criteria by considering the neuron outputs from training data. 
\citet{NACT_System:NLC} introduced layer-wise neuron coverage, focusing on interactions between neurons within the same layer.
The most recent work related to our paper is~\citet{NACT_DG:NeuronCoverage}, where they proposed to improve model generalization ability by maximizing neuron coverage during training. 
However, these existing definitions of neuron coverage still focus on the proportion of activated neurons in the \textit{entire network}, which disregards the activation details of individual neurons. 
Contrary to that, in this paper, we specifically define neuron activation coverage (NAC) for \textit{individual neurons}, which characterizes the coverage degree of each neuron state under InD data. This provides a more comprehensive perspective on understanding neuron behaviors under InD and OOD scenarios.











\vspace{-0.5mm}
\bfstart{OOD detection} 
The goal of OOD detection is to distinguish between InD and OOD data inputs, thereby refraining from using unreliable model predictions during deployment. 
Existing detection methods can be broadly categorized into three groups: 1) confidence-based ~\citep{ood_example1,OOD_Detect:MSP,OOD_Detect:MOS}, 2) distance-based~\citep{OOD_Detect:distance1,OOD_Detect:distance2,OOD_Detect:distance3}, and 3) density-based~\citep{OOD_Detect:density1,OOD_Detect:density2,OOD_Detect:density3} approaches. 
Confidence-based methods commonly resort to the confidence level of model outputs to detect OOD samples, \eg, maximum softmax probability~\citep{OOD_Detect:MSP}.
In contrast, distance-based approaches identify OOD samples by measuring the distance (\eg, Mahalanobis distance~\citep{OOD_Detect:Mahalanobis}) between input sample and typical InD centroids or prototypes. 
Likewise, density-based methods employ probabilistic models to explicitly model InD distribution and classify test data located in low-density regions as OOD.


\vspace{-0.5mm}
Specific to neuron behaviors, ReAct~\citep{OOD_Detect:ReAct} recently proposes the truncation of neuron activations to separate the InD and OOD data. However, such truncation can lead to a decrease in model classification ability~\citep{OOD_Detect:SimpleAct}. 
Similarly, LINe~\citep{OOD_Detect:LINe} seeks to find important neurons using the Shapley value~\citep{Shapley1988AVF} and then performs activation clipping.
Yet, this approach relies on a threshold-based strategy that categorizes neurons into binary states, disregarding valuable neuron distribution details. 
Unlike them, in this work, we show that by using natural neuron states, a distribution property (\ie, coverage) greatly facilitates the OOD detection.





\vspace{-0.5mm}
\bfstart{OOD generalization} 
OOD generalization aims to train models that can overcome distribution shifts between InD and OOD data. 
While a myriad of studies has emerged to tackle this problem~\citep{Baseline:MMD,Baseline:CORAL,Baseline:GroupDRO,Baseline:AND-Mask,Baseline:IRM,Baseline:DANN,Baseline:MLDG,Baseline:V-REx}, \citet{Setup:DomainBed} recently put forth the importance of model evaluation criterion, and demonstrated that a vanilla ERM~\citep{Baseline:ERM} along with a proper criterion could outperform most state-of-the-art methods. 
In line with this, \citet{Criterion:SMA} discovered that using validation accuracy as the evaluation criterion could be unstable for model selection, and thus proposed moving average to stabilize model training.
Contrary to that, this work sheds light on the potential of neuron activation coverage for model evaluation, showing that it outperforms the validation criterion in various cases.







\vspace{-1mm}
\section{Conclusion}
\vspace{-1mm}
In this work, we have presented a neuron activation view to reflect the OOD problem. 
We have shown that through our formulated neuron activation states, the concept of neuron activation coverage (NAC) could effectively facilitate two OOD tasks: OOD detection and OOD generalization. 
Specifically, we have demonstrated that 1) InD and OOD inputs can be more separable based on the neuron activation coverage, yielding substantially improved OOD detection performance; 
2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which highlights the potential of NAC-based criterion for model evaluation.
Along these lines, we hope this paper has further motivated the community to consider neuron behavior in the OOD problem. This is also the most considerable benefit eventually lies.








\bibliography{mybib}
\bibliographystyle{iclr2024_conference}



\newpage
\appendix


\renewcommand\cftaftertoctitle{\vskip2pt\par\hrulefill\vskip-2mm\par\vskip2pt}
\renewcommand\cftsecafterpnum{\vskip3pt}
\cftsetindents{section}{0em}{2em}
\cftsetindents{subsection}{2em}{2.5em}


\clearpage
\hypersetup{colorlinks,linkcolor={black},citecolor={CiteBlue},urlcolor={CiteBlue}}  
\begin{center}
	\vspace*{1cm}
	{\huge \textbf{Appendix}}
	\vskip5em
\end{center}


{\addtolength{\textheight}{-10cm}
	\tableofcontents
	\vskip-2mm
	\noindent\hrulefill
}



\clearpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
\hypersetup{colorlinks,linkcolor={red},citecolor={CiteBlue},urlcolor={CiteBlue}}  
\section{Potential Social Impact}
This study introduces neuron activation coverage (NAC) as an efficient tool for facilitating out-of-distribution (OOD) solutions. By improving OOD detection and generalization, NAC has the potential to significantly enhance the dependability and safety of modern machine learning models. 
Thus, the social impact of this research can be far-reaching, spanning consumer and business applications in digital content understanding, transportation systems including driver assistance and autonomous vehicles, as well as healthcare applications such as identifying unseen diseases.
Moreover, by openly sharing our code, we strive to offer machine learning practitioners a readily available resource for responsible AI development, ultimately benefiting society as a whole.
Although we anticipate no negative repercussions, we are committed to expanding upon our framework in future endeavors.



\section{Additional Theoretical Details}
\label{Appendix:Sec_KL_Details}
In this section, we present additional theoretical details for Eq.~(\ref{Eq:rational_z}) in the main paper. Concretely, we first elaborate on the calculation of gradients \textit{w.r.t.} the sample confidence, \ie, . Then, we show the detailed derivation of Eq.~(\ref{Eq:rational_z}).


\bfstart{Derivation of sample confidence}
As a reminder, in the main paper, we introduce the Kullback-Leibler (KL) divergence~\citep{tech:KL} between the network output and a uniform vector  as follows:

where , and  denotes -element in .
 is a constant. Let  indicates -th element in , we have . Then, by substituting the expression of , we can rewrite KL divergence as:


Subsequently, we can derive the gradients of KL divergence \textit{w.r.t.} the output logit  as: 


Since , we finally have: 



\bfstart{Derivation of Eq.(\ref{Eq:rational_z})} 
As shown above, we have . By substituting this expression, we can rewrite the formulation of neuron activation state  as:


By expanding the expression of  , we have:

where  denotes the gradients of -th element in the logit output .  is the number of neurons in , and  is the number of classes. In this way, we can reorganize Eq.(\ref{Appendix:Eq:rational_z}) as:














\section{Approximation Details}
\label{Appendix:Approximation_Details}
In this section, we demonstrate details for the approximation of PDF function, and further show the insights for the choice of  in our NAC function.

\subsection{Preliminaries} 
\bfstart{Probability density function (PDF)} The Probability Density Function (PDF), denoted by , measures the probability of a continuous random variable taking on a specific value within a given range. Accordingly,  should possess the following key properties: 
\begin{enumerate}[label=(\arabic*),topsep=1pt,parsep=1pt,itemindent=0.5em]
	\item {Non-Negativity}: , for all ; 
	\item {Normalization}: ; 
	\item {Probability Interpretation}: , 
\end{enumerate}
where  denotes the probability that random variable  has values within range . 


\bfstart{Cumulative distribution function (CDF)} In line with PDF, the Cumulative Distribution Function (CDF), denoted by , calculates the cumulative probability for a given -value. Formally,  gives the area under the probability density function up to the specified , 

By the Fundamental Theorem of Calculus, we can rewrite the function  as,



Note that in the main paper, we denote by  the PDF, and  the NAC function of -th neuron over the training dataset . In this appendix, we will omit the superscript  and subscript  for simplicity.

\subsection{Approximation}
\label{Sec:Appendix:Approx}
In line with the main paper, we approximate the PDF of neuron states following a simple histogram-based approach, where the neuron activation space is partitioned into   intervals/bins with logarithmic scales. Formally, suppose the width of a bin is , we can rewrite the PDF function as,

\vspace{-1mm}
where  is the neuron activation state, and  is the number of samples in the bin activating . During the PDF modeling process, we iteratively take a random batch of neuron states as input and assign them corresponding bins.


\bfstart{The choice of }
With the approximation of PDF, we can rewrite the NAC function as,

where  denotes the lower bound for achieving full coverage \textit{w.r.t.} state . 
However, for the above formulation, it could be challenging to search for a suitable , since various factors (\eg, InD dataset size ) could affect the significance of NAC scores .
In this sense, to further simplify this formulation in the practical deployment, we set , such that 

where  represents the minimum number of samples required for bin filling, and  is the number of samples activating the neuron state  in the bin. In this way, we can directly manipulate  to control the NAC function in the practical deployment.























\section{Experimental Details for OOD Detection}
\label{Appendix:OOD_Detection_Details}
We conduct experiments following the latest version of OpenOOD\footnote{https://github.com/Jingkang50/OpenOOD.}~\citep{Setup:OpenOOD,Setup:OpenOODv1.5}. 
In this section, we first provide more details for the utilized baselines (Section \ref{Appendix:OOD_Detection:Baseline}), datasets and evaluation protocol (Section \ref{Appendix:OOD_Detection:Benchmark}), and model architectures (Section \ref{Appendix:OOD_Detection:Model}). Then, we demonstrate the hyperparameters of \texttt{NAC-UE}, and the corresponding search space (Section \ref{Appendix:OOD_Detection:HP}).


\subsection{Baseline Methods} 
\label{Appendix:OOD_Detection:Baseline}
Since \texttt{NAC-UE} performs in a post-hoc fashion, we mainly compare our approach on three bechmarks with the 21 post-hoc OOD detection methods, including OpenMax~\citep{ood_example1}, MSP~\citep{OOD_Detect:MSP}, TempScale~\citep{OOD_Detect:TempScale}, ODIN~\citep{OOD_Detect:ODIN}, MDS~\citep{OOD_Detect:Mahalanobis}, MDSEns~\citep{OOD_Detect:Mahalanobis}, RMDS~\citep{OOD_Detect:RMDS}, Gram~\citep{OOD_Detect:Gram}, EBO~\citep{OOD_Detect:Energy}, OpenGAN~\citep{OOD_Detect:OpenGAN}, GradNorm~\citep{OOD_Detect:GradNorm}, ReAct~\citep{OOD_Detect:ReAct}, MLS~\citep{OOD_Detect:MLS}, KLM~\citep{OOD_Detect:MLS}, VIM~\citep{OOD_Detect:ViM}, KNN~\citep{OOD_Detect:KNN}, DICE~\citep{OOD_Detect:DICE}, RankFeat~\citep{OOD_Detect:RankFeat}, ASH~\citep{OOD_Detect:SimpleAct}, SHE~\citep{OOD_Detect:SHE}. 
In particular, ReAct and ASH are neuron-based methods, which modify the neuron activations for OOD detection.
The results presented in Table~\ref{Appendix:Tab:Full_OOD_Detection_CiFAR10}-\ref{Appendix:Tab:Full_OOD_Detection_ImageNet} are from the OpenOOD implementations.

\subsection{OOD Benchmarks} 
\label{Appendix:OOD_Detection:Benchmark}
We mainly utilize the Far-OOD track of OpenOOD for the evaluation, as it is well defined and supported by many existing studies, \eg, \cite{OOD_Detect:ViM} and \cite{OOD_Detect:InOut_Study}.

\paragraph{CIFAR benchmarks}
CIFAR-10 and CIFAR-100 are widely employed as in-distribution (InD) datasets in existing studies. CIFAR-10 consists of 10 classes, while CIFAR-100 contains 100 classes. 
In line with OpenOOD, we adopt the same split setup for CIFAR-10 and CIFAR-100 benchmarks.
Specifically, for both CIFAR-10 and CIFAR-100, we utilize the official train set with 50,000 training images, and hold out 1,000 samples from the test set as InD validation set. The remaining 9,000 test images are employed as \textit{InD test} set. 
The 1,000 images covering 20 categories are held out from Tiny ImageNet~\citep{OOD_Dataset:TinyImg}, serving as the \textit{OOD validation} set.
To assess the performance of OOD detection methods, we employ four commonly adopted datasets for \textit{OOD test}, which are disjoint with the \textit{OOD validation} set. The details of them are provided below:
\begin{enumerate}
	\item \texttt{MNIST}~\citep{OOD_Dataset:MNIST}: This is a 10-class handwriting digital dataset, contains 60,000 images for training	and 10,000 for test. We utilize the entire test set for OOD detection.
	
	\item \texttt{SVHN}~\citep{OOD_Dataset:SVHN}: This dataset consists of color images depicting house numbers, encompassing ten classes representing digits 0 to 9. We utilize the entire test set, containing 26,032 images.

	\item \texttt{Textures}~\citep{OOD_Dataset:Textures}: The Textures dataset comprises 5,640 real-world texture images classified into 47 categories. We employ the entire dataset for evaluation purposes.

	\item \texttt{Places365}~\citep{OOD_Dataset:Places}: Places365 contains a vast collection of photographs depicting scenes, classified into 365 scene categories. The test set consists of 900 images per category. 
	For OOD detection, we utilize the entire test dataset with 1,305 images removed due to the semantic overlap following~\citep{Setup:OpenOOD}.
	
	



\end{enumerate}



\begin{table*}[t]
	\centering
	\resizebox{1\textwidth}{!}{
		\begin{tabular}{c c l l}
			\toprule  
			\textbf{Architecture}	&{\textbf{Parameter}}		&{\textbf{Denotation}}		&{\textbf{Values}}	\\
			
			\midrule
			\multirow{4}[3]{*}{{ResNet-18}}
			&{-}		&layer choice 				&layer4 / layer3 / layer2 / layer1\\  
			&{}		&number of bins for PDF estimation 		&50 / 500 / 50 / 500\\  
			&{}		&sigmoid steepness		&100 / 1000 / 0.001 / 0.001\\
&{}		&number of samples required for bin filling	&50 / 100 / 5 / 100\\  			
			\bottomrule
		\end{tabular}
	}
	\caption{Hyperparameters and their default values on the CIFAR-10 benchmark. Note that  can be computed based on , as illustrated in Appendix~\ref{Sec:Appendix:Approx}}
	\label{Appendix:Tab:OOD_Detection_CIFAR10_Prams}
\end{table*}



\begin{table*}[t]
	\centering
	\resizebox{1\textwidth}{!}{
		\begin{tabular}{c c l l}
			\toprule  
			\textbf{Architecture}	&{\textbf{Parameter}}		&{\textbf{Denotation}}		&{\textbf{Values}}	\\
			
			\midrule
			\multirow{4}[3]{*}{{ResNet-18}}
			&{-}		&layer choice 				&layer4 / layer3 / layer2 / layer1\\  
			&{}		&number of bins for PDF estimation 		&50 / 1000 / 50 / 50\\  
			&{}		&sigmoid steepness		&50 / 10 / 1 / 0.005\\
&{}		&number of samples required for bin filling	&50 / 500 / 500 / 5\\  			
			\bottomrule
		\end{tabular}
	}
	\caption{Hyperparameters and their default values on the CIFAR-100 benchmark. Note that  can be computed based on , as illustrated in Appendix~\ref{Sec:Appendix:Approx}}
	\label{Appendix:Tab:OOD_Detection_CIFAR100_Prams}
\end{table*}



\begin{table*}[t]
	\centering
	\resizebox{1\textwidth}{!}{
		\begin{tabular}{c c l l}
			\toprule  
			\textbf{Architecture}	&{\textbf{Parameter}}		&{\textbf{Denotation}}		&{\textbf{Values}}	\\
			
			\midrule
			\multirow{5}[3]{*}{{Vit-b16}}
			&\multirow{2}{*}{-}		&\multirow{2}{*}{layer choice}			&\multirow{2}{*}{\makecell[l]{before\_head / block11 / \\block10 / block9}}\\  
			\vspace{1mm}
			&&&\\
			&{}		&number of bins for PDF estimation 		&50 / 500 / 500 / 1000\\  
			&{}		&sigmoid steepness		&100 / 1 / 10 / 1\\
			&{}		&number of samples required for bin filling	&500 / 50 / 10 / 10\\  	


			
			\midrule
			\multirow{4}[3]{*}{{ResNet-50}}
			&{-}		&layer choice 				&layer4 / layer3 / layer2 / layer1\\  
			&{}		&number of bins for PDF estimation 		&50 / 50 / 500 / 1000\\  
			&{}		&sigmoid steepness		&3000 / 300 / 0.01 / 1\\
			&{}		&number of samples required for bin filling	&10 / 500 / 50 / 5000\\  	


			
			\bottomrule
		\end{tabular}
	}
	\caption{Hyperparameters and their default values on the ImageNet benchmark. Note that  can be computed based on , as illustrated in Appendix~\ref{Sec:Appendix:Approx}}
	\label{Appendix:Tab:OOD_Detection_ImgNet_Prams}
\end{table*}



\paragraph{Large-scale ImageNet benchmark} 
We employ ImageNet-1k~\citep{Setup:ImageNet} as the in-distribution dataset, which contains about 1.2M training images. 
Following OpenOOD, we utilize 45,000 images from the ImageNet validation set as \textit{InD test} set, and the remaining 5,000 samples as \textit{InD validation} set. 
To search hyperparameters, 1,763 images from OpenImage-O~\citep{OOD_Detect:ViM} are picked out for
\textit{OOD validation}.
Finally, we leverage three commonly adopted datasets as \textit{OOD test} for evaluations:
\begin{enumerate}
	\item \texttt{iNaturalist}~\citep{OOD_Dataset:iNaturalist}: This dataset consists of 859,000 images of plants and animals, covering over 5,000 different species. Each image is resized to a maximum dimension of 800 pixels. Following~\citep{OOD_Detect:MOS,Setup:OpenOOD}, we evaluate our method on a randomly selected subset of 10,000 images, which are drawn from 110 classes that do not overlap with ImageNet-1k.

	\item \texttt{Textures}~\citep{OOD_Dataset:Textures}: This dataset contains 5,640 real-world texture images categorized into 47 classes. We utilize the entire dataset for evaluation purposes.

	\item \texttt{OpenImage-O}~\citep{OOD_Detect:ViM}: This dataset is curated based on the test set of OpenImage-v3, thereby enjoying natural class statistics to avoid initial design biases. It contains 17,632 images with large scale. Following OpenOOD, we utilize the entire dataset for OOD detection, except the images selected for OOD validation.
	







\end{enumerate}








\subsection{Model Architecture}
\label{Appendix:OOD_Detection:Model}

For CIFAR-10 and CIFAR-100 benchmarks, we employ the powerful \texttt{ResNet-18}~\citep{tech:ResNet} architecture. In line with the OpenOOD~\citep{Setup:OpenOOD,Setup:OpenOODv1.5}, we train \texttt{ResNet-18} for 100 epochs and evaluate OOD detection methods over three checkpoints. Pleas refer to OpenOOD for more training details.


Following OpenOOD, our experiments for ImageNet benchmark employ two model architectures: 
\begin{itemize}
	\item {\texttt{ResNet-50}}~\citep{tech:ResNet} is pretrained on ImageNet-1k. For this model, all images are resized to 224  224 at the test phase. We use the official checkpoints from Pytorch.

	\item {\texttt{Vit-b16}}~\citep{tech:ViT} is also pretrained on ImageNet-1k. Similar to ResNet-50, test images are resized to 224  224. The checkpoints from Pytorch are employed.

\end{itemize}




\subsection{Hyperparameters}
\label{Appendix:OOD_Detection:HP}
In all of our experiments, we utilize the InD and OOD validation sets to search for the best hyperparameters. In general, we search  in [50, 500, 1000], and  in [10, 50, 100, 500, 5000] across architectures and benchmarks. Since neurons in deeper network layers (\eg, layer4) often varies in a smaller range (See  in Figure~\ref{Fig:Detection_Distr} for an example), we search  in [50, 100, 300, 1000, 3000] for steeper sigmoid function. Otherwise, we search  in [0,001, 0.005, 0.01, 1, 10]. 

In Table~\ref{Appendix:Tab:OOD_Detection_CIFAR10_Prams}-\ref{Appendix:Tab:OOD_Detection_ImgNet_Prams}, we list the values of selected hyperparameters for different model architectures over CIFAR-10, CIFAR-100, and ImageNet benchmarks. As suggested in Table~\ref{table:OOD_Layer}, we use layer4, layer3, layer2, and layer1 together for OOD detection regrading the ResNet architectures. For Vit-b16 models, we use the \textit{mlp} layer in block11, block10, block9, and the neurons before the head layer.












\section{Experimental Details for OOD Generalization}
\label{Appendix:OOD_Generalization_Details}
\subsection{Domainbed Benchmark}
\paragraph{Datasets} We conduct experiments on the DomainBed~\citep{Setup:DomainBed} benchmark, which is an arguably fairer benchmark in OOD generalization\footnote{{{https://github.com/facebookresearch/DomainBed.}}}. Without utilizing digital images, we utilize four datasets: 
\begin{enumerate}
	\item \texttt{VLCS}~\citep{Dataset:VLCS}  is composed of photographic domains, namely \texttt{Caltech101}, \texttt{LabelMe}, \texttt{SUN09}, and \texttt{VOC2007}. This dataset consists of 10,729 examples with dimensions (3, 224, 224) and 5 classes.
	
	\item \texttt{PACS} dataset \citep{Dataset:PACS} consists of four domains: \texttt{art}, \texttt{cartoons}, \texttt{photos}, and \texttt{sketches}. It comprises a total of 9,991 examples with dimensions (3, 224, 224) and 7 classes.
	
	\item \texttt{OfficeHome}~\citep{Dataset:OfficeHome} includes domains: \texttt{art}, \texttt{clipart}, \texttt{product}, \texttt{real}. This
	dataset contains 15,588 examples of dimension (3, 224, 224) and 65 classes.
	
	\item \texttt{TerraInc}~\citep{Dataset:TerraIncognita} is a collection of wildlife photographs captured by camera traps at various locations: \texttt{L100}, \texttt{L38}, \texttt{L43}, and \texttt{L46}. Our version of this dataset contains 24,788 examples of dimension (3, 224, 224) and 10 classes.
\end{enumerate}





\paragraph{Settings} To ensure the reliability of final results, the data from each domain is partitioned into two parts: 80\% for training or testing, and 20\% for validation. This process is repeated three times with different seeds, such that reported numbers represent the mean and standard errors across these three runs. 
In our experiments, we report \textit{leave-one-out} test accuracy scores, whereby results are averaged over cases that uses a single domain for test and the others for training. 
Besides, we set the total training steps as 5000, and the evaluation frequency as 300 steps for all runs. 


\paragraph{Model evaluation criteria}
For model evaluation, we mainly compare our method with the \textit{validation criterion}, which measures model accuracy over 20\% source-domain (\ie, InD) validation data.
In addition, we also employ the \textit{oracle criterion} as the upper bound, which directly utilizes the accuracy over 20\% test-domain data for model evaluation. For more details, we suggest to refer~\cite{Setup:DomainBed}.

\subsection{Metric: Rank Correlation}
Rank correlation metrics are widely utilized to measure the relationship between two random variables.
The purpose of these metrics is to provide a quantitative way to assess the similarity in rankings of observations across the variables.
Following~\cite{Criterion:SMA}, we utilize the Spearman Rank Correlation (RC) for assessing the relationship between OOD test accuracy and the model evaluation scores, \ie, InD validation accuracy or InD \texttt{NAC-ME} scores. 

The rationale behind this choice is that during the training phase, the selection of the optimal model is frequently based on the ranking of model performance, such as validation accuracy. Therefore, utilizing the RC score enables us to directly measure the effectiveness of evaluation criteria in model selection (which naturally translates to early stopping). 
The value of RC ranges between -1 and 1, where a value of -1 signifies that the rankings of two random variables are exactly opposite to each other; whereas, a value of +1 indicates that the rankings are exactly the same.
Furthermore, a RC score of 0 indicates no linear relationship between the two variables. 







\subsection{Model Architecture}
In our experiments, we employ four model architectures: ResNet-18~\citep{tech:ResNet}, ResNet-50~\citep{tech:ResNet}, Vit-t16~\citep{tech:ViT}, and Vit-b16~\citep{tech:ViT}. All of them are pretrained on the ImageNet dataset, and are employed as the initial weight. 
For parameter choices, we suggest to refer~\cite{tech:swad}. 


\subsection{Hyperparameters}
In the case of ResNet architectures, \texttt{NAC-ME} computation is performed by using the neurons in \textit{layer-4}. For ResNet-50, layer-4 consists of 2048 neurons, while ResNet-18 has 512 neurons.
As for vision transformers, \texttt{NAC-ME} computation utilizes the neurons in \textit{MLP-11}, \ie, the mlp layer in block 11. 
In Vit-b16, MLP-11 comprises 3072 neurons; whereas Vit-t16 has 768 neurons. 
During this series of experiments, we employ the source-domain training data to formulate the NAC function.
Besides, to mitigate the noises in training samples, we merely utilize training data that can be correctly classified to build the NAC function.


In order to determine the best hyperparameters of \texttt{NAC-ME} for all models, we utilize the InD validation data for parameter search based on the distribution outlined in Table~\ref{Appendix:Tab:OOD_Gen_Params}. 
Specifically, given the unavailability of OOD data in this context, we select \texttt{NAC-ME} hyperparameters  based on the rank correlation with the InD validation accuracy. This is motivated by the fact that the validation accuracy can provide some insights into the model learning progress.














\vspace{5mm}
\begin{table*}[!hb]
	\centering
	\resizebox{1\textwidth}{!}{
		\begin{tabular}{l c l l}
			\toprule  
\multirow{1}{*}{\parbox{2cm}{\textbf{Dataset}}}	&\multirow{1}{*}{\parbox{3cm}{\centering No. of bins }}		&\multirow{1}{*}{\parbox{3.5cm}{Sigmoid steepness }}		&\multirow{1}{*}{\parbox{5cm}{No. of samples for bin filling }}	\\
\midrule
			VLCS /				&\multirow{4}{*}{[50, 1000]}	&[1,  500, 5000] /	&\multirow{4}{*}{\parbox{5cm}{[1, 500, 5000, 10000] if not TerraInc else [5, 10, 30, 50]}}\\  
			PACS  /				&		&[0.01, 0.1, 0.5] /		&\\
			OfficeHome  /			&		&[0.01, 1, 100] /			&\\  	
TerraInc			&		&[0.01, 0.1]			&\\  	
			\bottomrule
		\end{tabular}
	}
	\caption{Hyperparameters of our \texttt{NAC-ME} and their distributions for random search. Note that  can be computed based on , as illustrated in Appendix~\ref{Sec:Appendix:Approx}}
	\label{Appendix:Tab:OOD_Gen_Params}
\end{table*}





\section{Reproducibility}
We will publicly release our code with detailed instructions.

\subsection{Software and Hardware}
All experiments are performed on a single NVIDIA GeForce RTX 3090 GPU, with Python version 3.8.11. The deep learning framework used is PyTorch 1.10.0, and Torchvision version 0.11.1 is utilized for image processing. We leverage CUDA 11.3 for GPU acceleration.

\subsection{Runtime Analysis}
The total runtime of the experiments varies depending on the tasks and datasets. In the following, we provide details for two OOD tasks with resent50 architecture, using a single NVIDIA GeForce RTX 3090 GPU.
For OOD detection, the experiments (\eg, inference during the test phase) take approximately 10 minutes for all benchmarks. 
For OOD generalization, the experiments on average take approximately 4 hours for PACS and VLCS, 8 hours for OfficeHome, 8.5 hours for TerraInc. 



\begin{figure*}	[t]
	\centering \includegraphics[width=0.9\columnwidth]{supplements/appendix_data_num_ablation.pdf} \caption{Ablation studies on the number of training samples for building NAC. \texttt{NAC-UE} achieves promising performance though only 1\% of the training data are utilized, demonstrating the efficiency of our NAC-based approaches.}
	\label{Appendix:Fig:OOD_tr_sample_plot}
\end{figure*}







\section{Additional Ablation Studies}





\subsection{Ablation on Number of Training Samples}
\label{Appendix:Ablation_TrainingSamples}
As previously mentioned in the main paper, the NAC function is constructed using the in-distribution (InD) training data.
Specifically, we utilize a subset with 1,000 training images on the CIFAR-10 and CIFAR-100 benchmarks, representing approximately 2\% of the total training set.
In the case of ImageNet, we employ 1,000 and 50,000 images for ResNet-50 and Vit-b16, respectively, which correspond to approximately 0.1\% and 5\% of the complete training set.



Here, to gain further insights into the efficiency of our approach,  we analyze the performance of \texttt{NAC-UE} when constructing the NAC function with varying numbers of training samples. Figure~\ref{Appendix:Fig:OOD_tr_sample_plot} illustrates the results on CIFAR-10 and CIFAR-100 benchmarks, where we randomly sample training images at different ratios and repeat this process five times to ensure the validity of the results.
Notably, even when utilizing only 1\% of the training data, \texttt{NAC-UE} demonstrates remarkable performance that is comparable to the scenario where 100\% of the training data is used. This demonstrates the efficiency of our approach, especially in situations with limited data availability.








\subsection{Ablation on Neuron Activation State }
\label{Appendix:Ablation_NeuronState}
In the main paper (Figure~\ref{Fig:Detection_Distr}), we analyze the formulation of neuron activation state  with two neuron examples. In this section, we provide additional experiments to further verify the superiority of .


\bfstart{Distribution of coverage scores under InD and OOD}
To complement the previous analysis which mainly centers on individual neurons, we first investigate the overall neuron activities under different form of neuron states, \ie, raw neuron output , neuron gradients , and ours .
Figure~\ref{Appendix:Fig:OOD_Distr} illustrates the results, where we visualize the InD and OOD distributions of averaged coverage scores \textit{w.r.t} all neurons (See Eq.(\ref{Eq:NAC-UE})) on the ImageNet benchmark. 
We provide the main observations in the following:







Firstly, among all the three variants,  method performs the best, as it inherits the advantages from both  and . This spotlights the superiority of our defined neuron state again. 
Secondly, it can also be found that OOD samples generally present lower coverage scores compared to InD samples. This demonstrates that OOD data tend to provoke abnormal neuron behaviors in comparison to InD data, which confirms the rationale behind our NAC-based approaches.





\begin{figure*}[t]
	\centering \includegraphics[width=0.99\columnwidth]{supplements/appendix_ood_distr.pdf} \caption{Ablation studies on the neuron activation states .  We visualize the distribution of averaged coverage scores \textit{w.r.t} all neurons (See Eq.(\ref{Eq:NAC-UE})) on the ImageNet benchmark.}
	\label{Appendix:Fig:OOD_Distr}
\end{figure*}



\bfstart{Distribution of neuron states with varying  under InD and OOD} 
As illustrated in Table~\ref{Tab:OOD_Detection_Sig}, choosing a suitable sigmoid steepness  is crucial for the OOD detection of \texttt{NAC-UE}. 
To further investigate if this factor also affects other forms of neuron states (\eg, ), we visualize the distribution of different neuron states with varying  under InD and OOD. 


We present the results in Figure~\ref{Appendix:Fig:OOD_sm_analysis}. It can be observed that when the sigmoid steepness  is increased, the neurons behaviors of InD and OOD become more distinguishable in the form of . This leads to the superior performance of \texttt{NAC-UE} in OOD detection. 
On the other hand, when using the vanilla form of  and , the varying number of  has less of an effect. 
This result is consistent with our previous finding in Figure~\ref{Fig:Detection_Distr}, which further demonstrates the unique characteristic of our neuron activation state  in distinguishing InD and OOD data points.










\begin{figure*}[t]
	\centering \includegraphics[width=0.99\columnwidth]{supplements/appendix_sm_state_ablation.pdf} \vspace{-2mm}
	\caption{Ablation studies on the form of neuron activation states with varying sigmoid steepness . We visualize InD and OOD distributions for the \textbf{layer4 unit-894} on ResNet-50. \texttt{NAC-UE} achieves best performance when  (using state ), which outperforms other forms of neuron states, \ie,  and .}
	\label{Appendix:Fig:OOD_sm_analysis}
\end{figure*}



\clearpage
\section{Full CIFAR Results}





\vspace{8mm}
\begin{table*}[h]
	\centering
	\adjustbox{max width=1\textwidth}{

		
		\begin{tabular}{l cc cc cc cc cc}
			\toprule
			\multirow{3}{*}{Method}				&\multicolumn{2}{c}{MINIST}		&\multicolumn{2}{c}{SVHN}		&\multicolumn{2}{c}{Textures}		&\multicolumn{2}{c}{Places365}		&\multicolumn{2}{c}{Average}		\\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
			&FPR95	&AUROC	&FPR95	&AUROC		&FPR95		&AUROC				&FPR95		&AUROC					&FPR95		&AUROC					\\	
\midrule
			\multicolumn{11}{c}{\emph{CIFAR-10 Benchmark}} \\
			\midrule
			OpenMax & 23.33{\tiny4.67} & 90.50{\tiny0.44} & 25.40{\tiny1.47} & 89.77{\tiny0.45} & 31.50{\tiny4.05} & 89.58{\tiny0.60} & 38.52{\tiny2.27} & 88.63{\tiny0.28} & 29.69{\tiny1.21} & 89.62{\tiny0.19} \\ 
			MSP & 23.64{\tiny5.81} & 92.63{\tiny1.57} & 25.82{\tiny1.64} & 91.46{\tiny0.40} & 34.96{\tiny4.64} & 89.89{\tiny0.71} & 42.47{\tiny3.81} & 88.92{\tiny0.47} & 31.72{\tiny1.84} & 90.73{\tiny0.43} \\ 
			TempScale & 23.53{\tiny7.05} & 93.11{\tiny1.77} & 26.97{\tiny2.65} & 91.66{\tiny0.52} & 38.16{\tiny5.89} & 90.01{\tiny0.74} & 45.27{\tiny4.50} & 89.11{\tiny0.52} & 33.48{\tiny2.39} & 90.97{\tiny0.52} \\ 
			ODIN & 23.83{\tiny12.34} & \underline{95.24}{\tiny1.96} & 68.61{\tiny0.52} & 84.58{\tiny0.77} & 67.70{\tiny11.06} & 86.94{\tiny2.26} & 70.36{\tiny6.96} & 85.07{\tiny1.24} & 57.62{\tiny4.24} & 87.96{\tiny0.61} \\ 
			MDS & 27.30{\tiny3.55} & 90.10{\tiny2.41} & 25.96{\tiny2.52} & 91.18{\tiny0.47} & 27.94{\tiny4.20} & 92.69{\tiny1.06} & 47.67{\tiny4.54} & 84.90{\tiny2.54} & 32.22{\tiny3.40} & 89.72{\tiny1.36} \\ 
			MDSEns & \textbf{1.30}{\tiny0.51} & \textbf{99.17}{\tiny0.41} & 74.34{\tiny1.04} & 66.56{\tiny0.58} & 76.07{\tiny0.17} & 77.40{\tiny0.28} & 94.16{\tiny0.33} & 52.47{\tiny0.15} & 61.47{\tiny0.48} & 73.90{\tiny0.27} \\ 
			RMDS & 21.49{\tiny2.32} & 93.22{\tiny0.80} & 23.46{\tiny1.48} & 91.84{\tiny0.26} & 25.25{\tiny0.53} & 92.23{\tiny0.23} & \underline{31.20}{\tiny0.28} & \underline{91.51}{\tiny0.11} & 25.35{\tiny0.73} & 92.20{\tiny0.21} \\ 
			Gram & 70.30{\tiny8.96} & 72.64{\tiny2.34} & 33.91{\tiny17.35} & 91.52{\tiny4.45} & 94.64{\tiny2.71} & 62.34{\tiny8.27} & 90.49{\tiny1.93} & 60.44{\tiny3.41} & 72.34{\tiny6.73} & 71.73{\tiny3.20} \\ 
			EBO & 24.99{\tiny12.93} & 94.32{\tiny2.53} & 35.12{\tiny6.11} & 91.79{\tiny0.98} & 51.82{\tiny6.11} & 89.47{\tiny0.70} & 54.85{\tiny6.52} & 89.25{\tiny0.78} & 41.69{\tiny5.32} & 91.21{\tiny0.92} \\ 
			OpenGAN & 79.54{\tiny19.71} & 56.14{\tiny24.08} & 75.27{\tiny26.93} & 52.81{\tiny27.60} & 83.95{\tiny14.89} & 56.14{\tiny18.26} & 95.32{\tiny4.45} & 53.34{\tiny5.79} & 83.52{\tiny11.63} & 54.61{\tiny15.51} \\ 
			GradNorm & 85.41{\tiny4.85} & 63.72{\tiny7.37} & 91.65{\tiny2.42} & 53.91{\tiny6.36} & 98.09{\tiny0.49} & 52.07{\tiny4.09} & 92.46{\tiny2.28} & 60.50{\tiny5.33} & 91.90{\tiny2.23} & 57.55{\tiny3.22} \\ 
			ReAct & 33.77{\tiny18.00} & 92.81{\tiny3.03} & 50.23{\tiny15.98} & 89.12{\tiny3.19} & 51.42{\tiny11.42} & 89.38{\tiny1.49} & 44.20{\tiny3.35} & 90.35{\tiny0.78} & 44.90{\tiny8.37} & 90.42{\tiny1.41} \\ 
			MLS & 25.06{\tiny12.87} & 94.15{\tiny2.48} & 35.09{\tiny6.09} & 91.69{\tiny0.94} & 51.73{\tiny6.13} & 89.41{\tiny0.71} & 54.84{\tiny6.51} & 89.14{\tiny0.76} & 41.68{\tiny5.27} & 91.10{\tiny0.89} \\ 
			KLM & 76.22{\tiny12.09} & 85.00{\tiny2.04} & 59.47{\tiny7.06} & 84.99{\tiny1.18} & 81.95{\tiny9.95} & 82.35{\tiny0.33} & 95.58{\tiny2.12} & 78.37{\tiny0.33} & 78.31{\tiny4.84} & 82.68{\tiny0.21} \\ 
			VIM & \underline{18.36}{\tiny1.42} & 94.76{\tiny0.38} & \underline{19.29}{\tiny0.41} & \underline{94.50}{\tiny0.48} & \underline{21.14}{\tiny1.83} & \underline{95.15}{\tiny0.34} & 41.43{\tiny2.17} & 89.49{\tiny0.39} & \underline{25.05}{\tiny0.52} & \underline{93.48}{\tiny0.24} \\ 
			KNN & 20.05{\tiny1.36} & 94.26{\tiny0.38} & \underline{22.60}{\tiny1.26} & \underline{92.67}{\tiny0.30} & \underline{24.06}{\tiny0.55} & \underline{93.16}{\tiny0.24} & \underline{30.38}{\tiny0.63} & \underline{91.77}{\tiny0.23} & \underline{24.27}{\tiny0.40} & \underline{92.96}{\tiny0.14} \\ 
			DICE & 30.83{\tiny10.54} & 90.37{\tiny5.97} & 36.61{\tiny4.74} & 90.02{\tiny1.77} & 62.42{\tiny4.79} & 81.86{\tiny2.35} & 77.19{\tiny12.60} & 74.67{\tiny4.98} & 51.76{\tiny4.42} & 84.23{\tiny1.89} \\ 
			RankFeat & 61.86{\tiny12.78} & 75.87{\tiny5.22} & 64.49{\tiny7.38} & 68.15{\tiny7.44} & 59.71{\tiny9.79} & 73.46{\tiny6.49} & 43.70{\tiny7.39} & 85.99{\tiny3.04} & 57.44{\tiny7.99} & 75.87{\tiny5.06} \\ 
			ASH & 70.00{\tiny10.56} & 83.16{\tiny4.66} & 83.64{\tiny6.48} & 73.46{\tiny6.41} & 84.59{\tiny1.74} & 77.45{\tiny2.39} & 77.89{\tiny7.28} & 79.89{\tiny3.69} & 79.03{\tiny4.22} & 78.49{\tiny2.58} \\ 
			SHE & 42.22{\tiny20.59} & 90.43{\tiny4.76} & 62.74{\tiny4.01} & 86.38{\tiny1.32} & 84.60{\tiny5.30} & 81.57{\tiny1.21} & 76.36{\tiny5.32} & 82.89{\tiny1.22} & 66.48{\tiny5.98} & 85.32{\tiny1.43} \\ 
			GEN & 23.00{\tiny7.75} & 93.83{\tiny2.14} & 28.14{\tiny2.59} & 91.97{\tiny0.66} & 40.74{\tiny6.61} & 90.14{\tiny0.76} & 47.03{\tiny3.22} & 89.46{\tiny0.65} & 34.73{\tiny1.58} & 91.35{\tiny0.69} \\ 
			\rowcolor{LightGray}
			\textbf{NAC-UE} & \underline{15.14}{\tiny2.60}  & \underline{94.86}{\tiny1.36}  & \textbf{14.33}{\tiny1.24}  & \textbf{96.05}{\tiny0.47}  & \textbf{17.03}{\tiny0.59}  & \textbf{95.64}{\tiny0.44}  & \textbf{26.73}{\tiny0.80}  &  \textbf{91.85}{\tiny0.28}  & \textbf{18.31}{\tiny0.92} &  \textbf{94.60}{\tiny0.50} \\ 
			\bottomrule
		\end{tabular}
		
	}
	\caption{OOD detection results on the CIFAR-10 benchmark. We format \textbf{first}, \underline{second}, and \underline{third} results. 
	Following OpenOOD, we report the performance averaged over three checkpoints of ResNet-18, which are trained solely on the InD dataset, \ie, CIFAR-10.  denotes the higher value is better, while  indicates lower values are better. }
	\label{Appendix:Tab:Full_OOD_Detection_CiFAR10}
	\vspace{-2mm}
\end{table*}



\vspace{16mm}
\begin{table*}[h]
	\centering
	\adjustbox{max width=1\textwidth}{\begin{tabular}{l cc cc cc cc cc}
			\toprule
			\multirow{3}{*}{Method}				&\multicolumn{2}{c}{MINIST}		&\multicolumn{2}{c}{SVHN}		&\multicolumn{2}{c}{Textures}		&\multicolumn{2}{c}{Places365}		&\multicolumn{2}{c}{Average}		\\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
			&FPR95	&AUROC	&FPR95	&AUROC		&FPR95		&AUROC				&FPR95		&AUROC					&FPR95		&AUROC					\\	
\midrule
			\multicolumn{11}{c}{\emph{CIFAR-100 Benchmark}} \\
			\midrule
			OpenMax & 53.82{\tiny4.74} &  76.01{\tiny1.39} &  53.20{\tiny1.78} &  82.07{\tiny1.53} &  56.12{\tiny1.91} &  80.56{\tiny0.09} &  \underline{54.85}{\tiny1.42} &  \underline{79.29}{\tiny0.40} &  54.50{\tiny0.68} &  79.48{\tiny0.41 }\\ 
			MSP & 57.23{\tiny4.68} &  76.08{\tiny1.86} &  59.07{\tiny2.53} &  78.42{\tiny0.89} &  61.88{\tiny1.28} &  77.32{\tiny0.71} &  56.62{\tiny0.87} &  79.22{\tiny0.29} &  58.70{\tiny1.06} &  77.76{\tiny0.44 }\\ 
			TempScale & 56.05{\tiny4.61} &  77.27{\tiny1.85} &  57.71{\tiny2.68} &  79.79{\tiny1.05} &  61.56{\tiny1.43} &  78.11{\tiny0.72} &  56.46{\tiny0.94} &  79.80{\tiny0.25} &  57.94{\tiny1.14} &  78.74{\tiny0.51 }\\ 
			ODIN & \underline{45.94}{\tiny3.29} &  \underline{83.79}{\tiny1.31} &  67.41{\tiny3.88} &  74.54{\tiny0.76} &  62.37{\tiny2.96} &  79.33{\tiny1.08} &  59.71{\tiny0.92} &  79.45{\tiny0.26} &  58.86{\tiny0.79} &  79.28{\tiny0.21 }\\ 
			MDS & 71.72{\tiny2.94} &  67.47{\tiny0.81} &  67.21{\tiny6.09} &  70.68{\tiny6.40} &  70.49{\tiny2.48} &  76.26{\tiny0.69} &  79.61{\tiny0.34} &  63.15{\tiny0.49} &  72.26{\tiny1.56} &  69.39{\tiny1.39 }\\ 
			MDSEns & \textbf{2.83}{\tiny0.86} &  \textbf{98.21}{\tiny0.78} &  82.57{\tiny2.58} &  53.76{\tiny1.63} &  84.94{\tiny0.83} &  69.75{\tiny1.14} &  96.61{\tiny0.17} &  42.27{\tiny0.73} &  66.74{\tiny1.04} &  66.00{\tiny0.69 }\\ 
			RMDS & 52.05{\tiny6.28} &  79.74{\tiny2.49} &  51.65{\tiny3.68} &  84.89{\tiny1.10} &  53.99{\tiny1.06} &  83.65{\tiny0.51} &  \textbf{53.57}{\tiny0.43} &  \textbf{83.40}{\tiny0.46} &  \underline{52.81}{\tiny0.63} &  \underline{82.92}{\tiny0.42 }\\ 
			Gram & 53.53{\tiny7.45} &  80.71{\tiny4.15} &  \textbf{20.06}{\tiny1.96} &  \textbf{95.55}{\tiny0.60} &  89.51{\tiny2.54} &  70.79{\tiny1.32} &  94.67{\tiny0.60} &  46.38{\tiny1.21} &  64.44{\tiny2.37} &  73.36{\tiny1.08 }\\ 
			EBO & 52.62{\tiny3.83} &  79.18{\tiny1.37} &  53.62{\tiny3.14} &  82.03{\tiny1.74} &  62.35{\tiny2.06} &  78.35{\tiny0.83} &  57.75{\tiny0.86} &  79.52{\tiny0.23} &  56.59{\tiny1.38} &  79.77{\tiny0.61 }\\ 
			OpenGAN & 63.09{\tiny23.25} &  68.14{\tiny18.78} &  70.35{\tiny2.06} &  68.40{\tiny2.15} &  74.77{\tiny1.78} &  65.84{\tiny3.43} &  73.75{\tiny8.32} &  69.13{\tiny7.08} &  70.49{\tiny7.38} &  67.88{\tiny7.16 }\\ 
			GradNorm & 86.97{\tiny1.44} &  65.35{\tiny1.12} &  69.90{\tiny7.94} &  76.95{\tiny4.73} &  92.51{\tiny0.61} &  64.58{\tiny0.13} &  85.32{\tiny0.44} &  69.69{\tiny0.17} &  83.68{\tiny1.92} &  69.14{\tiny1.05 }\\ 
			ReAct & 56.04{\tiny5.66} &  78.37{\tiny1.59} &  50.41{\tiny2.02} &  83.01{\tiny0.97} &  55.04{\tiny0.82} &  80.15{\tiny0.46} &  \underline{55.30}{\tiny0.41} &  80.03{\tiny0.11} &  54.20{\tiny1.56} &  80.39{\tiny0.49 }\\ 
			MLS & 52.95{\tiny3.82} &  78.91{\tiny1.47} &  53.90{\tiny3.04} &  81.65{\tiny1.49} &  62.39{\tiny2.13} &  78.39{\tiny0.84} &  57.68{\tiny0.91} &  79.75{\tiny0.24} &  56.73{\tiny1.33} &  79.67{\tiny0.57 }\\ 
			KLM & 73.09{\tiny6.67} &  74.15{\tiny2.59} &  50.30{\tiny7.04} &  79.34{\tiny0.44} &  81.80{\tiny5.80} &  75.77{\tiny0.45} &  81.40{\tiny1.58} &  75.70{\tiny0.24} &  71.65{\tiny2.01} &  76.24{\tiny0.52 }\\ 
			VIM & 48.32{\tiny1.07} &  81.89{\tiny1.02} &  46.22{\tiny5.46} &  83.14{\tiny3.71} &  \underline{46.86}{\tiny2.29} &  \underline{85.91}{\tiny0.78} &  61.57{\tiny0.77} &  75.85{\tiny0.37} &  \underline{50.74}{\tiny1.00} &  81.70{\tiny0.62 }\\ 
			KNN & 48.58{\tiny4.67} &  82.36{\tiny1.52} &  51.75{\tiny3.12} &  84.15{\tiny1.09} &  \underline{53.56}{\tiny2.32} &  \underline{83.66}{\tiny0.83} &  60.70{\tiny1.03} &  79.43{\tiny0.47} &  53.65{\tiny0.28} &  \underline{82.40}{\tiny0.17 }\\ 
			DICE & 51.79{\tiny3.67} &  79.86{\tiny1.89} &  49.58{\tiny3.32} &  84.22{\tiny2.00} &  64.23{\tiny1.65} &  77.63{\tiny0.34} &  59.39{\tiny1.25} &  78.33{\tiny0.66} &  56.25{\tiny0.60} &  80.01{\tiny0.18 }\\ 
			RankFeat & 75.01{\tiny5.83} &  63.03{\tiny3.86} &  58.49{\tiny2.30} &  72.14{\tiny1.39} &  66.87{\tiny3.80} &  69.40{\tiny3.08} &  77.42{\tiny1.96} &  63.82{\tiny1.83} &  69.45{\tiny1.01} &  67.10{\tiny1.42 }\\ 
			ASH & 66.58{\tiny3.88} &  77.23{\tiny0.46} &  \underline{46.00}{\tiny2.67} &  \underline{85.60}{\tiny1.40} &  61.27{\tiny2.74} &  80.72{\tiny0.70} &  62.95{\tiny0.99} &  78.76{\tiny0.16} &  59.20{\tiny2.46} &  80.58{\tiny0.66 }\\ 
			SHE & 58.78{\tiny2.70} &  76.76{\tiny1.07} &  59.15{\tiny7.61} &  80.97{\tiny3.98} &  73.29{\tiny3.22} &  73.64{\tiny1.28} &  65.24{\tiny0.98} &  76.30{\tiny0.51} &  64.12{\tiny2.70} &  76.92{\tiny1.16 }\\ 
			GEN & 53.92{\tiny5.71} &  78.29{\tiny2.05} &  55.45{\tiny2.76} &  81.41{\tiny1.50} &  61.23{\tiny1.40} &  78.74{\tiny0.81} &  56.25{\tiny1.01} &  \underline{80.28}{\tiny0.27} &  56.71{\tiny1.59} &  79.68{\tiny0.75 }\\ 
			\rowcolor{LightGray}
			\textbf{NAC-UE} & \underline{21.97}{\tiny6.62 } &  \underline{93.15}{\tiny1.63 } &  \underline{24.39}{\tiny4.66 } &  \underline{92.40}{\tiny1.26 } &  \textbf{40.65}{\tiny1.94 } &  \textbf{89.32}{\tiny0.55 } &  73.57{\tiny1.16  } &  73.05{\tiny0.68 } &  \textbf{40.14}{\tiny1.86 } &  \textbf{86.98}{\tiny0.37 }\\ 
			
			\bottomrule
		\end{tabular}
		
	}
	\caption{OOD detection results on the CIFAR-100 benchmark. We format \textbf{first}, \underline{second}, and \underline{third} results. 
		Following OpenOOD, we report the performance averaged over three checkpoints of ResNet-18, which are trained solely on the InD dataset, \ie, CIFAR-100.  denotes the higher value is better, while  indicates lower values are better.}
	\label{Appendix:Tab:Full_OOD_Detection_CiFAR100}
	\vspace{-2mm}
\end{table*}







\clearpage
\section{Full ImageNet Results}


\vspace{50mm}
\begin{table*}[h]
	\centering
	\adjustbox{max width=1\textwidth}{\begin{tabular}{l ccc  ccc  ccc}
			\toprule
			\multirow{2}{*}{Method}				&\multicolumn{3}{c}{iNaturalist}		&\multicolumn{3}{c}{OpenImage-O}	&\multicolumn{3}{c}{Textures}\\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}
			& ResNet-50 & Vit-b16 & \multicolumn{1}{c}{Average} & {ResNet-50} & Vit-b16 & \multicolumn{1}{c}{Average} & ResNet-50 & Vit-b16 & Average\\
\midrule
      		OpenMax & 92.05 & \underline{94.93} & \underline{93.49} & 87.62 & 87.36 & 87.49  & 88.10 & 85.52 & 86.81 \\ 
			MSP & 88.41 & 88.19 & 88.30 & 84.86 & 84.86 & 84.86  & 82.43 & 85.06 & 83.75 \\ 
			TempScale & 90.50 & 88.54 & 89.52 & 87.22 & 85.04 & 86.13  & 84.95 & 85.39 & 85.17 \\ 
			ODIN & 91.17 & / & 91.17 & 88.23 & / & 88.23  & 89.00 & / & 89.00 \\ 
			MDS & 63.67 & \underline{96.01} & 79.84 & 69.27 & \textbf{92.38} & 80.83  & 89.80 & 89.41 & 89.61 \\ 
			MDSEns & 61.82 & / & 61.82 & 60.80 & / & 60.80  & 79.94 & / & 79.94 \\ 
			RMDS & 87.24 & \textbf{96.10} & 91.67 & 85.84 & \underline{92.32} & 89.08  & 86.08 & 89.38 & 87.73 \\ 
			Gram & 76.67 & / & 76.67 & 74.43 & / & 74.43  & 88.02 & / & 88.02 \\ 
			EBO & 90.63 & 79.30 & 84.97 & 89.06 & 76.48 & 82.77  & 88.70 & 81.17 & 84.94 \\ 
			OpenGAN & / & / & / & / & / & / & / & / & / \\ 
			GradNorm & 93.89 & 42.42 & 68.16 & 84.82 & 37.82 & 61.32  & 92.05 & 44.99 & 68.52 \\ 
			ReAct & \underline{96.34} & 86.11 & 91.23 & \underline{91.87} & 84.29 & 88.08  & 92.79 & 86.66 & 89.73 \\ 
			MLS & 91.17 & 85.29 & 88.23 & 89.17 & 81.60 & 85.39  & 88.39 & 83.74 & 86.07 \\ 
			KLM & 90.78 & 89.59 & 90.19 & 87.30 & 87.03 & 87.17  & 84.72 & 86.49 & 85.61 \\ 
			VIM & 89.56 & 95.72 & 92.64 & 90.50 & \underline{92.18} & \underline{91.34}  & \textbf{97.97} & 90.61 & \underline{94.29} \\ 
			KNN & 86.41 & 91.46 & 88.94 & 87.04 & 89.86 & 88.45  & \underline{97.09} & \underline{91.12} & \underline{94.11} \\ 
			DICE & 92.54 & 82.50 & 87.52 & 88.26 & 82.22 & 85.24  & 92.04 & 82.21 & 87.13 \\ 
			RankFeat & 40.06 & / & 40.06 & 50.83 & / & 50.83  & 70.90 & / & 70.90 \\ 
			ASH & \textbf{97.07} & 50.62 & 73.85 &\textbf{ 93.26} & 55.51 & 74.39  & 96.90 & 48.53 & 72.72 \\ 
			SHE & 92.65 & 93.57 & \underline{93.11} & 86.52 & 91.04 & 88.78  & 93.60 & \underline{92.65} & 93.13 \\ 
			GEN & 92.44 & 93.54 & 92.99 & 89.26 & 90.27 & \underline{89.77}  & 87.59 & 90.23 & 88.91 \\ 
			\rowcolor{LightGray}
			\textbf{NAC-UE} & \underline{96.52} & 93.72 & \textbf{95.12} & \underline{91.45} & 91.58 & \textbf{91.52} & \underline{97.9} & \textbf{94.17} & \textbf{96.04} \\ 
			\bottomrule
		\end{tabular}
		
	}
	\caption{OOD detection results on the ImageNet benchmark. We format \textbf{first}, \underline{second}, and \underline{third} results. Following OpenOOD, we report the AUROC scores over two backbones (ResNet-50 and Vit-b16), which are trained solely on the InD dataset, \ie, ImageNet-1k. }
	\label{Appendix:Tab:Full_OOD_Detection_ImageNet}
\end{table*}













\newpage
\section{Full DomainBed Results}
\label{Appendix:Sec_DomainBed_Results}


\vspace{8em}
\begin{table*}[h]
	\centering
	\adjustbox{max width=1\textwidth}{\newcolumntype{g}{>{\columncolor{LightGray}}c}
		\begin{tabular}{l g gg gg gg gg gg}
\toprule
				\rowcolor{white}
				\multirow{2}{*}{}		&\multirow{2}{*}{Method}		&\multicolumn{2}{c}{Caltech101}		&\multicolumn{2}{c}{LabelMe}		&\multicolumn{2}{c}{SUN09}		&\multicolumn{2}{c}{VOC2007}		&\multicolumn{2}{c}{Average}		\\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \rowcolor{white}
				&								&RC			&ACC						&RC			&ACC					&RC			&ACC					&RC			&ACC				&RC		&ACC		\\
				\midrule \rowcolor{white}
				& Oracle & - & 97.00{\tiny 0.6} & - & 65.60{\tiny 0.3} & - & 71.44{\tiny 0.8} & - & 76.64{\tiny 0.5} & - & 77.67  \\ 
				\cmidrule(lr){3-12} \rowcolor{white}
				& Validation & 36.03{\tiny 17.3} & 95.38{\tiny 0.9} & \textbf{17.57}{\tiny 13.2} & 63.62{\tiny 1.1} & 50.33{\tiny 13.6} & 67.73{\tiny 0.6} & 33.17{\tiny 15.7} & \textbf{73.75}{\tiny 0.7} & 34.27 & 75.12  \\ 
				& NAC-ME & \textbf{67.73}{\tiny 3.0} & \textbf{96.41}{\tiny 0.5} & 7.52{\tiny 3.4} & \textbf{63.72}{\tiny 0.8} & \textbf{64.22}{\tiny 7.2} & \textbf{70.89}{\tiny 1.1} & \textbf{61.68}{\tiny 10.2} & 72.29{\tiny 0.5} & \textbf{50.29} & \textbf{75.83}  \\ 					
				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN18}} 
				

				& Oracle & - & 98.53{\tiny 0.3} & - & 68.69{\tiny 0.8} & - & 73.88{\tiny 0.5} & - & 78.07{\tiny 0.3}  & - & 79.79  \\ 
				\cmidrule(lr){3-12} \rowcolor{white}
				& Validation & 20.75{\tiny 17.0} & 98.00{\tiny 0.2} & \textbf{35.29}{\tiny 13.2} & \textbf{65.16}{\tiny 1.4} & \textbf{33.01}{\tiny 3.1} & 70.37{\tiny 0.6} & \textbf{36.68}{\tiny 4.3} & \textbf{77.28}{\tiny 0.3}  & \textbf{31.43} & \textbf{77.70}  \\ 
				& NAC-ME & \textbf{54.90}{\tiny 2.6} & \textbf{98.50}{\tiny 0.3} & -2.04{\tiny 2.7} & 60.27{\tiny 0.6} & 28.27{\tiny 14.0} & \textbf{70.88}{\tiny 2.1} & 33.58{\tiny 8.9} & 76.00{\tiny 1.0}  & 28.68 & 76.41  \\ 					
				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN50}} 
				
				& Oracle & - & 98.88{\tiny 0.1} & - & 66.65{\tiny 0.3} & - & 74.78{\tiny 0.2} & - & 76.14{\tiny 0.3} & - & 79.11  \\ 
				\cmidrule(lr){3-12} \rowcolor{white}
				& Validation & \textbf{25.57}{\tiny 8.8} & \textbf{98.32}{\tiny 0.3} & 41.01{\tiny 4.6} & 63.87{\tiny 0.6} & 47.14{\tiny 2.7} & 72.44{\tiny 0.1} & 38.07{\tiny 12.3} & \textbf{75.08}{\tiny 0.6} & 37.95 & 77.43  \\ 
				& NAC-ME & 24.02{\tiny 0.2} & 98.26{\tiny 0.1} & \textbf{69.69}{\tiny 3.6} & \textbf{64.30}{\tiny 0.2} & \textbf{49.51}{\tiny 6.2} & \textbf{74.36}{\tiny 0.4} & \textbf{55.15}{\tiny 9.0} & 74.95{\tiny 0.3} & \textbf{49.59} & \textbf{77.97}  \\ 					
				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{Vit-t16}} 
				

				& Oracle & - & 98.65{\tiny 0.1} & - & 67.18{\tiny 0.5} & - & 78.24{\tiny 0.4} & - & 79.77{\tiny 0.5} & - & 80.96  \\ 
				\cmidrule(lr){3-12} \rowcolor{white}
				& Validation & -6.45{\tiny 10.2} & 95.49{\tiny 0.7} & \textbf{43.30}{\tiny 14.1} & \textbf{64.67}{\tiny 0.6} & 12.83{\tiny 12.2} & 76.68{\tiny 0.9} & 25.57{\tiny 26.9} & \textbf{77.96}{\tiny 0.9} & 18.81 & 78.70  \\ 					\multirow{-1}[9]{*}{\rotatebox[origin=c]{90}{Vit-b16}} 
				& NAC-ME & \textbf{47.79}{\tiny 2.2} & \textbf{97.44}{\tiny 0.1} & 38.48{\tiny 10.4} & 64.30{\tiny 1.4} & \textbf{30.07}{\tiny 11.6} & \textbf{77.22}{\tiny 0.4} & \textbf{33.33}{\tiny 4.1} & 77.85{\tiny 0.4} & \textbf{37.42} & \textbf{79.20}  \\ 
				\bottomrule
			\end{tabular}
		}
		\caption{OOD generalization results on VLCS dataset~\citep{Dataset:VLCS}. \textit{Oracle} denotes the upper bound, which uses OOD test data to evaluate models. The training strategy is ERM~\citep{Baseline:ERM}. All scores are averaged over 3 random trials. }
		\label{Appendix:Tab:OOD_Gen_Full_VLCS}
		\vspace{-2mm}
	\end{table*}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\vspace{10em}
	\begin{table*}[h]
		\centering
		\adjustbox{max width=1\textwidth}{\newcolumntype{g}{>{\columncolor{LightGray}}c}
			\begin{tabular}{l g gg gg gg gg gg}
\toprule
					\rowcolor{white}
					\multirow{2}{*}{}		&\multirow{2}{*}{Method}		&\multicolumn{2}{c}{Art}		&\multicolumn{2}{c}{Cartoon}		&\multicolumn{2}{c}{Photo}		&\multicolumn{2}{c}{Sketch}		&\multicolumn{2}{c}{Average}		\\
					\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \rowcolor{white}
					&								&RC			&ACC						&RC			&ACC					&RC			&ACC					&RC			&ACC				&RC		&ACC		\\
					\midrule \rowcolor{white}

			        & Oracle & - & 78.52{\tiny 0.2} & - & 75.09{\tiny 0.8} & - & 94.96{\tiny 0.3} & - & 73.47{\tiny 1.5} & - & 80.51  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
					& Validation & 72.22{\tiny 5.1} & 77.32{\tiny 0.7} & 65.20{\tiny 6.6} & \textbf{71.91}{\tiny 0.7} & 60.87{\tiny 7.1} & 94.44{\tiny 0.2} & 76.55{\tiny 1.2} & \textbf{72.36}{\tiny 1.1} & 68.71 & \textbf{79.01}  \\ 
					& NAC-ME & \textbf{75.49}{\tiny 5.8} & \textbf{77.89}{\tiny 0.3} & \textbf{74.84}{\tiny 1.3} & 71.54{\tiny 0.8} & \textbf{65.36}{\tiny 6.0} & \textbf{94.64}{\tiny 0.2} & \textbf{80.96}{\tiny 1.9} & 71.34{\tiny 2.4} & \textbf{74.16} & 78.85  \\ 				
					\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN18}} 
					
					& Oracle & - & 86.78{\tiny 0.5} & - & 81.31{\tiny 0.5} & - & 98.43{\tiny 0.0} & - & 77.87{\tiny 0.4}  & - & 86.10  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
					& Validation & 70.26{\tiny 9.1} & \textbf{86.72}{\tiny 0.5} & 65.93{\tiny 10.3} & 78.86{\tiny 1.3} & \textbf{38.73}{\tiny 12.3} & \textbf{97.83}{\tiny 0.1} & 59.23{\tiny 11.4} & 74.87{\tiny 1.1}  & 58.54 & 84.57  \\ 
					& NAC-ME & \textbf{73.61}{\tiny 1.4} & 86.56{\tiny 0.4} & \textbf{76.14}{\tiny 5.0} & \textbf{80.22}{\tiny 1.1} & 30.15{\tiny 15.3} & 97.68{\tiny 0.1} & \textbf{68.38}{\tiny 8.8} & \textbf{76.66}{\tiny 1.2}  & \textbf{62.07} & \textbf{85.28}  \\ 				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN50}} 
					
					& Oracle & - & 75.84{\tiny 0.1} & - & 66.01{\tiny 0.7} & - & 96.31{\tiny 0.2} & - & 49.79{\tiny 1.6} & - & 71.99  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
					& Validation & \textbf{88.97}{\tiny 3.7} & \textbf{75.66}{\tiny 0.2} & 92.32{\tiny 1.6} & \textbf{65.41}{\tiny 0.4} & 93.79{\tiny 1.8} & \textbf{96.16}{\tiny 0.2} & 82.27{\tiny 3.7} & 42.10{\tiny 2.2} & 89.34 & 69.83  \\ 
					& NAC-ME & 88.15{\tiny 3.8} & 75.64{\tiny 0.2} & \textbf{92.57}{\tiny 0.5} & 64.04{\tiny 0.6} & \textbf{95.02}{\tiny 2.0} & 96.11{\tiny 0.2} & \textbf{86.93}{\tiny 2.4} & \textbf{48.20}{\tiny 1.9} & \textbf{90.67} & \textbf{70.99}  \\ \midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{Vit-t16}} 
					
					& Oracle & - & 94.81{\tiny 0.3} & - & 86.57{\tiny 0.2} & - & 99.65{\tiny 0.0} & - & 79.89{\tiny 0.6} & - & 90.23  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
					& Validation & \textbf{22.96}{\tiny 7.7} & 92.58{\tiny 0.2} & 47.96{\tiny 4.3} & 84.54{\tiny 0.3} & \textbf{55.64}{\tiny 5.2} & \textbf{99.43}{\tiny 0.0} & 38.97{\tiny 3.1} & 74.66{\tiny 2.8} & 41.38 & 87.80  \\ \multirow{-1}[9]{*}{\rotatebox[origin=c]{90}{Vit-b16}} 
					& NAC-ME & 17.73{\tiny 3.9} & \textbf{93.25}{\tiny 0.5} & \textbf{63.24}{\tiny 3.1} & \textbf{85.09}{\tiny 1.1} & 37.17{\tiny 7.7} & 99.33{\tiny 0.1} & \textbf{62.01}{\tiny 6.0} & \textbf{77.66}{\tiny 0.4} & \textbf{45.04} & \textbf{88.83}  \\ 



					\bottomrule
				\end{tabular}
			}
			\caption{OOD generalization results on PACS dataset~\citep{Dataset:PACS}. \textit{Oracle} denotes the upper bound, which uses OOD test data to evaluate models. The training strategy is ERM~\citep{Baseline:ERM}. All scores are averaged over 3 random trials. }
			\label{Appendix:Tab:OOD_Gen_Full_PACS}
			\vspace{-2mm}
		\end{table*}
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		\newpage
		\begin{table*}
			\centering
			\adjustbox{max width=1\textwidth}{\newcolumntype{g}{>{\columncolor{LightGray}}c}
				\begin{tabular}{l g gg gg gg gg gg}
\toprule
						\rowcolor{white}
						\multirow{2}{*}{}		&\multirow{2}{*}{Method}		&\multicolumn{2}{c}{Art}		&\multicolumn{2}{c}{Clipart}		&\multicolumn{2}{c}{Product}		&\multicolumn{2}{c}{Real}		&\multicolumn{2}{c}{Average}		\\
						\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \rowcolor{white}
						&								&RC			&ACC						&RC			&ACC					&RC			&ACC					&RC			&ACC				&RC		&ACC		\\
						\midrule \rowcolor{white}

         				& Oracle & - & 48.04{\tiny 0.2} & - & 41.99{\tiny 0.2} & - & 66.26{\tiny 0.2} & - & 68.41{\tiny 0.2} & - & 56.18  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
						& Validation & \textbf{86.36}{\tiny 1.9} & \textbf{47.68}{\tiny 0.3} & 75.33{\tiny 3.2} & \textbf{41.16}{\tiny 0.6} & 88.73{\tiny 3.3} & 65.82{\tiny 0.1} & 83.58{\tiny 3.1} & 67.73{\tiny 0.4} & 83.50 & 55.60  \\ 
						& NAC-ME & 86.19{\tiny 2.5} & \textbf{47.68}{\tiny 0.1} & \textbf{77.45}{\tiny 5.8} & \textbf{41.16}{\tiny 0.6} & \textbf{91.83}{\tiny 1.2} & \textbf{66.15}{\tiny 0.2} & \textbf{84.15}{\tiny 4.5} & \textbf{68.04}{\tiny 0.3} & \textbf{84.91} & \textbf{55.76}  \\ 
						\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN18}} 
						
						& Oracle & - & 60.20{\tiny 0.3} & - & 51.76{\tiny 0.2} & - & 75.49{\tiny 0.1} & - & 76.37{\tiny 0.3}  & - & 65.95  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
						& Validation & 71.32{\tiny 4.2} & 59.01{\tiny 0.5} & 53.43{\tiny 6.5} & \textbf{50.29}{\tiny 0.4} & \textbf{81.21}{\tiny 5.7} & \textbf{74.96}{\tiny 0.5} & \textbf{65.77}{\tiny 7.0} & \textbf{75.88}{\tiny 0.2}  & 67.93 & 65.04  \\ 
						& NAC-ME & \textbf{78.68}{\tiny 7.0} & \textbf{60.20}{\tiny 0.3} & \textbf{59.15}{\tiny 3.1} & 50.19{\tiny 0.4} & 78.68{\tiny 5.3} & 74.66{\tiny 0.4} & 60.13{\tiny 7.3} & 75.86{\tiny 0.1}  & \textbf{69.16} & \textbf{65.23}  \\ 				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN50}} 
						
						& Oracle & - & 56.97{\tiny 0.1} & - & 43.58{\tiny 0.4} & - & 71.82{\tiny 0.1} & - & 73.41{\tiny 0.1} & - & 61.44  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
						& Validation &\textbf{ 98.77}{\tiny 0.3} & \textbf{56.39}{\tiny 0.4} & 98.45{\tiny 0.1} & 43.47{\tiny 0.5} & 98.28{\tiny 0.6} & 71.62{\tiny 0.2} & 99.35{\tiny 0.3} & \textbf{73.41}{\tiny 0.1} & 98.71 & 61.22  \\ 
						& NAC-ME & \textbf{98.77}{\tiny 0.5} & \textbf{56.39}{\tiny 0.4} & \textbf{98.86}{\tiny 0.4} & \textbf{43.55}{\tiny 0.4} & \textbf{99.35}{\tiny 0.3} & \textbf{71.73}{\tiny 0.1} & \textbf{99.59}{\tiny 0.1} & 73.39{\tiny 0.1} & \textbf{99.14} & \textbf{61.26 } \\ \midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{Vit-t16}} 
						
						& Oracle & - & 78.94{\tiny 0.2} & - & 68.12{\tiny 0.3} & - & 87.93{\tiny 0.1} & - & 89.91{\tiny 0.0} & - & 81.23  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
						& Validation & 54.66{\tiny 4.7} & 77.77{\tiny 0.3} & 56.70{\tiny 2.4} & 66.49{\tiny 0.3} & \textbf{61.03}{\tiny 5.9} & 87.19{\tiny 0.0} & \textbf{60.78}{\tiny 9.2} & 88.99{\tiny 0.1} & 58.29 & 80.11  \\ \multirow{-1}[9]{*}{\rotatebox[origin=c]{90}{Vit-b16}} 
						& NAC-ME & \textbf{70.83}{\tiny 1.0} & \textbf{78.03}{\tiny 0.3} & \textbf{65.03}{\tiny 2.3} & \textbf{67.52}{\tiny 0.7} & 56.13{\tiny 3.6} & \textbf{87.43}{\tiny 0.3} & 60.70{\tiny 3.2} & \textbf{89.12}{\tiny 0.2} & \textbf{63.17} & \textbf{80.52}  \\ 



						\bottomrule
					\end{tabular}
				}
				\caption{OOD generalization results on OfficeHome dataset~\citep{Dataset:OfficeHome}. \textit{Oracle} denotes the upper bound, which uses OOD test data to evaluate models. The training strategy is ERM~\citep{Baseline:ERM}. All scores are averaged over 3 random trials. }
				\label{Appendix:Tab:OOD_Gen_Full_Office}
				\vspace{-2mm}
			\end{table*}
			
			
			
			
			
			\begin{table*}
				\centering
				\adjustbox{max width=1\textwidth}{\newcolumntype{g}{>{\columncolor{LightGray}}c}
					\begin{tabular}{l g gg gg gg gg gg}
\toprule
							\rowcolor{white}
							\multirow{2}{*}{}		&\multirow{2}{*}{Method}		&\multicolumn{2}{c}{Loc100}		&\multicolumn{2}{c}{Loc38}		&\multicolumn{2}{c}{Loc43}		&\multicolumn{2}{c}{Loc46}		&\multicolumn{2}{c}{Average}		\\
							\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \rowcolor{white}
							&								&RC			&ACC						&RC			&ACC					&RC			&ACC					&RC			&ACC				&RC		&ACC		\\
							\midrule \rowcolor{white}

					        & Oracle & - & 54.94{\tiny 1.3} & - & 35.64{\tiny 0.7} & - & 52.32{\tiny 0.1} & - & 35.14{\tiny 0.6} & - & 44.51  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
							& Validation & \textbf{12.01}{\tiny 11.9} & 40.60{\tiny 2.5} & 49.75{\tiny 10.9} & 28.41{\tiny 2.9} & \textbf{58.17}{\tiny 12.8} & 48.31{\tiny 1.5} & 38.40{\tiny 10.3} & 32.12{\tiny 0.8} & 39.58 & 37.36  \\ 
							& NAC-ME & 10.29{\tiny 13.2} & \textbf{41.31}{\tiny 2.5} & \textbf{53.19}{\tiny 9.4} & \textbf{33.23}{\tiny 0.7} & 54.49{\tiny 8.2} & \textbf{50.26}{\tiny 0.5} & \textbf{43.71}{\tiny 10.6} & \textbf{33.01}{\tiny 0.2} & \textbf{40.42} & \textbf{39.45}  \\ 
							\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN18}} 
							
							& Oracle & - & 55.62{\tiny 0.5} & - & 45.12{\tiny 1.1} & - & 58.75{\tiny 0.3} & - & 43.55{\tiny 0.8}  & - & 50.76  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
							& Validation & 43.95{\tiny 7.6} & 49.08{\tiny 3.5} & \textbf{36.60}{\tiny 13.6} & 37.44{\tiny 2.3} & \textbf{28.02}{\tiny 8.6} & \textbf{56.12}{\tiny 0.3} & 39.71{\tiny 15.0} & \textbf{41.63}{\tiny 0.5}  & 37.07 & 46.07  \\ 
							& NAC-ME & \textbf{48.28}{\tiny 7.0} & \textbf{50.94}{\tiny 2.5} & 34.07{\tiny 15.4} & \textbf{40.93}{\tiny 2.0} & 26.06{\tiny 8.4} & 55.95{\tiny 0.6} & \textbf{52.21}{\tiny 15.1} & 40.59{\tiny 0.9}  & \textbf{40.16} & \textbf{47.10}  \\ 				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{RN50}} 
							
							& Oracle & - & 52.03{\tiny 0.3} & - & 27.38{\tiny 3.0} & - & 49.61{\tiny 0.4} & - & 36.14{\tiny 0.1}  & - & 41.29  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
							& Validation & 21.24{\tiny 11.8} & 43.51{\tiny 2.8} & 13.15{\tiny 4.0} & \textbf{20.85}{\tiny 2.1} &\textbf{ 20.02}{\tiny 18.6} & 46.55{\tiny 0.1} & 36.44{\tiny 14.2} & 34.20{\tiny 0.7}  & 22.71 & 36.28  \\ 
							& NAC-ME & \textbf{21.65}{\tiny 12.1} & \textbf{44.37}{\tiny 3.3} & \textbf{15.77}{\tiny 1.5} & 20.23{\tiny 0.7} & 18.30{\tiny 17.9} & \textbf{46.77}{\tiny 0.2} & \textbf{37.34}{\tiny 13.9} & \textbf{35.39}{\tiny 0.5}  & \textbf{23.26} & \textbf{36.69}  \\ 				\midrule \rowcolor{white} \multirow{-3}[12]{*}{\rotatebox[origin=c]{90}{Vit-t16}} 
							
							& Oracle & - & 62.23{\tiny 0.4} & - & 46.94{\tiny 1.7} & - & 57.45{\tiny 0.5} & - & 42.29{\tiny 0.1} & - & 52.23  \\ 				\cmidrule(lr){3-12} \rowcolor{white}
							& Validation & -1.31{\tiny 3.1} & 53.13{\tiny 2.0} & -16.91{\tiny 13.4} & 36.78{\tiny 2.2} & -3.27{\tiny 9.5} & \textbf{54.19}{\tiny 0.2} & \textbf{25.16}{\tiny 7.0} & 37.84{\tiny 0.4} & 0.92 & 45.49  \\ \multirow{-1}[9]{*}{\rotatebox[origin=c]{90}{Vit-b16}} 
							& NAC-ME & \textbf{32.60}{\tiny 11.5} & \textbf{58.98}{\tiny 0.7} & \textbf{11.44}{\tiny 19.7} & \textbf{40.48}{\tiny 2.6} & \textbf{15.60}{\tiny 19.7} & 53.63{\tiny 0.6} & 21.24{\tiny 2.7} & \textbf{38.35}{\tiny 0.4} & \textbf{20.22} & \textbf{47.86}  \\ 


							\bottomrule
						\end{tabular}
					}
					\caption{OOD generalization results on TerraInc dataset~\citep{Dataset:TerraIncognita}. \textit{Oracle} denotes the upper bound, which uses OOD test data to evaluate models. The training strategy is ERM~\citep{Baseline:ERM}. All scores are averaged over 3 random trials. }
					\label{Appendix:Tab:OOD_Gen_Full_Terra}
					\vspace{-2mm}
				\end{table*}
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				
			\end{document}
