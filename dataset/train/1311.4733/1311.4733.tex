\chapter{Monovariant Analysis and \ptime}
\chaptermark{Monovariant Analysis and PTIME}
\label{chapter-0cfa}

The monovariant form of flow analysis defined over the pure
-calculus has emerged as a fundamental notion of flow
analysis for higher-order languages, and some form of flow analysis is
used in most analyses for higher-order languages
\cite{heintze-mcallester-pldi97}.

In this chapter, we examine several of the most well-known variations
of monovariant flow analysis: Shivers' 0CFA
\citeyearpar{shivers-pldi88}, Henglein's simple closure analysis
\citeyearpar{henglein92d}, Heintze and McAllester's subtransitive flow
analysis \citeyearpar{heintze-mcallester-pldi97}, Ashley and Dybvig's
sub-0CFA \citeyearpar{ashley-dybvig-toplas98}, Mossin's single
source/use analysis \citeyearpar{mossin-njc98}, and others.

In each case, evaluation and analysis are proved equivalent for the
class of linear programs and a precise characterization of the
computational complexity of the analysis, namely \ptime-completeness,
is given.

\section{The Approximation of Monovariance}

To ensure tractability of any static analysis, there has to be an {\em
  approximation} of something, where information is deliberately {\em
  lost} in the service of providing what is left in a reasonable
amount of time.  A good example of what is lost during {\em
  monovariant} static analysis is that the information gathered for
each occurrence of a bound variable is merged.  When variable 
occurs twice in function position with two different arguments,

a monovariant flow analysis will blur which copy of the function is
applied to which argument.  If a function  flows into 
in this example, the analysis treats occurrences of  in  as
bound to {\em both}  and
. 

Shivers' 0CFA is among the most well-known forms of monovariant flow
analysis; however, the best known algorithm for 0CFA requires nearly
cubic time in proportion to the size of the analyzed program.

It is natural to wonder whether it is possible to do better, avoiding
this bottleneck, either by improving the 0CFA algorithm in some clever
way or by {\em further} approximation for the sake of faster
computation.

Consequently, several analyses have been designed to approximate 0CFA
by trading precision for faster computation.  Henglein's simple
closure analysis, for example, forfeits the notion of directionality
in flows.  Returning to the earlier example,

simple closure analysis, like 0CFA, will blur  and
 as arguments to , causing  to be bound to both.
But unlike 0CFA, a {\em bidirectional} analysis such as simple closure
analysis will identify two -terms with each other.  That is,
because both are arguments to the same function, by the
bi-directionality of the flows,  may flow out of  and {\em vice versa}.

Because of this further curtailing of information, simple closure
analysis enjoys an ``almost linear'' time algorithm.  But in making
trade-offs between precision and complexity, what has been given up
and what has been gained?  Where do these analyses differ and where do
they coincide?

We identify a core language---the linear -calculus---where
0CFA, simple closure analysis, and many other known approximations or
restrictions to 0CFA are rendered identical.  Moreover, for this core
language, analysis corresponds with (instrumented) evaluation.
Because analysis faithfully captures evaluation, and because the
linear -calculus is complete for \ptime, we derive
\ptime-completeness results for all of these analyses.

Proof of this lower bound relies on the insight that linearity of
programs subverts the approximation of analysis and renders it
equivalent to evaluation.  We establish a correspondence between
Henglein's simple closure analysis and evaluation for linear terms.
In doing so, we derive sufficient conditions effectively
characterizing not only simple closure analysis, but many known flow
analyses computable in less than cubic time, such as Ashley and
Dybvig's sub-0CFA, Heintze and McAllester's subtransitive flow
analysis, and Mossin's single source/use analysis.

By using a nonstandard, symmetric implementation of Boolean logic
within the linear lambda calculus, it is possible to simulate circuits
at analysis-time, and as a consequence, we prove that all of the above
analyses are complete for \ptime.  Any sub-polynomial algorithm for
these problems would require (unlikely) breakthrough results in
complexity, such as \ptime\  \logspace.

We may continue to wonder whether it is possible to do better, either
by improving the 0CFA algorithm in some clever way or by further
approximation for faster computation.  However these theorems
demonstrate the limits of both avenues.  0CFA is inherently
sequential, and so is {\em any} algorithm for it, no matter how
clever.  Designing a provably efficient parallel algorithm for 0CFA is
as hard as parallelizing all polynomial time computations.  On the
other hand, further approximations, such as simple closure analysis
and most other variants of monovariant flow analysis, make no
approximation on a linear program.  This means they too are inherently
sequential and no easier to parallelize.


\section{0CFA}
\label{sec:0cfa}

Something interesting happens when .  Notice in the application
rule of the CFA abstract evaluator of \autoref{fig-a-imperative}
that environments are extended as .  When ,
.  All contour environments map to
the empty contour, and therefore carry no contextual information.  As
such, 0CFA is a ``monovariant'' analysis, analogous to simple-type
inference, which is a monovariant type analysis.

Since there is only one constant environment (the ``everywhere
'' environment), environments of \autoref{sec:ai} can be
eliminated from the analysis altogether and the cache no longer needs
a contour argument.  Likewise, the set of abstract values collapses
from  into .

The result of 0CFA is an {\em abstract cache} that maps each program
point (i.e., label) to a set of lambda abstractions which potentially
flow into this program point at run-time:

Caches are extended using the notation , and
we write  to mean .  It is convenient to sometimes think of caches
as mutable tables (as we do in the algorithm below), so we abuse
syntax, letting this notation mean both functional extension and
destructive update.  It should be clear from context which is implied.

\paragraph{The Analysis:} We present the specification of the analysis
here in the style of \citet{nielson-nielson-hankin}.  Each
subexpression is identified with a unique superscript {\em label}
, which marks that program point;  stores all
possible values flowing to point ,  stores all
possible values flowing to the definition site of .  An {\em
  acceptable} 0CFA analysis for an expression  is written
 and derived according to the scheme given in
\autoref{fig-0cfa-acceptability}.

\begin{figure}[h]

\caption{0CFA abstract cache acceptability.}
\label{fig-0cfa-acceptability}
\end{figure}

The  relation needs to be coinductively defined since
verifying a judgment  may obligate
verification of  which in turn may require
verification of .  The above specification of
acceptability, when read as a table, defines a functional, which is
monotonic, has a fixed point, and  is defined coinductively
as the greatest fixed point of this functional.\footnote{See
  \citet{nielson-nielson-hankin} for details and a thorough discussion
  of coinduction in specifying static analyses.}

Writing  means ``the abstract cache
contains all the flow information for program fragment  at program
point .''  The goal is to determine the {\em least} cache
solving these constraints to obtain the most precise analysis.  Caches
are partially ordered with respect to the program of interest:


These constraints can be thought of as an abstract evaluator---
 simply means {\em evaluate} , which
serves {\em only} to update an (initially empty) cache.

\begin{figure}[h]

\caption{Abstract evaluator  for 0CFA, imperative style.}
\label{fig-0cfa-imperative}
\end{figure}

The abstract evaluator  is iterated until the finite cache
reaches a fixed point.

\paragraph{Fine Print:} A single iteration of  may in turn
make a recursive call  with no change in the cache, so care
must be taken to avoid looping.  This amounts to appealing to the
coinductive hypothesis  in verifying
.  However, we consider this inessential
detail, and it can safely be ignored for the purposes of obtaining our
main results in which this behavior is {\em never triggered}.

Since the cache size is polynomial in the program size, so is the
running time, as the cache is {\em monotonic}---values are put in, but
never taken out.  Thus the analysis and any decision problems
answered by the analysis are clearly computable within polynomial
time.

\begin{lemma}
The control flow problem for 0CFA is contained in \ptime.
\end{lemma}
\begin{proof}
0CFA computes a binary relation over a fixed structure.  The
computation of the relation is monotone: it begins as empty and is
added to incrementally.  Because the structure is finite, a fixed
point must be reached by this incremental computation.  The binary
relation can be at most polynomial in size, and each increment is
computed in polynomial time.
\end{proof}


\paragraph{An Example:} 

Consider the following program, which we will return to discuss
further in subsequent analyses:



The least 0CFA is given by the following cache:

where we write  as shorthand for , etc.


\section{Henglein's Simple Closure Analysis}
\label{sec:simple}

Simple closure analysis follows from an observation by Henglein some
15 years ago ``in an influential though not often credited technical
report'' \cite[page 4]{midtgaard-07}: he noted that the standard
control flow analysis can be computed in dramatically less time by
changing the specification of flow constraints to use equality rather
than containment \cite{henglein92d}.  The analysis bears a strong
resemblance to simple-type inference---analysis can be performed by
emitting a system of equality constraints and then solving them using
{\em unification}, which can be computed in almost linear time with a
union-find data structure.

Consider a program with both  and  as subexpressions.
Under 0CFA, whatever flows into  and  will also flow into the
formal parameter of all abstractions flowing into , but it is not
necessarily true that whatever flows into  {\em also} flows into
 and {\em vice versa}.  However, under simple closure analysis,
this is the case.  For this reason, flows in simple closure analysis
are said to be {\em bidirectional}.

\paragraph{The Analysis:} 

The specification of the analysis is given in
\autoref{fig-simple-declarative}.

\begin{figure}[h]

\caption{Simple closure analysis abstract cache acceptability.}
\label{fig-simple-declarative}
\end{figure}

\paragraph{The Algorithm:} 

We write  to mean
.


The abstract evaluator  is iterated until a fixed point
is reached.\footnote{The fine print of \autoref{sec:0cfa} applies as
well.}  By similar reasoning to that given for 0CFA, simple closure
analysis is clearly computable within polynomial time.

\begin{lemma}
The control flow problem for simple closure analysis is contained in \ptime.
\end{lemma}

\paragraph{An Example:}

Recall the example program of the previous section:


Notice that  is applied to itself and then to , so  will be bound to both  and ,
which induces an equality between these two terms.  Consequently,
everywhere that 0CFA was able to deduce a flow set of  or  will be replaced by  under a simple closure analysis.  The least simple closure
analysis is given by the following cache (new flows are underlined):


\section{Linearity: Analysis is Evaluation}
\label{sec:linearity}

It is straightforward to observe that in a {\em linear}
-term, each abstraction  can be applied to at
most one argument, and hence the abstracted value can be bound to at
most one argument.\footnote{Note that this observation is clearly
untrue for the {\em nonlinear} -term , as  is bound to , and also to .}
Generalizing this observation, analysis of a linear -term
coincides exactly with its evaluation.  So not only are the analyses
equivalent on linear terms, but they are also synonymous with
evaluation.

A natural and expressive class of such linear terms are the ones which
implement Boolean logic.  When analyzing the coding of a Boolean
circuit and its inputs, the Boolean output will flow to a
predetermined place in the (abstract) cache.  By placing that value in
an appropriate context, we construct an instance of the control flow
problem: a function  flows to a call site  iff the Boolean
output is .

Since the circuit value problem \cite{ladner-75}, which is complete
for \ptime, can be reduced to an instance of the 0CFA control flow
problem, we conclude this control flow problem is \ptime-hard.
Further, as 0CFA can be computed in polynomial time, the control flow
problem for 0CFA is \ptime-complete.


One way to realize the computational potency of a static analysis is
to subvert this loss of information, making the analysis an {\em
exact} computational tool.  Lower bounds on the expressiveness of an
analysis thus become exercises in hacking, armed with this newfound
tool.  Clearly the more approximate the analysis, the less we have to
work with, computationally speaking, and the more we have to do to
undermine the approximation.  But a fundamental technique has emerged
in understanding expressivity in static analysis---{\em linearity}.


In this section, we show that when the program is {\em linear}---every
bound variable occurs exactly once---analysis and evaluation are
synonymous.

First, we start by considering an alternative evaluator, given in
\autoref{figure-eval-alt}, which is slightly modified from the one
given in \autoref{figure-eval}.  Notice that this evaluator
``tightens'' the environment in the case of an application, thus
maintaining throughout evaluation that the domain of the environment
is exactly the set of free variables in the expression.  When
evaluating a variable occurrence, there is only one mapping in the
environment: the binding for this variable. Likewise, when
constructing a closure, the environment does not need to be
restricted: it already is.


\begin{figure}

\caption{Evaluator .}
\label{figure-eval-alt}
\end{figure}



This alternative evaluator  will be useful in reasoning
about linear programs, but it should be clear that it is equivalent to
the original, standard evaluator  of \autoref{figure-eval}.

\begin{lemma}
, when .
\end{lemma}

In a linear program, each mapping in the environment corresponds to
the single occurrence of a bound variable.  So when evaluating an
application, this tightening {\em splits} the environment  into
, where  closes the operator,  closes
the operand, and .

\begin{definition}
Environment  {\em linearly closes}  (or  is a {\em linear closure}) iff  is linear, 
closes , and for all ,  occurs exactly once
(free) in ,  is a linear closure, and for all
 does not occur (free or bound) in . The
{\em size} of a linear closure  is defined as:

\end{definition}

The following lemma states that evaluation of a linear closure cannot
produce a larger value.  This is the environment-based analog to the
easy observation that -reduction {\em strictly} decreases the
size of a linear term.
\begin{lemma}\label{lem:smaller}
If  linearly closes  and , then
.
\end{lemma}
\begin{proof}
Straightforward by induction on , reasoning by case analysis
on .  Observe that the size strictly decreases in the application
and variable case, and remains the same in the abstraction case.
\end{proof}

The function  is extended to closures and environments by
taking the union of all labels in the closure or in the range of the
environment, respectively.

\begin{definition}
The set of labels in a given term, expression, environment, or closure
is defined as follows:

\end{definition}

\begin{definition}
  A cache  {\em respects} 
  (written ) when,
\begin{enumerate}
\item  linearly closes ,
\item ,
\item , and
\item .
\end{enumerate}
\end{definition}
Clearly,  when  is closed and linear,
i.e.~ is a linear 

\begin{figure}[h]

\caption{Abstract evaluator  for 0CFA, functional style.}
\label{fig-0cfa-functional}
\end{figure}

\autoref{fig-0cfa-functional} gives a ``cache-passing'' functional
algorithm for  of \autoref{sec:simple}.  It is equivalent
to the functional style abstract evaluator of
\autoref{fig-a-functional} specialized by letting .
We now state and prove the main theorem of this section in terms of
this abstract evaluator.

\begin{theorem}\label{thm:main}
  If , ,
  , , and ,
  then , , and
  .
\end{theorem}
An important consequence is noted in Corollary \ref{cor:normal}.

\begin{proof} By induction on , reasoning by case analysis on .
\begin{itemize}
\item Case .

Since  and  linearly closes , thus  and  linearly closes
.  By definition,

Again since , , with which the
assumption  implies

and therefore .  It
remains to show that .
By definition, .  Since  and  do not
occur in  by linearity and assumption, respectively, it
follows that  and the case
holds.

\item Case .

By definition,

and by assumption , so  and therefore
.
By assumptions  and
, it follows that
 and
the case holds.

\item Case . Let

Clearly, for ,  and


By induction, for  and .  From this, it is straightforward to observe that
 and  where  and  are disjoint.  So let
.  It is
clear that .  Furthermore,


By Lemma \ref{lem:smaller},
, therefore 

Let

and by induction, , , and .  Finally, observe that
,
, and
, so the case holds.
\end{itemize}
\end{proof}
We can now establish the correspondence between analysis and
evaluation.

\begin{corollary}\label{cor:normal}
If  is the simple closure analysis of a linear program
, then  where
 and .
\end{corollary}

By a simple replaying of the proof substituting the containment
constraints of 0CFA for the equality constraints of simple closure
analysis, it is clear that the same correspondence can be established,
and therefore 0CFA and simple closure analysis are identical for
linear programs.

\begin{corollary}
If  is a linear program, then  is the simple closure
analysis of  iff  is the 0CFA of .
\end{corollary}

\paragraph{Discussion:}

Returning to our earlier question of the computationally potent
ingredients in a static analysis, we can now see that when the term is
linear, whether flows are directional and bidirectional is irrelevant.
For these terms, simple closure analysis, 0CFA, and evaluation are
equivalent.  And, as we will see, when an analysis is {\em exact} for
linear terms, the analysis will have a \ptime-hardness bound.



\section{Lower Bounds for Flow Analysis}
\label{sec:circuits}


There are at least two fundamental ways to reduce the complexity of
analysis.  One is to compute more approximate answers, the other is to
analyze a syntactically restricted language.

We use {\em linearity} as the key ingredient in proving lower bounds
on analysis.  This shows not only that simple closure analysis and
other flow analyses are \ptime-complete, but the result is rather
robust in the face of analysis design based on syntactic restrictions.
This is because we are able to prove the lower bound via a highly
restricted programming language---the linear -calculus.  So
long as the subject language of an analysis includes the linear
-calculus, and is exact for this subset, the analysis must be
at least \ptime-hard.

The decision problem answered by flow analysis, described in
\autoref{chap:foundations}, is formulated for monovariant analyses as
follows:
\begin{description}
\item[Flow Analysis Problem:] Given a closed expression , a term
, and label , is  in the analysis of ?
\end{description}

\begin{theorem}
If analysis corresponds to evaluation on linear terms, it is
\ptime-hard.
\end{theorem}
The proof is by reduction from the canonical \ptime-complete problem
of circuit evaluation \cite{ladner-75}:
\begin{description}
\item[Circuit Value Problem:] Given a Boolean circuit  of 
inputs and one output, and truth values , is
 accepted by ?
\end{description}

An instance of the circuit value problem can be compiled, using only
logarithmic space, into an instance of the flow analysis problem.  The
circuit and its inputs are compiled into a linear -term,
which simulates  on  via {\em evaluation}---it normalizes
to true if  accepts  and false otherwise.  But since the
analysis faithfully captures evaluation of linear terms, and our
encoding is linear, the circuit can be simulated by flow analysis.

The encodings work like this: \TT\ is the identity on pairs, and \FF\
is the swap.  Boolean values are either  or
, where the first component is the ``real''
value, and the second component is the complement.  


The simplest connective is \Not, which is an inversion on pairs, like
\FF.  A {\em linear} copy connective is defined as:

The coding is easily explained: suppose  is \True, then  is
identity and  twists; so we get the pair
.  Suppose  is \False, then  twists
and  is identity; we get .  We write
 to mean -ary fan-out---a straightforward extension of the
above.

The \AND\ connective is defined as follows:

Conjunction works by computing pairs  and
.  The former is the usual conjunction on the
first components of the Booleans :  can be read as ``if  then , otherwise false
(\FF).''  The latter is (exploiting De Morgan duality) the disjunction
of the complement components of the Booleans:
 is read as ``if  (i.e.~if not )
then true (\TT), otherwise  (i.e.~not ).''  The result of
the computation is equal to , but this leaves
 unused, which would violate linearity.  However, there is
symmetry to this {\em garbage}, which allows for its disposal.  Notice
that, while we do not know whether  is \TT\ or \FF\ and similarly
for , we do know that {\em one of them is \TT\ while the other is
  \FF}.  Composing the two together, we are guaranteed that .  Composing this again with another twist (\FF) results in
the identity function .  Finally,
composing this with  is just equal to , so ,
which is the desired result, but the symmetric garbage has been {\em
  annihilated}, maintaining linearity.

Similarly, we define truth-table implication:

Let us work through the construction once more: Notice that if 
is \True, then  is \TT, so  is \TT\ iff  is \True.  And
if  is \True, then  is \FF, so  is \FF\ iff  is
\False.  On the other hand, if  is \False,  is \FF, so 
is \TT, and  is \TT, so  is \FF.  Therefore  is \True\ iff , and \False\
otherwise. Or, if you prefer,  can be read
as ``if , then  else ''---the if-then-else description
of the implication  ---and  as its De Morgan dual .  Thus
 is the answer we want---and we need only
dispense with the ``garbage''  and .  De Morgan duality
ensures that one is , and the other is  (though we do not
know which), so they always compose to .

However, simply returning  violates linearity since
 go unused.  We know that  iff  and
 iff .  We do not know which is which, but
clearly .  Composing  with , we are guaranteed to get .  Therefore , and we have used all bound
variables exactly once.

This hacking, with its self-annihilating garbage, is an improvement
over that given by \citet{mairson-jfp04} and allows Boolean computation
without K-redexes, making the lower bound stronger, but also
preserving all flows.  In addition, it is the best way to do circuit
computation in multiplicative linear logic, and is how you compute
similarly in non-affine typed -calculus \cite{mairson-geocal06}.

By writing continuation-passing style variants of the logic gates, we
can encode circuits that look like straight-line code.  For example,
define CPS logic gates as follows:


Continuation-passing style code such as  can be read colloquially as a kind of low-level, straight-line
assembly language: ``compute the  of registers  and ,
write the result into register , and goto .''

\begin{figure} 
  \centering 
  \includegraphics{figure.18}
  \caption{An example circuit.}
  \label{fig-circuit}
\end{figure}

An example circuit is given in \autoref{fig-circuit}, which can be
encoded as:

Notice that each variable in this CPS encoding corresponds to a wire
in the circuit.

The above code says:
\begin{itemize} 
\item compute the \AND\ of  and , putting the result in
  register ,
\item compute the \AND\ of  and , putting the result in
  register ,
\item compute the \AND\ of  and , putting the result in
  register ,
\item make two copies of register , putting the values in registers
   and ,
\item compute the \Or\ of  and , putting the result in
  register ,
\item compute the \Or\ of  and , putting the result in
  register ,
\item compute the \Or\ of  and , putting the result in
  the  (``output'') register.
\end{itemize}

We know from corollary~\ref{cor:normal} that evaluation and analysis
of linear programs are synonymous, and our encoding of circuits will
faithfully simulate a given circuit on its inputs, evaluating to true
iff the circuit accepts its inputs.  But it does not immediately
follow that the circuit value problem can be reduced to the flow
analysis problem.  Let  be the encoding of the circuit
and its inputs.  It is tempting to think the instance of the flow
analysis problem could be stated:
\begin{center}
is \True\ in  in the analysis
of ?
\end{center}
The problem with this is there may be many syntactic instances of
``\True.''  Since the flow analysis problem must ask about a particular
one, this reduction will not work.  The fix is to use a context which
expects a Boolean expression and induces a particular flow (that can
be asked about in the flow analysis problem) iff that expression
evaluates to a true value.

We use The Widget to this effect.  It is a term expecting a Boolean
value.  It evaluates as though it were the identity function on
Booleans, , but it induces a specific flow we can ask
about. If a true value flows out of , then  flows out of
.  If a false value flows out of , then 
flows out of , where  and  are
distinguished terms, and the only possible terms that can flow out.
We usually drop the subscripts and say ``does \True\ flow out of
?''  without much ado.




Because the circuit value problem is complete for \ptime, we
conclude:

\begin{theorem}
The control flow problem for 0CFA is complete for \ptime.
\end{theorem}

\begin{corollary}
The control flow problem for simple closure analysis is complete for \ptime.
\end{corollary}

\section{Other Monovariant Analyses}

In this section, we survey some of the existing monovariant analyses
that either approximate or restrict 0CFA to obtain faster analysis
times.  In each case, we sketch why these analyses are complete for
\ptime.

\citet{shivers-sigplan04} noted in his retrospective on control flow
analysis that ``in the ensuing years [since 1988], researchers have
expended a great deal of effort deriving clever ways to tame the cost
of the analysis.''  Such an effort prompts a fundamental question: to
what extent is this possible?

Algorithms to compute 0CFA were long believed to be at least cubic in
the size of the program, proving impractical for the analysis of large
programs, and \citet{heintze-mcallester-lics97} provided strong
evidence to suggest that in general, this could not be improved.  They
reduced the problem of computing 0CFA to that of deciding two-way
nondeterministic push-down automata acceptance (2NPDA); a problem
whose best known algorithm was cubic and had remained so since its
discovery \cite{aho-hopcroft-ullman-ic68}---or so it was believed; see
\autoref{sec-2npda} for a discussion.

In the face of this likely insurmountable bottleneck, researchers
derived ways of further approximating 0CFA, thereby giving up
information in the service of quickly computing a necessarily less
precise analysis in order to avoid the ``cubic bottleneck.''

Such further approximations enjoy linear or near linear algorithms and
have become widely used for the analysis of large programs where the
more precise 0CFA would be to expensive to compute.  But it is natural
to wonder if the algorithms for these simpler analyses could be
improved.  Owing to 0CFA's \ptime-lower bound, its algorithms are
unlikely to be effectively parallelized or made memory efficient.  But
what about these other analyses?

\subsection{Ashley and Dybvig's Sub-0CFA}
\label{sec:sub0}

\citet{ashley-dybvig-toplas98} developed a general framework for
specifying and computing flow analyses; instantiations of the
framework include 0CFA and the polynomial 1CFA of
\citet{jagannathan-weeks-popl95}, for example.  They also developed a
class of instantiations, dubbed {\em sub-0CFA}, that are faster to
compute, but less accurate than 0CFA.

This analysis works by explicitly bounding the number of times the
cache can be updated for any given program point.  After this
threshold has been crossed, the cache is updated with a distinguished
 value that represents all possible -abstractions
in the program.  Bounding the number of updates to the cache for any
given location effectively bounds the number of passes over the
program an analyzer must make, producing an analysis that is  in
the size of the program.  Empirically, Ashley and Dybvig observe that
setting the bound to 1 yields an inexpensive analysis with no
significant difference in enabling optimizations with respect to 0CFA.


The idea is the cache gets updated once ( times in general) before
giving up and saying all -abstractions flow out of this
point.  But for a linear term, the cache is only updated at most once
for each program point.  Thus we conclude even when the sub-0CFA bound
is 1, the problem is \ptime-complete.

As Ashley and Dybvig note, for any given program, there exists an
analysis in the sub-0CFA class that is identical to 0CFA (namely by
setting  to the number of passes 0CFA makes over the given
program).  We can further clarify this relationship by noting that for
all linear programs, all analyses in the sub-0CFA class are identical
to 0CFA (and thus simple closure analysis).

\subsection{Subtransitive 0CFA}

\citet{heintze-mcallester-lics97} have shown the ``cubic bottleneck''
of computing full 0CFA---that is, computing all the flows in a
program---cannot be avoided in general without combinatorial
breakthroughs: the problem is {\sc{2npda}}-hard, for which the ``the
cubic time decision procedure [\dots] has not been improved since its
discovery in 1968.''

Forty years later, that decision procedure was improved to be slightly
subcubic by \citet{chaudhuri-popl08}.  However, given the strong
evidence at the time that the situation was unlikely to improve in
general, \citet{heintze-mcallester-pldi97} identified several simpler
flow questions\footnote{Including the decision problem discussed in
  this dissertation, which is the simplest; answers to any of the
  other questions imply an answer to this problem} and designed
algorithms to answer them for simply-typed programs. Under certain
typing conditions, namely that the type is within a bounded size,
these algorithms compute in less than cubic time.

The algorithm constructs a graph structure and runs in time linear in a
program's graph.  The graph, in turn, is bounded by the size of the
program's type.  Thus, bounding the size of a program's type results
in a linear bound on the running times of these algorithms.  

If this type bound is removed, though, it is clear that even these
simplified flow problems (and their bidirectional-flow analogs), are
complete for \ptime: observe that every linear term is simply typable,
however in our lower bound construction, the type size is proportional
to the size of the circuit being simulated.  As they point out, when
type size is not bounded, the flow graph may be exponentially larger
than the program, in which case the standard cubic algorithm is
preferred.

Independently, \citet{mossin-njc98} developed a type-based analysis
that, under the assumption of a constant bound on the size of a
program's type, can answer restricted flow questions such as single
source/use in linear time with respect to the size of the explicitly
typed program.  But again, removing this imposed bound results in
\ptime-completeness.

As \citet{hankin-games} point out: both Heintze and McAllester's and
Mossin's algorithms operate on type structure (or structure isomorphic
to type structure), but with either implicit or explicit
-expansion.  For simply-typed terms, this can result in an
exponential blow-up in type size.  It is not surprising then, that
given a much richer graph structure, the analysis can be computed
quickly.  

In this light, the results of \autoref{chap:linear-logic} on 0CFA of
-expanded, simply-typed programs can be seen as an improvement
of the subtransitive flow analysis since it works equally well for
languages with first-class control and can be performed with only a
fixed number of pointers into the program structure, i.e.~it is
computable in \logspace\ (and in other words, \ptime\  \logspace\
up to ).










\section{Conclusions}

When an analysis is {\em exact}, it will be possible to establish a
correspondence with evaluation.  The richer the language for which
analysis is exact, the harder it will be to compute the analysis.  As
an example in the extreme, \citet{mossin-sas97} developed a flow
analysis that is exact for simply-typed terms.  The computational
resources that may be expended to compute this analysis are {\em ipso
facto} not bounded by any elementary recursive function
\cite{statman79}.  However, most flow analyses do not approach this kind
of expressivity.  By way of comparison, 0CFA only captures \ptime, and
yet researchers have still expending a great deal of effort deriving
approximations to 0CFA that are faster to compute.  But as we have
shown for a number of them, they all coincide on linear terms, and so
they too capture \ptime.

We should be clear about what is being said, and not said.  There is a
considerable difference in practice between linear algorithms
(nominally considered efficient) and cubic---or near
cubic---algorithms (still feasible, but taxing for large inputs), even
though both are polynomial-time.  \ptime-completeness does not
distinguish the two.  But if a sub-polynomial (e.g., \logspace)
algorithm was found for this sort of flow analysis, it would depend on
(or lead to) things we do not know (\logspace\  \ptime).  

Likewise, were a parallel implementation of this flow analysis to run
in logarithmic time (i.e., \nc), we would consequently be able to
parallelize every polynomial time algorithm.  \ptime-complete problems
are considered to be the least likely to be in \nc.  This is because
logarithmic-space reductions (such as our compiler from circuits to
-terms) preserve parallel complexity, and so by composing
this reduction with a (hypothetical) logarithmic-time 0CFA analyzer
(or equivalently, a logarithmic-time linear -calculus
evaluator) would yield a fast parallel algorithm for {\em all}
problems in \ptime, which are by definition, logspace-reducible to the
circuit value problem \cite[page 377]{Papadimitriou94}.

The practical consequences of the \ptime-hardness result is that we
can conclude any analysis which is exact for linear programs, which
includes 0CFA, and many further approximations, does not have a fast
parallel algorithm unless \ptime\  \nc.
