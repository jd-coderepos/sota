

\documentclass[runningheads,a4paper]{llncs}

\usepackage{amsfonts,amsmath,amssymb}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{breqn}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{enumitem}

\usepackage{url}
\urldef{\mailsa}\path|{fan, osimpson}@ucsd.edu|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\def\polylog{\operatorname{polylog}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}

\newcommand{\vol}{\textmd{vol}}
\newcommand{\supp}{\textmd{supp}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\pr}{\textsf{pr}}
\newcommand{\localcheeg}{\Phi^*(S)}

\newcommand{\hkprseedalg}{\texttt{ApproxHKPRseed}}
\newcommand{\hkprseedalgparams}{\texttt{ApproxHKPRseed()}}
\newcommand{\rparam}{\frac{16}{\epsilon^3}\log n}
\newcommand{\kparam}{\frac{\log(\epsilon^{-1})}{\log\log(\epsilon^{-1})}}
\newcommand{\hkprcomplexity}{O\big(\frac{\log(\epsilon^{-1})\log n}{\epsilon^3\log\log(\epsilon^{-1})}\big)}
\newcommand{\partitionalg}{\texttt{ClusterHKPR}}
\newcommand{\partitionalgparams}{\texttt{ClusterHKPR()}}
\newcommand{\tparamcluster}{\phi^{-1}\log(\frac{2\sqrt{\varsigma}}{1-\epsilon} + 2\epsilon s)}


\begin{document}
\mainmatter

\title{Computing Heat Kernel Pagerank and a Local Clustering
Algorithm\footnote{An extended abstract appeared in~\cite{chung2014computing}.}}
\titlerunning{Computing Heat Kernel Pagerank}

\author{Fan Chung
\and Olivia Simpson}
\authorrunning{Chung and Simpson}

\institute{University of California, San Diego\\
La Jolla, CA 92093\\
\mailsa}

\toctitle{Computing Heat Kernel Pagerank}
\tocauthor{Chung and Simpson}
\maketitle

\begin{abstract}
Heat kernel pagerank is a variation of Personalized PageRank given in an
exponential formulation.  In this work, we present a sublinear time algorithm
for approximating the heat kernel pagerank of a graph.  The algorithm works by
simulating random walks of bounded length and runs in time ,
assuming performing a random walk step and sampling from a distribution with
bounded support take constant time.

The quantitative ranking of vertices obtained with heat kernel pagerank can be
used for local clustering algorithms.  We present an efficient local clustering
algorithm that finds cuts by performing a sweep over a heat kernel pagerank
vector, using the heat kernel pagerank approximation algorithm as a subroutine.
Specifically, we show that for a subset  of Cheeger ratio , many
vertices in  may serve as seeds for a heat kernel pagerank vector which will
find a cut of conductance .
\end{abstract}
\keywords{Heat kernel pagerank, heat kernel, local algorithms}

\section{Introduction}
\label{sec:introduction}
In large networks, many similar elements can be identified to a single, larger
entity by the process of clustering.  Increasing granularity in massive networks
through clustering eases operations on the network.  There is a large literature
on the problem of identifying clusters in a graph
(\cite{csz:spectralkway:94,shi2000normalized,njw:spectralcluster:02,kvv:clusterings:04,lc:powercluster:10,lc:textcluster:10}),
and the problem has found many applications.  However, in a variation of the
graph clustering problem we may only be interested in a single cluster near one
element in the graph.  For this, local clustering algorithms are of greater use.

As an example, the problem of finding a local cluster arises in protein
networks.  A protein-protein interaction (PPI) network has undirected edges that
represent an interaction between two proteins.  Given two PPI networks, the goal
of the pairwise alignment problem is to identify an optimal mapping between the
networks that best represents a conserved biological function.
In~\cite{liao:protein:09}, a local clustering algorithm is applied from a
specified protein to identify a group similar to that protein.  Such local
alignments are useful for analysis of a particular component of a biological
system (rather than at a systems level which will call for a global alignment).
Local clustering is also a common tool for identifying communities in a network.
A community is loosely defined as a subset of vertices in a graph which are more
strongly connected internally than to vertices outside the subset.  Properties
of community structure in large, real world networks have been studied
in~\cite{lldm:localnetwork:08}, for example, where local clustering algorithms
are employed for identifying communities of varying quality.

The goal of a local clustering algorithm is to identify a cluster in a graph
near a specified vertex.  Using only local structure avoids unnecessary
computation over the entire graph.  An important consequence of this are running
times which are often in terms of the size of the small side of the partition,
rather than of the entire graph.  The best performing local clustering
algorithms use probability diffusion processes over the graph to determine
clusters (see Section~\ref{sec:previouswork}).  In this paper we present a new
algorithm which identifies a cut near a specified vertex with simple
computations over a heat kernel pagerank vector.

The theory behind using heat kernel pagerank for computing local clusters has
been considered in previous work.  Here we give an efficient approximation
algorithm for computing heat kernel pagerank.  Note that we use a ``relaxed''
notion of approximation which allows us to derive a sublinear probabilistic
approximation algorithm for heat kernel pagerank, while computing an exact or
sharp approximation would require computation complexity of order similar to
matrix multiplication.  We use this sublinear approximation algorithm for
efficient local clustering.

\subsection{Previous work}\label{sec:previouswork}
\paragraph{Heat kernel and approximation of matrix exponentials.} Heat kernel
pagerank was first introduced in~\cite{chung:hkpr:07} as a variant of
personalized PageRank~\cite{haveliwala2002topic}.  While PageRank can be
viewed as a geometric sum of random walks, the heat kernel pagerank is an
exponential sum of random walks.  An alternative interpretation of the heat
kernel pagerank is related to the heat kernel of a graph as the fundamental
solution to the heat equation.  As such, it has connections with diffusion and
mixing properties of graphs and has been incorporated into a number of graph
algorithmic primitives.

Orecchia et al. use a variant of heat kernel random walks in their randomized
algorithm for computing a cut in a graph with prescribed balance
constraints~\cite{osv:balsep:11}.  A key subroutine in the algorithm is a
procedure for computing  for a positive semidefinite matrix  and a
unit vector  in time  for graphs on  vertices and  edges.
They show how this can be done with a small number of computations of the form
 and applying the Spielman-Teng linear
solver~\cite{st:graphpartitioning:stoc04}.  Their main result is a randomized
algorithm that outputs a balanced cut in time .  In a follow up
paper, Sachdeva and Vishnoi~\cite{sachdeva2013matrix} reduce inversion of
positive semidefinite matrices to matrix exponentiation, thus proving that
matrix exponentiation and matrix inversion are equivalent to polylog factors.
In particular, the nearly-linear running time of the balanced separator
algorithm depends upon the nearly-linear time Spielman-Teng solver.

Another method for approximating matrix exponentials is given by Kloster and
Gleich in~\cite{kloster:columnexp:waw13}.  They use a Gauss-Southwell iteration
to approximate the Taylor series expansion of the column vector  for
transition probability matrix  and  a standard basis vector.  The
algorithm runs in sublinear time assuming the maximum degree of the network is
.

\paragraph{Local clustering.} Local clustering algorithms were introduced
in~\cite{st:graphpartitioning:stoc04}, where Spielman and Teng present a
nearly-linear time algorithm for finding local partitions with certain balance
constraints.  Let  denote the cut ratio of a subset  that we will
later define as the Cheeger ratio.  Then, given a graph and a subset of vertices
 such that  and , their algorithm
finds a set of vertices  such that  and  in time .  This seminal
work incorporates the ideas of Lov\'asz and
Simonovitz~\cite{ls:mixingisoperimetric:90,ls:randomwalks:93} on isoperimetric
properties of random walks, and their algorithm works by simulating truncated
random walks on the graph.  Spielman and Teng later improve their approximation
guarantee to  in a revised version of the
paper~\cite{st:localcluster:08}.

The algorithm of~\cite{st:graphpartitioning:stoc04,st:localcluster:08} improves
the spectral methods of~\cite{donath1972algorithms} and a similar expression
in~\cite{aloniso85} which use an eigenvector of the graph Laplacian to partition
the vertices of a graph.  However, the local approach of Spielman and Teng
allows us to identify focused clusters without investigating the entire graph.
For this reason, the running time of this and similar local algorithms are
proportional to the size of the small side of the cut, rather than the entire
graph.

Andersen et al.~\cite{acl:prgraphpartition:focs06} give an improved local
clustering algorithm using approximate PageRank vectors.  For a vertex subset
 with Cheeger ratio  and volume , they show that a PageRank vector
can be used to find a set with Cheeger ratio .  Their
local clustering algorithm runs in time .  The analysis
of the above process was strengthened in~\cite{ac:sharpdrops:07} and emphasized
that vertices with higher PageRank values will be on the same side of the cut as
the starting vertex.

Andersen and Peres~\cite{ap:evolving:09} later simulate a volume-biased evolving
set process to find sparse cuts.  Although their approximation guarantee is the
same as that of~\cite{acl:prgraphpartition:focs06}, their process yields a
better ratio between the computational complexity of the algorithm on a given
run and the volume of the output set.  They call this value the
\emph{work/volume ratio}, and their evolving set algorithm achieves an expected
ratio of .  This result is improved by Gharan and
Trevisan in~\cite{gt:optimalcluster:12} with an algorithm that finds a set of
conductance at most  and achieves a work/volume
ratio of  for target volume
 and target conductance .  The complexity of their algorithm is
achieved by running copies of an evolving set process in parallel.

\subsection{Our contributions}
In this paper, we give a probabilistic approximation algorithm for computing a
vector that yields a ranking of vertices close to the heat kernel pagerank
vector.  The approximation algorithm, \hkprseedalg, works by simulating random
walks and computing contributions of these walks for each vertex in the graph.
Assuming access to a constant-time query which returns the destination of a heat
kernel random walk starting from a specified vertex, \hkprseedalg~runs in time
. In the context of this paper, we strictly address heat kernel
pagerank with a single vertex as a seed -- an analogy to Personalized PageRank
with total preference given to a single vertex.  Note that heat kernel pagerank
with a general preference vector (see Section~\ref{sec:preliminaries}) is a
combination of heat kernel pagerank with a single seed vertex.  We refer the
reader to~\cite{cs:imlinear:14} for this more general case.

Using \hkprseedalg~as a subroutine, we then present a local clustering algorithm
that uses a ranking according to an approximate heat kernel pagerank.  Let 
be a graph and  a proper vertex subset with volume 
and Cheeger ratio .  Then, with probability at least
, our algorithm outputs either a cutset  with  and -local Cheeger ratio at most  or a
certificate that no such set exists.  The algorithm has work/volume ratio of
.  This result is formalized in
Theorem~\ref{thm:localpart}.  A summary of previous results and our
contributions are given in Table~\ref{table:resultssummary}.

\begin{table}
\centering
\begin{tabular}{c|c|c}\hline
Algorithm & Conductance of output set & Work/volume ratio\\\hline
\cite{st:localcluster:08} &  & \\
\cite{acl:prgraphpartition:focs06} &  & \\
\cite{ap:evolving:09} &  & \\
\cite{gt:optimalcluster:12} &  & \\
This work &  & \\\hline
\end{tabular}
\caption{Summary of local clustering algorithms}
\label{table:resultssummary}
\end{table}

As a summary of the contributions of this work,
\begin{enumerate}[label={(\arabic*)}]
\item We present an algorithm for computing a heat kernel pagerank vector from a
single seed vertex with  approximation guarantee with high
probability in time .\label{point:hkpralg} 
\item We present a local clustering algorithm which uses a ranking according
to heat kernel pagerank.  In our clustering algorithm we use the probabilistic
approximation algorithm in \ref{point:hkpralg} as a subroutine, which gives a
sublinear-time local clustering algorithm.\label{point:clusteralg} 
\item Using the approximation guarantees of \ref{point:hkpralg} and the analysis
for \ref{point:clusteralg}, we present a local clustering algorithm which with
high probability returns a set with Cheeger ratio at most ,
given a target ratio , with work/volume ratio

where  is proportional to the volume of the output set.
\item We validate the performance analysis by implementing our algorithms using
several real and synthetic graphs as examples.  The clusters that were derived
in these examples using the local clustering algorithm and heat kernel pagerank
approximation have Cheeger ratios as guaranteed in Theorem~\ref{thm:localpart}.
\end{enumerate}

The theory behind finding local cuts with heat kernel pagerank vectors was first
presented in~\cite{chung:hkpr:07,chung:partitionhkpr:im09}.  Using some of this
analysis as a starting point, we provide the algorithm for computing local
clusters, called \partitionalg.

\subsection{Organization}
The remainder of the paper is organized as follows.  First, we give some
definitions and useful facts in Section~\ref{sec:preliminaries}.  We give a
sublinear-time algorithm for approximating heat kernel pagerank in
Section~\ref{sec:hkprapprox}.  In Section~\ref{sec:goodcuts} we give the
analysis justifying our local clustering algorithm, which we present in
Section~\ref{sec:localpartition}.  Sections~\ref{sec:rankings}
and~\ref{sec:expresults} contain experimental results.  In both sections,
experiments are performed on real data and on synthetic graphs generated with
random graph generators.  In Section~\ref{sec:rankings} we demonstrate how the
rankings obtained using approximate heat kernel pagerank vectors are compared
with rankings obtained using exact heat kernel pagerank vectors.  In
Section~\ref{sec:expresults} we compute local clusters by implementing the
\partitionalg~algorithm.  We compare the volume and Cheeger ratio of these
clusters to those output by two existing sweep-based local clustering
algorithms.  The first is by a sweep of an exact heat kernel
pagerank~\cite{chung:partitionhkpr:im09} to compare the effects of heat kernel
pagerank computation, and the second by a PageRank
vector~\cite{acl:prgraphpartition:focs06}.  PageRank has a similar expression as
heat kernel pagerank except PageRank is a geometric sum whereas heat kernel
pagerank can be viewed as an exponential sum.  We expect better convergence
rates from heat kernel (see Section~\ref{sec:heatkernelandhkpr}).

\section{Preliminaries}
\label{sec:preliminaries}
Let  be an undirected graph on  vertices and  edges.  We use  to denote .  The \emph{degree}, , of a vertex  is
the number of vertices  such that .  The \emph{volume} of a set of
vertices  is the total degree of its vertices, , and the \emph{edge boundary} of  is the set of edges with one
vertex in  and the other outside of , . When discussing the full vertex set, , we write
 and .

Let  be a row vector over the vertices of .  Then the support of
 is the set of vertices with nonzero values in , .  For a subset of vertices , we define .

\subsection{A local Cheeger inequality}
The quality of a cut can be measured by the ratio of the number of edges between
the two parts of the cut and the volume of the smaller side of the cut.  This is
called the \emph{Cheeger ratio} of a set, defined by

The \emph{Cheeger constant} of a graph is the minimal Cheeger ratio,

Finally, for a given subset  of a graph , the \emph{local Cheeger ratio}
is defined


Our local clustering algorithm is derived from a local version of the usual
Cheeger inequalities which relate the Cheeger constant of a graph to an
eigenvalue associated to the graph.  Namely, let the normalized Laplacian of a
graph be the matrix , where  is the diagonal
matrix of vertex degrees and  is the unweighted, symmetric adjacency matrix.
Also, let  be determined by a subset  of size  and define  where  and  are the restricted
matrices of  and  with rows and columns indexed by vertices in .  Then
the eigenvalues  of  are also known as the \emph{Dirichlet eigenvalues of
}, and are related to  by the following local Cheeger
inequality~\cite{chung:partitionhkpr:im09}:


The inequality (\ref{eq:cheegerinequality}) will be used to derive a
relationship between a ranking according to heat kernel pagerank and sets with
good Cheeger ratios.  Details will be given in Section~\ref{sec:goodcuts}.

\subsection{Heat kernel and heat kernel pagerank}
\label{sec:heatkernelandhkpr}
The \emph{heat kernel pagerank} vector has entries indexed by the vertices of
the graph and involves two parameters; a non-negative real value ,
representing the temperature, and a preference row vector ,
by the following equation:

where  is the transition probability matrix


When  is a probability distribution, the heat kernel pagerank can be regarded
as the expected distribution of a random walk according to the transition
probability matrix .  A starting distribution we will be particularly
concerned with is that with all probability initially on a single vertex ,
i.e.  where  is the indicator vector for vertex .  We
will denote the heat kernel pagerank vector over this distribution by  and refer to  as the \emph{seed} vertex.

The \emph{heat kernel} of a graph is defined  where 
is the Laplace operator .  Then an alternative definition for
heat kernel pagerank is , and we have that heat kernel pagerank
satisfies the heat differential equation


We can compare the heat kernel pagerank to the personalized PageRank vector,
given by

In this definition,  is often called the \emph{jumping} or \emph{reset}
constant, meaning that at any step the random walk may jump to a vertex taken
from  with probability .  When  for some , i.e.
preference is given to a single vertex, the random walk is ``reset" to the first
vertex of the walk, , with probability .  We note that, compared to
the personalized PageRank vector, which can be viewed as a geometric sum, we can
expect better convergence rates from the heat kernel pagerank, defined as an
exponential sum.

\section{Heat Kernel Pagerank Approximation}
\label{sec:hkprapprox}
We begin our discussion of heat kernel pagerank approximation with an
observation.  Each term in the infinite series defining heat kernel pagerank in
 is of the form  for
.  The vector  is the distribution after  random walk
steps with starting distribution .  Then, if we perform  steps of a random
walk given by transition probability matrix  from starting distribution 
with probability , the heat kernel pagerank vector
can be viewed as the expected distribution of this process.

This suggests a natural way to approximate the heat kernel pagerank.  That is,
we can obtain a close approximation to the expected distribution with
sufficiently many samples.  Our algorithm operates as follows.  We perform 
random walks to approximate the infinite sum, choosing  large enough to bound
the error.  We also use the fact that very long walks are performed with small
probability.  As such, we limit the lengths of our random walks by a finite
number .  Both  depend on a predetermined error bound .

In our analysis we will use the following definition of an
-approximate vector.

\begin{definition}
\label{def:eps-approx}
Let  be a graph on  vertices, and let  be a vector over
the vertices of .  Let  be the heat kernel pagerank vector according
to  and .  Then we say that  is an
\emph{-approximate vector} of  if
\begin{enumerate}
\item for every vertex  in the support of , 

\item for every vertex with , it must be that .
\end{enumerate}
\end{definition}

We note that this is a rather coarse requirement for an approximation, but
satisfies our needs for local clustering.  In the following algorithm, we
approximate  by an -approximate vector which we denote by
.  The running time of the algorithm is .  The
method and complexity of the algorithm, , are similar to the
ApproxRow algorithm for personalized PageRank given
in~\cite{bbct:sublinearpr:waw12}.

\begin{algorithm}[H]
\floatname{algorithm}{}
\caption*{\hkprseedalgparams}
\label{alg:hkprseed}
\algblock[Name]{Start}{End}
input: a graph , , seed vertex , error parameter .\\
output: , an -approximation of .\\
\begin{algorithmic}
  \State initialize a -vector  of dimension , where 
  \State 
  \State  for some choice of contant \\
  \For { iterations}
    \Start
      \State simulate a  random walk from vertex  where  steps are taken with probability  and 
      \State let  be the last vertex visited in the walk
      \State 
    \End
  \EndFor\\
  \State
  \Return   
\end{algorithmic} 
\end{algorithm}

\begin{theorem}\label{thm:hkpraccuracy}
Let  be a graph and let  be a vertex of .  Then, the algorithm
\hkprseedalgparams outputs an -approximate vector  of the
heat kernel pagerank  for  with probability at least
.  The running time of \hkprseedalg~is .
\end{theorem}

\subsection{Analysis of the heat kernel pagerank algorithm}
\label{sec:hkpranalysis}
Our analysis relies on the usual Chernoff bounds as stated below.

\begin{lemma}[\cite{bbct:sublinearpr:waw12}]\label{lem:chernoff}
Let  be independent Bernoulli random variables with .  Then, 
\begin{enumerate}
\item for , 
\item for , 
\item for , .
\end{enumerate}
\end{lemma}

\begin{proof}[Theorem~\ref{thm:hkpraccuracy}]
Consider the random variable which takes on value  with probability  for .  The expectation of this random
variable is exactly .  Heat kernel pagerank can be understood as a series
of distributions of weighted random walks over the vertices, and the weights are
related to the number of steps taken in the walk.  The series can be computed by
simulating this process, i.e., draw  according to  and compute 
with sufficiently many random walks of length .

We approximate the infinite sum by limiting the walks to at most  steps.  We
will take  to be .  These interrupts risk the loss of
contribution to the expected value, but can be upper bounded by
 provided that .
This is within the error bound for an approximate heat kernel pagerank.  If , the expected length of the random walk is

Thus we can ignore walks of length more than  while maintaining  for every vertex .

Next we show how many samples are necessary for our approximation vectors.  For
, our algorithm simulates  random walk steps with probability
.  To be specific, for a fixed , let  be the
indicator random variable defined by  if a random walk beginning from
vertex  ends at vertex  in  steps.  Let  be the random variable
that considers the random walk process ending at vertex  in \emph{at most}
 steps.  That is,  assumes the vector  with probability
.  Namely, we consider the combined random walk


Now, let  be the contribution to the heat kernel pagerank vector
 of walks of length at most .  The expectation of each  is
.  Then, by Lemma~\ref{lem:chernoff},

for every component with , since then .  Similarly,

We conclude the analysis for the support of  by noting that , and we achieve an -multiplicative error bound for
every vertex  with  with probability at least
.

On the other hand, if , by the third part of
Lemma~\ref{lem:chernoff}, .  We may conclude that, with high probability, .

For the running time, we use the assumptions that performing a random walk step
and drawing from a distribution with bounded support require constant time.
These are incorporated in the random walk simulation, which dominates the
computation.  Therefore, for each of the  rounds, at most  steps of the
random walk are simulated, giving a total of  queries.
\qed\end{proof}

\begin{remark}
This bound on  is not tight.  However, it is enough to use  for some
small constant  to cluster vertices with -approximate heat kernel
pagerank vectors computed with bounded random walks.  Regardless, this value
 is independent of the size of the graph and never affects the running
time.  See Section~\ref{sec:rankings} for a futher discussion.
\end{remark}

\begin{remark}
We note that the algorithm works for any , but a good choice of  will be
related to the size of the local cluster  and a desirable convergence rate.
In particular, the constraints put on  are necessary for our local clustering
results, presented in Section~\ref{sec:goodcuts}.
\end{remark}

The algorithm for efficient heat kernel pagerank computation has promise for a
variety of applications.  It has been shown in~\cite{cs:hklinear:13} how to
apply heat kernel pagerank in solving symmetric diagonally dominant linear
systems with a boundary condition, for example.


\section{Finding Good Local Cuts}
\label{sec:goodcuts}
The premise of the local clustering algorithm is to find a good cut near a
specified vertex by performing a \emph{sweep} over a vector associated to that
vertex, which we will specify.  Let  be a probability
distribution vector over the vertices of the graph of support size .  Then, consider a \emph{probability-per-degree} ordering of the
vertices where .  Let  be the set of the first  vertices per the
ordering.  We call each  a \emph{segment}.  Then the process of
investigating the cuts induced by the segments to find an optimal cut is called
performing a sweep over .  

In this section we will show how a sweep over a single heat kernel pagerank
vector finds local cuts.  Specifically, we show that for a subset  with
 and , and for a large number of
vertices , performing a sweep over the vector , where
 is an -approximation of , will find a set with
Cheeger ratio at most .

\begin{remark}
Though all the vertices in the support of the vector are sorted to build
segments, in practice the sweep will be aborted after the volume of the current
segment is larger than the target size.  This is the \emph{locality} of the
algorithm, and ensures that the amount of work performed is proportional to the
volume of the output set.
\end{remark}

The -local Cheeger ratio of a sweep over a vector  is the
minimum Cheeger ratio over segments  with volume .  Let  the -local Cheeger ratio of
cuts over a sweep of  that separates sets of volume between  and
.

We will use the following bounds for heat kernel pagerank in terms of local
Cheeger ratios and sweep cuts to reason that many vertices  can serve as good
seeds for performing a sweep.

\begin{lemma}
\label{thm:volumegoodset}
Let  be a graph and  a subset of vertices of volume .  Then the set of  satisfying

has volume at least .
\end{lemma}

To proof Lemma~\ref{thm:volumegoodset}, we begin with some bounds for heat
kernel pagerank in terms of local Cheeger ratios and sweep cuts.  For a subset
, define  to be the following distribution over the vertices,

Then the expected value of  over  in  is given by:


We will make use of the following result, given here without proof
(see~\cite{chung:partitionhkpr:im09}), which bounds the expected value of
 given by (\ref{eq:ehkpru}) in terms of local Cheeger ratios.

\begin{lemma}[\cite{chung:partitionhkpr:im09}]
\label{lem:lowerbound}
In a graph , and for a subset , the following holds:

\end{lemma}

Next, we use an upper bound on the amount of probability remaining in  after
sufficient mixing.  This is an extension of a theorem given
in~\cite{chung:partitionhkpr:im09}.

\begin{theorem}
\label{thm:eupperbound}
Let  be a graph and  a subset of vertices with volume .  Then,

\end{theorem}

To prove Theorem~\ref{thm:eupperbound}, we define the following for an arbitrary
function  and any integer  with ,

The above definition can be extended to all real values of ,


\begin{claim}\label{claim:f}
Let  be a segment according to a vector  such that
 and  for every .  Then 
\end{claim}

\begin{proof}
We are considering the maximum over a subset of vertex pairs  of size
.  Since we are only adding values over vertex pairs which are edges
in , this maximum is achieved when

\qed\end{proof}

\begin{proof}[Theorem~\ref{thm:eupperbound}]
Let  be the lazy random walk .  Then,


Let , and let  satisfy  and represent a volume of some set .  Then taking cue from the
above inequality, we can associate  to ,  to 
and  to the minimum Cheeger ratio of a set  satisfying , or .  Then using Claim~\ref{claim:f},

Now consider the following differential inequality,

Line (\ref{heatdiff}) follows from (\ref{eq:heateq}), and line (\ref{concave})
follows from the concavity of .

Consider  to be .  Then,

where we use the fact that  for
 in line (\ref{yineq}).  Now, since  and
, 
and in particular, .  Since , we may conclude
that

\qed\end{proof}

Using Lemma~\ref{lem:lowerbound} and Theorem~\ref{thm:eupperbound}, we arrive at
the following useful inequalities.

\begin{corollary}
\label{cor:bounds}
Let  be a graph and  a subset with volume .
Then,

\end{corollary}

We are now prepared to prove Lemma~\ref{thm:volumegoodset}.

\begin{proof}[Lemma~\ref{thm:volumegoodset}]
Let  be the set of seeds .
Then, by (\ref{eq:ehkpru}),


Now we consider the set of vertices not included in ,

Which implies

\qed\end{proof}

We can use Lemma~\ref{thm:volumegoodset} to reason that many vertices 
satisfy the above inequalities, and thus can serve as good seeds for performing
a sweep.


\subsection{A local graph clustering algorithm}
\label{sec:localpartition}
It follows from Lemma~\ref{thm:volumegoodset} that the ranking induced by a
heat kernel pagerank vector with appropriate seed vertex can be used to find a
cut with approximation guarantee  by choosing the appropriate
.  To obtain a sublinear time local clustering algorithm for massive graphs,
we use \hkprseedalg~to efficiently compute an -approximate heat kernel
pagerank vector, , to rank vertices.

The ranking induced by  is not very different from that of a true
vector  in the support of  (for an experimental analysis,
see Section~\ref{sec:rankings}).  Namely, using the bounds of
Lemma~\ref{lem:lowerbound}, we have
 
where .  In particular,

for a set of vertices  of volume at least .

\begin{theorem}\label{thm:main}
Let  be a graph and  a subset with , , and Cheeger ratio .  Let
 be an -approximate of  for some vertex .
Then there is a subset  with  for
which a sweep over  for any vertex  with
\begin{enumerate}
\item and
\item\label{assumption:sweepcheeg}
\end{enumerate}
finds a set with -local Cheeger ratio at most .
\end{theorem}

\begin{proof}
Let  be a vertex in  as described in the theorem statement.  Using the
inequalities (\ref{eq:approxineq}), we can bound the local Cheeger ratio by a
sweep over :

which implies

and by the assumption \ref{assumption:sweepcheeg}, we have


Let .
Then, 

Since  and , it follows that
.  In particular, a sweep over
 finds a cut with Cheeger ratio  as long as  is
contained in .
\qed\end{proof}

We are now prepared to give our algorithm for finding cuts locally with heat
kernel pagerank.  The algorithm takes as input a starting vertex , a desired
volume  for the cut set, and a target Cheeger ratio  for the
cut set.  Then, to find a set achieving a minimum -local Cheeger
ratio, we perform a sweep over an approximate heat kernel pagerank vector with
the starting vertex as a seed.

\begin{algorithm}[H]
\floatname{algorithm}{}
\caption*{\partitionalgparams}
\label{alg:localpart}
input: a graph , a vertex , target cluster size , target cluster volume
, target Cheeger ratio , error parameter
.\\
output: a set  with ,
.\\
\begin{algorithmic}[1]
  \State 
  \State \label{line:hkpr}
  \State sort the vertices of  in the support of  according to the ranking
\label{line:sort}
  \For{}\label{line:sweep}
    \State 
    \If{}\label{line:volume}
      \State output \texttt{NO CUT FOUND}, break
    \ElsIf{ and }\label{line:checks}
      \State output 
    \Else
      \State output \texttt{NO CUT FOUND}
    \EndIf
  \EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}{\label{thm:localpart}}
Let  be a graph which contains a subset  of volume at most 
and Cheeger ratio bounded by .  Further, assume that  is contained in
the set  as defined in Theorem~\ref{thm:main}.  Then
\partitionalgparams outputs a cutset  with -local Cheeger ratio at
most .  The running time is the same as that of~\hkprseedalg.
\end{theorem}

\begin{proof}
Since it is given that  for , and by the assumptions
on  and , Theorem~\ref{thm:main} states that a sweep over the approximate
heat kernel pagerank vector  will find a set with -local
Cheeger ratio at most .  The checks performed in
line~\ref{line:checks} of the algorithm discover such a set.

The computational work reduces to the main procedures of computing the heat
kernel pagerank vector in line~\ref{line:hkpr} and performing a sweep over the
vector in line~\ref{line:sweep}.  Performing a sweep involves sorting the
support of the vector (line~\ref{line:sort}) and calculating the conductance of
segments.  From the guarantees of an -approximate heat kernel pagerank
vector, any vertex with average probability less than  will be
excluded from the support.  Then the volume of a vector  output in
line~\ref{line:hkpr} is , and performing a sweep over
 can be done in  time.  The
algorithm is therefore dominated by the time to compute a heat kernel pagerank
vector, and the total running time is .
\qed\end{proof}


\section{Ranking Vertices with Approximate Heat Kernel Pagerank}
\label{sec:rankings}
The backbone procedure of the local clustering algorithm is the sweep: ranking
the vertices of the graph according to their approximate heat kernel pagerank
values, and then testing the quality of the cluster obtained by adding vertices
one at a time in the order of this ranking.  To this end, in this section we
compare the rankings of vertices obtained using exact heat kernel pagerank
vectors with approximate heat kernel pagerank vectors.  Specifically, we
consider how accuracy changes as the upper bound of random walk lengths, ,
vary.

In the following experiments, we approximate heat kernel pagerank vectors by
sampling random walks of length , where  is chosen with
probability .  We test the values computed with
different values of .  Since the expected value of a random walk length 
chosen with probability  is , we set  to range
from  to approximately  for a specified value of .

In each trial, for a given graph, seed vertex, and value of , we compute an
exact heat kernel pagerank vector  and an approximate heat kernel
pagerank vector  using \hkprseedalg~but limiting the length of
random walks to  for various  as described above.  We then measure how
similar the vectors are in two ways.  First, we compare the vector values
computed.  Second, we compare the rankings obtained with each vector.  The
following are the measures used:

\begin{enumerate}
\item \emph{Comparing vector values.}  We measure the error of the approximate
vector  by examining the values computed for each vertex and
comparing to an exact vector .  We use the following measures:
  \begin{itemize}
  \item \textbf{Average  error}: The average absolute error over all vertices of the graph,
  
  \item \textbf{-error}: The accumulated error in excess of an
-approximation (see Definition~\ref{def:eps-approx}),
  
  \end{itemize}
\item \emph{Comparing vector rankings.}  To measure the similarity of vertex
rankings we use the intersection difference (see
\cite{benzi2013total,fagin2003comparing} among others).  For a ranked list of
vertices , let  be the set of items with the top  rankings.  Then we
use the following measures:
  \begin{itemize}
  \item \textbf{Intersection difference}: Given two ranked lists of vertices,
 and , each of length , the intersection difference is
  
  where  denotes the symmetric difference .
  \item \textbf{Top- intersection difference}: The intersection difference 
among the top  elements in each ranking,
  
  \end{itemize}
Intersection difference values lie in the range , where a difference of
 is achieved for identical rankings, and  for totally disjoint lists.  In
these experiments,  is the list of vertices ranked according to an exact heat
kernel pagerank vector , and  is the list of vertices ranked
according to an -approximate heat kernel pagerank vector .
\end{enumerate}

In every trial we choose  as specified in the local clustering
algorithm stated in Section~\ref{sec:localpartition}.  This value depends on
several parameters, including desired Cheeger ratio, cluster size, and cluster
volume.  Specifics are provided for each set of trials.


\subsection{Synthetic graphs}
\label{sec:synthranking}

\subsubsection{Random graph models}
In this series of trials we use three different models of random graph
generation provided in the NetworkX~\cite{networkx} Python package, which we
describe presently.

The first is the Watts-Strogatz small world model~\cite{wattsstrogatz},
generated with the command \texttt{connected_watts_strogatz} in NetworkX.  In
this model, a ring of  vertices is created and then each vertex is connected
to its  nearest neightbors.  Then, with probability , each edge  in
the original construction is replaced by the edge  for a random existing
vertex .  The model takes parameters  as input.

The second is the preferential attachment (Barab\'{a}si-Albert)
model~\cite{barabasialbert}.  Graphs in this model are created by adding 
vertices one at a time, where each new vertex is adjoined to  edges where
each edge is chosen with probability proportional to the degree of the
neighboring vertex.  This is generated with the \texttt{barabasi_albert_graph}
generator in NetworkX.  The model takes parameters  as input.

The third NetworkX generator is \texttt{powerlaw_cluster_graph}, which uses the
Holme and Kim algorithm for generating graphs with powerlaw degree distribution
and approximate average clustering~\cite{holmekim}.  It is essentially the
Barab\'{a}si-Albert model, but each random edge forms a triangle with another
neighbor with probability .  The model takes parameters  as input.

Table~\ref{table:randomgraphs} lists the random graph models used and their
parameters.

\begin{table}
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{Model} & \multicolumn{1}{|c|}{Source} & \multicolumn{1}{|c|}{Parameters}\\
\hline\hline
small world & Watts-Stragatz\cite{wattsstrogatz} & , the size of the vertex set,\\
            &                & , the number of neighbors each vertex is
assigned,\\
            &                & , the probability of switching an edge.\\
\hline
preferential& Barab\'{a}si-Albert\cite{barabasialbert} & , the size of the
vertex set,\\
attachment  &                                          & , the number of
neighbors each vertex is assigned\\
\hline
powerlaw    & Holme and Kim~\cite{holmekim} & , the size of the vertex set,\\
cluster     &                               & , the number of neighbors each
vertex is assigned,\\
            &                               & , the probability of forming a
triangle\\
\hline
\end{tabular}
\caption{Random graph models used.}
\label{table:randomgraphs}
\end{table}

\subsubsection{Procedure} For every value of  that we test, we generate ten
random graphs using each of the three random graph models.  For each graph we
choose a random seed vertex  with probability proportional to degree, and we
choose  as  according to the values in
Table~\ref{table:synthrankingparams}.  Then for each graph we compare an exact
heat kernel pagerank vector  and the average of two
-approximate heat kernel pagerank vectors .  The results
we present are the average over all trials for each  and each type of graph.
We use  and  in every trial, and  for the first set of trials
(Figure~\ref{fig:random_rank_100}) and  for the second
(Figure~\ref{fig:random_rank_500}).  These parameters are outlined in
Table~\ref{table:synthrankingparams}.

\begin{table}
\centering
\begin{tabular}{|p{2cm}|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Model} &  & ~~~~ & ~~~~ & ~~~~ 
Target & Target & Target & \\
 & & & & & Cheeger ratio & cluster size & cluster volume &\\
\hline\hline
small world  &  &  &  &  &  &  &  & \\
             &  &  &  &  &  &  &
 & \\\hline
preferential &  &  & - &  &  &  &  & \\
attachment   &  &  & - &  &  &  &
 & \\\hline
powerlaw     &  &  &  &  &  &  &  & \\
cluster      &  &  &  &  &  &  &
 & \\
\hline
\end{tabular}
\caption{Parameters used for random graph generation and to compute  for vector computations.}
\label{table:synthrankingparams} 
\end{table}

\subsubsection{Discussion} For each graph and value of , we measure the
-error, the average  error, the intersection difference and the
top- intersection difference of an approximate heat kernel pagerank vector
as compared to an exact heat kernel pagerank vector.
Figure~\ref{fig:random_rank_100} plots the above measures for graphs over
 vertices, while Figure~\ref{fig:random_rank_500} plots these measures
for graphs over  vertices.  In both Figures~\ref{fig:random_rank_100}
and~\ref{fig:random_rank_500}, each subplot charts a different notion of error
(from top left, clockwise: -error, average  error, intersection
difference and top- intersection difference) on the y-axis against  on
the x-axis.  

\begin{figure}
\centering
\textbf{Trials on -vertex random graphs.}
\includegraphics[width=\textwidth]{random_rank_100}
\caption{Different measures of error for random graphs on  vertices when approximating heat
kernel pagerank with varying random walk lengths.}
\label{fig:random_rank_100}
\end{figure}

\begin{figure}
\centering
\textbf{Trials on -vertex random graphs.}
\includegraphics[width=\textwidth]{random_rank_500}
\caption{Different measures of error for random graphs on  when approximating heat
kernel pagerank with varying random walk lengths.}
\label{fig:random_rank_500}
\end{figure}

In both sets of plots and for every measure of error, we see that in the
preferential attachment and powerlaw graphs the error is minimized after
limiting random walks to only length , regardless of the size.  We observe
a shallower decline in -error, average  error, and intsersection
differance for the small world graphs.  In particular, we note that the
intersection difference drops significantly after  random walk steps for all
random graphs on both  and  vertices.  For ,  is enough to approximate the rankings for the
purpose of local clustering.


\subsection{Real graphs}
\label{sec:realranking}

\subsubsection{Network data}
For the experiments in this section, and later in Section~\ref{sec:expresults},
we use the following graphs compiled from real data.  The network data is
summarized in Table~\ref{table:realgraphs}.

\begin{enumerate}
\item \textbf{(dolphins)} A dolphin social network consisting of two
families~\cite{dolphins}.  The seed vertex is chosen to be a prominent member of
one of the families.\label{pt:dolphins}
\item \textbf{(polbooks)} A network of books about US politics published around
the time of the 2004 Presidential election and sold on Amazon~\cite{polbooks}.
Edges represent frequent copurchases.\label{pt:polbooks}
\item \textbf{(power)} The topology of the US Western States Power Grid
~\cite{powergrid}.\label{pt:powergrid}
\item \textbf{(facebook)} A combined collection of Facebook ego-networks,
including the ego vertices themselves~\cite{facebook}.\label{pt:facebook}
\item \textbf{(enron)} An Enron email communication network~\cite{enron},
in which vertices represent email addresses and an edge  exists if an
address  sent at least one email to address .\label{pt:enron}
\end{enumerate}

\begin{table}
\centering
\begin{tabular}{|p{2cm}|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{Network} & \multicolumn{1}{|c|}{Source} & ~~~~ & ~~~~ & Average degree\\
\hline\hline
dolphins & Dolphins social network~\cite{dolphins} &  &  & \\\hline
polbooks & Copurchases of political books~\cite{polbooks} &  &  &
\\\hline
power & Power grid topology~\cite{powergrid} &  &  & \\\hline
facebook & Facebook ego-networks~\cite{facebook} &  &  &
\\\hline
enron & Enron communication network~\cite{enron} &  &  &
\\
\hline
\end{tabular}
\caption{Graphs compiled from real data.}
\label{table:realgraphs}
\end{table}

The network data for graphs~\ref{pt:dolphins},~\ref{pt:polbooks}, and
\ref{pt:powergrid} were taken from Mark Newman's network data
collection~\cite{newmandata}.  The network data for graphs~\ref{pt:facebook} and
\ref{pt:enron} are from the SNAP Large Network Dataset
Collection~\cite{snapdata}.

\subsubsection{Procedure} In this series of experiments, the seed vertex  was
chosen to be a known member of a cluster.  As before,  was chosen according
to  with the values in Table~\ref{table:realrankingparams}.
For each graph and for each value of  we compare an exact heat kernel
pagerank vector  with an -approximate heat kernel pagerank
vector .  Specifically, we consider the average  distance
(\ref{eq:measure-l1err}) and the intersection difference
(\ref{eq:measure-isim}).  We again choose  to range from  to .

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
~~~~ & Target        & Target       & Target         & \\
               & Cheeger ratio & cluster size & cluster volume &\\
\hline
 &  &  &  & \\
\hline
\end{tabular}
\caption{Parameters used to compute  for vector computations.}
\label{table:realrankingparams} 
\end{table}

\subsubsection{Discussion}
Figure~\ref{fig:real_l1} plots the average  error on the y-axis against
different values of  on the x-axis for each of the dolphins, polbooks, and
power graphs.  Figure~\ref{fig:real_rank_isim} plots the intersection difference
on the y-axis against  on the x-axis.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{real_l1}
\caption{Average error in each component for -approximate heat kernel
pagerank vectors when allowing varying random walk lengths.}
\label{fig:real_l1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{real_rank_isim}
\caption{Intersersection difference of the ranked lists of vertices computed by
exact and -approximate heat kernel pagerank vectors when allowing
varying random walk lengths.}
\label{fig:real_rank_isim}
\end{figure}

First we discuss the average  error.  The dolphins and the polbooks graphs
exihibit properties of both the small world graphs and the preferential
attachment graphs of the previous section (Figures~\ref{fig:random_rank_100}
and~\ref{fig:random_rank_500}).  Like the preferential attachment models, there
is a significant drop in average  error after , and like the small
world model the error continues to drop for larger values of , approaching a
minimum error of .  The average  error in the power graph,
on the other hand,  is small for all values of .  We remark that,
representing a power grid, the graph has very small average vertex degree, so
few random walk steps are enough to approximate the stationary distribution.

As for the intersection difference, we observe a smaller variance in values for
the three graphs.  Regardless of the size or structure of the graph, the
intersection difference drops sharply from  to .  For values larger
than , where , the intersection
difference decreases only marginally.


\paragraph{}The purpose of these experiments was to evaluate how error and differences of
ranking change in heat kernel pagerank approximation when varying , the upper
bound on number of steps taken in random walks.  We found that setting an upper
bound for random walk lengths to  with 
according to Theorem~\ref{thm:hkpraccuracy} yields approximations which satisfy
the prescribed error bounds.  This value is independent of the size of the graph
and , and depends only on .  Namely, we observed that choosing 
this way results in a significant decrease in both average  error and
intersection difference as compared to smaller values of , and only slight
decrease in average  error and intersection difference for larger values of
 as demonstrated in
Figures~\ref{fig:random_rank_100},~\ref{fig:random_rank_500},~\ref{fig:real_l1},
and~\ref{fig:real_rank_isim}.  Further, we tested graphs of various size, random
graphs generated from various models (see Section~\ref{sec:synthranking}), and
graphs from real data representing social networks, copurchasing networks, and
topological grids (see Section~\ref{sec:realranking}).  We found this choice of
 was optimal for every graph regardless of size or structure.  That is, the
cutoff for random walk lengths does not depend on the size of the graph.

It is also worth mentioning that the most striking outlier among the subject
graphs is the small world graph, or expander graphs.  This is due to the fact
that the graph consists of a single cluster, which makes local cluster detection
ineffective.

\section{An Assessment of Cheeger Ratios Obtained with Local Clustering Algorithms}
\label{sec:expresults}
The goal of this section is to analyze the quality of local clusters computed
with a sweep over an approximate heat kernel pagerank vector (see
Section~\ref{sec:goodcuts} for details on sweeps).  We consider two objectives
for analysis.

The first objective is to validate the statement of Theorem~\ref{thm:localpart}.
To do this, we show that the Cheeger ratios of local clusters computed with
sweeps over approximate heat kernel pagerank vectors are within the
approximation guarantees of Theorem~\ref{thm:localpart}.  We use a slightly
modified version of \partitionalg~to compute these clusters.  We call this
modified algorithm HKPR, and it is described in the list below.  

The second objective is to compare clusters computed with sweeps over different
vectors.  Namely, for a given graph and seed vertex, we compare the local
clusters computed using the following sweep algorithms:

\begin{enumerate}
\item \textbf{(HKPR)} A sweep over an -approximate heat
kernel pagerank vector is performed.  The segment  with volume  of minimal Cheeger ratio is output.  This is the
\partitionalg~algorithm with the following modification: we allow segments of
volume up to  rather than limiting the search to segments of volume
, twice the target volume.\label{pt:epshkpr}
\item \textbf{(HKPR)} A sweep over an exact heat kernel pagerank vector is
performed.  The segment  with volume  of minimal Cheeger
ratio is output.  This algorithm was outlined, but not stated explicitely,
in~\cite{chung:partitionhkpr:im09}.
\item \textbf{(PR)} A sweep over a Personalized PageRank vector
(\ref{eq:pagerank}) is performed.  The segment  with volume  of minimal Cheeger ratio is output.  This is an adaptation of the
algorithm \texttt{PageRank-Nibble}\cite{acl:prgraphpartition:focs06} with the
following modifications: (i) rather than performing a sweep over an approximate
PageRank vector, perform a sweep over an exact PageRank vector, and (ii) allow
segments only as large as .
\end{enumerate}

We summarize the algorithms and parameters below in
Table~\ref{table:clusteralgs}.

\begin{table}
\centering
\begin{tabular}{|l|c|l|l|}
\hline
\multicolumn{1}{|c|}{Algorithm} & Sweep vector &
\multicolumn{1}{|c|}{Algorithm parameters} & \multicolumn{1}{|c|}{Sweep vector
parameters}\\
\hline\hline
HKPR &  & , target Cheeger ratio & \\
& & , target cluster size & , seed vertex\\
& & , target cluster volume & , approximation parameter\\\hline
HKPR &  & , target Cheeger ratio & \\
& & , target cluster size & , seed vertex\\\hline
PR &  & , target Cheeger ratio & \\
& & & , seed vertex\\\hline
\end{tabular}
\caption{Algorithms used for comparing local clusters.}
\label{table:clusteralgs}
\end{table}

Each trial will resemble Procedure~\ref{alg:compareclusters}, as stated below.

\begin{algorithm}[H]
\floatname{algorithm}{Procedure}
\caption{Compare Clusters}
\label{alg:compareclusters}
\begin{algorithmic}
\State Let  be a graph and  a seed vertex
\State Choose parameters , , , 
\State Let  be a local cluster computed using the algorithm HKPR
\State Let  be a local cluster computed using the algorithm HKPR
\State Let  be a local cluster computed using the algorithm PR
\State Compare .
\end{algorithmic}
\end{algorithm}

The following sections describe the experiments in more detail.

\subsection{Synthetic graphs}
In this section, we use graphs generated with three random graph models:
Watts-Strogatz small world, Barab\'{a}si-Albert preferential attachment, and
Holme and Kim's powerlaw cluster as described in Section~\ref{sec:synthranking}.

\subsubsection{Procedure} We perform twenty-five trials of
Procedure~\ref{alg:compareclusters} and take the averages of Cheeger ratios and
cluster volumes computed.  Specifically, we fix a model and algorithm
parameters.  Then, generate a random graph according to the model and
parameters.  For each random graph, pick a random seed vertex with probability
proportional to degree.  Then, for each seed vertex compute local clusters  using the algorithms HKPR, HKPR, and PR, respectively.  We
then use the average Cheeger ratio and cluster volume of the  for
comparison.  In Table~\ref{table:synthclusterparams} we summarize the parameters
used for each random graph model.

\begin{table}
\centering
\begin{tabular}{|p{2.6cm}|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Model}& ~~ & ~~ & ~~~~ & ~~~~ & Target        & Target       & Target\\
       &           &       &         &                & Cheeger ratio & cluster size & cluster volume\\
\hline\hline
small world  &    &  &  &  &  &   & \\
             &    &  &  &  &  &  & \\
             &    &  &  &  &  &  & \\
             &   &  &  &  &  &  & \\\hline
preferential &    &  &   -   &  &  &   & \\
attachment   &    &  &   -   &  &  &  & \\
             &    &  &   -   &  &  &  & \\\hline
powerlaw     &    &  &  &  &  &   & \\
cluster      &    &  &  &  &  &  & \\
             &    &  &  &  &  &  & \\
\hline
\end{tabular}
\caption{Algorithm parameters used to compare local clusters.}
\label{table:synthclusterparams} 
\end{table}


\subsubsection{Discussion} We address the first analytic objective listed in the
introduction of this section by discussing the clusters output by
HKPR.  Namely, we compare the clusters computed with HKPR to
the guarantees of Theorem~\ref{thm:localpart}.  The results for each graph are
given in Table~\ref{table:synthhkprclusterresults}.  The first three columns
indicate the random graph model and algorithm parameters used for each instance.
The last two columns demonstrate how the (average) Cheeger ratio of clusters
computed by HKPR compare to the approximation guarantee of
Theorem~\ref{thm:localpart}.  Namely, Theorem~\ref{thm:localpart} states that
the cluster output will have Cheeger ratio  with high
probability.  In every case the Cheeger ratio is well within the approximation
bounds.

\begin{table}
\centering
\textbf{Synthetic graphs}\\
\begin{tabular}{|p{2.6cm}|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Model} & ~~~~  & , Target & Cheeger ratio output by &
~~~~~~~~~~\\
             &            & Cheeger ratio & HKPR          & \\
\hline\hline
small world  &    &  &  & \\
             &    &  &  & \\
             &    &  &  & \\
             &   &  &  & \\\hline
preferential &    &  &  & \\
attachment   &    &  &  & \\
             &    &  &  & \\\hline
powerlaw     &    &  &  & \\
cluster      &    &  &  & \\
             &    &  &  & \\
\hline 
\end{tabular}
\caption{Cheeger ratios of cluster output by HKPR.}
\label{table:synthhkprclusterresults}
\end{table}

The second objective is to compare clusters computed with the three different
local clustering algorithms HKPR, HKPR, and PR.
Table~\ref{table:cheegersynth} is a collection of cluster statistics for the
trials.  For each graph instance we list the average Cheeger ratio and cluster
volume of local clusters computed using the PR, HKPR, and HKPR
algorithms, respectively.  

\begin{table}
\centering
\textbf{Synthetic graphs}\\
\begin{tabular}{|p{2cm}|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Model} &  & PR & HKPR & HKPR\\
\hline\hline
small world  & 100  &  &  & \\
             & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 500  &  &   & \\
             & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 800  &  &   & \\
             & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 1000 &  &  & \\
             & & (volume = ) & (volume = ) & (volume = )\\\hline
preferential & 100  &  &  & \\
attachment   & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 500  &  &  & \\
             & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 800  &  &  & \\
             & & (volume = ) & (volume = ) & (volume =
)\\\hline
powerlaw     & 100  &  &  & \\
cluster      & & (volume = ) & (volume = ) & (volume = )\\\cline{2-5}
             & 500  &  &  & \\
             & & (volume = ) & (volume = ) & (volume =
)\\\cline{2-5}
             & 800  &  &  & \\
             & & (volume = ) & (volume = ) & (volume = )\\
\hline 
\end{tabular}
\caption{Cheeger ratios of clusters output by different local clustering algorithms on
synthetic data.}
\label{table:cheegersynth}
\end{table}

We remark that for each graph there is little variation in Cheeger ratio and
volume of clusters computed by the three different algorithms.  We also note
that there is no obvious trend as graphs get larger.  The small world graphs
demonstrate the greatest variation in cluster quality.  However, as mentioned in
Section~\ref{sec:rankings}, expander graphs, such as small world graphs, consist
of one large cluster.

It is worth noting that in some trials the output volume is significantly
greater than twice the target volume.  While this may seem like a contradiction,
it is a consequence of our implementation.  During a sweep one may choose to
output a cluster of minimal Cheeger ratio, or one that satisfies volume
constraints, or both.  We are interested in comparing Cheeger ratios and so
allow the sweep to continue checking clusters that are well beyond twice the
target volume.

\subsection{Real graphs}
For these trials we use graphs generated from real data summarized in
Section~\ref{sec:realranking}.

\subsubsection{Procedure} We compare clusters computed by each of the three
algorithms as outlined in Procedure~\ref{alg:compareclusters}.  In these trials
we fix the seed vertex to be a member of a cluster with good Cheeger ratio.
Using this seed vertex, we compare the clusters computed using the
HKPR, HKPR, PR algorithms.

For each trial we use the parameters listed in Table~\ref{table:clusterparams}.
We note that in each case the target cluster volume is computed to be roughly
the target cluster size times the average vertex degree, and here we use
.  
\begin{table}
\centering
\begin{tabular}{|p{2cm}|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Network} &  &  & Average & ~~~~ & Target & Target & Target\\
        &       &       & degree  &                & Cheeger ratio & cluster size & cluster volume\\
\hline
dolphins  &     &     &     &  &  &   &   \\
polbooks  &    &     &   &  &  &   &   \\
power     &   &    &   &  &  &  &   \\
facebook  &   &   &  &  &  &  &  \\
enron     &  &  &    &  &  &  &  \\
\hline
\end{tabular}
\caption{Graph and algorithm parameters used to compare local clusters.}
\label{table:clusterparams} 
\end{table}


\subsubsection{Discussion}
Table~\ref{table:realhkprclusterresults} lists ratios output by HKPR compared with the approximation guarantees of
Theorem~\ref{thm:localpart}.  In each case, the Cheeger ratios are well within
the approximation bounds of Theorem~\ref{thm:localpart}.

\begin{table}
\centering
\textbf{Real graphs}\\
\begin{tabular}{|p{2cm}|c|c|c|}
\hline
\multicolumn{1}{|c|}{Network} & , Target & Cheeger ratio output by &
~~~~~~~~~~\\
        & Cheeger ratio & HKPR & \\
\hline\hline
dolphins &  &  &  \\
polbooks &  &  & \\
power    &  &  &  \\
facebook &  &  &  \\
enron    &  &  & \\
\hline 
\end{tabular}
\caption{Cheeger ratios of cluster output \partitionalg.}
\label{table:realhkprclusterresults}
\end{table}

The complete numerical data obtained from the set of the trials are given in
Table~\ref{table:cheegerreal}.  For each graph we list the Cheeger ratio,
cluster volume, and additionally the cluster size of local clusters computed
using each of the algorithms PR, HKPR, and HKPR, respectively.

\begin{table}
\centering
\textbf{Real graphs}\\
\begin{tabular}{|p{2cm}|c|c|c|}
\hline
\multicolumn{1}{|c|}{Network} & PR & HKPR & HKPR\\
\hline\hline
dolphins &  &  &  \\
         & (volume = ) & (volume = ) & (volume = )\\
         & (size = ) & (size = ) & (size = )\\\hline
polbooks &  &  & \\
         & (volume = ) & (volume = ) & (volume = )\\
         & (size = ) & (size = ) & (size = )\\\hline
power    &     &  &  \\
         & (volume = ) & (volume = ) & (volume = )\\
         & (size = ) & (size = ) & (size = )\\\hline
facebook &  &  &  \\
         & (volume = ) & (volume = ) & (volume = )\\
         & (size = ) & (size = ) & (size = )\\\hline
enron    &  & - & \\
         & (volume = ) & - &  (volume = )\\
\hline 
\end{tabular}
\caption{Cheeger ratios of cluster output by different local clustering
algorithms.}
\label{table:cheegerreal}
\end{table}

For each graph, the local cluster computed using HKPR has smaller
Cheeger ratio than the local cluster computed using PR.  For the power graph, we
observe that the cluster of minimal Cheeger ratio was computed using the HKPR
algorithm, but it is nearly a third the size of the entire network.  The
algorithms HKPR and PR, on the other hand, each return smaller
clusters.  We remark that for real graphs, the clusters computed using sweeps
over different vectors have more variation than for random graphs.

At this point we remark about our choice of parameters for the trials.  At this
point the sensitivity of the algorithm to the choice of  and
 has not been fully explored.  In particular, it is worth studying
the effect of  on the output cluster in future work.


To conclude, we include visualizations of clusters computed in the facebook
ego-network to illustrate the differences in local cluster detection.
Figure~\ref{fig:fb_ehkpr} colors the vertices in a local cluster computed using
the HKPR algorithm, as described in Table~\ref{table:cheegerreal}.
Figure~\ref{fig:fb_pr} colors the vertices in a local cluster compted using the
PR algorithm.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{fb_fa2_d_ehkpr}
\caption{Local cluster in facebook ego network computed using the HKPR
algorithm.}
\label{fig:fb_ehkpr}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{fb_fa2_plus_ud_pr_3}
\caption{Local cluster in facebook ego network computed using the PR
algorithm.}
\label{fig:fb_pr}
\end{figure}

The numerical data of the last two sections validate the effectiveness and
efficiency of local cluster detection using sweeps over -approximate
heat kernel pagerank.  The experiments of Section~\ref{sec:rankings} demonstrate
that sampling a number of random walks of at most  steps yield a ranking of
vertices within the error bounds of Theorem~\ref{thm:hkpraccuracy}.  This ranking
in turn is used to compute a local cluster.  What is more, this value  does
not depend on parameters other than .  Specifically, it does not
depend on the size of the graph or the desired cluster volume, size, or Cheeger
ratio.  Finally, the data of Section~\ref{sec:expresults} validate the
statements of Theorem~\ref{thm:localpart}.  That is, perfoming a sweep over an
approximate heat kernel pagerank vector detects clusters of Cheeger ratio at
most  for a desired Cheeger ratio .  The total cost of
computing this cluster is , sublinear in the size of the graph. 


\bibliographystyle{amsplain}
\bibliography{ejc_hkpr}  

\end{document}
