\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage{wrapfig, makecell}



\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{floatrow}
\floatsetup[table]{capposition=top}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{amsmath}

\title{AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints}




\author{Xingzhe He \hspace{5mm} Bastian Wandt \hspace{5mm} Helge Rhodin \\
University of British Columbia\\
{\tt\small \{xingzhe, wandt, rhodin\}@cs.ubc.ca}}



\usepackage{soul} 

\newif\ifdraft
\drafttrue

\newcommand{\netL}{{\it LayerNet}}
\newcommand{\netK}{{\it PiCNet}}
\newcommand{\netU}{{\it KeypointNet}}
\newcommand{\lost}{{\it LostGAN}}

\newcommand{\hA}{{\it ScaleHeuristic}}
\newcommand{\hB}{{\it OverlapHeuristic}}

\newcommand{\soft}{{\it soft-occlusion}}
\newcommand{\lsoft}{{\it local-soft-occlusion}}

\newcommand{\ST}{\mathcal{T}}
\newcommand{\SST}{\mathcal{T}_S}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Seg}{\mathbf{S}} \newcommand{\Latent}{\mathbf{L}}
\newcommand{\LatentG}{\Latent^{\text{3D}}} \newcommand{\LatentA}{\Latent^\text{app}} \newcommand{\LatentBG}{\mB} 

\newcommand{\dA}{{A}}
\newcommand{\dB}{{B}}

\newcommand{\HR}[1]{{\color{blue}{\bf hr: #1}}}
\newcommand{\hr}[1]{{\color{blue} #1}}
\newcommand{\BW}[1]{{\color{green}{\bf bw: #1}}}
\newcommand{\bw}[1]{{\color{green} #1}}
\newcommand{\XZ}[1]{{\color{cyan}{\bf xz: #1}}}
\newcommand{\xz}[1]{{\color{cyan} #1}}

\newcommand{\md}[1]{{\color{blue} #1}}
\newcommand{\mdB}[1]{{\color{red} #1}}

\newcommand{\TODO}[1]{\textcolor{cyan}{#1}}
\newcommand{\NEW}[1]{{\color{red}{#1}}}
\newcommand{\comment}[1]{}

\newcommand{\app}{appendix} \newcommand{\parag}[1]{{\bf{#1}}}



\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}

\newcommand{\mA}{\mathbf{A}}
\newcommand{\mB}{\mathbf{B}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\mD}{\mathbf{D}}
\newcommand{\mE}{\mathbf{E}}
\newcommand{\mF}{\mathbf{F}}
\newcommand{\mG}{\mathbf{G}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mJ}{\mathbf{J}}
\newcommand{\mK}{\mathbf{K}}
\newcommand{\mL}{\mathbf{L}}
\newcommand{\mM}{\mathbf{M}}
\newcommand{\mN}{\mathbf{N}}
\newcommand{\mO}{\mathbf{O}}
\newcommand{\mP}{\mathbf{P}}
\newcommand{\mQ}{\mathbf{Q}}
\newcommand{\mR}{\mathbf{R}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mT}{\mathbf{T}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\mV}{\mathbf{V}}
\newcommand{\mW}{\mathbf{W}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mY}{\mathbf{Y}}
\newcommand{\mZ}{\mathbf{Z}}

\newcommand{\cA}{\mathcal A}
\newcommand{\cB}{\mathcal B}
\newcommand{\cC}{\mathcal C}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cF}{\mathcal F}
\newcommand{\cG}{\mathcal G}
\newcommand{\cH}{\mathcal H}
\newcommand{\cI}{\mathcal I}
\newcommand{\cJ}{\mathcal J}
\newcommand{\cK}{\mathcal K}
\newcommand{\cL}{\mathcal L}
\newcommand{\cM}{\mathcal M}
\newcommand{\cN}{\mathcal N}
\newcommand{\cO}{\mathcal O}
\newcommand{\cP}{\mathcal P}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\cR}{\mathcal R}
\newcommand{\cS}{\mathcal S}
\newcommand{\cT}{\mathcal T}
\newcommand{\cU}{\mathcal U}
\newcommand{\cV}{\mathcal V}
\newcommand{\cW}{\mathcal W}
\newcommand{\cX}{\mathcal X}
\newcommand{\cY}{\mathcal Y}
\newcommand{\cZ}{\mathcal Z}


 \begin{document}

\maketitle
\begin{abstract}
Structured representations such as keypoints are widely used in pose transfer, conditional image generation, animation, and 3D reconstruction.
However, their supervised learning requires expensive annotation for each target domain.
We propose a self-supervised method that learns to disentangle object structure from the appearance with a graph of 2D keypoints linked by straight edges. 
Both the keypoint location and their pairwise edge weights are learned, given only a collection of images depicting the same object class. The resulting graph is interpretable, for example, AutoLink recovers the human skeleton topology when applied to images showing people.
Our key ingredients are i) an encoder that predicts keypoint locations in an input image, ii) a shared graph as a latent variable that links the same pairs of keypoints in every image, iii) an intermediate edge map that combines the latent graph edge weights and keypoint locations in a soft, differentiable manner, and iv)
an inpainting objective on randomly masked images.
Although simpler, 
AutoLink outperforms existing self-supervised methods on the established keypoint and pose estimation benchmarks 
and paves the way for structure-conditioned generative models on more diverse datasets.
Project website: \url{https://xingzhehe.github.io/autolink/}.
\end{abstract} \section{Introduction}

Object structure representations are widely used in modern computer graphics and computer vision techniques, including keypoints for image generation \cite{ma2017pose, ma2018disentangled, siarohin2018deformable} and skeletons for 3D reconstruction \cite{feng2018joint, jackson2017large, yu2017bodyfusion, pavlakos2019expressive, su2021anerf}.
However, the structure is usually supervised on large annotated datasets~\cite{lin2014microsoft, andriluka14cvpr, PoseTrack, mono3dhp2017} or via hand-crafted parametric models~\cite{SMPL2015, SMPLX2019, Xu2020ghum, osman2020star, blanz1999morphable, FLAME:SiggraphAsia2017}. Neither approach generalizes well to new domains and both require additional manual annotation whenever more detail is needed~\cite{Guler2018DensePose}. 

Our goal is to reconstruct the keypoint locations of an object by learning from an unlabelled image collection, thereby sidestepping the generalization problem.
Our key idea is to leverage that the same object shares the same topology by introducing an explicit graph that links the same pairs of keypoints in all instances.
By contrast, existing self-supervised keypoint learning methods model objects as a set of independent parts.
Their consistency over different instances of the same object is encouraged by either enforcing parts to follow hand-crafted image transformations~\cite{thewlis2017unsupervised, zhang2018unsupervised, jakab2018unsupervised, lorenz2019unsupervised, hung2019scops, liu2021unsupervised} or by adding implicit bias in the network architecture that encodes such spatial equivariances~\cite{he2022ganseg, he2021latentkeypointgan}.
Only~\cite{jakab2020self, schmidtke2021unsupervised} use an explicit skeleton representation, but both require predefined topology, rely on video input, and \cite{jakab2020self} is trained in a CycleGAN setting that still requires manually labeled examples.


We propose a simple yet effective method to learn both the keypoints and their links without supervision in terms of a sparse graph serving two purposes. 
First, the graph acts as a bottleneck that can only store structural information disentangled from appearance. 
Second, it forms a constraint that associates observations across training images. We enforce the same topology across instances of the same class by learning a shared graph with a single set of edge weights.
In the absence of labels, we train AutoLink with only an autoencoder reconstruction objective. Since the graph bottleneck should not model appearance, we additionally feed the decoder with the input image after masking the majority of its pixels.
In turn, it is important for the disentanglement that the heavily masked image contains appearance without leaking structural information. This is the case as inpainting methods are unable to infer the original image precisely from only sparse pixel colors \cite{zheng2019pluralistic, yu2021diverse}
unless conditioning on structure representations, such as edge maps~\cite{xiong2019foreground, nazeri2019edgeconnect, li2019progressive, lahiri2020prior, han2019finet}. 
Therefore, by forcing the autoencoder to reconstruct the original image, the detector converges to generate representative structures of images.


We demonstrate on 4 benchmarks that the trained detector has a significantly improved keypoint localization accuracy and on 6 additional datasets that it applies to a broader set of images spanning portraits, persons, animals, hands, and flowers, which we attribute to the explicit modeling of links in the graph. 
Figure~\ref{fig:teaser} shows the diverse set of image domains it applies to, including challenging textures and uncontrolled background, 
how both skeleton representations as well as object outlines are learned by varying the number of keypoints,
and exemplifies applications to controlled image generation. 


\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.98\textwidth]{images/teaser.pdf}
\end{center}
    \caption{\textbf{Teaser.}
    (a) AutoLink applies to diverse collections of images and automatically yields keypoints linked to a graph without ground truth. It recovers animal and human poses and object shapes in settings where previous methods struggle, including cluttered backgrounds, structured stripe textures, articulated fingers, and detailed faces. 
    (b) Example applications are conditional image generation with autoencoders (top) and GANs (bottom) that are driven by the learned keypoints.}
\label{fig:teaser}
\end{figure}



\textbf{Ethics - Risks.} \label{sec:ethics} The estimated keypoints and edges could be abused for deep fakes as the driving signal for generative models or for unwanted surveillance applications. However, our method works towards improved generality, including objects and animals, and does not improve upon supervised models that already exist in high detail for humans.
\textbf{Benefits.} Since our method is entirely self-supervised, it can be applied to a diverse set of persons, objects, animals, or situations that have not yet been labeled.




















%
 \section{Related Work}

Most representation learning methods focus on generic feature vectors for entire images to initialize deep networks for improved object classification \cite{wu2018unsupervised, chen2020simple, oord2018representation, he2020momentum, he2021masked}.
By contrast, our method introduces explicit object structure. We review the most related approaches in the following.

\noindent\parag{Self-supervised Keypoints Detection.} The most common idea to discover keypoints in an unsupervised manner is to rely on the notion that keypoints move as the image changes.
Various constraints have been used
to enforce that the keypoints follow a known transformation,
including
view changes in a multi-view recording \cite{suwajanakorn2018discovery, rhodin2018unsupervised, Rhodin_2019_CVPR} or the natural motion in videos \cite{dundar2020unsupervised, siarohin2019animating, kulkarni2019unsupervised, minderer2019unsupervised, dong2018supervision, kim2019unsupervised, jakab2020self}.
When only single images are available, artificial image deformation is applied, either from randomized
\cite{thewlis2017unsupervised, zhang2018unsupervised, jakab2018unsupervised, lorenz2019unsupervised} or learned \cite{wu2019transgaga, xu2020unsupervised} transformations within a pre-defined deformation space. 
However, 
learned keypoints may model the background \cite{zhang2018unsupervised, siarohin2019animating} and struggle with large pose variation \cite{hung2019scops} as image deformations do not separate foreground from background, which are usually tuned for each dataset
and bound to be small.
Most models leverage multi-branch network architectures to encode the structure and appearance separately and utilize multiple losses that need to be balanced. By contrast, we do not apply artificial transformations, use a single branch, and a single loss which eases and stabilizes training.
To overcome the need for artificial image deformation, He et al. \cite{he2021latentkeypointgan, he2022ganseg} exploit GANs to generate images along with corresponding keypoints and later use them to train a detector. However, this leads to even more complex network architectures and comes with instabilities in GAN training, limiting their applicability to complex objects like human bodies. \looseness=-1

\noindent\parag{Skeleton Representations.} Bone maps representing the keypoints connectivity as affinity fields \cite{cao2017realtime} or via explicit offsets \cite{papandreou2018personlab} are used in supervised 
human, animal, and object pose estimation.
We use a similar edge map representation but learn both the location and linking from scratch without annotations.
In the weakly-supervised setting, Jakab et al. \cite{jakab2020self} exploit CycleGAN \cite{zhu2017unpaired} to translate between image and edge maps. The graph connectivity is predefined to the human skeleton and edges are supervised by a large dataset of unpaired ground truth object edges, which can come from a different dataset but are manually annotated. Schmidtke et al. \cite{schmidtke2021unsupervised} overcome the manual labeling by deforming a template skeleton. However, they both require the known connectivity of the keypoints and videos for training while ours learns both the keypoints and connectivity from a collection of single images. Noguchi et al. \cite{noguchi2021watch} generate a skeleton heuristically by linking the centers of part-wise learned Signed Distance Fields \cite{malladi1995shape}. However, they require videos without the background of the same object, and the learned skeleton does not generalize to other objects of the same class.

\noindent\parag{Object Sketch Learning.} Sketches are made of strokes drawn by a pen. It is a concise and abstract representation, which can be used in object recognition \cite{Yu2015SketchaNetTB, xu2021multigraph} and image retrieval \cite{wang2015sketch, qian2016sketch, xu2018sketchmate}. There are two common sketch representations used in neural models~\cite{Xu2022DeepLF}:
black-white raster images \cite{wang2019learning, SasakiCGI2018learning,  YiLLR19APDrawingGAN}, often used for image-to-image translation \cite{isola2017image, zhu2017unpaired, pang2018deep, kampelmuhler2020synthesizing}
and sequences of points (pen coordinates) \cite{ha2018a, eitz2012hdhso, sangkloy2016the}, which is usually used by recurrent generation models \cite{ha2018a, chen2017sketch, Cao2019ai, kaiyrbekov2019deep, das2020beziersketch, ge2021creative}. This graph representation is similar to ours. However, instead of learning to mimic human drawings, ours directly predicts both the keypoints and their connectivity on real natural images.

\noindent\parag{Structure-enhanced Image Inpainting.} When key parts are missing in an image, e.g., eyes on faces or arms of humans, it is hard for inpainting networks to imagine the content accurately from scratch.
Therefore, additional structural cues are detected to guide the subsequent image generation. 
The cues can be supervised segmentation masks \cite{zhao2021prior, liao2020guidance, han2019finet, song2018spg}, foreground contours \cite{xiong2019foreground}, and landmarks \cite{lahiri2020prior, zhang2021face, yang2020generative}, or automatically extracted
 edges \cite{nazeri2019edgeconnect, li2019progressive, jie2020inpainting, xu2020anedge, cao2021learning} and low-frequency image components \cite{wang2020image, ren2019structureflow}. 
Our reconstruction objective can be seen as such two-stage inpainting, but self-supervised and with the image edges replaced with the learned graph edge representation. \looseness=-1

\noindent\parag{Self-supervised Foreground Segmentation.} Traditional methods use color \cite{zhu2014saliency}, contrast \cite{cheng2011global}, and hand-crafted features \cite{jiang2013salient} to cluster foreground pixels. 
A recent trend is exploiting inpainting techniques to segment the foreground. Chen et al.~\cite{chen2019unsupervised} and Arandjelovi{\'c} et al.~\cite{arandjelovic2019object} use a GAN to inpaint the background at the predicted segmentation mask, assuming that the object texture can be changed without changing the data distribution due to the independence of foreground and background. Yang et al.~\cite{yang2019unsupervised} propose Contextual Information Separation (CIS),
a general objective to segment the foreground by maximizing the error of inpainting the mask and its complement. It was first applied to optical flow maps and subsequently to RGB images by \cite{savarese2021information,yang2021dystab, katircioglu2021self}. 
When the object is small compared to the background, an additional object detection module 
\cite{katircioglu2021self,crawford2019spatially}
or  multi-view \cite{katircioglu2021human} information is required.
Different from these previous methods, we utilize a form of inpainting to learn sparse keypoints instead of segmentation. Our learned edges form a sparse foreground shape, but further extensions would be necessary to transfer from the edge maps to the boundary-aligned foreground segmentation.
 \section{Method} \label{sec:method}

We leverage that the objects in the dataset share the same topology and can be represented as a graph that connects keypoints by a shared set of edges. To learn the keypoints and edges, we design an autoencoder that aims to accurately reconstruct the input image, with the graph as the intermediate representation. To encode the input image into a graph, we detect the keypoints and create an edge heatmap based on learnable edge weights. To mostly obtain appearance information, we mask out the majority of the image, which randomizes the structure information and reduces the remainder to a very low  level.
The edge heatmap is combined with the masked image to reconstruct the original image.
Since the missing structure is important to reconstructing the original image, the network is forced to learn the structure of the object in a self-supervised manner. 
Figure~\ref{fig:overview} shows an overview of our method. 

Formally, given an image $\mI\in\R^{H\times W\times 3}$ with height $H$ and width $W$ we aim to learn a set of keypoints $\{\vk_i\}_{i=1}^K$, where $\vk_i\in[-1,1]\times[-1,1]\subset \R^2$ is the normalized keypoint coordinate, and $K$ is the number of keypoints. 
We use a ResNet with upsampling \cite{xiao2018simple} to detect keypoints. Afterward, we draw a differentiable edge \cite{mihai2021differentiable} between each pair of keypoints (details below). 
The edge map $\mS\in\R^{H\times W}$ is concatenated along the channel dimension with the randomly masked image $\mI_m\in\R^{H\times W\times 3}$ and fed into a UNet \cite{ronneberger2015u} to obtain the reconstructed image $\mI'$. The detailed network architectures can be found in Appendix~\ref{supp:archi}. \looseness=-1

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1.\textwidth]{images/overview.pdf}
\end{center}
   \caption{\textbf{Overview.} Given an image, we detect keypoints and draw differentiable edges between keypoints according to the learned graph edge weights that is visualized as a color matrix. The method is self-supervised in that the latent edge map and keypoints are learned by reconstructing the masked input images. Note that keypoints are image specific and edge maps are shared.}
\label{fig:overview}
\end{figure}

\subsection{Image Structure Representation}

In this section, we introduce the generation of keypoints and the edge map from the image.
Let $\mH\in\R^{H\times W\times K}$ be the $K$ heatmaps generated by a ResNet with upsampling \cite{xiao2018simple} from the image $\mI$. The keypoint $\vk_i$ is calculated by the differentiable soft-argmax function,
\begin{equation}
    \vk_i = \sum_\vp \frac{ \exp(\mH(\vp)) }{\sum_\vp (\exp\mH(\vp))} \vp,
\end{equation}
where $\vp\in [-1,1]\times[-1, 1]$ is the normalized pixel coordinates.

Given two keypoints $\vk_i, \vk_j$, we draw a differentiable edge map $\mS_{ij}$, where values are 1 on the edge linked by the two keypoints and decrease exponentially based on the distance to the line. Formally, the edge map $\mS_{ij}$ is a Gaussian extended along the line \cite{mihai2021differentiable}, defined as
\begin{equation}
    \mS_{ij}(\vp) = \exp\left(d^2_{ij}(\vp) / \sigma^2\right), 
\end{equation}
where $\sigma$ is a hyperparameter controlling the thickness of the edge, and $d_{ij}(\vp)$ is the $L_2$ distance between the pixel $\vp$ and the edge drawn by keypoints $\vk_i$ and $\vk_j$,
\begin{equation}
\vd_{ij}(\vp) = \left\{
\begin{aligned}
& \|\vp-\vk_i\|_2 &\text{ if } t\leq 0, \\
& \|\vp-((1-t)\vk_i + t\vk_j)\|_2 &\text{ if } 0<t<1, \\
& \|\vp-\vk_j\|_2 &\text{ if } t\geq 1,
\end{aligned}
\right.
\quad \text{where}\quad
t = \frac{(\vp-\vk_i)\cdot (\vk_j-\vk_i)}{\|\vk_i-\vk_j\|^2_2}.
\end{equation}

We assign a weight $w_{ij}>0$ to each edge, which is enforced to be positive by SoftPlus \cite{dugas2000incorporating}. This weight is learned during training and shared across all object instances in a dataset. 
Finally, we take the maximum at each pixel of the heatmaps to obtain the final edge map $\mS\in\R^{H\times W}$,
\begin{equation}
   \mS(\vp)  = \max_{ij}w_{ij}\mS_{ij}(\vp).
   \label{eq:max_heatmap}
\end{equation}
Taking the maximum at each pixel avoids the entanglement of the edge weights and the convolution kernel weights, which is further explained in Section~\ref{sec:ablation}.

\subsection{Image Reconstruction}
The masked image $\mI_m$ is generated by first uniformly dividing the image $\mI$ into a $16\times 16$ grid, and randomly masking out 80\% of the grid cells, similar to \cite{he2021masked}. 
We concatenate the masked image with the edge map and feed it into a UNet decoder \cite{ronneberger2015u} to reconstruct the original image,
\begin{equation}
    \mI'=\text{Decoder}(\alpha\mI_m \oplus \mS)
    \label{eq:recon}
\end{equation}
where $\oplus$ means concatenation along the channel dimension and $\alpha$ is a learnable parameter that compensates for the change of the edge weight magnitude during training. $\alpha$ is initialized to 1. We found this parameter to be helpful in training stability.
Different to \cite{he2021masked}, we condition on an edge map.
Different to \cite{jakab2020self}, we have no ground truth for the edge map. Our edge map is an unobserved latent variable. Thus we only minimize the difference of the original image and the reconstructed image by the perceptual loss~\cite{johnson2016perceptual},
\begin{equation}
    \cL=\frac{1}{N}\sum_{i=1}^N\|\Gamma(I_i)-\Gamma(I'_i)\|^2_2
\end{equation}
where $N$ is the number of examples and $\Gamma$ is the feature extractor.
The perceptual loss is believed to measure the structure similarity \cite{johnson2016perceptual, gatys2016image, dosovitskiy2016generating}, and leads to more robust training \cite{jakab2018unsupervised, jakab2020self}.

\subsection{Implementation Details} \label{sec:implementation_details}
We use the Adam optimizer \cite{KingmaB14} with a learning rate of $10^{-4}$ with $\beta_1=0.9$, $\beta_2=0.99$. The batch size is 64. We train for 20k iterations. It takes 3 hours to train on a single V100 GPU. All images are resized to $128\times 128$. The learning rate for the edge weights is multiplied by 512 due to the small gradient of SoftPlus \cite{dugas2000incorporating} when the value is close to 0.
To show the robustness of our model, we report all experiments on the sampling strategy of masking 80\% of the $16\times 16$ patches. 
We perform experiments with the same edge thickness of $\sigma^2=5e-5$ for all benchmark datasets. 
We train 10 times and report the mean and the standard deviation of the evaluation metrics. 
Although it already outperforms other work in most experiments, we also tune thicknesses to each individual dataset, as others did for their hyperparameters, which further improves the results. The tuned thicknesses can be found in Appendix~\ref{supp:ablation_test}.
The only other hyperparameter is the number of keypoints, which we set to that of the established benchmarks for quantitative comparisons, ranging from 4 to 32 points. \section{Experiments} \label{experiment}

\begin{figure}[t]
\centering
  \resizebox{0.98\linewidth}{!}{\begin{tabular}{cc}
\rotatebox{90}{$\phantom{xx}$ LatentKeypointGAN \cite{he2021latentkeypointgan}}&
\includegraphics[]{images/comp_latentkeypointgan.jpg}\\
\rotatebox{90}{$\phantom{x}$ \huge{GANSeg} \cite{he2022ganseg}}&
\includegraphics[]{images/comp_ganseg.jpg}\\
\rotatebox{90}{$\phantom{xxxxxxxx}$ \huge Ours} &
\includegraphics[]{images/comp_autolink.jpg}\\
\rotatebox{90}{$\phantom{xx}$ \huge  Ours (edges)} &
\includegraphics[]{images/comp_autolink_link.jpg}
\end{tabular}}\caption{\textbf{Qualitative comparison on detected keypoints}. Our model is more robust on wild face poses, and depicts more details on human bodies compared to \cite{he2021latentkeypointgan} and \cite{he2022ganseg}. For example, the feet poses in the middle four images are clearly detected.}
\label{fig:qualitative}
\end{figure}

In this section, we compare our results to the related methods, showing that our model is simple yet effective. Besides, we perform a number of ablation studies on hyperparameters and algorithm variants, exhibiting the robustness of our model and justifying the necessity of every model component.



\begin{table}[t]
\centering
\caption{\textbf{Landmark detection on CelebA}. The metric is the landmark regression (without bias) error in terms of $L_2$ distance normalized by inter-ocular distance (lower is better). While all methods perform well on aligned CelebA, ours is more robust on Wild CelebA. The sign $\star$ means being reported by \cite{hung2019scops} and $\dagger$ means being reported by \cite{liu2021unsupervised}.} 
\resizebox{0.9\linewidth}{!}{\begin{tabular}{llccc} \Xhline{1.5pt}
Method & Type & Aligned (K=10) & Wild (K=4) & Wild (K=8)  \\ \Xhline{1.5pt}

DFF \cite{collins2018deep} by \cite{hung2019scops} & Part Segmentation & - & - & \phantom{$\star$} 31.30\% $\star$ \\
SCOPS \cite{hung2019scops} (w/o saliency)  & Part Segmentation & - & 46.62\% & 22.11\% \\
SCOPS \cite{hung2019scops} (w/ saliency)  & Part Segmentation & - & 21.76\% & 15.01\% \\
Liu et al. \cite{liu2021unsupervised}  & Part Segmentation & - & 15.39\% & 12.26\% \\
Huang et al. \cite{huang2020interpretable} (w/ detailed label)  & Part Segmentation & - & - & \phantom{x}  8.40\% \\
GANSeg \cite{he2022ganseg} & Part Segmentation & 3.98\% & \textbf{12.26}\% & \phantom{x}  \textbf{6.18}\% \\ \hline

Thewlis et al. \cite{thewlis2017unsupervised} & Landmark & 7.95\% & - & \phantom{$\star$} 31.30\% $\star$ \\ 
Zhang et al. \cite{zhang2018unsupervised} & Landmark & 3.46\% & - & \phantom{$\star$} 40.82\% $\star$\\ 
LatentKeypointGAN \cite{he2021latentkeypointgan} & Landmark & 5.85\% & 25.81\% & 21.90\% \\
Lorenz et al. \cite{lorenz2019unsupervised} & Landmark & 3.24\% & \phantom{$\star$} 15.49\% $\dagger$ & \phantom{$\star$} 11.41\% $\dagger$\\
IMM \cite{jakab2018unsupervised} & Landmark &  \textbf{3.19}\% & \phantom{$\star$} 19.42\% $\dagger$ & \phantom{$\star$} \phantom{x} 8.74\% $\dagger$\\
LatentKeypointGAN-tuned \cite{he2021latentkeypointgan} & Landmark & 3.31\% & 12.10\% & \phantom{x} 5.63\% \\
Ours (general) & Landmark &  3.92$\pm$0.69\% & 7.72$\pm$0.47\% & 5.66$\pm$0.29\% \\
Ours (thickness-tuned) & Landmark &  3.54\% & \phantom{x}  \textbf{6.11}\% & \phantom{x}  \textbf{5.24}\% \\ \Xhline{1.5pt}
\end{tabular}}
\label{tab:celeba}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Landmark detection on Human Body}. Our model outperforms all the other unsupervised baselines. The metric for each dataset follows the corresponding description in the text. The sign $\dagger$ means being reported by \cite{siarohin2021motion} and the sign $\star$ means being reported by \cite{sandro2020unsupervised}. The number of keypoints is $K=16$ for Human3.6m and DeepFashion and $K=10$ for Taichi.}
\resizebox{0.9\linewidth}{!}{\begin{tabular}{llccc} \Xhline{1.5pt}
Method & Supervision & Human3.6m  $\downarrow$ & DeepFashion $\uparrow$ & Taichi $\downarrow$ \\ \Xhline{1.5pt}
Jakab et al. \cite{jakab2020self} & video \& unpaired ground truth & 2.73  & - & - \\ 
Newell et al. \cite{jakab2020self} & paired ground truth & \textbf{2.16}  & - & - \\ \hline
DFF \cite{collins2018deep}& testing dataset & - & - & \phantom{$\star$} 494.48 $\dagger$ \\
SCOPS \cite{hung2019scops}& saliency maps & - & - &  \phantom{$\star$} 411.38 $\dagger$ \\
Siarohin et al. \cite{siarohin2021motion} & videos & - & - & 389.78 \\  
Zhang et al. \cite{zhang2022self} & videos & - & - & \textbf{343.67} \\
Zhang et al. \cite{zhang2018unsupervised} & videos & 4.14  & - & - \\
Schmidtke et al. \cite{schmidtke2021unsupervised} & video \& T-pose template & 3.31 & - & - \\ 
Sun et al. \cite{sun2022self} & videos & \textbf{2.53}$\pm$0.06  & - & - \\ \hline
Thewlis et al. \cite{thewlis2017unsupervised}& unsupervised  & 7.51 & - & - \\
Zhang et al. \cite{zhang2018unsupervised} & unsupervised & 4.91  & - & - \\
LatentKeypointGAN \cite{he2021latentkeypointgan} & unsupervised & -  & 49\% & 437.69\\
Lorenz et al. \cite{lorenz2019unsupervised} & unsupervised & 2.79  & \phantom{$\star$} 57\% $\star$ & - \\ 
GANSeg \cite{he2022ganseg} & unsupervised & - & 59\% & 417.17 \\ 
Ours (general) & unsupervised & 2.81$\pm$0.07 &65$\pm$1.2\% & 337.50$\pm$25.08\\
Ours (thickness-tuned) & unsupervised & \textbf{2.76} & \textbf{66}\% & \textbf{316.10} \\ \Xhline{1.5pt}
\end{tabular}
}
\label{tab:human_body}
\end{table}





\begin{table}[t]
\centering
\caption{\textbf{Landmark detection on CUB Birds}. Our model outperforms most other baselines and achieves comparable results with the ones using ground truth segmentation masks. The metric is the landmark regression (without bias) error of $L_2$ distance normalized by the image size (lower is better). A star $\star$ means being reported by \cite{choudhury2021unsupervised}, $\dagger$ means being reported by \cite{hung2019scops}, and $\ddagger$ means tested by us with their official code; all other numbers are taken from the respective papers. The number of keypoints is $K=10$ for CUB-aligned and $K=4$ for CUB-001, CUB-002, CUB-003, and CUB-all.}
\resizebox{0.9\linewidth}{!}{\begin{tabular}{llcccccc} \Xhline{1.5pt}
Method & Supervision & CUB-aligned $\downarrow$ & CUB-001 $\downarrow$ & CUB-002 $\downarrow$ & CUB-003 $\downarrow$ & CUB-all $\downarrow$ \\ \Xhline{1.5pt}
SCOPS \cite{hung2019scops}& GT silhouette & - & \phantom{$\star$}18.3 $\star$ & \phantom{$\star$}17.7 $\star$ & \phantom{$\star$}17.0 $\star$ & \phantom{$\star$} 12.6 $\star$ \\
Choudhury et al. \cite{choudhury2021unsupervised} & GT silhouette & - & \textbf{11.3} & \textbf{15.0} & \textbf{10.6} & \textbf{9.2} \\ \hline
DFF \cite{collins2018deep}& testing dataset & - & \phantom{$\dagger$}22.4$\dagger$ & \phantom{$\dagger$}21.6$\dagger$ & \phantom{$\dagger$}22.0$\dagger$ & - \\
SCOPS \cite{hung2019scops}& saliency maps & - & \textbf{18.5} & \textbf{18.8} & \textbf{21.1} & - \\ \hline
Lorenz et al. \cite{lorenz2019unsupervised} & unsupervised & 3.91  &-&-& -& - \\ 
ULD \cite{zhang2018unsupervised, thewlis2017unsupervised} & unsupervised & - & \phantom{$\dagger$}30.1$\dagger$ & \phantom{$\dagger$}29.4$\dagger$ & \phantom{$\dagger$}28.2$\dagger$ & - \\
Zhang et al. \cite{zhang2018unsupervised} & unsupervised & 5.36 & \phantom{$\ddagger$}26.9$\ddagger$ & \phantom{$\ddagger$}27.6$\ddagger$ & \phantom{$\ddagger$}27.1$\ddagger$ & \phantom{$\ddagger$}22.4$\ddagger$ \\
LatentKeypointGAN \cite{he2021latentkeypointgan} & unsupervised & \phantom{$\ddagger$}5.21$\ddagger$ & \phantom{$\ddagger$}22.6$\ddagger$ & \phantom{$\ddagger$}29.1$\ddagger$ & \phantom{$\ddagger$}21.2$\ddagger$ & \phantom{$\ddagger$}14.7$\ddagger$ \\
GANSeg \cite{he2022ganseg} & unsupervised & \textbf{3.23} & \phantom{$\ddagger$}22.1$\ddagger$ & \phantom{$\ddagger$}22.3$\ddagger$ & \phantom{$\ddagger$}21.5$\ddagger$ & \phantom{$\ddagger$}12.1$\ddagger$\\ 
Ours (general) & unsupervised & 4.15 $\pm$ 0.24 & 20.6 $\pm$ 0.54 & 20.3 $\pm$ 0.96 & 19.7 $\pm$ 0.91 & 11.6 $\pm$ 0.33 \\
Ours (thickness-tuned) & unsupervised & 3.51 & \textbf{20.2} & \textbf{19.2} & \textbf{18.5} &\textbf{11.3} \\ \Xhline{1.5pt}
\end{tabular}
}
\label{tab:cub}
\end{table}












\subsection{Datasets and Evaluation Metrics} \label{sec:datasets}
\noindent\parag{CelebA-aligned} \cite{liu2015faceattributes} contains 200k celebrity faces aligned in center. We follow \cite{thewlis2017unsupervised} splitting it into three subsets: CelebA training set without MAFL (160k images), MAFL training set (19k), MAFL test set (1k). We train our network on the CelebA training set without MAFL. To quantitatively evaluate the consistency of our predicted keypoints, we follow \cite{thewlis2017unsupervised} training a linear regression without bias from our detected keypoints to the ground truth keypoints on the MAFL training set and reporting the mean $L_2$ error normalized by inter-ocular distance on the MAFL test set.

\noindent\parag{CelebA-in-the-wild} \cite{liu2015faceattributes} contains celebrity faces in unconstrained conditions. We follow \cite{hung2019scops} and first split it into three subsets as for CelebA-aligned, and then remove the images where a face covers less than 30\% of the area, which results in 45,609 images for model training, 5,379 with keypoint labels for regression, and 283 for testing. The evaluation metric is the same as CelebA-aligned.

\noindent\parag{Human3.6m} \cite{Ionescu2014human36m} contains human activity videos in static backgrounds. We follow \cite{zhang2018unsupervised} considering six activities (direction, discussion, posing, waiting, greeting, walking), and using subjects 1, 5, 6, 7, 8, 9 for training and 11 for testing. This results in 796,648 images for training and 87,975 images for testing. The evaluation metric is the regressed (without bias) mean $L_2$ error normalized by the image size. We remove the background as in \cite{zhang2018unsupervised, lorenz2019unsupervised} to make a fair comparison to others.
To underline the robustness against structured backgrounds we also report the numbers including background.

\noindent\parag{DeepFashion} \cite{liu2016deepfashion} contains 53k in-shop clothes images. We follow \cite{lorenz2019unsupervised} only keeping the full-body images. We use 10604 images for training and 1179 images for testing as in \cite{sandro2020unsupervised}. We use the keypoints generated by AlphaPose \cite{fang2017rmpe} as the ground truth. The evaluation metric is Percentage of Correct Keypoints of d=6 pixels in resolution $256\times 256$.

\noindent\parag{Taichi} \cite{Siarohin_2019_NeurIPS} contains 3049 training videos and 285 test
videos of people performing Tai-Chi, with the various appearance of foreground and background. We follow \cite{siarohin2021motion} using 5000 and 300 images (not contained in training data) for training a linear regression and for testing, respectively. The evaluation metric, mean average error (MAE), is calculated as the sum of the $L_2$ error in resolution $256\times 256$.

\noindent\parag{CUB-200-2011} \cite{WahCUB_200_2011} consists of 11,788 images of birds. We follow two established protocols \cite{lorenz2019unsupervised, choudhury2021unsupervised} to evaluate our method: 1) Images are cropped based on the bird landmarks, aligned to face to the left \cite{lorenz2019unsupervised}, and seabirds are removed; 2) Birds are cropped based on the given bounding box and the train/val/test split of \cite{choudhury2021unsupervised} is used. In both cases, the evaluation metric is the regressed (without bias) mean $L_2$ error normalized by the cropped image size.

\noindent\parag{Flower} \cite{Nilsback08}, \textbf{11k Hands} \cite{afifi201911kHands}, \textbf{Horses} \cite{zhu2017unpaired}, and \textbf{Zebras} \cite{zhu2017unpaired} are used for qualitative experiments. \textbf{VoxCeleb2} \cite{chung18voxceleb2} and \textbf{AFHQ} \cite{choi2020stargan} are used for pose transfer and conditional image generation, respectively. 
Horses and Zebras are extracted from the CycleGAN dataset \cite{zhu2017unpaired} by removing the images with multiple horses and aligning them to face left. Note that the horses and zebra are trained separately, yet the model learns similar structures. The train/test split of Flower follows \cite{chen2019unsupervised}. All the other datasets follow the train/test split specified by the dataset.

\subsection{Qualitative Analysis}

We qualitatively compare our detected keypoints with other methods and show the examples of the learned edges in Figure~\ref{fig:qualitative}. For visualization purposes, we scale the edge weights 
to obtain visible edges.
We use the same number of keypoints as the previous method \cite{he2022ganseg} for a fair comparison, which are 8 for CelebA-in-the-Wild, 16 for DeepFashion, and 10 for Taichi. We will discuss more on the choice of the number of keypoints in Ablation Study~\ref{sec:ablation}. As shown in Figure~\ref{fig:qualitative}, our model not only detects consistent keypoints but also learns reasonable edges, such as human skeletons in DeepFashion and Taichi. For example, the feet are clearly connected with the corresponding knees, and there is no edge between the left and right hands. We show 105 images with detected keypoints and visualized graph structure for each dataset in Figure~\ref{fig:gen_afhq_k32}-\ref{fig:taichi_k32} in the Appendix, demonstrating that our model works on various classes of objects of diverse appearance and complex backgrounds.


\subsection{Quantitative Analysis} \label{sec:quantitative}

We compare the keypoint detection results with other methods in Table~\ref{tab:celeba} (CelebA), Table~\ref{tab:human_body} (Human3.6, DeepFashion, Taichi), and Table~\ref{tab:cub} (CUB).
Our simple model outperforms all other unsupervised methods in all benchmarks, except \cite{hung2019scops} on three CUB subsets, which however requires saliency maps, and for the most constrained setting CelebA-aligned and CUB-aligned where all methods perform well and the results are comparable. 
The results on CelebA in Table~\ref{tab:celeba} confirms that our model is more robust to poses in the wild. Since self-supervised part segmentation methods are usually more robust on wild faces \cite{liu2021unsupervised, hung2019scops, he2022ganseg}, we also include them for comparison, demonstrating the robustness of our model over existing baselines.
The results on Human3.6m and DeepFashion in Table~\ref{tab:human_body} show the capability of our model to detect keypoints on human bodies of either similar or diverse appearances.
The result on Taichi in Table~\ref{tab:human_body} demonstrates the general applicability of our model to human bodies of diverse poses in complex backgrounds.

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1\textwidth]{images/ablation_kp_thick.png}
\end{center}
   \caption{\textbf{Ablation tests on the number of keypoints and edge thickness.} While the model shows better performance with more keypoints, it is robust to the edge thickness.}
\label{fig:ablation_kp_thick}
\end{figure}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1\textwidth]{images/ablation_sample.png}
\end{center}
   \caption{\textbf{Ablation tests on masking strategy.} Overall, the performance increases as the mask ratio increases. Too small or too large patch sizes can decrease the performance. Empirically, masking 80\% of the $16\times16$ patches is a golden rule.}
\label{fig:ablation_masking}
\end{figure}

\subsection{Ablation Tests} \label{sec:ablation}



In this section, we analyze the hyperparameters and demonstrate the robustness of our model. We also discuss the possible variants of our model and show the superiority of our design. 

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1\textwidth]{images/ablation_n_kp.jpg}
\end{center}
   \caption{\textbf{Examples of different numbers of detected keypoints.} With very few keypoints, the model only models a very basic shape, such as the box on the face, and sometimes cannot fully capture the structure. For example, with K=4, only legs are modeled on humans. With abundant keypoints, it is able to model details.}
\label{fig:ablation_kp}
\end{figure}



\parag{Number of Keypoints \& Thickness}. 
We show ablation test results on the different numbers of keypoints and edge thicknesses in Figure~\ref{fig:ablation_kp_thick}. The exact numbers can be found in Table~\ref{tab:hyper} in Appendix~\ref{supp:ablation_test}. Our model shows very strong robustness to edge thickness. The accuracy remains state-of-the-art while the thickness of the edges varies by multiple orders of magnitude.
On the other hand, with the increasing number of keypoints the accuracy increases. This is expected since more keypoints are able to capture structure in more detail, as shown in Figure~\ref{fig:ablation_kp}. Yet, some other methods fail for a large number of keypoints \cite{he2022ganseg}.

\parag{Masking Strategy}. 
In our standard setting, the image is divided into $16\times 16$ patches and 80\% of the patches are randomly masked. We investigate how the patch size and masking ratio affect the model performance. Figure~\ref{fig:ablation_masking} shows that a too low masking ratio enables the network to directly infer the structure from the masked image which is undesired in our case.
In these cases, the network would not choose to infer a set of compact keypoints from the original image. Figure~\ref{fig:ablation_masking} illustrates that the patch size cannot be too small ($4\times 4$) or too large ($64\times 64$). Although in some cases, such as CelebA-in-the-Wild, a different masking strategy gives better results (4.14 vs 5.24), we choose to report a unified strategy that we mask 80\% of $16\times16$ patches for simplicity and conciseness.

\begin{table}[t]
\centering
\caption{\textbf{Ablation tests on variants of edge heatmap generation}. The original design is proved to be the most robust one. Although in some cases it is not optimal, the difference is almost trivial.}
\resizebox{0.9\linewidth}{!}{\begin{tabular}{lcccc}
\Xhline{1.5pt}
Model & CelebA in The Wild $\downarrow$ & Human3.6m $\downarrow$ & DeepFashion $\uparrow$ & Taichi $\downarrow$\\ \Xhline{1.5pt}
original model & \textbf{5.24}\%  & \textbf{2.76}\% & 65.8\% & 316 \\ 
fixed $\alpha$ & 6.39\%  & 2.87\% & \textbf{66.0}\% & 374 \\
shared learnable thickness & 6.12\%  & 3.25\% & 49.1\% & 425 \\
independent learnable thickness & 5.94\%  & 3.73\% & 50.2\% & \textbf{311} \\
edge-specific heatmap & 5.65\%  & 3.83\% & 65.1\% & 407\\
only using keypoints without edges & 6.55\% & 3.58\% & 52.9\% & 722\\
\Xhline{1.5pt}
\end{tabular}
}
\label{tab:ablation_edge_gen}
\end{table}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1.\textwidth]{images/ablation_edge.pdf}
\end{center}
   \caption{(a) If we do not model the edges, the model may degenerate. (b) If we give different edges a different channel in the feature map, the model would give dummy edges. (c) If the model is trained on the dataset with structured background, the background would be modeled. However, the keypoints can be separated into two sets by spectral clustering. (d) failure case: left) the model cannot model the occlusion well; right) the model has left and right ambiguity.}
\label{fig:ablation_edge}
\end{figure}

\parag{Variants of Edge Heatmap Generation}. 
Besides the heatmap generation described in Method~\ref{sec:method}, we test four more ways to generate the edge maps: 1) we define the thickness as a globally learnable parameter; 2) we learn each edge thickness independently as a parameter; 3) we treat each edge heatmap as an independent channel of the feature map, instead of making them a single channel heatmap with Equation~\ref{eq:max_heatmap}; 4) instead of using the edge heatmap, we generate Gaussian heatmaps for the keypoints and use Equation~\ref{eq:max_heatmap} to combine them in a single channel heatmap. The results are listed in Table~\ref{tab:ablation_edge_gen}. Overall, these variants have worse performance. Although in some cases, the model has slightly better results on specific datasets, the performance boost does not hold in general. We observe that with only keypoints without edges, the model may degenerate, as shown in Figure~\ref{fig:ablation_edge}a. Interestingly, assigning each edge a different channel performs worse than simply combining all edges into a single channel. We believe it is caused by entangling the edge weights with the convolution kernel weights. As visualized in Figure~\ref{fig:ablation_edge}b, there exist dummy edges that do not model the object structure.
In addition, we tried to remove the learnable $\alpha$ in Equation~\ref{eq:recon}, fixing $\alpha=1$, but the overall performance decreases as shown in Table~\ref{tab:ablation_edge_gen}.



\parag{Does Texture Matter?} 
We trained two networks on horses and zebras separately. As shown in Figure~\ref{fig:teaser}a, the horse and the zebra share similar shape structures but only one is textured. The striped texture not having a significant impact on the learned structure shows that our model primarily learns the structure instead of texture features.

\parag{What if the model is trained on images with a structured background?} We tested on Human3.6m with a background, where all images are taken in a single room. The error is 5.02. As shown in Figure~\ref{fig:ablation_edge}c, our model captures the entrance in the background. It is expected since we assume the foreground object is structured. If we apply spectral clustering \cite{ng2001spectral} on the learned graph, the keypoints are clearly divided into two clusters, one for the room and one for the person. \section{Limitations and Future Work} \label{sec:limitation}
If the background is highly structured, the keypoints will appear on the background. Yet, 
we showed an avenue for future work, as already a simple 
graph clustering could separate the object from the background on the Human3.6M dataset. Similar to the previous 2D self-supervised methods \cite{lorenz2019unsupervised, siarohin2021motion, he2022ganseg}, our model cannot model occlusion well. We show in Figure~\ref{fig:ablation_edge}d left that the occluded right arm becomes the back when the person turns to the left. In addition, as for all other methods, the model cannot distinguish the left and right sides of the objects as shown in Figure~\ref{fig:ablation_edge}d middle and right. We believe it is necessary to model the structure in 3D to solve these problems.



\section{Conclusion}

We presented a simple approach for learning a spatial graph representation from unlabelled image collections by reconstructing masked images. The crucial part is our learnable graph design that models the relationship between different keypoints. 
It is simpler than existing alternatives 
and opens up a path for image understanding, image editing, and learning 3D models from 2D images. 
\section*{Acknowledgement}
This work was supported by the Compute Canada GPU servers, and a Huawei-UBC Joint Lab project.

\bibliographystyle{abbrvnat}
\bibliography{egbib}


\clearpage


\clearpage
\appendix

\section{More Results}  \label{supp:more_results}
We show supplemental videos in \url{https://xingzhehe.github.io/autolink/}.

To verify the robustness and generality, we show 105 images for each dataset we used in the main paper with keypoints and visualized graph representation in Figure~\ref{fig:gen_afhq_k32}-\ref{fig:taichi_k32} in the supplemental materials.


\section{Edge-Map Ablation Tests} \label{supp:ablation_test}

Table~\ref{tab:hyper} shows the results on the different numbers of keypoints and edge thickness. While a larger number of keypoints gives better details and thus higher accuracy, the performance is robust to the thickness.
Table~\ref{tab:mask_strategy} shows the results on different masking ratios and mask patch sizes. A too small masking ratio significantly decreases the performance since the structure can be directly extracted from the masked image with a low masking ratio. The thicknesses used in the main paper are those marked bold in Table~\ref{tab:hyper}. \looseness=-1

\begin{table}[h]
\centering
\resizebox{0.98\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{5}{c|}{CelebA-wild $\downarrow$} & \multicolumn{5}{c|}{Human3.6m $\downarrow$} & \multicolumn{5}{c|}{DeepFashion $\uparrow$} & \multicolumn{5}{c|}{Taichi $\downarrow$} \\ \hline
$\sigma^2$ &  K=2 & K=4  & K=8  & K=16 & K=32 &  K=2 &  K=4 & K=8  &K=16 &  K=32 &  K=2 &  K=4 &  K=8 & K=16 & K=32 & K=2 & K=4 & K=8 & K=16& K=32    \\ \cline{2-21}
    1.0e-5 & 60.9 & 52.5 & 42.6 & 34.9 & 31.9 & 5.67 & 5.13 & 3.37 & 2.90 & \textbf{2.81} & 17.8 & 48.1 & 58.1 & 62.6 & 63.8 & 667 & 657 & 622 & 592 & 458 \\
    2.5e-5 & 50.5 & 38.8 & 29.8 & 10.7 & 6.11 & 5.64 & 5.15 & 3.50 & 3.02 & 2.87 & 29.5 & 48.9 & 57.1 & 65.2 & 66.2 & 665 & 637 & 611 & 506 & 351 \\
    5.0e-5 & 49.4 & 8.06 & 5.41 & 4.88 & 4.65 & 6.10 & 5.03 & 3.76 & \textbf{2.76} & 2.91 & 22.5 & 50.4 & 58.7 & \textbf{65.8} & 66.6 & 654 & 550 & 338 & 338 & 287 \\ 
    7.5e-5 & 58.3 & 7.71 & 5.62 & 4.92 & 4.65 & 5.49 & 5.08 & \textbf{3.19} & 2.89 & 3.00 & 43.5 & 51.1 & \textbf{59.3} & 65.7 & \textbf{69.8} & 650 & 516 & 383 & 301 & 297 \\
    1.0e-4 & 54.5 & 7.44 & 5.71 & 4.93 & 4.64 & 5.56 & 5.09 & 3.25 & 2.96 & 2.96 & 44.5 & 49.0 & 58.1 & 64.1 & 67.6 & 647 & 512 & 385 & 329 & 280 \\
    2.5e-4 & 11.3 & 6.68 & 5.57 & 5.05 & 4.42 & 5.49 & 5.03 & 3.36 & 3.01 & 3.32 & 42.9 & 48.7 & 56.8 & 64.9 & 67.6 & 624 & 526 & 391 & 321 & 284 \\
    5.0e-4 & 11.4 & 6.56 & 5.77 & 5.01 & 4.43 & 5.49 & 5.10 & 3.45 & 2.94 & 3.16 & 42.0 & 47.5 & 58.0 & 65.4 & 67.2 & 612 & 531 & 381 & 307 & \textbf{275} \\
    7.5e-4 & \textbf{10.6} & \textbf{6.11} & 5.81 & 4.86 & \textbf{4.39} & 5.57 & 5.09 & 3.84 & 3.31 & 3.06 & 35.6 & 49.6 & 58.2 & 65.7 & 66.4 & 604 & 479 & 363 & 296 & 289 \\
    1.0e-3 & 13.3 & 6.84 & 5.70 & \textbf{4.43} & 4.50 & 5.49 & 5.04 & 3.42 & 3.47 & 3.18 & 40.4 & 51.0 & 57.1 & 64.6 & 68.8 & 608 & 462 & 342 & 306 & 275 \\
    2.5e-3 & 15.8 & 6.70 & \textbf{5.24} & 4.72 & 4.49 & 5.48 & 5.04 & 3.40 & 3.37 & 3.01 & 43.6 & \textbf{51.6} & 55.8 & 61.2 & 66.7 & 609 & \textbf{442} & 326 & \textbf{289} & 286 \\
    5.0e-3 & 12.6 & 6.29 & 5.53 & 4.69 & 4.48 & 5.52 & 5.03 & 3.59 & 3.36 & 3.12 & \textbf{44.7} & 50.3 & 56.7 & 62.3 & 68.3 & 598 & 510 & 333 & 323 & 302 \\
    7.5e-3 & 11.6 & 6.16 & 5.61 & 4.76 & 4.41 & 5.54 & 5.06 & 3.50 & 3.77 & 2.99 & 39.4 & 48.6 & 57.7 & 62.6 & 65.5 & 604 & 514 & \textbf{325} & 328 & 340 \\
    1.0e-2 & 11.2 & 6.22 & 6.66 & 4.89 & 4.47 & \textbf{5.45} & \textbf{5.02} & 3.39 & 3.05 & 2.94 & 44.6 & 50.3 & 56.5 & 60.9 & 65.1 & \textbf{593} & 515 & 327 & 327 & 341 \\ \hline
\end{tabular}
}
\caption{\textbf{Ablation Tests on the numbers of keypoints and edge thickness}. We remove the \% sign in metrics for simplicity. The best one for each number of keypoints is marked in bold.} 
\label{tab:hyper}
\end{table}

\begin{table}[h]
\centering
\resizebox{0.98\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{5}{c|}{CelebA-wild $\downarrow$} & \multicolumn{5}{c|}{Human3.6m $\downarrow$} & \multicolumn{5}{c|}{DeepFashion $\uparrow$} & \multicolumn{5}{c|}{Taichi $\downarrow$} \\ \hline
ratio & 4x4  & 8x8  & 16x16& 32x32& 64x64& 4x4  & 8x8  & 16x16& 32x32& 64x64& 4x4  & 8x8  & 16x16& 32x32& 64x64& 4x4 & 8x8 &16x16&32x32& 64x64    \\ \cline{2-21}
10\%  & 17.9 & 16.4 & 49.2 & 49.3 & 48.3 & 3.22 & 3.25 & 3.55 & 5.28 & 6.39 & 58.3 & 39.7 & 40.5 & 52.1 & 51.0 & 640 & 638 & 630 & 659 & 642 \\
20\%  & 19.7 & 29.4 & 44.6 & 43.3 & 39.8 & 3.06 & 3.52 & 3.21 & 4.29 & 5.99 & 62.1 & 55.5 & 43.4 & 39.6 & 49.3 & 522 & 605 & 614 & 638 & 613 \\
30\%  & 6.56 & 8.46 & 45.4 & 41.3 & 46.8 & 3.08 & 3.08 & 3.44 & 3.80 & 6.24 & 62.8 & 58.9 & 62.8 & 42.4 & 41.8 & 509 & 570 & 627 & 642 & 627 \\ 
40\%  & 8.31 & 6.71 & 7.58 & 6.31 & 6.81 & 3.15 & 2.91 & 3.38 & 3.40 & 4.19 & 62.5 & 59.2 & 63.4 & 41.2 & 40.0 & 500 & 428 & 634 & 640 & 651 \\
50\%  & 8.06 & 6.39 & 6.23 & 6.58 & 5.99 & 3.73 & 2.99 & 2.94 & 3.72 & 4.24 & 61.3 & 65.4 & 64.3 & 41.1 & 38.9 & 444 & 452 & 481 & 640 & 667 \\
60\%  & 7.69 & 6.22 & 5.52 & 6.98 & 5.56 & 3.11 & 3.13 & 2.95 & 3.35 & 4.87 & 62.8 & 62.0 & 61.7 & 41.4 & 39.9 & 483 & 396 & 418 & 654 & 643 \\
70\%  & 7.03 & 6.38 & 5.44 & 5.11 & 4.43 & 2.95 & 3.09 & 2.87 & 3.28 & 4.85 & 60.7 & 63.2 & 63.1 & 59.7 & 41.5 & 447 & 362 & 376 & 523 & 656 \\
80\%  & 6.95 & 6.68 & 5.24 & 4.73 & 4.65 & 3.63 & 3.47 & 2.76 & 2.97 & 4.08 & 58.3 & 61.7 & 65.8 & 61.6 & 39.6 & 501 & 371 & 316 & 347 & 642 \\
90\%  & 7.24 & 7.15 & 5.77 & 5.62 & 4.14 & 2.99 & 3.18 & 2.95 & 3.64 & 3.75 & 60.9 & 63.1 & 66.4 & 62.7 & 40.2 & 626 & 388 & 330 & 346 & 526 \\ \hline
\end{tabular}
}
\caption{\textbf{Ablation Tests on masking ratio and patch size}. We remove the \% sign in metrics for simplicity. The best one for each number of keypoints is marked in bold.} 
\label{tab:mask_strategy}
\end{table}


\section{Applications}
In this section, we briefly describe how we created the two applications, conditional Generative Adversarial Networks and pose transfer networks, based on the learned graph representation, as shown in the teaser. Note that, although we apply the graph representation to videos for pose transfer, it is only learned from the collections of single images.

\subsection{Conditional Generative Adversarial Network}
The conditional GAN is a simplified StyleGAN2 \cite{karras2020analyzing}, where the spatial noise injection is removed and the starting tensor is replaced by the feature map generated from the edge map. For simplicity, we do not use EqualLinear \cite{karras2019style} or Path Regularization \cite{karras2019style}.

Formally speaking, given an edge map $\mS\in\R^{H\times W}$, where $H, W$ are the spatial size, we use bicubic interpolation to downsample it to $32\times 32$ and feed it into a two-layer convolution network to generate a feature map $\mF\in\R^{8\times 8\times 512}$. In parallel, we generate the embedding vector $\vw\in\R^{256}$ from a noise vector $\vz\in\R^{256}$ by a three linear-layer MLP. The feature map $\mF$ is fed into a residual convolution-based generator where the kernel weights are modulated by the embedding vector $\vw$ \cite{karras2020analyzing}. The final image $\vx\in\R^{H\times W\times 3}$ is generated by a single convolution layer.

We denote $\cG$ as the generator and $\cD$ as the discriminator. We use the non-saturating loss \cite{goodfellow2014generative},
\begin{equation}
    \cL_\text{GAN}(\cG)=\mathbb E_{\vz\sim\cN}\log(\exp(-\cD(\cG(\vz)))+1)
\label{eq:gen_loss}
\end{equation}
for the generator, and logistic loss,
\begin{equation}
        \cL_\text{GAN}(\cD)=\mathbb E_{\vz\sim\cN}\log(\exp(\cD(\cG(\vz)))+1)+\mathbb E_{\vx\sim p_\text{data}}\log(\exp(-\cD(\vx))+1)
    \label{eq:dis_loss}
\end{equation}
for the discriminator, with gradient penalty~\cite{mescheder2018training} applied only on real data,
\begin{equation}
    \cL_\text{gp}(\cD)=\mathbb E_{\vx\sim p_\text{data}}\nabla\cD(\vx).
\label{eq:gradient penalty}
\end{equation}

\subsection{Pose Transfer Network}
We train the pose transfer network on videos. We randomly sample two frames $\mI_1, \mI_2\in\R^{H\times W\times 3}$, where $H,W$ are the spatial size. We use keypoint detector on the frame $\mI_1$ and generate the edge map $\mS_1\in\R^{H\times W}$. The edge map is fed into a UNet \cite{ronneberger2015u} to reconstruct $\mI_1$. The smallest feature map in the UNet is concatenated by a feature map of the appearance information, which is generated by frame $\mI_2$ to provide the appearance information. The loss is a combination of Mean Squared Error, VGG perceptual loss \cite{johnson2016perceptual} and the GAN loss in Eq~\ref{eq:gen_loss}, \ref{eq:dis_loss}, and \ref{eq:gradient penalty}.

\section{Network Architecture} \label{supp:archi}
Figure~\ref{fig:net_archi} shows the architectures we used in the main paper. 
The keypoint detector is a ResNet with upsampling \cite{xiao2018simple}, which is a simple baseline used in human pose estimation.
The decoder and the pose transfer network are UNets \cite{ronneberger2015u}.
The conditional GAN is a simplified StyleGAN2 \cite{karras2020analyzing}, where the spatial noise injection is removed and the starting tensor is replaced by the feature map generated from the edge map.
In Figure~\ref{fig:net_archi}, we denote Conv for 3x3 convolution \cite{lecun1989backpropagation}, BN for Batch Normalization \cite{ioffe2015batch}, LReLU for Leaky ReLU \cite{maas2013rectifier}, Up for 2x bilinear upsampling, and Down for 2x bilinear downsampling. 

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.98\textwidth]{images/network_architecture.pdf}
\end{center}
    \caption{\textbf{Network architectures. From left to right: detector (encoder), decoder, conditional GAN, conditional autoencoder.}
    The shortcuts in (a) and (c) are addition while the shortcuts in (b) and (d) are concatenation.
}
\label{fig:net_archi}
\end{figure}

\section{Edge Map Visualization}
For visualization purposes, we scale the edge weights by dividing the maximum value to obtain visible edges. For DeepFashion, before dividing the maximum value, we further add 0.01 to the edge weights larger than 0.0001. Although the models are trained on different thicknesses, we draw them in the same thickness $\sigma^2=5\times 10^{-4}$ for pleasing visualization.
 \begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/afhq_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from AFHQ (32 keypoints),} with the image-points-edge pairs overlaid.}
   \label{fig:gen_afhq_k32}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/celeba_wild.jpg}
\end{center}
   \caption{\textbf{105 samples from CelebA-in-The-Wild (32 keypoints),} with the image-points-edge pairs overlaid.}
   \label{fig:gen_face_k32}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/cub_align.jpg}
\end{center}
   \caption{\textbf{105 samples from CUB-aligned (10 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/cub_k4.jpg}
\end{center}
   \caption{\textbf{105 samples from CUB (4 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/deepfashion_k16.jpg}
\end{center}
   \caption{\textbf{105 samples from DeepFashion (16 keypoints),} with the image-points-edge pairs overlaid.}
   \label{fig:gen_deepfashion_k32}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/deepfashion_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from DeepFashion (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/flower_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Flower (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/h36m_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Human3.6m (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/h36mwobg_k16.jpg}
\end{center}
   \caption{\textbf{105 samples from Human3.6m without background (16 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/h36mwobg_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Human3.6m without background (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/hand_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from 11k Hands without background (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/horse_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Horses (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/zebra_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Zebra (32 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/taichi_k10.jpg}
\end{center}
   \caption{\textbf{105 samples from Taichi (10 keypoints),} with the image-points-edge pairs overlaid.}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{images/examples/taichi_k32.jpg}
\end{center}
   \caption{\textbf{105 samples from Taichi (32 keypoints),} with the image-points-edge pairs overlaid.}
   \label{fig:taichi_k32}
\end{figure*}
 

\end{document}