
\section{Experiments}
We first carry out experiments on the Cityscapes~\cite{Cityscapes} dataset, which is comprised of a large set of high-resolution  images in street scenes. This dataset has 5,000 images with high quality pixel-wise annotations for 19 classes, which is further divided into 2975, 500, and 1525 images for training, validation and testing. To be noted, coarse data are not used in this work. Besides, more experiments on Pascal Context~\cite{VOC}, ADE20K~\cite{ADE20K} and CamVid~\cite{CamVid} are summarised to further prove the generality of our method. 

\subsection{Experiments on Cityscapes}

\noindent
\textbf{Implementation details:} We use PyTorch~\cite{pytorch} framework to carry out following experiments. All networks are trained with the same setting, where stochastic gradient descent (SGD) with batch size of 16 is used as optimizer, with momentum of 0.9 and weight decay of 5e-4. All models are trained for 50K iterations with an initial learning rate of 0.01. As a common practice, the ``poly'' learning rate policy is adopted to decay the initial learning rate by multiplying  during training. Data augmentation contains random horizontal flip, random resizing with scale range of , and random cropping with crop size of . During inference, we use the whole picture as input to report performance unless explicitly mentioned. For quantitative evaluation, mean of class-wise intersection-over-union (mIoU) is used for accurate comparison, and number of float-point operations (FLOPs) and frames per second (FPS) are adopted for speed comparison.

\noindent
\textbf{Comparison with baseline methods: } Table~\ref{tab:city_ablation_architecture}(a) reports the  comparison results against baselines on the validation set of Cityscapes~\cite{Cityscapes}, where ResNet-18~\cite{resnet} serves as the backbone. Comparing with the naive FCN, dilated FCN improves mIoU by 1.1\%.  By appending the FPN decoder to the naive FCN, we get 74.8\% mIoU by an improvement of 3.2\%. By replacing bilinear upsampling with the proposed FAM, mIoU is boosted to 77.2\%, which improves the naive FCN and FPN decoder by 5.7\% and 2.4\% respectively. Finally, we append PPM (Pyramid Pooling Module)~\cite{pspnet} to capture global contextual information, which achieves the best mIoU of 78.7 \% together with FAM. Meanwhile, FAM is complementary to PPM by observing FAM improves PPM from 76.6\% to 78.7\%.

\begin{table}[!t]
	\centering
\begin{minipage}{\textwidth}
	    \begin{minipage}{\dimexpr.45 \linewidth}		
			\centering
			\resizebox{0.75\textwidth}{!}{\begin{tabular}{l c c c}
				\toprule[0.2em]
				Method &Stride & mIoU () &  \\
				\toprule[0.2em]
					FCN& 32 & 71.5  & -   \\
					Dilated FCN&8 & 72.6 & 1.1  \\
				\midrule
					+FPN &32 & 74.8 & 3.3  \\
					+FAM &32 & 77.2 & 5.7  \\ 
					+FPN + PPM &32 & 76.6 & 5.1  \\
					+FAM + PPM&32  & \textbf{78.7} & 7.2  \\
				\bottomrule[0.1em]
			    \end{tabular}
			}\par
			{(a) Ablation study on baseline model.}
		\end{minipage}
\begin{minipage}{\dimexpr.45 \linewidth}
			\centering
			\resizebox{0.75\textwidth}{!}{\begin{tabular}{c c c c l l}
			\toprule[0.2em]
			Method &  &  &  & mIoU(\%) &  \\  
			\toprule[0.2em]
			FPN+PPM &  &  &  & 76.6 & -  \\ 
			& \checkmark & & & 76.9 & 0.3  \\
			& & \checkmark & & 77.0 & 0.4  \\
			& & & \checkmark & 77.5 & 0.9  \\
			\midrule
			\midrule
			& &\checkmark &\checkmark & 77.8 & 1.2 \\
			&\checkmark &\checkmark &\checkmark &78.3& 1.7 \\
			\hline
		\end{tabular}
			}\par
	
			{ (b) Ablation study on insertion position.}
			
		\end{minipage}


	\end{minipage}
	
\begin{minipage}{\textwidth}
		\centering
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.75\textwidth}{!}{\begin{tabular}{l l l l }
				\toprule[0.2em]
				Method & mIoU(\%) &  & \#GFLOPs \\  
				\toprule[0.2em]
				FAM & 76.4  & -  & - \\
				+PPM~\cite{pspnet} & 78.3 & 1.9 & 123.5 \\
				+NL~\cite{non_local} & 76.8 & 0.4 & 148.0 \\
				+ASPP~\cite{deeplabv3} & 77.6 & 1.2 & 138.6 \\
				+DenseASPP~\cite{denseaspp} & 77.5 & 1.1 & 141.5 \\
				\hline
			\end{tabular}
			}\par
			{(c) Ablation study on context module.}
			
		\end{minipage}	
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.65\textwidth}{!}{\begin{tabular}{l c c c}
				\toprule[0.2em]
				Backbone & mIoU(\%) &  & \#GFLOPs \\  
				\toprule[0.2em]
				ResNet-50~\cite{resnet}& 76.8  & - & 332.6 \\
				w/ FAM & 79.2 & 2.4 & 337.1 \\ 
				ResNet-101~\cite{resnet}& 77.6 & - & 412.7 \\
				w/ FAM  & 79.8 & 2.2 & 417.5 \\
				\midrule
				ShuffleNetv2~\cite{shufflenetv2} & 69.8 & - & 17.8 \\
				w/ FAM  & 72.1 & 2.3 & 18.1 \\
				DF1~\cite{DF-seg-net} & 72.1 & - & 18.6 \\
				w/ FAM  & 74.3 & 2.2  & 18.7 \\
				DF2~\cite{DF-seg-net} & 73.2 & - & 48.2\\
				w/ FAM  & 75.8 & 2.6  & 48.5 \\
				\bottomrule[0.1em]
			\end{tabular}
			}\par
			{(d) Ablation on study on various backbones.}
		\end{minipage}
	\end{minipage}

\caption{Experiments results on network design using Cityscapes validation set.}
\label{tab:city_ablation_architecture}
\end{table}




\noindent
\textbf{Positions to insert FAM:} We insert FAM to different stage positions in the FPN decoder and report the results as Table~\ref{tab:city_ablation_architecture}(b). From the first three rows, FAM improves all stages and gets the greatest improvement at the last stage, which demonstrate that misalignment exists in all stages on FPN and is more severe in coarse layers. This is consistent with the fact that coarse layers containing stronger semantics but with lower resolution, and can greatly boost segmentation performance when they are appropriately upsampled to high resolution. The best result is achieved by adding FAM to all stages in the last row. Note that, for fast speed, we adopt FAMs only in the adjacent feature pyramids.

\noindent
\textbf{Ablation study on network architecture design: } \label{sec:ablation}
Considering current state-of-the-art contextual modules are used as heads on dilated backbone networks~\cite{deeplabv3, Co-Occurrent, DAnet, pspnet, psanet, denseaspp}, we further try different contextual heads in our methods where coarse feature map is used for contextual modeling. Table~\ref{tab:city_ablation_architecture}(c) reports the comparison results, where PPM~\cite{pspnet} delivers the best result, while more recently proposed methods such as Non-Local based heads~\cite{non_local} perform worse. Therefore, we choose PPM as our contextual head considering its better performance with lower computational cost. We further carry out experiments with different backbone networks including both deep and light-weight networks, where FPN decoder with PPM head is used as a strong baseline in Table~\ref{tab:city_ablation_architecture}(d). For heavy networks, we choose ResNet-50 and ResNet-101~\cite{resnet} as representation. For light-weight networks, ShuffleNetv2~\cite{shufflenetv2} and DF1/DF2~\cite{DF-seg-net} are employed. FAM significantly achieves better mIoU on all backbones with slightly extra computational cost.

\begin{table}[!t]
	\centering
\begin{minipage}{\textwidth}
	    \begin{minipage}{\dimexpr.45 \linewidth}		
			\centering
			\resizebox{0.65\textwidth}{!}{\begin{tabular}{l c}
				\toprule[0.2em]
				Method  & mIoU ()  \\
				\toprule[0.2em]
					bilinear upsampling & 78.3 \\
					deconvolution & 77.9 \\ 
					nearest neighbor & 78.2 \\
 				\bottomrule[0.1em]
			    \end{tabular}
			}\par
			{(a) Ablation study on Upsampling operation in FAM.}
		\end{minipage}
\begin{minipage}{\dimexpr.45 \linewidth}
			\centering
			\resizebox{0.50\textwidth}{!}{\begin{tabular}{l c c}
				\toprule[0.2em]
				Method  & mIoU () & Gflops\\
				\toprule[0.2em]
				     & 77.8 & 120.4 \\
				     & 78.3 & 123.5 \\
				     & 78.1 & 131.6 \\
                     & 78.0 & 140.5 \\
				\bottomrule[0.1em]
			    \end{tabular}
			}\par
			{ (b) Ablation study on kernel size  in FAM where 3 FAMs are involved.}
			
		\end{minipage}


	\end{minipage}
	
\begin{minipage}{\textwidth}
		\centering
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.65\textwidth}{!}{\begin{tabular}{l c c c}
				\toprule[0.2em]
				Method  & mIoU () &  \\
				\toprule[0.2em]
					FPN +PPM  & 76.6  & -   \\
				\midrule
				    correlation~\cite{FlowNet} & 77.2 & 0.6  \\
					Ours  & 77.5 & 0.9  \\
				\bottomrule[0.1em]
			    \end{tabular}
			}\par
			{(c) Ablation with FlowNet-C~\cite{FlowNet} in FAM.}
			
		\end{minipage}	
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.60\textwidth}{!}{\begin{tabular}{c c c c l l}
			\toprule[0.2em]
			Method &  &  &  & mIoU(\%) &  \\  
			\toprule[0.2em]
			FPN +PPM & -  & - & - &  76.6 & - \\
			DCN & &  &\checkmark & 76.9 & 0.3  \\
			Ours & &  &\checkmark & 77.5 & 0.9  \\
			\midrule
			\midrule
		    DCN	&\checkmark & \checkmark & \checkmark & 77.2 & 0.6 \\
			Ours &\checkmark &\checkmark & \checkmark & 78.3 & 1.7 \\
			\hline
		\end{tabular}
			}\par
			{(d) Comparison with DCN~\cite{deformable}.}
		\end{minipage}
	\end{minipage}

\caption{Experiments results on FAM design using Cityscapes validation set.}
\label{tab:city_ablation_FAM}
\end{table}



\noindent
\textbf{Ablation study on FAM design:} We first explore the effect of upsampling in FAM in Table~\ref{tab:city_ablation_FAM}(a). Replacing the bilinear upsampling with deconvolution and nearest neighbor upsampling achieves 77.9 mIoU and 78.2 mIoU, respectively, which are similar to the 78.3 mIoU achieved by bilinear upsampling. We also try the various kernel size in Table~\ref{tab:city_ablation_FAM}(b). Larger kernel size of  is also tried which results in a similar (78.2) but introduces more computation cost. In Table~\ref{tab:city_ablation_FAM}(c), replacing FlowNet-S with correlation in FlowNet-C also leads to slightly worse results (77.2) but increases the inference time. The results show that it is enough to use lightweight FlowNet-S for aligning feature maps in FPN. In Table~\ref{tab:city_ablation_FAM}(d), we compare our results with DCN~\cite{deformable}. We apply DCN on the concatenated feature map of bilinear upsampled feature map and the feature map of next level. We first insert one DCN in higher layers  where our FAM is better than it. After applying DCN to all layers, the performance gap is much larger. This denotes our method can also align low level edges for better boundaries and edges in lower layers, which will be shown in visualization part.

\noindent
\textbf{Aligned feature representation:} In this part, we give more visualization on aligned feature representation as shown in Figure~\ref{fig:vis_align_fea}. We visualize the upsampled feature in the final stage of ResNet-18. It shows that compared with DCN~\cite{deformable}, our FAM feature is more structural and has much more precise objects boundaries which is consistent with the results in Table~\ref{tab:city_ablation_FAM}(d). That indicates FAM is \textbf{not} an attention effect on feature similar to DCN, but actually aligns feature towards more precise shape as compared in red boxes. 

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.75\linewidth]{fig/sf_dcn_fea.pdf}
	\caption{
	Visualization of the aligned feature. Compared with DCN, our module outputs more structural feature representation. 
	}
	\label{fig:vis_align_fea}
\end{figure*}


\begin{figure*}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig/eccv_sf_flow.pdf}
	\caption{Visualization of the learned semantic flow fields. Column (a) lists three exemplary images. Column (b)-(d) show the semantic flow of the three FAMs in an ascending order of resolution during the decoding process, following the same color coding of Figure~\ref{fig:issue}. Column (e) is the arrowhead visualization of flow fields in column (d). Column (f) contains the segmentation results.
	}
	\label{fig:vis_flowfield}
\end{figure*}


\noindent
\textbf{Visualization of Semantic Flow: } 
Figure~\ref{fig:vis_flowfield} visualizes semantic flow from FAM in different stages. Similar with optical flow, semantic flow is visualized by color coding and is bilinearly interpolated to image size for quick overview. Besides, vector fields are also visualized for detailed inspection. From the visualization, we observe that semantic flow tends to diffuse out from some positions inside objects, where these positions are generally near object centers and have better receptive fields to activate top-level features with pure, strong semantics. Top-level features at these positions are then propagated to appropriate high-resolution positions following the guidance of semantic flow. In addition, semantic flows also have coarse-to-fine trends from top level to bottom level, which phenomenon is consistent with the fact that semantic flows gradually describe offsets between gradually smaller patterns.

\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig/eccv_sf_res.pdf}
	\caption{(a), Qualitative comparison in terms of errors in predictions, where correctly predicted pixels are shown as black background while wrongly predicted pixels are colored with their groundtruth label color codes.
	(b), Scene parsing results comparison against PSPNet~\cite{pspnet}, where significantly improved regions are marked with red dashed boxes. Our method performs better on both small scale and large scale objects.
	}
	\label{fig:error_map}
\end{figure}


\noindent
\textbf{Visual Improvement analysis:} Figure~\ref{fig:error_map}(a) visualizes the prediction errors by both methods, where FAM considerably resolves ambiguities inside large objects (e.g., truck) and produces more precise boundaries for small and thin objects (e.g., poles, edges of wall). Figure\ref{fig:error_map} (b) shows our model can better handle the small objects with shaper boundaries than dilated PSPNet due to the alignment on lower layers.


\iffalse
\begin{table*}[!t]\setlength{\tabcolsep}{2pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.65}{
			\begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c}
		   	\toprule[0.2em]
			Method & road & swalk & build & wall & fence & pole & tlight & sign & veg. & terrain & sky & person & rider & car & truck & bus & train & mbike & bike & mIoU \\
			\toprule[0.2em]
			BaseLine & 98.1 & 84.9 & 92.6 & 54.8 & 62.2 & 66.0 & 72.8 & 80.8 & 92.4 & 60.6 & 94.8 & 83.1 & 66.0 & 94.9 & 65.9 & 83.9 & 70.5 & 66.0 & 78.9 & 77.6 \\ 
			\hline
			w/ FAM & 98.3 & 85.9 & 93.2 & 62.2 & 67.2 & 67.3 & 73.2 & 81.1 & 92.8 & 60.5 & 95.6 & 83.2 & 65.0 & 95.7 & 84.1 & 89.6 & 75.1 & 67.7 & 78.8 & 79.8 \\
			\hline
		\end{tabular}
		}
		\caption{Quantitative per-category comparison results on Cityscapes validation set, where ResNet-101 backbone with the FPN decoder and PPM head serves as the strong baseline.}
		\label{tab:Detailed}
	\end{threeparttable}
\end{table*}
\fi



\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.65}{
			\begin{tabular}{l c c c c}
				\toprule[0.2em]
				Method  &   InputSize & mIoU () & \#FPS & \#Params \\
				\toprule[0.2em]
				ENet~\cite{ENnet} &   &  58.3  & 60 & 0.4M \\
				ESPNet~\cite{ESPNet} &  & 60.3 & 132 & 0.4M \\
				ESPNetv2~\cite{ESPNetv2} &   & 62.1 & 80 & 0.8M \\
				ERFNet~\cite{ERFNet} & & 69.7 & 41.9 & - \\
				BiSeNet(ResNet-18)~\cite{bisenet} &   &   & 43 & 12.9M  \\
				BiSeNet(Xception-39)~\cite{bisenet} &   &   & 72 & 5.8M  \\ 
				ICNet~\cite{ICnet} &  &  69.5 & 34 & 26.5M \\
				DF1-Seg~\cite{DF-seg-net} &  & 73.0 & 80 & 8.55M  \\
				DF2-Seg~\cite{DF-seg-net} &  & 74.8 & 55 & 8.55M  \\
				SwiftNet~\cite{swiftnet} &   & 75.5 & 39.9 & 11.80M \\
				SwiftNet-ens~\cite{swiftnet} &   & 76.5 & 18.4 & 24.7M \\
				DFANet~\cite{dfanet} &  & 71.3 & 100 & 7.8M \\
				CellNet~\cite{custom_search_seg} &  & 70.5  & 108 & -\\
				\midrule
				SFNet(DF1) &  &  & 74/\underline{121} & 9.03M \\
				SFNet(DF2) &  &  & 53/\underline{61} & 10.53M \\
				SFNet(ResNet-18) &  &  & 18/\underline{26} & 12.87M \\
				SFNet(ResNet-18)\textdagger &  &  & 18/\underline{26} & 12.87M \\
				\bottomrule[0.1em]
			\end{tabular}
		}
		\begin{tablenotes}
			 \item {\scriptsize \textdagger Mapillary dataset used for pretraining.
			 }
		\end{tablenotes}
		\caption{Comparison on Cityscapes {\it test} set with state-of-the-art real-time models.  For fair comparison, input size is also considered, and all models use single scale inference. }
		\label{table:cityscapes_sota_speed_acc}
	\end{threeparttable}
\end{table}


\noindent
\textbf{Comparison with real-time models:} All compared methods are evaluated by single-scale inference and input sizes are also listed for fair comparison. Our speed is tested on one GTX 1080Ti GPU with full image resolution  as input, and we report speed of two versions, i.e., without and with TensorRT acceleration. As shown in Table~\ref{table:cityscapes_sota_speed_acc}, our method based on DF1 achieves a more accurate result(74.5\%) than all methods faster than it. With DF2, our method outperforms all previous methods while running at 60 FPS. With ResNet-18 as backbone, our method achieves 78.9\% mIoU and even reaches performance of accurate models which will be discussed in the next experiment. By additionally using Mapillary~\cite{mapillary} dataset for pretraining, our ResNet-18 based model achieves 26 FPS with 80.4\% mIoU, which sets the new state-of-the-art record on accuracy and speed trade-off on Cityscapes benchmark. More detailed information are in the supplementary file. 


\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
		\begin{threeparttable}
		\scalebox{0.65}{
			\begin{tabular}{l c c c c}
				\toprule[0.2em]
				Method  & Backbone & mIoU () & \#Params & \#GFLOPs\tnote{\textdagger} \\
				\toprule[0.2em]
				SAC~\cite{sac}  &  ResNet-101  &   & - & - \\
				DepthSeg~\cite{depthseg} &   ResNet-101  &   & -  & - \\ 
				PSPNet~\cite{pspnet}  &   ResNet-101  &   & M &  \\ 
				BiSeNet~\cite{bisenet} & ResNe-18 & 77.7 & 12.3M & 82.2 \\
				BiSeNet~\cite{bisenet} &   ResNet-101  &   & M &  \\ 
				DFN~\cite{dfn} &  ResNet-101  &   & M & 1121.0 \\ 
				PSANet~\cite{psanet} &   ResNet-101  &   & 85.6M & 1182.6 \\ 
				DenseASPP~\cite{denseaspp}  &  DenseNet-161  &   & M & \\
				SPGNet~\cite{SPGNet} & 2ResNet-50 & 81.1 & - & -\\
				ANNet~\cite{annet} & ResNet-101 & 81.3 & 63.0M & 1089.8 \\ 
				CCNet~\cite{ccnet} & ResNet-101 & 81.4 & 66.5M & 1153.9 \\
				DANet~\cite{DAnet} & ResNet-101 & 81.5 & 66.6M & 1298.8\\ 
				\midrule
				SFNet &  ResNet-18 &  & \textbf{M} & \\
				SFNet &  ResNet-101 &  & \textbf{M} &  \\
				\bottomrule[0.1em]			\end{tabular}
		}
		\begin{tablenotes}
			\item[\textdagger] {\scriptsize \#GFLOPs calculation adopts   image as input.}
		\end{tablenotes}
		\caption{Comparison  on Cityscapes {\it test} set with state-of-the-art accurate models. For better accuracy, all models use multi-scale inference. }
		\label{table:cityscapes_sota_acc}
	\end{threeparttable}
\end{table}

\noindent
\textbf{Comparison with accurate models: } 
State-of-the-art accurate models~\cite{DAnet,pspnet,denseaspp,nvidia_seg_video} perform multi-scale and horizontal flip inference to achieve better results on the Cityscapes test server. For fair comparison, we also report multi-scale with flip testing results following previous methods~\cite{pspnet,DAnet}. Model parameters and computation FLOPs are also listed for comparison. Table~\ref{table:cityscapes_sota_acc} summarizes the results, where our models achieve state-of-the-art accuracy while costs much less computation. In particular, our method based on ResNet-18 is 1.1\% mIoU higher than PSPNet~\cite{pspnet} while only requiring \textbf{11\%} of its computation. Our ResNet-101 based model achieves better results than DAnet~\cite{DAnet} by 0.3\% mIoU and only requires \textbf{30\%} of its computation.


\subsection{Experiment on More Datasets}
 We also perform more experiments on other three data-sets including Pascal Context~\cite{pcontext-data}, ADE20K~\cite{ADE20K} and CamVid~\cite{CamVid} to further prove the effectiveness of our method. More detailed setting can be found in the supplemental file.

\begin{table}[!t]
	\centering
\begin{minipage}{\textwidth}
		\centering
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.85\textwidth}{!}{\begin{tabular}{l c c c }
			\toprule[0.2em]
			Method & Backbone & mIoU (\%) & \#GFLOPs\tnote{\textdagger} \\
			\toprule[0.2em]
			Ding~\textit{et al.}~\cite{ding2018context} & ResNet-101  & 51.6 & - \\
			EncNet~\cite{context_encoding} &ResNet-50   & 49.2 & - \\
			EncNet~\cite{context_encoding} &ResNet-101   &51.7 & - \\
			DANet~\cite{DAnet}&ResNet-50  & 50.1  &  186.4 \\
			DANet~\cite{DAnet}&ResNet-101  & 52.6  & 257.1 \\
			ANNet~\cite{annet} & ResNet-101 & 52.8 & 243.8 \\
			BAFPNet~\cite{BAFPNet} & ResNet-101 & 53.6 & - \\
			EMANet~\cite{emanet} & ResNet-101 & 53.1 & 209.3 \\
			\hline 
			w/o FAM & ResNet-50 & 49.0 & 74.5 \\
			SFNet & ResNet-50   & \textbf{50.7}(1.7 ) & 75.4 \\
			w/o FAM & ResNet-101 & 51.1 & 92.7\\
			SFNet & ResNet-101  & \textbf{53.8}(2.7 )& 93.6 \\
			\bottomrule[0.1em]
			\end{tabular}
			}\par
			{(a) Results on Pascal Context. Evaluated on 60 classes. }
			
		\end{minipage}	
		\begin{minipage}{\dimexpr.45 \linewidth}
		    \centering
			\resizebox{0.85\textwidth}{!}{\begin{tabular}{l l l l}
				\toprule[0.2em]
				Method & Backbone & mIoU (\%) & \#GFLOPs\tnote{\textdagger}  \\
				\toprule[0.2em]
				PSPNet~\cite{pspnet} & ResNet-50 & 42.78 & 167.6 \\
				PSPNet~\cite{pspnet} & ResNet-101 &  43.29 & 238.4 \\ 
				PSANet~\cite{psanet} & ResNet-101 &  43.77 & 264.9 \\ 
				EncNet~\cite{context_encoding} & ResNet-101 & 44.65 & - \\
				CFNet~\cite{Co-Occurrent} & ResNet101 & 44.82 & -\\
				\midrule
				w/o FAM & ResNet-50 & 41.12 & 74.8 \\
				SFNet & ResNet-50  & 42.81(1.69 )  & 75.7 \\
				w/o FAM & ResNet-101 & 43.08 & 93.1 \\
				SFNet & ResNet-101  & 44.67(1.59 ) & 94.0 \\
				\bottomrule[0.1em]
			\end{tabular}
			}\par
			{(b) Results on ADE20K.}
		\end{minipage}
	\end{minipage}
\caption{Experiments results on Pascal Context and ADE20k(Multi scale inference).  \#GFLOPs calculation adopts  image as input.}
\label{tab:pacal_context_ade_20k}
\end{table}

\noindent
\textbf{PASCAL Context: } The results are illustrated as Table~\ref{tab:pacal_context_ade_20k}(a), our method outperforms corresponding baselines by 1.7\% mIoU and 2.6\% mIoU with ResNet-50 and ResNet-101 as backbones respectively. In addition, our method on both ResNet-50 and ResNet-101 outperforms their existing counterparts by large margins with significantly lower computational cost.

\noindent
\textbf{ADE20K: } is a challenging scene parsing dataset. Images in this dataset are from different scenes with more scale variations. Table~\ref{tab:pacal_context_ade_20k}(b) reports the performance comparisons, our method improves the baselines by 1.69\% mIoU and 1.59\% mIoU respectively, and outperforms previous state-of-the-art methods~\cite{pspnet,psanet} with much less computation.

\noindent
\textbf{CamVid: } is another road scene dataset. This dataset involves 367 training images, 101 validation images and 233 testing images with resolution of . We apply our method with different light-weight backbones on this dataset and report comparison results in Table~\ref{table:camvid_res}. With DF2 as backbone, FAM improves its baseline by 3.2\% mIoU. Our method based on ResNet-18 performs best with 73.8\% mIoU while running at 35.5 FPS. 


\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.70}{
			\begin{tabular}{l  l l l}
				\toprule[0.2em]
				Method  &  Backbone & mIoU (\%) & FPS \\
				\toprule[0.2em]
ICNet~\cite{ICnet}  & ResNet-50 & 67.1 & 34.5 \\
			    BiSegNet~\cite{bisenet} & Xception-39 & 65.6 & - \\
			    BiSegNet~\cite{bisenet} & ResNet-18 & 68.7 & - \\
			    DFANet A~\cite{dfanet} & - & 64.7 & 120 \\
			    DFANet B~\cite{dfanet} & - & 59.3 & 160 \\
				\midrule
                w/o FAM &DF2 & 67.2 & 139.8 \\
				SFNet & DF2 & \textbf{70.4} (3.2 ) & 134.1 \\
				SFNet & ResNet-18 & \textbf{73.8} & 35.5 \\
				\bottomrule[0.1em]
			\end{tabular}
		}
		\caption{Accuracy and Speed comparison with previous state-of-the-art real-time models on CamVid~\cite{CamVid} test set where the input size is  with single scale inference.
		}
		\label{table:camvid_res}
	\end{threeparttable}
\end{table}