\begin{figure*}[t!]
	\centering
    \includegraphics[width=0.9\textwidth]{overview_v2.pdf}
    \mycaption{Overview of the proposed HSR learning scheme}{\small This learning scheme is conducted iteratively with clustering and network training. ICM are used to create additional hard positive training pairs and PBH is used to rectify the hard negative pairs in original clustering results. }
    \label{fig:proposed_method}
    \vspace{-6mm}
\end{figure*}

\vspace{-5mm}
\section{Proposed Methods}
\vspace{-3mm}
\subsection{Overview of our HSR learning scheme} 
\label{sub:overview}
\vspace{-2mm}
We first define the notation to be used in this paper. Given an unlabelled target dataset  containing total  training images, where  denotes the camera ID of image , and a source labelled dataset which serves as a preliminary knowledge base for learning re-ID, the goal of our model is to learn a discriminative ability to perform person re-ID on the target dataset. 
The learning scheme is shown in Fig.~\ref{fig:proposed_method}. Typically, a feature extractor  will be first learnt on the labelled source dataset as a pretrained feature embedding function , where  is the parameters learned on the source domain. 
Then, similar to~\cite{SSG,PAST}, a clustering algorithm called DBSCAN~\cite{DBSCAN}, which does not require the exact number of clusters or identities, will be used to generate the pseudo labels for the target unlabelled images based on the extracted feature vectors .
With ``estimated'' pseudo labels , we can learn the re-ID model in the typical supervised manner, which consists of the cross-entropy loss () that helps correctly classify the identities~\cite{softmax} and the triplet loss () for controlling the distance of the positive and negative pairs in the embedding feature space~\cite{triplet}.
The clustering and network optimization stages will be conducted iteratively, and the performance of re-ID model and the quality of clustering results will improve steadily.
However, it will reach a bottleneck owing to the situations caused by the hard samples as mentioned above.
To further enhance the model ability, we propose Hard Samples Rectification (HSR) learning scheme, which dually rectifies the hard positive and negative samples with two components: inter-camera mining (ICM) and part-based homogeneity (PBH) techniques, as shown in Fig.~\ref{fig:proposed_method}.
During training, ICM will mine possible hard positive pairs with different camera views and apply triplet loss to pull close those pairs in the feature space. On the other hand, PBH technique will refine the potential imperfect clusters by splitting the hard negative pairs within the same group.

\vspace{-4mm}
\subsection{Inter-Camera Mining} 
\vspace{-2mm}
\label{sub:ICM}
As mentioned in Section~\ref{sec:intro}, hard positive pairs may be assigned to different pseudo labels due to the variance of appearance under different cameras.
After several iterations of clustering and network training, it will leads to a vicious cycle that the positive pairs used to optimize the model are only those with similar content, which goes against the goal of person re-ID to match people across cameras.
Thus, we propose an inter-camera mining technique as a role of assisting the original clustering method to mine the hard positive samples. 


\begin{algorithm}[t]
    \caption{Inter-Camera Mining}
    \begin{algorithmic}[1]
    \REQUIRE 
        Image feature vectors  and its camera ID  on target domain
    \ENSURE 
        Possible hard positive pairs
    \STATE Calculate similarity matrix .
    \FOR{ = ;  ; =+ }
    \STATE Sort  in descending order.
    \STATE \textit{Rank}()  top- images  in  with 
    \ENDFOR
    \STATE Choose image pairs ( , ) conform to  \textit{Rank}() and \\ \textit{Rank}().
    \STATE return all chosen pairs.
    \end{algorithmic}
    \label{alg:ICM}
\end{algorithm}

In practice, shown in Algorithm~\ref{alg:ICM}, we first compute the similarity matrix  for all target images, where the element in the -th row and -th column is the negative Euclidean distance of   and .
Then, after sorting each row in descending order, we form the possible hard positive ranking list of each image by selecting its top- closest images according to the matrix , denoted as \textit{Rank}() with a total length of . It is worth noting that in order to emphasize on ``inter-camera'' positive pairs, we remove those images captured by the camera same as the image .
To ensure the robustness and correctness of our inter-camera mining, inspired by Dekel~\etal~\cite{BBP}, we additionally conduct a  mutually best-buddies pairs technique. 
That is to say, for every image  in \textit{Rank}(),  should as well be in \textit{Rank}(). Thus, only the image pair ( , ) that meets the above requirement would be taken into account as a reliable hard positive pair in the following CNN training.


With the mining hard positive pairs, we additionally apply the triplet loss , where the selection of positive samples is based on our ICM mining results. Notes that it differs from the original  which samples the positives based on the pseudo labels generated by the clustering algorithm.
As for the choice of negative samples of each anchor  in , we choose images with different pseudo labels from  and at the same time not in its rank list \textit{Rank}().
Different from~\cite{PAST}, we embed the accessible camera information and the mutual similarity, which benefits the correctness and the robustness of additional triplet pairs mining.
With our  iteratively shortening the distance of these mined hard positive samples, it can progressively ensure the ability of our model to match person regardless of the variation between camera views and at the same time improve the quality of the clustering results.  
\vspace{-5mm}
\subsection{Part-based Homogeneity} 
\vspace{-2mm}
\label{sub:PBH}
Different people with only subtle difference are possibly assigned with the same pseudo labels, which would degrade the model ability to discriminatively identify people in detail.
In the aim of separating imperfect clusters which possibly contain hard negative pairs, we develop a novel method called part-based homogeneity (PBH) as a rectification technique by utilizing the local features which provide finer information other than the global one.
First, we need to define the imperfectness of a cluster and select the candidates for applying our PBH. To this end, we utilize Silhouette score~\cite{silhouette}, which is an evaluation metric for measuring how well a sample is clustered to its group without the requirement of the ground truth labels. 
By computing the mean Silhouette score of data in each cluster , denoted as , we can further select the imperfect cluster with its  smaller than an empirically predefined threshold .
Our proposed PBH technique is then applied on every selected cluster to refine the original clustering results, as illustrated in Fig.~\ref{fig:PBS}.


To start with, we split and pool the output feature maps of every sample in the selected cluster  into two parts: upper and lower features, which are formulated as  and , where  is the number of samples in cluster .
Then, we respectively employ the K-means clustering with K = 2 on  and  to observe the data distribution of the finer local features.
Consequently, each sample is assigned with two temporary labels based on the groups of its upper and lower features, denoted as  and . 
With the part-based label pair , we can re-assign new pseudo labels to the samples in cluster  according to a look-up table, as shown in Fig.~\ref{fig:PBS}.
The idea behind is that only the data with both similar local parts, which means the same , can be assigned with the same pseudo label.
Notably, because the number of contained ground truth labels is unknown, we suppose that if the cluster is defined as an imperfect one, it would contain at least two ground truth labels. Furthermore, the progress of iterative learning can ensure that even the selected imperfect cluster contains more than two ground truth labels, the split clusters would still have the chance to be defined as imperfect ones in the next iteration. 
In summary, by considering the local features, our PBH maintains the homogeneity within the new cluster and avoids assigning the same pseudo label to globally similar hard negative pairs.   
\begin{figure}[t]
	\centering
    \includegraphics[width=0.75\linewidth]{pbh.pdf}
    \mycaption{Illustration of part-based homogeneity technique}{\small We extract local features of upper part and lower part for each sample in the imperfect cluster and apply K-means clustering respectively on both local features to obtain two kinds of part-based labels. With the two temporary local labels, the cluster is then split into at most four different groups according to the look-up table.}
    \label{fig:PBS}
    \vspace{-3mm}
\end{figure}

\vspace{-4mm}
\subsection{Optimization Procedure} 
\vspace{-2mm}
\label{sub:optimization}
For each iteration, after clustering the unlabelled data by DBSCAN, we would first verify the imperfect clusters and adopt the proposed PBH technique to refine the original estimated pseudo labels. Then, we jointly utilize the triplet loss () and the cross-entropy loss () to optimize the CNN network with those updated pseudo labels. Besides the positive and negative pairs sampled from the pseudo labels, we also jointly adopted the triplet loss  according to our ICM sampling technique.
The overall loss function can be written as follows:
\vspace{-4mm}

%