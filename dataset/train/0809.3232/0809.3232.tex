\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb,ifthen}

\newboolean{@laptop}
\newboolean{@todoon}
\setboolean{@laptop}{true}
\setboolean{@todoon}{true}
 
\ifthenelse{\boolean{@laptop}}{}{\usepackage{mathbbol}}

\def\todo#1{\ifthenelse{\boolean{@todoon}}{\marginpar{\textit{#1}}}{}}

\textheight 8.5in
\topmargin -0.2in
\oddsidemargin 0.2in
\textwidth 6.3in



\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{warning}[theorem]{Warning}
















 


\newcommand{\setj}[2]{S_{#1} (#2)}
\newcommand{\lamj}[2]{\lambda_{#1} (#2)}



\newcounter{bean}

\newenvironment{tightlist}{\begin{list}{}{\usecounter{bean}\setlength{\topsep}{0em}
  \setlength{\parsep}{0em}
  \setlength{\partopsep}{0em}
  \setlength{\itemsep}{0em}
}}{\end{list}}


\def\Span#1{\textbf{Span}\left(#1  \right)}

\newcommand\zzero{\boldsymbol{\mathit{0}}}


\def\calW{\mathcal{W}}

\def\util{\tilde{u}}
\def\vtil{\tilde{v}}
\def\Htil{\widetilde{H}}
\def\Dtil{\widetilde{D}}
\def\dtil{\tilde{d}}
\def\Ktil{\widetilde{K}}
\def\ktil{\tilde{k}}
\def\Khat{\widehat{K}}
\def\khat{\hat{k}}
\def\Ctil{\widetilde{C}}
\def\Chat{\widehat{C}}
\def\ctil{\tilde{c}}
\def\Gtil{\widetilde{G}}
\def\Etil{\widetilde{E}}
\def\Ftil{\widetilde{F}}
\def\ftil{\tilde{f}}
\def\etil{\tilde{e}}
\def\Atil{\widetilde{A}}
\def\Ahat{\widehat{A}}
\def\ahat{\hat{a}}
\def\That{\widehat{T}}
\def\Ehat{\widehat{E}}
\def\atil{\tilde{a}}
\def\Ghat{\widehat{G}}
\def\Btil{\widetilde{B}}
\def\btil{\tilde{b}}
\def\htil{\tilde{h}}
\def\wtil{\tilde{w}}
\def\fhat{\hat{f}}
\def\Fhat{\widehat{F}}
\def\mhat{\hat{m}}
\def\Stil{\widetilde{S}}

\def\supp#1#2{\sigma_{f} (#1, #2)}
\def\lap#1{\mathcal{L} (#1)}


\def\vs#1#2#3{#1_{#2},\ldots , #1_{#3}}


\def\round#1{\left[#1 \right]_{\epsilon }}

\def\have#1#2#3{\frac{1}{2}\Big(H_{#1} (#2-2\phi #3) + H_{#1} (#2+2\phi #3) \Big)}
\def\haveb#1#2{\frac{1}{2}\Big(H_{#1} (#2-2\phi \bar{#2}) + H_{#1} (#2+2\phi \bar{#2}) \Big)}

\def\jt{\tilde{j}}
\def\pt{\tilde{p}}
\def\pit{\tilde{\pi}}
\def\rhot{\tilde{\rho}}


\def\form#1#2{#1^{T} #2}

\def\calP{{\cal P}}
\def\calQ{{\cal Q}}
\def\calR{{\cal R}}

\def\myPhiSym{\ifthenelse{\boolean{@laptop}}{\varphi}
{\mathbb{\Phi}}
}

\def\calC{{\cal C}}
\def\calW{{\cal W}}
\def\phiprime#1#2{\Phi'_{#1}\left(#2\right)}
\def\bdry#1#2{\partial_{#1}\left(#2\right)}
\def\myphi#1#2{\myPhiSym_{#1}\left(#2\right)}
\def\graphmyphi#1{\myPhiSym_{#1}}
\def\cutsize#1{\textbf{cut-size}\left(#1\right)}

\def\balance#1{\textbf{bal}\left(#1\right)}
\def\level#1{\textbf{level}\left(#1  \right)}

\def\pleq{\preccurlyeq}
\def\pgeq{\succcurlyeq}

\def\edge#1{\textrm{edges}\left(#1  \right)}
\def\degree#1#2{\textrm{deg}_{#1}\left(#2  \right)}
\def\wdegree#1#2{\textrm{wdeg}_{#1}\left(#2  \right)}
\def\bridge#1#2{\textrm{bridge}\left(#1, #2  \right)}
\def\metaGraph#1#2#3{\textrm{metaGraph}\left(#1, #2 ,#3 \right)}

\def\edg#1{\textbf{(}#1 \textbf{)}}

\def\stretch#1#2{\textrm{stretch}_{#1} (#2)}
\def\weight#1{\textrm{weight} (#1)}
\def\res#1{\textrm{resistance} (#1)}

\def\union{\cup}
\def\intersect{\cap}
\def\Union{\bigcup}
\def\Intersect{\bigcap}

\def\defeq{\stackrel{\mathrm{def}}{=}}

\newcommand\xx{\boldsymbol{\mathit{x}}}

\newcommand\yy{\boldsymbol{\mathit{y}}}
\newcommand\zz{\boldsymbol{\mathit{z}}}
\newcommand\rr{\boldsymbol{\mathit{r}}}
\newcommand\bb{\boldsymbol{\mathit{b}}}

\newcommand\cc{\boldsymbol{\mathit{c}}}

\def\prob#1#2{\Pr_{#1}\left[ #2 \right]}
\def\expec#1#2{\mbox{\bf E}_{#1}\left[ #2 \right]}
\newcommand{\E}{\mbox{{\bf E}}}

\def\norm#1{\left\| #1 \right\|}
\def\onenorm#1{\left\| #1 \right\|_{1}}
\def\infnorm#1{\left\| #1 \right\|_{\infty }}
\def\fnorm#1{\left\| #1 \right\|_{F}}
\def\setof#1{\left\{#1  \right\}}
\def\sizeof#1{\left|#1  \right|}

\def\dist#1#2{\mbox{{\bf dist}}\left(#1, #2 \right)}
\def\diff#1{\, d #1 \,}

\def\setminus{-}



\def\pos#1{\mathcal{H} (#1)}

\newcommand\brho{\bar{\rho}}



\def\bvec#1{{\mbox{\boldmath }}}
\def\origin{{\mbox{\boldmath }}}

\def\pleq{\preccurlyeq}
\def\pgeq{\succcurlyeq}

\def\abs#1{\left|#1  \right|}
\def\intersect{\cap}

\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\floor}[1]{\lfloor#1\rfloor}



\def\wdilation#1#2{\textbf{wd}_{#1}\left(#2  \right)}
\def\wcong#1#2{\textbf{wc}_{#1}\left(#2  \right)}

\def\path#1#2{\textbf{path}_{#1}\left(#2  \right)}

\def\bigO#1{O\left(#1  \right)}

\def\Atil{\tilde{A}}
\def\Gtil{\tilde{G}}
\def\Ltil{\tilde{L}}
\def\xtil{\tilde{x}}
\def\setof#1{\left\{#1  \right\}}
\def\abs#1{\left|#1  \right|}


\def\lap#1{\mathcal{L} (#1)}

\newcommand\xxt{\boldsymbol{\mathit{\tilde{x}}}}

\newdimen\pIR
\pIR= -131072sp
\newcommand\StevesR{{\rm I\kern\pIR R}}
\def\Reals#1{\StevesR^{#1}}


\def\tA{\tilde{A}}
\def\tS{\tilde{S}}
\def\tB{\tilde{B}}
\def\tL{\tilde{L}}
\def\td{\tilde{d}}

\def\tP{\tilde{P}}
\def\tR{\tilde{R}}

\def\wedge#1#2#3{\left(\setof{#1,#2},#3 \right)}



\def\conduc#1#2{\Phi_{#1}\left(#2  \right)}
\def\conducin#1#2{\Phi^{G}_{#1}\left(#2  \right)}

\def\Conduc#1{\Phi_{#1}}
\def\Conducin#1{\Phi^{G}_{#1}}

\def\vol#1{\mu \left(#1  \right)}

\begin{document}

\title{A Local Clustering Algorithm for Massive Graphs and its Application to Nearly-Linear Time Graph
   Partitioning\thanks{This paper is the first in a sequence of three papers expanding
  on material that appeared first under the title
  ``Nearly-linear time algorithms for graph partitioning, 
    graph sparsification, and solving linear systems''~\cite{SpielmanTengPrecon}.
The second paper, ``Spectral Sparsification of Graphs''~\cite{SpielmanTengSparsifier}
  contains further results on partitioning graphs, and applies them to producing
  spectral sparsifiers of graphs.
The third paper, ``Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems''~\cite{SpielmanTengLinsolve} contains the results
  on solving linear equations and approximating eigenvalues and eigenvectors.
\vskip 0.01in
This material is based upon work supported by the National Science Foundation 
  under Grant Nos. 0325630, 0634957, 0635102 and 0707522.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
}
}
\author{
Daniel A. Spielman\\
Department of Computer Science\\
Program in Applied Mathematics\\
Yale University
\and
Shang-Hua Teng\\
Department of Computer Science\\
Boston University}

\maketitle

\begin{abstract}
We study the design of {\em local algorithms} for 
  massive graphs.
A local algorithm is one that
  finds a solution containing or near a given vertex without
  looking at the whole graph.
We present a local clustering algorithm.
Our algorithm finds a good cluster---a subset of vertices
  whose internal connections are significantly richer
  than its external connections---near a given vertex.
The running time of our algorithm, when it finds a non-empty
  local cluster, is nearly linear in the size
  of the cluster it outputs.

Our clustering algorithm could be  a useful primitive
  for handling massive graphs, such as social networks and web-graphs.
As an application of this clustering algorithm,
  we present a partitioning algorithm
  that finds an approximate sparsest cut with nearly optimal balance.
Our algorithm takes time nearly linear in the number edges of the graph.

Using the partitioning algorithm of this paper, we have designed a nearly-linear
  time algorithm for constructing spectral sparsifiers of graphs, which
  we in turn use in a nearly-linear time algorithm for solving linear
  systems in symmetric, diagonally-dominant matrices.
The linear system solver also leads to a nearly linear-time
  algorithm for approximating 
  the second-smallest eigenvalue and corresponding eigenvector
  of the Laplacian matrix of a graph.
These other results are presented in two companion papers.
\end{abstract}

\newpage
\section{Introduction}\label{sec:Intro}

Given a vertex of interest in a massive graph, we
  would like to find a small cluster around that vertex, 
  \textit{in time proportional to the size of the cluster}.
The algorithm we introduce will solve this problem while
  only examining vertices near the initial
  vertex, under some reasonable notion of nearness.
We call such an algorithm a \textit{local} algorithm.

Our local clustering algorithm provides a very powerful
  primitive for the design of fast graph algorithms.
In Section~\ref{sec:cut} of this paper, we use it to design
  the first nearly-linear time algorithm for graph partitioning
  that produces a partition of nearly-optimal balance among those
  approximating a target conductance.
In the papers~\cite{SpielmanTengSparsifier} and~\cite{SpielmanTengLinsolve},
  we proceed to use this graph partitioning algorithm to design
  nearly-linear time algorithms for sparsifying graphs and for solving
  symmetric, diagonally-dominant linear systems.


\subsection{Local Clustering}
We say that a graph algorithm is a \textit{local algorithm} if it is given
  a particular vertex as input,
  and at each step after the first only examines vertices connected to those
  it has seen before.
The use of a local algorithm naturally leads to the question of in which
  order one should explore the vertices of a graph.
While it may be natural to explore vertices in order of shortest-path
  distance from the input
  vertex, such an ordering is a poor choice in graphs of low-diameter,
  such as social network graphs~\cite{diameterSocialNetwork}.
We suggest first processing the vertices that are most likely
  to occur in short random walks from at the input vertex.
That is, we consider a vertex to be near the input vertex if it is likely
  to appear in a short random walk from the input vertex.

In Section~\ref{sec:cut}, we use a local graph exploration process
  to find a cluster that is near the input vertex.
Following Kannan, Vempala and Vetta~\cite{KannanVempalaVetta}, we say that
  a set of vertices is a good cluster if
  it has low \textit{conductance}; that is,
  if it has many more external than internal edges.
We give an efficient local clustering algorithm, \texttt{Nibble}, that
  runs in time proportional to the size of the cluster it outputs.
Although our algorithm may not find a local cluster
  for some input vertices,
  we will show that it is usually successful.
In particular, we prove the following theorem:
There exists a constant  such that for any
   target conductance  and any cluster  
   of conductance at most ,
   when given a random vertex   sampled according to degree
   inside ,
   \texttt{Nibble} will return a cluster  mostly inside   and with
  conductance at most ,
  with probability at least .

The local clustering algorithm \texttt{Nibble} makes a novel use of random walks.
For a positive integer , suppose  is
  the probability distribution of the -step
  random walk starting at .
As the support of ---the set of nodes with positive probability---could
  grow rapidly, \texttt{Nibble} maintains a truncated version
  of the distribution.
At each step of the truncated random walks,
  \texttt{Nibble} looks a for cluster among only nodes
  with high probability.
The truncation is critical to ensure that
    the clustering algorithm is output sensitive.
It guarantees that the size of the support of the distribution that
  \texttt{Nibble}
  maintains is not too much larger than the size of the cluster it produces.
The cluster that \texttt{Nibble}  produces is local to the starting
  vertex  in the sense that it consists of nodes that are among
  the most favored destinations of random walks starting from .

By using the personal PageRank vector~\cite{PageRank}
  to define nearness, Andersen, Chung and Lang~\cite{AndersenChungLang}, 
  have produced an improved version of our algorithm \texttt{Nibble},
  which they call \texttt{PageRank-Nibble}.
Following this work, other local algorithms have been designed by
  Andersen \textit{et. al.}~\cite{AndersenPageRank} for
  approximately computing Personal PageRank vectors, by
  Andersen~\cite{AndersenDense} for finding dense subgraphs
  and by Andersen, Chung and Lang~\cite{AndersenChungLang2} for partitioning
  directed graphs.

\subsection{Nearly Linear-Time Algorithms}

Our local clustering algorithm provides a powerful tool for
  designing fast graph algorithms.
In this paper and its two companion papers, we 
  show how to use it to design randomized, nearly linear-time
  algorithms for several important graph-theoretic and
  numerical problems.

The need for algorithms whose running time is
  linear or nearly linear in their input size
  has increased as algorithms handle larger inputs.
For example, in circuit design and simulation,
  an Intel Dual Core Itanium processor has more than
  one billion transistors, 
  which is  more than 100 times the number of
  transistors that the Pentium had in 2000~\cite{IntelMooresLaw};
  in scientific computing, one often needs to solve linear
  systems that involve hundreds of millions of variables~\cite{LargeScaleScientificComputing};
 in modern information infrastructure, the web has grown into
  a graph of hundreds billions of nodes~\cite{IndexableWeb2005}.
As a result of this rapid growth in problem size, what used to
  be considered an efficient algorithm, such as
  a -time algorithm, may no longer be adequate for
  solving problems of these scales.
Space complexity poses an even greater problem.

Many basic graph-theoretic problems such as connectivity
  and topological sorting can be solved in linear or nearly-linear time.
The efficient algorithms for these problems are built on
  linear-time primitives such as Breadth-First-Search
  (BFS) and Depth-First-Search (DFS).
Minimum Spanning Trees (MST) and Shortest-Path Trees are
  examples of other commonly used nearly linear-time primitives.
We hope to build up the library of nearly-linear time
  graph algorithms that may be used as primitives.
While the analyzable variants of the 
  algorithms we present here, and even their improved versions
  by Andersen, Chung and Lang~\cite{AndersenChungLang}, may not be
  immediately useful in practice, we 
  believe practical algorithms may be derived from them by
  making less conservative choices of parameters.




Our local clustering algorithm provides an exciting new
  primitive for developing nearly linear-time graph algorithms.
Because its running time is proportional to the size of
  the cluster it produces, we can repeatedly apply it
  remove many clusters from a graph, all within nearly-linear time.

In the second part of this paper,
  we use  \texttt{Nibble}  as a subroutine to
  construct a randomized graph partitioning
  algorithm that runs in nearly-linear time.
To the best of our knowledge, this is the first nearly linear-time
  partitioning algorithm that finds an approximate sparsest cut
  with approximately optimal balance.
In our first companion paper~\cite{SpielmanTengSparsifier}, we apply this new
  partitioning algorithm to develop a
  nearly-linear-time algorithm for producing spectral sparsifiers of graphs.
We begin that paper by extending the partitioning algorithm of this paper
  to obtain a stronger guarantee on its output: if it outputs a small set,
  then the complement must be contained in a subgraph whose conductance is
  higher than the target.

\section{Clusters and Conductance}\label{sec:Def}

Let   be an undirected graph  with .
A {\em cluster} of  is a subset of  that is
  richly intra-connected but sparsely connected
  with the rest of the graph.
The quality of a cluster can be measured by its conductance,
  the ratio of the number of its external connections to
  the number of its total connections.

We let  denote the degree of vertex .
For , we define 
  (often called the volume of ).
So, .
Let  be the set of edges connecting a vertex in
   with a vertex in .
We define the {\em conductance} of a set of vertices , written
   by

The {\em conductance} of  is then given by


We sometime refer to a subset  of  as a {\em cut} of 
  and refer to  as a {\em partition} of .
The {\em balance} of a cut  or a partition  is then equal to
  
We call  a \textit{sparsest cut}
  of  if  and .

In the construction of a partition of , we will be concerned with
  vertex-induced subgraphs of .
However, when measuring the conductance and volumes of vertices in
  these vertex-induced subgraphs, we will continue to measure the
  volume according to the degrees of vertices in the original graph.
For clarity, we define the conductance of a set  in the subgraph induced
  by  by

and

For convenience, we define  and, for ,
  .

For , we let  denote the subgraph of  induced by
  the vertices in .
We introduce the notation  to denote graph  to which self-loops
  have been added so that every vertex in  has the same degree
  as in .
Each self-loop adds 1 to the degree.
We remark that if  is the subgraph of  induced on the
  vertices in , then

So, when we prove lower bounds on , we obtain lower
  bounds on .

Clustering is an optimization problem:
Given an undirected graph  and
   a conductance parameter, find a cluster  such that
   , or determine no such cluster exists.
The problem is  NP-complete (see, for example~\cite{LeightonRao}
  or~\cite{NPcompleteCluster}).
But, approximation algorithms exist.
Leighton and Rao~\cite{LeightonRao} used linear programming to obtain
 -approximations of the sparsest cut.
Arora, Rao and Vazirani~\cite{AroraRaoVazirani} improved this to 
  through semi-definite programming.
Faster algorithms obtaining similar guarantees have been constructed by
  Arora, Hazan and Kale~\cite{AroraHazanKale},
  Khandekar, Rao and Vazirani~\cite{KhandekarRaoVazirani},
  Arora and Kale~\cite{AroraKale}, and
  Orecchia, Schulman, Vazirani, and Vishnoi~\cite{Orecchia}.



\subsection{The Algorithm \texttt{Nibble}}

The algorithm
  \texttt{Nibble}
  works by approximately computing the distribution
  of a few steps of the random walk starting at a
  seed vertex .
It is implicit in the analysis of the volume
  estimation algorithm of Lov\'asz and
  Simonovits~\cite{LovaszSimonovits}
  that one can find a cut with small conductance from the distributions
  of the steps of the
  random walk starting at any vertex from which
  the walk does not mix rapidly.
We will observe that a random vertex in a set of low conductance
  is probably such a vertex.
We then extend the analysis of Lov\'asz and Simonovits
  to show one can find a cut with small conductance from approximations
  of these distributions, and that
  these approximations can be computed quickly.
In particular, we will truncate all
  small probabilities that appear in the distributions to 0.
In this way, we reduce the work required to
  compute our approximations.

For the rest of this section, we will work with a graph
   with  vertices and  edges, so that
  .
We will allow some of these edges to be self-loops.
Except for the self-loops, which we allow to occur with multiplicities,
  the graph is assumed to be unweighted.
We will let  be the adjacency matrix of this graph.
That is,


We define the following two vectors supported on a set of vertices :


We will consider the random walk
  that at each time step stays at the current vertex with probability ,
  and otherwise moves to the endpoint of a random edge attached
  to the current vertex.
Thus, self-loops increase the chance the walk stays at the current vertex.
For example, if a vertex has  edges, one of which is a self-loop,
  then when the walk is at this vertex it has a  chance of staying
  at that vertex, and a  chance of moving to each of its  neighbors.

The matrix realizing this walk can be expressed by
 ,
  where
   is the degree of node ,
  and  is the diagonal matrix with diagonal entries
  .
Typically, a random walk starts at a node .
In this case, the distribution of the random walk at time 
  evolves according to .

We note that  is the steady-state distribution of the
  random walk, and that  is the restriction
  of that walk to the set .




We will use the truncation operation defined by



Our algorithm, \texttt{Nibble}, will generate the sequence of
  vectors starting at  by the rules



That is, at each time step, we will evolve the random walk one
  step from the current density, and then round every 
  that is less than  to 0.
Note that  and  are not necessarily probability vectors,
  as their components may sum to less than .

In the statement of the algorithm and its analysis, we will use the
  following notation.
For a vector , we let  be the set of  vertices 
  maximizing , breaking ties lexicographically.
That is, 
  where   is the permutation  such that

  for all ,
  and  when these two ratios are equal.
We then set

Note that  always equals .

Following Lov\'asz and Simonovits \cite{LovaszSimonovitsFOCS}, we set

This function  is essentially the same as the function 
  defined by Lov\'asz and Simonovits---it only differs by a linear transformation.


We remark that for ,
  , and that
   is linear in  between these points.
Finally, we let  denote the partial derivative of
   with respect to , with the convention that for
  ,

where  is the permutation specified above
  so that .

As  is non-increasing,
   is a non-increasing function in  and
   is a concave function in .

During the course of our exposition, we will need to set many constants,
  which we collect here for convenience.
For each, we provide a suitable value and indicate where it is first used in the paper. 

The following is an exhaustive list of the inequalities 
  we require these constants to satisfy.


Given a , we set constants that will play a prominent role in our analysis:

Note that


\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent \\
where  is a vertex\\
\\
 is a positive integer.
\begin{enumerate}
\item  Set


\item Set
   and
  .

\item  For  to 
\begin{enumerate}
\item Set 
\item  Set .

\item  If there exists a  such that
  \begin{enumerate}

\item [(C.1)] ,
\item [(C.2)]
    ,
\item [(C.3)]
      , and
\item [(C.4)]
    .
  \end{enumerate}
  then return 
  and quit.
  \end{enumerate}
\item  Return .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

Condition (C.1) guarantees that the set  has low conductance.
Condition (C.2) ensures that it does not contain too much volume,
  while condition (C.3) ensures that it does not contain too little.
Condition (C.4) guarantees that many elements of 
  have large probability mass.
While it would be more natural to define condition (C.4) as a constraint
  on  instead of
  , our proof of correctness requires the latter.


In the rest of this section, we will
  prove the following theorem on the performance of \texttt{Nibble}.

\begin{theorem}[\texttt{Nibble}]\label{thm:Nibble}
\mbox{\rm \texttt{Nibble}}
  can be implemented so that
  on all inputs, it runs in time
  .
Moreover, \mbox{\rm \texttt{Nibble}} satisfies the following properties.
\begin{itemize}
\item [\mbox{\rm (N.1)}]
When
 is non-empty,

\item [\mbox{\rm (N.2)}]
  Each set  satisfying

has a subset  such that
\begin{itemize}
\item [\mbox{\rm (N.2.a)}] , and

\item [\mbox{\rm (N.2.b)}]   and
   imply
  .
\end{itemize}
\item [\mbox{\rm (N.3)}] The set  may be partitioned into
  subsets 
  such that if , then the
  set  output by 
  will not be empty.
\end{itemize}
\end{theorem}



\subsection{Basic Inequalities about Random Walks}

We first establish some basic inequalities
  that will be useful in our analysis.
Readers who are eager to see the analysis of \texttt{Nibble}
  can first skip this subsection.
Suppose  is an undirected graph.
Recall ,  where  is the
  adjacency matrix of .


\begin{proposition}[Monotonicity of Mult by ]\label{pro:infnorm}
For all non-negative vectors ,

\end{proposition}
\begin{proof}
Applying the transformation , we see that it
  is equivalent to show that for all 

To prove this, we note that
  ,
  and the sum of the entries in each row of this matrix is 1.
\end{proof}

\begin{definition}\label{def:DS}
For a set , we define the matrix  to be
  the diagonal matrix such that
   if  and  otherwise.
\end{definition}

\begin{proposition}\label{pro:DS}
For every , all
 non-negative vectors  and ,
  and every ,

\end{proposition}
\begin{proof}
For , we observe

as , ,  , and  are all non-negative.
The proposition now follows by induction.
\end{proof}

\begin{proposition}[Escaping Mass]\label{pro:isopLeaving}
For all  and for all ,

\end{proposition}
\begin{proof}
Note that  is the distribution after
  a single-step walk from a random vertex in 
  and 
  is the probability that the walk stays inside .
Thus, 
  is the probability that a -step walk starting
  from a random vertex in  stays entirely in .

We first prove by induction that for all ,

The base case, , follows from the fact that
  .
To complete the induction, observe that if
   is a non-negative vector such that
  , then

where the second-to-last inequality follows from
  Proposition~\ref{pro:infnorm}.

We will now prove that for all ,

from which the proposition follows, as .

Observing that ,
  we compute

\end{proof}



\subsection{The Analysis of \texttt{Nibble}}

Our analysis of \texttt{Nibble} consists of three main steps.
First, we define the sets  mentioned in
  Theorem \ref{thm:Nibble} and establish property (N.2).
We then refine the structure of  to
  define sets  and prove property (N.3).
The sets  and  are defined in terms
  of the distributions of
  random walks from a vertex in , without reference to the
  truncation we perform in the algorithm.
We then analyze the impact of truncation used in \texttt{Nibble}
  and extend the theory of Lov\'asz and Simonovits \cite{LovaszSimonovits}
  to truncated random walks.

\subsection*{Step 1:  and its properties}



\begin{definition}[]\label{def:sg}
For each set , we define
   to be the set of nodes  in  such that
  for all ,

\end{definition}

Note that 
  denotes the probability that a -step random walk
  starting from  terminates outside .
Roughly speaking,  is the set of vertices 
  such that a random walk from 
  it is reasonably likely to still be in  after 
  time steps.
We will prove the following bound on the volume of .
\begin{lemma}[Volume of ]\label{lem:sizeSg}

\end{lemma}
\begin{proof}
Let , and let  be the diagonal matrix such that
   if  and  otherwise.
For ,

as 
  is a non-increasing function of .
Define

So, , and it
  suffices to prove that
  .

Applying Proposition~\ref{pro:isopLeaving}, we obtain

So, we may conclude

from which the lemma follows.
\end{proof}


We now prove the following lemma,  which says  that
  if \texttt{Nibble} is started from any
   with parameter  and returns a non-empty set ,
  then  .

\begin{lemma}[N2]\label{lem:N3}
Let  be a set of vertices such that
  .
If \texttt{Nibble} is run with parameter ,
  is started at a ,
  and outputs a non-empty set , then
  .
\end{lemma}
\begin{proof}
For , let  be given by \eqref{eqn:qt} and \eqref{eqn:rt}.
Then, for ,

where the second inequality follows from the definition of .


Let  be the index of the step at which the set  is generated.
Let  be the least integer such that
  .
Condition (C.3) implies .
As  is non-increasing in its second argument and
  constant between  and  ,
  Condition  guarantees that for all
  ,

Thus,

by \eqref{eqn:c2geqc4}.
So, ,
 and, as ,

\end{proof}

\subsection*{Step 2: Refining }

Before defining the sets ,
  we first recall some of the facts we can infer about
  the function  from the work of Lov\'asz and Simonovits.
These facts will motivate our definitions and analysis.

In the first part of the proof of Lemma~1.4 of~\cite{LovaszSimonovitsFOCS},
 Lov\'asz and Simonovits prove
\begin{lemma}\label{lem:LSeasy}
For every non-negative vector  and every ,

\end{lemma}

For each ,  is a concave function that
  starts at  and goes to .
Lemma~\ref{lem:LSeasy} says that for each , the curve defined
  by  lies below the curve defined
  by .
In particular,


If none of the sets  has conductance less than ,
  then Lov\'asz and Simonovits prove a bound on how far below
   the curve of
   must lie.
The following Lemma is a special case of Lemma~1.4 of~\cite{LovaszSimonovits},
  restricted to points  of the form .
Lov\'asz and Simonovits~\cite{LovaszSimonovitsFOCS} claim that the following
  is true for all , but point out in the journal version of their 
  paper~\cite{LovaszSimonovits} that this claim was false.
Fortunately, we do not need the stronger claim.

\begin{lemma}\label{lem:LShard}
For any non-negative vector ,
  if , then
 for ,

where  denotes .
\end{lemma}

The mistake in~\cite{LovaszSimonovitsFOCS} is the assertion in the beginning
  of the proof that the inequality holds for all  if it holds for all
   of form .

When this lemma applies, one may draw a chord across the curve
  of  around  of width proportional to
  , and know that  lies below.
Thus, we know that if none of the sets 
  has conductance less than , then the curve
   will approach a straight line.
On the other hand, Proposition~\ref{pro:isopLeaving} will tell
  us that some point of  lies well above this line
  (see Lemma~\ref{lem:lowerBound}).

We will now define the sets
 
  for ,
  simultaneously with two quantities--- and ,
  where  is such that \texttt{Nibble} will stop
  between iterations  and  and 
  is the -coordinate of a point that 
  may be shown in Lemmas~\ref{lem:lowerBound} and \ref{lem:C123} to
  contradict the conclusion
  of Lemma~\ref{lem:LShard} and thereby enable us to find a set
  of low conductance.

\begin{definition}[,  and ]\label{def:Sgb}
Given a , let .
For ,
  define  to be the real number such that

We write  instead of  when  is clear from context.
Define

We define

and for , we define

\end{definition}

\begin{proposition}\label{pro:Sgb}
The quantities  are well-defined and
  the sets  partition .
Moreover,  for all .
\end{proposition}
\begin{proof}
It follows from the definition of
   that 
  for a probability vector  the slope of 
  is always less than 1, and so
  .
If ,
  then 

  so there is an integer  such that
  , and so the quantities  are well-defined.

To see that the sets  partition , it now suffices to
  observe that .

Finally, to show that ,
  we apply Lemma~\ref{lem:LSeasy} to show

As  is non-decreasing
  and

we can conclude that .
\end{proof}


\subsection*{Step 3: Clustering and truncated random walks}


We now establish that vectors produced by the
  truncated random walk do not differ too much from those
  produced by the standard random walk.
\begin{lemma}[Low-impact Truncation]\label{lem:rounding}
For all  and ,

For all  and ,

\end{lemma}
\begin{proof}
The left-hand inequalities of \eqref{eqn:roundingp} are trivial.
To prove the right-hand inequality of \eqref{eqn:roundingp}, we consider
  , observe that by definition

and then apply Proposition~\ref{pro:infnorm}.
Inequality \eqref{eqn:roundingI} then
  follows from \eqref{eqn:I}.
\end{proof}


\begin{lemma}[Lower bound on I]\label{lem:lowerBound}
Let  be a set of vertices
  such that
   and
  , and
  let  lie in .
Define  by running 
 \texttt{Nibble} is with parameter .
\begin{itemize}
\item [1.] If , then

\item [2.] Otherwise,

\end{itemize}
\end{lemma}
\begin{proof}
In the case , we compute

As , we may use
 Lemma~\ref{lem:rounding} 
  to show

by  \eqref{eqn:epsilon}.

If , we compute

\end{proof}


\begin{lemma}[C.4]\label{lem:C4}
Let  be a set of vertices
  such that
   and
  , and
  let  lie in .
If \texttt{Nibble} is run with parameter ,
  then for all ,
  condition (C.4) is satisfied.
\end{lemma}
\begin{proof}
We first consider the case in which ,
  which by definition implies .

In this case,
  we have

where the first inequality follows from Lemma~\ref{lem:rounding}
and the second follows from Lemma~\ref{lem:LSeasy}.

As  is non-increasing in 
  and , we have

where the second inequality follows from Lemma~\ref{lem:LSeasy} and the definition
  of , and the last follows from \eqref{eqn:lower2}.

If , then  
  and we have ,
 and so

and by \eqref{eqn:c4geqc5} condition (C.4) is satisfied.

If , then , and so 
   for all ,
  which implies

and condition (C.4) is satisfied.

If , in which case
  
  and ,
  we apply Lemma~\ref{lem:rounding}  to show that
  for all ,

On the other hand,

As  is non-decreasing and ,
  we have

as ,
and so by \eqref{eqn:manyc1} condition (C.4) is satisfied.
\end{proof}

It remains to show that conditions (C.1--3) are met for some
  .
We will do this by showing that if at least one of these
  conditions fail for every  and every
  ,
  then the curve  will be too low, in violation
  of Lemma~\ref{lem:lowerBound}.

\begin{lemma}\label{lem:C1help}
If there exists a  and an
 , 
  such that for all
   and for all 
  either
\begin{enumerate}
\item [1.]  ,
\item [2.] , or
\item [3.] , 
\end{enumerate}
then, for all 

\end{lemma}
\begin{proof}
We will prove by induction that the conditions of the lemma
  imply that for all  and
  all ,

The base case is when , in which case
  \eqref{eqn:It} is satisfied because
\begin{itemize}
\item For ,
  .
\item For ,
  we have  as both are 
  at , the right-hand term dominates at , the
  left-hand term is linear in this region, and the right-hand term is concave.

\item For ,
  we note that at ,
  , and that
  we already know the right-hand term dominates  at .
  The inequality then follows from the facts that
  left-hand term is linear in this region, and the right-hand term is concave.
\end{itemize}

Let

Lov\'asz and Simonovits~\cite{LovaszSimonovitsFOCS} observe that

We now prove that \eqref{eqn:It} holds for , assuming it holds for ,
  by considering three cases.
As the right-hand side is concave and the left-hand side is piecewise-linear
  between points of the form ,
  it suffices to prove the inequality at the points
  .
If  and 
  , then \eqref{eqn:It} holds trivially.
Similarly, if ,
  then \eqref{eqn:It} holds trivially as well, as
  the left-hand side is at most 1, and the right hand side is at least 1.
In the other cases, we have ,
  in which case we may apply Lemma~\ref{lem:LShard} to show
  that for 

by~\eqref{eqn:LS}.
\end{proof}

We now observe that  has been chosen to ensure



\begin{lemma}[C.1-3]\label{lem:C123}
Let  be a set of vertices such that
  and
  , and
  let  lie in .
If \texttt{Nibble} is run with parameter ,
  then there exists a  and a 
  for which conditions (C.1-3) are satisfied.
\end{lemma}
\begin{proof}
We first show that for , (C.3) is implied
  by .
To see  this, note that for 

So, 
  implies ,
 and we may prove the lemma by exhibiting a  and  for which
  (C.1), (C.2) and 
  hold.
On the other hand, if  then 
  for all , so 
  trivially implies  and therefore .

We will now finish the proof by contradiction: we show that if no
  such  and  exist, then the curve 
  would be too low.
If for all  
  and all  one of (C.1), (C.2) or 
   fails,
  then Lemma~\ref{lem:C1help} tells us that for all 

by inequality~\eqref{eqn:tEnsure}.

In the case ,
  we obtain a contradiction by 
  plugging in   to find

which by \eqref{eqn:c5leqc1c6} contradicts \eqref{eqn:lower2}.

In the case in that ,
  and so , we
  substitute  to obtain

which by \eqref{eqn:manyc2} contradicts \eqref{eqn:lower1}.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:Nibble}}

Fact (N.1) follows from conditions (C.1) and (C.2)
  in the algorithm.
Given a set  satisfying ,
  the lower bound on the volume of the set 
  is established in Lemma~\ref{lem:sizeSg}.
If  and ,
  then Lemmas~\ref{lem:C123} and~\ref{lem:C4} show that
  the algorithm will output a non-empty set.
Finally, lemma~\ref{lem:N3} tells us that if ,
   and the algorithm outputs a non-empty set ,
  then it satisfies .

It remains to bound the running time of \texttt{Nibble}.
The algorithm will run for 
  iterations.
We will now show that with the correct implementation, each iteration
  takes time .
Instead of performing a dense vector multiplication in step (3.a),
  the algorithm should keep track of the set of vertices 
  at which .
Call this set .
The set  can be computed in time 
  in step (3.b).
Given knowledge of , the multiplication in step (3.a)
  can be performed in time proportional to 

Finally, the computation in step (3.c) might require sorting the vectors
  in  according to , which could take time at most
  .
Thus, the run-time of \texttt{Nibble} is bounded by



\section{Nearly Linear-Time Graph Partitioning}\label{sec:cut}

In this section, we apply \texttt{Nibble} to design a partitioning
  algorithm \texttt{Partition}.
This new algorithm runs in nearly linear-time.
It computes an approximate sparsest
  cut with approximately optimal balance.
In particular, we prove that there exists a constant 
  such that for any graph  that has a cut  of sparsity 
  
  and balance , with high probability, \texttt{Partition} finds a
  cut  with   and
  .
Actually, \texttt{Partition} satisfies an even stronger
  guarantee: with high probability
  either the cut it outputs is well balanced,

or touches most of the edges touching ,

The expected running time of \texttt{Partition} is .
Thus, it can be used to quickly find crude cuts.


\texttt{Partition}
  calls \texttt{Nibble} via a routine
  called \texttt{Random Nibble} that calls \texttt{Nibble}
  with carefully chosen random parameters.
\texttt{Random Nibble}
  has a very small expected running time, and
  is expected to remove a similarly small fraction of
  any set with small conductance.



\subsection{Procedure \texttt{Random Nibble}}

\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent 
\begin{enumerate}
\item [(1)] Choose a vertex  according to .

\item [(2)] Choose a  in 
  according to


\item [(3)] .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

\begin{lemma}[\texttt{Random Nibble}]\label{lem:randNibble}
Let  be the number of edges in .
The expected running time of \texttt{Random Nibble} is
  .
If the set  output by \texttt{Random Nibble} is non-empty, it satisfies
\begin{itemize}
\item [(R.1)] , and
\item [(R.2)] 
\end{itemize}
Moreover,
  for every set  satisfying

\begin{itemize}
\item [(R.3)] .
\end{itemize}
\end{lemma}

\begin{proof}
The expected running time of \texttt{Random Nibble}
  may be upper bounded by

Parts (R.1) and (R.2) follow directly from
  part (N.1) of Theorem~\ref{thm:Nibble}.
To prove part (R.3), define  by


So, .
For each , the chance that  lands in
   is .
Moreover, the chance that  is at least .
If  lands in , then by part (N.3) of
  Theorem~\ref{thm:Nibble},  satisfies

So,

\end{proof}


\subsection{\texttt{Partition}}

We now define \texttt{Partition} and analyze its performance.
First, define

and note



\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent ,
where  is a graph, .
\begin{enumerate}
\item [(0)] Set ,  and .
\item [(1)] While  and
,
\begin{enumerate}
\item [(a)] Set .
\item [(b)] Set 
\item [(c)] Set .
\end{enumerate}
\item [(2)] Set .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in



\begin{theorem}[\texttt{Partition}]\label{thm:Partition}
On input a graph with  edges,
  the expected running time of \mbox{\rm \texttt{Partition}} is 
  .
Let  be the output of ,
  where  is a graph and .
Then
\begin{itemize}
\item [(P.1)] ,
\item [(P.2)] If  then , and
\item [(P.3)]
If  is any
  set satisfying

  then with probability at least ,
  .
\end{itemize}
In particular, with probability at least  either
\begin{itemize}
\item [(P.3.a)] , or
\item [(P.3.b)] .
\end{itemize}
\end{theorem}

Property (P.3) is a little unusual and deserves some explanation.
It says that for every set  of low conductance, with high probability
  either
   is a large fraction of , or it is a large fraction of the
  entire graph.
While we would like to pick just one of these properties and guarantee that
  it holds with high probability, this would be unreasonable:  
  on one hand, there might be no big set  of small conductance;
  and, on the other hand, even if  is small the algorithm might
  cut out a large set  that completely avoids .


\begin{proof}[Proof of Theorem~\ref{thm:Partition}]
The bound on the expected running time of \texttt{Partition}
  is immediate from the bound on the running time of \texttt{RandomNibble}.

Let  be the iteration at which \texttt{Partition} stops,
  so that .
To prove (P.1), note that
   and so
  .
By part (R.2) of Lemma~\ref{lem:randNibble},
  .
So,


To establish (P.2),
  we first compute

So, if  ,
  then .
On the other hand, we established above that
  , from which
  it follows that

So,


Let .
To prove part (P.3), let  satisfy \eqref{eqn:P3} and
  consider what happens if we ignore the second condition in the
  while loop and run
  \texttt{Partition} for all
  potential  iterations, obtaining
  cuts .
Let 

We will prove that if neither 

hold at iteration , then with probability at least , one of these
  conditions will be satisfied by iteration .
Thus, after all  iterations, one of conditions
   or  will be satisfied with probability
  at least .
If the algorithm runs for fewer iterations, then condition 
  is satisfied.

To simplify notation, let  and ,
  for .
Assume that

For , define the random variable

As each set  is a subset of , and the  are mutually
  disjoint, we will always have

Define  to satisfy

and note that this ensures .
Moreover, if , then 
  
  will hold.

Let   be the event

We need to show that, with probability at least , either an
  event  holds, or .
To this end, we now show that if neither 
  nor  holds,
  then .
If , then

If  does not hold, then

So,

We also have ,
 so the conditions of 
 part  of Lemma~\ref{lem:randNibble} are satisfied and


Now, set 

So, for all  we have , 
  and so .
On the other hand,
  

So, with probability at least , 

This implies that with probability at least  either
   or some event  holds,
  which is what we needed to show.
\end{proof}



\bibliographystyle{alpha}
\bibliography{precon}


\end{document}
