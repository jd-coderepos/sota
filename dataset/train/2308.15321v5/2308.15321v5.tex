
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{floatflt}
\usepackage{caption}
\usepackage{array}
\usepackage{amsmath}


\title{Elucidating the Exposure Bias in Diffusion Models}



\author{
Mang Ning \\  
Utrecht University\\
\texttt{m.ning@uu.nl} \\
\And
Mingxiao Li \\
KU Leuven\\
\texttt{mingxiao.li@cs.kuleuven.be} \\
\AND
Jianlin Su \\
Moonshot AI Ltd.\\
\texttt{bojone@spaces.ac.cn} \\
\And
Albert Ali Salah \\
Utrecht University\\
\texttt{a.a.salah@uu.nl} \\
\And
Itir Onal Ertugrul \\
Utrecht University\\
\texttt{i.onalertugrul@uu.nl} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Diffusion models have demonstrated impressive generative capabilities, but their \textit{exposure bias} problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion frameworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings, and deterministic vs. stochastic sampling verify the effectiveness of our method. Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation. The code is available at \url{https://github.com/forever208/ADM-ES} and \url{https://github.com/forever208/EDM-ES}
\end{abstract}



\section{Introduction}
Due to the outstanding generation quality and diversity, diffusion models \citep{pmlr-v37-sohl-dickstein15,DDPM,song2019generative} have achieved unprecedented success in image generation \citep{ADM,nichol2022glide,LDM,Imagen}, audio synthesis \citep{diffwave,wavegrad} and video generation~\citep{ho2022imagen}. Unlike generative adversarial networks (GANs) \citep{GAN}, variational autoencoders (VAEs) \citep{VAE} and flow-based models \citep{NICE,NVP}, diffusion models stably learn the data distribution through a noise/score prediction objective and progressively removes noise from random initial vectors in the iterative sampling stage. 

A key feature of diffusion models is that good sample quality requires a long iterative sampling chain since the Gaussian assumption of reverse diffusion only holds for small step sizes \citep{xiao2022tackling}. However, \citet{ning2023input} claim that the iterative sampling chain also leads to the 
\textit{exposure bias} problem \citep{DBLP:journals/corr/RanzatoCAZ15,DBLP:conf/emnlp/Schmidt19}. Concretely, given the noise prediction network $\pmb{\epsilon_{\theta}}(\cdot)$, exposure bias refers to the input mismatch between training and inference, where the former is always exposed to the ground truth training sample $\pmb{x}_t$ while the latter depends on the previously generated sample $\hat{\pmb{x}}_t$. The difference between $\pmb{x}_t$ and $\hat{\pmb{x}}_t$ causes the discrepancy between $\pmb{\epsilon_{\theta}}(\pmb{x}_t)$ and $\pmb{\epsilon_{\theta}}(\hat{\pmb{x}}_t)$, which leads to the error accumulation and the sampling drift \citep{li2023alleviating}. 

We point out that the exposure bias problem in diffusion models lacks in-depth exploration. For example, there is no proper metric to quantify the exposure bias and no explicit error analysis for it. To shed light on exposure bias, we conduct a systematical investigation in this paper by first modelling the sampling distribution with prediction error. Based on our analysis, we find that the practical sampling distribution has a variance larger than the ground truth distribution at every single step, demonstrating the analytic difference between $\pmb{x}_{t}$ in training and $\hat{\pmb{x}}_{t}$ in sampling. Along with the sampling distribution analysis, we propose a metric $\delta_t$ to evaluate exposure bias by comparing the variance difference between training and sampling. Finally, we discuss potential solutions to exposure bias, and propose a simple yet effective \textit{training-free and plug-in} method called Epsilon Scaling to alleviate this issue. 

We test our approach on four different diffusion frameworks using deterministic and stochastic sampling, and on conditional and unconditional generation tasks. Without affecting the recall and precision \citep{kynkaanniemi2019improved}, our method yields dramatic Fréchet Inception Distance (FID) \citep{FID} improvements. Also, we illustrate that Epsilon Scaling effectively reduces the exposure bias by moving the sampling trajectory towards the training trajectory. Overall, our contributions to diffusion models are:
\begin{itemize}
    \item We systematically investigate the exposure bias problem and propose a metric for it.
    \item We suggest potential solutions to the exposure bias issue and propose a training-free, plug-in method (Epsilon Scaling) which significantly improves the sample quality.
    \item Our extensive experiments demonstrate the generality of Epsilon Scaling and its wide applicability to different diffusion architectures.
\end{itemize}



\section{Related Work}
Diffusion models were introduced by \citet{pmlr-v37-sohl-dickstein15} and later improved by \citet{song2019generative}, \citet{DDPM} and \citet{IDDPM}. \citet{VPVE} unify score-based models and Denoising Diffusion Probabilistic Models (DDPMs) via stochastic differential equations. Furthermore, \citet{karras2022elucidating} disentangle the design space of diffusion models and introduce the EDM model to further boost the performance in image generation. With the advances in diffusion theory, conditional generation \citep{ho2022classifier, choi2021ilvr} also flourishes in various scenarios, including text-to-image generation \citep{nichol2022glide, DALL-E-2, LDM, Imagen}, controllable image synthesis \citep{zhang2023adding, li2023gligen, zheng2023layoutdiffusion}, as well as generating other modalities, for instance, audio \citep{wavegrad, diffwave}, object shape \citep{zhou20213d} and time series \citep{rasul2021autoregressive}. In the meantime, accelerating the time-consuming reverse diffusion sampling has been extensively investigated in many works \citep{DDIM,lu2022dpm,liu2022pseudo}. For example, distillation \citep{DBLP:conf/iclr/SalimansH22}, Restart sampler \citep{xu2023restart} and fast ODE samplers \citep{zhao2023unipc} have been proposed to speed up the sampling.


The phenomenon of exposure bias within the diffusion model was first identified by \citet{ning2023input}. They introduced an extra noise at each step during the training to mitigate the discrepancy between training and inference, thereby reducing the impact of exposure bias. Additionally, another approach presented by~\citet{li2023alleviating} sought to address exposure bias without necessitating retraining of the model. Their method involved a manipulation of the time step during the backward generation process. However, the exposure bias in diffusion models still lacks illuminating research in terms of the explicit sampling distribution, metric and root cause, which is the objective of this paper. Besides, we propose a solution called Epsilon Scaling in the sampling phase based on the observation of the prediction deviation between training and sampling. This method, while straightforward, proves effective in mitigating the exposure bias issue.



\section{Exposure Bias in Diffusion Models}
\label{sec: Exposure Bias in Diffusion Models}

\subsection{Background}
We first briefly review DDPMs \citep{DDPM}. Given a sample $\pmb{x}_0$ from the data distribution $q(\pmb{x}_0)$ 
and a well-behaved noise schedule ($\beta_1, ..., \beta_T$), DDPM defines the forward process as a Markov chain $q(\pmb{x}_{1:T} | \pmb{x}_0) = \prod_{t=1}^T q(\pmb{x}_t | \pmb{x}_{t-1})$ and iteratively adds Gaussian noise by $q(\pmb{x}_t | \pmb{x}_{t-1}) = {\cal N} (\pmb{x}_t; \sqrt{1- \beta_t} \pmb{x}_{t-1}, \beta_t \pmb{I})$ until obtaining the prior $\pmb{x}_T~\sim~{\cal N} (\pmb{0}, \pmb{I})$. The Gaussian forward process allows us to sample $\pmb{x}_t$ directly conditioned on the input $\pmb{x}_0$:
\begin{equation}
\label{eq4}
q(\pmb{x}_t | \pmb{x}_0) = {\cal N} (\pmb{x}_t; \sqrt{\bar{\alpha}_t} \pmb{x}_0, (1-\bar{\alpha}_t) \pmb{I}), \qquad \pmb{x}_t = \sqrt{\bar{\alpha}_t} \pmb{x}_0 +   \sqrt{1 - \bar{\alpha}_t} \pmb{\epsilon},
\end{equation}

\noindent
where $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$, $\alpha_t = 1-\beta_t$ and $\pmb{\epsilon} \sim {\cal N} (\pmb{0}, \pmb{I})$.
Then, the reverse distribution $q(\pmb{x}_{t-1}|\pmb{x}_t)$ is approximated by a neural network, from which we can sample $\pmb{x}_T \sim {\cal N} (\pmb{0},\pmb{I})$ and iteratively run the reverse process $p_{\pmb{\theta}}(\pmb{x}_{t-1} | \pmb{x}_t) = {\cal N} (\pmb{x}_{t-1}; \mu_{\pmb{\theta}}(\pmb{x}_t, t), \sigma_t \pmb{I})$ to get a sample from $q(\pmb{x}_0)$. The optimisation objective is $D_{KL}(q(\pmb{x}_{t-1}|\pmb{x}_{t},\pmb{x}_{0}) \,||\, p_{\pmb{\theta}}(\pmb{x}_{t-1}|\pmb{x}_{t})))$ in which the ground truth forward process posterior $q(\pmb{x}_{t-1}|\pmb{x}_{t},\pmb{x}_{0})$ is tractable when conditioned on $\pmb{x}_0$ using Bayes theorem:
\begin{equation}
\label{eq7}
q(\pmb{x}_{t-1} | \pmb{x}_{t}, \pmb{x}_{0}) = {\cal N} (\pmb{x}_{t-1}; \pmb{\tilde{\mu}}(\pmb{x}_t, \pmb{x}_0), \tilde{\beta_t} \pmb{I})
\end{equation}

\begin{figure}[ht]
\vskip -0.2in
    \begin{minipage}{0.59\linewidth}
        \begin{equation}
            \label{eq8}
            \pmb{\tilde{\mu}}(\pmb{x}_t, \pmb{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \pmb{x}_{0} + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \pmb{x}_{t}
        \end{equation}
    \end{minipage}
    \hfill
    \begin{minipage}{0.39\linewidth}
        \begin{equation}
            \label{eq9}
            \tilde{\beta_{t}} = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}
        \end{equation}
    \end{minipage}
\end{figure}


Regarding parameterising $\mu_{\pmb{\theta}}(\pmb{x}_t, t)$, \citet{DDPM} found that using a neural network to predict $\pmb{\epsilon}$ (Eq. \ref{eq11}) worked better than predicting $\pmb{x}_0$ (Eq. \ref{eq10}) in practice:
\begin{align}
\mu_{\pmb{\theta}}(\pmb{x}_t, t) & = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \pmb{x_{\theta}}(\pmb{x}_t, t) + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \pmb{x}_{t} \label{eq10} \\
& = \frac{1}{\sqrt{\alpha_t}}(\pmb{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \pmb{\epsilon_{\theta}}(\pmb{x}_t, t)), \label{eq11}
\end{align}

\noindent
where $\pmb{x_{\theta}}(\pmb{x}_t, t)$ denotes the denoising model which predicts $\pmb{x}_0$ given $\pmb{x}_t$. \textit{For simplicity, we use $\pmb{x}^{t}_{\pmb{\theta}}$ as the short notation of $\pmb{x_{\theta}}(\pmb{x}_t, t)$} in the rest of this paper.



\subsection{Sampling Distribution with Prediction Error}
\label{sec: Sampling Distribution with Prediction Error}

The exposure bias problem of diffusion models is described as the input mismatch between $\pmb{x}_t$ during training and $\hat{\pmb{x}}_t$ during sampling \citep{ning2023input}. In this section, we explicitly derive the sampling distribution $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ and compare it with the training distribution $q(\pmb{x}_{t} | \pmb{x}_0)$, where the former shows the $\hat{\pmb{x}}_t$ seen by the network in the sampling phase, while the latter is the $\pmb{x}_t$ seen by the network during training at timestep $t$.

Comparing Eq.~\ref{eq8} with Eq.~\ref{eq10}, \citet{DDIM} emphasise that the sampling distribution $p_{\pmb{\theta}}(\pmb{x}_{t-1} | \pmb{x}_t)$ is in fact parameterised as $p_{\pmb{\theta}}(\pmb{x}_{t-1} | \pmb{x}_t) = q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ where $\pmb{x}^{t}_{\pmb{\theta}}$ means the predicted $\pmb{x}_0$ given $\pmb{x}_t$. Therefore, the practical sampling paradigm is that we first predict $\pmb{\epsilon}$ using $\pmb{\epsilon}_{\pmb{\theta}} (\pmb{x}_t, t)$. Then we derive the estimation $\pmb{x}^{t}_{\pmb{\theta}}$ for $\pmb{x}_0$ using Eq. \ref{eq4}. Finally, based on the ground truth posterior $q(\pmb{x}_{t-1} | \pmb{x}_{t}, \pmb{x}_{0})$, $\pmb{x}_{t-1}$ is generated using $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ by replacing $\pmb{x}_0$ with $\pmb{x}^{t}_{\pmb{\theta}}$. 
Since $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}}) = q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}_0)$ holds only if $\pmb{x}^{t}_{\pmb{\theta}} = \pmb{x}_0$, this requires the network to make no prediction error about $\pmb{x}_0$ to ensure $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ share the same variance with $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}_0)$. However, $\pmb{x}^{t}_{\pmb{\theta}} - \pmb{x}_0$ is practically non-zero and we claim that the prediction error of $\pmb{x}_0$ needs to be considered to derive the real sampling distribution. Following Analytic-DPM  \citep{bao2022analytic} and \citet{bao2022estimating}, we model $\pmb{x}^{t}_{\pmb{\theta}}$ as $p_{\pmb{\theta}}(\pmb{x}_{0} | \pmb{x}_{t})$ and approximate it by a Gaussian distribution:
\begin{equation}
\label{eq15}
p_{\pmb{\theta}}(\pmb{x}_{0} | \pmb{x}_{t}) = {\cal N} (\pmb{x}^{t}_{\pmb{\theta}}; \pmb{x}_{0}, e_{t}^{2} \pmb{I}), \qquad \pmb{x}^{t}_{\pmb{\theta}} = \pmb{x}_{0} + e_t \pmb{\epsilon}_0 \quad (\pmb{\epsilon}_0 \sim {\cal N} (\pmb{0}, \pmb{I}))
\end{equation}



\noindent
Taking the prediction error into account, we now compute $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$, which is the same distribution as $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$, by substituting with the index $t+1$ and using $\hat{\pmb{x}}_{t}$ to highlight that it is generated in the sampling stage. Based on Eq.~\ref{eq7}, we know $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}}) = {\cal N} (\hat{\pmb{x}}_{t}; \mu_{\pmb{\theta}}(\pmb{x}_{t+1}, t+1), \tilde{\beta}_{t+1} \pmb{I})$. Its mean and variance can be further derived according to Eq. \ref{eq10} and Eq. \ref{eq9}, respectively. Thus, a sample from the distribution is $\hat{\pmb{x}}_{t} = \mu_{\pmb{\theta}}(\pmb{x}_{t+1}, t+1) + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber$, namely:
\begin{equation}
\label{eq17}
\hat{\pmb{x}}_{t} = \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} \pmb{x}^{t+1}_{\pmb{\theta}} + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \pmb{x}_{t+1} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1
\end{equation}

\noindent
Plugging Eq.~\ref{eq15} and Eq.~\ref{eq4} (using index $t+1$ ) into Eq.~\ref{eq17}, we obtain the final analytical form of $\hat{\pmb{x}}_{t}$ (see Appendix \ref{Append:1} for the full derivation):
\noindent
\begin{equation}
\label{eq18}
\hat{\pmb{x}}_{t} = \sqrt{\bar{\alpha}_t} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_t + (\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2 } \pmb{\epsilon}_3
\end{equation}

\noindent
where, $\pmb{\epsilon}_1, \pmb{\epsilon}_3 \sim {\cal N} (\pmb{0}, \pmb{I})$. From Eq.~\ref{eq18}, we obtain the mean and variance of $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ and compare them with the parameters of $q(\pmb{x}_{t} | \pmb{x}_0)$. \textit{For simplicity, we denote $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ as $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ from now on}. In Table~\ref{tab: sampling distribution}, $q(\pmb{x}_{t} | \pmb{x}_0)$ shows the $\pmb{x}_{t}$ seen by the network during training while $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ indicates the $\hat{\pmb{x}}_{t}$ exposed to the network during sampling. Note that, the method of solving out $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ can be generalised to $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ (see Appendix \ref{Append:1.1}). In the same spirit of modelling $\pmb{x}^{t}_{\pmb{\theta}}$ as a Gaussian, we also derived the sampling distribution $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ for DDIM \citep{DDIM} in Appendix \ref{Append:1.5}. 

\begin{table}[ht]
\vskip -0.1in
\captionsetup{skip=2pt}
\caption{
The distribution $q(\pmb{x}_{t} | \pmb{x}_0)$ during training and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ during DDPM sampling.}
\label{tab: sampling distribution}
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
 & Mean & Variance \\ \midrule
$q(\pmb{x}_{t} | \pmb{x}_0)$ & $\sqrt{\bar{\alpha}_t} \pmb{x}_0$ & $(1-\bar{\alpha}_t) \pmb{I}$ \\
$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$  & $\sqrt{\bar{\alpha}_t} \pmb{x}_0$ & $(1-\bar{\alpha}_t + (\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2) \pmb{I}$ \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.2in
\end{table}



\subsection{Exposure Bias Due to Prediction Error}
\label{subsec: Exposure Bias Due to Prediction Error}

It is clear from Table \ref{tab: sampling distribution} that the variance of the sampling distribution $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ is always larger than the variance of the training distribution $q(\pmb{x}_{t} | \pmb{x}_0)$ by the magnitude $(\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2$. Note that, this variance gap between training and sampling is produced just in a single reverse diffusion step, given that the network $\pmb{\epsilon_{\theta}}(\cdot)$ can get access to the ground truth input $\pmb{x}_{t+1}$. What makes the situation worse is that the error of single-step sampling accumulates in the multi-step sampling, resulting in an explosion of sampling variance error. For instance, $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ (in Appendix \ref{Append:1.1}) shows the variance error in two consecutive sampling steps when compared with $q(\pmb{x}_{t-1} | \pmb{x}_0)$. On CIFAR-10 \citep{cifar10}, we designed an experiment to statistically measure both the single-step variance error of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ and multi-step variance error of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ using $20$-step sampling (see Appendix \ref{Append:2}). The results in Fig. \ref{fig: xt_std_error} indicate that the closer to $t=1$ (the end of sampling), the larger the variance error of multi-step sampling. The explosion of sampling variance error results in the sampling drift (exposure bias) problem and we attribute the prediction error $\pmb{x}^{t}_{\pmb{\theta}} - \pmb{x}_0$ as the root cause of the exposure bias in diffusion models.

\begin{wrapfigure}{r}{0.4\textwidth}
\vskip -0.2in
  \includegraphics[width=0.4\textwidth]{figures/cifar10_xt_std_error.pdf}
  \captionsetup{skip=2pt}
  \caption{Variance error in single-step and multi-step samplings.}
  \label{fig: xt_std_error}
\vskip -0.3in
\end{wrapfigure}


Intuitively, a possible solution to exposure bias is using a sampling noise variance $\beta^{'}$, which is smaller than $\tilde{\beta}_{t}$, to counteract the extra variance term $(\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2$ caused by the prediction error $\pmb{x}^{t}_{\pmb{\theta}} - \pmb{x}_0$. Unfortunately, $\tilde{\beta}_{t}$ is the lower bound of the sampling noise schedule $ \dot{\beta_t} \in [\tilde{\beta_t}, \beta_t]$, where the lower bound and upper bound are the sampling variances given by $q(\pmb{x}_0)$ being a delta function and isotropic Gaussian function, respectively \citep{IDDPM}. Therefore, we can draw a conclusion that the exposure bias problem can not be alleviated by manipulating the sampling noise schedule $\dot{\beta_t}$.

Interestingly, \citet{bao2022analytic} analytically provide the optimal sampling noise schedule $\beta^{\star}_t$ which is larger than the lower bound $\tilde{\beta_t}$. Based on what we discussed earlier, $\beta^{\star}_t$ would cause a more severe exposure bias issue than $\tilde{\beta_t}$. A strange phenomenon, but not explained by \citet{bao2022analytic} is that $\beta^{\star}_t$ leads to a worse FID than using $\tilde{\beta_t}$ under $1000$ sampling steps. We believe the exposure bias is in the position to account for this phenomenon: under the long sampling, the negative impact of exposure bias exceeds the positive gain of the optimal variance $\beta^{\star}_t$.



\subsection{Metric for Exposure Bias}
Although some literature has already discussed the exposure bias problem in diffusion models \citep{ning2023input, li2023alleviating}, there still lacks a well-defined and straightforward metric for this concept. We propose to use the variance error of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ to quantify the exposure bias at timestep $t$ under multi-step sampling. Specifically, our metric $\delta_t$ for exposure bias is defined as $\delta_t = (\sqrt{\hat{\beta}_t} - \sqrt{\bar{\beta}_t})^2 $, where $\bar{\beta}_t = 1-\bar{\alpha}_t$ denotes the variance of $q(\pmb{x}_{t} | \pmb{x}_0)$ during training and $\hat{\beta}_t$ presents the variance of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ in the regular sampling process. The metric $\delta_t$ is inspired by the Fréchet distance \citep{dowson1982frechet} between $q(\pmb{x}_{t} | \pmb{x}_0)$ and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$, which is $d^2 = N(\sqrt{\hat{\beta}_t} - \sqrt{\bar{\beta}_t})^2$ where $N$ is the dimensions of $\pmb{x}_{t}$. In Appendix \ref{Append: delta_t and FID}, we empirically find that $\delta_t$ exhibits a strong correlation with FID given a trained model. Our method of measuring $\delta_t$ is described in Algorithm \ref{alg: exposure bias measurement} (see Appendix \ref{Append:0}). The key step of Algorithm \ref{alg: exposure bias measurement} is that we subtract the mean $\sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ and the remaining term $\hat{\pmb{x}}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ corresponds to the stochastic term of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{T})$.


\subsection{Solution Discussion}
We now discuss possible solutions to the exposure bias issue of diffusion models based on the analysis throughout Section \ref{sec: Exposure Bias in Diffusion Models}. Recall that the prediction error of $\pmb{x}^{t}_{\pmb{\theta}} - \pmb{x}_0$ is the root cause of exposure bias. Thus, the most straightforward way of reducing exposure bias is learning an accurate  $\pmb{\epsilon}$ or score function \citep{song2019generative} prediction network. For example, by delicately designing the network and hyper-parameter tuning, EDM \citep{karras2022elucidating} improves the FID from 3.01 to 2.51 on CIFAR-10 dataset, presenting a significant improvement. Secondly, we believe that data augmentation can reduce the risk of learning inaccurate $\pmb{\epsilon}$ or score function for $\hat{\pmb{x}}_{t}$ by learning a denser vector field than vanilla diffusion models. For instance, \citet{karras2022elucidating} has shown that the geometric augmentation \citep{karras2020training} benefits the network training and sample quality. In the same spirit, DDPM-IP \citep{ning2023input} augments each training sample $\pmb{x}_{t}$ by a Gaussian term and achieves substantial improvements in FID. 

It is worth pointing out that the above-mentioned methods require retraining the network and expensive parameter searching during the training. This naturally drives us to the question: \textit{Can we alleviate the exposure bias in the sampling stage, without any retraining}?




\section{Method}

\subsection{Epsilon Scaling}
In Section \ref{subsec: Exposure Bias Due to Prediction Error}, we have concluded that the exposure bias issue can not be solved by reducing the sampling noise variance, thus another direction to be explored in the sampling phase is the prediction of the network $\pmb{\epsilon_{\theta}}(\cdot)$. Since we already know from Table \ref{tab: sampling distribution} that $\pmb{x}_t$ inputted to the network $\pmb{\epsilon_{\theta}}(\cdot)$ in training differs from $\hat{\pmb{x}}_t$ fed into the network $\pmb{\epsilon_{\theta}}(\cdot)$ in sampling, we are interested in understanding the difference in the output of $\pmb{\epsilon_{\theta}}(\cdot)$ between training and inference.

\begin{wrapfigure}{r}{0.4\textwidth}
\vskip -0.2in
  \includegraphics[width=0.4\textwidth]{figures/eps_norm.pdf}
  \captionsetup{skip=2pt}
  \caption{$\left\| \pmb{\epsilon_{\theta}}(\cdot) \right\|_2$ during training and sampling on CIFAR-10. We use 20-step sampling and report the L2-norm using 50k samples at each timestep.}
\label{fig: eps_norm}
\vskip -0.4in
\end{wrapfigure}

For simplicity, we denote the output of $\pmb{\epsilon_{\theta}}(\cdot)$ as $\pmb{\epsilon}^{t}_{\pmb{\theta}}$ in training and as $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ in sampling. Although the ground truth of $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ is not accessible during inference, we are still able to speculate the behaviour of $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ from the L2-norm perspective. 
In Fig. \ref{fig: eps_norm}, we plot the L2-norm of $\pmb{\epsilon}^{t}_{\pmb{\theta}}$ and $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ at each timestep. In detail, given a trained, frozen model and ground truth $\pmb{x}_t$, $\pmb{\epsilon}^{t}_{\pmb{\theta}}$ is collected by $\pmb{\epsilon}^{t}_{\pmb{\theta}} = \pmb{\epsilon_{\theta}}(\pmb{x}_t, t)$. In this way, we simulate the training stage and analyse its $\pmb{\epsilon}$ prediction. In contrast, $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ is gathered in the real sampling process, namely $\pmb{\epsilon}^{s}_{\pmb{\theta}} = \pmb{\epsilon_{\theta}}(\hat{\pmb{x}}_t, t)$. It is clear from Fig. \ref{fig: eps_norm} that the L2-norm of $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ is always larger than that of $\pmb{\epsilon}^{t}_{\pmb{\theta}}$. Since $\hat{\pmb{x}}_t$ lies around $\pmb{x}_t$ with a larger variance (Section \ref{sec: Sampling Distribution with Prediction Error}), we can infer the network learns an inaccurate vector field $\pmb{\epsilon_{\theta}}(\hat{\pmb{x}}_t, t)$ for each $\hat{\pmb{x}}_t$ in the vicinity of $\pmb{x}_t$ with the vector length longer than that of $\pmb{\epsilon_{\theta}}(\pmb{x}_t, t)$. 



One can infer that the prediction $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ could be improved if we can move the input $(\hat{\pmb{x}}_t, t)$ from the inaccurate vector field (green curve in Fig. \ref{fig: eps_norm}) towards the reliable vector field (red curve in Fig. \ref{fig: eps_norm}). To this end, we propose to scale down $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ by a factor $\lambda_t$ at sampling timestep $t$. Our solution is based on the observation: $\pmb{\epsilon}^{t}_{\pmb{\theta}}$ and $\pmb{\epsilon}^{s}_{\pmb{\theta}}$ share the same input $\pmb{x}_T \sim {\cal N} (\pmb{0}, \pmb{I})$ at timestep $t=T$, but from timestep $T-1$, $\hat{\pmb{x}}_t$ (input of $\pmb{\epsilon}^{s}_{\pmb{\theta}}$) starts to diverge from $\pmb{x}_t$ (input of $\pmb{\epsilon}^{t}_{\pmb{\theta}}$) due to the $\pmb{\epsilon_{\theta}}(\cdot)$ error made at previous timestep. This iterative process continues along the sampling chain and results in exposure bias. Therefore, we can push $\hat{\pmb{x}}_t$ closer to $\pmb{x}_t$ by scaling down the over-predicted magnitude of $\pmb{\epsilon}^{s}_{\pmb{\theta}}$. According to the regular sampling (Eq. \ref{eq11}), our sampling method only differs in the $\lambda_t$ term and is expressed as $\mu_{\pmb{\theta}}(\pmb{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}(\pmb{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \frac{\pmb{\epsilon_{\theta}}(\pmb{x}_t, t)}{\lambda_t} )$. Note that, our Epsilon Scaling serving as a plug-in method adds no computational load to the original sampling of diffusion models.



\subsection{The Design of Scaling Schedule}
Intuitively, the scaling schedule $\lambda_t$ should be directly decided by the L2-norm quotient $\left\| \pmb{\epsilon}^{s}_{\pmb{\theta}} \right\|_2 / \left\| \pmb{\epsilon}^{t}_{\pmb{\theta}} \right\|_2$, denoted as $\Delta N (t)$, at each timestep $t$. However, we emphasise that $\Delta N (t)$ reflects the accumulated effect of the biased prediction error made at each timestep $T, T-1,..., t$. Suppose the L2-norm of $\pmb{\epsilon}^{t}_{\pmb{\theta}}$ to be scaled at timestep $t$ is $\lambda_t$, then $\Delta N (t)$ satisfy $\Delta N (t) = \int_{t}^{T} \lambda_t dt$ given a linearly distributed vector field learned by $\pmb{\epsilon_{\theta}}(\cdot)$ in the vicinity of $\pmb{x}_{t}$: $\left\| \hat{\pmb{x}}_t - \pmb{x}_{t} \right\|_2 = c (\left\| \pmb{\epsilon_{\theta}}(\hat{\pmb{x}}_t) \right\|_2 - \left\| \pmb{\epsilon_{\theta}}(\pmb{x}_{t}) \right\|_2)$, where $c$ is a constant. In practice, $\Delta N (t) \approx \int_{t}^{T} \lambda_t dt$ holds for a non-overfitting network $\pmb{\epsilon_{\theta}}(\cdot)$. 

\begin{wrapfigure}{r}{0.4\textwidth}
\vskip -0.2in
  \includegraphics[width=0.4\textwidth]{figures/eps_norm_diff.pdf}
  \captionsetup{skip=2pt}  \caption{$\Delta N (t)$ at each timestep.}
\label{fig: eps_norm_diff}
\vskip -0.1in
\end{wrapfigure}

As shown by \citet{IDDPM} and \citet{benny2022dynamic}, the $\pmb{\epsilon_{\theta}}(\cdot)$ predictions near $t=0$ are very bad, with the loss larger than other timesteps by several orders of magnitude. Thereby, we can ignore the area close to $t=0$ to design $\lambda_t$, because scaling a problematic $\pmb{\epsilon_{\theta}}(\cdot)$ does not lead to a better predicted $\pmb{\epsilon}$. We plot the $\Delta N (t)$ curve in the cases of 20-step and 50-step sampling on CIFAR-10 in Fig. \ref{fig: eps_norm_diff}. It shows that $\Delta N (t)$ can be fitted by a quadratic function in the interval $t\sim(5, T)$. Thereby, the integrand $\lambda_t$ is a linear function $\lambda_t = kt+b$ where $k,b$ are constants. Another observation from Fig. \ref{fig: eps_norm_diff} is that $\Delta N (t)$ under 50-step sampling has a smaller curvature than 20-step sampling. This tendency applies to a larger sampling step, for example, 100-step sampling presents a flatter curvature than 50-step sampling in our experiments. Overall, the design principle for $\lambda_t$ is that the longer the sampling step, the smaller $k$ we should use. In Section \ref{subsec: ADM results}, we will see that $k$ is practically a small number and would decay to 0 around 50 sampling steps.



\section{Results}
\label{sec: results}
In this section, we evaluate the performance of Epsilon Scaling using FID \citep{FID}. To demonstrate that Epsilon Scaling is a generic solution to exposure bias, we test the approach on various diffusion frameworks, samplers and conditional settings. Following the fast sampling paradigm \citep{karras2022elucidating} in the diffusion community, we apply the sampling steps $T'$ less than the training diffusion step $T$ and we focus on $T'\leqslant100$ for practical usages. Our FID computation is consistent with
\citep{ADM} for equal comparison. All FIDs are reported using 50k generated samples and the full training set as the reference batch, except for the LSUN dataset where we follow \citep{ADM} and use 50k training samples as the reference batch. Lastly, Epsilon Scaling does not affect the precision and recall, and we report these results in Appendix~\ref{Append: recall and precision}.


\subsection{Main Results on ADM}
\label{subsec: ADM results}
Since Epsilon Scaling is a training-free method, we utilise the pre-trained ADM model as the baseline and compare it against our ADM-ES (ADM with Epsilon Scaling) on datasets CIFAR-10 \citep{cifar10}, LSUN tower \citep{LSUN} and FFHQ \citep{karras2019style} for unconditional generation and on datasets ImageNet 64$\times$64 and ImageNet 128$\times$128 \citep{imagenet32} for class-conditional generation. We employ the respacing sampling technique \citep{IDDPM} to enable fast stochastic sampling.


\begin{table*}[ht]
\vskip -0.1in
\scriptsize
  \begin{minipage}{0.57\linewidth}
    \centering
    \caption{FID on ADM baseline. We compare ADM with our ADM-ES (uniform $\lambda_t$) and ADM-ES$^*$ (linear $\lambda_t$). ImageNet 64$\times64$ results are reported without classifier guidance and ImageNet 128$\times128$ is under classifier guidance with scale$=$0.5}
    \setlength{\tabcolsep}{4pt}  \begin{tabular}{@{}lllllll@{}}
    \toprule
    \multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Unconditional} & \multicolumn{2}{c}{Conditional} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-7} 
     &  & \begin{tabular}[c]{@{}l@{}}CIFAR-10\\ 32$\times$32\end{tabular} & \begin{tabular}[c]{@{}l@{}}LSUN\\ 64$\times$64\end{tabular} & \begin{tabular}[c]{@{}l@{}}FFHQ\\ 128$\times$128\end{tabular} & \begin{tabular}[c]{@{}l@{}}ImageNet\\ 64$\times$64\end{tabular} & \begin{tabular}[c]{@{}l@{}}ImageNet\\ 128$\times$128\end{tabular} \\ \midrule
    100 & ADM & 3.37 & 3.59 & 14.52 & 2.71 & 3.55 \\
     & ADM-ES & \textbf{2.17} & \textbf{2.91} & \textbf{6.77} & \textbf{2.39} & \textbf{3.37} \\ \midrule
    \multirow{2}{*}{50} & ADM & 4.43 & 7.28 & 26.15 & 3.75 & 5.15 \\
     & ADM-ES & \textbf{2.49} & \textbf{3.68} & \textbf{9.50} & \textbf{3.07} & \textbf{4.33} \\ \midrule
    \multirow{3}{*}{20} & ADM & 10.36 & 23.92 & 59.35 & 10.96 & 12.48 \\
     & ADM-ES & 5.15 & 8.22 & 26.14 & 7.52 & 9.95 \\
     & ADM-ES$^*$ & \textbf{4.31} & \textbf{7.60} & \textbf{24.83} & \textbf{7.37} & \textbf{9.86} \\ \bottomrule
    \end{tabular}
    \label{tab: ADM results}
  \end{minipage}\hfill
  \begin{minipage}{0.4\linewidth}
    \centering
    \caption{We compare ADM-ES with recent stochastic diffusion (SDE) samplers regarding FID. We report their best FID achieved under $T'$ sampling steps.}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}lll@{}}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{$T'$} & Unconditional \\ \cmidrule(l){3-3} 
     &  & \begin{tabular}[c]{@{}l@{}}CIFAR-10\\ 32$\times$32\end{tabular} \\ \midrule
    EDM (VP) {\tiny\citep{karras2022elucidating}} & 511 & 2.27 \\
    EDM (VE) {\tiny\citep{karras2022elucidating}} & 2047 & 2.23 \\
    Improved SDE {\tiny\citep{karras2022elucidating}} & 1023 & 2.35 \\
    Restart (VP) {\tiny\citep{xu2023restart}} & 115 & 2.21 \\
    SA-Solver {\tiny\citep{xue2023sa}}  & 95 & 2.63 \\
    ADM-IP {\tiny\citep{ning2023input}} & 100 & 2.38 \\
    ADM-ES (ours) & \textbf{50} & 2.49 \\
    ADM-ES (ours) & 100 & \textbf{2.17} \\ \bottomrule
    \end{tabular}
    \label{tab: ADM-ES SOTA}
  \end{minipage}
\vskip -0.1in
\end{table*}



Table~\ref{tab: ADM results} shows that independently of the dataset and the number of sampling steps $T'$, our ADM-ES outperforms ADM by a large margin in terms of FID, indicating the remarkable effectiveness of Epsilon Scaling. For instance, on FFHQ 128$\times$128, ADM-ES exhibits less than half the FID of ADM, with 7.75, 16.65 and 34.52 FID improvement under 100, 50 and 20 sampling steps, respectively. Moreover, when compared with the previous best stochastic samplers, ADM-ES outperforms EDM \citep{karras2022elucidating}, Improved SDE \citep{karras2022elucidating}, Restart Sampler \citep{xu2023restart} and SA-Solver \citep{xue2023sa}, exhibiting state-of-the-art stochastic sampler (SDE solver). For example, ADM-ES not only achieves a better FID (2.17) than EDM and Improved SDE, but also accelerates the sampling speed by 5-fold to 20-fold (see Table \ref{tab: ADM-ES SOTA}). Even under 50-step sampling, Epsilon Scaling surpasses SA-Solver and obtains competitive FID against other samplers. 


Note that, ADM-ES refers to the uniform schedule $\lambda_t=b$ and ADM-ES$^*$ applies the linear schedule $\lambda_t=kt+b$ in Table~\ref{tab: ADM results}. In our experiments, we find that the slope $k$ is approaching 0 as the sampling step $T'$ increases. Taking CIFAR-10 as an example, ADM-ES$^*$ gains 0.84 FID improvement over ADM-ES at 20-step sampling with $k=0.0005$. By contrast, using 50-step sampling, the optimal $k=0.00007$ of ADM-ES$^*$ yields only 0.02 FID improvement (not shown in Table~\ref{tab: ADM results}) compared with ADM-ES. Given this fact, we suggest a uniform schedule $\lambda_t$ for practical application and the easy searching of the parameter $b$. We present the full parameters $k,b$ used in all experiments in Appendix \ref{Append: parameters} and provide detailed guidance on the choice of $k, b$. Overall, $\lambda_t$ is around 1.005 on ADM baseline and a smaller $\lambda_t$ should be used when a larger $T'$ is chosen. 


\subsection{Epsilon Scaling Alleviates Exposure Bias}
\label{sec: alleviate exposure bias}
Apart from the FID improvements, we now show the exposure bias alleviated by our method using the proposed metric $\delta_t$ and we also demonstrate the sampling trajectory corrected by Epsilon Scaling. Using Algorithm \ref{alg: exposure bias measurement}, we measure $\delta_t$ on the dataset CIFAR-10 under 20-step sampling for ADM and ADM-ES models. Fig.~\ref{fig: xt_var_error} shows that ADM-ES obtains a lower $\delta_t$ at the end of sampling $t=1$ than the baseline ADM, exhibiting a smaller variance error and sampling drift (see Appendix \ref{Append: Alleviates Exposure Bias} for results on other datasets). 


Based on Fig.~\ref{fig: eps_norm}, we apply the same method to measure the L2-norm of $\pmb{\epsilon_{\theta}}(\cdot)$ in the sampling phase with Epsilon Scaling. Fig.~\ref{fig: eps_norm_solution} indicates that our method explicitly moves the original sampling trajectory closer to the vector field learned in the training phase given the condition that the $\left\| \pmb{\epsilon_{\theta}}(\pmb{x}_{t}) \right\|_2$ is locally monotonic around $\pmb{x}_{t}$. This condition is satisfied in denoising networks~\citep{DeepLearning,song2019generative} because of the monotonic score vectors around the local maximal probability density. We emphasise that Epsilon Scaling corrects the magnitude error of $\pmb{\epsilon_{\theta}}(\cdot)$, but not the direction error. Thus we can not completely eliminate the exposure bias to achieve $\delta_t=0$ or push the sampling trajectory to the exact training vector field. 






\begin{figure*}[ht]
\vskip -0.1in
\scriptsize
  \begin{minipage}{0.46\linewidth}
    \begin{center}
    \centerline{\includegraphics[width=0.95\columnwidth]{figures/lsun_exposure_bias.pdf}}
    \caption{Exposure bias measured by $\delta_t$ on LSUN 64$\times$64. Epsilon Scaling achieves a smaller $\delta_t$ at the end of sampling ($t=1$)
    }
    \label{fig: xt_var_error}
    \end{center}
  \end{minipage}\hfill
  \begin{minipage}{0.52\linewidth}
    \begin{center}
    \centerline{\includegraphics[width=0.9\columnwidth]{figures/lsun_eps_norm_solution.pdf}}
    \caption{$\left\| \pmb{\epsilon_{\theta}}(\cdot) \right\|_2$ on LSUN 64$\times$64. After applying Epsilon Scaling, the sampling $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (blue) gets closer to the training $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (red).
    }
    \label{fig: eps_norm_solution}
    \end{center}
  \end{minipage}
\vskip -0.2in
\end{figure*}



\subsection{Results on DDIM/DDPM}
To show the generality of our proposed method, we conduct experiments on DDIM/DDPM framework across CIFAR-10 and CelebA 64$\times$64 datasets \citep{liu2015faceattributes}. The results are detailed in Table~\ref{tab: DDIM results}, wherein the designations $\eta=0$ and $\eta=1$ correspond to DDIM and DDPM samplers, respectively. The findings in Table 3 illustrate that our method can further boost the performance of both DDIM and DDPM samplers on the CIFAR-10 and CelebA datasets. Specifically, our proposed Epsilon Scaling technique improves the performance of DDPM sampler on CelebA dataset by $47.7\%$, $63.1\%$, $60.7\%$ with $20$, $50$, and $100$ sampling steps, respectively. Similar performance improvement can also be observed on CIFAR-10 dataset. We also notice that our method brings less performance improvement for DDIM sampler. This could arise from the FID advantage of deterministic sampling under a short sampling chain and the noise term in DDPM sampler can actively correct for errors made in earlier sampling steps \citet{karras2022elucidating}.


\begin{table*}[ht]
\vskip -0.1in
\scriptsize
  \begin{minipage}{0.52\linewidth}
    \caption{
    FID on DDIM baseline for unconditional generations.}
    \label{tab: DDIM results}
    \begin{center}
    \begin{tabular}{@{}llllll@{}}
    \toprule
    \multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}CIFAR-10\\ 32$\times$32\end{tabular}} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}CelebA\\ 64$\times$64\end{tabular}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
     &  & $\eta=0$ & $\eta=1$ & $\eta=0$ & $\eta=1$ \\ \midrule
    100 & DDIM & 4.06 & 6.73 & 5.67 & 11.33 \\
     & DDIM-ES (ours) & \textbf{3.38} & \textbf{4.01} & \textbf{5.05} & \textbf{4.45} \\ \midrule
    \multirow{2}{*}{50} & DDIM & 4.82 & 10.29 & 6.88 & 15.09 \\
     & DDIM-ES & \textbf{4.17} & \textbf{4.57} & \textbf{6.20} & \textbf{5.57} \\ \midrule
    \multirow{2}{*}{20} & DDIM & 8.21 & 20.15 & 10.43 & 22.61 \\
     & DDIM-ES & \textbf{6.54} & \textbf{7.78} & \textbf{10.38} & \textbf{11.83} \\ \bottomrule
    \end{tabular}
    \end{center}
  \end{minipage}\hfill
  \begin{minipage}{0.46\linewidth}
    \caption{
    FID on EDM baseline and CIFAR-10 dateset.}
    \label{tab: EDM results}
    \begin{center}
    \begin{tabular}{@{}llllll@{}}
    \toprule
    \multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{2}{l}{Unconditional} & \multicolumn{2}{l}{Conditional} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
     &  & Heun & Euler & Heun & Euler \\ \midrule
    \multirow{2}{*}{35} & EDM & 1.97 & 3.81 & 1.82 & 3.74 \\
     & EDM-ES (ours) & \textbf{1.95} & \textbf{2.80} & \textbf{1.80} & \textbf{2.59} \\ \midrule
    \multirow{2}{*}{21} & EDM & 2.33 & 6.29 & 2.17 & 5.91 \\
     & EDM-ES & \textbf{2.24} & \textbf{4.32} & \textbf{2.08} & \textbf{3.74} \\ \midrule
    \multirow{2}{*}{13} & EDM & 7.16 & 12.28 & 6.69 & 10.66 \\
     & EDM-ES & \textbf{6.54} & \textbf{8.39} & \textbf{6.16} & \textbf{6.59} \\ \bottomrule
    \end{tabular}
    \end{center}
  \end{minipage}
\vskip -0.1in
\end{table*}





\subsection{Results on EDM}
We test the effectiveness of Epsilon Scaling on EDM \citep{karras2022elucidating} because it achieves state-of-the-art image generation under a few sampling steps and provides a unified framework for diffusion models. Since the main advantage of EDM is its Ordinary Differential Equation (ODE) solver, we evaluate our Epsilon Scaling using their Heun $2^{nd}$ order ODE solver \citep{ascher1998computer} and Euler $1^{st}$ order ODE solver, respectively. Although the network output of EDM is not $\pmb{\epsilon}$, we still can extract the signal $\pmb{\epsilon}$ at each sampling step and then apply Epsilon Scaling. 

The experiments are implemented on CIFAR-10 dataset and we report the FID results in Table \ref{tab: EDM results} using VP framework. The sampling step $T'$ in Table \ref{tab: EDM results} is equivalent to the Neural Function Evaluations (NFE) used in EDM paper. Similar to the results on ADM and DDIM, Epsilon Scaling gains consistent FID improvement on EDM baseline regardless of the conditional settings and the ODE solver types. For instance, EDM-ES improves the FID from 3.81 to 2.80 and from 3.74 to 2.59 in the unconditional and conditional groups using the 35-step Euler sampler.

An interesting phenomenon in Table \ref{tab: EDM results} is that the FID gain of Epsilon Scaling in the Euler sampler group is larger than that in the Heun sampler group. We claim that there are two factors accounting for this phenomenon. On the one hand, higher-order ODE solvers (for example, Heun solvers) introduce less truncation error than Euler $1^{st}$ order solvers. On the other hand, the correction steps in the Heun solver reduce the exposure bias by pulling the drifted sampling trajectory back to the accurate vector field. We illustrate these two factors through Fig. \ref{fig: EDM sampler} which is plotted using the same method of Fig. \ref{fig: eps_norm}. It is apparent from Fig. \ref{fig: EDM sampler} that the Heun sampler exhibits a smaller gap between the training trajectory and sampling trajectory when compared with the Euler sampler. This corresponds to the truncation error factor in these two ODE solvers. Furthermore, in the Heun $2^{nd}$ ODE sampler, the prediction error (cause of exposure bias) made in each Euler step is corrected in the subsequent Correction step (Fig. \ref{fig: EDM_eps_norm_heun}), resulting in a reduced exposure bias.
This exposure bias perspective explains the superiority of the Heun solver in diffusion models beyond the truncation error viewpoint.

\begin{figure*}[tbh]
\vskip -0.1in
\centering
	\subfigure[EDM: Euler $1^{st}$ order sampler]{
		\begin{minipage}{6cm}
        \includegraphics[width=\textwidth]{figures/EDM_eps_norm_euler.pdf}
        \label{fig: EDM_eps_norm_euler}
		\end{minipage}
		\hspace{4mm}
	}
	\subfigure[EDM: Heun $2^{nd}$ order sampler]{
		\begin{minipage}{6cm}
		\includegraphics[width=\textwidth]{figures/EDM_eps_norm_heun.pdf}
		\label{fig: EDM_eps_norm_heun}
		\end{minipage}
		\hspace{4mm}
	} 
\captionsetup{skip=2pt}
\caption{$\left\| \pmb{\epsilon_{\theta}}(\cdot) \right\|_2$ during training and sampling on CIFAR-10.  We use 21-step sampling and report the L2-norm using 50k samples at each timestep. The sampling is from right to left in the figures.} 
\label{fig: EDM sampler}
\vskip -0.1in
\end{figure*}

\subsection{Results on LDM}
\begin{wraptable}{r}{0.4\textwidth}
\vskip -0.0in
\scriptsize
  \centering
  \captionsetup{skip=2pt}
  \caption{FID on LDM baseline using DDPM unconditional sampling.}
  \begin{tabular}{@{}llll@{}}
    \toprule
    $T'$ & Model & \begin{tabular}[c]{@{}l@{}}FFHQ\\ 256$\times$256\end{tabular} & \begin{tabular}[c]{@{}l@{}}CelebA-HQ\\ 256$\times$256\end{tabular} \\ \midrule
    \multirow{2}{*}{100} & LDM & 10.90 & 9.31 \\
     & LDM-ES (ours) & \textbf{9.83} & \textbf{7.36} \\ \midrule
    \multirow{2}{*}{50} & LDM & 14.34 & 13.95 \\
     & LDM-ES & \textbf{11.57} & \textbf{9.16} \\ \midrule
    \multirow{2}{*}{20} & LDM & 33.13 & 29.62 \\
     & LDM-ES & \textbf{20.91} & \textbf{15.68} \\ \bottomrule
    \end{tabular}
    \label{tab: LDM results}
\vskip -0.1in
\end{wraptable}

To further verify the generality of Epsilon Scaling, we adopt Latent Diffusion Model (LDM) as the base model which introduces an Autoeoconder and performs the diffusion process in the latent space \citep{LDM}. We test the performance of Epsilon Scaling (LDM-ES) on FFHQ 256$\times$256 and CelebA-HQ 256$\times$256 datasets using $T'$ steps DDPM sampler. It is clear from Table~\ref{tab: LDM results} that Epsilon Scaling gains substantial FID improvements on the two high-resolution datasets, where LDM-ES achieves 15.68 FID under $T'=20$ on CelebA-HQ, almost half that of LDM. Epsilon Scaling also yields better FID under 50 and 100 sampling steps on CelebA-HQ with 7.36 FID at $T'=100$. Similar FID improvements are obtained on FFHQ dataset over different $T'$.



\subsection{Qualitative Comparison}
\label{sec: qualitative comparison}

\begin{wrapfigure}{r}{0.45\textwidth}
\vskip -0.2in
  \includegraphics[width=0.45\textwidth]{figures/FFHQ128_qualitative.png}
  \captionsetup{skip=1pt}
  \caption{Qualitative comparison between ADM (first row) and ADM-ES (second row).}
\label{fig: ffhq128_qualitative}
\vskip -0.1in
\end{wrapfigure}

In order to visually show the effect of Epsilon Scaling on image synthesis, we set the same random seed for the base model and our Epsilon Scaling model in the sampling phase to ensure a similar trajectory for both models. Fig. \ref{fig: ffhq128_qualitative} displays the generated samples using 100 steps on FFHQ 128$\times$128 dataset. It is clear that ADM-ES effectively refines the sample issues of ADM, including overexposure, underexposure, coarse background and detail defects from left to right in Fig. \ref{fig: ffhq128_qualitative} (see Appendix \ref{Append: qualitative} for more qualitative comparisons). Besides, the qualitative comparison also empirically confirms that Epsilon Scaling guides the sampling trajectory of the base model to an adjacent but better trajectory because both models reach the same or similar modes given the common starting point $\pmb{x}_{T}$ and the same random seed at each sampling step.


\section{Conclusions}
In this paper, we elucidate the exposure bias issue in diffusion models by analytically showing the difference between the training distribution and sampling distribution. Moreover, we discuss solutions to exposure bias and propose a training-free method to refine the deficient sampling trajectory by explicitly scaling the prediction vector. Through extensive experiments, we demonstrate that Epsilon Scaling is a generic solution to exposure bias and its simplicity enables a wide range of applications. Finally, the significant FID improvement in our method indicates the benefits of generation quality by solving the exposure bias problem. Training an accurate $\pmb{\epsilon}$ or score network is a promising direction for future research.






\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}


\newpage
\appendix
\section{Appendix}

\subsection{Derivation of \texorpdfstring{$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$}{Lg} for DDPM}
\label{Append:1}

We show the full derivation of Eq. \ref{eq8} below. From Eq. \ref{eq23} to Eq. \ref{eq24}, we plug in $\pmb{x}^{t+1}_{\pmb{\theta}} = \pmb{x}_{0} + e_{t+1}\pmb{\epsilon}_0$ (Eq. \ref{eq15}) and $\pmb{x}_{t+1} = \sqrt{\bar{\alpha}_{t+1}} \pmb{x}_0 + \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon}$ (Eq. \ref{eq4}), thus a sample from $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ is:

\begin{align}
\hat{\pmb{x}}_{t} & = \mu_{\pmb{\theta}}(\pmb{x}_{t+1}, t+1) + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber \\
& = \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} \pmb{x}^{t+1}_{\pmb{\theta}} + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \pmb{x}_{t+1} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \label{eq23} \\
& = \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} (\pmb{x}_{0} + e_{t+1}\pmb{\epsilon}_0) + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} (\sqrt{\bar{\alpha}_{t+1}} \pmb{x}_0 + \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon}) + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \label{eq24} \\
& = \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} \pmb{x}_{0} + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{\bar{\alpha}_{t+1}} \pmb{x}_0 + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber \\
& = \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1} + \sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t}) \sqrt{\bar{\alpha}_{t+1}} }{1-\bar{\alpha}_{t+1}}  \pmb{x}_{0} + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber \\
& = \frac{\sqrt{\bar{\alpha}_{t}} (1-\alpha_{t+1})  + \sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t}) \sqrt{\bar{\alpha}_{t+1}} }{1-\bar{\alpha}_{t+1}}  \pmb{x}_{0} + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber \\
& = \frac{\sqrt{\bar{\alpha}_{t}} (1-\alpha_{t+1})  + \alpha_{t+1}(1-\bar{\alpha}_{t}) \sqrt{\bar{\alpha}_{t}} }{1-\bar{\alpha}_{t+1}}  \pmb{x}_{0} + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber \\
& = \frac{\sqrt{\bar{\alpha}_{t}} (1-\alpha_{t+1} + \alpha_{t+1} - \bar{\alpha}_{t+1})} {1-\bar{\alpha}_{t+1}}  \pmb{x}_{0} + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \nonumber\\
& = \sqrt{\bar{\alpha}_{t}} \pmb{x}_{0} + \frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} \pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1 \label{eq25}
\end{align}

\noindent
From Eq. \ref{eq25}, we know that the mean of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ is $ \sqrt{\bar{\alpha}_{t}} \pmb{x}_{0}$. We now focus on the variance by looking at $\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} \pmb{\epsilon} + \sqrt{ \tilde{\beta}_{t+1}} \pmb{\epsilon}_1$:

\begin{align}
Var(\hat{\pmb{x}}_{t}) & = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + (\frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} )^2 + \tilde{\beta}_{t+1} \label{eq40} \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + (\frac{\sqrt{\alpha_{t+1}}(1-\bar{\alpha}_{t})}{1-\bar{\alpha}_{t+1}} \sqrt{1 -\bar{\alpha}_{t+1}} )^2 + \frac{(1-\bar{\alpha}_{t})(1-\alpha_{t+1})} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + \frac{\alpha_{t+1}(1-\bar{\alpha}_{t})^2}{1-\bar{\alpha}_{t+1}} + \frac{(1-\bar{\alpha}_{t})(1-\alpha_{t+1})} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + \frac{\alpha_{t+1}(1-\bar{\alpha}_{t})^2 + (1-\bar{\alpha}_{t})(1-\alpha_{t+1})} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + \frac{(1-\bar{\alpha}_{t})[\alpha_{t+1}(1-\bar{\alpha}_{t}) + (1-\alpha_{t+1})]} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + \frac{(1-\bar{\alpha}_{t})[\alpha_{t+1} - \bar{\alpha}_{t+1} + 1-\alpha_{t+1}]} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + \frac{(1-\bar{\alpha}_{t})[1-\bar{\alpha}_{t+1}]} {1-\bar{\alpha}_{t+1}} \nonumber \\
& = (\frac{\sqrt{\bar{\alpha}_{t}} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1} )^2 + 1-\bar{\alpha}_{t} \label{eq41} \\
\nonumber
\end{align}



\subsection{Derivation of \texorpdfstring{$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$}{Lg} and More for DDPM}
\label{Append:1.1}
Since $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ contains two consecutive sampling steps: $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \hat{\pmb{x}}_{t})$, we can solve out $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ by iterative plugging-in. According to $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}) = {\cal N} (\hat{\pmb{x}}_{t}; \mu_{\pmb{\theta}}(\pmb{x}_{t+1}, t+1), \tilde{\beta}_{t+1} \pmb{I})$ and Eq. \ref{eq23}, we know that $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \hat{\pmb{x}}_{t}) = {\cal N} (\hat{\pmb{x}}_{t-1}; \mu_{\pmb{\theta}}(\hat{\pmb{x}}_{t}, t), \tilde{\beta}_{t} \pmb{I})$ and a sample from $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \hat{\pmb{x}}_{t})$ is:
\begin{equation}
\label{eq50}
\hat{\pmb{x}}_{t-1} = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \pmb{x}^{t}_{\pmb{\theta}} + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \hat{\pmb{x}}_{t} + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1.
\end{equation}


\noindent
From Table \ref{tab: sampling distribution}, we know that $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}) = {\cal N} (\hat{\pmb{x}}_{t}; \sqrt{\bar{\alpha}_t} \pmb{x}_0$, $(1-\bar{\alpha}_t + (\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2) \pmb{I})$, so plug in $\hat{\pmb{x}}_{t} = \sqrt{\bar{\alpha}_t} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_t + (\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2 } \pmb{\epsilon}_3$ into Eq. \ref{eq50}, we know a sample from $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ is:
\begin{equation}
\label{eq51}
\hat{\pmb{x}}_{t-1} = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \pmb{x}^{t}_{\pmb{\theta}} + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} (\sqrt{\bar{\alpha}_t} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_t + (\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2 } \pmb{\epsilon}_3) + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1.
\end{equation}

\noindent
By denoting $(\frac{\sqrt{\bar{\alpha}_t} \beta_{t+1}}{1-\bar{\alpha}_{t+1}} e_{t+1})^2$ as $f(t)$ and plugging in $\pmb{x}^{t}_{\pmb{\theta}} = \pmb{x}_{0} + e_{t}\pmb{\epsilon}_0$ (Eq. \ref{eq15}), we have:

\begin{align}
\hat{\pmb{x}}_{t-1} &= \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} (\pmb{x}_{0} + e_{t}\pmb{\epsilon}_0) + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} (\sqrt{\bar{\alpha}_t} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_t + f(t) } \pmb{\epsilon}_3) + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1 \label{eq52} \\
& \approx \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} (\pmb{x}_{0} + e_{t}\pmb{\epsilon}_0) + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} (\sqrt{\bar{\alpha}_t} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}_3 + \frac{1}{2\sqrt{1-\bar{\alpha}_t}} f(t)\pmb{\epsilon}_3) + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1 \label{eq53} \\
& \approx \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \pmb{x}_{0} +  \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} e_{t}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \sqrt{\bar{\alpha}_t} \pmb{x}_0 \nonumber \\
& + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} (\sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}_3 + \frac{1}{2\sqrt{1-\bar{\alpha}_t}} f(t)\pmb{\epsilon}_3) + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1 \label{eq54} \\
& \approx \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_{0} +  \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} e_{t}\pmb{\epsilon}_0 + \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} (\sqrt{1-\bar{\alpha}_t} 
+ \frac{1}{2\sqrt{1-\bar{\alpha}_t}} f(t))\pmb{\epsilon}_3 + \sqrt{ \tilde{\beta}_{t}} \pmb{\epsilon}_1 \label{eq55} \\
\nonumber
\end{align}

\noindent
Taylor's theorem is used from Eq. \ref{eq52} to Eq. \ref{eq53}. The process from Eq. \ref{eq54} to Eq. \ref{eq55} is similar to the simplification from Eq. \ref{eq24} to Eq. \ref{eq25}. From Eq. \ref{eq55}, we know that the mean of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ is $ \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_{0}$. We now focus on the variance:
\begin{align}
Var(\hat{\pmb{x}}_{t-1}) & = (\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} e_{t} )^2 + (\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \sqrt{1 -\bar{\alpha}_{t}} )^2 + \tilde{\beta}_{t} \nonumber \\
&+   (\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \frac{1}{2\sqrt{1-\bar{\alpha}_t}} f(t) )^2\label{eq56} \\
& = 1-\bar{\alpha}_{t-1} + (\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} e_{t} )^2 + \frac{\alpha_{t}(1-\bar{\alpha}_{t-1})^2}{4(1-\bar{\alpha}_{t})^3} f(t)^2 \label{eq57} \\
\nonumber
\end{align}
The above derivation is similar to the progress from Eq. \ref{eq40} to Eq. \ref{eq41}. Now we write the mean and variance of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ in Table \ref{tab: two-step sampling distribution}. In the same spirit of iterative plugging-in, we could derive $(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ which has the mean $\sqrt{\bar{\alpha}_{t}} \pmb{x}_0$ and variance larger than $(1-\bar{\alpha}_{t}) \pmb{I}$. 

\begin{table}[ht]
\vskip -0.0in
\captionsetup{skip=2pt}
\caption{
The distribution $q(\pmb{x}_{t-1} | \pmb{x}_0)$ during training and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$ during DDPM sampling.}
\label{tab: two-step sampling distribution}
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
 & Mean & Variance \\ \midrule
$q(\pmb{x}_{t-1} | \pmb{x}_0)$ & $\sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ & $(1-\bar{\alpha}_{t-1}) \pmb{I}$ \\
$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t+1})$  & $\sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ & $(1-\bar{\alpha}_{t-1} + (\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} e_{t} )^2 + \frac{\alpha_{t}(1-\bar{\alpha}_{t-1})^2}{4(1-\bar{\alpha}_{t})^3} f(t)^2) \pmb{I}$ \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}




\subsection{Derivation of \texorpdfstring{$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$}{Lg} for DDIM}
\label{Append:1.5}
We first review the derivation of the reverse diffusion $p_{\pmb{\theta}}(\pmb{x}_{t-1}|\pmb{x}_{t})$ for DDIM. To keep the symbols consistent in this paper, we continue to use the notations of DDPM in the derivation of DDIM. Recall that DDIM and DDPM have the same loss function because they share the same marginal distribution $q(\pmb{x}_t | \pmb{x}_0) = {\cal N} (\pmb{x}_t; \sqrt{\bar{\alpha}_t} \pmb{x}_0, (1-\bar{\alpha}_t) \pmb{I})$. But the posterior $q(\pmb{x}_{t-1} | \pmb{x}_{t}, \pmb{x}_{0})$ of DDIM is obtained under Non-Markovian diffusion process and is given by \citet{DDIM}:
\begin{equation}
\label{eq26}
q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}_0) = {\cal N} (\sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \frac{\pmb{x}_t-\sqrt{\bar{\alpha}_t} \pmb{x}_0}{\sqrt{1-\bar{\alpha}_t}}, \sigma_t^2 \pmb{I}).
\end{equation}

\noindent
Similar to DDPM, the reverse distribution of DDIM is parameterized as $p_{\pmb{\theta}}(\pmb{x}_{t-1} | \pmb{x}_t) = q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$, where $\pmb{x}^{t}_{\pmb{\theta}}$ means the predicted $\pmb{x}_0$ given $\pmb{x}_t$. Based on Eq. \ref{eq26}, the reverse diffusion $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ is: 
\begin{equation}
\label{eq27}
q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}}) = {\cal N} (\sqrt{\bar{\alpha}_{t-1}} \pmb{x}^{t}_{\pmb{\theta}} + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \frac{\pmb{x}_t-\sqrt{\bar{\alpha}_t} \pmb{x}^{t}_{\pmb{\theta}}}{\sqrt{1-\bar{\alpha}_t}}, \sigma_t^2 \pmb{I}).
\end{equation}

Again, we point out that $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}}) = q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}_0)$ holds only if $\pmb{x}^{t}_{\pmb{\theta}} = \pmb{x}_0$, this requires the network to make no prediction error about $\pmb{x}_0$. Theoretically, we need to consider the uncertainty of the prediction $\pmb{x}^{t}_{\pmb{\theta}}$ and model it as a probabilistic distribution $p_{\pmb{\theta}}(\pmb{x}_{0} | \pmb{x}_{t})$. Following Analytical-DPM \citep{bao2022analytic}, we approximate it by a Gaussian distribution $p_{\pmb{\theta}}(\pmb{x}_{0} | \pmb{x}_{t}) = {\cal N} (\pmb{x}^{t}_{\pmb{\theta}}; \pmb{x}_{0}, e_{t}^{2} \pmb{I})$, namely $\pmb{x}^{t}_{\pmb{\theta}} = \pmb{x}_{0} + e_t \pmb{\epsilon}_0$. Thus, the practical reverse diffusion $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ is
\begin{equation}
\label{eq28}
q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}}) = {\cal N} (\sqrt{\bar{\alpha}_{t-1}} (\pmb{x}_{0} + e_t \pmb{\epsilon}_0) + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \frac{\pmb{x}_t-\sqrt{\bar{\alpha}_t} (\pmb{x}_{0} + e_t \pmb{\epsilon}_0)}{\sqrt{1-\bar{\alpha}_t}}, \sigma_t^2 \pmb{I}).
\end{equation}

\noindent
Note that $\sigma_t=0$ for DDIM sampler, so a sample $\pmb{x}_{t-1}$ from $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ is:

\begin{align}
\pmb{x}_{t-1} & = \sqrt{\bar{\alpha}_{t-1}} (\pmb{x}_{0} + e_t \pmb{\epsilon}_0) + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \frac{\pmb{x}_t-\sqrt{\bar{\alpha}_t} (\pmb{x}_{0} + e_t \pmb{\epsilon}_0)}{\sqrt{1-\bar{\alpha}_t}} + \sigma_t \pmb{\epsilon}_4 \nonumber \\
& = \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_{0} + \sqrt{\bar{\alpha}_{t-1}} e_t \pmb{\epsilon}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \cdot \frac{\pmb{x}_t-\sqrt{\bar{\alpha}_t} \pmb{x}_0}{\sqrt{1-\bar{\alpha}_t}} - \sqrt{1-\bar{\alpha}_{t-1}} \cdot \frac{\sqrt{\bar{\alpha}_t} e_t \pmb{\epsilon}_0}{\sqrt{1-\bar{\alpha}_t}} 
\label{eq29} \\
& = \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_{0} + \sqrt{\bar{\alpha}_{t-1}} e_t \pmb{\epsilon}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \pmb{\epsilon}_5 - \sqrt{1-\bar{\alpha}_{t-1}} \cdot \frac{\sqrt{\bar{\alpha}_t} e_t \pmb{\epsilon}_0}{\sqrt{1-\bar{\alpha}_t}} \label{eq30} \\
& = \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_{0} + \sqrt{1-\bar{\alpha}_{t-1}} \pmb{\epsilon}_5 + (\sqrt{\bar{\alpha}_{t-1}} e_t - \sqrt{1-\bar{\alpha}_{t-1}} \cdot \frac{\sqrt{\bar{\alpha}_t} e_t}{\sqrt{1-\bar{\alpha}_t}})\pmb{\epsilon}_0 \label{eq31} \\
\nonumber
\end{align}

\noindent
From Eq. \ref{eq29} to Eq. \ref{eq30}, we plug in $\pmb{x}_t = \sqrt{\bar{\alpha}_t} \pmb{x}_0 +   \sqrt{1 - \bar{\alpha}_t} \pmb{\epsilon}_5$ where $\pmb{\epsilon}_5 \sim {\cal N} (\pmb{0}, \pmb{I})$. We now compute the sampling distribution $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ which is the same distribution as $q(\pmb{x}_{t-1} | \pmb{x}_t, \pmb{x}^{t}_{\pmb{\theta}})$ by replacing the index $t$ with $t+1$ and using $\hat{\pmb{x}}_{t}$ to highlight it is a generated sample. According to Eq. \ref{eq31}, a sample $\hat{\pmb{x}}_{t}$ from $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ is:
\begin{align}
\hat{\pmb{x}}_{t} & = \sqrt{\bar{\alpha}_{t}} \pmb{x}_{0} + \sqrt{1-\bar{\alpha}_{t}} \pmb{\epsilon}_5 + (\sqrt{\bar{\alpha}_{t}} e_{t+1} - \sqrt{1-\bar{\alpha}_{t}} \cdot \frac{\sqrt{\bar{\alpha}_{t+1}} e_{t+1}}{\sqrt{1-\bar{\alpha}_{t+1}}})\pmb{\epsilon}_0 \label{eq32} \\
\nonumber
\end{align}

\noindent
From Eq. \ref{eq32}, we know the mean of $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ is $ \sqrt{\bar{\alpha}_{t}} \pmb{x}_{0}$. We now calculate the variance by looking at $\sqrt{1-\bar{\alpha}_{t}} \pmb{\epsilon}_5 + (\sqrt{\bar{\alpha}_{t}} e_{t+1} - \sqrt{1-\bar{\alpha}_{t}} \cdot \frac{\sqrt{\bar{\alpha}_{t+1}} e_{t+1}}{\sqrt{1-\bar{\alpha}_{t+1}}})\pmb{\epsilon}_0$:
\begin{align}
Var(\hat{\pmb{x}}_{t}) & = (\sqrt{1-\bar{\alpha}_{t}})^2 + (\sqrt{\bar{\alpha}_{t}} e_{t+1} - \sqrt{1-\bar{\alpha}_{t}} \cdot \frac{\sqrt{\bar{\alpha}_{t+1}} e_{t+1}}{\sqrt{1-\bar{\alpha}_{t+1}}})^2  \nonumber \\
& = 1-\bar{\alpha}_{t} + (\sqrt{\bar{\alpha}_{t}} - \sqrt{1-\bar{\alpha}_{t}} \cdot \frac{\sqrt{\bar{\alpha}_{t+1}}}{\sqrt{1-\bar{\alpha}_{t+1}}})^2 e_{t+1}^2 \nonumber \\
& = 1-\bar{\alpha}_{t} + (\sqrt{\bar{\alpha}_{t}} - \frac{ \sqrt{1-\bar{\alpha}_{t}} \sqrt{\bar{\alpha}_{t}} \sqrt{\alpha_{t+1}} }{\sqrt{1-\bar{\alpha}_{t+1}}})^2 e_{t+1}^2 \nonumber \\
& = 1-\bar{\alpha}_{t} + (\sqrt{\bar{\alpha}_{t}} (1 - \frac{ \sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t+1}} }{\sqrt{1-\bar{\alpha}_{t+1}}}) )^2 e_{t+1}^2 \nonumber \\
& = 1-\bar{\alpha}_{t} + \bar{\alpha}_{t} (1 - \frac{ \sqrt{\alpha_{t+1} - \bar{\alpha}_{t+1}}}{\sqrt{1-\bar{\alpha}_{t+1}}}) ^2 e_{t+1}^2 \nonumber \\
& = 1-\bar{\alpha}_{t} +  (1 - \sqrt{\frac{\alpha_{t+1} - \bar{\alpha}_{t+1}}{1-\bar{\alpha}_{t+1}}} ) ^2 \bar{\alpha}_{t} e_{t+1}^2 \label{eq33} \\
\nonumber
\end{align}

\noindent
As a result, we can write the mean and variance of the sampling distribution $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$, i.e. $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$, and compare it with the training distribution $q(\pmb{x}_{t} | \pmb{x}_0)$ in Table \ref{tab: DDIM sampling distribution}. 

\begin{table}[ht]
\caption{
The mean and variance of $q(\pmb{x}_{t} | \pmb{x}_0)$ during training and $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ during DDIM sampling.}
\label{tab: DDIM sampling distribution}
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
 & Mean & Variance \\ \midrule
$q(\pmb{x}_{t} | \pmb{x}_0)$ & $\sqrt{\bar{\alpha}_t} \pmb{x}_0$ & $(1-\bar{\alpha}_t) \pmb{I}$ \\
$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$  & $\sqrt{\bar{\alpha}_t} \pmb{x}_0$ & $( 1-\bar{\alpha}_{t} +  (1 - \sqrt{\frac{\alpha_{t+1} - \bar{\alpha}_{t+1}}{1-\bar{\alpha}_{t+1}}} ) ^2 \bar{\alpha}_{t} e_{t+1}^2 )\pmb{I} $ \\ \bottomrule
\end{tabular}
\end{center}
\end{table}


Since $\alpha_{t+1} < 1$, $\sqrt{\frac{\alpha_{t+1} - \bar{\alpha}_{t+1}}{1-\bar{\alpha}_{t+1}}} < 1$ and $(1 - \sqrt{\frac{\alpha_{t+1} - \bar{\alpha}_{t+1}}{1-\bar{\alpha}_{t+1}}} ) > 0$ hold for any $t$ in Eq. \ref{eq33}. Similar to DDPM sampler, the variance of $q(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1}, \pmb{x}^{t+1}_{\pmb{\theta}})$ is always larger than that of $q(\pmb{x}_{t} | \pmb{x}_0)$ by the magnitude $(1 - \sqrt{\frac{\alpha_{t+1} - \bar{\alpha}_{t+1}}{1-\bar{\alpha}_{t+1}}} ) ^2 \bar{\alpha}_{t} e_{t+1}^2$, indicating the exposure bias issue in DDIM sampler.




\subsection{Practical Variance Error of \texorpdfstring{$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$}{Lg} and \texorpdfstring{$q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$}{Lg}}
\label{Append:2}

We measure the single-step variance error of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{t+1})$ and multi-step variance error of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ using Algorithm \ref{alg: single-step} and Algorithm \ref{alg: multi-step}, respectively. Note that, the multi-step variance error measurement is similar to the exposure bias $\delta_t$ evaluation and we denote the single-step variance error as $\Delta_t$ and represent the multi-step variance error as $\Delta_t'$. The experiments are implemented on CIFAR-10 \citep{cifar10} dataset and ADM model \citep{ADM}. The key difference between $\Delta_t$ and $\Delta_t'$ measurement is that the former can get access to the ground truth input $\pmb{x}_t$ at each sampling step $t$, while the latter is only exposed to the predicted $\hat{\pmb{x}}_t$ in the iterative sampling process.

\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Variance error under single-step sampling}
    \label{alg: single-step}
    \begin{algorithmic}[1]
        \STATE Initialize $\Delta_t = 0$, $n_t = list()$ ($\forall t \in \{1, ..., T-1 \}$)
        
        \FOR{$t := T, ..., 1$}
        \REPEAT
        \STATE $\pmb{x}_0 \sim q(\pmb{x}_0)$, $\pmb{\epsilon} \sim {\cal N} (\pmb{0}, \pmb{I})$
        \STATE $\pmb{x}_t = \sqrt{\bar{\alpha}_t} \pmb{x}_0 +   \sqrt{1 - \bar{\alpha}_t} \pmb{\epsilon}$
        \STATE $\hat{\pmb{x}}_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (\pmb{x}_{t} - \frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \pmb{\epsilon}_{\pmb{\theta}} (\pmb{x}_{t}, t)) + \sqrt{\tilde{\beta_t}} \pmb{z}$ \quad ($\pmb{z} \sim {\cal N} (\pmb{0}, \pmb{I})$)
        \STATE $n_{t-1}.append(\hat{\pmb{x}}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0)$
        \UNTIL {50k iterations}
        \ENDFOR
        
        
        \FOR{$t := T, ..., 1$}
        \STATE $\hat{\beta}_t = numpy.var(n_t)$
        \STATE $\Delta_t = \hat{\beta}_t - \bar{\beta}_t$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Variance error under multi-step sampling}
    \label{alg: multi-step}
    \begin{algorithmic}[1]
        \STATE Initialize $\delta_t = 0$, $n_t = list()$ ($\forall t \in \{1, ..., T-1 \}$)
        
        \REPEAT
        \STATE $\pmb{x}_0 \sim q(\pmb{x}_0)$, $\pmb{\epsilon} \sim {\cal N} (\pmb{0}, \pmb{I})$
        \STATE $\pmb{x}_T = \sqrt{\bar{\alpha}_T} \pmb{x}_0 +   \sqrt{1 - \bar{\alpha}_T}\pmb{\epsilon}$
        
        \FOR{$t := T, ..., 1$}
        \STATE if $t == T$ then $\hat{\pmb{x}}_{t} = \pmb{x}_T$
        \STATE $\hat{\pmb{x}}_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (\hat{\pmb{x}}_{t} - \frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \pmb{\epsilon}_{\pmb{\theta}} (\hat{\pmb{x}}_{t}, t)) + \sqrt{\tilde{\beta_t}} \pmb{z}$ \quad ($\pmb{z} \sim {\cal N} (\pmb{0}, \pmb{I})$)
        \STATE $n_{t-1}.append(\hat{\pmb{x}}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0)$ 
        \ENDFOR
        
        \UNTIL {50k iterations}
        \FOR{$t := T, ..., 1$}
        \STATE $\hat{\beta}_t = numpy.var(n_t)$
        \STATE $\Delta_t' = \hat{\beta}_t - \bar{\beta}_t$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}




\subsection{Metric for Exposure Bias}
\label{Append:0}
The key step of Algorithm \ref{alg: exposure bias measurement} is that we subtract the mean $\sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ and the remaining term $\hat{\pmb{x}}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0$ corresponds to the stochastic term of $q(\hat{\pmb{x}}_{t-1} | \pmb{x}_{t}, \pmb{x}^{t}_{\pmb{\theta}})$. In our experiments, we use $N=50,000$ samples to compute the variance $\hat{\beta}_t$.


\begin{algorithm}[h]
   \caption{Measurement of Exposure Bias $\delta_t$}
   \label{alg: exposure bias measurement}
\begin{algorithmic}[1]
        \STATE Initialize $\delta_t = 0$, $n_t = list()$ ($\forall t \in \{1, ..., T-1 \}$)
        
        \REPEAT
        \STATE $\pmb{x}_0 \sim q(\pmb{x}_0)$, $\pmb{\epsilon} \sim {\cal N} (\pmb{0}, \pmb{I})$
        \STATE compute $\pmb{x}_T$ using Eq. \ref{eq4}
        
        \FOR{$t := T, ..., 1$}
        \STATE if $t == T$ then $\hat{\pmb{x}}_{t} = \pmb{x}_T$
        \STATE $\hat{\pmb{x}}_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (\hat{\pmb{x}}_{t} - \frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \pmb{\epsilon}_{\pmb{\theta}} (\hat{\pmb{x}}_{t}, t)) + \sqrt{\tilde{\beta_t}} \pmb{z}$ \quad ($\pmb{z} \sim {\cal N} (\pmb{0}, \pmb{I})$)
        \STATE $n_{t-1}.append(\hat{\pmb{x}}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \pmb{x}_0)$ 
        \ENDFOR
        
        \UNTIL {$N$ iterations}
        \FOR{$t := T, ..., 1$}
        \STATE $\hat{\beta}_t = numpy.var(n_t)$
        \STATE $\delta_t = (\sqrt{\hat{\beta}_t} - \sqrt{\bar{\beta}_t})^2$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}



\subsection{Correlation between Exposure Bias Metric and FID}
\label{Append: delta_t and FID}

\begin{wrapfigure}{r}{0.4\textwidth}
\vskip -0.3in
  \includegraphics[width=0.4\textwidth]{figures/cifar10_delta_and_FID.pdf}
  \captionsetup{skip=2pt}
  \caption{Correlation between FID - $\delta_1$.}
  \label{fig: delta_t_and_FID}
\vskip -0.6in
\end{wrapfigure}

We define the exposure bias at timestep $t$ as 
$\delta_t = (\sqrt{\hat{\beta}_t} - \sqrt{\bar{\beta}_t})^2 $, where $\bar{\beta}_t = 1-\bar{\alpha}_t$ denotes the variance of $q(\pmb{x}_{t} | \pmb{x}_0)$ during training and $\hat{\beta}_t$ presents the variance of $q_{\pmb{\theta}}(\hat{\pmb{x}}_{t} | \pmb{x}_{T})$ in the regular sampling process. Although $\delta_t$ measures the discrepancy between network inputs and FID to evaluate the difference between training data and network outputs, we empirically find a strong correlation between $\delta_t$ and FID, which could arise from the benefit of defining $\delta_t$ from 
 the Fréchet distance \citet{dowson1982frechet} perspective. In Fig. \ref{fig: delta_t_and_FID}, we present the FID-$\delta_1$ relationships on CIFAR-10 and use 20-step sampling, wherein $\delta_1$ represents the exposure bias in the last sampling step $t=1$. Additionally, $\delta_t$ has the advantage of indicating the network input quality at any intermediate timestep $t$. Taking Fig. \ref{fig: xt_var_error} as an example, we can see that the input quality decreases dramatically near the end of sampling ($t=1$) as $\delta_t$ increases significantly.


\subsection{Recall and Precision Results}
\label{Append: recall and precision}
Our method Epsilon Scaling does not affect the recall and precision of the base model. We present the complete recall and precision \citep{kynkaanniemi2019improved} results in Table \ref{tab: recall and precision} using the code provided by ADM \citep{ADM}. ADM-ES achieve higher recalls and slightly lower previsions across the five datasets. But the overall differences are minor. 


\begin{table*}[h]
\small
\vskip -0.0in
\caption{Recall and precision of ADM and ADM-ES using 100-step sampling.} 
\label{tab: recall and precision}
\begin{center}
\setlength{\tabcolsep}{4pt}  \begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}CIFAR-10\\ 32$\times$32\end{tabular}} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}LSUN tower\\ 64$\times$64\end{tabular}} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}FFHQ\\ 128$\times$128\end{tabular}} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}ImageNet\\ 64$\times$64\end{tabular}} & \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}ImageNet\\ 128$\times$128\end{tabular}} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 & recall & precision & recall & precision & recall & precision & recall & precision & recall & precision \\ \midrule
ADM & 0.591 & \textbf{0.691} & 0.605 & \textbf{0.645} & 0.497 & \textbf{0.696} & 0.621 & \textbf{0.738} & 0.586 & 0.771 \\
ADM-ES & \textbf{0.613} & 0.684 & \textbf{0.606} & 0.641 & \textbf{0.545} & 0.683 & \textbf{0.632} & 0.726 & \textbf{0.592} & 0.771 \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.0in
\end{table*}



\subsection{Epsilon Scaling Parameters: \texorpdfstring{$k, b$}{Lg}}
\label{Append: parameters}
We present the parameters $k, b$ of Epsilon Scaling we used in all of our experiments in Table \ref{tab: k and b ADM}, Table \ref{tab: k and b LDM DDIM} and Table \ref{tab: k and b EDM} for reproducibility. Apart from that, we provide guidance on how to search for the optimal parameters even though they are dependent on the dataset and how well the base model is trained. Our suggestions are:

\begin{itemize}
    \item First search for the optimal uniform schedule $\lambda_t$ (i.e. $\lambda_t=b$) with a large stride, for example, $b={1.001, 1.003, 1.005...}$.  
    \item After locating the coarse range of the optimal $b$, apply a smaller stride to finetune the initial $b$.
    \item In general, the optimal $b$ will decrease as the number of sampling steps $T'$ increases.
    \item If one is interested in finding the optimal linear schedule $\lambda_t=kt+b$, we suggest taking the optimal uniform schedule $\lambda_t=b$ as the baseline and maintaining the mean of $\lambda_t=kt+b$ equal to the baseline. Then, a small $k$ (for example, 0.0001) is a good starting point. 
    \item Instead of generating 50k samples for FID computation, we find that 10k samples are enough for parameter searching. 
\end{itemize}



\begin{table*}[ht]
\scriptsize
\vskip -0.0in
\caption{Epsilon Scaling schedule $\lambda_t=kt+b$ we used on ADM baseline. We keep the FID results in the table for comparisons and remark $k, b$ underneath FIDs} 
\label{tab: k and b ADM}
\begin{center}
\setlength{\tabcolsep}{4pt}  \begin{tabular}{@{}lllllll@{}}
\toprule
\multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Unconditional} & \multicolumn{2}{c}{Conditional} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-7} 
 &  & CIFAR-10 32$\times$32 & LSUN tower 64$\times$64 & FFHQ 128$\times$128 & ImageNet 64$\times$64 & ImageNet 128$\times$128 \\ \midrule
100 & ADM & 3.37 & 3.59 & 14.52 & 2.71 & 3.55 \\
 & ADM-ES & \begin{tabular}[c]{@{}l@{}}2.17\\ \textbf{(b=1.017)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2.91\\ \textbf{(b=1.006)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}6.77\\ \textbf{(b=1.005)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2.39\\ 
 \textbf{(b=1.006)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}3.37\\ 
 \textbf{(b=1.004)}\end{tabular} \\ \midrule
\multirow{2}{*}{50} & ADM & 4.43 & 7.28 & 26.15 & 3,75 & 5.15 \\
 & ADM-ES & \begin{tabular}[c]{@{}l@{}}2.49\\ 
 \textbf{(b=1.017)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}3.68\\ 
 \textbf{(b=1.007)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}9.50\\ 
 \textbf{(b=1.007)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}3.07\\ 
 \textbf{(b=1.006)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}4.33\\ 
 \textbf{(b=1.004)}\end{tabular} \\ \midrule
\multirow{3}{*}{20} & ADM & 10.36 & 23.92 & 59.35 & 10.96 & 12.48 \\
 & ADM-ES & \begin{tabular}[c]{@{}l@{}}5.15\\ 
 \textbf{(b=1.017)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}8.22\\ 
 \textbf{(b=1.011)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}26.14\\ 
 \textbf{(b=1.008)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.52\\ 
 \textbf{(b=1.006)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}9.95\\ 
 \textbf{(b=1.005)} \end{tabular} \\
 & ADM-ES$^*$ & \begin{tabular}[c]{@{}l@{}}4.31\\ \textbf{(k=0.0025, b=1.0)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.60\\ \textbf{(k=0.0008, b=1.0034)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}24.83\\ \textbf{(k=0.0004, b=1.0042)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.37\\ \textbf{(k=0.0002, b=1.0041)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}9.86\\ \textbf{(k=0.00022, b=1.00291)} \end{tabular} \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.0in
\end{table*}


\begin{table*}[ht]
\scriptsize
\vskip -0.0in
\caption{Epsilon Scaling schedule $\lambda_t=kt+b, (k=0)$ we used on DDIM/DDPM and LDM baseline. We keep the FID results in the table for comparisons and remark $b$ underneath FIDs} 
\label{tab: k and b LDM DDIM}
\begin{center}
\begin{tabular}{@{}lllllllllll@{}}
\cmidrule(r){1-6} \cmidrule(l){8-11}
\multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{2}{l}{CIFAR-10 32$\times$32} & \multicolumn{2}{l}{CelebA 64$\times$64} &  & \multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multirow{2}{*}{FFHQ 256$\times$256} & \multirow{2}{*}{CelebA-HQ 256$\times$256} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
 &  & $\eta=0$ & $\eta=1$ & $\eta=0$ & $\eta=1$ &  &  &  &  &  \\ \cmidrule(r){1-6} \cmidrule(l){8-11} 
\multirow{2}{*}{100} & DDIM & 4.06 & 6.73 & 5.67 & 11.33 &  & \multirow{2}{*}{100} & LDM & 10.90 & 9.31 \\
 & DDIM-ES & \begin{tabular}[c]{@{}l@{}}3.38\\ \textbf{(b=1.0014)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}4.01\\ \textbf{(b=1.03)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}5.05\\ \textbf{(b=1.003)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}4.45\\ \textbf{(b=1.04)} \end{tabular} &  &  & LDM-ES & \begin{tabular}[c]{@{}l@{}}9.83\\ 
 \textbf{(b=1.00015)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.36\\ 
 \textbf{(b=1.0009)} \end{tabular} \\ \cmidrule(r){1-6} \cmidrule(l){8-11} 
\multirow{2}{*}{50} & DDIM & 4.82 & 10.29 & 6.88 & 15.09 &  & \multirow{2}{*}{50} & LDM & 14.34 & 13.95 \\
 & DDIM-ES & \begin{tabular}[c]{@{}l@{}}4.17\\ \textbf{(b=1.0030)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}4.57\\ \textbf{(b=1.04)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}6.20\\ \textbf{(b=1.004)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}5.57\\ \textbf{(b=1.05)} \end{tabular} &  &  & LDM-ES & \begin{tabular}[c]{@{}l@{}}11.57\\ 
 \textbf{(b=1.0016)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}9.16\\ 
 \textbf{(b=1.003)} \end{tabular} \\ \cmidrule(r){1-6} \cmidrule(l){8-11} 
\multirow{2}{*}{20} & DDIM & 8.21 & 20.15 & 10.43 & 22.61 &  & \multirow{2}{*}{20} & LDM & 33.13 & 29.62 \\
 & DDIM-ES & \begin{tabular}[c]{@{}l@{}}6.54\\ \textbf{(b=1.0052)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.78\\ \textbf{(b=1.05)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}10.38\\ \textbf{(b=1.001)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}11.83\\ 
 \textbf{(b=1.06)} \end{tabular} &  &  & LDM-ES & \begin{tabular}[c]{@{}l@{}}20.91\\ \textbf{(b=1.007)} \end{tabular} & \begin{tabular}[c]{@{}l@{}}15.68\\ \textbf{(b=1.010)} \end{tabular} \\ \cmidrule(r){1-6} \cmidrule(l){8-11} 
\end{tabular}
\end{center}
\vskip -0.0in
\end{table*}


\begin{table*}[ht]
\small
\vskip -0.0in
\caption{Epsilon Scaling schedule $\lambda_t=kt+b, (k=0)$ we used on EDM baseline. We keep the FID results in the table for comparisons and remark $b$ underneath FIDs} 
\label{tab: k and b EDM}
\begin{center}
\begin{tabular}{@{}llllll@{}}
\toprule
\multirow{2}{*}{$T'$} & \multirow{2}{*}{Model} & \multicolumn{2}{l}{Unconditional} & \multicolumn{2}{l}{Conditional} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
 &  & Heun & Euler & Heun & Euler \\ \midrule
\multirow{2}{*}{35} & EDM & 1.97 & 3.81 & 1.82 & 3.74 \\
 & EDM-ES & \begin{tabular}[c]{@{}l@{}}1.95\\ \textbf{b=1.0005}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2.80\\ \textbf{b=1.0034}\end{tabular} & \begin{tabular}[c]{@{}l@{}}1.80\\ \textbf{b=1.0006}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2.59\\ \textbf{b=1.0035}\end{tabular} \\ \midrule
\multirow{2}{*}{21} & EDM & 2.33 & 6.29 & 2.17 & 5.91 \\
 & EDM-ES & \begin{tabular}[c]{@{}l@{}}2.24\\ \textbf{b=0.9985}\end{tabular} & \begin{tabular}[c]{@{}l@{}}4.32\\ \textbf{b=1.0043}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2.08\\ \textbf{b=0.9983}\end{tabular} & \begin{tabular}[c]{@{}l@{}}3.74\\ \textbf{b=1.0045}\end{tabular} \\ \midrule
\multirow{2}{*}{13} & EDM & 7.16 & 12.28 & 6.69 & 10.66 \\
 & EDM-ES & \begin{tabular}[c]{@{}l@{}}6.54\\ \textbf{b=1.0060}\end{tabular} & \begin{tabular}[c]{@{}l@{}}8.39\\ \textbf{b=1.0048}\end{tabular} & \begin{tabular}[c]{@{}l@{}}6.16\\ \textbf{b=1.0070}\end{tabular} & \begin{tabular}[c]{@{}l@{}}6.59\\ \textbf{b=1.0051}\end{tabular} \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.0in
\end{table*}


\newpage
\subsection{Epsilon Scaling Alleviates Exposure Bias}
\label{Append: Alleviates Exposure Bias}
In Section \ref{sec: alleviate exposure bias}, we have explicitly shown that Epsilon Scaling reduces the exposure bias of diffusion models via refining the sampling trajectory and achieves a lower $\delta_t$ on CIFAR-10 dataset.

We now replicate these experiments on other datasets using the same base model ADM and 20-step sampling. Fig. \ref{fig: cifar10 exposure bias} and Fig. \ref{fig: ffhq128 exposure bias} display the corresponding results on CIFAR-10 and FFHQ 128$\times$128 datasets. Similar to the phenomenon on LSUN tower 64$\times$64 (Fig. \ref{fig: eps_norm_solution} and Fig. \ref{fig: xt_var_error}) , Epsilon Scaling consistently obtains a smaller exposure bias $\delta_t$ and pushes the sampling trajectory to the vector field learned in the training stage.


\begin{figure*}[ht]
\centering
	\subfigure[Exposure bias measured by $\delta_t$ on CIFAR-10]{
		\begin{minipage}{6.2cm}
		\includegraphics[width=\textwidth]{figures/cifar10_exposure_bias.pdf}
		\label{b}
		\end{minipage}
		\hspace{4mm}
	}
         \subfigure[L2-norm of $\pmb{\epsilon_{\theta}}(\cdot)$ on CIFAR-10]{
		\begin{minipage}{6.2cm}
         \includegraphics[width=\textwidth]{figures/cifar10_eps_norm_solution.pdf} 
         \label{a}
		\end{minipage}
		\hspace{4mm}
	}
\caption{Left: Epsilon Scaling achieves a smaller $\delta_t$ at the end of sampling ($t=1$). Right: after applying Epsilon Scaling, the sampling $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (blue) gets closer to the training $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (red)} 
\label{fig: cifar10 exposure bias}
\end{figure*}



\begin{figure*}[ht]
\centering
	\subfigure[Exposure bias measured by $\delta_t$ on FFHQ 128$\times$128]{
		\begin{minipage}{6.2cm}
		\includegraphics[width=\textwidth]{figures/ffhq128_exposure_bias.pdf}
		\label{d}
		\end{minipage}
		\hspace{4mm}
	}
        \subfigure[L2-norm of $\pmb{\epsilon_{\theta}}(\cdot)$ on FFHQ 128$\times$128]{
		\begin{minipage}{6.2cm}
         \includegraphics[width=\textwidth]{figures/ffhq128_eps_norm_solution.pdf} 
         \label{c}
		\end{minipage}
		\hspace{4mm}
	}
\caption{Left: Epsilon Scaling achieves a smaller $\delta_t$ at the end of sampling ($t=1$). Right: after applying Epsilon Scaling, the sampling $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (blue) gets closer to the training $\left\| \pmb{\epsilon}_{\pmb{\theta}} \right\|_2$ (red).} 
\label{fig: ffhq128 exposure bias}
\end{figure*}




\subsection{Qualitative Comparison}
\label{Append: qualitative}

In Section \ref{sec: qualitative comparison}, we have presented the sample quality comparison between the base model sampling and Epsilon Scaling sampling on FFHQ 128$\times$128 dataset. Applying the same experimental settings, we show more qualitative contrasts between ADM and ADM-ES on the dataset CIFAR-10 32$\times$32 (Fig. \ref{fig: cifar10_qualitative}), LSUN tower 64$\times$64 (Fig. \ref{fig: lsun_qualitative}), ImageNet 64$\times$64 (Fig. \ref{fig: imagenet64_qualitative}) and ImageNet 128$\times$128 (Fig. \ref{fig: imagenet128_qualitative}). Also, we provide the qualitative comparison between LDM and LDM-ES on the dataset CelebA-HQ 256$\times$256 (Fig. \ref{fig: celeba256_qualitative}). These sample comparisons clearly state that Epsilon Scaling effectively improves the sample quality from various perspectives, including illumination, colour, object coherence, background details and so on.


\begin{figure}[ht]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/cifar10_qualitative.png}}
\caption{Qualitative comparison between ADM (first row) and ADM-ES (second row) on CIFAR-10 32$\times$32 
}
\label{fig: cifar10_qualitative}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{figures/lsun_qualitative.png}}
\caption{Qualitative comparison between ADM (first row) and ADM-ES (second row) on LSUN tower 64$\times$64 
}
\label{fig: lsun_qualitative}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{figures/imagenet64_qualitative.png}}
\caption{Qualitative comparison between ADM (first row) and ADM-ES (second row) on ImageNet 64$\times$64 
}
\label{fig: imagenet64_qualitative}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figures/imagenet128_qualitative.png}}
\caption{Qualitative comparison between ADM (first row) and ADM-ES (second row) on ImageNet 128$\times$128 
}
\label{fig: imagenet128_qualitative}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/celeba256_qualitative.png}}
\caption{Qualitative comparison between LDM (first row) and LDM-ES (second row) on CelebA-HQ 256$\times$256 
}
\label{fig: celeba256_qualitative}
\end{center}
\vskip -0.2in
\end{figure}

\end{document}
