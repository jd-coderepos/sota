\documentclass[11pt]{article}
\usepackage{ifthen}
\usepackage{microtype}\usepackage{boxedminipage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage[small,bf]{caption}
\usepackage{authblk}
\usepackage{amssymb,amsfonts,amsthm,epsfig}
\setlength{\topmargin}{-0.25in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\textheight}{8.5in}


\newtheorem{theorem}{Theorem}
\newtheorem{conj}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{observation}{Observation}
\newtheorem{cond}{Condition}
\newtheorem{proofi}{Proof Idea}


\newfloat{Algorithm}{hbtp}{lop}[section]

\newcommand{\MA}{\mathcal{MA}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\pr}{{\bf Pr}}
\newcommand{\eps}{{\epsilon}}
\newcommand{\C}[1]{{{C}^{(#1)}}}
\newcommand{\itS}[1]{{{S}^{(#1)}}}
\newcommand{\itT}[1]{{{T}^{(#1)}}}
\newcommand{\etal}{{\it et al.}}


\bibliographystyle{plain}

\begin{document}

\title{Improved analysis of -sampling based PTAS for -means and other Clustering problems}

\author[1]{Ragesh Jaiswal \thanks{Corresponding author: \texttt{rjaiswal@cse.iitd.ac.in}}}
\author[1]{Mehul Kumar}
\author[1]{Pulkit Yadav}

\affil[1]{Department of Computer Science and Engineering, IIT Delhi}

\date{}

\maketitle

\begin{abstract}
We give an improved analysis of the simple -sampling based PTAS for the -means clustering problem given by Jaiswal, Kumar, and Sen~\cite{jks12}.
The improvement on the running time is from  to .
\end{abstract}


\section{Introduction}
We present this work as a short note and this may be regarded as an extension of the work by Jaiswal, Kumar, and Sen~\cite{jks12}.
We would like to point the reader to \cite{jks12} for more elaborate discussions regarding the past work in designing PTAS for the -means problem and sampling based algorithms.
Here, we will only discuss our improved analysis.
However, we will not skip any essential details about the algorithm or the analysis and this paper will be self-contained.
Note that this will be at the cost of having some basic material borrowed from \cite{jks12}.


\paragraph{Generalized -median problem and -means} 
The general -median problem may be defined as follows: 
Let  be any input space and let  denote the distance measure defined over pair of points of this input space. 
Given a set  with , and a parameter , find a set  with  such that the following objective function is minimized: .
Here,  denotes the distance of  from the closest center in .
The -means problem is a special case of the generalized -median problem where  and , where  denotes the Euclidean distance between points  and .
We will discuss both the generalized problem and the -means problem.
We give the algorithm and analysis for the generalized problem and then state the result related to the -means problem.

\paragraph{Related work}
The -means problem is -hard and there are exact algorithms that solve the problem in time .
Moreover, there are hardness of approximation results known for this problem showing that approximating beyond a fixed constant factor is unlikely. 
However, there has been significant research in obtaining -approximation algorithms for the problem that run in time that is exponential in  since in many cases  can be assumed to be a small constant.
We usually call such algorithms PTAS (under the implicit assumption that  is a constant).
We advise the reader to see a more detailed description of the previous work in \cite{jks12}.
The running time of our algorithm match those of Ackermann \etal~\cite{abs10}.
They use an improved analysis of a PTAS by Kumar \etal~\cite{KumarSS10} and show that the running time of their PTAS is .
\footnote{ notation hides a  factor which simplifies the expression.}


\paragraph{Our contributions} 
In this paper, we give an improved analysis of the PTAS of Jaiswal \etal~\cite{jks12} and using this obtain a PTAS for the -means problem with running time . 
The analysis in~\cite{jks12} crucially used a {\em separation} or {\em irreducibility} property. 
The main idea of the new analysis is to remove this dependency.
\footnote{The reader familiar with \cite{jks12} may note that the separation condition was used in \cite{jks12} to show that while sampling, there is an optimal cluster such that {\em all} points from that cluster have a good chance of being sampled.
The crucial observation in this work is that this property is not really required since the points that have small chance to be sampled are close to the already chosen centers. 
So, we just need to worry about picking a good center for the points that have a good chance of being sampled.}
Using the coreset construction of Chen~\cite{Chen09} with our algorithm, we can obtain a -approximation algorithm with running time .
This is a standard method to obtain a faster algorithm.
The reader is encouraged to see the work of Chen~\cite{Chen09} and Ackermann and Bl\"{o}mer~\cite{ab10} for this.


\paragraph{Organization} We give some preliminary results in Section~\ref{sec:pre}. We give the description of our algorithm and its analysis in Section~\ref{sec:algo-analysis}. In Section~\ref{sec:k-means}, we state our results for the -means problem.












\section{Preliminaries}
\label{sec:pre}

Given a set of points  and a set of centers , .
For singleton , we shall often abuse notation and use .
For any positive integer , we denote the cost of the optimal -means solution for  using .  

Our results hold, given that the dissimilarity measure satisfies certain properties. 
Note that many of the measures that are actually used in practice, do satisfy these properties and hence they are not restrictive. 
The reader should see previous works \cite{jks12, abs10} for detailed discussions.
Here, we state the properties that we will be using.
First, we will describe a property that apart from minor differences, is basically a definition by Ackermann et al. \cite{abs10} (see Theorem 1.1) who discuss PTAS 
for the -median problem with respect to metric and non-metric distance measures.

\begin{definition}[-Sampling property]
Given  and , a distance measure  over space  is said to have -sampling property if the following holds:
for any set , a uniformly random sample  of  points from  satisfies 

where  denotes the mean of points in .
\end{definition}

\begin{definition}[Centroid property]
A distance measure  over space  is said to satisfy the centroid property if for any subset  and any point , we have: ,
where  denotes the mean of the points in .
\end{definition}

\begin{definition}[-approximate triangle inequality]
Given , a distance measure  over space  is said to satisfy -approximate triangle inequality if for any three points 
\end{definition}

\begin{definition}[-approximate symmetry]
Given , a distance measure  over space  is said to satisfy -symmetric property if for any pair of points , 
\end{definition}

\noindent
We will also use the Chernoff bound.
\begin{theorem}[Chernoff bound]\label{thm:chernoff}
Let  be independent 0/1 random variables. Let  and . 
Let  be any real number. Then  .
\end{theorem}






\section{Our Algorithm and Analysis}
\label{sec:algo-analysis}

\begin{definition}[-sampling]
Given a set of points  and a set of centers , a point  is said to be sampled using {\em -sampling}
with respect to  if the probability of it being sampled, ,  is given by

\end{definition}

Here is a high level description of our algorithm. 
Essentially, the algorithm maintains a set  of centers, where . 
Initially  is empty, and in each iteration, it adds one center to  till its size reaches . 
Given a set , it samples a multiset  of  points from  using -sampling with respect to . 
Then it picks  a subset  of  of size , and adds the centroid of  to . 
The algorithm cycles through all possible  subsets of size  of  as choices for , and for each such choice,
repeats the above steps to find the next center, and so on. 
Repetition is done for probability amplification.
Algorithm~\ref{fig:k} gives a concise representation of the above algorithm.
Next, we state our main theorem with respect to our algorithm and prove this theorem in the rest of this section.

\begin{center}
\begin{Algorithm}[h]
\begin{boxedminipage}{6.5in}
{\bf Find--means()}

\hspace{0.1in} - Let , , 
, 

\hspace{0.1in} - Repeat  times and output the the set of centers  that give least cost

\hspace{0.3in} - Make a call to {\bf Sample-centers} and select  from the set of solutions

\hspace{0.4in} that gives the least cost.

{\bf Sample-centers}

\hspace{0.1in} (1) If  then add  to the set of solutions

\hspace{0.1in} (2) else

\hspace{0.3in} (a) Sample a multiset  of  points with -sampling (w.r.t. centers )

\hspace{0.3in} (b) For all 

\hspace{0.5in} (i) Let  be the  subset\footnote{To be able to define this, we fix an arbitrary ordering of points in  that defines an ordering on the points in . 
Also, for any set of size , we fix an arbitrary ordering of the subsets of size  of this set. } of . 
.\footnote{ denote the centroid of the points in  i.e., .}

\hspace{0.5in} (ii) {\bf Sample-centers}
\end{boxedminipage}
\caption{{\bf Find--means()} gives -approximation for data sets , where the distance measure  over  that satisfies -approximate triangle inequality, -approximate symmetry, Centroid property, and -sampling property.}
\label{fig:k}
\end{Algorithm}
\end{center}

\begin{theorem}[Main Theorem]\label{thm:other}
Let . 
Let , , and  be constants and let . Let . Let  be a distance measure over space  that  follows:
(a) -approximate symmetry property,
(b) -approximate triangle inequality,
(c) Centroid property, and
(d) -sampling property.
{\bf Find--means} runs in time  and gives a -approximation to the -median objective for any point set .
\end{theorem}

We develop some notation first. 
Let  be the set  at the end of the  recursive call of {\bf Sample-centers}.
To begin with  is empty. 
Let  be the multiset  sampled during the  recursive call, and  be the corresponding set  (which is the  subset of ).
Let  be the optimal clusters, and  denote the respective optimal cluster centers. 
Further, let  denote .
Let  denote the average cost paid by a point in , i.e.,
.


We give an outline of the proof. 
Suppose before the   recursive call to {\bf Sample-centers}, 
we have found centers which are close to the centers of some  clusters in the optimal solution. 
Conditioned on this fact, we show that in the next call, we are likely to sample enough {\em far-away} points from one of the remaining clusters.
We will show that the points that are closer to the currently chosen centers have a very small overhead if assigned to their closest center.
Further, we show that the samples from this new cluster are close to uniform distribution (c.f. Lemma~\ref{lem:key-repeat}). 
Since such a sample does not come from exactly uniform distribution, we cannot use the -sampling property directly. 

We now show that the following invariant will hold for all calls: let  consist of centers
 (added in this order). 
Then, with probability at least , there exist distinct indices  such that for all , .
Here,  is a fixed constant that depends on  and . With foresight, we fix the value of .
Suppose this invariant holds for  (the base case is easy since  is empty).
We will then show that this invariant holds for  as well (unless the current set of centers already give a -approximation in which case there is nothing more to be shown). 
In other words, we just show that in the  recursive call, with probability at least ,  the algorithm finds a center   such that
, where  is an index distinct from
.
This will basically show that at the end of the last call, we will have  centers that give a -approximation with probability at least .


We now show that the invariant holds for . We use the notation developed above for . 
Let  denote the set of indices . 
Now let  be the index  for which 
is maximum. 
Intuitively, conditioned on sampling from  using -sampling, it is  likely that
enough points from  will be sampled.
A simple corollary of the next lemma shows that there is good chance that elements from the sets  will be sampled, unless the current set of centers already gives the required -approximation factor in which case we are done.

\begin{lemma}
\label{lem:sampled-repeat}
If , then .
\end{lemma}
\begin{proof}
We have:
{
\allowdisplaybreaks

}
\end{proof}

\noindent
We get the following corollary easily.
\begin{corollary}
\label{cor:sample-repeat}
If , then 
\end{corollary}

The above lemma and its corollary say that: given that the current set of centers do not give the desired approximation, points in the set  will be sampled with probability at least .
However, the points within  are not sampled uniformly.
Some points in  might be sampled with higher probability than other points.
Let us partition the points in  into two sets, one with points that have a large conditional probability of being sampled and those that have small conditional probability.
Let  such that 

Let .
In the next two lemmas, we show that for each point  is small and hence the contribution to the total potential with respect to  is small. 

\begin{lemma}
\label{lem:key-repeat}
For any  and any point , .
\end{lemma}
\begin{proof}
Using (\ref{eqn:partition}), we get the following:

The last inequality is using the -approximate triangle inequality. 
We get the following from the above:

\end{proof}

\begin{lemma}
For any , .
\end{lemma}
\begin{proof}
Using the previous lemma, we get the following:

\end{proof}

Now, suppose that in the  iteration, we manage to pick a point  such that 
.
Then using the previous two lemmas, we have . 
In the next two lemmas, we show that there is a good probability of finding such a point.
The proof of these lemmas are almost the same (except for minor change in parameters) as the proof of Lemmas 12 and 13 in \cite{jks12}.


\begin{lemma}
\label{lem:clinaba-repeat}
Let  be a set of  points, and  be a parameter, . Define a random variable  as follows :
with probability , it picks an element of  uniformly at random, and with  probability , it does not
pick any element (i.e., is null). 
Let  be  independent copies of , where 
Let  denote the multi-set of elements of  picked by . Then, with probability at least ,
 contains a subset  of size  which satisfies

\end{lemma}
\begin{proof}
Define a random variable , which is a subset of the index set , as follows
Q. 
Conditioned on , note that the random variables  are independent uniform samples from . 
Thus if , then sampling property wrt.  implies that with probability at least , the desired event~(\ref{eq:clinaba-repeat}) happens. 
But the expected value of  is , and so, from Chernoff bound (Theorem~\ref{thm:chernoff})  with probability at least . 
Hence, the statement in the lemma is true.
\end{proof}

\begin{lemma}
\label{lem:final-repeat}
With probability at least , there exists a subset  of  of size at most  such that
.
\end{lemma}
\begin{proof}
Recall that  contains  independent samples of  (using -sampling). 
We are interested in .
Let  be   independent random variables defined as follows : 
for any , ,  is obtained by sampling an element of  using -sampling with respect to . 
If this sampled element is not in , then it just discards it (i.e.,  is null) otherwise  is assigned that element.
Let  denote . 
Corollary~\ref{cor:sample-repeat} and Lemma~\ref{lem:key-repeat} imply that  is assigned a particular element of  with probability at least . 
We would now like to apply Lemma~\ref{lem:clinaba-repeat} (observe that ). 
We can do this by a simple coupling argument as follows.
For any point , let the probability of it being sampled using  sampling be denoted by .
Note that .
So, for a particular element ,  is assigned  probability .
One way of sampling a random variable  as in Lemma~\ref{lem:clinaba-repeat} is as follows --  first sample using . 
If  is null, then  is also null. 
Otherwise, suppose  is assigned an element  of . 
Then  is equal to  with probability , and null otherwise. 
It is easy to check that with probability ,  is a uniform sample from , and null with probability . 
Now, observe that the set of elements of  sampled by  is always a superset of . 
We can now use Lemma~\ref{lem:clinaba-repeat} to finish the proof.
\end{proof}

Thus, we will take the index  in Step 2(b) as the index of the set  as guaranteed by the lemma above.
Finally, by repeating the entire process  times, we make sure that we get a -approximate solution
with high probability.
Note that the total running time of our algorithm is .





\section{The -means Problem}\label{sec:k-means}

Note that in the -means problem  and the distance measure  is square of the Euclidean distance. 
Since this distance measure satisfies all the properties used in the analysis, we get the following result:

\begin{theorem}
{\bf Find--means} runs in time  and gives a -approximation to the -means problem.
\end{theorem}
\begin{proof}
The proof follows from the proof of Theorem~\ref{thm:other} and the fact that the squared Euclidean distance measure over  satisfies -approximate symmetry (trivial), -approximate triangle inequality (see Lemma~3 in~\cite{jks12}), Centroid property (see Lemma~2 in \cite{jks12}), and -sampling property for  (see Lemma~1 in~\cite{jks12}).
\qedhere
\end{proof}

\bibliography{paper}


\end{document}
