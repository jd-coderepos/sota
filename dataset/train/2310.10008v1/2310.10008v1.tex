
\documentclass{article} \usepackage{iclr2024_conference,times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{algorithmic,algorithm}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{array}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{caption} 
\usepackage{xcolor}
\usepackage{subcaption} 
\usepackage[nopar]{lipsum}
\usepackage{listings}
\usepackage[export]{adjustbox}
\usepackage{makecell}

\usepackage{mathtools}
\usepackage{siunitx}
\usepackage[nopar]{lipsum}
\usepackage{listings}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\def\Model{UniDG }
\usepackage{xcolor}
\usepackage{wrapfig} 
\definecolor{myy}{RGB}{126,95,0}
\definecolor{mygray}{gray}{.9}
\definecolor{Gray}{gray}{0.9}
\definecolor{bblue}{RGB}{30,80,120}
\definecolor{mygray1}{gray}{.7}
\definecolor{ggray}{RGB}{127,127,127}
\definecolor{defaultcolor}{gray}{.9}
\definecolor{dark-gray}{gray}{0.20}
\newcommand{\pub}[1]{{\color{dark-gray}{\tiny{[{#1}]}}}}
\newcommand{\default}[1]{\cellcolor{defaultcolor}{#1}}
\newcommand{\reshl}[2]{
	\textbf{#1} \fontsize{7.5pt}{1em}\selectfont\color{mygreen}{ \textbf{#2}}
}
\definecolor{mygreen}{HTML}{39b54a}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\z}{\boldsymbol{z}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\title{Towards Unified and Effective Domain Generalization}



\author{
Yiyuan Zhang\textsuperscript{1,2}\thanks{Equal Contribution},
~~~ Kaixiong Gong\textsuperscript{1,2},
~~~ Xiaohan Ding\textsuperscript{4},\\
~~\textbf{Kaipeng Zhang}\textsuperscript{2}\thanks{Correspondence},
~~~\textbf{Fangrui Lv}\textsuperscript{5},
\quad~~\textbf{Kurt Keutzer}\textsuperscript{3},
\quad~~ \textbf{Xiangyu Yue}\textsuperscript{1}\\
\textsuperscript{1}Multimedia Lab, The Chinese University of Hong Kong\\
\textsuperscript{2}OpenGVLab, Shanghai AI Lab~~~
\textsuperscript{3}UC Berkeley
\textsuperscript{4}Tencent AI Lab
\textsuperscript{5}Tsinghua University\\
{\tt\small \{yiyuanzhang.ai, kaixionggong\}@gmail.com}
~~~{\tt\small xyyue@ie.cuhk.edu.hk}\\
\quad \quad ~~~\url{https://invictus717.github.io/Generalization}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
	
	
	\maketitle
	
	\begin{abstract}
        We propose \textbf{UniDG}, a novel and \textbf{Uni}fied framework for \textbf{D}omain \textbf{G}eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures. The core idea of UniDG is to finetune models during the inference stage, which saves the cost of iterative training. Specifically, we encourage models to learn the distribution of test data in an unsupervised manner and impose a penalty regarding the updating step of model parameters. The penalty term can effectively reduce the catastrophic forgetting issue as we would like to maximally preserve the valuable knowledge in the original model. Empirically, across 12 visual backbones, including CNN-, MLP-, and Transformer-based models, ranging from 1.89M to 303M parameters, UniDG shows an average accuracy improvement of +5.4\% on DomainBed. These performance results demonstrate the superiority and versatility of UniDG. The code is publicly available at \url{https://github.com/invictus717/UniDG}.
	\end{abstract}
	
	\section{Introduction} ~\label{sec:intro}
The Out-Of-Distribution (OOD) problem is a prevalent topic in the machine learning and computer vision communities~\citep{long2015learning,saito2020universal,sun2016deep,ebrahimi2020adversarial} as models of various architectures and scales are suffering from this problem~\citep{zhou2022domain,li2022sparse,chen2022compound,peng2022semantic}. Therefore, training deep models to generalize well on new domains has become a prevalent research topic~\citep{long2015learning,li2018domain,wang2019transferable,chen2022contrastive,cha2021swad,cha2022domain}. 
\begin{wrapfigure}{r}[0cm]{0pt}
    \begin{minipage}{0.49\linewidth}
		\vspace{-1mm}
        \includegraphics[width=1.0\linewidth]{figures/fig1.pdf}
        \vspace{-5.5mm}
        \caption{Large-scale pretrained foundation models are still suffering from domain shifts.}
        \label{fig:moti}
        \vspace{-5mm}
    \end{minipage}
    \end{wrapfigure}
    To overcome the domain shift problem, pretraining-based methods~\citep{radford2021learning,singh2022revisiting,cha2022domain} utilize large-scale data to obtain better generalization ability. However, in practice, domain shift can be so significant that even though the powerful foundation models have been pretrained on huge-scale datasets, directly generalizing the models to new domains still delivers unsatisfactory performance, as shown in Figure~\ref{fig:moti}.
Another drawback of pretraining-based methods is the inferior finetuning performance, finetuning pretrained models leads to catastrophic forgetting and limited improvement on new domains~\citep{cha2022domain,li2022uncertainty,chen2022self}. As a workaround, pretraining-based methods may add data from the new domains into the pretraining dataset and retrain the models from scratch~\citep{shu2023clipood}. When the pretraining dataset is large (\textit{e.g.}, CLIP~\citep{radford2021learning} uses LAION-400M), this approach becomes significantly expensive.
    
	In contrast to the pretraining-based methods, Test-Time Adaptation (TTA) ~\citep{sun2020test,wang2020tent,wang2022continual,wang2022generalizing} is an alternative to mitigate domain shift on new domains. First, TTA requires no pretraining with novel data, and can directly leverage the off-the-shelf models. Second, by updating parameters in both training and evaluation stages~\citep{sun2020test}, TTA reduces the reliance of models on annotations in new domains. However, we would like to note several drawbacks of existing TTA methods.
    \begin{wrapfigure}{l}[0cm]{0pt}
    \begin{minipage}{0.43\linewidth}
    \vspace{-5.5mm}
    \includegraphics[width=1.0\linewidth]{figures/fig2.pdf}
    \vspace{-6mm}
    \caption{A comparison between existing methods and UniDG on the accuracy averaged across the PACS, VLCS, OfficeHome, and TerraInc datasets.}
    \label{fig:cmp}
    \vspace{-4mm}
    \end{minipage}
    \end{wrapfigure}
     \textbf{1)} Most TTA methods~\citep{wang2020tent,iwasawa2021test,jang2022test} require updating Batch Normalization (BN) \citep{ioffe2015batch} layers in the original model to adapt to the distribution of test data. However, recent visual foundation models such as vision transformers~\citep{dosovitskiy2020image} are developed with Layer Normalization (LN) layers. Due to the essential difference between BN and LN, simply adapting the ideas of BN-based methods to LN layers results in minimal improvement (around 0.5\%, see Appendix~\S~\ref{sec:supp:exp}). \textbf{2)} Recent TTA methods~\citep{zhang2023adanpc,park2023test,zhang2023domainadaptor, chen2023improved} show limited scalability on common visual foundation models ranging from small to large scales.
     For example, with TTA, only limited improvements (less than 2\%) on large-scale foundation models~\citep{radford2021learning,liu2022convnet} are observed. \textbf{3)} From a theoretical perspective, we find these TTA methods reduce the Neural Tangent Kernel~\citep{jacot2018neural} in the adaptation process, which limits the further generalization (theoretical analysis is presented in Appendix~\S~\ref{sec:theory}).

    \begin{wrapfigure}{l}[0cm]{0pt}
    \begin{minipage}{0.48\linewidth}
    \vspace{-5.5mm}
    \includegraphics[width=1.0\linewidth]{figures/fig3.pdf}
    \vspace{-5.5mm}
    \caption{UniDG brings out an average of 5.4\% improvement to 12 backbones which scale from 1.59M to 303M parameters.}
    \label{fig:backbone}
    \vspace{-5mm}
    \end{minipage}
    \end{wrapfigure}
 To address the aforementioned drawbacks, we focus on an important topic of TTA method - how to effectively update the encoder (\textit{i.e.}, feature extractor) for TTA. Prior works either update the encoder via back-propagation or freeze it, but either way has its own weaknesses. 1) If we allow the encoder to update, similar to the weakness of finetuning a pretrained encoder as discussed above, catastrophic forgetting can happen during TTA and result in a significantly lower quality of extracted features. 2) When the encoder is frozen, it struggles to adapt effectively to new domains. Consequently, the extracted features require further refinement through additional mechanisms to be optimally utilized by the classifier.

 

 In this paper, we propose a novel method, named \textbf{Marginal Generalization}, to update the encoder for TTA. Intuitively, Marginal Generalization aims to let the encoder learn representations of the target data \emph{within a certain distance from the representations obtained by the initial model}. Here we use a simplified notation for brevity. Let  be the specified distance,  be the fixed initial encoder and  be a learnable copy of ,  be the samples of the target domain,  be the classifier which takes the representations  as inputs, the objective is to
    


By doing so, we overcome the drawbacks of the two traditional approaches mentioned above. 
1) Intuitively, while the encoder  is trying to adapt to the novel data, it always refers to the original model  and keeps the representations within a distance  from the original, which means the pretrained source knowledge can be preserved and catastrophic forgetting is avoided. 2) As we keep updating the encoder via entropy minimization on the test data, it cooperates better with the classifier and yields more discriminative features on the target domain.

We would like to note that Marginal Generalization is \textbf{uni}versal because it does not require any specific structures in the original model nor the properties of the data, as well as effective (achieving improvement of 3.3\% on average accuracy as shown in Table~\ref{tab:ablation}). In addition, the features extracted by the updated encoder can be utilized by multiple TTA mechanisms. For example, by naturally combining Marginal Generalization and Memory Bank~\citep{wu2018unsupervised}, we propose \textbf{Differentiable Memory Bank}, which demonstrates superior performance over the traditional memory bank methods. For example, compared with T3A~\citep{iwasawa2021test} that adopts typical memory bank, our method with ResNet-50 backbone outperforms it by 4.3\% on average accuracy across 4 datasets as shown in Table~\ref{tab:tta}. Intuitively, UniDG simultaneously utilizes i) the local minimum of distances between adapted and source representations, and ii) the local maximum of information entropy between adapted representations and pseudo labels in the test-time process to continuously approximate the models on the test data with preserved pretrained knowledge. 
The details will be presented in Section~\ref{sec:method:feat}, and \ref{sec:method:dyn}.

Based on Marginal Generalization, we propose a framework composed of an adaptation method of the encoder (which is a \textbf{uni}versal method to extract better features) and Differentiable Memory Bank (which is a \textbf{uni}versal mechanism to refine features for DG) so that the framework is named \textbf{UniDG}, which delivers state-of-the-art performance on multiple domain generalization benchmarks. For example, UniDG delivers an average accuracy of 79.6\% on 5 widely adopted benchmarks including PACS, VLCS, and OfficeHome, outperforming the second-best CAR-FT~\citep{mao2022context} by 1.0\%. Additionally, UniDG is an architecture-agnostic framework that consistently yields significant improvements when applied to a wide range of visual backbones, including models of varying scales such as MobileNet V3~\citep{howard2019searching}, ConvNeXt-Base~\citep{liu2022convnet}, and ViT-Large~\citep{dosovitskiy2020image}, demonstrating its strong scalability as shown in Figure~\ref{fig:backbone}. For example, UniDG improves the mean accuracy scores by 5.4\% with such 12 models on PACS~\citep{torralba2011unbiased}, VLCS~\citep{li2017deeper}, OfficeHome~\citep{venkateswara2017deep}, and TerraInc~\citep{beery2018recognition} as shown in Figure~\ref{fig:cmp}. We would like to note that Marginal Generalization and Differentiable Memory Bank can also be used separately and combined with other methods. When we combine these two schemes, we observe an average improvement of +5.0\%, as shown in Table~\ref{tab:dg}.
   
    Our contributions are summarized as the following:
    \begin{itemize}
        \item We propose Marginal Generalization, which significantly mitigates the problems of feature encoder adaptation during TTA.
        \item With Marginal Generalization, we naturally upgrade the traditional memory bank mechanism to Differentiable Memory Bank and propose a universal TTA framework named UniDG.
        \item UniDG consistently outperforms the previous state-of-the-art methods by a significant margin (\textit{e.g.} +5.4\% on DomainBed), and it is applicable to a wide range of models with different architectures and varying scales.
        \item We show that the components in UniDG can also be separately combined with other methods (\textit{e.g.} averagely bringing out +5.0\% on DomainBed), demonstrating its flexibility.
    \end{itemize}

    \section{Method} ~\label{sec:method}
    
    We first introduce the formulation of domain generalization and test-time adaptation in \S~\ref{sec:method:pre}. The framework of UniDG comprises two components: 1) we employ \emph{Marginal  Generalization} (\S~\ref{sec:method:feat}) to adapt the encoder, 2) we utilize prototypes with \emph{Differentiable Memory Bank } (\S~\ref{sec:method:dyn}) for learning a discriminative classifier on the target domain. The whole framework is summarized in Figure~\ref{fig:framework}. 
    
    \subsection{Preliminary} \label{sec:method:pre}
    
    \paragraph{Domain Generalization.}
    Given a set of source domains , each domain  containing images and labels, , where  denotes an image and  indicates the corresponding ground truth label, the goal of DG is to train a model on source domains  in a way that it can effectively generalize to a novel target domain  which differs from any of the source domains. We denote the mapping function of the model as , where  is the prediction and  is the number of categories.  consists of two steps: feature extraction with the encoder  and prediction with the classifier  based on the features. Let  be the parameters,  can be formulated as . 

    \paragraph{Training on source domains.}
    Use  to denote the cross-entropy function, and the objective of training on the source domains is to optimize  as
    

    \paragraph{Test-Time Adaptation.}
   With  trained on the source domains , test-time adaptation is a self-supervised learning process to further adapt parameters to the target domain . The encoder parameters during test time can be optimized as the following, where  is the softmax entropy:
    
    

    
    \subsection{Marginal Generalization}~\label{sec:method:feat}
    Marginal Generalization aims to constrain the discrepancy between features extracted by the source encoder  and the adapted encoder  during the adaptation process so that the adapted model will be able to maintain general representation bias and relieve catastrophic forgetting while updating parameters. Here we adopt Euclidean distance as the metric out of its simplicity and universality, which is formulated with the Frobenius norm . We use  to denote the parameters of the encoder, which is a subset of , so that the encoder can be formulated as . Given the pre-defined distance threshold , the objective then becomes
    
    The motivation is that we desire to gradually update the parameters of the adapted encoder under the condition that the representation bias will not get sharply adapted. For the source feature extractor , we freeze it and still use it to extract the representation from target domains as pretrained knowledge. For the adapted encoder , we initialize it with the source-pretrained parameters . Therefore, the discrepancy between the original and adapted representations can be formulated as the distance between  and .
    
    To approximate such a hard constraint with a back-propagation-based method, we propose a novel loss function named \emph{Marginal Adaptation Loss} to constrain the update of the parameters of the encoder. The Marginal Adaptation Loss can be formulated as:
    

    The update of parameters of the classifier  and encoder is guided by the entropy on the target domain. Based on the extracted representations , we use a linear layer to work as a classifier and obtain the classification probability  using a \texttt{softmax} operation. Then we take the entropy as the loss function to derive the gradients for updating the classifier and encoder, through which we can introduce the probabilistic distribution of target domains to our classifier: 
        

        \begin{figure}[t]
        \centering
        \includegraphics[width=1.0\linewidth]{figures/fig4.pdf}
    \caption{Illustration of UniDG, which consists of Marginal Generalization (\S~\ref{sec:method:feat}) and the Differentiable Memory Bank mechanism (\S~\ref{sec:method:dyn}).}
        \label{fig:framework}
        \vspace{-4mm}
    \end{figure}
   
    \subsection{Differentiable Memory Bank}~\label{sec:method:dyn}
With Marginal Generalization, we are able to learn a well-adapted encoder that can extract discriminative features on the target domain. However, since there is no labeled data on the target domain, only training with the unsupervised losses  and  is hard to get a classifier  with high performance on the target. To mitigate this issue, 
we propose to update the classifier with a differentiable memory bank. We utilize the memory bank to select prototypes suitable for the new domain, develop class-wise prototypes directly differentiable with loss function, and update the whole bank in every forward step.

    \textbf{Class-wise prototypes} are stored in the memory bank in order to enhance the classifier. Specifically, for each class , the prototype  is initialized with the corresponding weights of the source classifier layer. In the self-supervised adaptation process, for each target sample , we extract the representations  and obtain the output of classifier . Then we predict pseudo labels  and utilize the entropy between representations and pseudo labels as the criterion to select the Top- instances of each class with highest classification confidence, where  is a pre-defined hyper-parameter. After that, we utilize the representations of the Top- samples 
    to produce the class-wise prototypes . 

    \textbf{Memory bank} is set to store the prototypes of each class , where , , and  denote the memory bank, the number of classes, and feature dimension. At each forward step, we compute the prototypes, which will be further used to update the classifier weights . For a given sample  with feature , the classification probability of class  can be computed as:
    
    where  is the -th element of the weight matrix . Note that for  to classify target samples correctly, the weight  needs to be representative of features of the corresponding class . This indicates that the meaning of  coincides with the ideal cluster prototype of class  in the target domain. Thus, we propose to use the estimate of the ideal target cluster prototypes  to update the classifier weights: . This process is essential in learning a robust classifier for the target domain with no labeled data. 
    
\subsection{UniDG Learning for Domain Generalization}
In UniDG framework, the marginal generalization is proposed to learn a well-adapted feature encoder without catastrophic forgetting, and the differentiable memory bank is proposed to learn a discriminative classifier for the target domain. 
While updating  with target prototypes, the overall learning objective is: 





\section{Experiments} ~\label{sec:exp}
\subsection{Setup} ~\label{sec:exp:setup}
\textbf{\noindent{Dataset}} 
VLCS~\citep{torralba2011unbiased} contains 10,729 instances of 5 classes derived from four photographic datasets in accordance with different domains. PACS~\citep{li2017deeper} comprises four domains: art, cartoons, photos, and sketches, including 9,991 instances of classes. OfficeHome ~\citep{venkateswara2017deep} derives from domains like art, clipart, product, and real, containing 15,588 images of 65 classes. TerraIncognita~\citep{beery2018recognition}is a real-world dataset that collects photos of wild animals taken by cameras at different locations. It contains 24,788 photos of 10 classes according to the species of animals. DomainNet~\citep{peng2019moment} is the largest dataset for domain generalization tasks, including 6 domains, 345 classes, and a total of 586,575 images. \par

\textbf{\noindent{Evaluation Metric}} We evaluate \Model by taking 3 parallel trials with random seeds to calculate means and standard errors of classification accuracy (\%) on 5 datasets. There are 22 different novel environments to evaluate the abilities of the network for generalization. We report detailed results for each environment in Appendix~\ref{sec:supp:exp}. \par

\textbf{\noindent{Implementation Details}} 
All experimental results are conducted on NVIDIA A100 GPUs. If not specified, we utilize ResNet-50~\citep{he2016deep} for extracting visual features and a single classifier for classification. On test-time benchmarks, we utilize ERM~\citep{vapnik1991principles} algorithm as our default method for training source models. We also follow default hyper-parameters of DomainBed~\citep{gulrajani2020search} like initial learning rate of , weight decay of , batch size of , holdout fraction of , and  of 0.15 (see Appendix \S~\ref{sec:imple} for more discussions).
\subsection{Main Results} ~\label{sec:exp:main_results}
We report experimental results on the domain generalization (\S~\ref{sec:exp:main_results:dg}) and test-time adaptation benchmarks (\S~\ref{sec:exp:main_results:tta}). \Model delivers new state-of-the-art performances on such benchmarks.
\subsubsection{Domain Generalization Benchmarks} \label{sec:exp:main_results:dg}
\Model prominently achieves a brilliant performance on Domain generalization tasks. Table~\ref{tab:dg} shows the performances of the existing advanced approaches for DG tasks using different pre-training methods. The upper part of the table demonstrates that with ImageNet pre-training, \Model significantly outperforms various classic models and shows satisfactory stability. Specifically, it achieved an average accuracy of 68.5 on VLCS, PACS, OfficeHome, Terrain, and DomainNet, exceeding AdaNPC by +2.0\%, and the best results on VLCS, PACS, terrain, and DomainNet. The remaining part of Table~\ref{tab:dg} shows more results with large-scale CLIP and SWAG pre-training. Expectedly, the CLIP- and SWAG-trained models outperform the traditional ImageNet-trained ones. However, impressively, with only ImageNet pre-training, \Model outperforms the CAR-FT model with CLIP pre-training by 1.1\% in the average accuracy (79.6\% vs. 78.5\%). On the terrain data set with complex domain shift, the accuracy of \Model reached 62.4\%, outperforming CAR-FT by 0.5\%.
\begin{table*}[t]
	\centering
	\caption{Overall out-of-domain accuracies with train-validation selection criterion on the \textbf{DomainBed} benchmark. The best result
		is highlighted in \textbf{bold}. \Model achieves the best performances on PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet datasets. }
  \vspace{-3mm}
	\label{tab:dg}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{l|c|c|cccccc} 
			\toprule
			Algorithm & Venue & Pretraining & PACS & VLCS & OfficeHome & TerraInc & DomainNet & Avg.\\
			\hline ERM~(ResNet-50)~\citep{vapnik1991principles} & & \multirow{15}{*}{ImageNet} &  &  &  &  &  &  \\ 
			DANN~\citep{ganin2016domain} & JMLR'16 & &  &  &  &  &  &  \\
			
			MMD~\citep{li2018domain} & CVPR'18 & &  &  &  &  &   &  \\
			IRM ~\citep{arjovsky2019invariant} & ArXiv'20 &  &  &  &  &  &   &  \\
			FISH~\citep{shi2021gradient} & ICLR'22 &  &  &  &  &  &  &  \\
			SWAD~\citep{cha2021swad} & NeurIPS'21 &  &  &  &  &  &   &   \\
			ERM (ViT-S/16)~\citep{dosovitskiy2020image} & ICLR'21 &  &  &  &  &  &   &  \\
			Fishr~\citep{rame2022fishr} & ICML'22&  &  &  &  &  &  &  \\
			MIRO\citep{cha2022domain} & ECCV'22 &  &  &  &  &  &  &  \\
			GMoE-S/16~\citep{li2022sparse} & ICLR'23 &  &  &  &  &  &  &  \\ 
                ITTA~\citep{chen2023improved} & CVPR'23 & &   &  &  &  &  & 60.2 \\
                DomainAdaptor~\citep{zhang2023domainadaptor} & ICCV'23 & &  &  &  & - & - & - \\ 
                AdaNPC~\citep{zhang2023adanpc} & ICML'23 &  &  &  &  &   &   & 66.5 \\
			\rowcolor{mygray}
                \Model & Ours & &   
			&                &               &          &   &  \\
			\midrule 
                \multicolumn{9}{c}{ \textit{UniDG with Existing DG Methods} } \\ \hline
                CORAL~\citep{sun2016deep} & ECCV'16 & {ImageNet} &  &  &  &  &   &   \\ 
                \rowcolor{mygray}
                \Model + CORAL &Ours & {ImageNet} &   
			&                &               &          &   &  \textcolor{mygreen}{+5.2} \\ \hline 
                MIRO\citep{cha2022domain} & ECCV'22 &  {ImageNet} &  &  &  &  &  &  \\ 
                \rowcolor{mygray}
                \Model + MIRO & Ours &  {ImageNet} &  &  &  &  &  &  \textcolor{mygreen}{+4.9} \\ 
                \midrule
			\multicolumn{9}{c}{ViT-B/16~\citep{dosovitskiy2020image} Backbone} \\ \hline
			ERM~\citep{vapnik1991principles}  & & & 93.7 & 82.7 & 78.5 & 52.3 & 53.8 & 72.2 \\
			MIRO~\citep{cha2022domain} & ECCV'22 & \multirow{3}{*}{CLIP}& 95.6 & 82.2 & 82.5 & 54.3 & 54.0 & 73.7 \\
			DPL~\citep{zhang2021domain} & Arxiv'22 &   & 97.3 & 84.3 & 84.2 & 52.6 & 56.7 & 75.0 \\ 
			CAR-FT~\citep{mao2022context} & Arxiv'22 &   & 96.8 & 85.5 & 85.7 & 61.9 & \textbf{62.5} & 78.5 \\ \hline
                \rowcolor{mygray}
                UniDG & Ours &  &  &  &  &  &  & 78.6  \\ \hline
			\multicolumn{9}{c}{Base-scale Visual Backbone} \\ \hline
			
			ERM~\citep{vapnik1991principles}  & &  \multirow{2}{*}{SWAG} &  &  &  &  &  & 68.0 \\ 
			MIRO~\citep{cha2022domain}~(RegNetY-16GF~\citep{radosavovic2020designing}) & ECCV'22    &  &  &  &  &  &  & 74.1   \\ \hline
			\rowcolor{mygray}
			\Model + CORAL~\citep{sun2016deep} + ConvNeXt-B~\citep{liu2022convnet}  &Ours & ImageNet &  &  &  &  &  &   \\ 
			\bottomrule
		\end{tabular}	
	}
	\vspace{-4mm}
\end{table*}
\begin{table*}[t]
	\begin{center}
		\vskip 0.10in
		\caption{Average accuracy () using classifiers learned by ERM on the domain generalization benchmarks. We use ResNet-18/50 as backbones. \textbf{Bold} indicates the best for each benchmark.}
  \vspace{-3mm}
		\label{tab:tta}
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{l|l|c|ccccc}
				\toprule
				Generalization Algorithm & Test-Time Algorithm  &Backbone& \text{VLCS} & \text{PACS} & \text{OfficeHome} & \text{TerraIncognita} & \text{Avg} \\ \midrule
				CLIP~\citep{radford2021learning} & Zero-Shot & ViT-B16 & 82.60.0 & 95.60.0 & 79.10.0 & 31.10.0 & 72.2\\ \hline
				\multirow{9}{*}{ERM~\citep{vapnik1991principles}}&  + None
				&\multirow{10}{*}{ResNet-18}& 74.90.5 & 79.30.8 & 62.10.3 & 40.61.2 & 64.2 \\
				& \text{+ PL}~\pub{ICMLW'13}~\citep{lee2013pseudo} && 63.02.7 & 71.01.8 & 58.23.2 & 37.47.2 & 57.4 \\
				& \text{+ PLClf}~\pub{ICMLW'13}~\citep{lee2013pseudo} && 74.90.6 & 78.12.3 & 61.90.4 & 41.81.9 & 64.2 \\
				& \text{+ SHOT}~\pub{ICML'20}~\citep{liang2020we} && 65.22.3 & 82.40.6 & 62.60.4 & 33.61.0 & 60.9 \\
				& \text{+ Tent}~\pub{ICLR'21}~\citep{wang2020tent} && 72.90.8 &  83.90.5 & 60.90.4 & 33.71.1 & 62.8 \\
				& \text{+ TentBN}~\pub{ICLR'21}~\citep{wang2020tent}  && 67.01.2 & 80.81.0 & 62.60.4 & 40.00.8 & 62.6 \\
				& \text{+ TentClf}~\pub{ICLR'21}~\citep{wang2020tent}  && 73.01.5 & 78.61.8 & 59.30.6 & 38.33.4 & 62.3 \\
				& \text{+ T3A}~\pub{NeurIPS'21}~\citep{iwasawa2021test} && 77.31.5 & 80.80.7 & 63.20.5 & 40.20.6 & 65.4 \\ 
				& \text{+ TAST}~\pub{ICLR'23}~\citep{jang2022test} && {77.30.7} & 81.90.4 & {63.70.5} & {42.60.7} & {66.4} \\ 
				\rowcolor{mygray}
				& \text{+ \Model}~\pub{Ours} &  & \textbf{80.9  0.1}              & {81.7  0.1}              & {58.4  0.1}             & \textbf{47.9  0.7}              &  \reshl{67.2}{0.8}                       \\
				\midrule
				\multirow{9}{*}{ERM~\citep{vapnik1991principles}}&  + None 
				&\multirow{10}{*}{ResNet-50}& 76.70.5 & 83.21.1 & 67.11.0 & 45.91.3 & 68.3 \\
				& \text{+ PL}~\pub{ICMLW'13}~\citep{lee2013pseudo} && 69.43.1 & 81.74.6 & 62.93.1 & 38.12.4 & 63.0 \\
				& \text{+ PLClf}~\pub{ICMLW'13}~\citep{lee2013pseudo} && 75.70.9 & 83.31.6 & 67.01.0 & 46.72.1 & 68.2 \\ 
				& \text{+ SHOT}~\pub{ICML'20}~\citep{liang2020we} & &67.10.9 & 84.11.2 & 67.70.7 & 35.20.8 & 63.5 \\ 
				& \text{+ Tent}~\pub{ICLR'21}~\citep{wang2020tent} && 73.01.3 & 85.20.6 & 66.30.8 & 37.12.0 & 65.4 \\ 
				& \text{+ TentBN}~\pub{ICLR'21}~\citep{wang2020tent} && 69.71.2 & 83.71.2 & 67.90.9 & 43.91.3 & 66.3 \\
				& \text{+ TentClf}~\pub{ICLR'21}~\citep{wang2020tent} && 75.80.7 & 82.71.6 & 66.81.0 & 43.62.6 & 67.2 \\ 
				& \text{+ T3A}~\pub{NeurIPS'21}~\citep{iwasawa2021test} && 77.30.4 & 83.91.1 & 68.30.8 & 45.61.1 & 68.8 \\
				& \text{+ TAST}~\pub{ICLR'23}~\citep{jang2022test} && {77.70.5} & 84.11.2 & 68.60.7 & {47.42.1}  & {69.5} \\
				\rowcolor{mygray}
				& \text{+ \Model}~\pub{Ours} & & \textbf{81.6  0.1}   & \textbf{89.0  0.3}              & \textbf{68.9  0.1}              & \textbf{52.9  0.2}              & \reshl{73.1}{3.6}                        \\
				\bottomrule 
			\end{tabular} \label{table:main_erm} 
		}
	\end{center}
	\vspace{-3mm}
\end{table*}  



\subsubsection{Test-Time Adaptation Benchmarks} \label{sec:exp:main_results:tta}
\Model remarkably outperforms all existing test-time methods including the state-of-the-art method, TAST~\citep{jang2022test}. Specifically, as shown in Table~\ref{tab:tta}, we choose ResNet-18 and ResNet-50 as the backbone and average accuracy as the metric to evaluate several test-time methods. \Model achieves an average accuracy of 67.2\% with ResNet-18 on VLCS, PACS, OfficeHome, and terrain, which is 0.8\% higher than the best-performing test-time method. The superiority of \Model is even more significant with ResNet-50: \Model achieves an average accuracy of 73.1\% on four benchmarks, largely exceeding the last state of the art, TAST~\citep{jang2022test}, by 3.5\%.

Except for ResNet-18 and ResNet-50, we further use UniDG with 12 mainstream backbones including CNN, MLP, and transformer architectures, and report the results in Figure~\ref{fig:backbone}. It turns out that \Model can significantly improve the performance of all the 12 backbones so that we conclude \Model is a universal architecture-agnostic method. Notably, the number of parameters of these models ranges from 1.59M to 303M, but \Model can significantly and consistently improve the performance by 5.4\% on average.
\begin{table*}[t]
	\centering
	\caption{Domain generalization accuracy with different backbone networks. \Model improves the
		performance agnostic to visual backbones. \textbf{Bold} type indicates performance improvement.}
  \vspace{-3mm}
	\label{tab:backbone}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|c|cccc|l}
			\hline Type & Backbone & Method & VLCS & PACS & OfficeHome & Terra & Avg \\
\hline \multirow{6}{*}{Light-weight Networks}
			& \multirow{2}{*}{ResNet-18~\citep{he2016deep}} & ERM &     &     &    &      &                                      \\
			& & + \Model & \reshl{80.9  0.1}{4.4}              & \reshl{81.7  0.1}{2.5}              & \reshl{58.4  0.1}{2.4}              & \reshl{47.9  0.7}{7.6}              & \reshl{67.2}{4.2}\\ \cline{2-8}
			
			& \multirow{2}{*}{MobilenetV3~\citep{howard2019searching}} & ERM
			&  &  &  &  &  \\    
			& & + \Model & \reshl{76.2  0.1}{10.7}              & \reshl{85.3  0.4}{6.2}              & \reshl{65.1  0.2}{4.3}             & \reshl{34.7  0.2}{4.3}             & \reshl{65.3}{6.4} \\ \cline{2-8}
			
			& \multirow{2}{*}{EfficientNetV2~\citep{tan2021efficientnetv2}} & ERM 
			&  &  &  &  &  \\
			& & + \Model       & \reshl{78.6  0.2}{8.7}              & \reshl{90.9  0.1}{1.7}              & \reshl{77.2  0.1}{3.6}              & \reshl{41.7  0.4}{5.7}             & \reshl{72.1}{4.9}                       \\		
\hline \multirow{6}{*}{Convolution Networks} & \multirow{2}{*}{ResNet-50~\citep{he2016deep}} & ERM
			&  &  &  &  & 67.6\\
			&  & + \Model & \reshl{81.6  0.1}{4.5}              & \reshl{89.0  0.3}{6.1}              & \reshl{68.9  0.1}{3.7}              & \reshl{52.9  0.2}{7.5}              & \reshl{73.1}{5.5}  \\
\cline{2-8} & \multirow{2}{*}{ResNet-101~\citep{he2016deep}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model & \reshl{80.5  0.2}{4.1}              & \reshl{88.3  0.1}{2.2}              & \reshl{70.3  0.2}{2.9}              & \reshl{50.0  0.5}{7.3}              & \reshl{72.3}{4.2} \\
\cline{2-8} & \multirow{2}{*}{ConvNeXt-B~\citep{liu2022convnet}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model
			& \reshl{85.8  0.3}{6.4}              & \reshl{95.3  0.2}{2.6}              & \reshl{88.5  0.0}{2.6}              & \reshl{65.3  0.3}{4.4}              & \reshl{83.7}{4.0}                         \\
			


\hline \multirow{10}{*}{Transformer Networks}  & \multirow{2}{*}{ViT-B16~\citep{dosovitskiy2020image}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model & \reshl{83.6  0.1}{5.0}              & \reshl{85.4  0.5}{5.1}              & \reshl{81.0  0.0}{5.4}              & \reshl{51.4  0.2}{8.0}              & \reshl{75.4}{5.9} \\
			
\cline{2-8} & \multirow{2}{*}{ViT-L16~\citep{dosovitskiy2020image}} & ERM 
			&  &  &  &  &  \\  
			&  & + \Model 
			& \reshl{83.2  0.2}{6.8}              & \reshl{95.2  0.1}{4.0}              & \reshl{87.5  0.2}{4.2}              & \reshl{53.9  0.4}{8.4}              & \reshl{79.9}{5.8}                        \\
			
\cline{2-8} & \multirow{2}{*}{Hybrid ViT~\citep{dosovitskiy2020image}} & ERM 
			&  &  &  &  &  \\  
			&  & + \Model 
			& \reshl{83.5  0.1}{4.4}              & \reshl{93.5  0.1}{4.4}              & \reshl{81.3  0.1}{1.7}              & \reshl{60.1  0.4}{7.2}              & \reshl{79.6}{4.2}      
			\\
\cline{2-8} & \multirow{2}{*}{DeiT~\citep{touvron2021training}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model  
			& \reshl{85.1  0.1}{5.6}              & \reshl{92.6  0.3}{4.7}              & \reshl{79.5  0.1}{2.5}              & \reshl{54.1  0.4}{4.8}              & \reshl{77.8}{4.4} \\
			
\cline{2-8} & \multirow{2}{*}{Swin Transformer~\citep{liu2021swin}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model 
			& \reshl{85.0  0.1}{5.0}              & \reshl{94.3  0.2}{4.1}              & \reshl{84.6  0.1}{3.0}              & \reshl{62.0  0.3}{5.0}              & \reshl{81.5}{4.3}                       \\
			\hline 
\multirow{4}{*}{Multi-Layer Perceptron}& \multirow{2}{*}{Mixer-B16~\citep{tolstikhin2021mlp}} & ERM 
			&  &  &  &  &  \\ 
			&  & +\Model 	   & \reshl{81.3  0.2}{7.7}              & \reshl{82.3  0.1}{6.5}              & \reshl{57.7  0.3}{5.2}              & \reshl{41.2  0.5}{14.4}             & \reshl{65.6}{8.4}                        \\
\cline{2-8} & \multirow{2}{*}{Mixer-L16~\citep{tolstikhin2021mlp}} & ERM 
			&  &  &  &  &  \\ 
			&  & + \Model 	   & \reshl{83.0  0.1}{4.9}              & \reshl{88.5  0.2}{3.5}              & \reshl{75.6  0.1}{5.3}              & \reshl{45.0  1.4}{8.4}              & \reshl{73.0}{5.6} \\			
			\hline
		\end{tabular}
	}
	\vspace{-2mm}
\end{table*}



\subsection{Ablation Study} ~\label{sec:exp:ablation_study}

\textbf{Effectiveness of Marginal Generalization.}
Table~\ref{tab:ablation} shows Marginal Generalization significantly improves the performance on target domains compared with the baseline model by +3.3\% (70.9\% vs. 67.6\%). With the classifier adaptation scheme (\S~\ref{sec:method:dyn}) but no Marginal Generalization, the performance reaches 70.8\%, bringing a +3.2\% improvement. While further integrating the two schemes, the ability of the network for domain generalization gets significantly boosted, increasing from 67.6\% to 71.9\%.

\textbf{ Effectiveness of Differentiable Memory Bank}
As shown in the 4th, 5th, and 7th rows of Table~\ref{tab:ablation}, Differentiable Memory Bank~(\S~\ref{sec:method:dyn}) also significantly improves the generalization ability of the model. Referring to the 4th row, the memory bank effectively boosts the performance of the base model from 67.6\% to 70.4\% (+2.8\%). 
\begin{wrapfigure}{r}{7.3cm}
	\begin{minipage}{0.52\textwidth}
		\begin{center}
        \vspace{-7mm}
        \begin{table}[H]
            \centering
            \caption{\textbf{Ablation Study}. We take the mean accuracy (mAcc) on the PACS, VLCS, OfficeHome, and TerraInc datasets as the evaluation metric.}
        \vspace{-3mm}
        \label{tab:ablation}
        \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{cccc|c|c}
            \hline \multicolumn{4}{c}{Ablation Components} & mAcc  &  \\
            \hline
            ~~(Eq.~\ref{eq:margin}) & ~~(Eq
            .~\ref{eq:cls}) 
            &  &  & (\%) & (\%)\\
            \hline 
            \ding{56} & \ding{56} & \ding{56} & \ding{56} 
            &  &  \\ 
            \hline
            \ding{51} & \ding{56} & \ding{56} & \ding{56}
            &  &  \\
            \ding{56} & \ding{51} & \ding{56} & \ding{56}
            &  &  \\
            \ding{56} & \ding{56} & \ding{51} & \ding{56}
            &  &  \\
            \ding{56} & \ding{56} & \ding{56} & \ding{51}
            &  &  \\
            \hline
            \ding{51} & \ding{51} & \ding{56} & \ding{56}
            &  &  \\
            \ding{56} & \ding{56} & \ding{51} & \ding{51}
            &  &  \\
            \hline
            \ding{51} & \ding{51} & \ding{51} & \ding{51}
            &  &   \\
            \hline
        \end{tabular}
        }
        \end{table}
		\end{center}
		\vspace{-15mm}
	\end{minipage}
\end{wrapfigure}
Meanwhile, when combining differentiable memory bank and Marginal Generalization, a further improvement of +5.5\% can be achieved. It reveals that the proposed schemes can be mutually beneficial, where the adapted model has refined gradients and a differentiable memory bank receives better prototypes. Thus they enhance the ability of networks to generalization together.


\subsection{Quantitative Analysis} ~\label{sec:exp:vis}
\begin{figure*}[t]
	\centering
	\caption{Accuracy accumulation curves on VLCS. \Model outperforms the base ERM model by about 5\% in accuracy. Note we randomly select \textbf{10 different trial seeds} for better comparison.}
	\vspace{-3mm}
	\label{fig:curve}
	{\includegraphics[width=0.24\linewidth]{figures/fig5-a.pdf}}
	{\includegraphics[width=0.24\linewidth]{figures/fig5-b.pdf}}
	{\includegraphics[width=0.24\linewidth]{figures/fig5-c.pdf}}
	{\includegraphics[width=0.24\linewidth]{figures/fig5-d.pdf}}
 \vspace{-3mm}
\end{figure*}
Figure~\ref{fig:curve} shows the accumulation curves of each instance interval across four domains on the VLCS~\citep{li2017deeper} dataset by 10 parallel trials. \Model brings significant and stable improvements on each domain, for which the fluctuation range of accumulation accuracy is close to the base model and mean scores are prominently improved.

\begin{table}[ht]
    \vspace{-3mm}
    \caption{Source knowledge preserve and training efficiency of UniDG.}
    \label{tab:source_preserve}
    \centering
    \vspace{-3mm}	
    \subfloat[Source Knowledge Preserve~\label{tab:source}]{\resizebox{0.45\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    VLCS & L & S & V \\ \midrule
    Source model       & 96.02 & 97.14 & 98.33 \\
    TENT  & 92.15 \textcolor{mygreen}{3.9}  & 94.23 \textcolor{mygreen}{2.9} & 95.13 \textcolor{mygreen}{3.2} \\
    UniDG        & 94.32 \textcolor{mygreen}{1.7} & 96.68 \textcolor{mygreen}{0.4} & 97.63 \textcolor{mygreen}{0.7} \\
    \bottomrule
\end{tabular}		}
    }
    \subfloat[Efficiency of UniDG~\label{tab:efficiency}]{
    \resizebox{0.40\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    Method            & Wall Clock Time (s)       \\ \midrule
    TENT (Res50)  &   0.581           \\
    UniDG (Res50) &   0.587  \\
    \bottomrule
    \end{tabular}}
} 
\vspace{-3mm}	
\end{table}
 As shown in Table~\ref{tab:source_preserve}, 1) referring to Table~\ref{tab:source}, we observe a smaller performance decrease of UniDG after adaption on the source domains. It proves that UniDG can better preserve pretrained source knowledge. 2) In Table~\ref{tab:efficiency}, we detail the training efficiency of UniDG and compare our method with TENT on wall clock time with the NVIDIA A100 GPU. It reveals that although we propose to update the parameters of the whole network, the computation burden will not sharply increase.
\subsection{Commonality Analysis} ~\label{sec:exp:backbone}
1) \textbf{Light-weight Networks}
\Model brings out significant average improvements of 5.1\% on light-weight MobileNet V3~\citep{howard2019searching}, EfficientNet V2~\citep{tan2021efficientnetv2}, and ResNet 18~\citep{he2016deep}. For example, the accuracy of MobileNet V3 has been improved by as much as 6.4\%, which proves the strong feasibility of \Model to improve the performance of edge devices for generalizing in unseen environments.
2) \textbf{Architecture-Free}
\Model is a unified solution based on online adaptation to handle domain shifts. As shown in Table~\ref{tab:backbone}, \Model has a general improvement of about 5\% on 10+ mainstream visual networks including CNN, Transformer, and MLP as their backbones. The highest improvement comes from Mixer-B16~\citep{tolstikhin2021mlp}, which increased from 57.2\% to 65.6\%.
\vspace{-3mm}

\section{Related Work} ~\label{sec:related}
\textbf{Domain Generalization} Domain Generalization (DG) can be classified into three types:
1) \textbf{Representation Learning}: These methods extract specific features from source domains and assume them robust in target domains. One approach is domain alignment~\citep{li2018deep,li2018domain,dg_mmld}, extracting domain-invariant representations from source domains, which is a non-trivial task. Therefore feature disentanglement~\citep{rojas2018invariant,piratla2020efficient,christiansen2021causal,mahajan2021domain,sun2021recovering,liu2021learning}, loosens the constraint, learning disentangled representations.
2) \textbf{Foundation Models}: different backbones reveal the diverse ability to tackle the DG problem. These methods~\citep{li2017deeper,ding2017deep,carlucci2019domain,li2022sparse}  optimize the architecture of the mainstream backbone for DG. GMoE~\citep{li2022sparse} based on ViT~\citep{dosovitskiy2020image}, replaces the FFN layers with mixture-of-experts, allowing different experts to focus on the different visual attributes.
3) \textbf{Learning Strategy}: These methods utilize machine learning strategy to enhance the model’s generalization capability on target domains, including meta-learning and ensemble learning. Meta-learning~\citep{li2018learning,li2019feature,li2019episodic,dou2019domain,liu2020shape,chen2022ost,li2021metasaug} divide training data into meta-train and meta-test sets, then simulate domain shift and update parameters during training. Ensemble learning~\citep{ding2017deep,zhou2021domain,cha2021swad} learns model copies to extract features and migrate their ensemble to target domains.

\textbf{Continual Learning} Continual learning~\citep{de2021continual} aims to relieve continuous domain shifts, which face complicated catastrophic forgetting. Existing methods~\citep{rebuffi2017icarl,zenke2017continual,kirkpatrick2017overcoming,li2017learning,lao2020continuous} propose regularization and replay to reinforce learning representations space from parameters and data stream perspectives.
Recently,self-supervised learning~\citep{radford2015unsupervised,he2022masked,grill2020bootstrap} utilize prior knowledge obtained by pre-training with massive datasets and have shown strong performance in DG.~\cite{radford2021learning} trains image encoder and text encoder jointly, matching 400 million (image, text)pairs. Besides, researchers have noted the superiority of causal learning~\citep{zhou2021domain,mahajan2021domain} in domain generalization.

\textbf{Test-time Adaptation} TTA schemes~\citep{karani2021test,iwasawa2021test,sun2020test,park2023test} propose to update model parameters based on target data. 

1) \textbf{Adversarial Learning}: With the advancement of generative adversarial networks,~\cite{li2020model,yeh2021sofa,kurmi2021domain} generate target data with generative models, improving the ability to handle domain shift without the support of source data.2) \textbf{normalization-based}: The normalization method replaces the batch normalization (BN) statistics of the trained model with the BN statistics estimated on test data and updates parameters of the BN layers only, with the backbone network frozen.~\cite{wang2020tent} aims to minimize entropy during testing.~\cite{schneider2020improving} uses Wasserstein distance between source and target statistics as the measurement.
3) \textbf{Bayesian Learning}: \cite{zhou2021training} analyses TTA from a Bayesian perspective~\citep{li2016revisiting,hu2021mixnorm,you2021test} and proposes a regularized entropy minimization procedure achieved by approximating the probability density during the training time.



\section{Discussion and Conclusion } ~\label{sec:conclusion}
Aiming at the OOD problem, this paper proposes a general self-supervised online learning scheme, named UniDG, to update all the parameters of the model during the testing phase. Specifically, \Model contains Marginal Generalization and Differentiable Memory Bank, which can successfully balance the conservation of source knowledge and generalization ability to novel environments. Our method shows high effectiveness and potential for complex domain shifts in actual scenarios. On four domain generalization benchmarks, \Model achieved a new state-of-the-art performance with an average accuracy of 79.6\%. Additionally, \Model improved 12 backbone models by an average of 5.4\%. By comparing with existing pre-trained model and other test-time methods, we show it is a promising direction to develop the online adaptation method to deal with the OOD problem.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\clearpage
\appendix
{\Large\textbf{Appendix}}
\section{Summary}
This appendix describes more details of the ICLR 2024 submission, titled \textit{Towards Unified and Efficient Domain Generalization}. The appendix is organized as follows:
\begin{itemize}
	\item \S~\ref{sec:theory} theoretically discusses why \Model can perform better under the setting of test-time adaptation than the BN-based approaches.
	\item \S~\ref{sec:imple} provides more implementation details and analytical experiment for the hyper-parameter  of Equation~\ref{eq:margin}.
	\item \S~\ref{sec:pseu} summarizes the pseudo-code of our proposed \Model.
	\item \S~\ref{sec:vis} illustrates the effectiveness of the proposed \Model by quantities of T-SNE visualization results compared with existing advanced methods including TENT~\cite{wang2020tent} and TAST~\cite{jang2022test}.
	\item \S~\ref{sec:supp:exp} exhibits a more detailed comparison between the other test-time schemes and provides improvements of each visual backbone on all domains.
	
\end{itemize}

\section{Theoretical Insight}~\label{sec:theory}
To mitigate catastrophic forgetting during test-time adaptation, existing methods such as TENT~\cite{wang2020tent}, SHOT~\cite{liang2020we}, and TAST~\cite{jang2022test} propose adapting the parameters of Batch Normalization (\texttt{BN}) layers. We argue that adapting only the \texttt{BN} layers may be insufficient for effectively handling the unseen novel domains compared to adapting the entire network parameters. To substantiate our claim, we provide a theoretical analysis from two perspectives:

1) Neural Tangent Kernels (NTK)~\cite{jacot2018neural}: NTK is a kernel that elucidates the evolution of neural networks during training via gradient descent. It bridges the gap between neural networks and classical kernel methods in machine learning. When the width of the hidden layers in a neural network approaches infinity, the network's training behavior becomes more predictable. In this paper, we employ neural tangent kernels to assess the network's ability to generalize to unseen domains  while training on the source domain .

2) Gradient descent process of \texttt{BN} layers restricts the expansion of neural tangent kernels: We argue that solely adapting the \texttt{BN} layers could limit the growth of neural tangent kernels, affecting the model's generalization capability for unseen domains.

In summary, our theoretical analysis highlights the potential limitations of adapting only the \texttt{BN} layers in handling novel domains and suggests that a more comprehensive approach might be necessary to achieve better generalization.\par
\subsection{Neural Tangent Kernel in DG}~\label{sec:theory:nek+dg}
For domain generalization, the networks can be formulated from two parts: feature extractor  and classifier . And Neural Tangent Kernel  formulates the impact of gradients between different instances  to the learning representations of the neural network in the gradient decent progress:

where  is the learning rate and  denotes the parameters.
Accordingly, we can obtain the relationship between parameters and learning representations with learning rate  and neural tangent kernel  on the source and target domains:

Meanwhile, we can also obtain the formula of neural tangent kernels according to its definition~\cite{jacot2018neural}:

where  indicates the mathematical expectation of network parameter in the parameters space , and  and  denotes the gradients of the parameters with representation on the source and target domains, respectively. And the neural tangent kernels in domain generalization can be regarded as the inner product of the gradients for learning representations on the source and target domains.
\subsection{Backward Gradients in BN Layers}~\label{sec:theory:bn}
Batch Normalization operation is focused on introducing means  and standard error  to normalize learning representations in a mini-batch  containing  samples. And \texttt{BN} layer introduces linear projection with two learnable parameters  and  based on the Batch Normalization operation. Its computation can be formulated as:

where  is the output of \texttt{BN} layer, which can be simplified as: ,  is a value to smooth the computation. And we can obtain the formula to describe it with involved variables including :
 
According to Equation~\ref{eq:y}, we can utilize chain rules to calculate the gradients between representations and input with intermediate variables:

Furthermore, we can derive the gradient of learning representation  in \texttt{BN} layers according to Equation~\ref{eq:bn} for learnable parameters :
 
And we can also obtain the gradients for representations and statistic variables in the mini-batch by chain rules:
 
and we have obtained  and  in Equation~\ref{eq:grad:mu+sigma}. Then, for the unknown gradients of , we can utilize the results in Equation~\ref{eq:grad:mu+sigma} to simplify:
 
In addition, the gradients of  is directly calculated with chain rules as: Referring to Equation~\ref{eq:bn}, these gradients can be simply obtained , , and . Therefore, we can compute the gradient  as:

According to Equation~\ref{eq:grad:mu+sigma}, \ref{eq:grad:f+mu+sigma}, \ref{eq:grad:f+mu}, we compute the gradient as:

Therefore, refer to Equation~\ref{eq:grad:f+mu+sigma}, we can obtain the gradients  related to , which can be directly calculated with known gradients including . Finally, the backward gradients in the BN layer can be computed as:

According to results in Equation~\ref{eq:ntk} and \ref{eq:grad+bn}, we can describe the adaptation process for these approaches adapting \texttt{BN} layers:

\subsection{Conclusion}
Referring to Equation~\ref{eq:ntk+bn}, it illustrates that \textbf{neural tangent kernels for networks pretrained on source has less width to get transferred on target domains if the algorithm is focused on adapting parameters of the \texttt{BN} layers.} Therefore, we propose to adapt the comprehensive parameter space to enhance the ability to generalize on the novel environments.

\section{Implementation Details}~\label{sec:imple}

We split the dataset into training and validation sets, where 80\% of the samples are randomly selected as the training data and the remaining 20\% is the validation data. The validation set is exploited to select the hyper-parameter with the criterion of maximizing the accuracy of the validation set. 

In the test-time adaptation setting, the adaptation starts with a trained source model, which will be trained using source data in advance. To build a test-time adaptation method for domain generalization (DG), we first acquired a source model with data from multiple source domains. In addition, two pre-training algorithms are exploited for obtaining the source models, i.e., ERM and CORAL. The experiment results are the averages of three trials with different random seeds. We train the source model using an Adam optimizer with a learning rate of  and a batch size of 32. The learning rate for adaptation phase is varying .

\begin{figure}[t]
	\centering
	\includegraphics[width = 0.83\linewidth]{figures/fig6.pdf}
	\caption{Hyper-parameter impact of .}
	\label{fig:hyer}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width = 0.96\linewidth]{figures/fig7.pdf}
	\caption{Qualitative Results of \Model on challenging TerraIncognita~\cite{beery2018recognition} dataset. \Model (4th column) aggregates intra-class feature embeddings better with the pretrained ERM model (1st column) compared with advanced methods including TENT~\cite{wang2020tent} (2nd column), and TAST~\cite{jang2022test} (3rd column).}
	\label{fig:vis-tsne}
\end{figure*}
In Figure~\ref{fig:hyer}, we visualize the sensitivity of hyper-parameter  in Equation~\ref{eq:margin} on the PACS~\cite{torralba2011unbiased} dataset. We take hyper-parameter  It is also worth noting that when  is set in , the network achieves better performances on the PACS dataset. Therefore, we choose to set .
\section{Pseudo code for \Model} ~\label{sec:pseu}
For feature extractor , we detach the gradient and freeze the parameters of the trained source model, and utilize it to extract the representation of images from target domains as the source knowledge. And we initialize another new network  with the trained parameters of  and formulate the representation discrepancy between  and  via Frobenius norm :

For representations , we utilize a linear layer to work as a classifier and obtain the classification results . Then we take the  entropy as the loss function to update the classifier:

The memory bank is set to store the class-wise prototypes of each class , where , , and  denote the memory bank, the number of classes, and feature dimensions. For each class , the prototype  is initialized with parameters of classifier :

Besides utilizing the Top- re-ranking approach to select the prototypes, to further exploit the potential of the test-time scheme, we make the memory iteration learnable, which directly optimizes these prototypes via gradient descent. We construct the learnable memory terms by introducing matrix products of learning representations , prototypes , and pseudo labels :

Therefore, our algorithm can be summarized as follows:
\begin{algorithm}[ht]
	\caption{\Model Algorithm}
	\begin{algorithmic}[1]
		\REQUIRE number of steps ; feature extractor  and classifier  pretrained from source domains; the unlabeled target domain.
		\STATE 
		\STATE Copy a new model with the source one:  and freeze .
\FOR{}
		\STATE 
		\STATE Arrived target data 
		\STATE Update Prototypes  in ~Eq.~\ref{eq:prototype} using 
		\STATE Calculate the loss  using Eq.~\ref{eq:margin}
		\STATE Calculate the loss  according to Eq.~\ref{eq:ent}.
		\STATE Calculate the overall loss: 
		\STATE Update the network models  using  in step 10.
		\ENDFOR
	\end{algorithmic}
	\label{alg}
\end{algorithm}




\section{Visualization Results}~\label{sec:vis}
In Figure~\ref{fig:vis-tsne}, we visualize feature embeddings of ERM~\cite{vapnik1998statistical}, TENT~\cite{wang2020tent}, TAST~\cite{jang2022test}, and proposed \Model. Referring to the figure, we find that, feature embeddings extracted by \Model have better intra-class compactness and inter-class separability.
\section{Detailed Results}~\label{sec:supp:exp}
In this section, we provide more detailed experimental results. These results are organized into three parts: 
\begin{itemize}
        \item We provide experiments of transferring BN-based methods to adapt LN layers methods on the VLCS dataset in Table~\ref{tab:supp-ln}.
	\item Table~\ref{tab:supp-1}-Table~\ref{tab:supp-4} demonstrate the comparison between existing test-time methods with ResNet-18~\cite{he2016deep} backbones and \Model on the VLCS~\cite{li2017deeper}, PACS~\cite{torralba2011unbiased}, Office Home~\cite{venkateswara2017deep}, and TerraIncognita~\cite{beery2018recognition} datasets.
	\item  Table~\ref{tab:supp-5}-Table~\ref{tab:supp-8} show the comparison between \Model and the other test-time methods with the ResNet-50~\cite{he2016deep} visual backbones.
	\item  Table~\ref{tab:supp-9}-Table~\ref{tab:supp-12} detail improvements on each domain between base ERM~\cite{vapnik1998statistical} models with different 12 visual backbones on the aforementioned 4 datasets.
\end{itemize}

\begin{table}[ht]
    \centering
    \caption{Comparsion between transferring existing BN-based method and our proposed UniDG with ViT-B16 backbone on the VLCS dataset.}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        Method & C & L & S & V & Avg  \\ \midrule
         Source & 98.50 & 63.44 & 74.79 & 77.79  & 78.63\\ 
         TENT~\citep{wang2020tent} & 99.23 & 64.36 & 75.23 & 77.94 & 79.19 \\ \hline
         UniDG~(LN) & 99.73 & 66.65 & 78.44 & 79.79 & \reshl{81.15}{2.52} \\ 
         UniDG & 99.91  & 69.60 &   82.86 &     82.82 & \reshl{83.80}{4.61} \\ \hline
    \end{tabular}
    }
    \label{tab:supp-ln}
\end{table}
\begin{table*}[htb]
	\centering \caption{Full results using classifiers trained by ERM for Table 2 on VLCS. We use ResNet-18 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc}
			\toprule
			Method & C & L & S & V & Avg  \\ \midrule
			ERM &  94.701.33 & 63.791.30 & 67.901.97 & 73.151.37 & 74.88 \\ 
			+Tent & 89.822.89 & 61.981.10 & 65.511.91 & 74.211.61 & 72.88  \\ 
			+TentBN & 79.804.74 & 58.511.44 & 61.620.92 & 68.141.74 & 67.02  \\ 
			+TentClf & 94.751.43 & 63.741.41 & 67.922.22 & 65.406.91 & 72.96  \\ 
			+SHOT & 91.456.83 & 48.261.77 & 54.752.59 & 66.511.25 & 65.24  \\
			+SHOTIM & 90.287.00 & 47.961.45 & 54.662.47 & 66.521.19 & 64.86  \\ 
			+PL & 93.572.24 & 53.822.51 & 50.589.50 & 53.912.78 & 62.97  \\ 
			+PLClf & 94.671.38 & 63.641.31 & 67.902.21 & 73.341.00 & 74.89  \\ 
			+T3A & 97.521.99 & 65.322.24 & 70.703.48 & 75.511.75 & 77.26  \\ 
			+TAST  & 99.170.60 & 65.871.90 & 68.131.76 & 75.921.75 & 77.27  \\ 
			+TAST-BN & 92.608.66 & 64.751.29 & 67.273.14 & 76.233.73 & 75.21  \\ \hline
			Base                  & 95.76  0.00     & 66.31  0.00     & 70.07  0.00     & 74.64  0.00     & 76.7                 \\
			+\Model                        & \reshl{99.79  0.09}{4.03}      & \reshl{68.78  0.24}{2.47}      & \reshl{75.01  0.30}{4.94}      & \reshl{79.93  0.24}{5.29}      & \reshl{80.88  0.14}{4.18}  \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:supp-1}
\end{table*}


\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on PACS. We use ResNet-18 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & A & C & P & S & Avg  \\ \midrule
			ERM & 77.780.81 & 75.091.22 & 95.190.29 & 69.111.22 & 79.29  \\ 
			+Tent &  82.211.07 & 81.200.51 & 95.320.33 & 76.821.97 & 83.89 \\ 
			+TentBN & 78.890.67 & 77.450.82 & 95.770.40 & 70.892.75 & 80.75 \\ 
			+TentClf & 78.161.05 & 75.011.53 & 95.500.35 & 65.605.96 & 78.57  \\ 
			+SHOT & 81.090.86 & 79.680.91 & 96.180.27 & 72.482.04 & 82.36 \\
			+SHOTIM &  81.100.90 & 79.660.95 & 96.180.27 & 72.352.03 & 82.33   \\ 
			+PL & 76.424.89 & 61.055.48 & 95.700.56 & 50.758.79 & 70.98  \\ 
			+PLClf & 79.091.41 & 75.462.93 & 95.430.32 & 62.487.31 & 78.11  \\ 
			+T3A &   78.810.97 & 77.141.20 & 95.920.36 & 71.441.63 & 80.83 \\ 
			+TAST  &  80.560.53 & 78.260.99 & 96.440.20 & 72.520.77 & 81.94\\ 
			+TAST-BN & 86.490.20 & 83.702.57 & 97.230.11 & 80.851.42 & 87.07  \\ \hline
			Base                  & 76.51  0.00     & 73.72  0.00     & 92.37  0.00     & 76.08  0.00     & 79.7                 \\
			+\Model                      & \reshl{80.19  0.13}{3.68}      & \reshl{76.47  0.37}{2.75}      & \reshl{95.41  0.15}{3.04}      & \reshl{74.86  0.38}{-1.22}     & \reshl{81.73  0.06}{2.03}     
			\\\bottomrule
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on OfficeHome. We use ResNet-18 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & A & C & P & R & Avg  \\ \midrule
			ERM & 55.190.49 & 47.761.02 & 72.220.53 & 73.210.89 & 62.10  \\ 
			+Tent & 53.390.61 & 48.280.88 & 70.500.68 & 71.290.72 & 60.86 \\ 
			+TentBN & 55.530.43 & 49.530.95 & 72.470.27 & 73.011.23 & 62.64\\ 
			+TentClf & 55.170.67 & 36.731.94 & 72.210.52 & 73.220.97 & 59.33 \\ 
			+SHOT & 55.140.57 & 50.271.18 & 71.690.45 & 73.210.91 & 62.58 \\
			+SHOTIM &  55.080.56 & 50.291.17 & 71.710.40 & 73.210.90 & 62.57  \\ 
			+PL & 54.491.06 & 34.6613.13 & 71.450.37 & 72.200.65 & 58.20 \\ 
			+PLClf & 55.140.70 & 47.701.25 & 72.210.54 & 72.620.96 & 61.92  \\ 
			+T3A &  55.100.74 & 49.561.14 & 74.100.55 & 74.071.18 & 63.21 \\ \midrule
			+TAST  & 56.150.68 & 50.041.31 & 74.330.28 & 74.281.23 & 63.70  \\ 
			+TAST-BN & 55.110.58 & 51.350.85 & 72.580.80 & 72.130.78 & 62.79  \\ 
			\hline
			Base                  & 46.91  0.00     & 45.42  0.00     & 65.06  0.00     & 66.49  0.00     & 56.0                 \\
			+\Model                      & \reshl{49.02  0.11}{2.11}      & \reshl{47.33  0.24}{1.91}      & \reshl{69.14  0.15}{4.08}      & \reshl{68.17  0.22}{1.68}      & \reshl{58.42  0.06}{2.42}  
			\\ \bottomrule
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on TerraIncognita. We use ResNet-18 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & L100 & L38 & L43 & L46 & Avg  \\ \midrule
			ERM & 37.182.46 & 36.124.20 & 53.181.27 & 36.021.37 & 40.62   \\ 
			+Tent & 38.290.48 & 25.823.91 & 41.531.59 & 29.151.83 & 33.70 \\ 
			+TentBN & 40.551.46 & 37.442.22 & 46.331.32 & 35.301.26 & 39.91 \\ 
			+TentClf & 34.4413.31 & 34.195.76 & 52.712.03 & 31.862.26 & 38.30 \\ 
			+SHOT & 33.870.66 & 28.582.10 & 40.992.07 & 30.831.26 & 33.57 \\
			+SHOTIM &  33.831.29 & 28.132.30 & 40.812.18 & 30.641.46 & 33.35 \\ 
			+PL & 51.921.19 & 35.6120.74 & 39.9710.98 & 22.268.21 & 37.44\\ 
			+PLClf & 45.222.45 & 36.035.81 & 52.761.54 & 33.102.27 & 41.78 \\ 
			+T3A & 36.221.89 & 40.081.98 & 50.721.02 & 33.791.25 & 40.20   \\ 
			+TAST  & 43.672.83 & 39.243.79 & 52.643.02 & 35.011.09 & 42.64 \\ 
			+TAST-BN & 51.067.31 & 32.747.54 & 41.702.86 & 32.213.05 & 39.43 \\ \midrule
			Base                  & 45.03  0.00     & 32.67  0.00     & 47.54  0.00     & 35.80  0.00     & 40.3                 \\
			+\Model                      & \reshl{54.34  0.21}{9.31}      & \reshl{51.48  2.16}{18.81}     & \reshl{48.27  0.46}{0.73}      & \reshl{37.64  0.21}{1.84}      & \reshl{47.93  0.65}{7.63} 
			\\ \bottomrule
		\end{tabular}
	}
	\label{tab:supp-4}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on VLCS. We use ResNet-50 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc}
			\toprule
			Method & C & L & S & V & Avg  \\ \midrule
			ERM & 97.660.64 & 63.871.71 & 71.211.52 & 74.092.06  & 76.71  \\ 
			+Tent & 92.362.44 & 58.463.29 & 67.842.03 & 73.192.68  & 72.96  \\ 
			+TentBN & 85.363.49 & 58.353.46 & 66.472.71 & 68.422.11  & 69.65 \\ 
			+TentClf & 97.610.58 & 63.672.10 & 68.771.27 & 73.161.31  & 75.80 \\ 
			+SHOT & 98.721.50 & 46.822.57 & 55.701.78 & 67.042.88  & 67.07 \\
			+SHOTIM & 98.651.46 & 46.542.32 & 55.812.32 & 66.732.82  & 66.93  \\ 
			+PL & 98.480.34 & 53.452.82 & 59.459.24 & 66.248.63  & 69.41  \\ 
			+PLClf &  97.630.64 & 63.362.10 & 69.740.78 & 71.864.53  & 75.65 \\ 
			+T3A & 99.170.38 & 64.781.61 & 73.013.24 & 72.202.84  & 77.29   \\ 
			+TAST  & 99.350.30 & 65.641.78 & 73.633.58 & 72.012.68  & 77.66  \\ 
			+TAST-BN & 96.092.40 & 60.226.08 & 65.786.51 & 71.995.90  & 73.52  \\ \midrule
			Base                  & 97.70  0.00     & 63.20  0.00     & 70.18  0.00     & 78.82  0.00     & 77.50                 \\
			+\Model                      & \reshl{99.71  0.05}{2.01}      & \reshl{71.11  0.02}{7.91}      & \reshl{73.60  0.38}{3.42}      & \reshl{81.99  0.06}{3.17}      & \reshl{81.60  0.08}{4.10}
			\\ 
			\bottomrule
		\end{tabular}
	}
	\label{tab:supp-5}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on PACS. We use ResNet-50 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method &A&C&P&S& Avg  \\ \midrule
			ERM & 82.921.65 & 78.053.36 & 96.500.32 & 75.383.31 & 83.21  \\ 
			+Tent & 82.541.32 & 84.901.35 & 95.450.93 & 77.741.36 & 85.16   \\ 
			+TentBN & 82.752.01 & 79.502.26 & 96.780.20 & 75.733.22 & 83.69 \\ 
			+TentClf & 83.001.87 & 77.864.20 & 96.550.36 & 73.256.14 & 82.66   \\ 
			+SHOT & 84.671.70 & 80.171.39 & 96.580.52 & 74.862.95 & 84.07 \\
			+SHOTIM &  84.621.79 & 80.241.41 & 96.540.46 & 75.162.88 & 84.14 \\ 
			+PL & 84.595.51 & 76.352.57 & 96.410.68 & 69.5411.22 & 81.72  \\ 
			+PLClf & 83.882.00 & 78.933.68 & 96.530.40 & 73.966.08 & 83.33  \\ 
			+T3A & 83.562.03 & 79.753.14 & 96.990.24 & 75.363.57 & 83.92   \\ 
			+TAST  & 83.852.05 & 79.153.03 & 96.930.27 & 76.493.13 & 84.11 \\ 
			+TAST-BN &  87.112.04 & 88.501.93 & 97.790.47 & 83.231.42 & 89.16  \\ \midrule
			Base                  & 86.52  0.00     & 71.75  0.00     & 97.16  0.00     & 75.57  0.00     & 82.7                 \\
			+\Model                      & \reshl{91.26  0.11}{4.74}      & \reshl{84.86  0.63}{13.11}     & \reshl{98.13  0.05}{0.97}      & \reshl{81.77  0.56}{6.2}       & \reshl{89.00  0.30}{6.30} 
			\\ \bottomrule
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on OfficeHome. We use ResNet-50 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & A&C&P&R & Avg  \\ \midrule
			ERM & 61.320.69 & 53.441.11 & 75.841.10 & 77.900.92 & 67.13  \\ 
			+Tent & 60.980.67 & 53.941.24 & 74.490.71 & 75.750.53 & 66.29  \\ 
			+TentBN & 62.630.45 & 54.901.17 & 76.201.09 & 77.921.01 & 67.91 \\ 
			+TentClf & 61.350.73 & 52.721.40 & 75.231.05 & 77.861.07 & 66.79 \\ 
			+SHOT & 61.910.33 & 55.580.91 & 75.491.54 & 77.600.80 & 67.65  \\
			+SHOTIM & 61.840.32 & 55.630.92 & 75.561.60 & 77.570.79 & 67.65  \\ 
			+PL &  59.421.55 & 42.4012.31 & 73.802.26 & 75.771.50 & 62.85   \\ 
			+PLClf & 61.350.40 & 52.871.96 & 75.861.09 & 77.941.10 & 67.01   \\ 
			+T3A &  61.910.59 & 55.071.14 & 77.391.38 & 78.670.61 & 68.26   \\ 
			+TAST  & 62.430.80 & 55.811.26 & 77.461.07 & 78.830.93 & 68.63  \\ 
			+TAST-BN &  63.220.85 & 58.200.98 & 77.141.10 & 76.940.39 & 68.88 \\ \midrule
			Base                  & 57.67  0.00     & 53.69  0.00     & 73.90  0.00     & 75.70  0.00     & 65.2                 \\
			+\Model                      & \reshl{62.84  0.31}{5.17}      & \reshl{55.86  0.12}{2.17}      & \reshl{78.16  0.05}{4.26}      & \reshl{78.69  0.02}{2.99}      & \reshl{68.89  0.07}{3.69}
			\\ \bottomrule 
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM for Table 2 on TerraIncognita. We use ResNet-50 as a backbone network.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & L100&L38&L43&L46 & Avg  \\ \midrule
			ERM & 46.841.96 & 43.242.51 & 53.321.92 & 40.301.93 & 45.93  \\ 
			+Tent & 41.202.71 & 29.723.59 & 41.352.92 & 36.032.85 & 37.08  \\ 
			+TentBN & 46.641.17 & 41.113.16 & 49.311.05 & 38.522.04 & 43.89 \\ 
			+TentClf & 49.873.80 & 43.313.19 & 53.012.31 & 28.406.19 & 43.64 \\ 
			+SHOT & 36.172.70 & 29.802.92 & 41.000.30& 33.831.86 & 35.20  \\
			+SHOTIM & 35.562.76 & 27.494.01 & 40.770.45 & 33.671.84 & 34.37   \\ 
			+PL & 56.755.78 & 46.121.03 & 29.4410.14 & 20.064.65 & 38.09 \\ 
			+PLClf &  52.283.95 & 43.762.96 & 52.782.15 & 37.812.49 & 46.66   \\ 
			+T3A & 45.131.26 & 44.672.56 & 52.520.78 & 40.132.31 & 45.61    \\ 
			+TAST  & 53.013.95 & 43.273.21 & 53.792.72 & 39.663.65 & 47.43 \\ 
			+TAST-BN & 55.752.37 & 33.929.86 & 43.874.70 & 32.334.40 & 41.47   \\ \midrule
			Base                  & 49.70  0.00     & 40.51  0.00     & 56.45  0.00     & 33.63  0.00     & 45.1                 \\
			+\Model                      & \reshl{64.37  0.11}{14.67}     & \reshl{46.87  0.32}{6.36}      & \reshl{57.37  0.22}{0.92}      & \reshl{42.82  0.57}{9.19}      & \reshl{52.86  0.17}{7.76} 
			\\ \bottomrule
		\end{tabular}
	}
	\label{tab:supp-8}
\end{table*}
\begin{table*}[htb]
	\centering \caption{Full results using classifiers trained by ERM with different visual backbones for Table 3 on the VLCS dataset. }
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc}
			\toprule
			Method & C & L & S & V & Avg  \\ \midrule
			ResNet-101~\cite{he2016deep}                  & 97.00  0.00     & 65.22  0.00     & 70.64  0.00     & 73.60  0.00     & 76.6                 \\
			+\Model                       & \reshl{99.35  0.05}{2.35}      & \reshl{69.71  0.30}{4.49}      & \reshl{75.01  0.13}{4.37}      & \reshl{78.05  0.30}{4.45}      & \reshl{80.53  0.16}{3.93}         \\ \midrule			 
			ViT-B16~\cite{dosovitskiy2020image}                  & 98.50  0.00     & 63.44  0.00     & 74.79  0.00     & 77.67  0.00     & 78.6                 \\
			+\Model                      & \reshl{99.94  0.02}{1.44}      & \reshl{70.07  0.17}{6.63}      & \reshl{82.20  0.63}{7.41}      & \reshl{82.07  0.10}{4.4}       & \reshl{83.57  0.13}{4.97} \\
			\midrule
			DeiT~\cite{touvron2021training}                   & 97.08  0.00     & 66.96  0.00     & 74.64  0.00     & 79.27  0.00     & 79.5                 \\
			+\Model                      & \reshl{99.94  0.02}{2.86}      & \reshl{69.49  0.19}{2.53}      & \reshl{83.69  0.13}{9.05}      & \reshl{87.39  0.28}{8.12}      & \reshl{85.13  0.05}{5.63}      \\
			\midrule
			HViT~\cite{dosovitskiy2020image}                  & 96.73  0.00     & 64.38  0.00     & 75.40  0.00     & 79.90  0.00     & 79.1                 \\
			+\Model                      & \reshl{99.53  0.09}{2.8}       & \reshl{69.49  0.05}{5.11}      & \reshl{79.27  0.16}{3.87}      & \reshl{85.82  0.22}{5.92}      & \reshl{83.53  0.12}{4.43}      \\
			\midrule
			Swin Transformer~\cite{liu2021swin}                   & 94.79  0.00     & 65.27  0.00     & 79.82  0.00     & 80.04  0.00     & 80.0                 \\
			+\Model                      & \reshl{100.00  0.00}{5.21}     & \reshl{69.51  0.35}{4.24}      & \reshl{83.79  0.32}{3.97}      & \reshl{86.81  0.15}{6.77}      & \reshl{85.03  0.10}{5.03}      \\
			\midrule
			MobileNet V3~\cite{howard2019searching}                  & 72.8  0.0       & 57.6  0.0       & 58.3  0.0  & 74.4   0.0         & 65.8                 \\
			+\Model                      & \reshl{93.8  0.3}{21.0}        & \reshl{63.2  0.1}{5.6}         & \reshl{69.1  0.2}{10.8}        & \reshl{78.8  0.1}{4.4}         & \reshl{76.2  0.1}{10.4}         \\
			\midrule
			ConvNeXt~\cite{liu2022convnet}                  & 97.97  0.00     & 68.09  0.00     & 78.37  0.00     & 73.64  0.00     & 79.5                 \\
			+\Model                      & \reshl{100.00  0.00}{2.03}     & \reshl{72.97  0.69}{4.88}      & \reshl{84.77  0.09}{6.4}       & \reshl{85.50  0.47}{11.86}     & \reshl{85.81  0.25}{6.31}      \\
			\midrule
			ViT-L16~\cite{dosovitskiy2020image}                  & 97.88  0.00     & 61.74  0.00     & 71.90  0.00     & 73.94  0.00     & 76.4                 \\
			+\Model                      & \reshl{100.00  0.00}{2.12}     & \reshl{69.05  0.38}{7.31}      & \reshl{77.89  0.55}{5.99}      & \reshl{85.71  0.44}{11.77}     & \reshl{83.16  0.23}{6.76}      \\
			\midrule
			Mixer-B16~\cite{tolstikhin2021mlp}                  & 93.46  0.00     & 58.87  0.00     & 70.26  0.00     & 73.23  0.00     & 74.0                 \\
			+\Model                      & \reshl{99.20  0.08}{5.74}      & \reshl{67.69  0.47}{8.82}      & \reshl{79.14  0.74}{8.88}      & \reshl{79.13  0.18}{5.9}       & \reshl{81.29  0.23}{7.29}      \\
			\midrule
			Mixer-L16~\cite{tolstikhin2021mlp}                  & 97.61  0.00     & 61.65  0.00     & 75.55  0.00     & 77.42  0.00     & 78.1                 \\
			+\Model                      & \reshl{99.91  0.04}{2.3}       & \reshl{65.80  0.28}{4.15}      & \reshl{82.53  0.06}{6.98}      & \reshl{83.94  0.05}{6.52}      & \reshl{83.05  0.09}{4.95}      \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:supp-9}
\end{table*}


\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM with different visual backbones for Table 3 on PACS dataset.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & A & C & P & S & Avg  \\ \midrule
			ResNet-101~\cite{he2016deep}                  & 83.83  0.00     & 82.30  0.00     & 97.98  0.00     & 79.29  0.00     & 85.9                 \\
			+\Model                        & \reshl{87.29  0.14}{3.46}      & \reshl{85.61  0.09}{3.31}      & \reshl{98.58  0.00}{0.6}       & \reshl{81.91  0.43}{2.62}      & \reshl{88.35  0.06}{2.45}      \\
			\midrule	
			ViT-B16~\cite{dosovitskiy2020image}                  & 85.54  0.00     & 82.62  0.00     & 98.88  0.00     & 54.20  0.00     & 80.3                 \\
			+\Model                      & \reshl{89.85  0.20}{4.31}      & \reshl{87.28  0.15}{4.66}      & \reshl{99.78  0.04}{0.9}       & \reshl{64.84  2.01}{10.64}     & \reshl{85.44  0.54}{5.14}      \\
			\midrule	
			DeiT~\cite{touvron2021training}                   & 90.73  0.00     & 83.74  0.00     & 99.03  0.00     & 77.99  0.00     & 87.9                 \\
			+\Model                      & \reshl{94.55  0.18}{3.82}      & \reshl{90.60  0.51}{6.86}      & \reshl{99.70  0.00}{0.67}      & \reshl{85.41  0.55}{7.42}      & \reshl{92.57  0.31}{4.67}      \\
			\midrule
			HViT~\cite{dosovitskiy2020image}                  & 93.53  0.00     & 81.66  0.00     & 98.65  0.00     & 85.56  0.00     & 89.9                 \\
			+\Model                      & \reshl{96.24  0.12}{2.71}      & \reshl{88.84  0.33}{7.18}      & \reshl{99.63  0.04}{0.98}      & \reshl{89.26  0.08}{3.7}       & \reshl{93.49  0.10}{3.59}      \\
			\midrule
			Swin Transformer~\cite{liu2021swin}                   & 93.11  0.00     & 85.82  0.00     & 99.48  0.00     & 83.97  0.00     & 90.6                 \\
			+\Model                      & \reshl{97.38  0.03}{4.27}      & \reshl{90.65  0.49}{4.83}      & \reshl{99.78  0.04}{0.3}       & \reshl{89.50  0.30}{5.53}      & \reshl{94.33  0.16}{3.73}      \\
			\midrule
			MobileNet V3~\cite{howard2019searching}                  & 78.8  0.0       & 77.7  0.0       & 94.2  0.0       & 66.5  0.0       & 79.3                 \\
			+\Model                      & \reshl{84.9  0.2}{6.1}        & \reshl{83.5  0.2}{5.8}        & \reshl{97.7  0.1}{3.5}         & \reshl{74.8  0.3}{8.3}         & \reshl{85.3  0.4}{6.0}         \\
			\midrule
			ConvNeXt~\cite{liu2022convnet}                  & 95.73  0.00     & 85.34  0.00     & 99.25  0.00     & 90.81  0.00     & 92.8                 \\
			+\Model                      & \reshl{98.21  0.09}{2.48}      & \reshl{90.23  0.50}{4.89}      & \reshl{99.93  0.00}{0.68}      & \reshl{92.88  0.16}{2.07}      & \reshl{95.31  0.16}{2.51}      \\
			\midrule
			ViT-L16~\cite{dosovitskiy2020image}                  & 93.17  0.00     & 88.06  0.00     & 99.55  0.00     & 83.30  0.00     & 91.0                 \\
			+\Model                      & \reshl{98.19  0.02}{5.02}      & \reshl{92.54  0.11}{4.48}      & \reshl{99.98  0.02}{0.43}      & \reshl{90.21  0.36}{6.91}      & \reshl{95.23  0.12}{4.23}      \\
			\midrule
			Mixer-B16~\cite{tolstikhin2021mlp}                  & 76.02  0.00     & 71.59  0.00     & 94.39  0.00     & 62.09  0.00     & 76.0                 \\
			+\Model                      & \reshl{84.16  0.35}{8.14}      & \reshl{82.16  0.38}{10.57}     & \reshl{96.96  0.05}{2.57}      & \reshl{65.91  0.10}{3.82}      & \reshl{82.30  0.13}{6.3}       \\
			\midrule
			Mixer-L16~\cite{tolstikhin2021mlp}                  & 84.93  0.00     & 82.89  0.00     & 98.73  0.00     & 73.66  0.00     & 85.1                 \\
			+\Model                      & \reshl{91.74  0.16}{6.81}      & \reshl{85.89  0.38}{3.0}       & \reshl{99.38  0.07}{0.65}      & \reshl{76.86  0.33}{3.2}       & \reshl{88.47  0.16}{3.37}      
			\\\bottomrule
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM with different visual backbones for Table 3 on OfficeHome dataset.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & A & C & P & R & Avg  \\ \midrule
			ResNet-101~\cite{he2016deep}                  & 61.23  0.00     & 55.18  0.00     & 75.56  0.00     & 77.74  0.00     & 67.4                 \\
			+\Model                      & \reshl{65.00  0.28}{3.77}      & \reshl{58.17  0.51}{2.99}      & \reshl{78.60  0.12}{3.04}      & \reshl{79.54  0.02}{1.8}       & \reshl{70.33  0.20}{2.93}      \\
			\midrule	
			ViT-B16~\cite{dosovitskiy2020image}                  & 71.37  0.00     & 60.17  0.00     & 84.40  0.00     & 86.60  0.00     & 75.6                 \\
			+\Model                      & \reshl{78.29  0.32}{6.92}      & \reshl{66.46  0.39}{6.29}      & \reshl{89.10  0.16}{4.7}       & \reshl{89.99  0.12}{3.39}      & \reshl{80.96  0.05}{5.36}      \\
			\midrule
			DeiT~\cite{touvron2021training}                   & 75.28  0.00     & 62.69  0.00     & 84.23  0.00     & 85.86  0.00     & 77.0                 \\
			+\Model                      & \reshl{78.30  0.07}{3.02}      & \reshl{66.56  0.46}{3.87}      & \reshl{85.82  0.07}{1.59}      & \reshl{87.14  0.06}{1.28}      & \reshl{79.46  0.10}{2.46}      \\
			\midrule
			HViT~\cite{dosovitskiy2020image}                  & 74.67  0.00     & 70.85  0.00     & 86.37  0.00     & 87.44  0.00     & 79.8                 \\
			+\Model                      & \reshl{76.67  0.11}{2.0}       & \reshl{72.27  0.54}{1.42}      & \reshl{87.33  0.14}{0.96}      & \reshl{89.08  0.22}{1.64}      & \reshl{81.34  0.06}{1.54}      \\
			\midrule
			Swin Transformer~\cite{liu2021swin}                   & 80.74  0.00     & 72.22  0.00     & 88.20  0.00     & 89.30  0.00     & 82.6                 \\
			+\Model                      & \reshl{83.26  0.17}{2.52}      & \reshl{74.42  0.03}{2.2}       & \reshl{89.90  0.10}{1.7}       & \reshl{90.69  0.05}{1.39}      & \reshl{84.57  0.06}{1.97}      \\
			\midrule
			MobileNet V3~\cite{howard2019searching}                  & 56.7  0.0       & 46.2  0.0       & 68.9  0.0       & 73.0  0.0       & 61.2                 \\
			+\Model                      & \reshl{61.2  0.5}{4.5}         & \reshl{50.7  0.5}{4.5}        & \reshl{73.1  0.1}{4.2}        & \reshl{75.4  0.1}{2.4}        & \reshl{65.1  0.2}{3.9}        \\
			\midrule
			ConvNeXt~\cite{liu2022convnet}                  & 83.88  0.00     & 78.12  0.00     & 90.62  0.00     & 91.37  0.00     & 86.0                 \\
			+\Model                      & \reshl{87.68  0.11}{3.8}       & \reshl{80.95  0.07}{2.83}      & \reshl{92.25  0.04}{1.63}      & \reshl{93.23  0.09}{1.86}      & \reshl{88.53  0.04}{2.53}      \\
			\midrule
			ViT-L16~\cite{dosovitskiy2020image}                  & 81.31  0.00     & 71.56  0.00     & 89.33  0.00     & 90.71  0.00     & 83.2                 \\
			+\Model                      & \reshl{86.25  0.29}{4.94}      & \reshl{79.71  0.20}{8.15}      & \reshl{91.35  0.16}{2.02}      & \reshl{92.85  0.11}{2.14}      & \reshl{87.54  0.17}{4.34}      \\
			\midrule
			Mixer-B16~\cite{tolstikhin2021mlp}                  & 30.69  0.00     & 47.97  0.00     & 70.33  0.00     & 60.90  0.00     & 52.5                 \\
			+\Model                      & \reshl{32.60  0.24}{1.91}      & \reshl{53.68  0.73}{5.71}      & \reshl{77.18  0.13}{6.85}      & \reshl{67.26  0.22}{6.36}      & \reshl{57.68  0.27}{5.18}      \\
			\midrule
			Mixer-L16~\cite{tolstikhin2021mlp}                  & 67.61  0.00     & 62.54  0.00     & 82.49  0.00     & 68.10  0.00     & 70.2                 \\
			+\Model                      & \reshl{71.78  0.36}{4.17}      & \reshl{68.41  0.23}{5.87}      & \reshl{87.08  0.15}{4.59}      & \reshl{75.04  0.16}{6.94}      & \reshl{75.58  0.08}{5.38}     
			\\ \bottomrule
		\end{tabular}
	}
\end{table*}

\begin{table*}[htb]
	\centering
	\caption{Full results using classifiers trained by ERM with different visual backbones for Table 3 on TerraIncognita dataset.}
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{lccccc} 
			\toprule
			Method & L100 & L38 & L43 & L46 & Avg  \\ \midrule
			ResNet-101~\cite{he2016deep}                  & 37.44  0.00     & 36.78  0.00     & 57.08  0.00     & 39.11  0.00     & 42.6                 \\
			+\Model                      & \reshl{50.65  0.47}{13.21}     & \reshl{46.71  0.76}{9.93}      & \reshl{59.87  0.58}{2.79}      & \reshl{42.89  0.85}{3.78}      & \reshl{50.03  0.46}{7.43}      \\
			\midrule	
			ViT-B16~\cite{dosovitskiy2020image}                  & 53.97  0.00     & 36.12  0.00     & 48.46  0.00     & 35.25  0.00     & 43.4                 \\
			+\Model                      & \reshl{65.50  1.36}{11.53}     & \reshl{48.83  1.34}{12.71}     & \reshl{51.61  0.27}{3.15}      & \reshl{39.68  0.16}{4.43}      & \reshl{51.40  0.22}{8.0}\\
			\midrule
			DeiT~\cite{touvron2021training}                   & 56.45  0.00     & 42.20  0.00     & 55.07  0.00     & 43.40  0.00     & 49.3                 \\
			+\Model                      & \reshl{64.56  0.60}{8.11}      & \reshl{51.00  1.93}{8.8}       & \reshl{56.55  0.52}{1.48}      & \reshl{44.32  0.13}{0.92}      & \reshl{54.11  0.42}{4.81}\\
			\midrule
			HViT~\cite{dosovitskiy2020image}                  & 61.59  0.00     & 46.13  0.00     & 61.56  0.00     & 42.11  0.00     & 52.8                 \\
			+\Model                      & \reshl{73.64  1.23}{12.05}     & \reshl{54.52  0.60}{8.39}      & \reshl{63.57  0.49}{2.01}      & \reshl{48.57  0.98}{6.46}      & \reshl{60.08  0.36}{7.28}      \\
			\midrule
			Swin Transformer~\cite{liu2021swin}                   & 80.74  0.00     & 72.22  0.00     & 88.20  0.00     & 89.30  0.00     & 82.6                 \\
			+\Model                      & \reshl{83.26  0.17}{2.52}      & \reshl{74.42  0.03}{2.2}       & \reshl{89.90  0.10}{1.7}       & \reshl{90.69  0.05}{1.39}      & \reshl{84.57  0.06}{1.97}      \\
			\midrule
			MobileNet V3~\cite{howard2019searching}                  & 30.6  0.0       & 26.4  0.0       & 31.9  0.0       & 31.2  0.0       & 30.0               \\
			+\Model                      & \reshl{39.2  0.3}{8.6}         & \reshl{29.8  1.3}{3.4}         & \reshl{31.5  0.1}{-0.4}       & \reshl{38.3  0.8}{8.2}         & \reshl{34.7  0.2}{4.7}        \\
			\midrule
			ConvNeXt~\cite{liu2022convnet}         & 67.70  0.00     & 55.75  0.00     & 67.51  0.00     & 53.24  0.00     & 61.0                 \\
			+\Model                      & \reshl{77.05  0.18}{9.35}      & \reshl{59.84  0.77}{4.09}      & \reshl{68.25  0.20}{0.74}      & \reshl{56.23  0.48}{2.99}      & \reshl{65.34  0.27}{4.34}      \\
			\midrule
			ViT-L16~\cite{dosovitskiy2020image}                  & 48.85  0.00     & 42.48  0.00     & 51.35  0.00     & 38.18  0.00     & 45.2                 \\
			+\Model                      & \reshl{64.57  1.42}{15.72}     & \reshl{54.77  0.38}{12.29}     & \reshl{55.06  0.11}{3.71}      & \reshl{41.06  0.31}{2.88}      & \reshl{53.86  0.42}{8.66}      \\
			\midrule
			Mixer-B16~\cite{tolstikhin2021mlp}                  & 26.84  0.00     & 26.95  0.00     & 30.07  0.00     & 22.37  0.00     & 26.6                 \\
			+\Model                      & \reshl{46.89  2.11}{20.05}     & \reshl{53.87  0.18}{26.92}     & \reshl{37.97  0.99}{7.9}       & \reshl{26.10  0.17}{3.73}      & \reshl{41.21  0.48}{14.61}     \\
			\midrule
			Mixer-L16~\cite{tolstikhin2021mlp}                  & 38.18  0.00     & 26.72  0.00     & 47.23  0.00     & 33.97  0.00     & 36.5                 \\
			+\Model                      & \reshl{57.20  0.75}{19.02}     & \reshl{36.55  5.39}{9.83}      & \reshl{49.70  0.23}{2.47}      & \reshl{36.47  0.38}{2.5}       & \reshl{44.98  1.40}{8.48}   \\   
			\bottomrule
		\end{tabular}
	}
	\label{tab:supp-12}
\end{table*}
\end{document}
