

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\usepackage{cite}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage[symbol]{footmisc}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\def\tableautorefname{Table}
\def\figureautorefname{Fig.}
\def\equationautorefname{Eq.}
\def\sectionautorefname{\S}
\def\subsectionautorefname{\S}

\makeatletter
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\makeatother

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{document}
\title{Learning Human-Object Interactions by \\Graph Parsing Neural Networks}


\titlerunning{Graph Parsing Neural Networks (ECCV 2018)}
\author{Siyuan Qi\and
Wenguan Wang\and
Baoxiong Jia\and\\
Jianbing Shen\and
Song-Chun Zhu}
\authorrunning{S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu}


\institute{University of California, Los Angeles\and
International Center for AI and Robot Autonomy (CARA)\and
Beijing Institute of Technology\and
Peking University\and
Inception Institute of Artificial Intelligence\\
\email{syqi@cs.ucla.edu\quad
wenguanwang.ai@gmail.com\quad
baoxiongjia@ucla.edu\\
shenjianbing@bit.edu.cn\quad
sczhu@stat.ucla.edu}
}
\maketitle              \begin{abstract}
This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings. The code is available at \url{https://github.com/SiyuanQi/gpnn}.
\keywords{Human-Object Interaction \and Message Passing \and Graph Parsing \and Neural Networks}
\end{abstract}

\let\thefootnote\relax\footnotetext{ Equal contribution.  Corresponding author.}

\section{Introduction}
\label{sec:introduction}
The task of human-object interaction (HOI) understanding aims to infer the relationships between human and objects, such as ``riding a bike'' or ``washing a bike''. Beyond traditional visual recognition of individual instances, \textit{e.g.}, human pose estimation, action recognition, and object detection, recognizing HOIs requires a deeper semantic understanding of image contents. Recently, deep neural networks (DNNs) have shown impressive progress on above individual tasks of instance recognition, while relatively few methods  \cite{chao2015hico,chao2017learning,shenscaling,gkioxari2017detecting} were proposed for HOI recognition. This is mainly because it requires \textit{reasoning} beyond \textit{perception}, by integrating information from human, objects, and their complex relationships.

In this paper, we propose a novel model, Graph Parsing Neural Network (GPNN), for HOI recognition. The proposed GPNN offers a general framework that explicitly represents HOI structures with graphs and automatically parses the optimal graph structures in an end-to-end manner. In principle, it is an generalization of Message Passing Neural Network (MPNN)~\cite{gilmer2017neural}. An overview of GPNN is shown in \autoref{fig:overview}. The following two aspects motivate our design.

First, we seek a unified framework that utilizes the learning capability of neural networks and the power of graphical representations. Recent deep learning based HOI models showed promising results, but few touched how to interpret well and explicitly leverage spatial and temporal dependencies and human-object relations in such structured task. Aiming for this, we introduce GPNN. It inherits the complementary strengths of neural networks and graphical models, for forming a coherent HOI representation with strong learning ability. Specifically, with the structured representation of an HOI graph, the rich relations are explicitly utilized, and the information from individual elements can be efficiently integrated and broadcasted over the structures. The whole model and message passing operations are well-defined and fully differentiable. Thus it can be efficiently learned from data in an end-to-end manner.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/overview.pdf}
\caption{\textbf{Illustration of the proposed GPNN for learning HOI.} GPNN offers a generic HOI representation that applies to (a) HOI detection in images and (b) HOI recognition in videos. With the integration of graphical model and neural network, GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi). The final parse graph explains a given scene with the graph structure (\eg, the link between the person and the knife) and the node labels (\eg, lick). A thicker edge corresponds to stronger information flow between nodes in the graph.}
\label{fig:overview}
\end{figure}

Second, based on our efficient HOI representation and learning power, GPNN applies to diverse HOI tasks in both static and dynamic scenes. Previous studies for HOI achieved good performance in their specific domains (spatial~\cite{chao2017learning,gkioxari2017detecting} or temporal~\cite{jain2016structural,qi2017predicting,qi2018generalized}). However, none of them addresses a generic framework for representing and learning HOI in both images and videos. The key difficulty lies in the diverse relations between components. Given a set of human and objects candidates, there may exist an uncertain number of human-object interaction pairs (see \autoref{fig:overview} (a.ii) as an example). The relations become more complex after taking temporal factors into consideration. Thus pre-fixed graph structures, as adopted by most previous graphical or structured DNN models~\cite{koppula2016anticipating,jain2016structural,Wang_2018_CVPR,Fang2018}, are not an optimal choice. Seeking a better generalization ability, GPNN incorporates an essential \textit{link function} for addressing the problem of graph structure learning. It learns to infer the adjacency matrix in an end-to-end manner and thus can infer a parse graph that explicitly explains the HOI relations. With such learnable graph structure, GPNN could also limit the information flow from irrelevant nodes while encouraging message to propagate between related nodes, thus improving graph parsing.

We extensively evaluate the proposed GPNN on three HOI datasets, namely HICO-DET~\cite{chao2017learning}, V-COCO~\cite{gupta2015visual} and CAD-120~\cite{koppula2016anticipating}, for HOI detection from images (HICO-DET, V-COCO) and HOI recognition and anticipation in spatial-temporal settings (CAD-120). The experimental results verify the generality and scalability of our GPNN based HOI representation and show substantial improvements over state-of-the-art approaches, including pure graphical models and pure neural networks. We also demonstrate GPNN outperforms its variants and other graph neural networks with pre-fixed structures.

This paper makes three major contributions. \textbf{First}, we propose the GPNN that incorporates structural knowledge and DNNs for learning and inference. \textbf{Second}, with a set of well defined modular functions, GPNN addresses the HOI problem by jointly performing graph structure inference and message passing. \textbf{Third}, we empirically show that GPNN offers a scalable and generic HOI representation that applies to both static and dynamic settings.











\section{Related Work}
\label{sec:related_work}
\noindent\textbf{Human-Object Interaction.} Reasoning human actions with objects (like ``playing baseball'', ``playing guitar''), rather than recognizing individual actions (``playing'') or object instances (``baseball'', ``guitar''), is essential for a more comprehensive understanding of what is happening in the scene. Early work in HOI understanding studied Bayesian model~\cite{gupta2007objects,gupta2009observing}, utilized contextual relationship between human and objects~\cite{yao2010grouplet,yao2010modeling,yao2011human}, learned structured representations with spatial interaction and context~\cite{delaitre2011learning}, exploited compositional models~\cite{desai2012detecting}, or referred to a set of HOI exemplars~\cite{hu2013recognising}. They were mainly based on handcrafted features (\eg, color, HOG, and SIFT) with object and human detectors. More recently, inspired by the notable success of deep learning and the availability of large-scale HOI datasets~\cite{chao2015hico,chao2017learning}, several deep learning based HOI models were then proposed. Specifically, Mallya \etal~\cite{mallya2016learning} modified Fast RCNN model \cite{girshick2015fast} for HOI recognition, with the assistance of Visual Question Answering (VQA). In~\cite{shenscaling}, zero-shot learning was applied for addressing the long-tail problem in HOI recognition. In~\cite{chao2017learning}, the human proposals, object regions, and their combinations were fed into a multi-stream network for tackling the HOI detection problem. Gkioxari \etal~\cite{gkioxari2017detecting} estimated an action-type specific density map for identifying the interacted object locations, with a modified Faster RCNN architecture~\cite{ren2015faster}.

Although promising results were achieved by above deep HOI models, we still observe two unsolved issues. First, they lack a powerful tool to represent the structures in HOI tasks explicitly and encodes them into modern network architectures efficiently. Second, despite the successes in specific tasks, a complete and generic HOI representation is missing. These approaches can not be easily extended to HOI recognition from videos. Aiming to address those issues, we introduce GPNN for imposing high-level relations into DNN, leading to a powerful HOI representation that is applicable in both static and dynamic settings.

\noindent\textbf{Neural Networks with Graphs/Graphical Models.}
In the literature, some approaches were proposed to combine graphical models and neural networks. The most intuitive approach is to build graphical models upon DNN, where the network that generates features is trained first, and its output is used to compute potential functions for the graphical predictor. Typical methods were used in human pose estimation~\cite{tompson2014joint}, human part parsing~\cite{xia2016pose,park2017attribute}, and semantic image segmentation~\cite{chen2016deeplab,chen2015learning}. These methods lack a deep integration in the sense that the computation process of graphical models cannot be learned end-to-end.
Some attempts~\cite{zheng2015conditional,wu2016deep,monti2016geometric,kipf2017semi,simonovsky2017dynamic,defferrard2016convolutional,niepert2016learning,seo2016structured} were made to generalize neural network operations (\eg, convolutions) directly from regular grids (\eg, images) to graphs. For the HOI problem, however, a structured representation is needed to capture the high-level spatial-temporal relations between humans and objects. Some other work integrated network architectures with graphical models~\cite{jain2016structural,gilmer2017neural} and gained promising results on applications such as scene understanding~\cite{xu2017scene, marino2016more, li2017situation}, object detection and parsing~\cite{liang2016semantic, yuan2017temporal}, and VQA~\cite{teney2016graph}. However, these methods only apply to problems that have pre-fixed graph structures. Liang \etal~\cite{liang2017interpretable} merged graph nodes using Long Short-Term Memory (LSTM) for human parsing problem, under the assumption that the nodes are mergeable.

Those methods achieved promising results in their specific tasks and well demonstrated the benefit in completing deep architectures with domain-specific structures. However, most of them are based on pre-fixed graph structures, and they have not yet been studied in HOI recognition.
In this work, we extend previous graphical neural networks with learnable graph structures, which well addresses the rich and high-level relations in HOI problems. The proposed GPNN can automatically infer the graph structure and utilize that structure for enhancing information propagation and further inference. It offers a generic HOI representation for both spatial and spatial-temporal settings. To the best of our knowledge, this is a first attempt to integrate graph models with neural networks in a unified framework to achieve state-of-art results in HOI recognition.






\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/gpnn.pdf}
\caption{\textbf{Illustration of the forward pass of GPNN.} GPNN takes node and edge features as input, and outputs a parse graph in a message passing fasion. The structure of the parse graph is given by a soft adjacency matrix. It is computed by the \textit{link function} based on the features (or hidden node states). The darker the color in the adjacency matrix, the stronger the connectivity is. Then \textit{message functions} compute incoming messages for each node as a weighted sum of the messages from other nodes. Thicker edges indicate larger information flows. The \textit{update functions} update the hidden internal states of each node. Above process is repeated for several steps, iteratively and jointly learning the computation of graph structures and message passing. Finally, for each node, the \textit{readout functions} output HOI action or object labels from the hidden node states. See \autoref{sec:gpnn} for more details.}
\label{fig:gpnn}
\end{figure*}




\section{Graph Parsing Neural Network for HOI}
\label{sec:gpnn}





\subsection{Formulation}
\label{sec:gpnn_forward_pass}
For HOI understanding, human and objects are represented by nodes, and their relations are defined as edges. Given a complete HOI graph that includes all the possible relationships among human and objects, we want to automatically infer a parse graph by keeping the meaningful edges and labeling the nodes.

Formally, let  denote the complete HOI graph. Nodes  take unique values from . Edges  are two-tuples . Each node  has a output state  that takes a value from a set of labels  (\eg, actions). A parse graph  is a sub-graph of , where  and . Given the node features  and edge features , we want to infer the optimal parse graph  that best explains the data according to a probability distribution :

where . Here  evaluates the graph structure, and  is the labeling probability for the nodes in the parse graph.


This formulation provides us a principled guideline for designing the GPNN. We design the network to approximate the computations of  and . We introduce four types of functions as individual modules in the forward pass of a GPNN: \textit{link functions}, \textit{message functions}, \textit{update functions}, and \textit{readout functions} (as illustrated in \autoref{fig:gpnn}).
The link functions  estimate the graph structure, giving an approximation of . The message, update and readout functions together resemble the belief propagation process and approximate .

Specifically, the link function  (\includegraphics[scale=0.08]{figs/link.pdf}) takes edge features (\includegraphics[scale=0.04]{figs/feature.pdf}) as input and infers the connectivities between nodes. The soft adjacency matrix (\includegraphics[scale=0.04]{figs/adjacency.pdf}) is thus constructed and used as weights for messages passing through edges between nodes. The incoming messages for a node are summarized by the message function (\includegraphics[scale=0.06]{figs/message.pdf}), then the hidden embedding state of the node is updated based on the messages by an update function (\includegraphics[scale=0.08]{figs/update.pdf}). Finally, readout functions (\includegraphics[scale=0.08]{figs/readout.pdf}) compute the target outputs for each nodes. Those four types of functions are defined as follows:



\noindent\textbf{Link Function.} We first infer an adjacency matrix that represents connectivities (\ie, the graph structure) between nodes by a link function. A link function  takes the node features , and edge features  as input and outputs an adjacency matrix :


where  denotes the -th entry of the matrix . Here we overload the notation and let  denote node features and  denote edge features.
In this way, the structure of a parse graph  can be approximated by the adjacency matrix. Then we start to propagate messages over the parse graph, where the soft adjacency matrix controls the information to be passed through edges.

\noindent\textbf{Message and Update Functions.} Based on the learned graph structure, the message passing algorithm is adopted for inference of node labels. During belief propagation, the hidden states of the nodes are iteratively updated by communicating with other nodes. Specially, message functions  summarize messages to nodes coming from other nodes, and update functions  update the hidden node states according to the incoming messages. At each iteration step , the two functions computes:

where  is the summarized incoming message for node  at -th iteration and  is the hidden state for node . The node connectivity  encourages the information flow between nodes in the parse graph. The message passing phase runs for  steps towards convergence. At the first step, the node hidden states  are initialized by node features .

\noindent\textbf{Readout Function.} Finally, for each node, hidden state is fed into a readout function to output a label:

Here the readout function  computes output  for node  by activating its hidden state  (node embeddings).

\noindent\textbf{Iterative Parsing.} Based on the above four functions, the messages are passed along the graph and weighted by the learned adjacency matrix . We further extend above process into a joint learning framework that iteratively infers the graph structure and propagates the information to infer node labels. In particular, instead of learning  only at the beginning, we iteratively infer  with the updated node information and edge features at each step :

Then the messages in \autoref{eqn:message_function} are redefined as:

In this way, both the graph structure and the message update can be jointly and iteratively learned in a unified framework. In practice, we find such a strategy would bring better performance (detailed in \autoref{sec:abastudy}).

In next section, we show that by implementing each function by neural networks, the entire system is differentiable end-to-end. Hence all the parameters can be learned using gradient-based optimization.

\subsection{Network Architecture}
\label{sec:net_ar}


\noindent\textbf{Link Function.} Given the complete HOI graph , we use  and  to denote the dimension of the node features and the edge features, respectively. In a message passing step , we first concatenate all the node features (hidden states)  and all the edge features (messages)  to form a feature matrix  (see \includegraphics[scale=0.04]{figs/feature.pdf} in \autoref{fig:gpnn}). The link function is defined as a small neural network with one or several convolutional layer(s) (with ) kernels) and a  activation. Then the adjacency matrix  can be computed as:

where  is the learnable parameters of the link function network  and  denotes conv operation. The  operation  is for normalizing the values of the elements of  into . The essential effect of multiple convolutional layers with  kernels is similar to fully connected layers applied to each individual edge features, except that the filter weights are shared by all the edges. In practice, we find such operation generates good enough results and leads to a high computation efficiency.




For spatial-temporal problems where the adjacency matrices should account for the previous states, we use convolutional LSTMs~\cite{xingjian2015convolutional} for modeling  in temporal domain. At time , the link function takes  as input features and the previous adjacency matrix  as hidden state: .
Again, the kernel size for the conv layer in convLSTM is .

\noindent\textbf{Message Function.}
In our implementation, the message function  in \autoref{eqn:message_function} is computed by:

where  denotes concatenation. It concatenates the outputs of linear transforms (\ie, fully connected layers parametrized by  and ) that takes node hidden states  or edge features  as input. 

\noindent\textbf{Update Function.} Recurrent neural networks~\cite{elman1990finding,hochreiter1997long} are natural choices for simulating the iterative update process, as done by previous works \cite{gilmer2017neural}. Here we apply Gated Recurrent Unit (GRU)~\cite{cho2014properties} as the update function, because of its recurrent nature and smaller amount of parameters. Thus the update function in \autoref{eqn:update_function} is implemented as:

where  is the hidden state and  is used as input features. As demonstrated in \cite{li2015gated}, the GRU is more effective than vanilla recurrent neural networks.


\noindent\textbf{Readout Function.}
A typical implementation of readout functions is combining several fully connected layers (parameterized by ) followed by an activation function:

Here the activation function  can be used as \textit{softmax} (one-class outputs) or \textit{sigmoid} (multi-class outputs) according to different HOI tasks.

In this way, the entire GPNN is implemented to be fully differentiable and end-to-end trainable. The loss for specific HOI task can be computed for the outputs of readout functions, and the error can propagate back according to chain rule. In next section, we will offer more details for implementing GPNN for HOI tasks on spatial and spatial-temporal settings and present qualitative as well as quantitative results.

\section{Experiments}
To verify the effectiveness and generic applicability of GPNN, we perform experiments on two HOI problems: i) HOI detection in images \cite{chao2017learning,gupta2015visual}, and ii) HOI recognition and anticipation from videos~\cite{koppula2016anticipating}. The first experiment is performed on HICO-DET~\cite{chao2017learning} and V-COCO~\cite{gupta2015visual} datasets, showing that our approach is scalable to large datasets (about 60K images in total) and achieves a good detection accuracy over a large number of classes (more than 600 classes of HOIs). The second experiment is reported on CAD-120 dataset~\cite{koppula2016anticipating}, showing that our method is well applicable to spatial-temporal domains.






\subsection{Human-Object Interaction Detection in Images}
\label{sec:humanobjectinter}
For HOI detection in an image, the goal is to detect pairs of a human and an object bounding box with an interaction class label connecting them. 

\begin{table}[!ht]\caption{\textbf{HOI detection results (mAP) on HICO-DET dataset~\cite{chao2017learning}}. Higher values are better. The best scores are marked in \textbf{bold}.
}
\label{tab:hico_result}
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{l||c||c||c}  \hline\thickhline
Methods & Full (mAP \%)  & Rare (mAP \%)  & Non-rare (mAP \%)  \\
\hline
\hline
 Random &  &  &  \\
 Fast-RCNN(union)~\cite{girshick2015fast} & 1.75 & 0.58 & 2.10 \\
 Fast-RCNN(score)~\cite{girshick2015fast} & 2.85 & 1.55 & 3.23 \\
 HO-RCNN~\cite{chao2017learning} & 5.73 & 3.21 & 6.48 \\
 HO-RCNN+IP~\cite{chao2017learning} &7.30 &4.68 &8.08\\
 HO-RCNN+IP+S~\cite{chao2017learning} & 7.81 & 5.37 & 8.54 \\
 Gupta \etal \cite{gupta2015visual} &9.09 &7.02 &9.71\\
 Shen \etal \cite{shenscaling} &6.46 &4.24 &7.12\\
 InteractNet \cite{gkioxari2017detecting} &9.94 &7.16 &10.77\\
 \hline
\textbf{GPNN} & \textbf{13.11} & \textbf{9.34} & \textbf{14.23} \\
\textit{Performance Gain}(\%) 	& 31.89 & 30.45 & 32.13 \\
\hline
\end{tabular}
}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/result_HICO.pdf}
\caption{\textbf{HOI detection results on HICO-DET~\cite{chao2017learning} test images.} Human and objects are shown in red and green rectangles, respectively. Best viewed in color.}
\label{fig:hico_results}
\end{figure*}

\noindent\textbf{Datasets.} We use HICO-DET~\cite{chao2017learning} and V-COCO~\cite{gupta2015visual} datasets for benchmarking our GPNN model. HICO-DET provides more than 150K annotated instances of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It has the same 80 object categories as MS-COCO~\cite{lin2014microsoft} and 117 action categories. V-COCO is a subset of MS-COCO~\cite{lin2014microsoft}. It consists of a total of 10,346 images with
16,199 people instances, where 2.5K images in the train set, 2.8K images for validation and 4.9K images for testing. Each annotated person has binary
labels for 26 different action classes. Note that three actions (\ie, \emph{cut}, \emph{eat}, and \emph{hit}) are annotated with two types of targets: \textit{instrument} and \textit{direct object}.

\noindent\textbf{Implementation Details.}  Humans and objects are represented by nodes in the graph, while human-object interactions are represented by edges. In this experiment, we use a pre-trained  deformable convolutional network~\cite{dai2017deformable} for object detection and features extraction. Based on the detected bounding boxes, we extract node features () from the position-sensitive region of interest (PS RoI) pooling layer from the deformable ConvNet.
We extract the edge feature from a combined bounding box, \ie, the smallest bounding box that contains both two nodes' bounding boxes.
The functions of GPNN are implemented as follows. We use a convolutional network (128-128-1)-Sigmoid() with  kernels for the link function.
The message functions are composed of a fully connected layer, concatenation, and summation. For a node , the neighboring node feature  and edge feature  are passed through a fully connected layer and concatenated. The final incoming message is a weighted sum of messages from all neighboring nodes. Specifically, the message for node  coming from node  through edge  is the concatenation of output from FC(-) and FC(-). A GRU() is used for the update function. The propagation step number  is set to be 3. For the readout function, we use a FC(-117)-Sigmoid() and FC(-26)-Sigmoid() for HICO-DET and V-COCO, respectively.


The probability of an HOI label of a human-object pair is given by the product of the final output probabilities from the human node and the object node.
We employ an L1 loss for the adjacency matrix. For the node outputs, we use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the training examples are not balanced, and it is essentially a multi-label problem for each node (there might not even exist a meaningful human-object interaction for detected humans and objects).

Our model is implemented using PyTorch and trained with a machine with a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the rate decays every 5 epochs by 0.8. The training process takes about 20 epochs (15 hours) to roughly converge with a batch size of 32.




\begin{table}[t]\caption{\textbf{HOI detection results (mAP) on V-COCO~\cite{gupta2015visual}  dataset}. \textbf{Legend:} \textit{Set 1} indicates 18 HOI actions with one object, and \textit{Set 2} corresponds to 3 HOI actions (\ie, \emph{cut}, \emph{eat}, \emph{hit}) with two objects (\textit{instrument} and \textit{object}). }
\label{tab:vcoco_result}
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{l||c||c||c}  \hline\thickhline
Method & Set 1 (mAP \%)  & Set 2 (mAP \%)  & Ave. (mAP \%)  \\
\hline
\hline
 Gupta \etal \cite{gupta2015visual} &33.5 &26.7 &31.8\\
 InteractNet \cite{gkioxari2017detecting} &42.2 &33.2 &40.0\\
 \hline
\textbf{GPNN} &\textbf{44.5} &\textbf{42.8} &\textbf{44.0} \\
\textit{Performance Gain}(\%) 	& 5.5 & 28.9 & 10.0 \\
\hline
\end{tabular}
}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/result_VCOCO.pdf}
\caption{\textbf{HOI detection results on V-COCO~\cite{gupta2015visual} test images.} Human and objects are shown in red and green rectangles, respectively. Best viewed in color.}
\label{fig:vcoco_results}
\end{figure*}

\noindent\textbf{Comparative Methods.} We compare our method with eight baselines: (1) Fast-RCNN (union)~\cite{girshick2015fast}: for each human-object
proposal from detection results, their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-RCNN (score)~\cite{girshick2015fast}: given human-object proposals, HOI is predicted by linearly combining the human and object detection scores. (3) HO-RCNN~\cite{chao2017learning}: a multi-stream architecture with a ConvNet to classify human, object and human-object proposals, respectively. The final output is computed by combining the scores from all the three streams. (4) HO-RCNN+IP~\cite{chao2017learning} and (5) HO-RCNN+IP+S~\cite{chao2017learning}:  HO-RCNN with additional components. Interaction Patterns (IP) acts as a attention filter to images. S is an extra path with a single neuron that uses the raw object detection score to produce an offset for the final detection. More detailed descriptions of above five baselines can be found in~\cite{chao2017learning}. (6) Gupta \etal \cite{gupta2015visual}: trained based on Fast-RCNN \cite{girshick2015fast}. We use the scores reported in \cite{gkioxari2017detecting}.
(7) Shen \etal \cite{shenscaling}: final predictions are from two Faster RCNN \cite{ren2015faster} based networks which are trained for predicting verb and object classes, respectively. (8) InteractNet \cite{gkioxari2017detecting}: a modified Faster RCNN \cite{ren2015faster} with an additional human-centric branch that estimates an action-specific density map for locating objects.


\noindent\textbf{Experiment Results.} Following the standard settings in HICO-DET and V-COCO  benchmarks, we evaluate HOI detection using mean average precision (mAP). An HOI detection is considered as a true positive when the human detection, the object detection, and the interaction class are all correct. The human and object bounding boxes are considered as true positives if they overlap with a ground truth bounding boxes of the same class with an intersection over union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over three different HOI category sets: i) all 600 HOI categories in HICO (Full); ii) 138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI categories with 10 or more training instances (Non-Rare). For V-COCO dataset, since we concentrate on HOI detection, we report the mAP on three groups: i) 18 HOI action classes with one target object; ii) 3 HOI categories with two types of objects; iii) all 24 (=) HOI classes.
Results are evaluated on the test sets and reported in \autoref{tab:hico_result} and \autoref{tab:vcoco_result}.

As shown in \autoref{tab:hico_result}, the proposed GPNN substantially outperforms the comparative methods, achieving \textbf{31.89\%}, \textbf{30.45\%}, and \textbf{32.13\%} improvement over the second best methods on the three HOI category sets on the HICO-DET dataset. The results on V-COCO dataset (in \autoref{tab:vcoco_result}) also consistently demonstrate the superior performance of the proposed GPNN.
Two important \textbf{conclusions} can be drawn from the results:
\textbf{i)} our method  is scalable to large datasets; \textbf{ii)} and our method performs better than pure neural network.
Some visual results can be found in \autoref{fig:hico_results} and \autoref{fig:vcoco_results}.






\subsection{Human-Object Interaction Recognition in Videos}
\label{sec:human_activity_recognition}
The goal of this experiment is to detect and predict the human sub-activity labels and object affordance labels as the human-object interaction progresses in videos. The problem is challenging since it involves complex interactions that humans make with multiple objects, and objects also interact with each other.

\noindent\textbf{CAD-120 dataset~\cite{koppula2016anticipating}.} It has 120 RGB-D videos of 4 subjects performing 10 activities, each of which is a sequence of sub-activities involving 10 actions (\eg, reaching, opening), and 12 object affordances (\eg, reachable, openable) in total.

\begin{table}[t]\caption{\textbf{Human activity detection and future anticipation results on CAD-120~\cite{koppula2016anticipating} dataset}, measured via F1-score. }
\label{tab:cad120_result}
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{l||c|c||c|c}  \hline\thickhline
 & \multicolumn{2}{c||}{Detection (F1-score) } & \multicolumn{2}{c}{Anticipation (F1-score) } \\
\cline{2-5}
 Method&\makecell{Sub\\-activity(\%)} &\makecell{Object\\Affordance(\%)} &\makecell{Sub\\-activity(\%)} &\makecell{Object\\Affordance(\%)}\\
\hline
\hline
 ATCRF~\cite{koppula2016anticipating} & 80.4 & 81.5 & 37.9 & 36.7 \\
 S-RNN~\cite{jain2016structural} & 83.2 & 88.7 & 62.3 & 80.7 \\
 S-RNN (multi-task)~\cite{jain2016structural} & 82.4 & \textbf{91.1} & 65.6 & 80.9 \\
\hline
\textbf{GPNN} &\textbf{88.9} &88.8 &\textbf{75.6} &\textbf{81.9}\\
\textit{Performance Gain}(\%) 	& 8.1 & - & 15.2 & 1.2 \\
\hline
\end{tabular}
}
\end{table}

\noindent\textbf{Implementation Details.} The link function is implemented as: convLSTM(1024-1024-1024-1)-Sigmoid() (\ie, a four-layer convLSTM). We use the same architecture as the previous experiment for message functions and update functions: [FC(-), FC(-)] for message function and GRU() for update function. The propagation step number  is set to be 3.
We use a FC(-10)-Softmax() and a FC(-12)-Softmax() for readout functions of sub-activity and object affordance detection/anticipation, respectively. We employ an L1 loss for the adjacency matrix and a cross entropy loss for the node outputs. We use the publicly available node and edge features from \cite{koppula2013learning}.






\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/confusion_matrices.pdf}
\caption{\textbf{Confusion matrices of HOI detection (a)(b) and anticipation (c)(d) results on CAD-120~\cite{koppula2016anticipating} dataset.} Zoom in for more details.}
\label{fig:cad_detection_confusion}
\end{figure*}






\noindent\textbf{Comparative Methods.}  We compare our method with two baselines: anticipatory temporal CRF (ATCRF)~\cite{koppula2016anticipating} and structural RNN (S-RNN)~\cite{jain2016structural}. ATCRF is a top-performing graphical model approach for this problem, while S-RNN is the state-of-art method using structured neural networks. ATCRF models the human activities through a spatial-temporal conditional random field. S-RNN casts a pre-defined spatial-temporal graph as an RNN mixture by representing nodes and edges as LSTMs.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/CADresults.pdf}
\caption{\textbf{HOI detection results on a ``cleaning objects'' activity} on CAD-120~\cite{koppula2016anticipating} dataset. Human are shown in red rectangle. Two objects are shown in green and blue rectangles, respectively. Detection and anticipation results are shown by different bars. For anticipation task, the label of the sub-activity at time  is anticipated at time . }
\label{fig:cad_qualitative}
\end{figure*}

\noindent\textbf{Experiment Results.} In \autoref{tab:cad120_result} we show the quantitative comparison of our method with other competitors. It shows the F1-scores averaged over all classes on detection and activity anticipation tasks. GPNN greatly improves over ATCRF and S-RNN, especially on anticipation task. Our method outperforms the other two for the following reasons. i) Comparing to ATCRF limited to the Markov assumption, our method allows arbitrary graph structures with improved representation ability. ii) Our method enjoys the benefit of deep integration of graphical models and neural networks and can be learned in an end-to-end manner. iii) Rather than relying on a pre-fixed graph structure as in S-RNN, we infer the graph structure via learning an adjacency matrix and thus be able to control the information flow between nodes during massage passing. \autoref{fig:cad_detection_confusion} show the confusion matrices for detecting and predicting the sub-activities and object affordances, respectively. From above results we can draw two important \textbf{conclusions}:
\textbf{i)} our method is well applicable to the spatio-temporal domain; and \textbf{ii)} our method outperforms pure graphical models (\eg, ATCRF) and deep networks with pre-fixed graph structures (\eg, S-RNN).
\autoref{fig:cad_qualitative} shows a qualitative visualization of ``cleaning objects''. We show one representative frame for each sub-activity as well as the corresponding detections and anticipations.








\subsection{Ablation Study}
\label{sec:abastudy}
In this section, we analyze the contributions of different model components to the final performance and examine the effectiveness of our main assumptions. \autoref{tab:graph_comparison} shows the detailed results on all three datasets.

\noindent\textbf{Integration of DNN with Graphical Model.} We first examine the influence of integrating DNN with a graphical model. We directly feed the features, which are originally used for GPNN, into different fully connected networks for predicting HOI action or object classes. From \autoref{tab:graph_comparison}, we can observe the performance of \textit{w/o graph} is significantly worse than GPNN model over various HOI datasets. This supports our view that modeling high-level structures and leveraging learning capabilities of DNNs together is essential for HOI tasks.

\noindent\textbf{GPNN with Fixed Graph Structures.} In \autoref{sec:gpnn}, GPNN automatically infers graph structures (\ie, parse graph) via learning a soft adjacency matrix. To assess this strategy, we fix all the entries in the soft adjacency matrices to be constant 1. This way the graph structures are fixed and the information flow between nodes are not weighted. For \textit{constant graph} baseline, we see obvious performance decrease, compared with the full GPNN model. This indicates that inferring graph structures is critical to get reasonable performance.

\noindent\textbf{GPNN without Supervision on Link Functions.} We perform experiments by turning off the L1 loss on adjacency matrices (\textit{w/o graph loss} in \autoref{tab:graph_comparison}). We can observe that the intermediate L1 loss is effective, further verifying our design to learn the graph structure. Another interesting observation is that training the model without this loss has a similar effect to training with constant graph. Hence supervision on the graph is fairly important.

\noindent\textbf{Jointly Learning Parse Graph and Message Passing.} We next study the effect of jointly learning graph structures and message passing. By isolating graph parsing from message passing, we obtain \textit{w/o joint parsing}, where the adjacency matrices are directly computed by link functions from edge features at the beginning. We observe a performance decrease in \autoref{tab:graph_comparison}, showing that learning graph structures and message passing together indeed boost the performance.

\noindent\textbf{Iterative Learning Process.} Next we examine the effect of iterative message passing, we report three baselines: \textit{1 iteration}, \textit{2 iterations}, and \textit{4 iterations}, which correspond to the results from different message passing iterations. The baseline \textit{GPNN} (first row in \autoref{tab:graph_comparison}) are the results after three iterations. From the results we observe that the iterative learning process is able to gradually improve the performance in general. We also observe that when the iteration round is increased to a certain extent, the performance drops slightly.





\begin{table}[t]\caption{\textbf{Ablation study of GPNN model.} Higher values are better.
}
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{2pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{l|l||c|c|c|c|c|c|c|c|c|c}  \hline\thickhline
 &&  \multicolumn{3}{c|}{V-COCO~\cite{gupta2015visual}} & \multicolumn{3}{c|}{HICO-DET~\cite{chao2017learning}} & \multicolumn{4}{c}{CAD-120~\cite{koppula2016anticipating}}\\
\cline{3-12}
 &&  \multicolumn{3}{c|}{\makecell{HOI Detection \\mAP(\%) }} & \multicolumn{3}{c|}{\makecell{HOI Detection \\mAP(\%) }} & \multicolumn{2}{c|}{\makecell{HOI Detec. \\ F1-score(\%) }} & \multicolumn{2}{c}{\makecell{HOI Antici. \\F1-score(\%) }} \\
\cline{3-12}
Aspect &Method & Set 1 &Set 2 &Ave. & Full & Rare & \makecell{Non-\\rare}& \makecell{Sub-\\activity} & \makecell{Object\\Aff.(\%)} & \makecell{Sub-\\activity} &\makecell{Object\\Aff.(\%)}\\
\hline
\hline
&\makecell{GPNN\3 iterations)} & \textbf{44.5}  & \textbf{42.8}  & \textbf{44.0}  & \textbf{13.11} & \textbf{9.34} & \textbf{14.23} & \textbf{88.9} & \textbf{88.8} & 75.6 & \textbf{81.9} \\
\hline
\hline
& w/o graph   & 27.4  & 30.0  & 28.1  & 7.88 & 2.04 & 9.62 & 50.2 & 20.8 & 32.3 & 19.6 \\
\textit{graph }& constant graph & 34.6  & 33.3  & 34.3  & 8.75 & 1.94 & 10.79 & 85.3 & 85.6 & 73.8 & 79.1 \\
\textit{structure} & w/o graph loss & 37.7  & 40.5  & 38.4  & 8.15  & 6.24  & 8.72  & 85.2 & 85.8 & 74.7 & 79.2 \\
& w/o joint parsing & 43.6  & 39.4  & 42.5  & 10.17  & 5.81  & 11.47  & 79.3 & 79.2 & 74.7 & 80.3 \\
\hline
\textit{iterative} & 1 iteration & 42.0  & 40.7  & 41.7  & 11.38 & 7.27 & 12.61 & 80.5 & 80.7 & 75.2 & 81.1 \\
\textit{learning} & 2 iterations & 44.1  & 42.2  & 43.6  & 12.37 & 9.01 & 13.38 & 87.9 & 86.1 & \textbf{76.1} & 81.5 \\
&4 iterations & 43.6  & 40.9  & 42.9  & 12.39 & 8.95 & 13.41 & 87.9 & 85.7 & 75.5 & 80.6 \\
\hline
\end{tabular}
}
\label{tab:graph_comparison}
\end{table}

\section{Conclusion}
In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring a parse graph in an end-to-end manner. The network can be decomposed into four distinct functions, namely link functions, message functions, update functions and readout functions, for iterative graph inference and message passing. GPNN provides a generic HOI representation that is applicable in both spatial and spatial-temporal domains. We demonstrate a substantial performance gain on three HOI datasets, showing the effectiveness of the proposed framework.\\



\noindent\textbf{Acknowledgments.}
The authors thank Prof. Ying Nian Wu from UCLA Statistics Department for helpful comments on this work.
This research is supported by DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, ARO W911NF1810296, and N66001-17-2-3602.

{\small
\bibliographystyle{splncs04}
\bibliography{paper}
}

\end{document}
