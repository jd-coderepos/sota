\section{Experiment}

\subsection{Datasets}

To demonstrate the capability and generality of DG-STGCN, 
we conduct extensive experiments on four datasets: NTURGB+D~\cite{liu2020ntu,shahroudy2016ntu}, Kinetics-400~\cite{carreira2017quo}, 
BABEL~\cite{punnakkal2021babel}, and Toyota SmartHome~\cite{Das_2019_ICCV}. 
Following the convention, we report the \textbf{Top-1} accuracy on NTURGB+D and Kinetics-400; report the \textbf{Mean Top-1} accuracy for BABEL and Toyota SmartHome.

\noindent
\textbf{NTURGB+D~\cite{liu2020ntu,shahroudy2016ntu}. } 
NTURGB+D is a large-scale human action recognition dataset collected in indoor environments. 
It has two versions, NTU-60 and NTU-120 (a superset of NTU-60).
NTU-60 contains 57K videos of 60 human actions, while NTU-120 contains 114K videos of 120 human actions. 
The datasets are split in three ways: Cross-subject (X-Sub), Cross-view (X-View, for NTU-60), Cross-setup (X-Set, for NTU-120), for which action subjects, camera views, camera setups are different in training and validation.
For skeleton action recognition, we take the 3D joints as inputs, while our preprocessing follows CTR-GCN~\cite{chen2021channel}. 
To show the generality of DG-STGCN, we also list the performance of models taking 2D joints (estimated with HRNet~\cite{sun2019deep}, provided by \cite{duan2021revisiting}) as inputs.   

\noindent
\textbf{Kinetics-400~\cite{carreira2017quo}. }
Kinetics-400 is a large-scale action recognition dataset with 306K videos and 400 actions. 
For Kinetics-400 experiments, we adopt the 2D joints estimated by OpenPose~\cite{8765346} (provided by ~\cite{yan2018spatial}) as inputs.

\noindent
\textbf{BABEL~\cite{punnakkal2021babel}. }
BABEL is a large-scale dataset for human movement with semantics. 
It annotates 43.5 hours of mocap from AMASS~\cite{mahmood2019amass}. 
BABEL predicts the 25-joint skeleton used in NTURGB+D from the vertices of the SMPL-H~\cite{romero2022embodied} mesh and uses it for skeleton-based action recognition.
BABEL has two versions, which consist of 60 (BABEL60) and 120 (BABEL120) action categories respectively. 
We report the results on both of them. 

\noindent
\textbf{Toyota SmartHome~\cite{Das_2019_ICCV}. }
Toyota SmartHome is a video dataset focusing on the real-world activities of humans.
The dataset contains 16K videos and 31 different actions in daily life. 
The videos are taken from 7 different camera views.
The dataset is split in two ways: Cross-subject (CS) or Cross-view (CV1: one view for training; CV2: five views for training). 
We only use 3D joints in its Pose V1.2 annotations for training and report results for CS and CV1 splits.

\subsection{Implementation Details} 
DG-STGCN follows the basic settings of ST-GCN. 
Specifically, there are 10 GCN blocks in DG-STGCN, while each GCN block consists of a DG-GCN and a DG-TCN. 
The base channel width is set to 64. 
At the  GCN block, we perform temporal pooling to halve the temporal length and also get the channel width doubled.
By default, all models are trained with SGD with momentum 0.9, weight decay 5e. 
We set the batch size to 128, set the initial learning rate to 0.1, and train the models for 100 epochs with CosineAnnealing learning rate scheduler. 
We use Uniform Sampling to sample subsequences from skeleton data to form training samples.
In the ablation study, we set the input length to 64. 
For comparisons with state-of-the-arts, we set the input length to 100.

\vspace{-3mm}
\subsection{Ablation Study} 

In the ablation study, we train DG-STGCN on two large-scale benchmarks: NTU120-XSub and NTU120-XSet.
We take the joint modality as inputs and follow the preprocessing in \cite{chen2021channel}. 
Our baseline uses GCN with randomly initialized coefficient matrices (3 components) for spatial modeling and a multi-branch TCN (with six branches: 4 kernel-3 1D convolution branches with dilation [1, 2, 3, 4], a kernel-3 max-pooling branch, and a  convolution branch) for temporal modeling.


\begin{wraptable}{r}{.47\linewidth}
    \captionsetup{font=small, position=top}
	\centering 
    \vspace{-8mm}
    \caption{The topology learned from scratch works well.}
    \vspace{-5mm}
    \label{tab-prelim}
    \resizebox{\linewidth}{!}{
	\tablestyle{4pt}{1.2}
    \begin{tabular}{ccc}
    \\ \shline
    Setting & XSub & XSet \\\shline
    Pre-defined + fix & 83.1 & 84.6 \\ 
    Pre-defined + refine & 83.8 & 85.4 \\ 
    From scratch & 83.6 & 85.3 \\ \shline
    \end{tabular}}
    \vspace{-9mm}
\end{wraptable} 

\vspace{-3mm}
\subsubsection{Preliminary: Is a pre-defined topology indispensable? }
We first conduct pilot experiments to find out how important a prescribed graphical structure is for spatial modeling. 
We consider three alternatives: 1). a fixed topology~\cite{shi2019two} which is pre-defined; 2). \textbf{1} with a static learnable refinement; 
3). a topology learned from scratch, randomly initialized at the beginning.
Table~\ref{tab-prelim} shows that a learnable refinement largely improves the recognition performance. 
However, removing the pre-defined topology only leads to a moderate drop ( 0.2\%). 
Following Occam's Razor, we do not use any prescribed graphical structure in the following experiments.
This opens a great design space for spatial GCNs.

\vspace{-3mm}
\subsubsection{Dynamic Group GCN (DG-GCN). } 
Thanks to the from-scratch learning of skeleton graphical structures, it is now possible to use an arbitrary number of coefficient matrices for spatial modeling. 
To promote the flexibility of the GCN module, we propose DG-GCN, which divides skeleton features into multiple groups and uses a specific coefficient matrix  for spatial modeling in each group. 
This section presents the ablation study on the number of groups and the different choices of components in .  

\begin{table*}[t]
	\centering
	\captionsetup{font=small}
	\captionsetup[subfloat]{font=small, position=bottom}
\caption{\small \textbf{Ablation study on different choices for DG-GCN design.}}
    \vspace{1mm}
	\label{tab-dggcn}
	\resizebox{\linewidth}{!}{
	\subfloat[\textbf{Ablation on the number of groups.} \label{tab-dggcn-groups}]{
		\tablestyle{6pt}{1.3}
		\begin{tabular}{ccc}
            \multicolumn{3}{c}{} \\ \shline
			 & XSub & XSet \\ \shline
			4 & 83.7 & 85.2 \\ 
			8 & \textbf{84.1} & \textbf{86.0} \\ 
			12 & 83.9 & 86.0 \\ \shline
		\end{tabular}} 
	\hspace{2mm}
	\subfloat[\textbf{Ablation on each individual component in . } 
		\label{tab-dggcn-individual}]{
		\tablestyle{8pt}{1.3}
			\begin{tabular}{ccc} 
                \multicolumn{3}{c}{} \\
				\shline
				Comp. & XSub & XSet\\ \shline
				 & \textbf{84.1} & \textbf{86.0} \\
				 & 82.3 & 84.8 \\
				 & 83.4 & 85.0 \\  \shline
			\end{tabular}}
	\hspace{2mm}
	\subfloat[\textbf{Ablation on different combinations of components in .}  
	\label{tab-dggcn-combination}]{
		\tablestyle{4pt}{1.3}
			\begin{tabular}{ccc} 
				\shline
				Comps. & XSub & XSet \\ \shline
				 & 84.1 & 86.0 \\
				 & 84.2 & 86.2 \\
				 & 84.8 & 86.7 \\  
                 & \textbf{85.1} & \textbf{87.2} \\ \shline
			\end{tabular}}}
    \vspace{-13mm}
\end{table*}

\noindent
\textbf{Ablation on the group number. }
We first explore using different values for the hyper-parameter , which is the number of groups in DG-GCN.
Since the channel width of each group is proportional to , adjusting the value of  does not affect either the parameter size or the computation amounts consumed.
We gradually increase  from 4 to 12 and list the recognition performance in Table~\ref{tab-dggcn-groups}. 
More groups improve the flexibility of DG-GCN and benefit the skeleton spatial modeling. 
Empirically, we find that eight groups lead to a reasonably good result (adding more groups does not help a lot).
Thus we use eight groups in the following experiments. 

\noindent
\textbf{Ablation on the topology components. }
As a network parameter, the learned from-scratch topology is a static component () in the final coefficient matrices. 
In DG-GCN, we also incorporate two dynamic terms, namely  (channel-agnostic) and  (channel-specific). 
Two dynamic components share the same encoding layers (two  convolutions), 
which means all models with dynamic components share a similar parameter size (1.69M) and FLOPs (1.65G). 
Compared to using only the static component (1.25M params, 1.63G FLOPs), 
using dynamic terms increases the parameter size by , while the computation consumed almost remains the same. 
We first evaluate each component individually and show the recognition performance in Table~\ref{tab-dggcn-individual}. 
Dynamic terms used alone cause severe degradation in recognition performance compared to the static term , 
which partially explains why previous purely dynamic approaches (like SGN~\cite{zhang2020semantics}) can not beat the state-of-the-art. 
We further investigate the effects of different combinations of terms and present the results in Table~\ref{tab-dggcn-combination}. 
For spatial modeling, the channel-specific term  plays a more critical role than the channel-agnostic term . 
Combining three components leads to the best recognition results.

\noindent
\textbf{How much data does DG-GCN require? }  
Learning joint relationships solely from data without any prior knowledge, 
we wonder how much data DG-GCN requires to perform well. 
Thus we conduct a series of experiments on the NTU120-XSub split, changing the amounts of training data to see what happens. 
The data amount we use covers a wide range: from  of the original training set to the full scale. 
Figure~\ref{fig-dataamount} compares the three spatial modeling strategies with different amounts of training data.  
A pre-defined fixed topology performs worst across all experiments. 
When the training set is of small scale (\emph{i.e.,} smaller than  of NTU120-XSub training set), 
a manually defined topology is beneficial to the spatial modeling of skeleton data.
However, as the training set grows, the advantage of DG-GCN gradually gets larger. 
Experiment results demonstrate that the potential of DG-GCN is promising if trained on large-scale data. 

\noindent
\begin{table*}[t]
    \centering
	\captionsetup{font=small}
    \begin{minipage}[b]{.53\linewidth}
        \centering
        \includegraphics[width=.93\linewidth]{figs/data_amount}
        \vspace{1mm}
		\captionof{figure}{\textbf{Spatial modeling strategies with different amounts of training data. }
		DG-GCN's advantage grows with the data scale. }
		\label{fig-dataamount}
	\end{minipage}
    \hfill
	\begin{minipage}[b]{.44\linewidth}
		\centering
        \resizebox{\linewidth}{!}{
	        \tablestyle{4pt}{1.3}
            \begin{tabular}{cccc}
            \shline
             & Xsub & XSet & GFLOPs\\ \shline
            TCN (K9) & 83.1 & 84.7 & 3.46 \\
            Multi-Group TCN & 83.6 & 85.3 & 1.63 \\ 
            w. JSF (concat) & 83.5 & 86.0 & 1.94 \\ 
            w. JSF (sum) & 83.2 & 85.7 & 1.65\\ 
            w. D-JSF & \textbf{84.0} & \textbf{86.2} & 1.65\\ \shline
            \end{tabular}}
        \vspace{1mm}
		\captionof{table}{\textbf{Ablation on dynamic temporal modeling. } 
            The Multi-Group TCN improves temporal modeling capability with reduced computations, 
            while D-JSF promotes dynamic joint-skeleton fusion with minimal additional cost. }
		\label{tab-temporal}
	\end{minipage}
    \vspace{-8mm}
\end{table*}

\vspace{-10mm}
\subsubsection{Dynamic Group TCN (DG-TCN). }
We conduct ablation experiments to validate our design choice for the temporal module. 
Instead of the vanilla implementation (a single 1D convolution with kernel 9) in ST-GCN, we adopt the DG-TCN for temporal modeling.
DG-TCN consists of multiple branches with different receptive fields for dynamic temporal modeling.
It is also equipped with the dynamic joint-skeleton fusion module (D-JSF) for adaptive fusion between joint motion and skeleton motion. 
Table~\ref{tab-temporal} shows that, besides improving the temporal modeling capability, the multi-group design also substantially reduces the computation cost by using a small channel width for every single branch\footnote{The parameter size is also significantly reduced: 2.99M  1.25M}. 
For the JSF module, besides fusing the skeleton-level feature with joint-level feature dynamically with learnable coefficients, 
we also test two simpler alternatives, \emph{e.g.}, directly concatenating or summing up the two features. 
From Table~\ref{tab-temporal}, we see that D-JSF performs the best among the three alternatives. 
It's worth noting that D-JSF is an efficient module that only consumes 1\% additional computation cost of the backbone model. 

\begin{table}[t]
    \captionsetup{font=small, position=top}
	\centering 
    \caption{\textbf{Validating Uniform Sampling and two alternatives adopted in previous works on multiple backbones. 
     denotes the gain from Uniform Sampling compared to the baseline without temporal augmentation. }}
    \vspace{1mm}
	\label{tab-aug}
	\resizebox{\linewidth}{!}{
	\tablestyle{4pt}{1.4}
    \begin{tabular}{c|cccc|cccc}
    \shline
    & \multicolumn{4}{c|}{NTU120-XSub} & \multicolumn{4}{c}{NTU120-XSet} \\
    Aug & None & Random Crop & Uni-Sample &  & None & Random Crop & Uni-Sample &  \\ \shline
    ST-GCN~\cite{yan2018spatial} & 81.1 & 82.2 & 82.2 & 1.1 & 84.8 & 83.9 & 85.8 & 1.0 \\
    AGCN~\cite{shi2019two} & 81.6 & 82.3 & 82.8 & 1.2 & 84.0 & 84.8 & 85.3 & 1.3 \\
    MS-G3D~\cite{liu2020disentangling} & 83.3 & 83.1 & 84.2 & 0.9 & \textbf{85.7} & 85.0 & 85.8 & 0.1 \\
    CTR-GCN~\cite{chen2021channel} & 82.4 & 83.4 & 83.8 & 1.4 & 85.2 & 85.6 & 85.8 & 0.6 \\
    DG-STGCN & \textbf{83.5} & \textbf{84.0} & \textbf{85.3} & \textbf{1.8} & 85.1 & \textbf{86.5} & \textbf{87.5} & \textbf{2.4} \\ \shline
    \end{tabular}}
    \vspace{-6mm}
\end{table}

\vspace{-5mm}
\subsubsection{Uniform Sampling as Temporal Data Augmentation. }
With the improved network capacity and flexibility, DG-STGCN requires stronger data augmentations to work well.
Inspired by \cite{duan2021revisiting}, we propose to use Uniform Sampling as a temporal data augmentation strategy, which samples subsequences from skeleton data to form training samples. 
In Table~\ref{tab-aug}, we compare Uniform Sampling with two alternative practices adopted in previous works: 
1). No temporal augmentation~\cite{liu2020disentangling,shi2019two}: circularly padding a skeleton sequence to a maximum length (300 for NTU); 
2). Random Cropping~\cite{chen2021channel}: cropping a substring from the skeleton sequence and interpolating it to the given length 64. 
We find that Uniform Sampling is a \textbf{general} data augmentation strategy, leading to steady improvement across all backbones and benchmarks we tested. 
Specifically, the dynamic DG-STGCN obtains the most significant gain with the strong data augmentation.

\vspace{-3mm}
\subsection{Comparisons with the State-of-the-Art. }
For comparisons with state-of-the-art approaches, we adopt a longer training schedule. 
We train each model with the CosineAnnealing scheduler for 150 epochs. 
We use Uniform Sampling to form each training sample and set the input length to 100 by default.
For Toyota SmartHome, which consists of long sequences up to thousands of frames, we set the input length to 200.
We report the Top-1 accuracy for NTURGB+D and Kinetics-400 and the mean per-class accuracy for BABEL and Toyota SmartHome (two highly unbalanced datasets).

\begin{table}[t]
    \captionsetup{font=small, position=top}
	\centering 
    \caption{\textbf{Classification accuracy comparisons with state-of-the-art methods on NTURGB+D and Kinetics-400. The * notation means using 2D pose estimation results provided by \cite{duan2021revisiting}. }}
    \vspace{1mm}
	\label{tab-sota}
	\resizebox{\linewidth}{!}{
	\tablestyle{6pt}{1.2}
    \begin{tabular}{c|ccccc}
    \shline
    & NTU60-XSub & NTU60-XView & NTU120-XSub & NTU120-XSet & Kinetics \\ \shline
    ST-GCN~\cite{yan2018spatial}    & 81.5  & 88.3  & 70.7  & 73.2  & 30.7 \\
    SGN~\cite{zhang2020semantics}   & 86.6  & 93.4  & -     & -     & -    \\
    AS-GCN~\cite{li2019actional}    & 86.8  & 94.2  & 78.3  & 79.8  & 34.8 \\
    RA-GCN~\cite{song2020richly}    & 87.3  & 93.6  & 78.3  & 79.8  & 34.8 \\
    AGCN~\cite{shi2019two}  & 88.5  & 95.1  & -     & -     & 36.1 \\
    DGNN~\cite{shi2019skeleton} & 89.9  & 96.1  & -     & -     & 36.9 \\
    FGCN~\cite{yang2021feedback}    & 90.2  & 96.3  & 85.4  & 87.4  & - \\
    ShiftGCN~\cite{cheng2020skeleton}   & 90.7  & 96.5  & 85.9  & 87.6  & - \\
    DSTA-Net~\cite{shi2020decoupled}    & 91.5  & 96.4  & 86.6  & 89.0  & - \\
    MS-G3D~\cite{liu2020disentangling}  & 91.5  & 96.2  & 86.9  & 88.4  & 38.0 \\
    CTR-GCN~\cite{chen2021channel}  & 92.4  & 96.8  & 88.9  & 90.6  & - \\ 
    ST-GCN++~\cite{duan2022pyskl} & 92.6 & 97.4 & 88.6 & 90.8 & \\ \shline
    2s DG-STGCN & 92.9  & 97.3  & 89.2  & 91.2  & 39.5 \\
    DG-STGCN & \textbf{93.2}  & \textbf{97.5}  & \textbf{89.6}  & \textbf{91.4}  & \textbf{40.3} \\ \shline
    & NTU60-XSub* & NTU60-XView* & NTU120-XSub* & NTU120-XSet* &  \\ \shline
    MS-G3D++~\cite{liu2020disentangling} & 92.2 & 96.6 & 87.2 & 89.0 & \\ 
    PoseC3D~\cite{duan2021revisiting} & 94.1 & 97.1 & 86.9 & 90.3 & \\
    ST-GCN++~\cite{duan2022pyskl} & 93.2 & 98.5 & 86.4 & 90.3 & \\ \shline
    2s DG-STGCN & 93.9 & 98.5 & 87.3 & 91.1 & \\
    DG-STGCN & \textbf{94.1} & \textbf{98.6} & \textbf{87.5} & \textbf{91.3} & \\ \shline
    \end{tabular}}
    \vspace{-5mm}
\end{table}


\vspace{-3mm}
\subsubsection{Results on NTURGB+D and Kinetics-400. }

For NTURGB+D and Kinetics-400 experiments, we follow the widely used practice~\cite{chen2021channel,shi2020skeleton} to train models 
on four skeleton modalities (joint, bone, joint motion, and bone motion) and report the recognition performance of the ensemble for a fair comparison.
We compare our models with the state-of-the-art methods on NTURGB+D and Kinetics-400 in Table~\ref{tab-sota} (upper).
Our DG-STGCN outperforms \textbf{all} existing methods across four NTURGB+D benchmarks. 
Specifically, with joint\texttt{+}bone fusion (2s) only, 
our approach outperforms the current state-of-the-art CTR-GCN using four modality fusion with a notable margin. 
Fusing predictions of four skeleton modalities, we significantly improve the accuracy on four NTURGB+D benchmarks by nearly 1\%. 
On Kinetics-400, DG-STGCN surpasses the state-of-the-art MS-G3D by 2.3\% in Top-1 Acc. 
To demonstrate its generality, we also train DG-STGCN with high-quality 2D estimated skeleton~\cite{duan2021revisiting}. 
Results in Table~\ref{tab-sota} (lower) show that while consuming almost  less computation, DG-STGCN achieves better performance than the 3D-CNN-based PoseC3D. 


\begin{table}[t]
    \captionsetup{font=small, position=top}
	\centering 
    \begin{minipage}{.45\linewidth}
        \caption{\textbf{Results on BABEL.} We report both bone-only and joint\texttt{+}bone fusion (2s) performance for DG-STGCN. }
        \vspace{1mm}
        \centering
        \resizebox{.98\linewidth}{!}{
	        \tablestyle{4pt}{1.4}
            \begin{tabular}{ccc}
            \shline
            BABEL60 & Mean Acc & Top-1 \\ \shline
            2s-AGCN~\cite{shi2019two} & 30.4 & 33.4 \\ 
            DG-STGCN (Bone) & 36.2 & 39.0 \\ 
            2s DG-STGCN & \textbf{38.1} & \textbf{40.0} \\ \shline
            BABEL120 & Mean Acc & Top-1 \\ \shline
            2s-AGCN~\cite{shi2019two} & 26.2 & 27.9 \\
            DG-STGCN (Bone) & 32.3 & 31.3 \\ 
            2s DG-STGCN & \textbf{33.7} & \textbf{32.8} \\ \shline
            \end{tabular}}
    \end{minipage}\hfill
    \begin{minipage}{.53\linewidth}
        \caption{\textbf{Results on Toyota SmartHome.} 
        \texttt{++} indicates using high-quality 2D Pose. 
        * denotes being pre-trained on another dataset.}
        \vspace{1mm}
        \centering
        \resizebox{\linewidth}{!}{
	        \tablestyle{8pt}{1.3}
            \begin{tabular}{ccc}
            \shline
             & CS & CV1 \\ \shline
            LSTM~\cite{mahasseni2016regularizingls} & 42.5 & 13.4 \\ 
            MS-AAGCN~\cite{shi2019skeleton} & 56.5 & - \\
            2s-AGCN\texttt{++} ~\cite{shi2019two} & 57.1 & 22.1 \\
            2s-AGCN\texttt{++} w. PRS~\cite{yang2021selectivesa} & 60.9 & 22.5 \\
            2s-UNIK\texttt{++}* w. PRS~\cite{yang2021unikau} & 64.3 & \textbf{36.1} \\
            2s DG-STGCN & \textbf{64.8} & 23.2 \\ 
            2s DG-STGCN* & \textbf{65.1} & \textbf{41.8} \\ \shline
            \end{tabular}}
    \end{minipage} 
    \vspace{-4mm}
\end{table}


\vspace{-1mm}
\subsubsection{Results on BABEL and Toyota SmartHome. }
We also train DG-STGCN on BABEL and Toyota SmartHome to test its generality.
BABEL~\cite{punnakkal2021babel} predicts the 25 joints defined by NTURGB+D based on meshes of AMASS mocap data. 
Thus there exists a distribution shift for joint coordinates compared to the original NTURGB+D. 
Meanwhile, Toyota SmartHome~\cite{Das_2019_ICCV} contains activities of daily living and provide 13 3D joints estimated with LCRNet\texttt{++}~\cite{rogez2019lcr}. 
Following \cite{punnakkal2021babel}, we train DG-STGCN with a class-balanced focal loss on BABEL60 and BABEL120. 
On both benchmarks, DG-STGCN with single bone modality outperforms the 2s-AGCN baseline by around 6\% in mean per-class accuracy. 
With joint\texttt{+}bone fusion, the gap is extended to \%.
On Toyota SmartHome \textbf{CS} split, DG-STGCN trained with 3D keypoints outperform all previous state-of-the-arts 
in skeleton-based action recognition (including entries that take high-quality 2D poses as inputs).
With NTU120-XSet pre-training (with 54K training samples), 
DG-STGCN surpasses UNIK pretrained on Posetics~\cite{yang2021unikau} with 142K training samples on the \textbf{CV1} split. 
The results demonstrate the impressive transferring capability of DG-STGCN:
it works even when the definitions of the joint sets are entirely different for the source dataset and the target dataset 
(25 joints in NTURGB+D \emph{vs.} 13 joints in Toyota SmartHome). 
