\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{EMNLP2022}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{todonotes} 
\usepackage{CJK}

\title{VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding}


\author{Dou Hu\thanks{{\ \ }This work was done when the author was at Ping An.} ,
        Xiaolong Hou,
        Xiyang Du,
        Mengyuan Zhou, \\
        {\bf Lianxin Jiang, 
        Yang Mo, 
        Xiaofeng Shi } \\
         Institute of Information Engineering, Chinese Academy of Sciences  \\
         School of Cyber Security, University of Chinese Academy of Sciences \\
         Ping An Life Insurance Company of China, Ltd. \\
        \texttt{hudou@iie.ac.cn, \{houxiaolong430, duxiyang037, zhoumengyuan425,} \\ \texttt{jianglianxin769, moyang853, shixiaofeng309\}@pingan.com.cn} \\
}

\begin{document}
\maketitle
\begin{abstract}
Pre-trained language models have achieved promising performance on general benchmarks, but underperform when migrated to a specific domain. Recent works perform pre-training from scratch or continual pre-training on domain corpora. However, in many specific domains, the limited corpus can hardly support obtaining precise representations. To address this issue, we propose a novel Transformer-based language model named VarMAE for domain-adaptive language understanding. Under the masked autoencoding objective, we design a context uncertainty learning module to encode the token's context into a smooth latent distribution. The module can produce diverse and well-formed contextual representations. Experiments on science- and finance-domain NLU tasks demonstrate that VarMAE can be efficiently adapted to new domains with limited resources. 

\end{abstract}
\section{Introduction}
Pre-trained language models (PLMs) have achieved promising performance in natural language understanding (NLU) tasks on standard benchmark datasets \cite{DBLP:conf/emnlp/WangSMHLB18,DBLP:conf/coling/XuHZLCLXSYYTDLS20}.
Most works \cite{DBLP:conf/naacl/DevlinCLT19,DBLP:journals/corr/abs-1907-11692} leverage the Transformer-based pre-train/fine-tune paradigm to learn contextual embedding from large unsupervised corpora. 
Masked autoencoding, also named masked language model in BERT \cite{DBLP:conf/naacl/DevlinCLT19}, is a widely used pre-training objective that randomly masks tokens in a sequence to recover. 
The objective can lead to a deep bidirectional representation of all tokens in a BERT-like architecture. 
However, these models, which are pre-trained on standard corpora (e.g., Wikipedia), tend to underperform when migrated to a specific domain due to the \textit{distribution shift} \cite{DBLP:journals/bioinformatics/LeeYKKKSK20}. 

Recent works perform pre-training from scratch \cite{, DBLP:journals/health/GuTCLULNGP22,yao2022nlp} or continual pre-training \cite{DBLP:conf/acl/GururanganMSLBD20,DBLP:conf/iclr/WuCLLQH22} on large domain-specific corpora.
But in many specific domains (e.g., finance), effective and intact unsupervised data is difficult and costly to collect due to data accessibility, privacy, security, etc.
The limited domain corpus may not support pre-training from scratch \cite{DBLP:conf/emnlp/ZhangRSCFFKRSW20}, and also greatly limit the effect of continual pre-training due to the \textit{distribution shift}.
Besides, some scenarios (i.e., non-industry academics or professionals) have limited access to computing power for training on a massive corpus.
Therefore, how to obtain effective contextualized representations from the limited domain corpus remains a crucial challenge.




Relying on the distributional similarity hypothesis \cite{DBLP:journals/corr/abs-1301-3781}
in linguistics, 
that similar words have similar contexts, 
masked autoencoders (MAEs) leverage co-occurrence between the context of words to learn word representations.
However, when pre-training on the limited corpus, most word representations can only be learned from fewer co-occurrence contexts, leading to sparse word embedding in the semantic space.
Besides, in the reconstruction of masked tokens, it is difficult to perform an accurate point estimation \cite{DBLP:conf/emnlp/LiGLPLZG20}  based on the partially visible context for each word.
That is, the possible context of each token should be diverse.
The limited data only provides restricted context information, which causes MAEs to learn a relatively poor context representation in a specific domain.


To address the above issue, we propose a novel \textbf{Var}iational \textbf{M}asked \textbf{A}uto\textbf{e}ncoder (\textbf{VarMAE}), a regularized version of MAEs, for a better domain-adaptive language understanding. 
Based on the vanilla MAE, we design a context uncertainty learning (CUL) module for learning a precise context representation when pre-training on a
limited corpus.
Specifically, the CUL encodes the token's point-estimate context in the semantic space into a smooth latent distribution.
And then, the module reconstructs the context using feature regularization specified by prior distributions of latent variables.
In this way, latent representations of similar contexts can be close to each other and vice versa \cite{DBLP:conf/emnlp/LiHNBY19}.
Accordingly, we can obtain a smoother space and more structured latent patterns.


We conduct continual pre-training on unsupervised corpora in two domains (science and finance) and then fine-tune on the corresponding downstream NLU tasks.
The results consistently show that VarMAE outperforms representative language models including vanilla pre-trained \cite{DBLP:journals/corr/abs-1907-11692} and continual pre-training methods \cite{DBLP:conf/acl/GururanganMSLBD20}, when adapting to new domains with limited resources.
Moreover, compared with masked autoencoding in MAEs, the objective of VarMAE can produce a more diverse and well-formed context representation.

\section{VarMAE}
In this section, we develop a novel Variational Masked Autoencoder (VarMAE) to improve vanilla MAE for domain-adaptive language understanding. The overall architecture is shown in Figure~\ref{fig:model}. Based on the vanilla MAE, we design a context uncertainty learning (CUL) module for learning a precise context representation when pre-training on a limited corpus.

\subsection{Architecture of Vanilla MAE}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{model.pdf} 
    \caption{The architecture of VarMAE. Based on the vanilla MAE, a CUL module is used to learn diverse and well-formed context representations for all tokens. }
    \label{fig:model}
\end{figure}
\paragraph{Masking}

We randomly mask some percentage of the input tokens and then predict those masked tokens.
Given one input tokens  and  is the sentence length, the model selects a random set of positions (integers between 1 and ) to mask out , where  indicates 15\% of tokens are masked out.
The tokens in the selected positions are replaced with a  token. 
The masked sequence can be denoted as .

\paragraph{Transformer Encoder}
{Vanilla MAE usually} adopts a multi-layer bidirectional Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} as basic encoder like previous pre-training model  \cite{DBLP:journals/corr/abs-1907-11692}.
Transformer can capture the contextual information for each token in the sentence via self-attention mechanism, and generate a sequence of contextual embeddings. Given the masked sentence , the context representation is denoted as .



\paragraph{Language Model Head}
We adopt the language model (LM) head to predict the original token based on the reconstructed representation. The number of output channels of LM head equals the number of input tokens. 
Based on the context representation , the distribution of the masked prediction is estimated by:
   
where  and  denote the weight matrices of one fully-connected layer.  refers to the trainable parameters.
The predicted token can be obtained by , where  denotes the predicted original token.



\subsection{Context Uncertainty Learning} \label{sec:cul}
Due to the flexibility of natural language, one word may have different meanings under different domains.
In many specific domains, the limited corpus can hardly support obtaining precise representations.
To address this, we introduce a context uncertainty learning (CUL) module to learn regularized context representations for all tokens.
These tokens include masked tokens with more noise and unmasked tokens with less noise.
Inspired by variational autoencoders (VAEs) \cite{DBLP:journals/corr/KingmaW13,DBLP:conf/iclr/HigginsMPBGBML17}, 
we use latent variable modeling techniques to quantify the \textit{aleatoric uncertainty}\footnote{The aleatoric uncertainty, or data uncertainty, is the uncertainty that captures noise inherent in the observations.}
\cite{der2009aleatory,abdar2021review} of these tokens.




Let us consider the input token  is generated with an unobserved continuous random variable .
We assume that  is generated from a conditional distribution , where  is generated from an isotropic Gaussian prior distribution 
.
To learn the joint distribution of the observed variable  and its latent variable factors , the optimal objective is to maximize the marginal log-likelihood of  in expectation over the whole distribution of latent factors :


Since masked and unmasked tokens have relatively different noise levels, the functions to quantify the \textit{aleatoric uncertainty} of these two types should be different.
{We take CUL for masked tokens as an example.}
Given each input masked token  and its corresponding context representation ,
the true posterior  is approximated as   due to the distributional similarity hypothesis \cite{DBLP:journals/corr/abs-1301-3781}.
Inspired by \citet{DBLP:journals/corr/KingmaW13},
we assume  takes on an approximate Gaussian form with a diagonal covariance, 
and let the variational approximate posterior
be a multivariate Gaussian with a diagonal covariance structure.
This variational approximate posterior is denoted as :

where  is diagonal covariance,  is the variational parameters.
Both parameters (mean as well as variance) are input-dependent and  predicted by MLP (a fully-connected neural network with a single hidden layer), i.e., , , where  and  refer to the model parameters respectively w.r.t output  and .
Next, we sample a variable  from the approximate posterior , and then feed it into the LM head to predict the original token.

Similarly, CUL for each unmasked token  adopts in a similar way and samples a latent variable  from the variational approximate posterior ,  where  and   are predicted by MLP.


In the implementation, we adopt  with shared parameters to obtain  and .
Conversely, two  with independent parameters are used to obtain 
 and , for  with more noise and  with less noise, respectively.
After that, batch normalization \cite{DBLP:conf/icml/IoffeS15} is applied to avoid the \textit{posterior collapse}\footnote{The posterior collapse, or KL vanishing, is that the decoder in VAE learns to reconstruct data independent of the latent variable , and the KL vanishes to .} \cite{DBLP:conf/acl/ZhuBLMLW20}.
By applying the CUL module, the context representation is not a deterministic point embedding any more, but a stochastic embedding sampled from  in the latent space.
Based on the reconstructed representation, the LM head is adopted to predict the original token.



\subsection{Training Objective}
To learn a smooth space where latent representations of similar contexts are close to each other and vice versa, the objective function is:

where  is a constraint, and 
 is the variational approximate posterior of the true posterior  (see Section~\ref{sec:cul}).
 denotes the KL-divergence term, which serves as the regularization that forces prior distribution  to approach the approximated posterior .
Then, for each input sequence, the loss function is developed as a weighted sum of loss functions for masked tokens  and unmasked tokens .
The weights are normalization factors of masked/unmasked tokens in the current sequence. 

where  and  are trade-off hyper-parameters.
Please see Appendix~\ref{app:loss} for more details.

As the sampling of  is a stochastic process, we use  \textit{re-parameterization} trick \cite{DBLP:journals/corr/KingmaW13} to make it trainable:
    
where  refers to an element-wise product.
Then, KL term    is computed as:



For all tokens, the CUL forces the model to be able to reconstruct the context using feature regularization specified by prior distributions of latent variables.
Under the objective of VarMAE, latent vectors with similar contexts are encouraged to be smoothly organized together.
After the pre-training, we leverage the Transformer encoder and  to fine-tune on downstream tasks.

\begin{table*}[t]
    \centering
      \resizebox{0.9\linewidth}{!}{}
    \caption{Results on 
    science- and finance-domain downstream tasks.
    All compared pre-trained models are fine-tuned on the task dataset.
    For each dataset, we run three random seeds and report the average result of the test sets.
    We report the micro-average F1 score for CLS and TM, entity-level F1 score for NER, and token-level F1 score for SE.
    Best results are highlighted in bold.}
    \label{tab:results}
\end{table*}
\section{Experiments}
We conduct experiments on science- and finance-domain NLU tasks to evaluate our method.

\subsection{Domain Corpus and Downstream Tasks}



\begin{table}[t]
    \centering
          \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c} 
    \hline
      \multicolumn{1}{c|}{\multirow{2}{*}{Corpus Size}}   & \multicolumn{2}{c|}{\multirow{1}{*}{\it Science-domain}}   & \multicolumn{2}{c}{\multirow{1}{*}{\it  Finance-domain}}   \\    \cline{2-5} 
    & \multicolumn{1}{c|}{DAPT}  & \multicolumn{1}{c|}{VarMAE}
    & \multicolumn{1}{c|}{DAPT}  & \multicolumn{1}{c}{VarMAE} \\ \hline
                
        &  76.77       &  77.82                            
        &  59.56    &   62.04                           \\   
     
        & 75.99     &  78.32                
        & 58.10     &  62.30                            \\  \hline 
    \end{tabular}
    }
    \caption{Average results on all downstream tasks against different corpus sizes of pre-training.
     is the corpus size for corresponding domain.}
    \label{tab:pretrain_corpus}
\end{table}




\begin{table}[t]
    \centering
          \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|p{2.6cm}<{\centering}|p{2.6cm}<{\centering}}
    \hline
      \multicolumn{1}{c|}{Masking Ratio}   & \multirow{1}{*}{\it Science-domain}   & \multirow{1}{*}{\it Finance-domain}   \\    \hline 
    5\%                                     &  77.27                            &  58.54                               \\   
    \multirow{1}{*}{15\%}                   &  78.32                            &  62.30                            \\  
    30\%                                    &  76.95                            &  59.12                             \\  \hline
    \end{tabular}
    }
    \caption{Average results of VarMAE on all downstream tasks against different masking ratios of pre-training.
    }
    \label{tab:mask}
\end{table}



\begin{table*}[t]
    \centering
          \resizebox{\linewidth}{!}{
    \begin{tabular}{c|p{0.37\linewidth}|c|c|c|c}
    \hline
      \multicolumn{1}{c|}{\multirow{2}{*}{No.}}  & \multicolumn{1}{c|}{\multirow{2}{*}{Example}}  
      & \multicolumn{1}{c|}{\multirow{2}{*}{Gold}}  & \multicolumn{1}{c|}{Pred.}
      & \multicolumn{1}{c|}{Pred.}  & \multicolumn{1}{c}{Pred.}  \\ 
      & &  \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{(RoBERTa)} 
      & \multicolumn{1}{c|}{(DAPT)} & \multicolumn{1}{c}{(VarMAE)} \\ \hline
      \multirow{1}{*}{1} 
      & {\it Can forearm superficial injury insure accidental injury?}  
      & \multicolumn{1}{c|}{\multirow{2}{*}{ 
      \makecell[c]{
      {\it Accident}\begin{CJK}{UTF8}{gbsn} \footnotesize{{(意外)}}\end{CJK}; \\ {{\it Disease underwriting} \begin{CJK}{UTF8}{gbsn}\footnotesize {(疾病核保)} \end{CJK}}}}
      }
      & \multicolumn{1}{c|}{\multirow{1}{*}{\it Disease underwriting}}
      & \multicolumn{1}{c|}{\multirow{1}{*}{\it Accident}}  
      &  \multicolumn{1}{c}{\multirow{2}{*}{\makecell[c]{{\it Accident}; \\ \it Disease underwriting}}}
      \\ 
      & {\begin{CJK}{UTF8}{gbsn}\footnotesize (前臂浅表损伤是否投保意外保险？) \end{CJK}} 
      &  &  &  \\  \hline
        \multirow{1}{*}{2} 
      & \textit{Medical demands inspire quality care.} 
      &  \multicolumn{1}{c|}{\multirow{2}{*}{
      \makecell[c]{\begin{CJK}{UTF8}{gbsn}{{\it Pension} {\footnotesize (养老)};} \end{CJK} \\
      \begin{CJK}{UTF8}{gbsn}{{\it Risk education} {\footnotesize (风险教育)}} \end{CJK}
      } 
      }}
      &  \multicolumn{1}{c|}{\multirow{1}{*}{\it Pension}}
      &  \multicolumn{1}{c|}{\multirow{1}{*}{\it Pension}}
      &  \multicolumn{1}{c}{\multirow{2}{*}{
      \makecell{{\it Pension}; \\ \it Risk education}}
      } \\
      &   {\begin{CJK}{UTF8}{gbsn}\footnotesize (医疗需求激发品质养老。) \end{CJK}} &  &  &  \\   \hline
    \multirow{1}{*}{3} 
      & \textit{How does high incidence cancer protection calculate the risk insurance?} 
      & \multicolumn{1}{c|}{\multirow{2}{*}{
      \makecell[c]{\begin{CJK}{UTF8}{gbsn}{{\it Critical illness} {\footnotesize (重疾)};} \end{CJK} \\
      \begin{CJK}{UTF8}{gbsn}{{\it Insurance rules} {\footnotesize (投保规则)}} \end{CJK} }
      }}
      &  \multicolumn{1}{c|}{\multirow{1}{*}{\it Insurance rules}}
      &  \multicolumn{1}{c|}{\multirow{1}{*}{\it Insurance rules}}
      & \multicolumn{1}{c}{\multirow{2}{*}{
      \makecell{{\it Critical illness};\\ \it Insurance rules}}}
    \\ 
    &  {\begin{CJK}{UTF8}{gbsn}\footnotesize (高发癌症保障计划如何计算风险保额？) \end{CJK}}   &  &  &   \\  \hline
      \multirow{1}{*}{4} 
      & \textit{What are the features of ABC Comprehensive Care Program? }
        & \multicolumn{1}{c|}{\multirow{2}{*}{
        \makecell{\begin{CJK}{UTF8}{gbsn}{{\it Product introduction} {\footnotesize{(产品介绍)}};} \end{CJK} \\
      \begin{CJK}{UTF8}{gbsn}{{\it Critical illness} {\footnotesize{(重疾)}}} \end{CJK} }
        }}
      & \multicolumn{1}{c|}{\multirow{1}{*}{\it Product introduction}}
      & \multicolumn{1}{c|}{\multirow{1}{*}{\it Product introduction}}
      & \multicolumn{1}{c}{\multirow{1}{*}{\it Product introduction}} \\
    &    {\begin{CJK}{UTF8}{gbsn}\footnotesize (ABC全面呵护计划特色包括什么内容？) \end{CJK}} &  &  &  \\
      \hline
    \end{tabular}
    }
    \caption{
    Case studies in the multi-label topic classification (MTC) task of financial business scenarios. The table shows four examples of spoken dialogues in the test set, their gold labels and predictions by three methods (RoBERTa, DAPT and VarMAE). We translate original Chinese to English version for readers.    }
    \label{tab:case}
\end{table*}


\paragraph{Domain Corpus}
For science domain,  we collect 0.6 million English abstracts (0.1B tokens) of computer science and broad biomedical fields, which are sampled from Semantic Scholar corpus \cite{DBLP:conf/naacl/AmmarGBBCDDEFHK18}.
For finance domain, we collect 2 million cleaned Chinese sentences (0.3B tokens) from finance-related online platforms (e.g., \textit{Sina Finance}\footnote{\url{https://finance.sina.com.cn/}}, \textit{Weixin Official Account Platform}\footnote{\url{https://mp.weixin.qq.com/}}, and \textit{Baidu Zhidao}\footnote{\url{https://zhidao.baidu.com/}}) 
and business scenarios\footnote{\url{https://life.pingan.com/}\label{code-life}}.
The 1 million sentences in this corpus are from finance news, sales/claims cases, product introduction/clauses, and finance encyclopedia entries, while the remaining 1 million sentences are collected from the internal corpus and log data in business scenarios.
\paragraph{Downstream Tasks and Datasets}
We experiment with four categories of NLP downstream tasks: text classification (CLS), named entity recognition (NER), span extraction (SE), and text matching (TM).
For science domain, we choose four public benchmark datasets: ACL-ARC \cite{DBLP:journals/tacl/JurgensKHMJ18} and SciCite \cite{DBLP:conf/naacl/CohanAZC19} for citation intent classification task, JNLPBA \cite{DBLP:conf/bionlp/CollierK04} for bio-entity recognition task, EBM-NLP \cite{DBLP:conf/acl/NenkovaLYMWNP18} for PICO extraction task.
For finance domain, we choose four real-world  financial business datasets\textsuperscript{\ref{code-life}}: OIR for outbound intent recognition task, MTC for multi-label topic classification task, IEE for insurance-entity extraction task, and PSM for pairwise search match task. 
The details of datasets are included in Appendix~\ref{sec:appendix_dataset}.





\subsection{Experimental Setup}
We compare VarMAE with the following baselines:
\textbf{RoBERTa} \cite{DBLP:journals/corr/abs-1907-11692}
is an optimized BERT with a masked autoencoding objective, and is to directly fine-tune on given downstream tasks.
\textbf{TAPT} \cite{DBLP:conf/acl/GururanganMSLBD20} is a continual pre-training model on a task-specific corpus.
\textbf{DAPT} \cite{DBLP:conf/acl/GururanganMSLBD20} is a continual pre-training model on a domain-specific corpus.



Experiments are conducted under PyTorch\footnote{\url{https://pytorch.org/}} and using 2/1 NVIDIA Tesla V100 GPUs with 16GB memory for pre-training/fine-tuning. 
During pre-training, we use 
\textit{roberta-base}\footnote{ \url{https://huggingface.co/}\label{code}} and
\textit{chinese-roberta-wwm-ext}\textsuperscript{\ref{code}} to initialize the model for science (English) and finance domains (Chinese), respectively.
During the pre-training of VarMAE, we freeze the embedding layer and all layers of Transformer encoder to avoid catastrophic forgetting \cite{DBLP:conf/nips/French93,DBLP:conf/emnlp/ArumaeSB20} of previously general learned knowledge. And then we optimize other network parameters (e.g., the LM Head and CUL module) by using Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with the learning rate of . 
The number of epochs is set to 3.
We use gradient accumulation step of 50 to achieve the large batch sizes (i.e., the batch size is 3200). The trade-off co-efficient  is set to 10 for both domains selected from .
For fine-tuning on downstream tasks, most hyperparameters are the same as in pre-training,  except for the following settings due to the limited computation. The batch size is set to 128 for OIR, and 32 for other tasks.
The maximum sequence length is set to 64 for OIR, and 128 for other tasks.
The number of epochs is set to 10.
More details are listed in Appendix~\ref{sec:appendix_exp}.






\subsection{Results and Analysis}
Table~\ref{tab:results} shows the results on science- and finance-domain downstream tasks.
In terms of the average result, VarMAE yields 1.41\% and 3.09\% absolute performance improvements over the best-compared model on science and finance domains, respectively. 
It shows the superiority of domain-adaptive pre-training with context uncertainty learning.
DAPT and TAPT obtain inferior results. 
It indicates that the small domain corpus limits the continual pre-training due to
the \textit{distribution shift}.




We report the average results on all tasks against different corpus sizes of pre-training in Table~\ref{tab:pretrain_corpus}  {(see Appendix~\ref{sec:appendix_corpus} for details)}. 
VarMAE consistently achieves better performance than DAPT even though a third of the corpus is used.
When using full corpus, DPAT's performance decreases but VarMAE's performance increases, which proves our method has a promising ability to adapt to the target domain with a limited corpus.


Table~\ref{tab:mask} shows the average results of VarMAE on all tasks against different masking ratios of pre-training {(see Appendix~\ref{sec:appendix_mask} for details)}. 
Under the default masking strategies\footnote{{
80\% for replacing the target token with  symbol, 10\% for keeping the target token as is, and 10\% for replacing the target token with another random token.}}, 
the best masking rate is 15\%, which is the same as BERT and RoBERTa.



\subsection{Case Study}
As shown in Table~\ref{tab:case}, we randomly choose several samples from the test set in the multi-label topic classification (MTC) task. 

For the first {case}, RoBERTa and DAPT each predict one label correctly. 
It shows that both general and domain language knowledge have a certain effect on the domain-specific task.
However, none of them identify all the tags completely.
This phenomenon reflects that the general or limited continual PLM is not sufficient for the domain-specific task.
For the second and third {cases}, these two comparison methods cannot classify the topic label \textit{Risk education} and \textit{Critical illness}, respectively.
It indicated that they perform an isolated point estimation and have a relatively poor context representation.
Unlike other methods, our VarMAE can encode the token's context into a smooth latent distribution and produce diverse and well-formed contextual representations.
As expected, VarMAE predicts the first three examples correctly with limited resources.

For the last case, all methods fail to predict \textit{Critical illness}. 
We notice that \textit{ABC Comprehensive Care Program} is a product name related to critical illness insurance. Classifying it properly may require some domain-specific structured knowledge. 

\section{Conclusion}

We propose a novel Transformer-based language model named VarMAE for domain-adaptive language understanding with limited resources. A new CUL module is designed to produce a diverse and well-formed context representation. Experiments on science- and finance-domain tasks demonstrate that VarMAE can be efficiently adapted to new domains using a limited corpus. 
Hope that VarMAE can guide future foundational work in this area. 



\section*{Limitations}
All experiments are conducted on a small pre-training corpus  due to the limitation of computational resources. The performance of VarMAE pre-training on a larger corpus  needs to be further studied. 
Besides, VarMAE cannot be directly adapted to downstream natural language generation tasks since our model does not contain a decoder for the generation. 
This will be left as future work. 

\vspace{-0.5mm}
\section*{Acknowledgements}
This research is supported by Ping An Life Insurance. We thank the reviewers for their insightful and constructive comments. 

\vspace{-4mm}
\begin{thebibliography}{59}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Abdar et~al.(2021)Abdar, Pourpanah, Hussain, Rezazadegan, Liu,
  Ghavamzadeh, Fieguth, Cao, Khosravi, Acharya et~al.}]{abdar2021review}
Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li~Liu,
  Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U~Rajendra
  Acharya, et~al. 2021.
\newblock A review of uncertainty quantification in deep learning: Techniques,
  applications and challenges.
\newblock \emph{Information Fusion}, 76:243--297.

\bibitem[{Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jindi, Naumann,
  and McDermott}]{alsentzer2019publicly}
Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di~Jindi, Tristan
  Naumann, and Matthew McDermott. 2019.
\newblock Publicly available clinical bert embeddings.
\newblock In \emph{Proceedings of the 2nd Clinical Natural Language Processing
  Workshop}, pages 72--78.

\bibitem[{Ammar et~al.(2018)Ammar, Groeneveld, Bhagavatula, Beltagy, Crawford,
  Downey, Dunkelberger, Elgohary, Feldman, Ha, Kinney, Kohlmeier, Lo, Murray,
  Ooi, Peters, Power, Skjonsberg, Wang, Wilhelm, Yuan, van Zuylen, and
  Etzioni}]{DBLP:conf/naacl/AmmarGBBCDDEFHK18}
Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz~Beltagy, Miles Crawford,
  Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu~Ha,
  Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu{-}Han Ooi,
  Matthew~E. Peters, Joanna Power, Sam Skjonsberg, Lucy~Lu Wang, Chris Wilhelm,
  Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018.
\newblock Construction of the literature graph in semantic scholar.
\newblock In \emph{{NAACL-HLT} {(3)}}, pages 84--91. Association for
  Computational Linguistics.

\bibitem[{Arumae et~al.(2020)Arumae, Sun, and
  Bhatia}]{DBLP:conf/emnlp/ArumaeSB20}
Kristjan Arumae, Qing Sun, and Parminder Bhatia. 2020.
\newblock An empirical investigation towards efficient multi-domain language
  model pre-training.
\newblock In \emph{{EMNLP} {(1)}}, pages 4854--4864. Association for
  Computational Linguistics.

\bibitem[{Beltagy et~al.(2019)Beltagy, Lo, and
  Cohan}]{DBLP:conf/emnlp/BeltagyLC19}
Iz~Beltagy, Kyle Lo, and Arman Cohan. 2019.
\newblock Scibert: {A} pretrained language model for scientific text.
\newblock In \emph{{EMNLP/IJCNLP} {(1)}}, pages 3613--3618. Association for
  Computational Linguistics.

\bibitem[{Bertsekas(1997)}]{bertsekas1997nonlinear}
Dimitri~P Bertsekas. 1997.
\newblock Nonlinear programming.
\newblock \emph{Journal of the Operational Research Society}, 48(3):334--334.

\bibitem[{Bird et~al.(2008)Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley,
  Radev, and Tan}]{DBLP:conf/lrec/BirdDDGJKLPRT08}
Steven Bird, Robert Dale, Bonnie~J. Dorr, Bryan~R. Gibson, Mark~Thomas Joseph,
  Min{-}Yen Kan, Dongwon Lee, Brett Powley, Dragomir~R. Radev, and Yee~Fan Tan.
  2008.
\newblock The {ACL} anthology reference corpus: {A} reference dataset for
  bibliographic research in computational linguistics.
\newblock In \emph{{LREC}}. European Language Resources Association.

\bibitem[{Boukkouri et~al.(2022)Boukkouri, Ferret, Lavergne, and
  Zweigenbaum}]{DBLP:conf/lrec/BoukkouriFLZ22}
Hicham~El Boukkouri, Olivier Ferret, Thomas Lavergne, and Pierre Zweigenbaum.
  2022.
\newblock Re-train or train from scratch? comparing pre-training strategies of
  {BERT} in the medical domain.
\newblock In \emph{{LREC}}, pages 2626--2633. European Language Resources
  Association.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{DBLP:conf/nips/BrownMRSKDNSSAA20}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}.

\bibitem[{Chalkidis et~al.(2020)Chalkidis, Fergadiotis, Malakasiotis, Aletras,
  and Androutsopoulos}]{DBLP:conf/emnlp/ChalkidisFMAA20}
Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras,
  and Ion Androutsopoulos. 2020.
\newblock {LEGAL-BERT:} "preparing the muppets for court'".
\newblock In \emph{{EMNLP} (Findings)}, volume {EMNLP} 2020 of \emph{Findings
  of {ACL}}, pages 2898--2904. Association for Computational Linguistics.

\bibitem[{Clark et~al.(2020)Clark, Luong, Le, and
  Manning}]{DBLP:conf/iclr/ClarkLLM20}
Kevin Clark, Minh{-}Thang Luong, Quoc~V. Le, and Christopher~D. Manning. 2020.
\newblock {ELECTRA:} pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{{ICLR}}. OpenReview.net.

\bibitem[{Cohan et~al.(2019)Cohan, Ammar, van Zuylen, and
  Cady}]{DBLP:conf/naacl/CohanAZC19}
Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019.
\newblock Structural scaffolds for citation intent classification in scientific
  publications.
\newblock In \emph{{NAACL-HLT} {(1)}}, pages 3586--3596. Association for
  Computational Linguistics.

\bibitem[{Collier and Kim(2004)}]{DBLP:conf/bionlp/CollierK04}
Nigel Collier and Jin{-}Dong Kim. 2004.
\newblock Introduction to the bio-entity recognition task at {JNLPBA}.
\newblock In \emph{NLPBA/BioNLP}.

\bibitem[{Cui et~al.(2021)Cui, Che, Liu, Qin, and
  Yang}]{DBLP:journals/taslp/CuiCLQY21}
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. 2021.
\newblock Pre-training with whole word masking for chinese {BERT}.
\newblock \emph{{IEEE} {ACM} Trans. Audio Speech Lang. Process.},
  29:3504--3514.

\bibitem[{Der~Kiureghian and Ditlevsen(2009)}]{der2009aleatory}
Armen Der~Kiureghian and Ove Ditlevsen. 2009.
\newblock Aleatory or epistemic? does it matter?
\newblock \emph{Structural safety}, 31(2):105--112.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{DBLP:conf/naacl/DevlinCLT19}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{{NAACL-HLT} {(1)}}, pages 4171--4186. Association for
  Computational Linguistics.

\bibitem[{Diao et~al.(2020)Diao, Bai, Song, Zhang, and
  Wang}]{DBLP:conf/emnlp/DiaoBSZW20}
Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. 2020.
\newblock {ZEN:} pre-training chinese text encoder enhanced by n-gram
  representations.
\newblock In \emph{{EMNLP} (Findings)}, volume {EMNLP} 2020 of \emph{Findings
  of {ACL}}, pages 4729--4740. Association for Computational Linguistics.

\bibitem[{French(1993)}]{DBLP:conf/nips/French93}
Robert~M. French. 1993.
\newblock Catastrophic interference in connectionist networks: Can it be
  predicted, can it be prevented?
\newblock In \emph{{NIPS}}, pages 1176--1177. Morgan Kaufmann.

\bibitem[{Gu et~al.(2022)Gu, Tinn, Cheng, Lucas, Usuyama, Liu, Naumann, Gao,
  and Poon}]{DBLP:journals/health/GuTCLULNGP22}
Yu~Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
  Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2022.
\newblock Domain-specific language model pretraining for biomedical natural
  language processing.
\newblock \emph{{ACM} Trans. Comput. Heal.}, 3(1):2:1--2:23.

\bibitem[{Gururangan et~al.(2020)Gururangan, Marasovic, Swayamdipta, Lo,
  Beltagy, Downey, and Smith}]{DBLP:conf/acl/GururanganMSLBD20}
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug
  Downey, and Noah~A. Smith. 2020.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{{ACL}}, pages 8342--8360. Association for Computational
  Linguistics.

\bibitem[{Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot,
  Botvinick, Mohamed, and Lerchner}]{DBLP:conf/iclr/HigginsMPBGBML17}
Irina Higgins, Lo{\"{\i}}c Matthey, Arka Pal, Christopher Burgess, Xavier
  Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{{ICLR} (Poster)}. OpenReview.net.

\bibitem[{Howard and Ruder(2018)}]{DBLP:conf/acl/RuderH18}
Jeremy Howard and Sebastian Ruder. 2018.
\newblock Universal language model fine-tuning for text classification.
\newblock In \emph{{ACL} {(1)}}, pages 328--339. Association for Computational
  Linguistics.

\bibitem[{Hu et~al.(2022)Hu, Mengyuan, Du, Yuan, Zhi, Jiang, Yang, and
  Shi}]{hu2022pali}
Dou Hu, Zhou Mengyuan, Xiyang Du, Mengfei Yuan, Jin Zhi, Lianxin Jiang,
  Mo~Yang, and Xiaofeng Shi. 2022.
\newblock Pali-nlp at semeval-2022 task 4: Discriminative fine-tuning of
  transformers for patronizing and condescending language detection.
\newblock In \emph{Proceedings of the 16th International Workshop on Semantic
  Evaluation (SemEval-2022)}, pages 335--343.

\bibitem[{Hu and Wei(2020)}]{DBLP:conf/seke/HuW20}
Dou Hu and Lingwei Wei. 2020.
\newblock {SLK-NER:} exploiting second-order lexicon knowledge for chinese
  {NER}.
\newblock In \emph{{SEKE}}, pages 413--417. {KSI} Research Inc.

\bibitem[{Huang et~al.(2019)Huang, Altosaar, and
  Ranganath}]{DBLP:journals/corr/abs-1904-05342}
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019.
\newblock Clinicalbert: Modeling clinical notes and predicting hospital
  readmission.
\newblock \emph{CoRR}, abs/1904.05342.

\bibitem[{Ioffe and Szegedy(2015)}]{DBLP:conf/icml/IoffeS15}
Sergey Ioffe and Christian Szegedy. 2015.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{{ICML}}, volume~37 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pages 448--456. JMLR.org.

\bibitem[{Joshi et~al.(2020)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy}]{DBLP:journals/tacl/JoshiCLWZL20}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S. Weld, Luke Zettlemoyer, and
  Omer Levy. 2020.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 8:64--77.

\bibitem[{Jurgens et~al.(2018)Jurgens, Kumar, Hoover, McFarland, and
  Jurafsky}]{DBLP:journals/tacl/JurgensKHMJ18}
David Jurgens, Srijan Kumar, Raine Hoover, Daniel~A. McFarland, and Dan
  Jurafsky. 2018.
\newblock Measuring the evolution of a scientific field through citation
  frames.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 6:391--406.

\bibitem[{Karush(2014)}]{karush2014minima}
William Karush. 2014.
\newblock Minima of functions of several variables with inequalities as side
  conditions.
\newblock In \emph{Traces and Emergence of Nonlinear Programming}, pages
  217--245. Springer.

\bibitem[{Kim et~al.(2003)Kim, Ohta, Tateisi, and Tsujii}]{kim2003genia}
Jin{-}Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun'ichi Tsujii. 2003.
\newblock {GENIA} corpus - a semantically annotated corpus for bio-textmining.
\newblock In \emph{{ISMB} (Supplement of Bioinformatics)}, pages 180--182.

\bibitem[{Kingma and Ba(2015)}]{DBLP:journals/corr/KingmaB14}
Diederik~P. Kingma and Jimmy Ba. 2015.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}.

\bibitem[{Kingma and Welling(2014)}]{DBLP:journals/corr/KingmaW13}
Diederik~P. Kingma and Max Welling. 2014.
\newblock Auto-encoding variational bayes.
\newblock In \emph{{ICLR}}.

\bibitem[{Lee et~al.(2020)Lee, Yoon, Kim, Kim, Kim, So, and
  Kang}]{DBLP:journals/bioinformatics/LeeYKKKSK20}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang. 2020.
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock \emph{Bioinform.}, 36(4):1234--1240.

\bibitem[{Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{DBLP:conf/acl/LewisLGGMLSZ20}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
\newblock {BART:} denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{{ACL}}, pages 7871--7880. Association for Computational
  Linguistics.

\bibitem[{Li et~al.(2019)Li, He, Neubig, Berg{-}Kirkpatrick, and
  Yang}]{DBLP:conf/emnlp/LiHNBY19}
Bohan Li, Junxian He, Graham Neubig, Taylor Berg{-}Kirkpatrick, and Yiming
  Yang. 2019.
\newblock A surprisingly effective fix for deep latent variable modeling of
  text.
\newblock In \emph{{EMNLP/IJCNLP} {(1)}}, pages 3601--3612. Association for
  Computational Linguistics.

\bibitem[{Li et~al.(2020)Li, Gao, Li, Peng, Li, Zhang, and
  Gao}]{DBLP:conf/emnlp/LiGLPLZG20}
Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and
  Jianfeng Gao. 2020.
\newblock Optimus: Organizing sentences via pre-trained modeling of a latent
  space.
\newblock In \emph{{EMNLP} {(1)}}, pages 4678--4699. Association for
  Computational Linguistics.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{DBLP:journals/corr/abs-1907-11692}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{CoRR}, abs/1907.11692.

\bibitem[{Mikolov et~al.(2013{\natexlab{a}})Mikolov, Chen, Corrado, and
  Dean}]{DBLP:journals/corr/abs-1301-3781}
Tom{\'{a}}s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
  2013{\natexlab{a}}.
\newblock Efficient estimation of word representations in vector space.
\newblock In \emph{{ICLR} (Workshop Poster)}.

\bibitem[{Mikolov et~al.(2013{\natexlab{b}})Mikolov, Sutskever, Chen, Corrado,
  and Dean}]{DBLP:conf/nips/MikolovSCCD13}
Tom{\'{a}}s Mikolov, Ilya Sutskever, Kai Chen, Gregory~S. Corrado, and Jeffrey
  Dean. 2013{\natexlab{b}}.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In \emph{{NIPS}}, pages 3111--3119.

\bibitem[{Nye et~al.(2018)Nye, Li, Patel, Yang, Marshall, Nenkova, and
  Wallace}]{DBLP:conf/acl/NenkovaLYMWNP18}
Benjamin~E. Nye, Junyi~Jessy Li, Roma Patel, Yinfei Yang, Iain~James Marshall,
  Ani Nenkova, and Byron~C. Wallace. 2018.
\newblock A corpus with multi-level annotations of patients, interventions and
  outcomes to support language processing for medical literature.
\newblock In \emph{{ACL} {(1)}}, pages 197--207. Association for Computational
  Linguistics.

\bibitem[{Pennington et~al.(2014)Pennington, Socher, and
  Manning}]{DBLP:conf/emnlp/PenningtonSM14}
Jeffrey Pennington, Richard Socher, and Christopher~D. Manning. 2014.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{{EMNLP}}, pages 1532--1543. {ACL}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{DBLP:conf/naacl/PetersNIGCLZ18}
Matthew~E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer. 2018.
\newblock Deep contextualized word representations.
\newblock In \emph{{NAACL-HLT}}, pages 2227--2237. Association for
  Computational Linguistics.

\bibitem[{Qin et~al.(2021)Qin, Lin, Takanobu, Liu, Li, Ji, Huang, Sun, and
  Zhou}]{DBLP:conf/acl/QinLT00JHS020}
Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie
  Huang, Maosong Sun, and Jie Zhou. 2021.
\newblock {ERICA:} improving entity and relation understanding for pre-trained
  language models via contrastive learning.
\newblock In \emph{{ACL/IJCNLP} {(1)}}, pages 3350--3363. Association for
  Computational Linguistics.

\bibitem[{Qin et~al.(2022)Qin, Zhang, Lin, Liu, Li, Sun, and
  Zhou}]{DBLP:conf/acl/QinZLL0SZ22}
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie
  Zhou. 2022.
\newblock {ELLE:} efficient lifelong pre-training for emerging data.
\newblock In \emph{{ACL} (Findings)}, pages 2789--2810. Association for
  Computational Linguistics.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{DBLP:journals/jmlr/RaffelSRLNMZLL20}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:140:1--140:67.

\bibitem[{Sun et~al.(2019)Sun, Wang, Li, Feng, Chen, Zhang, Tian, Zhu, Tian,
  and Wu}]{DBLP:journals/corr/abs-1904-09223}
Yu~Sun, Shuohuan Wang, Yu{-}Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin
  Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019.
\newblock {ERNIE:} enhanced representation through knowledge integration.
\newblock \emph{CoRR}, abs/1904.09223.

\bibitem[{Tai et~al.(2020)Tai, Kung, Dong, Comiter, and
  Kuo}]{DBLP:conf/emnlp/Tai0DCK20}
Wen Tai, H.~T. Kung, Xin Dong, Marcus~Z. Comiter, and Chang{-}Fu Kuo. 2020.
\newblock exbert: Extending pre-trained models with domain-specific vocabulary
  under constrained training resources.
\newblock In \emph{{EMNLP} (Findings)}, volume {EMNLP} 2020 of \emph{Findings
  of {ACL}}, pages 1433--1439. Association for Computational Linguistics.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{DBLP:conf/nips/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{{NIPS}}, pages 5998--6008.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{DBLP:conf/emnlp/WangSMHLB18}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman. 2018.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{BlackboxNLP@EMNLP}, pages 353--355. Association for
  Computational Linguistics.

\bibitem[{Wang et~al.(2020)Wang, Bi, Yan, Wu, Xia, Bao, Peng, and
  Si}]{DBLP:conf/iclr/0225BYWXBPS20}
Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and
  Luo Si. 2020.
\newblock Structbert: Incorporating language structures into pre-training for
  deep language understanding.
\newblock In \emph{{ICLR}}. OpenReview.net.

\bibitem[{Wei et~al.(2020)Wei, Hu, Zhou, Tang, Zhang, Wang, Han, and
  Hu}]{DBLP:conf/pkdd/WeiHZTZWHH20}
Lingwei Wei, Dou Hu, Wei Zhou, Xuehai Tang, Xiaodan Zhang, Xin Wang, Jizhong
  Han, and Songlin Hu. 2020.
\newblock Hierarchical interaction networks with rethinking mechanism for
  document-level sentiment analysis.
\newblock In \emph{{ECML/PKDD} {(3)}}, volume 12459 of \emph{Lecture Notes in
  Computer Science}, pages 633--649. Springer.

\bibitem[{Wu et~al.(2022)Wu, Caccia, Li, Li, Qi, and
  Haffari}]{DBLP:conf/iclr/WuCLLQH22}
Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan{-}Fang Li, Guilin Qi, and
  Gholamreza Haffari. 2022.
\newblock Pretrained language model in continual learning: {A} comparative
  study.
\newblock In \emph{{ICLR}}. OpenReview.net.

\bibitem[{Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian,
  Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou,
  Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and
  Lan}]{DBLP:conf/coling/XuHZLCLXSYYTDLS20}
Liang Xu, Hai Hu, Xuanwei Zhang, Lu~Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai
  Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo~Shi, Yiming
  Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina
  Patterson, Zuoyu Tian, Yiwen Zhang, He~Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng
  Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong
  Lan. 2020.
\newblock {CLUE:} {A} chinese language understanding evaluation benchmark.
\newblock In \emph{{COLING}}, pages 4762--4772. International Committee on
  Computational Linguistics.

\bibitem[{Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le}]{DBLP:conf/nips/YangDYCSL19}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime~G. Carbonell, Ruslan Salakhutdinov,
  and Quoc~V. Le. 2019.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{NeurIPS}, pages 5754--5764.

\bibitem[{Yao et~al.(2022)Yao, Zheng, Yang, and Yang}]{yao2022nlp}
Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2022.
\newblock Nlp from scratch without large-scale pretraining: A simple and
  efficient framework.
\newblock In \emph{International Conference on Machine Learning}, pages
  25438--25451. PMLR.

\bibitem[{Yao et~al.(2021)Yao, Huang, Wang, Dong, and
  Wei}]{DBLP:conf/acl/YaoHWDW21}
Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li~Dong, and Furu Wei. 2021.
\newblock Adapt-and-distill: Developing small, fast and effective pretrained
  language models for domains.
\newblock In \emph{{ACL/IJCNLP} (Findings)}, volume {ACL/IJCNLP} 2021 of
  \emph{Findings of {ACL}}, pages 460--470. Association for Computational
  Linguistics.

\bibitem[{Zhang et~al.(2020)Zhang, Reddy, Sultan, Castelli, Ferritto, Florian,
  Kayi, Roukos, Sil, and Ward}]{DBLP:conf/emnlp/ZhangRSCFFKRSW20}
Rong Zhang, Revanth~Gangi Reddy, Md.~Arafat Sultan, Vittorio Castelli, Anthony
  Ferritto, Radu Florian, Efsun~Sarioglu Kayi, Salim Roukos, Avirup Sil, and
  Todd Ward. 2020.
\newblock Multi-stage pre-training for low-resource domain adaptation.
\newblock In \emph{{EMNLP} {(1)}}, pages 5461--5468. Association for
  Computational Linguistics.

\bibitem[{Zhang et~al.(2019)Zhang, Han, Liu, Jiang, Sun, and
  Liu}]{DBLP:conf/acl/ZhangHLJSL19}
Zhengyan Zhang, Xu~Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019.
\newblock {ERNIE:} enhanced language representation with informative entities.
\newblock In \emph{{ACL} {(1)}}, pages 1441--1451. Association for
  Computational Linguistics.

\bibitem[{Zhu et~al.(2020)Zhu, Bi, Liu, Ma, Li, and
  Wu}]{DBLP:conf/acl/ZhuBLMLW20}
Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. 2020.
\newblock A batch normalized inference network keeps the {KL} vanishing away.
\newblock In \emph{{ACL}}, pages 2636--2649. Association for Computational
  Linguistics.

\end{thebibliography}



\clearpage
\appendix

\section*{Appendix Overview}


In this supplementary material, we provide: 
(i) the related work, 
(ii) objective derivation of the proposed VarMAE,
(iii) detailed description of experimental setups,
(iv) detailed results,  
and (v) our contribution highlights.

\section{Related Work}
\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|l|l|r|r|r|r|c|r|l}
    \hline
    \multicolumn{2}{c|}{Dataset Name}   & \multicolumn{1}{c|}{Task Name}   
                                  &  \multicolumn{1}{c|}{Train}     &  \multicolumn{1}{c|}{Dev}   & \multicolumn{1}{c|}{Test} & \multicolumn{1}{c|}{\# Entities}
                                  &  \multicolumn{1}{c|}{Avg/Min/Max}       
                                  &  \multicolumn{1}{c|}{Class}  
                                  &  \multicolumn{1}{c}{Source} \\ \hline 
    \multirow{4}{*}{\rotatebox{90}{\it Science}} &
    ACL-ARC 
    & Citation Intent Classification    &  	1,688    & 	114	    &  139	    & -& 42/4/224     & 	6	
    &  NLP field \\ 
   & SciCite 
    &	Citation Intent Classification	        & 	7,320    & 	916     & 	1,861   &  -& 34/7/228     & 	3
    & Multiple scientific fields \\
    & JNLPBA 
    & Bio-entity Recognition            &   16,807&	1,739	& 3,856	& 59,963 &27/2/204      &   5
    &	Biomedical field     \\     
    & EBM-NLP     
    & PICO Extraction                   & 27,879 &	7,049 &	2,064  & 77,360& 37/1/278     &   3   
    & Clinical medicine field \\
    \hline
       \multirow{4}{*}{\rotatebox{90}{\it Finance}} 
    &OIR     & Outbound Intent Recognition & 36,885 & 9,195 & 3,251 & - & 16/2/69 & 34 & F1, F2 \\   
   & MTC     & Multi-label Topic Classification  & 66,670 & 2,994 & 4,606 & - & 15/2/203 & 39 
  & F1, F2, F3, F4  
    \\  
   & IEE      & Insurance-entity Extraction           & 19,136  &   4,784  &   19,206  &  13,128 & 21/1/388  &   2 
  & F1, F2  \\  
& \multirow{1}{*}{PSM}     & \multirow{1}{*}{Pairwise Search Match}
                             & \multirow{1}{*}{11,812}     &  \multirow{1}{*}{1,476}  & \multirow{1}{*}{1,477}
                             &  \multirow{1}{*}{-} & 7/2/100; 14/1/134  
                             & \multirow{1}{*}{4} 
                             & \multirow{1}{*}{F1, F2}       \\ 
                            \hline
    \end{tabular}
    }
    \caption{Dataset statistics of science- and finance-domain downstream tasks. Avg, Min, and Max indicate the average, minimum, and maximum length of sentences, respectively. ``Class"  refers to the number of classes. 
    F1, F2, F3 and F4 mean the insurance, sickness, job and legal fields, respectively.
    }
    \label{tab:datasets_sci_fi}
\end{table*}

\begin{table}[t]
    \centering
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{p{0.58\linewidth}|p{0.4\linewidth}<{\centering}}
    \hline
    \multicolumn{1}{c|}{\multirow{1}{*}{\bf Hyperparameter}}
    & \multicolumn{1}{c}{\bf Assignment} \\  \hline
    Number of Epoch                          &      3  \\
    Trade-off Weight                               &      10  \\ 
    Number of Layers                              & 12       \\
    Hidden size                                             & 768        \\
    FFN inner hidden size                              & 3072        \\
    Attention heads                                  &   12       \\
    Attention head size                                &   64      \\
    Dropout                                         &  0.1          \\
    Attention Dropout                             &  0.1        \\
    Peak Learning Rate                         &     \\ 
    Maximum Length                             &  128       \\
    Batch Size                                            &  64       \\
    Gradient Accumulation Steps  & 50  \\
    Optimization Steps      &  \{504, 1830\}      \\ 
    Weight Decay              &  0.0     \\ 
    Adam                &     \\
    Adam          & 0.9        \\
    Adam                                    & 0.98      \\ \hline 
    \end{tabular}
    }
    \caption{Hyperparameters for pre-training on a domain-specific corpus for each domain.
    The optimization steps are 504 and 1830 for science- and finance-domain, respectively.}
    \label{tab:app_params}
\end{table}

\begin{table}[t]
    \centering
     \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{p{0.58\linewidth}|p{0.4\linewidth}<{\centering}}
    \hline
    \multicolumn{1}{c|}{\multirow{1}{*}{\textbf{Hyperparameter}}}   &   \textbf{Assignment}  \\ \hline 
    Number of Epoch         &  10   \\ 
    Maximum Length          &  \{64, 128\}      \\
    Batch Size              &   \{32, 128\}        \\   
    Learning Rate           & \multicolumn{1}{c}{}    \\   
    Dropout                 &   0.1               \\ 
    Weight Decay            &   \multicolumn{1}{c}{0.0} \\   
    Warmup ratio            &   
    \multicolumn{1}{c}{0.06} \\  
    \hline 
    \end{tabular}
    }
    \caption{Hyperparameters for fine-tuning on science- and finance-domain downstream tasks. 
    The maximum sequence length is set to 64 for OIR, and is set to 128 for other tasks.
    The batch size is set to 128 for OIR, and is set to 32 for other tasks.
    }
    \label{tab:app_params_fine}
\end{table}



\subsection{General PLMs}
Traditional works \cite{DBLP:conf/nips/MikolovSCCD13,DBLP:conf/emnlp/PenningtonSM14}
represent the word as a single vector representation, which cannot disambiguate the word senses based on the surrounding context.
Recently, unsupervised pre-training on large-scale corpora significantly improves performance, either for Natural Language Understanding (NLU) \cite{DBLP:conf/naacl/PetersNIGCLZ18,DBLP:conf/naacl/DevlinCLT19,DBLP:journals/taslp/CuiCLQY21} or for Natural Language Generation (NLG) \cite{DBLP:journals/jmlr/RaffelSRLNMZLL20,DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:conf/acl/LewisLGGMLSZ20}. 
Following this trend, considerable progress \cite{DBLP:journals/corr/abs-1907-11692,DBLP:conf/nips/YangDYCSL19,DBLP:conf/iclr/ClarkLLM20,DBLP:journals/tacl/JoshiCLWZL20,DBLP:conf/iclr/0225BYWXBPS20,DBLP:conf/emnlp/DiaoBSZW20} has been made to boost the performance via improving the model architectures or exploring novel pre-training tasks.
Some works \cite{DBLP:journals/corr/abs-1904-09223,DBLP:conf/acl/ZhangHLJSL19,DBLP:conf/acl/QinLT00JHS020} enhance the model by integrating structured knowledge from external knowledge graphs.



Due to the flexibility of natural language, one word may have different meanings under different domains. These methods underperform when migrated to specialized domains. Moreover, simple fine-tuning \cite{DBLP:conf/acl/RuderH18,DBLP:conf/seke/HuW20,DBLP:conf/pkdd/WeiHZTZWHH20,hu2022pali} of PLMs is also not sufficient for domain-specific applications.

\subsection{Domain-adaptive PLMs}
Recent works perform pre-training from scratch \cite{ DBLP:journals/health/GuTCLULNGP22,yao2022nlp} or continual pre-training \cite{alsentzer2019publicly,DBLP:journals/corr/abs-1904-05342,DBLP:journals/bioinformatics/LeeYKKKSK20,DBLP:conf/acl/GururanganMSLBD20,DBLP:conf/iclr/WuCLLQH22,DBLP:conf/acl/QinZLL0SZ22} on domain-specific corpora.

Remarkably, \citet{DBLP:conf/emnlp/BeltagyLC19,DBLP:conf/emnlp/ChalkidisFMAA20} 
explore different strategies to adapt to new domains, including pre-training from scratch and further pre-training.
\citet{DBLP:conf/lrec/BoukkouriFLZ22} find that both of them perform at a similar level when pre-training on a specialized corpus, but the former 
requires more resources. 
\citet{yao2022nlp} jointly optimize the task and language modeling objective from scratch.
\citet{DBLP:conf/emnlp/ZhangRSCFFKRSW20,DBLP:conf/emnlp/Tai0DCK20,DBLP:conf/acl/YaoHWDW21} extend the vocabulary of the LM with domain-specific terms for further gains.
\citet{DBLP:conf/acl/GururanganMSLBD20} show that domain- and task-adaptive pre-training methods can offer gains in specific domains.
\citet{DBLP:conf/acl/QinZLL0SZ22} present an efficient lifelong pre-training method for emerging domain data.



In most specific domains, collecting large-scale corpora is usually inaccessible. 
The limited data makes pre-training from scratch infeasible and restricts the performance of continual pre-training.
Towards this issue, we investigate domain-adaptive language understanding with a limited target corpus, and propose a novel language modeling method named VarMAE.
The method performs a context uncertainty learning module to produce diverse and well-formed contextual representations,
and can be efficiently
adapted to new domains with limited resources.



\section{Derivation of Objective Function} \label{app:loss}
Here, we take the objective for masked tokens as the example to give derivations of the loss function.
The objective for unmasked tokens is similar. For simplifying description, we omit  the superscripts that use to distinguish masked tokens from unmasked tokens.
To learn a smooth space of masked tokens where latent representations of similar contexts are close to each other and vice versa, the objective function is:

where  is a constraint, and
 is the variational approximate posterior of the true posterior  (see Section~\ref{sec:cul}).
 denotes the KL-divergence term, which serves as the regularization that forces the prior distribution  to approach the approximated posterior .

In order to encourage this disentangling property in the inferred \cite{DBLP:conf/iclr/HigginsMPBGBML17},
we introduce a constraint  over  by matching it to a prior .
The objective can be computed as a Lagrangian under the KKT condition \cite{bertsekas1997nonlinear,karush2014minima}.
The above optimization problem with only one inequality constraint is equivalent to maximizing the following equation,

where the KKT multiplier  is the regularization coefficient that constrains the capacity of the latent information channel  and puts implicit independence pressure on the learnt posterior due to the isotropic nature of the Gaussian prior .
Since , the function is further defined as,

where the multiplier  can be considered as a hyperparameter.
 not only encourages more efficient latent encoding but also creates a trade-off between context reconstruction quality and the extent of disentanglement.
We train the model by minimizing the loss  to push up its evidence lower bound.



\begin{table*}[t]
    \centering
      \resizebox{\linewidth}{!}{|\mathcal{D}|/3|\mathcal{D}|/3|\mathcal{D}||\mathcal{D}|}
    \caption{Results of DAPT and VarMAE on all
downstream tasks against different corpus sizes of pre-training.  is the corpus size. 
    For each dataset, we run three random seeds and report the average result of the test sets.
    We report the micro-average F1 score for CLS and TM, entity-level F1 score for NER, and token-level F1 score for SE.
    }
    \label{tab:pretrain_corpus_full}
\end{table*}


\begin{table*}[t]
    \centering
      \resizebox{\linewidth}{!}{}
    \caption{Results of VarMAE on all downstream tasks against different masking ratios of pre-training.
    For each dataset, we run three random seeds and report the average result of the test sets.
    We report the micro-average F1 score for CLS and TM, entity-level F1 score for NER, and token-level F1 score for SE.
    }
    \label{tab:pretrain_mask_full}
\end{table*}



\section{Detailed Experimental Setup} \label{sec:appendix_exper_setups}
\subsection{Datasets of Downstream Tasks}
\label{sec:appendix_dataset}
The statistics of datasets and their corresponding tasks are reported in Table~\ref{tab:datasets_sci_fi}.
\paragraph{Science Domain}
We choose four public benchmark datasets from the science domain.

\textbf{ACL-ARC} \cite{DBLP:journals/tacl/JurgensKHMJ18}
is a dataset of citation intents based on a
sample of papers from the ACL Anthology Reference Corpus \cite{DBLP:conf/lrec/BirdDDGJKLPRT08} in the NLP field. 

\textbf{SciCite} \cite{DBLP:conf/naacl/CohanAZC19} is a dataset of citation intents. 
It provides coarse-grained categories and covers a variety of scientific domains.

\textbf{JNLPBA} \cite{DBLP:conf/bionlp/CollierK04}
is a named entity dataset in the biomedical field and is derived from five superclasses in the GENIA corpus \cite{kim2003genia}.

\textbf{EBM-NLP} \cite{DBLP:conf/acl/NenkovaLYMWNP18} annotates PICO (Participants, Interventions, Comparisons and Outcomes) spans in clinical trial abstracts. The corresponding PICO Extraction task aims to identify the spans in clinical trial abstracts that describe the respective PICO elements.

\paragraph{Finance Domain}
We choose four real-world business datasets\textsuperscript{\ref{code-life}} from the 
financial domain.


\textbf{OIR} is a dataset of the outbound intent recognition task. It aims to identify the intent of customer response in the outbound call scenario. 

\textbf{MTC} is a dataset of the multi-label topic classification task. It aims to identify the topics of the spoken dialogue. 

\textbf{PSM} is a dataset of the pairwise search matching task. It aims to identify the semantic similarity of a sentence pair in the search scenario.


\textbf{IEE} is a dataset of the Insurance-entity extraction task. Its goal is to locate named entities mentioned in the input sentence.

For OIR and MTC, we use an ASR (automatic speech recognition) tool to convert acoustic signals into textual sequences in the pre-processing phase.



\subsection{Implementation Details} \label{sec:appendix_exp}

\subsubsection{Pre-training Hyperparameters} \label{sec:appendix_exp_pretrain}
Table~\ref{tab:app_params} describes the hyperparameters for pre-training on a domain-specific corpus.



\subsubsection{Fine-tuning Hyperparameters}  \label{sec:appendix_exp_fine}
Table~\ref{tab:app_params_fine} reports the fine-tuning hyperparameters for downstream tasks.




\section{Detailed Results} \label{sec:appendix_result}


In this part, we provide detailed results on science- and finance-domain downstream tasks.


\subsection{Results Against Different Corpus Sizes} \label{sec:appendix_corpus}
The detailed results of DAPT and VarMAE on all downstream tasks against different corpus sizes of pre-training are reported in Table~\ref{tab:pretrain_corpus_full}.


\subsection{Results Against Different Masking Ratios} \label{sec:appendix_mask}
{The detailed results of VarMAE on all downstream tasks against different masking ratios of pre-training are reported in Table~\ref{tab:pretrain_mask_full}.}




\section{Contribution and Future Work}
The main contributions of this work are as follows:
\textbf{1)} 
We present a domain-adaptive language modeling method named VarMAE based on the combination of variational autoencoders and masked autoencoders.
\textbf{2)} 
We design a context uncertainty learning module to model the point-estimate context of each token into a smooth latent distribution. 
The module can produce diverse and well-formed contextual representations. 
\textbf{3)}
Extensive experiments on science- and finance-domain NLU tasks demonstrate that VarMAE can be efficiently adapted to new domains with limited resources.

For future works, we will build domain-specific structured knowledge to further assist language understanding, and apply our method for domain-adaptive language generation.

\end{document}
