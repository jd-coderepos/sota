
\documentclass{article} 

\usepackage{iclr2021_conference,times}
\iclrfinalcopy

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{url}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{subcaption}
\usepackage{amssymb,amsmath}\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{subeqnarray}
\usepackage{setspace} \usepackage{graphicx} \usepackage{tcolorbox}
\usepackage{IEEEtrantools}
\usepackage{mathtools}  \usepackage{bm}
\usepackage[lined,commentsnumbered]{algorithm2e}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{amsthm}
\usepackage{chngcntr}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}


\renewcommand{\vec}{\boldsymbol}
\newcommand{\pprob}{\operatorname{p}\probarg}
\DeclarePairedDelimiterX{\probarg}[1]{(}{)}{\ifnum\currentgrouptype=16 \else\begingroup\fi
  \activatebar#1
  \ifnum\currentgrouptype=16 \else\endgroup\fi
}

\newcommand{\qprob}{\operatorname{q}\probargq}
\DeclarePairedDelimiterX{\probargq}[1]{(}{)}{\ifnum\currentgrouptype=16 \else\begingroup\fi
  \activatebar#1
  \ifnum\currentgrouptype=16 \else\endgroup\fi
}
\newcommand{\innermid}{\nonscript\;\delimsize\vert\nonscript\;}
\newcommand{\activatebar}{\begingroup\lccode`\~=`\|
  \lowercase{\endgroup\let~}\innermid
  \mathcode`|=\string"8000
}
\newcommand{\matr}[1]{\mathbf{#1}}
\RequirePackage{xcolor}

\newcommand{\diff}{\mbox{d}\,}


\newcommand{\matrl}[2]{\mathbf{#1}^{(#2)}}



\newcommand{\vecl}[2]{\vec{#1}^{(#2)}}



\newcommand{\kprime}{k^{\prime}}


\newcommand{\scall}[2]{#1^{(#2)}}

\newcommand{\bcdot}{\boldsymbol{\cdot}}

\newcommand{\expt}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\exptt}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\eqdef}{\vcentcolon=}
\newcommand{\kl}[2]{\mathbb{KL}\left[{#1}\parallel#2\right]}


\newcommand{\indicator}[1]{\textbf{1}_{\left[#1\right]}}
\newcommand{\ethantodo}[1]{\textcolor{red}{todo:#1}}
\newcommand{\rowa}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\best}[1]{\textbf{#1}}
\newcommand{\second}[1]{\underline{#1}}

\newcommand{\revised}[1]{\textcolor{blue}{#1}}

\title{Neural Topic Model via Optimal Transport}


\author{He Zhao, Dinh Phung, Viet Huynh, Trung Le, Wray Buntine
\\
Department of Data Science and Artificial Intelligence, 
Faculty of Information Technology\\
Monash University, Australia\\
\texttt{\{ethan.zhao,dinh.phung,viet.huynh,trunglm,wray.buntine\}@monash.edu} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility.
To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss.
Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.
\end{abstract}

\section{Introduction}
As an unsupervised approach, topic modelling has enjoyed great success in   automatic text analysis.
In general, a topic model aims to discover a set of latent topics from a collection of documents, each of which describes an interpretable semantic concept.
Topic models like Latent Dirichlet Allocation (LDA)~\citep{blei2003latent} and its hierarchical/Bayesian extensions, e.g., in~\citet{blei2010nested,paisley2015nested,gan2015learning,zhou2016augmentable} have achieved impressive performance for document analysis.
Recently, the developments of Variational AutoEncoders (VAEs) and Autoencoding Variational Inference (AVI)~\citep{kingma2013auto,rezende2014stochastic}
have facilitated the proposal of Neural Topic Models (NTMs) such as in~\citet{miao2016neural,srivastava2017autoencoding,krishnan2018challenges,burkhardt2019decoupling}.
Inspired by VAE, many NTMs use an encoder that takes the Bag-of-Words (BoW) representation of a document as input and approximates the posterior distribution of the latent topics. The posterior samples are further input into a decoder to reconstruct the BoW representation.
Compared with conventional topic models, NTMs usually enjoy better flexibility and scalability, which are important for the applications on large-scale data.

Despite the promising performance and recent popularity, there are several shortcomings for existing NTMs, which could hinder their usefulness and further extensions.
\textbf{i)} The training and inference processes of NTMs are typically complex due to the prior and posterior constructions of latent topics.
To encourage topic sparsity and smoothness, Dirichlet~\citep{burkhardt2019decoupling} or gamma~\citep{zhang2018whai} distributions are usually used as 
the prior and posterior of topics, but reparameterisation is inapplicable to them, thus, complex sampling schemes or approximations have to be used, which could limit the model flexibility.
\textbf{ii)} A desideratum of a topic model is to generate better topical representations of documents with more coherent and diverse topics; 
but for many existing NTMs, it is hard to achieve good document representation and coherent/diverse topics at the same time.
This is because the objective of NTMs is to achieve lower reconstruction error, which usually means topics are less coherent and diverse, as observed and analysed in~\cite{srivastava2017autoencoding,burkhardt2019decoupling}. 
\textbf{iii)} It is well-known that topic models degrade their performance severely on short documents such as tweets, news headlines and product reviews, as each individual document contains insufficient word co-occurrence information.
This issue can be exacerbated for NTMs because of the use of the encoder and decoder networks, which are usually more vulnerable to data sparsity.

To address the above shortcomings for NTMs, we in this paper propose a neural topic model, which is built upon a novel Optimal Transport (OT) framework derived from a new view of topic modelling.
For a document, we consider its content to be encoded by two representations: the observed representation, $\vec{x}$, a distribution over all the words in the vocabulary and the latent representation, $\vec{z}$, a distribution over all the topics.
$\vec{x}$ can be obtained by normalising a document's word count vector while $\vec{z}$ needs to be learned by a model.
For a document collection, the vocabulary size (i.e., the number of unique words) can be very large but one individual document usually consists of a tiny subset of the words. Therefore, $\vec{x}$ is a sparse and low-level representation of the semantic information of a document.
As the number of topics is much smaller than the vocabulary size, $\vec{z}$ is the relatively dense and high-level representation of the same content.
Therefore, the learning of a topic model can be viewed as the process of learning the distribution $\vec{z}$ to be as close to the distribution $\vec{x}$ as possible.
Accordingly, it is crucial to investigate how to measure the distance between two distributions with different supports (i.e., words to $\vec{x}$ and topics to $\vec{z}$).
As optimal transport is a powerful tool for measuring the distance travelled in transporting the mass in one distribution to match another given a specific cost function,
and recent development on computational OT (e.g., in~\citet{cuturi2013sinkhorn,frogner2015learning,seguy2018large,peyre2019computational}) has shown the promising feasibility to efficiently compute OT for large-scale problems, it is natural for us to develop a new NTM based on the minimisation of OT.

Specifically, our model leverages an encoder that outputs topic distribution $\vec{z}$ of a document by taking its word count vector as input like standard NTMs, but we minimise the OT distance between $\vec{x}$ and $\vec{z}$, which are two discrete distributions on the support of words and topics, respectively. Notably, the cost function of the OT distance specifies the weights between topics and words, which we define as the distance in an embedding space. To represent their semantics, all the topics and words are embedded in this space. By leveraging the pretrained word embeddings, the cost function is then a function of topic embeddings, which will be learned jointly with the encoder. With the advanced properties of OT on modelling geometric structures on spaces of probability distributions, our model is able to achieve a better balance between obtaining good document representation and generating coherent/diverse topics. In addition, our model eases the burden of designing complex sampling schemes for the posterior of NTMs. More interestingly, our model is a natural way of incorporating pretrained word embeddings, which have been demonstrated to alleviate the issue of insufficient word co-occurrence information in short texts~\citep{zhao2017word,dieng2019topic}. With extensive experiments, our model can be shown to enjoy the state-of-the-art performance in terms of both topic quality and document representations for both regular and short texts.





\section{Background}
In this section, we recap the essential background of neural topic models and optimal transport.
\subsection{Neural Topic Models}
\label{sec-ntm}
Most of existing NTMs can be viewed as the extensions of the framework of VAEs where the latent variables can be interpreted as topics.
Suppose the document collection to be analysed has $V$ unique words (i.e., vocabulary size).
Each document consists of a word count vector denoted as $\vec{x} \in \mathbb{N}^V$ and a latent distribution over $K$ topics: $\vec{z} \in \mathbb{R}^K$.
An NTM assumes that $\vec{z}$ for a document is generated from a prior distribution $p(\vec{z})$ and $\vec{x}$ is generated by the conditional distribution $p_{\phi}(\vec{x}|\vec{z})$ that is modelled by a decoder $\phi$.
The model's goal is to infer the topic distribution given the word counts, i.e., to calculate the posterior $p(\vec{z} | \vec{x})$, which is approximated by the variational distribution $q_{\theta}(\vec{z} | \vec{x})$ modelled by an encoder $\theta$. Similar to VAEs, the training objective of NTMs is the maximisation of the Evidence Lower BOund (ELBO):
\begin{equation}
\label{eq-elbo}
\max_{\theta, \phi} \left( \exptt{q_{\theta}(\vec{z} | \vec{x})}{\log p_{\phi}(\vec{x}|\vec{z})} - \kl{q_{\theta}(\vec{z} | \vec{x})}{p(\vec{z})} \right).
\end{equation}

The first term above is the expected log-likelihood or reconstruction error. As $\vec{x}$ is a count-valued vector, it is usually assumed to be generated from the multinomial distribution: $p_\phi(\vec{x}|\vec{z})\eqdef\text{Multi}(\phi(\vec{z}))$, where $\phi(\vec{z})$ is a probability vector output from the decoder. Therefore, the expected log-likelihood is proportional to $\vec{x}^T \log \phi(\vec{z})$. The second term is the Kullbackâ€“Leibler (KL) divergence that regularises $q_{\theta}(\vec{z} | \vec{x})$ to be close to its prior $p(\vec{z})$. To interpret topics with words, $\phi(\vec{z})$ is usually constructed by a single-layer network~\citep{srivastava2017autoencoding}: $\phi(\vec{z}) \eqdef \text{softmax}(\matr{W} \vec{z})$, where $\matr{W} \in \mathbb{R}^{V \times K}$ indicates the weights between topics and words.
Different NTMs may vary in the prior and the posterior of $\vec{z}$, for example, the model in~\citet{miao2017discovering} applies Gaussian distributions for them and \citet{srivastava2017autoencoding,burkhardt2019decoupling} show that Dirichlet is a better choice. However, reparameterisation cannot be directly applied to a Dirichlet, so various approximations and sampling schemes have been proposed.

\subsection{Optimal Transport}
OT distances have been widely used for the comparison of probabilities. Here we limit our discussion to OT for discrete distributions, although it applies for continuous distributions as well.
Specifically, let us consider two probability vectors $\vec{r} \in \Delta^{D_r}$ and $\vec{c} \in \Delta^{D_c}$, where $\Delta^D$ denotes a $D-1$ simplex.
The OT distance\footnote{To be precise, an OT distance becomes a ``distance metric'' in mathematics only if the cost function $\matr{M}$ is induced from a distance metric. We call it ``OT distance'' to assist the readability of our paper.} between the two probability vectors can be defined as:
\vspace{0.01cm}
\begin{equation}
\label{eq-def-ot}
d_{\matr{M}}(\vec{r}, \vec{c}) \eqdef \min_{\matr{P} \in U(\vec{r}, \vec{c})} \langle \matr{P} , \matr{M} \rangle~,
\end{equation}
where $\langle\cdot,\cdot\rangle$ denotes the Frobenius dot-product;
$\matr{M} \in \mathbb{R}_{\ge 0}^{D_r \times D_c}$ is the cost matrix/function of the transport;
$\matr{P} \in \mathbb{R}_{>0}^{D_r \times D_c}$ is the transport matrix/plan;
$U(\vec{r}, \vec{c})$ denotes the transport polytope of $\vec{r}$ and $\vec{c}$, which is the polyhedral set of $D_r \times D_c$ matrices:
$U(\vec{r}, \vec{c}) \eqdef \{P \in \mathbb{R}^{D_r \times D_c}_{>0} | P \boldsymbol{1}_{D_c} = \vec{r}, P^T \boldsymbol{1}_{D_r} = \vec{c}\}$;
and $\boldsymbol{1}_{D}$ is the $D$ dimensional vector of ones.
Intuitively, if we consider two discrete random variables $X \sim \text{Categorical}(\vec{r})$ and $Y \sim \text{Categorical}(\vec{c})$, the transport matrix $\matr{P}$ is a joint probability of $(X, Y)$, i.e., $\pprob{X=i, Y=j} = p_{ij}$ and $U(\vec{r}, \vec{c})$ is the set of all the joint probabilities. The above optimal transport distance can be computed by finding the optimal transport matrix $\matr{P}^*$.
It is also noteworthy that the Wasserstein distance can be viewed as a specific case of the OT distances.

As directly optimising Eq.~(\ref{eq-def-ot}) can be time-consuming for large-scale problems, a regularised optimal transport distance with an entropic constraint is introduced in~\cite{cuturi2013sinkhorn}, named the Sinkhorn distance:
\begin{equation}
\label{eq-def-sh}
d_{\matr{M},\alpha}(\vec{r}, \vec{c}) \eqdef \min_{\matr{P} \in U_{\alpha}(\vec{r}, \vec{c})} \langle \matr{P} , \matr{M} \rangle~,
\end{equation}
where $U_{\alpha}(\vec{r}, \vec{c}) \eqdef \{\matr{P} \in U(\vec{r}, \vec{c}) | h(\matr{P}) \ge h(\vec{r}) + h(\vec{c}) -\alpha$\}, $h(\cdot)$ is the entropy function, 
and $\alpha \in [0, \infty)$.
To compute the Sinkhorn distance, a Lagrange multiplier is introduced for the entropy constraint to minimise Eq.~(\ref{eq-def-sh}), resulting in the Sinkhorn algorithm, widely-used for discrete OT problems.

\section{Proposed Model}
\label{sec-nstm}
Now we introduce the details of our proposed model.
Specifically, we present each document as a distribution over $V$ words, $\tilde{\vec{x}} \in \Delta^V$ obtained by normalising $\vec{x}$:
 $\tilde{\vec{x}} \eqdef \vec{x}/ S$ where $S \eqdef \sum_{v=1}^V \vec{x}$ is the length of a document.
Also, each document is associated with a distribution over $K$ topics: $\vec{z} \in \Delta^K$, each entry of which indicates the proportion of one topic in this document.
Like other NTMs, we leverage an encoder to generate $\vec{z}$ from $\tilde{\vec{x}}$: $\vec{z} = \text{softmax}(\theta(\tilde{\vec{x}}))$. Notably, $\theta$ is implemented with a neural network with dropout layers for adding randomness.
As $\tilde{\vec{x}}$ and $\vec{z}$ are two distributions with different supports for the same document, to learn the encoder, we propose to minimise the following OT distance to push $\vec{z}$ towards $\tilde{\vec{x}}$:
\begin{equation}
\label{eq-nstm-1}
\min_{\theta} d_{\matr{M}}(\tilde{\vec{x}}, \vec{z})~.
\end{equation}
Here $\matr{M} \in \mathbb{R}_{>0}^{V \times K}$ is the cost matrix,
where $m_{vk}$ indicates the semantic distance between topic $k$ and word $v$. Therefore, each column of $\matr{M}$ captures the importance of the words in the corresponding topic.
In addition to the encoder, $\matr{M}$ is a variable that needs to be learned in our model. However, learning the cost function is reported to be a non-trivial task~\citep{cuturi2014ground,sun2020learning}. To address this problem, we specify the following construction of $\matr{M}$:
\begin{equation}
\label{eq-m}
m_{vk} = 1 - \text{cos}(\vec{e}_v, \vec{g}_k)~,
\end{equation} where $\text{cos}(\bcdot, \bcdot) \in [-1,1]$ is the cosine similarity; $\vec{g}_k \in \mathbb{R}^{L}$ and $\vec{e}_v \in \mathbb{R}^L$ are the embeddings of topic $k$ and word $v$, respectively. 

The embeddings are expected to capture the semantic information of the topics and words.
Instead of learning the word embeddings, we propose to feed them with pretrained word embeddings such as word2vec~\citep{mikolov2013efficient} and GloVe~\citep{pennington2014glove}. This not only reduces the parameter space to make the learning of $\matr{M}$ more stable but also enables us to leverage the rich semantic information in pretrained word embeddings, which is beneficial for short documents.
Here the cosine distance instead of others is used for two reasons: it is the most commonly-used distance metric for word embeddings and the cost matrix  $\matr{M}$ is positive thus the similarity metric requires to be upper-bounded. As cosine similarity falls in the range of $[-1, 1]$, we have $\matr{M} \in [0,2]^{V \times K}$.


For easy presentation, we denote $\matr{G} \in \mathbb{R}^{L \times K}$ and $\matr{E} \in \mathbb{R}^{L \times V}$ as the collection of the embeddings of all topics and words, respectively. Now we can rewrite Eq.~(\ref{eq-nstm-1}) as:
\begin{equation}
\label{eq-nstm-2}
\min_{\theta, \matr{G}} d_{\matr{M}}(\tilde{\vec{x}}, \vec{z})~.
\end{equation}


Although the mechanisms are totally different, both $\matr{M}$ of our model and $\matr{W}$ in NTMs (See Section~\ref{sec-ntm}) capture the relations between topics and words ($\matr{M}$ is distance while $\matr{W}$ is similarity).
Here $\matr{M}$ is the cost function of our OT loss while $\matr{W}$ is the weights in the decoder of NTMs.
Different from other NTMs based on VAEs, our model does not explicitly has a decoder to project $\vec{z}$ back to the word space to reconstruct $\vec{x}$, as the OT distance facilitates us to compute the distance between $\vec{z}$ and $\tilde{\vec{x}}$ directly.
To further understand our model, we can actually project $\vec{z}$ to the space of $\vec{x}$ by ``virtually'' defining a decoder: $\phi(\vec{z}) \eqdef \text{softmax}((2 - \matr{M}) \vec{z})$. With the notation of $\phi(\vec{z})$, we show the following theorem to reveal the relationships between other NTMs and ours, whose proof is shown in Section~\ref{a-sec-proof} of the appendix.
\begin{thm}
\label{thm}
When $V\ge8$ and $\matr{M} \in [0,2]^{V \times K}$, we have:
\begin{equation}
d_{\matr{M}}(\tilde{\vec{x}}, \vec{z}) \le - \tilde{\vec{x}}^T \log{\phi(\vec{z})}.
\end{equation} 
\end{thm}

With Theorem~\ref{thm}, we have:
\begin{lemma}
Maximising the expected multinomial log-likelihood of NTMs is equivalent to minimising the upper bound of the OT distance in our model.
\end{lemma}

\citet{frogner2015learning} propose to minimise the OT distance between the predicted and true label distributions for classification tasks.
It is reported in the paper that combining the OT loss with the conventional cross-entropy loss gives better performance on using either of them.
As the expected multinomial log-likelihood is easier to learn and can be helpful to guide the optimisation of the OT distance, empirically inspired by \citet{frogner2015learning} and theoretically motivated by Theorem~\ref{thm}, we propose the following joint loss for our model that combines the OT distance with the expected log-likelihood:
\begin{equation}
\label{eq-final-loss-0}
\max_{\theta, \matr{G}} \left(\tilde{\vec{x}}^T \log \phi(\vec{z}) - d_{\matr{M}}(\tilde{\vec{x}}, \vec{z})\right).
\end{equation}

If we compare the above loss with the ELBO of Eq.~(\ref{eq-elbo}), it can be observed that similar to the KL divergence of NTMs, our OT distance can be viewed as a regularisation term to the expected log-likelihood ($\tilde{\vec{x}}^T \log \phi(\vec{z})\eqdef \frac{1}{S}\vec{x}^T \log \phi(\vec{z})$).
Compared with other NTMs, our model eases the burden of developing the prior/posterior distributions and the associated sampling schemes. 
Moreover, with OT's ability to better modelling geometric structures, our model is able to achieve better performance in terms of both document representation and topic quality.
In addition, the cost function of the OT distance provides a natural way of incorporating pretrained word embeddings, which boosts our model's performance on short documents.

Finally, we replace the OT distance with the Sinkhorn distance~\citep{cuturi2013sinkhorn}, which leads to the final loss function:
\begin{equation}
\label{eq-final-loss}
\max_{\theta, \matr{G}} \left(\epsilon \tilde{\vec{x}}^T \log \phi(\vec{z}) - d_{\matr{M}, \alpha}(\tilde{\vec{x}}, \vec{z})\right).
\end{equation}
where $\vec{z} = \text{softmax}(\theta(\tilde{\vec{x}}))$; $\matr{M}$ is parameterised by $\matr{G}$; $\phi(\vec{z}) \eqdef \text{softmax}((2 - \matr{M}) \vec{z})$; $\vec{x}$ and $\tilde{\vec{x}}$ are the word count vector and its normalisation, respectively; $\epsilon$ is the hyperparameter that controls the weight of the expected likelihood; $\alpha$ is the hyperparameter for the Sinkhorn distance.

\begin{figure}
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Input documents, Pretrained word embeddings $\matr{E}$, Topic number $K$, $\epsilon$, $\alpha$}
\Output{$\theta, \matr{G}$}

Randomly initialise $\theta$ and $\matr{G}$\;
{}
\While{Not converged}
{
	Sample a batch of $B$ input documents $\matr{X}$\;

	Column-wisely normalise $\matr{X}$ to get $\tilde{\matr{X}}$

	Compute $\matr{M}$ with $\matr{G}$ and $\matr{E}$ by Eq.~(\ref{eq-m})\;

  Compute $\matr{Z} = \text{softmax}(\theta(\tilde{\matr{X}}))$\;

  Compute the first term of Eq.~(\ref{eq-final-loss})\;

  \# \textit{Sinkhorn iterations} \#

  $\matr{\Psi_1} = \text{ones}(K, B) / K, \matr{\Psi_2} = \text{ones}(V, B) / V$\;
  $\matr{H} = e^{-\matr{M} / \alpha}$\;
  \While{$\matr{\Psi_1}$ changes or any other relevant stopping criterion}
	{
      $\matr{\Psi_2} = \tilde{\matr{X}} \odot 1 / (\matr{H}  \matr{\Psi_1})$\;
      $\matr{\Psi_1} = \matr{Z} \odot 1 / (\matr{H}^T \matr{\Psi_2})$\;
  }
  Compute the second term of Eq.~(\ref{eq-final-loss}): $d_{\matr{M}, \alpha} = \text{sum}(\matr{\Psi_2}^T (\matr{H} \odot \matr{M}) \matr{\Psi_1})$\;

  Compute the gradients of Eq.~(\ref{eq-final-loss}) in terms of $\theta, \matr{G}$\;

  Update $\theta, \matr{G}$ with the gradients\;
}
\caption{Training algorithm for NSTM. $\matr{X} \in \mathbb{N}^{V \times B}$ and $\matr{Z} \in \mathbb{R}_{>0}^{K \times B}$ consists of the word count vectors and topic distributions for all the documents, respectively; $\odot$ is the element-wise multiplication.}
\label{alg}
\end{algorithm}
 \end{minipage}
\end{figure}

To compute the Sinkhorn distance, we leverage the Sinkhorn algorithm~\citep{cuturi2013sinkhorn}. Accordingly, we name our model \textbf{N}eural \textbf{S}inkhorn \textbf{T}opic \textbf{M}odel (NSTM), whose training algorithm is shown in Algorithm ~\ref{alg}. 
It is noteworthy that the Sinkhorn iterations can be implemented with the tensors of TensorFlow/PyTorch~\citep{patrini2020sinkhorn}.
Therefore, the loss of Eq.~(\ref{eq-final-loss}) is differentiable in terms of  $\theta$ and $\matr{G}$, which can be optimised jointly in one training iteration.
After training the model, we can infer $\vec{z}$ by conducting a forward-pass of the encoder $\theta$ with the input $\tilde{\vec{x}}$. In practice, $\vec{x}$ can be normalised by other methods e.g., softmax or one can use TF-IDF as the input data of the encoder.



\section{Related works}


We first consider NTMs (e.g. in \cite{miao2016neural,srivastava2017autoencoding,krishnan2018challenges,card2018neural,burkhardt2019decoupling, dieng2019topic} reviewed in Section~\ref{sec-ntm} as the closest line of related works to ours.
For a detailed survey of NTMs, we refer to~\cite{zhao2021topic}. Connections and comparisons between our model and NTMs have been discussed in Section~\ref{sec-nstm}.
In addition, word embeddings have been recently widely-used as complementary metadata for topic models, especially for modelling short texts. For Bayesian probabilistic topic models, word embeddings are usually incorporated into the generative process of word counts, such as in~\cite{petterson2010word,nguyen2015improving,li2016topic,zhao2017word}.
Due to the flexibility of NTMs, word embeddings can be incorporated as part of the encoder input, such as in~\citet{card2018neural} or they can be used in the generative process of words such as in~\citet{dieng2019topic}.
Our novelty with NSTM is that word embeddings are naturally incorporated in the cost function of the OT distance.

To our knowledge, the works that connect topic modelling with OT are still very limited.
In \cite{yurochkin2019hierarchical} authors proposed to compare two documents' similarity with the OT distance between their topic distributions extracted from a pretrained LDA, but the aim is not to learn a topic model.
Another recent work related to ours is Wasserstein LDA (WLDA)~\citep{nan2019topic}, which adapts the framework of Wasserstein AutoEncoders (WAEs)~\citep{tolstikhin2017wasserstein}.
The key difference from ours is that WLDA minimises the Wasserstein distance between the fake data generated with topics and real data, which can be viewed as an OT variant to VAE-NTMs.
However, our NSTM directly minimises the OT distance between $\vec{z}$ and $\vec{x}$, where there are no explicit generative processes from topics to data. Other two related works are Distilled Wasserstein Learning (DWL)~\citep{xu2018distilled} and Optimal Transport LDA (OTLDA)~\citep{huynh2020otlda}, which adapt the idea of Wasserstein barycentres and Wasserstein Dictionary Learning~\citep{rolet2016fast,schmitz2018wasserstein}. 
There are fundamental differences of ours from DWL and OTLDA in terms of the relations between documents, topics, and words. Specifically, 
in DWL and OTLDA, documents and topics locate in one space of words (i.e., both are distributions over words) and $\vec{x}$ can be approximated with the weighted Wasserstein barycentres of all the topic-word distributions, where the weights can be interpreted as the topic proportions of the document, i.e., $\vec{z}$.
However, in NSTM, a document locates in both the topic space and the word space and topics and words are embedded in the embedding space.
These differences lead to different views of topic modelling and different frameworks as well. Moreover, DWL mainly focuses on learning word embeddings and representations for International Classification of Diseases (ICD) codes, while NSTM aims to be a general method of topic modelling. Finally, DWL and OTLDA are not neural network models while ours is.





\section{Experiments}

We conduct extensive experiments on several benchmark text datasets to evaluate the performance of NSTM against the state-of-the-art neural topic models.

\subsection{Experimental settings}
\textbf{Datasets:~}
Our experiments are conducted on five widely-used benchmark text datasets, varying in different sizes, including 20 News Groups (\textbf{20NG})\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}, Web Snippets (\textbf{WS})~\citep{phan2008learning}, Tag My News (\textbf{TMN})~\citep{vitale2012classification}\footnote{\url{http: //acube.di.unipi.it/tmn-dataset/}}, \textbf{Reuters} extracted from the Reuters-21578 dataset\footnote{\url{https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html}}, Reuters Corpus Volume 2 (\textbf{RCV2})~\citep{lewis2004rcv1}\footnote{\url{https://trec.nist.gov/data/reuters/reuters.html}}. The statistics of the datasets in the experiments are shown in Table~\ref{tb-data}. In particular, WS and TMN are short documents; 20NG, WS, and TMN are associated with document labels\footnote{We do not consider the labels of Reuters and RCV2 as there are multiple labels for one document.}. 

\begin{table}[]
\rowa{1.3}
\centering
\caption{Statistics of the datasets}
\label{tb-data}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}ccccccc@{}} \toprule
           & Number of docs & Vocabulary size (V) & Total number of words & Number of labels \\ \cmidrule{2-5} 
20NG       & 18,846         & 22,636              & 2,037,671       & 20      \\
WS & 12,337         & 10,052             & 192,483        &    8    \\
TMN  & 32,597         & 13,368              & 592,973       &   7      \\
Reuters    & 11,367         & 8,817               & 836,397     & N/A         \\
RCV2       & 804,414        & 7,282               & 60,209,009  &  N/A        \\
\bottomrule 
\end{tabular}
}
\end{table}


\textbf{Evaluation metrics:~}
We report Topic Coherence (\textbf{TC}) and Topic Diversity (\textbf{TD}) as performance metrics for topic quality.
TC measures the semantic coherence in the most significant words (top words) of a topic, given a reference corpus.
We apply the widely-used Normalized Pointwise Mutual Information (NPMI)~\citep{aletras2013evaluating,lau2014machine} computed over the top 10 words of each topic, by the Palmetto package~\citep{roder2015exploring}\footnote{\url{http://palmetto.aksw.org}}.
As not all the discovered topics are interpretable~\citep{yang2015efficient,zhao2018dirichlet}, to comprehensively evaluate the topic quality,
we choose the topics with the highest NPMI and report the average score over those selected topics.
We vary the proportion of the selected topics from 10\% to 100\%, where 10\% indicates the top 10\% topics with the highest NPMI are selected and 100\% means all the topics are used.
TD, as its name implies, measures how diverse the discovered topics are. We define topic diversity to be the percentage of unique words in the top 25 words~\citep{dieng2019topic} of the selected topics, similar in TC. TD close to 0 indicates redundant topics; TD close to 1 indicates more varied topics.
As doc-topic distributions can be viewed as unsupervised document representations, to evaluate the quality of such representations, we perform document clustering tasks and report the purity and Normalized Mutual Information (NMI)~\citep{manning2008introduction} on 20NG, WS, and TMN, where the document labels are considered. With the default training/testing splits of the datasets, we train a model on the training documents and infer the topic distributions $\vec{z}$ on the testing documents. Given $\vec{z}$, we adopt two strategies to perform the document clustering task: \textbf{i)} Following~\citet{nguyen2015improving}, we use the most significant topic of a testing document as its clustering assignment to compute purity and NMI (denoted by \textbf{top-Purity} and \textbf{top-NMI}); \textbf{ii)} We apply the KMeans algorithm on $\vec{z}$ (over all the topics) of the testing documents and report the purity and NMI of the KMeans clusters (denoted by \textbf{km-Purity} and \textbf{km-NMI}). For the first strategy, the number of clusters equals to the number of topics while for the second one, we vary the number of clusters of KMeans in the range of $\{20, 40, 60, 80, 100\}$. Note that our goal is not to achieve the state-of-the-art document clustering results but compare document representations of topic models. For all the metrics, higher values indicate better performance. 


\textbf{Baseline methods and their settings:~}
We compare with the state-of-the-art NTMs, including:
LDA with Products of Experts (\textbf{ProdLDA})~\citep{srivastava2017autoencoding}, which replaces the mixture model in LDA with a product of experts and uses the AVI for training;
Dirichlet VAE (\textbf{DVAE})~\citep{burkhardt2019decoupling}, which is a neural topic model imposing the Dirichlet prior/posterior on $\vec{z}$. We use the variant of DVAE with rejection sampling VI, which is reported to perform the best;
Embedding Topic Model (\textbf{ETM})~\citep{dieng2019topic}, which is a topic model that incorporates word embeddings and is learned by AVI;
Wasserstein LDA (\textbf{WLDA})~\citep{nan2019topic}, which is a WAE-based topic model.
For all the above baselines, we use their official code with the best reported settings.

\begin{figure}[t]
        \centering
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \caption{20NG}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/20News_tc-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \caption{WS}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/webs_tc-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \caption{TMN}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/TMN_tc-crop.pdf}
         \end{subfigure}\begin{subfigure}[b]{0.19\linewidth}
                 \centering
                \caption{Reuters}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/reuters_tc-crop.pdf}
         \end{subfigure}
          \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                \caption{RCV2}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/rcv_tc-crop.pdf}
         \end{subfigure}
         \vspace{0.3cm}
        \\
               \centering
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/20News_td-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/webs_td-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/TMN_td-crop.pdf}
         \end{subfigure}\begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/reuters_td-crop.pdf}
         \end{subfigure}
          \begin{subfigure}[b]{0.19\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/rcv_td-crop.pdf}
         \end{subfigure}
\caption{The first row shows the TC scores for all the datasets and the second row shows the corresponding TD scores. In each subfigure, the horizontal axis indicates the proportion of selected topics according to their NPMIs.}
\label{fig-tc-td}
 \vspace{-0.5cm}
\end{figure}
\begin{figure}
        \centering
        \begin{minipage}{0.68\textwidth}
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \caption{20NG}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/20News_km_pp-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \caption{WS}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/webs_km_pp-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \caption{TMN}
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/TMN_km_pp-crop.pdf}
         \end{subfigure}\vspace{0.3cm}
        \\
               \centering
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/20News_km_nn-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/webs_km_nn-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.32\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr_rebuttal/TMN_km_nn-crop.pdf}
         \end{subfigure}\vspace{0.3cm}
\caption{The first row shows the km-Purity scores and the second row shows the corresponding km-NMI scores. In each subfigure, the horizontal axis indicates the number of KMeans clusters.}
\label{fig-km}
\end{minipage}
\hspace{0.0005\textwidth}
\begin{minipage}{0.3\textwidth}
        \begin{subfigure}[b]{0.9\linewidth}
                 \centering
                 \caption{Over batches}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/losses_rcv2-crop.pdf}
         \end{subfigure}
       \vspace{0.1cm}
        \\
         \begin{subfigure}[b]{0.9\linewidth}
                 \centering
               \caption{Over seconds}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/times_rcv2-crop.pdf}
         \end{subfigure} 
\caption{Training loss.}
\label{fig-time}
\end{minipage}
\end{figure}

\textbf{Settings for NSTM:~}
NSTM is implemented on TensorFlow. For the encoder $\theta$, to keep simplicity, we use a fully-connected neural network with one hidden layer of 200 units and ReLU as the activation function, followed by a dropout layer (rate=0.75) and a batch norm layer, same to the settings of~\cite{burkhardt2019decoupling}. For the Sinkhorn algorithm, following~\cite{cuturi2013sinkhorn}, the maximum number of  iterations is 1,000 and the stop tolerance is 0.005\footnote{The Sinkhorn algorithm usually reaches the stop tolerance in less than 50 iterations in NSTM}. In all the experiments, we fix $\alpha=20$ and $\epsilon=0.07$. 
We further vary the two hyperparameters to study our model's sensitivity to them in Figure~\ref{fig-parameter} of the appendix. Finetuning the parameters specifically to a dataset may give better results. The optimisation of NSTM is done by Adam~\citep{kingma2014adam} with learning rate 0.001 and batch size 200 for maximally 50 iterations. For NSTM and ETM, the 50-dimensional (i.e., $L=50$, see Eq.~(\ref{eq-m})) GloVe word embeddings~\citep{pennington2014glove} pre-trained on Wikipedia\footnote{\url{https://nlp.stanford.edu/projects/glove/}} are used.
We use the number of topics $K=100$ in most cases and set $K=500$ on RCV2 to test our model's scalability. 

\subsection{Results}






\begin{table}[]
\rowa{1.3}
\centering
\caption{top-Purity and top-NMI for document clustering. The best and second scores of each dataset are highlighted in boldface and with an underline, respectively.}
\label{tb-clustering}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}cccccccc@{}} \toprule
        & \multicolumn{3}{c}{top-Purity} & \phantom{a} &  \multicolumn{3}{c}{top-NMI} \\ \cmidrule{2-4} \cmidrule{6-8}
        & 20NG    & WS      & TMN     && 20NG   & WS     & TMN    \\ \cmidrule{2-4} \cmidrule{6-8}
LDA & 0.398$\pm$0.013  & \second{0.446}$\pm$0.022  & 0.470$\pm$0.008  && 0.320$\pm$0.010 & \second{0.185}$\pm$0.013 & 0.125$\pm$0.006 \\
ProdLDA & \second{0.417}$\pm$0.004  & 0.293$\pm$0.023  & 0.405$\pm$0.157  && \second{0.321}$\pm$0.004 & 0.066$\pm$0.016 & 0.091$\pm$0.101 \\
DVAE    & 0.281$\pm$0.006  & 0.284$\pm$0.005  & 0.477$\pm$0.012  && 0.187$\pm$0.005 & 0.059$\pm$0.001 & 0.113$\pm$0.004\\
ETM     & 0.063$\pm$0.003  & 0.215$\pm$0.001  & \second{0.556}$\pm$0.022  && 0.005$\pm$0.005 & 0.003$\pm$0.003 & \second{0.328}$\pm$0.010 \\
WLDA    & 0.117$\pm$0.001  & 0.239$\pm$0.003  & 0.260$\pm$0.002  && 0.060$\pm$0.001 & 0.026$\pm$0.001 & 0.009$\pm$0.001 \\
NSTM    & \best{0.477}$\pm$0.011  & \best{0.451}$\pm$0.009  & \best{0.637}$\pm$0.010  && \best{0.415}$\pm$0.012 & \best{0.201}$\pm$0.004 & \best{0.334}$\pm$0.004 \\
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\end{table}



\textbf{Quantitative results:~} We run all the models in comparison five times with different random seeds and report the mean and standard deviation (as error bars).
We show the results of TC and TD in Figure~\ref{fig-tc-td} and top-Purity/NMI in Table~\ref{tb-clustering}, and km-Purity/NMI in Figure~\ref{fig-km}, respectively.
We have the following remarks about the results:
\textbf{i)} Our proposed NSTM outperforms the others significantly in terms of topic coherence while obtaining high topic diversity on all the datasets. Although others may have higher TD than ours in one dataset or two, they usually cannot achieve a high TC at the same time.
\textbf{ii)} In terms of document clustering, our model performs the best in general with a significant gap over other NTMs, except the case where ours is the second for the KMeans clustering on 20NG. This demonstrates that NSTM is not only able to discover interpretable topics with better quality but also learn good document representations for clustering. It also shows that with the OT distance, our model can achieve a better balance among the comprehensive metrics of topic modelling.
\textbf{iii)} For all the evaluation metrics, our model is consistently the best on the short documents including WS and TMN. This demonstrates the effectiveness of our way of incorporating pretrained word embeddings, which shows our model's potential on short text topic modelling. Although ETM also uses pretrained word embeddings, its performance is incomparable to ours.





\textbf{Scalability:~}
NSTM has comparable scalability with other NTMs and is able to scale on large datasets with a large number of topics. To demonstrate the scalability, we run NSTM, DVAE, ProdLDA (as these three are implemented in TensorFlow, while ETM is in PyTorch, and WLDA is in MXNet) on RCV2 with $K=500$. The three models run on a Titan RTX GPU with batch size 1,000. Figure~\ref{fig-time} shows the training losses, which demonstrate that NSTM has similar learning speed to ProdLDA, better than DVAE. The TC and TD scores of this experiment are shown in Section~\ref{a-sec-rcv-500} of the appendix, where it can be observed that with 500 topics, our model shows similar performance advantage over others.


\textbf{Qualitative analysis:~}
As topics in our model are embedded in the same space as pretrained word embeddings, they share similar geometric properties.
Figure~\ref{fig-vis} shows a qualitative analysis.
For the t-SNE~\citep{maaten2008visualizing} visualisation, we select the top 50 topics with the highest NPMI learned by a run of NSTM on RCV2 with $K=100$ and feed their (50 dimensional) embeddings into the t-SNE method.
We also show the top five words and the topic number (1 to 50) of each topic. We can observe that although the words of the topics are different, the semantic similarity between the topics captured by the embeddings is highly interpretable.
In addition, we take the GloVe embeddings of the polysemantic word ``apple'' and find the closest 10 related words among the 0.4 million words of the GloVe vocabulary according to their cosine similarity. It can be seen that by default ``apple'' refers to the Apple company more in GloVe. Either adding the embeddings of topic 1 that describes the concept of ``food'' or subtracting the embeddings of topic 46 that describes the concept of ``tech companies'' reveals the fruit semantic for the word ``apple''.
More qualitative analysis on topics are provided in Section~\ref{a-sec-more-vis} of the appendix.





\begin{figure}
\centering
\begin{subfigure}[b]{0.64\linewidth}
                 \centering
                 \includegraphics[width=1.0\textwidth]{figs/tsne_rcvB.png}
         \end{subfigure}\begin{subfigure}[b]{0.36\linewidth}
                 \centering
                 \includegraphics[width=1.0\textwidth]{figs_iclr/apple.pdf}
         \end{subfigure} 
\caption{Left: t-SNE visualisation of topic embeddings on RCV2. One red dot represents a topic. The top 5 words and the topic number (1 to 50) of each topic are also shown. Right: interactions between word and topic embeddings.}
\label{fig-vis}
\end{figure}

\section{Conclusion}
In this paper, we presented a novel neural topic model based on optimal transport, where a document is endowed with two representations: the word distribution, $\tilde{\vec{x}}$, and the topic distribution, $\vec{z}$. An OT distance is leveraged to compare the semantic distance between the two distributions, whose cost function is defined according to the cosine similarities between topics and words in the embedding space.
$\vec{z}$ is obtained from an encoder that takes $\tilde{\vec{x}}$ as input and is trained by minimising the OT distance between $\vec{z}$ and $\tilde{\vec{x}}$.
With pretrained word embeddings, topic embeddings are learned by the same minimisation of the OT distance in terms of the cost function. Our model has shown appealing properties that are able to overcome several shortcomings of existing neural topic models.
extensive experiments have been conducted, showing that our model achieves state-of-the-art performance on both discovering quality topics and deriving useful document representations  for both regular and short texts. 

\subsubsection*{Acknowledgments}
Trung Le was supported by AOARD grant FA2386-19-1-4040.
Wray Buntine was supported by the Australian Research Council under award DP190100017.


\bibliographystyle{iclr2021_conference}
\bibliography{iclr_cr}
\newpage
\appendix
\numberwithin{equation}{section}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\section{Proof of Theorem 1}
\label{a-sec-proof}
\begin{proof}
Before showing the proof, we introduce the following notations:
We denote $k \in \{1,\cdots, K\}$ and $v \in \{1, \cdots, V\}$ as the indexes; 
The $s^{\text{th}}$ ($s \in \{1,\cdots, S\}$) token of the document picks a word in the vocabulary, denoted by $w_s \in \{1,\cdots,V\}$;
the normaliser in the softmax function of $\phi(\vec{z})$ is denoted as $\hat{\phi}$
so:
\[
\hat{\phi} = \sum_{v=1}^V e^{\sum_{k=1}^K z_k (2 - m_{v k})}
= e^2 \sum_{v=1}^V e^{-\sum_{k=1}^K z_k m_{v k}} ~.
\]

With these notations, we first have the following equation for the multinomial log-likelihood:

\begin{IEEEeqnarray}{+rCl+x*}
\label{eq-multi-likelihood}
\tilde{\vec{x}}^T \log{\phi(\vec{z})} &=& \frac{1}{S}\sum_{s=1}^S \log \phi(\vec{z})_{w_s} \nonumber \\
&=&  \frac{1}{S}\sum_{s=1}^S \left( \sum_{k=1}^K z_k (2 - m_{w_s k}) - \log{\hat{\phi}} \right)\nonumber\\
&=& 2 - \log{\hat{\phi}} - \frac{1}{S}\sum_{s=1}^S \sum_{k=1}^K z_k m_{w_s k}~.
\end{IEEEeqnarray}

Recall that in Eq.~(1) of the main paper, the transport matrix $\matr{P}$ is one of the joint distributions of  $\tilde{\vec{x}}$ and $\vec{z}$. We introduce the conditional distribution of $\vec{z}$ given $\tilde{\vec{x}}$ as $\matr{Q}$, where
$q(v, k)$ indicates the probability of assigning a token of word $v$ to topic $k$.


Given that $\matr{P}$ satisfies $\matr{P} \in U(\tilde{\vec{x}}, \vec{z})$ and $p_{vk} = \tilde{x}_v q(v, k)$,  $\matr{Q}$ must satisfy $U'(\tilde{\vec{x}}, \vec{z}) \eqdef \{\matr{Q} \in \mathbb{R}_{>0}^{V \times K} | \sum_{v=1}^V \tilde{x}_v q(v, k) = z_k\}$.
With $\matr{Q}$, we can rewrite the OT distance as:

\begin{IEEEeqnarray}{+rCl+x*}
d_{\matr{M}} (\tilde{\vec{x}}, \vec{z}) &=& \min_{\matr{Q} \in U'(\tilde{\vec{x}}, \vec{z})} \sum_{v=1, k=1}^{V, K} \tilde{x}_v q(v, k) m_{vk} \nonumber \\
&=& \frac{1}{S} \min_{\matr{Q} \in U'(\tilde{\vec{x}}, \vec{z})} \sum_{k=1}^{K} \sum_{s=1}^{S}  q(w_s, k) m_{w_s k}. \nonumber
\end{IEEEeqnarray}

If we let $q(v, k) = z_k$, meaning that all the tokens of a document to the topics according to the document's doc-topic distribution,
then $\matr{Q}$ satisfies $U'(\tilde{\vec{x}}, \vec{z})$, which leads to:

\begin{IEEEeqnarray}{+rCl+x*}
d_{\matr{M}} (\tilde{\vec{z}}, \vec{x}) \le \frac{1}{S}  \sum_{k=1}^{K} \sum_{s=1}^{S}  z_k m_{w_s k}~.
\end{IEEEeqnarray}

Together with Eq.~(\ref{eq-multi-likelihood}), the definition
of $\hat{\phi}$, and the fact that $m_{v k}\le 2$, we have:

\begin{IEEEeqnarray}{+rCl+x*}
\tilde{\vec{x}}^T \log{\phi(\vec{z})} &=& 2 - \log{\hat{\phi}} - \frac{1}{S} \sum_{s=1}^S \sum_{k=1}^K z_k m_{w_s k} \nonumber\\
&\le& -\log\left(\sum_{v=1}^V e^{-\sum_{k=1}^K z_k m_{vk}} \right)- d_\matr{M}(\tilde{\vec{x}},\vec{z}) \nonumber\\
&\le& -(\log V -2 )- d_\matr{M}(\tilde{\vec{x}},\vec{z}) \nonumber\\
&\le&- d_\matr{M}(\tilde{\vec{x}},\vec{z})~,
\end{IEEEeqnarray}
where the last equation holds if $\log{V} > 2$, i.e., $V \ge 8$.
\end{proof}


\newpage






\section{Parameter sensitivity}
In the previous experiments, we fix the values of $\epsilon$ and $\alpha$,
which control the weight of the multinomial likelihood in Eq~(\ref{eq-final-loss}) and the weight of the entropic regularisation in the Sinkhorn distance, respectively. 
Here we report the performance of NSTM on 20NG (blue lines) under different settings of the two hyperparameters in Figure~\ref{fig-parameter}.
Moreover, we propose two variants of NSTM. The first one removes the Sinkhorn distance in the training loss of Eq.~(\ref{eq-final-loss}) (i.e., only the expected log-likelihood term left) and its performance is shown as the red lines.
The second variant removes the the expected log-likelihood term in the training loss of Eq.~(\ref{eq-final-loss}) (i.e., only Sinkhorn distance left) and its performance is shown as yellow lines. 




\begin{figure}[t]
        \centering
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \caption{TC}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_gtc_epsilon-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \caption{TD}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_td_epsilon-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \caption{top-Purity}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_p_epsilon-crop.pdf}
         \end{subfigure}\begin{subfigure}[b]{0.16\linewidth}
                 \centering
                \caption{top-NMI}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_n_epsilon-crop.pdf}
         \end{subfigure}
                  \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                \caption{km-Purity}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_km_p_epsilon-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                \caption{km-NMI}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_km_n_epsilon-crop.pdf}
         \end{subfigure}
         \vspace{0.3cm}
        \\
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_gtc_lambda-crop.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_td_lambda-crop.pdf}
         \end{subfigure} 
         \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_p_lambda-crop.pdf}
         \end{subfigure}\begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_n_lambda-crop.pdf}
         \end{subfigure}
          \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_km_p_lambda-crop.pdf}
         \end{subfigure}
                  \begin{subfigure}[b]{0.16\linewidth}
                 \centering
                 \includegraphics[width=0.99\textwidth]{figs_iclr/20News_K100_km_n_lambda-crop.pdf}
         \end{subfigure}
\caption{Parameter sensitivity of NSTM on 20News. The first and second show the performance with different values of $\epsilon$ and $\alpha$, respectively. In the first row, we fix $\alpha=20$ and vary $\epsilon$ while in the second row, we fix $\epsilon=0.07$ and vary $\alpha$.}
\label{fig-parameter}
\end{figure}


\section{TC and TD on RCV2 with 500 Topics}
\label{a-sec-rcv-500}
The results are shown in Figure~\ref{fig-rcv-500}.


\begin{figure}[t]
        \centering
         \begin{subfigure}[b]{0.45\linewidth}
                 \centering
                 \caption{TC}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/RCV2_500_tc-crop.pdf}
         \end{subfigure}
          \hspace{0.0005\textwidth}
         \begin{subfigure}[b]{0.45\linewidth}
                 \centering
                 \caption{TD}
                 \includegraphics[width=0.99\textwidth]{figs_iclr/RCV2_500_td-crop.pdf}
         \end{subfigure} 
\caption{TC and TD on RCV2 with 500 topics.}
\label{fig-rcv-500}
\end{figure}


\section{Average Sinkhorn distance with varied number of topics}
In Figure~\ref{fig-d}, we show the average Sinkhorn distance with varied number of topics on 20NG, WS, TMN, and Reuters. It can be observed that when $K$ increases, there is a clear trend that $d_\matr{M}(\vec{z},\vec{x})$ decreases.

\begin{figure}[t]
        \centering
         \begin{subfigure}[b]{0.23\linewidth}
                 \centering
                 \caption{20NG}
                 \includegraphics[width=0.99\textwidth]{figs/d_20ng-crop.pdf}
         \end{subfigure}
          \hspace{0.0005\textwidth}
         \begin{subfigure}[b]{0.23\linewidth}
                 \centering
                 \caption{WS}
                 \includegraphics[width=0.99\textwidth]{figs/d_ws-crop.pdf}
         \end{subfigure} 
         \hspace{0.0005\textwidth}
         \begin{subfigure}[b]{0.23\linewidth}
                 \centering
                 \caption{TMN}
                 \includegraphics[width=0.99\textwidth]{figs/d_tmn-crop.pdf}
         \end{subfigure}\hspace{0.0005\textwidth}
         \begin{subfigure}[b]{0.23\linewidth}
                 \centering
                \caption{Reuters}
                 \includegraphics[width=0.99\textwidth]{figs/d_reuters-crop.pdf}
         \end{subfigure}
\caption{Sinkhorn distance with varied $K$. Vertical axis: the average Sinkhorn distance over all the training documents, i.e., mean $d_{\matr{M}}(\vec{z}, \vec{x})$. Horizontal axis: the number of topics, i.e., $K$}.
\label{fig-d}
\end{figure}


\newpage
\section{More topic embedding visualisations}
\label{a-sec-more-vis}
In Figure~\ref{fig-tsne-20ng}, \ref{fig-tsne-ws}, \ref{fig-tsne-tmn}, and \ref{fig-tsne-reuters}, we show the visualisations of 20NG, WS, TMN, and Reuters, respectively. We note that the topic embeddings in general present much better clustering structures of topics in the semantic space. Such topic correlations can only be detected by specialised topic models (e.g.,in~\cite{lafferty2006correlated,blei2010nested,zhou2016augmentable}).
Instead, the correlations of topics in our model are implicitly captured by the semantic embeddings.

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{figs/tsne_20ng.pdf}
\caption{t-SNE visualisation of topic embeddings on 20NG.}
\label{fig-tsne-20ng}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{figs/tsne_webs.pdf}
\caption{t-SNE visualisation of topic embeddings on WS.}
\label{fig-tsne-ws}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{figs/tsne_tmn.pdf}
\caption{t-SNE visualisation of topic embeddings on TMN.}
\label{fig-tsne-tmn}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{figs/tsne_reuters.pdf}
\caption{t-SNE visualisation of topic embeddings on Reuters.}
\label{fig-tsne-reuters}
\end{figure}
























\end{document}
