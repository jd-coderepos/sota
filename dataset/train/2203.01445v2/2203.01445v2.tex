\documentclass{midl} 



\usepackage{mwe} \usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\jmlrvolume{Accepted for Publication in MIDL 2022 }
\jmlryear{2022}


\title[LILE]{LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives}












\midlauthor{\Name{Danial Maleki\nametag{}} \Email{dmaleki@uwaterloo.ca}\\
\Name{H.R Tizhoosh\nametag{}} \Email{tizhoosh.hamid@mayo.edu}\\
\addr   Kimia Lab, University of Waterloo, Waterloo, ON, Canada \\
\addr  Artificial Intelligence and Informatics, Mayo Clinic, Rochester, MN, USA 
	\\
	\addr  Corresponding author
}

\begin{document}

\maketitle

\begin{abstract}
The volume of available data has grown dramatically in recent years in many applications. Furthermore, the age of networks that used multiple modalities separately has practically ended. Therefore, enabling bidirectional cross-modality data retrieval capable of processing has become a requirement for many domains and disciplines of research. This is especially true in the medical field, as data comes in a multitude of types, including various types of images and reports as well as molecular data.
Most contemporary works apply cross attention to highlight the essential elements of an image or text in relation to the other modalities and try to match them together. However, regardless of their importance in their own modality, these approaches usually consider features of each modality equally. In this study, self-attention as an additional loss term will be proposed to enrich the internal representation provided into the cross attention module.  
This work suggests a novel architecture with a new loss term to help represent images and texts in the joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO and ARCH, show the effectiveness of the proposed method.
\end{abstract}

\begin{keywords}
Cross-Modal Retrieval, Histopathology, Attention
\end{keywords}

\section{Introduction}
A plethora of real-world applications is emerging due to the rapid expansion in the volume and variety of data on the one hand and the recent success of deep learning on the other hand. However, the majority of existing models can only work with a single modality of data, e.g., only analyzing images or text. This restriction on the data source prevents the models from having a more generalizable and robust problem representation for downstream tasks. The absence of a model that can be used across several modalities and that can use information from multiple sources of data is in great demand and has become increasingly common in both academia and industry. One task that has gained popularity over the years is retrieving the relation between one of the data modalities; a task called ``cross-modality retrieval''. This task seeks to bring a pair of data from two distinct sources together by recognizing their intrinsic relationship. Because such cross-modal topologies need to learn the inter-modal correspondence and modality representation separately, their design and training can be rather challenging. Image and text are the most extensively utilized and commonly observed modalities in real-world data. However, The diversity in information sources can be observed more commonly in the medical field. Images, reports and molecular data in patients' medical records are different and indispensable sources of information. A model able to retrieve a source of data when a different one is available can help provide complete information about the patient.

The common solution to approach to explore the relationship between image and text is to map the visual semantic embeddings \cite{karpathy2015deep,fromedeep} of an image and the corresponding words/phrases/sentences into a common latent embedding space \cite{barnard2003matching,berg2004names,socher2010connecting,chong2009simultaneous}. In these methods, the goal is generally to find a common space where the corresponding representations of images and text are as close as possible, hence making recognizing their relationship easier. Recent studies investigate the use of the attention mechanism to understand the similarity between the two modalities. The majority of studies in this category have employed cross-attention mechanisms, which allow the model to selectively attend to the parts of an instance that are relevant to the context from the other modal \cite{lee2018stacked,huang2017instance}. Other methods attempt to refine their representation regarding the information of the other modal \cite{chen2020imram}.
Nonetheless, due to the gap between the representation of images and texts, these methods may not find the optimal representation. Moreover, ambiguity in text documents is a common challenge posing a learning obstacle to the model if it uses an injective embedding. Aside from that, humans use a hierarchical structure to organize and store diverse semantic concepts. However, the majority of the currently available approaches group semantics together in a consistent manner \cite{lee2018stacked,chen2020imram,song2019polysemous}.

In this study, the mentioned issues are addressed by providing an iterative regime to capture related information in the other modality and extract the most significant features by looking into its context. It can help the model capture the information related to the other modality and itself simultaneously. Accordingly, such an approach can help extract richer latent embeddings for each instance. Moreover, training the model in an iterative regime can help the model to capture higher-level features gradually based on the features acquired in the previous steps. Furthermore, refining the extracted features based on the intersection between modalities can help the model to enrich its representations. 


\section{Literature Review}


Many works on cross-modality retrieval have been looking for a similar pattern in the feature representation of both modalities \cite{karpathy2015deep,lee2018stacked, huang2018learning,lu202012,radford2021learning}. These studies can generally be categorized into global and local feature matching approaches. 

\textbf{Global feature matching methods} were among first attempts to solve the image-text retrieval task. Kiros \textit{et al.} \cite{kiros2014unifying} proposed to use a CNN and a RNN to encode given images and text data. For the loss function, they proposed an idea to implement a pairwise ranking loss objective to rank images and correspondence descriptions. Faghri \textit{et al.} \cite{faghri2017vse++} modified the loss function with hard negatives in the triplet loss function and applied on a similar architecture to \cite{kiros2014unifying}. Furthermore, by improving the quality of generative models \cite{zhu2017unpaired,karras2019style}, some studies have  attempted to exploit the capability of generative models to enhance the performance of proposed methods \cite{patrick2020support,peng2019cm,gu2018look}.

\textbf{Local feature matching methods} consider the correspondence between image and text feature representations at the level of image regions and words. Karpathy \textit{et al.} \cite{karpathy2015deep} proposed a method to extract features for image regions at object level using Faster R-CNN \cite{ren2015faster} and text words, and then align them into a common space. However, as the application of the attention mechanism in a variety of tasks has become more common, several approaches in image-text retrieval seek to leverage the attention mechanism  \cite{chen2020imram,lee2018stacked,song2019polysemous,thomas2020preserving, lee2018stacked}. Yale \textit{et al.} \cite{song2019polysemous} proposed a method to combine global features with locally-guided features via a multi-head self-attention module. Christopher \textit{et al.} \cite{thomas2020preserving} in a similar network proposed a novel within-modality loss function that drives feature representation toward more coherence. The majority of these methods rely on the premise that vision and text are mutually exclusive and equally important. However, grounding (or base) representation of each modality derived from another modality to finer details is critical for bridging the gap between vision and text modality. Kuang-Huei  \cite{lee2018stacked} was one of the first studies that investigated the use of cross-attention to explore full latent matching using image regions and words in the text as context.

Furthermore, Hui \textit{et al.} in IMRAM paper \cite{chen2020imram} presented an iterative alignment method that captures the fine-grained correspondence between image and text progressively using a cross-attention module. By the development of Transformer models, serious approaches were proposed for cross-modality retrieval task using Transformers \cite{lu2019vilbert,tan2019lxmert}. Authors in \cite{lu2019vilbert} proposed a network architecture that consists of two single-modal Transformer networks applied on input images and texts data respectively.

Recent studies have shown that training on large-scale datasets could help models to achieve significant improvement in image-text retrieval task \cite{li2020oscar,radford2021learning,jia2021scaling}. Jia \textit{et al.} in ALIGN paper \cite{jia2021scaling} leveraged a dataset of over one billion paired image and alt-text data and claimed that a simple dual-encoder network could learn to align image and text representation using a contrastive loss. Similar to ALIGN paper, Xiujun \textit{et al.} in Oscar paper \cite{li2020oscar} collected a dataset over 6.5 million paired of image and text to train their network. Another example for boosting the performance using a large amount of data can be CLIP \cite{radford2021learning} paper. Authors in CLIP paper also demonstrated that the task of image-text retrieval could be outperformed using a massive dataset, 400 million in this case. However, a bottleneck of these large-dataset-dependent approaches emerges in those domains which are unlikely to be included in their collected dataset, such as histopathology domain. Moreover, training on that amount of data requires huge computational resources.

Cross-Modality Retrieval is new to the field of histopathology. There are not many studies that explore retrieving a description correspondence to an image or vice versa. As a result, a few works were trying to retrieve or utilize image-text modalities in their suggested methods. 
Zhang \textit{et al.} \cite{zhang2017mdnet} was among the first authors who proposed a model and dataset for image-text in pathology. Authors proposed a new framework, namely MDNet, to establish a direct multi-modal mapping between images and diagnostic reports. Moreover, they developed a private patch-based dataset based on pathology bladder cancer images. For descriptions, they asked pathologists to write 5 short sentences for each patch. The proposed method is capable of generating short diagnostic reports and retrieving images based on symptom description. Yet the developed dataset was too small, private and limited to only H\&E images. Recently, Gamper \textit{et al.} \cite{gamper2021multiple} developed a new publicly available dataset, ARCH, that contains more than 7,000 patch-based images with the corresponded captions. The authors of the paper used PubMed medical articles \cite{PubMed60:online} and academic pathology textbooks to collect the ARCH dataset.
\section{Methodology}


The proposed model, as demonstrated in Figure \ref{architecutre} includes different components. LILE, a dual attention network using Transformers,  takes images and texts as inputs and extracts feature representations for each of those using Transformers. Then, a self-attention module \cite{vaswani2017attention},  which is applied for extracting the most significant parts of each modality with respect to itself, is applied. For the next step, a cross-attention module and gated memory block are applied to help the model refine the representation of each instance with respect to the outputs of attention modules. Additionally, an iterative matching scheme using a gated memory block is applied to refine the extracted features for each modality.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/diagram.pdf}
    \caption{The architecture of LILE for cross-modality retrieval }
    \label{architecutre}
\end{figure}


\subsection{Feature Representation}
\label{feature}
Data representation, or representation learning \cite{bengio2013representation}, is a collection of approaches in ML that allows a model to automatically discover the representations required from the given data. Recently, Transforms become more prevalent for representation learning. the Transformers' modular architecture enables the processing of different modalities (e.g., images, videos, text, and voice) leveraging similar processing blocks. It scales efficiently to huge capacity networks for complex tasks and performs well with massive datasets. Transformer architecture is chosen for text and image feature extractors in this study due to these advantages.

\textbf{Image Representation:} 
\label{image}
In this study, a pre-trained ViT \cite{dosovitskiy2020image} is implemented to encode input images . Each image is split into a sequence of fixed-size, non-overlapping image patches before being fed to the ViT.
Utilizing object detection models is another method for extracting visual information from an image. These models are trained on the input image in order to extract the objects. As a result, the input image can be represented as a set of extracted feature maps for all of the objects in the image \cite{lee2018stacked,chen2020imram}.
Depending on the dataset and the availability of annotations for the object detection task, one of these approaches is used in this study.

\textbf{Text Representation:}
Text representation can be defined as a method for encoding sentences into vectors. Transformers have recently emerged as the top-performing solution for the majority of NLP tasks and outperform many other approaches \cite{liu2019roberta,radford2019language, UsingDee40:online}.
In this study, a pre-trained ``Roberta'' architecture  \cite{liu2019roberta} which is a Transform-based model is deployed to encode the input text. It receives a text description as an input and returns a feature map that represents the given data. 

\subsection{Attention and Gated Memory Blocks}\label{memory_sec}
After the representation for each modality instance has been extracted, a multi-head self-attention module is applied to obtain  enhanced feature maps for extracted features from the previous stage similar to the \cite{wei2020multi}. These  representations highlight the most significant features of each modality in relation to itself. 

A cross-attention mechanism is utilized to attend to distinct parts of one modality given the context of another modality as proposed by authors in \cite{chen2020imram, wei2020multi}. The application of cross-attention in the proposed approach is to attend to image regions regarding the text input tokens and vice versa. The gated memory block seeks to refine the extracted features from each modality considering the cross-attention feature maps. The input modalities are denoted as  and  which  and  can be either image or text features, where  is the number of attention heads in the multi-head self-attention module in the previous step. The cross-attention module helps to have the highlighted information for modality  related to modality . To achieve this goal, a suitable measurement is needed to quantify the similarity between each feature map and feature maps in another modality.

In the suggested solution, the cosine similarity is applied \cite{ji2019saliency, jia2021scaling,lee2018stacked}. The similarity between each instance in modality  and  is determined as Eq.\ref{cosine}, where  denotes the similarity between the -th feature map for modality X and the -th from modality Y. 
Furthermore, it is beneficial to threshold the similarity at 0 and normalize it \cite{karpathy2014deep,lee2018stacked}.







To attend on set  with respect to a given feature  in , a weighted combination of  is defined. The definition of attention function in Eq.\ref{cosine} is a variant of the ``dot product attention'' that is commonly used in other studies  \cite{luong2015effective}:



 
In Eq.\ref{attn}, the parameter  is the inverse temperature of the softmax function  \cite{chorowski2015attention}. A more smooth attention function can be achieved by adjusting .  is defined as  where each element of  captures significant  parts of each  given the whole  set as context. To refine the extracted features of  regarding the important parts of each  given , a memory unit has been used. It would dynamically update and refine the feature maps of  by looking to both  and  as Eq.\ref{memory}, where  can be defined differently \cite{kalra2020learning,weston2014memory,burtsev2020memory}. In this study, a gated mechanism has been adopted for  as follows:








\subsection{Iterative Matching}
\label{iterative}
As stated in section \ref{memory_sec}, gated memory can assist the model in refining the feature representation regarding the shared information between two modalities as proposed by authors in \cite{chen2020imram}. Having the gated memory in an iterative scheme can be beneficial as each iteration step, with the help of  feature representation of  can be re-calibrated. The iterative scheme can be summarized as Eq.\ref{iterative_eq}, where  is the iteration step that will be performed to refine the alignment for the next iteration: 


At iteration step , the similarity score between image I and text T is calculated as follows:

where  is a learnable scalar weight parameter that balances the influence of the similarity score terms.  and  are defined as similarity score between image regions and text  and text tokens and image  respectively proposed by authors in \cite{chen2020imram} . These similarity scores are derived as 



The similarity score can be boosted by including directly the similarity between image and text as  and .

This can assist the model in preserving the semantic meaning of each instance while it attempts to bring paired instances closer together.
To put all  steps together, the similarity score between image I and text T will be derived as Eq.\ref{similarity}, where  is the number of matching steps that will be set as a hyper-parameter. See appendix \ref{appd}.



 
\subsection{Loss Function}
\label{loss}
In this study, N-pairs bi-directional triplet-loss is implemented.


In Eq.\ref{seprated_loss},  is a margin value and . The term  is defined in Eq.\ref{similarity} and measures the similarity between image  and text . This similarity score forms a similarity score matrix  of size , where  is symmetric, See appendix \ref{appa}. In the training phase,  is the size of mini-batch. Images and text with a same subscript are paired instances which means the diagonal of the matrix should have the largest value compared to the other indices. This regime will be trained in an end-to-end manner.


\section{Experiments}
We used two benchmark datasets to evaluate the effectiveness of the proposed method:
\begin{itemize}
    \item \textbf{MS-COCO} \cite{lin2014microsoft}, a large-scale object detection, segmentation, key-point detection, and captioning dataset, and 
    \item \textbf{ARCH} \cite{gamper2021multiple},  a computational pathology multiple-instance captioning dataset, see appendix \ref{appb}.
    \end{itemize}
    To compare the proposed method with other approaches, Recall at K (R@K) is used, which measures the fraction of queries retrieved correctly among top  search results\cite{chen2020imram,lee2018stacked}. To show the efficiency of the proposed method, the reported results also includes "R@sum" which is the summation of evaluation at different , i.e., R@1+R@5+R@10 as in \cite{huang2017instance}.

\begin{table}[!h]
\centering
\caption{Comparison with state-of-the-art methods on MS-COCO}
\label{table-coco}
\resizebox{0.75\textwidth}{!}{\begin{tabular}{@{}lccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} &
  \multicolumn{3}{c}{Text Retrieval} &
  \multicolumn{3}{c}{Image Retrieval} &
  \multirow{2}{*}{R@sum} \\
\multicolumn{1}{c}{}                                    & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 &       \\ \midrule
\multicolumn{8}{c}{1K}                                                                                    \\ \midrule
SCO\cite{huang2018learning}            & 69.9 & 92.9 & 97.5 & 56.7 & 87.5 & 94.8 & 499.3 \\
SCAN\cite{lee2018stacked}              & 72.7 & 94.8 & 98.4 & 58.8 & 88.4 & 94.8 & 507.9 \\
PVSE\cite{song2019polysemous}          & 69.2 & 91.6 & 96.6 & 55.2 & 86.5 & 93.7 & 492.8 \\
VSRN\cite{li2019visual}                & 76.2 & 94.8 & 98.2 & 62.8 & 89.7 & 95.1 & 516.8 \\
IMRAM\cite{chen2020imram}              & 76.7 & \textbf{95.6} & \textbf{98.5} & 61.7 & 89.1 & 95.0 & 516.6 \\ \midrule
OSCAR (Fine-tuned)\cite{li2020oscar}   & 89.8 & 98.8 & 99.7 & 78.2 & 95.8 & 98.3 & 563.6 \\ \midrule
\textbf{LILE}                                                & \textbf{77.7}   &  95.4    &    98.3  & \textbf{64.1}     & \textbf{91.0}     & \textbf{96.4}     &       \textbf{522.9}     \\ \midrule
\multicolumn{8}{c}{5K}                                                                                    \\ \midrule
SCO\cite{huang2018learning}            & 42.8 & 72.3 & 83.0 & 33.1 & 62.9 & 75.5 & 369.6 \\
SCAN\cite{lee2018stacked}              & 50.4 & 82.2 & 90.0 & 38.6 & 69.3 & 80.4 & 410.9 \\
PVSE\cite{song2019polysemous}          & 45.2 & 74.3 & 84.5 & 32.4 & 63.0 & 75.0 & 374.4 \\
VSRN\cite{li2019visual}                & 53.0 & 81.1 & 89.4 & 40.5 & 70.6 & 81.1 & 415.7 \\
IMRAM\cite{chen2020imram}              & 53.7 & \textbf{83.2} & 91.0 & 39.7 & 69.1 & 79.8 & 416.5 \\
\midrule
ALIGN (Fine-tuned) \cite{jia2021scaling} &77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.8 & 500.4 \\
OSCAR (Fine-tuned)\cite{li2020oscar}   & 73.5 & 92.2 & 96.0 & 57.5 & 82.8 & 89.8 & 491.8 \\
CLIP\cite{radford2021learning}         & 58.4 & 81.5 & 88.1 & 37.8 & 62.4 & 72.2 & 400.4 \\ \midrule
\textbf{LILE}                                                    & \textbf{55.6}     &    82.4  & \textbf{91.0}     & \textbf{41.5}     & \textbf{72.1}     &    \textbf{82.2}  &     \textbf{424.8}  \\ \bottomrule
\end{tabular}}
\end{table}


\begin{table*}[ht!]
\centering
\caption{Comparison with previous methods using the ARCH dataset}
\label{table-arch}
\resizebox{0.7\textwidth}{!}{\begin{tabular}{@{}lccc|ccc|c@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Text Retrieval}            & \multicolumn{3}{c}{Image Retrieval}           & \multirow{2}{*}{R@sum} \\
                 & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 &       \\ \midrule
PVSE             & 36.2 & 48.8 & 53.3 & 29.3 & 43.8 & 52.6 & 264.0 \\
IMRAM            & 40.3 & 52.5 & 59.5 & 32.4 & 48.2 & 56.1 & 289.0 \\
CLIP (Zero-shot) & 38.2 & 51.1 & 57.0 & 30.8 & 46.1 & 53.2 & 276.4 \\ \midrule
\textbf{LILE}                    & \textbf{44.8} & \textbf{57.4} & \textbf{64.4} & \textbf{36.7} & \textbf{52.2} & \textbf{61.7} & \textbf{317.1}         \\ \bottomrule
\end{tabular}}
\end{table*}
The results are shown in Table \ref{table-coco} and Table \ref{table-arch} for MS-COCO and ARCH datasets respectively. The proposed method, called LILE, outperformed the previous best model, i.e., IMRAM \cite{chen2020imram}, by a large margin of  and  in terms of overall performance R@sum in MS-COCO(1K) and MS-COCO(5K) datasets,  respectively. 

Because there is no previously reported results on the ARCH dataset, publicly available methods like IMRAM \cite{chen2020imram}, PVSE \cite{song2019polysemous} and CLIP \cite{radford2021learning} were selected and modified to evaluate them on the ARCH dataset. LILE significantly outperformed the best previous best model, i.e., IMRAM, with a large margin of  in terms of overall performance R@sum. These results demonstrate the effectiveness of the proposed approach for the cross-modality retrieval task. The implementation details described in appendix \ref{appc}.


The cross-modal retrieval task can benefit greatly from the use of large amounts of collected data and powerful computational resources. Consequently, methods trained on large-scale datasets (i.e., CLIP \cite{radford2019language}, ALIGN \cite{jia2021scaling}, and OSCAR \cite{li2020oscar}) can not be directly compared to methods other methods and LILE which trained exclusively on smaller datasets such as MS-COCO. The values for those methods trained on massive datasets are reported to highlight latter point.

Additionally, the results on ARCH dataset show even more improvement compared to MS-COCO results. One of the reasons that may have influenced this enhancement could be the use of Transformer for encoding the input data as IMRAM \cite{song2019polysemous} and PVSE \cite{song2019polysemous} applied GRU for their text encoding. Moreover, analyzing the results on ARCH dataset can prove that methods like CLIP \cite{radford2021learning} which have been trained on massively large datasets cannot perform very well on tasks like image-text retrieval in pathology. This type of tasks are specialized and apparently need more sophisticated methods like the proposed framework.

\section{Conclusion}
The proposed cross-modality approach can simultaneously capture the most salient features of each modality in relation to itself and other modalities. A multi-head self-attention module was used to aid the network in gaining a better understanding of each modality. Meanwhile, to assist the model in identifying all potential alignments between two modalities, the multi-head self-attention module output is fed into a cross-attention module. This approach can aid the model in adjusting retrieved features from one modality considering the effect of the other one. Additionally, the suggested novel loss objective can assist the model in determining the optimal weight to balance both implicit and explicit sources of information used to match paired instances in different modalities. Experiments on the MS-COCO and ARCH datasets demonstrated the efficiency of LILE in terms of recall, a typical metric for retrieval tasks in both general-purpose and histopathology contexts.

\bibliography{midl-fullpaper}
 
\newpage
\appendix

\section{Similarity Matrix}\label{appa}
Figure \ref{matrix} illustrates the similarity matrix.  Images and text with a same subscript are paired instances which means the diagonal of the matrix should have the largest value compared to the other indices


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/matrix.pdf}
    \caption{Similarity matrix for a mini-batch with size of . Element  in the matrix shows the similarity score between image  and text . As the paired data has a same subscript, similarity matrix should be diagonal. }
    \label{matrix}
\end{figure}

\section{ARCH dataset}\label{appb} ARCH dataset\cite{gamper2021multiple} is a computational pathology multiple-instance captioning dataset that includes morphological descriptions and diagnoses for a wide variety of tissue types and staining. Some samples from the dataset are shown in Figure \ref{arch-dataset-sample}. The dataset images and corresponding descriptions were mined from PubMed medical articles and pathology text books. It contains 7,579 images and a description for each image. For experiments conducted using this dataset, the 5-fold cross validation was applied.


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{./images/arch-sample.pdf}
    \caption{Three samples from the ARCH dataset. As it can be seen, images are from different staining and and normalizations with varied description structure.}
    \label{arch-dataset-sample}
\end{figure}
\section{Implementation Details}\label{appc}
For those experiments using ARCH dataset, DeiT architecture \cite{touvron2021training} is used as the image encoder. This architecture includes 12 layers and 768 hidden state and split the input image into  patches. As the result, each input image can be seen as 196 patches with the size of . DeiT model individually is trained for a classification task on the dataset that had been used in ``KimiaNet'' \cite{riasatian2021fine}. In the next step, the weights for the image encoder are initialized from the trained model in the previous step. As the size of the dataset was not large enough to properly train the network, only the the last two blocks of image encoder were trainable and the rest of the network were frozen.  For the text encoder, the network weights are initialized from the BioMed-RoBERTa-base \cite{domains} which had been trained on 2.68 million scientific papers from Semantic scholar corpus \cite{Semantic31:online}. 
This model is a language model based on RoBERTa-base architecture \cite{liu2019roberta} which includes 12, 768 hidden dimensions with 110M learnable parameters. Same as image encoder, only 2 last layers of text encoder were trainable and the rest of the network were frozen. The challenging part for ARCH dataset as it shown in Figure \ref{token_length} is related to the descriptions length. The descriptions are mostly non-uniform, i.e., ranging from 2 tokens to 484 tokens and much longer compared to MS-COCO dataset. To address this problem, each description is split into sentences. Then, concatenation of the first sentence with every other sentence are considered as new individual descriptions. By doing this, each image can have one or more than one description which help to augment the data and uniform the descriptions. 
 
\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{./images/arch_vs_mscoco.png}
    \caption{(a): Frequency of the length of the descriptions tokens in the ARCH dataset. (b): Frequency of the length of the descriptions in the MS-COCO dataset. The MS-COCO dataset has an average of 13.81 and a maximum of 65 tokens per caption, whereas The ARCH dataset has an average of 51.49 and a maximum of 484 for each description.}
    \label{token_length}
\end{figure}

For experiments using MS-COCO dataset, the same architectures are implemented for both image encoder and text encoder. The only differences are the weight initialization applied for these networks and the number of layers that set to be trainable. The image encoder weights are initialized from a pre-trained model that trained on a classification task on ImageNet-1k dataset \cite{deng2009imagenet}, which has 1 million images with 1,000 classes. Moreover, the input data for image encoder is the feature maps that had been extracted using Faster R-CNN 
as explained in section method section. The last 6 layers of the image encoder were trainable and the rest were frozen. The text encoder weights are initialized from a pre-trained Roberta-base network that had been trained on the reunion of five general datasets\footnote{BookCorpus \cite{Homembwe2:online}, which contains 11,038 unpublished books, English Wikipedia \cite{EnglishW87:online}, CC-News \cite{NewsData24:online}, which contains 63M English news articles, OpenWebText \cite{GitHubjc79:online}, which is a WebText dataset used to train GPT-2 and Stories which is a subset of CommonCrawl \cite{NewsData14:online} data.}. The last 6 layers of the text encoder were trainable and the rest were frozen.

All the experiments are implemented in Pytorch V1.3 and run with two NVIDIA V100 GPUs. For the conducted experiments, images were resized to  pixels and normalized before feeding into the model. The hyper-parameter , which specifies the number of self-attention heads, is set as 16 for optimal performance across all experiments. The Adam optimizer \cite{kingma2014adam} is used with an initial learning rate of . The learning rate is decreased when the metric has stopped improving. The margine for contrastive loss function set to be .

\section{Effect of the iterative Matching scheme, } \label{appd} To better understand the effect of  in the proposed approach,  gradually is increased from 1 to 4 to train and test on the MS-COCO (5K) benchmark dataset. Table \ref{table-iteration_exp} shows the results for R@1, R@5, R@10 and R@sum metrics on image-to-text retrieval and back. It can be observed for  that the model consistently achieved better performance than other values for . The gap between the performance for ,  which is a baseline without the iterative scheme, and other values for  is significant. That gap can prove the importance of the iterative scheme in the proposed approach.


\begin{table}[]
\centering
\caption{The effect of the number of matching steps, , in MS-COCO dataset.}
\label{table-iteration_exp}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{2}{*}{K} & \multicolumn{3}{c}{Text Retrieval} & \multicolumn{3}{c}{Image Retrieval} & \multirow{2}{*}{R@sum} \\
                   & R@1        & R@5       & R@10      & R@1        & R@5        & R@10      &                        \\ \midrule
1                  & 77.2       & 96.4      & 99.0      & 64.4       & 92.0       & 96.6      & 525.6                  \\
2                  & 79.6       & 97.4      & 99.7      & 67.0       & 92.8       & 97.2      & 533.7                  \\
3                  & 79.9       & 97.0      & 99.0      & 66.2       & 92.3       & 97.3      & 531.7                  \\
4                  & 78.5       & 96.7      & 98.8      & 65.9       & 92.5       & 97.4      & 529.8                  \\ \bottomrule
\end{tabular}\end{table}

\end{document}
