\begin{figure*}[t]
\centering
\vspace{-10mm}
\includegraphics[width=.9\textwidth]{iccv2023AuthorKit/figure/unfold_mmreact_zoom.pdf}
\vspace{-1ex}
\caption{An example of \modelname's full execution flow. The blue circles with numbered indices indicate the order in which different models are called (\ie, the executions). The executions, highlighted by bold underlined text, can be either a ChatGPT call (\eg, ``\textbf{\underline{ChatGPT:}}'') or running one or multiple selected vision experts (\eg, ``\textbf{\underline{Image Captioning}}''). We add a commentary text \textit{action execution} in dashed boxes to help understand the expert execution. Each ChatGPT execution takes the preceding text as input and generates the text leading up to the next execution (\eg, \textit{``This is an image. Assistant, what $\ldots$ image? \textless ImagePath\textgreater''} for Execution~1).
Texts in gray represent \modelname's thoughts or vision experts' actions and outputs, which are invisible to users. This multimodal reasoning and action process occurs behind the scene to gather the necessary information for generating final responses to users, which are shown in black.
	}
\label{fig:prompt}
\end{figure*}

\section{Experiments}
\subsection{Experiment Setup}
We implement \modelname~based on the LangChain codebase~\cite{langchain} and reference ideas from ReAct~\cite{yao2022react}. We access ChatGPT via the Azure ``gpt-3.5-turbo'' API that has a token length limit of 4,096, and utilize vision experts publicly available via the Azure Cognitive Services APIs\footnote{\url{https://azure.microsoft.com/en-us/products/cognitive-services/vision-services}}, including the ones for image captioning,
image tagging, dense captioning, optical character recognition (OCR), and specialized recognition models for celebrities, receipts, \etc. We further expand the toolset with customized tools for spatial understanding and image editing, and tools from other modalities such as Bing search and PAL math.

\subsection{\textbf{\modelname}'s Full Execution Flow}
\label{sec:4.2}
Figure~\ref{fig:prompt} provides an example to illustrate \modelname's full execution flow.
We highlight the exact order to call different models (\ie, executions) with numbered blue circles. The executions, highlighted by bold underlined text, can be either a ChatGPT call (\eg, ``\textbf{\underline{ChatGPT:}}'') or the execution of one or multiple selected vision experts (\eg, ``\textbf{\underline{Image Captioning}}''). We add a commentary text \textit{action execution} in dashed boxes to help understand the vision expert execution. The \textit{action execution} is not an actual input or output in the \modelname~flow. ChatGPT executions can be used to generate thought (reasoning) and action texts that allocate vision experts~\includegraphics[height=12pt]{iccv2023AuthorKit/figure/placeholder/think.png} (invisible to users), or produce the final response to users~\includegraphics[height=12pt]{iccv2023AuthorKit/figure/placeholder/react.png}. Each ChatGPT execution takes the preceding text as input and generates the text leading up to the next execution (\eg, \textit{``This is an image. Assistant, what objects do you see
in this image? \textless ImagePath\textgreater''} for Execution~1). ChatGPT ``learns'' the proper text to generate based on the instructions and in-context examples in the prompt prefix, as detailed in Section~\ref{sec:3.3}.
Additional examples of the reasoning and execution procedures are in Figures~\ref{fig:mmreact-unfolded-2}-\ref{fig:mmreact-unfolded-4}.

\subsection{\textbf{\modelname}~Capabilities and Applications}
Figures~\ref{fig:exp1_math}-\ref{fig:exp2_video} show the representative capabilities and application scenarios that \modelname~demonstrates. Specifically, we examine \modelname's capabilities in visual math and text reasoning (Figure~\ref{fig:exp1_math}), understanding visual-conditioned jokes and memes (Figure~\ref{fig:exp1_meme}), spatial and coordinate understanding, visual planning and prediction (Figure~\ref{fig:exp1_spatial_plan}), multi-image reasoning (Figure~\ref{fig:exp1_multi_image3}),  multi-hop document understanding on bar charts (Figure~\ref{fig:exp1_multi_hop}), floorplans (Figure~\ref{fig:exp1_floor}), flowcharts (Figure~\ref{fig:exp1_flow}),  tables (Figure~\ref{fig:exp1_table}), open-world concept understanding (Figure~\ref{fig:exp1_open_world}), and video analysis and summarization (Figure~\ref{fig:exp1_video},~\ref{fig:exp2_video}). We provide an example of the unfolded steps in Figure~\ref{fig:mmreact-unfolded-2}.


\subsection{Capability Comparison with PaLM-E}
\modelname~is a training-free scheme which composes existing vision experts with ChatGPT,
while PaLM-E~\cite{driess2023palme} trains a vision-language model which combines an image encoder and a text decoder with dedicated datasets.
Figures~\ref{fig:palme1}-\ref{fig:palme4} shows our \modelname~can achieve
competitive results to PaLM-E.
We further illustrate the complete multimodal reasoning and action procedures in Figures~\ref{fig:mmreact-unfolded-3},\ref{fig:mmreact-unfolded-4}.



\subsection{\textbf{\modelname}~Extensibility}
\label{sec:4.5}
In Figure~\ref{fig:chatgpt_gpt4_1} and~\ref{fig:chatgpt_gpt4_2}, we explore the enhancement of \modelname's LLM from ChatGPT (``gpt-3.5-turbo'') to GPT-4 (language-only). We access the language-only GPT-4 via the ChatGPT website and reference the multimodal GPT-4 demo provided by OpenAI for comparison. These examples demonstrate the benefit of \modelname's extensibility: \modelname~equipped with GPT-4 correctly answers the physics question (Figure~\ref{fig:chatgpt_gpt4_1}), while the version with ChatGPT (GPT-3.5) fails. Furthermore, \modelname~is designed with the flexibility to incorporate new tools without training. Figure~\ref{fig:ext_image_edit} provides a case study of plugging an image editing tool from X-decoder~\cite{zou2022xdecoder} for multi-round, dialogue-based image editing.



\subsection{Limitations}
We identify the following limitations. 
1). Considering the recognition capability in the wild, we find it hard to systematically evaluate the performance  with concrete accuracy numbers, due to 
a lack of annotated benchmarks. 
Thus, it is worth investing efforts towards how to effectively evaluate such system's performance.
2). The vision capability is limited by the integrated vision
experts. On one hand, the integrated experts may make mistakes; on the other hand, the system may fail if the necessary experts are missing. 
3). We inject the vision experts' knowledge in the prefix, and thus the number of experts is limited by the context window (4096 tokens) of ChatGPT. 
4). Visual signals are converted to text words
for ChatGPT understanding, which  might not be the optimal solution for certain vision tasks. 
5). \modelname~requires manual prompt engineering. We expect research work to automate this process, making the system even easier to develop.

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_math.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{visual math and text reasoning}. 
	}
\label{fig:exp1_math}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_meme.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios:  \textbf{visual-conditioned joke/meme}. We provide an example of the unfolded multi-modal reasoning and action steps for meme understanding in Figure~\ref{fig:mmreact-unfolded-2}.
	}
\label{fig:exp1_meme}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_spatial_plan.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{spatial/coordinate understanding} and \textbf{visual planning and prediction}.
	}
\label{fig:exp1_spatial_plan}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_1x1_multi_image.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{multi-image reasoning}. For the multi-image inputs shown above, we input one receipt image at a time. Once all four receipt images are provided as inputs to  \modelname, we prompt it to answer questions that require reasoning over multiple images.
	}
\label{fig:exp1_multi_image3}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_1x1_multi_hop.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{multi-hop document understanding (bar charts)}. The unfolded multi-modal reasoning and action steps of this example are detailed in Figure~\ref{fig:mmreact-unfolded-chart}.
	}
\label{fig:exp1_multi_hop}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/floor_plan.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{document understanding (floorplan)}.
	}
\label{fig:exp1_floor}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_flowchart.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{document understanding (flowchart)}. We provide an example of the unfolded multi-modal reasoning and action steps for flowchart understanding in Figure~\ref{fig:mmreact-unfolded-flowchart}.
	}
\label{fig:exp1_flow}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/table_1.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{document understanding (table)}.
	}
\label{fig:exp1_table}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp1_open_world.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{open-world concept understanding}.
	}
\label{fig:exp1_open_world}
\end{figure*}

\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[height=1.08\textheight]{iccv2023AuthorKit/figure/exp1_video_long.pdf}
\vspace{-5mm}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{video summarization/event localization}.
	}

\label{fig:exp1_video}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=.95\textheight]{iccv2023AuthorKit/figure/exp2_video.pdf}
\caption{Case studies of \modelname's capabilities and application scenarios: \textbf{video summarization/event localization}. 
	}
\label{fig:exp2_video}
\end{figure*}

\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/palme_compare/palme_compare_1.pdf}
\caption{Comparison of \modelname~with PaLM-E~\cite{driess2023palme} on illustrated capabilities. We empirically show that text prompts are as effective as expensive joint fine-tuning in solving complicated vision problems. 
	}
\label{fig:palme1}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/palme_compare/palme_compare_3.pdf}
\caption{Comparison of \modelname~with PaLM-E~\cite{driess2023palme} on illustrated capabilities. We empirically show that text prompts are as effective as expensive joint fine-tuning in solving complicated vision problems. 
	}
\label{fig:palme3}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/palme_compare/palme_compare_4.pdf}
\caption{Comparison of \modelname~with PaLM-E~\cite{driess2023palme} on illustrated capabilities. We empirically show that text prompts are as effective as expensive joint fine-tuning in solving complicated vision problems.
	}
\label{fig:palme4}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-2-new.pdf}
\vspace{-0mm}
\caption{Unfolded multimodal reasoning and action steps for example of visual-conditioned joke/meme in Figure~\ref{fig:exp1_meme}.
	}
\label{fig:mmreact-unfolded-2}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-20mm}
 \begin{adjustwidth}{-1cm}{-1cm}
 \centering
\includegraphics[width=1.15\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-chart-3col.pdf}
\end{adjustwidth}
\vspace{-5mm}
\caption{Unfolded multimodal reasoning and action steps for multi-hop document understanding (bar charts) in Figure~\ref{fig:exp1_multi_hop}. 
	}
\label{fig:mmreact-unfolded-chart}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-15mm}
\includegraphics[width=1\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-flowchart.pdf}
\vspace{-5mm}
\caption{Unfolded multimodal reasoning and action steps for document understanding (flowchart) in Figure~\ref{fig:exp1_flow}.
	}
\label{fig:mmreact-unfolded-flowchart}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-20mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-4-new.pdf}
\vspace{-0mm}
\caption{Unfolded multimodal reasoning and action steps for example of multi-image relationships in Figure\ref{fig:palme1}.
	}
\label{fig:mmreact-unfolded-3}
\end{figure*}
\begin{figure*}[t]
\centering
\vspace{-10mm}
\includegraphics[width=1.\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-3-new.pdf}
\vspace{-0mm}
\caption{Unfolded multimodal reasoning and action steps for example of scene text reasoning in Figure~\ref{fig:palme3}.
	}
\label{fig:mmreact-unfolded-4}
\end{figure*}



\begin{figure*}[t]
\centering
\vspace{-25mm}
\begin{adjustwidth}{-1cm}{-1cm}
\centering
\includegraphics[height=1.1\textheight]{iccv2023AuthorKit/figure/chatgpt_gpt4_1_v2.pdf}
\end{adjustwidth}
\vspace{-5mm}
\caption{Case studies of \modelname's \textbf{extensibility}. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in \modelname~to GPT-4 (Langauge-only) and further improve the results of \modelname. In Figure~\ref{fig:mmreact-unfolded-physics}, we provide the unfolded multimodal reasoning and action steps based on Ours w/ ChatGPT.
	}
\label{fig:chatgpt_gpt4_1}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{iccv2023AuthorKit/figure/chatgpt_gpt4_2_v2.pdf}
\caption{Case studies of \modelname's \textbf{extensibility}. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in \modelname~to GPT-4 (Langauge-only)  and further improve the results of \modelname.
	}
\label{fig:chatgpt_gpt4_2}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=.98\textwidth]{iccv2023AuthorKit/figure/ext_image_edit.pdf}
\caption{Case studies of \modelname's \textbf{extensibility}. \modelname~is designed with the flexibility to plug in new experts. In this example, \modelname~is enhanced with the image editing model from X-decoder~\cite{zou2022xdecoder} and can perform dialogue-based image editing.
	}
\label{fig:ext_image_edit}
\end{figure*}


\begin{figure*}[t]
\centering
 \begin{adjustwidth}{-1cm}{-1cm} \centering
 \includegraphics[width=1.15\textwidth]{iccv2023AuthorKit/figure/mmreact-unfolded-physics-3col.pdf}
\end{adjustwidth}
\caption{Unfolded multimodal reasoning and action steps with ChatGPT to tackle physics problem in Figure~\ref{fig:chatgpt_gpt4_1}. 
	}
\label{fig:mmreact-unfolded-physics}
\end{figure*}
%
