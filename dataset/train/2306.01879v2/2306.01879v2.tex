\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx} \usepackage{multirow}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{nicematrix}
\definecolor{mygray}{gray}{0.85}
\definecolor{softgray}{rgb}{0.9, 0.9, 0.9}
\definecolor{softblue}{rgb}{0.88, 0.92, 1.0}
\definecolor{softgreen}{rgb}{0.88, 1.0, 0.88}
\definecolor{softyellow}{rgb}{1.0, 1.0, 0.88}
\definecolor{softred}{rgb}{1.0, 0.88, 0.88}
\definecolor{softpink}{rgb}{1.0, 0.88, 0.94}




\usepackage{color}
\usepackage{ifthen}



\definecolor{zhiqiu_color}{rgb}{0,0.5,1}
\definecolor{deva_color}{rgb}{0.2,.64,0}
\definecolor{deepak_color}{rgb}{1,0,1}
\definecolor{xinyue_color}{rgb}{1,0,1}
\definecolor{pengchuan_color}{rgb}{1,0,1}

\newif\ifsubmit
\submitfalse
\ifsubmit
    \newcommand{\zhiqiu}[1]{}
    \newcommand{\deva}[1]{}
    \newcommand{\deepak}[1]{}
    \newcommand{\xinyue}[1]{}
    \newcommand{\pengchuan}[1]{}
\else
    \newcommand{\zhiqiu}[1]{\textsf{\textcolor{zhiqiu_color}{[{\bf Zhiqiu}: #1]}}}
    \newcommand{\devaOld}[1]{\textsf{\textcolor{deva_color}{[{\bf Deva}: #1]}}}
    \newcommand{\deepak}[1]{\textsf{\textcolor{deepak_color}{[{\bf Deepak}: #1]}}}
    \newcommand{\xinyue}[1]{\textsf{\textcolor{xinyue_color}{[{\bf Xinyue}: #1]}}}
    \newcommand{\pengchuan}[1]{\textsf{\textcolor{pengchuan_color}{[{\bf Pengchuan}: #1]}}}
\fi

\usepackage{amsmath}
\usepackage{bbm}
 
\title{Revisiting the Role of Language Priors in Vision-Language Models}



\newcommand{\deva}[1]{{\leavevmode\color[rgb]{1,0,1}[Deva: #1]}}


\author{Zhiqiu Lin*\quad Xinyue Chen* \quad Deepak Pathak\quad Pengchuan Zhang\quad Deva Ramanan \\
  CMU \quad\quad Meta \\ \\
  \href{https://linzhiqiu.github.io/papers/visual_gpt_score}{Open-source code in webpage}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study \textit{generative VLMs} that are trained for next-word generation given an image. We explore their zero-shot performance on the illustrative task of image-text retrieval across 8 popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the {\em Visual Generative Pre-Training Score} (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a ``blind'' language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.


\end{abstract}


\section{Introduction}
Vision-language models (VLMs) trained on web-scale datasets will likely serve as the foundation for next-generation visual understanding systems. One reason for their widespread adoption is their ability to be used in an ``off-the-shelf" (OTS) or zero-shot manner without fine-tuning for specific target applications. In this study, we explore their OTS use on the task of image-text retrieval (e.g., given an image, predict the correct caption out of  options) across a suite of 8 popular benchmarks. 

{\bf Challenges.} While the performance of foundational VLMs is impressive, many open challenges remain. Recent analyses~\citep{kamath2023text, aro} point out that leading VLMs such as CLIP~\citep{clip} may often degrade to ``bag-of-words" that confuse captions such as {\tt "the horse is eating the grass"} and {\tt "the grass is eating the horse"}. This makes it difficult to use VLMs to capture {\em compositions} of objects, attributes, and their relations. But somewhat interestingly, large-scale language models (LLMs) trained for autoregressive next-token prediction~\citep{gpt3} seem to be able to discern such distinctions, which we investigate below. A related but under-appreciated difficulty is that of {\em benchmarking} the performance of visio-linguistic reasoning. 
Perhaps the most well-known example in the community is that of the influential VQA benchmarks~\citep{vqa}, which could be largely solved by exploiting linguistic biases in the dataset -- concretely, questions about images could often be answered by ``blind" language-only models that did not look at the image~\citep{vqa2}. Notably, we find that such blind algorithms still excel on many contemporary image-text retrieval benchmarks where VLMs may struggle.

{\bf Generative models for discriminative tasks.} We tackle the above challenges by revisiting the role of language priors through a probabilistic lens. To allow for a probabilistic treatment, we focus on generative VLMs that take an image as input and stochastically generate text via next-token prediction~\citep{blip, blip2}. We first demonstrate that such models can be easily repurposed for discriminative tasks (such as retrieval) by setting the match score for an image-text pair to be the probability that the VLM would generate that text from the given image, or . We call this probability score the Visual Generative Pre-Training Score, or VisualGPTScore. Computing the VisualGPTScore is even more efficient than next-token generation since given an image, all tokens from a candidate text string can be evaluated in parallel. Though conceptually straightforward, such an approach (to our knowledge) has not been proposed in the literature. In fact, the generative VLMs~\citep{blip} that we analyze train {\em separate} discriminative heads for matching/classifying image-text pairs, but we find that their language generation head itself produces better scores for matching (since it appears to better capture compositions). Indeed, the OTS VisualGPTScore performs surprisingly well on many benchmarks, even producing near-perfect accuracy on ARO~\citep{aro}. But it still struggles on other benchmarks such as Winoground~\citep{winoground}. We analyze this below. 

\begin{figure}[t]
\centering
    \scalebox{0.77}{
    \begin{tabular}{l l | r r}
        \multicolumn{2}{c}{{\Large Scenario 1}} & \multicolumn{2}{c}{{\Large Scenario 2}}\\
\includegraphics[width=0.41\textwidth]{images/flickr_order.png} &
        \includegraphics[width=0.22\textwidth]{images/same_prior.png} &
\includegraphics[width=0.28\textwidth]{images/sugarcrepe.png} &
        \includegraphics[width=0.21\textwidth]{images/uniform_prior.png} \\
\end{tabular}
    }
    \caption{\small {\bf Two train-test shifts encountered in image-to-text retrieval tasks.} Scenario 1 ({\bf left}) constructs negative captions by shuffling words in the true caption (as in ARO-Flickr), but this produces implausible text such as ``{\tt white a duck spreads its wings in while the water}". Here, exploiting the language bias of the training set will help since it will downweight the match score for such implausible negative captions. In fact, we show that a blind language-only model can easily identify the correct caption.  Scenario 2 ({\bf right}) constructs negative captions that are curated to be plausible (as in SugarCrepe). Here, the language bias of the training set may hurt, since it will prefer to match common captions that score well under the language prior; i.e., the incorrect caption of ``{\tt people are cooking in a kitchen}" is more likely than the true caption of ``{\tt people are posing in a kitchen}" under the language prior, and so removing the language bias improves performance.} 
\label{fig:prior_difference}
\end{figure}

{\bf The role of language priors.} We analyze the discrepancy in performance across benchmarks from a probabilistic perspective. Our key insight is that many benchmark biases can be formalized as mismatching distributions over text between train and test data -  versus . We use a first-principles analysis to account for distribution shift by simply reweighting the VisualGPTScore with the Bayes factor , a process we call {\em debiasing.} To compute the Bayes reweighting factor, we need access to both the train and test language prior. We compute  from an OTS VLM by drawing Monte-Carlo samples of  from trainset or Guassian noise images. Because  may require access to the test set, we explore simplifying assumptions that it is (a) identical to , (b) uninformative/uniform, or (c) tunable from a held-out val set. Our analysis helps explain the strong performance of the VisualGPTScore on certain benchmarks and its poor performance on others. Moreover, our analysis offers simple strategies for improving performance through debiasing. We conclude by showing a theoretical connection between debiasing and mutual information, which can be seen as a method for removing the effect of marginal priors when computing joint probability scores.






{\bf Empirical Analysis.} We conduct a thorough empirical evaluation of the OTS VisualGPTScore (and its debiased variants) for open-sourced image-conditioned language models~\citep{blip, blip2} across 8 popular vision-language benchmarks. We first point out that the VisualGPTScore by itself produces SOTA accuracy on certain benchmarks like ARO~\citep{aro} where their inherent language biases help remove incorrect captions that are also unnatural (such as {\tt ``a white duck the its wings while in water"} as shown in Fig.~\ref{fig:prior_difference}). In fact, we show that blind baselines also do quite well on these benchmarks, since language-only models can easily identify such implausible captions. However, such language biases do not work well on benchmarks where incorrect captions are also realistic. Here, VisualGPTScore should be debiased so as not to naively prefer more common captions that score well under its language prior. When given access to a validation set that reveals the amount of language bias in the benchmark, debiasing consistently improves performance on benchmarks such as Flickr30K~\citep{flickr30k} and Winoground~\citep{winoground}. Interestingly, we find that debiasing can also improve accuracy on the {\em train} set used to learn the generative VLMs, indicating that such models learn biased estimates of the true conditional distribution . We describe this further in our appendix.



























\section{Related Works}


{\bf Vision-language modelling.} State-of-the-art VLMs like CLIP~\citep{clip} are pre-trained on web-scale image-text datasets~\citep{laion400m, laion5b} using discriminative objectives including image-text contrastive (ITC)~\citep{clip, align} and image-text matching (ITM)~\citep{albef, blip} loss, typically formulated as . These pre-trained models exhibit robust zero-shot and few-shot~\citep{lin2023multimodality, wortsman2022robust} performance on traditional discriminative tasks~\citep{deng2009imagenet, coco}, often on par with fully-supervised models. More recently, image-conditioned language models like Flamingo~\citep{flamingo} and BLIP~\citep{blip, blip2} incorporate generative objectives~\citep{bengio2003neural} primarily for downstream tasks such as captioning~\citep{nocaps} and VQA~\citep{vqa2}. 



{\bf Visio-linguistic compositionality.} Benchmarks like ARO~\citep{aro}, Crepe~\citep{crepe}, Winoground~\citep{winoground}, EqBen~\citep{eqben}, VL-CheckList~\citep{vlchecklist}, and SugarCrepe~\citep{sugarcrepe} show that discriminative scores of VLMs, such as ITCScore and ITMScore, fail on their image-text retrieval tasks that assess compositional reasoning. Concurrently, advances on these tasks often involve fine-tuning discriminative VLMs with more data. One of the most popular approaches, NegCLIP~\citep{aro}, augments CLIP using programmatically generated negatives from original texts. Extending this, subsequent studies propose more expensive and heavily-engineered solutions. SyViC~\citep{cascante2023going} fine-tunes VLMs on million-scale synthetic images to augment spatial, attributive, and relation understanding. SGVL~\citep{herzig2023incorporating} and Structure-CLIP~\citep{huang2023structure} sample negatives using costly scene graph annotations. MosaiCLIP~\citep{singh2023coarse} and SVLC~\citep{doveh2022teaching} use linguistic tools such as scene graph parsers and LLMs to design better negative captions. The most recent DAC~\citep{doveh2023dense} leverages a combination of foundation models including BLIP2, ChatGPT, and SAM to rewrite and augment image captions. 

{\bf Generative pre-training and scoring. } Vision models trained with {\em discriminative} objectives often lack incentives to learn structure information~\citep{brendel2019approximating, tejankar2021fistful}. Similarly, early LLMs trained with {\em discriminative} approaches, such as BERT~\citep{bert} and RoBERTa~\citep{roberta}, have also been criticized as bag-of-words models insensitive to word order~\citep{bertolini2022testing, hessel2021effective, papadimitriou2022classifying, sinha2021masked}. Conversely, generative pre-trained LLMs~\citep{gpt2} demonstrate exceptional compositional understanding while pre-trained solely with a next-token prediction~\citep{bengio2003neural} loss. Furthermore, generative scores of LLMs~\citep{gpt4, flan, opt} have flexible usage in downstream tasks, such as text evaluation~\citep{bartscore, fu2023gptscore} and reranking~\citep{keskar2019ctrl}. 














\section{The role of language priors}
\label{sec:language_prior}


In this section, we present a simple probabilistic treatment for analyzing the role of language priors in image-conditioned language models (or generative VLMs). Motivated by their strong but inconsistent performance across a variety of image-text retrieval benchmarks, we analyze their behavior when there exists a mismatch between training and test distributions, deriving simple schemes for addressing the mismatch with reweighting. We conclude by exposing a connection to related work on mutual information.





{\bf Computing .} To begin our probabilistic treatment, we first show that image-conditioned language models (that probabilistically generate text based on an image) can be repurposed for computing a score between a given image  and text caption . The likelihood of a text sequence  conditioned on image  is naturally factorized as an autoregressive product~\citep{bengio2003neural}:

Image-conditioned language models return back  softmax distributions corresponding to the  terms in the above expression. Text generation requires {\em sequential} token-by-token prediction, since token  must be generated before it can be used as an input to generate the softmax distribution over token . Interestingly, given an image  and text sequence , the above probability can be computed in {\em parallel} because the entire sequence of tokens  are already available as input. We provide a visual illustration in \autoref{fig:visual_gpt_score}-a.



{\bf Train-test shifts.} Given the image-conditioned model of  above, we now analyze its behavior when applied to test data distributions that differs from the trainset, denoted as  versus . 
Recall that any joint distribution over images and text can be factored into a product over a language prior and an image likelihood . Our analysis makes the strong assumption that the image likelihood  is identical across the train and test data, but the language prior  may differ. Intuitively, this assumes that the visual appearance of entities (such as a {\tt "white duck"}) remains consistent across the training and test data, but the frequency of those entities (as manifested in the set of captions ) may vary. We can now derive  via Bayes rule:

The above shows that the generative pre-training score  need simply be weighted by the {\em ratio} of the language priors in the testset versus trainset. Intuitively, if a particular text caption appears {\em more} often in the testset than the trainset, one should {\em increase} the score reported by the generative model. However, one often does not have access to the text distribution on the testset. For example, real-world deployments and benchmark protocols may not reveal this. In such cases, one can make two practical assumptions; either the language distribution on test is identical to train, or it is uninformative/uniform (see \autoref{fig:prior_difference}):

{\bf Tunable .} In reality, a testset might be a mix of both scenarios. To model this, we consider a soft combination where the language prior on the testset is assumed to be a flattened version of the language prior on the trainset, for some temperature parameter :

By setting  to 0 or 1, one can obtain the two scenarios described above. Some deployments
(or benchmarks) may benefit from tuning  on a held-out validation set.



{\bf Implications for retrieval benchmarks.} We speculate some benchmarks like ARO-Flickr~\citep{aro} are close to scenario 1 because they include negative captions  that are {\em implausible}, such as ``{\tt a white duck the its wings while in water spreads}''. Such captions will have a low score under the language prior  and so reporting the raw generative score  (that keeps its language prior or bias) will improve accuracy. In fact, we show that applying a {\em blind} language model (that ignores all image evidence) can itself often identify the correct caption. On the other hand, for test datasets with more {\em realistic} negative captions (scenario 2), it may be useful to remove the language bias of the trainset, since that will prefer to match to common captions (even if they do not necessarily agree with the input image). This appears to be the case for SugarCrepe~\citep{sugarcrepe}, which uses LLMs like ChatGPT to ensure that the negative captions are realistic. 

{\bf Relationship to prior approaches.} Our approach to debiasing is reminiscent of mutual information, which can also be seen as a method for removing the
effect of marginal priors when computing joint probability scores. In fact, our \autoref{app:pmi} derives that -debiasing is equivalent to a form of pointwise mutual information (PMI) known as  for .

\section{Experimental results on I-to-T retrieval}
\label{sec:visual_gpt_score}
In this section, we verify our hypothesis on I-to-T retrieval benchmarks using state-of-the-art multimodal generative VLMs. In particular, we adopt image-conditioned language models such as BLIP~\citep{blip} as the learned estimator of . Then, we discuss how we perform Monte Carlo estimation of , including a novel efficient sampling method based on ``content-free'' Gaussian noise images. Finally, we show the state-of-the-art results of our generative approach on existing I-to-T retrieval tasks.

\begin{figure*}[t]
  \centering
  \scalebox{0.96}{
      \begin{tabular}{c|c}
{\includegraphics[width=0.5\textwidth]{images/teaser.png}} & \includegraphics[width=0.45\textwidth]{images/prior.png} \\
          \multicolumn{1}{c}{(a)  through generative VLMs} & \multicolumn{1}{c}{(b)  via Monte Carlo sampling}
      \end{tabular}
  }
  \caption{\small {\bf Estimating  and  from generative VLMs.} Figure (a) shows how image-conditioned language models such as \citet{blip} that generate text based on an image can be repurposed for computing , which is factorized as a product of  for a sequence of  tokens. These terms can be efficiently computed in {\em parallel}, unlike {\em sequential} token-by-token prediction for text generation. Figure (b) shows two approaches for Monte Carlo sampling of . While the straightforward approach is to sample trainset images, we find that using as few as three ``null'' (Gaussian noise) images can achieve more robust estimates.}
\label{fig:visual_gpt_score}
\end{figure*}

{\bf Preliminaries.} We leverage OTS image-conditioned language models~\citep{yu2022coca, flamingo, blip2} to estimate . For ablation, we use the open-sourced BLIP models~\citep{blip}, trained on public image-text corpora using discriminative (ITC and ITM) and generative (captioning) objectives. Discriminative objectives typically model . For example, ITCScore calculates cosine similarity scores between image and text features using a dual-encoder; ITMScore jointly embeds image-text pairs via a fusion-encoder and returns softmax scores from a binary classifier. Lastly, we term the generative score as {\bf Visual} {\bf G}enerative {\bf P}re-{\bf T}raining Score ({\bf VisualGPTScore}). While BLIP is pre-trained using all three objectives, this generative score has not been applied to discriminative tasks before our work.



{\bf Implementing VisualGPTScore.} Our method calculates an average of the log-likelihoods of  at each token position  and applies an exponent to cancel the log:

To condition on an input image, BLIP uses a multimodal casual self-attention mask~\citep{blip} in its image-grounded text decoder, i.e., each text token attends to all its preceding vision and text tokens. We emphasize that VisualGPTScore has the same computational cost as ITMScore, which uses the same underlying transformer but with a bi-directional self-attention mask to encode an image-text pair. We address potential biases of this estimator in \autoref{app:biased}.









\begin{table}[h]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{cc}
        \begin{NiceTabular}{llcccc}
        \CodeBefore
          \rectanglecolor{softred}{6-3}{8-6}
          \rectanglecolor{softblue}{9-3}{9-6}
\rectanglecolor{softyellow}{22-3}{22-6}
          \rectanglecolor{softgreen}{24-3}{24-6}
        \Body
\toprule
               \multirow{2}{*}{Score} &  \multirow{2}{*}{Method} & \multicolumn{4}{c}{\bf ARO} 
               \\ \cmidrule{3-6}
               & & Rel&  Attr&  COCO& Flickr\\ 
             \midrule
               Random & - &  50.0&  50.0&  20.0& 20.0\\ \hline
               \multirow{2}{*}{Text-Only} & Vera & 61.7 & 82.6 & 59.8 & 63.5 \\ 
               & Grammar & 59.6 & 58.4 & 74.3 & 76.3  \\ \hline
               \multirow{3}{*}{} & BART & 81.1& 73.6 & 95.0 & 95.2 \\
               & Flan-T5 & 84.4& 76.5 & 98.0 & 98.2 \\
               & OPT & 84.7 & 79.8 & 97.9 & 98.6
               \\  \hline
                & BLIP & 87.6 &	80.7 &	98.6 &	99.1 \\ \hline
               \multirow{13}{*}{}&  CLIP &  59.0 &  62.0 &  59.0 & 46.0 \\ 
               &  LAION2B-CLIP &   51.6 & 61.9 & 25.2 & 30.2 \\ 
               &  LAION5B-CLIP &   46.1 & 57.8 & 26.1 & 31.0 \\ 
               &  NegCLIP &  81.0 &  71.0 &  91.0 & 86.0 \\ 
               &  Structure-CLIP &  \cellcolor{softgray}83.5 &  85.1 &  - & - \\ 
&  SyViC &  80.8 & 72.4 &  92.4 & 87.2 \\ 
               &  SGVL &  - &  - & 87.2 & 91.0 \\ 
                &  MosaiCLIP &  82.6 &  78.0 &  87.9 & 86.3 \\ 
      & DAC-LLM & 81.3 & 73.9 & \cellcolor{softgray}94.5 & \cellcolor{softgray}95.7 \\ 
      & DAC-SAM & 77.2 & 70.5 & 91.2 & 93.9 \\ 
      & BLIP-ITC & 63.1 & 81.6 &	34.3 &	41.7 \\
      & BLIP-ITM & 58.7 &	\cellcolor{softgray}90.3 &	45.1 & 51.3 \\ \hline
      \multirow{3}{*}{\Large } & Ours () & {\bf 89.1} &	{\bf 95.3} &	{\bf 99.4} &	{\bf 99.5} \\
      & Ours () & 68.1 &	87.9 &	32.4 &	44.5 \\
      & Ours () & {\bf 89.1} &  {\bf 95.4} &	{\bf 99.4} &	{\bf 99.5} \\
     \bottomrule
\end{NiceTabular}
        & \quad
        
        \begin{NiceTabular}{llccc}
        \CodeBefore
          \rectanglecolor{softred}{6-3}{8-5}
          \rectanglecolor{softblue}{9-3}{9-5}
\rectanglecolor{softyellow}{21-3}{21-5}
          \rectanglecolor{softgreen}{23-3}{23-5}
        \Body
        
        \toprule
               \multirow{2}{*}{Score} &  \multirow{2}{*}{Method} & \multicolumn{3}{c}{\bf VL-CheckList} 
               \\ \cmidrule{3-5}
               & & Object&  Attribute&  Relation\\ 
             \midrule
               Random & - &  50.0&  50.0&  50.0\\ \hline
               \multirow{2}{*}{Text-Only} & Vera & 82.5 & 74.0 & 85.7 \\ 
               & Grammar & 58.0 & 52.4 & 68.5  \\ \hline
               \multirow{3}{*}{} & BART & 52.0& 51.0 & 45.1 \\
               & Flan-T5 & 60.3& 55.0 & 49.3 \\
               & OPT & 59.3 & 48.8 & 60.0 
               \\  \hline
                & BLIP & 68.2 &	58.7 &	75.9 \\ \hline
               \multirow{9}{*}{}&  CLIP &  81.6 &  67.6 &  63.1 \\ 
               &  LAION2B-CLIP &   84.7 & 67.8 & 66.5 \\
               &  LAION5B-CLIP &   87.9 & 70.3 & 63.9 \\
               &  NegCLIP &  81.4 &  72.2 &  63.5 \\ 
               &  SyViC &  - & 70.4 &  69.4 \\ 
               &  SGVL &  85.2 &  78.2 & 80.4 \\ 
                &  SLVC &  85.0 &  72.0 &  69.0 \\ 
                & DAC-LLM & 87.3 & 77.3 & 86.4 \\ 
      & DAC-SAM & 88.5 & 75.8 & \cellcolor{softgray}89.8 \\ 
      & BLIP-ITC & \cellcolor{softgray}90.6 & 80.3 &	73.5 \\
      & BLIP-ITM & 89.9 &	\cellcolor{softgray}80.7 & 67.7	 \\ \hline
      \multirow{3}{*}{\Large } & Ours () & 92.6 & 78.7 & 90.8 \\
      & Ours () & 90.4 &	77.6   & 77.8 \\
      & Ours () & {\bf 94.4} &  {\bf 82.1} &	{\bf 92.8} \\
     \bottomrule
\end{NiceTabular}
        
         \\ 
        \multicolumn{1}{c}{\Large \bf (a) Accuracy on ARO} & \multicolumn{1}{c}{\Large \bf (b) Accuracy on VL-CheckList} \\ \\
        \begin{NiceTabular}{llccc}
        \CodeBefore
          \rectanglecolor{softred}{6-3}{8-5}
          \rectanglecolor{softblue}{9-3}{9-5}
\rectanglecolor{softyellow}{16-3}{16-5}
          \rectanglecolor{softgreen}{18-3}{18-5}
        \Body
        \toprule
                \multirow{2}{*}{Score} &  \multirow{2}{*}{Method} & \multicolumn{3}{c}{\bf SugarCrepe} 
               \\ \cmidrule{3-5}
                &   &  Replace &  Swap &  Add \\ 
             \midrule
Random & - &  50.0&  50.0&  50.0\\ \hline
               \multirow{2}{*}{Text-Only} & Vera & 49.5 & 49.3 & 49.5 \\ 
               & Grammar & 50.0 & 50.0 & 50.0  
               \\  \hline
               \multirow{3}{*}{} & BART & 48.4& 51.9 & 61.2 \\
               & Flan-T5 & 51.4& 57.6 & 40.9  \\
               & OPT & 58.5 & 66.6 & 45.8
               \\  \hline
                & BLIP & 75.9 &	77.1 &	70.9 \\ \hline
               \multirow{6}{*}{}&  CLIP &  80.8 &  63.3 &  75.1 \\ 
               &  LAION2B-CLIP &  86.5 &  68.6 &  88.4 \\ 
               &  LAION5B-CLIP &  85.0 &  68.0 &  89.6 \\
               &  NegCLIP &  88.3 &  76.2 &  \cellcolor{softgray}90.2 \\ 
& BLIP-ITC & 85.8 & 73.8 &	85.7  \\
              & BLIP-ITM & \cellcolor{softgray}88.7 &	\cellcolor{softgray}81.3 &	87.6 \\ 
              \hline
              \multirow{3}{*}{\Large } & Ours () & 93.3 &	91.0 &	91.0 \\
              & Ours () & 83.2 &	85.5 &	85.9\\
              & Ours () & {\bf 95.1} &  {\bf 92.4} &	{\bf 97.4}  \\
             \bottomrule
\end{NiceTabular}
        & \quad
        
\begin{NiceTabular}{llccc}
        \CodeBefore
          \rectanglecolor{softred}{6-3}{8-5}
          \rectanglecolor{softblue}{9-3}{9-5}
\rectanglecolor{softyellow}{15-3}{15-5}
          \rectanglecolor{softgreen}{17-3}{17-5}
        \Body
        \toprule
                \multirow{2}{*}{Score} &  \multirow{2}{*}{Method} & \multicolumn{3}{c}{\bf Crepe} 
               \\ \cmidrule{3-5}
                &   &  Atom &  Swap &  Negate \\ 
             \midrule
Random & - &  16.7&  16.7&  16.7\\ \hline
               \multirow{2}{*}{Text-Only} & Vera & 43.7 & 70.8 & 66.2 \\ 
               & Grammar & 18.2 & 50.9 & 9.8  
               \\  \hline
               \multirow{3}{*}{} & BART & 38.8 & 53.3 & 44.4  \\
               & Flan-T5 & 43.0& 69.5 & 13.6   \\
               & OPT & 53.3& 72.7 & 5.0 
               \\  \hline
                & BLIP & 55.4 &	69.7 &	60.8  \\ \hline
               \multirow{5}{*}{}&  CLIP &  22.3 &  \cellcolor{softgray}26.6 &  \cellcolor{softgray}28.8 \\ 
               &  LAION2B-CLIP &  23.6 &  24.8 &  18.0 \\ 
               &  LAION5B-CLIP &  24.2 &  23.9 &  20.1 \\
               & BLIP-ITC & 24.8 & 17.7 &	26.5 \\ 
              & BLIP-ITM & \cellcolor{softgray}29.5  & 20.7 &	25.5  \\ 
              \hline
              \multirow{3}{*}{\Large } & Ours () & {\bf 73.2} & {\bf 78.1} & {\bf 79.6} \\
              & Ours () & 20.6 &	28.3 &	35.6\\
              & Ours () & {\bf 73.3} &  {\bf 78.1} &	{\bf 79.6}  \\
             \bottomrule
\end{NiceTabular}
         \\ 
        \multicolumn{1}{c}{\Large \bf (c) Accuracy on SugarCrepe} & \multicolumn{1}{c}{\Large \bf (d) Accuracy on Crepe} \\ 
    \end{tabular}
    }
    \caption{\small {\bf OTS generative VLMs are SOTA on image-to-text retrieval benchmarks.} We begin by evaluating \colorbox{softred}{blind language models (in red)}. Surprisingly, this already produces SOTA accuracy on certain benchmarks such as ARO-Flickr, compared to the \colorbox{softgray}{best discriminative approaches (in gray)}. We also find that blind inference of generative VLMs, \colorbox{softblue}{\smash{ via sampling Gaussian noise images (in blue)}\vphantom{g}}, often performs better and achieve above-chance performance even on the most recent SugarCrepe. Next, we show that simply repurposing a generative VLM's language generation head for computing image-text scores \colorbox{softyellow}{(VisualGPTScore in yellow)}, which corresponds to , consistently produces SOTA accuracy across all benchmarks. Finally, debiasing this score by \colorbox{softgreen}{tuning  on val set (in green)} further improves performance, establishing the new SOTA.} 
\label{tab:i_to_t_retrieval}
\end{table}

{\bf Estimating  using Monte Carlo sampling (oracle approach).} Given , we can estimate  via classic Monte Carlo sampling~\citep{shapiro2003monte}, by drawing  images from the train distribution, such as LAION114M~\citep{laion400m} for BLIP:



{\bf Reducing sampling cost with content-free images (our approach).} The above \autoref{eq:monte_carlo} requires many trainset samples to achieve robust estimates. To address this, we draw inspiration from \citep{calibratellm}, which uses a {\em content-free} text prompt ``{\tt N/A}'' to calibrate the probability of a text from LLMs, i.e., . To apply this to our generative VLMs, we choose to sample ``null'' inputs as Gaussian noise images. As a result, our approach requires as few as three images to compute Eq.~\ref{eq:monte_carlo} by sampling from Gaussian noise images with a mean of 0.4 and a standard deviation of 0.25. We find this method to be less computationally demanding and just as effective as sampling thousands of images from trainset. We provide a visual illustration of this method in \autoref{fig:visual_gpt_score}-b. We include sampling details in \autoref{app:alpha_ablation}. 


{\bf Benchmarks and evaluation protocols.} We comprehensively report on four popular I-to-T retrieval benchmarks, including ARO~\citep{aro}, Crepe~\citep{crepe}, SugarCrepe~\cite{sugarcrepe}, and VL-CheckList~\citep{vlchecklist}. In these datasets, each image has a single positive caption and multiple negative captions. ARO~\citep{aro} has four datasets: VG-Relation, VG-Attribution, COCO-Order, and Flickr30k-Order. SugarCrepe~\citep{sugarcrepe} has three datasets: Replace, Swap, and Add. For Crepe~\citep{crepe}, we use the entire productivity set and report on three datasets: Atom, Negate, and Swap. VL-CheckList~\citep{vlchecklist} has three datasets: Object, Attribute, and Relation. We visualize all datasets in Appendix~\autoref{tab:i2t_datasets}.







{\bf SOTA performance on all four benchmarks. } In \autoref{tab:i_to_t_retrieval}, we show that our OTS generative approaches, based on the BLIP model pre-trained on LAION-114M with ViT-L image encoder, achieves state-of-the-art results on all benchmarks. We outperform the best discriminative VLMs, including LAION5B-CLIP, and consistently surpass other heavily-engineered solutions, including NegCLIP, SyViC, MosaiCLIP, DAC, SVLC, SGVL, Structure-CLIP, all of which fine-tune CLIP on much more data. Details on how we report the baseline results can be found in \autoref{app:other}. For reference, we also include results of text-only Vera and Grammar from \citet{sugarcrepe}. To show that even the most recent SugarCrepe is not exempt from language biases, we run two more text-only methods:
\begin{enumerate}
    \item : passing captions into a pure LLM, such as BART-base~\citep{bartscore}, FLAN-T5-XL~\citep{flan}, and OPT-2.7B~\citep{opt}, to compute a text-only GPTScore~\citep{fu2023gptscore}.
    \item : passing both captions and Gaussian noise images to BLIP as shown in \autoref{fig:visual_gpt_score}.
\end{enumerate}


\begin{table}[h]
\centering
\scalebox{0.87}{
\begin{tabular}{cc|cc}
\toprule
\textbf{Alpha-Tuning} & \textbf{Prior Frequency} & \textbf{Alpha-Tuning} & \textbf{Prior Frequency}  \\
\midrule
\includegraphics[width=0.19\textwidth]{images/diagnosis/vg_relation_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/vg_relation_frequency.png} & \includegraphics[width=0.19\textwidth]{images/diagnosis/crepe_atom_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/crepe_atom_frequency.png} \\
\multicolumn{2}{c}{ARO (VG-Relation)} & \multicolumn{2}{c}{Crepe (Atom-Foils)} \\
\midrule
\includegraphics[width=0.19\textwidth]{images/diagnosis/vg_attribution_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/vg_attribution_frequency.png} & \includegraphics[width=0.19\textwidth]{images/diagnosis/crepe_negate_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/crepe_negate_frequency.png} \\
\multicolumn{2}{c}{ARO (VG-Attribution)} & \multicolumn{2}{c}{Crepe (Negate)} \\
\midrule
\includegraphics[width=0.19\textwidth]{images/diagnosis/flickr_order_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/flickr_order_frequency.png} & \includegraphics[width=0.19\textwidth]{images/diagnosis/crepe_swap_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/crepe_swap_frequency.png} \\
\multicolumn{2}{c}{ARO (Flickr30K-Order)} & \multicolumn{2}{c}{Crepe (Swap)} \\
\midrule
\includegraphics[width=0.19\textwidth]{images/diagnosis/sugarcrepe_add_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/sugarcrepe_add_frequency.png} & \includegraphics[width=0.19\textwidth]{images/diagnosis/vl_checklist_action_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/vl_checklist_action_frequency.png} \\
\multicolumn{2}{c}{SugarCrepe (Add)} & \multicolumn{2}{c}{VL-CheckList (Relation)} \\
\midrule
\includegraphics[width=0.19\textwidth]{images/diagnosis/sugarcrepe_replace_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/sugarcrepe_replace_frequency.png} & \includegraphics[width=0.19\textwidth]{images/diagnosis/sugarcrepe_swap_alpha.png} & \includegraphics[width=0.31\textwidth]{images/diagnosis/sugarcrepe_swap_frequency.png} \\
\multicolumn{2}{c}{SugarCrepe (Replace)} & \multicolumn{2}{c}{SugarCrepe (Swap)} \\
\bottomrule
\end{tabular}
}
\caption{\small {\bf -debiasing on I-to-T benchmarks and  frequency charts of both positive and negative captions.} Increasing  from 0 to 1 hurts performance
on benchmarks with non-sensical negative captions such as ARO and Crepe. Such negative captions are easier to identify because of their low score under the language prior  , implying such benchmarks may even be solved with blind algorithms that avoid looking at images. On the other hand, for benchmarks like SugarCrepe with more balanced  between positives and negatives, tuning  may lead to performance gain.
}
\label{tab:datasets_plots}
\end{table}

{\bf Visualization of -debiasing.} Finally, we observe that -debiasing can consistently improve the performance. For visualization, we attach the results of -debiasing in \autoref{tab:datasets_plots}. We show side-by-side frequency charts of  for positive and negative captions. 




















































































\section{Additional Experimental Results}
\label{sec:additional_exp}
In this section, we apply our OTS generative approaches to more benchmarks, including two compositionality benchmarks Winoground~\citep{winoground} and EqBen~\citep{eqben}, and two classic large-scale retrieval benchmarks COCO~\citep{coco} and Flickr30K~\citep{flickr30k}. While naively applying VisualGPTScore leads to bad performance on these benchmarks, our training-free debiasing solution can consistently improve its performance with a held-out validation set. Furthermore, we derive the optimal text-to-image (T-to-I) retrieval objective and show that OTS generative scores can achieve robust T-to-I performance. 

{\bf Evaluation protocols of \citet{winoground}.} While prior analysis~\citep{diwan2022winoground, aro} suggests that Winoground is too out-of-distribution to evaluate compositionality, we argue that evaluation protocols of Winoground and EqBen are more robust for future evaluations of VLMs. In these two benchmarks, each sample consists of two image-text pairs, ensuring {\bf uniform image and text priors}. For simplicity, we consider a single Winoground sample:  and . The joint probabilities are . Meanwhile, . Applying the law of total probability gives . A similar derivation can show that image priors are uniform too. In addition, Winoground's evaluation metrics (text score and image score) penalize unimodal shortcut solutions. For example, in I-to-T retrieval, the {\em text score} gets 1 point only if {\em both images} are matched to the correct caption. Therefore, ``blind'' solutions that choose the same text regardless of images will get 0 text score. Similarly, for T-to-I retrieval, the {\em image score} gets 1 point only if {\em both captions} are matched to the correct image.





{\bf Tuning  through cross validation.} In \autoref{tab:other_benchmarks}-a, we first show that OTS generative scores without debiasing (=0) lead to inferior performance on these I-to-T benchmarks. This confirms the importance of -debiasing; even a simple  can consistently and often significantly improve their I-to-T results. Furthermore, we try to use a held-out validation set to tune for optimal . We sample half of the data as validation set to search for  (using a step size of 0.001) and report the performance on the other half. We repeat this process 10 times to and report the mean and std. We observe that the optimal alpha is usually stable under the same dataset, regardless of the sampled val set.
For COCO and Flickr30K, we perform -debiasing using Recall@1 (R@1) on the official validation split. Because sampling additional Gaussian noise images can be too costly on these large-scale benchmarks, we directly approximate  by averaging the scores of testset images, without incurring any computational cost.  More ablation studies such as -debiasing using testset can be found in \autoref{app:alpha_ablation}. We also include the results of the ITMScore of BLIP for reference. While our debiasing solution can always boost performance, we observe that generative approaches still lag behind the ITMScore. This motivates us to study biases of generative scores towards more ``common'' texts in \autoref{app:biased}.



\begin{table}[h]
  \centering
  \scalebox{0.59}{
    \begin{tabular}[h]{cc}
        \begin{NiceTabular}{llccccc}
            \CodeBefore
              \rectanglecolor{softyellow}{3-4}{6-4}
              \rectanglecolor{softpink}{3-5}{6-5}
              \rectanglecolor{softgreen}{3-6}{6-6}
            \Body
            \toprule
            \multirow{2}{*}{Metric} & \multirow{2}{*}{Benchmark}
            & \multirow{2}{*}{ITMScore} & \multicolumn{4}{c}{\Large } \\ \cmidrule{4-7} 
            & & & =0 & =1 & = &   \\
            \midrule
            \multirow{2}{*}{Text Score}& Winoground   & 35.5 & 27.5 & 33.7 & 36.6 &  0.855  \\
            & EqBen   & 26.1 & 9.6 & 19.8 & 19.8 &  0.992 \\
            \midrule
            \multirow{2}{*}{{\small R@1 / R@5}}& COCO   & 71.9 / 90.6 & 19.7 / 40.6 & 46.2 / 73.1 & 48.0 / 74.2 & 0.819 \\
            & Flickr30k   & 88.8 / 98.2 & 34.6 / 59.0 & 58.7 / 88.0 & 63.6 / 89.2 & 0.719 \\

            \bottomrule
        \end{NiceTabular}
        &
        \begin{tabular}{llcc}
            \toprule
            \multirow{1}{*}{Metric} & \multirow{1}{*}{Benchmark}
            & \multirow{1}{*}{{\footnotesize ITMScore}} & {}\\
            \midrule
            \multirow{2}{*}{{Image Score}}& Winoground &  15.8 & 21.5\\
            & EqBen   & 20.3 & 26.1\\
            \midrule
            \multirow{2}{*}{{\footnotesize R@1 / R@5}}& COCO   &  54.8 / 79.0 & 55.6 / 79.2\\
            & Flickr30k   & 77.8 / 93.9 & 76.8 / 93.4\\

            \bottomrule
        \end{tabular} \\
        \multicolumn{1}{c}{{\large (a) -debiasing on val sets for I-to-T retrieval}} & \multicolumn{1}{c}{{\large (b) T-to-I retrieval}}
    \end{tabular}
    }
    \caption{\small {\bf Additional results on Winoground/EqBen/COCO/Flickr30K retrieval benchmarks.} Table (a) shows the importance of -debiasing on these compositionality and large-scale retrieval benchmarks. While OTS generative scores do not work well, debiasing with a larger  close to 1 can consistently and often significantly improve I-to-T performance. To highlight the improvement, we mark \colorbox{softyellow}{results without debiasing () (in yellow)}, \colorbox{softpink}{debiasing with a fixed  (in pink)}, and \colorbox{softgreen}{cross-validation using held-out val sets () (in green)}. Table (b) shows that OTS generative scores can obtain favorable results on all T-to-I retrieval tasks, competitive with the ITMScore. }
    \label{tab:other_benchmarks}
\end{table}


{\bf Extending to T-to-I retrieval.} Though not the focus of our work, we also show that image-conditioned language models can be applied to T-to-I retrieval. Given a text caption , we can rewrite the Bayes optimal T-to-I retrieval objective as:

\autoref{eq:text_to_image} is hard to implement because we do not have access to . However, when  is approximately uniform, one can directly apply  for optimal performance. We report T-to-I performance on all four benchmarks in \autoref{tab:other_benchmarks}-b, where our generative approach obtain competitive results compared against ITMScore, presumably because T-to-I retrieval is less affected by language biases.












\section{Discussion and Limitations}
{\bf Summary.} Our study shows the efficacy of {\em generative} pre-training scores in solving {\em discriminative} tasks. With the rise of generative pre-training in recent models like GPT-4~\citep{gpt4}, we see our work as a reliable starting point for future tasks. We present a first-principles analysis to account for mismatching distributions over text between train and test data. Based on this, we introduce a robust training-free (zero-shot) solution to debias linguistic priors in generative scores, achieving consistent and often significant improvement on all I-to-T retrieval tasks. Our thorough analysis also explains the performance discrepancy of generative scores on different benchmarks, and we hope it can encourage future work to revisit the issue of language biases in vision-language benchmarks.

{\bf Limitations and future work.} Our approach depends on generative VLMs pre-trained on noisy web datasets, which may result in inherited biases~\citep{mehrabi2021survey}. We do not explore fine-tuning techniques due to computational constraints, but it is possible to improve the I-to-T retrieval performance using hard negative samples, such as with controllable generation~\citep{keskar2019ctrl}. Furthermore, our analysis is based on simplified assumptions. For instance, the image-conditioned language model might not accurately represent , a phenomenon we examine in \autoref{app:biased}. Estimating   by sampling Gaussian noise images can be suboptimal; future VLMs could directly model , or use techniques like coreset selection~\citep{guo2022deepcore} or dataset distillation~\citep{wu2023multimodal} to sample more representative images. Finally, we leave debiasing on the T-to-I retrieval task for future work.













\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\vspace{-0.1in}
\section*{\Large Appendix}







        





















\section{Comparison to PMI}
\label{app:pmi}
By assuming  to be a ``flatten'' version of , our \autoref{eq:debias_i2t} can interpolate between scenario 1 (same train and test priors) and 2 (balanced test priors):

In fact, the above equation can be rewritten using the language of PMI~\citep{role2011handling, daille1994approche}, a well-known variant of PMI that controls the amount of debiasing~\citep{pmimt1, pmimt2, pmiretrieval} in information retrieval:

where 

PMI is an information-theoretic measure that quantifies the {\em association} between two variables~\citep{yao2010co, henning2017estimating, shrivastava2021clip}. In the context of image-text retrieval, it measures how much more (or less) likely the image-text pair co-occurs than if the two were independent. Eq.~\ref{eq:PMI} has found applications in diverse sequence-to-sequence modelling tasks~\citep{pmiretrieval, pmimt2, pmimt1} as a retrieval (reranking) objective. Compared to the conditional likelihood , PMI reduces the learned bias for preferring "common" texts with high marginal probabilities ~\citep{pmimt1, pmimt2, pmiretrieval}. This can be an alternative explanation for the effectiveness of our debiasing solutions. 

\section{Ablation Studies on -Debiasing}
\label{app:alpha_ablation}
{\bf Estimating  via null (Gaussian noise) images is more sample-efficient.} We use Winoground to show that sampling Gaussian noise images to calculate  can be more efficient than sampling trainset images. As demonstrated in \autoref{tab:sampling_test}, a limited number of Gaussian noise images (e.g., 3 or 10) can surpass the results obtained with 1000 LAION images. Moreover, using null images produces less variance in the results.








\begin{table}[h]

\centering
\scalebox{0.9}{
\begin{tabular}{lcc|cc}
\toprule
\multirow{2}{*}{Sample Size} & \multicolumn{2}{c}{Guassian Noise Images} & \multicolumn{2}{c}{Trainset Images} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} & = &  & = &  \\
\midrule
3 & & 0.821 &  & 0.706 \\
10 & & 0.827 & & 0.910 \\
100 & & 0.840 & & 0.910 \\
1000 & & 0.850 & & 0.960\\
\bottomrule

\end{tabular}
}
\caption{\small {\bf Comparing sampling of Gaussian noise images and trainset images 
for estimating .} We report text scores of -debiasing on Winoground I-to-T retrieval task. We ablate 3/10/100/1000 Gaussian noise and LAION samples and report both mean and std using 5 sampling seeds. The optimal  is searched on testset via a step size of . The Gaussian noise images are sampled with a mean calculated from the LAION subset and a fixed std of . }
\label{tab:sampling_test}


\end{table}



{\bf Details of Gaussian noise samples. } Unless otherwise specified, the Gaussian noise images are sampled with a mean of  and a standard deviation of . By default, we use 100 images for Winoground, 30 images for EqBen, and 3 images for the rest of the benchmarks. We also fix the sampling seed in our code to ensure reproducibility. We leave more advanced techniques of generating null images to future works.

{\bf Alternative approach on COCO/Flickr30k: estimating  using testset images. } For large-scale retrieval benchmarks like COCO~\citep{coco} and Flickr30k~\citep{flickr30k}, we can directly average scores of all candidate images (in the order of thousands) to efficiently approximate  without the need to sample additional images. This approach incurs zero computation cost as we have already pre-computed scores between each candidate image and text. We show in \autoref{tab:retrieval_Gaussian_vs_testset} that using testset images indeed results in better performance than sampling 3 Gaussian noise images.



\begin{table}[h]
  \centering
  \scalebox{0.79}{
    \begin{tabular}[h]{cc}
        \multicolumn{2}{c}{
            \begin{tabular}[h]{llccccc}
                \toprule
                \multirow{2}{*}{Metric} & \multirow{2}{*}{Benchmark}
                & \multirow{2}{*}{} & \multirow{2}{*}{Sampling Method} & \multicolumn{3}{c}{} \\ \cmidrule{5-7} 
                & & & & =1 & = &   \\
                \midrule
                \multirow{4}{*}{{\small R@1 / R@5}}& \multirow{2}{*}{COCO}   & \multirow{2}{*}{19.7 / 40.6} & Testset Images & 46.2 / 73.1 & 48.0 / 74.2 & 0.819 \\
                & & & Null Images  & 24.4 / 52.6 & 40.4 / 66.6 & 0.600 \\ \cmidrule{2-7} & \multirow{2}{*}{Flickr30k}   & \multirow{2}{*}{34.6 / 59.0} & Testset Images & 58.7 / 88.0 & 63.6 / 89.2 & 0.719 \\
                & & & Null Images & 27.8 / 62.2 & 48.5 / 79.0 & 0.427 \\ 

                \bottomrule
            \end{tabular} 
        }
    \end{tabular}
    }
    \caption{\small {\bf I-to-T retrieval on COCO/Flickr30k using different sampling methods.} Estimating  by averaging the scores of testset images (with zero computational cost) demonstrates superior performance compared to sampling additional Gaussian noise images.}
    \label{tab:retrieval_Gaussian_vs_testset}
\end{table}

{\bf Tuning  with a validation set.} In \autoref{tab:coco_flickr_curve}, similar performance trends are observed across validation and test splits of COCO and Flickr30k I-to-T retrieval benchmarks using the same . Furthermore,  and  are empirically close. As such, our method can function as a reliable training-free debiasing method. Future studies may explore fine-tuning methods to further improve the debiasing performance.



\begin{table}[h]
    \centering
    \scalebox{0.68}{
        \begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{images/diagnosis/COCO_Retrieval_blip-large_lm_both.png} &
                \includegraphics[width=0.49\textwidth]{images/diagnosis/Flickr30k_Retrieval_blip-large_lm_both.png}
\\
            \multicolumn{1}{c}{(b) Alpha-tuning on COCO Retrieval} & \multicolumn{1}{c}{(c) Alpha-tuning on Flickr Retrieval}
        \end{tabular}
    }
    \caption{\small {\bf -debiasing results on both val set and test set for COCO/Flickr30k I-to-T retrieval.} We observe that validation and test performance are strongly correlated while we interpolate .}
    \label{tab:coco_flickr_curve}
\end{table}





\section{Is VisualGPTScore a Biased Estimator of ?}
\label{app:biased}
{\bf Retrieval performance on trainset (LAION).} This paper is built on the assumption that VisualGPTScore is a reliable estimator of . However, this simplifying assumption does not completely hold for the BLIP model we examine. We speculate that such OTS generative scores are biased towards more common texts. We witness this same phenomenon in \autoref{tab:laion_retrieval_all}, where we perform image-text retrieval on random subsets from training distribution LAION-114M~\citep{blip}.

\begin{table}[h]
  \vspace{-2mm}
  \centering
  \scalebox{0.67}{
    \begin{tabular}[h]{cc}
        \begin{NiceTabular}{llllllll}
            \CodeBefore
              \rectanglecolor{softyellow}{4-3}{9-3}
              \rectanglecolor{softpink}{4-4}{9-4}
              \rectanglecolor{softgreen}{4-5}{9-5}
\Body
            \toprule
            \multirow{3}{*}{Dataset Size} & \multicolumn{5}{c}{I-to-T Retrieval} & \multicolumn{2}{c}{T-to-I Retrieval}
            \\ \cmidrule(lr){2-6} \cmidrule(lr){7-8}
            & \multirow{2}{*}{ITM} & \multicolumn{4}{c}{} & \multirow{2}{*}{ITM} & \multirow{2}{*}{} \\ \cmidrule{3-6} 
            & & =0 & =1 & = &   \\
            \midrule
            100   & \textbf{96.0} & 59.0 & 94.0 & \textbf{95.0} & 0.535 & 95.0 & \textbf{97.0} \\
            1000   & \textbf{90.9} & 37.1 & 71.7 & 85.7 & 0.733 & 92.0 & \textbf{93.1} \\
            2000   & \textbf{87.2} & 32.8 & 62.3 & 64.3 & 0.840 & 87.8 & \textbf{89.8}\\
            5000   & \textbf{79.8} & 25.1 & 50.9 & 54.1 & 0.727 & 81.9 & \textbf{84.4}\\
            \bottomrule
        \end{NiceTabular}
        &
        \begin{tabular}[h]{l}
        \includegraphics[width=0.45\textwidth]{images/diagnosis/Laion_Retrieval.png}
        \end{tabular} \\
        \multicolumn{1}{c}{{\large (a) Performance on LAION trainset retrieval}} & \multicolumn{1}{c}{{\large (b) Alpha-tuning on LAION}}
    \end{tabular}
    }
    \caption{\small {\bf Retrieval performance on randomly sampled LAION114M subsets with varied sizes.} Table (a) shows that while OTS generative scores are robust for T-to-I retrieval, its performance degrades on I-to-T retrieval tasks when the number of candidate texts increases. This implies that OTS generative scores suffer from language biases towards certain texts even in the training set. Nonetheless, we show that our debiasing solution using either  or optimal  with a step size of 0.001, can consistently boost the performance. Figure (b) visualizes -debiasing results on LAION subsets, where each curve represents a different sample size.}
    \label{tab:laion_retrieval_all}
\end{table}





{\bf Modelling the language bias in VisualGPTScore.} As evidenced in \autoref{tab:laion_retrieval_all}, we believe VisualGPTScore is biased towards more common texts due to modelling error. To consider this error in our analysis, we rewrite the VisualGPTScore as:

where  represents the (biased) model estimate and  represents the true distribution. The model bias towards common texts is encoded by an unknown parameter .


{\bf Monte Carlo estimation using .} Because our Monte Carlo sampling method relies on , it is also a biased estimator of :



{\bf Rewriting optimal I-to-T objective with .} We can rewrite \autoref{eq:optimal_i2t} as:



{\bf -debiasing with .} Using \autoref{eq:optimal_i2t_biased_version}, we can reformulate -debiasing (\autoref{eq:debias_i2t}) as follows:


where . Notably, the above equation has the same structure as before (\autoref{eq:debias_i2t}). This implies that even if , we still anticipate . This accounts for why the optimal  is not 0 when we perform I-to-T retrieval on trainset in \autoref{tab:laion_retrieval_all}. 


{\bf Implication for vision-language modelling.} Our analysis indicates that similar to generative LLMs~\citep{pmimt1, pmimt2}, contemporary image-conditioned language models also experience issues related to imbalanced learning~\citep{kang2019decoupling}. Potential solutions could be: (a) refined sampling techniques for Monte Carlo estimation of  such as through dataset distillation~\citep{wu2023multimodal}, and (b) less biased modelling of  such as through controllable generation~\citep{keskar2019ctrl}.







\section{Experiments with BLIP-2}
\label{app:blip_2}
We provide BLIP-2 results for completeness.

{\bf BLIP-2~\citep{blip2} overview.} BLIP-2 leverages frozen pre-trained image encoders~\citep{fang2022eva} and large language models~\citep{flan, opt} to bootstrap vision-language pre-training. It proposes a lightweight Querying Transformer (Q-Former) that is trained in two stages. Similar to BLIP~\citep{blip}, Q-Former is a mixture-of-expert model that can calculate ITC, ITM, and captioning loss given an image-text pair. Additionally, it introduces a set of trainable query tokens, whose outputs serve as {\em visual soft prompts} prepended as inputs to LLMs. In its first training stage, Q-Former is fine-tuned on the same LAION dataset using the same objectives (ITC+ITM+captioning) as BLIP. In the second stage, the output query tokens from Q-Former are fed into a frozen language model, such as FLAN-T5~\citep{flan} or OPT~\citep{flan}, after a linear projection trained only with captioning loss. BLIP-2 achieves state-of-the-art performance on various vision-language tasks with significantly fewer trainable parameters.  


{\bf BLIP-2 results.} We present retrieval performance of the BLIP-2 model that uses ViT-L as the frozen image encoder. We report results for both the first-stage model (denoted as Q-Former) and the second-stage model which employs FLAN-T5~\citep{flan} as the frozen LLM. 


\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{llccccc}
\toprule
\multirow{2}{*}{Benchmark} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{Random}  & \multicolumn{3}{c}{w. Q-Former} & w. Flan-T5 \\ 
\cmidrule(lr){4-6} \cmidrule(lr){7-7} & & & ITC & ITM &  &  \\
\midrule
\multirow{4}{*}{ARO} & VG-Relation     & 50.0 & 46.4 & 67.2 & 90.7 & 89.1 \\
& VG-Attribution  & 50.0 & 76.0 & 88.1 & 94.3  & 90.9 \\
& COCO-Order      & 20.0 & 28.5 & 25.2 & 96.8 & 99.3 \\
& Flickr30K-Order & 20.0 & 25.3 & 28.6 & 97.5 & 99.7 \\ 
\midrule
\multirow{3}{*}{Crepe} & Atom-Foils      & 16.7 & 20.8 & 20.9 & 74.7 & 69.7 \\
& Negate          & 16.7 & 13.4 & 14.2 & 79.1 & 90.0 \\
& Swap            & 16.7 & 13.4 & 18.0 & 79.5 & 79.1\\ 
\midrule
VL-CheckList & Object   & 50.0 & 89.7 & 89.2 & 90.1 & 84.1 \\ 
VL-CheckList & Attribute   & 50.0 & 76.6 & 79.3 & 73.9 & 70.6 \\ 
VL-CheckList & Relation   & 50.0 & 70.5 & 72.3 & 89.9 & 56.7 \\ 
\midrule
SugarCrepe & Replace   & 50.0 & 86.7 & 88.5 & 93.0 & 82.4 \\ 
SugarCrepe & Swap   & 50.0 & 69.8 & 80.9 & 91.2 & 80.8 \\ 
SugarCrepe & Add   & 50.0 & 86.5 & 88.0 & 92.7 & 76.2 \\ 
\bottomrule
\end{tabular}
}
\vspace{1mm}
\caption{\small {\bf BLIP-2 on ARO/Crepe/VL-CheckList/SugarCrepe.}}
\label{tab:blip2_vanilla}
\end{table}


\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{NiceTabular}{lllllllllll}
        \CodeBefore
          \rectanglecolor{softyellow}{4-5}{9-5}
          \rectanglecolor{softpink}{4-6}{9-6}
          \rectanglecolor{softgreen}{4-7}{9-7}
\Body
\toprule
\multirow{3}{*}{Benchmark} & \multirow{3}{*}{Model} & \multicolumn{6}{c}{I-To-T (Text Score)} & \multicolumn{3}{c}{T-To-I (Image Score)} \\ \cmidrule(lr){3-8} \cmidrule(lr){9-11}
&  & \multirow{2}{*}{ITC} & \multirow{2}{*}{ITM} & \multicolumn{4}{c}{} & \multirow{2}{*}{ITC} & \multirow{2}{*}{ITM} & \multirow{2}{*}{} \\
\cmidrule(lr){5-8} 
 & & & & =0 & =1 & = &  &  & \\
\midrule
\multirow{3}{*}{Winoground} & BLIP & 28.0 & 35.8 & 27.0 & 33.0 & 36.5 & 0.836 & 9.0 & 15.8 & 21.5 \\
& BLIP2-QFormer & 30.0 & 42.5 & 24.3 & 29.3 & 33.0& 0.882 & 10.5 & 19.0 & 20.0\\
& BLIP2-FlanT5 & - & - & 25.3 & 31.5 & 34.3 & 0.764 & - & - & 19.5 \\
\midrule
\multirow{3}{*}{EqBen (Val)} & BLIP & 20.9 & 26.0 & 9.6 & 19.8 & 19.8 & 0.982 & 20.3 & 20.3 & 26.1 \\
& BLIP2-QFormer & 32.1 & 36.2 & 12.2 & 21.9 & 22.2 & 0.969 & 23.4 & 28.4 & 26.6 \\
& BLIP2-FlanT5 & - & - & 8.5 & 22.0 & 22.0 & 1.000 & - & - & 20.9 \\
\bottomrule
\end{NiceTabular}
}
\caption{\small {\bf BLIP-2 on Winoground/EqBen.} 
}
\label{tab:blip2_other_benchmarks}
\end{table}














\section{Additional Reports}
\label{app:other}


{\bf Computational resources.} All experiments use a single NVIDIA GeForce 3090s GPU. 

{\bf Details of \autoref{tab:i_to_t_retrieval}.} For CLIP, LAION2B-CLIP, and LAION5B-CLIP, we report the results from \citet{sugarcrepe} using the ViT-B-32, ViT-bigG-14, and xlm-roberta-large-ViT-H-14 models respectively. The results of NegCLIP, Structure-CLIP, SVLC, SGVL, DAC-LLM, and DAC-SAM are directly copied from their original papers. We run BLIP-ITC and BLIP-ITM using our own codebase, which will be released to the public.

{\bf Group scores on Winoground/EqBen using BLIP (\autoref{tab:blip_groupscore}).} 

\begin{table}[h]
\centering
\scalebox{0.8}{

\begin{tabular}{lccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Winoground} & \multicolumn{3}{c}{EqBen} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} & Text Score & Image Score & Group Score & Text Score & Image Score & Group Score \\
\midrule
ITCScore & 28.0 & 9.0 & 6.5 &  20.9 & 20.3 & 10.6 \\
ITMScore & 35.8 & 15.8 & 13.3 &  26.0 &20.3 &12.6 \\
VisualGPTScore & 36.5 &21.5 & 16.8 &20.4 &26.1 &11.7 \\
\bottomrule
\end{tabular}
}
\caption{\small Performance comparison of BLIP's ITCScore, ITMScore, and -tuned VisualGPTScore on Winoground (all) and EqBen (val). }
\label{tab:blip_groupscore}
\end{table}

{\bf Fine-grained tags on Winoground (\autoref{tab:finegrained_winoground_blip}).} 

\begin{table}[h]
\centering
\scalebox{0.9}{
\begin{tabular}{lllccc}
\toprule
Dataset & Size & Method & Text Score & Image Score & Group Score \\
\midrule
\multirow{3}{*}{NoTag} & \multirow{3}{*}{171} & ITCScore & 32.6 & 11.6 & 8.1 \\
& & ITMScore &41.9 & 21.5& 19.2\\
& & VisualGPTScore & 43.0 & 28.5 & 23.8\\
\midrule

\multirow{3}{*}{NonCompositional} & \multirow{3}{*}{30} & ITCScore & 43.3 & 16.7 & 16.7\\
& & ITMScore & 50.0& 23.3& 16.7\\
& & VisualGPTScore & 43.3& 33.3 & 26.7 \\
\midrule

\multirow{3}{*}{AmbiguouslyCorrect} & \multirow{3}{*}{46} & ITCScore & 32.6 & 8.7 &6.5 \\
& & ITMScore &28.3 & 6.5& 2.2\\
& & VisualGPTScore &26.1 & 19.6& 8.7\\
\midrule

\multirow{3}{*}{VisuallyDifficult} & \multirow{3}{*}{38} & ITCScore & 29.0& 7.9& 7.9\\
& & ITMScore & 26.3 & 10.5& 7.9\\
& & VisualGPTScore & 31.6& 13.2& 7.9\\
\midrule

\multirow{3}{*}{UnusualImage} & \multirow{3}{*}{56} & ITCScore &32.5 &8.9 & 8.9\\
& & ITMScore & 21.4 &10.7 & 7.1 \\
& & VisualGPTScore &30.4 & 10.7 & 8.9\\
\midrule

\multirow{3}{*}{UnusualText} & \multirow{3}{*}{50} & ITCScore & 20.0 & 8.0 & 6.0\\
& & ITMScore & 38.0 & 12.0& 12.0\\
& & VisualGPTScore &30.0 & 18.0& 12.0\\
\midrule

\multirow{3}{*}{ComplexReasoning} & \multirow{3}{*}{78} & ITCScore & 16.7 & 2.6 & 1.3 \\
& & ITMScore & 21.8 & 5.1 & 2.6\\
& & VisualGPTScore & 21.8& 10.3& 6.4\\

\bottomrule

\end{tabular}
}
\caption{\small BLIP performance on Winoground subtags~\citep{diwan2022winoground}. We report the number of test instances for each subtag and their respective text score, image score, group score.}
\label{tab:finegrained_winoground_blip}
\end{table}
{\bf Performance on SugarCrepe (\autoref{tab:sugar_crepe}).} 

\begin{table}[h!]
\centering
\scalebox{0.9}{
\begin{NiceTabular}{llccc|c}
\CodeBefore
          \rectanglecolor{softred}{7-3}{9-6}
          \rectanglecolor{softblue}{10-3}{10-6}
          \rectanglecolor{softyellow}{18-3}{20-6}
          \rectanglecolor{softgreen}{21-3}{23-6}
\Body
\toprule

\multirow{2}{*}{Method} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{SugarCrepe} \\
\cmidrule(lr){3-6}
& & {\bf Replace} & {\bf Swap} & {\bf Add} & {\bf AVG} \\
\midrule
Human Performance & - & 98.67 & 99.50 & 99.00 & 99.06 \\
Random Chance & - & 50.00 & 50.00 & 50.00 & 50.00 \\
\hline
\multirow{2}{*}{Text-Only Baseline} & Vera & 49.46 & 49.30 & 49.50 & 49.42 \\
& Grammar & 50.00 & 50.00 & 50.00 & 50.00  \\
\hline
\multirow{3}{*}{} & Bart & 48.41 & 51.93 & 61.16 & 53.83 \\
& Flan-T5 & 51.41 & 57.59 & 40.94 & 49.98 \\
& OPT & 58.53 & 66.58 & 45.78 & 56.96 \\
\hline
\multirow{1}{*}{} & BLIP &  {75.90} & {77.14} & { 70.89} & {74.64} \\
\hline
\multirow{5}{*}{ITCScore} & CLIP-LAION2B & 86.50 & 68.56 & 88.37 & 81.14 \\
& CLIP-LAION5B & 84.98 & 67.95 & 89.62 & 80.85 \\
& BLIP & 85.76 & 73.79 & 85.66 & 81.74 \\
& BLIP-2 & 86.66 & 69.77 & 86.50 & 80.98 \\
& NegCLIP-SugarCrepe & 88.27 & 74.89 & 90.16 & 84.44 \\
\hline
\multirow{2}{*}{ITMScore} & BLIP & 88.68 & 81.29 & 87.57 & 85.85 \\
 & BLIP2-Qformer & 88.45 & 80.87 & 87.96 & 85.76 \\
\hline
\multirow{3}{*}{} & BLIP & {\bf 93.33} & {\bf 91.00} & {\bf 90.98} & {\bf 91.77} \\
& BLIP2-Qformer & {\bf 93.00} & {\bf 91.24} & {\bf 92.69} & {\bf 92.31} \\
& BLIP2-FlanT5 & 82.44 & 76.57 & 76.24 & 78.42 \\
\hline
\multirow{3}{*}{} & BLIP &  {\bf 95.09} & {\bf 92.39} & {\bf 97.36} & {\bf 94.95} \\
& BLIP2-Qformer & {\bf 94.62} & {\bf 92.27} & {\bf 97.58} & {\bf 94.82} \\
& BLIP2-FlanT5 & 85.69 & 78.80 & 91.76 & 85.42 \\
\bottomrule
\end{NiceTabular}
}
\caption{\small {\bf Performance on SugarCrepe~\citep{sugarcrepe}.} SugarCrepe is the most recent visio-linguistic compositionality benchmark which improves upon previous Crepe~\citep{crepe} by using state-of-the-art large language models (including ChatGPT), instead of rule-based templates, to generate more natural negative text captions. We show that text-only baselines and LLM-based methods indeed fail to succeed on SugarCrepe. However, our OTS generative approaches still achieve competitive results compared against SOTA discriminative approaches. The results of human performance, text-only baseline, and SOTA CLIP and NegCLIP-SugarCrepe are directly taken from the ~\citet{sugarcrepe}. For other approaches, we evaluate their performance following the same procedure as described in main texts.}
\label{tab:sugar_crepe}
\end{table}

\clearpage

\section{Benchmark Visualization}
\label{app:dataset_visualization}
We include random samples from each benchmark in \autoref{tab:i2t_datasets}.

\begin{table}[h!]
\centering
\scalebox{0.58}{
\begin{tabular}{lcll}
\toprule
\midrule
Dataset & Image & \multicolumn{1}{l}{Positive Caption} & \multicolumn{1}{l}{Negative Caption(s)} \\ \midrule
 VG-Relation & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/vg_relation.png}} & {\textcolor{black}{the bus is to the right of the trees}} & { \textcolor{purple}{the trees is to the right of the bus}}  \\ \addlinespace \midrule
 VG-Attribution & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/vgattribution.png}} & {\textcolor{black}{the striped zebra and the large tree}} & { \textcolor{purple}{the large zebra and the striped tree}}  \\ \addlinespace \midrule
 COCO-Order & \raisebox{-0.5\height}{\includegraphics[height=1.5cm]{images/datasets/cocoorder.png}} & {\small \textcolor{black}{two dogs sharing a frisby in their mouth in the snow}} & \parbox{0.4\textwidth}{\scriptsize \textcolor{purple}{two frisby sharing a mouth in their snow in the dogs} \par \textcolor{purple}{in dogs the in frisby sharing two mouth their a snow} \par \textcolor{purple}{two dogs sharing in a frisby their mouth in snow the} \par \textcolor{purple}{a frisby in the snow two dogs sharing their mouth in}}  \\ \addlinespace \midrule
 Flickr30K-Order & \raisebox{-0.5\height}{\includegraphics[height=1.5cm]{images/datasets/flickr_order.png}} & {\small \textcolor{black}{a white duck spreads its wings while in the water}} & \parbox{0.4\textwidth}{\scriptsize \textcolor{purple}{a white wings spreads its water while in the duck} \par \textcolor{purple}{a white duck the its wings while in water spreads} \par \textcolor{purple}{white a duck spreads its wings in while the water} \par \textcolor{purple}{while in the spreads its wings water a white duck}}  \\ \addlinespace \midrule
 \parbox{0.2\textwidth}{SugarCrepe \\ Add-Attribute} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_add_att.jpg}} & {\textcolor{black}{They are going to serve pizza for lunch today.}} & {\textcolor{purple}{They are going to serve pizza topped with pineapple for lunch today.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Add-Object} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_add_obj.jpg}} & {\textcolor{black}{A man kisses the top of a woman's head.}} & {\textcolor{purple}{A man kisses the top of a woman's head with a flower in his hand.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Replace-Attribute} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_replace_att.jpg}} & {\textcolor{black}{A kid standing with a small suitcase on a street.}} & {\textcolor{purple}{A kid standing with a big suitcase on a street.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Replace-Object} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe.jpg}} & {\textcolor{black}{A duck floating in the water near a bunch of grass and rocks}} & {\textcolor{purple}{A swan floating in the water near a bunch of grass and rocks.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Replace-Relation} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_replace_rel.jpg}} & {\textcolor{black}{A clock tower stands in front of a large mirrored sky scraper.}} & {\textcolor{purple}{A clock tower stands behind a large mirrored sky scraper.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Swap-Attribute} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_swap_att.jpg}} & {\textcolor{black}{A tennis player is taking a swing on a red court.}} & {\textcolor{purple}{A red player is taking a swing on a tennis court.}}  \\ \addlinespace \midrule
  \parbox{0.2\textwidth}{SugarCrepe \\ Swap-Object} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/sugarcrepe_swap_obj.jpg}} & {\textcolor{black}{A woman holding a game controller with a man looking on.}} & {\textcolor{purple}{A man holding a game controller with a woman looking on.}}  \\ \addlinespace \midrule
Crepe-AtomFoils & \raisebox{-0.5\height}{\includegraphics[height=1.5cm]{images/datasets/crepeatom.png}} & {\small \textcolor{black}{microwave in a kitchen, and sink in a kitchen.}} & \parbox{0.4\textwidth}{\scriptsize \textcolor{purple}{microwave in a cupboard, and sink in a kitchen} \par \textcolor{purple}{microwave in a bar, and sink in a kitchen} \par \textcolor{purple}{line in a kitchen, and sink in a kitchen} \par \textcolor{purple}{microwave in a kitchen, and shower in a kitchen} \par \textcolor{purple}{microwave in a kitchen, and tap in a kitchen}}  \\ \addlinespace \midrule
Crepe-Negate & \raisebox{-0.5\height}{\includegraphics[height=1.5cm]{images/datasets/crepe_negate.png}} & {\small \textcolor{black}{a chair next to a table, with the back of the chair visible.}} & \parbox{0.55\textwidth}{\scriptsize \textcolor{purple}{A chair is not next to a table, with the back of the chair visible} \par \textcolor{purple}{A chair next to a table, with the back not of the chair visible} \par \textcolor{purple}{A chair next to a table, with the back of the chair visible} \par \textcolor{purple}{A chair next to a table, with something of the chair visible. There is no back.} \par \textcolor{purple}{There is no chair next to a table, with the back of the chair visible}}  \\ \addlinespace \midrule
Crepe-Swap & \raisebox{-0.5\height}{\includegraphics[height=1.5cm]{images/datasets/crepe_swap.png}} & {\small \textcolor{black}{a car driving on a road with a line next to a tree.}} & \parbox{0.55\textwidth}{\scriptsize \textcolor{purple}{a car driving on a bright green leaves with a line next to a tree} \par \textcolor{purple}{a bright green leaves driving on a road with a line next to a tree} \par \textcolor{purple}{a car driving on a tree with a line next to a road} \par \textcolor{purple}{a car driving on a road with a line next to a white car} \par \textcolor{purple}{a car driving on a road with a line next to a street}}  \\ \addlinespace \midrule
\parbox{0.2\textwidth}{\small VL-CheckList \\ Relation (spatial)} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/vlchecklist_relation_action.png}} & {\textcolor{black}{person read book}} & {\textcolor{purple}{person carry book}}  \\ \addlinespace \midrule
\parbox{0.2\textwidth}{\small VL-CheckList \\ Relation (action)} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/vlchecklist_relation_spatial.png}} & {\textcolor{black}{sign near boy}} & {\textcolor{purple}{sign far from book}}  \\ \addlinespace \midrule
\midrule
\multirow{2}{*}{Winoground} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/winoground_0.jpg}} & {\textcolor{black}{a  person on top of the world}} & {\textcolor{purple}{the world on top of a person}}  \\ \addlinespace
& \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/winoground_1.jpg}} & {\textcolor{black}{the world on top of a person}} & {\textcolor{purple}{a person on top of the world}}  \\ \addlinespace \midrule
\multirow{2}{*}{EqBen} & \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/eqben_0_new.png}} & {\textcolor{black}{The person is touching the dish which is in front of him/her.}} & {\textcolor{purple}{The person is holding the dish which is in front of him/her.}}  \\ \addlinespace
& \raisebox{-0.5\height}{\includegraphics[height=1.2cm]{images/datasets/eqben_1_new.png}} & {\textcolor{black}{The person is holding the dish which is in front of him/her.}} & {\textcolor{purple}{The person is touching the dish which is in front of him/her.}}  \\ \addlinespace \midrule
\bottomrule
\end{tabular}
}
\caption{\small {\bf Visualization of benchmarks.} ARO (VG-Relation/VG-Attribution/COCO-Order/Flickr30K-Order), Crepe (AtomFoils/Negate/Swap), VL-CheckList (Object/Attribute/Relation), SugarCrepe (Replace/Swap/Add) are constructed by generating hard negative captions for an image-text pair. On the other hand, each sample of Winoground and EqBen has two image-text pairs. }
\label{tab:i2t_datasets}
\end{table}



\end{document}
