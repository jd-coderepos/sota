\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 

\usepackage{booktabs}
\usepackage{multirow, makecell}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{color, colortbl}
\usepackage{mathtools}
\usepackage{appendix}

\usepackage{xspace}


\usepackage{hyperref}
\usepackage{url}

\usepackage{enumitem}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.3em \@plus1ex \@minus.2ex}{-.7em}{\normalfont\normalsize\bfseries}}\makeatother

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{Open-vocabulary Object Detection via \\
Vision and Language Knowledge Distillation}



\author{
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui \\
Google Research,
Nvidia\thanks{Work done while Xiuye was a Google AI Resident and Tsung-Yi was at Google.}\\
\texttt{\{xiuyegu, weicheng, yincui\}@google.com}\quad\quad\texttt{tsungyil@nvidia.com}
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother



\newcommand{\bigCI}{\mathrel{\text{\scalebox{1.07}{}}}}

\captionsetup[table]{font=footnotesize}
\captionsetup[figure]{font=footnotesize}

\newenvironment{betteritemize}
{ \begin{itemize}[leftmargin=*]
  \setlength{\itemsep}{0ex}
}
{ \end{itemize} } 

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs.
The fundamental challenge is the availability of training data. 
It is costly to further scale up the number of classes contained in existing object detection datasets.
To overcome this challenge, we propose \textbf{ViLD}, a training method via \textbf{Vi}sion and \textbf{L}anguage knowledge \textbf{D}istillation.
Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student).
Specifically, we use the teacher model to encode category texts and image regions of object proposals.
Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher.
We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. 
ViLD obtains 16.1 mask AP with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves \textbf{26.3} AP.
The model can directly transfer to other datasets without finetuning, achieving 72.2 AP on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365.
On COCO, ViLD outperforms the previous state-of-the-art~\citep{zareian2021openvocabulary} by 4.8 on novel AP and 11.4 on overall AP.
Code and demo are open-sourced at \url{https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild}.
\end{abstract}




\section{Introduction}
Consider Fig.~\ref{fig:teaser}, can we design object detectors beyond recognizing only \textit{base categories} (\eg, toy) present in training labels and expand the vocabulary to detect \textit{novel categories} (\eg, toy elephant)? In this paper, we aim to train an \textit{open-vocabulary} object detector that detects objects in any novel categories described by text inputs, using only detection annotations in base categories.

Existing object detection algorithms often learn to detect only the categories present in detection datasets. A common approach to increase the detection vocabulary is by collecting images with more labeled categories.
The research community has recently collected new object detection datasets with large vocabularies~\citep{lvis,OpenImages}.
LVIS~\citep{lvis} is a milestone of these efforts by building a dataset with 1,203 categories.
With such a rich vocabulary, it becomes quite challenging to collect enough training examples for all categories.
By Zipf's law, object categories naturally follow a long-tailed distribution. To find sufficient training examples for rare categories, significantly more data is needed~\citep{lvis}, which makes it expensive to scale up detection vocabularies.

On the other hand, paired image-text data are abundant on the Internet.
Recently, \citet{radford2021clip} train a joint vision and language model using 400 million image-text pairs and demonstrate impressive results on directly transferring to over 30 datasets.
The pretrained text encoder is the key to the zero-shot transfer ability to arbitrary text categories.
Despite the great success on learning image-level representations, learning object-level representations for open-vocabulary detection is still challenging.
In this work, we consider borrowing the knowledge from a pretrained open-vocabulary classification model to enable open-vocabulary detection.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/new_teaser_iclr.pdf}
    \vspace{-1.5ex}
    \caption{\textbf{An example of our open-vocabulary detector with arbitrary texts.} After training on base categories (purple), we can detect novel categories (pink) that are not present in the training data.
    }
\label{fig:teaser}
\vspace{-3ex}
\end{figure}


We begin with an R-CNN~\citep{girshick2014rcnn} style approach.
We turn open-vocabulary detection into two sub-problems: 1) generalized object proposal and 2) open-vocabulary image classification.
We train a region proposal model using examples from the base categories. Then we use the pretrained open-vocabulary image classification model to classify cropped object proposals, which can contain both base and novel categories.
We benchmark on LVIS~\citep{lvis} by holding out all rare categories as novel categories and treat others as base categories.
To our surprise, the performance on the novel categories already surpasses its supervised counterpart.
However, this approach is very slow for inference, because it feeds object proposals one-by-one into the classification model.

To address the above issue, we propose \textbf{ViLD} (\textbf{Vi}sion and \textbf{L}anguage knowledge \textbf{D}istillation) for training two-stage open-vocabulary detectors. ViLD consists of two components: learning with text embeddings (ViLD-text) and image embeddings (ViLD-image) inferred by an open-vocabulary image classification model, \eg, CLIP. 
In \textbf{ViLD-text}, we obtain the text embeddings by feeding category names into the pretrained text encoder.
Then the inferred text embeddings are used to classify detected regions.
Similar approaches have been used in prior detection works~\citep{bansal2018zero,rahman2018zero,zareian2021openvocabulary}.
We find text embeddings learned jointly with visual data can better encode the visual similarity between concepts, compared to text embeddings learned from a language corpus, \eg, GloVe~\citep{pennington2014glove}.
Using CLIP text embeddings achieves \textbf{10.1} AP (AP of novel categories) on LVIS, significantly outperforming the \textbf{3.0} AP of using GloVe.
In \textbf{ViLD-image}, we obtain the image embeddings by feeding the object proposals into the pretrained image encoder.
Then we train a Mask R-CNN whose region embeddings of detected boxes are aligned with these image embeddings.
In contrast to ViLD-text, ViLD-image distills knowledge from both base and novel categories since the proposal network may detect regions containing novel objects, while ViLD-text only learns from base categories.
Distillation enables ViLD to be \textit{general} in choosing teacher and student architectures. ViLD is also \textit{energy-efficient} as it works with off-the-shelf open-vocabulary image classifiers. We experiment with the CLIP and ALIGN~\citep{align} teacher models with different architectures (ViT and EfficientNet).

We show that ViLD achieves \textbf{16.1} AP for novel categories on LVIS, surpassing the supervised counterpart by \textbf{3.8}.
We further use ALIGN as a stronger teacher model to push the performance to \textbf{26.3} novel AP, which is close (only 3.7 worse) to the 2020 LVIS Challenge winner~\citep{huang2020joint} that is fully-supervised.
We directly transfer ViLD trained on LVIS to other detection datasets without finetuning, and obtain strong performance of 72.2 AP on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365.
We also outperform the previous state-of-the-art open-vocabulary detector on COCO~\citep{zareian2021openvocabulary} by 4.8 novel AP and 11.4 overall AP.


\begin{figure}[t]
\centering
   \scalebox{1.0}[0.95]{\includegraphics[width=0.9\linewidth]{figures/new_overview_new_font.pdf}}
   \vspace{-1.5ex}
   \caption{
   \textbf{An overview of using ViLD for open-vocabulary object detection.}
   ViLD distills the knowledge from a pretrained open-vocabulary image classification model.
   First, the category text embeddings and the image embeddings of cropped object proposals are computed, using the text and image encoders in the pretrained classification model. 
   Then, ViLD employs the text embeddings as the region classifier (ViLD-text) and minimizes the distance between the region embedding and the image embedding for each proposal (ViLD-image).
   During inference, text embeddings of novel categories are used to enable open-vocabulary detection.
   }
\label{fig:overview}
\vspace{-2.5ex}
\end{figure}



\section{Related Work}
\vspace{-5pt}
\paragraph{Increasing vocabulary in visual recognition:}
Recognizing objects using a large vocabulary is a long-standing research problem in computer vision.
One focus is zero-shot recognition, aiming at recognizing categories not present in the training set. Early works~\citep{farhadi2009describing,rohrbach2011evaluating,jayaraman2014zero} use visual attributes to create a binary codebook representing categories, which is used to transfer learned knowledge to unseen categories. In this direction, researchers have also explored class hierarchy, class similarity, and object parts as discriminative features to aid the knowledge transfer~\citep{rohrbach2011evaluating,akata2016multi,zhao2017open_vocab_scene_parsing,elhoseiny2017link,ji2018stacked,cacheux2019modeling,xie2020region}. Another focus is learning to align latent image-text embeddings, which allows to classify images using arbitrary texts. \citet{frome2013devise} and \citet{norouzi2013zero} are pioneering works that learn a visual-semantic embedding space using deep learning. \citet{wang2018zero} distills information from both word embeddings and knowledge graphs. Recent work CLIP~\citep{radford2021clip} and ALIGN~\citep{align} push the limit by collecting million-scale image-text pairs and then training joint image-text models using contrastive learning. 
These models can directly transfer to a suite of classification datasets and achieve impressive performances.
While these work focus on image-level open-vocabulary recognition, we focus on detecting objects using arbitrary text inputs.

\paragraph{Increasing vocabulary in object detection:}
It's expensive to scale up the data collection for large vocabulary object detection.
\citet{zhao2020object} and \citet{zhou2021simple} unify the label space from multiple datasets. \citet{joseph2021towards} incrementally learn identified unknown categories.
Zero-shot detection (ZSD) offers another direction. Most ZSD methods align region features to pretrained text embeddings in base categories~\citep{bansal2018zero,demirel2018zero,rahman2019transductive,hayat2020synthesizing,zheng2020background}. However, there is a large performance gap to supervised counterparts. To address this issue, \citet{zareian2021openvocabulary} pretrain the backbone model using image captions and finetune the pretrained model with detection datasets. In contrast, we use an image-text pretrained model as a teacher model to supervise student object detectors.
All previous methods are only evaluated on tens of categories, while we are the first to evaluate on more than 1,000 categories. 



\section{Method}
\label{sec:methold}

\vspace{-5pt}
\paragraph{Notations:} We divide categories in a detection dataset into the base and novel subsets, and denote them by  and . Only annotations in  are used for training. We use  to denote the text encoder and  to denote the image encoder in the pretrained open-vocabulary image classifier.




\subsection{Localization for novel categories}\label{subsec:method_localization}
The first challenge for open-vocabulary detection is to localize novel objects.
We modify a standard two-stage object detector, \eg, Mask R-CNN~\citep{he2017mask}, for this purpose. 
We replace its class-specific localization modules, \ie, the second-stage bounding box regression and mask prediction layers, with class-agnostic modules for general object proposals.
For each region of interest, these modules only predict a single bounding box and a single mask for all categories, instead of one prediction per category.
The class-agnostic modules can generalize to novel objects.

\subsection{Open-vocabulary detection with cropped regions}
\label{subsec:detection_with_cropped_regions}
Once object candidates are localized, we propose to reuse a pretrained open-vocabulary image classifier to classify each region for detection. 

\paragraph{Image embeddings:} We train a proposal network on base categories  and extract the region proposals  offline.
We crop and resize the proposals, and feed them into the pretrained image encoder  to compute image embeddings , where  is the image.


We ensemble the image embeddings from  and  crops, as the  crop provides more context cues. The ensembled embedding is then renormalized to unit norm: 


\paragraph{Text embeddings:} We generate the text embeddings offline by feeding the category texts with prompt templates, \eg, ``a photo of \{category\} in the scene'', into the text encoder .
We ensemble multiple prompt templates and the synonyms if provided.

Then, we compute cosine similarities between the image and text embeddings. 
A softmax activation is applied, followed by a per-class NMS to obtain final detections.
The inference is slow since every cropped region is fed into .

\subsection{ViLD: Vision and Language knowledge Distillation.}\label{subsec:vild}
We propose \textbf{ViLD} to address the slow inference speed of the above method.
\textbf{ViLD} learns region embeddings in a two-stage detector to represent each proposal .
We denote region embeddings by , where  is a backbone model and  is a lightweight head that generates region embeddings.
Specifically, we take outputs before the classification layer as region embeddings.


\begin{figure}[t]
\centering
  \includegraphics[width=0.9\linewidth]{figures/new_head_loss_iclr_ty_adjusted.pdf}
  \vspace{-1ex}
   \caption{\textbf{Model architecture and training objectives.} \textbf{(a)} The classification head of a vanilla two-stage detector, \eg, Mask R-CNN. \textbf{(b)} ViLD-text replaces the classifier with fixed text embeddings and a learnable background embedding. The projection layer is introduced to adjust the dimension of region embeddings to be compatible with the text embeddings. \textbf{(c)} ViLD-image distills from the precomputed image embeddings of proposals with an  loss. \textbf{(d)} ViLD combines ViLD-text and ViLD-image.
   }
\label{fig:model_architecture}
\vspace{-2ex}
\end{figure}


\paragraph{Replacing classifier with text embeddings:}
We first introduce \textbf{ViLD-text}. Our goal is to train the region embeddings such that they can be classified by text embeddings. Fig.~\ref{fig:model_architecture}(b) shows the architecture and training objective. ViLD-text replaces the learnable classifier in Fig.~\ref{fig:model_architecture}(a) with the text embeddings introduced in Sec.~\ref{subsec:detection_with_cropped_regions}.
Only , the text embeddings of , are used for training. 
For the proposals that do not match any groundtruth in , they are assigned to the background category. Since the text ``background'' does not well represent these unmatched proposals, we allow the background category to learn its own embedding .
We compute the cosine similarity between each region embedding  and all category embeddings, including  and .
Then we apply softmax activation with a temperature  to compute the cross entropy loss. 
To train the first-stage region proposal network of the two-stage detector, we extract region proposals  online, and train the detector with ViLD-text from scratch.
The loss for ViLD-text can be written as:

where ,  denotes elements in ,
 denotes the class label of region ,  is the number of proposals per image (), and  is the cross entropy loss.

During inference, we include novel categories () and generate 
(sometimes  only) for open-vocabulary detection (Fig.~\ref{fig:overview}). Our hope is that the model learned from annotations in  can generalize to novel categories .


\paragraph{Distilling image embeddings:}
We then introduce \textbf{ViLD-image}, which aims to distill the knowledge from the teacher image encoder  into the student detector.
Specifically, we align region embeddings
 
to image embeddings
 
introduced in Sec.~\ref{subsec:detection_with_cropped_regions}.

To make the training more efficient, we extract  proposals  offline for each training image, and precompute the  image embeddings. These proposals can contain objects in both  and , as the network can generalize. 
In contrast, ViLD-text can only learn from .
We apply an  loss between the region and image embeddings to minimize their distance.
The ensembled image embeddings in Sec.~\ref{subsec:detection_with_cropped_regions} are used for distillation:

Fig.~\ref{fig:model_architecture}(c) shows the architecture. \citet{zhu2019deformable} use a similar approach to make Faster R-CNN features mimic R-CNN features, however, the details and goals are different: They reduce redundant context to improve supervised detection; while ViLD-image is to enable open-vocabulary detection on novel categories.


The total training loss of \textbf{ViLD} is simply a weighted sum of both objectives:

where  is a hyperparameter weight for distilling the image embeddings.
Fig.~\ref{fig:model_architecture}(d) shows the model architecture and training objectives. 
ViLD-image distillation only happens in training time.
During inference, ViLD-image, ViLD-text and ViLD employ the same set of text embeddings as the detection classifier, and use the same architecture for open-vocabulary detection (Fig.~\ref{fig:overview}).


\subsection{Model ensembling}\label{subsec:ensembling}
In this section, we explore model ensembling for the best detection performance over base and novel categories. 
First, we combine the predictions of a ViLD-text detector with the open-vocabulary image classification model. The intuition is that ViLD-image learns to approximate the predictions of its teacher model, and therefore, we assume using the teacher model directly may improve performance.
We use a trained ViLD-text detector to obtain top  candidate regions and their confidence scores. 
Let  denote the confidence score of proposal  belonging to category .
We then feed  to the open-vocabulary classification model to obtain the teacher's confidence score . 
Since we know the two models have different performance on base and novel categories, we introduce a weighted geometric average for the ensemble:

 is set to , which weighs the prediction of ViLD-text more on base categories and vice versa. Note this approach has a similar slow inference speed as the method in Sec.~\ref{subsec:detection_with_cropped_regions}.

Next, we introduce a different ensembling approach to mitigate the above inference speed issue.
Besides, in ViLD, the cross entropy loss of ViLD-text and the  distillation loss of ViLD-image is applied to the same set of region embeddings, which may cause contentions.
Here, instead, we learn two sets of embeddings for ViLD-text (Eq.~\ref{eq:vild-text}) and ViLD-image (Eq.~\ref{eq:vild-image}) respectively, with two separate heads of identical architectures.
Text embeddings are applied to these two regions embeddings to obtain confidence scores  and , which are then ensembled in the same way as Eq.~\ref{eq:ensemble}, with  replacing . 
We name this approach \textbf{ViLD-ensemble}. 


\section{Experiments}\label{sec:experiments}
\paragraph{Implementation details:} We benchmark on the Mask R-CNN~\citep{he2017mask} with ResNet~\citep{resnet} FPN~\citep{fpn} backbone and use the same settings for all models unless explicitly specified.
The models use 10241024 as input image size, large-scale jittering augmentation of range , synchronized batch normalization~\citep{bn1,Detectron2018} of batch size 256, weight decay of 4e-5, and an initial learning rate of 0.32. We train the model from scratch for 180,000 iterations, and divide the learning rate by 10 at 0.9, 0.95, and 0.975 of total iterations. We use the publicly available pretrained CLIP model\footnote{\url{https://github.com/openai/CLIP}, ViT-B/32.} as the open-vocabulary classification model, with an input size of 224224.
The temperature  is set to 0.01, and the maximum number of detections per image is 300.
We refer the readers to Appendix~\ref{appendix_sec:details} for more details.

\subsection{Benchmark settings}
We mainly evaluate on LVIS~\citep{lvis} with our new setting. To compare with previous methods, we also use the setting in \citet{zareian2021openvocabulary}, which is adopted in many zero-shot detection works.


\paragraph{LVIS:} 
We benchmark on LVIS v1.
LVIS contains a large and diverse set of vocabulary (1,203 categories) that is more suitable for open-vocabulary detection.
We take its 866 frequent and common categories as the base categories , and hold out the 337 rare categories as the novel categories . AP, the AP of rare categories, is the main metric.


\paragraph{COCO:}
\citet{bansal2018zero} divide COCO-2017~\citep{coco} into 48 base categories and 17 novel categories, removing 15 categories without a synset in the WordNet hierarchy. 
We follow previous works and do not compute instance masks. We evaluate on the generalized setting.



\subsection{Learning generalizable object proposals}\label{sec:proposal}
We first study whether a detector can localize novel categories when only trained on base categories. 
We evaluate the region proposal networks in Mask R-CNN with a ResNet-50 backbone.
Table~\ref{table:proposal} shows the average recall (AR)~\citep{coco} on novel categories. Training with only base categories performs slightly worse by  2 AR at 100, 300, and 1000 proposals, compared to using both base and novel categories. This experiment demonstrates that, without seeing novel categories during training, region proposal networks can generalize to novel categories, only suffering a small performance drop. We believe better proposal networks focusing on unseen category generalization should further improve the performance, and leave this for future research.

\begin{table}[h]
\vspace{-1ex}
\caption{\textbf{Training with only base categories achieves comparable average recall (AR) for novel categories on LVIS.} We compare RPN trained with base only \vs base+novel categories and report the bounding box AR.}
\label{table:proposal}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lccc}
\toprule
Supervision & AR & AR & AR \\
\midrule
base & 39.3 & 48.3 & 55.6 \\
base + novel & 41.1 & 50.9 & 57.0 \\
\bottomrule
\end{tabular}
}
\vspace{-2ex}
\end{table}




\subsection{Open-vocabulary classifier on cropped regions}
\label{sec:classify_by_clip}

In Table~\ref{table:clip_results}, we evaluate the approach in Sec.~\ref{subsec:detection_with_cropped_regions}, \ie, using an open-vocabulary classifier to classify cropped region proposals.
We use CLIP in this experiment and find it tends to output confidence scores regardless of the localization quality (Appendix~\ref{appendix_sec:analysis}). Given that, we ensemble the CLIP confidence score with a proposal objectness score by geometric mean. Results show it improves both base and novel APs.
We compare with supervised baselines trained on base/base+novel categories, as well as Supervised-RFS~\citep{mahajan2018exploring,lvis} that uses category frequency for balanced sampling.
CLIP on cropped regions already outperforms supervised baselines on AP by a large margin, without accessing detection annotations in novel categories. However, the performances of AP and AP are still trailing behind.
This experiment shows that a strong open-vocabulary classification model can be a powerful teacher model for detecting novel objects, yet there is still much improvement space for inference speed and overall AP.

\begin{table}[h]
\caption{
\textbf{Using CLIP for open-vocabulary detection achieves high detection performance on novel categories.} We apply CLIP to classify cropped region proposals, with or without ensembling objectness scores, and report the mask average precision (AP). The performance on novel categories (AP) is far beyond supervised learning approaches. However, the overall performance is still behind.
} 
\label{table:clip_results}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lr>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r}

\toprule
Method  & AP & AP & AP & AP\\
\midrule
Supervised (base class only) &	0.0 &	22.6 &	32.4 & 22.5\\
CLIP on cropped regions w/o objectness & 13.0 & 10.6 & 6.0 & 9.2\\
CLIP on cropped regions & \textbf{18.9} & 18.8 & 16.0 & 17.7 \\
\hline
Supervised (base+novel)  &	4.1 &	23.5 &	33.2 & 23.9\\
Supervised-RFS (base+novel) & 12.3 &	24.3 &	32.4 & 25.4\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h]
\caption{
\textbf{Performance of ViLD and its variants. ViLD outperforms the supervised counterpart on novel categories.}
Using ALIGN as the teacher model achieves the best performance without bells and whistles. 
All results are mask AP. We average over 3 runs for R50 experiments.
: methods with R-CNN style; runtime is 630 of Mask R-CNN style.
: for reference, fully-supervised learning with additional tricks.
}
\label{table:main_results}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{llr>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r}

\toprule
Backbone & Method  & AP &  AP & AP & AP\\
\midrule
\multirow{2}{*}{ResNet-50+ViT-B/32} & CLIP on cropped regions  & 18.9 & 18.8 & 16.0 & 17.7 \\
& ViLD-text+CLIP & \textbf{22.6} & 24.8 & 29.2 & 26.1\\
\hline
\multirow{6}{*}{ResNet-50} & Supervised-RFS (base+novel)  & 12.3 &	24.3 &	32.4 & 25.4\\
& GloVe baseline & 	3.0	& 20.1 & 30.4 & 21.2\\
& ViLD-text &  10.1	& 23.9 &	32.5 & 24.9\\
& ViLD-image & 11.2 & 	11.3 & 	11.1 & 11.2\\
& ViLD (=0.5) & 16.1 &	20.0 &	28.3 & 22.5\\
& ViLD-ensemble (=0.5) & \textbf{16.6} & 24.6 & 30.3 & 25.5\\
\hline
\multirow{2}{*}{EfficientNet-b7} & ViLD-ensemble w/ ViT-L/14 (=1.0) & 21.7 &	29.1 & 33.6 & 29.6\\
 & ViLD-ensemble w/ ALIGN (=1.0) & \textbf{26.3} &	27.2 & 32.9 & 29.3\\
\hline
ResNeSt269+HTC & 2020 Challenge winner~\citep{huang2020joint}  & 30.0 & 41.9 &	46.0 & 41.5\\
\bottomrule
\end{tabular}
}
\vspace{-2ex}
\end{table}


\subsection{Vision and language knowledge distillation}\label{sec:exp_vild}
We evaluate the performance of ViLD and its variants (ViLD-text, ViLD-image, and ViLD-ensemble), which are significantly faster compared to the method in Sec.~\ref{sec:classify_by_clip}.
Finally, we use stronger teacher models to demonstrate our best performance. Table~\ref{table:main_results} summarizes the results.

\paragraph{Text embeddings as classifiers (ViLD-text):}
We evaluate ViLD-text 
using text embeddings generated by CLIP,
and compare it with GloVe text embeddings~\citep{pennington2014glove} pretrained on a large-scale text-only corpus.
Table~\ref{table:main_results} shows
ViLD-text achieves 10.1 AP, which is significantly better than 3.0 AP using GloVe. This demonstrates the importance of using text embeddings that are jointly trained with images. 
ViLD-text achieves much higher AP and AP compared to CLIP on cropped regions (Sec.~\ref{sec:classify_by_clip}), because ViLD-text uses annotations in  to align region embeddings with text embeddings. The AP is worse, showing that using only 866 base categories in LVIS does not generalize as well as CLIP to novel categories.


\paragraph{Distilling image embeddings (ViLD-image):}
We evaluate ViLD-image, which distills from the image embeddings of cropped region proposals, inferred by CLIP's image encoder, 
with a distillation weight of 1.0. Experiments show that ensembling with objectness scores doesn't help with other ViLD variants, so we only apply it to ViLD-image.
Without training with any object category labels, ViLD-image achieves 11.2 AP and 11.2 overall AP.
This demonstrates that visual distillation works for open-vocabulary detection but the performance is not as good as CLIP on cropped regions.


\paragraph{Text+visual embeddings (ViLD):}
ViLD shows the benefits of combining distillation loss (ViLD-image) with classification loss using text embeddings (ViLD-text).
We explore different hyperparameter settings in Appendix Table~\ref{table:distill_weights} and observe a consistent trade-off between AP and AP, which suggests there is a competition between ViLD-text and ViLD-image.
In Table~\ref{table:main_results}, we compare ViLD with other methods.
Its AP is 6.0 higher than ViLD-text and 4.9 higher than ViLD-image, indicating combining the two learning objectives boosts the performance on novel categories.
ViLD outperforms Supervised-RFS by 3.8 AP, showing our open-vocabulary detection approach is better than supervised models on rare categories. 

\paragraph{Model ensembling: }
 We study methods discussed in Sec.~\ref{subsec:ensembling} to reconcile the conflict of joint training with ViLD-text and ViLD-image. We use two ensembling approaches: 1) ensembling ViLD-text with CLIP (\textbf{ViLD-text+CLIP}); 2) ensembling ViLD-text and ViLD-image using separate heads (\textbf{ViLD-ensemble}).
 As shown in Table~\ref{table:main_results}, ViLD-ensemble improves performance over ViLD, mainly on AP and AP. This shows ensembling reduces the competition. ViLD-text+CLIP obtains much higher AP, outperforming ViLD by 6.5, and maintains good AP. Note that it is slow and impractical for real world applications. This experiment is designed for showing the potential of using open-vocabulary classification models for open-vocabulary detection.


\paragraph{Stronger teacher model:} We use CLIP ViT-L/14 and ALIGN~\citep{align} to explore the performance gain with a stronger teacher model 
(details in Appendix~\ref{appendix_sec:details}). As shown in Table~\ref{table:main_results}, both models achieve superior results compared with R50 ViLD w/ CLIP. The detector distilled from ALIGN is only trailing to the fully-supervised 2020 Challenge winner~\citep{huang2020joint} by 3.7 AP, which employs two-stage training, self-training, and multi-scale testing \etc. The results demonstrate ViLD scales well with the teacher model, and is a promising open-vocabulary detection approach.



\subsection{Performance comparison on COCO dataset}
Several related works in zero-shot detection and open-vocabulary detection are evaluated on COCO.
To compare with them, we train and evaluate ViLD variants following the benchmark setup in~\citet{zareian2021openvocabulary} and report box AP with an IoU threshold of 0.5. We use the ResNet-50 backbone, shorten the training schedule to 45,000 iterations, and keep other settings the same as our experiments on LVIS.
Table~\ref{table:coco} summarizes the results.
ViLD outperforms~\citet{zareian2021openvocabulary} by 4.8 Novel AP and 13.5 Base AP. Different from~\citet{zareian2021openvocabulary}, we do not have a pretraining phase tailored for detection. Instead, we use an off-the-shelf classification model. The performance of ViLD-text is low because only 48 base categories are available, which makes generalization to novel categories challenging. In contrast, ViLD-image and ViLD, which can distill image features of novel categories, outperform all existing methods (not apple-to-apple comparison though, given different methods use different settings).

\begin{table}[t]
\caption{
\textbf{Performance on COCO dataset compared with existing methods.} ViLD outperforms all the other methods in the table trained with various sources by a large margin, on both novel and base categories.
}
\label{table:coco}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lcr>{\color{gray}}r>{\color{gray}}r}
\toprule
Method  & Training source & Novel AP &  Base AP & Overall AP\\
\midrule
\citet{WSDDN}       & \multirow{2}{*}{image-level labels in } & 19.7 & 19.6 & 19.6 \\
\citet{Cap2Det}     &       & 20.3 & 20.1 & 20.1 \\
\hline
\citet{bansal2018zero}          & \multirow{3}{*}{instance-level labels in }   & 0.31 & 29.2 & 24.9 \\
\citet{zhu2020don}        &       & 3.41 & 13.8 & 13.0 \\
\citet{rahman2020improved}  &       & 4.12 & 35.9 & 27.9 \\
\hline
\citet{zareian2021openvocabulary} & \makecell{image captions in \\instance-level labels in }  & 22.8 & 46.0 & 39.9\\
\hline
CLIP on cropped regions &  \multirow{4}{*}{\makecell{image-text pairs from Internet\\ (may contain )\\instance-level labels in }}  & 26.3 & 28.3 & 27.8\\
ViLD-text & & 5.9 & 61.8 & 47.2\\
ViLD-image & & 24.1 & 34.2 & 31.6 \\
ViLD () & & \textbf{27.6} & 59.5 & 51.3 \\
\bottomrule
\end{tabular}
}
\vspace{-2.5ex}
\end{table}

\subsection{Transfer to other datasets}\label{subsec:exp_transfer}
Trained ViLD models can be transferred to other detection datasets, by simply switching the classifier to the category text embeddings of the new datasets.
For simplicity, we keep the background embedding trained on LVIS.
We evaluate the transferability of ViLD on PASCAL VOC~\citep{pascal}, COCO~\citep{coco}, and Objects365~\citep{obj365}.
Since the three datasets have much smaller vocabularies, category overlap is unavoidable and images can be shared among datasets, \eg, COCO and LVIS. 
As shown in Table~\ref{table:transfer}, ViLD achieves better transfer performance than ViLD-text. In PASCAL and COCO, the gap is large. This improvement should be credited to visual distillation, which better aligns region embeddings with the text classifier.
We also compare with supervised learning and finetuning the classification layer.
Although across datasets, ViLD has 3-6 AP gaps compared to the finetuning method and larger gaps compared to the supervised method, it is the first time we can directly transfer a trained detector to different datasets using language.



\begin{table}
\caption{\textbf{Generalization ability of ViLD.} We evaluate the LVIS-trained model with ResNet-50 backbone on PASCAL VOC 2007 test set, COCO validation set, and Objects365 v1 validation set. Simply replacing the text embeddings, our approaches are able to transfer to various detection datasets. The supervised baselines of COCO and Objects365 are trained from scratch. : the supervised baseline of PASCAL VOC is initialized with an ImageNet-pretrained checkpoint. All results are box APs.}
\label{table:transfer}
\centering
\vspace{-1ex}
{\footnotesize
    \begin{tabular}{l|cc|ccc|ccc}
    \toprule
    \multirowcell{2}[0ex][l]{Method}&\multicolumn{2}{c|}{PASCAL VOC} & \multicolumn{3}{c|}{COCO} & \multicolumn{3}{c}{Objects365}\\
    & AP & AP & AP & AP & AP & AP & AP & AP \\
    \midrule
    ViLD-text & 40.5 & 31.6   & 28.8 & 43.4 & 31.4       & 10.4 & 15.8 & 11.1 \\
    ViLD      & 72.2 & 56.7   & 36.6 & 55.6 & 39.8       & 11.8 & 18.2 & 12.6\\
    Finetuning & 78.9 & 60.3  & 39.1 & 59.8 & 42.4       & 15.2 & 23.9 & 16.2\\
    Supervised & 78.5 & 49.0  & 46.5 & 67.6 & 50.9       & 25.6 & 38.6 & 28.0\\
    \bottomrule
    \end{tabular}
}
\end{table}

\begin{figure}[t]
\centering
   \includegraphics[width=0.9\linewidth]{reduced_figures/new_main_qual.pdf}
   \vspace{-1ex}
   \caption{\textbf{Qualitative results on LVIS, COCO, and Objects365.} First row: ViLD is able to correctly localize and recognize objects in novel categories. For clarity, we only show the detected novel objects. Second row: The detected objects on base+novel categories. The performance on base categories is not degraded with ViLD. Last two rows: ViLD can directly transfer to COCO and Objects365 without further finetuning.}\label{fig:main_qual}
\vspace{-2ex}
\end{figure}


\subsection{Qualitative results}
\paragraph{Qualitative examples:} In Fig.~\ref{fig:main_qual}, we visualize ViLD's detection results. It illustrates ViLD is able to detect objects of both novel and base categories, with high-quality mask predictions on novel objects, \eg, it well separates banana slices from the crepes (novel category). We also show qualitative results on COCO and Objects365, and find ViLD generalizes well.
We show more qualitative results, \eg, interactive detection and systematic expansion, in Appendix~\ref{appendix_sec:qualitative}.

\paragraph{On-the-fly interactive object detection:} We tap the potential of ViLD by using arbitrary text to interactively recognize fine-grained categories and attributes.
We extract the region embedding and compute its cosine similarity with a small set of on-the-fly arbitrary texts describing attributes and/or fine-grained categories; we apply softmax with temperature  on top of the similarities. To our surprise, though never trained on fine-grained dog breeds (Fig.~\ref{fig:two_examples}), it correctly distinguishes husky from shiba inu. It also works well on identifying object colors (Fig.~\ref{fig:teaser}). The results demonstrate knowledge distillation from an open-vocabulary image classification model helps ViLD to gain understanding of concepts not present in the detection training. Of course, ViLD does not work all the time, \eg, it fails to recognize poses of animals.

\begin{figure}[h]
\centering
     \subfigure[Fine-grained breeds and colors.]{
        \includegraphics[width=0.35\linewidth]{figures/interactive_qualitative2_scale_vector.pdf}
        \label{fig:interactive_qual1}
     }
    \hspace{8mm}
    \subfigure[Colors of body parts.]{
        \includegraphics[width=0.35\linewidth]{figures/interactive_bird.pdf}
        \label{fig:interactive_qual2}
    }
    \vspace{-1ex}
    \caption{\textbf{On-the-fly interactive object detection.} 
    One application of ViLD is using on-the-fly arbitrary texts to further recognize more details of the detected objects, \eg, fine-grained categories and color attributes.
    }\label{fig:two_examples}
    \vspace{-2ex}
\end{figure}


\paragraph{Systematic expansion of dataset vocabulary:} In addition, we propose to systematically expand the dataset vocabulary () with a set of attributes () as follows: 

where  denotes the region embedding. We assume , \ie, given  the event the object belongs to category  is conditionally independent to the event it has attribute .

Let  denote the temperature used for softmax and  denote the text encoder as in Eq.~\ref{eq:vild-text}. Then


In this way, we are able to expand  vocabularies into a new set of  vocabularies with attributes. The conditional probability approach is similar to YOLO9000~\citep{yolo9000}.
We show a qualitative example of this approach in Fig.~\ref{fig:cond_prob}, where we use a color attribute set as .
Our open-vocabulary detector successfully detects fruits with color attributes.

We further expand the detection vocabulary to fine-grained bird categories by using all 200 species from CUB-200-2011~\citep{wah2011caltech}.
Fig.~\ref{fig:fg_bird_good} shows successful and failure examples of our open-vocabulary fine-grained detection on CUB-200-2011 images. In general, our model is able to detect visually distinctive species, but fails at other ones.

\begin{figure}[t!]
\centering
     \subfigure[Using LVIS vocabulary.]{
        \includegraphics[width=0.45\linewidth]{reduced_figures/cond_prob_fruit_original.pdf}
        \label{fig:cond_prob_original}
     }
    \subfigure[After expanding vocabulary with color attributes.]{
        \includegraphics[width=0.48\linewidth]{reduced_figures/cond_prob_fruit.pdf}
        \label{fig:cond_prob_after}
    }
    \vspace{-1.5ex}
    \caption{\textbf{Systematic expansion of dataset vocabulary with colors.} We add 11 color attributes (\textit{red orange, dark orange, light orange, yellow, green, cyan, blue, purple, black, brown, white}) to LVIS categories, which expand the vocabulary size by 11. Above we show an example of detection results. Our open-vocabulary detector is able to assign the correct color to each fruit. A class-agnostic NMS with threshold 0.9 is applied. Each figure shows top 15 predictions. }\label{fig:cond_prob}
    \vspace{1ex}
\end{figure}

\begin{figure}
\centering
    \includegraphics[width=0.9\linewidth]{reduced_figures/birds_combined.pdf}
    \vspace{-1.5ex}
    \caption{\textbf{Systematic expansion of dataset vocabulary with fine-grained categories.} 
    We use the systematic expansion method to detect 200 fine-grained bird species in CUB-200-2011. \textbf{(a)}: Our open-vocabulary detector is able to perform fine-grained detection \textbf{(bottom)} using the detector trained on LVIS \textbf{(top)}. \textbf{(b)}: It fails at recognizing visually non-distinctive species. It incorrectly assigns ``Western Gull'' to ``Horned Puffin'' due to visual similarity.}
    \label{fig:fg_bird_good}
    \vspace{-1ex}
\end{figure}

\section{Conclusion}
We present ViLD, an open-vocabulary object detection method by distilling knowledge from open-vocabulary image classification models. ViLD is the first open-vocabulary detection method evaluated on the challenging LVIS dataset. It attains 16.1 AP for novel cateogires on LVIS with a ResNet50 backbone, which surpasses its supervised counterpart at the same inference speed. With a stronger teacher model (ALIGN), the performance can be further improved to 26.3 novel AP. We demonstrate that the detector learned from LVIS can be directly transferred to 3 other detection datasets.
We hope that the simple design and strong performance make ViLD a scalable alternative approach for detecting long-tailed categories, instead of collecting expensive detection annotations.



\clearpage

\section*{Ethics Statement}
Our paper studies open-vocabulary object detection, a sub-field in computer vision.
Our method is based on knowledge distillation, a machine learning technique that has been extensively used in computer vision, natural language processing, \etc.
All of our experiments were conducted on public datasets with pretrained models that are either publicly available or introduced in published papers.
The method proposed in our paper is a principled method for open-vocabulary object detection that can be used in a wide range of applications.
Therefore, the ethical impact of our work would primarily depends on the specific applications.
We foresee positive impacts if our method is applied to object detection problems where the data collection is difficult to scale, such as detecting rare objects for self-driving cars.
But the method can also be applied to other sensitive applications that could raise ethical concerns, such as video surveillance systems.

\section*{Reproducibility Statement}
We provide detailed descriptions of the proposed method in Sec.~\ref{sec:methold}.
Details about experiment settings, hyper-parameters and implementations are presented in Sec.~\ref{sec:experiments}, Appendix~\ref{appendix_sec:quantitative} and Appendix~\ref{appendix_sec:details}.
We release our code and pretrained models at \url{https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild} to facilitate the reproducibility of our work.


\bibliography{ref}
\bibliographystyle{iclr2022_conference}


\clearpage

\appendix
\section*{Appendix}

\section{Additional qualitative results}\label{appendix_sec:qualitative}

\paragraph{Transfer to PASCAL VOC:}
In Fig.~\ref{fig:pascal_qual}, we show qualitative results of transferring an open-vocabulary detector trained on LVIS~\citep{lvis} to PASCAL VOC Detection (2007 test set)~\citep{pascal}, without finetuning (Sec.~\ref{subsec:exp_transfer} in the main paper). Results demonstrate that the transferring works well. 
\begin{figure}[h]
\centering
    \includegraphics[width=1.0\linewidth]{reduced_figures/new_pascal_qual.pdf}
    \vspace{-3.5ex}
    \caption{\textbf{Transfer to PASCAL VOC.} ViLD correctly detects objects when transferred to PASCAL VOC, where images usually have lower resolution than LVIS (our training set). In the third picture, our detector is able to find tiny bottles, though it fails to detect the person.}
    \label{fig:pascal_qual}
\end{figure}


\paragraph{Failure cases:}
In Fig.~\ref{fig:lvis_failure}, we show two failure cases of ViLD. The most common failure cases are the missed detection. A less common mistake is misclassifying the object category.
\begin{figure}[h]
\centering
   \includegraphics[width=0.7\linewidth]{reduced_figures/LVIS_failure.pdf}
   \vspace{-1ex}
   \caption{\textbf{Failure cases on LVIS novel categories}. The red bounding boxes indicate the groundtruths of the failed detections. \textbf{(a)} A common failure type where the novel objects are missing, \eg, the elevator car is not detected. \textbf{(b)} A less common failure where (part of) the novel objects are misclassified, \eg, half of the waffle iron is detected as a calculator due to visual similarity. }
\label{fig:lvis_failure}
\vspace{1ex}
\end{figure}


We show a failure case of mask prediction on PASCAL VOC in Fig.~\ref{fig:localization_error}.
It seems that the mask prediction is sometimes based on low-level appearance rather than semantics.

\begin{figure}[h]
\centering
    \includegraphics[width=0.45\linewidth]{figures/000011_predicted.pdf}
    \vspace{-1ex}
    \caption{\textbf{An example of ViLD on PASCAL VOC showing a mask of poor quality.} The class-agnostic mask prediction head occasionally predicts masks based on low-level appearance rather than semantics, and thus fails to obtain a complete instance mask.}
    \label{fig:localization_error}
    \vspace{-1ex}
\end{figure}





\section{Analysis of CLIP on cropped regions}\label{appendix_sec:analysis}
In this section, we analyze some common failure cases of CLIP on cropped regions and discuss possible ways to mitigate these problems.





\paragraph{Visual similarity:}
This confusion is common for any classifiers and detectors, especially on large vocabularies. In Fig.~\ref{fig:analysis2}(a), we show two failure examples due to visual similarity. Since we only use a relatively small ViT-B/32 CLIP model, potentially we can improve the performance with a higher-capacity pretrained model. In Table~\ref{table:align_on_cropped_regions}, when replacing this CLIP model with an EfficientNet-l2 ALIGN model, we see an increase on AP.

\begin{table}[h]
\caption{
\textbf{ALIGN on cropped regions achieves superior AP, and overall very good performance.} It shows a stronger open-vocabulary classification model can improve detection performance by a large margin. We report box APs here.
}
\label{table:align_on_cropped_regions}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lr>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r}

\toprule
Method  & AP & AP & AP & AP\\
\midrule
CLIP on cropped regions & 19.5 & 19.7 & 17.0 & 18.6 \\
ALIGN on cropped regions & \textbf{39.6} & 32.6 & 26.3 & 31.4 \\
\bottomrule
\end{tabular}
}
\vspace{-2ex}
\end{table}


\begin{figure}[h]
\centering
    \includegraphics[width=0.5\linewidth]{reduced_figures/analysis2.pdf}
    \vspace{-1.5ex}
    \caption{\textbf{Typical errors of CLIP on cropped regions.} \textbf{(a):} The prediction and the groundtruth have high visual similarity. \textbf{(b):} Directly resizing the cropped regions changes the aspect ratios, which may cause troubles. \textbf{(c):} CLIP's predictions are sometimes affected by other objects appearing in the region, rather than predicting what the entire bounding box is.}
    \label{fig:analysis2}
    \vspace{-2ex}
\end{figure}



\paragraph{Aspect ratio:}
This issue is introduced by the pre-processing of inputs in CLIP. We use the ViT-B/32 CLIP with a fixed input resolution of . It resizes the shorter edge of the image to 224, and then uses a center crop. However, since region proposals can have more extreme aspect ratios than the training images for CLIP, and some proposals are tiny, we directly resize the proposals to that resolution, which might cause some issues. For example, the thin structure in Fig.~\ref{fig:analysis2}(b) right will be highly distorted with the pre-processing. And the oven and fridge can be confusing with the distorted aspect ratio. There might be some simple remedies for this, \eg, pasting the cropped region with original aspect ratio on a black background. We tried this simple approach with both CLIP and ALIGN. Preliminary results show that it works well on the fully convolutional ALIGN, while doesn't work well on the transformer-based CLIP, probably because CLIP is never trained with black image patches.

\paragraph{Multiple objects in a bounding box:}
Multiple objects in a region interfere CLIP's classification results, see Fig.~\ref{fig:analysis2}(c), where a corner of an aquarium dominates the prediction.  This is due to CLIP pretraining, which pairs an entire image with its caption. The caption is usually about salient objects in the image.
It's hard to mitigate this issue at the open-vocabulary classification model's end.
On the other hand, a supervised detector are trained to recognize the object tightly surrounded by the bounding box. So when distilling knowledge from an open-vocabulary image classification model, keeping training a supervised detector on base categories could help, as can be seen from the improvement of ViLD over ViLD-image (Sec.~\ref{sec:exp_vild}). 


\paragraph{Confidence scores predicted by CLIP do not reflect the localization quality:} For example, in Fig.~\ref{fig:analysis1}(a), CLIP correctly classifies the object, but gives highest scores to partial detection boxes. CLIP is not trained
to measure the quality of bounding boxes. Nonetheless, in object detection, it is important for the higher-quality boxes to have higher scores. In Fig.~\ref{fig:analysis1}(c), we simply re-score by taking the geometric mean of the CLIP confidence score and the objectness score from the proposal model, which yields much better top predictions. In Fig.~\ref{fig:analysis1}(b), we show top predictions of the Mask R-CNN model. Its top predictions have good bounding boxes, while the predicted categories are wrong. 
This experiment shows that it's important to have both an open-vocabulary classification model for better recognition, as well as supervision from detection dataset for better localization.
\begin{figure}[h]
\centering
    \includegraphics[width=1.0\linewidth]{reduced_figures/analysis1_iclr_reduced.pdf}
    \vspace{-3.5ex}
    \caption{\textbf{The prediction scores of CLIP do not reflect the quality of bounding box localization.}
    \textbf{(a):} Top predictions of CLIP on cropped region. Boxes of poor qualities receive high scores, though the classification is correct. \textbf{(b):} Top predictions of a vanilla Mask R-CNN model. Box qualities are good while the classification is wrong. \textbf{(c):} We take the geometric mean of CLIP classification score and objectiveness score, and use it to rescore (a). In this way, a high-quality box as well as the correct category rank first.}
    \label{fig:analysis1}
    \vspace{1ex}
\end{figure}





\section{Additional quantitative results}\label{appendix_sec:quantitative}

\paragraph{Hyperparameter sweep for visual distillation:}
Table~\ref{table:distill_weights} shows the parameter sweep of different distillation weights using  and  losses.
Compared with no distillation, additionally learning from image embeddings generally yields better performance on novel categories.
We find  loss can better improve the AP performance with the trade-off against AP and AP. This suggests there is a competition between ViLD-text and ViLD-image.

\begin{table}[h]
\caption{\textbf{Hyperparameter sweep for visual distillation in ViLD.}  loss is better than  loss. For  loss, there is a trend that AP increases as the weight increases, while AP decrease. For all parameter combinations, ViLD outperforms ViLD-text on AP.
We use ResNet-50 backbone and shorter training iterations (84,375 iters), and report mask AP in this table.
}
\label{table:distill_weights}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lcr>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r}
\toprule
Distill loss & Distill weight  & AP & AP & AP & AP\\
\midrule
No distill & 0.0  & 10.4 &	22.9 &	31.3 & 24.0\\
\hline
\multirow{3}{*}{ loss} & 0.5 & \textbf{13.7} &	21.7 &	31.2 & 24.0\\
& 1.0 & 	12.4 &	22.7 &	31.4 & 24.3\\
& 2.0 & 	13.4 &	22.0 &	30.9 & 24.0\\
\hline
\multirow{4}{*}{ loss} & 0.05 & 	12.9 &	22.4 &	31.7 & 24.4\\
& 0.1 & 	14.0	& 20.9 &	31.2 & 23.8\\
& 0.5 & 	16.3 &	19.2 &	27.3 & 21.9\\
& 1.0 & 	\textbf{17.3} &	18.2 &	25.1 & 20.7\\
\bottomrule
\end{tabular}
}
\vspace{-2ex}
\end{table}



\begin{table}
\caption{\textbf{Performance of ViLD variants.} This table shows additional box APs for models in Table~\ref{table:main_results} and ResNet-152 results.}
\label{table:main_results_w_box_aps}
\vspace{-1ex}
\centering
\scalebox{0.84}{
\begin{tabular}{ll|r>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r|r>{\color{gray}}r>{\color{gray}}r>{\color{gray}}rr}
\toprule
\multirowcell{2}[0ex][l]{Backbone}& \multirowcell{2}[0ex][l]{Method} & \multicolumn{4}{c|}{Box} & \multicolumn{4}{c}{Mask}\\
&   & AP & AP & AP & AP & AP & AP & AP & AP\\
\midrule
\multirowcell{2}[0ex][l]{ResNet-50\\+ViT-B/32}& CLIP on cropped regions & 19.5 & 19.7 & 17.0 & 18.6 & 18.9 & 18.8 & 16.0 & 17.7\\
& ViLD-text+CLIP & \textbf{23.8} & 26.7 & 32.8 & 28.6 & \textbf{22.6} & 24.8 & 29.2 & 26.1\\
\midrule
\multirowcell{6}[0ex][l]{ResNet-50}& Supervised-RFS (base+novel)  & 13.0 &	26.7 &	37.4 & 28.5 & 12.3 &	24.3 &	32.4 & 25.4\\
& GloVe baseline & 	3.2 &	22.0 &	34.9 & 23.8 & 3.0 & 20.1 & 30.4 & 21.2\\
& ViLD-text &  10.6 & 26.1 & 37.4 & 27.9 & 10.1 & 23.9 &	32.5 & 24.9\\
& ViLD-image & 10.3 &	11.5 &	11.1 & 11.2 & 11.2 & 	11.3 & 	11.1 & 11.2\\
& ViLD (=0.5) & 16.3 &	21.2 &	31.6 & 24.4 & 16.1 &	20.0 &	28.3 & 22.5\\
& ViLD-ensemble (=0.5) & \textbf{16.7} & 26.5 & 34.2 & 27.8 & \textbf{16.6} & 24.6 & 30.3 &  25.5\\
\midrule
\multirowcell{5}[0ex][l]{ResNet-152}& Supervised-RFS (base+novel)  & 16.2 & 29.6 & 39.7 & 31.2 & 14.4 &	26.8 &	34.2 & 27.6\\
& ViLD-text &  12.3 & 28.3 & 39.7 & 30.0 & 11.7 & 25.8 & 34.4 & 26.7\\
& ViLD-image & 12.5 &	13.9 &	13.4  & 13.4 & 13.1 &	13.4 &	13.0 & 13.2\\
&  ViLD (=1.0) & 19.1 &	22.4 &	31.5 & 25.4 & \textbf{18.7} &	21.1	& 28.4 & 23.6\\
& ViLD-ensemble (=2.0) & \textbf{19.8} & 27.1 & 34.5 & 28.7 & \textbf{18.7} & 24.9 & 30.6 & 26.0\\
\midrule
\multirowcell{2}[0ex][l]{EfficientNet-b7} & ViLD-ensemble w/ ViT-L/14 (=1.0) & 22.0 & 31.5 & 38.0 & 32.4 & 21.7 &	29.1 & 33.6 & 29.6\\
& ViLD-ensemble w/ ALIGN (=1.0) & \textbf{27.0} & 29.4 & 36.5 & 31.8 & \textbf{26.3} &	27.2 & 32.9 & 29.3\\
\bottomrule
\end{tabular}
}
\vspace{1ex}
\end{table}



\paragraph{Box APs and ResNet-152 backbone:}
Table~\ref{table:main_results_w_box_aps} shows the corresponding box AP of Table~\ref{table:main_results} in the main paper. In general, box AP is slightly higher than mask AP.  In addition, we include the results of ViLD variants with the ResNet-152 backbone. The deeper backbone improves all metrics. 
The trend/relative performance is consistent for box and mask APs, as well as for different backbones. 
ViLD-ensemble achieves the best box and mask AP.

\paragraph{Ablation study on prompt engineering:}
We conduct an ablation study on prompt engineering. We compare the text embeddings ensembled over synonyms and 63 prompt templates (listed in Appendix~\ref{appendix_sec:details}) with a non-ensembled version: Using the single prompt template ``a photo of \{article\} \{category\}''. Table~\ref{table:prompt_ablation} illustrates that ensembling multiple prompts slightly improves the performance by 0.4 AP.

\begin{table}[h]
\caption{
\textbf{Ablation study on prompt engineering.} Results indicate ensembling multiple prompt templates slightly improves AP. ViLD w/ multiple prompts is the same ViLD model in Table~\ref{table:main_results}, and ViLD w/ single prompt only changes the text embeddings used as the classifier.
}
\label{table:prompt_ablation}
\vspace{-1ex}
\centering
{\footnotesize
\begin{tabular}{lr>{\color{gray}}r>{\color{gray}}r>{\color{gray}}r}
\toprule
Method  & AP & AP & AP & AP\\
\midrule
ViLD w/ single prompt    & 15.7 &	19.7 &	28.9 & 22.6\\
ViLD w/ multiple prompts & \textbf{16.1} &	20.0 &	28.3 & 22.5\\
\bottomrule
\end{tabular}
}
\end{table}





\section{More implementation details}\label{appendix_sec:details}
\paragraph{ViLD-ensemble architecture:} In Fig.~\ref{fig:vild_ensemble}, we show the detailed architecture and learning objectives for ViLD-ensemble, the ensembling technique introduced in Sec.~\ref{subsec:ensembling}.

\begin{figure}[t]
\centering
    \includegraphics[width=0.6\linewidth]{figures/vild_ensemble_arch.pdf}
    \caption{\textbf{Model architecture and training objectives for ViLD-ensemble.} The learning objectives are similar to ViLD. Different from ViLD, we use two separate heads of identical architecture in order to reduce the competition between ViLD-text and ViLD-image objetvies. During inference, the results from the two heads are ensembled as described in Sec.~\ref{subsec:ensembling}. Please refer to Fig.~\ref{fig:model_architecture} for comparison with other ViLD variants.}
    \label{fig:vild_ensemble}
\end{figure}


\paragraph{Model used for qualitative results:}
For all qualitative results, we use a ViLD model with ResNet-152 backbone, whose performance is shown in Table~\ref{table:main_results_w_box_aps}.

\paragraph{Details for supervised baselines:}
For a fair comparison, we train the second stage box/mask prediction heads of Supervised and Supervised-RFS baselines in the class-agnostic manner introduced in Sec.~\ref{subsec:method_localization}.

\paragraph{Details for R-CNN style experiments:}
We provide more details here for the R-CNN style experiments: CLIP on cropped regions in Sec.~\ref{sec:proposal} and ViLD-text+CLIP in Sec.~\ref{sec:classify_by_clip}.
1) Generalized object proposal:
We use the standard Mask R-CNN R50-FPN model.
To report mask AP and compare with other methods, we treat the second-stage refined boxes as proposals and use the corresponding masks.

We apply a class-agnostic NMS with 0.9 threshold, and output a maximum of 1000 proposals. 
The objectness score is one minus the background score.
2) Open-vocabulary classification on cropped regions:
After obtaining CLIP confidence scores for the 1000 proposals, we apply a class-specific NMS with a threshold of 0.6, and output the top 300 detections as the final results.

\paragraph{Additional details for ViLD variants:} Different from the R-CNN style experiments, for all ViLD variants (Sec.~\ref{subsec:vild}, Sec.~\ref{subsec:ensembling}), we use the standard two-stage Mask R-CNN with the class-agnostic localization modules introduced in Sec.~\ref{subsec:method_localization}. Both the  offline proposals and  online proposals are obtained from the first-stage RPN~\citep{ren2015faster}. In general, the R-CNN style methods and ViLD variants share the same concept of class-agnostic object proposals. We use the second-stage outputs in R-CNN style experiments only because we want to obtain the Mask AP, the main metric, to compare with other methods. For ViLD variants, we remove the unnecessary complexities and show that using a simple one-stage RPN works well.
 
\paragraph{Architecture for open-vocabulary image classification models:} Popular open-vocabulary image classification models~\citep{radford2021clip,align} perform contrastive pre-training on a large number of image-text pairs. Given a batch of paired images and texts, the model learns to maximize the cosine similarity between the embeddings of the corresponding image and text pairs, while minimizing the cosine similarity between other pairs. Specifically, for CLIP~\citep{radford2021clip}, we use the version where the image encoder adopts the Vision Transformer~\citep{dosovitskiy2020image} architecture and the text encoder is a Transformer~\citep{vaswani2017attention}. For ALIGN~\citep{align}, its image encoder is an EfficientNet~\citep{tan2019efficientnet} and its text encoder is a BERT~\citep{devlin2018bert}.

\paragraph{Details for ViLD with stronger teacher models:}
In both experiments with CLIP ViT-L/14 and ALIGN, we use EfficientNet-b7 as the backbone and ViLD-ensemble for better performance. 
We also crop the RoI features from only FPN level  in the feature pyramid.
The large-scale jittering range is reduced to [0.5, 2.0].
For CLIP ViT-L/14, since its image/text embeddings have 768 dimensions, we increase the FC dimension of the Faster R-CNN heads to 1,024, and the FPN dimension to 512.
For ViLD w/ ALIGN, we use the ALIGN model with an EfficientNet-l2 image encoder and a BERT-large text encoder as the teacher model.
We modify several places in the Mask R-CNN architecture to better distill the knowledge from the teacher.
We equip the ViLD-image head in ViLD-ensemble with the MBConvBlocks in EfficientNet. Since the MBConvBlocks are fully-convolutional, we apply a global average pooling to obtain the image embeddings, following the teacher.
The ViLD-text head keeps the same Faster R-CNN head architecture as in Mask R-CNN. Since ALIGN image/text embeddings have 1,376 dimensions (2.7 CLIP embedding dimension), we increase the number of units in the fully connected layers of the ViLD-text head to 2,048, and the FPN dimension to 1,024.

\paragraph{Text prompts:}
Since the open-vocabulary classification model is trained on full sentences, we feed the category names into a prompt template first, and use an ensemble of various prompts.
Following \citet{radford2021clip}, we curate a list of 63 prompt templates.
We specially include several prompts containing the phrase ``in the scene'' to better suit object detection, \eg, ``There is \{article\} \{category\} in the scene''.

Our list of prompt templates is shown below:
\begin{verbatim}
    'There is {article} {category} in the scene.'
    'There is the {category} in the scene.'
    'a photo of {article} {category} in the scene.'
    'a photo of the {category} in the scene.'
    'a photo of one {category} in the scene.'
    'itap of {article} {category}.'
    'itap of my {category}.'
    'itap of the {category}.'
    'a photo of {article} {category}.'
    'a photo of my {category}.'
    'a photo of the {category}.'
    'a photo of one {category}.'
    'a photo of many {category}.'
    'a good photo of {article} {category}.'
    'a good photo of the {category}.'
    'a bad photo of {article} {category}.'
    'a bad photo of the {category}.'
    'a photo of a nice {category}.'
    'a photo of the nice {category}.'
    'a photo of a cool {category}.'
    'a photo of the cool {category}.'
    'a photo of a weird {category}.'
    'a photo of the weird {category}.'
    'a photo of a small {category}.'
    'a photo of the small {category}.'
    'a photo of a large {category}.'
    'a photo of the large {category}.'
    'a photo of a clean {category}.'
    'a photo of the clean {category}.'
    'a photo of a dirty {category}.'
    'a photo of the dirty {category}.'
    'a bright photo of {article} {category}.'
    'a bright photo of the {category}.'
    'a dark photo of {article} {category}.'
    'a dark photo of the {category}.'
    'a photo of a hard to see {category}.'
    'a photo of the hard to see {category}.'
    'a low resolution photo of {article} {category}.'
    'a low resolution photo of the {category}.'
    'a cropped photo of {article} {category}.'
    'a cropped photo of the {category}.'
    'a close-up photo of {article} {category}.'
    'a close-up photo of the {category}.'
    'a jpeg corrupted photo of {article} {category}.'
    'a jpeg corrupted photo of the {category}.'
    'a blurry photo of {article} {category}.'
    'a blurry photo of the {category}.'
    'a pixelated photo of {article} {category}.'
    'a pixelated photo of the {category}.'
    'a black and white photo of the {category}.'
    'a black and white photo of {article} {category}.'
    'a plastic {category}.'
    'the plastic {category}.'
    'a toy {category}.'
    'the toy {category}.'
    'a plushie {category}.'
    'the plushie {category}.'
    'a cartoon {category}.'
    'the cartoon {category}.'
    'an embroidered {category}.'
    'the embroidered {category}.'
    'a painting of the {category}.'
    'a painting of a {category}.'
\end{verbatim}


\end{document}
