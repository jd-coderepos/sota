\section{Experiments}
In this section, we evaluate the performance of the proposed approach in detail and also compare it with the state-of-the-art methods for human pose and shape estimation. 

\subsection{Datasets}
\noindent\textbf{Human3.6M}~\cite{h36m_pami}: We follow the standard protocol~\cite{hmrKanazawa18} and use five subjects (S1, S5, S6, S7, S8) for training and test on two subjects (S9 and S11) on the frontal camera view. 

\noindent\textbf{3D Poses in-the-Wild} (3DPW)~\cite{vonMarcard2018}: consists of 60 videos recorded in diverse environments. The ground-truth poses are obtained with the help of IMU sensors attached to the body. We follow the standard protocol and use its test-set for evaluation and do not train on this dataset. 

\noindent\textbf{RenderPeople}: This is a synthetic dataset with ground-truth annotations for all  keypoints used in our method.  We used 10 characters from RenderPeople~\cite{renderpeople} dataset and generated 80k images under a variety of poses using CMU MoCap dataset~\cite{cmu_mocap} while using 100 outdoor HDRI image from HDRI Haven~\cite{hdrihaven} for lighting and backgrounds. We manually annotated the vertices corresponding to eyes, ears, nose, toes and heels for each character as they are not part of the rigged skeletons.

\noindent\textbf{MS-COCO} (COCO)~\cite{lin2014microsoft}: We use this dataset as the weakly-labeled set for the training of the 3D keypoint regressor. The dataset provides 2D annotations for 18 keypoints. A subset of the dataset was augmented by~\cite{cao2019openpose} with annotations for 3 additional keypoints on each foot. 

\subsection{Evaluation and Training Setting} We report Mean Per-Vertex Error (MPVE) and 3D reconstruction error in millimeters (mm) for all experiments. Following the standard practice \cite{hmrKanazawa18}, we extract 14 keypoints for evaluation from the recovered mesh using a pre-trained linear regressor.  

For 3DPW dataset, different methods use different datasets for training which include Human3.6M~\cite{h36m_pami}, MSCOCO~\cite{lin2014microsoft}, MPI-INF-3DHP~\cite{mono20173dhp}, MuCo-3DHP~\cite{mehta2018multi}
MPII~\cite{andriluka14cvpr}, LSP~\cite{johnson2010lsp}, UP~\cite{lassner2017unite}, SURREAL~\cite{varol17b} and etc. In this work, we only use Human3.6M, MSCOCO and the 80k synthetic images from RenderPeople dataset to train the model used for evaluation on 3DPW dataset. 

For evaluation on Human3.6M, we train only using Human3.6M and MSCOCO datasets. The ground-truth mesh annotations for Human3.6M are only available to a sparse set of researchers as the distribution has been  discontinued. Hence, we only report reconstruction error for Human3.6M. We also found that there are discrepancies between the 14 keypoints extracted using the keypoint regressor provided by~\cite{bogo2016keep} and the ground-truth marker locations from Human3.6M. This is not a problem when ground-truth mesh annotations are available as the consistent ground-truth keypoints can be extracted from the meshes. Since the mesh annotations are not available to us, we remove these discrepancies by training another linear regressor ( weight matrix) using training data, and apply it to the extracted 14 keypoints before evaluation (see supp. for details). 


\begin{table}[t]
\scriptsize
\centering
\begin{tabularx}{1\columnwidth}{X|cc}
\toprule
\multirow{1}{*}{\bf Methods} & \bf MPVE & \bf Recon. Error \\
\midrule
\multicolumn{3}{c}{articulation using 3D keypoints \eqref{eq:rotation_rules}} \\
\midrule
KAMA w/o twist removal                & 124.8 & 64.0  \\
KAMA with twist removal                   & 107.7 & 54.5 \\
\midrule
\multicolumn{3}{c}{pose \& shape refinement using~\eqref{eq:optimization}} \\
\midrule
Initialization using~\eqref{eq:rotation_rules}   \\ 
\quad                                                    & 152.5 & 87.8  \\
\quad  +                          & 115.8 & 63.5 \\
\quad  +   +  & 100.4 & 53.0 \\
\quad  +   +   +    & 97.0 & 51.1 \\   
\midrule
No initialization                                   & 106.6      &  56.4  \\
SMPLify3D - initialization using mean pose          & 100.7 & 55.8   \\ 
SMPLify2D - initialzation using mean pose           &   115.0    &  69.3       \\
\midrule
\multicolumn{3}{c}{impact of 3D keypoint quality - using GT 3D keypoints} \\
\midrule
KAMA                &   47.4   &  18.0 \\
KAMA with refinement using~\eqref{eq:optimization}                &   44.1 & 17.0 \\
\bottomrule
\end{tabularx}
\caption{Impact of different components in the proposed approach.\vspace{-5mm}}
\label{tab:ablation}
\end{table} 



\subsection{Ablation Study}
In Tab.~\ref{tab:ablation}, we evaluate all components of the proposed approach. We chose 3DPW datasets for all ablative studies as it represents more general in-the-wild scenarios, and also provides ground-truth mesh annotations. First, we evaluate the performance of our approach for mesh articulation (KAMA) using regressed 3D keypoints. If we do not remove ambiguous twists from the calculated rotations, as explained in Sec~\ref{sec:twist_removal}, the estimated meshes yield a MPVE and 3D reconstruction error of 124.8mm and 64.0mm, respectively. Removing the ambiguous twists results in a significant decrease in the error (124.8mm vs. 107.7mm and 64.0mm vs. 54.5mm) and shows the importance of this step. Note that ambiguous twists have higher impact on the MPVE since the body surface is impacted more with incorrect twist rotations as compared to body keypoint positions. We would like to emphasize that the errors of 107.7mm and 54.5mm are the state-of-the-art on 3DPW dataset, even though we only use mean shape values (\ie, ) in KAMA. Thanks to eq.~\eqref{eq:scale_and_translation}, we can find an optimal global scale for the mesh without having to optimize beta. 


Next, we evaluate the contributions of different error terms used in~\eqref{eq:total_error}. In all cases we initialize the body joint rotations using~\eqref{eq:rotation_rules} and body translation and scale using~\eqref{eq:scale_and_translation}. If we only optimize for the re-projection loss , the errors increase significantly (from 107.7mm~to~152.5mm and 54.5mm~to~87.8mm) due to the well known 2D-3D ambiguities. Adding  reduces the errors significantly (from 152.2~to~115.8mm and 87.8mm~to~63.5mm)  but remain higher than what we can achieve by using KAMA only. This is because optimizing 2D and 3D losses only is still susceptible to ambiguous twists. In KAMA, on the other hand, we explicitly handle the twist by either discarding it or estimating it with the help of adjacent keypoints. Enforcing body pose priors using  significantly reduces the errors. As compared to KAMA only, the MPVE and joint reconstruction errors are reduced from 107.7mm~to~100.4mm and 54.5mm~to~53.0mm, respectively.  As mentioned earlier,  encourages plausible poses. In our case, it helps to recover the twists of keypoints with single child and to reduce the ambiguities due to missing 3D keypoints and differences in the skeleton structures. Finally, adding  results in further decrease in the errors demonstrating the importance of optimal body shape parameters. 

To emphasize the usefulness of KAMA for optimization based method, we also evaluate the case when no initialization for  is used.  Since we have the estimated 3D keypoints, we can obtain reasonable initial values for global scale , global translation  and the global orientation  by calculating a rigid transformation between the regressed keypoints and skeleton of the canonical mesh using Procrustes analysis. We initialize   and  with zeros. Optimizing~\eqref{eq:optimization} without a good initialization results in a 3D error of 56.4mm which is significantly higher than the case when KAMA is used as the initialization (51.1mm),  demonstrating the importance of KAMA and accurate initialization. 

\begin{table}[t]
\footnotesize
\centering
\begin{tabularx}{1.0\columnwidth}{Xc|c|cc}
\toprule
\multirow{2}{*}{Methods}                    &           & Mesh                  & \multirow{2}{*}{MPVE}    & Recon.  \\
                                            &           & Supervision           &                          & Error  \\
\midrule
SMPLify \cite{bogo2016keep}*                 & ECCV'16   & \textcolor{green}{N}  &   -     &  106.1 \\
HMR                                         & CVPR'18   & \textcolor{red}{Y}    & 161.0   & 81.3  \\
Kundu \etal \cite{kundu2020mesh}            & ECCV'20   & \textcolor{green}{N}  &  -      & 78.2  \\
ExPose \cite{choutas2020expose}             & ECCV'20   & \textcolor{red}{Y}    & -       & 60.7  \\
Rong~\etal~\cite{Rong_2019_ICCV}            & CVPR'19   & \textcolor{red}{Y}    & 152.9   & -     \\
SPIN   \cite{kolotouros2019spin}            & ICCV'19   & \textcolor{red}{Y}    & 112.8   & 59.2  \\
Pose2Mesh \cite{choi2020pose}               & ECCV'20   & \textcolor{red}{Y}    & -       & 58.9  \\
I2L-Mesh  \cite{moon2020i2l}                & ECCV'20   & \textcolor{red}{Y}    & 110.1   & 58.6  \\
Zanfir \etal \cite{zanfir2020weakly}        & ECCV'20   & \textcolor{red}{Y}    &   -     & 57.1  \\
Song \etal \cite{song2020human}*             & ECCV'20   & \textcolor{green}{N}  & -       &  55.0 \\
\midrule
KAMA (ours)                                        &           & \textcolor{green}{N}    & \bf 107.7 & \bf 54.5  \\
\multicolumn{2}{l|}{KAMA w. refinement*}                 & \textcolor{green}{N}   & \bf 97.0 & \bf 51.1  \\
\bottomrule
\end{tabularx}
\caption{Comparison with state-of-the-art methods on \textbf{3DPW} dataset. *optimization-based methods\vspace{-5mm}}
\label{tab:tab:sota_3dpw}
\end{table} 


In order to further evaluate the impact of initialization, we also implement a 3D version of SMPLify~\cite{bogo2016keep} which initializes the pose parameters  with the mean body pose. We initialize the global orientation using the rotation of the  keypoint obtained using~\eqref{eq:rotation_rules} and global scale and translation using~\eqref{eq:scale_and_translation}. This results in a MPVE and 3D reconstruction error of 100.7mm and 55.8mm, respectively, that are significantly higher than the case when predictions from KAMA are used for initialization, demonstrating that KAMA can be used as a very good initialization approach. In fact, KAMA without any optimization achieves better 3D reconstruction error than SMPLify3D (54.5mm vs 55.8). Finally, for completeness, we also implement a 2D version of SMPLify by removing  from the objective while keeping all other error terms and initialization as SMPLify3D. The errors increase significantly showing that the 3D keypoints are important for accurately reconstruction. 

Finally, we also evaluate the impact of 3D keypoint accuracy on body mesh reconstruction using KAMA. For this, we extracted  keypoints from the ground-truth meshes, and used KAMA to reconstruct body mesh. This setting serves as an upper-bound for KAMA. Using ground-truth 3D keypoints significantly decreases the errors showing that the performance can be improved further by using a more accurate 3D keypoint regressor. Notably, the difference between KAMA and KAMA-with-refinement decreases which indicates that the refinement step may not be required with more accurate 3D keypoints. 





\begin{figure*}[hbt!]
  \centering
    \setlength{\tabcolsep}{1pt}
    \renewcommand{\arraystretch}{0.1}
\scalebox{0.95}{
  \begin{tabular}{ccc}
    \includegraphics[trim={9.1cm 0cm 0cm 0cm},clip, width=0.33\textwidth]{figures/coco-qualitatives/00006.png} &
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00496.png} & 
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00303.png} \\
        \includegraphics[trim={9.1cm 0cm 0cm 0cm},clip, width=0.33\textwidth]{figures/coco-qualitatives/00122.png} &
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00096.png} & 
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00055.png} \\
    \includegraphics[trim={9.1cm 0cm 0cm 0cm},clip, width=0.33\textwidth]{figures/coco-qualitatives/00246.png} &
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00659.png} & 
    \includegraphics[trim={9.1cm 0cm 0cm 0cm 0},clip,width=0.33\textwidth]{figures/coco-qualitatives/00556.png} \\
  \end{tabular}
}
\caption{Some qualitative results from the validation set of COCO dataset.}
 \vspace{-4mm} 
\label{fig:qualitative}
\end{figure*}




\subsection{Comparison to the State-of-the-Art}
We compare the performance of our approach with the state-of-the-art on 3DPW and Human3.6M datasets. We only compare with methods that use static images as input and perform body mesh estimation. 

Tab.~\ref{tab:tab:sota_3dpw} compares our proposed method with the state-of-the-art on 3DPW dataset. We chose the best numbers reported in all papers. The MPVE for SPIN~\cite{kolotouros2019spin} and I2L-Mesh~\cite{moon2020i2l} are obtained using the publicly available source codes, whereas the MPVE for HMR is obtained from~\cite{Rong_2019_ICCV}. KAMA without any additional refinement already outperforms all state-of-the-art methods. Refining the mesh estimates using~\eqref{eq:optimization} further improves the results and sets a new state-of-the-art on 3DPW dataset. Note that the methods~\cite{hmrKanazawa18, kolotouros2019convolutional, kolotouros2019spin, pavlakos2019texture, guler2019holo, moon2020i2l, zanfir2020weakly} use images annotated with ground-truth mesh annotations, and the method~\cite{song2020human} relies on very strong pose priors learned from a massive corpus of MOSHed~\cite{Loper:SIGASIA:2014} motion capture data, AMASS~\cite{AMASS:ICCV:2019}.
KAMA, in contrast, does not require any mesh annotations and is trained using 3D keypoints supervision only, yet it outperforms them with a large margin. This demonstrates that a good keypoint regressor combined with KAMA can yield state-of-the-art mesh reconstruction without the need of hard-to-acquire body mesh annotations.  

\begin{table}[t]
\footnotesize
\centering
\begin{tabularx}{1\columnwidth}{Xc|c|c}
\toprule
\multirow{2}{*}{Methods}                    &           & Mesh                  &     Reconstruction  \\
                                            &           & Supervision           &       Error   \\

\midrule
SMPLify  \cite{bogo2016keep}*                            & ECCV'16  & \textcolor{green}{N} & 82.3 \\
SMPLify-X \cite{SMPL-X:2019}*                            & CVPR'19  & \textcolor{green}{N} & 75.9 \\
HMR  \cite{hmrKanazawa18}                               & CVPR'18  &  \textcolor{red}{Y} & 56.8\\
Song \etal \cite{song2020human}*                         & ECCV'20  & \textcolor{green}{N} & 56.4 \\
GraphCMR \cite{kolotouros2019convolutional}             & CVPR'19  & \textcolor{red}{Y} & 50.1\\
STRAPS \cite{akash2020synthetic}                        & BMVC'20  & \textcolor{green}{N} & 55.4 \\
Pose2Mesh \cite{choi2020pose}                           & ECCV'20  & \textcolor{red}{Y} & 47.0 \\
TexturePose \cite{pavlakos2019texture}                  & ICCV'19  & \textcolor{red}{Y} & 49.7 \\
Kundu \etal \cite{kundu2020mesh}                        & ECCV'20  & \textcolor{green}{N} & 48.1\\
HoloPose \cite{guler2019holo}                           & CVPR'19  & \textcolor{red}{Y} & 46.5 \\
DSD \cite{sun2019human}                                 & ICCV'19  & \textcolor{red}{Y} & 44.3 \\
I2L-MeshNet \cite{moon2020i2l}                          & ECCV'20 & \textcolor{red}{Y} &  41.7  \\
SPIN \etal  \cite{kolotouros2019spin}                   & ICCV'19 & \textcolor{red}{Y} & 41.1 \\
\midrule
Ours                                                    &  & \textcolor{green}{N} & \bf 41.5  \\
Ours w. refinement*                                      &  & \textcolor{green}{N} & \bf 40.2 \\
\bottomrule
\end{tabularx}
\caption{Comparison with the state-of-the-art methods on \textbf{Human3.6M} dataset. *optimization-based methods 
\vspace{-5mm}
}
\label{tab:sota_h36m}
\end{table} 


Tab.~\ref{tab:sota_h36m} compares our proposed method with the state-of-the-art on Human3.6M dataset. On this dataset, KAMA without refinement performs on-par with SPIN~\cite{kolotouros2019spin} and I2L-MeshNet~\cite{moon2020i2l}. This is likely because of the limited diversity of Human3.6M where these methods can overfit using the full mesh annotations. Nonetheless, as before, refining the predictions of KAMA using~\eqref{eq:optimization} reduces the errors and results in state-of-the-art performance on Human3.6M. We would also like to emphasize that KAMA, unlike many other methods, provides meshes in absolute camera coordinates and uses perspective projection to project the meshes on to the images. This is in contrast to \eg, SPIN~\cite{kolotouros2019spin} and other methods that only predict root-relative meshes and use weak-perspective projection, hence, incur lower errors as compared to our method. 

Finally, in Fig.~\ref{fig:qualitative}, we provide some qualitative results of our approach on  in-the-wild images taken from MS-COCO~\cite{lin2014microsoft} dataset.\footnote{More qualitative results: \url{https://youtu.be/mPikZEIpUE0}}
