\documentclass[11pt]{article}
\usepackage{array,ifthen,amsmath,amssymb,amsthm,latexsym,epsfig,bm,verbatim}

\setlength{\oddsidemargin}{5mm}

\setlength{\evensidemargin}{5mm} \setlength{\textwidth}{150mm}
\setlength{\headheight}{0mm} \setlength{\headsep}{12mm}
\setlength{\topmargin}{0mm} \setlength{\textheight}{220mm}

\def\eric#1{\marginpar{\fbox{E}}\footnote{~{\sf #1 --Eric}}}
\def\daniel#1{\marginpar{\fbox{D}}\footnote{~{\sf #1 --Daniel}}}

\newcommand{\CR}{\noindent\hrule}
\newcommand{\MCR}{\vskip 0.2cm\CR\vskip 0.2cm}


\def\eee{\mathrm{e}}
\def\eps{\varepsilon}
\def\al{\alpha}
\def\be{\beta}
\newcommand{\positivereals}{\mathbb{R}_+}
\newcommand{\reals}{\mathbb{R}}

\def\imp{\ \Rightarrow\ }
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def\iff{\ \Leftrightarrow\ }
\def\OR{\ \vee\ }
\def\AND{\ \wedge\ }

\def\F{{\mathbb F}}
\def\R{{\mathbb R}}
\def\Q{{\mathbb Q}}
\def\Z{{\mathbb Z}}
\def\C{{\mathbb C}}

\def\e{{\mathrm e}}
\def\ii{{\mathrm i}}

\def\N{{\mathcal N}}
\def\P{{\mathcal P}}

\def\<{\langle}
\def\>{\rangle}
\def{\right)} \def\hatU{\widehat{U}}
\def\What{\widehat{W}}
\def\Shat{\widehat{S}}


\newtheorem{thm}{Theorem}[section]
\newtheorem{pro}[thm]{Proposition}
\newtheorem{obs}[thm]{Observation}
\newtheorem{exc}{Exercise}
\newtheorem{nte}{Note}
\newtheorem{rem}[thm]{Remark}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{con}{Conjecture}
\newtheorem{exm}[thm]{Example}
\newtheorem{dfn}[thm]{Definition}
\newtheorem{fct}[thm]{Fact}
\newtheorem{que}[thm]{Question}

\newenvironment{prf}[1]{\noindent{\bf{Proof #1\\}}}{\nopagebreak[4]\vskip 0.3cm}


\def\argmax{\mathrm{argmax}}

\def\BS{\begin{footnotesize}}
\def\ES{\end{footnotesize}}
\def\BCON{\begin{con}}
\def\ECON{\end{con}}
\def\BOBS{\begin{obs}}
\def\EOBS{\end{obs}}
\def\QUED{\nopagebreak[4]\vskip 0.3cm}
\def\QUET{\nopagebreak[4]\vskip 0.3cm}
\def\BPRO{\begin{pro}}
\def\EPRO{\end{pro}}
\def\BIDE{\begin{ide}}
\def\EIDE{\end{ide}}
\def\BTHM{\begin{thm}}
\def\ETHM{\end{thm}}
\def\BDEF{\begin{dfn}\rm}
\def\EDEF{\end{dfn}}
\newcommand\BPRF[1][:]{\begin{prf}{#1}}
\def\EPRF{\end{prf}}
\def\BLEM{\begin{lem}}
\def\ELEM{\end{lem}}
\def\BEX{\begin{exm}\rm}
\def\EEX{\end{exm}}
\def\BEXC{\begin{exc}\rm}
\def\EEXC{\end{exc}}
\def\BCOR{\begin{cor}}
\def\ECOR{\end{cor}}
\def\BQUE{\begin{que}}
\def\EQUE{\end{que}}
\newcommand\BSOL[1][:]{\begin{sol}{#1}}
\def\ESOL{\end{sol}}
\def\BNTE{\begin{nte}}
\def\ENTE{\end{nte}}

\def\BIT{\begin{itemize}}
\def\EIT{\end{itemize}}
\def\BREM{\begin{rem}\rm}
\def\EREM{\end{rem}}
\def\BC{\begin{center}}
\def\EC{\end{center}}
\def\BEQ{}




\newcommand\Exp[1]{{\mathrm e}^{#1}}


\def\Prob#1{{\mathrm{Pr}\left({#1}\right)}}
\def\ProbSub#1#2{{\mathrm{Pr}_{#1}\left({#2}\right)}}
\def\ProbCond#1#2{{\mathrm{Pr}\left({#1} \mid {#2} \right)}}
\def\ProbCondSub#1#2#3{{\mathrm{Pr}_{#1}\left({#2} \mid {#3} \right)}}
\def\Expect#1{{\mathrm{E}\left({#1}\right)}}
\def\ExpCond#1#2{{\mathrm{E}\left({#1} \mid {#2} \right)}}
\def\ExpCondSub#1#2#3{{\mathrm{E}_{#1}\left({#2} \mid {#3} \right)}}
\def\ExpSub#1#2{{\mathrm{E}_{#1}\left({#2}\right)}}
\def\Var#1{{\mathrm{Var}\left({#1}\right)}}

\def\VarSub#1#2{{\mathrm{Var}_{#1}\left({#2}\right)}}


\def\dtv#1{\| #1 \|_{\mathrm TV}}
\def\Ldist#1#2{\left\| \frac{#1}{#2} - 1 \right\|_{2,#2}}
\def\Lsquare#1#2{ \left\| \frac{#1}{#2} - 1 \right\|_{2,#2}^2}


\def\M{\mathcal{M}}

\newcommand{\leftexp}[2]{{\vphantom{#2}}^{#1}{#2}}

\date{}

\title{Approximating -distances between mixture distributions using random projections}

\author{Satyaki Mahalanabis  Daniel \v{S}tefankovi\v{c}
\vspace{0.2cm}\\
Department of Computer Science\\ University of Rochester\\ Rochester, NY 14627\\
{\tt \{smahalan,stefanko\}@cs.rochester.edu}}

\begin{document}

\maketitle

\begin{abstract}
We consider the problem of computing -distances between every pair of
probability densities from a given family. We point out that the technique
of Cauchy random projections~\cite{I06} in this context turns into stochastic
integrals with respect to Cauchy motion.

For piecewise-linear densities these integrals can be sampled from
if one can sample from the stochastic integral of the function
. We give an explicit density function for this
stochastic integral and present an efficient (exact) sampling
algorithm. As a consequence we obtain an efficient algorithm to
approximate the -distances with a small relative error.

For piecewise-polynomial densities we show how to approximately
sample from the distributions resulting from the stochastic
integrals. This also results in an efficient algorithm to
approximate the -distances, although our inability to get
exact samples worsens the dependence on the parameters.
\end{abstract}

\section{Introduction}

Consider a finite class  of probability densities.
We want to compute the distance between every pair of members of .
We are interested in the case where each member of  is a mixture of finitely
many probability density functions, each having a particular functional form
(e.\,g., uniform, linear, exponential, normal, etc.). Such classes of distributions are frequently
encountered in machine learning (e.\,g., mixture models, see~\cite{Bish06}) and  nonparametric density
estimation (e.\,g., histograms, kernels, see~\cite{DG01}). The number of distributions in a mixture
gives a natural measure of complexity which we use to express the running time of our
algorithms.

For some classes of distributions exact algorithms are possible, for example, if each distribution in
 is a piecewise linear function consisting of  pieces then we can compute the distances
between all pairs in time . For other classes of distributions (for example,
mixtures of normal distributions) exact computation of the distances might not be possible.
Thus we turn to randomized approximation algorithms. A {\em -relative-error
approximation scheme} computes  such that with probability at least  we have

A {\em -absolute-error approximation scheme} computes  such that
with probability at least  we have


A direct application of the Monte Carlo method (\cite{MU49}, see \cite{M87}) immediately yields the following
absolute-error approximation scheme. Let  be sampled according to  and let
, where  is
the sign function. The expected value of  is equal to , indeed

Thus, to obtain a -absolute-error approximation scheme it is enough to
approximate each  with absolute error  and confidence .
By the Chernoff bound  samples from each  are enough.
(The total number of samples from the  is ,
since we can use the same sample from  for . The total
number of evaluations is .) The running time of this
algorithm will compare favorably with the exact algorithm if {\em sampling} from the densities
and {\em evaluation} of the densities at a point can be done fast.
(For example, for piecewise linear densities both sampling and evaluation can be done
in  time, using binary search.)
Note that the evaluation oracle is essential (cf. \cite{BFRSW00} who only allow use of sampling oracles).

In the rest of the paper we will focus on the harder relative-error approximation
schemes (since the -distance between two distributions is at most ,
a relative-approximation scheme immediately yields an absolute-error approximation scheme).
Our motivation comes from an application (density estimation) which requires a
relative-error scheme~\cite{MS08}.

Now we outline the rest of the paper. In Section~\ref{z2} we review Cauchy random projections;
in Section~\ref{z3} we point out that for density functions Cauchy random projections become stochastic
integrals; in Section~\ref{z4} we show that for piecewise linear functions we can sample from these
integrals (using rejection sampling, with bivariate student distribution as the envelope) and
as a consequence we obtain efficient approximation algorithm for relative-error all-pairs--distances.
Finally, in Section~\ref{z5}, we show that for piecewise polynomial functions one can
approximately sample from the integrals, leading to slightly less efficient approximation algorithms.


\section{Cauchy random projections}\label{z2}

Dimension reduction (the most well-known example is the Johnson-Lindenstrauss lemma
for -spaces~\cite{JL84}) is a natural technique to use here. We are interested in
-spaces for which the
analogue of the Johnson-Lindenstrauss lemma is not possible~\cite{BC05,LN04} (that is,
one cannot project points into a low dimensional -space and preserve distances
with a small relative error). However one can still project points to short vectors from
which -distances between the original points can be approximately recovered
using {\em non-linear} estimators~\cite{LHC07,I06}.

A particularly fruitful view of the dimensionality ``reduction'' (with non-linear estimators)
is through stable distributions (\cite{JS82,I06}): given vectors  one defines
(dependent) random variables  such that the distance of  and  can be
recovered from  (for all ). For example, in the case of -distances
 will be from Cauchy distribution , and hence the recovery problem is
to estimate the scale parameter  of Cauchy distribution . This
is a well-studied problem (see, e.\,g.,~\cite{HBA70}). We can, for example, use the following nonlinear
estimator (other estimators, e.\,g., the median are also possible~\cite{I06}):

\BLEM[Lemma~7 of ~\cite{LHC07}]\label{l:lhctail}
Let  be independent samples from the Cauchy distribution .
Define the geometric mean estimator without bias-correction  as

Then for each , we have
\hat{D}_{gm} \in [ (1-\eps)D, (1+\eps)D ] 
\ELEM

We first illustrate how Cauchy random projections immediately give an efficient relative-error approximation
scheme for piecewise uniform distributions.

Let  consist of  piecewise uniform densities, that is, each member of  is
a mixture of  distributions each uniform on an interval. Let  be the
endpoints of all the intervals that occur in  sorted in the increasing order
(note that ). Without loss of generality, we can assume that
each distribution  is specified by  pairs
 where
,
and for each pair  we are also given
a number  which is the value of  on
the interval .

Now we will use Cauchy random projections to compute the pairwise -distances between the 
efficiently. For  let  be independent from the
Cauchy distribution . Let , for .
Finally, let

Note that  is a sum of Cauchy random variables and hence has Cauchy distribution (in fact it
is from ). Thus  will be from Cauchy distribution as well. The coefficient of
 in  is the difference of  and  on interval .
Hence the contribution of  to  is from Cauchy distribution
, and thus
 is from Cauchy distribution .

\BREM\label{det}
In the next section we will generalize the above approach to piecewise degree--polynomial
densities. In this case for each  we are given a vector
 such that the value of  on interval 
is given by the following polynomial (written as an inner product):

\EREM

\section{Cauchy motion}\label{z3}

A natural way of generalizing the algorithm from the previous section to arbitrary
density functions is to take infinitesimal intervals. This leads one to the well-studied area of stochastic
integrals w.r.t. symmetric -stable L\'evy motion (also called Cauchy motion). Cauchy motion
is a stochastic process  such that ,  has independent increments
(i.\,e., for any  the random variables
 are independent), and  is
from Cauchy distribution . Intuitively, stochastic integral of a {\em deterministic function}
w.r.t. Cauchy motion is like a regular integral, except one uses  instead of 
for the length of an interval (see section 3.4 of~\cite{ST94} for a readable formal
treatment).

We will only need the following basic facts about
stochastic integrals of deterministic functions w.r.t. Cauchy motion (which we will denote ),
see~\cite{ST94}, Chapter 3.

\begin{fct}\label{fact1}
Let  be a (Riemann) integrable function. Let
.
Then  is a random variable from Cauchy distribution  where

\end{fct}

\begin{fct}\label{fact2}
Let  be (Riemann) integrable functions. Let .
Let .
Then  is a random variable with characteristic function
-\int_{a}^b |c_1f_1(x)+\dots+c_df_d(x)|\,{\rm d}x
\end{fct}

\begin{fct}\label{fact3}
Let  be (Riemann) integrable functions. Let . Then

Let . Then

\end{fct}

From facts~\ref{fact1} and \ref{fact3} it follows that the problem of approximating the -distances
between densities can be solved if we can evaluate stochastic integrals w.r.t. Cauchy motion;
we formalize this in the following observation.

\BOBS
Let  be probability densities. Let  be defined by
. Consider

For all  we have that  is from Cauchy distribution .
\EOBS

Note that the  defined by \eqref{stint} are in fact computing the integral in~\eqref{pakoq}. For
piecewise uniform densities it was enough to sample from the Cauchy distribution to compute the integral.
For piecewise degree--polynomial densities it will be enough to sample from the following distribution.

\BDEF
Let  be defined by . Let 
be the distribution of , where

\EDEF

Note that given a sample from , using  arithmetic operations
we can obtain a sample from , using Fact~\ref{fact3}.

\BLEM\label{lpiece}
Let  consist of  piecewise degree--polynomial densities, each consisting of 
pieces (given as in Remark~\ref{det}). Let  be an integer.
Assume that we can sample from  using  operations. We can obtain
-relative-error approximation of -distances between all
pairs in~, using  arithmetic operations.
\ELEM

\BPRF
For  let  be independent from  distribution. Let
, for . Finally, for each , let

Note that  is from 
and hence

Thus  defined by \eqref{stint2} compute \eqref{pakoq}.

For every  we have that  is from Cauchy distribution

If we have  samples from each  then using Lemma~\ref{l:lhctail} and union bound with
probability  we recover all  with relative error .

Note that  and hence for the  we used  samples from 
distribution, costing us  arithmetic operation. Computing the  takes  operations.
Computing the  takes  operations. The final estimation of the distances
takes  operations.
\EPRF

\section{Piecewise linear functions}\label{z4}

The density function of  can be computed explicitly, using the inverse Fourier transform;
the proof is deferred to the appendix. The expression for the density allows us to construct
efficient sampling algorithm, which in turn yields an efficient approximation algorithm for
all-pairs--distances for piecewise linear densities. We obtain the following result.

\BTHM\label{tele}
Let  consist of  piecewise linear densities, each consisting of 
pieces (given as in Remark~\ref{det}). We can obtain
-relative-error approximation of -distances between all
pairs in~, using  arithmetic operations.
\ETHM

Now we state the density of . In the following  denotes the
real part of a complex number .

\BTHM\label{looo}
Let  be the function . Let

For  the density function of  is given by
\frac{{\rm atan}(iQ/(x_1-2x_2))}
{Q^{3/2}}
where

For  the density is given by

\ETHM


\begin{figure}[htb]
    \begin{center}
        \includegraphics[type=eps,ext=.eps,read=.eps,angle=0,scale=1]{pic}
      \caption{The density plot of .
      The contours are at levels .
      }
      \label{fig:piecewise}
    \end{center}
\end{figure}


Next we show how to efficiently sample from the  distribution
by rejection sampling using the bivariate student distribution
as the envelope.

Let  be a positive-definite  matrix. The bivariate
student distribution with  degree of freedom is given by the following formula
(see, e.\,g.,~\cite{ES00}, p.~50)
1+\frac{{\mathbf x}^T \Sigma^{-1} {\mathbf x}}{2}
It is well-known how to sample  from this distribution: let ,
where  are independent with  (the two dimensional gaussian) and
 (chi-squared distribution with  degree of freedom).

We are going to use the bivariate student distribution with the following density
1+x_1^2+(2x_2-x_1)^2

We show that the density function of the  distribution
is bounded by a constant multiple of \eqref{ZZZ} (the proof is deferred to
the appendix).

\BLEM\label{kkkk}
Let  be given by \eqref{eden} and \eqref{eden2}. Let
 be given by \eqref{ZZZ}. For every  we
have

where .
\ELEM

As an immediate corollary of Lemma~\ref{kkkk} we obtain an efficient sampling
algorithm for  distribution, using rejection sampling (see, e.\,g.,
~\cite{ES00}).

\BCOR\label{cansample}
There is a sampler from  which uses
a constant number of samples from from  and 
(in expectation).
\ECOR

\BPRF[of Theorem~\ref{tele}:]
The theorem follows from Corollary~\ref{cansample} and Lemma~\ref{lpiece}.
\EPRF

\BREM
Lemma~\ref{kkkk} is true with  (we skip the technical proof).
The constant  is tight (see equation \eqref{leq13}
with  and ).
\EREM

\section{Piecewise polynomial functions}\label{z5}

Some kernels used in machine learning (e.\,g., the Epanechnikov kernel, see~\cite{DG01}, p.85)
are piecewise polynomial. Thus it is of interest to extend the result from the previous
section to higher-degree polynomials.

For  we do not know how to sample from distribution  exactly. However we can still
approximately sample from this distribution, as follows. Let  be an integer. Let 
be independent from Cauchy distribution . Consider the following distribution,
which we call -approximation of :

Now we show that if  is large enough then the distribution given by~\eqref{apd} can be used instead of
distribution  for our purpose. As a consequence we will obtain the following.

\BTHM\label{trwe}
Let  consist of  piecewise degree--polynomial densities, each consisting of 
pieces (given as in Remark~\ref{det}). We can obtain
-relative-error approximation of -distances between all
pairs in~, using  arithmetic operations.
\ETHM

\BREM
Note that for  Theorem~\ref{trwe} gives worse (in ) running time that
Theorem~\ref{tele}. This slowdown is caused by the additional integration used to
simulate .
\EREM

The proof of Theorem~\ref{trwe} will be based on the following result which shows that
\eqref{apd} is in some sense close to .

\BLEM\label{lesa}
Let  be a polynomial of degree . Let 
be sampled from the distribution given by~\eqref{apd}, with  (where
 is an absolute constant). Let
. Then  is from the Cauchy distribution , where

\ELEM

We defer the proof of Lemma~\ref{lesa} to the end of this section. Note that
having \eqref{zwer} instead of \eqref{mrch} (which sampling from  would yield)
will introduce small relative error to the approximation of the -distances.

\vskip 0.2cm
\BPRF[of Theorem~\ref{trwe}:]
The proof is analogous to the proof of Lemma~\ref{lpiece}. Let .
For  let  be independent from
-approximation of  distribution.
Let
, for . Finally, for each , let

By Lemma~\ref{lesa}, for every  we have that  is from Cauchy distribution  where
.

If we have  samples from each  then using
Lemma~\ref{l:lhctail} and union bound with probability  we recover
all  with relative error .

Note that  and hence for the  we used  samples from
r-approximation of  distribution, costing us  arithmetic operation.
Computing the  takes  operations. Computing the  takes  operations.
The final estimation of the distances takes  operations.
\EPRF

To prove Lemma~\ref{lesa} we will use the following Bernstein-type inequality from~\cite{Erd00}.

\BTHM\label{l:erdelyi}
(Theorem 3.1 of~\cite{Erd00})
There exists a constant  such that for any degree  polynomial ,

\ETHM

We have the following corollary of Theorem~\ref{l:erdelyi}.

\BLEM\label{l:interpolate}
There exists a constant  such that for any polynomial  of degree ,
any , any  with ,
and any , we have

\ELEM

\BPRF
We will use induction on the degree  of the polynomial. For  the sum and the integrals in \eqref{e:interpolate}
are equal.

Now assume . For each , we use the Taylor expansion of  about  for
. This yields for each , where .
Let  be the point  that maximizes . We have

Since  is of degree , by induction hypothesis the right-hand side of~\eqref{e:interpolate1}
is bounded as follows

where in the last inequality we used Theorem~\ref{l:erdelyi}. Hence the lemma follows.
\EPRF

\BPRF[of Lemma~\ref{lesa}:]
We have

where  are from Cauchy distribution . Thus  is from Cauchy distribution , where

Using Lemma~\ref{l:interpolate} we obtain \eqref{zwer}.
\EPRF

\BREM
An alternate view of Lemma~\ref{l:interpolate} is that a piecewise degree--polynomial
density with  pieces can be approximated by a piecewise uniform density with
 pieces. The approximation distorts -distances between any pair
of such densities by a factor at most . To obtain a relative-approximation
of the -distances in a family  one can now directly use the algorithm from Section~\ref{z2} without
going through the stochastic integrals approach (for  the price for this
method is a  factor slowdown).
\EREM

\BREM{\bf (on -distances)}
For -distances the dimension reduction uses normal distribution instead of Cauchy distribution.
For infinitesimal intervals the corresponding process is Brownian
motion, which is much better understood than Cauchy motion. Evaluation of a stochastic
integral of a deterministic function  w.r.t. Brownian motion is a
-dimensional gaussian (whose covariance matrix is easy to obtain), for example

is from  where  is the  Hilbert matrix (that
is, the -th entry of  is ).
\EREM

\BQUE
How efficiently can one sample from  distribution? A reasonable
guess seems to be that one can sample from a distribution within -distance
 from  using  samples.
\EQUE

\vskip 0.2cm
\noindent
{\bf\Large Acknowledgement}

\vskip 0.2cm
\noindent
The authors would like to thank Carl Mueller for advice on stochastic integrals.

\begin{comment}
\section{Approximating -distances}

Not main focus  not significant (invariance), easier

\BLEM\label{l:gaussint}
Consider a Brownian motion  on . Define random variables  for . Then
for each , the distribution  of  is  where
 is the Hilbert matrix


\ELEM

\BPRF It is easy to see that  is distributed as a gaussian. The  entry of  is .
\EPRF

\BREM
The matrix  (Lemma~\ref{l:gaussint}) has the following Cholesky decomposition:
 where

\EREM

\BREM
The matrix  (Lemma~\ref{l:gaussint}) has the following Cholesky decomposition:  where

\EREM

\BREM Consider the family of polynomials  where for each ,  is of degree  and defined as

From decomposition~\eqref{e:cholesky}, it follows that ,  are independent.
\EREM

We state below the first few such polynomials.


\BLEM\label{l:gausstail}
There is an estimate  given independent samples  from  such that for each , if  then

\ELEM

\BPRF
Consider a set of  i.i.d samples  and define . Then  and . By Chebyshev's inequality,

Define  to be the median of  for   such sets of independent samples. An application of Chernoff bound now yields the lemma.



\EPRF

\BTHM\label{t:l2}
Let  be a class of  piecewise polynomial densities (defined in~\eqref{e:polydense}). There exists an algorithm which for any such  and on input any , outputs for each ,  such that with probability at least ,

The algorithm requires  time.
\ETHM

\BPRF
As usual let  () denote the points  in sorted order. Also let . Intuitively, we just need to use a Brownian motion  (instead of a Cauchy process). We compute  matrices  of size  each such that for each ,  is drawn independently from  (as defined in Lemma~\ref{l:gaussint}). Also, for each  and , we compute -dimensional vectors  whose  entry is  where . Finally for each  we compute

Observe that for each ,  and are i.i.d. The estimates for each , are then given by Lemma~\ref{l:gausstail} applied to .
\EPRF

\MCR
\BIT
\item  - densities
\item  - 
\item  - function to be integrated
\item  - random variables
\item  - degree of polynomial, dimension
\item  - complexity of the densities
\item  - the number of densities
\item  - integers for sums
\item  - polynomials
\item  - relative precision,  - absolute precision,  - confidence
\item , 
\item  - estimate of distance
\EIT
\MCR

\end{comment}

\bibliographystyle{alpha}
\bibliography{pairwisel1}

\section{Appendix}

\subsection{Stochastic integral of (constant, linear) function }

In this section we give an explicit formula for the density
function of the random variable

where , and  is the Cauchy
motion.

We will obtain the density function from the characteristic
function. The following result will be used in the inverse Fourier
transform. (We use  to denote the real part of a complex
number.)

\BLEM\label{leee}
Let . Let

where  is the Cauchy motion. The density function  of  is given by
\frac{(n-1)!}{(2\pi)^n}
\int_{-\infty}^\infty\dots\int_{-\infty}^\infty
\frac{2}{(A+iB)^n}
{\rm d}b_1 \dots {\rm d}b_{n-1}
where

and

\ELEM

\BPRF The characteristic function of  is (see, e.\,g.,
proposition 3.2.2 of ~\cite{ST94}):
-\int_0^1 \left| a_1\phi_1(x)+\dots+a_n\phi_n(x) \right|
We will use the following integral, valid for any  (see,
e.\,g.,~\cite{GR07}):
\frac{1}{(A-iB)^n}+\frac{1}{(A+iB)^n}
We would like to compute the inverse Fourier transform of , which, since
 is symmetric about the origin, is given by

Substitution  into \eqref{ew} yields
-t \int_0^1 \left| b_1\phi_1(x)+\dots+b_{n-1}\phi_{n-1}(x) + \phi_n(x) \right|t(b_1x_1 + \dots + b_{n-1} x_{n-1} + x_n)
Note that the inner integral has the same form as \eqref{intog} and hence
we have
\frac{(n-1)!}{(2\pi)^n}
\int_{-\infty}^\infty\dots\int_{-\infty}^\infty
\frac{2}{(A+iB)^n}
{\rm d}b_1 \dots {\rm d}b_{n-1}
where  and  are given by \eqref{zzz} and \eqref{zzzb}. The
last equality in \eqref{ewr} follows from the fact that the two
summands in the integral are conjugate complex numbers.
\EPRF

Now we apply Lemma~\ref{leee} for the case of two functions,
one constant and one linear.
\vskip 0.3cm

\BPRF[of Theorem~\ref{looo}:] Plugging , , and
 into \eqref{zzz} and \eqref{zzzb} we obtain

and

Our goal now is to evaluate the integral \eqref{eeeeq}. We split the integral into
 parts according to the behavior of .

We will use the following integral

For  and  we have . Using
\eqref{nein1} for  and  given by \eqref{eqB} and \eqref{eqA}) we obtain

and

We have (see, e.\,g.,~\cite{GR07}))
S + 2 z)/\sqrt{ 4 T -S^2}
For  and  we have .
Using \eqref{nein} we obtain
\frac{i x_1+1}{\sqrt{Q}}\frac{i x_1-1}{\sqrt{Q}}
where  is given by \eqref{qeq}.

Summing \eqref{e1xx}, \eqref{e3xx}, and \eqref{moui} we obtain
\frac{i x_1+1}{\sqrt{Q}}\frac{i x_1-1}{\sqrt{Q}}
We have

with equality only if . Hence if  then using \eqref{eeer2} we have
\frac{i x_1+1}{\sqrt{Q}}\frac{i x_1-1}{\sqrt{Q}}
and by applying
\frac{8}{Q(1+x_1^2)}
in \eqref{ekrt} we obtain \eqref{eden}.

If  then  and using
\frac{i x_1+1}{\sqrt{Q}}\frac{i x_1-1}{\sqrt{Q}}
in \eqref{ekrt} we obtain \eqref{eden2}.
\EPRF

\subsection{Bounding the -distribution}\label{sbound}

Now we prove that the multivariate student distribution gives
an efficient envelope for the -distribution.

\vskip 0.2cm
\BPRF[of Lemma~\ref{kkkk}:]
To simplify the formulas we use the following substitutions:  and .
The density  becomes
1+u^2+4w^2
For  (which corresponds to ) the density  becomes

and hence Lemma~\ref{kkkk} is  true, as
\frac{1}{\pi}^{-3/2}

For , density \eqref{eden} becomes

\frac{4}{(1+u^2)^2+(4w)^2}+\frac{{\rm atan}(iM/(2w))}{M^{3}}-\frac{{\rm atan}(iM'/(2w))}{{M'}^{3}}
where  and . We are going to show


Note that both sides of \eqref{leq2} are unchanged when we flip the sign of  or the sign of .
Hence we can, without loss of generality, assume  and .

There are unique  and  such that  and  (to see this
notice that substituting  into the second equation yields ,
where the right-hand side is a strictly increasing function going from  to ).
Note that  and . Also note that

After the substitution equation \eqref{leq2} simplifies as follows
\frac{1}{a}+\frac{i}{b}\frac{1}{a}-\frac{i}{b}
Now we expand  and  and simplify \eqref{leq4} into
{\rm atan}+{\rm atan}{\rm atan}-{\rm atan}
Now we substitute  and  into \eqref{leq5} and obtain
A+iBA-iBA+iBA-iB
Note that  and  and the constraint \eqref{econs} becomes

Multiplying both sides of \eqref{leq6} by  we obtain
A+iBA-iBA+iBA-iB
Finally, we substitute  and  with .
Note that the constraint \eqref{econs2} becomes

and hence  is restricted to .

Equation \eqref{leq7} then becomes
{\rm atan}+{\rm atan}{\rm atan}-{\rm atan}
We prove~\eqref{leq8} by considering three cases.

\vskip 0.2cm
\underline{{\bf CASE: }}. We can use \eqref{eeer2} to simplify~\eqref{leq8} as follows
\frac{2T\sin(\alpha)}{1-T^2}\frac{2T\cos(\alpha)}{1+T^2}
For  we have  and hence to prove~\eqref{leq9}
it is enough to show
2T\sin(\alpha)2T\cos(\alpha)
which is implied by the following inequality which holds for :

For  we directly prove~\eqref{leq10}
\frac{2T\sin(\alpha)}{1-T^2}\frac{2T\cos(\alpha)}{1+T^2}

\underline{{\bf CASE: }.} We can use \eqref{eeer3} and \eqref{lh1} to simplify~\eqref{leq8} as follows
\pi+{\rm atan}\frac{2T\cos(\alpha)}{1+T^2}
From \eqref{econs3} we have  and hence . Therefore
\eqref{leq13} can be proved as follows.
\pi+{\rm atan}\frac{2T\cos(\alpha)}{1+T^2}

\underline{{\bf CASE: }.}
Equation \eqref{leq8} simplifies as follows
\cos(\alpha)
The left-hand side is bounded from above by  which is less than 
which lower-bounds the right-hand side of \eqref{leq15}.
\EPRF

\subsection{Basic properties of trigonometric functions}

In this section we list the basic properties of trigonometric
functions that we used. For complex parameters these are
multi-valued functions for which we choose the branch in the
standard way. The {\em logarithm} of a complex number
, where , and  is . The {\em inverse
tangent} of a complex number 
is the solution of  with . In
terms of the logarithm we have
\ln(1-iz)-\ln(1+iz)
The inverse hyperbolic tangent function is defined analogously, for
 we have
\ln(1+z)-\ln(1-z)
For non-negative real numbers  we have the following inequality

The  function (even as a multi-valued function) satisfies

for any values of , with .

For  the real part of  is from . Hence
\frac{x+y}{1-xy}
For  and  the real part of  is from .

For  the real part of  is from . Hence
for any  with  we have
\frac{2ib}{1+a^2+b^2}

\end{document}
