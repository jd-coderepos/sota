In this section we consider another application that has received significant attention in the recent past: the max-min fair allocation problem
\cite{dani:05,julia:focs09,asadpour:stoc07,asadpour-feige-saberi,bansal:stoc06}. We provide a new algorithm for max-min fair allocation based on bipartite dependent rounding \cite{gkps:dep-round} and its generalization to weighted graphs. Bipartite dependent rounding has found many applications in combinatorial optimization \cite{srin:level-sets,gkps:dep-round,kmps:unified-sched}, and can be seen as a special case of {\bf RandMove} on bipartite graphs. We also consider an ``equitable allocations'' version of
such problems, in Theorem~\ref{thm:santa2}: this theorem follows from \cite{shmoys-tardos:gap} as pointed out by the referee. 

In the max-min fair allocation problem,
there are  goods that need to be distributed indivisibly among  persons. Each person  has a non-negative integer valuation  for good . The valuation functions are linear, i.e.,  for any set of  goods. The goal is to allocate each good to a person such that the ``least happy person is as happy as possible'': i.e.,  is maximized. Our main contribution  in this regard
is to near-optimally pin-point the integrality gap of a {\em configuration LP} previously proposed and analyzed in \cite{bansal:stoc06,asadpour:stoc07}.


\noindent
\paragraph{\textbf{The Configuration LP for Max-Min Fair Allocation}}

The configuration LP formulation for the max-min fair allocation problem was first considered in
\cite{bansal:stoc06}. A {\it configuration} is a subset of items,
and the LP has a
variable for each valid configuration. Using binary search, first the optimum solution value  is guessed and then
we define valid configurations based on the approximation factor  sought; we will set

We call a configuration   {\it valid for person } if either of the following two conditions hold:
\begin{itemize}
\item   and each item  in  has value less than . These are called {\em small} items.
\item  contains only one item  and . We call such an item  to be a {\em big} item for person .
\end{itemize}

We define a variable  for assigning a valid configuration  to person .
Let  denote the set of all valid configurations corresponding to person  with respect to . The configuration LP relaxation
 of the problem is as follows:



The above LP formulation may have an exponential number of variables, However, if the LP is feasible, then a fractional allocation where each person receives either a big item or at least a utility of  can be computed in
polynomial time for any constant  \cite{bansal:stoc06}.
In the subsequent discussion and analysis, we ignore the multiplicative
 factor; it is hidden in the  notation of
the ultimate approximation ratio.

The worst-case integrality gap of the above configuration LP is
lower-bounded by  \cite{bansal:stoc06}.
In \cite{asadpour:stoc07}, Asadpour and Saberi
gave a rounding procedure for the configuration LP that achieved an approximation factor of . Here we further lower
the gap and prove the following theorem;  our proof is also
significantly simpler than that of \cite{asadpour:stoc07}.


\begin{theorem}
\label{thm:santa1}
Given any feasible solution to the configuration LP, it can be rounded to a feasible integer solution such that every person gets  at least  fraction of the optimal utility with probability at least , in polynomial time.
\end{theorem}


Note that the work of Chakrabarty, Chuzhoy and Khanna \cite{Chuzhoy09}
yields an improved approximation factor of 
for any positive constant
, but it does not use the configuration LP
(also note that ).


In the context of fair allocation, an additional important criterion can be
an \emph{equitable partitioning} of goods: we may impose an upper
bound on the number of items a person might receive. For example,
we may want each person to receive at most  goods. Theorem \ref{thm:matching} then directly leads to the following.
\begin{theorem}
\label{thm:santa2}
Suppose, in max-min allocation, we are given upper bounds  on the
number of items that each person  can receive, in addition to the
 utility values . Let  be the optimal max-min allocation value
that satisfies  for all . Then, we can efficiently construct an allocation
in which for each person  the bound  holds and she receives a total
utility of at least .
\end{theorem}

This generalizes the result of \cite{dani:05}, which yields the
``'' value when no bounds such as the  are given.
To our knowledge, the results of
\cite{julia:focs09,asadpour:stoc07,asadpour-feige-saberi,bansal:stoc06}
do not carry over to the setting of such ``fairness bounds'' .

\subsection{Algorithm for Max-Min Fair Allocation}
\label{subsec:algo}

We now describe the algorithm and proof for Theorem \ref{thm:santa1}.

\subsubsection{Algorithm}

We define a weighted bipartite graph  with the vertex set  corresponding to the persons and the items respectively. There is an edge between a vertex corresponding to person  and item , if a configuration  containing  is fractionally assigned to . Define

i.e.,  is the
fraction of item  that is allocated to person  by the fractional solution of the LP. An edge  is called a {\em matching edge}, if the item  is big for person . Otherwise it is called a {\em flow edge}.





Let  and  represent the set of matching and flow edges respectively. For each vertex , let  denote the total fractional weight of the matching edges incident to it. That is if  is a person then  And if  is a job then   Also define . The main steps of the algorithm are as follows.

\bigskip


\begin{enumerate}
\item[1.] Guess the value of the optimal solution  by doing a binary search. Solve LP (\ref{eqn:lp}).
  Obtain the set  and  for each vertex  in  constructed from the LP solution.

  \bigskip

\item[2] {\bf Allocating Big Items}: Select a random matching from edges in  using { \em bipartite dependent rounding}
(see Section \ref{subsec:match}) such that for every , the probability that  is matched by the matching is .

\bigskip

\item[3] {\bf Allocating Small Items}: Let .
\begin{enumerate}
\item Discard any item  with , and also discard all the persons and the items matched by the matching.
\item (Scaling) In the remaining graph containing only flow edges for unmatched persons and items, set for each
person , .
\item Further discard any item   with , where  is defined below.
\item Scale down the weights on all the remaining edges by a factor of  and run the algorithm of \cite{dani:05} to assign the small items.
\end{enumerate}

matched
\end{enumerate}

 \noindent{\bf Choice of .}



Let us consider the functions  (note that
 is asymptotically zero.) and

For large enough , say , the following holds.

 This is easily verified by plugging the fact  into (\ref{eqn:Phi-bound}).

\bigskip


We now analyze each step. The main proof idea is in showing that there remains enough left-over utility in the flow graph for each person
not matched by the matching. This is obtained through proving a negative correlation property among the random variables
defined on a collection of vertices. Previously, the negative correlation property due to bipartite dependent
rounding was known for variables defined on edges incident on any particular vertex. We adapt the proof according to our need.

\subsubsection{Allocating Big Items}
\label{subsec:match}
Consider the edges in  in the person-item bipartite graph. Remove all the edges  that have already been rounded to  or . Additionally, if an edge is rounded to , remove both its endpoints  and .  We initialize for each , , and modify the  values probabilistically in rounds using bipartite dependent rounding.


\paragraph{{\bf Bipartite Dependent Rounding}\cite{gkps:dep-round}}
\label{subsec:bipartite}
 We give a brief sketch of bipartite dependent rounding introduced in \cite{gkps:dep-round} for the sake of completeness.

  The bipartite dependent rounding selects an even cycle  or a maximal path  in , and partitions the edges in  or  into two matchings  and . Then, two positive scalars  and  are chosen as follows:
 
 

Now with probability , set

with complementary probability of , set


The above rounding scheme satisfies the following two properties, which are easy to verify:




Thus, if  denotes the final rounded values then Property (\ref{dep:prop1}) guarantees for every edge ,  . This gives the following corollary.



\begin{corollary}
\label{cor:match1}
The probability that a vertex  is matched in the matching generated by the algorithm is .
\end{corollary}

\begin{proof}
Let there be  edges  that are incident on . Then,


Here the second equality follows by replacing the union bound by sum since the events are mutually exclusive.
\end{proof}

\paragraph*{Negative Correlation over Multiple Vertices} 
Now we show additional properties of this rounding to be used crucially for the analysis of the next step.
Recall the notion of negative correlation from Definition~\ref{defn:neg-correl}.
We show a useful negative-correlation property for dependent rounding on bipartite graphs over multiple vertices. The proof is syntactically similar to Lemma~2.2 of \cite{gkps:dep-round}. However, \cite{gkps:dep-round} only shows negative correlation property for random variables defined on edges incident to a single vertex; here a stronger negative correlation property is proven for random variables defined on multiple vertices. We state the theorem here, and prove it in the appendix. 



\begin{theorem}
\label{theorem:neg1}
Define an indicator random variable  for each item  with , such that 
 if item  is matched by the matching. Then, the indicator random variables  are negatively correlated.
 \end{theorem}



 As a corollary of Theorem \ref{theorem:neg1}, we get the following:
 \begin{corollary}
\label{cor:neg2}
Define an indicator random variable  for each person , such that 
 if person  is matched by the matching. Then, the indicator random variables  are negatively correlated.
 \end{corollary}

\begin{proof}
Do the same analysis as in Theorem \ref{theorem:neg1} with items replaced by persons.
\end{proof}


\subsubsection{Allocating small items}
\label{subsec:alloc}

We start by proving in Lemma \ref{lemma:small} that after the matching phase, we have with high probability that
each unmatched person has available items with utility at least  in the flow graph. Additionally we prove in Lemma  \ref{lemma:item} that given any \emph{particular} item , we have with probability at least  that  is claimed at most
 times. Note that this probability is not large enough to afford a union bound over all the  possible values of , since  is not
bounded as a function of ; Lemma~\ref{lemma:stepc} shows how to get around this issue. Both of these probabilistic results use
Theorem~\ref{thm:chbound-negcorrel}.



\begin{lemma}
\label{lemma:small}
After Step 2 of  allocation of big items by bipartite dependent rounding, we have the probability for all unmatched person to have a total utility of at least
 from the unmatched items is at least .
\end{lemma}


\begin{proof}
Consider a person  who is unsatisfied by the matching. Define . Then according to LP (\ref{eqn:lp}) solution



In step (a) of {\bf Allocation of Small Items}, all items  with 
 at least  are discarded; recall that . Since the total sum of  can be at most  (the number of persons), there can be at most  items with  at least . Therefore, for the remaining items, we have . Each person is connected only to small items in the flow graph. After removing the items with  at least , the remaining utility in the flow graph for person  is  at least
 
 Now consider random variables  for each of these unmatched items:


Since  and , the
 are random variables bounded in . Person  is unmatched by the matching
with probability . Each such person   gets
a fractional utility of  from the small (with respect to the person) item  in the flow graph, if item  is
not matched by the matching. The latter happens with probability .

Define . Then  is the total fractional utility after step (b). It
follows from (\ref{eq:1}) that


Thus, since , we have for sufficiently large  that


That the 's are negatively correlated follows from Theorem \ref{theorem:neg1}. Therefore, applying Theorem~\ref{thm:chbound-negcorrel}(i)
with ,

i.e.,
 

Hence,
 

Therefore the net fractional utility that remains for each person  in the flow graph after scaling is at least , with probability at least .
\end{proof}

\begin{lemma}
\label{lemma:item}
Fix any item  that is unmatched after Step 2. After the matching and the scaling (step (b)),  has a total fractional incident edge-weight
from the unmatched persons to be at most ,  with probability at least .
\end{lemma}

\begin{proof}
Note that for any person  for which  is small for , ; hence,
. Define a random variable  for each person  as:


Let . Then  is the total weight of all the edges incident on item  in the flow graph
after scaling and removal of all matched persons. We have .
The fact that the variables  are negatively correlated follows from Corollary \ref{cor:neg2}. Thus, applying
Theorem~\ref{thm:chbound-negcorrel}(ii) with  and  along with (\ref{eqn:Phi-bound}), we obtain

This completes the proof.
\end{proof}

Recall the third step, step (c), of {\bf Allocating Small Items}. Any job in the remaining flow graph with total weight of
incident edges more than  is discarded in this step. We now calculate the utility that remains for
each person in the flow graph after step (c).

\begin{lemma}
\label{lemma:stepc}
After removing all the items that have total degree more than  in the flow graph, that is after step (c)
of {\bf Allocating Small Items}, the probability that all unmatched persons have remaining utility in the flow graph at least  is at least .
\end{lemma}
\begin{proof}
Fix a person  and consider the utility that  obtains from the fractional assignments in the flow graph before step (c). It is at least  from Lemma \ref{lemma:small}.
Define a random variable for each item that  claims with nonzero value in the flow graph at step (b):



We have  from Lemma \ref{lemma:item}.
Therefore, the expected utility for  from all the items in the flow graph that have total incident weight
more than    is at most . By Markov's
inequality, the probability that the utility for  from the discarded items is more than , is at most . Applying
the union bound, the probability of the utility from the discarded items being more than  for some person,
is at most . The initial utility before step (c) was at least  with probability .
Thus after step (c), the remaining utility is at least  with probability at least .
\end{proof}

The next and the final step (d) of allocations is to run \cite{dani:05} on a {\em scaled-down} flow graph. 
The weight on the remaining edges is scaled down by a factor of  and hence
for every item node that has not been matched after step (c), the total edge-weight incident on it is at most . Hence after scaling down the utility of any person  in the flow graph is
, where  denote the scaled down weight on the edge . Also, note that the maximum utility of any item in the flow graph is at most . Hence, by running the algorithm of \cite{dani:05}, which is a simpler version of Theorem~\ref{thm:matching}, we get the following lemma.







\begin{lemma}
\label{lemma:unsat}
For all persons unmatched by the matching, the total utility received is at least  after step (d) with probability at least .
\end{lemma}
 \begin{proof}
 Let  denote the fractional weight on the scaled down flow graph. Then for every item  in the flow graph, . And for every person  considering the items in the flow graph,  with probability at least . We can now employ the rounding algorithm of \cite{dani:05} which is a simplification of Theorem \ref{thm:matching} without any capacity constraint. We get an integer solution where each person receives a utility of at least , and every item is assigned to at most one person. Since , we get the desired result.
\end{proof}
~\\
{\it Theorem~\ref{thm:santa1}
Given any feasible solution to the configuration LP, it can be rounded to a feasible integer solution such that every person gets  at least  fraction of the optimal utility with probability at least , in polynomial time.
}
\begin{proof}
Any person that is matched by step  of the algorithm {\bf Allocating Big Items} receives a utility of . From Lemma~\ref{lemma:unsat}, each person unmatched by the matching receives a utility of  with probability at least . Noting that , we therefore, get the theorem.
\end{proof}

Thus, our approximation ratio is .
This provides an upper bound of  on the integrality gap of the configuration LP for max-min fair allocation, nearly
matching the  lower bound of  due to \cite{bansal:stoc06}. 


\section{Designing Overlay Multicast Networks For Streaming}
\label{section:spaa}
The work of \cite{DBLP:conf/spaa/AndreevMMS03} studies approximation algorithms
for designing a multicast overlay network. We first describe the problem and state the results in \cite{DBLP:conf/spaa/AndreevMMS03} (Lemma \ref{lem:spaa1} and Lemma \ref{lem:spaa2}). Next, we show our main improvement in Lemma \ref{lem:color}.

\subsection{Background} The background text here is largely borrowed from \cite{DBLP:conf/spaa/AndreevMMS03}. An overlay network can be represented as a tripartite digraph . The nodes  are partitioned into sets of entry points called sources (), reflectors (), and edge-servers or sinks (). There are multiple commodities or streams, that must be routed from sources, via reflectors, to the sinks that are designated to serve that stream to end-users. Without loss of generality, we can assume that each source holds a single stream. 
There is a cost associated with usage of every link and reflector. 
 There are capacity constraints, especially on the reflectors,
that dictate the maximum total bandwidth (in bits/sec) that the reflector is allowed to send. 
To ensure reliability, multiple copies of each stream may be sent to the designated edge-servers.
\begin{table*}

\caption{Integer Program for Overlay Multicast Network Design}
\label{table:spaa}
\end{table*}

All these requirements can be captured by an integer program. Let us use indicator variable  for building reflector ,  for delivery of -th stream to the -th reflector and   for delivering -th stream to the -th sink through the -th reflector.  denotes the fanout constraint for each reflector . Let  denote the failure probability on any edge (source-reflector or reflector-sink). We transform the probabilities into weights: . Therefore,  is the negative log of the probability of a commodity   failing to reach sink  via reflector . On the other hand, if  is the minimum required success probability for commodity  to reach sink , we instead use . Thus  denotes the negative log of maximum allowed failure.  is the cost for opening the reflector  and  is the cost for using the link  to send commodity . Thus we have the IP (see Table \ref{table:spaa}).

Constraints (\ref{eqn:cons1}) and (\ref{eqn:cons2}) are natural consistency
requirements; constraint (\ref{eqn:fanout}) encodes the fanout restriction.
Constraint (\ref{eqn:weight}), the \emph{weight} constraint, ensures
quality and reliability. Constraint (\ref{eqn:integrality}) is the standard
integrality-constraint that will be relaxed to construct the LP relaxation.

There is an important  stability requirement that is referred as \emph{color constraint} in \cite{DBLP:conf/spaa/AndreevMMS03}. Reflectors are grouped into  color classes, . We want each group of reflectors to deliver not more than one copy of a stream into a sink. This constraint translates to


Each group of reflectors can be thought to belong to the same ISP. Thus we want to make sure that a client is served only with one -- the best -- stream possible from a certain ISP. This diversifies the stream distribution over different ISPs and provides stability. If an ISP goes down, still most of the sinks will be served. We refer the LP-relaxation of  integer program
(Table \ref{table:spaa}) with the color constraint (\ref{eqn:color})
as \textbf{LP-Color}.

All of the above is from \cite{DBLP:conf/spaa/AndreevMMS03}.
The work of \cite{DBLP:conf/spaa/AndreevMMS03} uses a two-step rounding procedure and obtains the following guarantee.

First stage rounding: Rounds  and  for all  and  to decide which reflector should be open and which streams should be sent to a reflector. The results here can be summarized in the following
lemma:

\begin{lemma}
\textbf{(\cite{DBLP:conf/spaa/AndreevMMS03})}
\label{lem:spaa1}
The first-stage rounding algorithm incurs a cost at most a factor of  higher than the optimum cost, and with high probability violates the weight constraints by at most a factor of  and the fanout constraints by at most a factor of . Color constraints are all satisfied.
\end{lemma}




Second stage rounding: Rounds 's using the open reflectors and streams that are sent to different reflectors in the first stage. The results in this stage can be summarized as follows:

\begin{lemma}
\textbf{(\cite{DBLP:conf/spaa/AndreevMMS03})}
\label{lem:spaa2}
The second-stage rounding incurs a cost at most a factor of  higher than the optimum cost and violates each of fanout, color and weight constraint by at most a factor of .
\end{lemma}

\subsection{Main Contribution}

Our main contribution is an improvement of the second-stage rounding
through the use of repeated \textbf{RandMove} and by judicious choices of
constraints to drop. Let us call the linear program that remains just at the end of first stage \textbf{LP-Color2}:



We show:

\begin{lemma}
\label{lem:color}
\textbf{LP-Color2} can be efficiently rounded such that the cost and weight constraints are satisfied exactly, fanout constraints are violated at most by additive , and the color constraints are violated at most by additive .
\end{lemma}

The proof is very similar to Theorem~\ref{thm:matching}. Note that, here instead of having capacity constraints, we have fanout constraints. Weight constraints correspond to load constraints in Theorem~\ref{thm:matching}, but now they provide lower bounds. Moreover, the color constraints can be thought of as additional capacity constraints imposed on a set of reflectors. This constitutes the main change from Theorem~\ref{thm:matching}, and we need new conditions to drop color constraints . The color constraints being all disjoint help us in the rounding.

\begin{proof}
Let  denote the fraction of stream generated from source  reaching destination
 routed through reflector  after the first stage of rounding. Initialize . The algorithm
consists of several iterations. the random value at the end of iteration  is denoted by . Each iteration  conducts a randomized update using {\bf RandMove} on the polytope of a linear system constructed from a subset of constraints of {\bf LP-Color2}. Therefore by induction on , we will have for all  that . Thus the cost constraint is maintained exactly on expectation. The entire procedure can be derandomized by the method of conditional probabilities, yielding the required bounds on the cost.

Let  and  denote the set of reflectors and (source, destination) pairs respectively.
Suppose we are at the beginning of some iteration  of the overall algorithm and currently looking
at the values . We will maintain two invariants:
\begin{description}
\item[(I1'')] Once a variable  gets assigned to  or , it is never changed;
\item[(I2'')] Once a constraint is dropped in some iteration, it is never reinstated.
\end{description}
Iteration  of rounding consists of three main steps:

\begin{enumerate}
\item Since we aim to maintain ({\bf I1''}), let us remove all ; i.e.,
we project  to those coordinates  for which , to obtain the
current vector  of floating (yet to be rounded) variables; let  denote
the current linear system that represents {\bf LP-Color2}. In particular, the fanout constraint
for a reflector in  is its residual fanout ; i.e.,  minus the
number of streams that are routed through it.
\item Let  denote the number of floating variables, i.e., . We now drop the following constraint:
    \begin{description}
    \item[(D1'')] Drop fanout constraint for degree  reflector denoted , i.e, reflectors with only one floating variable associated with it. For any degree  reflectors denoted , if
        it has a tight fanout of  drop its fanout constraint.
    \item[(D2'')] Drop color constraint for a group of reflectors , if they have at most four floating variables associated with them.
    \end{description}
    \end{enumerate}
Let  denote the polytope defined by this reduced system of constraints.
A key claim is that  is not a vertex of  and thus we can apply
{\bf RandMove} and make progress either by rounding a new variable or by dropping
a new constraint. We count the number of variables  and the number of tight constraints
  separately. We have  where
  is the number of tight color constraints for the stream generated at source 
 and to be delivered to the  destination .  We further have , and that
. Thus by averaging,  A moment's reflection shows that the
 system can become underdetermined only if there is no color constraint associated with a stream , each reflector  has two floating variables associated with it with total contribution  towards fanout and each stream  is routed fractionally through two reflectors. But in this situation all the fanout constraints are dropped violating fanout at most by an additive one and making the system underdetermined once again.
The color constraints are dropped only when there are less than four
floating variables associated with that group  of reflectors; hence, the color constraints can get violated at most by an additive . The fanout constraint is dropped only for singleton reflectors or degree-2 reflectors with fanout equaling . Hence the fanout is violated only by an additive
excess of . The weight constraint is never dropped, and
is hence maintained exactly.
\end{proof}
