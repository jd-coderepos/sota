
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 



\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[font=small]{caption}

\newenvironment{centerentry}[2][]
  {\renewcommand{\tabularxcolumn}[1]{m{##1}}
   \noindent
   \tabularx{\linewidth}{ @{} m{\imagecolwidth} X @{} }
     \includegraphics[width=\linewidth,#1]{#2} &
  }{\endtabularx }
\newlength{\imagecolwidth}
\setlength{\imagecolwidth}{2em}

\definecolor{newlightblue}{RGB}{0,75,255}
\usepackage[pagebackref,breaklinks,colorlinks, urlcolor={newlightblue}, citecolor={newlightblue}]{hyperref} 

\makeatletter
\def\adl@drawiv#1#2#3{\hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}#2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{\noalign{\vskip\aboverulesep
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip\belowrulesep}}
\makeatother

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{4785} \def\confName{CVPR}
\def\confYear{2023}


\newcommand{\andrew}[1]{\textcolor{blue}{\textbf{Andrew:} {\slshape #1}}}
\newcommand{\ziyang}[1]{\textcolor{green}{\textbf{Ziyang:} {\slshape #1}}}
\newcommand{\chao}[1]{\textcolor{red}{\textbf{Chao:} {\slshape #1}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bx}[0]{{\mathbf x}}


\newcommand{\mypar}[1]{\vspace{-3mm}\paragraph{#1}}
\def\upvspacefig{\vspace{-0.0mm}}
\setlength{\belowcaptionskip}{-0.0em}
\setlength{\abovecaptionskip}{0.5em} 
\newcommand{\supparxiv}[2]{#2}



\begin{document}

\title{Self-Supervised Video Forensics by Audio-Visual Anomaly Detection}


\author{Chao Feng \qquad Ziyang Chen \qquad Andrew Owens \vspace{3.5mm}\\ 
University of Michigan~~~~\\
}
\maketitle

\begin{abstract}
Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: \href{https://cfeng16.github.io/audio-visual-forensics/}{https://cfeng16.github.io/audio-visual-forensics}.















\end{abstract} 

\section{Introduction}
\label{sec:intro}


















Supervised learning underlies today's most successful methods for image and video forensics. However, the difficulty of collecting large, labeled datasets that fully capture all of the possible manipulations that one might encounter in the wild places significant limitations on this approach. A longstanding goal of the forensics community has been to design methods that, instead, learn to detect manipulations using cues discovered by analyzing large amounts of {\em real} data through self-supervision~\cite{huh2018fighting,cozzolino2019noiseprint}.


\looseness=-1
We propose a method that identifies manipulated video through {\em anomaly detection}. Our model learns how audio and visual data temporally co-occur by training on large amounts of real, unlabeled video. At test time, we can then flag videos that our model assigns low probability, such as those whose video and audio streams are inconsistent.



\begin{figure}[t!]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{image/teaser.pdf}
    
      \vspace{-0.5mm}
            \caption{{\bf Audio-visual anomaly detection.}  We identify fake videos by finding anomalies in their audio-visual features, using generative models trained entirely on {\em real} videos. In one variation of our model (shown here), we use the {\em time delay} between the two modalities as our feature set, i.e., temporal misalignment between each video frame and the audio stream. We learn the distribution of these sequences, then flag sequences with low probability.
      } 
      \vspace{-2mm}
    \label{teaser}
\end{figure} 
 
One might expect that this problem could be posed as simply {detecting} {out-of-sync} examples, such as by finding  cases in which a speaker's mouth does not open precisely at the onset of a spoken word.
Unfortunately, videos in the wild are often ``naturally'' misaligned due to errors in encoding or recording, such as by having a single, consistent shift by a few frames~\cite{chung2016out,afouras2021selfsupervised}.

Instead, we pose the problem as detecting anomalies in what we call {\em synchronization features}: audio-visual features that are designed to convey the temporal alignment between vision and sound. We evaluate several feature sets, each extracted from a model that has been trained to temporally align audio and visual streams of a video~\cite{chen2021audio,owens2018audio,chung2016out}. In Figure~\ref{teaser}, we show one such feature set:  the amount of time that each video frame appears to be temporally offset from its corresponding sound. To detect anomalies, we fit an autoregressive generative model~\cite{vaswani2017attention,radford2019language} to sequences of synchronization features extracted from real videos, and identify low probability examples.





A key advantage of our formulation is that it does not require any manipulated examples for training. It also does not require the speakers in the test set to already be present in the training set. This is in contrast to previous audio-visual forensics approaches, which either require finetuning on datasets of manipulated video~\cite{haliassos2022leveraging}, or which are based on verifying that the speaker's voice matches previously observed examples~\cite{cozzolino2022audio}.

We evaluate our model on videos that have manipulated a person's speech and face, using datasets of lip-synced and audio-driven face reenactment videos, some of which are also manipulated by faceswap techniques. Our model obtains strong performance on the FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} and KoDF~\cite{kwon2021kodf} datasets, despite the fact that it is trained entirely on real examples obtained from other video datasets. Our model generalizes to other spoken languages without retraining and obtains robustness to a variety of postprocessing operations, such as compression and blurring. We show through our experiments that: \begin{itemize}[leftmargin=*,topsep=1pt, noitemsep]
   \item Video forensics can be posed as an audio-visual anomaly detection problem.
   \item Synchronization features convey information about video manipulations.
   \item Our model can successfully detect fake videos, while training solely on real videos.
   \item Our model generalizes to many types of image postprocessing operations and to speech videos from spoken languages not observed during training.
\end{itemize}
































 
\section{Related Work}
\label{sec:related}


\paragraph{Audio-visual forensics.}
In early work, Malik and Farid~\cite{malik2010audio} detected audio manipulations by finding inconsistencies in reverberation. Recent work has focused on detecting manipulated speech videos using audio-visual inconsistencies.
Several approaches have directly trained audio-visual networks through supervised learning, using labels indicating whether a video is manipulated~\cite{mittal2020emotions, chugh2020not}. A variety of methods have recently used audio-visual self-supervision for pretraining supervised models, which are finetuned with ``real or fake'' labels. Zeng \etal~\cite{zeng2021contrastive} used local and global contrastive learning methods to learn video features. Haliassos \etal~\cite{haliassos2022leveraging} jointly solved a negative-less contrastive learning problem~\cite{grill2020bootstrap} and a forensics task. Other work~\cite{haliassos2021lips} pretrains using lip-reading data. Zhou and Lim~\cite{zhou2021joint} used audio-visual synchronization signal implicitly, and proposed a dataset for audio-visual deepfake detection\footnote{Their dataset is not publicly available.}. In contrast to these methods, our approach is trained entirely using {\em real} data and does not require any labels or examples of fake videos. Other work has used speaker verification~\cite{cozzolino2022audio} and phoneme-viseme mismatches~\cite{agarwal2020detecting} to detect fake videos and it also detects face swap manipulations, which preserve the synchronization between modalities. In contrast, our approach detects misaligned images and sounds and does not require that examples from the speaker be present in the training set.




\mypar{Audio-visual representation learning.} 
A variety of methods have been proposed to learn audio-visual representation from videos via self-supervision. Researchers have leveraged the natural semantic correspondence in the videos between frames and audio tracks~\cite{asano2020labelling,zeng2021contrastive,morgado2021audio} to learn multi-modal features and applied them to downstream tasks such as sound localization~\cite{hu2022mix,arandjelovic2018objects,mo2022_ezvsl}. Other work studies temporal synchronization between audio and visual signals to learn audio-visual features~\cite{chung2016out,owens2018audio,korbar2018cooperative}, which can be used for active speaker detection~\cite{tao2021someone,alcazar2021maas,kopuklu2021design}, source separation~\cite{majumder2021move2hear,gao2021visualvoice,zhou2020sep}, lip reading~\cite{afouras2020asr,martinez2020lipreading,ma2021towards} and so on. Our method uses the off-the-shelf audio-visual synchronization model to perform anomaly detection. 


\mypar{Visual face forensics.} A major focus of the forensics field has been on the problem of detecting manipulated videos of human faces. In recent years, a variety of visual face manipulation datasets are proposed, such as FaceForensics++~\cite{rossler2019faceforensics++}, VideoForensicsHQ~\cite{fox2021videoforensicshq} and \textit{FFIW}~\cite{zhou2021face}. Meanwhile, many methods are proposed to detect synthetic contents to fight against their potential threats. Some work~\cite{bianchi2012image, li2018exposing, guo2022eyes} has proposed to use hand-crafted features to capture inconsistent visual or JPEG artifacts. Other work has proposed to use deep learning to inspect specific artifacts, such as blending~\cite{li2020face}, frequency domain~\cite{qian2020thinking, durall2020watch, frank2020leveraging}, or texture~\cite{liu2020global}. A variety of methods have studied the generalization between detection classifiers~\cite{wang2019cnn,chai2020makes}.
 




















\mypar{Anomaly detection.} 
A variety of methods have learned a distribution, then flagged unusual examples. These examples are often considered anomalies~\cite{zong2018deep, schlegl2017unsupervised, zenati2018adversarially,liu2019generative} or outliers~\cite{sabokrou2018adversarially, pidhorskyi2018generative}, and are used as part of open-set recognition~\cite{kong2021opengan, zhang2020hybrid}. 
We formulate video forensics as the task of detecting anomalies, using a feature set that conveys information that would be hard for a forger to create. There have been a variety of methods proposed for learning this distribution, such as  GAN discriminators~\cite{schlegl2017unsupervised,sabokrou2018adversarially,pidhorskyi2018generative,zenati2018adversarially,liu2019generative,kong2021opengan,zong2018deep,NIPS2014_5ca3e9b1}, flow-based models~\cite{zhang2020hybrid}, and autoregressive models~\cite{song2017pixeldefend}. Similarly, our model is based on an autoregressive generative model~\cite{radford2019language,bengio2000neural}, since they have achieved strong performance at modeling complex distributions. Other work addresses goals similar to anomaly detection by creating methods that model uncertainty~\cite{liang2017enhancing} or that leverage outlier exposure~\cite{dhamija2018reducing,ruff2019deep,hendrycks2018deep}. Some work~\cite{huh2018fighting,cozzolino2016single,kalyan2018satellite,d2017autoencoder,bondi2017tampering,khalid2020oc,perez2019deep} has used special-purpose anomaly detection methods for image/video forensics. Other work~\cite{fei2022learning,zhao2021learning,wu2019mantra,hu2021dynamic} uses supervised learning to find anomalous patterns. In contrast, our method builds the likelihood function entirely on real videos and views low-probability examples as fake. 

 
 
\section{Method}
\begin{figure*}[t!]
    \centering
    \upvspacefig
    \includegraphics[width=\linewidth]{image/overview.pdf}
    \begin{flushleft}
        \vspace{-3mm}
        \hspace{6mm} (a) Synchronization feature extraction \hspace{45mm} (b) Anomaly detection
         \vspace{-3mm}
    \end{flushleft}    
    \caption{\textbf{Audio-visual anomaly detection model.}
    (a) We extract a feature set from an audio-visual synchronization network: the number of frames of delay between video frames and sound, the distribution over delays at each frame, and feature activations from the audio and visual subnetworks. (b) We train an autoregressive Transformer model to assign probabilities to synchronization features. At test time, we flag low probability examples.}
    \label{model_overview}
\end{figure*} \label{sec:method}



We formulate the problem of detecting manipulated videos as an anomaly detection problem. We model the distribution of audio-visual examples, then flag examples that have low probability. If we were to fit a model on the raw data, then this would be a very challenging learning task. Instead, we learn the distribution over a feature set that conveys subtle properties that are unlikely to be accurately captured in manipulated video. 





\subsection{Estimating audio-visual synchronization}\label{sync_estimate}





We obtain our feautres from a network that performs audio-visual synchronization~\cite{chung2019perfect,chung2016out,owens2018audio,chen2021audio}. We use the model of Chen \etal~\cite{chen2021audio}. We learn a function  that indicates how likely video clip  temporally co-occurs with audio clip . We estimate the synchronization score  of all audio-visual pairs in a temporal window: 
where  is maximum time difference between two streams, and  is calculated using late fusion by a visual encoder , audio encoder  and the fusion module . We also interpret  as synchronization probability. We maximize the synchronization of true audio-visual pairs  using the InfoNCE loss~\cite{oord2018representation}: 

for a video of length . We provide details about the architecture and training procedure in \supparxiv{the supplement}{\cref{appendix:imple}}.

After training, we can use the learned model to obtain a feature set for anomaly detection. For example, we can use the rows of , which provide a probability distribution over possible alignments between video clips and audio clips.












\subsection{Audio-visual anomaly detection}\label{av_anomaly}

We use our learned model to obtain a feature set for anomaly detection.  We learn the distribution of these features on a training set of real videos. Then at test time, videos with low probability will be flagged as potential fakes. We now explore two key design decisions that go into such a system: what feature set to use, and how the distribution is learned. 

Given features for each frame, we learn a distribution . We generally use autoregressive models to learn this distribution, given their success in modeling complex distributions~\cite{brown2020language,yu2022scaling}. These models take the form: 

We train a model  that estimates the features of the next frame, given all of the features from the previous frames. Maximizing the log probability can be posed as minimizing a per-frame loss, :


 We now describe different formulations of the loss function , the feature representation . In each case, we implement  as a Transformer~\cite{vaswani2017attention}.




















\mypar{Discrete time delays.} 
We first consider a simple model that uses discrete {\em time delay} as our features, following the success of autoregressive models for fitting discrete data~\cite{van2017neural, razavi2019generating, esser2021taming}. Inspired by work on time delay estimation~\cite{knapp1976generalized,chen2022sound}, for every video frame, we estimate how far ahead (or behind) it appears to be from the audio signal. For each frame, we set  to be the time delay with the highest probability, \ie, . We then set  to be the cross entropy loss between the ground truth and predicted time delay. This amounts to solving a categorization problem with  possible labels for each frame.












\mypar{Distributions over delays.}\label{kl-model} 
While discrete time delays are straightforward to represent in the model, they discard important information, such as when there is ambiguity in the delay.
We, therefore, propose a model that directly predicts the entries of the time delay distribution. 
We set the features  to be the rows of , i.e. the probability of each possible delay, and use cross entropy loss:


We constrain the predictions made by our model  to sum to 1 by applying a softmax.






\mypar{Audio-visual network activations.}
The feature activations within the audio-visual synchronization network convey information about the time delay. We, therefore, ask whether these activations can be directly used as features for anomaly detection. We concatenate the representations of the visual and audio subnetworks,  and . To provide a straightforward comparison with the time delay distribution model, we reduce the dimensionality of the features by projecting them onto the top  principal components, following other work in autoregressive models of features~\cite{ramesh2022hierarchical}. We use squared distance as our loss: .










 
\section{Results}
We evaluate the different variations of our model on a variety of video forensics tasks. 

\subsection{Implementation details}
\label{implementation}
\paragraph{Synchronization model.} 

Following Chen \etal~\cite{chen2021audio}, we use ResNet-18 2D+3D\cite{he2016deep, hara2018can} as the visual encoder, using 5 frames~(25 fps) as input. The audio encoder uses VGG-M~\cite{chatfield2014return} and extracts features from 0.2s audio clips~(16kHz).
We fuse audio and visual data using a Transformer that has 3 standard Transformer encoder blocks~\cite{vaswani2017attention}, 4 attention heads, and 512 channels. We train using the cropped faces provided by each dataset. Please see \supparxiv{the supplement}{\cref{appendix:imple}} for details.


\mypar{Anomaly detection model.} 
We use a decoder-only autoregressive Transformer~\cite{liu2018generating,esser2021taming,radford2019language} to learn the distribution over synchronization features. We use 2 decoder blocks~\cite{vaswani2017attention}, each with 16 attention heads and 256 channels. 
For models that use time delay or continuous distribution, we set the maximum delay to be  frames, resulting in the distribution  for each video frame. We use sequences of length  from 2.0s video.







\mypar{Hyperparameters.} 
We resample videos to 25 fps and audios to 16kHz. We represent audio segments as mel spectrograms of size  by short-time Fourier transform~(STFT) with 80 mel filter banks, a hop length of 160, and a window size of 320. Please see more details in \supparxiv{the supp}{\cref{appendix:imple}}.


\begin{table*}[t!]
\renewcommand{\arraystretch}{1.1}
\centering
\upvspacefig
\resizebox{\textwidth}{!}{
\begin{tabular}{clclccccccccccc:cc}
\toprule

&  &   & \multirow{4}{*}{\begin{tabular}[c]{c}Pretrained\\ dataset\end{tabular}} & \multicolumn{13}{c}{Category}
\\
\cmidrule(lr){5-17} 
    &        Method       &        Modality                   &                                & \multicolumn{2}{c}{RVFA} & & \multicolumn{2}{c}{FVRA-WL} & \multicolumn{2}{c}{FVFA-FS} & \multicolumn{2}{c}{FVFA-GAN} & \multicolumn{2}{c}{FVFA-WL}& \multicolumn{2}{c}{AVG-FV}
    \\
\cmidrule(lr){5-6} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
&                                            &                             &                                                                               & AP         & AUC   &       & AP            & AUC            & AP            & AUC            & AP             & AUC            & AP            & AUC         & AP            & AUC       \\
\midrule 
\parbox[t]{4mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Supervised}}} 
 & Xception~\cite{rossler2019faceforensics++}                         &                         &            ImageNet~\cite{deng2009imagenet}                                                           &   --         &     --     &    &   88.2&  88.3     &  92.3     &     93.5     & 67.6       &    68.5    &91.0     & 91.0  &   84.8  &  85.3       \\
& LipForensics~\cite{haliassos2021lips}                         &                         &            LRW~\cite{Chung16}                                                               &   --         &     --  &    &    \textbf{97.8}       &       \textbf{97.7}         &       99.9      &      99.9         &        61.5      &     68.1      &        98.6   &     98.7  & 89.4 & 91.1  \\
& AD DFD~\cite{zhou2021joint}              &                                                &     Kinetics~\cite{kay2017kinetics}                                                                          &       \textbf{74.9}       &       \textbf{73.3} &   &    97.0         &     97.4          &      99.6       &     99.7         &         58.4   &   55.4       &        \textbf{100.}    &      \textbf{100.}    & 88.8 & 88.1     \\
& FTCN~\cite{zheng2021exploring}                                   &                              &          --                                                                     &    --        &   --       &   &   96.2    &      97.4       &    \textbf{100.}          &      \textbf{100.}        &      77.4       &       78.3    &    95.6       &    96.5    & 92.3 &   93.1    \\
& RealForensics~\cite{haliassos2022leveraging}                         &                              & LRW~\cite{Chung16}                                                                          &   --        &  --   &  &   88.8      &     93.0          &      99.3       &         99.1    &           \textbf{99.8}     &       \textbf{99.8}      &   93.4          &     96.7  & \textbf{95.3} & \textbf{97.1}       \\
\cmidrule(lr){1-17}
\parbox[t]{4mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Unsupervised}}} 
& AVBYOL~\cite{haliassos2022leveraging,grill2020bootstrap}               &                                                      & LRW~\cite{Chung16}                                                                         &    50.0        &      50.0 &  & 73.4  &       61.3      &     88.7     &  80.8 &  60.2     &      33.8            &   73.2         & 61.0 & 73.9  & 59.2 \\
& VQ-GAN ~\cite{esser2021taming}                                 &                         & LRS2~\cite{Afouras18c}                                                                        & --       &     --  &   &  50.3     &   49.3       & 57.5  &   53.0      &    49.6      &     48.0    &     62.4        &  56.9  &  55.0 & 51.8  \\
\cdashlinelr{2-17}
& Ours                &                                                   & LRS2~\cite{Afouras18c}                                                                         &      62.4    &  71.6    &    &     \textbf{93.6}         &     \textbf{93.7}    &  \textbf{95.3} &    \textbf{95.8}     &       \textbf{94.1}       &       \textbf{94.3}       &   \textbf{93.8}           & \textbf{94.1}  &\textbf{94.2} & \textbf{94.5}  \\
& Ours                &                                                   & LRS3~\cite{afouras2018lrs3}        &      \textbf{70.7}      &     \textbf{80.5} &     &     91.1         &    93.0         &   91.0   &  92.3        &   91.6           &      92.7       &   91.4           & 93.1   & 91.3 & 92.8\\ 
\bottomrule
\end{tabular}
}
\caption{\textbf{Manipulation detection on FakeAVCeleb.} We report AP scores  and AUC scores , following the evaluation protocol of Haliassos et al.~\cite{haliassos2021lips}, in which supervised methods are evaluated on unseen manipulation types (unsupervised methods are not trained with labels and fake examples). We report results with combinations of real/fake video/audio, using different manipulation algorithms. We report the average performance over four fake video~(FV) categories in AVG-FV. We retrained all supervised models on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}.}
\label{comparison_with_others}
\end{table*}
 
\subsection{Dataset}
We train our model on real, unlabeled speech video, and evaluate it on forensics datasets.

\mypar{Training datasets.} We train our models on Lip Reading Sentences 2~(LRS2, 97k videos)~\cite{Afouras18c} and Lip Reading Sentences 3~(LRS3, 120k videos)~\cite{afouras2018lrs3}. The videos in each contain tightly cropped face tracks. We divide each dataset into 3 splits and train the audio-visual synchronization model and the autoregressive model on different splits.




\mypar{Evaluation datasets.} 
We evaluate on two video forensics datasets, spanning several different types of manipulations that change the speech and face of a human speaker. 
\textbf{FakeAVCeleb}~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}, which is derived from VoxCeleb2~\cite{chung2018voxceleb2}. This dataset contains 500 real videos and 19,500 fake videos manipulated by Faceswap~\cite{korshunova2017fast}, FSGAN~\cite{nirkin2019fsgan}, and Wav2Lip~\cite{prajwal2020lip}, and fake sounds that are generated by SV2TTS~\cite{jia2018transfer}. The examples in the dataset contain different combinations of these manipulations. We use the dataset's provided face crops. We sample 2400 videos (400 real videos and 2000 fake videos) as train/val splits and 600 videos (100 real videos and 500 fake videos) as test split. We note that our method does not use any videos from train/val splits, since it is trained from another dataset (LRS2 or LRS3). Second, we evaluate on \textbf{KoDF}~\cite{kwon2021kodf}, a large-scale Korean-language deepfake detection dataset. It contains 62,166 real videos and 175,776 fake videos, where fake videos are generated by 6 synthesized methods: FaceSwap~\cite{faceswap}, FSGAN~\cite{nirkin2019fsgan}, DeepFaceLab~\cite{perov2020deepfacelab}, FOMM~\cite{siarohin2019first}, ATFHP~\cite{yi2020audio} and Wav2Lip~\cite{prajwal2020lip}. We extract faces by using face detection~\cite{zhang2017s3fd} and alignment~\cite{bulat2017far}. 









\subsection{Evaluation methods}
Following common practice~\cite{wang2019cnn, rossler2019faceforensics++, li2020face, chai2020makes, qian2020thinking, haliassos2021lips, zheng2021exploring, haliassos2022leveraging, kwon2021kodf, NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}, we evaluate using average precision~(AP) and AUC. These evaluation metrics are widely used for cross-dataset generalization and unsupervised models since they avoid the need to threshold the predictions. We compare our approach to both supervised and self-supervised methods at the video level. Unless otherwise stated, we use time delay distributions as our feature set (\cref{kl-model}). 

\mypar{Supervised methods.}
For supervised methods, we retrain several state-of-the-art detectors on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}: 1)~{\bf Xception}~\cite{rossler2019faceforensics++}: a popular baseline for forensics detection; 2)~{\bf LipForensics}~\cite{haliassos2021lips}: a detector is built on high-level semantic embeddings of mouth and targets irregularities in mouth movements; 3)~{\bf AD DFD}~\cite{zhou2021joint}: a multimodal detector with audio and video branches, utilizes audio-visual synchronization signal implicitly for detection; 4)~{\bf FTCN}~\cite{zheng2021exploring}: a video forensics detector leverages temporal incoherence to boost generalization capability; 5)~{\bf RealForensics}~\cite{haliassos2022leveraging}: it first pretrains the network by audio-visual BYOL~\cite{grill2020bootstrap} framework and then finetunes the pretrained model on forensics datasets by multi-task learning to obtain robust and general face forgery detection.

\mypar{Self-supervised methods.}
Since we are not aware of any existing methods that consider self-supervised speech video forensics, we adapt two existing methods to the task. First, we consider an audio-visual contrastive learning model, which we call  {\bf AVBYOL}, that learns to determine whether the visual and audio streams of a video do (or do not) match, an approach that has been used as a part of other audio-visual forensics models~\cite{haliassos2022leveraging,cozzolino2022audio}. We adapt the model of Haliassos \etal~\cite{haliassos2022leveraging}, which uses BYOL~\cite{grill2020bootstrap} to learn a joint audio-visual embedding for pretraining. Instead of pretraining, we directly use the model's audio-visual similarity score to flag fake examples.
Second, we use a generative model {\bf VQGAN}~\cite{esser2021taming}, trained on LRS2~\cite{Afouras18c}, for anomaly detection. VQGAN converts an image into a sequence of discrete codes, then uses an autoregressive Transformer to learn the distribution of codes. We use the code's log likelihood, averaged over each video frame, for anomaly detection.





\subsection{Evaluation}
In real-world scenarios, the deployed detectors are expected to recognize fake videos manipulated by unseen techniques. Thus, following the standard procedure used in \cite{haliassos2022leveraging, haliassos2021lips, zheng2021exploring, zhou2021joint}, we conduct the experiment to evaluate the cross-manipulation generalization ability of our model on the FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} which videos are manipulated in various ways. Since our approach and other self-supervised baselines learn from real, unmanipulated videos and perform zero-shot fake video detection, all the fake videos during evaluation are considered as manipulated by unseen methods.

We split FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} into five categories based on the manipulation methods and manipulated modalities: 
1)~{\bf RVFA}: real video with fake audio by SV2TTS~\cite{jia2018transfer}; 
2)~{\bf FVRA-WL}: real audio with fake video by Wav2Lip~\cite{prajwal2020lip}; 
3)~{\bf FVFA-WL}: fake video by Wav2Lip~\cite{prajwal2020lip}, and fake audio by SV2TTS~\cite{jia2018transfer};
4)~{\bf FVFA-FS}: fake video by Faceswap~\cite{korshunova2017fast} and Wav2Lip\cite{prajwal2020lip}, and fake audio by SV2TTS~\cite{jia2018transfer};
5)~{\bf FVFA-GAN}: fake video by FaceswapGAN~\cite{nirkin2019fsgan} and Wav2Lip\cite{prajwal2020lip}, and fake audio by SV2TTS~\cite{jia2018transfer}.
For supervised methods, we hold out the evaluated category and train the models on the remaining categories. Note that some approaches are only able to detect the manipulation on a certain modality, we do not report their performance on the categories with the manipulation only on the other modalities (since they can not differentiate real and fake videos).

We show our results in \cref{comparison_with_others}. Our method substantially outperforms both self-supervised methods AVBYOL~\cite{haliassos2022leveraging, grill2020bootstrap} and VQGAN~\cite{esser2021taming} on each category by a large margin. More importantly, our method works on par with or outperforms some supervised methods on certain categories, especially FVFA-GAN, even though our method does not use any labeled supervision or fake examples.  Moreover, our method has quite consistent performances and it can achieve AP over 90\% in the most of categories. While Xception~\cite{rossler2019faceforensics++}, LipForensics~\cite{haliassos2021lips}, AD DFD~\cite{zhou2021joint} and  FTCN~\cite{zheng2021exploring} work well on 75\% of the settings, there are settings where performance collapses to near-chance~(\eg, AD DFD~\cite{zhou2021joint} on FVFA-GAN). Interestingly, two self-supervised baselines struggle to detect fake videos, perhaps because both models do not necessarily capture the subtle information that would be needed to detect manipulations. In addition, 
VQGAN~\cite{esser2021taming} compresses the visual signal using a codebook, which might drop the artifact clues and harm the detection performance. Moreover, our model trained on LRS2~\cite{Afouras18c} works on par with the one trained on LRS3~\cite{afouras2018lrs3}, indicating that our method's performance is not tied to a single training set.









\begin{table}[t!]
\centering
\upvspacefig
\resizebox{1\columnwidth}{!}{
\begin{tabular}{llccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multirow{2}{*}{Modality} &\multicolumn{2}{c}{KoDF~\cite{kwon2021kodf}} \\
\cmidrule(lr){4-5}
\multicolumn{2}{c}{}           &             & AP          & AUC         \\
\midrule
\multirow{5}{*}{\shortstack[c]{Supervised \\ (transfer)}}
 & Xception~\cite{rossler2019faceforensics++}   &   &   76.9  &  77.7\\
                            & LipForensics~\cite{haliassos2021lips}   &   & 89.5       &    86.6       \\
                            & AD DFD~\cite{zhou2021joint}   &       &   79.6         &   82.1       \\
                            & FTCN~\cite{zheng2021exploring} &            &     66.8       &     68.1       \\
                            & RealForensics~\cite{haliassos2022leveraging}   &         &  \textbf{95.7}    &          \textbf{93.6}   \\
\midrule
\multirow{3}{*}{Unsupervised}                  
& AVBYOL~\cite{haliassos2022leveraging, grill2020bootstrap} &   & 74.9     & 78.9 \\

& VQ-GAN ~\cite{esser2021taming} &    &   46.8      &   45.5     \\

\cdashlinelr{2-5}
& Ours       &     &        \textbf{87.6}    &      \textbf{86.9 }        \\

\bottomrule
\end{tabular}
}
\caption{\textbf{Generalization to Korean speech.} AP scores () and AUC scores () are reported on KoDF dataset~\cite{kwon2021kodf}. Supervised methods are trained on FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. Ours is trained on LRS2~\cite{Afouras18c}. Best results are in \textbf{bold}.}
\label{kodf_result}
\end{table}
 \mypar{Cross-dataset generalization.} 
We also evaluate the generalization capability of our model by evaluating it on the KoDF dataset~\cite{kwon2021kodf}, following \cite{haliassos2022leveraging, haliassos2021lips, zheng2021exploring, zhou2021joint}. We focus on the audio-driven synthesis examples in the dataset, where videos are manipulated by ATFHP~\cite{yi2020audio} or Wav2Lip~\cite{prajwal2020lip}, and randomly selected 100 real and 100 fake videos. We train the supervised models on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} to evaluate their generalization ability. Many of these training videos share the same technique used in KoDF for synthesis~\cite{prajwal2020lip}. As the results are shown in \cref{kodf_result}, our approach obtains a comparable performance to many supervised methods. Although our system is trained on the English speech datasets, it still generalizes to KoDF~\cite{kwon2021kodf} dataset of Korean speech, perhaps because it learns low-level lip motion cues that are broadly useful. We provide more results in \supparxiv{the supplement}{\cref{appendix:crossdataset}}.






\begin{figure}[t!]
    \centering
    \upvspacefig
    \includegraphics[width=\linewidth]{image/heatmap.pdf}
    \caption{\textbf{Time delay distribution predictions for real vs. fake examples.} We visualize the time delay distributions from the synchronization model and predicted results generated by the autoregressive model for four random samples from different categories. Synchronization probabilities are in a range from 0~\includegraphics[width=0.10\linewidth, height=0.03\linewidth]{image/colorbar.png}~1. We show the predictions of the autoregressive model when feeding it ground truth observations of the previous timesteps. We show cumulative prediction error (indicating the probability of being fake) for each sample over time steps in the last row. } \label{heatmap}
\end{figure} 
\begin{figure*}[t!]
    \centering
    \upvspacefig
 \includegraphics[width=0.95\linewidth]{image/robustness.pdf}
    \caption{\textbf{Robustness to unseen perturbations.} AUC scores  of different detectors as a function of perturbation intensities. There are 6 intensity levels in total from~\cite{jiang2020deeperforensics}. ``Average'' represents the average over 7 perturbations under each intensity. ``Rival'' means we pick the worst performance across 7 perturbations under each intensity.}
    \label{robustness}
\end{figure*}



 
\mypar{Qualitative results.}
We visualize the ground truth and predicted time delay distributions generated by our autoregressive continuous time delay model (\cref{av_anomaly}) in \cref{heatmap}. We use the four main categories from FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. For each one, we display a heat map indicating the predicted time delay distribution, using a model that obtains the ground truth distributions of the previous frames as input.
We also plot the cumulative prediction loss (Eq.~\ref{eq:loss}) over time. From \cref{heatmap}, we can see that our autoregressive model accurately predicts the ground truth for real video, which results in a lower score. For fake videos, we can find clear differences between ground truth and predicted time delay distribution, leading to higher prediction loss. 





\subsection{Robustness to unseen perturbations}\label{subsec_robustness}

When the fake video is redistributed, it may undergo many types of postprocessing that result in corruption, making detection more difficult. Thus, it is important for forensics models to be robust to the types of postprocessing operations they may encounter in the wild. Following \cite{haliassos2021lips,zheng2021exploring,haliassos2022leveraging}, we use the set of visual perturbations proposed in \cite{jiang2020deeperforensics}: 1)~Color saturation change; 2)~Block-wise distortion; 3)~Color contrast change; 4)~Gaussian blur; 5)~Gaussian noise; 6)~JPEG compression; 7)~Video compression rate change. We set the intensity levels from 0 to 5 for each perturbation. 

We compare our model with four supervised methods XceptionNet~\cite{rossler2019faceforensics++}  FTCN~\cite{zhou2021joint}, AD DFD~\cite{zhou2021joint} and RealForensics~\cite{haliassos2022leveraging}. 
As shown in \cref{robustness}, our self-supervised model is overall more robust to unseen visual perturbations on average compared with these supervised methods, with the exception of RealForensics~\cite{haliassos2022leveraging}. This is also true when we consider ``worst case'' performance, by taking the minimum performance over all types of augmentation of a given intensity level. Interestingly, we obtain this performance even though our model is trained in a very different way from other works, suggesting that the feature set continues to convey useful information to the anomaly detection model, even in the presence of significant corruption. 









\subsection{Feature set analysis}
\label{feature_analysis}
We evaluate the effectiveness of different feature sets used by our anomaly detection model. As described in \cref{av_anomaly}, we start with {\bf discrete time delays} as our feature representation and optimize the model with the cross entropy loss. Then, we use {\bf continuous time delay distributions} as representations instead, where we optimize models with different objective functions: 1) Soft CE: we use the time delay distribution as the target (akin to a ``soft'' label) and use the cross entropy loss (\cref{av_anomaly}); 2) CE: we map each distribution into one-hot encoding as the target by using  and employ the cross entropy loss; 3) BCE: we use the distribution as the target while treating each synchronization score  (\cref{sync_estimate}) within the same time step independently. We use the sigmoid function and binary cross entropy loss to train the model. We also use our network's {\bf feature activations} as in \cref{av_anomaly}: 1) audio-visual feature activations (activation-AV); 2) visual-only feature activations (activation-V). Besides, we consider using a combination of different feature sets where we concatenate {\bf continuous time delay distributions} and {\bf audio-visual feature activations}~(Act.-AVdist.) as a new feature. Similar to audio-visual feature activations as in \cref{av_anomaly}, we use squared distance as the loss for the concatenation of these two types of feature sets. 

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lllcccc}
\toprule
\multirow{2}{*}{Model} &\multirow{2}{*}{Feature set} & \multirow{2}{*}{} & \multicolumn{2}{c}{AVG-ALL}&\multicolumn{2}{c}{AVG-FV} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7}
     &      &  & AP & AUC & AP& AUC\\
\midrule
Bayes &-  & -& 73.1 & 85.1 & 72.4 &  86.0 \\
\cdashlinelr{1-7}
\multirow{7}{*}{Ours} & discrete delay & CE &    80.8 &    86.5 & 80.0 & 86.6 \\
 & distribution &   CE &  84.8 &   87.9 & 90.3 & 92.2 \\
  &  distribution & BCE &  78.6 &   83.4   &80.5 &84.8 \\
   &  distribution & Soft CE &   \textbf{87.8} &  \textbf{90.0}  & \textbf{94.2}  & \textbf{94.5} \\
      &  activation-AV &  MSE &    86.5 & 87.1 &91.5  & 91.9 \\
        & Act.-AVdist.& MSE &  85.5 &  87.0 &     90.0  & 91.3 \\
       &  activation-V &  MSE &    -- &  -- & 77.6  & 85.9 \\
    & discrete prob. &  -- &  83.4 &     86.9  & 88.6 &  91.1 \\
\bottomrule
\end{tabular}}
\caption{\textbf{Feature set analysis.} AP~() and AUC~() are reported on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} when using different feature sets. Best results are in \textbf{bold}. AVG-ALL means the average over all categories. AVG-FV represents the average over four fake video categories.}
\label{naive_bayes_comparison_main}
\end{table} 
 We also compare with a simple model based on {\bf Naive Bayes} and discrete time delays. This model assumes that each frame's time delay is independent, and obtains a probability for the entire sequence by multiplying the probability of each frame's time delay. This amounts to simply detecting large misalignments since in practice the Naive Bayes model will assign probability solely based on the magnitude of each delay.
 
 

Finally, we consider a version of the model that autoregressively predicts the entire distribution of time delays, inspired by autoregressive models, such as PixelCNN~\cite{van2016conditional} that generate images in a raster scan order. We autoregressively predict each element of the 2D matrix , where  is created by vector quantizing the entries  of the synchronization probability  using -means (see \supparxiv{supp.}{\cref{appendix:feature_set}} for details). 

\mypar{Analysis.} We evaluate each variant on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} and report results in \cref{naive_bayes_comparison_main}. These results suggest that all formulations achieve performance significantly better than chance, indicating that these feature sets are useful for anomaly detection. As in \cref{naive_bayes_comparison_main}, the time delay distribution model outperforms the discrete time delay model, suggesting that there is important information conveyed in the probability of unlikely delays. The autoregressive model that uses distribution as input and soft labels (soft CE) performs best since it forces the output prediction to match the distribution from the synchronization model. Interestingly, the model that uses audio-visual feature activations obtains performance close to that of the soft CE model, indicating that the networks' audio-visual features convey useful information.  Finally, the multimodal activation-AV model significantly outperforms the visual-only activation-V model, suggesting that having access to both modalities is useful for our anomaly detection model.






 







\subsection{Ablation study}


\begin{table}[t!]
\centering
\upvspacefig
\resizebox{0.89\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Synchronization\\ dataset\end{tabular}} &\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Auto-regression\\ dataset\end{tabular}}& \multicolumn{2}{c}{AVG}\\
\cmidrule(lr){3-4}
     &   & AP           & AUC          \\
\midrule
\multirow{3}{*}{LRS2~\cite{Afouras18c}}  & LRS2~\cite{Afouras18c} & \textbf{87.8}          &      90.0       \\
  & LRS3~\cite{afouras2018lrs3} &   85.0       &   89.6 \\       
 & LRS2~\cite{Afouras18c}LRS3~\cite{afouras2018lrs3} &  85.1       &   89.9 \\     
\cdashlinelr{1-4}
\multirow{3}{*}{LRS3~\cite{afouras2018lrs3}} & LRS2~\cite{Afouras18c}   &   86.6        & 89.0 \\

& LRS3~\cite{afouras2018lrs3}   &    87.2       &  90.3 \\
& LRS2~\cite{Afouras18c}LRS3~\cite{afouras2018lrs3} &  87.2      &   {\bf 90.6} \\    

\bottomrule
\end{tabular}
}
\caption{\textbf{Dataset ablation.} AP scores () and AUC scores () are reported on FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} dataset by using different datasets to train synchronization model or atuoregressive model. Best results are in \textbf{bold}.}
\label{combined_dataset_main}
\end{table} 
\paragraph{Different training dataset.}
\label{training_dataset}
We ask how the choice of dataset affects the quality of the model. To test this, we train our synchronization and autoregressive models on different datasets to analyze the generalization abilities of each component, \ie, training the synchronization model on LRS2/LRS3 and training the autoregressive model on LRS3/LRS2 or LRS2LRS3 with the same hyperparameters.  As shown in \cref{combined_dataset_main}, there is no significant performance change when we train these two components on different combinations of datasets, including when they are trained on the same dataset. This suggests that the distribution of time delay predictions may be stable between these speech video datasets. 






\mypar{Influence of sequence length.}
To explore the influence of input sequence length for the autoregressive model, we sample the same amount of training videos for sequence lengths  of 10, 20, 30, 40, 50, and 60, and keep other hyperparameters the same. We test these models on FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. \cref{clip_size_fig} shows that as the sequence length increases, the performance increases with it. 





\mypar{Effect of time delay distribution maximum offset.}
We also study how the length of time delay distribution would affect the performance of the autoregressive model with distribution over delays. We experiment with maximum offset  resulting in the delay distribution length of . We test these models on the FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. \cref{clip_size_fig} shows that as distribution length increases, the performance first increases, after which point results plateau or slightly decrease. This may be due to the fact that when considering larger ranges of offsets, the distribution spreads over a large number of unlikely possibilities, making important information less apparent after normalization.


\begin{figure}[t!]
    \centering
    \upvspacefig
    \includegraphics[width=0.9\linewidth]{image/dist_len.pdf}
    \caption{\textbf{Hyperparameter ablation.} 
    We evaluate with different input sequence lengths for our autoregressive model on FakeAVCeleb (left), and study the effect of the time delay distribution's maximum temporal offset (right).} 
    \label{clip_size_fig}
\end{figure} 







 
\section{Conclusion}
We have proposed a method for detecting video manipulation by self-supervised anomaly detection. To do this, we create novel feature sets that convey audio-visual synchronization. We then show that fake videos can be detected by flagging examples with unlikely sequences of these features, according to a learned distribution. Our model obtains strong performance on the FakeAVCeleb and KoDF datasets, despite the fact that it was trained only on real video. It also obtains robustness to visual postprocessing operations and to videos containing other spoken languages. We see our work as opening in two directions. The first is in posing forensics as an anomaly detection problem with a self-supervised feature set. While we have proposed one such model, based on autoregressive sequence models, the field of anomaly detection offers many possible future approaches. The second direction is in developing new feature sets that are well-suited to forensics problems, beyond the synchronization features used in this work. 






\mypar{Limitations and Broader Impacts. } Our work provides methods that can potentially be applied to detecting malicious video manipulations and disinformation. While we have shown that our model is capable of detecting several types of fake video, there may be other techniques that our model fails to detect. In particular, due to the design of our use of synchronization-based features, our model is not well suited to detecting manipulations that leave the synchronization between motion and sound relatively consistent, such as those that change a speaker's appearance without significantly changing the motion of their mouth. 

\mypar{Acknowledgements.} We thank David Fouhey, Richard Higgins, Sarah Jabbour, Yuexi Du, Mandela Patrick, Deva Ramanan, Haochen Wang, and Aayush Bansal for helpful discussions. This work was supported in part by DARPA Semafor. The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. 




 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{avforensics}
}

\clearpage
\appendix
\supparxiv{
\setcounter{page}{1}
\twocolumn[{\renewcommand\twocolumn[1][]{#1}\begin{center}
    \vspace{-2.0em}
    {\bf \large Supplementary Material:\\Self-Supervised Video Forensics by Audio-Visual Anomaly Detection}
    \vspace{2.0em}
\end{center}

}]

}{}


\renewcommand{\thesection}{A.\arabic{section}}
\setcounter{section}{0}


\section{Video Results}
We provide some  qualitative video results of some random samples from the FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} in \href{https://cfeng16.github.io/audio-visual-forensics/}{our webpage} with audio. We show ``ground truth'' outputs from the synchronization model and autoregressive predictions over time. We also show a score indicating the probability that the example is fake~(\cref{eq:loss}, main paper). We use the time delay distribution as our feature set, and display the predictions as a heat map. Each step  of the predicted outputs was obtained by providing the ground truth features from times  into the autoregressive model.



\section{Cross-dataset Generalization}
\label{appendix:crossdataset}
We also experiment with the another audio-driven video editing method LipGAN~\cite{kr2019towards}. We test on a test set consisting of 100 real videos and 100 fake videos, which are based on the real videos of test set from FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. We show results on \cref{appendix-tab:crossdataset}. Our method leverages continuous time delay distribution as in \cref{feature_analysis} and outperforms many supervised methods although it is only trained on real videos, indicating that our method possesses certain generalization capability to different manipulation methods. 

\begin{table}[h!]
\centering
\upvspacefig
\resizebox{1\columnwidth}{!}{
\begin{tabular}{llccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multirow{2}{*}{Modality} &\multicolumn{2}{c}{LipGAN~\cite{kr2019towards}}\\
\cmidrule(lr){4-5}
\multicolumn{2}{c}{}           &             & AP          & AUC         \\
\midrule
\multirow{5}{*}{\shortstack[c]{Supervised \\ (transfer)}}
 & Xception~\cite{rossler2019faceforensics++}   &   & 67.6   & 65.5  \\
                            & LipForensics~\cite{haliassos2021lips}   &   &   82.2    &   85.2      \\
                            & AD DFD~\cite{zhou2021joint}   &       &  82.7     &   84.8   \\
                            & FTCN~\cite{zheng2021exploring} &            &    76.9      &  73.9         \\
                            & RealForensics~\cite{haliassos2022leveraging}   &         &  \textbf{94.3}    & \textbf{96.5}          \\
\midrule
\multirow{3}{*}{Unsupervised}                  
& AVBYOL~\cite{haliassos2022leveraging, grill2020bootstrap} &   &  65.0    & 73.0 \\

& VQ-GAN ~\cite{esser2021taming} &    &  50.7     &  50.4     \\

\cdashlinelr{2-5}
& Ours     &     &     \textbf{93.3}      &   \textbf{94.1}          \\

\bottomrule
\end{tabular}
 }
\caption{\textbf{Generalization to LipGAN method}~\cite{kr2019towards}. AP scores () and AUC scores () are reported on real videos of test set from FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. Fake videos are manipulated by LipGAN~\cite{kr2019towards}. Supervised methods are trained on FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}. Ours is trained on LRS2~\cite{Afouras18c} and uses the feature set of continuous time delay distribution. Best results are in \textbf{bold}.}
\label{appendix-tab:crossdataset}
\end{table}
 


\section{Ablation Study}
\paragraph{Feature activation. }
We study the influence of the number of principal components  for the variation of the anomaly detection model that projects the audio-visual feature activations into a lower dimensional space. We set  to  and keep other hyperparameters the same. \cref{number_pca} shows that as the  increases, the accuracy of the anomaly detection model decreases. The reason might be that the higher dimensional prediction problem is also more challenging. 




\section{Feature Set Variations}
\label{appendix:feature_set}
We describe more details about building the autoregressive model on some of our feature sets in this section.

\mypar{Binary cross entropy (BCE) model.} 
We assume that the  possible time delays of each time step are independent, and use BCE loss for probability  of each time delay.  
Thus, we can decompose  as in \cref{prob_decom}:

We then maximize  using BCE loss:


We constrain the prediction  to the range of  via a sigmoid function.
\begin{figure}[t]
    \centering
    \upvspacefig
    \includegraphics[width=0.95\linewidth]{image/pca_plot.pdf}
    \caption{\textbf{Number of PCA projections.} We evaluate with different number of principal components for the model that obtains a feature set by projecting the audio-visual feature activations to a lower dimensional space using PCA. We report average AP scores () and AUC scores () on FakeAVCeleb dataset~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495}.} 
    \label{number_pca}
\end{figure} \begin{figure*}[t!]
    \centering
    \upvspacefig
 \includegraphics[width=1\linewidth]{image/dist_pixelcnn.pdf}
    \caption{\textbf{Visualization of discrete probability grid.} We use K-means clustering to quantize the probability space to convert continuous synchronization probability  to discrete probability bin . Then we build a autoregressive Transformer~\cite{vaswani2017attention} model on the probability grid. }
    \label{pixelcnn}
\end{figure*}
 \mypar{Discrete 2D probability model.} 
The {\em discrete prob.} model generates the entire 2D frame/time delay matrix autoregressively in ``raster scan" order, similar to models such as PixelCNN~\cite{van2016pixel,salimans2017pixelcnn++}. We unroll the time delay distribution sequence into 2D grid like a grayscale image as shown in \cref{pixelcnn}. We use -means () clustering to quantize each entry and assume each probability  of time delay not only depends on the previous time delay distributions but also the previous probabilities within the same distribution. Then we decompose :

We utilize a similar loss function in PixelCNN~\cite{salimans2017pixelcnn++,van2016pixel} to supervise the autoregressive model.



\section{Implementation Details}
\label{appendix:imple}
\paragraph{Audio-visual synchronization model.}
Following prior work~\cite{Afouras20b,chen2021audio,korbar2018cooperative}, we utilize curriculum learning to train the audio-visual synchronization model. Specifically, we use the two-stage training procedure. During the first phase, the negatives are from different videos, while for the second stage, the negatives are sampled within the same videos randomly (the starting time steps are sampled randomly). 


\mypar{Anomaly detection model.}
We process each input vector  with an affine transformation, before passing it into the autoregressive model. This projects the input (e.g., a time delay distribution ) to . Also, we add an affine transformation to project the embedding back into the feature set space, e.g.,  for the time delay distribution. We use learnable positional encodings for both the synchronization and autoregressive models. 



\mypar{Hyperparameters.}
For the synchronization model, we use Adam~\cite{kingma2014adam} with a learning rate of  , with a batch size of 16 for the first phase and 40 for the second. During the second stage, we sample four 5-frame short clips per video. For the autoregressive model (anomaly detection model), we use the Adam optimizer~\cite{kingma2014adam} with  learning rate, weight decay of  and warm-up and cosine learning rate decay~\cite{loshchilov2016sgdr} strategies. We use a batch size of 16 and the dropout~\cite{srivastava2014dropout} rate of . We train the synchronization model on 8 NVIDIA A40 GPUs and use a single GeForce RTX 2080 Ti GPU for the autoregressive model.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{image/corrupted_images.pdf}
    \caption{\textbf{Visualization of corrupted images.} Examples of the corruptions taken into account at intensity level 5. The set of corruptions is introduced in Jiang et al.~\cite{jiang2020deeperforensics}, and it consists of color saturation, local block-wise distortion, color contrast, Gaussian blur, white Gaussian noise, JPEG compression, and video compression rate change.}
    \label{corrupted_images}
\end{figure} \section{Visualization of perturbed images}
We visualize some images used for unseen perturbations robustness test in \cref{corrupted_images}. 


\section{Temporal localization}

To test the temporal localization ability of our method, we selected random 5-frame subsequences from real videos of FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} test set and manipulated frames using Wav2Lip~\cite{prajwal2020lip} with audio (since none of our benchmark datasets provided relevant videos for the task). Our model predicts the fake score for each frame~(\cref{temporal_localization_arxiv}). We can see that the score was significantly higher for manipulated frames, suggesting that our model is able to temporally localize manipulations. Besides, we can achieve top-5 accuracy of 92.0\%. It is worth mentioning that all supervised and unsupervised baselines can not localize the manipulated frames temporally. 
\begin{figure}[h]
    \centering

    \includegraphics[width=\linewidth]{image/localization_arxiv.pdf}

        \caption{\textbf{Temporal localization.} We manipulate a random 9-time step interval~(\textcolor{pink}{pink region}). We show the fake score~(negative log-likelihood) for each frame. We scale the  axis by the maximum score. } 

        
    \label{temporal_localization_arxiv}
\end{figure} 
\section{Robustness to background noise}

We test our method when the audio stream of FakeAVCeleb~\cite{NEURIPS_DATASETS_AND_BENCHMARKS2021_d9d4f495} test set is added Gaussian noise under different signal-to-noise ratio (SNR). As shown in \cref{noise}, our detector can basically maintain its performance when SNR decreases, indicating that our method is robust to the background Gaussian noise (our method has never seen audio Gaussian noise during training). 
\begin{figure}[h]
    \centering
    \upvspacefig
    \includegraphics[width=0.45\linewidth]{image/noise.pdf}
    \caption{\textbf{Robustness to background noise.} AUC scores  of our detector as a function of signal-to-noise ratio (SNR) when Gaussian noise is added to the audio stream. ``w/o" means no noise is added. AVG-ALL means the average over all the categories. AVG-FV represents the average over four fake video categories.} 
    \label{noise}
\end{figure}  \end{document}
