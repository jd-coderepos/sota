\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl} 

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{pifont}

\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\newcommand{\anningzhe}[1]{{ \color{blue} [Anningzhe says ``#1'']}}
\newcommand{\yufei}[1]{{ \color{red} [yufei says ``#1'']}}
\newcommand{\benyou}[1]{{ \color{orange} [benyou says ``#1'']}}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\theoremstyle{claim}
\newtheorem{claim}{Claim}
\theoremstyle{rethinking}
\newtheorem{rethinking}{Rethinking}
\theoremstyle{researchquestion}
\newtheorem{researchquestion}{RQ}
\theoremstyle{findings}
\newtheorem{findings}{Findings}
\theoremstyle{fact}
\newtheorem{fact}{Fact}
\newtheorem{corollary}{Corollary}\theoremstyle{definition}
\newtheorem{definition}{Definition}\theoremstyle{lemma}
\newtheorem{lemma}{Lemma}
\usepackage{wrapfig,lipsum,booktabs}
\theoremstyle{proper}
\newtheorem{proper}{Property}\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}




\title{  Outcome-supervised Verifiers for Planning in Mathematical Reasoning}


\author{Fei Yu, Anningzhe Gao, Benyou Wang \\
  The Chinese Unviersity of Hong Kong, Shenzhen, China \\
  Shenzhen Research Institute of Big Data \\
  222043013@link.cuhk.edu.cn, gaoanningzhe@sribd.cn,  wangbenyou@cuhk.edu.cn \\
  \url{https://github.com/FreedomIntelligence/OVM}
  }
\begin{document}
\maketitle




\begin{abstract}


Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem.


Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for \textit{planning} by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up to  13B parameters}; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning.
\end{abstract}





\section{Introduction}

Multi-step reasoning problems are challenging for even large language models (LLMs)(\citealp{selection-inference23}, \citealp{compositionality-gap22}, \citealp{cot22}). Especially, it seems that the model capability to implicitly compose multiple information in an end-to-end manner is not improved as model size increases~\citep{compositionality-gap22}. A simple method, Chain of Thought (CoT), which first outputs a series of intermediate reasoning steps and then finally the answer, can significantly improve the multi-step reasoning performance(\citealp{cot22}, \citealp{cot-bbh23}). However, errors in the previous steps can affect accuracy of the following steps and the answer in such a single chain,  which is also known as \textit{error propagation}. For example, \citet{tot23} found that GPT-4 often fails at the first step in Game of 24, blocking the success, while it actually can solve the task in multiple attempts.  




\textbf{Verifiers for complete reasoning paths}
To address the   error propagation,  (\citealp{gsm8k21}, \citealp{deepmind-process22}, \citealp{li2023making}, \citealp{khalifa2023guided}, \citealp{openai-process23})   employ a verifier model to assess generated solution candidates  in terms of correctness  on the overall reasoning path. 
Typically, there are two types of supervision for verifier training: \textit{outcome supervision} and \textit{process supervision}. 
(\citealp{deepmind-process22}, ~\citealp{openai-process23}) have demonstrated a a clear advantage  of \textit{process supervision} over   \textit{outcome supervision} for training verifiers in terms of verifying complete reasoning paths, as the former provide more fine-grained supervisions.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/ps_and_os.png}
    \caption{\label{fig:ps_and_os} Process supervision and outcome supervision on training verifiers for complete path verification. Given a question  and a solution path , the verifier is trained to predict path correctness, which is the circled output scalar on the last token. Training is conducted at the token level for robustness. Outcome supervision focuses solely on the final answer's correctness, replicating this scalar label across all tokens (indicated by shaded labels), while process supervision assigns labels at each step. This ``copy behavior'' accidentally endows outcome supervision with foresight. Correct steps and answers are colored in \textcolor{yellow}{yellow} and incorrect ones in \textcolor{gray}{grey}.}
\end{figure*}


\textbf{Verifiers for intermediate steps. }
This paper, diverging from the traditional approach of verifying \textit{complete reasoning paths}, focuses on verifying \textit{intermediate steps}. The latter  provides a finer granularity of verification, facilitating the early elimination of unreliable reasoning while fostering potentially promising inferences~\cite{tot23}.  
Contrast to  \textit{complete path verification}, we argue that an outcome supervision verifier might have advantages over its process supervision counterpart in \textit{partial path verification}. 
The outcome supervision verifier selects  intermediate steps that are more likely to yield accurate answers, unlike the process supervision verifier that assesses each step  in an immediate sense without considering its potential contribution for the final result.
This is analogous to the difference between \textit{reward model} and \textit{value model} in  Reinforcement Learning: a \textit{value} model forecasts long-term outcomes versus a \textit{reward} model's immediate incentives, see theoretical backing  in Sec.~\ref{sec:proof}





















In this paper, we propose to use outcome based supervision to train a verifier to verify intermediate steps, called \textbf{OVM}. Experiments are conduct on two popular multi-step mathematical reasoning datasets -- GSM8K~\citep{gsm8k21} and Game of 24~\citep{tot23}. In GSM8K, our OVM-7B model obtains state-of-the-art performance among models with up to 13B parameters, attaining a 84.7\% accuracy without resorting to supplementary datasets, GPT-4, or executing programs. 
In Game of 24,  OVM-7B reaches 78.7\% success rate with merely 20 nodes visited per step, in stark contrast to its 11\% greedy success rate and 11.7\% with majority voting over 100 reasoning paths. Furthermore, we demonstrate that our method attains competitive, and often superior, performance using fewer sampled reasoning paths compared to complete path verification on both GSM8K and Game of 24. This  indicates the effectiveness of OVM in value estimation, particularly in complex questions requiring longer steps.



In summary, our contributions are  three-fold.
\begin{itemize}

    


    \item  \textbf{Planning with outcome-supervised verifiers}: we are the first to propose   outcome-supervised verifiers for planning. This method emphasizes the correctness of the final answer rather than focusing solely on the current (partial) pathâ€™s correctness, which also evidenced by theoretical understanding. Moreover,  OVM also obviates the need for costly step-level annotations required by process supervision,
    
    \item \textbf{SOTA models in mathematical reasoning}:  OVM achieves state-of-the-art results in GSM8K among LLMs up to 13B parameters; especially it does not leverage  additional data, GPT-4, or code execution.
    


    \item \textbf{Systematic rethink on verifiers}: We conduct systematic rethink on existing work regarding verifiers in mathematical reasoning. Especially we discuss the fine-grained supervision hypothesis in complete path verification and whether it could be directly applied in  partical path verification.   We clarify the fundamental distinction between process supervision (akin to reward) and outcome supervision (akin to value), proving that "the outcome matters" in planning contexts, contrary to prior comparisons in literature.

    
    
\end{itemize}



\section{Rethinking Verifiers in mathematical reasoning}
\label{sec:background}

We first give the problem definition of mathmatical reasoning where verifiers work in two manners:  complete path verification and partial path verification. 
Then, we introduce two approaches for intermediate state evaluation in partial path sampling. The main involved notations are shown in Table.~\ref{tab:notations}.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{cl}
\hline
\textbf{Notation} & \textbf{Description} \\ \hline
 & Multi-step question requiring a sequence of steps \\
 & a question set\\
 & Solution path for a question,  \\
 & The i-th step in a solution path \\
 & Set of solution path candidates,  \\
 & Number of steps in a solution path \\
 & Final answer in a solution path \\
 & The language model as the generator  \\
 & Sampling size for solution candidates \\
 & a verifier model  that  maps a path to a positive number:    \\
 & Predicted solution path with the highest score \\
 & Partial solution path up to step  \\
 & Set of candidate partial paths,  \\
 & Score of a state (partial path) \\
ORM & Outcome Reward Model, trained with outcome supervision \\
OVM & Outcome Value Model, trained with outcome supervision \\
PRM & Process Reward Model, trained with process supervision \\
\bottomrule
\end{tabular}
\caption{Summary of Notations Used in the Paper}
\label{tab:notations}
\end{table}


\subsection{Problem Definition}

\subsubsection{Mathematical  Reasoning Problem}
In this paper, we focus on mathematical reasoning problem, typical involves multiple steps.

\begin{definition}
   \textbf{A mathematical  reasoning question} \textit{ requires a sequence of steps to be addressed, whose solution path is , where  represents the i-th step,  is the number of  steps, and  is the final answer.} 
\end{definition}



\paragraph{Generate-and-verify Paradigm} To alleviate the issue of potential error propagation in a single solution chain, one approach is sampling multiple solutions from the generator  for the question , resulting in a -sized solution candidate set , where  is the sampling size. To relive the issue of error propagation, this paper focuses on verifiers for reranking solution  candidates, which has been shown to outperform majority-voting~\citep{self-consistency23}. 
There are two  verifier usage methods: \textbf{I) Complete Path Verification}, and \textbf{II) Partial Path Verification}; the former assesses the correctness of the entire solution path, the latter concentrates on verifying  intermediate steps within a partial path.







\subsubsection{Scenario I: Complete Path Verification}
For a set of complete solution paths ,  a simple protocol is  to verify the complete path: each pathâ€™s correctness is scored by an external verifier model, also known as `reward model' (\citealp{li2023stepv}, \citealp{deepmind-process22}, \citealp{openai-process23}), denoted as  that maps a solution to a positive number . One could obtain the predicted solution (i.e., a reasoning path) with the  highest score:










To train verifiers, one could utilize an outcome supervision or process supervision demonstrated in Figure~\ref{fig:ps_and_os}. 
Verifiers employing outcome supervision
are trained solely with the correctness of the final answer, whereas verifiers with process supervision
is trained using labels of per-step correctness. See further details in Appendix~\ref{app:verifier}.
We conjecture a Fine-grained Supervision Hypothesis in complete-path verification as below:
\begin{definition}
\label{def:hypothesis}
   \textbf{Fine-grained Supervision Hypothesis} \textit{Process supervision -based verifiers  outperform outcome supervision -based verifiers}.
\end{definition}

~\citep{openai-process23}  empirically demonstrated  the Hypothesis in complete-path verification.
The rationale could be that labels of process supervision are at finer-grained step level instead of a solution level in outcome supervision~\citep{gsm8k21}, leading to  more informative supervisions. 





 
\subsubsection{Scenario II: Partial Path Verification} Contrast to the complete path verification, partial path verification dynamically guides generation in each step.
Here, the generator  produces tokens based on linguistic probabilities, pausing for evaluation at each step. Given  steps already generated by the generator, there exists K candidate solutions , and each  candidate  solution  in first t steps is defined as   .
Supposed there exists a verifier model  that could verify an incomplete solution within  first  steps. The  candidate set with top- selected incomplete path  using verifier  is 

































In general, partial path verification might outperform  complete path verification as the former provides finer-grained verification than the latter (\citealp{khalifa2023guided}).
In  partial path verification, one could also use either  an outcome-based verifier or process-based verifier. A open question remains as below:
\begin{rethinking}
    \textit{Does Fine-grained Supervision Hypothesis for verifiers still hold in  partial path verification?}
\end{rethinking}

This is discussed in the following Sec.~\ref{sec:reward_value}.

\subsection{Reward Model vs. Value Model for Partial Path Verification}
\label{sec:reward_value}

Contrast to the complete path verification,  here we discuss the scenario when the path is not complete and needs to be completed. This turns the problem from \textit{ one-shot ranking}  to  \textit{multi-step planning}. 
Depending on the type of supervision (process vs. outcome) employed by verifiers, the planning criteria in reasoning step planning vary. Verifiers under process supervision assess per-step correctness during planning, similar to a \textit{reward model} in reinforcement learning; whereas those under outcome supervision focus on the likelihood of achieving a correct final answer, similar to a \textit{value model} in reinforcement learning.


\begin{itemize}
    \item \textbf{Reward-based Models}   accesses only  the correctness of the current steps, i.e. . Obviously, annotating such step-level correctness labels are labor-intensive. Recently, \citet{grace23} proposed an auto-labeling algorithm aligning with reference solutions, yet it requires ground solution paths.
    \item \textbf{Value-based Models}  cares about the correctness of the final answer in the future, which will be generated in the latter steps. Formally, suppose the future steps beginning from timestamp  is , value-based evaluation assesses the ``value'' . 
\end{itemize}




Intuitively,  in Partial Path Verification, verifiers should adopt a future-oriented approach that emphasizes the correctness of the final answer rather than focusing solely on the current (partial) path's correctness, akin to prioritizing a value model over a reward model. Here comes another rethinking point which seems contraction to the Fine-grained Supervision Hypothesis in Definition~\ref{def:hypothesis} but in the context of future-oriented planning, since process supervision leads to a reward-based model while outcome supervision leads to a value-based model. See further discussion in Section~\ref{sec:methodology}.

\begin{rethinking}
In reasoning step planning,  verifiers with outcome supervision  (i.e., a value model) might outperform that with process supervision (i.e., a reward model)  in partial path verification. 
\end{rethinking}
Moreover, verifiers with outcome supervision is easier for scalability since it does not need labor-intensive  annotation on  step-level correctness.





\subsection{Our Solution}

A few existing  work tried value-based methods (\citealp{tot23}, \citealp{rap2023}, \citealp{alphazero-like23}), which takes the future outcomes into consideration. Those implementation are prompt-based, we focus on training an evaluation model which can capture the generator's distribution in this paper.
The challenge of training a value model in  partial path verification lies in the value estimation since the possible continued sequences  can be infinite. To approximate , standard reinforcement learning (RL) optimizes a value model with sampled paths based on algorithms such as TD and MC. Recently, ~\citet{alphazero-like23} trains a value model using standard RL algorithms. 



However, we will show in Section~\ref{sec:methodology} that outcome supervision accidentally enables the model to predict  with a new strategy.  
We call the process of guided decoding with the value-based state evaluation, which is future-oriented and guides the model to correct answers, as ``planning''. 





















\section{Outcome Supervision for Planning}
\label{sec:methodology}

In this section, we first give an explanation on why we can directly apply outcome supervision, initially designed to train a reward model (verifier), to develop a value model, despite reward model and value model are very different. Then, we detail our implementations. For simplicity, we name the model as Outcome Value Model (OVM).




\subsection{Motivation}
We first introduce the intuition of outcome-supervised model for value estimation, then we give an explanation from a theoretical perspective.


\subsubsection{Intuitive explanation}
\textbf{Although it is originally intended for assessing the correctness of complete reasoning paths, the token-level extension of outcome supervision labels, unexpectedly aids each step in learning to predict future outcomes}. As shown in Figure~\ref{fig:ps_and_os}, the model is trained to predict the correctness of the final answer at each step, irrespective of the fact that the final answer is yet to be produced and remains unseen at that point in time. 

Moreover, the model is trained to predict each step's value, which correlates with its likelihood of leading to a correct answer. Let's imagine that, if a specific step in the training set always leads to correct answers, the model learns to predict 1 since all of the training labels are 1, indicating it invariably leads to a correct answer. If it always leads to incorrect answers, it predicts 0. For a step leading to both correct and incorrect answers, the model learns with both labels and finally predict a value between 0 and 1, tending towards the one more frequently observed in the training set.





\subsubsection{Theoretical Explanation}
\label{sec:proof}




We will show the theoretical reason why outcome supervision can lead to a value model in this subsection. 





\begin{claim}
    For a  OVM model  parameterized by the optimal parameter ,
the OVM's score of a given state within first  steps is the approximated probability to reach a correct answer, i.e.,
    
\end{claim}

\begin{proof}

Suppose for each question , we have the generator producing  solution paths  and the corresponding answers .
We define the correctness  as 1 if  is correct otherwise 0.
The MSE loss of outcome supervision is 


Given the training question set , 
the overall objective is


Denote , the partial derivation of  is
 

By optimizing , we get 
 
which is , whose estimation's accuracy depends on the sampling.





\end{proof}





\subsection{Implementation}


\begin{algorithm}[t]
\small
\caption{\label{algo:beam_search}Value-Guided Beam Search}
\begin{algorithmic}[1]

\State  Question , Beam size , Sampled steps per state , Maximum step count 
\State  Best solution sequence for 
\State  Generator  and OVM 

\Procedure{ValueGuidedBeamSearch}{}
    \State Initialize step sequences 
    \State Sample initial steps 
    \State Evaluate values  for each step
    \State Select top  valued steps and add to 
    \State 
    \While{sequences in  are not complete and }
        \State 
        \State 
        \For{each sequence  in }
            \For{ to }
                \State 
                \State 
                \State 
                \State 
            \EndFor
        \EndFor

\State  top  valued sequences from 
        \State 
        \State 
    \EndWhile
    \State \textbf{return} sequence with highest final value in 
\EndProcedure

\end{algorithmic}
\end{algorithm}


\paragraph{Model architecture} 
OVM is a decoder-only autoregressive language model, with an additional linear layer plus a single bias parameter after the model's final unembedding layer. It is a model independent of the generator.

\paragraph{OVM training}
Given a training question set, we generate  solution paths per question using the generator, then detect answer correctness, such as through string matching to the ground answer. This allows us to label numerous OVM training samples using just question-answer pairs, without path annotations. OVM is then trained on these labels with MSE loss under outcome supervision.


\paragraph{Inference - beam search with planning}
In this paper, we conduct beam search strategy guided by OVM during inference. Diverging from the traditional token-level probability-guided beam search, it is guided by the step-level estimated value. The algorithm is detailed in Algorithm~\ref{algo:beam_search}.









\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/what_is_planning.png}
    \caption{\label{fig:what_is_planning} Difference between complete path verification and partial path verification. Planning is the latter one, evaluate and selects paths based on value estimation during inference.}
\end{figure*}







\section{Experiment Results and Analysis}

\subsection{Experimental settings}

\paragraph{Benchmarks} We conduct experiments on two mathematical reasoning datasets, GSM8K~\citep{gsm8k21} and Game of 24~\citep{tot23}, in this paper. Each inference experiment was repeated 3 times, with the average results and their standard variation reported.

\textbf{GSM8K} is a grade school math word problem dataset consisting of 7473 training questions and 1319 test questions, each with a single annotated solution path. The problems require 2-11 reasoning steps, averaging around 3.5 steps. It is classical for multi-step mathematical reasoning and also the pioneering to employ the verifier approach in the complete path verification framework~\citep{gsm8k21}. 

\textbf{Game of 24} is a mathematical reasoning challenge requiring models to combine four numbers with basic operations (+-*/) to reach a total of 24. Its expansive solution space, filled with numerous valid yet unproductive combinations, makes it an ideal benchmark for evaluating planning capabilities. Following \citet{tot23}, we use relatively hard problems index 901-1,000 for testing. 



\paragraph{Label annotations} In GSM8K, the answer correctness is determined through precise string matching, where all ground answers are integers. In Game of 24, the answer correctness is based on the validity of the equation equating to 24 and the singular usage of input numbers, following~\citep{tot23}. 








\paragraph{Baselines} We benchmark our method's performance against the current state-of-the-art models in GSM8K, and the notable Tree-of-Thought (ToT) in Game of 24. Additionally, We compare the effectiveness of OVM planning with non-planning version of the same model implemented by ours, including greedy, Self-Consistency (SC) and Outcome-based Reward Model (ORM). For our method, we report the best results across available beam size, e.g. the result of  is the best one among .


\paragraph{Implementation} We use the newline character as the marker for the end of each step. \textbf{Generators}: In GSM8K, we fine-tuned Mistral-7B and Llama2-7B on the training set. Given that GSM8K provides calculation annotations, our models were also trained to utilize calculators. In Game of 24, we fine-tuned Llama2-7B on simpler problems (indices 1-900) with enumerated solution paths. For both datasets, the fine-tuning was carried out for 2 epochs, with a batch size of 128. We set the maximum learning rate to 1e-5, using a linear scheduler with the AdamW optimizer. \textbf{OVMs/ORMs}~\footnote{Since we train the value model with outcome supervision, the objective originally intended for ORM, the same model is simultaneously used as OVM and ORM in this paper.}: OVMs were initialized from the corresponding generator checkpoints. We sampled 100 solution paths for each question from the respective generator (resulting in 747,300 for GSM8K and 90,000 for Game of 24), with a temperature setting of 0.7. In GSM8K, OVM was trained for 1 epoch with a batch size of 512. In Game of 24, OVM is trained for 10 epochs with a batch size of 128, due to its smaller training set. The optimizer was AdamW and the maximum learning rate was set to 1e-5 for Llama2-7B and 2e-6 for Mistral-7B respectively, following a linear scheduler. During inference, we set the maximum number of steps as 10 and the maximum new token length is 400. We implement FlashAttention for Llama2-7B (\citealp{flashattention22}, \citealp{flashattention23}).


\subsection{Overall Performance}

\paragraph{Benchmark against the current state-of-the-art models} 
The overall performance of ours in GSM8K and Game of 24 are shown in Table~\ref{tab:benchmark_gsm8k_others} and Table~\ref{tab:benchmark_game24_others} respectively. 
Significantly, our Mistral-based 7B model outperforms all other models under 70B in GSM8K, while our Llama2-based 7B model reaches the best among 7B models, excluding those based on Mistral. Furthermore, in Game of 24, planning with OVM significantly improves the performance of fine-tuned Llama2-7B, boosting its accuracy from a mere 11\% to an impressive 78.7\%, with 20 sampled solution paths.

\paragraph{Comparison with non-planning} 
The comparisons between our planning with OVM and the non-planning implementations are demonstrated in Table~\ref{tab:benchmark_all_self} and Figure~\ref{fig:all_k_acc}. 
The results show that planning with OVM can generally outperform complete path verification using ORM when the number of sampled paths is equal. An exception is observed with Mistral-7B at K=100, where the performance of both planning with OVM and complete path verification tends to reach saturation.











 







\begin{table*}[t]
 \footnotesize
 \caption{Accuracy on GSM8K. In the third column, we mark models that use GPT for inference or are trained with GPT-generated data. SC denotes `Self-Consistency' and RM denotes `Reward Model'.}
  
 \label{tab:benchmark_gsm8k_others}
 \centering
 \begin{tabular}{lrp{1.5cm}p{1.5cm}l}
 \toprule
 Model  & Size & GPT-3.5/GPT-4 & Data Augmentation  & Accuracy \\
\midrule  
 \multicolumn{5}{c}{Open-Source Models without Code Execution} \10pt]
 ToRA-Code (\citealp{ToRA-Code23}) 
        & 7B   & \cmark   & \cmark             & 72.6\%    \\
 ToRA-Code (\citealp{ToRA-Code23}) 
        & 13B  & \cmark   & \cmark             & 75.8\%    \\
 \makecell[l]{ToRA-Code (SC, K=50) \\ (\citealp{ToRA-Code23})} 
        & 34B  & \cmark   & \cmark             & 85.1\%    \\
 ToRA (\citealp{ToRA-Code23}) 
        & 70B  & \cmark   & \cmark             & 84.3\%    \\
 \makecell[l]{ToRA (SC, K=50) \\ (\citealp{ToRA-Code23})}  
        & 70B  & \cmark   & \cmark             & 88.3\%    \\

\midrule
  \multicolumn{5}{c}{Closed-Source Models} \8pt]
 \makecell[l]{DeepMind (RM, K=96) \\ (\citealp{deepmind-process22})}  
        & 70B  &          &                    & 87.3\%    \8pt]
 GPT-4 (\citealp{GPT-4})  
        & -    & \cmark  &                    & 87.1\%    \L(S,\bm{y}^l;q)=\left(f(q;S)-\bm{y}^l\right)^2,\quad l=o\text{ or }p
Additionally, the model is jointly trained with language modeling loss unweighted, following \citet{gsm8k21}.





\end{document}
