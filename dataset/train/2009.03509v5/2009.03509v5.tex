\section{Experiments}

We propose a Unified Message Passing Model (UniMP) for semi-supervised node classification, which incorporates the feature and label propagation jointly by a Graph Transformer and employs a masked label prediction strategy to optimize it. We conduct the experiments on the Node Property Prediction of Open Graph Benchmark (OGBN), which includes several various challenging and large-scale datasets for semi-supervised classification, split in the procedure that closely matches the real-world application \cite{hu2020open}. To verify our models effectiveness, we compare our model with others state-of-the-art (SOTA) models in \emph{ogbn-products}, \emph{ogbn-proteins} and \emph{ogbn-arxiv} three OGBN datasets. We also provide more experiments and comprehensive ablation studies to show our motivation more intuitively, and how LPA improves our model to achieve better results.


\subsection{Datasets and Experimental Settings}





\begin{table}[htbp]
	\setlength{\abovecaptionskip}{-0cm}
	\begin{center}
		\setlength{\tabcolsep}{1.2mm}{
		\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccccc}
			\hline
{\bf Name}  & {\bf Node} &  {\bf Edges}  &  {\bf Tasks}    &  {\bf Task Type} &  {\bf Metric}	\\
				\hline 
				ogbn-products  &2,449,029&61,859,140 & 1 &Multi-class class &Accuracy \\
				ogbn-proteins   &132,534&39,561,252&112  &Binary class  &ROC-AUC\\
				ogbn-arxiv     &169,343&1,166,243&1 &Multi-class class  &Accuracy \\
				\hline
		\end{tabular}}}
	\end{center}
	\caption{Dataset statistics of OGB node property prediction}
	\label{table:ogb}
	\vspace{-4mm}
\end{table}

\paragraph{Datasets.} Most of the frequently-used graph datasets are extremely small compared to graphs found in real applications. And the performance of GNNs on these datasets is often unstable due to several issues including
their small-scale nature, non-negligible duplication or leakage rates, unrealistic data splits \cite{hu2020open}. Consequently, we conduct our experiments on the recently released datasets of Open Graph Benchmark (OGB) \cite{hu2020open}, which overcome the main drawbacks of commonly used datasets and thus
are much more realistic and challenging. OGB datasets cover a variety of real-world applications and span several important domains ranging from social and information networks to biological networks, molecular graphs, and knowledge graphs. They also span a variety of prediction tasks at the level of nodes, graphs, and links/edges. As shown in table \ref{table:ogb}, in this work, we performed our experiments on the three OGBN datasets with different sizes and tasks for getting credible result, including \emph{ogbn-products} about 47 products categories classification with given 100-dimensional nodes features, \emph{ogbn-proteins} about 112 kinds of proteins function prediction with given 8-dimensional edges features and \emph{ogbn-arxiv} about 40-class topics classification with given 128 dimension nodes features. More details about these datasets are provided in appendix A in the supplementary file.



\begin{table}[htbp]
	\setlength{\abovecaptionskip}{-0cm}
	
	\begin{center}
	    \resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
		\hline
			& {\bf ogbn-products} &  {\bf ogbn-proteins	}  &  {\bf ogbn-arxiv} \\
			\hline
			sampling\_method &NeighborSampling & Random Partition & Full-batch \\
			num\_layers &  3 & 7 & 3\\ 
			hidden\_size & 128 & 64 & 128\\ 
			num\_heads & 4 & 4 & 2\\ 
			dropout  & 0.3 & 0.1 & 0.3\\ 
			lr & 0.001 & 0.001 & 0.001\\ 
			weight\_decay& * & * & 0.0005\\ 
			label\_rate& 0.625 & 0.5 & 0.625\\ 
			\hline
		\end{tabular}
		}
	\end{center}
	\caption{The hyper-paramerter setting of our model}
	\label{table:params}
	\vspace{-4mm}
\end{table}

\paragraph{Implementation details.}  As mentioned above, these datasets are different from each other in sizes or tasks. So we evaluate our model on them with different sampling methods following previous studies~\cite{li2020deepergcn}, getting credible comparison results. In \emph{ogbn-products} dataset, we use NeighborSampling with size =10 for each layer to sample the subgraph during training and use full-batch for inference. In \emph{ogbn-proteins} dataset, we use Random Partition to split the dense graph into subgraph to train and test our model. As for small-size \emph{ogbn-arxiv} dataset, we just apply full batch for both training and test. We set the hyper-parameter of our model for each dataset in Table \ref{table:params}, and the label\_rate means the percentage of labels we preserve during applying masked label prediction strategy. We use Adam optimizer with lr = 0.001 to train our model. Specially, we set weight decay to 0.0005 for our model in small-size \emph{ogbn-arxiv} dataset to prevent overfitting. More details about the tuned hyper-parameters are provided in appendix B in the supplementary file.
 




\subsection{Comparison with SOTA Models}
Baseline and other comparative SOTA models are provided by OGB leaderboard. And all these results are guaranteed to be reproducible with open source codes. Following the requirement of OGB, we run our experimental results for each dataset 10 times and report the mean and standard deviation. As shown in Table \ref{table:ret_products}, Table \ref{table:ret_proteins}, and Table \ref{table:ret_arxiv}, our unified model outperform all other comparative models in three OGBN datasets.
Since most of the compared models only consider optimizing their models for the features propagation, these results demonstrate that incorporating label propagation into GNN models can bring significant improvements.
Specifically, we gain 82.56\% ACC in \emph{ogbn-products}, 86.42\% ROC-AUC in \emph{ogbn-proteins}, which achieves about 0.6-1.6\%Â  absolute improvements compared to the newly SOTA methods like DeeperGCN \cite{li2020deepergcn}. In \emph{ogbn-arxiv}, our method gains 73.11\% ACC, achieve 0.37\% absolute improvements compared to GCNII \cite{chen2020simple}, whose parameters are four times larger than ours. 


\begin{table}[htbp]
	\setlength{\abovecaptionskip}{0.cm}
	\setlength{\belowcaptionskip}{-.2cm}
	\begin{center}
	    \resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc}
		    \hline
			{\bf Model}  & {\bf Test Accuracy	} &  {\bf Validation Accuracy	}  &  {\bf Params} \\
			\hline 
			GCN-Cluster \cite{chiang2019cluster} & 0.7897 $\pm$ 0.0036 & 0.9212 $\pm$ 0.0009 & 206,895 \\
			GAT-Cluster & 0.7923 $\pm$ 0.0078 & 0.8985 $\pm$ 0.0022& 1,540,848 \\ 
			GAT-NeighborSampling & 0.7945 $\pm$ 0.0059 & - & 1,751,574 \\
			GraphSAINT \cite{zeng2019graphsaint} & 0.8027 $\pm$ 0.0026 & - & 331,661 \\
			DeeperGCN  \cite{li2020deepergcn} & 0.8090 $\pm$ 0.0020 & 0.9238 $\pm$ 0.0009 & 253,743  \\
			\hline 
			{\bf UniMP} &  \bf{0.8256} $\pm$ \bf{0.0031} & \bf{ 0.9308} $\pm$ \bf{0.0017} & 1,475,605 \\
			\hline
		\end{tabular}}
	\end{center}
	\caption{Results for ogbn-products}
	\label{table:ret_products}
	\vspace{-4mm}
\end{table}

\begin{table}[htbp]
	\setlength{\abovecaptionskip}{0.cm}
	\setlength{\belowcaptionskip}{-.2cm}
	\begin{center}
     	\resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc}
		    \hline 
			{\bf Model}  & {\bf Test ROC-AUC} &  {\bf Validation ROC-AUC	}  &  {\bf Params} \\
			\hline 
			GaAN \cite{zhang2018gaan} & 0.7803 $\pm$ 0.0073 & - & - \\
			GeniePath-BS \cite{liu2020bandit} & 0.7825 $\pm$ 0.0035& - & 316,754 \\ 
			MWE-DGCN& 0.8436 $\pm$ 0.0065 & 0.8973$\pm$ 0.0057&538,544 \\
			DeepGCN \cite{li2019deepgcns}& 0.8496 $\pm$ 0.0028 & 0.8921 $\pm$ 0.0011 & 2,374,456 \\
			DeeperGCN \cite{li2020deepergcn}  & 0.8580 $\pm$ 0.0017 & 0.9106 $\pm$ 0.0016 & 2,374,568  \\
			\hline 
			{\bf UniMP} & \bf{0.8642} $\pm$\bf{0.0008} & \bf{0.9175} $\pm$ \bf{0.0007} &1,909,104\\
			\hline 
		\end{tabular}}
	\end{center}
	\caption{Results for ogbn-proteins}
	\label{table:ret_proteins}
	\vspace{-4mm}
\end{table}

\begin{table}[htbp]
	\setlength{\abovecaptionskip}{0.cm}
	\setlength{\belowcaptionskip}{-.2cm}
	\begin{center}
	    \resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc}
		    \hline 
			{\bf Model}  & {\bf Test Accuracy	} & {\bf Validation Accuracy}  & {\bf Param} \\
			\hline 
			DeeperGCN \cite{li2020deepergcn} & 0.7192 $\pm$ 0.0016 & 0.7262 $\pm$ 0.0014 &  1,471,506 \\ 
			GaAN \cite{zhang2018gaan} & 0.7197 $\pm$ 0.0024 & - &  1,471,506 \\ 
			DAGNN \cite{liu2020towards} & 0.7209 $\pm$ 0.0025 & -  & 1,751,574 \\
			JKNet \cite{xu2018representation} & 0.7219 $\pm$ 0.0021 & 0.7335 $\pm$ 0.0007 &  331,661 \\
			GCNII \cite{chen2020simple}  & 0.7274 $\pm$ 0.0016  & - & 2,148,648  \\
\hline 
			{\bf UniMP} & \bf{0.7311} $\pm$ \bf{0.0021} & \bf{0.7450} $\pm$ \bf{0.0005} &473,489 \\
			\hline 
		\end{tabular}}
	\end{center}
	\caption{Results for ogbn-arxiv}
	\label{table:ret_arxiv}
	\vspace{-4mm}
\end{table}

\begin{figure*}[htbp]
\setlength{\belowcaptionskip}{-.2cm}
	\centering
	\begin{minipage}[c]{0.03\textwidth}
	\subcaption{}
	\label{figure:fig1}
	\end{minipage}
	\begin{minipage}[c]{0.20\textwidth}
	    \includegraphics[width=\linewidth]{ogbn-arxiv-label-rate}
	\end{minipage}
    \begin{minipage}[c]{0.03\textwidth}
    \subcaption{}
	\label{figure:fig2}
	\end{minipage}
	\begin{minipage}[c]{0.20\textwidth}
		\includegraphics[width=\linewidth]{arxiv-curve-label-rate}
    \end{minipage}
    \begin{minipage}[c]{0.03\textwidth}
    \subcaption{}
	\label{figure:fig3}
	\end{minipage}
	\begin{minipage}[c]{0.20\textwidth}
		\includegraphics[width=\linewidth]{arxiv-test-label-rate}
	\end{minipage}
	\begin{minipage}[c]{0.03\textwidth}
    \subcaption{}
	\label{figure:fig4}
	\end{minipage}
	\begin{minipage}[c]{0.20\textwidth}
		\includegraphics[width=\linewidth]{ogbn-arxiv-edge-accuracy}
	\end{minipage}
	\caption{Exploration of how label coverage affects label propagation: (a) Training with different label\_rate; (b) Training with different proportion of labeled data; (c) Testing with different label\_rate; (d) Test accuracy with different neighbors. }
		\label{figure:fig_curves}
	
\end{figure*}

 
\begin{table*}[htbp]
	\centering
   
	\small
	\setlength{\tabcolsep}{5mm}{
	\begin{tabular}{c|c|c|c|c}
		\hline
		&        & \multicolumn{3}{c}{\textbf{Datasets}}                      \\ \cline{3-5} 
		\multirow{-2}{*}{\textbf{Inputs}}        & \multirow{-2}{*}{\textbf{Model}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}ogbn-products   \\ Test ACC\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}ogbn-proteins    \\  Test ROC-AUC\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}ogbn-arxiv \\ Test ACC\end{tabular}}} \\ \hline
		$\mathbf{X}$   & Multilayer Perceptron    &       0.6106 $\pm$ 0.0008                    &          0.7204 $\pm$ 0.0048    &               0.5765 $\pm$ 0.0012 \\ \hline
		& GCN    &    0.7851 $\pm$ 0.0011         &           0.8265 $\pm$ 0.0008            &   0.7218 $\pm$ 0.0014                  \\ 
		& GAT    &     0.8002 $\pm$ 0.0063                &    0.8376 $\pm$ 0.0007                   &     0.7246 $\pm$ 0.0013              \\ 
		\multirow{-3}{*}{$\mathbf{X,A}$}         & Graph Transformer                      &   0.8137 $\pm$ 0.0047                  &           0.8347  $\pm$  0.0014           &       0.7292 $\pm$ 0.0010             \\ \hline
		& GCN    &          0.7832 $\pm$ 0.0013           &   0.8083 $\pm$ 0.0021                   &                      0.7018  $\pm$ 0.0009        \\ 
		& GAT    &               0.7751 $\pm$ 0.0054      &  0.8247 $\pm$ 0.0033                    &       0.7055 $\pm$ 0.0012            \\
		\multirow{-3}{*}{$\mathbf{A,\hat{Y}}$}   & Graph Transformer &          0.7987 $\pm$ 0.0104           &                  0.8160 $\pm$ 0.0007    &       0.7090 $\pm$ 0.0007 \\ \hline
		& GCN    &            0.7987 $\pm$ 0.0104     &     0.8247 $\pm$ 0.0032             &   0.7264 $\pm$ 0.0003        \\
		& GAT    &      0.8193 $\pm$ 0.0017            &         0.8556  $\pm$ 0.0009              &               0.7278 $\pm$ 0.0009    \\
		& Graph Transformer  &       \bf{0.8256} $\pm$ \bf{0.0031}              &    0.8560 $\pm$ 0.0003                   &   \bf{0.7311} $\pm$ \bf{ 0.0021 }   \\
		\multirow{-4}{*}{$\mathbf{X,A,\hat{Y}}$}  & \text{\ \ \ \ \ \  \ \ \ \ }$\llcorner$ w/ Edge Feature                      & *  &    \bf{0.8642} $\pm$ \bf{0.0008}    &  * \\
		\hline
	\end{tabular}
	}

	\caption{This is the ablation studies on models with different inputs, where
$\mathbf{X}$ denotes the nodes features, $\mathbf{A}$ is the graph adjacent matrix and $\mathbf{\hat{Y}}$ is the observed labels. In \emph{ogbn-proteins}, nodes features are not provided initially. We average the edge features as their nodes features and provide the experimental result of Transformer without edge features for fair comparison in this experiment, which is slightly different from Table \ref{table:ret_proteins}.
}
 \label{table:ablation-stuidies}
 \vspace{-4mm}
\end{table*}
 
 \subsection{Ablation Studies}
 In this section, to better identify the improvements from different components of our proposed model, we conduct extensive studies with the following four aspects:
 


\begin{itemize}
 	\item Firstly, we apply the masked label prediction strategy on kinds of GNNS to show the effectiveness and robustness of incorporation LPA and GNN, shown in Table \ref{table:ablation-stuidies}.
 	\item In order to get a more practical and effective solution to apply masked label prediction strategy, we tune the label\_rate during training and inference to explore the relationship between label coverage and GNNs performance, shown in Figure \ref{figure:fig_curves}.
\item We also analyze how LPA affects the GNN to make it performs better, shown in Figure \ref{fig:attenion_image}.
 	\item Furthermore, in Table \ref{table:ablation in unimp_v1}, we provide more ablation studies on UniMP, compared with GAT, showing the superiority of our model. 
\end{itemize}
 
 
 \subsubsection{Graph Neural Networks with Different Inputs}
 In Table \ref{table:ablation-stuidies}, we apply masked label prediction on kinds of GNNs to improve their performance. Firstly, we re-implement classical GNN methods like GCN and GAT, following the same sampling methods and model setting shown in Table \ref{table:params}. The hidden size of GCN is head\_num*hidden\_size since it doesn't have head attention. Secondly, we change different inputs for these models to study the effectiveness of feature and label propagation, using our {\bf masked label prediction} to train the models with partial nodes label $\mathbf{\hat{Y}}$ as input.
  


Row 4 in Table \ref{table:ablation-stuidies} shows that only with $\mathbf{\hat{Y}}$ and  $\mathbf{A}$ as input, GNNs still work well in all three datasets, outperforming those MLP model only given $\mathbf{X}$. This implies that one's label relies heavily on its neighborhood instead of its feature. Comparing Row 3 and 5 in Table \ref{table:ablation-stuidies}, models with $\mathbf{X}$, $\mathbf{A}$ and $\mathbf{\hat{Y}}$ outperform the models with $\mathbf{X}$ and $\mathbf{A}$, which indicates that it's a waste of information for GNNs in semi-supervised classification when they making predictions without incorporating the ground truth train labels $\mathbf{\hat{Y}}$. Row 3-5 in Table \ref{table:ablation-stuidies} also show that our Graph Transformer can outperform GAT, GCN with different input settings.


\subsubsection{Relation between Label Coverage and Performance}

 Although we have verified the effectiveness of using this strategy to combine LPA and GNN, the relation between label coverage and its impact on GNNs performance remains uncertain. Therefore, shown in Figure \ref{figure:fig_curves}, we conduct more experiments in \emph{ogbn-arxiv} to investigate their relationship in the following different scenarios:
\begin{itemize}
	\item In Figure \ref{figure:fig1}, we train UniMP using $\mathbf{X, \hat{Y}, A}$ as inputs. We tune the input label\_rate which is the hyper-parameter of masked label prediction task and display the validation and test accuracy. Our model achieves better performance when label\_rate is about 0.625.
	\item Figure \ref{figure:fig2} describes the correlation between the proportion of training data and the effectiveness of label propagation. We fix the input label\_rate with 0.625. The only change is the training data proportion. It's common sense that with the increased amount of training data, the performance is gradually improving. And the model with label propagation ${\mathbf{{\hat{Y}}}}$ can gain greater benefits from increasing labeled data proportion.
	\item Our unified model always masks a part of the training input label and tries to recover them. But in the inference stage, our model utilizes all training labels for predictions, which is slightly inconsistent with the one in training. In Figure \ref{figure:fig3}, we fix our input label\_rate with 0.625 during training and perform different input label\_rate in inference. the training stage, It's found that UniMP might have worse performance (less than 0.70) than the baseline (about 0.72) when lowering the label\_rate during prediction. However, when the label\_rate climbs up, the performance can boost up to 0.73.
	
	\item In Figure \ref{figure:fig4}, we calculate the accuracy for unlabeled nodes grouped by the number of neighbors. The experimental result shows that nodes with more neighbors have higher accuracy. And the model with label propagation ${\mathbf{\hat{Y}}}$ can always have improvements even with different numbers of training neighbors.
\end{itemize}


\subsubsection{Measuring the Connection between Nodes}


In Figure \ref{fig:attenion_image}, we analyze how LPA affects GNN to make it perform better. Wang~\shortcite{wang2019unifying} has pointed out that using LPA for GCN during training can enable nodes within the same class/label to connect more strongly, increasing the accuracy (ACC) of model's prediction. Our model can be regarded as an upgraded version of them, using LPA in both training and testing time for our Graph Transformer. Therefore, we try to experimentally verify the above idea based on our model.

\begin{equation}
\resizebox{.80\linewidth}{!}{$
\begin{aligned}
MSF =\dfrac{1}{N}\sum_{i=1}^{N} \log \bigg ( 1+ \sum_{j\in \mathcal{N}(i)_{pos}} \sum_{k\in \mathcal{N}(i)_{neg}}e^{\alpha_{i,j}}-e^{\alpha_{i,k}} \bigg)\\
\end{aligned}
\label{eq:11}$}
\end{equation}

We use the Margin Similarity Function (MSF) as shown in Equation \ref{eq:11} to reflect the connection tightness between nodes within the same class (the higher scores, the stronger connection they have. We conduct the experiment on \emph{ogbn-arxiv}. And as shown in Figure \ref{fig:attenion_image}, the ACC of models' prediction is proportional to Margin Similarity. Unifying feature and label propagation can further strengthen their connection, improving their ACC. Moreover, our Graph Transformer outperforms GAT in both connection tightness and ACC with different inputs.

\begin{figure}[htpb]
\centering
	\begin{subfigure}[b]{.48\columnwidth}
		\includegraphics[width=\linewidth,trim= 0 0.2cm 0 0.2cm,clip]{attention_accuracy_arxiv}
		\label{figure:attention_accuracy}
	\end{subfigure}
	\begin{subfigure}[b]{.48\columnwidth}
		\includegraphics[width=\linewidth,trim= 0 0.2cm 0 0.2cm,clip]{attention_margin_arxiv}
		\label{figure:attention_margin}
	\end{subfigure}
\setlength{\belowcaptionskip}{-.4cm}
	\caption{Correlation between accuracy and margin similarity between neighbors.}
	\label{fig:attenion_image}
\end{figure}



\subsubsection{More Ablation Studies on UniMP}

Finally, we provide more ablation studies on our UniMP model, compared with GAT, from the following 4 aspects: 
(1) vanilla transformer with dot-product attention or GAT with sum attention; 
(2) simple residual or gated residual;
(3) with train labels as inputs;
(4) with train and validation labels as inputs.
As shown in Table \ref{table:ablation in unimp_v1}, we can find that dot-product attention can outperform sum attention, since dot-product provides more interactions between nodes. Besides, residual and gated residual can also strengthen the GNNs with shallow layers. Moreover, our unified model can take the additional validation labels as input to further boost model's performance without more training steps. Therefore, when we apply the model to the real scene, and the labeled data are accumulated progressively, the accuracy of the unlabeled data can keep increasing without training our model from scratch, while other GNNs without explicit label modeling can't fully utilize the benefits of additional labels. 


\begin{table}[htbp]
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc}
\hline
 Model                                                                                                               & \textbf{ogbn-prdouct}                                                     & \textbf{ogbn-arxiv}                                                       \\ \hline
\begin{tabular}[l]{@{}l@{}}GAT (sum attention)\\ $\llcorner$ w/ residual \\ $\llcorner$ w/ gated residual\end{tabular}                                          & \begin{tabular}[c]{@{}c@{}} 0.8002 \\ 0.8033\\ 0.8050\end{tabular}                   & \begin{tabular}[c]{@{}c@{}} 0.7246 \\ 0.7265\\ 0.7272\end{tabular}                   \\ \hline
\begin{tabular}[l]{@{}l@{}}Transformer (dot-product)\\ $\llcorner$ w/ residual \\ $\llcorner$ w/ gated residual\\ ~~~~$\llcorner$ w/ train label (UniMP) \\  ~~~~~~~~$\llcorner$ w/ validation labels\end{tabular} & \begin{tabular}[c]{@{}c@{}} 0.8091\\ 0.8125\\ 0.8137\\ 0.8256\\ \bf{0.8312} \end{tabular} & \begin{tabular}[c]{@{}c@{}} 0.7259 \\ 0.7271 \\ 0.7292\\ 0.7311\\ \bf{0.7377}\end{tabular} \\ \hline
\end{tabular}}
\caption{Ablation studies in UniMP, compared with GAT}
	\label{table:ablation in unimp_v1}
	\vspace{-4mm}
\end{table}
