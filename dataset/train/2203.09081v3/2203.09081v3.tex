\documentclass{article}




\usepackage[final,nonatbib]{neurips_2022}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{xcolor}         

\usepackage[breaklinks=true,colorlinks,bookmarks=true]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{epsfig}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{marvosym}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}





\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\m}{\mathbf{m}}

\newcommand{\F}{{\mathbf{F}}}
\newcommand{\W}{{\mathbf{W}}}
\newcommand{\U}{{\mathbf{U}}}
\newcommand{\M}{{\mathbf{M}}}
\renewcommand{\H}{{\mathbf{H}}}

\newcommand{\X}{\bm{\mathbf{X}}}
\newcommand{\Y}{\bm{\mathbf{Y}}}
\newcommand{\A}{\bm{\mathbf{A}}}
\renewcommand{\P}{\bm{\mathbf{P}}}

\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}
\newcommand{\prox}{\mbox{prox}}
\newcommand{\sign}{\mbox{sign}}
\newcommand{\argmin}{\mbox{argmin}}
\newcommand{\argmax}{\mbox{argmax}}
\newcommand{\proj}{\operatorname{Proj}}




\begin{document}


\title{{Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?}}


\author{Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, Dacheng Tao\vspace{1mm} \\
\small{JD Explore Academy, Beijing, China}\\
\small{Key Lab. of Machine Perception (MoE), School of Intelligence Science and Technology, Peking University}\\
\small{State Key Lab of CAD\&CG, Zhejiang University}\\
\small{Institute for Artificial Intelligence, Peking University}\\
\small{Pazhou Laboratory, Guangzhou, China}\\
}

\maketitle





{\let\thefootnote\relax\footnotetext{: corresponding author.}}

\vspace{-4mm}
\begin{abstract}
	\vspace{-2mm}
	Modern deep neural networks for classification usually jointly learn a backbone for representation and a linear classifier to output the logit of each class. A recent study has shown a phenomenon called \emph{neural collapse} that the within-class means of features and the classifier vectors converge to the vertices of a simplex equiangular tight frame (ETF) at the terminal phase of training on a balanced dataset. Since the ETF geometric structure maximally separates the pair-wise angles of all classes in the classifier, it is natural to raise the question, \emph{why do we spend an effort to learn a classifier when we know its optimal geometric structure?} In this paper, we study the potential of learning a neural network for classification with the classifier randomly initialized as an ETF and fixed during training. Our analytical work based on the layer-peeled model indicates that the feature learning with a fixed ETF classifier naturally leads to the neural collapse state even when the dataset is imbalanced among classes. We further show that in this case the cross entropy (CE) loss is not necessary and can be replaced by a simple squared loss that shares the same global optimality but enjoys a better convergence property. Our experimental results show that our method is able to bring significant improvements with faster convergence on multiple imbalanced datasets. Code address: \href{https://github.com/NeuralCollapseApplications/ImbalancedLearning}{link}.
	
	


\end{abstract}



\vspace{-3mm}

\section{Introduction}
\vspace{-2mm}
\label{intro}
Modern deep neural networks for classification are composed of a backbone network to extract features, and a linear classifier in the last layer to predict the logit of each class. As widely adopted in various deep learning fields, the linear classifier has been learnable jointly with the backbone network using the cross entropy (CE) loss function for classification problems \cite{lecun2015deep,he2016deep,huang2017densely}. 




\iffalse
\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.35\linewidth]{fig1.pdf}
		\vspace{-2mm}
		\caption{An illustration of a simplex equiangular tight frame when  and . The black spheres are the vertices of the ETF. The  ``+'' and asterisks signs in different colors refer to features and classifier vectors of different classes, respectively. Neural collapse depicts the phenomenon that the last-layer feature means and the corresponding classifier vectors converge to the same ETF.}
		\label{fig1}
	\end{center}
	\vskip -0.2in
\end{figure}
\fi






A recent study reveals a very symmetric phenomenon named neural collapse, that the last-layer features of the same class will collapse to their within-class mean, and the within-class means of all classes and their corresponding classifier vectors will converge to the vertices of a simplex equiangular tight frame (ETF) at the terminal phase of training on a balanced dataset \cite{papyan2020prevalence}. A simplex ETF describes a geometric structure that maximally separates the pair-wise angles of  vectors in , , and all vectors have an equal  norm. As shown in Figure~\ref{fig1}, when , the ETF reduces to a regular simplex. Following studies focus on unveiling the physics of such a phenomenon 
based on the layer-peeled model (LPM) \cite{fang2021exploring} or unconstrained feature model \cite{mixon2020neural}. They peel off the topmost layer, so the features are independent variables to optimize \cite{weinan2020emergence}. Although this toy model is impracticable for application, it inherits the nature of feature and classifier learning in real deep networks.
\begin{wrapfigure}[21]{tr}{0.3\textwidth}
\vspace{-1mm}
	\centering
	\includegraphics[width=0.9\linewidth]{fig1.pdf}
	\vspace{-1mm}
	\caption{An illustration of a simplex equiangular tight frame when  and . The black spheres are the vertices of the ETF. The  ``+'' and ``'' signs in different colors refer to features and classifier vectors of different classes, respectively. Neural collapse indicates that the features and classifier vectors are aligned with the same simplex ETF.}  
	\label{fig1}
\end{wrapfigure}
It has been shown that the optimality of LPM satisfies neural collapse under the CE loss with constraints \cite{lu2020neural,fang2021exploring,weinan2020emergence,graf2021dissecting}, regularization \cite{zhu2021geometric}, or even no explicit constraint \cite{ji2021unconstrained}. 
The other studies turn to analyze the mean squared error (MSE) loss and also derive the neural collapse solution as global optimality \cite{mixon2020neural,han2021neural,poggio2020explicit,tirer2022extended}. However, all these theoretical results are only valid for balanced training. 








Albeit neural collapse is an observed empirical result and has not been entirely understood from a theoretical point, it is intuitive and sensible. Features collapsing to their means minimize the within-class variability, while the ETF geometric structure maximizes the between-class variability, so the Fisher discriminant ratio  \cite{fisher1936use,rao1948utilization} is maximized, which corresponds to an optimal linearly separable state for classification. Several works have shown that neural networks progressively increase the linear separability during training \cite{oyallon2017building,papyan2020traces,zarka2020separation}. Therefore, a classifier that satisfies the ETF structure should be a ``correct'' answer for network training. However, as pointed out by \cite{fang2021exploring}, in the training on an imbalanced dataset, the classifier vectors of minor classes will be merged, termed as \emph{minority collapse}, which breaks up the ETF structure and deteriorates the performance on test data. So a learnable classifier does not always lead to neural collapse when training on imbalanced data. With the spirit of \emph{Occam’s Razor} \cite{smith1980bayes}, we raise and study the question: 

\emph{Do we really need to learn a linear classifier at the end of deep neural network?} 





Since a backbone network is usually over-parameterized and can align the output feature with any direction, what makes classification effective should be the feature-classifier geometric structure, instead of their specific directions. In this paper, we propose to initialize the classifier using a randomly generated ETF and fix it during training, \emph{i.e.,} only the backbone is learnable. It turns out that this practice makes the LPM more mathematically tractable. Our analytical work indicates that even in the training on an imbalanced dataset, the features will converge to the vertices of the same ETF formed by the fixed classifier, \emph{i.e.,} neural collapse inherently emerges regardless of class balance. 



We further analyze the cross entropy loss function. We point out the reason for minority collapse from the perspective of imbalanced gradients with respect to a learnable classifier. As a comparison, our fixed ETF classifier does not suffer from this dilemma. We also show that the gradients with respect to the last-layer feature are composed of a ``pull'' term that pulls the feature into the direction 
of its corresponding classifier vector of the same class,
and a ``push'' term that pushes it away from the other classifier vectors. It is the pull-push mechanism in the CE loss that makes the 
features collapsed and separated. 
However, in our case, the classifier has been fixed as a ``correct'' answer. As a result, the ``pull'' term is always an accurate gradient towards the optimality, and we no longer need the ``push'' term that may cause inaccurate gradients. Inspired by the analyses, we further propose a simple squared loss, named dot-regression (DR) loss, which shares a similar ``pull'' gradient with the CE loss, but does not have any ``push'' term. It has the same 
neural collapse global optimality,
but is proved to enjoy a  better convergence property than the CE loss. 


The contributions of this study can be listed as follows:
\begin{itemize}
	\vspace{-1mm}
\item We theoretically show that the neural collapse optimality can be induced even in imbalanced learning as long as the learnable classifier is fixed as a simplex ETF. 
	\item Our analysis indicates that the broken neural collapse in imbalanced learning is caused by the imbalanced gradients \emph{w.r.t.} a learnable classifier. 
	\item We point out that our fixed ETF classifier no longer relies on the ``push'' gradient \emph{w.r.t} feature that is crucial for the CE loss but may be inaccurate. We further propose a new loss function with only ``pull'' gradient. Its better convergence property is theoretically proved.  
	\item Statistical results show that the model with our method converges more closely to neural collapse. In experiments, our method is able to improve the performance of classification on multiple imbalanced datasets by a significant margin. Our method enjoys a faster convergence compared with the traditional learnable classifier with the CE loss. As an extension, we find that our method also works for fine-grained classification. 
\end{itemize}













\section{Related Work}
\vspace{-2mm}




\textbf{Neural Network for Classification.} Despite the success of deep learning for classification \cite{le2014distributed,vgg,he2016deep,huang2017densely,yang2018convolutional,hershey2017cnn}, a theoretical foundation that can guide the design of neural architecture still remains an open problem and inspires many studies from different perspectives \cite{ge2018learning,mei2018mean,li2018optimization,yang2019dynamical,arora2019fine,NTK,xiao2020disentangling}. The last-layer linear classifier has been relatively transparent. In most cases, it is jointly optimized with the backbone network. In long-tailed classification, a two-stage method of training backbone and classifier successively is preferred \cite{Cao_nips,kang2019decoupling,zhong2021improving}. Some prior studies have shown that fixing the classifier as regular polytopes \cite{pernici2019maximally,pernici2021regular} and Hadamard matrix \cite{hoffer2018fix} does not harm the classification performance. 
\cite{zhu2021geometric} tries the practice of fixing the classifier as a simplex ETF in experiment, but does not provide any benefit except for the saved computation cost.
We also study the potential of using a fixed classifier throughout the training. But different from these studies, our proposed practice is proved to be beneficial to imbalanced learning by both theoretical and experimental results.














\vspace{-1mm}

\textbf{Neural Collapse.} In \cite{papyan2020prevalence}, neural collapse was observed at the terminal phase of training on a balanced dataset. 
Albeit the phenomenon is intuitive, its reason has not been entirely understood, which inspires several lines of theoretical work on it.  \cite{papyan2020prevalence} proves that if features satisfy neural collapse, the optimal classifier vectors under the MSE loss will also converge to neural collapse based on \cite{webb1990optimised}. Some studies turn to a simplified model that only considers the last-layer features and classifier as independent variables. They prove that neural collapse emerges under the CE loss with proper constraints or regularizations \cite{weinan2020emergence,fang2021exploring,lu2020neural,zhu2021geometric,ji2021unconstrained,graf2021dissecting}. Other studies focus on the neural collapse under the MSE loss \cite{mixon2020neural,han2021neural,poggio2020explicit,tirer2022extended,zhou2022optimization,rangamani2022neural}. \cite{ergen2021revealing} proposes a convex formulation for a particular network and also explains neural collapse. \emph{\textbf{However, current results are only valid for balanced training.}}  Inspired by neural collapse, \cite{xie2022neural} modifies the CE loss for imbalanced learning, but still cannot rigorously induce neural collapse. 
Our work differs from these studies in that \emph{\textbf{we theoretically show that neural collapse can inherently happen even in imbalanced learning}}. We also derive a new loss function that theoretically enjoys a better convergence property than the CE loss.








\vspace{-1.5mm}
\section{Preliminaries}
\vspace{-1mm}

\subsection{Neural Collapse}
\vspace{-1mm}
\cite{papyan2020prevalence} reveals the neural collapse phenomenon, that the last-layer features converge to their within-class means, and the within-class means together with the classifier vectors collapse to the vertices of a simplex equiangular tight frame at the terminal phase of training on a balanced dataset.
\begin{definition}[Simplex Equiangular Tight Frame]
	\label{ETF}
	A collection of vectors , , , is said to be a simplex equiangular tight frame if:
	
	where ,  allows a rotation and satisfies ,  is the identity matrix, and  is an all-ones vector. 
\end{definition}

All vectors in a simplex ETF have an equal  norm and the same pair-wise angle, \emph{i.e.,}

where  equals to  when  and  otherwise. The pair-wise angle  is the maximal equiangular separation of  vectors in  \cite{papyan2020prevalence}. 


Then the neural collapse (NC) phenomenon can be formally described as:



\textbf{(NC1)} Within-class variability of the last-layer features collapse: , and , where  is the last-layer feature of the -th sample in the -th class, and  is the within-class mean of the last-layer features in the -th class;

\textbf{(NC2)} Convergence to a simplex ETF: , satisfies Eq. (\ref{mimj}), where   is the global mean of the last-layer features, \emph{i.e.,} ;

\textbf{(NC3)} Self duality: , where  is the classifier vector of the -th class;

\textbf{(NC4)} Simplification to the nearest class center prediction: , where  is the last-layer feature of a sample to predict for classification.


\subsection{Layer-peeled Model}

Neural collapse has attracted a lot of researchers to unveil the physics of such an elegant phenomenon. Currently most studies target the cross entropy loss function that is widely used in deep learning for classification. It is defined as:

where  is the feature output by a  backbone network with input ,  is a learnable classifier, and  is the class label of . 

However, deep neural network as a highly interacted function is difficult to analyze due to its non-convexity. A simplification is always necessary to make tractable analysis. For neural collapse, current studies often consider the case where only the last-layer features and classifier are learnable without considering the layers in the backbone network. It is termed as layer-peeled model (LPM) \cite{fang2021exploring}, and can be formulated as\footnote{Note that the sample-wise constraint of  and the class-wise constraint of  in Eq. (\ref{LP}) are more strict than the overall constraints in \cite{fang2021exploring}, but are still active with the same global optimality. The model is also known as unconstrained feature model \cite{mixon2020neural,zhu2021geometric} when the norm constraints are omitted or replaced by regularizations.}:

where  is the feature of the -th sample in the -th class,  is a learnable classifier,  is the cross entropy loss function defined in Eq. (\ref{CE-loss}),  is the number of samples, and  and  are the  norm constraints for feature  and classifier vector , respectively. 

Albeit the LPM cannot be applied to real application problems, it serves as an analytical tool and inherits the learning behaviors of the last-layer features and classifier in deep neural network. Actually, the learning of a  backbone network is through the multiplication between the Jacobian and the gradient with respect to the last-layer features, \emph{i.e.}, , where  denotes the parameters in the backbone network, and  is the collection of the last-layer features. 



It has been shown that the global optimality for the LPM in Eq. (\ref{LP}) satisfies neural collapse in the balanced case \cite{fang2021exploring,graf2021dissecting}. The CE loss with regularization also has a similar conclusion \cite{zhu2021geometric}. \emph{However, the results in current studies are only valid for training on a balanced dataset}, \emph{i.e.,} . 
	
	



\iffalse
\begin{theorem}[Layer-peeled model in the balanced case \cite{fang2021exploring,graf2021dissecting}]
	\label{LPM}
	In the balanced case, \emph{i.e.,} , any global minimizer  and  of Eq. (\ref{LP}) obeys to neural collapse, \emph{i.e.,}
	
	where  forms a simplex equiangular tight frame as defined in Eq. (\ref{ETF_M}).
\end{theorem}
\begin{proof} 
	Please refer to \cite{fang2021exploring,graf2021dissecting} for the proof.
	 
\end{proof}
\fi






\section{Main Results}


\subsection{ETF Classifier}

From the neural collapse solution, NC1 minimizes the within-class covariance , and NC2 maximizes the between-class covariance  by the ETF structure. So the Fisher discriminant ratio, defined as , is maximized, which can measure the linear separability and has been used to extract features to replace the CE loss \cite{stuhlsatz2012feature}. So we deem an ETF as the optimal geometric structure for the linear classifier. Considering that a backbone network is usually over-parameterized and can produce features aligned with any direction, in this paper, we study the potential of learning a network with the linear classifier fixed as an ETF, named ETF classifier. 

Concretely, we initialize the linear classifier  as a random simplex ETF by Eq. (\ref{ETF_M}) with a scaling of  as the fixed length for each classifier vector, and only optimize the features . In this case, the layer-peeled model (LPM) in Eq. (\ref{LP}) reduces to the following problem:

where  is the fixed classifier as a simplex ETF and satisfies:

where  equals to  when  and  otherwise. 

We observe that this practice decouples the multiplied learnable variables  and  of LPM in Eq. (\ref{LP}), and makes the model in Eq. (\ref{LP_noW}) a convex problem that is more mathematically tractable. We term the decoupled LPM in Eq. (\ref{LP_noW}) as \textbf{DLPM} for short. We have the global optimality for DLPM in the imbalanced case with the ETF classifier in the following theorem. 
\begin{theorem}
	\label{ETF_classifier}
	No matter the data distribution is balanced or not among classes (it is allowed that , , such that ), any global minimizer  of Eq. (\ref{LP_noW}) converges to a simplex ETF with the same direction as  and a length of , \emph{i.e.,}
which means that the neural collapse phenomenon emerges regardless of class balance. 
\end{theorem}
\begin{proof} 
	Please refer to Appendix \ref{app_1} for our proof.   
\end{proof}

\begin{remark}
	\label{remark1}
{As observed in \cite{fang2021exploring}, LPM in the extreme imbalance case would suffer from ``\emph{minority collapse}'', where the classifier vectors of minor classes are close or even merged into the same vector, which explains the deteriorated classification performance of imbalanced training. As a comparison, Theorem \ref{ETF_classifier} shows that DLPM with our ETF classifier can inherently produce the neural collapse solution even in the training on imbalanced data. }
\end{remark}


Although our practice of using a fixed ETF classifier simplifies the problem, it actually brings theoretical merits that also get validated in our long-tailed classification experiments. 



\subsection{Rethinking the Cross Entropy Loss}
\vspace{-1mm}


In this subsection, we rethink the CE loss  from the perspective of gradients with respect to both feature and classifier to analyze its learning behaviors. 

\vspace{-0.5mm}
\subsubsection{Gradient \emph{w.r.t} Classifier}
\vspace{-0.5mm}

We first analyze the gradient of  \emph{w.r.t} a learnable classifier :

where  is the predicted probability that  belongs to the -th class. It is calculated by the softmax function and takes the following form in the CE loss:

We make the following definitions:

where 




\iffalse

\fi
It reveals that the negative gradient \emph{w.r.t}   is decomposed into two terms. The ``pull'' term  pulls  towards the directions of the features of the same class, \emph{i.e.}, , while the ``push'' term  pushes  away from the directions of the features of the other classes, \emph{i.e.}, , . 

\begin{remark}
	\label{remark2}
Note that  and  have different magnitudes. When the dataset is in extreme imbalance, the direction of  for a minor class is dominated by the push term  because  is small, while  is large. In this case, the classifier vectors of two minor classes are optimized towards nearly the same direction and would be close or merged after training. {So the deteriorated performance of classification with imbalanced training data results from the imbalanced gradient \emph{w.r.t} a learnable classifier in the CE loss. As a comparison, our proposed practice of fixing the classifier as an ETF does not suffer from this dilemma. }
\end{remark}


\subsubsection{Gradient \emph{w.r.t} Feature}

The gradient of CE loss in Eq. (\ref{CE-loss}) with respect to  is:

where  is the class label of , and  is the probability that  belongs to the -th class as defined in  Eq. (\ref{p_i}). 

It is shown that Eq. (\ref{grad_h}) has a similar form to that of Eq. (\ref{grad_w}). The negative gradient  can also be decomposed as the addition of ``pull'' and ``push'' terms defined as:

\iffalse

\fi
The ``pull'' term  pulls  towards the classifier vector of the same class, \emph{i.e.,} , while the ``push'' term  pushes  away from the other classifier vectors,  \emph{i.e.,} , .

\begin{remark}
	\label{remark3}
We note that Eq. (\ref{F_w}) and Eq. (\ref{F_h}) have a similar form and are very symmetric. The ``pull'' terms  and  force  and  of the same class to converge to the same direction, which is in line with (NC1) and (NC3). The ``push'' terms  and   make them of different classes separated, which is in line with (NC2). Generally, (NC1)-(NC3) can lead to (NC4). {We remark that it is the ``pull-push'' mechanism in the CE loss that leads to the neural collapse solution in the case of training on balanced data. }
\end{remark}



The ``push'' gradient  in Eq. (\ref{F_h}) is crucial for a learnable classifier. However, in our case where the classifier has been fixed as an ETF, the ``push'' gradient  is unnecessary. As shown in Figure  \ref{fig_grad_h}, the ``pull'' gradient  in our case is always accurate towards the optimality , which corresponds to the neural collapse solution. But the ``push'' gradient does not necessarily direct to the optimality. 
So, we no longer rely on the ``push'' term  that may cause deviation. It inspires us to develop a new loss function specified for our ETF classifier. 

 








\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.77\linewidth]{grad_h_size_pullpush.pdf}
		\vspace{-2.mm}
		\caption{An empirical comparison of gradient directions \emph{w.r.t} an  (belongs to the 3-rd class) of the CE loss (left) and our proposed DR loss (right). Because  is close to , the gradient of the CE loss is dominated by   whose direction deviates from the optimality , while the gradient of DR only has a ``pull'' term and always directs to .}
		\label{fig_grad_h}
	\end{center}
	\vspace{-3mm}
\end{figure}

\vspace{-1mm}
\subsection{Dot-Regression Loss}
We consider the following squared loss function:

where  is the class label of ,  is a fixed ETF classifier, and  and  are the -norm constraints (predefined and \emph{not learnable}) given in Eq. (\ref{LP_noW}). It performs regression between the dot product of  and  and the multiplication of their lengths. We term this simple loss function as dot-regression (DR) loss. Its gradient with respect to  takes the form:

where  denotes the cosine similarity between  and . 





We see that the gradient has a similar form to the first term in Eq. (\ref{grad_h}), which plays the role of ``pull'', but has no ``push'' term. It is easy to identify that if we replace the CE loss in the decoupled layer-peeled model (DLPM) defined in Eq. (\ref{LP_noW}) with the DR loss defined in Eq. (\ref{DR_loss}), the same global minimizer Eq. (\ref{LPNOW_opt}) still holds. The global optimality happens when  and  accordingly. Then we give a formal analysis of the convergence properties of both CE and DR loss functions in the DLPM. 




\begin{definition}\label{def2}
	Given ,  for any  satisfying   and  the regularity number of function  is defined by the convergence rate of the projected gradient method. That is, there exists    such that:
	
	where  is the learning rate such that  is as small as possible.
\end{definition}
Note that  is decided by  For many problems, we cannot find its uniform upper bound . The smaller  is, the better property the loss function has. 






\begin{theorem}
	\label{reg_dot_loss}
	Assume that given a small , when ,  defined in Eq. (\ref{p_i}) satisfies that\footnote{Its rational lies in that  aligned with  has equal dot products with , , and  is close to .}  , , where  is the label of . When optimizing the DLPM defined in Eq. (\ref{LP_noW}) with the CE and DR loss funcitons, for any fixed learning rate , we have:
	
	where  and  are the -regularity numbers of the CE and DR loss functions, respectively. 
\end{theorem}
\begin{proof} 
	Please refer to Appendix \ref{app_2} for our proof. 	  
\end{proof}

Eq. (\ref{eta}) indicates that the DR loss has a better convergence property when  is close to . In implementations, we train a backbone network with our ETF classifier and DR loss. To induce balanced gradients \emph{w.r.t} the backbone network parameters, we define the length of each classifier vector according to class distribution. Please refer to Appendix \ref{imple} for the complete implementation details. In experiments, we also compare our method with the CE loss weighted by class distribution. 




\iffalse
\subsection{Implementation}

In implementations, we train a backbone network with our proposed ETF classifier and DR loss. We simply perform an  normalization for the features output from the backbone network, which means . Our analytical work has shown that using a fixed ETF classifier does not suffer from the imbalanced gradient \emph{w.r.t} classifier. In implementations, we still need to consider the gradient \emph{w.r.t} the backbone network parameters :

Then we have:

where  denotes the largest singular value of the Jacobian , ,  is the length of the -th classifier vector, and  by Eq. (\ref{DR_grad}). Although we cannot realize a balanced gradient \emph{w.r.t}  among classes, we can balance the upper bound of its gradient norm of each class by controlling . In implementations, we set , which is equivalent to performing a weighted loss function on different classes. In experiments, we also compare our method with the weighted CE loss for fair comparison. 
\fi

\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=1.0\linewidth]{hh_hw_train_val.pdf}
		\vspace{-6mm}
		\caption{Averages and standard deviations of  (two columns on the left), and  (two columns on the right) with (red) and without (black) our method on train set (a) and test set (b).  is the global mean. ,  are within-class means, where  and  are all pairs of \textbf{different} classes. The models are trained on CIFAR-100 with the imbalance ratio  of 0.02, where  and  are the minimal and maximal numbers of training samples in all classes.}
		\label{trainval}
	\end{center}
	\vspace{-3mm}
\end{figure*}

\vspace{-1mm}
\section{Experiments}
\vspace{-1mm}

In experiments, we first make empirical observations of neural collapse convergence in the imbalanced training with and without our method, and then compare the performances on long-tailed classification. As an extension, we surprisingly find that our method is also able to improve the performance of fine-grained classification, which can be deemed as another imbalanced problem where a majority of features are close to each other. The datasets and training details are described in Appendix \ref{details}. 


\subsection{Empirical Results}
\label{empirical result}

Following \cite{papyan2020prevalence}, we calculate statistics during training to show the neural collapse convergence. We first compare the averages and standard deviations of two cosine similarities,  and , where  is the global mean, for all pairs of different classes , with and without our method. As shown in Figure \ref{trainval}, the averages of both  and  converge to a negative value near zero. It is consistent with neural collapse that the feature means or classifier vectors of different classes should have a cosine similarity of . However, their standard deviations are much smaller when our method is used. It indicates that the models with our method converge to neural collapse more closely. 




We further calculate the averages of , , and  in Figure \ref{diag} and \ref{F2} in Appendix \ref{exp}. It reveals that the model using our method generally has a higher  and a lower , which indicates that the feature means and classifier vectors of the same class are better aligned. We observe no advantage of ResNet on STL-10 and DenseNet on CIFAR-100 in Figure \ref{diag} and \ref{F2}. In Table \ref{longtail}, we see that the two cases are right the failure cases, which shows consistency between neural collapse convergence and classification performance.
















\begin{table*}[t]
	\small
\renewcommand\arraystretch{0.8}
	\setlength{\tabcolsep}{0.95pt}
	\caption{An ablation study with ResNet on CIFAR-10 \cite{krizhevsky2009learning} using different classifiers and loss functions. The numbers in the second row denote the imbalance ratio , where  and  are the minimal and maximal numbers of training samples in all classes. Results are the mean of three repeated experiments with different seeds. * indicates that the CE loss is weighted by  to induce class balance, where  is the number of classes and  is the number of samples in class .}
	\label{ablation}
	\vspace{-2mm}
	\begin{center}
\begin{tabular}{l|ccccc|ccccc}
				\toprule
				\multirow{2}{*}{Methods} & \multicolumn{5}{c|}{without Mixup} & \multicolumn{5}{c}{with Mixup}\\
				&   &  &  &  & balanced  & &  &  & &  balanced\\
				\midrule
				{\tiny \textbf{Learnable Classifier + CE}}   & 66.1{\tiny}& 71.0{\tiny}& 77.1{\tiny}  & 87.4{\tiny} & 93.4{\tiny} & 67.3{\tiny}& 72.8{\tiny} & 78.6{\tiny} & 87.7{\tiny}& 93.6{\tiny}\\
				{\tiny \textbf{Learnable Classifier + CE}} & 66.8{\tiny}& 72.1{\tiny} & 77.6{\tiny}  & 87.4{\tiny} & 93.1{\tiny}  & 68.5{\tiny}& 73.9{\tiny} & 79.3{\tiny} & 87.8{\tiny}& 93.2{\tiny}\\
{\tiny \textbf{ETF Classifier +  CE}}   & 60.4{\tiny} & 72.9{\tiny} & 79.5{\tiny} & 87.2{\tiny} & 92.6{\tiny} & 60.6{\tiny} & 67.0{\tiny} & 77.2{\tiny} & 87.0{\tiny}& 93.3{\tiny} \\
				{\tiny \textbf{ETF Classifier +  DR}}    & 68.4{\tiny} & 73.0{\tiny} & 78.4{\tiny} & 86.9{\tiny} & 92.9{\tiny}  & 71.9{\tiny} & 76.5{\tiny} & 81.0{\tiny} & 87.7{\tiny} & 92.0{\tiny}\\
				\bottomrule
			\end{tabular}
\end{center}
\vspace{-4mm}
\end{table*}



\begin{table*}[t]
	\vspace{-3mm}
	\small
			\renewcommand\arraystretch{0.8}
	\setlength{\tabcolsep}{2.5pt}
	\caption{Long-tailed classification accuracy (\%) with ResNet and DenseNet on four datasets. Results are the mean of three repeated experiments with different seeds.} \label{longtail}
	\vspace{-1mm}
	\begin{center}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
				\toprule
				\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{CIFAR-10 \cite{krizhevsky2009learning}} &  \multicolumn{3}{c|}{CIFAR-100 \cite{krizhevsky2009learning}} & \multicolumn{3}{c|}{SVHN \cite{netzer2011reading}} & \multicolumn{3}{c}{STL-10 \cite{coates2011analysis}}\\
				&   &  &  &  &  &  &  &  &  &  &  &  \\
				\midrule
				\emph{ResNet} \\
				Learnable Classifier + CE  & 67.3 & 72.8 & 78.6 & 38.7& 43.0 & 48.1 & 40.5&40.9 & 49.3& 33.1&37.9&38.8 \\
				ETF Classifier +  DR   & 71.9 & 76.5 & 81.0 & 40.9 & 45.3 & 50.4 & 42.8 & 45.7 & 49.8 & 33.5 & 37.2 & 37.9 \\
				\rowcolor{lightgray}  Improvements  & \textbf{+4.6} & \textbf{+3.7} & \textbf{+2.4} & \textbf{+2.2} & \textbf{+2.3} & \textbf{+2.3} & \textbf{+2.3} & \textbf{+4.8} & \textbf{+0.5} & \textbf{+0.4} & -0.7 & -0.9\\
				\midrule
				\emph{DenseNet} \\
Learnable Classifier + CE  & 71.1 & 77.7 & 84.1 & 40.3 & 43.8 & 49.8 & 39.7 & 40.5 & 46.4& 38.5 & 41.2 & 44.9\\
                ETF Classifier +  DR   & 72.9 & 78.5 & 83.4 & 40.1 & 44.0 & 49.7 & 40.5 & 44.8 & 48.4 & 39.5 & 42.9 & 46.3 \\				
				\rowcolor{lightgray} Improvements  & \textbf{+1.8} & \textbf{+0.8} & -0.7 & -0.2 & \textbf{+0.2} & -0.1 & \textbf{+0.8}  & \textbf{+4.3}  & \textbf{+2.0}  & \textbf{+1.0} & \textbf{+1.7} & \textbf{+1.4} \\				
				\bottomrule
			\end{tabular}
\end{center}
	\vspace{-2mm}
\end{table*}


\iffalse
\begin{table}[!h]
\renewcommand\arraystretch{0.8}
			\setlength{\tabcolsep}{3.0pt}
	\caption{Fine-grained classification accuracies (\%) on CUB-200-2011 with ResNet backbone networks pre-trained on ImageNet.}
	\label{finegrain}
	\vspace{-3mm}
	\begin{center}
		\begin{small}
\begin{tabular}{l|l|c}
				\toprule
				Backbone & {Methods} & Acc. (\%)\\
				\midrule
\multirow{2}{*}{ResNet-34} & Learnable Classifier + CE   &  82.2\\
				& ETF Classifier +  DR  & 83.0 \\
\midrule
\multirow{2}{*}{ResNet-50} & Learnable Classifier + CE    &  85.5\\
				& ETF Classifier +  DR    & 86.1 \\				
\midrule
\multirow{2}{*}{ResNet-101} & Learnable Classifier + CE    &  86.2\\
				& ETF Classifier +  DR    & 87.0 \\				
\bottomrule
			\end{tabular}
\end{small}
	\end{center}
\vspace{-2mm}
\end{table}
\fi


\begin{table}
	\small
	\vspace{-1mm}
	\begin{minipage}[th!]{\textwidth}
\begin{minipage}[t]{0.48\textwidth}
			\renewcommand\arraystretch{0.8}
			\setlength{\tabcolsep}{3.0pt}
			\centering
\caption{Long-tailed classification accuracy (\%) on ImageNet-LT \cite{liu2019large} with ResNet-50 backbone and different training epochs.}
			\begin{tabular}{l|l|c}
	\toprule
	Epoch & {Methods} & Acc. (\%)\\
	\midrule
\multirow{2}{*}{90} & Learnable Classifier + CE   &  34.6\\
	& ETF Classifier +  DR  & 41.8\\
\midrule
\multirow{2}{*}{120} & Learnable Classifier + CE    &  41.9\\
	& ETF Classifier +  DR    & 43.2 \\				
\midrule
\multirow{2}{*}{150} & Learnable Classifier + CE    &  42.5\\
	& ETF Classifier +  DR    & 43.8 \\				
	\midrule
		\multirow{2}{*}{180} & Learnable Classifier + CE    &  44.3\\
	& ETF Classifier +  DR    & 44.7\\		
\bottomrule
\end{tabular}
\vspace{2mm}
			\label{imgnet}
		\end{minipage}
		\quad \
		\begin{minipage}[t]{0.47\textwidth}
						\renewcommand\arraystretch{0.8}
			\setlength{\tabcolsep}{3.0pt}
\centering
\caption{Fine-grained classification accuracy (\%) on CUB-200-2011 \cite{wah2011caltech} with different ResNet backbones pre-trained on ImageNet. Training details are described in Appendix \ref{details}.}
			\begin{tabular}{l|l|c}
	\toprule
	Backbone & {Methods} & Acc. (\%)\\
	\midrule
\multirow{2}{*}{ResNet-34} & Learnable Classifier + CE   &  82.2\\
	& ETF Classifier +  DR  & 83.0 \\
\midrule
\multirow{2}{*}{ResNet-50} & Learnable Classifier + CE    &  85.5\\
	& ETF Classifier +  DR    & 86.1 \\				
\midrule
\multirow{2}{*}{ResNet-101} & Learnable Classifier + CE    &  86.2\\
	& ETF Classifier +  DR    & 87.0 \\				
\bottomrule
\end{tabular}
\vspace{2.5mm}
	\label{finegrain}
		\end{minipage}
	\end{minipage}
	\vspace{-6mm}
\end{table}

\vspace{-1mm}
\subsection{Performances on Long-tailed Classification}

We conduct an ablation study with ResNet on CIFAR-10. As shown in Table \ref{ablation}, when we replace the learnable classifier with our fixed ETF classifier, the performances get improved for the imbalance ratio  of 0.01 and 0.02 without Mixup \cite{zhang2018mixup}. They also achieve comparable results for  and the balanced setting (). However, only using the ETF classifier does not work for the extreme imbalance case where . Besides, it is not compatible with Mixup, which is a strong augmentation tool to alleviate the bias brought by adversarial training samples. When the DR loss that is specifically designed for the ETF classifier is used, we achieve significant performance improvements for  of 0.005, 0.01, and 0.02 both with and without Mixup. Generally our proposed ETF classifier with the DR loss has better performances on multiple long-tailed cases than the learnable classifier with the original or weighted CE loss.  

In Table \ref{longtail}, the advantage of our method is further verified on more datasets with ResNet and DenseNet. Concretely, we achieve an average improvement of 2.0\% for ResNet and 1.1\% for DenseNet. We also test our method on the large-scale dataset, ImageNet-LT \cite{liu2019large}, which is an imbalanced version of the ImageNet dataset \cite{russakovsky2015imagenet}. As shown in Table \ref{imgnet}, we compare our method with baseline by training the models for different epochs. We observe that the superiority of our method is more remarkable when training for less epochs. It can be explained by the fact that our method directly has the classifier in its optimality and optimizes the features towards the neural collapse solution, while the learnable classifier with the CE loss needs a sufficient training process to separate classifier vectors of different classes. So our method can be preferred when fast convergence or limited training time is required. The accuracy curves in training for the results in Table \ref{imgnet} are shown in Figure \ref{acc_curves} in Appendix \ref{exp}.








\vspace{-2mm}
\subsection{Performances on Fine-grained Classification}
\vspace{-1mm}

Fine-grained classification can also be deemed as an imbalanced problem as the features of multiple classes are close to each other. We surprisingly find that our method is also helpful for fine-grained classification even though most of the analytical work is conducted under the case of class imbalance. As shown in Table \ref{finegrain}, our method achieves 0.7\%-0.8\% accuracy improvements on CUB-200-2011.







\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}

In this paper, we study the potential of training a network with the last-layer linear classifier randomly initialized as a simplex ETF and fixed during training. This practice enjoys theoretical merits under the layer-peeled analytical framework. We further develop a simple loss function specifically for the ETF classifier. Its advantage gets verified by both theoretical and experimental results. We conclude that it is not necessary to learn a linear classifier for classification networks, and our simplified practice even helps to improve the long-tailed and fine-grained classification performances with no cost. Our work may help to further understand neural collapse and the neural network architecture for classification. Limitations and societal impacts are discussed in Appendix \ref{limit}. As suggested by a reviewer of this paper, we compare with \cite{fang2021exploring,pernici2021regular,zhu2021geometric} in more details in Appendix \ref{detailed compare}. 






\begin{ack}
	Z. Lin was supported by the NSF China (No.s 62276004 and 61731018), NSFC Tianyuan Fund for Mathematics (No. 12026606), and Project 2020BD006 supported by PKU-Baidu Fund.
\end{ack}

\small
\bibliographystyle{ieee}
\bibliography{example_paper}


\normalsize
\section*{Checklist}

\iffalse
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
	justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
	\item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
	\item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
	\item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
\fi

\begin{enumerate}
	
	
	\item For all authors...
	\begin{enumerate}
		\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
		\answerYes{See the last paragraph in Section \ref{intro}.}
		\item Did you describe the limitations of your work?
		\answerYes{See Appendix F.}
		\item Did you discuss any potential negative societal impacts of your work?
		\answerYes{See Appendix F.}
		\item Have you read the ethics review guidelines and ensured that your paper conforms to them?
		\answerYes{We have read the guidelines and ensured that our paper conforms to them.}
	\end{enumerate}
	
	
	\item If you are including theoretical results...
	\begin{enumerate}
		\item Did you state the full set of assumptions of all theoretical results?
		\answerYes{See Theorem \ref{ETF_classifier} and \ref{reg_dot_loss}.}
		\item Did you include complete proofs of all theoretical results?
		\answerYes{See Appendix A and B.}
	\end{enumerate}
	
	
	\item If you ran experiments...
	\begin{enumerate}
		\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
		\answerYes{See the supplementray material.}
		\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
		\answerYes{See Appendix D.}
		\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
		\answerYes{See Tabel \ref{ablation}.}
		\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
		\answerYes{See Appendix D.}
	\end{enumerate}
	
	
	\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
	\begin{enumerate}
		\item If your work uses existing assets, did you cite the creators?
		\answerYes{We cite their papers, as shown in Table \ref{longtail}, \ref{imgnet}, and \ref{finegrain}.}
		\item Did you mention the license of the assets?
		\answerNA{}
		\item Did you include any new assets either in the supplemental material or as a URL?
		\answerNA{}
		\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
		\answerNA{}
		\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
		\answerNA{}
	\end{enumerate}
	
	
	\item If you used crowdsourcing or conducted research with human subjects...
	\begin{enumerate}
		\item Did you include the full text of instructions given to participants and screenshots, if applicable?
		\answerNA{}
		\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
		\answerNA{}
		\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
		\answerNA{}
	\end{enumerate}
	
	
\end{enumerate}







\newpage
\appendix


\begin{center}
	{\bf Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?\\label{LP_noW_k}
		\min_{\H}\quad & \frac{1}{N}\sum_{i=1}^{n_k}\mathcal{L}_{CE}(\h_{k,i}, \W^*), \\
		s.t. \quad & ||\h_{k,i}||^2\le E_H, \  1\le i\le n_k. \notag
	
		\tilde{L}=\frac{1}{N}\sum_{i=1}^{n_k}-\log\frac{\exp(\h_{k,i}^T\w_k^*)}{\sum_{j=1}^K\exp(\h_{k,i}^T\w_j^*)}+\sum_{i=1}^{n_k}\mu_i\left(||\h_{k,i}||^2- E_H\right),
	
		\frac{\partial \tilde{L}}{\partial\h_{k,i}}=-\frac{\left(1-p_k\right)\w_k^*}{N}+\frac{1}{N}\sum_{j\ne k}^{K}p_j\w_j^*+2\mu_i\h_{k, i},\ 1\le i\le n_k.
	
		\sum_{j\ne k}^{K}p_j\w_j^* = 	\sum_{j\ne k}^{K}p_j\w_k^*.
	
		\frac{K}{K-1}\sum_{j\ne k}^{K}p_j=0,
	\label{constraint}
		||\h_{k,i}||^2=E_H,
	\label{lag}
		\frac{\partial \tilde{L}}{\partial\h_{k,i}^*}=\frac{1}{N}\sum_{j\ne k}^{K}p_j(\w_j^*-\w_k^*)+2\mu_i\h_{k,i}^*=0,
	\label{pi}
		E_Wp_{j'}\left(1+\frac{1}{K-1}\right)+2N\mu_i\langle\h_{k,i}^*, \w_{j'}^*\rangle=0.
	
		\frac{p_j}{p_{j'}}=\frac{\exp(\langle\h_{k,i}^*, \w_{j}^*\rangle)}{\exp(\langle\h_{k,i}^*, \w_{j'}^*\rangle)}=\frac{\langle\h_{k,i}^*, \w_{j}^*\rangle}{\langle\h_{k,i}^*, \w_{j'}^*\rangle}.
	
		\langle\h_{k,i}^*, \w_{j}^*\rangle=\langle\h_{k,i}^*, \w_{j'}^*\rangle=C,\ p_j=p_{j'}=p,\ \forall j,j'\ne k,
	
		p=\frac{1-K}{K}\cdot\frac{2N\mu_iC}{E_W},\ 
	
		1-p_k = (K-1)p=\frac{(1-K)(K-1)}{K}\cdot\frac{2N\mu_iC}{E_W}, 
	\label{1-p+p}
		1-p_k+p = (1-K)\cdot\frac{2N\mu_iC}{E_W}.
	
		\h_{k,i}^*=\frac{1}{2N\mu_i}\left[\left(1-p_k\right)\w_k^*-\sum_{j\ne k}^Kp_j\w_j^*\right].
	
		\begin{aligned}
			\h_{k,i}^*&=\frac{1}{2N\mu_i}\left(1-p_k+p\right)\w_k^*\\
			&=\frac{\left(1-K\right)C}{E_W}\w_k^*,
		\end{aligned}
	
		C=\langle\h_{k,i}^*, \w_{j}^*\rangle=-\frac{\sqrt{E_HE_W}}{K-1},\ \forall j\ne k,
	
		\h_{k,i}^*=\sqrt{\frac{E_H}{E_W}}\w_{k}^*,
	 \h_{k,i}^{t+1} = \proj_{\mathrm{E_H}} ( \h_{k,i}^{t}  - \gamma \frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}) = \proj_{\mathrm{E_H}} ( \h_{k,i}^{t}  - \gamma \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right)\w_k^* ), 
		\begin{aligned}
			|| \h_{k, i}^{t+1} - \h_{k,i}^* ||^2 & \leq  || \h_{k,i}^t - \gamma  \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right)\w_k^*  - \h_{k,i}^*||^2 \\
			& =  || \h_{k,i}^t - \h_{k,i}^* ||^2 - 2\gamma\left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right) \left\langle \h_{k,i}^t - \h_{k,i}^* , \w_k^*\right\rangle + \gamma^2 \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right)^2 \mathrm{E_W}\\
			& = 2\mathrm{E_H}( 1 -  \cos\angle(\h_{k,i}^t,\w_k^*) ) - 2\gamma \sqrt{\mathrm{E_W E_H}}\left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right) ^ 2  + \gamma^2\mathrm{E_W} \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right)^2 
		\end{aligned}
	\label{eq:convergence_DR}
		\begin{aligned}
			|| \h_{k, i}^{t+1} - \h_{k,i}^* ||^2 & \leq  
			2\mathrm{E_H}( 1 -  \cos\angle(\h_{k,i}^t,\w_k^*) ) -    \mathrm{  E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2   \\
			& = \frac{1+\cos\angle(\h_{k,i}^t,\w_k^*)}{2}  || \h_{k, i}^{t} - \h_{k,i}^* ||^2 .
		\end{aligned}
	\label{proof_DR_0} \h_{k,i}^{t+1} = \proj_{\mathrm{E_H}} \left( \h_{k,i}^{t}  - \gamma \frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}\right) = \proj_{\mathrm{E_H}} \left( \h_{k,i}^{t}  - \gamma \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right)\w_k^* \right),     
	
		\begin{aligned}
		\left\|\h_{k,i}^{t}  - \gamma \frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}\right\|^2 
		& = E_H - 2\sqrt{E_H E_W}\gamma \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right) \cos\angle(\h_{k,i}^t,\w_k^*)\\ 
		&\quad\ + \gamma^2 E_W \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right)^2 \\
		&\geq E_H,
		\end{aligned}
	\label{proof_DR_1}
		\begin{aligned}
			\left\| \h_{k, i}^{t+1} - \h_{k,i}^* \right\|^2 & \leq  \left\| \h_{k,i}^t - \gamma  \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1 \right)\w_k^*  - \h_{k,i}^*\right\|^2 \\
			& =  \left\| \h_{k,i}^t - \h_{k,i}^* \right\|^2 - 2\gamma\left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right) \left\langle \h_{k,i}^t - \h_{k,i}^* , \w_k^*\right\rangle \\
			&\quad\ + \gamma^2{E_W} \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right)^2\\
			& \overset{a}{=} 2{E_H}\left(1 -  \cos\angle(\h_{k,i}^t,\w_k^*)\right) - 2\gamma \sqrt{{E_W E_H}}\left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right) ^ 2  \\
			&\quad\ + \gamma^2{E_W} \left( \cos\angle(\h_{k,i}^t,\w_k^*) - 1\right)^2,
		\end{aligned}
	\label{eq:convergence_DR}
		\begin{aligned}
			\left\| \h_{k, i}^{t+1} - \h_{k,i}^* \right\|^2 & \leq  
			2{E_H}\left( 1 -  \cos\angle(\h_{k,i}^t,\w_k^*) \right) -  {E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right)^2 \\
			& = \frac{1+\cos\angle(\h_{k,i}^t,\w_k^*)}{2}  \left\| \h_{k, i}^{t} - \h_{k,i}^* \right\|^2.
		\end{aligned}
	 \eta_{\h}^{(DR)} =  \frac{1+\cos\angle(\h_{k,i}^t,\w_k^*)}{2}  .  \h_{k,i}^{t+1} = \proj_{{E_H}} \left( \h_{k,i}^{t}  - \gamma \frac{\partial \mathcal{L}_{CE}}{\partial \h_{k,i}}\right) = \proj_{{E_H}} \left( \h_{k,i}^{t}  + \gamma  \left(1-p_k\right)\w_k^*-\gamma\sum_{j\ne k}^{K}p_j\w_j^* \right). 
		\begin{aligned}
			\left\| \h_{k, i}^{t+1} - \h_{k,i}^* \right\|^2 & \leq  \left\| \h_{k,i}^t  + \gamma  \left(1-p_k\right)\w_k^*-\gamma\sum_{j\ne k}^{K}p_j\w_j^* - \h_{k,i}^*\right\|^2 \\
& =  \left\| \h_{k,i}^t - \h_{k,i}^* \right\|^2 - 2\gamma\left( p_k - 1\right) \left\langle \h_{k,i}^t - \h_{k,i}^* , \w_k^*\right\rangle + \gamma^2 \left( p_k - 1\right)^2 {E_W}\\
			&\quad\ + 2 \gamma\left\langle \h_{k,i}^*-\h_{k,i}^t   ,    \sum_{j\ne k}^{K}p_j\w_j^* \right\rangle  \\
			&\quad\ -    2\gamma\left\langle (1-p_k) \gamma  \w_k^*,    \sum_{j\ne k}^{K}p_j\w_j^* \right\rangle  + \left\|\gamma\sum_{j\ne k}^{K}p_j\w_j^* \right\|^2 \\
& = \left\| \h_{k,i}^t - \h_{k,i}^* \right\|^2 +\gamma^2 \left( p_k - 1\right)^2 {E_W} + \left\|\gamma\sum_{j\ne k}^{K}p_j\w_j^* \right\|^2 + M,\\
		\end{aligned}
	
		\begin{aligned}
			M & = 2\gamma(1-p_k )\left\langle \h_{k,i}^t - \h_{k,i}^* , \w_k^*\right\rangle -    2\gamma\left\langle (1-p_k) \gamma  \w_k^*,    \sum_{j\ne k}^{K}p_j\w_j^* \right\rangle+ 2 \gamma\left\langle \h_{k,i}^*-\h_{k,i}^t   ,    \sum_{j\ne k}^{K}p_j\w_j^* \right\rangle \\
			& = 2\gamma(1-p_k )\left(\left\langle \h_{k,i}^t - \h_{k,i}^*, \w_k^*\right\rangle+\frac{\gamma E_W}{K-1} \sum_{j\ne k}^{K}p_j \right)+ 2 \gamma\left\langle \h_{k,i}^*-\h_{k,i}^t   ,    \sum_{j\ne k}^{K}p_j\w_j^* \right\rangle \\
			& = 2\gamma(1-p_k )\left(\left\langle\h_{k,i}^t , \w_k^*\right\rangle-\sqrt{E_HE_W}+\frac{\gamma E_W\left(1-p_k\right)}{K-1}\right)-2 \gamma\h_{k,i}^t \sum_{j\ne k}^{K}p_j\w_j^* -\frac{2\gamma\left(1-p_k\right)}{K-1}\sqrt{E_HE_W}\\
			& = 2\gamma(1-p_k )\left\langle\h_{k,i}^t , \w_k^*\right\rangle-2 \gamma\h_{k,i}^t \sum_{j\ne k}^{K}p_j\w_j^*-2\gamma\left(1-p_k\right)\frac{K}{K-1}\sqrt{E_HE_W}+\frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1}.\\
		\end{aligned}
	
		\begin{aligned}
			&2\gamma(1-p_k )\left\langle\h_{k,i}^t , \w_k^*\right\rangle-2 \gamma\h_{k,i}^t \sum_{j\ne k}^{K}p_j\w_j^*\\
			=&-2\gamma\left\langle\h_{k,i}^t , \left(1-p_k\right)\sum_{j\ne k}^{K}\w_j^*+ \sum_{j\ne k}^{K}p_j\w_j^*\right\rangle\\
			\overset{a}{=}&-2\gamma\left\langle\h_{k,i}^t , \frac{K\left(1-p_k\right)}{K-1}\sum_{j\ne k}^{K}\w_j^*\right\rangle\\
			=&-\frac{2K}{K-1}\left(1-p_k\right)\gamma\sqrt{E_H E_W}\left(\sum_{j\ne k}^{K}\cos\angle\left(\h_{k,i}^t, \w_j^*\right)\right)\\
			\overset{b}{=}&\frac{2K}{K-1}\left(1-p_k\right)\gamma\sqrt{E_H E_W}\cos\angle\left(\h_{k,i}^t, \w_k^*\right),
		\end{aligned}
	
		\begin{aligned}
			M&= \frac{2K}{K-1}\left(1-p_k\right)\gamma\sqrt{E_H E_W}\cos\angle\left(\h_{k,i}^t, \w_k^*\right)-2\gamma\left(1-p_k\right)\frac{K}{K-1}\sqrt{E_HE_W}+\frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1}\\
			& = -2\left(1-p_k\right)\frac{K}{K-1}\gamma\sqrt{E_H E_W}\left(1-\cos\angle\left(\h_{k,i}^t, \w_k^*\right)\right)+\frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1},\\
		\end{aligned}
	\label{eq41}
		\begin{aligned}
			&\left\| \h_{k, i}^{t+1} - \h_{k,i}^* \right\|^2  \\
			\leq & \left\| \h_{k,i}^t - \h_{k,i}^* \right\|^2 - \frac{2K\sqrt{E_H E_W}}{K-1}\gamma\left(1-p_k\right)\left(1-\cos\angle\left(\h_{k,i}^t, \w_k^*\right)\right)\\
			& +\gamma^2 \left( p_k - 1\right)^2 {E_W} +\left\|\gamma\sum_{j\ne k}^{K}p_j\w_j^* \right\|^2 + \frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1}\\
			= &2{E_H}\left( 1 -  \cos\angle(\h_{k,i}^t,\w_k^*) \right) -   {E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2\\
			&  +{E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2 - \frac{2K\sqrt{E_H E_W}}{K-1}\gamma\left(1-p_k\right)\left(1-\cos\angle\left(\h_{k,i}^t, \w_k^*\right)\right)\\
			& +\gamma^2 \left( p_k - 1\right)^2 {E_W} +\frac{ \gamma^2\left(1-p_k\right)^2E_W}{(K-1)^2} + \frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1}\\
			=&2{E_H}\left( 1 -  \cos\angle(\h_{k,i}^t,\w_k^*) \right) -   {E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2\\
			&  +{E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2 - \frac{2K\sqrt{E_H E_W}}{K-1}\gamma\left(1-p_k\right)\left(1-\cos\angle\left(\h_{k,i}^t, \w_k^*\right)\right)\\
			& + \left(\frac{\gamma(1-p_k)K}{K-1}\right)^2E_W.
		\end{aligned}
	\label{eq42}
	\min_{s_k}\quad {  E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2-  2 s_k \sqrt{E_H E_W} \left(1-\cos\angle\left(\h_{k,i}^t, \w_k^*\right)\right)+s_k^2{E_W}. 
	\label{eq:convergence_CE}
		\begin{aligned}
			\left\| \h_{k, i}^{t+1} - \h_{k,i}^* \right\|^2 & \leq  
			2{E_H}\left(1 -  \cos\angle(\h_{k,i}^t,\w_k^*)\right) - {E_H} \left( 1-\cos\angle(\h_{k,i}^t,\w_k^*) \right) ^ 2 \\
			& = \frac{1+\cos\angle(\h_{k,i}^t,\w_k^*)}{2}  \left\| \h_{k, i}^{t} - \h_{k,i}^* \right\|^2,
		\end{aligned}
	
		\eta_{\h}^{(CE)}\ge \frac{1+\cos\angle(\h_{k,i}^t,\w_k^*)}{2}=\eta_{\h}^{(DR)},
	
		M\rightarrow\frac{2\gamma^2\left(1-p_k\right)^2E_W}{K-1}, 
	
		|| \h_{k, i}^{t+1} - \h_{k,i}^* ||^2\leq  || \h_{k,i}^t - \h_{k,i}^* ||^2 +\gamma^2 \left( p_k - 1\right)^2 \mathrm{E_W} + ||\gamma\sum_{j\ne k}^{K}p_j\w_j^* ||^2 + \frac{2\gamma^2\left(1-p_k\right)^2}{K-1}, 
	
	\begin{aligned}
		\frac{1}{N}\frac{\partial \mathcal{L}_{DR}}{\partial \W_{1:L-1}}&=\frac{1}{N}\frac{\partial \H}{\partial \W_{1:L-1}}\frac{\partial \mathcal{L}_{DR}}{\partial \H}\\
		&=\frac{1}{N}\sum_{k=1}^K\sum_{i=1}^{n_k}\frac{\partial \h_{k,i}}{\partial \W_{1:L-1}}\frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}.\\
	\end{aligned}

	\begin{aligned}
		\frac{1}{N}\left\|\frac{\partial \mathcal{L}_{DR}}{\partial \W_{1:L-1}}\right\|_2&\le \frac{1}{N}\sum_{k=1}^K\sum_{i=1}^{n_k}\left\|\frac{\partial \h_{k,i}}{\partial \W_{1:L-1}}\frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}\right\|_2\\
		&\le \frac{1}{N}\sum_{k=1}^K\sum_{i=1}^{n_k}\left\|\frac{\partial \h_{k,i}}{\partial \W_{1:L-1}}\right\|_2\cdot\left\|\frac{\partial \mathcal{L}_{DR}}{\partial \h_{k,i}}\right\|_2\\
		&\le \frac{2}{N}\sum_{k=1}^K\sum_{i=1}^{n_k}\sigma_{\rm{max}}\sqrt{E_{\w_k}},\\
	\end{aligned}

where  denotes the largest singular value of the Jacobian , ,  is the length of the -th classifier vector, and  by Eq. (\ref{DR_grad}). Although we cannot realize a balanced gradient \emph{w.r.t}  among classes, we can balance the upper bound of its gradient norm of each class by controlling . In implementations, we set , which is equivalent to performing a weighted loss function on different classes. In experiments, we also compare our method with the weighted CE loss for fair comparison. 




\section{Datasets and Training Details}
\label{details}

We conduct long-tailed classification experiments on the four datasets, CIFAR-10, CIFAR-100, SVHN, and STL-10, with two architectures, ResNet-32 and DenseNet with a depth of 150, a growth rate of 12, and a reduction of 0.5. All models are trained with the same training setting. Concretely, we train for 200 epochs, with an initial learning rate of 0.1, a batchsize of 128, a momentum of 0.9, and a weight decay of . The learning rate is divided by 10 at epoch 160 and 180. The hyper-parameter in the  distribution used for Mixup is set as 1.0 when Mixup is used. We use the code released by \cite{zhong2021improving} to produce the imbalanced datasets. The numbers of training samples are decayed exponentially among classes. We adopt the standard data normalization and augmentation for the four datasets. 

We also conduct long-tailed classification experiments on ImageNet-LT with ResNet-50. We train all models for 90, 120, 150, and 180 epochs, with a batchsize of 1024 among 8 NVIDIA A-100 GPUs. Following \cite{zhong2021improving}, we use the SGD optimizer with a momentum of 0.9 and a weight decay of . The initial learning is 0.1 and decays following the cosine annealing schedule. 

We conduct fine-grained classification experiments on CUB-200-2011 with ResNet-34, ResNet-50, and ResNet-101. The ResNet backbone networks are pre-trained on ImageNet. We train for 300 epochs on CUB-200-2011 with a batchsize of 64 and an initial learning rate of 0.04, which is dropped by 0.1 at epoch 90, 180, and 270. The standard data normalization and augmentation are adopted. Other training settings are the same as the long-tailed experiments. 

\section{Additional Empirical and Experimental Results}
\label{exp}


\begin{figure*}[!t]
\begin{center}
\includegraphics[width=1.01\linewidth]{diag.pdf}
		\vspace{-6mm}
		\caption{Averages of , , where  is the global mean, with (red) and without (black) our method, using ResNet (up) and DenseNet (bottom) on four datasets. The models are trained on CIFAR-100 with an imbalance ratio of 0.02.}
		\label{diag}
	\end{center}
	\vspace{-1mm}
\end{figure*}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=1.01\linewidth]{F2.pdf}
		\vspace{-5mm}
		\caption{Statistics of  during training, where , , and , using ResNet (up) and DenseNet (bottom) on four datasets. The models are trained on CIFAR-100 with an imbalance ratio of 0.02}
		\label{F2}
	\end{center}
	\vspace{-1mm}
\end{figure*}



\textbf{Empirical Results.} We provide more empirical results that have been discussed in Section \ref{empirical result}. 
We calculate the averages of , , and  in Figure \ref{diag} and \ref{F2}. It reveals that the model using our method generally has a higher  and a lower , which indicates that the feature means and classifier vectors of the same class are better aligned. We observe no advantage of ResNet on STL-10 and DenseNet on CIFAR-100 in Figure \ref{diag} and \ref{F2}. In Table \ref{longtail}, we see that the two cases are right the failure cases, which shows consistency between neural collapse convergence and classification performance.












\begin{figure*}[!t]
	\begin{center}
		\includegraphics[width=1.01\linewidth]{imgnet_accuracies_size.pdf}
		\vspace{-5mm}
		\caption{Top-1 accuracy curves on ImageNet-LT using the ResNet-50 backbone with ``learnable classifier + CE loss'' and our proposed ``ETF classifier + DR loss'' for (a) 90, (b) 120, (c) 150, and (d) 180 epochs of training.}
		\label{acc_curves}
	\end{center}
\end{figure*}


\textbf{Accuracy Curves of Long-tailed Classification on ImageNet-LT.} As shown in Figure \ref{acc_curves}, we depict the accuracy curves in training of the long-tailed classification results in Table \ref{imgnet}. It reveals that the model using our proposed ETF classifier and DR loss converges faster than the traditional learnable classifier with the CE loss. Especially when we train for less epochs, the accuracy curves of our ``ETF classifier + DR loss'' are less affected, while those of ``learnable classifier + CE loss'' are deteriorated and converge slowly. Thus the superiority in performance of our method is more remarkable when training for limited epochs. It can be explained by the fact that our method directly has the classifier in its optimality and optimizes the features towards the neural collapse solution, while the learnable classifier with the CE loss needs a sufficient training process to separate classifier vectors of different classes. So our method can be preferred when fast convergence or limited training time is required.

\section{Limitations and Societal Impacts}
\label{limit}

\textbf{Limitations.} The limitations of this study may include: (1) The benefits of our proposed ETF classifier and DR loss are mainly analyzed for the case of imbalanced training. But our methods are general and applicable to all classification problems. The advantages and disadvantages of our methods for other machine learning areas, such as label noise learning and few-shot learning, are not discussed in this study, and deserve our future work. (2) In a neural network with a learnable classifier, the lengths, \emph{i.e.,} -norms, of both features and classifier vectors are usually increasing as training. But for our fixed classifier, the lengths of the classifier vectors are fixed during training. As shown in Eq. (\ref{grad_h}) (for CE loss) and Eq. (\ref{DR_grad}) (for DR loss), the gradient norm with respect to feature is decided by the lengths of classifier vectors. When feature is in a large length, its gradient using our fixed classifier may have a  limited step size. So an adaptive mechanism to adjust the length of the ETF classifier may be more favorable and further improve the performance.



\textbf{Societal Impacts.} Our study proposes a new paradigm for neural network classification. It enjoys some theoretical benefits for imbalanced training, which is an important topic in machine learning. Our method is a general technique for network training, but is not related to any potential impact on privacy, public health, fairness, and other societal issues. Besides, our proposed ETF classifier and DR loss actually reduce the computation cost compared with the traditional learnable classifier with the CE loss, so will not introduce extra environment burden. 

\section{Detailed Comparison with Some Studies}
\label{detailed compare}

As suggested by a reviewer of this paper, we compare with \cite{fang2021exploring,pernici2021regular,zhu2021geometric} in more details. 

The objective of LPM studied in \cite{fang2021exploring} is also the CE loss with constraints of feature and classifier. They prove that (1) neural collapse is the global optimality of this objective when training on a balanced dataset; (2) in imbalanced training, neural collapse will be broken, and the prototypes of minor classes will be merged, which explains the difficulty of imbalanced training.

We also study the objective of CE loss with feature and classifier constraints. As a comparison, (1) We prove that neural collapse can also be the global optimality for imbalanced training as long as the classifier is fixed as an ETF (Theorem \ref{ETF_classifier}); (2) We analyze from the gradient perspective and show that the broken neural collapse in imbalanced training is caused by the imbalanced magnitude of gradients of the CE loss (Remark \ref{remark2}). We also show that the “pull-push” mechanism is crucial for the emergence of neural collapse in the CE loss in balanced training (Remark \ref{remark3}); (3) Inspired by the analyses, we propose a new loss function with a provable advantage over the CE loss (Theorem \ref{reg_dot_loss}).


The objective of LPM studied in \cite{zhu2021geometric} is the CE loss with regularizations of feature and classifier. They prove that: (1) neural collapse is the global optimality of this objective when training on a balanced dataset; (2) despite being nonconvex, the landscape of the objective in this case is benign, which means that there is no spurious local minimum, so gradient-based optimization method can easily escape from the strict saddle points to look for the global minimizer.

In contrast, the objective we consider is the CE loss with constraints of feature and classifier, instead of regularizations. We mainly consider neural collapse in imbalanced learning, and accordingly propose a new loss function, which are different from the two results in \cite{zhu2021geometric}.

\cite{pernici2019maximally,pernici2021regular} show that fixing a learnable classifier as the vertices of regular polytopes, including d-simplex, d-cube, and d-orthoplex, helps to learn stationary and maximally separated features. It does not harm the performance, and in many cases improves the performance. It also brings faster speed of convergence and reduces the model parameters. Their spirit of learning maximally separated features is very similar to neural collapse. Compared with \cite{pernici2019maximally,pernici2021regular}, we prove that neural collapse can be the global optimality of the CE loss even in imbalanced learning based on the LPM analytical tool. We also propose a new loss function with a provable advantage over the CE loss.



\end{document}
