\documentclass[sigconf]{acmart}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{multirow}
\usepackage{url}
\settopmatter{printacmref=false} \renewcommand\footnotetextcopyrightpermission[1]{}


\begin{document}


\title{Path-aware Siamese Graph Neural Network for Link Prediction}

\author{Jingsong Lv, Zhao Li, Hongyang Chen, Yao Qi, Chunqi Wu}
\affiliation{\institution{Research Center for Graph Computing, Zhejiang Lab}
  \city{Hangzhou}
  \state{Zhejiang Province}
  \country{China}
}
\email{{jingsonglv,hongyang,qiy}@zhejianglab.com, lzjoey@gmail.com, francis_chun@163.com}

\renewcommand{\shortauthors}{Jingsong Lv, et al.}


\begin{abstract}
  In this paper, we propose an algorithm of Path-aware Siamese Graph neural network(PSG) for link prediction tasks. First, PSG captures both nodes and edge features for given two nodes, namely the structure information of k-neighborhoods and relay paths information of the nodes. Furthermore, a novel multi-task GNN framework with self-supervised contrastive learning is proposed for differentiation of positive links and negative links while content and behavior of nodes can be captured simultaneously. We evaluate the proposed algorithm PSG  on two link property prediction datasets, ogbl-ddi and ogbl-collab. PSG achieves top 1 performance on ogbl-ddi until submission and top 3 performance on ogbl-collab. The experimental results verify the superiority of our proposed PSG.
\end{abstract}


\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{Siamese Network, Graph Neural Networks, Contrastive Learning, Representation Learning, Link Prediction.}

\maketitle

\section{Introduction}
The task of link prediction is often used to predict missing links in static networks.  It is widely applied in the scenarios of recommender system~\cite{bennett2007netflix}, social network analysis~\cite{adamic2003friends}, bio-computing~\cite{zhao2018bipartite}, computational pharmacy~\cite{stanfield2017drug}, knowledge graph~\cite{nickel2015review} and
 other graph based network analysis. Generally speaking, it can be inferenced by computing the similarity of the  nodes to judge whether missing links exist between two nodes. This straightforward idea leads to research direction on the similarity measures and representation learing of graph nodes. Intuitively, common neighbors can measure the similarity of two nodes, therefore stimulate heuristic based methods on the context of nodes. Furthermore, network embedding methods are studied  as representation learning to explore latent factors as feature vectors of nodes. Recently, Graph Neural Networks (GNNs) based methods have been widely applied to the work of neural link predition due to its nonlinear feature extraction power of graph structure~\cite{wu2020comprehensive,zhou2020graph}.

\par Given ogbl-ddi dataset of Open Graph Benchmark (OGB)~\cite{hu2020ogb}, which is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network ~\cite{wishart2018drugbank}, the objective is to rank true drug-drug interaction higher than false drug-drug interaction. In the OGB leaderboard, existing approaches  mainly focus on GNNs that capture node features of neighborhood, while some ones integrate edge features, such as distance encoding~\cite{li2020distance}, into existing GNN framework. In addition, based on GraphSage~\cite{hamilton2017inductive}, PLNLP ~\cite{wang2021pairwise} introduces a pairwise loss funtion, which is common in siamese network for ranking problem, and achieves better performance.  

In this paper, we propose a Path-aware Siamese Graph neural network(PSG) for link prediction tasks. Our contributions can be summarized as follows: (1) \textbf{A Novel GNN-Friendly Algorithm.} We propose the first Path-aware Siamese GNN algorithm for link prediction, which can be easily adapted to existing backbone GNN networks, such as GraphSage, PLNLP, and GIDN~\cite{wang2022gidn}. (2) \textbf{A Novel Self-Supervised Representation Learning Framework.} We introduce clustering to get node soft labels and propose a novel multi-task representation learning framework to simultaneously learn label task and link prediction task. Therefore, positive links and negative links can be differentiated by pairwise learning while both content and behavior of nodes are aligned by contrastive learning. (3) \textbf{High Performance.} We evaluate the proposed algorithm PSG  on two link property prediction datasets of Open Graph Benchmark (OGB), ogbl-ddi and ogbl-collab. The experimental results demonstrate the superiority of the PSG.

In the following, we present our work in four sections. Section ~\Ref{section_relatedwork} introduces related work on link prediction and GNN. Section ~\Ref{section_method} formally defines the link prediction problem, and presents our solution PSG model. Section ~\Ref{section_exp} shows experiments and evaluation details of the model. Finally, Section ~\Ref{section_conclusion} summarizes this work.

\section{Related Work} \label{section_relatedwork}
\subsection{Link Prediction} For the task of link prediction,  approaches can be summarized as follows: neighborhood based, factor decomposition based and neural network based. Neighborhood based methods mainly utilize wide-based or depth-based neighborhood information to measure nodes similarity, such as common neighbors~\cite{liben2007link},  Adamic-Adar~\cite{adamic2003friends}, SimRank~\cite{jeh2002simrank} and Node2Vec~\cite{grover2016node2vec}. As the former method focus only on the explicit factors of graph, factor decomposition based method PNRL~\cite{wang2017predictive}. Furthermore, to capture deep hierarchical structure features, neural network based methods are proposed to perform end-to-end encoding of graph structure features with neural layers, such as GCN~\cite{kipf2016semi}, SEAL~\cite{zhang2018link}. 

\subsection{Graph Neural Network} Deep neural networks have achieved great success in Euclidean domains, such as computer vision~\cite{liu2021swin}, speech synthesis and recognition~\cite{snyder2018x}, natural language understanding~\cite{devlin2018bert}. It inspires the research of applying GNNs on non-Euclidean domains. In earlier stage, based on the theory of convolution in digital signal process, spectral domain-based GNNs attract lots of attention and research~\cite{wu2020comprehensive,zhou2020graph}. Graph Convolution Network(GCN)~\cite{kipf2016semi} is one of the most famous models that achieves better performance than traditional methods. As time and space complexity of GCN is high,  variants of spectral domain based methods~\cite{defferrard2016convolutional, levie2018cayleynets, huang2022graph} aim to reduce the time cost and space cost, and to generate better approximations and performance. Furthermore, spatial domain based methods are proposed to directly encode local structure features based on neighborhood aggregation and graph pooling~\cite{gilmer2017neural}. As computations increase exponentially with the increase of neighborhood range, sampling ~\cite{hamilton2017inductive} and attentions methods~\cite{velivckovic2017graph} are proposed to solve the problem of computation efficiency. As a side effect, the methods often improve the final performance as the extracted information and the capacity of neural layers increase. These methods can also be regarded as graph re-connection approaches. To further improve the effectiveness and efficiency, few-shot learning~\cite{lin2022structure}, contrastive learning~\cite{liu2021anomaly} are also introduced and integrated into GNNs.



\section{Method}  \label{section_method}
In this section, we present the problem definition and our solution PSG model.

\subsection{Problem Formulation}
Let  represent a homogeneous, unweighted, undirected graph, where  represents the nodes set of the graph, and  represents the edges set of the graph. Let  and . Given an edge , it can be represented as node pair , where .  Now let  represent the corresponding symmetric adjacency matrix, where matrix element  indicates whether edge . For node features, let  represent the feature matrix, where the matrix element  represents the  element of feature vector of node .

Now given a  network , the problem is to predict whether a missing link edge  exists. 
Suppose the prediction value of a positive sample is higher than that of a negative sample, namely , where  is prediction model function,    is a positive edge sample and  is a uncertain negative sample. Intuitively, a harder learning objective is to rank all edge samples by maximizing the difference of  and . Formally, we optimize the model by the following objective loss function, squared least surrogate loss~\cite{gao2015consistency, wang2021pairwise}:


\noindent where  is a weight hyper-parameter  of L2 regularization. 

\subsection{Model Buildup}
To solve the problem described in the former subsection, we propose a Path-aware Siamese GNN model, or simply PSG model. In the following, we introduce the details of the  model.

\subsubsection{Model Architecture}
\begin{figure*}[htbp]
\includegraphics[width=100mm,scale=0.5]{PSG_model.pdf}
\caption{Overview of PSG model architecture.} \label{fig_model}
\centering
\end{figure*}

In general, PSG model is a combinatorial model, which can be represented as a function of combination of two associated paired nodes, as shown in Formula ~\Ref{model_comb}.


Each associated node is learned and represented as a hidden feature embedding based on a representation learning function. So the model can be reformed as a link prediction function of representation function of two associated nodes, as shown in Formula ~\Ref{model_gh}.


For undirected graph, representation functions of nodes can share the same one, as shown in Formula ~\Ref{model_emb}.


Here, GNN network is adopted as a representation function model of graph nodes, as shown in Formula ~\Ref{model_gnn}.



And multi-layer perceptron (MLP) is applied as link prediction function, as shown in Formula  ~\Ref{model_mlp}.


Based on the above combinatorial model framework, PSG can be divided into four parts: edge featuring, node encoder, link predictor and pairwise objective loss function. The overview of the PSG model architecture is shown in Figure ~\Ref{fig_model}.

In the following, we present more details of the four parts.

\subsubsection{Edge Featuring} For a traditional machine learning task, feature engineering can directly improve the model performance due to correlation improvement of features.  Therefore, a feasible solution is to utilize strong edge feature. It can be implemented by calculating the Shortest Path Distance(SPD)~\cite{li2020distance, shitao2021struct} for any given paired nodes, or simply called edge. To improve the path structure capturing ability, a relay path sampling method is applied to generate multiple relay path-aware SPDs. For robustness, each relay path sampling selects a random sampling area and relay nodes. Then, a relay path-aware edge feature is generated as expectation of SPDs through relay paths. Such a process is executed for several times, namely . Hence, we get edge feature vector  with dimension . Finally, an edge feature tensor with dimension  is generated.

\noindent where  means concatenation  times,  represents Average operator,  represents a relay node in relay sampling area, and  represents the Shortest Path Distance between nodes  and .

\subsubsection{Node Encoder} For representation of two associated nodes, a shared siamese GNN model is used to encode the node features and path-aware edge features of neighborhood. The model shares the network structure and weights for representation of associated nodes from positive and negative edge samples. Similar to GraphSAGE~\cite{hamilton2017inductive}, the node encoder block mainly contains neighborhood sampling, neighborhood aggregation and node update. Besides, path-aware edge features are integrated into the shared siamese GNN network, which improves the contrastive learning of different relay path features between positive  and negative edge samples. Specifically, Formula ~\Ref{model_gnn} can  be depicted by Formula ~\Ref{model_sageedge}.


\noindent where  represents the -hop neighborhood node set of node , in which the distance to node  is not greater than . And  represents the aggregation function of neighborhood nodes, which is set to  as default.

\subsubsection{Link Predictor} As a decoder, MLP~\cite{murtagh1991multilayer} is adopted as link predictor function. Like node encoder block, link predictor block also shares the network structure and weights for decoding associated nodes. In addition, feature intersection is applied within this block. Formula ~\Ref{model_mlp} is rewritten as Formula ~\Ref{model_mlpreform}.

\noindent where  represents Hadamard product and  is the corresponding weight matrix.


\subsubsection{Pairwise Objective Loss Function } To guarantee the generalized performance of the model, the former pairwise objective loss function depicted by Formula ~\Ref{model_loss} is reformed as Formula ~\Ref{model_finalloss}. 


\noindent where  is a weight matrix of the parameter , and  is the corresponding weight hyper-parameter.

\subsection{Contrastive Learning}
Based on the former backbone GNN network, node embeddings can be learned and generated as representations of nodes interaction behaviors, called behavior representations. In the condition of unsupervised settings, the distribution of behavior representations can be different from the distribution of nodes content representations. To alleviate the deviation between the distributions of behavior representations and content representations, we introduce node content clustering, label predictor and multi-task learning as a guide of the former backbone GNN network, or simply contrastive learning of behavior representations and content representations.

\subsubsection{Content Clustering}
For clustering of high dimensional vectors, a common approach is K-means~\cite{sculley2010web} due to its efficiency. We adopt it to generate the content labels of nodes as Formula \Ref{clustering_kmeans}.


\subsubsection{Label Predictor}
Like link predictor, MLP~\cite{murtagh1991multilayer} is also adopted as label predictor function. And the output of label predictor is a label vector, whose dimension equals to the number of total content labels, which can be figured out from the result of Formula ~\Ref{clustering_kmeans}. The label predictor block shares the network structure and weights for predicting the behavior labels of adjacent nodes of positive edges. The label predictor function is depicted as Formula \Ref{model_mlp_lp}.


\subsubsection{Multi-task learning}
For adjacent nodes of any given positive edge, their behavior labels is supposed to be the same and as close as possible to the content label of one node. For this objective, contrastive learning loss is introduced and depicted as Formula \Ref{model_labelloss}.

\noindent where  represents Cross Entropy loss,  represents Hadamard product, and  is the corresponding weight hyper-parameter.

Finally, the objective loss function is figured out as the weighted sum of pairwise objective loss  and contrastive objective loss , which is shown as Formula \Ref{model_finalloss}

\noindent where  is a weight hyper-parameter, which is range from 0 to 1.

\section{Experiments}  \label{section_exp}
Below we evaluate the performance of our algorithm PSG on OGB dataset ogbl-ddi and ogbl-collab ~\cite{hu2020ogb}.

\subsection{Datasets and task} As mentioned before, the ogbl-ddi dataset is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network~\cite{wishart2018drugbank}, containing 4267 nodes and 2135822 edges. Similarly, the ogbl-collab dataset is also an undirected graph, representing a subset of the collaboration between authors indexed by MAG~\cite{wang2020microsoft}. In addition, all nodes have 128-dimensional content embedding associated with published papers, and two attributes: the year and the edge weight, representing the number of co-authored papers published in that year. The task is to predict links given information on already known links for the two given datasets, which can be reformed as to rank true links higher than false links.


\subsection{Baselines} We compare our algorithm PSG with PLNLP~\cite{wang2021pairwise}, GraphSAGE+Edge Attr~\cite{shitao2021struct}, GIDN/AGDN~\cite{wang2022gidn, sun2020adaptive} and other GNN based algorithms on the leaderboad of OGB. PLNLP introduces pairwise loss function and data augmentation based on the framework of GraphSAGE node encoder and MLP link predictor. GraphSAGE+Edge Attr utilizes distance encoding to integrate edge features based on GraphSAGE encoder and MLP link predictor. GIDN/AGDN further utilizes hop-wise attention within k-hop neighborhood sub-graph to improve message aggregation of sub-graph features. 

\subsection{Metrics and settings} Recommended by OGB, the evaluation metric is Hits@K, which is the ratio of positive edges that are ranked at K-place or above, among a set of approximately 100,000 randomly-sampled negative edges. Here,  for ogbl-ddi and  for ogbl-collab. Commonly, each result is achieved by running 10 times with batch size 65536, Adam optimizer, and learning rate is 0.001, activation function is relu, and dropout is 0.3. For other parameters, it is summarized in Tabel ~\ref{tab_param}. The experiments are executed in PyG with Pytorch under the circumstance of Tesla V100 GPU(32G RAM).

\begin{table}[htbp]
    \centering
    \caption{Hyperparameters of PSG model.}\label{tab_param}
    \begin{tabular}{lccc}
    \hline
    Hyperparameter                                    & DDI & COLLAB         \\ \hline
    Epoches                                & 500     &    1500           \\
    Parameters                                & 3499009       &   60823091          \\
    Number of shared GNN layers                                & 2     &      1         \\
    Number of readout MLP layers                                & 2         &     2      \\
    Number of edge features                              & 3    &        131        \\
    Node embedding dimensions       & 512     &    256         \\
    Hidden channels of GNN\&MLP    & 512   &    256            \\
    Number of Label MLP layers                                & None       &     2      \\
    Number of Node Content Clustering                 & None       &     50      \\
    val as input                                & None       &     Yes      \\
    year for filtering training samples                    & None       &     2011      \\
                                    & None       &     0.5      \\
    \hline
    \end{tabular}
\end{table}

\subsection{Abalation Study} As a contrast, some important results of baselines are extracted from the ogbl-ddi and ogbl-collab leaderboads and put in the upper parts of Table ~\ref{tab_ddi_result}  and ~\ref{tab_collab_result}. In the lower parts of the two tables, the results are figured out by conducting the listed algorithms in the same environment with different associated settings, which are proposed by corresponding authors. Firstly, in Table ~\ref{tab_ddi_result}, PSG beats the state-of-the-art algorithm PLNLP on the leaderboad until submission and achieves  2.2\% more performance improvement in terms of average test Hits@20. It indicates that edge featuring part is efficiently integrated into the backbone PLNLP network for dataset ogbl-ddi. Secondly, in Table ~\ref{tab_collab_result}, PSGv2 with label task achieves 0.77\% performance gain with respect to PSG and GIDN+PSGv2 also achieves 0.60\% performance gain with respect to GIDN in the same environment. It shows that our contrastive learning part can improve the graph representation learning of GNN networks, and hence can improve the prediction effectiveness of link prediction based GNN models for dataset ogbl-collab. Finally, PSGv2 beats all other GNN models based on our runs and environment. It verifies the effectiveness of PSG network model especially on dataset ogbl-ddi and ogbl-collab.




\begin{table}[htbp]
    \centering
    \caption{Performance of GNN models on dataset ogbl-ddi.}\label{tab_ddi_result}
    \begin{tabular}{lccc}
    \hline
    Algorithm                                    & Test Hits@20  & Val Hits@20             \\ \hline
    SEAL                                & 0.3056 ± 0.0386          & 0.2849 ± 0.0269         \\
    GCN       & 0.3707 ± 0.0507         & 0.5550 ± 0.0208          \\
    GraphSAGE & 0.5390 ± 0.0474         & 0.6262 ± 0.0037        \\
    GCN+JKNet    & 0.6056 ± 0.0869          & 0.6776 ± 0.0095          \\
    GraphSAGE + Edge Attr   & 0.8781 ± 0.0474 & 0.8044 ± 0.0404 \\
    PLNLP   & 0.9088 ± 0.0313 & 0.8242 ± 0.0253 \\  \hline
    PSG(epochs = 400)   & \textbf{0.9118 ± 0.0235} & 0.8161 ± 0.0232 \\
    PSG(epochs = 500)   & \textbf{0.9284 ± 0.0047} & \textbf{0.8306 ± 0.0134} \\
    PSG(epochs = 600)   & \textbf{0.9189 ± 0.0298} & \textbf{0.8562 ± 0.0181} \\ \hline
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Performance of GNN models on dataset ogbl-collab.}\label{tab_collab_result}
    \begin{tabular}{lccc}
    \hline
    Algorithm            & Test Hits@50  & Val Hits@50             \\ \hline
    GCN       & 0.4475 ± 0.0107         & 0.5263 ± 0.0115          \\
    GraphSAGE & 0.4810 ± 0.0081         & 0.5688 ± 0.0077        \\
    GraphSAGE (val as input) & 0.5463 ± 0.0112         & 0.5688 ± 0.0077        \\
    DeeperGCN   & 0.5273 ± 0.0047 & 0.6187 ± 0.0045 \\
    SEAL-nofeat (val as input)        & 0.6474 ± 0.0043          & 0.6495 ± 0.0043        \\
    PLNLP (val as input)   & 0.6872 ± 0.0052 & 1.0000 ± 0.0000 \\  \hline
    PLNLP + SIGN   & 0.7062 ± 0.0023 & 1.0000 ± 0.0000 \\
    GIDN + PSGv2   & \textbf{0.7069 ± 0.0052} & \textbf{0.9632 ± 0.0002} \\
    GIDN   & 0.7027 ± 0.0046 & 0.9621 ± 0.0003 \\
    PSGv2(Label task)   & \textbf{0.6565 ± 0.0064} & \textbf{0.9428 ± 0.0115} \\ 
    PSG(val as input)   & \textbf{0.6505 ± 0.0055} & \textbf{0.9472 ± 0.0085} \\ \hline
    \end{tabular}
\end{table}



\section{Conclusion}  \label{section_conclusion}
In this paper, we propose a PSG algorithm for link prediction tasks, which includes edge featuring, node encoder, link predictor,  pairwise objective loss, label predictor and contrastive learning framework. PSG achieves top 1 performance on ogbl-ddi until submission and top 3 performance on ogbl-collab. The experimental results verify the superiority of PSG. We will optimize contrastive learning, clustering and attention mechanisms in future work. 


\bibliographystyle{IEEEtran}
\bibliography{mybibliography}
\end{document}