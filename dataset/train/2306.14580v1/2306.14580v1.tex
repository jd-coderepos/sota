\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{cases}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{color}
\usepackage{stmaryrd}
\usepackage{makecell}
\usepackage{graphicx,subfig}
\usepackage{multirow}
\usepackage{pifont}







\title{TransERR: Translation-based Knowledge Graph Completion via Efficient Relation Rotation}



\author{Jiang Li ,  Xiangdong Su \thanks{\ \ Corresponding Author} \\
 College of Computer Science, Inner Mongolia University, Hohhot, China \\  National \& Local Joint Engineering Research Center of Intelligent Information \\ Processing Technology for Mongolian, Hohhot, China \\ \texttt{lijiangimu@gmail.com, cssxd@imu.edu.cn}}




\begin{document}
\maketitle
\begin{abstract}
	This paper presents translation-based knowledge graph completion method via efficient relation rotation (TransERR), a straightforward yet effective alternative to traditional translation-based knowledge graph completion models. Different from the previous translation-based models, TransERR encodes knowledge graphs in the hypercomplex-valued space, thus enabling it to possess a higher degree of translation freedom in mining latent information between the head and tail entities. To further minimize the translation distance, TransERR adaptively rotates the head entity and the tail entity with their corresponding unit quaternions, which are learnable in model training.  The experiments on 7 benchmark datasets validate the effectiveness and the generalization of TransERR. The results also indicate that TransERR can better encode large-scale datasets with fewer parameters than the previous translation-based models. Our code is available at: \url{https://github.com/dellixx/TransERR}.

\end{abstract}

\section{Introduction}
Knowledge graphs (KGs), also known as semantic networks, represent networks of real-world entities ( objects, events, concepts, etc.) and describe the associations between them. In fact, KGs contain factual triplets , which are denoted as . Several open source KGs are available, including FreeBase~\cite{4bollacker2008freebase}, DBpedia~\cite{45lehmann2015dbpedia} and  NELL~\cite{46mitchell2018never}. They facilitate the development of downstream tasks, such as question answering~\cite{2}, semantic search~\cite{3junior2020knowledge} and relation extraction~\cite{1}. However, there is a problem with missing links in KGs. Therefore,  knowledge graph completion (KGC) task has recently received considerable attention.

The mainstream approaches are to learn low-dimensional representations of entities and relations and utilize them to predict new facts. Most of them learn the embeddings of KGs based on score functions of the translational distance between the head and tail entities. TransE~\cite{7bordes2013translating}, TransH~\cite{81wang2014knowledge}, TransR~\cite{9lin2015learning} and TransD~\cite{10ji2015knowledge} adjust embeddings via different projection strategies to represent each relation as a translation vector  to connect head entity  and tail entity  in the real-valued space. 
Recently, to enrich the freedom of translation, RotatE~\cite{31sun2019rotate} and Rotat3D~\cite{15gao2020rotate3d} define each relation as a rotation from  the head entity to the tail entity in the complex space and quaternion space, respectively.




Although the RotatE model has proven effective in modeling complex relational patterns in knowledge graphs, it is constrained by the limitation of being restricted to a single plane of rotation. While the recent Rotat3D model partially addresses this limitation by allowing for rotations in multiple planes, it still only provides one degree of freedom for relational rotation. To deal with above problem, PairRE~\cite{52chao2020pairre}, TripleRE~\cite{yu2022triplere}, and TranSHER~\cite{li-etal-2022-transher}  address the limitations of single-plane rotation by incorporating multiple relations to increase the degree of freedom for relational rotation. However, they are limited by rotation plane restrictions, which can lead to models that are not efficient in representing complex relational patterns. 





To address this issue, we propose TransERR, which encodes knowledge graphs in the hypercomplex-valued space and utilizes two unit quaternion vectors to rotate the head entity and the tail entity, respectively. The unit quaternion vectors are learnable in model training, and they are highly to smooth rotation and spatial translation in the hypercomplex-valued vector space. In addition, two unit quaternion rotation vectors can further narrow the translation distance between the head and tail entities. As a result, TransERR possesses a higher degree of translation freedom in mining latent information between the head and tail entities. We evaluate the effectiveness and generalisation of our model on 7 KG benchmark datasets of different sizes. The experimental results show that TransERR significantly outperforms the existing state-of-the-art distance-based models.




\section{Related Work}

\textbf{Translation Models.} Translation-based models, also known as distance-based models. Motivated by the translation invariance in word2vec \cite{19mikolov2013efficient}, TransE \cite{7bordes2013translating}  defines the distance between  and   with the  or  norm constraint. After that, TransH \cite{81wang2014knowledge}, TransR \cite{9lin2015learning} and TransD \cite{10ji2015knowledge}  employ different projection strategies to adjust graph embeddings. TranSparse \cite{20ji2016knowledge} overcomes heterogeneity and imbalance by combining TranSparse (share) and TranSparse (separate). TransMS \cite{21yang2019transms} performs multi-directional semantic transfer with non-linear functions and linear deviation vectors. RotatE \cite{31sun2019rotate} defines each relation as a rotation for a triplet from the head entity to the tail entity. Rotat3D \cite{15gao2020rotate3d} maps entities into 3D space and defines relations similar to RotatE. However, Rotat3D is limited by optimising the translation distance with only relation embedding information. Recently, PairRE~\cite{52chao2020pairre}, TripleRE~\cite{yu2022triplere} and TranSHER~\cite{li-etal-2022-transher} employ multiple relations to improve the degree of freedom for relational rotation. Different from previous work, TransERR processes a high degree of rotation in the quaternion space.  Benefiting from our model structure, TransERR allows for better optimization of the translation distance between entities, and thus better mine the latent information.





\textbf{Semantic Matching Models.}
Semantic matching  models mine possible semantic associations between entities and relations. The RESCAL~\cite{27nickel2011three} represents each relation as a non-singular matrix. DistMult~\cite{29yang2014embedding} uses a diagonal matrix rather than a non-singular matrix to address the problem of the excessive number of parameters in RESCAL. ComplEx~\cite{14trouillon2016complex} introduces complex-valued spaces into the KGs. TuckER~\cite{TuckER2019tucker} employs Tucker decomposition of a binary tensor to model a KG. SimplE~\cite{58fatemi2019improved} extends SimplE~\cite{kazemi2018simple} and focus on encoding the subrelation pattern. QuatE~\cite{zhang2019quaternion} takes advantage of quaternion representations to enable rich interactions between entities and relations. Deep neural networks have received a great deal of attention in recent years, e.g., ConvE~\cite{dettmers2018convolutional}, ConvKB~\cite{nguyen2017novel}, InteractE~\cite{vashishth2020interacte} and R-GCNs~\cite{schlichtkrull2018modeling}. However, they are difficult to analyze as they work as a black box.





\begin{figure*}[!htb]
	\centering
	\includegraphics[width=1.9\columnwidth]{img/transqr1.png}
	\caption{Illustration of TransE, RotatE and TransERR. TransE, RotatE and TransERR encode knowledge graphs in the real-valued space, complex-valued space and hypercomplex-valued space, respectively.  denotes Hadamard product.  The distance function of TransERR is .}
	\label{fig:transqr}
\end{figure*}





\section{Background and Notation}\label{sec3}

\textbf{Link Prediction.}
A knowledge graph is usually described as , where ,   and  denote the set of entities, relations and triplets , respectively. Specifically, given , link prediction is to predict the tail entity in the triplet. Alternatively, given , the task is to predict the head entity. The existing predicting models calculate the score function  and expect that the scores of correct triplets are higher than those of invalid triplets.



\textbf{Quaternion Algebra.} The quaternion is an extension of the complex number in the four-dimensional space. It consists of a real part and three imaginary parts, which is proposed by William Rowan Hamilton \cite{34hamilton1844theory}.
A quaternion  is defined as , where  is real unit and , ,  are three imaginary units. In addition, .


\begin{itemize}
	\item The unit quaternion  is defined as
	      

	\item Hamilton product of two quaternions is
	      
\end{itemize}





\section{Methodology}

\subsection{TransERR}\label{sec4.1}
Quaternions enable expressive rotation in the hypercomplex-valued space and have more degree of freedom than translation in the real-valued space. Hence, to obtain a greater degree of translation freedom in the link prediction, TransERR encodes knowledge graphs in the hypercomplex-valued space. In addition, we rotate head entity  and tail entity  via two unit quaternion vectors  and , as shown in Figure~\ref{fig:transqr}. Unlike TransE and RotatE, TransERR utilizes Hamilton product  rather than Hadamard product  in project operation to better capture  the underlying semantic features between the head entity and the tail entity embeddings. Firstly, given a triplet , we encode ,  and  in the quaternion space, which are denoted as

where , ,  and . Next, we define two additional quaternion vectors to  rotate the head and tail entity, which are represented as  and , where  and . Then, we normalize these two additional quaternions ( and ) to the unit quaternions ( and ) to eliminate the scaling effect. They are denoted as








The normalized operation ensures the stability of entity rotation in the quaternion space. The unit quaternion vectors are highly desirable to smooth rotation and spatial translation in the hypercomplex-valued space. In addition, two unit quaternion rotation vectors can further narrow the translation distance between the head and tail entities. Finally, we employ  and  to rotate head entity  and tail entity , respectively. Specifically, we use Hamilton product to achieve rotation operation since Hamilton product makes quaternion more expressive at rotational capability. For each triplet , we define the distance function of TransERR as



The score function  and  is defined in Section~\ref{sec3}. Following \citet{31sun2019rotate}, we employ the self-adversarial negative sampling loss for TransERR, which is defined as



where  is the sigmoid function, and  is a fixed margin.  and  represent the -th negative triplet and the distance function of the -th negative triplet. 



\linespread{1.2}
\begin{table*}[!htb]
	\centering
	\scalebox{0.9}{\begin{tabular}{ccrcc}
			\hline
			         & \multicolumn{4}{c}{\textbf{ogbl-wikikg2}}                                                                                                        \\ \cmidrule(lr){2-5}
			         & \#Dim                                     & \#Params                                & Test MRR           & Valid MRR                    \\ \hline
			TransE   & 500                                       & 1250.57M                                & 0.4256  0.0030             & 0.4272  0.0030                         \\
			DistMult & 500                                       & 1250.57M                                & 0.3729  0.0045             & 0.3506  0.0042                        \\
			ComplEx  & 250                                       & 1250.44M                                & 0.4027  0.0027            & 0.3759  0.0016	                        \\
			RotatE   & 250                                       & 1250.44M                                & 0.4332  0.0025             & 0.4353  0.0028                        \\
			PairRE   & 200                                       & 500.33M                                 & 0.5208  0.0027            & 0.5423   0.0020           \\
			TripleRE   & 200                                       & 500.76M                                 & 0.5794  0.0020            & 0.6045   0.0024            \\ \hline
			
			TransERR (ours)  & 100                                       & \textbf{250.22M}                                 & \underline{0.6100}  0.0020 & \underline{0.6246}  0.0019               \\
			TransERR (ours) & 200                                       & 500.44M                                 & \textbf{0.6359}  0.0020    & \textbf{0.6518}  0.0011       \\ \hline
		\end{tabular}}
	\caption{Results on ogbl-wikikg2. Results are taken from  official leaderboard \cite{hu2020open}.  Best results are in bold.}
	\label{tab:ogbl}
\end{table*}


\linespread{1.2}
\begin{table}[h]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{cccccc}\hline
			\textbf{Dataset} &  &  & \textbf{\#Train} & \textbf{\#Valid} & \textbf{\#Test} \\ \hline
			ogbl-wikikg2     & 2,500k          & 535             & 16,109k          & 429k             & 598k            \\
			YAGO3-10         & 123k            & 37              & 1,079k           & 5k               & 5k              \\
			DB100K           & 100k            & 470             & 597k             & 50k              & 50k             \\
			FB15K            & 15k             & 237             & 483k             & 50k              & 50k             \\
			WN18             & 41k             & 18              & 141k             & 5k               & 5k              \\
			FB15K-237        & 15k             & 237             & 272k             & 18k              & 20k             \\
			WN18RR           & 41k             & 11              & 87k              & 3k               & 3k              \\ \hline
		\end{tabular}
	}
	\caption{Statistics of the datasets in the experiment.}

	\label{tab:dataset}
\end{table}


\begin{table*}[!htb]
	\centering
	\scalebox{0.84}{
		\begin{tabular}{ccccccccccc}
			\hline
			                   & \multicolumn{5}{c}{\textbf{WN18RR}} & \multicolumn{5}{c}{\textbf{FB15K-237}}                                                                                                                                       \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
			                   & MR                                  & MRR                                    & Hits@10        & Hits@3         & Hits@1         & MR           & MRR            & Hits@10        & Hits@3         & Hits@1         \\ \hline
			TransE    & 3384                                & 0.266                                  & 0.501          & -              & -              & 357          & 0.294          & 0.465          & -              & -              \\
			DistMult  & 5110                                & 0.43                                   & 0.49           & 0.44           & 0.39           & 254          & 0.241          & 0.419          & 0.263          & 0.155          \\
			ComplEx   & 5261                                & 0.44                                   & 0.51           & 0.46           & 0.41           & 339          & 0.247          & 0.428          & 0.275          & 0.158          \\
			TuckER             & -                                   & 0.470                                  & 0.526          & 0.482          & 0.443          & -            & 0.358          & 0.544          & 0.394          & \textbf{0.266} \\

			RotatE    & 3340                                & 0.476                                  & 0.571          & 0.492          & 0.428          & 177          & 0.338          & 0.533          & 0.375          & 0.241          \\
			Rotat3D    & 3328                                & 0.489                                  & 0.579          & 0.505          & 0.442          & 165          & 0.347          & 0.543          & 0.385          & 0.250          \\
			QuatE              & 3472                                & 0.481                                  & 0.564          & 0.500          & 0.436          & 176          & 0.311          & 0.495          & 0.342          & 0.221          \\
			ATTH               & -                                   & 0.466                                  & 0.551          & 0.484          & 0.419          & -            & 0.324          & 0.501          & 0.354          & 0.236          \\
			Rot\_Pro           & 2815                                & 0.457                                  & 0.557          & 0.482          & 0.397          & 201          & 0.344          & 0.540          & 0.383          & 0.246          \\
			PairRE             &     -                               & -                                      & -              & -              & -              & 160          & 0.351          & 0.544          & 0.387          & 0.256          \\ 
			TripleRE   & -                                     & -                                   & -              & -              & -              & 142   & 0.351          & 0.544          & 0.387          & 0.256          \\
			TranSHER   & -                                     & -                                   & -              & -              & -              & -   & \textbf{0.360}          & 0.551          & 0.397          & 0.264          \\  \hline
			
			
			TransERR            & \textbf{1167}                       & \textbf{0.501}                         & \textbf{0.605} & \textbf{0.520} & \textbf{0.450} & \textbf{125} & \textbf{0.360} & \textbf{0.555} & \textbf{0.396} & 0.264          \\ \hline
		\end{tabular}}
	\caption{Results on WN18RR and FB15K-237. Results of  are taken from  \citet{31sun2019rotate}. The best results are in bold. The dashes means that the results are not reported in the responding literature.}

	\label{tab:wn18rr+fb237}
\end{table*}




\section{Experimental Setup}


\subsection{Datasets}
We utilize the 7 most commonly utilized link prediction datasets and validate the effectiveness and generalizability of TransERR. We summarise the details of datasets in Table~\ref{tab:dataset}. ogbl-wikikg2~\cite{hu2020open} is a very large-scale dataset extracted from the Wikidata knowledge base~\cite{10.1145/2629489}. YAGO3-10~\cite{6suchanek2007yago} is a subset of YAGO3, which are mainly from Wikipedia. DB100k~\cite{63ding2018improving} is a subset of DBpedia. FB15K~\cite{7bordes2013translating} is a subset of the Freebase knowledge base, while {FB15k-237} \cite{36toutanova2015observed} removes the inversion relations from FB15K. {WN18}~\cite{7bordes2013translating} is extracted from WordNet~\cite{5miller1995wordnet}. {WN18RR} \cite{11dettmers2018convolutional} removes inversion relations similar to FB15K-237.






\subsection{Evaluation Protocol}

Following the SOTA methods \cite{31sun2019rotate,52chao2020pairre}, the quality of the ranking of each test triplet is evaluated via calculating all possible substitutions of head entity and tail entity :  and , where , . We evaluate the performance using the standard evaluation metrics, including Mean Rank (MR), Mean Reciprocal Rank (MRR) and Hits@N. Hits@N measures the percentage of correct entities in the top  predictions. Higher values of MRR and Hits@N indicate better performance. Nevertheless, MR is the exact opposite of MRR and Hits@n. Hits@N ratio with cut-off values . For experiments on ogbl-wikikg2, we follow the evaluation protocol of ogbl-wikikg2 benchmarks~\cite{hu2020open}. Other experimental setups can be found in the Appendix~\ref{app-c}.



\subsection{Implementation} 
We implement our proposed model via pytorch. We use Adam~\cite{adam} optimizer, and employ grid search to find the best hyperparameters based on the performance on the validation datasets. We report averaged test results across ten runs and use the random seeds from 0 to 9. We omit the variance as it is generally low.  We employ the official implementations~\cite{hu2020open} for ogbl-wikikg2 \footnote{\url{https://ogb.stanford.edu/}}. 

\linespread{1.2}
\begin{table*}[!htb]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{ccccccccc}
			\hline
																																						   & \multicolumn{2}{c}{\textbf{FB15K}} & \multicolumn{2}{c}{\textbf{FB15K-237}} & \multicolumn{2}{c}{\textbf{WN18}} & \multicolumn{2}{c}{\textbf{WN18RR}}                                                                      \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			\textbf{Scoring Function}                                                                                                                      & MRR                                & Hits@10                                & MRR                               & Hits@10                             & MRR            & Hits@10        & MRR            & Hits@10         \\ \hline
			  & 0.391                              & 0.561                                  & 0.235                             & 0.419                               & 0.909          & 0.944          & 0.469          & 0.563           \\
			  & 0.732                              & 0.860                                  & 0.352                             & 0.537                               & 0.921          & 0.963          & 0.481          & 0.580           \\
			                                   & 0.425                              & 0.598                                  & 0.290                             & 0.454                               & 0.950          & 0.961          & 0.492          & 0.586           \\
			 & \textbf{0.815}                     & \textbf{0.896}                         & \textbf{0.360}                    & \textbf{0.555}                      & \textbf{0.953} & \textbf{0.965} & \textbf{0.501} & \textbf{ 0.605} \\ \hline
		\end{tabular}}
		\caption{Ablation of TransERR on FB15K, FB15K-237, WN18 and WN18RR.  and  are Hadamard product and Hamilton product, respectively.  and  are normalized complex vectors.  and  are normalized quaternion vectors.}
	\label{abs}
\end{table*}

\section{Results and Analysis}\label{sec6}

\subsection{Main Results}\label{sec6.1}

To evaluate the effectiveness and the generalization of TransERR, we perform the experiments on 7 benchmark datasets of different scales. The results on ogbl-wikikg2 are shown in Table~\ref{tab:ogbl}. TransERR achieves competitive results on large-scale datasets. On ogbl-wikikg2,  TransERR obtains significant improvements of  than TripleRE with the same number of parameters (\#Dim 200) in Test MRR.  It is worth noting that TransERR still outperforms PairRE, TripleRE with fewer parameters (\#Dim 100). The results demonstrate that TransERR has a powerful capability to model large-scale KGs.








The comparison results for WN18RR and FB15K-237 are shown in Table~\ref{tab:wn18rr+fb237}. TransERR outperforms all the baselines in all metrics on WN18RR. Compared with RotatE, TransERR achieves improvements of , , ,  and , respectively. For FB15K-237, TransERR gains competitive results than existing distance-based models in MR, MRR, Hit@10 and Hits@3. The above results confirm that TransERR has merit in modeling graph embeddings and improves the link prediction performance. This is because quaternions enable expressive rotation in the hypercomplex-valued space, and two unit quaternion vectors further narrow the translation distance between the head and tail entities. Please refer to Appendix~\ref{app-d} for additional dataset results. 




	





\subsection{Ablation Study}\label{sec6.5}
In this part, in order to demonstrate TransERR can better capture latent information between entity embeddings in the hypercomplex-valued space than in the complex-valued space, we encode TransERR in the complex space and conduct experiments on FB15K, FB15K-237, WN18 and WN18RR. As shown in the second and fourth rows of Table~\ref{abs},  we can observe that TransERR behaves better in the quaternion space. This is further evidence that TransERR can facilitate interaction information between the head and tail entities in the hypercomplex-valued space. Furthermore, we conduct an ablation study on quaternion normalization for TransERR, where rows one and two are a control group and rows three and four are a control group in Table~\ref{abs}. We remove the normalization step for  and  and utilize   and  to rotate head entities and tail entities in the complex-valued space and in the hypercomplex-valued space, respectively. We conclude that the relational rotation's geometric property is lost, leading to poor performance. In addition, the unit quaternion has a high degree of smooth rotation and spatial transformation ability than the unit complex number.




\section{Conclusion}
This paper proposes a simple yet effective distance-based KGC model (TransERR), which rotates entities with two normalized quaternion vectors in the hypercomplex-valued space. TransERR possesses a higher degree of translation freedom for graph embeddings. The experiments also suggest that TransERR can maximize interaction information between entities in the hypercomplexvalued space. The experimental results fully illustrate the effectiveness and generalizability of our model. In addition, the results show that two unit quaternions can further narrow the distance between the head and tail entities, and thus avoid information loss in rotation. 



\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix





\section{Hyperparameters}\label{app-c}


We list the best hyperparameters setting of TransERR on 7 benchmark datasets in Table~\ref{hyper}. , , ,  and  denote the embedding size, the learning rate, the negative sample, the self-adversarial sampling temperature and fixed margin, respectively.  In general, the embedding size  is tuned amongst , the learning rate  is tuned amongst , the negative sample  is selected in , the self-adversarial sampling temperature  is selected from , the fixed margin  are searched from 5 to 30.  All optimal hyperparameters are selected on the validation set.
\begin{table*}[!htb]
	\centering

	\scalebox{0.85}{
		\begin{tabular}{cccccc}
			\hline
			\textbf{Benchmark} & \textbf{embedding dimension}  & \textbf{learning rate}  & \textbf{negative sample}  &  &  \\ \hline
			ogbl-wikikg2       & 200                              &                           & 128                          & 1.0      & 15       \\
			ogbl-wikikg2       & 100                              &                           & 128                          & 1.0      & 8        \\
			YAGO3-10           & 1000                             &                           & 256                          & 0.5      & 30       \\
			DB100K             & 1000                             &                           & 128                          & 0.5      & 20       \\
			FB15K              & 1500                             &                           & 256                          & 1.0      & 26       \\
			WN18               & 1000                             &                           & 128                          & 0.5      & 10       \\
			FB15K-237          & 1000                             &                           & 128                          & 0.5      & 20       \\
			WN18RR             & 1000                             &                           & 128                          & 0.5      & 12       \\ \hline
		\end{tabular}}
		\caption{The best hyperparameter setting of TransERR on 10 benchmarks datasets.}
		\label{hyper}
\end{table*}



\section{Additional Dataset Results}\label{app-d}


The comparison results for WN18 and FB15K are shown in Table~\ref{tab:wn+fb}. We employ the same hyper-parameter settings and implementation compared with the previous works.  Since both the baselines and TransERR almost obtains the upper bound on Hits@N, the improvement of TransERR is still considered effective. We can see that TransERR achieves significant improvements on WN18 and FB15K.


The results on YAGO3-10 and DB100K datasets are shown in Table~\ref{tab:yago+db}. TransERR achieves the best results in all metrics for YAGO3-10. On DB100K, compared with the latest TranSHER, TransERR produces the optimal performance in all metrics except MR, which obtains significant improvements of , ,  and , respectively.




\begin{table*}[htb]
	\centering
	\scalebox{0.85}{
		\begin{tabular}{ccccccccccc}
			\hline
			                   & \multicolumn{5}{c}{\textbf{WN18}} & \multicolumn{5}{c}{\textbf{FB15K}}                                                                                                                                      \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
			                   & MR                                & MRR                                & Hits@10        & Hits@3         & Hits@1         & MR          & MRR            & Hits@10        & Hits@3         & Hits@1         \\ \hline
			TransE    & -                                 & 0.495                              & 0.943          & 0.888          & 0.111          & -           & 0.463          & 0.749          & 0.578          & 0.297          \\
			DistMult  & 665                               & 0.797                              & 0.946          & -              & -              & 42          & 0.798          & 0.893          & -              & -              \\
			ComplEx   & -                                 & 0.941                              & 0.947          & 0.945          & 0.936          & -           & 0.692          & 0.840          & 0.759          & 0.599          \\
TuckER             & -                                 & \textbf{0.953}                     & 0.958          & 0.955          & \textbf{0.949} & -           & 0.795          & 0.892          & 0.833          & 0.741          \\
			RotatE    & 309                               & 0.949                              & 0.959          & 0.952          & 0.944          & 40          & 0.797          & 0.884          & 0.830          & 0.746          \\
			Rotat3D    & 214                               & 0.951                              & 0.961          & 0.953          & 0.945          & 39          & 0.789          & 0.887          & 0.835          & 0.728          \\
			QuatE              & 338                               & 0.949                              & 0.960          & 0.954          & 0.941          & 41          & 0.770          & 0.878          & 0.821          & 0.700          \\
			PairRE             & 401                                 & 0.941                                  & 0.956              & 0.950              & 0.940              & 37 & 0.811          & \textbf{0.896} & 0.845          & 0.765          \\ 
			TripleRE   & -                                     & -                                   & -              & -              & -              & \textbf{35}   & 0.747          & 0.877          & 0.813          & 0.662          \\ \hline
			
			TransERR            & \textbf{82}                       & \textbf{0.953}                     & \textbf{0.965} & \textbf{0.957} & 0.945          & 41          & \textbf{0.815} & \textbf{0.896} & \textbf{0.848} & \textbf{0.767} \\ \hline
		\end{tabular}}
		\caption{Results on WN18 and FB15K. Results of  are taken from \citet{31sun2019rotate}.  Other results are taken from the corresponding original papers. The best results are in bold. The dashes means that the results are not reported in the responding literature.}
		\label{tab:wn+fb}
	\end{table*}

\begin{table*}[h]
	\centering
	\scalebox{0.85}{
		\begin{tabular}{ccccccccccc}
			\hline
			         & \multicolumn{5}{c}{\textbf{YAGO3-10}} & \multicolumn{5}{c}{\textbf{DB100K}}                                                                                                                              \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
			         & MR                                    & MRR                                 & Hits@10        & Hits@3         & Hits@1         & MR  & MRR            & Hits@10        & Hits@3         & Hits@1         \\ \hline
			TransE   & -                                     & -                                   & -              & -              & -              & -   & 0.111          & 0.270          & 0.164          & 0.016          \\
			DistMult & 5926                                  & 0.34                                & 0.54           & 0.38           & 0.24           & -   & 0.233          & 0.448          & 0.301          & 0.115          \\
			ComplEx  & 6351                                  & 0.36                                & 0.55           & 0.40           & 0.26           & -   & 0.242          & 0.440          & 0.312          & 0.126          \\
			ConvE    & 1671 & 0.44  & 0.62    & 0.49   & 0.35   & -   & -     & -       & -      & -      \\
			Rot\_Pro & 1797                                  & 0.542                               & 0.699          & 0.596          & 0.443          & -   & -              & -              & -              & -              \\
			PairRE   & -                                     & -                                   & -              & -              & -              & -   & 0.412          & 0.600          & 0.472          & 0.309          \\ 
			TranSHER   & -                                     & -                                   & -              & -              & -              & -   & 0.431          & 0.589          & 0.476          & 0.345          \\ \hline
			
			TransERR  & \textbf{476}                          & \textbf{0.546}                      & \textbf{0.706} & \textbf{0.601} & \textbf{0.456} & \textbf{571} & \textbf{0.465} & \textbf{0.622} & \textbf{0.510} & \textbf{0.380} \\ \hline
		\end{tabular}}
	\caption{Results on YAGO3-10 and DB100K. Results are taken from the corresponding original papers. The best results are in bold. The dashes means that the results are not reported in the responding literature.}
	\label{tab:yago+db}
\end{table*}
\end{document}
