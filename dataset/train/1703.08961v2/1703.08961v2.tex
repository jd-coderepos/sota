\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{tikz}
\usepackage{iccv}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\newtheorem{prop}{Proposition}


\newcommand{\citep}{\cite}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\iccvfinalcopy

\ificcvfinal\pagestyle{empty}\fi
\def\iccvPaperID{3037} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Scaling the Scattering Transform: Deep Hybrid Networks}

\author{Edouard Oyallon\\
D\'epartement Informatique\\
Ecole Normale Sup\'erieure\\
Paris, France\\
{\tt\small edouard.oyallon@ens.fr}
\and
Eugene Belilovsky\\
University of Paris-Saclay\\ INRIA
and KU Leuven\\
{\tt\small eugene.belilovsky@inria.fr}
\and
Sergey Zagoruyko\\
 Universit\'e Paris-Est\\
  \'Ecole des Ponts ParisTech\\
 Paris, France\\
{\tt\small sergey.zagoruyko@enpc.fr}
}

\maketitle




\begin{abstract}






We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of  convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of  on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset. 

\end{abstract}




\section{Introduction}
Image classification is a high dimensional problem that requires building lower dimensional representations that reduce the non-informative images variabilities. For example, some of the main source of variability are often due to geometrical operations such as translations and rotations. An efficient classification pipeline necessarily builds invariants to these variabilities. Deep architectures build representations that lead to state-of-the-art results on image classification tasks \citep{he2015deep}. These architectures are designed as very deep cascades of non-linear end-to-end learned modules \citep{lecun2010convolutional}.  When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets \citep{zeiler2014visualizing,huh2016makes}, which indicate they have captured generic properties of a supervised task that consequently do not need to be learned. Indeed several works indicate geometrical structures in the filters of the earlier layers \cite{krizhevsky2012imagenet,waldspurger2015these} of Deep CNNs. However, understanding the precise operations performed by those early layers is a complicated \cite{szegedy2013intriguing,oyallon2017building} and possibly intractable task. In this work we investigate if it is possible to replace these early layers, by simpler cascades of non-learned operators that reduce variability while retaining discriminative information.

Indeed, there can be several advantages to incorporating pre-defined geometric priors, via a hybrid approach of combining pre-defined and learned representations. First, end-to-end pipelines can be data hungry and ineffective when the number of samples is low. Secondly, it could permit to obtain more interpertable classification pipelines which are amenable to analysis. Finally, it can reduce the spatial dimensions and the required depth of the learned modules.


A potential candidate for an image representation is the SIFT descriptor \cite{lowe1999object} that was widely used before 2012 as a feature extractor in classification pipelines \cite{sanchez2011high,sanchez2013image}. This representation was typically encoded via an unsupervised Fisher Vector (FV) and fed to a linear SVM. However, several works indicate that this is not a generic enough representation to build further modules on top of  \cite{le2011learning,bo2013multipath}. Indeed end-to-end learned features produce substantially better classification accuracy.
A major improvement over SIFT can be found in the scattering transform \cite{mallat2012group,bruna2013invariant,sifre2013rotation}, which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability. Scattering transforms have been shown to already produce representations that lead to the top results on complex image datasets when compared to other unsupervised representations (even learned ones) \citep{oyallon2015deep}. This makes them an excellent candidate for the initial layers of a deep network. We thus investigate the use of scattering as a generic representation to combine with deep neural networks.

Related to our work \cite{perronnin2015fisher} proposed a hybrid representation for large scale image recognition combining a predefined representation and Neural Networks (NN), that uses Fisher Vector encoding of SIFT and leverages NNs as scalable classifiers. In contrast we use the scattering transform in combination with convolutional architectures. Our main contributions are as follows: First, we demonstrate that using supervised local descriptors, obtained by shallow  convolutions, with very small spatial window sizes permits to obtain AlexNet accuracy on the imagenet classification task (Subsection \ref{supervised_encoding}). We show empirically these encoders build explicit invariance to local rotations (Subsection \ref{sec:sle_first_sec}). Second, we propose hybrid networks that combine scattering with modern CNNs  (Section \ref{small2}) and show that using scattering and a ResNet of reduced depth, we obtain similar accuracy to ResNet-18 on Imagenet (Subsection \ref{hybimnet}). Finally, we demonstrate in Subsection \ref{verysmall} that scattering permits a substantial improvement in accuracy in the setting of limited data.  

Our highly efficient GPU implementation of the scattering transform is, to our knowledge, orders of magnitude faster than any other implementations, and allows training very deep networks applying scattering on the fly. Our scattering implementation \footnote{\url{http://github.com/edouardoyallon/pyscatwave}} and pre-trained hybrid models \footnote{\url{http://github.com/edouardoyallon/scalingscattering}}are available.
















\if false
Deep architectures build generic and low-dimensional representations that lead to state-of-the-art results on complex tasks such as classification \citep{he2015deep},  game strategies  \citep{silver2016mastering}, or image generation \citep{radford2015unsupervised}.
These architectures are designed as very deep cascades of non-linear end-to-end learned modules. It means that contrary to methods using predefined representations \cite{ref}, they can adapt their representation to a specific task such that they do not reduce important variabilities relatively to their objective. When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets \citep{zeiler2014visualizing,huh2016makes}, which indicate they have captured generic properties of a supervised task. However, understanding the nature of the internal layers is a difficult task \citep{szegedy2013intriguing}.

Until 2012, unsupervised pipelines composed of predefined features and possibly , have led to state of the arts on challenging datasets, including the imagenet. Usually in such an approach, generic local \cite{otero2015anatomy} SIFT decriptors  \cite{lowe1999object} are densely extracted at the initial layer. A major improvement over the SIFT can be found in the Scattering transform \cite{} which permits to retain discriminative information normally discarded. They consist of a cascade of  predefined wavelet transforms and modulus nonlinearities followed by a final spatial averaging and have proven to be successful  in classification tasks involving textures \citep{bruna2013invariant,sifre2013rotation}, hand-written digits \citep{bruna2013invariant}, and sound \citep{anden2014deep}. It is a deep generic interpretable representation that has also shown to lead to the top results on complex image datasets when compared to other unsupervised representations (even learned ones) \citep{oyallon2015deep}. Nevertheless, these representations do not adapt to the specific bias of each dataset and there can be a large performance gap between supervised and unsupervised representations \citep{oyallon2015deep}. 

While several works \cite{???} suggest that cascading a deep networks on top of pre-defined features is not efficient, more recently \cite{perronnin2015fisher} showed it was possible to combine a supervised neural network with Fisher Vectors (FVs) learned from SIFTs, and obtain an accuracy that approaches that of the AlexNet \cite{krizhevsky2012imagenet}. 
Indeed, incorporating pre-defined geometric priors, via a hybrid approach might be very helpful in the case of limited samples (common in application like medical imaging, as few high-dimensional samples are available and pretrained CNN features are unefficient \cite{}. It could also permit to obtain more interpertable classification pipelines which are amenable to analysis, and is likely to reduce the required depth of the learned modules.


In this work, we cascade deep neural network architectures on top of Scattering Networks. Notably, we introduce a \textit{Shared Local Encoder}, that is a supervised encoder of scattering representations which lead to AlexNet accuracy, while being relatively shallow. The learned portion consists of only three  convolutional layers and 2 fully connected layers. We show it builds explicit invariances to the rotation group. Secondly, we show that it is possible to achieve performance comparable to similar accuracies on ILSVRC2012 by cascading a Resnet \cite{he2015deep,zagoruyko2016wide} on top of a Scattering Network. Finally, we exhibit the state of the art on unsupervised CIFAR10 and on the STL10 datasets, without using the unlabeled samples.









Section \ref{hyb}  describes how the scattering network is constructed, and explains the use of supervision on top of scattering, while the last section motivates the use of a Shared Local Encoder. Section \ref{sle_sec} analyses the performance of our encoder on ILSVRC2012, its first layer and the discriminability properties of the inner layers. Finally, we analyze the performances of our network in Section \ref{small2}, with standard architectures, on CIFAR10, ILSVRC2012 and STL10. Numerical experiments are reproducible; code and models will be available online.
\fi




 
\section{Scattering Networks and Hybrid Architectures}
\label{hyb}









We introduce the scattering transform and motivate its use as a generic input for supervised tasks. A scattering network belongs to the class of CNNs whose filters are fixed as wavelets \citep{oyallon2015deep}. The construction of this network has strong mathematical foundations \citep{mallat2012group}, meaning it is well understood, relies on  few parameters and is stable to a large class of geometric transformations.  In general, the parameters of this representation do not need to be adapted to the bias of the dataset \citep{oyallon2015deep}, making its output a suitable generic representation.
 
We then propose and motivate the use of  supervised CNNs built on top of the scattering network. Finally we propose a supervised encodings of scattering coefficients using 1x1 convolutions, that can retain interpertability and locality properties.






\subsection{Scattering Networks}
\label{scatnet}









\begin{figure}\begin{center}
\label{archi}


 
  
\begin{tikzpicture}\node at (-1,0) [draw=none,line width=0] (x) {};

\node at (0,0) [minimum width=0.8cm,draw,color=black!60!green] (W1) {};
\node at (1.5,0) [minimum width=0.8cm,draw,color=red] (W2) {};
\node at (2.8,0) [minimum width=0.8cm,draw] (A1) {};
\node at (3.8,0) [draw=none,line width=0] (S) {};


\draw[->,,black] (x) -- (W1);
\draw[->,black] (W1) -- (W2);
\draw[->,black] (W2) -- (A1);
\draw[->,black] (A1) -- (S);
\draw [-<,black] (0.8,0)  -| (0.8,0.5) --(2.8,0.5)|- (2.8,0.32);
\draw [->,black] (-0.6,0) -| (-0.6,-0.5) --(2.8,-0.5)|- (2.8,-0.28);

\end{tikzpicture}

\end{center}
\caption{A scattering network.  concatenates the averaged signals.}
\end{figure}




In this section, we recall the definition of the scattering transform. Consider a signal , with  the spatial position index and an integer , which is the spatial scale of our scattering transform. Let  be a local averaging filter with a spatial window of scale  (here, a Gaussian smoothing function). Applying the local averaging operator,  we obtain the zeroth order scattering coefficient, . This operation builds an approximate invariant to translations smaller than , but it also results in a loss of high frequencies that are necessary to discriminate signals. 

A solution to avoid the loss of high frequency information is provided by the use of wavelets. A wavelet is an integrable and localized function in the Fourier and space domain, with zero mean. A family of wavelets is obtained by dilating a complex mother wavelet  (here, a Morlet wavelet) such that , where  is the rotation by , and  is the scale of the wavelet. A given wavelet  has thus its energy concentrated at a scale , in the angular sector . Let  be an integer parametrizing a discretization of . A wavelet transform is the convolution of a signal with the family of wavelets introduced above, with an appropriate downsampling:

Observe that  and  have been discretized: the wavelet is chosen to be selective in angle and localized in Fourier. 
With appropriate discretization \cite{oyallon2015deep},  is approximatively an isometry on the set of signals with limited bandwidth, and this implies the energy of the signal is preserved. This operator then belongs to the category of multi-resolution analysis operators, each filter being excited by a specific scale and angle, but with the output coefficients not being invariant to translation. To achieve invariance we can not apply  to  since it gives a trivial invariant, namely zero.



To tackle this issue, we apply a non-linear point-wise complex modulus to , followed by an averaging , which builds a non trivial invariant. Here, the mother wavelet is analytic, thus  is  regular \citep{bernstein2013generalized} which implies that the energy in Fourier of  is more likely to be contained in a lower frequency domain than . Thus,  preserves more energy of . It is possible to define , which can also be written as: ; this is the first order scattering coefficients. Again, the use of the averaging  builds an invariant to translation up to .

Once more, we apply a second wavelet transform , with the same filters as , on each channel. This permits the recovery of the high-frequency lost due to the averaging applied to the first order, leading to , which can also be written as . We only compute increasing paths, e.g.  because non-increasing paths have been shown to bear no energy \citep{bruna2013invariant}. We do not compute higher order scatterings, because their energy is negligible \citep{bruna2013invariant}. We call  the final scattering coefficient corresponding to the concatenation of the order 0, 1 and 2 scattering coefficients, intentionally omitting the path index of each representation. In the case of colored images, we apply independently a scattering transform to each RGB channel of the image, which means   has a size equal to , and the original image is down-sampled by a factor  \cite{bruna2013invariant}.

This representation is proved to linearize small deformations \cite{mallat2012group} of images, be non-expansive and almost complete \cite{dokmanic2016inverse,bruna2013audio}, which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator. We discuss it as an ideal initialization in the next subsection.  

\subsection{Cascading a supervised Deep architecture}
We now motivate the use of a supervised architecture on top of a scattering network. 
Scattering transforms have yielded excellent numerical results \cite{bruna2013invariant} on datasets where the variabilities are completely known, such as MNIST or FERET. In these task, the problems encountered are linked to sample and geometric variance and handling these variances leads to solving these problems. However, in classification tasks on more complex image datasets, such variabilities are only partially known as there are also non geometrical intra-class variabilities. Although applying the scattering transform on datasets like CIFAR or Caltech leads to nearly state-of-the-art results in comparison to other unsupervised representations there is a large gap in performance when comparing to supervised representations \cite{oyallon2015deep}. CNNs fill in this gap, thus we consider the use of deep neural networks utilizing generic scattering representations in order to reduce more complex variabilities than geometric ones.





 Recent works \citep{mallat2016understanding,bruna2013learning,jacobsen2017multiscale} have suggested that  deep networks could  build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions. This group of symmetry corresponds to some of the non-informative intra class variabilities, which must be reduced by a supervised classifier. \citep{mallat2016understanding} motivates that to each layer corresponds an approximated Lie group of symmetry, and this approximation is progressive, in the sense that the dimension of these groups is increasing with depth. For instance, the main linear Lie group of symmetry of an image is the translation group, . In the case of a wavelet transform obtained by rotation of a mother wavelet, it is possible to recover a new subgroup of symmetry after a modulus non-linearity, the rotation , and the group of symmetry at this layer is the roto-translation group: . If no non-linearity was applied, a convolution along  would be equivalent to a spatial convolution. Discovering explicitly the next new and non-geometrical groups of symmetry is however a difficult task \cite{jacobsen2017multiscale}; nonetheless, the roto-translation group seems to be a good initialization for the first layers. In this work, we investigate this hypothesis and avoid learning those well-known symmetries.




Thus, we consider two types of cascaded deep network on top of scattering. The first, referred to as the \textit{Shared Local Encoder} (SLE), learns a supervised local encoding of the scattering coefficients. We motivate and describe the SLE in the next subsection as an intermediate representation between unsupervised local pipelines, widely used in computer vision prior to 2012, and modern supervised deep feature learning approaches. 
The second, referred to as a hybrid CNN, is a cascade of a scattering network and a standard CNN architecture, such as a ResNet \cite{he2015deep}. In the sequel we empirically analyse hybrid CNNs, which permits to greatly reduce the spatial dimensions on which convolutions are learned and can reduce sample complexity.   
  



 
\subsection{Shared Local Encoder for Scattering Representations}
\label{supervised_encoding}






We now discuss the spatial support of different approaches, in order to motivate our local encoder for scattering. In CNNs constructed for large scale image recognition, the representations at a specific spatial location and depth depend upon large parts of the initial input image and thus mixes global  information. For example, at  depth 2 of \cite{krizhevsky2012imagenet}, the effective spatial support of the corresponding filter is already 32 pixels (out of 224). The specific representations derived from CNNs trained on large scale image recognition are often used as representations in other computer vision tasks or datasets \cite{yosinski2014transferable,zeiler2014visualizing}. 

On the other hand prior to  2012 local encoding methods led to state of the art performance on large scale visual recognition tasks \cite{sanchez2011high}. In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors \cite{lowe1999object}, HOG \cite{dalal2005histograms}, and wavelet transforms \cite{serre2004realistic}. They were also often combined with an unsupervised encoding, such as sparse coding \cite{boureau2011ask} or Fisher Vectors(FVs) \cite{sanchez2011high}. Indeed, many works in classical image processing or  classification \cite{koenderink1999structure,boureau2011ask,sanchez2011high,perronnin2015fisher} suggests that the local encoding of an image permit to describe efficiently an image. Additionally for some algorithms that rely on local neighbourhoods, the use of local descriptors is essential \cite{lowe1999object}. Observe that a representation based on local non overlapping spatial neighborhood is simpler to analyze, as there is no ad-hoc mixing of spatial information. Nevertheless, on large scale classification, this approach was surpassed by fully supervised learned methods \cite{krizhevsky2012imagenet}.










We show that it is possible to apply, a similarly local, yet supervised encoding algorithm to a scattering transform, as suggested in the conclusion of \cite{perronnin2015fisher}. First observe that at each spatial position , a scattering coefficient  corresponds to a descriptor of a local neighborhood of spatial size . As explained in the first Subsection \ref{scatnet}, each of our scattering coefficients are obtained using a stride of , which means the final representation can be interpreted as a non-overlapping concatenation of  descriptors. Then, let   be   a cascade of fully connected layers that we identically apply on each .  Then  is a cascade of CNN  operators with spatial support size , thus we write . In the sequel, we do not make any distinction between the  CNN operators and the operator acting on . We refer to  as a \textit{Shared Local Encoder}. We note that similarly to ,  corresponds to non-overlapping encoded descriptors. To learn a supervised classifier on a large scale image recognition task, we cascade fully connected layers on top of the SLE.

Combined with a scattering network, the supervised SLE, has several advantages. Since the input corresponds to scattering coefficients, whose channels are structured, the first layer of  is as well structured. We further explain and investigate this first layer in Subsection \ref{sec:sle_first_sec}. Unlike standard CNNs, there is no linear combinations of spatial neighborhoods of the different feature maps, thus the analysis of this network need only focus on the channel axis.  Observe that if  was fed with raw images, for example in gray scale, it could not build any non-trivial operation except separating different level sets of these images.  


In the next section, we investigate empirically this supervised SLE trained on the ILSVRC2012 dataset.















 





\section{Local Encoding of Scattering}
\label{sle_sec}
We evaluate the supervised SLE on the Imagenet ILSVRC2012 dataset. This is a large and challenging natural color image dataset consisting of  million training images and  validation images, divided into  classes. We then show some unique properties of this network and evaluate its features on a separate task.
\subsection{Shared Local Encoder on Imagenet}
\label{sec:sle_exp}




\begin{figure}
\begin{center}
\begin{tikzpicture}


\node at (0,-0.6) (d1) { };
\node at (0,1.8)  (d21) { };
\node at (3.5,-0.6) (d12) { };
\node at (3.5,1.8)  (d22) { };
\node at (0,1.2) [minimum width=0.5cm,line width=0] (S1) {\footnotesize };
\node at (0,0.6) [minimum width=0.5cm,line width=0] (S2) {\footnotesize  };
\node at (0,0) [minimum width=0.5cm,line width=0] (S3) {\footnotesize };

\node at (4.5,0.6) [minimum width=0.5cm,line width=0,draw] (F_4) {};
\node at (5.5,0.6) [minimum width=0.5cm,line width=0,draw] (F_5) {};
\node at (6.5,0.6) [minimum width=0.5cm,line width=0,draw] (F_6) {};
\node at (7.2,0.6)  (F_7) {};
\foreach \i in {0,...,2}
{

\foreach \j in {1,...,3}
{

\node at (1*\j+0.5,0.6*\i) [minimum width=0.5cm,line width=0,draw] (\j_F_\i) {};
}
}

\foreach \i in {0,...,2}
{

\foreach \j[evaluate = \j as \jp using int(\j+1)] in {1,...,2} 
{
\draw[->,black] (\j_F_\i) -- (\jp_F_\i);

}
}

\foreach \i in {0,...,2}
{
\draw[->,black] (3_F_\i) -- (F_4);
}

\draw[->,black] (S1) -- (1_F_2);
\draw[->,black] (S2) -- (1_F_1);
\draw[->,black] (S3) -- (1_F_0);

\draw[->,black,dotted] (d22.east) -- (F_4);
\draw[->,black,dotted] (d12.east) -- (F_4);

\draw[->,black] (F_4) -- (F_5);
\draw[->,black] (F_5) -- (F_6);
\draw[->,black] (F_6) -- (F_7);
\end{tikzpicture}
\end{center}
   \caption{Architecture of the SLE, which is a cascade of 3  convolutions followed by 3 fully connected layers. The ReLU non-linearity are included inside the  blocks for clarity.}
\label{fig:model}
\end{figure}


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\bf Method  &\bf Top 1 &\bf Top 5  \\
\hline







FV + FC    \cite{perronnin2015fisher} &55.6  & 78.4 \\FV + SVM   \cite{sanchez2011high}          & 54.3 & 74.3\\AlexNet  & 56.9 &\bf 80.1\\Scat + SLE  & \bf 57.0&79.6\\\hline
\end{tabular}
\end{center}
\label{res_1x1}
\caption{Top 1 and Top 5 percentage accuracy reported from one single crop on ILSVRC2012. We compare to other local encoding methods, and SLE outperforms them.  \cite{perronnin2015fisher} single-crop result was provided by private communication.}
\end{table}

We first describe our training pipeline, which is similar to \cite{zagoruyko2016wide}. We trained our network for 90 epochs to minimize the standard cross entropy loss, using SGD with momentum 0.9 and a batch size of 256. We used a weight decay of . The initial learning rate is , and is dropped off by  at epochs . During the training process, each image is randomly rescaled, cropped, and flipped as in \citep{he2015deep}. The final crop size is . At testing, we rescale the image to a size of 256, and extract a center crop of size . 

We use an architecture which consists of a cascade of a scattering network, a SLE , followed by fully connected layers. Figure \ref{fig:model} describes our architecture. We select the parameter  for our scattering network, which means the output representation has size  spatially and 1251 in the channel dimension.  is implemented as 3 layers of 1x1 convolutions  with layer size 1024. There are 2 fully connected layers of ouput size 1524. For all learned layers we use batch normalization \cite{ioffe2015batch} followed by a ReLU \cite{krizhevsky2012imagenet} non-linearity. We compute the mean and variance of the scattering coefficients on the whole Imagenet, and standardized each spatial scattering coefficients with it.

Table \ref{res_1x1} reports our numerical accuracies obtained with a single crop at testing, compared with local encoding methods, and the AlexNet that was the state-of-the-art approach in 2012. We obtain 20.4\% at Top 5 and 43.0\% Top 1 errors. The performance is analogous to the AlexNet \cite{krizhevsky2012imagenet}. In term of architecture, our hybrid model is analogous, and comparable to that of \cite{sanchez2011high,perronnin2015fisher}, for which SIFT features are extracted followed by FV \cite{sanchez2013image}  encoding. Observe the FV is an unsupervised encoding compared to our supervised encoding. Two approaches are then used: either the  spatial localization is handled either by a Spatial Pyramid Pooling \cite{lazebnik2006beyond}, which is then fed to a linear SVM, either the spatial variables are directly encoded in the FVs, and classified with a stack of four fully connected layers. This last method is a major difference with ours, as the obtained descriptor does not have a spatial indexing  anymore which are instead quantified. Furthermore, in both case, the SIFT are densely extracted which correspond to approximatively  descriptors, whereas in our case, only  scattering coefficients are extracted. Indeed, we tackle the non-linear aliasing (due to the fact the scattering transform is not oversampled) via random cropping during training, allowing to build an invariant to small translations. In Top 1, \cite{sanchez2011high} and \cite{perronnin2015fisher} obtain respectively 44.4\% and 45.7\%. Our method brings a substantial improvement of 1.4\% and 2.7\% respectively.

The BVLC AlexNet \footnote{https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val} obtains a  of 43.1\%  single-crop Top 1  error, which is nearly equivalent to the 43.0\% of our SLE network. The AlexNet has 8 learned layers and as explained before, large receptive fields. On the contrary, our training pipeline consists in 6 learned layers with constant receptive field of size , except for the fully connected layers that build a  representation mixing spatial information from different locations. This is a surprising result, as it seems to suggest context information is only necessary at the very last layers, to reach AlexNet accuracy.

We study briefly the local SLE, which has only a spatial extent of , as a generic local image descriptor. We use the Caltech-101 benchmark which is a dataset of 9144 image and 102 classes. We followed the standard protocol for evaluation \cite{boureau2011ask} with 10 folds and evaluate per class accuracy, with 30 training samples per class, using a linear SVM used with the SLE descriptors. Applying our raw scattering network leads to an accuracy of , and the outputs features from  brings respectively an absolute improvement of . The accuracy of the final SLE descriptor is thus  , similar to that reported for the final AlexNet final layer in \cite{zeiler2014visualizing} and sparse coding with SIFT \cite{boureau2011ask}. However in both cases spatial variability is removed, either by Spatial Pyramid Pooling \cite{lazebnik2006beyond}, or the cascade of large filters. By contrasts  the concatenation of SLE descriptors are completely local.









 \subsection{Interprating SLE's first layer}
\label{sec:sle_first_sec}
\label{interpretation}




\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{bullsit.eps} 
\end{center}
   \caption{Histogram of   amplitude for first and second order coefficients. The vertical lines indicate a threshold that is used in Subsection \ref{interpretation} to sparsify . Best viewed in color.}
\label{fig:histo}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{little_wood_1d.eps} 
\end{center}
   \caption{Energy   (left)  and  (right) from Eq. \ref{eq1} for given angular frequencies. Best viewed in color.}
\label{fig:littlehood1}
\end{figure}









Finding structure in the kernel of the layers of depth less than  \cite{waldspurger2015these,zeiler2014visualizing} is  a complex task, and few empirical analyses exist that shed light on the structure \cite{jacobsen2017multiscale} of deeper layers. A scattering transform with scale  can be interpreted as a CNN with depth  \cite{oyallon2015deep}, whose  channels indexes correspond to different scattering frequency indexes, which is a structuration. This structure is consequently inherited by the first layer  of our SLE . We analyse  and show that it builds explicitly invariance to local rotations, yet also that the Fourier bases associated to rotation are a natural bases of our operator. It is a promising direction to understand the nature of the two next layers.

We first establish some mathematical notions linked to the rotation group that we use in our analysis. For the sake of clarity, we do not consider the roto-translation group.
For a given input image , let  be the image  rotated by angle , which corresponds to  the linear action of rotation on images. Observe the scattering representation is  covariant with the rotation in the following sense:

Besides, in the case of the second order coefficients,  is covariant with rotations, but  is an invariant to rotation that correspond to a relative rotation. 

Unitary representation framework \cite{sugiura1990unitary} permits the building of a Fourier transform on compact group, like rotations. It is even possible to build a scattering transform on the roto-translation group \cite{sifre2013rotation}. Fourier analysis permits the measurement of the smoothness of the operator and, in the case of CNN operator, it is a natural basis.





We can now numerically analyse the nature of the operations performed along angle variables by the first layer  of , with output size . Let us define as  the restrictions of  to the order 0,1,2 scattering coefficients respectively. Let  an index of a feature channel and  the color index. In this case,  is simply the weights associated to the smoothing .  depends only , and  depends on . We would like to characterize the smoothness of these operators with respect to the variables , because  is covariant to rotations.

To this end, we define by ,  the Fourier transform of these operators along the variables  and  respectively. These operator are expressed in the tensorial Frequency domain, which corresponds to a change of basis. In this experiment, we normalized each filter of  such that they have a  norm equal to 1, and each order of the scattering coefficients are normalized as well. Figure \ref{fig:histo} shows the distribution of the amplitude of . We observe that the distribution is shaped as a Laplace distribution, which is an indicator of sparsity. 

To illustrate that this is a natural basis we explicitly sparsify this operator in its frequency basis and verify that empirically the network accuracy is minimally changed. We do this by thresholding  by  the coefficients of the operators in the Fourier domain. Specifically we replace the operators ,  by  and . We select an  that sets  of the coefficients to 0, which is indicated on Figure \ref{fig:histo}. \textit{Without retraining} our network performance degrades by only an absolute value of  worse on Top 1 and Top 5 ILSVRC2012. We have thus shown that this basis permits a sparse approximation of the first layer, . We now show evidence that this operator builds an explicit invariant to local rotations.

To aid our analysis we introduce the following quantities:
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}



They correspond to the energy propagated by  for a given frequency, and permit to quantify the smoothness of our first layer operator w.r.t. the angular variables. Figure \ref{fig:littlehood1} shows variation of   and  along frequencies. For example, if  and  were convolutional along  and , these quantities would correspond to their respective singular values. One sees that the energy is concentrated in the low frequency domain, which indicates that  builds explicitly an invariant to local rotations.








 
\section{Numerical performances of hybrid networks}
\label{small2}
We now demonstrate cascading modern CNN architectures on top of the scattering network can produce high performance classification systems. We apply hybrid convolutional networks on the Imagenet ILSVRC 2012 dataset as well as the CIFAR-10 dataset and show that they can achieve performance comparable to modern end-to-end learned approaches. We then evaluate the hybrid networks in the setting of limited data by utilizing a subset of CIFAR-10 as well as the STL-10 dataset and show that we can obtain substantial improvement in performance over analogous end-to-end learned CNNs. 

\subsection{Deep Hybrid CNNs on ILSVRC2012}
\label{hybimnet}
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\bf Method  &\bf Top 1 &\bf Top 5 &\bf Params \\
\hline
AlexNet & 56.9&80.1&61M\\VGG-16 \cite{han2015learning}&68.5&88.7&138M\\Scat + Resnet-10 (ours)&68.7&88.6&12.8M\\Resnet-18 (ours) & 68.9&88.8&11.7M\\Resnet-200 \cite{zagoruyko2016wide} & \textbf{78.3} & \textbf{94.2} & 64.7M  \\\hline
\end{tabular}
\end{center}
\caption{ILSVRC-2012 validation accuracy (single crop) of hybrid scattering and 10 layer resnet, a comparable 18 layer resnet, and other well known benchmarks. We obtain comparable performance using analogous amount of parameters while learning parameters at a spatial resolution of 28  28}
\label{tab:imagenet_full}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\bf Method & \bf Accuracy \\
  \hline 

\small \bf Unsupervised Representations   &  \\
Roto-Scat + SVM   \cite{oyallon2015deep}  &82.3\\
ExemplarCNN \cite{dosovitskiy2014discriminative} & 84.3 \\
DCGAN \cite{radford2015unsupervised}& 82.8 \\
Scat + FC   (ours)     & \bf 84.7\\
\hline
\small \bf Supervised and Hybrid    &  \\
Scat + Resnet  (ours) & 93.1       \\
Highway  network \cite{srivastava2015highway}& 92.4 \\
All-CNN \cite{springenberg2014striving}& 92.8 \\
WRN 16 - 8 \cite{zagoruyko2016wide}  & 95.7\\
WRN 28 - 10 \cite{zagoruyko2016wide} &  \textbf{96.0} \\
\hline
\end{tabular}
\end{center}
\label{tab:CIFAR_Main}
\caption{Accuracy of scattering compared to similar architectures on CIFAR10. We set a new state-of-the-art in the unsupervised case and obtain competitive performance with hybrid CNNs in the supervised case.}
\end{table}

We showed in the previous section that a SLE followed by FC layers can produce results comparable with the AlexNet \cite{krizhevsky2012imagenet} on the Imagenet classification task. Here we consider cascading the scattering transform with a modern CNN architecture, such as Resnet \cite{zagoruyko2016wide,he2015deep}. We take the Resnet-18 \cite{zagoruyko2016wide}, as a reference and construct a similar architecture with only 10 layers on top of the scattering network.  We utilize a scattering transform with  such that the CNN is learned over a spatial dimension of  and a channel dimension of 651 (3 color channels of 217 each). The ResNet-18 typically has 4 residual stages of 2 blocks each which gradually decrease the spatial resolution \cite{zagoruyko2016wide}. Since we utilize the scattering as a first stage we remove two blocks from our model. The network is described in Table \ref{table:arch_imagenet}.

\newcommand{\blocka}[2]{
  -.1em]
        \text{33, #1}
      \end{array}
    \right]\times\times28\times28J=3, 6511\times114\times148\times824\times24J=2\times\times\timesn\timesn1\times18\times812\times12k3\times3n=2n=4\%\%5\times10^41\times10^4J=28\times85\times10^{-4}1.1\times10^4k32\times k1\times 193.1\%\pm\pm\pm\pm\pm\pm\bf 1.1\substack{\text{\small \bf  Supervised methods}}  \pm\pm\substack{\text{\small \bf  Unsupervised methods}}  \pm\pm\pm96\times 9696\times 96\%87.6\%81.3\%$ reported in \cite{hoffer2016deep} using unsupervised learning and the full unlabeled and labeled training set. The competing techniques add several hyper parameters and require an additional engineering process. Applying a hybrid network is on the other hand straightforward and is very competitive with all the existing approaches, without using any unsupervised learning. 

In addition to showing hybrid networks perform well in the small sample regime these results, along with our unsupervised CIFAR-10 results, suggest that completely unsupervised feature learning on natural image data, for downstream discriminative tasks, may still not outperform supervised learning methods and pre-defined representations. One possible explanation is that in the case of natural images, learning in an unsupervised way more complex variabilities than geometric ones ( e.g the rototranslation group), might be very challenging or possibly ill-posed.














 
\section{Conclusion}
This work demonstrates a competitive approach for large scale visual recognition, based on scattering networks, in particular for  ILSVRC2012. When compared with unsupervised representation on CIFAR-10 or small data regimes on CIFAR-10 and STL-10, we demonstrate state-of-the-art results. We build a supervised Shared Local Encoder that permits the scattering networks to surpass other local encoding methods on ILSVRC2012. This network of just 3 learned layers permits analysis on the operation performed.

Our work also suggests that pre-defined features are still of interest and can provide enlightenment on deep learning techniques and to allow them to be more interpretable. Combined with appropriate learning methods, they could permit having more theoretical guarantees that are necessary to engineer better deep models and stable representations.



\subsubsection*{Acknowledgments}
The authors would like to thank  Mathieu Andreux, Matthew Blaschko, Carmine Cella, Bogdan Cirstea, Michael Eickenberg, St\'ephane Mallat for helpful discussions and support. The authors would also like to thank Rafael Marini and Nikos Paragios for use of computing resources. We would like to thank Florent Perronnin for providing important details of their work. This   work   is   funded   by   the   ERC   grant   InvariantClass   320959,   via   a   grant   for   PhD   Students of the Conseil r\'egional d'Ile-de-France (RDM-IdF), Internal Funds KU Leuven, FP7-MC-CIG 334380, an Amazon Academic Research Award, and DIGITEO 2013-0788D - SOPRANO.  
{\small
\bibliographystyle{ieee}
\bibliography{iccv}
}


\end{document}
