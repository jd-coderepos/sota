\documentclass[conference]{IEEEtran}
\usepackage{listings}
\lstset{ }
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{hyperref}
\newcommand{\starss}{{StarSs}}
\newcommand{\smpss}{{SMPSs}}
\newcommand{\lbc}{{\em lbc}}
\newcommand{\laki}{{\em Laki}}
\begin{document} 

\title{Hybrid MPI/\starss{} -- a case study}
\author{
  \IEEEauthorblockN{
    Jos\'e Gracia\footnote{gracia@hlrs.de}\IEEEauthorrefmark{1}, 
    Christoph Niethammer\IEEEauthorrefmark{1},
    Manuel Hasert\IEEEauthorrefmark{2},
    Steffen Brinkmann\IEEEauthorrefmark{1},
    Rainer Keller\IEEEauthorrefmark{1},
    Colin W. Glass\IEEEauthorrefmark{1}}
  \IEEEauthorblockA{\IEEEauthorrefmark{1}High Performance Computing
    Center Stuttgart (HLRS), University of Stuttgart, 70550
    Stuttgart, Germany}
\IEEEauthorblockA{\IEEEauthorrefmark{2}German Research School for Simulation Sciences
   GmbH, 52062 Aachen, Germany}
}
\maketitle
\begin{abstract}
  Hybrid parallel programming models combining distributed and shared
  memory paradigms are well established in high-performance
  computing. The classical prototype of hybrid programming in HPC is
  MPI/OpenMP, but many other combinations are being
  investigated. Recently, the data-dependency driven, task parallel model
  for shared memory parallelisation named \starss{} has been suggested
  for usage in combination with MPI. In this paper we apply hybrid
  MPI/\starss{} to a Lattice-Boltzmann code. In particular, we present
  the hybrid programming model, the benefits we expect, the
  challenges in porting, and finally a comparison of the
  performance of MPI/\starss{} hybrid, MPI/OpenMP hybrid
  and the original MPI-only versions of the same code.
\end{abstract}


\section{Introduction}

The Message Passing Interface (MPI) is the \emph{de facto} standard in
high performance computing. While MPI is a distributed memory
parallelisation scheme, it is also frequently used on SMP-like
systems, as for instance today's multi-core CPUs. On such systems,
the parallel applications developer may resort to shared
memory parallelisation schemes as for instance OpenMP. Hybrid
parallelisation models consisting of a distributed memory part to
pass messages between compute nodes in a cluster, and a
shared memory part to exploit all available cores on a node, have been
used in HPC with success.
Yet, the general perception is, that MPI-only jobs
are still the working horse in research groups relying on numerical
computations. 

The MPI-only approach has come under pressure; mainly it is challenged by today's prevalence of
multi-core and many-core systems. One aspect of the problem
certainly is that many applications are structured in such a way that
they cannot scale efficiently to large numbers of MPI
ranks; they have limits on MPI-scalability that may well
depend on problem size or simulation parameters. Often these limits
could be lifted or pushed further out by an algorithmic
redesign. However, most research groups will \emph{not} go down such a road
lightly.

In this paper we will not delve into the lack of scalability of MPI
itself, whether perceived or real (see \cite{B+09} for a discussion on
  the topic), but will assume that an
application's limit on MPI-scalability is inherently a problem of a
given application itself. Users hitting the limits of such an MPI-only
application have three choices: 1) find a set of parameters
(e.g. problem size) that allows to scale out further, 2) redesign and
rewrite their application to use MPI more efficiently, 3) go hybrid,
which also means redesigning and rewriting the code. Often the first
is not possible or interesting from a scientific point of view.  From
the last two choices, the hybrid approach is usually more straightforward, as
the re-design can be implemented incrementally, allowing to
assess its correctness along the way. Note, however, that in view of
Amdahl's law, any hybridization needs to cover the code in its
completeness in order to be efficient.

The standard paradigm for hybrid parallel programming in HPC is MPI/OpenMP,
but many other combinations are being investigated. Recently, the data-flow
driven, task parallel model for shared memory parallelisation named
\emph{\starss{}} has been suggested as a partner for MPI. In this paper, we
will first introduce the main aspects of \starss{} briefly. Then we will
apply the hybrid MPI/\starss{} model to an MPI-only Lattice-Boltzmann code
while describing the challenges we encountered and how they have been solved.
Furthermore, a hybrid MPI/OpenMP version is developed and used as a basis 
for the assessment of the hybrid MPI/\starss{} version.
Next, we show performance of the current hybrid implementation, contrast it
to the original, and finally draw our conclusions.


\section{The hybrid MPI/\starss{} programming model}

We start by giving a short overview of the \starss{} programming
model~\cite{*Ss:BPBL06}. It is based on task parallelisation.  First
of all, the programmer needs to identify suitable units of work, in
general at subroutine boundaries, and designate them as tasks through
pragma based code annotations. 
In contrast to, e.g., pthreads or OpenMP, where task synchronization
has to be specified by the programmer explicitly, \starss{} infers the synchronization from
data dependencies in the program. These dependencies are automatically
detected, based on the directionality of subroutine arguments,
using code directives to distinguish between
input, output and input-output arguments. If programming in Fortran
the already present 
\verb!intent(...)!  clauses in subroutine interfaces are used.

At runtime, the data directionalities and the actual data arguments,
i.e. their memory address, are taken to build up a dynamic task
dependency graph. In the simplest case, tasks will be scheduled for
execution sequentially in the order in which they have been added to
the graph. This is essentially equal to serial execution. In general, the
task graph will expose concurrency which can be exploited by
dispatching independent tasks onto a range of available compute cores and running these
in parallel. Data dependencies then will ensure that no task is
scheduled before all task that modify its inputs have completed. 
The \starss{} model allows to write programs without any kind of
explicit synchronisation; in fact, this is the rule. Synchronisation
is in principle necessary only when code executed synchronously
outside of tasks (executed by a master thread) and code executed
inside of tasks (executed asynchronously by worker threads) use the same
data. In this sense synchronisation is required only between
sequential code running on the master thread and tasks, not between
tasks. Naturally, one can avoid this kind of master-worker
synchronization by taskifiying all relevant parts of the code.

Compared to classical parallelisation approaches using parallel loops
or explicitly synchronised tasks, \starss{} has several
advantages. Firstly, it is a dynamic process which will allow
efficient parallelisation even for different input sets; each run
might result in a different dependency graph and thus in different
program execution behaviours. Secondly, and much more importantly, the
lack of explicit synchronisation between tasks can improve load
balancing and solve some of the scalability problems.

The third point to be mentioned concerns the hybrid MPI/\starss{}
approach; MPI communication can also be off-loaded into \starss{}
tasks. This allows overlapping of computation and communication in a
simple and more efficient way than hand-written code using,
e.g.,  non-blocking MPI operations. In this paper we will explore this
possibility only briefly, however.

In some sense, \starss\ is a family of programming models including,
among others, SMPSs and OmpSs. All of them very similar in spirit,
particularly the focus on data-dependencies between tasks, but with
slight differences in the specifics of the compiler directives and the
capabilities of the runtime. For this work, we have used the SMPSs
compiler, which is tailored to multi-core and SMP-like machines, but
also aware of MPI allowing to off-load MPI tasks onto a dedicated
communication thread. We did not consider the more recent alternative
OmpSs, simply because it did not support Fortran at the time of
writing this paper.


\section{A case study: Lattice-Boltzmann}
Before starting to port an application to a new programming model,
most application developers in applied computational science will
balance expected performance gain against the expected effort. Any new
programming model should therefore not only promise high performance,
but should also be simple to use.  As presented in the previous section,
applying the \starss{} programming model to an existing application
is in principle straightforward as long as the data-flow is clearly
exposed in the given code. We have used \starss{} to
hybridise a previously MPI-only fluid dynamics code which implements
the Lattice-Boltzmann Method (LBM). In this paper we present our
parallelisation approach, discuss the challenges we encountered,
suggest how to overcome them, and finally put the resulting
performance into perspective.

We have purposely chosen to undertake this experiment with a code
written by a graduate student, which is not heavily optimised
regarding scalar or MPI performance. \lbc{}~\cite{HKR11} -- short for {\em
  l}attice-{\em b}oltzmann {\em c}ode -- is a Lattice-Boltzmann code
using the BGK~\cite{BGK54} approximation for the collision term. The
algorithm is a standard two-grid approach, i.e. at every algorithmic step 
the code reads from one grid and writes into the other, without any spatial
or temporal blocking. The code is written in Fortran90 and uses
modules, pointers, and user-defined types throughout. Similarly, the
MPI parallelisation is based on a simple multi-dimensional domain
decomposition. Ghost cells are exchanged at the end of each timestep,
no attempt to overlap computation and calculation is done.

The Lattice-Boltzmann method consist of three consecutive
computational steps -- \verb!stream()!, \verb!collide()!, and
\verb!boundaries()!. In order to increase data reuse, the former two
are often combined into a \verb!stream_collide()!, while the latter
applies external boundary conditions. In the distributed memory case,
ghost cells are exchanged with MPI neighbours in
\verb!exchange_ghosts()!.  In our case the main time-stepping loop is
\begin{lstlisting}
do timestep=1, timesteps
  call stream_collide(block)
  call boundaries(block)
  call exchange_ghosts(comm, block)
end do
\end{lstlisting}
where \verb!block! is a variable of user-defined type \verb!type(lb)! encapsulating
all data structures necessary for the Lattice-Boltzmann algorithm,
most importantly the working arrays \verb!flip! and \verb!flop!;
\verb!comm! is another user-defined type encapsulating information
regarding MPI communication as for instance the rank of neighbours, etc.


\subsection{Tiling of the algorithm with data copies} \label{sec:copies}

\starss{} is a task parallel programming model. We want to
apply it to an algorithm, i.e. LBM, which is as many others in CFD
naturally data parallel, but at first glance does not seem to have more
than a few  obvious subroutines suitable as tasks. Even
worse, the candidates \verb!stream()!, \verb!collide()!,
\verb!boundaries()!, and \verb!exchange_ghosts()! have flow
dependencies on each other and may not be executed concurrently. 

In order to apply \starss{} to this code, we need to block (or tile) the
algorithm, in order to convert data parallelism to task
parallelism. In the following we use the term \emph{tiling}.
Rather than tiling on the loop level, we do so at the
top program level by introducing an additional domain decomposition
for tiling on top of the MPI domain decomposition. In the
following we will refer to an MPI subdomain as {\em block} and to a
\starss{} subdomain as {\em tile}. Note, that tiles are subdomains of a
particular block.  In the code, the variable \verb!block! represents
a block and tiles are represented by \verb!tile!. For simplicity
each tile is based on the same user type as blocks allowing
to reuse much of the original code. 

Similar to blocks on the MPI level, tiles need to exchange ghost
cells with every neighbour. So, before doing the LBM step, tiles pull
ghost cells from a buffer (\verb!pull_ghosts()!) and push them back into a
buffer (\verb!push_ghosts()!) after concluding the LBM step. We have chosen to use the MPI
level block as buffer. The tiled main time-stepping loop is then:
\begin{lstlisting}
call init_tiles(block, tiles)

do timestep=1, timesteps
  do iTile=1, nTiles
    tile => tiles(iTile)
    call pull_ghosts(tile, block)
  end do

  do iTile=1, nTiles
    tile => tiles(iTile)
    call stream_collide(tile)
    call push_ghosts(tile, block)
  end do

  call boundaries(block)
  call exchange_ghosts(comm, block)
end do

call gather_tiles(block, tiles)
\end{lstlisting}
Note, that inside the time-stepping loop \verb!block! only carries
 valid data for those array elements that are required to exchange ghost
 cells, either with MPI neighbours in \verb!exchange_ghosts()! or with
 neighbour tiles in \verb!push/pull_ghosts()!, respectively. At the end
 of the timestep loop, or before any output for that matter, data in the tiles
 needs to be gathered back into the block, which is done in
 \verb!gather_tiles()!. Similarly,  \verb!init_tiles()! initialises
 tiles from data in the block.

\subsection{Tiling of the algorithm with data
  aliasing} \label{sec:aliasing}

A major drawback of the tiled algorithm described in the section
above, is the data copies between the tiles and the block. This is true even if
only those array elements are copied which serve as ghost cells for adjacent
tiles or for neighbouring MPI blocks. The resulting code suffers
significant performance penalties in the order of several tens of
percent, as will be detailed later on.

Rather than replicating block's data in the tiles, we sought a way of
reusing the data in a block, by mapping or aliasing it into the tiles. 
We made use of the fact that in Fortran, array pointers can not only point to whole
arrays, but also into sub-arrays as
\begin{lstlisting}
  tiletile\end{lstlisting}
where the array boundaries in the block, i.e. , , etc, are
distinct for each tile.  Note, that in general the pointers will alias
sub-arrays which are non-contiguous in memory. With this
implementation tiles and the block are just different views on the
same underlying memory region. The data is organised in such a way, that
it simplifies data access in different parts of the code, respectively.

Obviously, it is no longer necessary to keep tiles and blocks
consistent, nor to copy ghost cells between adjacent tiles. The
subroutines \verb!pull_ghosts()!, \verb!push_ghosts()!, and
\verb!gather_tiles()! are no longer needed. The only purpose of
\verb!init_tiles()! is to setup the aliases for \verb!flip! and
\verb!flop!. The basic algorithm thus simplifies considerably to
\begin{lstlisting}
call init_tiles(block, tiles)

do timestep=1, timesteps
  do iTile=1, nTiles
    tile => tiles(iTile)
    call stream_collide(tile, block)
  end do

  call boundaries(block)
  call exchange_ghosts(comm, block)
end do
\end{lstlisting}
Note, that this code resembles very closely the original non-tiled
version. The only difference apart from the initialisation of tiles is
the additional tiles loop embedding the call to the main computational
step \verb!stream_collide()!. The inclusion of the argument
\verb!block! to task \verb!stream_collide()! will become clear in
section \ref{sec:issues}.

\subsection{Task identification and data directionality}
\label{sec:tasks}

All subroutines in the main time-stepping loop are in principle suitable as
task. In \starss{} neither scheduling of tasks nor synchronisation amongst
them is specified explicitly. Instead, the developer has to declare the
directionality for each argument of any task subroutine, i.e. input, output,
or inout. In C, this is done through additional clauses to the \starss{} task
directive. The Fortran interface uses the Fortran clause \verb!intent(...)!,
which becomes mandatory for each argument of task subroutines. The interface
of our tasks is:

\begin{lstlisting}
interface
  !CSS TASK
  subroutine pull_ghosts(tile, block)
    type(lb), intent(inout) :: tile
    type(lb), intent(in)    :: block
  end subroutine
  !CSS TASK
  subroutine boundaries(block)
    type(lb), intent(inout)  :: block
  end subroutine
  !32^340\%256\times256\times22450t_1 = 115.04 s6.48\%864588832\times256\times224810248t_8 = 21.13 s4.33210\%10241.81.13841.25T(n)niT_8\tau^i_0\mu^in\log nn^2\mu^s = (1.0 \pm 0.1) \times 10^{-4}\mu^m = (8.6 \pm 0.2) \times
10^{-4}\mu^o = (1.2 \pm 0.2)
\times 10^{-4}116084709860\mu^m/\mu^s = 8.5 \pm 0.5\mu^m/\mu^o = 7.3 \pm 1.0\tau^i_0\mu^i \, n5\%T(n)nT_1\tau^i_0\beta^i\eta^in \log nn^2\beta^i\beta^i=0\eta^i \,
(n-1)\eta^m/\eta^s = 2.3 \pm 0.2\eta^m/\eta^o = 1.8 \pm 0.2\eta^i575673250\, \mathrm{ms}12825632256\muÂ´^s \sim 3 \times 10^{-5}32000$ cores based on an extrapolation of the curve. This makes
the hybrid MPI/\starss{} version scale much better than the MPI-only
version in this range. 

\section{Conclusions}

We set out to investigate if the hybrid MPI/\starss{} programming
model allows typical applications to scale to larger number of
cores than MPI-only applications and how much porting effort this
model requires.
Most research groups applying computational methods to
solve their domain specific problems will have only limited resources
to do such a port.

In our LBM example we have shown that the hybrid version,
combining MPI with the task-based programming model \starss{}, is a)
competitive with the MPI-only version on a single node, in fact
outperforms it, b) leverages the full scaling efficiency
across nodes, and c) results in much more stable execution times
reducing the effect of load-imbalance (internal or external as,
e.g., due to OS jitter). 
We have also evaluated a hybrid MPI/OpenMP version based on the same
code refactoring done for MPI/\starss{}.

Both hybrid versions are competitive with the MPI-only version on a
single core regarding strong scaling. The overhead of \starss{} task
instantiation, management, and scheduling is negligible for
the chosen task granularity. In fact, the \starss{} version is
slightly faster than the MPI version when the full node is used. This
stems from a better cache re-use due to tiling which leads to 
a lower contention among cores on the memory system. The same is
true for the hybrid MPI/OpenMP version but to a somewhat lower degree.

Doing hybrid distributed/shared memory parallelisation is often
motivated by a reduction of MPI ranks involved in a calculation in
order to reduce communication times which depend on the number of
ranks involved. At the very least, hybrid models should allow to scale
a given application to a larger number of cores than the MPI-only
version. Ideally, the range to which an application scales is extended
by a factor equal to the number of threads running inside a single MPI
rank.  Scalar overheads of the hybrid model should
remain small in order not to counteract the higher scaling efficiency
-- doing so increases the parallel efficiency of the application
to a much larger number of total {\em cores}, in particular on
multi- or many-core systems.

We have shown that the hybrid versions, in particular the
MPI/\starss{}, make up on their promise and leverage the full potential
by allowing to scale out by a factor equal to the number of cores on a
single node. In addition, \starss{} allows to hide communication
latencies in a straightforward way. In our case this turned out not to be very
relevant, as the communication time increases very rapidly and
eventually can not be hidden. Up to this stage, however, the weak
scaling curve is practically flat.  At scale, it is more efficient to
use the hybrid version than the MPI-only version.
 
In a future paper we will investigate different methods to overlap
communication and computation in more detail. In \starss{} this is
particularly attractive as the dynamic scheduling driven by data
dependencies allows to hide communication latencies as long as
sufficient concurrency is available. Also, the \smpss{} runtime
specifically allows to off-load MPI communication onto a special
communication thread \cite{MLAV10} that, in most cases, will not use
cycles and allows to have multiple transfers on the fly at the same
time. In that case the programmer does not need to bother with
non-blocking asynchronous MPI calls, but may continue to use the most
simple MPI calls and still get asynchronous communication with latency
hiding.

The MPI version has a much larger scatter of execution
times for the weak and the strong scaling experiments than both of the
hybrid versions. While we do not have execution traces to support our
hypothesis, we argue that the scatter stems from load imbalances due
to OS jitter. All code versions compete for resources with the OS,
however, any delay caused to a single task of the hybrid versions will
not affect the tasks running on the remaining cores, yet for the MPI
version all cores have to idle eventually waiting on the delayed core
to finish its work thus multiplying the initially small disturbance.   
In any case hybrid models help with application specific load
imbalances.

Finally, the porting effort was limited to tiling the algorithm, which
is not particularly difficult for LBM codes, especially in Fortran
where aliasing of sub-array can be used. Given the fact, that the
MPI-only version also benefits from this tiling, it is very much worth the
effort. Once done, executing the code in hybrid mode, either in
combination with OpenMP or \starss{} allows to scale out to much
larger number of cores. A future work will investigate if and how
\starss{} can be used to do temporal blocking in addition to the spatial
blocking presented in this work.


\section{Acknowledgments. }
\begin{sloppypar}
This work was supported by the European Community's Seventh Framework
Programme [FP7-INFRASTRUCTURES-2010-2] project TEXT under grant
agreement number 261580 and by the project PRACE-1IP under contract RI-261557.
\end{sloppypar}

\bibliographystyle{IEEEtran}
\bibliography{library}

\end{document} 
