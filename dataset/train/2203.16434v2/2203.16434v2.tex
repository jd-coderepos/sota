In this Appendix, we present additional visualizations of the different attention mechanisms in our space-time decoder in Section~\ref{sec:viz}. 
Section~\ref{sec:adddetails} provides additional implementation details. 
We then give detailed results for ablations in Section~\ref{sec:ablations} on the VidSTG dataset~\cite{vidstg} split by sentence type in Section~\ref{sec:addablation}.
Next we present an ablation of our fast and aggregation modules in Section~\ref{sec:addexp}.
Finally we discuss broader impact in Section~\ref{sec:impact}.

\section{Visualization of space, time and language attention patterns in the decoder}\label{sec:viz}
This section illustrates attention mechanisms of our space-time decoder over space, language and time for the spatio-temporal video grounding example presented in Figure~\ref{fig:spaceattn}. 
For this example the time-aligned cross-attention for the visual modality is also shown in Figure~\ref{fig:spaceattn}.
We note that spatially, attention at each timestep is particularly focused on humans that are receiving the sports ball and gesturing.
Additionally, the time-aligned cross-attention for the textual modality is illustrated in Figure~\ref{fig:textattn}. 
We observe that the words \textit{adult} and \textit{grabs} are the most attended overall, and that attention weights on the different words (\eg \textit{sports} and \textit{ball}) vary over time. $\hat{t}_s$ and $\hat{t}_e$ in Figure~\ref{fig:textattn} denote the predicted start and end times of the output tube.
Next, the temporal self-attention is illustrated in Figure~\ref{fig:timeattn}. 
We notice long-range temporal interactions: a certain number of time queries attend to various temporally distant time queries, \eg time queries located around the start of the video between the eighth and sixteenth frames.

\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]{figures/text_attn_cr}
\vspace{+0.1cm}
\caption{\small \textbf{Time-aligned cross-attention visualization (textual modality).} 
Visualization of the attention weights between the time query (y-axis) and its time-aligned visually-contextualized text features (x-axis) at different times in our space-time decoder. 
These attention weights are averaged across all 8 heads and all 6 layers, and renormalized by the maximum weight at each timestep (\ie each row) for the purpose of visualization. Lighter colors correspond to higher attention weights (see the colorbar on the right).
}
\vspace{+0.1cm}
\label{fig:textattn}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figures/spatial_attn_cr} 
\caption{\small \textbf{Time-aligned cross-attention visualization (visual modality).} 
Top rows: Input frames with the predicted (yellow) and ground truth (green) spatio-temporal tubes overlaid.
Bottom rows: Visualization of the attention weights between the time query and its time-aligned text-contextualized visual features at different times in our space-time decoder. 
These attention weights are averaged across all 8 heads and all 6 layers, and renormalized by the maximum weight at each timestep for the purpose of visualization.
Attention at each timestep is particularly focused on humans that are receiving the sports ball and gesturing.
}
\label{fig:spaceattn}
\vspace{-1cm}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{figures/time_attn_cr}
\caption{\small \textbf{Temporal self-attention visualization.}
Visualization of the attention weights between the different time queries in our space-time decoder. 
The column $t$ corresponds to the weights of the different time queries for the time query at time $t$.
These attention weights are averaged across all 8 heads and all 6 layers, and renormalized by the maximum weight at each timestep (\ie each column) for the purpose of visualization.
$\hat{t}_s$ and $\hat{t}_e$ denote the predicted start and end times of the output tube.
Lighter colors correspond to higher attention weights (see the colorbar on the right).
}
\label{fig:timeattn}
\end{figure*}

\section{Additional implementation details}\label{sec:adddetails}

In our transformer, the number of heads is 8 and the hidden dimension of the feed-forward layers is 2048.
We set the initial learning rates to $1e^{-5}$ for the visual backbone, and $5e^{-5}$ for the rest of the network.
The learning rate follows a linear schedule with warm-up for the text encoder and the learning rate is constant for the rest of the network.
We use the AdamW optimizer~\cite{loshchilov2017decoupled} and weight-decay $1e^{-4}$.
Video data augmentation includes spatial random resizing, spatial random cropping preserving box annotations, and temporal random cropping preserving the annotated time interval.
Dropout~\cite{srivastava2014dropout} with probability 0.1 is applied in our transformer layers, and dropout with probability 0.5 is applied in the temporal localization head.
We use exponential moving average with a decay rate of 0.9998, and an effective batch size of 16 videos.
For temporal stride $k=1$ the fast and aggregation modules in the encoder are not active, as their goal is to recover local spatial and temporal information when $k>1$.

\begin{table*}[t]
\centering
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{6pt}
\resizebox{.9\linewidth}{!}{
\begin{tabular}{ccc|ccccc|ccccc}
& \multirow{2}{*}{\makecell{\small{Time} \\ \small{Encoding}}} & \multirow{2}{*}{\makecell{\small{Self} \\ \small{Attention}}} & \multicolumn{5}{c}{Declarative Sentences} & \multicolumn{5}{c}{Interrogative Sentences} \\
& & & \small{m\_tIoU} & \small{m\_vIoU} & \makecell{\small{vIoU} \\ \small{@0.3}} & \makecell{\small{vIoU} \\ \small{@0.5}}
& \small{\siou{}} 
& \small{m\_tIoU} & \small{m\_vIoU} & \makecell{\small{vIoU} \\ \small{@0.3}} & \makecell{\small{vIoU} \\ \small{@0.5}}
& \small{\siou{}} \\ 
\hline
1. & \xmark & - 
& 24.4 & 13.6 & 17.8 & 7.3 & 51.9
& 23.5 & 11.1 & 13.3 & 5.2 & 43.1 \\ 
2. & \xmark & Temporal 
& 25.3 & 14.1 & 18.6 & 7.3 & 52.3
& 25.0 & 12.1 & 15.4 & 5.9 & 43.3 \\ 
3. & \cmark & - 
& 42.1 & 23.2 & 31.8 & 19.5 & 51.3
& 41.5 & 19.7 & 26.2 & 15.8 & 42.5 \\ 
4. & \cmark & Temporal 
& \textbf{46.4} & \textbf{26.6} & \textbf{36.1} & \textbf{24.7} & \textbf{52.8} 
& \textbf{45.6} & \textbf{22.5} & \textbf{30.8} & \textbf{19.8} & \textbf{43.6} \\ 
\end{tabular}}
\vspace{-0.3cm}
\caption{\small Effect of the time encoding and the temporal self-attention in our space-time decoder on the VidSTG validation set.}
\vspace{-0.5cm}
\label{table:adddecoder}
\end{center}
\end{table*}

\begin{table*}[t]
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{5pt}
\resizebox{.9\linewidth}{!}{
\begin{tabular}{ccc|ccccc|ccccc}
& \multirow{2}{*}{\makecell{\small{Pre-} \\ \small{Training}}} & \multirow{2}{*}{\makecell{\small{Decoder Self-} \\ \small{Attention Transfer}}} & \multicolumn{5}{c}{Declarative Sentences} & \multicolumn{5}{c}{Interrogative Sentences} \\
& & & \small{m\_tIoU} & \small{m\_vIoU} & \makecell{\small{vIoU} \\ \small{@0.3}} & \makecell{\small{vIoU} \\ \small{@0.5}}
& \small{\siou{}}
& \small{m\_tIoU} & \small{m\_vIoU} & \makecell{\small{vIoU} \\ \small{@0.3}} & \makecell{\small{vIoU} \\ \small{@0.5}}
& \small{\siou{}} \\ 
\hline
1. & \xmark & \xmark 
& 42.9 & 19.8 & 26.7 & 16.8 & 41.1
& 42.8 & 18.0 & 23.9 & 14.6 & 36.5 \\ 
2. & \cmark & \xmark 
& 44.0 & 24.5 & 32.9 & 21.5 & 51.5
& 43.6 & 20.8 & 27.5 & 17.2 & 42.6 \\ 
3. & \cmark & \cmark 
& \textbf{46.4} & \textbf{26.6} & \textbf{36.1} & \textbf{24.7} & \textbf{52.8} 
& \textbf{45.6} & \textbf{22.5} & \textbf{30.8} & \textbf{19.8} & \textbf{43.6} \\ 
\end{tabular}}
\vspace{-0.3cm}
\caption{\small Effect of the weight initialization for our model on the VidSTG validation set.}
\vspace{-0.5cm}
\label{table:addinit}
\end{center}
\end{table*}

\begin{table*}[!htbp]
\begin{center}
\setlength\tabcolsep{2pt}
\resizebox{.9\linewidth}{!}{
\begin{tabular}{llll|ccccc|ccccc|c}
& \multirow{2}{*}{Fast} & \multirow{2}{*}{Res.} & \multirow{2}{*}{\makecell{ \small{Temp.} \\ \small{Stride}}} & \multicolumn{5}{c}{Declarative Sentences} & \multicolumn{5}{c}{Interrogative Sentences} 
& \multirow{2}{*}{\makecell{\small{Mem.} \\ \small{(GB)}}} \\
& & & & \small{m\_tIoU} & \small{m\_vIoU} & \small{vIoU@0.3} & \small{vIoU@0.5}
& \small{\siou{}} & \small{m\_tIoU} & \small{m\_vIoU} & \small{vIoU@0.3} & \small{vIoU@0.5} &
\small{\siou{}} & \\
\hline
1. & --- & 224 & 1 
& 46.9 & 27.6 & 37.7 & 25.7 & 54.2 
& 46.1 & 23.3 & 31.3 & 20.8 & 44.9 & 23.9 \\
2. & \cmark & 224 & 2 
& 46.6 & 27.4 & 38.0 & 25.7 & 54.3 &
45.5 & 23.0 & 31.3 & 20.7 & 44.7 & 16.2 \\
3. & \cmark & 224 & 5 
& 46.4 & 26.6 & 36.1 & 24.7 & 52.8 & 
45.6 & 22.5 & 30.8 & 19.8 & 43.6 & 11.8 \\
4. & \cmark & 288 & 2 
& 47.0 & 28.2 & 38.3 & 26.3 & 55.7 & 
46.0 & 24.1 & 32.4 & \textbf{22.0} & 46.3 & 23.7 \\
5. & \cmark & 320 & 3 
& 46.9 & 28.3 & 39.2 & 26.4 & 56.0 & 
45.9 & 24.0 & 32.8 & 21.5 & \textbf{46.4} & 23.6 \\
6. & \cmark & 352 & 4 
& 47.2 & \textbf{28.7} & \textbf{39.6} & \textbf{27.1} & \textbf{56.4}
& \textbf{46.6} & \textbf{24.2} & \textbf{33.2} & 21.7 & 46.2 & 24.4 \\
7. & \xmark & 352 & 4 
& 47.1 & 27.1 & 37.4 & 24.1 & 53.7
& 46.2 & 22.9 & 31.3 & 19.6 & 44.0 & 18.1 \\
8. & \cmark & 384 & 5 
& \textbf{47.4} & 28.4 & 38.9 & 27.0 & 55.3 & 
46.4 & 24.0 & 32.8 & 21.7 & 45.6 & 26.1 \\
\end{tabular}}
\vspace{-0.3cm}
\caption{\small Comparison of performance-memory trade-off with various temporal strides $k$, frame spatial resolutions (Res.), with or without the fast branch in our video-text encoder, on the VidSTG validation set.}
\label{table:addresolution}
\end{center}
\vspace{-0.7cm}
\end{table*}

\section{Detailed ablation results}~\label{sec:addablation}
In this section, we provide detailed results split by sentence type (declarative, interrogative) on the VidSTG dataset for the ablation studies presented in Section~\ref{sec:ablations}.

\noindent \textbf{Space-time decoder.}\label{sec:adddecoderres}
We first provide detailed results for the ablation on our space-time decoder. 
The analysis is similar for both declarative and interrogative sentences.
In detail, Table~\ref{table:adddecoder} shows that there is a substantial improvement over the space-only decoder when using both time encoding and temporal self-attention (+18.3\% on $vIoU@0.3$ for declarative sentences and +17.5\% on $vIoU@0.3$ for interrogative sentences between rows 1~and 4). 
The gain comes mostly from the temporal localization (+22.0\% on $m\_tIoU$ for declarative sentences and +22.1\% on $m\_tIoU$ for interrogative sentences), while the spatial grounding moderately increases (+0.9\% in $\siou{}$ for declarative sentences and +0.5\% in $\siou{}$ for interrogative sentences). 
Furthermore, we can observe that the time encoding brings most of the gain (+14.0\% on $vIoU@0.3$ for declarative sentences and +12.9\% on $vIoU@0.3$ for interrogatives sentences between rows 1 and 3).
Finally, the temporal self-attention results in an additional improvement (+4.3\% on $vIoU@0.3$ for declarative sentences and +4.6\% on $vIoU@0.3$ for interrogative sentences between rows 3 and 4) over using time encoding only. 

\noindent \textbf{Initialization.}\label{sec:addinit}
We now provide detailed results for the ablation on our weight initialization.
The analysis is similar for both declarative and interrogative sentences.
In detail, Table~\ref{table:addinit} shows that pretraining is highly beneficial for spatio-temporal video grounding (+9.4\% on $vIoU@0.3$ for declarative sentences and +6.9\% on $vIoU@0.3$ for interrogative sentences between rows 1 and 3). 
The gain mainly comes from the spatial grounding performance (+11.7\% on $\siou{}$ for declarative sentences and +7.1\% on $\siou{}$ for interrogative sentences).
Additionally, we observe the benefit of using the spatial self-attention weights from the MDETR decoder to initialize the temporal self-attention in our decoder (+3.2\% on $vIoU@0.3$ for declarative sentences and +3.3\% on $vIoU@0.3$ for interrogative sentences between rows 2 and 3).

\begin{table*}[!htbp]
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{1.5pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lllll|ccccc|ccccc}
& \multirow{2}{*}{Slow} & \multirow{2}{*}{\makecell{\small{Spatial} \\ \small{Pool.}}}
& \multirow{2}{*}{f} & \multirow{2}{*}{g} & 
\multicolumn{5}{c}{Declarative Sentences} & \multicolumn{5}{c}{Interrogative Sentences} \\ 
& & & & & \small{m\_tIoU} & \small{m\_vIoU} & \small{vIoU@0.3} & \small{vIoU@0.5} & \small{\siou{}} & \small{m\_tIoU} & \small{m\_vIoU} & \small{vIoU@0.3} & \small{vIoU@0.5} & \small{\siou{}} \\
\hline
1. & \xmark & \xmark & Linear & Sum + Linear
& 42.7 & 18.6 & 25.0 & 14.8 & 39.6 & 
42.5 & 16.9 & 22.0 & 12.9 & 35.1 \\
2. & \cmark & - & 0 & 0 
& 46.2 & 24.9 & 34.4 & 21.8 & 49.7 & 
45.1 & 20.9 & 28.3 & 17.9 & 40.5 \\
3. & \cmark & \cmark & Linear & Sum + Linear 
& 45.8 & 25.0 & 34.7 & 22.1 & 50.2
& 44.9 & 21.1 & 29.2 & 17.8 & 40.9 \\
4. & \cmark & \xmark & Linear & Product + $\sigma$
& 46.2 & 26.2 & 36.0 & 23.9 & 52.0
& 45.4 & 22.1 & 30.1 & 18.8 & 43.0 \\  
5. & \cmark & \xmark & Transformer & Sum + Linear 
& \textbf{46.4} & 26.4 & \textbf{36.4} & 23.8 & \textbf{52.8}
& 45.3 & 22.2 & 30.2 & 19.6 & 43.3 \\  
6. & \cmark & \xmark & Linear & Sum + Linear 
& \textbf{46.4} & \textbf{26.6} & 36.1 & \textbf{24.7} & \textbf{52.8} 
& \textbf{45.6} & \textbf{22.5} & \textbf{30.8} & \textbf{19.8} & \textbf{43.6} \\ 
\end{tabular}}
\vspace{-0.3cm}
\caption{\small Comparison of designs for the video-text encoder, with or without the slow branch, with or without spatial pooling in the fast branch, with variants of the fast module $f$ and aggregation module $g$, on the VidSTG validation set.}
\label{table:highfre}
\vspace{-0.5cm}
\end{center}
\end{table*}

\noindent \textbf{Impact of spatial resolution and temporal stride $k$.}\label{sec:addresolution} 
In this section, we provide detailed results on the VidSTG dataset for the ablation on the impact of the spatial frame resolution and the temporal stride $k$.
The analysis is similar for both declarative and interrogative sentences.
In detail, Table~\ref{table:addresolution} shows that increasing the resolution is an important factor of performance for spatio-temporal video grounding (see rows 2 and 4).
Our proposed video-text encoder enables us to train on higher resolutions at a given memory usage.
This leads to a better performance-memory trade-off (rows 4, 5, 6, 8) compared to the baseline variant with temporal stride $k=1$ (row 1).
In particular, the best spatio-temporal video grounding results ($m\_vIoU$ and $vIoU@R$) are obtained with temporal stride $k=4$ and resolution 352 (row 6).

\noindent \textbf{Impact of the fast branch.} 
Finally, we provide detailed results on the VidSTG dataset for the ablation on the importance of our fast branch where we compare, for the best variant, temporal stride $k=4$ and resolution 352, our slow-fast video-text encoder to a slow-only variant.
The analysis is similar for both declarative and interrogative sentences.
By comparing rows 6 and 7 in Table~\ref{table:addresolution}, our fast branch significantly improves the spatio-temporal video grounding performance (+2.2\% $vIoU@0.3$ for declarative sentences and +1.9\% $vIoU@0.3$ for interrogative sentences) with low computational memory overhead.
This shows that the fast branch recovers useful spatio-temporal details lost by the temporal sampling operation in the slow branch.

\section{Additional Experiments}\label{sec:addexp}
In this section, we provide additional ablation studies. As in the ablations presented Section~\ref{sec:ablations}, unless stated otherwise, we use spatial frame resolution of 224 pixels and temporal stride $k=5$.

\noindent \textbf{Design of the fast and aggregation modules.}\label{sec:design}
Here we further ablate the fast and aggregation modules $f$ and $g$ used in our dual-branch encoder.
We report results in Table~\ref{table:highfre}.
The comparison between our slow-fast design (row 6) and the slow-only variant (row 2) is discussed in Section~\ref{sec:ablations}.
Likewise, we compare our slow-fast design to a fast-only variant (row 1). The fast-only variant does not use the slow multi-modal branch, in which case the video-text features are the fast visual-only features concatenated with the text features.
As shown in Table \ref{table:highfre}, our slow-fast design outperforms the fast-only variant, showing the importance of the slow multi-modal branch. 
We further compare the design of our fast and aggregation modules $f$ and $g$ (row 6) to other alternatives:
row 3, a variant with the same primitives $f$ and $g$ but with $f$ operating on features pooled over the spatial dimension;
row 4, a variant which uses the same fast module $f$ but a gating aggregation module $g(h_v(v, t), f(v)) = \sigma(h_v(v, t) * f(v))$ where $\sigma$ is the sigmoid function;
row 5, a variant that uses the same aggregation module $g$ but a fast temporal transformer module $f$, which models temporal interactions between spatially-detailed features.
As shown in Table \ref{table:highfre}, our design outperforms row 3, showing that preserving spatial information for each frame is crucial for the effectiveness of the fast branch. 
Additionally, our design slightly improves over row 4, indicating that further forcing the network to use the slow branch is not helpful.
Finally, our design slightly improves over row 5, suggesting that additional modeling of temporal interactions in our encoder is not necessarily helpful.