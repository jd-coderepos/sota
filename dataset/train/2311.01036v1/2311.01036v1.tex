\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}




\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[bb=dsserif]{mathalpha}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.9}
\usepackage{amssymb}

\newcommand{\minisection}[1]{\smallskip\noindent\textbf{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\attn}{\mathrm{A}}
\DeclareMathOperator*{\sattn}{\attn_{\mathrm{self}}}
\DeclareMathOperator*{\merge}{\mathrm{M}}
\DeclareMathOperator*{\tran}{\mathrm{T}}

\newcommand{\layernorm}{\ell}
\newcommand{\ff}{\operatorname{FF}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\where}{\mathrm{where}}
\newcommand{\st}{\text{s.t.}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\corr}{\operatorname{corr}}
\newcommand{\resize}{\mathcal{U}}
\newcommand{\PV}{\mathrm{P}}
\newcommand{\GV}{\mathrm{G}}
\newcommand{\infer}{\operatorname{infer}}
\newcommand{\answer}{\operatorname{answer}}
\newcommand{\Eq}{\mathcal{E}}
\newcommand{\1}{\mathbb{1}}




\title{ATHENA: Mathematical Reasoning with Thought Expansion}





\author{
JB. Kim\textsuperscript{1} \qquad
Hazel H. Kim\textsuperscript{2} \qquad
Joonghyuk Hahn\textsuperscript{1} \qquad
Yo-Sub Han\textsuperscript{1}\\
\textsuperscript{1}Yonsei University \qquad
\textsuperscript{2}Classting AI Research  \\
\texttt{
jb@thejb.net,
hazel.kimh@gmail.com,
\{greghahn,emmous\}@yonsei.ac.kr} 
}

\begin{document}


\maketitle
\begin{abstract}
Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions.
Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations.
Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge.
We introduce \textbf{A}ttention-based \textbf{TH}ought \textbf{E}xpansion \textbf{N}etwork \textbf{A}rchitecture (\textbf{ATHENA}) to tackle the challenges of real-world practices by mimicking human thought expansion mechanisms in the form of neural network propagation.
A thought expansion recurrently generates the candidates carrying the thoughts of possible math expressions driven from the previous step and yields reasonable thoughts by selecting the valid pathways to the goal.
Our experiments show that ATHENA achieves a new state-of-the-art stage toward the ideal model that is compelling in variant questions even when the informativeness in training examples is restricted.\footnote{The source code is available at \url{https://github.com/the-jb/athena-math}.}
\end{abstract}


\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure-thoughts.pdf}
    \vspace{-1.5em}
    \caption{Visualization of thoughts constructed for solving two problem samples with the same context description from the UnbiasedMWP dataset, one of our benchmarks.}
    \label{fig:thoughts}
\end{figure}



 \begin{table*}
    \centering
    \small
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{p{.55\textwidth}p{.45\textwidth}}
        \toprule
        \multicolumn{2}{p{\textwidth}}{
        \textbf{Context} The school playground was originally [80] meters long and [40] meters wide. Later when the school is remodeled, the length is increased by [10] meters and the width is increased by [15] meters.
        } \\
\midrule
        \multicolumn{2}{c}{
\scriptsize Train on an example of a question-solution pair under the context above.
        }\.5ex]
        \multicolumn{2}{p{\textwidth}}{
        \textbf{Q0} How many times the length of the original playground was the width?
        } \.7ex]
		\textbf{DeductReasoner} & \hspace{1em} \textbf{ATHENA} \\
        {\scriptsize UnbiasedMWP \textcolor{red}{} } (X)
        & \hspace{1em} {\scriptsize UnbiasedMWP \textcolor{blue}{} } (O)\\
        {\scriptsize UnbiasedMWP (1:N) \textcolor{red}{}} (X)
        & \hspace{1em} {\scriptsize UnbiasedMWP (1:N) \textcolor{blue}{} } (O)\\

        \midrule[.1pt]
        \multicolumn{2}{p{\textwidth}}{
        \textbf{Q2} How many square meters are increased by the current playground area compared to the original one?
		} \.5ex]
\multicolumn{2}{p{\textwidth}}{
        \textbf{Context} The school basketball court was [20] meters long and [12] meters wide. After the renovation, the length is increased by [8] meters, and the width increases by [3] meters.
        }\\
        \textbf{Question} How many square meters are increased? & \hspace{1em} \textbf{Solution}  \\

         \bottomrule
    \end{tabular}
    \caption{Predictions of DeductReasoner~\citep{jie-etal-2022-learning} and ATHENA on a sample that has variant questions while sharing the common context for the problems. The observation above is when models use RoBERTa-large on UnbiasedMWP~\citep{yang-etal-2022-unbiased}.}
    \label{tab:dr_athena}
    \vspace{-.1em}
\end{table*} 
Math word problem (MWP) solving is one of the fundamental reasoning tasks of answering a mathematical question by understanding a complex, intricate system of human lexical expressions.
Models' ability to solve a problem depends on a method that articulates the problem, the lens through which they view human lexical expressions.
Ideal MWP models understand the diverse applications of the same mathematical operations in real-world situations, which require lexically sophisticated.
For example, ``'' can count all elements equally divided in multiple boxes, but also calculate area from length and width, or tax fee from the tax rate and income.

It is significant how we estimate if the model has learned mathematical reasoning to qualify for the ideal model.
We state that the ideal models that learn mathematical problems must be able to solve unseen problems if they are applications of mathematical operations that models have already seen, or soundly solve problems even when given examples to learn are restricted.

Humans learn mathematical knowledge by formulating and understanding underlying principles from seen cases rather than just recognizing the common lexical patterns.
Models currently face two challenges to reach human-level mathematical understanding: \textit{conceptual knowledge} to understand the practices of mathematical principles, and \textit{procedural knowledge} to deductively derive the answer through the principles, which are indispensable for each other in their development and usage~\citep{rittle1999conceptual, byrnes1991role, canobi2009concept, rittle-etal-2014-developing}.

Prior approaches mostly adopt the transduction-based models such as sequence-to-sequence~\citep{ling-etal-2017-program, wang-etal-2018-translating}, sequence-to-tree~\citep{xie-sun-2019-goal, liu-etal-2019-tree} or graph-to-tree methods~\citep{zhang-etal-2020-graph-tree, li-etal-2020-graph-tree} and concentrate on enhancing problem-level encoding~\citep{shen-jin-2020-solving, zhang-etal-2020-graph-tree, lin-etal-2021-hms, yu-etal-2021-improving}.
These works have limitations in obtaining procedural knowledge due to their prediction strategies that operate in a counter-intuitive order.
For instance, sequence-to-tree approaches determine mathematical operations before the operands in the inference steps.
Recently, \citet{jie-etal-2022-learning} proposed a deductive prediction strategy, but it still entails procedural bias, accepting only one particular reasoning pathway.

Overall, the prior studies show limitations in learning mathematical procedures, which is significant for achieving successful mathematical skills.
As a result, despite their high accuracies on some benchmarks, the current approaches fail to solve variant questions that are simply mutated from already trained examples~\citep{patel-etal-2021-nlp, yang-etal-2022-unbiased}.
As shown in Table~\ref{tab:dr_athena}, we empirically argue that they learn the repeated patterns in given problems rather than the underlying principles formulating the equations.

The cognitive inadequacy of previous models motivates us to propose a new reasoning architecture that maximizes feasible reasoning pathways.
We design ATHENA that reasons with thought expansion inspired by the studies of human reasoning
\citep{johnson2008we, rittle-etal-2014-developing}.
The key idea is to implement two types of thoughts in the expansion process: \textit{thoughts before considering goals} formed by conceptual knowledge and \textit{goal-directed thoughts} yielded by procedural knowledge.


Figure~\ref{fig:thoughts} illustrates an example of thoughts formed via asking different questions (goal) within the same situation (context).
Because it is tricky for models to answer different questions that share a lexically similar problem context, we state that the two different thoughts would lead to the right answer by properly utilizing mathematical knowledge.
We expand the thoughts by applying conceptual knowledge to obtain candidate thinking pathways and procedural knowledge to evaluate the potential answers.
This is how we endow models with mathematical reasoning ability and empirically demonstrate that the model has actually learned the knowledge.

ATHENA puts the aforementioned thinking process explicitly into neural network propagation.
Defining the term \textit{thought} as a representation of each math expression driven from the problem, we shape \textit{candidate thoughts} and the goal-directed thoughts named \textit{reasonable thoughts}.
The model generates candidate thoughts by applying mathematical operations and yields reasonable thoughts by filtering with solidly updated premises until it meets the appropriate answer.
With this recurrent process, we develop a neural model of processing thoughts based on multi-head attention~\citep{vaswani-etal-2017-attention} that effectively carries the subtle feature changes during expansion.

Our experiments show that the proposed approach is strong at predicting mathematical expressions requiring sophisticated comprehension as shown in Table~\ref{tab:dr_athena}.
We observe that ATHENA produces a solid performance when the model needs to deal with previously unseen questions.
ATHENA is also very compelling to solve variant questions once it has learned one question established from the shared context.
From the experimental results, we conclude that ATHENA reaches a new state-of-the-art stage toward the ideal MWP model that we define as the one that can learn mathematical reasoning.

\section{Math Word Problem}

Math word problem (MWP) solving is the task of answering a mathematical question by understanding natural language descriptions.

\subsection{Problem Formulation}

Our task of solving MWPs is defined as follows.
Each example in the MWP dataset  has a problem sequence~ in natural language as input and an equation~ as expected output.
 consists of ~(problem, equation) tuples, where  is the number of examples:

We use a pre-trained language model~(PLM) to embed .
Let  denote a tokenized sequence of ,
where  represents each subword token.
The PLM output of  is denoted as , where  is an embedding vector of each token~.

\subsection{Related Work}



MWP problems have begun with feature engineering via hand-crafted rules or statistical concepts~\citep{bakman-etal-2007-robust, hosseini-etal-2014-learning, mitra-baral-2016-learning}.
Early works have adopted neural network approaches through end-to-end learning strategies such as sequence-to-sequence~\citep{ling-etal-2017-program, wang-etal-2018-translating, li-etal-2019-modeling} or sequence-to-tree~\citep{xie-sun-2019-goal, liu-etal-2019-tree, chiang-chen-2019-semantically, qin-etal-2020-semantically}. The previous approaches often 
use the networks or manipulate the representation with tree or graph templates to generate mathematical equations in a structurally sophisticated manner~\cite{wang-etal-2017-deep, zhang-etal-2020-graph-tree}.

Having developed and become accessible to pre-training and transfer learning, several approaches have promoted their performance with pre-trained language models~\citep{shen-etal-2021-generate-rank, yu-etal-2021-improving, huang-etal-2021-disenqnet, zhang-etal-2020-teacher, liang-etal-2022-mwp}, aiming to enhance the encoder with pre-trained embeddings.
Other approaches have made a key contribution by utilizing additional knowledge such as semantic meaning. Some take advantage of structural information such as hierarchical dependency~\citep{shen-jin-2020-solving,lin-etal-2021-hms,yu-etal-2021-improving}, formula structure~\citep{huang-etal-2020-neural}, graph-edge connection information~\citep{zhang-etal-2020-graph-tree,wu-etal-2021-edge-enhanced, li-etal-2020-graph-tree} and more~\citep{li-etal-2022-seeking, shen-etal-2021-generate-rank}.

All these approaches mostly aim at enhancing problem-level information but the recent studies demonstrate the significance of inference procedures in reasoning tasks.
\citet{wei-etal-2022-chain} show impressive success for large-scale language models in complex reasoning tasks by adopting chain-of-thought prompting.
The reasoning extraction method~\citep{jie-etal-2022-learning} has recently reached decent performance by constructing the deductive order in solving MWP.


\section{ATHENA}
\label{sec:athena}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/athena-overview.pdf}
    \vspace{-1.5em}
    \caption{Overall process of ATHENA. First, extract initial thoughts, an initial reasoning vector, and a goal vector from PLM. Second, expand thoughts by transform  or merge  and generate candidate thoughts. Third, infer the candidate thoughts to obtain new reasonable thoughts. Last, give reasonable thoughts to the next expansion. Repeat until meeting a thought that answers the goal vector.}
    \label{fig:athena_overview}
\end{figure*}

\textbf{A}ttention-based \textbf{TH}ought \textbf{E}xpansion \textbf{N}etwork \textbf{A}rchitecture (ATHENA) is an architecture that expands its thoughts to solve MWP.
Figure~\ref{fig:athena_overview} illustrates an overall process of ATHENA.
ATHENA extracts initial thoughts  from PLM and expands them with inferring through premises until it reaches the final thought.
We first clarify what is a 
\textit{thought}---a foundational ingredient of our model---and explain the premise and goal vectors that measure the thoughts.

\paragraph{Thought.}
A thought is an embedding of a possible math expression derived from quantities in a problem representing the contextual meaning of the expression.
Let  denote a thought with hidden size  corresponding to an expression .
A goal of the model is to find a thought~ that satisfies the ground-truth expression :


\paragraph{Premise Vector.}
A premise vector represents previously inferred thoughts to evaluate and filter candidate thoughts in each depth.
Let  denote a premise vector for depth .
We set an initial premise vector  with the [CLS] token from the problem descriptions.

\paragraph{Goal Vector.}
A goal vector plays a role as ground-truth measurement to evaluate if a thought is an appropriate answer to the question.
We set a goal vector  with a tokenized embedding of the punctuation mark in the question description.  


\subsection{Initial Thought}
\label{sub:initial_stage}

An initial thought is an embedding that carries each quantity representation illustrated in a context or question description.
We mask quantities with [MASK] token and obtain the embeddings that capture contextual information from the perspective of corresponding quantities.
We denote a set of thoughts in the initial depth by :


Certain quantity representations such as  are necessary for generating mathematical expressions despite not being presented in the contexts or questions. We collect them from a training set and randomly initialize their embeddings.
We also put their embeddings to initial thoughts .

\subsection{Thought Expansion}
In each depth, thought expansion constructs candidate thoughts  and filters them to obtain the \textit{reasonable} thoughts .
Reasonable thoughts are the waypoint thoughts to reach the final thought.

The two stages in a thought expansion are:
(1) our model generates candidate thoughts  from previous thoughts  through the operations
and (2) it reasons about the candidates if they are worth to be reasonable thoughts .
Expansion keeps going until finding one of the reasonable thoughts qualified to be a final thought .

\subsubsection{Candidate Thought}
Our model generates a set of possible new 
thoughts~ from the previous thoughts  as the candidates.
A new thought  is a thought of a math expression obtained by combining two previous thoughts  with an arithmetic operation:


To make a new thought, we introduce two operation layers whose combination can represent the arithmetic operations: merge  and transform .
These layers aim to maximize the reflection of the characteristics of arithmetic operations rather than the separate layers of individual arithmetic operations.
The definitions of merge and transform are shown below.

\paragraph{Merge.}
Merge layer  merges a pair of thoughts  into a new thought  such that  applies addition and multiplication to  and :
\vspace{-1em}


\paragraph{Transform.}
Transform layer  transforms a thought  into a new thought  such that
 applies inverse operations of addition and multiplication to :
\vspace{-1em}


We use Feed-Forward Network (FFN) and multi-head attention inspired by \citet{vaswani-etal-2017-attention} for the implementation of the operation layers.
We use FFN referred to as  for transform layer .
Using multi-head self-attention  and layer normalization , we implement merge layer  followed by:
\vspace{-.3em}


\vspace{.7em}

This implementation satisfies  to be commutative
for :

We apply transform layer  for depth 
and merge layer  for depth  to generate the candidates.
In the case of the beginning depth , we use the initial thoughts  as the candidates.


\subsubsection{Reasonable Thought}
After obtaining candidate thoughts , our model yields reasonable thoughts  that constitute the final thought .
In each depth , it selects reasonable thoughts from candidate thoughts through the inference layer  with a premise vector .

\paragraph{Infer.}
The inference layer calculates the correlation score between the premise vector  and each candidate thought  using multi-head attention  and feed-forward network  with sigmoid function  to evaluate if a thought is acceptable within the premises:

A thought  is \textit{reasonable} if its correlation score  exceeds a threshold .
In the next iteration , the reasonable thoughts in the current depth  become the input.

\paragraph{Update Premises.}
A previously obtained consequence can become a premise for the next inference step.
Accordingly, our model updates the premise vector  with the reasonable thoughts  obtained in the current depth  to prepare a premise vector for the next step .
It gains the updated premise vector by concatenating all reasonable thoughts  after the multi-head attention  using the parameters of the inference layer :



\begin{algorithm}[t]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\small
\caption{\small Thought Expansion Process of ATHENA}\label{alg:athena}
\begin{algorithmic}
\Require 
\Ensure 
\State 
\State  
\While{}
\State 
\State 
\If{} \State 
\ElsIf{} \State 
\EndIf
\State 
\EndWhile
\State  \\
\Return 
\end{algorithmic}
\end{algorithm}


\subsection{Final Thought}
\label{sec:final}

A final thought  is the answer to the question.
When the thought expansion process finishes, our model decides the final thought by selecting a thought with the maximum score.
We have two criteria to terminate the iteration;
(1) when the depth reaches the maximum expansion depth ;
(2) if there is a thought with the score that exceeds a confidence threshold  on iteration.
We calculate the score of each reasonable thought   using the multi-head attention  and feed-forward network  with the goal vector , activated by sigmoid :

where .

A thought with the maximum score in the reasonable thoughts becomes a final thought : 

The model bestows the final thought the fidelity to shape the answer to the goal of the problem.

Algorithm~\ref{alg:athena}
shows the overall process to derive the final answer  from inputs .



\section{Experiments}

We conduct experiments across a comprehensive range of MWP) solving tasks to show that ATHENA outperforms strong baselines in both full datasets and variant versions of the original datasets while being more interpretable in terms of intermediate steps toward the answers.






\subsection{Experimental Setups}



\paragraph{Baselines.}
We select four representative approaches as the baselines to compare with ATHENA: 
Transformer~\citep{vaswani-etal-2017-attention}\footnote{We follow hyperparameters by \citet{lan-etal-2022-mwptoolkit} for both vanilla transformer and RoBERTa-based transformer.},
a goal-driven tree-structured model (GTS)~\citep{xie-sun-2019-goal},
Graph-to-Tree~\citep{zhang-etal-2020-graph-tree}\footnote{We follow the best hyperparameter settings in \citet{patel-etal-2021-nlp} for both vanilla models and RoBERTa-based models.}
and Deduct\-Reasoner~\citep{jie-etal-2022-learning}.\footnote{We use their hyperparameter setups. We use the MAWPS setup for testing ASDiv-A, and use the Math23k setup for UnbiasedMWP. Since the authors do not provide setups for RoBERTa-large, we optimize the model and report the best score with half batch size and half learning rate from those used in the RoBERTa-base setup.}
Transformer is a sequence-to-sequence approach that uses multi-head attention mechanism while
GTS is a strong baseline of sequence-to-tree model.
Graph-to-Tree is another approach that adds a graph encoder on top of GTS.
We adopt DeductReasoner as an additional baseline that introduces a complex relation extraction method for deductive steps and hence achieves the state-of-the-art performance.



\paragraph{Implementation Details.}
We use RoBERTa-base and RoBERTa-large as our base pre-trained embeddings~\citep{liu-etal-2019-roberta} and Chinese-RoBERTa \citep{chinese-roberta} for Chinese benchmarks to compare our baselines.
We use pre-layer normalization \citep{xiong-etal-2020-layer} for our multi-head attention method to fully leverage a dynamic range of embeddings.
We set  by the maximum value of the reasoning depth of test examples for each dataset.\footnote{We present the values of each dataset in Table~\ref{tab:stop}.}
We set  and train our model by giving ideal accepted prior thoughts  and labels of  and  in each depth to calculate the loss with binary cross entropy over all labels.\footnote{We present detailed training settings and hyperparameters in Appendix~\ref{sec:appendix_training}}
We perform our experiments with Nvidia RTX 3090 GPU.

\paragraph{Dataset.}
We test ATHENA on both standard MWP benchmarks 
and relatively new benchmarks that contain various linguistic expressions in contexts or questions for mathematical reasoning. The standard benchmarks are \textbf{MAWPS}~\citep{koncel-kedziorski-etal-2016-mawps}, \textbf{ASDiv-A}~\citep{miao-etal-2020-diverse}, and \textbf{Math23k}~\citep{wang-etal-2017-deep}. MAWPS is an English corpus collected from the online MWP repository, and
Math23k is a Chinese corpus crawled from online posts. ASDiv-A is an acronym of An arithmetic subset of Academia Sinica Diverse dataset (ASDiv-A), consisting of diverse English lexical patterns.

The relatively new benchmarks either alter the standard benchmarks or vary the grounded expressions from the collected data to evaluate the model performance without bias from learned data. \textbf{SVAMP}~\citep{patel-etal-2021-nlp} varies in the components of one of the standard benchmarks, ASDiv-A to evaluate various contextual expressions on elementary-level arithmetic problems.
\textbf{UnbiasedMWP}~\citep{yang-etal-2022-unbiased} is an online-crawled Chinese corpus that augments the questions from the same context to evaluate models if they are able to generate adequate corresponding mathematical expressions. We split MAWPS, ASDiv-A, Math23k, SVAMP, following \citet{jie-etal-2022-learning} and \citet{patel-etal-2021-nlp}, respectively.




\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lccc|cc|cc}
        \toprule
         & \textbf{MAWPS} & \textbf{ASDiv-A} &  \textbf{Math23k} &\textbf{SVAMP} & \textbf{UnbiasedMWP} & \textbf{SVAMP (1:N)} & \textbf{UnbiasedMWP (1:N)}\.65ex]
        \textbf{ATHENA}(Ours)  
        & \textbf{92.2}{\tiny\textpm0.10} & \textbf{86.4}{\tiny\textpm0.11} &  84.4{\tiny\textpm0.24} & \textbf{45.6}{\tiny\textpm0.50} & \textbf{36.2}{\tiny\textpm0.67} & \textbf{52.5}{\tiny\textpm0.50} (70.1) & \textbf{35.4}{\tiny\textpm0.45} (80.5) \\
        \bottomrule[0.01em]
        {\tiny RoBERTa-large} \\
        DeductReasoner  & 92.6{\tiny\textpm0.16} & 89.1{\tiny\textpm0.46}
        & 85.8{\tiny\textpm0.42}  & 50.3{\tiny\textpm0.30} & 34.9{\tiny\textpm0.11}  & 51.6{\tiny\textpm0.38} (75.4)   & 33.7{\tiny\textpm0.60} (83.2) \-.5ex]
         \multicolumn{1}{c}{\tiny{Avg depth}} & \tiny{3.87} & \tiny{3.46} & \tiny{3.47} & \tiny{5.18} & \tiny{4.44} &  \tiny{3.47} & \tiny{4.44} & \tiny{4.05} \\
         \midrule
\textbf{ATHENA}  & \textbf{92.2} & \textbf{86.4} & \textbf{45.6} 
        & \textbf{85.1} & 36.2  & \textbf{52.5} & \textbf{35.4} & \textbf{62.0} \\
        {\scriptsize  update} & 92.1 & 84.8 & 44.9 & 82.7 & 34.9 & 52.4 & 34.7 & 60.9 \\
        {\scriptsize  premise} &90.6&85.0&44.7& 65.7 & \textbf{36.3} & 51.5 & 34.6 & 58.3 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation studies on premise vector construction. (1) `` update'' is the premise vector without updating strategies and (2) `` premise'' is the direct classification method without premise vectors.}
    \label{tab:reasoning}
\end{table*} 
\paragraph{Visualization of Thoughts.}
We interpret the thoughts using attention scores between reasonable thoughts and the problem sequence.\footnote{We use  layer to calculate the attention score, giving the problem sequence embedding as an input, instead of the goal vector.}
As illustrated in Figure~\ref{fig:attention}, we observe how the thought relates to the words.  
Most of the initial thoughts are related to the ``playground'', while the thoughts carrying the meaning of increased size show a strong correlation to the word ``Later''.
The thoughts carrying width sizes [15] and [40+15] show high attention scores on ``width'', while the other thoughts do not have high attention scores on them.
Thoughts that calculate the area produce high attention scores on words ``square meter'' or ``area''.
The final thought marks a high score on ``compared'', which asks for the difference between the increased and original areas.



\paragraph{Ablation on Premise Vectors.}
A premise vector is a criterion for determining thoughts in each inference step to obtain the valid pathways to reach the goal.
We conduct an ablation study to evaluate how ATHENA composes the premise vectors to ultimately generate optimal final thoughts.  

For evaluating the impact of the premise vectors in generating reasonable thoughts, we adopt two different settings:
(1) We do not \textit{update} premise vectors but use the initial premise vector (i.e., [CLS] token) in all expansion depths: .
We aim to see how the existence of thoughts that update the premise vector impacts models to help find solid reasonable thoughts.
(2) We do not use the premise vector and directly classify the thoughts for the next iteration:  .


Table~\ref{tab:reasoning} shows the results of the different premise construction strategies for reaching the appropriate conclusion.
Despite slight fluctuations across different methods, ATHENA without premise vectors decreases the overall performances by up to 3.7\%p compared to our proposed method.
When the model does not update the premise vectors in the thought expansion iteration while still adopting the initial one, the performance decreases relatively by 1.1\%p.
It is notable that Math23k, a dataset of the deepest average depth, shows the performance degradation even worse, 2.4\%p and 19.4\%p respectively.
From these observations, we conclude that the premise vector plays an important role in properly deriving the final thoughts.
Especially, considering that the model applies the update on every expansion depth, the large performance gap for Math23k strongly supports our premise update method for its effectiveness.


\section{Conclusion}
We state that an ideal MWP model needs to be practical in real-world settings that are critical to capture the diverse applications of the same mathematical operations.
For this reason, we conclude that ATHENA with thought expansion reaches significant improvements toward the ideal model due to its decent performance on unseen problems or restricted examples to learn.

\section*{Limitations}
The paper has the following limitations.
First, we only consider arithmetic problems, not algebraic, calculus, or other topics of mathematical problems.
Especially, for a fair comparison with other 
models, we only evaluate the performance using MWP datasets with single equations, while the model is able to handle multi-equation problems by simply adding ``='' operation on Merge.
Second, we do not compare ATHENA with large-scale language models (LLMs) since 
we focus on acquiring knowledge from limited mathematical samples.



\section*{Ethics Statement}
This work breaks down a process of reasoning from the human cognitive perspective and instantiates individual thoughts with symbolic representation so that it can clarify and handle the intermediate procedures of the model.
Although the perspective may have the potential to filter harmful or toxic thoughts from the broad sight of thoughts, this work does not consider or validate the effectiveness of such applications.
Therefore, we do not suggest using our work for this purpose without thorough experiments for its possibilities.

\section*{Acknowledgements}
This research was supported by the NRF grant (RS-2023-00208094) and the AI Graduate School Program (No.~2020-0-01361) funded by the Korean government~(MSIT).
Han is a corresponding author.
\bibliography{athena}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\section*{Appendices}
\section{Training Details}
In this section, we provide detailed information about our training settings.
\label{sec:appendix_training}

\paragraph{Loss.}
Given an answer equation , let  denote the target of  for a thought  and  denote the target of the final decision  for a thought :

where  denotes that  contains the sub-expression  (e.g., ``''  ``'').

Let  denote the binary cross entropy function, the training objective is to minimize loss :


\paragraph{Optimizer.}
We use AdamW optimizer \citep{loshchilov-etal-2017-decoupled} with weight decay .
Learning rate  for each epoch  is decayed every  epoch with factor 
starting from :


\paragraph{Regularization.}
We adopt dropout with probability  to every layer and
stochastic weight averaging \citep{izmailov-etal-2018-averaging} for last  epochs.


\paragraph{Hyperparameters.}
We present our experiments for hyperparameters in Table~\ref{tab:hyper},
with the bold text denoting the best performance.
We train our model for  epochs.
In the result, we observe that RoBERTa-base and RoBERTa-large share the best hyperparameter settings except for learning rate .


\section{Dataset Statistics}
In this section, we show the statistics of datasets and their requirements.

\paragraph{One-to-many Split.}
In Section~\ref{sec:one_to_many}, we explain building one-to-many dataset splits.
We provide how many groups and examples are made from the contexts in Table~\ref{tab:otm_statistics}.

\paragraph{Number of Thoughts.}
We present the required number of thoughts for each dataset in Table~\ref{tab:thoughts}.
While Math23k requires a large number of candidate thoughts in total depth, we show a thought expansion in each depth does not require huge memory space. Therefore, efficient implementation strategies such as removing unselected candidate thoughts from memory space are enough to manage computational resources.

\section{Additional Experiments}
This section presents additional studies to further clarify the robustness and fairness of our experiments for some minor strategies by showing their performance independence.

\paragraph{Punctuation Mark.}
In Section~\ref{sec:athena}, we initialize goal vector  with the punctuation mark of the question sequence or the last punctuation mark (i.e., the question mark in most cases).
The motivation of this strategy is from \citet{clark-etal-2019-bert} showing the punctuation mark gets high attention from other tokens in the last layers.
Intuitively, high attention can generalize the question sequence,
so we conduct experiments to evaluate the generalization ability of the punctuation mark compared to using all question sequences as a goal vector .
We conduct experiments for all datasets except Math23k~\citep{wang-etal-2017-deep} since it does not provide the question subsequence.


As shown in Table~\ref{tab:question}, using the punctuation mark effectively generalizes the question to represent a goal in most cases.
It shows even better performances than using the question sequence.
From an intuitive interpretation, the question sequence holds some tokens that are not informative for reasoning targets, so a punctuation mark representation helps the model to focus on a reasoning goal.


\paragraph{Stop Criteria.}
In Section~\ref{sec:final} and Algorithm~\ref{alg:athena}, we present the two stop criteria: (1) the depth reaches the maximum expansion depth , or (2) one of the final scores exceeds a threshold .
In addition to the main experiments setting the  and  with an arbitrary value,
we conduct the experiments of the higher maximum expansion depth and  to show the performance differences from the values.
As shown in Table~\ref{tab:stop}, the scores are fairly equal with a trivial gap.
This demonstrates that the performances of our model do not rely on stop criteria parameters but are solid achievements.



\begin{table*}
    \centering
    \setlength{\tabcolsep}{3pt}
    \small
    \begin{tabular}{lcccccc}
        \toprule
        & Batch Size &  &  &  &  &  \\
        \midrule
        RoBERTa-base & [\textbf{4}, 8]  & [5e-6, 7e-6, 1e-5, \textbf{1.3e-5}, 1.5e-5, 2e-5] & [\textbf{10}, 15, 20] & [\textbf{0.5}, 0.7] & [0.1, \textbf{0.5}] & [\textbf{30}, 50, 70] \\
        RoBERTa-large & [\textbf{4}, 8]  & [5e-6, \textbf{7e-6}, 1e-5, 1.3e-5, 1.5e-5, 2e-5] & [\textbf{10}, 15, 20] & [\textbf{0.5}, 0.7] & [0.1, \textbf{0.5}] & [\textbf{30}, 50, 70] \\
         \bottomrule
    \end{tabular}
    \caption{Hyperparameter search spaces of ATHENA}
    \label{tab:hyper}
\end{table*}

\begin{table*}
    \centering
\small
    \begin{tabular}{lcc}
        \toprule
         & \textbf{SVAMP (1:N)} & \textbf{UnbiasedMWP (1:N)} \\
        \midrule
        \# examples in original split  & 3138 / 0 / 1000 & 2507 / 200 / 685 \\
        \# groups of single examples & 438 & 45 \\
        \# groups of multiple examples & 205 & 154\\
        \# examples in one-to-many split & 3343 (+205) / 438 (+438) / 357 (-562) & 2661 (+154) / 245 (+45) / 486 (-199) \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of one-to-many test splits}
    \label{tab:otm_statistics}
\end{table*}

\begin{table*}
    \centering
    \setlength{\tabcolsep}{5pt}
\small
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \textbf{Dataset}
        & \multicolumn{3}{|c|}{\# candidates in total depth} & \multicolumn{3}{c|}{\# in a reasoning path}
        & \multicolumn{3}{c|}{\# candidates in last depth} & \multicolumn{3}{c}{depth of reasoning path} \\
        \addlinespace[0.4ex]
        \midrule
        \addlinespace[0.4ex]
        & min & average & max & min & average & max & min & average & max & min & average & max\.4ex]
        ASDiv-A & 16 & 26.86\textpm0.42 & 71 & 3 & 4.10\textpm0.03 & 7 & 6 & 9.65\textpm0.09 & 22 & 1 & 3.46\textpm0.02 & 5\.4ex]
        Math23k & 4 & 65.1\textpm0.31 & 939 & 1 & 6.33\textpm0.02 & 29 & 2 & 14.85\textpm0.06 & 108 & 1 & 5.18\textpm0.01 & 41\-.5ex]
         \multicolumn{1}{c}{\tiny{Avg depth}} & \tiny{3.87} & \tiny{3.46} & \tiny{3.47} & \tiny{4.44} &  \tiny{3.47} & \tiny{4.44} & \tiny{4.05} \\
         \midrule
         {\tiny RoBERTa-base} \\
         punctuation mark  & \textbf{92.2} & \textbf{86.4} & \textbf{45.6} 
         & 36.2  & \textbf{52.5} & \textbf{35.4} & \textbf{58.1} \\
        question sequence & 92.0 & 86.3 & 44.9  & \textbf{36.3} & 51.0 & 33.4 & 57.3 \\
        \midrule
        {\tiny RoBERTa-large} \\
        punctuation mark & \textbf{93.0} & 91.0 & \textbf{54.8}
          & \textbf{42.0} & \textbf{67.8} & \textbf{48.4} & \textbf{66.2} \\
          question sequence & 92.9 & \textbf{91.2} & 54.4  & 41.0 & 66.9 & 46.8 & 65.5 \\
        \bottomrule
    \end{tabular}
    \caption{Comparing goal vector using the whole question sequence from the punctuation mark}
    \label{tab:question}
\end{table*}



\begin{table*}
    \centering
    \setlength{\tabcolsep}{2pt}
    \small
    \begin{tabular}{lccccc|c}
        \toprule
         \textbf{Stop Criteria} & \textbf{MAWPS} & \textbf{ASDiv-A} & \textbf{SVAMP} & \textbf{Math23k} & \textbf{UnbiasedMWP} & \textbf{Average}\-.5ex]
          & \tiny{D=7} & \tiny{D=5} & \tiny{D=5} & \tiny{D=19} &  \tiny{D=9} &  \\
         \midrule
         , max. expansion depth= & \textbf{92.2{\tiny\textpm0.10}} & 86.4{\tiny\textpm0.11} & \textbf{45.6{\tiny\textpm0.50}} & 84.4{\tiny\textpm0.24} & 36.2{\tiny\textpm0.67} & \textbf{69.0} \\
         \midrule
         , max. expansion depth= & 92.0{\tiny\textpm0.15} & 86.3{\tiny\textpm0.24} & 45.3{\tiny\textpm0.37} & 84.7{\tiny\textpm0.20} & 36.0{\tiny\textpm0.39} & 68.9 \\
         , max. expansion depth= & 91.9{\tiny\textpm0.07} & \textbf{86.5{\tiny\textpm0.28}} & 45.2{\tiny\textpm0.41} & 84.6{\tiny\textpm0.18} & 35.8{\tiny\textpm0.75} & 68.8 \\
         , max. expansion depth= & 92.0{\tiny\textpm0.09} & \textbf{86.5{\tiny\textpm0.28}} & 45.2{\tiny\textpm0.41} & \textbf{84.8{\tiny\textpm0.27}} & \textbf{36.4{\tiny\textpm0.44}} & \textbf{69.0} \\
        \midrule
    \end{tabular}
    \caption{Performances among the different parameters of stop criteria}
    \label{tab:stop}
\end{table*}



\end{document}
