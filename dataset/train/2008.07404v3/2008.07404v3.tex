

\documentclass[review]{cvpr}


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}

\usepackage{amssymb}
\usepackage[caption=false]{subfig}

\usepackage{graphicx}


\usepackage{pgfplots}
\pgfplotsset{compat=1.12}

\usepackage{amsmath,amssymb} \usepackage{xcolor-material}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\confYear{CVPR 2021}



\begin{document}

\title{Skeleton-based Action Recognition \\via Spatial and  Temporal Transformer Networks}

\author{Chiara Plizzari \qquad Marco Cannici \qquad Matteo Matteucci \\
Politecnico di Milano, Italy\\
{\tt\small chiara.plizzari@mail.polimi.it} \\
{\tt\small \{marco.cannici,matteo.matteucci\}@polimi.it}


}

\maketitle


\begin{abstract}
Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. 
In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer \textit{self-attention} operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, outperforming the state-of-the-art on NTU-RGB+D w.r.t. models using the same input data, i.e., joint information.
\end{abstract}
 \begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.46\textwidth]{figures/Fig_0.pdf}
    \end{center}
    \caption{{Self-attention on skeleton joints}. (1) For each body joint, a query , a key  and a value vector  are calculated. (2) Then, the dot product () between the query of the joint and the key of all the other nodes is performed, representing the connection strength between each pair of nodes. (3) Finally, each node is scaled by its correlation w.r.t. the current node, (4) whose new features are obtained summing the weighted nodes together. 
    }
    \label{fig0}
\end{figure}
\section{Introduction}
Human Action Recognition is achieving increasing interest in recent years for the progress achieved in deep learning and computer vision and for the interest of its applications in human-computer interaction, eldercare and healthcare assistance, and video surveillance. 
Recent advances in 3D depth cameras such as Microsoft Kinect \cite{zhang2012microsoft,robust-hand-gesture} and Intel RealSense \cite{Keselman_2017_CVPR_Workshops} sensors, and advanced human pose estimation algorithms \cite{openpose,multi-context} made it possible to estimate 3D skeleton coordinates quickly and accurately with cheap devices (refer to the survey by \cite{giancola2018survey} for an in-depth analysis of recent devices). 
Nevertheless, several aspects of skeleton-based action recognition still remain open \cite{a-comprehensive-survey,human-activity,a-survey}. 
The most widespread method to perform skeleton-based action recognition has nowadays become Graph Neural Networks (GNNs), and in particular, Graph Convolutional Networks (GCNs) since, being an efficient representation of non-Euclidean data, they are able to effectively capture spatial (intra-frame) and temporal  (inter-frame) information. Models making use of GCN were first introduced in skeleton-based action recognition by~\cite{yan2018spatial} and they are usually referred to as Spatial-Temporal Graph Convolutional Networks (ST-GCNs). These models process spatial information by operating on skeleton bone-connections along space, and temporal information by considering additional time-connections between each skeleton joint along time. Despite being proven to perform very well on skeleton data, ST-GCN models have some structural limitations, some of them already addressed by~\cite{dirgraph,2s-cnn,shift,disent}.

First of all, the topology of the graph representing the human body is fixed for all layers and all the actions; this may prevent the extraction of rich representations for the skeleton movements during time, especially if graph links are directed and information can only flow along a predefined path. Secondly, both Spatial and Temporal Convolution are implemented starting from a standard 2D convolution. As such, they are limited to operate in a local neighborhood, somehow restricted by the convolution kernel size. And finally, as a consequence of the previous, correlations between body joints not linked in the human skeleton, e.g., the left and right hands, are underestimated even if relevant in actions such as ``clapping".
In this paper, we face all these limitations by employing a modified Transformer self-attention operator, as depicted in Figure~\ref{fig0}. Despite being originally designed for Natural Language Processing (NLP) tasks, the sequentiality and hierarchical structure of human skeleton sequences, as well as the flexibility of Transformer self-attention \cite{attention} in modeling long-range dependencies, make this model a perfect solution to tackle ST-GCN weaknesses. Recently, \cite{DBLP:journals/corr/abs-1904-09925} employed self-attention to overcome the locality of the convolution operator by capturing the global context of pixels in an image. In our work, we aim to apply the same mechanism to spatial-temporal skeleton-based architectures, and in particular to joints representing the human skeleton with the goal of modeling long-range interactions within human actions both in space, through a Spatial Self-Attention module (SSA), and time, through a Temporal Self-Attention module (TSA) module.
\cite{san} also proposed a Self-Attention Network (SAN) to extract long-term semantic information; however, since it focuses on temporally segmented clips, it solves the locality limitations of convolution only partially. 

Main contributions of this paper are summarized as follows: 
\begin{itemize}
    \item We propose a novel two-stream Transformer-based model for skeleton activity recognition tasks, employing \textit{self-attention} on both the spatial and the temporal dimensions
    \item We design a \textit{Spatial Self-Attention} (SSA) module to dynamically build links between skeleton joints, representing the relationships between human body parts, conditionally on the action and independently from the natural human body structure. On the temporal dimension, we introduce a  \textit{Temporal Self-Attention} (TSA) module to study the dynamics of a joint along time. We made both layers publicly available for experiments replication and further use \footnote{Code at \color{magenta}\url{https://github.com/Chiaraplizz/ST-TR}}
\item Our model outperforms ST-GCN \cite{yan2018spatial} and A-GCN \cite{Shi2018TwoStreamAG} baselines on all datasets and outperforms previous state-of-the-art methods using the same input data on NTU.
\end{itemize}

\section{Related Works}

\subsection{{Skeleton-based Action Recognition}}{Most of the early studies in skeleton-based action recognition relied on handcrafted features \cite{jointly,points,locations} exploiting relative 3D rotations and translations between joints. Deep learning revolutionized activity recognition by proposing methods capable of increased robustness \cite{a-comparative} and able to achieve unprecedented performance. Methods that fall into this category rely on different aspects of skeleton data: (1) Recurrent neural network (RNN) based methods \cite{Rnn,modeling,global-context,hrnn} leverage on the sequentiality of joint coordinates, treating input skeleton data as time series. (2) Convolutional neural network (CNN) based methods \cite{p-cnn,2s-cnn,investigation,liu2017enhanced,bo} leverage spatial information, in a complementary way to RNN-based ones. Indeed, 3D skeleton sequences are mapped into a pseudo-image, representing temporal dynamics and skeleton joints respectively in rows and columns. (3) Graph neural network (GNN) based methods \cite{yan2018spatial,Shi2018TwoStreamAG,dirgraph}, make use of both spatial and temporal data by exploiting information contained in the natural topological graph structure of the human skeleton. These latter methods have demonstrated to be the most expressive among the three, and among these, the first model capturing the balance between spatial and temporal dependencies has been the Spatio-Temporal Graph Convolutional Network (ST-GCN) \cite{yan2018spatial}. In ST-GCN the human skeleton is represented as a graph where joints are encoded as nodes and bones as arcs, while time is modeled with additional edges linking together the same joint along time. In this work, we used ST-GCN as the baseline model; its functioning is presented in details in Section~\ref{st-gcn}. }


\subsection{Graph Neural Networks} 
\textit{Geometric deep learning} \cite{DBLP:journals/corr/BronsteinBLSV16} refers to all emerging techniques attempting to generalize deep learning models to non-Euclidean domains such as graphs. The notion of Graph Neural Network (GNN) was initially outlined by~\cite{Gori2005ANM} and further elaborated by~\cite{Scarselli2009TheGN}. The intuitive idea underlying GNNs is that nodes in a graph represent objects or concepts while edges represent their relationships. Due to the success of Convolutional Neural Networks, the concept of convolution has later been generalized from grid to graph data. GNNs iteratively process the graph, each time representing nodes as the result of applying a transformation to nodes' and their neighbors' features. The first formulation of CNNs on graphs is due to~\cite{ae482107de73461787258f805cf8f4ed}, who generalized convolution to signals using a~\textit{spectral} construction. This approach had computational drawbacks that have been subsequently addressed by~\cite{DBLP:journals/corr/HenaffBL15} and~\cite{DBLP:journals/corr/DefferrardBV16}. The latter has been further simpliÔ¨Åed and extended by \cite{Kipf:2016tc}. A complementary approach is the \textit{spatial} one, where graph convolution is defined as information aggregation \cite{article7,DBLP:journals/corr/NiepertAK16,DBLP:journals/corr/SuchSDPZMCP17}. In this work we make use of the spectral construction proposed by \cite{Kipf:2016tc}, whose formulation is provided in Section~\ref{st-gcn}.
\begin{figure*}[t]
\begin{center}
    \subfloat[Spatial Self-Attention \label{2a}]{
    \begin{minipage}[b]{.41\linewidth}
\includegraphics[width=\textwidth]{figures/ssa_fig.pdf}
\end{minipage}}
\hspace{1cm}
\subfloat[Temporal Self-Attention \label{2b}]{\begin{minipage}[b]{.41\linewidth}
\includegraphics[width=\textwidth]{figures/tsa_fig.pdf}
\end{minipage}}
\end{center}
\caption{{Spatial Self-Attention (SSA)} and {Temporal Self-Attention (TSA)}. Self-attention operates on each pair of nodes, by computing a weight for each of them which represents the strength of their correlation. Those weights are then used to score the contribution of each body joint , proportionally to how relevant the node is w.r.t. to all the others. Please notice that on SSA (a), the procedure is illustrated only of a group of five nodes for simplicity, while in practice it operates on all the nodes.}
\label{SSA_TSA}
\end{figure*}

\subsection{Transformer}\label{transformer}
The Transformer is the leading neural model for Natural Language Processing (NLP), proposed by \cite{attention} as an alternative to recurrent networks. It has been designed to face two key problems: (i) the processing of very long sequences, which are often intractable both for LSTMs and RNNs, and (ii) the limitations in parallelizing sentence processing, which is usually performed sequentially, word by word, in standard RNNs architectures. The Transformer follows a usual encoder-decoder structure, but it relies solely on \textit{multi-head self-attention} \cite{attention}. Recently, self-attention mechanisms have been also applied to visual tasks by~\cite{DBLP:journals/corr/abs-1904-09925}, with the purpose of augmenting standard convolution. Our work is currently one of the few in literature applying self-attention on graphs \cite{li2019graph}.


\section{Background}
In this section, Spatial-Temporal Graph Convolutional Networks (ST-GCN) by \cite{yan2018spatial} and the original Transformer self-attention mechanism by \cite{attention} are summarized, being the basic blocks of the model we propose in this paper.

\subsection{Skeleton Sequences Representation}

Given a sequence of skeletons, we define  as the number of joints representing each skeleton and  as the total number of skeletons composing the sequence, also named frames in the following.
In order to represent the sequence, a spatial temporal graph is built, i.e., , where  represents the set of all the nodes  of the graph, i.e., the body joints of the skeleton along all the time sequence, and  represents the set of all the connections between nodes.  consists of two subsets; the first subset  is composed by the intra-skeleton connections at each time interval , for any pair of joints  connected by a bone in the human skeleton. The subset  of intra-skeleton connections is commonly further divided into  disjoint partitions, based on some criterion \cite{yan2018spatial} (e.g., distance from the center of gravity), and encoded using a set of adjacency matrices . The second subset  consists of all the inter-frame connections between joints along consecutive time frames. The result is a graph extending on both the spatial and the temporal dimension.


\subsection{Spatial Temporal Graph Convolutional Networks}\label{st-gcn}

Spatial Temporal Graph Convolutional Networks (ST-GCN) have been introduced by~\cite{yan2018spatial}. A ST-GCN is structured as a hierarchy of stacked spatial-temporal blocks, which are internally composed of a spatial convolution (GCN) followed by a temporal convolution (TCN). 

The spatial sub-module uses the Graph Convolution formulation proposed by~\cite{Kipf:2016tc}, which can be summarized as it follows:

where  is the kernel size on the spatial dimension,  is the adjacency matrix of the undirected graph representing intra-body connections, \textbf{I} is the identity matrix and  is a trainable weight matrix. The temporal convolution sub-module (TCN) is implemented as a  2D convolution operating on  dimensions of the  input volume, where  is the number of frames considered within the kernel receptive field. 

As shown in Equation \ref{eq1}, the graph structure is predefined, being the adjacency matrix fixed. In order to make it \textit{adapative}, \cite{Shi2018TwoStreamAG} introduced the \textit{Adaptive Graph Convolutional Network} (A-GCN), where the GCN formulation in Equation \ref{eq1} is replaced by the following:

where  is the same as the one in Equation \ref{eq1},  is learned during training, and  determines whether two vertices are connected or not through a similarity function. 

\subsection{Transformer Self-Attention}
The original Transformer model of~\cite{attention} employs \textit{self-attention}, i.e., a \textit{non-local operator} originally designed to operate on words in NLP tasks with the goal of enriching the embedding of each word based on the surrounding context. In the Transformer, new word embeddings are computed by comparing pairs of words an then mixing their embeddings together based on how much a word is relevant w.r.t. the others. By gathering clues from the surrounding context, self-attention enables to extract a better meaning from each word, dynamically building relations within and between phrases.

In particular, for each word embedding , a query , a key  and a value vector  are computed through trainable linear transformations starting from each the word embeddings, independently. Then, a score for each word embedding is obtained by taking the dot product  , where  is the total number of nodes being considered. This score represents how much the word  is relevant for word . To compute the final embedding for word , a weighted sum is computed by first multiplying the value vector of each other word  by the corresponding score , scaled through the softmax function, and than summing these vectors together. 

This process, also called \textit{scaled dot-product attention}, can be written in matrix form as it follows:



where , , and  are matrices containing the predicted query, key and value vectors, respectively, packed together and  is the channel dimension of the key vectors. The division by  is performed in order to increase gradients stability during training. In order to obtain better performance, a mechanism called \textit{multi-headed attention} is usually applied, which consists in applying attention, i.e., a head, multiple times with different learnable parameters and then finally combining the results.





\section{Spatial Temporal Transformer Network}

We propose the \textit{Spatial Temporal Transformer (ST-TR)} network, an architecture which uses Transformer self-attention to operate on both space and time. We propose to achieve this goal using two modules, the \textit{Spatial Self-Attention (SSA)} and the \textit{Temporal Self-Attention (TSA)} modules, each one focusing on extracting correlations on one of the two dimensions. 



\subsection{Motivation}
The idea behind the original Transformer self-attention is to allow the encoding of both short- and long-range correlations between words in the sentence. Our intuition is that the same approach can be applied to skeleton-based action recognition as well, as correlations between nodes are crucial both on the spatial and on the temporal dimension. 
We consider the joints comprising the skeleton as a bag-of-words and make use of the Transformer self-attention to extract node embeddings encoding the relation between surrounding joints, just like words in a phrase in NLP. Contrary to a standard graph convolution, where only the adjacent nodes are compared, we discard any predefined skeleton structure and instead let the Transformer self-attention automatically discover joint relations which are relevant for predicting the current action. The resulting operation acts similarly to a graph convolution, but in which the kernel values are \textit{dynamically predicted} based on the discovered joint relations. The same idea is also applied at the sequence level, by analyzing how each joint changes during the action and building \textit{long-range relations} that span different frames, similarly to how relations between phrases are built in NLP. The resulting operator is capable of obtaining a dynamical representation extending both on the spatial and the temporal dimension. 



 
 

\subsection{Spatial Self-Attention (SSA)}
\label{sec:ssa-descr}

The Spatial Self-Attention module applies self-attention \textit{inside each frame} to extract low-level features embedding the relations between body parts. This is achieved by computing correlations between each pair of joints in every single frame independently, as depicted in Figure~\ref{2a}. 
Given the frame at time , for each node  of the skeleton, a \textit{query} vector  , a \textit{key} vector  and a \textit{value} vector  are first computed by applying trainable linear transformations to the node features  with parameters , , , shared across all nodes. Then, for each pair of body nodes , a \textit{query-key dot product} is applied to obtain a weight  representing the strength of the correlations between the two nodes. The resulting score  is used to weight each joint value , and a weighted sum is computed to obtain a new embedding  for node , as in the following:

where  (with  the number of output channels) constitutes the new embedding of node .


Multi-head attention is applied by repeating this embedding extraction process  times, each time with a different set of learnable parameters. The set  of node embeddings thus obtained, all referring to the same node , is then combined with a learnable transformation, i.e., , and constitutes the output features of SSA.

As shown in Figure~\ref{2a}, the relations between nodes (i.e., the  scores) are dynamically \textit{predicted} in SSA; the correlation structure in the skeleton is then not fixed for all the actions, but it changes adaptively for each sample. SSA operates similar to a graph convolution on a fully connected graph where, however, the kernel values (i.e., the  scores) are predicted dynamically based on the skeleton pose.


\subsection{Temporal Self-Attention (TSA)}\label{sec:st}

With the Temporal Self-Attention (TSA) module, the dynamics of each joint is studied separately \textit{along all the frames}, i.e., each single joint is considered as independent and correlations between frames are computed by comparing the change in the embeddings of the same body joint along the temporal dimension (see Figure~\ref{2b}). The formulation is symmetrical to the one reported in Equation \eqref{eq5} for SSA:

where  indicate the same joint  in two different instants ,  is the correlation score,  is the query associated to ,  and  are the key and value associated to joint  (all computed using trainable linear transformations as in SSA), and  is the resulting node embedding. Note that  the notation used in this section is opposite w.r.t. the one used in Section~\ref{sec:ssa-descr}; subscripts indicate time while superscripts indicate the joint. Multi-head attention is applied in TSA as in SSA. An example of TSA is depicted in Figure~\ref{2b}.

The TSA module, by extracting inter-frame relations between nodes in time, can learn how to correlate frames apart from each other (e.g., nodes in the first frame with those in the last one), capturing discriminant features that are not otherwise possible to capture with a standard ST-GCN convolution, being this limited by the kernel size.
\begin{figure}
    \begin{center}
    \includegraphics[width=0.48\textwidth]{figures/architecture.pdf}
    \end{center}
    \caption{Illustration of two 2s-ST-TR architecture. On each stream, the first three layers extract low level features. On the S-TR stream, at each layer SSA is used to extract spatial information, followed by a 2D convolution on time dimension (TCN), while on the T-TR stream, at each layer, TSA is used to extract temporal information, while spatial features are extracted by a standard graph convolution (GCN) \cite{yan2018spatial}.}
    \label{architecture}
\end{figure}

\subsection{Two-Stream Spatial Temporal Transformer Network}\label{2s}

To combine the SSA and TSA modules, a two-stream architecture named 2s-ST-TR is used, as similarly proposed by~\cite{Shi2018TwoStreamAG} and~\cite{dirgraph}. In our formulation, the two streams differentiate on the way the proposed self-attention mechanisms are applied: SSA operates on the spatial stream (named S-TR), while TSA on the temporal one (named T-TR). On both streams, node features are first extracted by a three-layers residual network, where each layer processes the input on the spatial dimension through graph convolution (GCN), and on the temporal dimension through a standard 2D convolution (TCN), as done by~\cite{yan2018spatial}\footnote{In principle other features, e.g., visual features, could be added here but we want in this paper to focus on pure skeleton base action recognition and we leave this option for future investigations.}.
SSA and TSA are then applied on the S-TR and on the T-TR stream in the subsequent layers in substitution to the GCN and TCN feature extraction modules respectively (Figure~\ref{architecture}). The sub-networks outputs are eventually fused together by summing up their softmax output scores to obtain the final prediction, as proposed by \cite{Shi2018TwoStreamAG} and \cite{dirgraph}. 

\begin{figure}
    \begin{center}
    \includegraphics[width=0.38\textwidth]{figures/implementation.pdf}
    \end{center}
    \caption{Illustration of a SSA module (the implementation of TSA is the same, with the only difference that the dimension  corresponds to  and viceversa). The input  is reshaped by moving  in the batch dimension, such that self-attention operates on each time frame separately. SSA is implemented as a matrix multiplication, where  and  are the query, key and value matrix respectively, and  denotes the matrix multiplication. }
    \label{implementation}
\end{figure}


\paragraph*{\textbf{Spatial Transformer Stream (S-TR)}} In the spatial stream, self-attention is applied at the skeleton level through a SSA module, which focuses on spatial relations between joints. The output of the SSA module is passed to a 2D convolutional module with kernel  on the temporal dimension (TCN), as done by \cite{yan2018spatial}, in order to extract temporally relevant features, as shown in Figure \ref{architecture} and expressed in the following: 

Following the original Transformer structure, the input is pre-normalized passing through a Batch Normalization layer~\cite{ioffe2015batch, bn_tr}, and skip connections are used to sum the input to the output of the SSA module (see Figure \ref{implementation}).


\paragraph*{\textbf{Temporal Transformer Stream (T-TR)}} 
The temporal stream, instead, focuses on discovering inter-frame temporal relations. Similarly to the S-TR stream, inside each T-TR layer, a standard graph convolution sub-module \cite{yan2018spatial} is followed by the proposed Temporal Self-Attention module:

TSA operates on graphs linking the same joint along all the time dimension (e.g., all left feet, or all right hands).
















\subsection{Implementation of SSA and TSA} \label{sec:implementation}

The matrix implementation of SSA (and of TSA) is based on the implementation of Transformer on pixels by~\cite{DBLP:journals/corr/abs-1904-09925}. As shown in Figure~\ref{implementation}, given an input tensor of shape , where  is the number of input features,  is the number of frames and  is the number of nodes, a matrix  is obtained by rearranging the input. Here the  dimension is moved inside the batch dimension, effectively implementing parameter sharing along the temporal dimension and applying the transformation separately on each frame:

where the product with ,  and  gives rise respectively to ,  and , being  the number of heads, and  a learnable linear transformation combining the heads outputs. 
The output of the Spatial Transformer is then rearranged back into . The TSA matrix implementation has the same expression as Equation \eqref{tr_con}, differing only in the way the input  is processed. Indeed, in order to be processed by each TSA module, the input is reshaped into a matrix , where the  dimension has been moved in the first position and aggregated to the batch dimension, not reported here explicitly, in order to operate separately on each joint along the time dimension. 
The formulation is analogous to Equation \eqref{tr_con}, differing only in the shape of matrices, which become ,  and .



\section{Model Evaluation}
To understand the impact of both the Spatial and Temporal Transformer streams, we analyze their performance separately and in different configurations through extensive experiments on NTU-RGB+D 60 \cite{ntu} (see Table \ref{table:1a}-\ref{table:4}). Then, for a comparison with the state-of-the-art, we test the resulting best configurations on the Kinetics dataset \cite{Kin}, which are used by most of previous works, and on the NTU-RGB+D 120 dataset \cite{ntu120}, which represents to date one of the most complex skeleton-based action recognition benchmarks (see Table \ref{table:3}-\ref{table:kinetics}).


\subsection{Datasets}

\paragraph*{\textbf{\textit{NTU RGB+D 60 and NTU RGB+D 120}}}{The NTU RGB+D 60 (NTU-60) dataset is a large-scale benchmark for 3D human action recognition collected using Microsoft Kinect v2 by \cite{ntu}. It contains RGB videos, depth sequences, skeleton data, and infrared frames collected in  RGB+D video samples. Skeleton information consists of 3D coordinates of  body joints and a total of  different action classes. The NTU-60 dataset follows two different criteria for evaluation. The first one, called \textit{Cross-View Evaluation} (X-View), uses  training and  test samples, split according to the camera views from which the action is taken. The second one, called \textit{Cross-Subject Evaluation} (X-Sub), is composed instead of  training and  test samples. Data collection has been performed with  different subjects performing actions and divided into two groups, one for training and the other for testing. NTU RGB+D 120 \cite{ntu120} (NTU-120) is an extension of NTU-60, which adds  new skeleton sequences representing  new actions, for a total of  videos referring to  classes from  subjects under  camera setups. In order to perform the evaluation, the extended dataset follows two criteria: the first one is the \textit{Cross-Subject Evaluation (X-Sub)}, the same used for NTU-60, while the second one is called \textit{Cross-Setup Evaluation (X-Set)}, which substitutes Cross-View by splitting training and testing samples based on the parity of the camera setup IDs.  } 

\paragraph*{\textbf{\textit{Kinetics}}} The Kinetics skeleton dataset \cite{yan2018spatial} is obtained by extracting skeleton annotations from videos composing the Kinetics  dataset \cite{Kin}, by using the OpenPose toolbox \cite{openpose}. It consists of  training and  testing samples, representing a total of  action classes. Each skeleton is composed by 18 joints, each one provided with the 2D coordinates and a confidence score. For each frame, a maximum of 2 people are selected based on the highest confidence scores. To compare our methods with the literature, Top-1 and Top-5 accuracy are reported.


\subsection{Model Complexity}\label{sec:complexity}
Before studying the accuracy benefits of the proposed ST-TR, we perform an analysis on the complexity of the different self-attention modules we designed, and compare them to ST-GCN modules \cite{yan2018spatial}, based on standard convolution, and to 1s-AGCN \cite{Shi2018TwoStreamAG} modules, based on adaptive graph convolution. 
First, we compare, singularly, a layer of standard convolution with our transformer mechanism, setting  channels. The number of parameters of each configuration is shown in Figure~\ref{fig:ssa_vs_gc_params} as a function of the channel dimension.
The number of parameters introduced by self-attention is expressed mathematically in Equation~\ref{eq1_sm}:

where  and  in our case. This is the result of a  standard  convolution on  input channels and  output channels to calculate query, key, and value, and a   convolution combining the output of each head with  input and output channels. This results in the same number of parameters for both TSA and SSA, since the convolutions performed internally are the same (having fixed the same kernel dimensions) and both the query-key dot product and the logit-value product are parameter free.


\begin{figure}[t]

\begin{center}
    \begin{minipage}{.94\linewidth}

    \subfloat{
    \resizebox {\linewidth} {!} {
        \begin{tikzpicture}

\begin{axis}[
          enlargelimits=false,
          ylabel={params},
xmin=40, xmax=512,
           ymin=0, ymax=2500000,
          xtick={64,128, 256, 512},
legend pos=north west,
          ymajorgrids=true,
          grid style=dashed,
          width=10cm,
          height=5cm
          ]
        
      \addplot[color=MaterialBlue600,
      mark=diamond*]
    table[x index=0,y index=1,col sep=comma]
    {plots/linea4.txt};
    \addlegendentry{TC} 
    
        \addplot[
          color=MaterialBlue300,
          mark=diamond*]
        table[x index=0,y index=1,col sep=comma]
        {plots/linea1.txt};
        \addlegendentry{GC} 

     \addplot[
          color=MaterialGreen300,
          mark=diamond*]
        table[x index=0,y index=1,col sep=comma]
        {plots/agcn.txt};
        \addlegendentry{AGC} 
        
    
    \addplot[
      thick,
      color=MaterialOrange600,
      mark=oplus*]
    table[x index=0,y index=1,col sep=comma]
    {plots/linea3.txt};
    \addlegendentry{SSA, TSA} 
        
        
        
\end{axis}
        
        
        \end{tikzpicture}}
    \label{fig:ssa_vs_gc_params}
}
\end{minipage}
\begin{minipage}[t]{.05\linewidth}{\small (a)}\end{minipage}
    
\hfill
\begin{minipage}{.94\linewidth}
\subfloat{
\resizebox {\columnwidth} {!} {
    \begin{tikzpicture}
\begin{axis}[
      enlargelimits=false,
      ylabel={params},
      xlabel={channels},
       xmin=40, xmax=512,
       ymin=0, ymax=3750000,
      xtick={64,128, 256, 512},
legend pos=north west,
      ymajorgrids=true,
      grid style=dashed,
      width=10cm,
      height=5.5cm
    ]

 \addplot[
      color=MaterialGreen300,
      mark=diamond*]
    table[x index=0,y index=1,col sep=comma]
    {plots//agcn+conv.txt};
        \addlegendentry{1s-AGCN} 
    
    \addplot[
      color=MaterialBlue300,
      mark=diamond*]
    table[x index=0,y index=1,col sep=comma]
    {plots//conv+conv.txt};
    \addlegendentry{ST-GCN} 
    
    \addplot[
    thick,
      color=MaterialOrange300,
      mark=oplus*]
    table[x index=0,y index=1,col sep=comma]
    {plots//linea6.txt};
        \addlegendentry{S-TR} 

    \addplot[
    thick,
      color=MaterialOrange600,
      mark=oplus*]
    table[x index=0,y index=1,col sep=comma]
    {plots//linea7.txt};
        \addlegendentry{T-TR} 
        
   
    \end{axis}
    \end{tikzpicture}}
  \label{fig:nets_params}
  }
\end{minipage}
\begin{minipage}[t]{.05\linewidth}{\small (b)}\end{minipage}
\end{center}

\caption{(a) Difference in terms of parameters between Graph Convolution (GC), Adaptive Convolution (AGC) Spatial Self-Attention (SSA) modules of  channels, and between Temporal Convolution (TC) and Temporal Self-Attention (TSA) modules; (b) comparison in terms of parameters between ST-GCN, 1s-AGCN and our novel S-TR and T-TR. Best viewed in colors.}
\label{fig:ssa_tsa_params}

\end{figure}

From Figure~\ref{fig:ssa_vs_gc_params} it can be seen that Spatial Self-Attention introduces less parameters than Graph Convolution, especially when dealing with a large number of channels, where the maximum , i.e., the decrease in terms of parameters, is . When dealing with Adaptive Graph Convolution (AGC), an additional number of parameters has to be considered, resulting in a difference with respect to SSA of . 
On the temporal dimension  reaches a value of . Temporal convolution in~\cite{yan2018spatial} is implemented as a  convolution with filter , where  is the number of frames considered along the time dimension, and it is usually set to , striding along  frames. Thus, substituting it with a self-attention mechanism results in a great complexity reduction, in addition to better performance, as reported in the next sections. 




Finally, in Figure \ref{fig:nets_params} we also compare the entire stream architectures, i.e., ST-GCN \cite{yan2018spatial} and 1s-AGCN \cite{Shi2018TwoStreamAG} with the proposed S-TR and T-TR streams in terms of parameters.
As expected from the considerations above, the biggest improvement in parameters reduction is achieved by substituting temporal convolution with Temporal Self-Attention, i.e., in T-TR, with a . On the spatial dimension the difference in terms of parameters is not as pronounced as in temporal dimension, but it is still significant, with a  and . 

\setlength{\tabcolsep}{1pt}


\begin{table}[t]
\setlength{\tabcolsep}{1.5pt}

    


    \begin{center}
    \begin{tabular}{lcccc}
    \hline\noalign{\smallskip}
    \textbf{Method} & GCN & TCN &Params  & Top-1\\
          &  &   &  & \\

    \hline
    ST-GCN  & \checkmark & \checkmark & 31.0 & 92.7 \\
    ST-GCN-fc  &  & \checkmark & 26.5 & 93.7 \\
    1s-AGCN  &  & \checkmark &34.7& 93.7\\
    1s-AGCN w/o A  &  & \checkmark & 33.1 & 93.4\\
    \hline
    S-TR & SSA & \checkmark & 30.7 & \textbf{94.0} \\
    T-TR& \checkmark & TSA &  17.6 & 93.6 \\

    \hline
    \end{tabular}
    \end{center}
 
\caption{Comparison between the baseline and our self-attention modules in terms of both performance (accuracy (\%)) and efficiency (number of parameters) on NTU-60 (X-View)}
       \label{table:1a}





\end{table}

\subsection{Experimental Settings} Using PyTorch \cite{paszke2019pytorch} framework, we trained our models for a total of  epochs with batch size  and SGD as optimizer on NTU-60 and NTU-120, while on Kinetics we trained our models for a total of  epochs, with batch size . The learning rate is set to  at the beginning and then reduced by a factor of  at the epochs \{, \} and \{, \} for NTU and Kinetics respectively. These schedulings have been selected as they have been shown to provide good results on ST-GCN networks used by~\cite{dirgraph}. Moreover, we preprocessed the data with the same procedure used by~\cite{Shi2018TwoStreamAG} and \cite{dirgraph}. In order to avoid overfitting, we also used \textit{DropAttention}, a particular dropout technique introduced by~\cite{Lin2019DropAttentionAR} for regularizing attention weights in Transformer netorks, that consists in randomly dropping columns of the attention logits matrix. In all of these experiments, the \textit{number of heads} for multi-head attention is set to , and  embedding dimensions to  in each layer, as done in \cite{DBLP:journals/corr/abs-1904-09925}. We did not perform grid search on these parameters. As far as it concerns the model architecture, each stream is composed by 9 layers, of channel dimension  and . Batch normalization is applied to input coordinates, and a global average pooling layer is applied before the \textit{softmax} classifier and each stream is trained using the standard cross-entropy loss.








\subsection{{Results}}

To verify in a fair way the effectiveness of our SSA and TSA modules, we compare the S-TR and T-TR streams individually against the ST-GCN \cite{yan2018spatial} baseline (whose results are reported using our learning rate scheduling) and other models that modify its basic GCN module (see Table~\ref{table:1a}): (i) \textit{ST-GCN (fc)}: we implemented a version of ST-GCN whose adjacency matrix is composed of all ones (referred as ), to simulate the fully-connected skeleton structure underlying our SSA module and verify the superiority of self-attention over graph convolution on the spatial dimension; (ii) \textit{1s-AGCN}: Adaptive Graph Convolutional Network (AGCN) \cite{Shi2018TwoStreamAG} (see Section \ref{st-gcn}), as it demonstrated in the literature to be more robust than standard ST-GCN, in order to remark the robustness of our SSA module over more recent methods; (iii) \textit{1s-AGCN w/o A}: 1s-AGCN without the static adjacency matrix, to verify the effectiveness of our SSA over graph convolution in a similar setting where all the links between joints are exclusively learnt. 
All these methods use the same implementation of convolution on the temporal dimension (TCN). We make a comparison both in terms of model accuracy and number of parameters.

\setlength{\tabcolsep}{0.5pt}
\begin{table}[t]
\begin{center}

 \begin{minipage}{0.4\linewidth}
        \subfloat[]{
\begin{tabular}{lccc}

\hline\noalign{\smallskip}
\textbf{Method} & Bones & X-Sub & X-View\\\noalign{\smallskip}
\hline
\noalign{\smallskip}

S-TR &  &86.4 & 94.0 \\
T-TR  &   &86.0 & 93.6 \\
\hline
ST-TR  & &88.7 & \textbf{95.6} \\
\hline 

S-TR   & \checkmark &87.9 & 94.9 \\
T-TR  &\checkmark &87.3 & 94.1\\
T-TR-agcn & \checkmark &86.1 &94.3\\
\hline
ST-TR  & \checkmark& 89.9 & \textbf{96.1}\\
ST-TR-agcn & \checkmark &89.3& \textbf{96.1}\\

\hline
\label{table:1-a}

\end{tabular}

}

\end{minipage}
\hfill
\hspace{-10pt}
 \begin{minipage}{0.42\linewidth}
        \subfloat[]{
\begin{tabular}{lc}
\hline\noalign{\smallskip}
\textbf{Method}  & X-View\\\noalign{\smallskip}
\hline
S-TR-all-layers  &93.3 \\
            T-TR-all-layers & 91.3 \\
            \hline
            ST-TR-all-layers & 95.0\\
            \hline 
            S-TR-augmented   & 94.5 \\
            T-TR-augmented  &  90.2 \\
            \hline
            ST-TR-augmented & 94.9 \\
            \hline ST-TR-1s & 93.3 \\
\hline
\noalign{\smallskip}

\label{table:1-b}

\end{tabular}

}

\end{minipage}
\end{center}
\caption{a) Comparison of S-TR and T-TR streams, and the combination of the two (ST-TR) on NTU-60, w and w/o bones. b) Ablations of different model configurations}
\label{table:1}

\end{table}

\setlength{\tabcolsep}{1.4pt}

Regarding SSA, the performance of S-TR is superior to all methods mentioned above, demonstrating that self-attention can be used in place of graph convolution, increasing the network performance while also decreasing the number of parameters. 
In fact, as it can be seen from Table~\ref{table:1a}, S-TR introduces  parameters less then ST-GCN and  less than 1s-AGCN, with a performance increment w.r.t. all GCN configurations. Similarly, regarding TSA, what emerges from the comparison between T-TR and the ST-GCN baseline adopting standard convolution, is that by using self-attention on the temporal dimension the model is significantly lighter ( less parameters), and achieves an increment of accuracy of . 


In Table \ref{table:1} we first analyze the performance of the S-TR stream, T-TR stream and their combination by using input data consisting of joint information only. As it can be seen from Table \ref{table:1-a}, on NTU-60 the S-TR stream achieves slightly better performance (+0.4\%) than the T-TR stream, on both X-View and X-Sub. This can be motivated by the fact that SSA in S-TR operates on 25 joints only, while on temporal dimension the number of correlations is proportional to the huge number of frames. Again, as shown in Table \ref{table:1a}, applying self-attention instead of convolution clearly benefits the model on both spatial and temporal dimensions. The combination of the two streams achieves 88.7\% of accuracy on X-Sub and 95.6\% of accuracy on X-View,  outperforming the baseline ST-GCN by up to  and surpassing other two-stream architectures (see Table \ref{table:3}).


As adding bones information demonstrated to lead to better results in previous works \cite{dirgraph,2s-cnn}, we also studied our Transformer modules on combined joint and bones information. For each node  and , the bone connecting the two is calculated as 
    .
Both joint and bone information are concatenated along the channel dimension, and then fed to the network. At each layer, the dimension of the input and output channels are doubled as done by \cite{dirgraph} and \cite{2s-cnn}. Results are shown again in Table~\ref{table:1-a}, where all previous configurations improve when bones information is added as input. This highlights the flexibility of our method, which is capable of adapting to different input types and network configurations. 

To further test its flexibility, we also perform additional experiments in which the GCN module is substituted by the AGCN adaptive module on the temporal stream. As it can be seen from Table \ref{table:1-a}, these configurations (\textit{T-TR-agcn}) achieve better results than the one using standard GCN (T-TR-agcn: , T-TR: ) on X-View.














\subsection{Effect of Applying Self-Attention since Feature Extraction}
We designed our streams to operate starting from high-level features, rather than directly from coordinates, extracted using a sequence of residual GCN and TCN modules as reported in Section \ref{2s}. This set of experiments validates our design choice. In these experiments SSA (TSA) substitutes GCN (TCN) on the S-TR (T-TR) stream, from the very first layer. The configurations reported in Table~\ref{table:1-b} (named \textit{S-TR-all-layers}), perform worse than the corresponding ones in Table \ref{table:1-a}, while still outperforming the baseline ST-GCN \cite{dirgraph} by 2.3\% (see Table \ref{table:4}). Notice that on T-TR, in order to deal with the great number of frames in the very first layers (), we divided frames into blocks within which SSA is applied, and then gradually reduce the number of blocks going deeper in the architecture ( where ,  where , and a single block of  on layers  with ).




\begin{table}
    \begin{center}
    \begin{tabular}{lccc}
    \hline\noalign{\smallskip}
\multicolumn{4}{c}{\textbf{NTU-60}}\\
\cline{1-4}\noalign{\smallskip}
\textbf{Method} & \textbf{Bones} & X-Sub & X-View\\    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
STA-LSTM \cite{sta-lstm}  & &73.4 & 81.2 \\
    VA-LSTM \cite{va-lstm}  & &79.4 & 87.6 \\
    AGC-LSTM \cite{att-aug} & &89.2 & 95.0 \\
ST-GCN \cite{yan2018spatial}  & & 81.5 & 88.3\\
     1s-AGCN \cite{Shi2018TwoStreamAG} & & 86.0 & 93.7 \\
    1s Shift-GCN \cite{shift}  & &87.8 & 95.1 \\
        SAN \cite{san}  &&87.2 & 92.7 \\
\hline
    ST-TR (Ours) & &\textbf{88.7} & \textbf{95.6} \\
\hline 
        2s-AGCN \cite{Shi2018TwoStreamAG}  & \checkmark& 88.5 & 95.1 \\
    DGCNN \cite{dirgraph} &\checkmark &  89.9 & 96.1 \\
2s Shift-GCN \cite{shift}  & \checkmark &89.7 & 96.0\\

    MS-G3D \cite{disent} &\checkmark& \textbf{91.5} & \textbf{96.2} \\
\hline
    ST-TR (Ours) & \checkmark &{{89.9}} & {{96.1}}
    \\
    ST-TR-agcn (Ours) &\checkmark& 89.3&{96.1}\\

    \hline
    \end{tabular}
    \end{center}
    \caption{Comparison with state-of-the-art accuracy (\%) on~NTU60}
        \label{table:4}

\end{table}
\subsection{Effect of Augmenting Convolution with Self-Attention} 
Motivated by the results in \cite{DBLP:journals/corr/abs-1904-09925}, we studied the effect of applying the proposed Transformer mechanism as an augmentation procedure to the original ST-GCN modules. In this configuration,  features result from GCN (TCN) and they are concatenated to the remaining  features from SSA (TSA), a setup that
has proven to be effective in \cite{DBLP:journals/corr/abs-1904-09925}. To compensate the reduction of attention channels, \textit{wide attention} is used, i.e., half of the attention channels are assigned to each head, then recombined together while merging heads. The results are reported in Table \ref{table:1-b} (referred as \textit{ST-TR-augmented}). Graph convolution is the one that benefits the most from SSA attention (S-TR-augmented, 94.5\%), to be compared with S-TR's 94\% in Table \ref{table:1-a}. Nevertheless, the lower number of output features assigned to self-attention prevent temporal convolution improving on T-TR stream. 


\subsection{Effect of combining SSA and TSA in a single stream}
We tested the efficiency of the model when SSA and TSA are combined in a single stream architecture (see Table \ref{table:1-b}, referred as \textit{S-TR-1s}). In this configuration, feature extraction is still performed by the original GCN and TCN modules, while from the 4th layer on, each layer is composed by SSA followed by TSA, i.e., .

We also  tested this configuration on NTU-60, obtaining an accuracy of , slightly lower than the  accuracy obtained by the two-stream configuration (see Table~\ref{table:1a}, ST-TR). However, it should be noted that the S-TR-1s configuration presents  parameters, drastically reducing the complexity of the baseline ST-GCN which consists in  parameters. Nevertheless, it outperforms the ST-GCN baseline by  using half of the parameters.

\begin{table}[t!]
\setlength{\tabcolsep}{1pt}
    \begin{center}
    \begin{tabular}{lcc}
    \hline\noalign{\smallskip}
    \multicolumn{3}{c}{\textbf{NTU-120}}\\
    \cline{1-3}\noalign{\smallskip}
    {\textbf{Method}} & X-Sub & X-Set\\
    \noalign{\smallskip}
    \hline
    {ST-LSTM \cite{st-lstm}} & 55.7 & 57.9 \\
    {GCA-LSTM \cite{gca}} & 61.2 & 63.3 \\
    {RotClips+MTCNN \cite{lcr}} &62.2&61.8\\

    {Pose Evol. Map \cite{bpe}} & 64.6 & 66.9\\
    1s Shift-GCN \cite{shift} & 80.9 & 83.2 \\
    \hline
    \noalign{\smallskip}
    {S-TR} (Ours) & 78.6 & 80.7 \\
    {T-TR} (Ours) &   78.4 & 80.5 \\
    {T-TR-agcn} (Ours) & 80.3 & 81.8\\
    
    \hline
    {ST-TR} (Ours) & 81.9 & 84.1 \\
    ST-TR-agcn (Ours) &\textbf{82.7} &\textbf{84.7} \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Comparison with state-of-the-art accuracy (\%) of S-TR, T-TR, and their combination (ST-TR) on NTU-120}
    \label{table:3}

\end{table}


\section{Comparison with State-Of-The-Art Results}\label{comparison}
In addition to NTU-60, we compare our methods on NTU-120 and Kinetics, for a fair comparison, w.r.t. other methods making use of joint or joint+bones information on a one- or two-stream architecture, as we also did.
On NTU-120 (Table \ref{table:3}), the model based on joint information only, achieves an accuracy of  on X-Sub and  on X-Set, outperforming all state-of-the-art methods that use the same information. On Kinetics and NTU-60, we test our model by using also bones information. On Kinetics (Table \ref{table:kinetics}), our model using only joints outperforms the ST-GCN baseline by . When using bones information, the best configuration that uses AGCN instead of GCN (ST-TR-agcn) as a backbone outperforms the baseline 2s-AGCN by , and DGCNN \cite{dirgraph} by . On NTU-60 (Table~\ref{table:4}), our configuration that uses joint information only, outperforms all the state-of-the-art models using the same type of information. In particular, it outperforms SAN \cite{san}, another method employing self-attention in skeleton-based action recognition, by up to , and the baseline 1s-AGCN by up to . Finally, when using bones information, both our configurations, the one based on the standard ST-GCN and the one making use of the AGCN backbone, outperform the 2s-AGCN baseline, with ST-TR performing the best and achieving an improvement up to .



\begin{table}[t!]
\setlength{\tabcolsep}{1pt}

    \begin{center}

    \begin{tabular}{lccc}
    \hline\noalign{\smallskip}
    \multicolumn{4}{c}{\textbf{Kinetics}}\\
    \cline{1-4}\noalign{\smallskip}
    \textbf{Method} & \textbf{Bones} & Top-1 & Top-5\\
    \noalign{\smallskip}
    \hline


\multicolumn{2}{l}{ST-GCN \cite{yan2018spatial}} & 30.7 & 52.8 \\
        \multicolumn{2}{l}{SAN \cite{san}} & 35.1 & 55.7\\
    \hline
    \noalign{\smallskip}
    S-TR (Ours)&  & 32.4 & 55.3\\
    T-TR  (Ours)& & 32.4 & 55.2 \\
    ST-TR (Ours)&
    & 34.5 & 57.6 \\
    \hline
    \noalign{\smallskip}
            \multicolumn{1}{l}{2s-AGCN \cite{Shi2018TwoStreamAG}} &\checkmark& 36.1 & 58.7 \\
        \multicolumn{1}{l}{DGCNN \cite{dirgraph}} & \checkmark&36.9 & 59.6 \\
        
        \multicolumn{1}{l}{MS-G3D \cite{disent}} & \checkmark&\textbf{38.0} & \textbf{60.9} \\ \hline
    S-TR (Ours)& \checkmark  & 35.4 & 57.9 \\
    T-TR (Ours) & \checkmark & 33.1 &  55.86 \\

    T-TR-agcn (Ours) & \checkmark&  33.7 & 55.1 \\
    \hline
        ST-TR (Ours) & \checkmark&  37.0 & 59.7 \\

    ST-TR-agcn (Ours) & \checkmark& {37.4} & {59.8}\\
    \hline
    \end{tabular}
    \end{center}
\caption{Comparison with state-of-the-art accuracy (\%) of S-TR, T-TR, and their combination (ST-TR) on Kinetics}
\label{table:kinetics}

\end{table}

\section{Conclusions}
In this paper we propose a novel approach that introduces Transformer self-attention in skeleton activity recognition as an alternative to graph convolution. Through extensive experiments on NTU-60, NTU-120 and Kinetics, we demonstrated that our Spatial Self-Attention module (SSA) can replace graph convolution, enabling more flexible and dynamic representations. Similarly, Temporal Self-Attention module (TSA) overcomes the strict locality of standard convolution, enabling the extraction of long-range dependencies between joints in the action. Moreover, our final Spatial-Temporal Transformer network (ST-TR) achieves state-of-the-art performance on NTU-RGB+D w.r.t. methods using same input information and stream setup, and competitive results on Kinetics with no major hyperparameter tuning.

As combining exclusively self-attention modules revealed to be suboptimal w.r.t. using them separately on two different streams, a possible future work is to search for a fully self-attentional solution, leading to a unified Transformer architecture able to replace graph convolutional networks in a variety of tasks. 







{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\end{document}
