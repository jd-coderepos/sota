\subsection{Dataset Collection}
\label{sec:apx1}
We use five few-shot classification datasets in all of our experiments: mini-ImageNet, CUB, Cars, Places, and Plantae.
We follow the setting in \citet{ravi2017metalstm} and \citet{hilliard2018few} to process mini-ImageNet and CUB datasets.
As for the other datasets, we manually process the dataset by random splitting the classes.
The number of training, validation, testing categories for each dataset are summarized in \tabref{datasets}.

\begin{table}[h]\tiny
\centering
\caption{\textbf{Summarization of the datasets~(domains)}. We additionally collect and split the Cars, Places, and Plantae datasets.}
\label{tab:datasets}
\begin{tabular}{l ccccc}
  \toprule
  Datasets & mini-ImageNet & CUB & Cars & Places & Plantae \\
  \midrule
  Source & \citet{deng2009imagenet} & \citet{WelinderEtal2010cub} & \citet{KrauseStarkDengFei-Fei_3DRR2013} & \citet{zhou2017places} & \citet{van2018inaturalist} \\ 
  \# Training categories & 64 & 100 & 98 & 183 & 100 \\
  \# Validation categories & 16 & 50 & 49 & 91 & 50 \\
  \# Testing categories & 20 & 50 & 49 & 91 & 50 \\
  Split setting & \citet{ravi2017metalstm} & \citet{hilliard2018few} & randomly split & randomly split & randomly split \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Additional implementation details}
\label{sec:apx2}
We use the implementation and adopt the setting of hyper-parameters from Chen~\etal~\citep{chen2019closerfewshot}.\footnote{\url{https://github.com/wyharveychen/CloserLookFewShot}}
We train the metric-based model and feature-wise transformation layers with a learning rate of $0.001$ and $40,000$ iterations.
For feature-wise transformation layers, we apply L2 regularization with a weight of $10^{-8}$.
The number of inner iterations adopted in the learning-to-learn scheme is set to be $1$.

\paragraph{Matching Network.}
We cannot utilize the MatchingNet implementation from Chen~\etal~\citep{chen2019closerfewshot} since they applied the Pytorch built-in LSTM module, which does not support second-order backpropagation.
Without the second-order backpropagation, we are unable to optimize the feature augmentation layers using the proposed learning-to-learn algorithm.
As a result, we re-implement the LSTM module for the MatchingNet model.
To verify the correctness of our implementation, we evaluate the 5-way 5-shot performance with the ResNet-10~\citep{he2016deep} backbone network on the mini-ImageNet dataset~\citep{ravi2017metalstm}.
Our implementation reports $68.88 \pm 0.69\%$ accuracy, which is similar to the result~(\ie $68.82 \pm 0.65\%$) reported by Chen~\etal~\citep{chen2019closerfewshot}.
We make the source code and datasets public available to foster future progress in this field.\footnote{\url{https://github.com/hytseng0509/CrossDomainFewShot}}

\subsection{Additional experimental results}

\paragraph{Ablation study on pre-trained metric encoder.}
As described in \secref{exp_1}, we pre-trained the metric encoder $E$ by minimizing the cross-entropy classification loss using the $64$ training categories from the mini-ImageNet dataset.
To understand the impact of the pre-training, we conduct an ablation study using the leave-one-out experiment illustrated in \secref{exp_3}.
As shown in \tabref{ablation_pretrain}, pre-training the metric encoder $E$ substantially improve the few-shot classification performance of metric-based frameworks.
Note that such a pre-training process is also adopted by several recent frameworks~\citep{rusu2018leo,gidaris2018dynamic,lifchitz2019dense} to boost the few-shot classification performance.
\begin{table}[t]\footnotesize
	\centering
	\caption{\textbf{Ablation study on pre-trained metric encoder.} We conduct leave-one-out setting to select the unseen domain to study the effectiveness of pre-training the feature encoder $E$ on the mini-ImageNet dataset.}
	\begin{tabular}{@{}l c cccc@{}} 
	    \toprule
	    1-Shot & Pre-trained & CUB & Cars & Places & Plantae \\
		\midrule
		MatchingNet & - & 
		$37.37 \pm 0.55\%$ & $\mathbf{30.60 \pm 0.51\%}$ & $41.42 \pm 0.59\%$ & $31.93 \pm 0.51\%$ \\
	    & \checkmark & 
		$37.90 \pm 0.55\%$ & $28.96 \pm 0.45\%$ & $\mathbf{49.01 \pm 0.65\%}$ & $\mathbf{33.21 \pm 0.51\%}$ \\
		\midrule
		RelationNet & - & 
		$38.46 \pm 0.56\%$ & $30.77 \pm 0.51\%$ & $37.49 \pm 0.58\%$ & $32.86 \pm 0.53\%$ \\
		& \checkmark & 
		$\mathbf{44.33 \pm 0.59\%}$ & $29.53 \pm 0.45\%$ & $\mathbf{47.76 \pm 0.63\%}$ & $\mathbf{33.76 \pm 0.52\%}$ \\
		\midrule
		GNN & - & 
		$37.21 \pm 0.63\%$ & $29.01 \pm 0.56\%$ & $36.06 \pm 0.62\%$ & $34.99 \pm 0.63\%$ \\
		& \checkmark & 
		$\mathbf{49.46 \pm 0.73\%}$ & $\mathbf{32.95 \pm 0.56\%}$ & $\mathbf{51.39 \pm 0.80\%}$ & $\mathbf{37.15 \pm 0.60\%}$ \\
	    \midrule
	    \midrule
		5-Shot & Pre-trained & CUB & Cars & Places & Plantae \\
		\midrule
		MatchingNet & - & 
		$49.83 \pm 0.55\%$ & $39.41 \pm 0.53\%$ & $59.18 \pm 0.60\%$ & $43.53 \pm 0.53\%$ \\
		& \checkmark & 
		$\mathbf{51.92 \pm 0.80\%}$ & $39.87 \pm 0.51\%$ & $\mathbf{61.82 \pm 0.57\%}$ & $\mathbf{47.29 \pm 0.51\%}$ \\
		\midrule
		RelationNet & - & 
		$55.85 \pm 0.55\%$ & $\mathbf{42.55 \pm 0.58\%}$ & $59.85 \pm 0.54\%$ & $45.24 \pm 0.55\%$ \\
		& \checkmark & 
		$\mathbf{62.13 \pm 0.74\%}$ & $40.64 \pm 0.54\%$ & $\mathbf{64.34 \pm 0.57\%}$ & $\mathbf{46.29 \pm 0.56\%}$ \\
		\midrule
		GNN & - & 
		$60.13 \pm 0.64\%$ & $43.60 \pm 0.67\%$ & $56.67 \pm 0.64\%$ & $49.17 \pm 0.62\%$ \\
		& \checkmark & 
		$\mathbf{69.26 \pm 0.68\%}$ & $\mathbf{48.91 \pm 0.67\%}$ & $\mathbf{72.59 \pm 0.67\%}$ & $\mathbf{58.36 \pm 0.68\%}$\\
		\bottomrule 
	\end{tabular}
	\label{tab:ablation_pretrain}
\end{table}


\paragraph{Number of ways in testing stage.}
In this experiment, we consider a practical scenario that the number of ways $N_w$ in the testing phase is \emph{different} from that in the training stage.
Since the GNN~\citep{garcia2018gnn} framework requires the numbers of ways to be consistent in the training and testing, we evaluate the MatchingNet~\citep{vinyals2016matching} and RelationNet~\citep{sung2018learning} model with this setting.
\tabref{nway} reports the performances of the models trained on the mini-ImageNet, Cars, Places, and Plantae domains under the 5-way 5-shot setting (\ie corresponding to the fourth and fifth block of the second column in~\tabref{manydomain}). 
Our proposed learning-to-learned feature-wise transformation layers are capable of improving the generalization of metric-based models to the unseen domain under various numbers of ways in the testing stage.

\begin{table}[t]\footnotesize
	\centering
	\caption{\textbf{Few-shot classification results under various numbers of ways in testing stage.} We compare the 5-shot performance under various number of ways in the testing phase. The CUB dataset is select as the testing (unseen) domain. All the models are trained with 5-way 5-shot setting.}
	\begin{tabular}{l l cccc} 
	    \toprule
	    5-Shot &  & CUB 2-way & CUB 5-way & CUB 10-way & CUB 20-way \\
		\midrule
		MatchingNet & - &
		$78.46 \pm 0.78\%$ & $51.92 \pm 0.80\%$ & $38.22 \pm 0.38\%$ & $26.17 \pm 0.24\%$ \\
		 & FT &
		$80.74 \pm 0.77\%$ & $56.29 \pm 0.80\%$ & $41.09 \pm 0.39\%$ & $29.19 \pm 0.24\%$\\
		 & LFT & 
		$\mathbf{83.88 \pm 0.72\%}$ & $\mathbf{61.41 \pm 0.57\%}$ & $\mathbf{45.69 \pm 0.39\%}$ & $\mathbf{32.81 \pm 0.23\%}$\\
		\midrule
		RelationNet & - &
		$84.25 \pm 0.72\%$ & $62.13 \pm 0.74\%$ & $47.15 \pm 0.40\%$ & $34.52 \pm 0.24\%$ \\
		& FT &
		$\mathbf{85.48 \pm 0.69\%}$ & $63.64 \pm 0.77\%$ & $48.35 \pm 0.38\%$ & $35.30 \pm 0.24\%$ \\
		& LFT & 
		$\mathbf{85.44 \pm 0.72\%}$ & $\mathbf{64.99 \pm 0.54\%}$ & $\mathbf{49.90 \pm 0.40\%}$ & $\mathbf{37.20 \pm 0.25\%}$\\ 
		\bottomrule 
	\end{tabular}
	\label{tab:nway}
\end{table}


\paragraph{Pre-determined hyper-parameters of feature-wise transformation layers.}
We demonstrate the difficulty to hand-tune the hyper-parameters of the proposed feature-wise transformation layers in this experiment.
Different from the setting described in~\secref{4_2}, we set the hyper-parameters $\theta_\gamma$ and $\theta_\beta$ in all feature-wise transformation layers to be $1$.
The model is trained under $5$-way setting using the mini-ImageNet domain, and evaluate it on the other domains.
We report the $1$-shot and $5$-shot performance in \tabref{handtune}.
We denote applying feature-wise transformation layers with $\{\theta_\gamma,\theta_\beta\}=\{0.3,0.5\}$ as FT and those with $\{\theta_\gamma,\theta_\beta\}=\{1,1\}$ as FT*.
We observe that the metric-based models applied with FT perform favorably against to those applied with FT*.
In several cases, applying FT* even yields inferior results compared to the original training without the feature-wise transformation layers.
This suggests the difficulty of hand-tuning the hyper-parameters and the importance of the proposed learning-to-learn scheme for optimizing the hyper-parameters of the feature-wise transformation layers.

\begin{table}[t]\scriptsize
	\centering
	\caption{\textbf{Few-shot classification results by applying different pre-determined hyper-parameters of feature-wise transformation layers.} We train the model on the mini-ImageNet with a different set of pre-determined hyper-parameters of feature-wise transformation layers. FT and FT* indicate that we apply the feature-wise transformation layers with hyper-parameters $\{\theta_\gamma,\theta_\beta\}$ to be $\{0.3,0.5\}$ and $\{1,1\}$, respectively.}
	\begin{tabular}{@{}ll|c|cccc@{}} 
	    \toprule
	    1-Shot & & mini-ImageNet & CUB & Cars & Places & Plantae \\
		\midrule
		MatchingNet & FT &
		$58.76 \pm 0.61\%$ & $36.61 \pm 0.53\%$ & $29.82 \pm 0.44\%$ & $51.07 \pm 0.68\%$ & $33.48 \pm 0.50\%$\\
		& FT* & 
$51.66 \pm 0.64\%$ & $31.74 \pm 0.51\%$ & $27.08 \pm 0.41\%$ & $45.04 \pm 0.64\%$ & $28.73 \pm 0.42\%$\\
		\midrule
		RelationNet & FT & 
		$58.64 \pm 0.85\%$ & $44.07 \pm 0.77\%$ & $28.63 \pm 0.59\%$ & $50.68 \pm 0.87\%$ & $33.14 \pm 0.62\%$\\
		& FT* & 
	    $57.45 \pm 0.66\%$ & $40.20 \pm 0.53\%$ & $29.15 \pm 0.45\%$ & $49.40 \pm 0.64\%$ & $33.21 \pm 0.47\%$\\
		\midrule
		GNN & FT &
		$66.32 \pm 0.80\%$ & $47.47 \pm 0.75\%$ & $31.61 \pm 0.53\%$ & $55.77 \pm 0.79\%$ & $35.95 \pm 0.58\%$\\
		& FT* & 
		$62.63 \pm 0.76\%$ & $44.61 \pm 0.66\%$ & $31.56 \pm 0.52\%$ & $53.39 \pm 0.74\%$ & $36.73 \pm 0.57\%$\\
	    \midrule
	    \midrule
		5-Shot & & mini-ImageNet & CUB & Cars & Places & Plantae \\
		\midrule
		MatchingNet & FT &
		$72.53 \pm 0.69\%$ & $55.23 \pm 0.83\%$ & $41.24 \pm 0.65\%$ & $64.55 \pm 0.75\%$ & $41.69 \pm 0.63\%$\\
		& FT* & 
		$64.93 \pm 0.60\%$ & $42.83 \pm 0.61\%$ & $32.19 \pm 0.48\%$ & $59.47 \pm 0.63\%$ & $39.61 \pm 0.49\%$\\
		\midrule
		RelationNet & FT &
		$73.78 \pm 0.64\%$ & $59.46 \pm 0.71\%$ & $39.91 \pm 0.69\%$ & $66.28 \pm 0.72\%$ & $45.08 \pm 0.59\%$\\
		& FT* & 
	    $72.79 \pm 0.64\%$ & $59.18 \pm 0.57\%$ & $40.54 \pm 0.54\%$ & $65.73 \pm 0.52\%$ & $43.64 \pm 0.49\%$\\
		\midrule
		GNN & FT &
		$81.98 \pm 0.55\%$ & $66.98 \pm 0.68\%$ & $44.90 \pm 0.64\%$ & $73.94 \pm 0.67\%$ & $53.85 \pm 0.62\%$\\
		& FT* & 
		$82.40 \pm 0.58\%$ & $66.33 \pm 0.73\%$ & $47.63 \pm 0.64\%$ & $75.48 \pm 0.65\%$ & $51.92 \pm 0.59\%$\\
		\bottomrule 
	\end{tabular}
	\label{tab:handtune}
\end{table}

\paragraph{Hyper-parameter initialization for learning-to-learn.}
For all the experiments, we initialize the parameters $\theta_\gamma$ and $\theta_\beta$ to $0.3$ and $0.5$, which we empirically determine in~\secref{4_2}, to train the feature-wise transformation layers. 
In practice, we find that the cross-domain performance is not sensitive as long as the initialized values are within the same order~(\eg~$0.1$ and $0.3$). 
Here we report the results of training the RelationNet with the initialization $\{\theta_\gamma,\theta_\beta\}=\{0.1,0.3\}$.
In this experiment, we use the CUB dataset as the unseen domain for evaluation and conduct the training described in~\algref{alg}.
The $5$-way $5$-shot classification accuracy on the CUB dataset is $64.79\pm0.55\%$, which is similar to the one we report in~\tabref{manydomain}~(~\ie~$64.99 \pm 0.54\%$).


\paragraph{Learning-to-learn using a single domain.}
The proposed learning-to-learn method requires multiple domains for training.
Here we apply the learning-to-learn method based on one single domain.
More specifically, we randomly sample two different tasks from the mini-ImageNet dataset in each iteration of the training process described in~\algref{alg}. 
One task serves as the pseudo-seen task, while the other one serves as the pseudo-unseen task.
We train the RelationNet model with the above-mentioned setting on 5-way 5-shot classification using the mini-ImageNet dataset only. 
As shown in~\tabref{l2lsingle}, the performance improvement of the RelationNet model is not significant compared to the model trained with pre-determined hyper-parameters $\{\theta_\gamma,\theta_\beta\}=\{0.3,0.5\}$.
This suggests that utilizing one single domain for learning the feature-wise transformation is not as effective as that using multiple domains (demonstrated in~\tabref{manydomain}).
The reason is that during the training phase, the discrepancy of the feature distributions extracted from the psudo-seen and pseudo-unseen tasks is not as significant since these two tasks are sampled from the same domain.
As a result, the hyper-parameters $\{\theta_\gamma,\theta_\beta\}$ cannot be effectively optimized to capture the variation of feature distributions sampled from various domains.

\begin{table}[t]\scriptsize
	\centering
	\caption{\textbf{Few-shot classification results by applying the learning-to-learn approach trained with a single seen domain.} We attempt to conduct the proposed learning-to-learn trainin with as singe seen domain, denoted as LFT*. We train the model using the mini-ImageNet dataset and report the 5-way 5-shot classification accuracy.}
	\begin{tabular}{ll|c|cccc} 
	    \toprule
	    5-way 5-Shot &  & mini-ImageNet & CUB & Cars & Places & Plantae\\
		\midrule
		RelationNet & FT & $73.78 \pm 0.64\%$ & $59.46 \pm 0.71\%$ & $39.91 \pm 0.69\%$ & $66.28 \pm 0.72\%$ & $45.08 \pm 0.59\%$\\
		RelationNet & LFT* & $73.50 \pm 0.50\%$ & $58.19 \pm 0.52\%$ & $39.35 \pm 0.54\%$ & $66.17 \pm 0.57\%$ & $46.75 \pm 0.51\%$\\
		\bottomrule 
	\end{tabular}
	\label{tab:l2lsingle}
\end{table}



\paragraph{Comparison with the state-of-the-art few-shot classification on the mini-ImageNet.}
We compare the metric-based frameworks applied with the proposed feature-wise transformation layers to the state-of-the-art few-shot classification methods in~\tabref{sota}.
In this experiment, we train the model with the pre-determined hyper-parameters of feature-wise transformation layers on the training set of the mini-ImageNet~\citep{ravi2017metalstm} dataset.
Note that we do not use the \emph{learned} version of the feature-wise transformation layers in the training to ensure fair comparison.
Combining~\tabref{ablation_pretrain} and~\tabref{sota}, we observe that the metric-based frameworks train with 1) pre-trained feature encoder, and 2) feature-wise transformation layers with carefully hand-tuned hyper-parameters can demonstrate competitive performance.


\begin{table}[t]\footnotesize
	\centering
	\caption{\textbf{Comparison to the state-of-the-art few-shot classification algorithms.} We compare the metric-based frameworks applied with the proposed feature-wise transformation layers using pre-determined hyper-parameter $\{\theta_\gamma, \theta_\beta\} = \{0.3, 0.5\}$ (denoted as FT) to other state-of-the-art few-shot classification methods. Note that all the methods are trained only on the mini-ImageNet dataset. To ensure fair comparisons with other methods, we are unable to use the \emph{learned} version of the feature-wise transformation layers described in \secref{LFT}. By augmenting existing metric-based few-shot classification models with the proposed feature-wise transformation layer, we obtain competitive performance when compared with many recent and more complicated methods. The best results in each block are highlighted in bold.}
	\begin{tabular}{ll l cc} 
	    \toprule
	    backbone & method &  & $5$-way $1$-shot & $5$-way $5$-shot \\
	    \midrule
	    ResNet-12 & \multicolumn{2}{l}{TADAM~\citep{oreshkin2018tadam}} & $58.50 \pm 0.30\%$ & $76.70 \pm 0.30\%$ \\
	    & \multicolumn{2}{l}{DC~\citep{lifchitz2019dense}} & $62.53 \pm 0.19\%$ & $78.95 \pm 0.13\%$ \\
	    & \multicolumn{2}{l}{DC + IMP~\citep{lifchitz2019dense}} & - & $79.77 \pm 0.19\%$ \\
	    & \multicolumn{2}{l}{MetaOptNet-SVM-trainval~\citep{lee2019rr}} & $\mathbf{64.09 \pm 0.62\%}$ & $\mathbf{80.00 \pm 0.45\%}$ \\
	    WRN-28 & \multicolumn{2}{l}{\citet{qiao2018few}} & $59.60 \pm 0.41\%$ & $77.74 \pm 0.19\%$ \\
	    & \multicolumn{2}{l}{LEO~\citep{rusu2018leo}} & $61.76 \pm 0.08\%$ & $77.59 \pm 0.12\%$ \\
	    \midrule
	    ResNet-10 & MatchingNet & - & $59.10 \pm 0.64\%$ & $70.96 \pm 0.65\%$ \\
		& & FT & $58.76 \pm 0.61\%$ & $72.53 \pm 0.69\%$ \\
		& RelationNet & - & $57.80 \pm 0.88\%$ & $71.00 \pm 0.69\%$ \\
		& & FT & $58.64 \pm 0.85\%$ & $73.78 \pm 0.64\%$ \\
		& GNN & - & $60.77 \pm 0.75\%$ & $80.87 \pm 0.56\%$ \\
		& & FT & $\mathbf{66.32 \pm 0.80\%}$ & $\mathbf{81.98 \pm 0.55\%}$ \\
		\bottomrule 
	\end{tabular}
	\label{tab:sota}
\end{table}

\paragraph{Comparison to the state-of-the-art few-shot classification under domain shift.}
We evaluate the metric-based frameworks with the proposed feature-wise transformation layers and the state-of-the-art MetaOptNet approach~\cite{lee2019rr}.
We use the model trained on the mini-ImageNet dataset provided by the authors for the evaluation on the other datasets.\footnote{\url{https://github.com/kjunelee/MetaOptNet}}
As shown in~\tabref{sotacross}, while the MetaOptNet method achieves state-of-the-art performance on the mini-ImageNet dataset, this approach also suffers from the domain shifts in the cross-domain setting.
Training the GNN framework with the pre-trained feature encoder and the proposed feature-wise transformation layers performs favorably against the MetaOptNet method under the cross-domain setting.

\begin{table}[t]\footnotesize
	\centering
	\caption{\textbf{Evaluation with the state-of-the-art approach under the cross-domain setting.} We evaluate the metric-based frameworks with the proposed feature-wise transformation layers using pre-determined hyper-parameter $\{\theta_\gamma, \theta_\beta\} = \{0.3, 0.5\}$ (denoted as FT) against the state-of-the-art MetaOptNet-SVM-trainval~\cite{lee2019rr} method. 
Note that all the methods are trained only on the mini-ImageNet dataset. 
To ensure fair comparisons with other methods, we do not use the \emph{learned} version of the feature-wise transformation layers described in \secref{LFT}. 
By augmenting the existing metric-based few-shot classification models with the proposed feature-wise transformation layer, we obtain competitive performance when compared with recent and more complicated methods. The best results are highlighted in bold.}
	\begin{tabular}{@{}lc cccc@{}} 
	    \toprule
	    method &  & CUB & Cars & Places & Plantae \\
	    \midrule
	    \multicolumn{2}{l}{MetaOptNet-SVM-trainval} & $54.67 \pm 0.56\%$ & $\mathbf{45.90 \pm 0.49\%}$ & $65.83 \pm 0.57\%$ & $46.48 \pm 0.52\%$\\
	    \midrule
	    MatchingNet & - & $51.37 \pm 0.77\%$ & $38.99 \pm 0.64\%$ & $63.16 \pm 0.77\%$ & $46.53 \pm 0.68\%$ \\
		& FT & $55.23 \pm 0.83\%$ & $41.24 \pm 0.65\%$ & $64.55 \pm 0.75\%$ & $41.69 \pm 0.63\%$ \\
		RelationNet & - & $57.77 \pm 0.69\%$ & $37.33 \pm 0.68\%$ & $63.32 \pm 0.76\%$ & $44.00 \pm 0.60\%$ \\
	    & FT & $59.46 \pm 0.71\%$ & $39.91 \pm 0.69\%$ & $66.28 \pm 0.72\%$ & $45.08 \pm 0.59\%$ \\
		GNN & - & $62.25 \pm 0.65\%$ & $44.28 \pm 0.63\%$ & $70.84 \pm 0.65\%$ & $52.53 \pm 0.59\%$ \\
		& FT & $\mathbf{66.98 \pm 0.68\%}$ & $44.90 \pm 0.64\%$ & $\mathbf{73.94 \pm 0.67\%}$ & $\mathbf{53.85 \pm 0.62\%}$ \\
		\bottomrule 
	\end{tabular}
	\label{tab:sotacross}
\end{table}
