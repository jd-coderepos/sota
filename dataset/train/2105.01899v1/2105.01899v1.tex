\documentclass{article} \usepackage{iclr2021_conference,times}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 



\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}

\usepackage{dsfont}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage[frozencache=true,cachedir=minted-cache]{minted} 
\usemintedstyle{borland}


\usepackage{dsfont}

\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage[linesnumbered,ruled,noline]{algorithm2e}
\SetKwInput{KwInput}{Input}                \SetKwInput{KwOutput}{Output}              \SetKwInput{KwData}{Data}                \SetKwInput{KwInit}{Initialization}              \SetKwInput{kwReturn}{Return}

\usepackage{subcaption}
\usepackage{caption}

\usepackage{amsthm}

\usepackage{wrapfig}


\newcommand{\gv}{\mathbf{g}}
\newcommand{\Gv}{\mathbf{G}}
\newcommand{\av}{\mathbf{a}}
\newcommand{\Av}{\mathbf{A}}
\newcommand{\Cv}{\mathbf{C}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\Ym}{\mathcal{Y}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\Zv}{\mathbf{Z}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Xm}{\mathcal{X}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\Rv}{\mathbf{R}}
\newcommand{\Iv}{\mathbf{I}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\xvtilde}{\tilde{\xv}}
\newcommand{\hv}{\mathbf{h}}
\newcommand{\Hv}{\mathbf{H}}
\newcommand{\zerov}{\mathbf{0}}
\newcommand{\fv}{\mathbf{f}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\Uv}{\mathbf{U}}

\newcommand{\ud}{\mathrm{d}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newcommand{\ep}{\mathbb{E}}
\newcommand{\congwei}[1]{\textcolor{purple}{#1}}
\newcommand{\junz}[1]{\textcolor{red}{[zj: #1]}}

\renewcommand{\thesubtable}{\roman{subtable}}

\usepackage{hyperref}



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}




\newcommand{\cx}[1]{\textcolor{blue}{CX:#1}}

\title{MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering}




\author{Tsung Wei Tsai, Chongxuan Li, Jun Zhu~\thanks{Corresponding author.} \\
Dept. of Comp. Sci. \& Tech., Institute for AI, BNRist Center\\
Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, 100084 China \\
\small{\texttt{\{peter83112414,chongxuanli1991\}@gmail.com, dcszj@mail.tsinghua.edu.cn}}}








\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}







We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner.
To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. 
Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results~\footnote{Code is available at: \url{https://github.com/TsungWeiTsai/MiCE}} than various previous methods and a strong contrastive learning baseline.


\end{abstract}


\section{Introduction}





Unsupervised clustering is a fundamental task that aims to partition data into distinct groups of similar ones without explicit human labels. 
Deep clustering methods~\citep{xie2016unsupervised,wu2019deep}
exploit the representations learned by neural networks and
have made large progress on high-dimensional data recently. Often, such methods learn the representations for clustering by reconstructing data in a deterministic~\citep{ghasedi2017deep} or probabilistic manner~\citep{jiang2016variational}, or maximizing certain mutual information~\citep{hu2017learning,ji2019invariant} (see Sec.~\ref{sec:related_work} for the related work).
Despite the recent advances, the representations learned by existing methods may not be discriminative enough to capture the semantic similarity between images.




The instance discrimination task~\citep{wu2018unsupervised,he2019momentum} in contrastive learning has shown promise in pre-training representations transferable to downstream tasks through fine-tuning. Given that the literature~\citep{shiran2019multi,niu2020gatcluster} shows improved representations can lead to better clustering results, we hypothesize that instance discrimination can improve the performance as well. A straightforward approach is to learn a classical clustering model, e.g. spherical -means~\citep{dhillon2001concept}, directly on the representations pre-trained by the task. Such a two-stage baseline can achieve excellent clustering results (please refer to Tab.~\ref{tab:fullexp}). 
However, because of the independence of the two stages, the baseline may not fully explore the semantic structures of the data when learning the representations and lead to a sub-optimal solution for clustering.


To this end, we propose  Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering method that utilizes the instance discrimination task as a stepping stone to improve clustering. In particular,  to capture the semantic structure explicitly, we formulate a mixture of conditional models by introducing latent variables to represent cluster labels of the images, which is inspired by the mixture of experts (MoE) formulation.
In MiCE, each of the conditional models, also called an {\it expert}, learns to discriminate a subset of instances, 
while an input-dependent {\it gating function} partitions the dataset into subsets according to the latent semantics by allocating weights among experts. 
Further, we develop a scalable variant of the  Expectation-Maximization (EM) algorithm~\citep{dempster1977maximum} for the nontrivial inference and learning problems. In the E-step, we obtain the approximate inference of the posterior distribution of the latent variables given the observed data. In the M-step, we maximize the evidence lower bound (ELBO) of the log conditional likelihood with respect to all parameters. Theoretically, we show that the ELBO is bounded and the  proposed EM algorithm leads to the convergence of ELBO. Moreover, we carefully discuss the algorithmic relation between MiCE and the two-stage baseline and show that the latter is a special instance of the former in a certain extreme case.





Compared with existing clustering methods, MiCE has the following advantages. (i) \textbf{Methodologically unified}: MiCE conjoins the benefits of both the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model within a unified probabilistic framework. (ii) \textbf{Free from regularization}: MiCE trained by EM optimizes a single objective function, which does not require auxiliary loss or regularization terms. (iii) \textbf{Empirically effective}: Evaluated on four widely adopted  natural image datasets, MiCE achieves significantly better results than a strong contrastive baseline and extensive prior clustering methods on several benchmarks without any form of pre-training.

\section{Related Work}\label{sec:related_work}
{\bf Deep clustering.} 
Inspired by the success of deep learning, many researchers propose to learn the representations and cluster assignments simultaneously~\citep{xie2016unsupervised,yang2016joint,yang2017towards} based on data reconstruction~\citep{xie2016unsupervised,yang2017towards}, pairwise relationship among instances~\citep{chang2017deep,haeusser2018associative,wu2019deep}, multi-task learning~\citep{shiran2019multi,niu2020gatcluster}, etc. The joint training framework often ends up optimizing a weighted average of multiple loss functions.
However, given that the validation dataset is barely provided, tuning the weights between the losses may be impractical~\citep{ghasedi2017deep}. 




Recently, several methods also explore probabilistic modeling, and they introduce latent variables to represent the underlying classes. On one hand, deep generative approaches~\citep{jiang2016variational,dilokthanakul2016deep,chongxuan2018graphical,mukherjee2019clustergan,yang2019deep} attempt to capture the data generation process with a mixture of Gaussian prior on latent representations. However, the imposed assumptions can be violated in many cases, and capturing the true data distribution is challenging but may not be helpful to the clustering~\citep{krause2010discriminative}. On the other hand, discriminative approaches~\citep{hu2017learning,ji2019invariant,darlow2020dhog} directly model the mapping from the inputs to the cluster labels and maximize a form of mutual information, which often yields superior cluster accuracy. Despite the simplicity, the discriminative approaches discard the instance-specific details that can benefit clustering via improving the representations.










Besides, MIXAE~\citep{zhang2017deep}, DAMIC~\citep{chazan2019deep}, and MoE-Sim-VAE~\citep{kopf2019mixture-of-experts} 
combine the mixture of experts (MoE) formulation~\citep{jacobs1991adaptive} with the data reconstruction task. However, either pre-training, regularization, or an extra clustering loss is required.































{\bf Contrastive learning.}
To learn discriminative representations, contrastive learning~\citep{wu2018unsupervised,oord2018representation,he2019momentum,tian2019contrastive,chen2020simple} incorporates various contrastive loss functions with different pretext tasks
such as colorization~\citep{zhang2016colorful}, context auto-encoding~\citep{pathak2016context}, and instance discrimination~\citep{dosovitskiy2015discriminative,wu2018unsupervised}. The pre-trained representations often achieve promising results on downstream tasks, \textit{e.g.}, depth prediction, object detection~\citep{ren2015faster,he2017mask}, and image classification~\citep{,kolesnikov2019revisiting}, after fine-tuning with human labels.
In particular, InstDisc~\citep{wu2018unsupervised} learns from instance-level discrimination using NCE~\citep{gutmann2010noise}, and maintains a memory bank to compute the loss function efficiently. MoCo replaces the memory bank with a queue and maintains an EMA of the student network as the teacher network to encourage consistent representations.
A concurrent work called PCL~\citep{li2020prototypical} also explores the semantic structures in contrastive learning.
They add an auxiliary cluster-style objective function on top of the MoCo's original objective, which differs from our method significantly. 
PCL requires an auxiliary -means~\citep{lloyd1982least} algorithm to obtain the posterior estimates and the prototypes. Moreover, their aim of clustering is to induce transferable embeddings instead of discovering groups of data that correspond to underlying semantic classes. 








\section{Preliminary}
We introduce the contrastive learning methods based on the instance discrimination task~\citep{wu2018unsupervised,ye2019unsupervised,he2019momentum,chen2020simple}, with a particular focus on the recent state-of-the-art method, MoCo~\citep{he2019momentum}. Let  be a set of images without the ground-truth labels, and each of the datapoint  is assigned with a unique surrogate label  such that \footnote{The value of the surrogate label can be regarded as the index of the image.}. 
To learn representations in an unsupervised manner, instance discrimination considers a discriminative classifier that maps the given image to its surrogate label.
Suppose that we have two encoder networks  and  that generate  -normalized embeddings  and , respectively, given the image  with the surrogate label . We show the parameters of the networks in the subscript, and images are transformed by a stochastic data augmentation module before passing to the networks (please see Appendix~\ref{sec:detail_exp}). We can model the probability classifier with:

where  is the temperature hyper-parameter controlling the concentration level~\citep{hinton2015distilling} \footnote{Due to summation over the entire dataset in the denominator term, it can be computationally prohibitive to get Maximum likelihood estimation (MLE) of the parameters~\citep{ma2018noise}.}.

The recent contrastive learning methods mainly differ in: 
(1) The contrastive loss used to learn the network parameters, including NCE~\citep{wu2018unsupervised}, InfoNCE~\citep{oord2018representation}, and the margin loss~\citep{schroff2015facenet}.
(2) The choice of the two encoder networks based on deep neural networks (DNNs) in which  can be an identical~\citep{ye2019unsupervised,chen2020simple}, distinct~\citep{tian2019contrastive}, or an exponential moving average (EMA)~\citep{he2019momentum} version of .  

In particular, MoCo~\citep{he2019momentum} learns by minimizing the InfoNCE loss:

where  is a queue of size  storing previous embeddings from . While it adopts the EMA approach to avoid rapidly changing embeddings in the queue that adversely impacts the performance~\citep{he2019momentum}. For convenience, we refer  and  as the student and teacher network respectively~\citep{tarvainen2017mean,tsai2019d}.  In the following, we propose a unified latent mixture model based on contrastive learning to tackle the clustering task. 


\section{Mixture of Contrastive Experts}
Unsupervised clustering aims to partition a dataset  with  observations into  clusters.  We introduce the latent variable  to be the cluster
label of the image  and naturally extend  Eq.~(\ref{eqn:contrasive_p_y_x}) to Mixture of Contrastive Experts (MiCE):

where  is an indicator function. 
The formulation explicitly introduces a mixture model to capture the latent semantic structures, which is inspired by the mixture of experts (MoE) framework~\citep{jacobs1991adaptive}. In Eq.~(\ref{eqn:mice}),
 is one of the \textit{experts} that learn to discriminate a subset of instances and  is a \textit{gating function} that partitions the dataset into subsets according to the latent semantics by routing the given input to one or a few experts. 
With a divide-and-conquer principle, the experts are often highly specialized in particular images that share similar semantics, which improves the learning efficiency. 
Notably, MiCE is generic to the choice of the underlying contrastive methods~\citep{wu2018unsupervised,he2019momentum,chen2020simple}, while in this paper, we focus on an instance based on MoCo. Also, please see Fig.~\ref{fig:MiCE} for an illustration of MiCE with three experts.



In contrast to the original MoE used in the supervised settings~\citep{jacobs1991adaptive}, our experts learn from instance-wise discrimination
instead of human labels.
In addition, both gating and expert parts of MiCE are based on DNNs to fit the high-dimensional data. In the following, we will elaborate on how we parameterize the gating function and the experts to fit the clustering task. For simplicity, we omit the parameters in all probability distributions in this section. 






































\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{MiCE_flow.pdf}
    \caption{An illustration of MiCE with three experts. Best view in color.}
    \label{fig:MiCE}
\end{figure}
{\bf Gating function.} The gating function organizes the instance discrimination task into  simpler subtasks by weighting the experts based on the semantics of the input image.
We define  as an encoder network that outputs an embedding for each input image. We denote the output vector for image  as . The gating function is then parameterized as:

where  is the temperature, and  represent the gating prototypes. All prototypes and image embeddings are -normalized in the  space. Hence, the gating function performs a soft partitioning of the dataset based on the cosine similarity between the image embeddings and the gating prototypes. We can view it as a prototype-based discriminative clustering module, whereas we obtain the cluster labels using posterior inference to consider additional information in the experts.




{\bf Experts.} In MiCE, every expert learns to solve the instance discrimination subtask arranged by the gating function. We define the expert 
in terms of the unnormalized model  following~\citet{wu2018unsupervised,he2019momentum}. Therefore,  
the probability of the image  being recognized as the -th one by the -th expert is formulated as follows:

where   is a normalization constant that is often computationally intractable. 

Similar to MoCo, we have the student network  that maps the image  into  continuous embeddings . Likewise, the teacher network  outputs  given . To be specific,  and  are the student embedding and the teacher embedding for images  under the -th expert, respectively. We then parameterize the unnormalized model as:

where  is the temperature and  represent  the cluster prototypes for the experts. In 
Eq.~(\ref{unnormalized}), the first instance-wise dot product explores the \textit{instance-level} information to induce discriminative representations within each expert.
The second instance-prototype dot product incorporates the \textit{class-level} information into representation learning, encouraging a clear cluster structure around the prototype. 
Overall, the learned embeddings are therefore encoded with semantic structures while being discriminative enough to represent the instances. Eq.~(\ref{unnormalized}) is built upon MoCo with the EMA approach, while in principle, many other potential solutions exist to define the experts, which are left for future studies. Besides, the parameters  and  are partially shared, please refer to the Appendix~\ref{sec:detail_exp} for more details on the architecture.


























































































































































\section{Inference and Learning}
We first discuss the evidence lower bound (ELBO), the single objective used in MiCE, in Sec.~\ref{sec.elbo}. Then, we present a scalable variant of the Expectation-Maximization (EM) algorithm~\citep{dempster1977maximum} to deal with the non-trivial inference and learning of MiCE in Sec.~\ref{sec.em}. Lastly, in Sec.~\ref{sec.two_stage}, we show that a naïve two-stage baseline, in which we run a spherical -means algorithm on the embeddings learned by MoCo, is a special case of MiCE.


\subsection{Evidence lower bound (ELBO)}\label{sec.elbo}
The parameters to update include the parameters  of the student and gating network respectively, and the expert prototypes .
The learning objective of MiCE is to maximize the evidence lower bound (ELBO) of the log conditional likelihood  of the entire dataset. The ELBO of the datapoint  is given by:

where  is a variational distribution to infer the latent cluster label given the observed data. 
The first term in Eq.~(\ref{eqn:elbo}) encourages   to be high for the experts that are good at discriminating the input images. Intuitively, it can relief the potential {\it degeneracy} issue~\citep{caron2018deep,ji2019invariant}, where all images are assigned to the same cluster. This is because a degenerated posterior puts the pressure of discriminating all images on a single expert, which may result in a looser ELBO.
The second term in Eq.~(\ref{eqn:elbo}) is the Kullback–Leibler divergence between the variational distribution and the distribution defined by the gating function. With this term, the gating function is refined during training and considers the capability of the experts when partitioning data. Notably, MiCE does not rely on auxiliary loss or regularization terms as many prior methods~\citep{haeusser2018associative,shiran2019multi,wu2019deep,niu2020gatcluster} do. 











\subsection{EM algorithm}\label{sec.em}






{\bf E-step.}~\label{Sec_estep} Inferring the posterior distribution of latent variables given the observations is an important step to apply MiCE to clustering. According to Bayes' theorem, the posterior distribution given the current estimate of the model parameters is:

Comparing with the gating function , the posterior provides better estimates of the latent variables by incorporating the supplementary information of the experts. However, we cannot tractably compute the posterior distribution because of the normalization constants . In fact,
given the image  and the cluster label ,  sums over the entire dataset, which is prohibitive for large-scale image dataset.
We present a simple and analytically tractable estimator to approximate them. Specifically, we maintain a queue  that stores  previous outputs of the teacher network, following MoCo closely. Formally, the estimator  is:

The estimator is biased, while its bias decreases as  increases and we can get a sufficient amount of embeddings from the queue  efficiently\footnote{Even though the bias does not vanish due to the use of queue, we find that the approximation works well empirically.}.
With Eq.~(\ref{approx_z}), we approximate the posterior as:


















{\bf M-step.} We leverage the stochastic gradient ascent to optimize ELBO with respect to the network parameters  and the expert prototypes . We approximate the normalization constants appear in ELBO in analogy to the E-step, formulated as follows:

Sampling a mini-batch  of datapoints, we can construct an efficient stochastic estimator of ELBO over the full dataset to learn  and :

It requires additional care on the update of the prototypes, as discussed in many clustering methods~\citep{sculley2010web-scale,xie2016unsupervised,yang2017towards,shiran2019multi}. Some of them carefully adjust the learning rate of each prototype separately~\citep{sculley2010web-scale,yang2017towards}, which can be very different from the one used for the network parameters. Since evaluating different learning rate schemes on the validation dataset is often infeasible in unsupervised clustering, we employ alternative strategies which are free from using per-prototype learning rates in MiCE.

As for the expert prototypes, we observe that using only the stochastic update can lead to bad local optima. Therefore, at the end of each training epoch, we apply an additional analytical update derived from the ELBO as follows: 

where   is the hard assignment of the cluster label.
Please refer to Appendix~\ref{sec:derive_expert_update} for the detailed derivation.
Intuitively, the analytical update in Eq.~(\ref{expert_update}) considers all the teacher embeddings assigned to the -th cluster, instead of only the ones in a mini-batch, to avoid bad local optima.













Beside, we fix the gating prototypes  to a set of pre-defined embeddings to stabilize the training process.
However, using randomly initialized prototypes may cause unnecessary difficulties in partitioning the dataset if some of them are crowded together. We address the potential issue by using the means of a Max-Mahalanobis distribution (MMD)~\citep{pang2018max} which is a special case of the mixture of Gaussian distribution. The untrainable means in MMD provide the optimal inter-cluster dispersion~\citep{pang2019rethinking} that stabilizes the gating outputs. 
We provide the algorithm of MMD in Appendix~\ref{appendix:mmd_algo} and a systematical ablation study in Tab.~\ref{tab.ablation} to investigate the effect of the updates on  and . Lastly, we provide the formal proof on the convergence of the EM algorithm in Appendix~\ref{appendix:em_convergence}.














\subsection{Relations to a two-stage baseline}\label{sec.two_stage}
The combination of a contrastive learning method and a clustering method is a natural baseline of MiCE. Our analysis reveals that MiCE is the general form of the two-stage baseline in which we learn the image embeddings with MoCo~\citep{he2019momentum} and subsequently run a spherical -means algorithm~\citep{dhillon2001concept} to obtain the cluster labels. 


On one hand, in the extreme case where  (Assumption A3),  the student embeddings  and teacher embeddings  are identical for different  (Assumption A4), and the class-level information in Eq.~(\ref{unnormalized}) is omitted (Assumption A5), we arrive at the same Softmax classifier (Eq.~(\ref{eqn:contrasive_p_y_x})) and the InfoNCE loss (Eq.~(\ref{eqn:infoNCE})) used by MoCo
as a special case of our method. On the other hand, of particular relevance to the analytical update on expert prototypes (Eq.~(\ref{expert_update})) is the spherical -means algorithm~\citep{dhillon2001concept} that leverages the cosine similarity to cluster -normalized data~\citep{hornik2012spherical}.
In addition to Assumptions A3 and A4, if we assume the unnormalized model is perfectly self-normalized (Assumption A2),  using the hard assignment to get the cluster labels together with the analytical update turns out to be a single-iteration spherical -means algorithm on the teacher embeddings. Please refer to the Appendix~\ref{appendix_assumption} for a detailed derivation.






The performance of the baseline is limited by the independence of the representation learning stage and the clustering stage. In contrast, MiCE provides a unified framework to align the representation learning and clustering objectives in a principled manner. See a comprehensive comparison in Tab.~\ref{tab:fullexp}.























\begin{table}[]
\centering
\caption{Unsupervised clustering performance of different methods on four datasets. The first sector presents the results from the literature, the later ones display the results of the baseline and the proposed MiCE. In the last two sectors, the bold results indicating the one with the highest values. Methods with the legend\textdagger{} are the ones that required post-processing by -means to obtain the clusters since they do not learn the clustering function directly, except that we use spherical -means for MoCo. We calculate the mean and standard deviation (Std.) of MiCE and MoCo based on five runs.}
\label{tab:fullexp}
\resizebox{1.0\columnwidth}!{
\setlength{\tabcolsep}{1.0mm}
{
\begin{tabular}{@{}lccc|ccc|ccc|ccc@{}}
\toprule
Datasets               & \multicolumn{3}{c}{CIFAR-10}                  & \multicolumn{3}{c}{CIFAR-100}              & \multicolumn{3}{c}{STL-10}                    & \multicolumn{3}{c}{ImageNet-Dog} \\ \midrule
Methods/Metrics (\%)        & NMI           & ACC           & ARI           & NMI           & ACC           & ARI           & NMI           & ACC           & ARI           & NMI       & ACC       & ARI      \\ \hline -means~\citep{lloyd1982least}              & 8.7           & 22.9          & 4.9           & 8.40          & 13.0          & 2.8           & 12.5          & 19.2          & 6.1           & 5.5       & 10.5      & 2.0      \\
SC~\citep{zelnikmanor2004self-tuning}                     & 10.3          & 24.7          & 8.5           & 9.0           & 13.6          & 2.2           & 9.8           & 15.9          & 4.8           & 3.8       & 11.1      & 1.3      \\
AE\textdagger~\citep{bengio2006greedy}          & 23.9          & 31.4          & 16.9          & 10.0          & 16.5          & 4.8           & 25.0          & 30.3          & 16.1          & 10.4      & 18.5      & 7.3      \\
DAE\textdagger~\citep{vincent2010stacked}         & 25.1          & 29.7          & 16.3          & 11.1          & 15.1          & 4.6           & 22.4          & 30.2          & 15.2          & 10.4      & 19.0      & 7.8      \\
SWWAE\textdagger~\citep{zhao2015stacked}       & 23.3          & 28.4          & 16.4          & 10.3          & 14.7          & 3.9           & 19.6          & 27.0          & 13.6          & 9.4       & 15.9      & 7.6      \\
GAN\textdagger~\citep{radford2015unsupervised}         & 26.5          & 31.5          & 17.6          & 12.0          & 15.3          & 4.5           & 21.0          & 29.8          & 13.9          & 12.1      & 17.4      & 7.8      \\
VAE\textdagger~\citep{kingma2013auto}         & 24.5          & 29.1          & 16.7          & 10.8          & 15.2          & 4.0           & 20.0          & 28.2          & 14.6          & 10.7      & 17.9      & 7.9      \\
JULE~\citep{yang2016joint}                   & 19.2          & 27.2          & 13.8          & 10.3          & 13.7          & 3.3           & 18.2          & 27.7          & 16.4          & 5.4       & 13.8      & 2.8      \\
DEC~\citep{xie2016unsupervised}                    & 25.7          & 30.1          & 16.1          & 13.6          & 18.5          & 5.0           & 27.6          & 35.9          & 18.6          & 12.2      & 19.5      & 7.9      \\
DAC~\citep{chang2017deep}                    & 39.6          & 52.2          & 30.6          & 18.5          & 23.8          & 8.8           & 36.6          & 47.0          & 25.7          & 21.9      & 27.5      & 11.1     \\
DCCM~\citep{wu2019deep}                   & 49.6          & 62.3          & 40.8          & 28.5          & 32.7          & 17.3          & 37.6          & 48.2          & 26.2          & 32.1      & 38.3      & 18.2     \\
IIC~\citep{ji2019invariant}                    & -             & 61.7          & -             & -             & 25.7          & -             & -             & 49.9          & -             & -         & -         & -        \\
DHOG~\citep{darlow2020dhog}                   & 58.5          & 66.6          & 49.2          & 25.8          & 26.1          & 11.8          & 41.3          & 48.3          & 27.2          & -         & -         & -        \\
AttentionCluster~\citep{niu2020gatcluster}       & 47.5          & 61.0          & 40.2          & 21.5          & 28.1          & 11.6          & 44.6          & 58.3          & 36.3          & 28.1      & 32.2      & 16.3     \\
MMDC~\citep{shiran2019multi}                   & 57.2          & 70.0          & -             & 25.9          & 31.2          & -             & 49.8          & 61.1          & -             & -         & -         & -        \\ 
PICA~\citep{huang2020deep}                   & 59.1          & 69.6          & 51.2             & 31.0          & 33.7          & 17.1             & 61.1          & 71.3          & 53.1             & 35.2         & 35.2         & 20.1       \\ 
\hline
MoCo (Mean)\textdagger~\citep{he2019momentum} & 66.0          & 74.7          & 59.3          & 38.8          & 39.5          & 24.0          & 60.5          & 70.7          & 53.0          & 34.2      & 30.8      & 18.4     \\
MoCo (Std.)\textdagger~\citep{he2019momentum}  & 0.6           & 1.7           & 0.9           & 0.2           & 0.1           & 0.4           & 0.9           & 2.0           & 0.8           & 0.3       & 1.7       & 0.9      \\
MiCE (Mean, \textbf{Ours})            & \textbf{73.5} & \textbf{83.4} & \textbf{69.5} & \textbf{43.0} & \textbf{42.2} & \textbf{27.7} & \textbf{61.3}          & \textbf{72.0}          & \textbf{53.2}          & \textbf{39.4} & \textbf{39.0}          & \textbf{24.7} \\
MiCE (Std., \textbf{Ours})            & 0.2           & 0.2           & 0.3           & 0.5           & 1.4           & 0.4           & 1.2           & 1.8           & 2.4           & 1.8           & 3.0           & 2.4           \\ \hline
MoCo (Best)\textdagger~\citep{he2019momentum}  & 66.9          & 77.6          & 60.8          & 39.0          & 39.7          & 24.2          & 61.5          & 72.8          & 52.4          & 34.7          & 33.8          & 19.7          \\
MiCE (Best, \textbf{Ours})            & \textbf{73.7} & \textbf{83.5} & \textbf{69.8} & \textbf{43.6} & \textbf{44.0} & \textbf{28.0} & \textbf{63.5} & \textbf{75.2} & \textbf{57.5} & \textbf{42.3} & \textbf{43.9} & \textbf{28.6} \\ \bottomrule
\end{tabular}}}
\end{table}



\section{Experiments}
\label{sec:experiments_start}

\begin{wraptable}{r}{8.0cm}
\caption{Statistics of the datasets. 
}
\begin{tabular}{lccc}
    \toprule
    Dataset       & Images  & Clusters & Image Size                                   \\ \hline
    CIFAR-10       & 60,000  & 10      &  \\
    CIFAR-100   & 60,000  & 20      &  \\
    STL-10         & 13,000 & 10      &     \\ 
    ImageNet-Dog & 19,500 & 15     &  \\\bottomrule
    \end{tabular}\label{data_stat}
\end{wraptable}
In this section, we present experimental results to demonstrate the effectiveness of MiCE.
We compare MiCE with extensive prior clustering methods and the contrastive learning based two-stage baseline on four widely adopted benchmarking datasets for clustering, including STL-10~\citep{coates2011an}, CIFAR-10~\citep{krizhevsky2009learning}, CIFAR-100~\citep{krizhevsky2009learning}, and ImageNet-Dog~\citep{chang2017deep}. 
The experiment settings follow the literature closely~\citep{chang2017deep,wu2019deep,ji2019invariant,shiran2019multi,darlow2020dhog} and the numbers of the clusters are known in advance. The statistics of the datasets are summarized in Tab.~\ref{data_stat}. We adopt three common metrics to evaluate the clustering performance, namely normalized mutual information (NMI), cluster accuracy (ACC), and adjusted rand index (ARI). All the metrics are presented in percentage (\%). We use a 34-layer ResNet (ResNet-34)~\citep{he2016deep} as the backbone for MiCE and MoCo following the recent methods~\citep{ji2019invariant,shiran2019multi} for fair comparisons. 
We set both temperatures  and  as 1.0, and the batch size as 256. 
The datasets, network, hyper-parameters, and training settings are discussed detailedly in Appendix~\ref{sec:detail_exp}.






\subsection{Main clustering results}
{\bf Comparison with existing deep clustering methods.} As shown in Tab.~\ref{tab:fullexp}, MiCE outperforms the previous clustering approaches by a significant margin on all datasets. The comparison highlights the importance of exploring the discriminative representations and the semantic structures of the dataset.








{\bf Comparison with the two-stage baseline.} Compared to the straightforward combination of MoCo and spherical -means, MiCE explores the semantic structures of the dataset that improve the clustering performance.
From Tab.~\ref{tab:fullexp}, we can see that MiCE consistently outperforms the  baseline in terms of the mean performance, which agrees with the analysis in Sec.~\ref{sec.two_stage}.
Specifically, regarding ACC, we improve upon the strong baseline by 8.7\%, 2.7\%, and 8.2\% on CIFAR-10, CIFAR-100, and ImageNet-Dog, respectively.
Taking the measurement variance into consideration, our performance overlaps with MoCo only on STL-10. We conjecture that the small data size may limit the performance as each expert learns from a subset of data. Nevertheless, the comparison manifests the significance of aligning the representation learning and clustering objectives in a unified framework, and we believe that MiCE points out a promising direction for future studies in clustering. 

\begin{figure}[]
\begin{center}
\subfloat[Epoch 1 ()]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch0_200.png}
}
\subfloat[Epoch 500 ()]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch500_200.png}
}
\subfloat[Epoch 1000 ()]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch1000_200.png}
}
\end{center}
\caption{Visualization of the image embeddings of MiCE on CIFAR-10 with t-SNE. Different colors denote the different ground-truth class labels (unknown to the model). The cluster ACC is shown in the parenthesis. MiCE learns a clear cluster structure that matches the latent semantics well. Best view in color.
}
\label{fig:tSNE_mice_true_label}
\end{figure}

{\bf Visualization of the learned embeddings.} We visualize the image embeddings produced by the gating network using t-SNE~\citep{maaten2008visualizing} in Fig.~\ref{fig:tSNE_mice_true_label}. Different colors denote the different ground-truth class labels. At the beginning, the embeddings from distinct classes are indistinguishable. MiCE progressively refines its estimates and ends up with embeddings that show a clear cluster structure. The learned clusters align with the ground-truth semantics well, which verifies the effectiveness of our method. Additional visualizations and the comparisons with MoCo are in Appendix~\ref{appendix:additional_exp}.

































\subsection{Ablation Studies}
{\bf Simplified model (Tab.~\ref{tab.ablation} (left)).} We investigate the gating function and the unnormalized model to understand the contributions of different components. Using a simpler latent variable model often deteriorates the performance.  (1) With a uniform prior, the experts would take extra efforts to become specialized in a set of images with shared semantics. 
(2 \& 3) The teacher embedding  is pushed to be close to all expert prototypes at the same time. It may be difficult for the simplified expert to encode the latent semantics while being discriminative. (4) The performance drop shows that the class-level information is essential for the image embeddings to capture the semantic structures of the dataset, despite the learned representations are still discriminative between instances. Without the term, the learned embeddings are mixed up and scattered over the embedding space without a clear cluster structure.



\begin{table}[t]
    \centering
    \caption{Ablations of MiCE on the probabilistic model (left) and different ways of learning the gating and expert prototypes (right). Each row shows the ACC (\%) on CIFAR-100 when applying the single change to MiCE. The assumptions are detailed in the Appendix~\ref{appendix_assumption}.}
{
    \begin{subtable}[h]{0.47\textwidth}
    \centering
    \resizebox{1.0\columnwidth}!
        {
\begin{tabular}{@{}lc@{}}
\toprule
\multicolumn{1}{c}{} & CIFAR-100  \\ \midrule
(1) A3 ( )            &  40.7    \\
(2) A4 (Single output layer)        & 39.3 \\
(3) A3 + A4                         & 33.6 \\
(4) A5 (Discard class-level information)                  & 17.0 \\ \midrule
\multicolumn{1}{c}{MiCE (Ours)}   & 42.2 \\ \bottomrule
\end{tabular}
}
\end{subtable}}\quad
    \begin{subtable}[h]{0.49\textwidth}
    \centering
\resizebox{1.0\columnwidth}!
{
\begin{tabular}{@{}lc@{}}
\toprule
\multicolumn{1}{c}{}                & CIFAR-100    \\ \midrule
(a) No analytical update on  in  Eq.~(\ref{expert_update}) & 21.3        \\
(b) No gradient update on    & 41.0 \\
(c) Initialize  with a uniform distribution       & 41.0 \
    &  \log p(\mathbf{Y}|\mathbf{X}; \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}) \\
&= 
    \mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}
    \left[
    \log
        \frac{p(\mathbf{Y}, \mathbf{Z} | \mathbf{X}; \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu})}
        {q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}
    \right]
    +
    D_\text{KL}(q(\mathbf{Z} | \mathbf{X}, \mathbf{Y}) \| p(\mathbf{Z} |  \mathbf{X}, \mathbf{Y};\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}) \\
&\ge 
\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{X}, \mathbf{Y})\\
&:=\mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}
    \left[
    \log
        \frac{p(\mathbf{Y}, \mathbf{Z} | \mathbf{X}; \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu})}
        {q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}
    \right]
\\
&= 
\mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}
    \left[
    \log
    p(\mathbf{Y}, \mathbf{Z} | \mathbf{X}; \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu})
    \right]
- \mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})}\left[         \log q(\mathbf{Z} | \mathbf{X}, \mathbf{Y})
\right]\\
&= 
\sum_{n=1}^N
    \sum_{k=1}^K
    q(z_n=k|\mathbf{x}_n, y_n)
    \left[
    \log
    p(z_n=k| \mathbf{x}_n; \psi)
    +
    \log
    p(y_n |  \mathbf{x}_n, z_n=k; \boldsymbol{\theta}, \boldsymbol{\mu})
    - \log q(z_n=k| \mathbf{x}_n, y_n)
    \right].

    \hat{q}(z_n | \mathbf{x}_n, y_n)=\left\{\begin{array}{ll}
1, & \text { if } z_n =\underset{k}{\argmax } \quad q(k | \mathbf{x}_n, y_n), \\
0, & \text { otherwise. }
\end{array}\right.

\boldsymbol{\mu}_{k} &\leftarrow 
\argmax_{\boldsymbol{\mu}_{k}} \mathcal{L}( \boldsymbol{\mu}_k; \mathbf{X}, \mathbf{Y})\\
&= 
    \argmax_{\boldsymbol{\mu}_k} 
        \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left[
    \log
    p(k | \mathbf{x}_n)
    +
    \log
    p(y_n |  \mathbf{x}_n, k; \boldsymbol{\mu}_k)
    - \log \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \right] \\
&= 
\argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left[
    \log
    p(y_n |  \mathbf{x}_n, k;  \boldsymbol{\mu}_k)
    \right] 
    \\
&=
\argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left[
    \log
    \frac{\Phi( \mathbf{x}_n, y_n, k) }{ {Z}(\mathbf{x}_n, k)}
    \right] \\
&=
\argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left[
    \log
    \Phi( \mathbf{x}_n, y_n, k)
    \right] \tag{Assumption A2} \\
&=
\argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left[
    \log
\exp\left(  
        \mathbf{v}^{\top}_{y_n, k}
        \mathbf{f}_{n, k} / \tau + \mathbf{v}^{\top}_{y_n, k} \boldsymbol{\mu}_{k}  / \tau
        \right)    \right]  \\
&=
    \argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
    \left(
        \mathbf{v}^{\top}_{y_n, k}
        \mathbf{f}_{n,k} / \tau + \mathbf{v}^{\top}_{y_n, k} \boldsymbol{\mu}_{k}  / \tau
          \right)   \\
&= 
\argmax_{\boldsymbol{\mu}_k} 
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}^{\top}_{y_n, k} \boldsymbol{\mu}_{k}  / \tau, 
    \\

    \argmax_{\boldsymbol{\mu}_k}  \quad
    \lambda ( 1 - \boldsymbol{\mu}_{k}^{\top} \boldsymbol{\mu}_{k}) +
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}^{\top}_{y_n, k} \boldsymbol{\mu}_{k}  / \tau
\label{eqn:argmax_lagrangian}

   & \nabla_{\boldsymbol{\mu}_k} \quad \lambda ( 1 - \boldsymbol{\mu}_{k}^{\top} \boldsymbol{\mu}_{k}) + 
    \sum_{n=1}^N
        \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}^{\top}_{y_n, k} \boldsymbol{\mu}_{k}  / \tau
\\
&= -2 \lambda \boldsymbol{\mu}_{k} +     \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}_{y_n, k}  / \tau = 0, 

     \boldsymbol{\mu}_{k} = \frac{
     \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}_{y_n, k}}{2 \lambda \tau
} 
:= \frac{ \hat{\boldsymbol{\mu}}_{k} }{2 \lambda \tau},
\label{eqn:mu}

    {\boldsymbol{\mu}_{k}}^{\top}     {\boldsymbol{\mu}_{k}} = 1.
    \label{eqn:lambda}

1
    &=
    \frac{1}{4 \tau^2 \lambda^2}
    \hat{\boldsymbol{\mu}}_{k}^{\top} \hat{\boldsymbol{\mu}}_{k} = 
    \frac{1}{4 \tau^2 \lambda^2}
    \|\hat{\boldsymbol{\mu}}_{k}\|^2.

&{\lambda}= \frac{\|\hat{\boldsymbol{\mu}}_{k}\|}{2 \tau} , \\
&{\boldsymbol{\mu}}_k = 
\frac{\hat{\boldsymbol{\mu}}_{k}}{ 2 \tau {\lambda}}
= 
\frac{ \hat{\boldsymbol{\mu}}_{k}}{ \|\hat{\boldsymbol{\mu}}_{k}\|}. 

    &\widetilde{\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{x}_n, y_n) \\
&=    
\mathbb{E}_{q(z_n | \mathbf{x}_n, y_n)
    }
\left[
    \log
        \frac{\Phi\left( \mathbf{x}_{n}, y_n,  z_n; \boldsymbol{\theta}, \boldsymbol{\mu} \right)}
    {
        \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    } 
    \right] \tag*{ }
    - D_\text{KL} (
    q(z_n | \mathbf{x}_n, y_n) \|  p(z_n | \mathbf{x}_n; \boldsymbol{\psi})  ) \\
&=
\mathbb{E}_{q(z_n | \mathbf{x}_n, y_n)
    }
\left[
    \log
        \frac{\Phi\left( \mathbf{x}_{n}, y_n,  z_n; \boldsymbol{\theta}, \boldsymbol{\mu} \right)}
    {
        {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    } 
    +     \log
        \frac{
        {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        }
    {
        \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    } 
    \right] \tag*{ } 
    - D_\text{KL} (
    q(z_n | \mathbf{x}_n, y_n) \|  p(z_n | \mathbf{x}_n; \boldsymbol{\psi})  ) \\
&=
  {\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{x}_n, y_n)  
  + \mathbb{E}_{q(z_n | \mathbf{x}_n, y_n)
    }
\left[
  \log
        \frac{
        {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        }
    {
        \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    } 
    \right] \tag*{ }.

    Z(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        = \sum_{i=1}^N \exp\left(  
    \mathbf{v}^{\top}_{y_i, z_n}
    \left(
    \mathbf{f}_{n, z_n} + \boldsymbol{\mu}_{z_n} \right) / \tau\right),

    \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        = \sum_{j = 1}^{\nu}
     \exp\left(  
\mathbf{q}_{j, z_n}^{\top}
    \left(
    \mathbf{f}_{n, z_n} + \boldsymbol{\mu}_{z_n} \right) / \tau\right) \tag*{As in Eq.~(\ref{approx_z})}, 

    \frac{-2}{\tau} \le   
    \mathbf{v}^{\top}_{y_i, z_n}
    \left(
    \mathbf{f}_{n, z_n} + \boldsymbol{\mu}_{z_n} \right) / \tau \le \frac{2}{\tau},

     {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu}) \le N \exp({\frac{2}{\tau}}), 

         \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu}) \ge \nu \exp({\frac{-2}{\tau}}).

    \log
        \frac{
        {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        }
    {
        \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    }  \le \log N - \log \nu + \frac{4}{\tau}

\widetilde{\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{x}_n, y_n) 
&=
    {\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{x}_n, y_n)  
  + \mathbb{E}_{q(z_n | \mathbf{x}_n, y_n)
    }
\left[
  \log
        \frac{
        {Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
        }
    {
        \hat{Z}(\mathbf{x}_n, z_n; \boldsymbol{\theta}, \boldsymbol{\mu})
    } 
    \right] \\
&\le
    \log p(y_n | \mathbf{x}_n; \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu} ) + \log N - \log \nu + \frac{4}{\tau}.

    p( z_n | \mathbf{x}_n) = \frac{1}{K}, \quad \forall n.

        \Phi(\mathbf{x}_n, y_n, k)
&=    
\exp\left(  
    \mathbf{v}^{\top}_{y_n}
    (\mathbf{f}_{n} + \boldsymbol{\mu}_k)  / \tau\right),

        \Phi(\mathbf{x}_n, y_n, z_n)
&=    
\exp\left(  
    \mathbf{v}^{\top}_{y_n, z_n}
    \mathbf{f}_{n, z_n}  / \tau\right).

   p(y_n| \mathbf{x}_n) = \frac{
    \exp (\mathbf{v}^{\top}_{y_n}
    \mathbf{f}_{n} / \tau)
    }{
        \sum_{i=1}^N \exp (\mathbf{v}^{\top}_{i} \mathbf{f}_{n} / \tau)
    }. 

     p(y_n | \mathbf{x}_n, z_n) 
&= 
    \frac{\Phi(\mathbf{x}_n, y_n, z_n)}{Z(\mathbf{x}_n, z_n)} 
=
\frac{
    \exp\left(  
    \mathbf{v}^{\top}_{y_n}
    (\mathbf{f}_{n} + \boldsymbol{\mu}_{z_n})  / \tau\right)
    }{
        \sum_{i=1}^N \exp\left(  
    \mathbf{v}^{\top}_{i}
    (\mathbf{f}_{n} + \boldsymbol{\mu}_{z_n})  / \tau\right)
    }
    \qquad \tag{Assumption A4} \\
&= 
\frac{
    \exp (\mathbf{v}^{\top}_{y_n}
    \mathbf{f}_{n} / \tau)
    }{
        \sum_{i=1}^N \exp (\mathbf{v}^{\top}_{i} \mathbf{f}_{n} / \tau)
    }. \qquad \tag{Assumption A5}

p(y_n| \mathbf{x}_n)
&= \sum_{k=1}^K  p(z_n = k | \mathbf{x}_n) p(y_n | \mathbf{x}_n, z_n = k) \\
&= \sum_{k=1}^K \frac{1}{K} p(y_n | \mathbf{x}_n, z_n = k) \qquad \tag{Assumption A3} \\
&= 
\frac{
    \exp (\mathbf{v}^{\top}_{y_n}
    \mathbf{f}_{n} / \tau)
    }{
        \sum_{i=1}^N \exp (\mathbf{v}^{\top}_{i} \mathbf{f}_{n} / \tau)
    }.

    p(z_n | \mathbf{x}_n, y_n) 
&=     
    \frac{
    p(z_n | \mathbf{x}_n )
    {\Phi\left(\mathbf{x}_{n}, y_n,  z_n \right)} /
    {
        {Z}(\mathbf{x}_n, z_n)
        } 
    }
    { 
    \sum_{k=1}^K
    p(k | \mathbf{x}_n)
    {
        \Phi\left(  \mathbf{x}_{n}, y_n,  k\right)
    } /
    {
        {Z}(\mathbf{x}_n, k)}
    }\\&=
    \frac{
    {\Phi\left( \mathbf{x}_{n}, y_n,  z_n \right)} /
    {
        {Z}(\mathbf{x}_n, z_n)
        } 
    }
    { 
    \sum_{k=1}^K
    {
        \Phi\left( \mathbf{x}_{n}, y_n,  k\right)
    } /
    {
        {Z}(\mathbf{x}_n, k)}
    } \tag{Assumption A3} \\
&=
    \frac{
\frac{
    \exp (\mathbf{v}^{\top}_{y_n}
    \mathbf{f}_{n} / \tau)
    }{
        \sum_{i=1}^N \exp (\mathbf{v}^{\top}_{i} \mathbf{f}_{n} / \tau)
    } 
    }
    { 
    \sum_{k=1}^K
    \frac{
    \exp (\mathbf{v}^{\top}_{y_n}
    \mathbf{f}_{n} / \tau)
    }{
        \sum_{i=1}^N \exp (\mathbf{v}^{\top}_{i} \mathbf{f}_{n} / \tau)
    }
    } \tag{Theorem 2} \\
&= \frac{1}{K}.

    \widetilde{\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\mu}; \mathbf{x}_n, y_n)
&=    
\mathbb{E}_{q(z_n | \mathbf{x}_n, y_n)
    }
[
    \log
        \frac{\Phi\left(  \mathbf{x}_{n}, y_n, z_n \right)}
    {
        \hat{Z}(\mathbf{x}_n, z_n)
    } 
    ] 
    - D_\text{KL} (
    q(z_n | \mathbf{x}_n, y_n) \|  p(z_n | \mathbf{x}_n) \tag*{ }\\
&=
        \log
        \frac{\Phi\left( \mathbf{x}_{n}, y_n, z_n\right)}
    {
        \hat{Z}(\mathbf{x}_n,z_n)
    } \tag{Lemma 1} \\
&=
    \log \frac{\exp \left(   \mathbf{v}_{y_n}^{\top} \mathbf{f}_n / \tau\right)  }{\exp \left(   \mathbf{v}_{y_n}^{\top} \mathbf{f}_n / \tau\right) +  \sum_{i =1}^{\nu} \exp \left(\mathbf{q}_i^{\top} \mathbf{f}_n / \tau\right)}.
    \tag{Theorem 2} 

    q (z_n | \mathbf{x}_n, y_n)
&=  
\frac
    {\Phi\left( \mathbf{x}_{n}, y_n,  z_n \right)}
    {
        \sum_{k=1}^K
        \Phi\left( \mathbf{x}_{n}, y_n,  k\right)
    }  \tag{Assumptions A2 and A3} \\
&=
    \frac
    { \exp\left(  
    \mathbf{v}^{\top}_{y_{n, z_n}}
    \mathbf{f}_{n, z_n} / \tau + \mathbf{v}^{\top}_{y_{n, z_n}} \boldsymbol{\mu}_{z_n}  / \tau
    \right)}
    { 
            \sum_{k=1}^K
        \exp\left(  
        \mathbf{v}^{\top}_{y_{n, k}}
        \mathbf{f}_{n, k} / \tau + \mathbf{v}^{\top}_{y_{n, k}} \boldsymbol{\mu}_{k}  / \tau
        \right)} \\&=
    \frac
    { \exp\left(  
    \mathbf{v}^{\top}_{y_n}
    \mathbf{f}_n / \tau + \mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_{z_n}  / \tau
    \right)}
    { 
            \sum_{k=1}^K
        \exp\left(  
        \mathbf{v}^{\top}_{y_n}
        \mathbf{f}_n / \tau + \mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_{k}  / \tau
        \right)} \tag{Assumption A4}\\
&=
    \frac
    { \exp\left(  
 \mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_{z_n}  / \tau
    \right)}
    { 
            \sum_{k=1}^K
        \exp\left(  
    \mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_{k}  / \tau
        \right)},

    \hat{q}(z_n | \mathbf{x}_n, y_n)=\left\{\begin{array}{ll}
1, & \text { if } z_n =\underset{k}{\argmax } \quad q(k | \mathbf{x}_n, y_n) = \underset{k}{\argmax }  \quad \mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_k, \\
0, & \text { otherwise. }
\end{array}\right.

    \argmax_{\boldsymbol{\mu}_k}  \quad
    \lambda ( 1 - \boldsymbol{\mu}_{k}^{\top} \boldsymbol{\mu}_{k}) +
    \sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}^{\top}_{y_n} \boldsymbol{\mu}_{k}  / \tau.

&\hat{\boldsymbol{\mu}}_{k} := 
\sum_{n=1}^N
    \hat{q}(z_n=k | \mathbf{x}_n, y_n)
\mathbf{v}_{y_n}, \\
&{\boldsymbol{\mu}}_k 
= 
\frac{ \hat{\boldsymbol{\mu}}_{k}}{ \|\hat{\boldsymbol{\mu}}_{k}\|},

where the cluster assignment step and prototype update rule are the same as the spherical -means.



\end{proof}




























































































































\section{Experiement settings}\label{sec:detail_exp}


We mainly compare with the methods that are trained from scratch without using the pre-training model and the experiment settings follow the literature closely~\citep{chang2017deep,wu2019deep,ji2019invariant,shiran2019multi,darlow2020dhog}.
For CIFAR-10, CIFAR-100, and STL-10, all the training and test images are jointly utilized, and the  superclasses of CIFAR-100 are used instead of the fine labels. The  classes of dog images are selected from the ILSVRC2012 1K~\citep{deng2009imagenet} dataset and resized to  to form the ImageNet-Dog dataset~\citep{chang2017deep,wu2019deep}. Note that the numbers of the clusters are known in advance as in ~\citet{chang2017deep,ji2019invariant,wu2019deep,shiran2019multi}. The statistics of the datasets are summarized in Tab.~\ref{data_stat}. We adopt three common metrics to evaluate the clustering performance, namely normalized mutual information (NMI), cluster accuracy (ACC), and adjusted rand index (ARI). All the metrics are presented in percentage (\%).   




Regarding the network architecture, MiCE mainly use a ResNet-34~\citep{he2016deep} as the backbone following the recent methods~\citep{ji2019invariant,shiran2019multi} for fair comparisons. For the gating network, the output layer is replaced by a single fully connected layer that generates a -normalized embeddings in . As for the student network, it uses the same ResNet-34 backbone as the gating network and includes  more fully connected layers which map the images to  embeddings. Therefore, the parameters of student and gating networks are shared except for the output layers. The teacher network  is the exponential moving average (EMA) version of the student network , which stabilizes the learning process~\citep{he2019momentum,tarvainen2017mean}. The update rule follows  with  being the smoothing coefficient. In practice, we let  following MoCo. Since the images of CIFAR-10 and CIFAR-100 are smaller than ImageNet images, following~\citep{chen2020simple}, we replace the first 7x7 Conv of stride 2 with a 3x3 Conv of stride 1 for all experiments on CIFAR-10 and CIFAR-100. The first max-pooling operation is removed as well~\citep{wu2018unsupervised,chen2020simple,ye2019unsupervised}. Please kindly note that if the first max-pooling operation is not removed, MiCE can still achieve 83.6\% ACC on CIFAR-10. For fair comparisons, MoCo also uses a ResNet-34 with a -dimensional output and follows the same hyper-parameter settings as MiCE.

As it is often infeasible to tune the hyper-parameters with a validation dataset in real-world clustering tasks~\citep{ghasedi2017deep}, we set both temperatures  and  as 1.0. The queue size  is set to  for STL-10 because of the smaller data size and  for the other three datasets.
The data augmentation follows MoCo closely. Specifically, before passing into any of the embedding networks, images are randomly resized and cropped to the same size, followed by random gray-scale, random color jittering, and random horizontal flip. For a fair comparison, MoCo in the two-stage baseline also uses a ResNet-34 backbone. For all datasets, we use a batch size equals to 256. Note that the data augmentation strategy is critical to contrastive learning methods and MiCE, and we follow the one used by MoCo for fairness. 




In terms of the optimization details, we use stochastic gradient descent (SGD) as our optimizer on the negative ELBO. We set the SGD weight decay as  and the SGD momentum as ~\citep{he2019momentum}. The learning rate is initiated as  and is multiplied by  at three different epochs. For different datasets, the number of training epochs is different to accommodate the data size to have a similar and reasonable training time. For CIFAR-10/100, we train for  epochs in total and multiply the learning rate by  at , , and  epochs. For STL-10, the total epochs are  and the learning rate is multiplied by  at , , and  epochs. Lastly, for ImageNet-Dog, the total epochs are  and the learning rate is multiplied by  at , , and  epochs. Also, the learning rate for expert prototypes is the same as the one for network parameters. All the experiments are trained on a single GPU.


For all experiment settings in the main text, we follow the recent methods~\citep{wu2019deep,ji2019invariant,shiran2019multi} closely where the models are trained from scratch. The setting is different from some of the methods including VaDE~\citep{jiang2016variational}, DGG~\citep{yang2019deep}, and LTVAE~\citep{li2019learning} from two aspects. Firstly, we do not use any form of the pre-trained model. Secondly, we focus on a purely unsupervised setting. In contrast, VaDE~\citep{jiang2016variational} and DGG~\citep{yang2019deep} use a supervised pre-trained model on ImageNet for STL-10. For fairness, in the original submission, we compare to many previous methods that use the same settings.  




















































\section{Additional experiments and visualizations}~\label{appendix:additional_exp}



\begin{figure}[]
\begin{center}
\includegraphics[width=0.49\columnwidth]{all_prob_hist_posterior.pdf}
\includegraphics[width=0.49\columnwidth]{hist_cluster_label_count.pdf}
\end{center}
\caption{The histogram of (left) approximate posterior distributions of MiCE at the initial and final stage of training and (right) the predicted cluster labels obtained during testing. Here, we train and evaluate the model using CIFAR-10 which has 10 classes and 60,000 images. 
We divide the probability distribution into 10 discrete bins. Best view in color.}
\label{fig:hist}
\end{figure}


{\bf Posterior distribution and cluster predictions.} From the left side of Fig.~\ref{fig:hist}, we can see that in the initial stage (epoch 1), MiCE is yet certain about the cluster labels of any give images. At the end of the training (epoch 1000), the major values of the approximated posterior distribution fall in the  interval, indicating that the model is confident that images do not belong to those clusters. After training, the learned model is able to generate sparse posterior distributions. The predicted cluster labels are also balanced across different clusters, which is shown on the right side of Fig.~\ref{fig:hist}.








{\bf Visualization of embeddings of MiCE and MoCo.} 
We present the t-SNE visualization of the embeddings learned by MiCE and MoCo in Fig.~\ref{fig:tSNE_mice_cluster_label_full} and Fig.~\ref{fig:tSNE_mice_true_label_full} to investigate whether the cluster structure and the latent semantics are captured by the models. The two figures differ in the way we color the datapoints. 

In Fig.~\ref{fig:tSNE_mice_cluster_label_full}, different colors represent different cluster labels predicted by the clustering methods. We get the predicted cluster labels of MiCE based on the hard assignments using the posterior distributions. The cluster labels for MoCo are the outputs of the spherical -means algorithm. MiCE can learn a distinct structure at the end of the training where each expert would mainly be responsible for one cluster. By comparing Fig.~\ref{fig:tSNE_mice_cluster_label_full} (c) and (f), we can see that the boundaries between the clusters learned by spherical -means do not match the structure learned by MoCo well. The divergence is caused by the independence of representation learning and clustering. MiCE solves the issue with a unified framework.



In Fig.~\ref{fig:tSNE_mice_true_label_full}, the datapoints are colored according to the ground-truth class labels that are unknown to the models. In Fig.~\ref{fig:tSNE_mice_true_label_full}(c), the cluster structure is highly aligned with the underlying semantics. Most of the clusters are filled with images from the same classes while having some difficult ones lying mostly on the boundaries. This verifies that the gating network learns to divide the dataset based on the latent semantics and allocates each of the images to one or a few experts. Please kindly note that we use the embeddings from the gating network instead of the student network for simplicity since the embeddings are from the same output head. In contrast, the cluster structure learned by MoCo does not align the semantics well. For all the above t-SNE visualizations, we set the perplexity hyper-parameter to . 










\begin{figure}[]
\begin{center}
\subfloat[MiCE (epoch 1)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch0_200_clusterLabel.png}
}
\subfloat[MiCE (epoch 500)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch500_200_clusterLabel.png}
}
\subfloat[MiCE (epoch 1000)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch1000_200_clusterLabel.png}
} \\
\subfloat[MoCo (epoch 1)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch0_200_clusterLabel.png}
}
\subfloat[MoCo (epoch 500)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch500_200_clusterLabel.png}
}
\subfloat[MoCo (epoch 1000)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch1000_200_clusterLabel.png}
}
\end{center}
\caption{
Visualization of the image embeddings of MiCE (upper row) and MoCo (lower row) on
CIFAR-10 with t-SNE. Different colors correspond to various \textbf{cluster labels} obtained based on the
posterior distribution (MiCE) or spherical k-means (MoCo). The embeddings of MiCE depict a clear
cluster structure, as shown in (c). In contrast, the structure in (f) is ambiguous for a large portion of
data and it does not match the cluster labels well.
}
\label{fig:tSNE_mice_cluster_label_full}
\end{figure}





\begin{figure}[]
\begin{center}
\subfloat[MiCE (epoch 1)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch0_200.png}
}
\subfloat[MiCE (epoch 500)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch500_200.png}
}
\subfloat[MiCE (epoch 1000)]{
    \includegraphics[width=0.32\columnwidth]{tsne_comi_gating_epoch1000_200.png}
} \\
\subfloat[MoCo (epoch 1)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch0_200.png}
}
\subfloat[MoCo (epoch 500)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch500_200.png}
}
\subfloat[MoCo (epoch 1000)]{
    \includegraphics[width=0.32\columnwidth]{tsne_moco_gating_epoch1000_200.png}
}
\end{center}
\caption{Visualization of the image embeddings of MiCE (upper row) and MoCo (lower row) on CIFAR-10 with t-SNE. Different colors denote the different \textbf{ground-truth class labels } (unknown to the model). Comparing to MoCo, the clusters learned by MiCE better correspond with the underlying class semantics.
}
\label{fig:tSNE_mice_true_label_full}
\end{figure}






{\bf Training time.} We present the training time of MiCE and MoCO on CIFAR-10.  It takes around 17 and 30 hours to train MoCo and MiCE for 1000 epochs, respectively. For all four datasets, experiments are conducted on a single GPU (NVIDIA GeForce GTX 1080 Ti).







{\bf Extra ablation studies on updating expert prototypes.} A bad specification of the expert prototypes can lead to a bad result if it is not properly handled. Specifically, in Tab.~(\ref{tab.ablation}) (a), we see that the ACC on CIFAR-10 is only 21.3\%. We empirically verify two principled methods that solve the issue: (1) extra end-of-epoch training only on  with stochastic gradient ascent or (2) using Eq.~(\ref{expert_update}).


The first method is used as follows. At the end of each epoch, we update  while fixing the network parameters until the pre-defined convergence criteria are met. The convergence can be determined based on either the norm of the prototypes or the change of the objective function. 
To control the training time, we stop the update at the current epoch once we iterate through the entire dataset 20 times. We discover that with additional training, we can achieve  ACC. The result shows that with a proper update on , the proposed model can identify semantic clusters with stochastic gradient ascent alone. 
However, it requires more than 10 times of training time (around  hours).

The above discussions manifest the benefits of using the Eq.~(\ref{expert_update}) which is derived based on the same objective function. With the prototypical update, we can achieve similar results with minimal computational overhead. On average, we can achieve  ACC on CIFAR-100 as shown in Sec.~\ref{sec:experiments_start}. 






\begin{table}[]
\centering
\caption{Comparing the cluster accuracy ACC (\%) of SCAN~\citep{van2020scan} and MiCE on CIFAR-10. Following SCAN, we show the data augmentation strategy in the parenthesis if it is different from the one MiCE and MoCo use. “SimCLR” indicates the augmentation strategy used in~\citep{chen2020simple}, and “RA” is the RandAugment~\citep{cubuk2020randaugment}.  For a fair comparison, the first sector compares MiCE to SCAN without the self-labeling step, and the second sector compares SCAN with self-labeling to MiCE with pre-training. }
\label{tab:compare_scan}
\begin{tabular}{@{}lc@{}}
\toprule
Methods/Dataset                             & CIFAR-10      \\ \midrule
SCAN-Loss (SimCLR)                          & 78.7          \\
SCAN-Loss (RA)                              & 81.8          \\
MiCE (\textbf{Ours})                                 & \textbf{83.4} \\ \midrule
SCAN-Loss (SimCLR) + Self-Labeling (SimCLR) & 10.0          \\
SCAN-Loss (SimCLR) + Self-Labeling (RA)     & 87.4          \\
SCAN-Loss (RA) + Self-Labeling (RA)         & 87.6          \\
MiCE pre-training + MiCE (\textbf{Ours})             & 89.3          \\
MiCE pre-training + MiCE (RA) (\textbf{Ours})        & 88.3          \\
MiCE pre-training + MiCE (SimCLR) (\textbf{Ours})    & \textbf{90.3} \\ \bottomrule
\end{tabular}
\end{table}


{\bf Comparing MiCE to SCAN~\citep{van2020scan}.}
We provide additional experiment results to compare with SCAN~\citep{van2020scan} that uses unsupervised pre-training under a comparable experiment setting. As SCAN adopts a three-step training procedure, we think that it will provide additional insights by comparing MiCE to SCAN at the steps after the pre-training step. The detail results on CIFAR-10 are presented in Tab.~\ref{tab:compare_scan}.

Firstly, we focus on SCAN with two steps of training. MiCE outperforms SCAN with two steps of training. SCAN obtains 78.7\% and 81.8\% on CIFAR-10 with the SimCLR augmentation and RandAugment, respectively. In contrast, MiCE can get 83.4\% despite we are using a weaker augmentation strategy following MoCo~\citep{he2019momentum} and InstDisc~\citep{wu2018unsupervised}. 

Since SCAN involves pre-training using SimCLR~\citep{chen2020simple}, it takes additional advantages when directly comparing to other methods without pre-training. Thus, we fine-tune the MiCE model with the same training protocol described in Appendix~\ref{sec:detail_exp} (except the learning rate can be smaller). We discover that MiCE can obtain higher results and outperforms SCAN, as shown in the second sector in Tab.~\ref{tab:compare_scan}. To be specific, in the fine-tuning stage, we load the network parameters from the pre-trained model and a smaller initial learning rate of 0.1 with all the other settings remain the same. We show the augmentation strategy in the parenthesis if we use a different one from the pre-training stage. For the RandAugment~\citep{cubuk2020randaugment}, we follow the implementation (and hyper-parameters) based on the public code provided by SCAN. As mentioned in~\citet{van2020scan}, the self-labeling stage requires a shift in the augmentation, otherwise, it will lead to a \textit{degenerated solution}. In contrast, MiCE with pre-training is \textit{not prone to degeneracy can get comparable or better performance than SCAN regardless of the choice of the augmentation strategy}.












\section{Additional discussions}

{\bf The number of the experts.} In the cases where the number of the experts L differs from the number of ground-truth clusters K, the model will partition the datasets into L subsets instead of K. Even though the number of experts is currently tied with K, it is not a drawback and does not prevent us from applying to the common clustering settings. Also, MiCE does not use additional knowledge comparing to the baseline methods. If the ground-truth K is not known, we may treat K as a hyper-parameter and decide K following the methods described in~\citet{smyth2000model,mclachlan2004finite}, which is worth investigating in the future.

{\bf Overclustering.} The overclustering technique~\citep{ji2019invariant} is orthogonal to our methods and can be applied with minor adaptations. However, it may require additional hyper-parameter tuning to ensure overclustering improves the results. From the supplementary file provided by IIC~\citep{ji2019invariant}, we see that the numbers of clusters (for overclustering) are set differently for different datasets. Despite overclustering is an interesting technique, we would like to highlight the simplicity of the current version of MiCE.  

{\bf Similarity between the two set of prototypes.}
We do not expect the two prototypes to be similar even though their dimensions are the same. In fact, they have different aims: the gating ones aim to divide the datasets into simpler subtasks for the experts, while the expert ones help the expert to solve the instance discrimination subtasks by introducing the class-level information. Therefore, the derived ELBO objective does not encourage the two sets of prototypes to be similar or maintaining a clear correspondence between them.

Empirically, we calculate the cosine similarity between any pair of  and . The absolute values we get are less than 0.25, which showed that they are not similar. If we force them to be similar during training, the performance may be negatively affected due to the lack of flexibility.


\end{document}
