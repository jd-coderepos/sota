\section{Experiments}

\subsection{Datasets}
Following the encoder-based transformers~\cite{lin2021metro,lin2021graphormer}, we train our FastMETRO with \textbf{Human3.6M}~\cite{wang2014h36m}, \textbf{UP-3D}~\cite{lassner2017up3d}, \textbf{MuCo-3DHP}~\cite{mehta2018muco}, \textbf{COCO}~\cite{lin2014coco} and \textbf{MPII}~\cite{mykhaylo2014mpii} training datasets, and evaluate the model on P2 protocol in Human3.6M.
Then, we fine-tune our model with \textbf{3DPW}~\cite{marcard20183dpw} training dataset, and evaluate the model on its test dataset.

\begin{table}[t!]
    \centering
    \caption{
        Comparison with transformers for monocular 3D human mesh recovery on Human3.6M~\cite{wang2014h36m}.
         and  indicate training for 60 epochs and 200 epochs, respectively.
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccccccc}
        \hline
        \multicolumn{1}{c}{}
            & \multicolumn{2}{c}{\,CNN Backbone\,}
            & \multicolumn{1}{c}{\;}
            & \multicolumn{2}{c}{\,Transformer\,}
            & \multicolumn{1}{c}{\;}
            & \multicolumn{2}{c}{\,Overall\,}
            & \multicolumn{1}{c}{\;}  
        \\
        \cline{2-3}
        \cline{5-6}
        \cline{8-9}
        Model
            & \#Params
            & \,Time\,(ms) 
            & 
            & \#Params
            & \,Time\,(ms)
            & 
            & \#Params
            & \,FPS\,
            & \,PA-MPJPE 
        \\
        \hline
        \hline
            METRO--R50~\cite{lin2021metro}
            & 23.5M 
            & 7.5 
            & 
            & 102.3M 
            & 24.2 
            &
            & 125.8M 
            & 31.5 
            & 40.6
        \\
            METRO--H64~\cite{lin2021metro}
            & 128.1M 
            & 49.0 
            &
            & 102.3M 
            & 24.2 
            &
            & 230.4M 
            & 13.7 
            & 38.0
        \\
            METRO--H64~\cite{lin2021metro}
            & 128.1M 
            & 49.0 
            &
            & 102.3M 
            & 24.2 
            &
            & 230.4M 
            & 13.7 
            & 36.7
        \\
            MeshGraphormer--H64~\cite{lin2021graphormer}
            & 128.1M 
            & 49.0 
            &
            & 98.4M 
            & 24.5
            &
            & 226.5M 
            & 13.6 
            & 35.8
        \\
            MeshGraphormer--H64~\cite{lin2021graphormer}
            & 128.1M 
            & 49.0 
            &
            & 98.4M 
            & 24.5 
            &
            & 226.5M 
            & 13.6 
            & 34.5
        \\
        \hhline{-|-|-|-|-|-|-|-|-|-|}
            \cellcolor{gray!7.5}\textbf{FastMETRO--S--R50}
            & \cellcolor{gray!7.5}23.5M 
            & \cellcolor{gray!7.5}7.5 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}\textbf{9.2M} 
            & \cellcolor{gray!7.5}\textbf{9.6} 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}\textbf{32.7M} 
            & \cellcolor{gray!7.5}\textbf{58.5} 
            & \cellcolor{gray!7.5}39.4
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--M--R50}
            & \cellcolor{gray!7.5}23.5M 
            & \cellcolor{gray!7.5}7.5 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}17.1M 
            & \cellcolor{gray!7.5}15.0 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}40.6M 
            & \cellcolor{gray!7.5}44.4 
            & \cellcolor{gray!7.5}38.6
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--L--R50}
            & \cellcolor{gray!7.5}23.5M 
            & \cellcolor{gray!7.5}7.5 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}24.9M 
            & \cellcolor{gray!7.5}20.8 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}48.4M 
            & \cellcolor{gray!7.5}35.3 
            & \cellcolor{gray!7.5}37.3
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--L--H64}
            & \cellcolor{gray!7.5}128.1M 
            & \cellcolor{gray!7.5}49.0 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}24.9M 
            & \cellcolor{gray!7.5}20.8 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}153.0M 
            & \cellcolor{gray!7.5}14.3 
            & \cellcolor{gray!7.5}\textbf{33.7}
        \\
        \hline
    \end{tabular}}
    \label{table:compare_trasnformer}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{arXiv_Figures/convergence_15.pdf}
    \caption{
    Comparison with encoder-based transformers~\cite{lin2021metro,lin2021graphormer} and our proposed models on Human3.6M~\cite{wang2014h36m}.
    The small variant of our FastMETRO shows much faster inference speed, and its large variant converges faster than the transformer encoders.
    }
    \label{fig:convergence}
\end{figure}

Following the common practice~\cite{choi2020pose2mesh,Moon_2020_ECCV_I2L-MeshNet,lin2021metro,lin2021graphormer}, we employ the pseudo 3D human mesh obtained by \mbox{SMPLify-X}~\cite{pavlakos_2019_smplify_x} to train our model with Human3.6M~\cite{wang2014h36m};
there is no available ground-truth 3D human mesh in the Human3.6M training dataset due to the license issue.
For fair comparison, we employ the ground-truth 3D human body joints in Human3.6M during the evaluation of our model.
Regarding the experiments on 3DPW~\cite{marcard20183dpw}, we use its training dataset for fine-tuning our model as in the encoder-based transformers~\cite{lin2021metro,lin2021graphormer}.

\subsection{Evaluation Metrics}
We evaluate our FastMETRO using three evaluation metrics:
MPJPE~\cite{wang2014h36m}, PA-MPJPE~\cite{zhou2018monocap}, MPVPE~\cite{pavlakos_2018_mpve}.
The unit of each metric is millimeter.

\noindent \textbf{MPJPE.}
This metric denotes Mean-Per-Joint-Position-Error.
It measures the Euclidean distances between the predicted and ground-truth joint coordinates.

\noindent \textbf{PA-MPJPE.}
This metric is often called \textit{Reconstruction Error}.
It measures MPJPE after 3D alignment using Procrustes Analysis (PA)~\cite{gower_1975_pa}.

\noindent \textbf{MPVPE.}
This metric denotes Mean-Per-Vertex-Position-Error.
It measures the Euclidean distances between the predicted and ground-truth vertex coordinates.

\begin{table}[t!]
    \centering
    \caption{
        Comparison with the state-of-the-art monocular 3D human pose and mesh recovery methods on 3DPW~\cite{marcard20183dpw} and Human3.6M~\cite{wang2014h36m} among image-based methods.
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccc}
        \hline
        \multicolumn{1}{c}{}
            & \multicolumn{3}{c}{\; 3DPW \;}
            & \multicolumn{1}{c}{\;\;}
            & \multicolumn{2}{c}{\; Human3.6M \;}  
        \\
        \cline{2-4}
        \cline{6-7}
            Model
            & \, MPVPE  \,
            & \, MPJPE  \,
            & \, PA-MPJPE  \,
            & 
            & \, MPJPE  \,
            & \, PA-MPJPE  \,
        \\
        \hline
        \hline
            HMR--R50~\cite{hmrKanazawa17}
            & -- 
            & 130.0 
            & 76.7 
            & 
            & 88.0 
            & 56.8
        \\
            GraphCMR--R50~\cite{kolotouros2019cmr}
            & -- 
            & -- 
            & 70.2 
            & 
            & -- 
            & 50.1
        \\
            SPIN--R50~\cite{kolotouros2019spin}
            & 116.4 
            & 96.9 
            & 59.2 
            & 
            & 62.5 
            & 41.1
        \\
            I2LMeshNet--R50~\cite{Moon_2020_ECCV_I2L-MeshNet}
            & -- 
            & 93.2 
            & 57.7 
            & 
            & 55.7 
            & 41.1
        \\
            PyMAF--R50~\cite{zhang2021pymaf}
            & 110.1
            & 92.8
            & 58.9
            & 
            & 57.7 
            & 40.5
        \\
            ROMP--R50~\cite{sun2021monocular}
            & 105.6
            & 89.3
            & 53.5
            & 
            & -- 
            & --
        \\
            ROMP--H32~\cite{sun2021monocular}
            & 103.1
            & 85.5
            & 53.3
            & 
            & -- 
            & --
        \\
            PARE--R50~\cite{Kocabas_PARE_2021}
            & 99.7
            & 82.9 
            & 52.3 
            & 
            & --
            & --
        \\
            METRO--R50~\cite{lin2021metro}
            & --
            & -- 
            & -- 
            & 
            & 56.5 
            & 40.6
        \\
            DSR--R50~\cite{dwivedi2021dsr}
            & 99.5
            & 85.7 
            & 51.7 
            & 
            & 60.9
            & 40.3
        \\
            METRO--H64~\cite{lin2021metro}
            & 88.2 
            & 77.1 
            & 47.9 
            & 
            & 54.0 
            & 36.7
        \\
            PARE--H32~\cite{Kocabas_PARE_2021}
            & 88.6
            & 74.5 
            & 46.5 
            & 
            & --
            & --
        \\
            MeshGraphormer--H64~\cite{lin2021graphormer}
            & 87.7 
            & 74.7 
            & 45.6 
            & 
            & \textbf{51.2} 
            & 34.5
        \\
        \hhline{-|-|-|-|-|-|-|}
            \cellcolor{gray!7.5}\textbf{FastMETRO--S--R50}
            & \cellcolor{gray!7.5}91.9 
            & \cellcolor{gray!7.5}79.6 
            & \cellcolor{gray!7.5}49.3 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}55.7 
            & \cellcolor{gray!7.5}39.4
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--M--R50}
            & \cellcolor{gray!7.5}91.2 
            & \cellcolor{gray!7.5}78.5 
            & \cellcolor{gray!7.5}48.4 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}55.1 
            & \cellcolor{gray!7.5}38.6
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--L--R50}
            & \cellcolor{gray!7.5}90.6 
            & \cellcolor{gray!7.5}77.9 
            & \cellcolor{gray!7.5}48.3 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}53.9 
            & \cellcolor{gray!7.5}37.3
        \\
            \cellcolor{gray!7.5}\textbf{FastMETRO--L--H64}
            & \cellcolor{gray!7.5}\textbf{84.1} 
            & \cellcolor{gray!7.5}\textbf{73.5} 
            & \cellcolor{gray!7.5}\textbf{44.6} 
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}52.2 
            & \cellcolor{gray!7.5}\textbf{33.7}
        \\
        \hline
    \end{tabular}}
    \label{table:previous}
\end{table}

\subsection{Experimental Results}
We evaluate the model-size variants of our FastMETRO on the 3DPW~\cite{marcard20183dpw} and Human3.6M~\cite{wang2014h36m} datasets.
In this paper, the inference time is measured using a single NVIDIA V100 GPU with a batch size of 1.

\noindent \textbf{Comparison with Encoder-Based Transformers.}
In Table~\ref{table:compare_trasnformer}, we compare our models with METRO~\cite{lin2021metro} and Mesh Graphormer~\cite{lin2021graphormer} on the Human3.6M~\cite{wang2014h36m} dataset.
Note that encoder-based transformers~\cite{lin2021metro,lin2021graphormer} are implemented with \mbox{ResNet-50}~\cite{resnet} (\textbf{R50}) or
\mbox{HRNet-W64}~\cite{wang2019hrnet} (\textbf{H64}).
\mbox{FastMETRO-S} outperforms METRO when both models employ the same CNN backbone (R50), 
although our model demands only 8.99\% of the parameters in the transformer architecture.
Regarding the \mbox{overall} inference speed, our model is 1.86 faster.
It is worth noting that 
\mbox{FastMETRO-L-R50} achieves similar results with \mbox{METRO-H64}, but our model is 2.58 faster. 
\mbox{FastMETRO-L} outperforms \mbox{Mesh Graphormer} 
when both models employ the same CNN backbone (H64), 
while our model demands only 25.30\% of the parameters in the transformer architecture.
Also, our model converges much faster than the encoder-based methods as shown in Figure~\ref{fig:convergence}.

\noindent \textbf{Comparison with Image-Based Methods.}
In Table~\ref{table:previous}, we compare our \mbox{FastMETRO} with the image-based methods for 3D human mesh reconstruction on 3DPW~\cite{marcard20183dpw} and Human3.6M~\cite{wang2014h36m}.
Note that existing methods are implemented with
\mbox{ResNet-50}~\cite{resnet} (\textbf{R50}) or
\mbox{HRNet-W32}~\cite{wang2019hrnet} (\textbf{H32}) or 
\mbox{HRNet-W64}~\cite{wang2019hrnet} (\textbf{H64}).
When all models employ R50 as their CNN backbones,
\mbox{FastMETRO-S} achieves the best results without 
iterative fitting procedures or test-time optimizations.
\mbox{FastMETRO-L-H64} achieves the state of the art in every evaluation metric on the 3DPW dataset and PA-MPJPE metric on the Human3.6M dataset.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{arXiv_Figures/qualitative_human_8.pdf}
    \caption{
    Qualitative results of our FastMETRO on Human3.6M~\cite{wang2014h36m} and 3DPW~\cite{marcard20183dpw}. 
    We visualize the 3D human mesh estimated by FastMETRO-L-H64.
    By leveraging the attention mechanism in the transformer,
    our model is robust to partial occlusions.
    }
    \label{fig:qualitative_human}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{arXiv_Figures/qualitative_self_cross_attn_v3.pdf}
    \caption{
    Qualitative results of FastMETRO-L-H64 on COCO~\cite{lin2014coco}.
    We visualize the attentions scores in \mbox{self-attentions} (top two rows) and \mbox{cross-attentions} (bottom two rows).
    The brighter lines or regions indicate higher attention scores.
    }
    \label{fig:qualitative_self_cross_attn}
\end{figure}

\noindent \textbf{Visualization of Self-Attentions.}
In Figure~\ref{fig:qualitative_self_cross_attn}, the first and second rows show the visualization of the attention scores in \mbox{self-attentions} between a specified body joint and mesh vertices.
We obtain the scores by averaging attention scores from all attention heads of all \mbox{multi-head} \mbox{self-attention} modules in our transformer decoder.
As shown in Figure~\ref{fig:qualitative_self_cross_attn}, our FastMETRO effectively captures the \mbox{non-local} relations among joints and vertices via \mbox{self-attentions} in the transformer.
This improves the robustness to environment variations such as occlusions.

\noindent \textbf{Visualization of Cross-Attentions.}
In Figure~\ref{fig:qualitative_self_cross_attn}, the third and fourth rows show the visualization of the attention scores in \mbox{cross-attentions} between a specified body joint and image regions.
We obtain the scores by averaging attention scores from all attention heads of all \mbox{multi-head} \mbox{cross-attention} modules in our transformer decoder.
As shown in Figure~\ref{fig:qualitative_self_cross_attn}, the input tokens used in our transformer decoder focus on their relevant image regions.
By leveraging the \mbox{cross-attentions} between disentangled modalities, 
our \mbox{FastMETRO} effectively learns to regress the 3D coordinates of joints and vertices from a 2D image.

\subsection{Ablation Study}
We analyze the effects of different components in our FastMETRO as shown in Table~\ref{table:ablation}.
Please refer to the supplementary material for more experiments.

\noindent \textbf{Attention Masking.}
To effectively learn the local relations among mesh vertices, \mbox{GraphCMR~\cite{kolotouros2019cmr}} and Mesh Graphormer~\cite{lin2021graphormer} perform graph convolutions based on the topology of SMPL human triangle mesh~\cite{SMPL:2015}.
For the same goal,
we mask self-attentions of non-adjacent vertices according to the topology.
When we evaluate our model without masking the attentions,
the regression accuracy drops as shown in the first row of Table~\ref{table:ablation}.
This demonstrates that masking the attentions of non-adjacent vertices is effective.
To compare the effects of attention masking with graph convolutions,
we train our model using graph convolutions without masking the attentions.
As shown in the second row of Table~\ref{table:ablation}, we obtain similar results but this requires more parameters.
We also evaluate our model when we mask the attentions in half attention heads, \ie, there is no attention masking in other half attention heads.
In this case,
we get similar results using the same number of parameters as shown in the third row of Table~\ref{table:ablation}.

\begin{table}[t!]
    \centering
    \caption{
        Ablation study of our FastMETRO on Human3.6M~\cite{wang2014h36m}. 
        The effects of different components are evaluated.
        The default model is FastMETRO-S-R50.
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccc}
        \hline
        \multicolumn{1}{c}{}
            & \multicolumn{1}{c}{\;\;}
            & \multicolumn{2}{c}{\; Human3.6M \;}  
        \\
        \cline{3-4}
            Model
            & \; \#Params \;
            & \, MPJPE 
            & \, PA-MPJPE  \,
        \\
        \hline
        \hline
            w/o attention masking
            & \textbf{32.7M} 
            & 58.0
            & 40.7
        \\
            w/o attention masking + w/ graph convolutions
            & 33.1M
            & 56.6
            & \textbf{39.4}
        \\
            w/ attention masking in half attention heads
            & \textbf{32.7M}
            & 55.8
            & \textbf{39.4}
        \\
            w/ learnable upsampling layers
            & 45.4M 
            & 58.1
            & 41.1
        \\
            w/o progressive dimensionality reduction
            & 39.5M 
            & \textbf{55.5}
            & 39.6
        \\
        \hhline{-|-|-|-|}
            \cellcolor{gray!7.5}\textbf{FastMETRO--S--R50}
            & \cellcolor{gray!7.5}\textbf{32.7M} 
            & \cellcolor{gray!7.5}55.7
            & \cellcolor{gray!7.5}\textbf{39.4}
        \\
        \hline
    \end{tabular}}
    \label{table:ablation}
\end{table}

\noindent \textbf{Coarse-to-Fine Mesh Upsampling.}
The existing transformers~\cite{lin2021metro,lin2021graphormer} also first estimate a coarse mesh, then upsample the mesh to obtain a fine mesh.
They employ two learnable linear layers for the upsampling.
In our FastMETRO, we use the pre-computed upsampling matrix  to reduce optimization difficulty as in~\cite{kolotouros2019cmr};
this upsampling matrix is a sparse matrix which has only about  \mbox{non-zero} elements.
When we perform the mesh upsampling using learnable linear layers instead of the matrix ,
the regression accuracy drops as shown in the fourth row of Table~\ref{table:ablation},
although it demands much more parameters.

\noindent \textbf{Progressive Dimensionality Reduction.}
Following the existing transformer encoders~\cite{lin2021metro,lin2021graphormer}, 
we also progressively reduce the hidden dimension sizes in our transformer via linear projections.
To evaluate its effectiveness, 
we train our model using the same number of transformer layers but without progressive dimensionality reduction, \ie, hidden dimension sizes in all transformer layers are the same.
As shown in the fifth row of Table~\ref{table:ablation},
we obtain similar results but this requires much more parameters.
This demonstrates that the dimensionality reduction is helpful for our model to achieve decent results using fewer parameters.


\noindent \textbf{Generalizability.}
Our model can reconstruct any arbitrary 
3D objects by changing the number of input tokens used in the transformer 
decoder.
Note that we can employ learnable layers for coarse-to-fine mesh upsampling without masking attentions.
For 3D hand mesh reconstruction, there is a pre-computed upsampling matrix and a human hand model such as MANO~\cite{MANO:SIGGRAPHASIA:2017}. 
Thus, we can leverage the 
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{arXiv_Figures/mano_topology_3.pdf}
    \caption{
        Hand Joints and Mesh Topology.
    }
    \label{fig:hand_mesh_topology}
\end{wrapfigure}
matrix for mesh upsampling and mask self-attentions of non-adjacent vertices in the same way with 3D human mesh recovery.
As illustrated in Figure~\ref{fig:hand_mesh_topology}, we can obtain an adjacency matrix and construct 
joint and vertex tokens from the human hand mesh topology. 
To validate the generalizability of our method,
we train FastMETRO-L-H64 on the \mbox{FreiHAND}~\cite{zimmermann2019freihand} training
dataset and evaluate the model.
As shown in Table~\ref{table:freihand}, our proposed model achieves competitive results on FreiHAND.

\begin{table}[t!]
    \centering
    \caption{
        Comparison with transformers for monocular 3D hand mesh recovery on FreiHAND~\cite{zimmermann2019freihand}.
        Test-time augmentation is not applied to these transformers.
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccccc}
        \hline
        \multicolumn{1}{c}{}
        & \multicolumn{1}{c}{\;\;}
        & \multicolumn{1}{c}{\, Transformer \,}
        & \multicolumn{1}{c}{\;\;}
        & \multicolumn{1}{c}{\, Overall \,}
        & \multicolumn{1}{c}{\;\;}
        & \multicolumn{2}{c}{\; FreiHAND \;}  
        \\
        \cline{3-3}
        \cline{5-5}
        \cline{7-8}
            Model
            & 
            & \, \#Params \,
            &
            & \, \#Params \,
            &
            & \, PA-MPJPE  \,
            & \, F@15mm  \,
        \\
        \hline
        \hline
            METRO--H64~\cite{lin2021metro}
            & 
            & 102.3M
            &
            & 230.4M
            &
            & 6.8
            & 0.981
        \\
        \hhline{-|-|-|-|-|-|-|-|}
            \cellcolor{gray!7.5}\textbf{FastMETRO--L--H64}
            & \cellcolor{gray!7.5} 
            & \cellcolor{gray!7.5}\textbf{24.9M}
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}\textbf{153.0M}
            & \cellcolor{gray!7.5}
            & \cellcolor{gray!7.5}\textbf{6.5}
            & \cellcolor{gray!7.5}\textbf{0.982}
        \\
        \hline
    \end{tabular}}
    \label{table:freihand}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{arXiv_Figures/qualitative_hand_4.pdf}
    \caption{
    Qualitative results of our FastMETRO on FreiHAND~\cite{zimmermann2019freihand}. 
    We visualize the 3D hand mesh estimated by FastMETRO-L-H64.
    By leveraging the attention mechanism in the transformer,
    our model is robust to partial occlusions.
    }
    \label{fig:qualitative_hand}
\end{figure}