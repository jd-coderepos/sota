\section{Proofs}\label{app:proof}
We first summarize the notations and assumptions used in our theorems.
\paragraph{Notations} 
The drift and diffusion coefficients of the SDE in \cref{eq:sde} are denoted as $\vf: \mbb{R}^D \times [0,T] \to \mbb{R}^D$ and $g: [0, T] \to \mbb{R}$ respectively, where $[0, T]$ represents a fixed time horizon, and $\times$ denotes the Cartesian product. The solution to \cref{eq:sde} is a stochastic process $\{\rvx(t) \}_{t\in[0, T]}$. We use $p_t$ to represent the marginal distribution of $\rvx(t)$, and $p_{0t}(\rvx' \mid \rvx)$ to denote the transition distribution from $\rvx(0)$ to $\rvx(t)$. The data distribution and prior distribution are given by $p$ and $\pi$. We use $\mcal{C}$ to denote all continuous functions, and let $\mcal{C}^k$ denote the family of functions with continuous $k$-th order derivatives. For any vector-valued function $\vh: \mbb{R}^D \times [0,T] \to \mbb{R}^D$, we use $\nabla \cdot \vh(\rvx, t)$ to represent its divergence with respect to the first input variable.
\paragraph{Assumptions}
We make the following assumptions throughout the paper:
\begin{enumerate}[label=(\roman*)]
    \item $p(\rvx) \in \mcal{C}^2$ and $\E_{\rvx \sim p}\big[\norm{\rvx}_2^2\big] < \infty$.
    \item $\pi(\rvx) \in \mcal{C}^2$ and $\E_{\rvx \sim \pi}\big[ \norm{\rvx}_2^2 \big] < \infty$.
    \item $\forall t\in[0,T]: \vf(\cdot, t) \in \mcal{C}^1$, $\exists C>0~\forall \rvx \in \mbb{R}^D, t\in [0,T]: \norm{\vf(\rvx, t)}_2 \leq C(1 + \norm{\rvx}_2)$.
    \item $\exists C>0, \forall \rvx, \rvy \in \mbb{R}^D: \norm{\vf(\rvx, t) - \vf(\rvy, t)}_2 \leq C \norm{\rvx - \rvy}_2$.
    \item $g \in \mcal{C}$ and $\forall t\in[0,T], |g(t)| > 0$.
    \item For any open bounded set $\mcal{O}$, $\int_0^T \int_\mcal{O} \norm{p_t(\rvx)}_2^2 + D g(t)^2 \norm{\nabla_\rvx p_t(\rvx)}_2^2 \ud \rvx \ud t < \infty$.
    \item $\exists C>0~\forall \rvx \in \mbb{R}^D, t \in [0,T]: \norm{\nabla_\rvx \log p_t(\rvx)}_2 \leq C(1 + \norm{\rvx}_2)$.
    \item $\exists C>0, \forall \rvx, \rvy \in \mbb{R}^D: \norm{\nabla_\rvx \log p_t(\rvx) - \nabla_\rvy \log p_t(\rvy)}_2 \leq C \norm{\rvx - \rvy}_2$.
    \item $\exists C>0~\forall \rvx \in \mbb{R}^D, t \in [0,T]: \norm{\vs_\vtheta(\rvx, t)}_2 \leq C(1 + \norm{\rvx}_2)$.
    \item $\exists C>0, \forall \rvx, \rvy \in \mbb{R}^D: \norm{\vs_\vtheta(\rvx, t) - \vs_\vtheta(\rvy, t)}_2 \leq C \norm{\rvx - \rvy}_2$.
    \item Novikov's condition: $\E\Big[ \exp\Big( \frac{1}{2}\int_0^T \norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 \ud t \Big) \Big] < \infty$.
    \item $\forall t \in [0,T]~\exists k > 0: p_t(\rvx) = O(e^{-\norm{\rvx}_2^k})$ as $\norm{\rvx}_2 \to \infty$.
\end{enumerate}
Below we provide all proofs for our theorems.
\bound*
\begin{proof}
We denote the path measure of $\{\rvx(t)\}_{t\in[0,T]}$ and $\{\hat{\rvx}_\vtheta(t)\}_{t\in[0,T]}$ as $\bm{\mu}$ and $\bm{\nu}$ respectively. Due to assumptions (i) (ii) (iii) (iv) (v) (ix) and (x), both $\bm{\mu}$ and $\bm{\nu}$ are uniquely given by the corresponding SDEs. Consider a Markov kernel $K(\{\rvz(t)\}_{t\in[0,t]}, \rvy) \coloneqq \delta(\rvz(0) = \rvy)$. Since $\rvx(0) \sim p_0$ and $\hat{\rvx}_\vtheta(0) \sim p_\vtheta$, we have the following result
\begin{align*}
    \int K(\{\rvx(t)\}_{t\in[0,T]}, \rvx) \ud \bm{\mu}(\{\rvx(t)\}_{t\in[0,T]}) &= p_0(\rvx)\\
    \int K(\{\hat{\rvx}_\vtheta(t)\}_{t\in[0,T]}, \rvx) \ud \bm{\nu}(\{\hat{\rvx}_\vtheta(t)\}_{t\in[0,T]}) &= p_\vtheta(\rvx).
\end{align*}
Here the Markov kernel $K$ essentially performs marginalization of path measures to obtain ``sliced'' distributions at $t = 0$. We can use the data processing inequality with this Markov kernel to obtain
\begin{align}
    &\KL(p \mathrel\| p_\vtheta) =\KL(p_0 \mathrel\| p_\vtheta)  \notag\\
    = & \KL\bigg(\int K(\{\rvx(t)\}_{t\in[0,T]}, \rvx) \ud \bm{\mu}(\{\rvx(t)\}_{t\in[0,T]})\mathrel\bigg\| \int K(\{\hat{\rvx}_\vtheta(t)\}_{t\in[0,T]}, \rvx) \ud \bm{\nu}(\{\hat{\rvx}_\vtheta(t)\}_{t\in[0,T]})\bigg)\notag \\
    \leq & \KL(\bm{\mu}\mathrel\| \bm{\nu}).\label{eqn:data_processing}
\end{align}
Recall that by definition $\rvx(T) \sim p_T$ and $\hat{\rvx}_\vtheta(T) \sim \pi$. Leveraging the chain rule of KL divergences (see, for example, Theorem 2.4 in \cite{leonard2014some}), we have
\begin{align}
    \KL(\bm{\mu} \mathrel\| \bm{\nu}) = \KL(p_T \mathrel\| \pi ) + \E_{\rvz \sim p_T}[\KL(\bm{\mu}(\cdot \mid \rvx(T) = \rvz) \mathrel\| \bm{\nu}(\cdot \mid \hat{\rvx}_\vtheta(T) = \rvz))].\label{eqn:chain_rule}
\end{align}
Under assumptions (i) (iii) (iv) (v) (vi) (vii) (viii), the SDE in \cref{eq:sde} has a corresponding reverse-time SDE given by
\begin{align}
\ud \rvx = [\vf(\rvx, t) - g(t)^2 \nabla_\rvx \log p_t(\rvx)]\ud t + g(t) \ud \bar{\rvw}. \label{eqn:reverse_sde_proof}
\end{align}
Since \cref{eqn:reverse_sde_proof} is the time reversal of \cref{eq:sde}, it induces the same path measure $\bm{\mu}$. As a result, $\KL(\bm{\mu}(\cdot \mid \rvx(T) = \rvz)\mathrel\| \bm{\nu}(\cdot \mid \hat{\rvx}_\vtheta(T) = \rvz))$ can be viewed as the KL divergence between the path measures induced by the following two (reverse-time) SDEs:
\begin{gather*}
    \ud \rvx = [\vf(\rvx, t) - g(t)^2 \nabla_\rvx \log p_t(\rvx)]\ud t + g(t) \ud \bar{\rvw}, \quad \rvx(T) = \rvx\\
    \ud \hat{\rvx} = [\vf(\hat{\rvx}, t) - g(t)^2 \vs_\vtheta(\hat{\rvx}, t)]\ud t + g(t) \ud \bar{\rvw}, \quad \hat{\rvx}_\vtheta(T) = \rvx.
\end{gather*}
The KL divergence between two SDEs with shared diffusion coefficients and starting points exists under assumptions (vii) (viii) (ix) (x) (xi) (see, \eg, \cite{tzen-NeuralStochasticDifferential-2019a,li-ScalableGradientsStochastic-2020}), and can be computed via the Girsanov theorem~\cite{oksendal2013stochastic}:
\begin{align}
&\KL(\bm{\mu}(\cdot \mid \rvx(T) = \rvz)\mathrel\| \bm{\nu}(\cdot \mid \hat{\rvx}_\vtheta(T) = \rvz)) \notag\\
=& -\E_{\bm{\mu}}\Big[\log \frac{\ud \bm{\nu}}{\ud \bm{\mu}}\Big]\\
\stackrel{(i)}{=}& \E_{\bm{\mu}}\bigg[\int_0^T g(t) (\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)) \ud \bar{\rvw}_t + \frac{1}{2} \int_0^T g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 \ud t\bigg]\notag \\
\stackrel{(ii)}{=}& \E_{\bm{\mu}}\left[\frac{1}{2} \int_0^T g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 \ud t\right]\notag \\
=& \frac{1}{2} \int_0^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2] \ud t \notag\\
=& \jsm(\vtheta; g(\cdot)^2),\label{eqn:girsanov}
\end{align}
where (i) is due to Girsanov Theorem II~\cite[Theorem 8.6.6]{oksendal2013stochastic}, and (ii) is due to the martingale property of It\^{o} integrals.
Combining \cref{eqn:data_processing,eqn:chain_rule,eqn:girsanov} completes the proof.
\end{proof}

\thm*
\begin{proof}
When $\pi = q_T$ and $\vs_\vtheta(\rvx, t) \equiv \nabla_\rvx \log q_t(\rvx)$, the reverse-time SDE that defines $\psde$, \ie,
\begin{align}
\ud \hat{\rvx} = [\vf(\hat{\rvx}, t) - g(t)^2 \vs_\vtheta(\hat{\rvx}, t)] \ud t + g(t) \ud \bar{\rvw}, \quad \hat{\rvx}_\vtheta(T) \sim \pi,
\end{align}
becomes equivalent to
\begin{align}
    \ud \hat{\rvx} = [\vf(\hat{\rvx}, t) - g(t)^2 \nabla_{\hat{\rvx}} \log q_t(\hat{\rvx})] \ud t + g(t) \ud \bar{\rvw}, \quad \hat{\rvx}_\vtheta(T) \sim q_T,
\end{align}
which yields the same stochastic process as the following forward-time SDE
\begin{align}
    \ud \hat{\rvx} = \vf(\hat{\rvx}, t) \ud t + g(t) \ud \rvw, \quad \hat{\rvx}_\vtheta(0) \sim q. \label{eqn:q_sde}
\end{align}
Since $\hat{\rvx}_\vtheta(0) \sim \psde$ by definition, we immediately have $\psde = q$. Similarly, the ODE that defines $\pode$ is
\begin{align}
    \frac{\ud \tilde{\rvx}}{\ud t} = \vf_\vtheta(\tilde{\rvx}, t) - \frac{1}{2}g(t)^2 \vs_\vtheta(\tilde{\rvx}, t), \quad \tilde{\rvx}_\vtheta(T) \sim \pi,
\end{align}
which is equivalent to the following when $q_T = \pi$ and $\vs_\vtheta(\rvx, t) \equiv \nabla_\rvx \log q_t(\rvx)$,
\begin{align}
    \frac{\ud \tilde{\rvx}}{\ud t} = \vf_\vtheta(\tilde{\rvx}, t) - \frac{1}{2}g(t)^2 \nabla_{\tilde{\rvx}} \log q_t(\tilde{\rvx}, t), \quad \tilde{\rvx}_\vtheta(T) \sim q_T. \label{eqn:q_ode}
\end{align}
The theory of probability flow ODEs~\cite{song2020score} guarantees that \cref{eqn:q_sde} and \cref{eqn:q_ode} share the same set of marginal distributions, $\{q_t\}_{t\in[0,T]}$, which implies that $\tilde{\rvx}_\vtheta(0) \sim q$. Since by definition $\tilde{\rvx}_\vtheta(0) \sim \pode$, we have $\pode = q$.

The next part of the theorem can be proved by first rewriting the KL divergence from $p$ to $q$ in an integral form:
\begin{align}
    \KL(p(\rvx)~\|~q(\rvx)) &\stackrel{(i)}{=} \KL(p_0(\rvx)\mathrel\| q_0(\rvx)) - \KL(p_T(\rvx)\mathrel\| q_T(\rvx)) + \KL(p_T(\rvx)\mathrel\| q_T(\rvx))\notag \\
    &\stackrel{(ii)}{=} \int_{T}^0 \frac{\partial \KL(p_t(\rvx)\mrel\| q_t(\rvx))}{\partial t} \ud t + \KL(p_T(\rvx)\mathrel\| q_T(\rvx)), \label{eqn:int}
\end{align}
where (i) holds due to our definition $p_0(\rvx) \equiv p(\rvx)$ and $q_0(\rvx) \equiv q(\rvx)$; (ii) is due to the fundamental theorem of calculus.

Next, we show how to rewrite \cref{eqn:int} as a mixture of score matching losses. 
The Fokker--Planck equation for the SDE in \cref{eq:sde} describes the time-evolution of the stochastic process's associated probability density function, and is given by
\begin{align*}
    \frac{\partial p_t(\rvx)}{\partial t} = \nabla_\rvx \cdot \Big( \frac{1}{2} g^2(t) p_t(\rvx) \nabla_{\rvx} \log p_t(\rvx) - \vf(\rvx,t) p_t(\rvx) \Big) = \nabla_{\rvx} \cdot (\vh_p(\rvx, t) p_t(\rvx)),
\end{align*}
where for simplified notations we define $\vh_p(\rvx, t) := \frac{1}{2}g^2(t) \nabla_{\rvx} \log p_t(\rvx) - \vf(\rvx, t)$. Similarly, $\frac{\partial q_t(\rvx)}{\partial t} = \nabla_{\rvx} \cdot (\vh_q(\rvx, t) q_t(\rvx))$. Since we assume $\log p_t(\rvx)$ and $\log q_t(\rvx)$ are smooth functions with at most polynomial growth at infinity (assumption (xii)), we have $\lim_{\rvx \to \infty} \vh_p(\rvx, t)p_t(\rvx) = \bm{0}$ and $\lim_{\rvx \to \infty}\vh_q(\rvx, t)q_t(\rvx) = \bm{0}$ for all $t$. Then, the time-derivative of $\KL(p_t~\|~q_t)$ can be rewritten in the following way:

\begin{align*}
    \frac{\partial \KL(p_t(\rvx)~\|~q_t(\rvx))}{\partial t}  =& \frac{\partial}{\partial t}\int p_t(\rvx) \log \frac{p_t(\rvx)}{q_t(\rvx)} \ud \rvx\\\
    =&\int \frac{\partial p_t(\rvx)}{\partial t} \log \frac{p_t(\rvx)}{q_t(\rvx)} \ud \rvx + \underbrace{\int \frac{\partial p_t(\rvx)}{\partial t} \ud \rvx}_{=0} - \int \frac{p_t(\rvx)}{q_t(\rvx)} \frac{\partial q_t(\rvx)}{\partial t} \ud \rvx\\
    =& \int \nabla_{\rvx} \cdot (\vh_p(\rvx, t) p_t(\rvx)) \log \frac{p_t(\rvx)}{q_t(\rvx)} \ud \rvx -\int \frac{p_t(\rvx)}{q_t(\rvx)}\nabla_{\rvx} \cdot (\vh_q(\rvx, t) q_t(\rvx))  \ud \rvx\\
    \stackrel{(i)}{=}& -\int p_t(\rvx)[\vh_p\tran(\rvx, t) - \vh_q\tran(\rvx, t)][\nabla_\rvx \log p_t(\rvx) - \nabla_\rvx \log q_t(\rvx)] \ud \rvx \\
    =& -\frac{1}{2} \int p_t(\rvx) g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx) - \nabla_\rvx \log q_t(\rvx)}_2^2 \ud \rvx,
\end{align*}
where (i) is due to integration by parts. Combining with \cref{eqn:int}, we can conclude that
\begin{align}
   \KL(p~\|~q) = \frac{1}{2}\int_0^T \E_{\rvx \sim p_t(\rvx)}[g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx) - \nabla_\rvx \log q_t(\rvx)}_2^2] \ud t + \KL(p_T~\|~q_T). 
\end{align}
Since $\psde = q$ and $q_T = \pi$, we also have
\begin{align}
   \KL(p~\|\psde) &= \frac{1}{2}\int_0^T \E_{\rvx \sim p_t(\rvx)}[g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx) - \nabla_\rvx \log q_t(\rvx)}_2^2] \ud t + \KL(p_T~\|~q_T)\notag\\
   &= \jsm(\vtheta; g(\cdot)^2) + \KL(p_T\mathrel\| q_T),
\end{align}
which completes the proof.
\end{proof}



Using a similar technique to \cref{thm:1}, we can express the entropy of a distribution in terms of a time-dependent score function, as detailed in the following theorem.
\begin{theorem}\label{thm:entropy}
Let $\mcal{H}(p(\rvx))$ be the differential entropy of the initial probability density $p(\rvx)$. Under the same conditions in \cref{thm:1}, we have
\begin{align}
    \mcal{H}(p(\rvx)) &= \mcal{H}(p_T(\rvx)) 
    + \frac{1}{2} \int_0^T \E_{\rvx \sim p_t(\rvx)}\Big[2 \vf(\rvx, t)\tran \nabla_\rvx \log p_t(\rvx) - g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2\Big]\ud t.\label{eqn:entropy1}\\
    &= \mcal{H}(p_T(\rvx)) 
    - \frac{1}{2} \int_0^T \E_{\rvx \sim p_t(\rvx)}\Big[2 \nabla\cdot \vf(\rvx, t) + g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2\Big]\ud t.
    \label{eqn:entropy}
\end{align}
\end{theorem}
\begin{proof}
Once more we proceed analogously to the proofs of \cref{thm:1}.
We have
\begin{align}
    &\mcal{H}(p(\rvx)) - \mcal{H}(p_T(\rvx)) 
    = \int_{T}^{0} \frac{\partial}{\partial t} \mcal{H}(p_{t}(\rvx)) \ud t.
    \label{eqn:thm3-entropy-integral}
\end{align}
Expanding the integrand, we have
\begin{align*}
    \frac{\partial}{\partial t} \mcal{H}(p_{t}(\rvx)) 
    &= - \frac{\partial}{\partial t} \int p_{t}(\rvx) \log p_{t}(\rvx) \ud \rvx \\
    &= - \int \frac{\partial p_{t}(\rvx)}{\partial t} \log p_{t}(\rvx) + \frac{\partial p_{t}(\rvx)}{\partial t} \ud \rvx \\
    &= - \int \frac{\partial p_{t}(\rvx)}{\partial t} \log p_{t}(\rvx) \ud \rvx - \frac{\partial }{\partial t} \underbrace{\int p_{t}(\rvx) \ud \rvx}_{= 1} \\
    &= - \int \nabla_\rvx \cdot (\vh_{p}(\rvx, t)p_t(\rvx)) \log p_{t}(\rvx) \ud \rvx \\
    &\stackrel{(i)}{=} \int p_t(\rvx) \vh_p\tran(\rvx,t) \nabla_{\rvx} \log p_{t}(\rvx) \ud \rvx \\
    &=  \frac{1}{2} \E_{\rvx \sim p_{t}(\rvx)} [ g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2 - 2 \vf(\rvx, t)\tran \nabla_\rvx \log p_t(\rvx) ],
\end{align*}
where again (i) follows from integration by parts and the limiting behaviour of $ \vh_p $ given by assumption (xii).
Plugging this expression in for the integrand in \cref{eqn:thm3-entropy-integral} then completes the proof for \cref{eqn:entropy1}. For \cref{eqn:entropy}, we can once again perform integration by parts and leverage the limiting behavior of $p_t(\rvx)$ in assumption (xii) to get
\begin{align*}
    \E_{p_t(\rvx)}[\vf(\rvx, t)\tran \nabla_\rvx \log p_t(\rvx)] &= \int \vf(\rvx, t)\tran \nabla_\rvx p_t(\rvx) \ud \rvx
    = -\int p_t(\rvx) \nabla\cdot \vf(\rvx, t) \ud \rvx,
\end{align*}
which establishes the equivalence between \cref{eqn:entropy} and \cref{eqn:entropy1}.
\end{proof}

\paragraph{Remark} The formula in \cref{thm:entropy} provides a new way to estimate the entropy of a data distribution from \iid samples. Specifically, given $\{\rvx_1, \rvx_2, \cdots, \rvx_N\} \stackrel{\text{i.i.d.}}{\sim} p(\rvx)$ and an SDE like \cref{eq:sde}, we can first apply score matching to train a time-dependent score-based model such that $\vs_{\vtheta}(\rvx, t) \approx \nabla_\rvx \log p_t(\rvx)$, and then plug $\vs_{\vtheta}(\rvx, t)$ into \cref{eqn:entropy1} to obtain the following estimator of $\mcal{H}(p(\rvx))$:
\begin{align*}
    \mcal{H}(p_T(\rvx))  + \frac{1}{2N} \sum_{i=1}^N \int_0^T \Big[2 \vf(\rvx_i, t)\tran \vs_\vtheta(\rvx_i, t) - g(t)^2 \norm{\vs_\vtheta(\rvx_i, t)}_2^2\Big]\ud t,
\end{align*}
or plug it into \cref{eqn:entropy} to obtain the following alternative estimator
\begin{align*}
    \mcal{H}(p_T(\rvx))  - \frac{1}{2N} \sum_{i=1}^N \int_0^T \Big[2 \nabla \cdot \vf(\rvx_i, t) + g(t)^2 \norm{\vs_\vtheta(\rvx_i, t)}_2^2\Big]\ud t.
\end{align*}
Both estimators can be computed from a score-based model alone, and do not require training a density model. 

\begin{theorem}\label{thm:elbo_bound}
Let $p_{0t}(\rvx' \mid \rvx)$ denote the transition kernel from $p_0(\rvx)$ to $p_t(\rvx)$ for any $t \in (0,T]$. With the same conditions and notations in \cref{thm:bound}, we have
\begin{align}
-\mbb{E}_{p(\rvx)}[\log \psde(\rvx)] 
\leq & -\mbb{E}_{p_T(\rvx)}[\log \pi(\rvx)] 
+ \frac{1}{2}\int_{0}^T \mbb{E}_{\rvx \sim p_t(\rvx)}[2g(t)^2 \nabla \cdot \vs_\vtheta(\rvx, t) \notag \\
&+ g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2 -2 \nabla \cdot \vf(\rvx, t)] \ud t. \label{eqn:elbo1}\\
=&  -\mbb{E}_{p_T(\rvx)}[\log \pi(\rvx)]  \notag \\
&+ \frac{1}{2}\int_{0}^T \mbb{E}_{p_{0t}(\rvx' \mid \rvx)p(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx', t) - \nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2 \notag\\
&- g(t)^2\norm{\nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2 -2 \nabla \cdot \vf(\rvx', t)] \ud t.\label{eqn:elbo2}
\end{align}
\end{theorem}
\begin{proof}
Since $-\E_{p(\rvx)}[\log \psde(\rvx)] = \KL(p\mathrel\| \psde) + \mcal{H}(p)$, we can combine \cref{thm:bound} and \cref{thm:entropy} to obtain
\begin{align}
-\E_{p(\rvx)}[\log \psde(\rvx)] &\leq \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2]\ud t + \KL(p_T\mathrel\| \pi) \notag \\
 & \quad + \mcal{H}(p_T(\rvx)) - \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[2 \nabla\cdot \vf(\rvx, t) + g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2]\ud t \notag \\
 &= -\E_{p_T(\rvx)}[\log \pi(\rvx)] \notag \\
 &\quad + \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 - g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2]\ud t \notag \\
 &\quad - \int_0^T \E_{p_t(\rvx)}[\nabla \cdot \vf(\rvx, t)]\ud t.\label{eqn:combine}
\end{align}
The second term of \cref{eqn:combine} can be simplified via integration by parts
\begin{align}
    & \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 - g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2]\ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2 - 2 g(t)^2 \vs_\vtheta(\rvx,t)\tran \nabla_\rvx \log p_t(\rvx)] \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2]\ud t
    - \int_0^T \E_{p_t(\rvx)}[g(t)^2 \vs_\vtheta(\rvx,t)\tran \nabla_\rvx \log p_t(\rvx)] \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2]\ud t
    - \int_0^T g(t)^2 \int p_t(\rvx) \vs_\vtheta(\rvx,t)\tran \nabla_\rvx \log p_t(\rvx) \ud \rvx \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2]\ud t
    - \int_0^T g(t)^2 \int \vs_\vtheta(\rvx,t)\tran \nabla_\rvx p_t(\rvx) \ud \rvx \ud t \notag \\
    \stackrel{(i)}{=}& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2]\ud t
    + \int_0^T g(t)^2 \int p_t(\rvx) \nabla \cdot \vs_\vtheta(\rvx, t) \ud \rvx \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2 + 2g(t)^2 \nabla \cdot \vs_\vtheta(\rvx, t)]\ud t, \label{eqn:partial_integral}
\end{align}
where (i) is due to integration by parts and the limiting behavior of $p_t(\rvx)$ given by assumption (xii). Combining \cref{eqn:partial_integral} and \cref{eqn:combine} completes the proof for \cref{eqn:elbo1}.

The proof for \cref{eqn:elbo2} parallels that of denoising score matching~\cite{vincent2011connection}. Observe that $p_t(\rvx) = \int p(\rvx') p_{0t}(\rvx \mid \rvx') \ud \rvx'$. As a result,
\begin{align}
    &\int_0^T \E_{p_t(\rvx)}[g(t)^2 \vs_\vtheta(\rvx, t)\tran \nabla_\rvx \log p_t(\rvx)]\ud t = \int_0^T g(t)^2 \int \vs_\vtheta(\rvx, t)\tran \nabla_\rvx p_t(\rvx) \ud \rvx \ud t \notag \\
    =& \int_0^T g(t)^2 \int \vs_\vtheta(\rvx, t)\tran \nabla_\rvx \int p(\rvx') p_{0t}(\rvx \mid \rvx') \ud \rvx' \ud \rvx \ud t \notag \\
    =& \int_0^T g(t)^2 \int \vs_\vtheta(\rvx, t)\tran \int p(\rvx') \nabla_\rvx p_{0t}(\rvx \mid \rvx') \ud \rvx' \ud \rvx \ud t \notag\\
    =& \int_0^T g(t)^2 \int \vs_\vtheta(\rvx, t)\tran \int p(\rvx') p_{0t}(\rvx \mid \rvx')\nabla_\rvx \log p_{0t}(\rvx \mid \rvx') \ud \rvx' \ud \rvx \ud t \notag \\
    =& \int_0^T \E_{p(\rvx)p_{0t}(\rvx' \mid \rvx)}[g(t)^2  \vs_\vtheta(\rvx', t)\tran \nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)] \ud t. \label{eqn:denoising}
\end{align}
Substituting \cref{eqn:denoising} into the second term of \cref{eqn:combine}, we have
\begin{align}
    & \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2 - g(t)^2 \norm{\nabla_\rvx \log p_t(\rvx)}_2^2]\ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2 - 2 g(t)^2 \vs_\vtheta(\rvx,t)\tran \nabla_\rvx \log p_t(\rvx)] \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p_t(\rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx, t)}_2^2 - 2 g(t)^2 \vs_\vtheta(\rvx,t)\tran \nabla_\rvx \log p_t(\rvx)] \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p(\rvx)p_{0t}(\rvx'\mid \rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx', t)}_2^2 - 2 g(t)^2 \vs_\vtheta(\rvx',t)\tran \nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)] \ud t \notag \\
    =& \frac{1}{2}\int_0^T \E_{p(\rvx)p_{0t}(\rvx'\mid \rvx)}[g(t)^2\norm{\vs_\vtheta(\rvx', t) - \nabla_{\rvx'}\log p_{0t}(\rvx' \mid \rvx)}_2^2 - g(t)^2 \norm{\nabla_{\rvx'}\log p_{0t}(\rvx' \mid \rvx)}_2^2] \ud t. \label{eqn:denoising_2}
\end{align}
We can now complete the proof for \cref{eqn:elbo2} by combining \cref{eqn:denoising_2} an \cref{eqn:combine}.
\end{proof}

\pointelbo*
\begin{proof}
The result in \cref{thm:elbo_bound} can be re-written as
\begin{align*}
-\mbb{E}_{p(\rvx)}[\log \psde(\rvx)] 
\leq & -\mbb{E}_{p(\rvx)p_{0T}(\rvx' \mid \rvx)}[\log \pi(\rvx')] 
+ \frac{1}{2}\int_{0}^T \mbb{E}_{p(\rvx)p_{0t}(\rvx'\mid \rvx)}[2g(t)^2 \nabla \cdot \vs_\vtheta(\rvx', t) \notag \\
&+ g(t)^2 \norm{\vs_\vtheta(\rvx', t)}_2^2 -2 \nabla \cdot \vf(\rvx', t)] \ud t.\\
=&  -\mbb{E}_{p(\rvx)p_{0T}(\rvx' \mid \rvx)}[\log \pi(\rvx')]  \notag \\
&+ \frac{1}{2}\int_{0}^T \mbb{E}_{p(\rvx)p_{0t}(\rvx' \mid \rvx)}[g(t)^2 \norm{\vs_\vtheta(\rvx', t) - \nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2 \notag\\
&- g(t)^2\norm{\nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2 -2 \nabla \cdot \vf(\rvx', t)] \ud t.
\end{align*}
Given a fixed SDE (and its transition kernel $p_{0t}(\rvx' \mid \rvx)$), \cref{thm:elbo_bound} holds for any data distribution $p$ that satisfies our assumptions. Leveraging proof by contradiction, we can easily see that $\E_{p(\rvx)}$ in both sides of \cref{eqn:elbo1,eqn:elbo2} can be cancelled to get 
\begin{align*}
    -\log \psde(\rvx) \leq \boundsm(\rvx) = \bounddsm(\rvx),
\end{align*}
which finishes the proof.
\end{proof}


\section{Numerical stability}\label{app:stability}
In our previous theoretical discussion, we always assume that data are perturbed with an SDE starting from $t=0$. However, in practical implementations, $t=0$ often leads to numerical instability. As a pragmatic solution, we choose a small non-zero starting time $\epsilon > 0$, and consider the SDE in the time horizon $[\epsilon, T]$. Using the same proof techniques, we can easily see that when the time horizon is $[\epsilon, T]$ instead of $[0, T]$, the original bound in \cref{thm:bound},
\begin{align*}
    \KL(p\mathrel\| \psde) &\leq \jsm(\vtheta; g(\cdot)^2) + \KL(p_T\mathrel\| \pi)\\
    &= \frac{1}{2} \int_{0}^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2] \ud t  + \KL(p_T\mathrel\| \pi)
\end{align*}
shall be replaced with
\begin{align}
    \KL(\tilde{p}\mathrel\| \ptsde) \leq \frac{1}{2} \int_{\epsilon}^T \E_{p_t(\rvx)}[g(t)^2\norm{\nabla_\rvx \log p_t(\rvx) - \vs_\vtheta(\rvx, t)}_2^2] \ud t  + \KL(p_T\mathrel\| \pi) \label{eqn:kl_bound_tilde}
\end{align}
where $\tilde{p}(\rvx) \coloneqq \int p(\tilde{\rvx}) p_{0\epsilon}(\rvx \mid \tilde{\rvx}) \ud \rvx$, and $\ptsde$ denotes the marginal distribution of $\hat{\rvx}_\vtheta(\epsilon)$. Here the stochastic process $\{ \hat{\rvx}_\vtheta(t) \}_{t\in[0,T]}$ is defined according to \cref{eqn:reverse_sde_model}. When $\epsilon$ is sufficiently small, we always have
\begin{align*}
    \KL(\tilde{p}\mathrel\| \ptsde) \approx \KL(p\mathrel\| \psde),
\end{align*}
so we train with \cref{eqn:kl_bound_tilde} to approximately maximize the model likelihood for $\psde$. However, at test time, we should report the likelihood bound for $\psde$ for mathematical rigor, not $\ptsde$. To this end, we first derive an analogous result to \cref{thm:elbo_bound2} with the time horizon $[\epsilon, T]$, given as below.
\begin{theorem}\label{thm:bound_epsilon}
Let $p_{0t}(\rvx' \mid \rvx)$ denote the transition distribution from $p_0(\rvx)$ to $p_t(\rvx)$ for the SDE in \cref{eq:sde}. With the same notations and conditions in \cref{thm:elbo_bound2}, as well as the definitions of $\tilde{p}$ and $\ptsde$ given above, we have
\begin{align}
-\E_{p_{0\epsilon}(\rvx' \mid \rvx)}[\log \ptsde(\rvx')] \leq \boundsm(\rvx, \epsilon) = \bounddsm(\rvx, \epsilon),
\end{align}
where $\boundsm(\rvx, \epsilon)$ is defined as
\begin{align*}
\resizebox{\linewidth}{!}{$\displaystyle 
-\mbb{E}_{p_{0T}(\rvx' \mid \rvx)}[\log \pi(\rvx')] 
+ \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{0t}(\rvx' \mid \rvx)}\left[2g(t)^2 \nabla_{\rvx'} \cdot \vs_\vtheta(\rvx', t)
+ g(t)^2 \norm{\vs_\vtheta(\rvx', t)}_2^2 -2 \nabla_{\rvx'} \cdot \vf(\rvx', t)\right] \ud t$},
\end{align*}
and $\bounddsm(\rvx, \epsilon)$ is given by
\begin{small}
\begin{multline*}
 -\mbb{E}_{p_{0T}(\rvx' \mid \rvx)}[\log \pi(\rvx')] 
+ \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{0t}(\rvx' \mid \rvx)}\left[g(t)^2 \norm{\vs_\vtheta(\rvx', t) - \nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2\right] \ud t\\
- \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{0t}(\rvx' \mid \rvx)}\left[g(t)^2\norm{\nabla_{\rvx'} \log p_{0t}(\rvx' \mid \rvx)}_2^2 + 2 \nabla_{\rvx'} \cdot \vf(\rvx', t)\right] \ud t.
\end{multline*}
\end{small}
\end{theorem}
\begin{proof}
The proof closely parallels that of \cref{thm:elbo_bound2}, by noting that $\tilde{p}(\rvx) = \int p(\rvx')p_{0\epsilon}(\rvx \mid \rvx') \ud \rvx'$.
\end{proof}

Although $\ptsde$ is a probabilistic model for $\tilde{p}$, we can transform it into a probabilistic model for $p$ leveraging a denoising distribution $q_\vtheta(\rvx \mid \rvx')$ that approximately converts $\tilde{p}$ to $p$. Suppose $p_{0\epsilon}(\rvx'\mid \rvx) = \mcal{N}(\rvx' \mid \alpha \rvx, \beta^2 \bm{I})$. Inspired by Tweedie's formula, we choose 
\begin{align*}
    q_\vtheta(\rvx \mid \rvx') \coloneqq \mcal{N}\bigg(\rvx \mathrel\bigg| \frac{\rvx'}{\alpha} + \frac{\beta^2}{\alpha} \vs_\vtheta(\rvx', \epsilon), \frac{\beta^2}{\alpha^2}\bm{I}\bigg),
\end{align*}
and define $p_\vtheta(\rvx) \coloneqq \int q_\vtheta(\rvx \mid \rvx') \ptsde(\rvx') \ud \rvx'$, which is a probabilistic model for $p$. With slight abuse of notation, we identify $p_\vtheta$ with $\psde$ in \cref{tab:results}. With Jensen's inequality, we have
\begin{align*}
    -\log p_\vtheta(\rvx) \leq -\E_{p_{0\epsilon}(\rvx' \mid \rvx)}\bigg[ \log \frac{q_\vtheta(\rvx \mid \rvx') \ptsde(\rvx')}{p_{0\epsilon}(\rvx' \mid \rvx)} \bigg].
\end{align*}
Combined with \cref{thm:bound_epsilon}, we have
\begin{align}
    -\log p_\vtheta(\rvx) &\leq -\E_{p_{0\epsilon}(\rvx' \mid \rvx)}[\log q_\vtheta(\rvx \mid \rvx') - \log p_{0\epsilon}(\rvx' \mid \rvx)] + \boundsm(\rvx, \epsilon)\\
    &= -\E_{p_{0\epsilon}(\rvx' \mid \rvx)}[\log q_\vtheta(\rvx \mid \rvx') - \log p_{0\epsilon}(\rvx' \mid \rvx)] + \bounddsm(\rvx, \epsilon) \label{eqn:correction}
\end{align}
The above bound \cref{eqn:correction} was applied to both computing the test-time likelihood bounds in \cref{tab:results}, and training the flow model used in variational dequantization. Note that it was not used to train the time-dependent score-based model.

In practice, we choose $\epsilon = 10^{-5}$ for VP SDEs and $\epsilon = 10^{-2}$ for subVP SDEs, except that on ImageNet we use $\epsilon = 5 \times 10^{-5}$ for VP SDE models trained with likelihood weighting and importance sampling. Note that \cite{song2020score} chooses $\epsilon = 10^{-5}$ for all cases. We found that when using likelihood weighting and optionally importance sampling, $\epsilon=10^{-5}$ for subVP SDEs can cause stiffness for numerical ODE solvers. In contrast, using $\epsilon=10^{-2}$ for subVP SDEs sidesteps numerical issues without hurting the performance for score-based models trained with original weightings in \cite{song2020score}. For the bound values in \cref{tab:results}, we draw 1000 time values uniformly in $[\epsilon, T]$ and use them to estimate $\bounddsm$ for each datapoint, with the same importance sampling technique in \cref{eqn:importance_sampling}. We use the correction in \cref{eqn:correction} and report upper bounds for $-\log p_\vtheta(\rvx)$. For computing the likelihood of $\pode$, we use the Dormand-Prince RK45 ODE solver~\cite{dormand1980family} with absolute and relevant tolerances set to $10^{-5}$. We do not use the correction in \cref{eqn:correction} for $\pode$, because it is still a valid likelihood for the data distribution even in the time horizon $[\epsilon, T]$.

Below is a related result to bound $\log \ptsde(\rvx)$ directly. We include it here for completeness, though we do not use it for either training or inference in our experiments.
\begin{theorem}\label{thm:bound_epsilon_2}
Let $p_{0t}(\rvx' \mid \rvx)$ denote the transition distribution from $p_0(\rvx)$ to $p_t(\rvx)$ for the SDE in \cref{eq:sde}. With the same notations and conditions in \cref{thm:elbo_bound2}, as well as the definitions of $\tilde{p}$ and $\ptsde$ in \cref{thm:bound_epsilon}, we have
\begin{align}
-\log \ptsde(\rvx) \leq \boundsme(\rvx) = \bounddsme(\rvx),
\end{align}
where $\boundsme(\rvx)$ is defined as
\begin{align*}
\resizebox{\linewidth}{!}{$\displaystyle 
-\mbb{E}_{p_{\epsilon T}(\rvx' \mid \rvx)}[\log \pi(\rvx')] 
+ \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{\epsilon t}(\rvx' \mid \rvx)}\left[2g(t)^2 \nabla_{\rvx'} \cdot \vs_\vtheta(\rvx', t)
+ g(t)^2 \norm{\vs_\vtheta(\rvx', t)}_2^2 -2 \nabla_{\rvx'} \cdot \vf(\rvx', t)\right] \ud t$},
\end{align*}
and $\bounddsme(\rvx)$ is given by
\begin{small}
\begin{multline*}
 -\mbb{E}_{p_{\epsilon T}(\rvx' \mid \rvx)}[\log \pi(\rvx')] 
+ \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{\epsilon t}(\rvx' \mid \rvx)}\left[g(t)^2 \norm{\vs_\vtheta(\rvx', t) - \nabla_{\rvx'} \log p_{\epsilon t}(\rvx' \mid \rvx)}_2^2\right] \ud t\\
- \frac{1}{2}\int_{\epsilon}^T \mbb{E}_{p_{\epsilon t}(\rvx' \mid \rvx)}\left[g(t)^2\norm{\nabla_{\rvx'} \log p_{\epsilon t}(\rvx' \mid \rvx)}_2^2 + 2 \nabla_{\rvx'} \cdot \vf(\rvx', t)\right] \ud t.
\end{multline*}
\end{small}
\end{theorem}
\begin{proof}
Proof closely parallels those of \cref{thm:elbo_bound2,thm:bound_epsilon}.
\end{proof}

\section{Experimental details}\label{app:exp}
\paragraph{Datasets} All our experiments are performed on two image datasets: CIFAR-10~\cite{krizhevsky2014cifar} and down-sampled ImageNet~\cite{van2016pixel}. Both contain images of resolution $32\times 32$. CIFAR-10 has 50000 images as the training set and 10000 images as the test set. Down-sampled ImageNet has 1281149 training images and 49999 test images. It is well-known that ImageNet contains some personal sensitive information and may cause privacy concern~\cite{yang2021study}. We minimize this risk by using the dataset with a small resolution ($32\times 32$).

\paragraph{Model architectures}
Our variational dequantization model, $q_\phi(\rvu \mid \rvx)$, follows the same architecture of Flow++~\cite{ho2019flow++}. We do not use dropout for score-based models trained on ImageNet. We did not tune model architectures or training hyper-parameters specifically for maximizing likelihoods. All likelihood values were reported using the last checkpoint of each setting.

\paragraph{Training} We follow the same training procedure for score-based models in \cite{song2020score}. We also use the same hyperparameters for training the variational dequantization model, except that we train it for only 300000 iterations while fixing the score-based model. All models are trained on Cloud TPU v3-8 (roughly equivalent to 4 Tesla V100 GPUs). The baseline DDPM++ model requires around 33 hours to finish training, while the deep DDPM++ model requires around 44 hours. The variational dequantization model for the former requires around 7 hours to train, and for the latter it requires around 9.5 hours.



\paragraph{Confidence intervals}
All likelihood values are obtained by averaging the results on around 50000 datapoints, sampled with replacement from the test dataset. We can compute the confidence intervals with Student's t-test. On CIFAR-10, the radius of 95\% confidence intervals is typically around 0.006 bits/dim, while on ImageNet it is around 0.008 bits/dim. 

\paragraph{Sample quality}
All FID values are computed on 50000 samples from $\pode$, generated with numerical ODE solvers as in \cite{song2020score}. We compute FIDs between samples and training/test data for CIFAR-10/ImageNet. Although likelihood weighting + importance sampling slightly increases FID scores, their samples have comparable visual quality, as demonstrated in \cref{fig:cifar,fig:imagenet}.

\begin{figure}
     \centering
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cifar10_best.jpg}
         \caption{DDPM++ (deep, subVP)~\protect\cite{song2020score}, FID = 2.86}
     \end{subfigure}
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cifar10_nll.jpg}
         \caption{ScoreFlow, FID = 5.34}
     \end{subfigure}
     \caption{Samples on CIFAR-10. (a) Model with the best FID. (b) ScoreFlow trained with likelihood weighting + importance sampling + VP SDE. Samples of both models are generated with the same random seed.}\label{fig:cifar}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/imagenet_best.jpg}
         \caption{DDPM++ (VP)~\protect\cite{song2020score}, FID = 8.34}
     \end{subfigure}
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/imagenet_nll.jpg}
         \caption{ScoreFlow, FID = 10.18}
     \end{subfigure}
     \caption{Samples on ImageNet $32\times 32$. (a) Model with the best FID. (b) ScoreFlow trained with likelihood weighting + importance sampling + VP SDE. Samples of both models are generated with the same random seed.}\label{fig:imagenet}
\end{figure}







