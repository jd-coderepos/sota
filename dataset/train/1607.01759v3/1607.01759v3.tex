\begin{table*}[t]
\centering
\small
\begin{tabular}{@{\hspace{3pt}}l@{\hspace{3pt}}ccccccccc}
\toprule
Model && AG & Sogou & DBP & Yelp P. & Yelp F. & Yah. A. & Amz. F. & Amz. P. \\
\midrule
BoW~\cite{zhang2015character}          && 88.8 & 92.9 & 96.6 & 92.2 & 58.0 & 68.9 & 54.6 & 90.4 \\
ngrams~\cite{zhang2015character}       && 92.0 & 97.1 & 98.6 & 95.6 & 56.3 & 68.5 & 54.3 & 92.0 \\
ngrams TFIDF~\cite{zhang2015character} && 92.4 & 97.2 & 98.7 & 95.4 & 54.8 & 68.5 & 52.4 & 91.5 \\
char-CNN~\cite{zhang2015text}          && 87.2 & 95.1 & 98.3 & 94.7 & 62.0 & 71.2 & 59.5 & 94.5 \\
char-CRNN~\cite{xiao2016efficient}     && 91.4 & 95.2 & 98.6 & 94.5 & 61.8 & 71.7 & 59.2 & 94.1 \\
VDCNN~\cite{conneau2016}               && 91.3 & 96.8 & 98.7 & 95.7 & 64.7 & 73.4 & 63.0 & 95.7 \\
\midrule
\texttt{fastText},              && 91.5 & 93.9 & 98.1 & 93.8 & 60.4 & 72.0 & 55.8 & 91.2 \\
\texttt{fastText}, , bigram     && 92.5 & 96.8 & 98.6 & 95.7 & 63.9 & 72.3 & 60.2 & 94.6 \\
\bottomrule
\end{tabular}
\caption{Test accuracy [\%] on sentiment datasets.
\texttt{FastText} has been run with the same parameters for all the datasets. 
It has  hidden units and we evaluate it with and without bigrams.
For char-CNN, we show the best reported numbers without data augmentation. 
}\label{tab:sent_res}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{@{\hspace{3pt}}cc@{\hspace{3pt}}cccccccc}
\toprule
&& \multicolumn{2}{c}{\newcite{zhang2015text}} && \multicolumn{3}{c}{\newcite{conneau2016}} && \texttt{fastText} \\ \cmidrule(l){3-4}\cmidrule(l){6-8}
&& small char-CNN & big char-CNN && depth=9 & depth=17 & depth=29 && , bigram \\ \midrule
AG      && 1h & 3h && 24m  & 37m  & 51m  && 1s  \\Sogou   && - & -   && 25m  & 41m  & 56m  && 7s \\DBpedia && 2h & 5h && 27m  & 44m  & 1h   && 2s  \\Yelp P. && - & -   && 28m  & 43m  & 1h09 && 3s \\Yelp F. && - & -   && 29m  & 45m  & 1h12 && 4s \\Yah. A. && 8h & 1d && 1h   & 1h33 & 2h   && 5s \\Amz. F. && 2d & 5d && 2h45 & 4h20 & 7h   && 9s \\Amz. P. && 2d & 5d && 2h45 & 4h25 & 7h   && 10s \\\bottomrule
\end{tabular}
\caption{Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN.
  }\label{tab:sent_speed}
\end{table*}

\section{Experiments}

We evaluate~\texttt{fastText} on two different tasks.  First, we compare it to
existing text classifers on the problem of sentiment analysis.  Then, we
evaluate its capacity to scale to large output space on a tag prediction
dataset.  Note that our model could be implemented with the Vowpal Wabbit
library,\footnote{Using the options \texttt{--nn}, \texttt{--ngrams} and \texttt{--log\_multi}}
but we observe in practice, that our tailored implementation is at
least~2-5 faster.

\subsection{Sentiment analysis}

\paragraph{Datasets and baselines.}
We employ the same~8 datasets and evaluation protocol of~\newcite{zhang2015character}.
We report the n-grams and~TFIDF baselines from~\newcite{zhang2015character}, as
well as the character level convolutional model~(char-CNN)
of~\newcite{zhang2015text}, the character based convolution recurrent
network~(char-CRNN) of~\cite{xiao2016efficient} and the very deep convolutional
network~(VDCNN) of~\newcite{conneau2016}.
We also compare to~\newcite{tang2015document} following their evaluation protocol.
We report their main baselines as well as their two approaches based on recurrent
networks~(Conv-GRNN and LSTM-GRNN). 

\paragraph{Results.}
We present the results in~Figure~\ref{tab:sent_res}.
We use~10 hidden units and run~\texttt{fastText} for~5 epochs with a learning rate selected
on a validation set from~0.05,~0.1,~0.25,~0.5.
On this task, adding bigram information improves the performance by~1-4. 
Overall our accuracy is slightly better than char-CNN and char-CRNN and, a bit worse than~VDCNN.
Note that we can increase the accuracy slightly by using more n-grams, for example
with trigrams, the performance on Sogou goes up to~97.1. 
Finally, Figure~\ref{tab:tang_res} shows that our method is competitive with the methods presented
in~\newcite{tang2015document}. We tune the hyper-parameters on the validation set and observe
that using n-grams up to~5 leads to the best performance.
Unlike~\newcite{tang2015document},~\texttt{fastText} does not use pre-trained word embeddings, which
can be explained the 1 difference in accuracy.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{\hspace{2pt}}l@{\hspace{2pt}}cccc}
\toprule
Model & Yelp'13 & Yelp'14 & Yelp'15 & IMDB \\
\midrule
SVM+TF & 59.8 & 61.8 & 62.4 & 40.5 \\
CNN & 59.7 & 61.0 & 61.5 & 37.5\\
Conv-GRNN & 63.7 & 65.5 & 66.0 & 42.5 \\
LSTM-GRNN & 65.1 & 67.1 & 67.6 & 45.3 \\
\midrule
\texttt{fastText}          & 64.2 & 66.2 & 66.6 & 45.2 \\
\bottomrule
\end{tabular}
\caption{Comparision with \protect\newcite{tang2015document}. The hyper-parameters
are chosen on the validation set. We report the test accuracy.}\label{tab:tang_res}
\end{table}


\begin{table*}[t]
\centering
\small
\begin{tabular}{p{20em}clcp{15em}}
\toprule
Input &~~& Prediction &~~& Tags \\
\midrule
taiyoucon 2011 digitals: individuals digital photos from the anime convention
taiyoucon 2011 in mesa, arizona. if you know the model and/or the character,
please comment.
&&
{\#}cosplay
&&
{\#}24mm {\#}anime {\#}animeconvention {\#}arizona {\#}canon {\#}con {\#}convention {\#}cos \textbf{{\#}cosplay} {\#}costume {\#}mesa {\#}play {\#}taiyou {\#}taiyoucon
\\ 
\midrule[0.02em]
2012 twin cities pride 2012 twin cities pride parade
&&
{\#}minneapolis
&&
{\#}2012twincitiesprideparade \textbf{{\#}minneapolis} {\#}mn {\#}usa 
\\
\midrule[0.02em]
beagle enjoys the snowfall 
&&
{\#}snow
&&
{\#}2007 {\#}beagle {\#}hillsboro {\#}january {\#}maddison {\#}maddy {\#}oregon \textbf{{\#}snow} 
\\
\midrule[0.08em]
christmas && {\#}christmas && {\#}cameraphone {\#}mobile
\\
\midrule[0.02em]
euclid avenue && {\#}newyorkcity && {\#}cleveland {\#}euclidavenue 
\\
\bottomrule
\end{tabular}
\caption{Examples from the validation set of YFCC100M dataset obtained with \texttt{fastText}
with  hidden units and bigrams. We show a few correct and incorrect tag predictions.
}\label{tab:tag_ex}
\end{table*}

\paragraph{Training time.}
Both char-CNN and VDCNN are trained on a~NVIDIA Tesla~K40~GPU, while our
models are trained on a~CPU using~20 threads.
Table~\ref{tab:sent_speed} shows that methods using convolutions are several orders of
magnitude slower than~\texttt{fastText}. While it is
possible to have a 10 speed up for char-CNN by using more recent~CUDA
implementations of convolutions,~\texttt{fastText} takes less than a minute to
train on these datasets. The~GRNNs method of~\newcite{tang2015document} takes
around~12 hours per epoch on~CPU with a single thread.
Our speed-up compared to neural network based methods increases with the size
of the dataset, going up to at least a~15,000 speed-up.

\subsection{Tag prediction}

\paragraph{Dataset and baselines.}
To test scalability of our approach, further evaluation is carried on the~YFCC100M dataset~\cite{ni15} which consists
of almost~100M images with captions, titles and tags.
We focus on predicting the tags according to the title and caption~(we do not use the images).
We remove the words and tags occurring less than~100 times and split the data
into a train, validation and test set.  The train set contains~91,188,648
examples~(1.5B tokens). The validation has~930,497 examples and the test set~543,424. 
The vocabulary size is~297,141 and there are~312,116 unique tags.
We will release a script that recreates this dataset so that our numbers could be reproduced.
We report precision at~1.

We consider a frequency-based baseline which predicts the most frequent tag. We
also compare with Tagspace~\cite{weston2014tagspace}, which is a tag prediction
model similar to ours, but based on the~Wsabie model of
~\newcite{weston2011wsabie}.  While the Tagspace model is described using
convolutions, we consider the linear version, which achieves comparable
performance but is much faster.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{\hspace{2pt}}l@{\hspace{2pt}}ccc}
\toprule
\multirow{2}[3]{*}{Model} & \multirow{2}[3]{*}{prec@1} & \multicolumn{2}{c}{Running time}\\
\cmidrule(l){3-4}
& &  Train & Test \\
\midrule
Freq. baseline           & 2.2  & -    & - \\
Tagspace,          & 30.1 & 3h8  & 6h \\
Tagspace,         & 35.6 & 5h32 & 15h \\
\midrule
\texttt{fastText},           & 31.2 & 6m40 & 48s \\
\texttt{fastText}, , bigram  & 36.7 & 7m47 & 50s \\ \texttt{fastText},          & 41.1 & 10m34  & 1m29 \\
\texttt{fastText}, , bigram & 46.1 & 13m38  & 1m37 \\ 
\bottomrule
\end{tabular}
\caption{Prec@1 on the test set for tag prediction on
YFCC100M. We also report the training time and test time.
Test time is reported for a single thread, while training uses 20 threads for both models.
}\label{tab:tag_res}
\end{table}

\paragraph{Results and training time.}
Table~\ref{tab:tag_res} presents a comparison of~\texttt{fastText} and the baselines.
We run~\texttt{fastText} for~5 epochs and compare it to~Tagspace for two sizes of the
hidden layer,~i.e.,~50 and~200.
Both models achieve a similar performance with a small hidden layer, but adding
bigrams gives us a significant boost in accuracy. At test
time, Tagspace needs to compute the scores for all the classes which makes it
relatively slow, while our fast inference gives a significant speed-up when the
number of classes is large~(more than~300K here). Overall, we are more than an order of magnitude
faster to obtain model with a better quality. The speedup of the test phase is
even more significant~(a~600 speedup). Table~\ref{tab:tag_ex} shows
some qualitative examples.
