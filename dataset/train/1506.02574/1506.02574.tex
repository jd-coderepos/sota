\documentclass[11pt]{article}
\usepackage[dvipsnames,usenames]{color}
\usepackage{amsfonts,amssymb,amsthm,mathtools}
\usepackage{paralist}
\usepackage[ruled, noend, noline, linesnumbered]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{xspace}
\usepackage{xspace,prettyref}
\usepackage{framed}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{mathbbol}
\usepackage[pagebackref,letterpaper=true,colorlinks=true,pdfpagemode=none,urlcolor=blue,linkcolor=blue,citecolor=BrickRed,pdfstartview=FitH]{hyperref}
\usepackage{fullpage}

\newcommand{\savehyperref}[2]{\texorpdfstring{\hyperref[#1]{#2}}{#2}}
\newcommand{\comment}[1]{ {\color{BrickRed} \footnotesize[#1]}\marginpar{\footnotesize\textbf{\color{red} To Do!}}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemmata}[theorem]{Lemmata}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{subclaim}{Subclaim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{observation}{Observation}

\newcommand{\EX}{\hbox{\bf E}}
\newcommand{\E}{\EX}
\newcommand{\pr}{{\rm Pr}}
\newcommand{\var}{{\rm var}}

\def\R{\RR}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\def\eqdef{~\triangleq~}
\def\eps{\varepsilon}
\def\bar{\overline}
\def\floor#1{\lfloor {#1} \rfloor}
\def\ceil#1{\lceil {#1} \rceil}
\def\script#1{\mathcal{#1}}

\newenvironment{proofof}[1]{\smallskip\noindent{\bf Proof of #1:}}{\hspace*{\fill}\par}



\def\B{{\mathsf D}}
\def\Hyp{{\sf Hyp}}
\def\D{{\sf K}}
\def\G{{\mathsf G}}
\def\d{{\sf d}}
\def\a{{\mathfrak a}}
\def\b{{\mathfrak b}}
\def\c{{\mathfrak c}}
\def\db{{\mathfrak d}}
\def\e{{\mathfrak e}}
\def\f{{\mathfrak f}}
\def\h{{\mathfrak h}}
\def\g{{\mathfrak g}}


\newcommand{\hu}{\widehat{u}}

\newcommand{\hcd}{\mathsf{hcd}}
\newcommand{\VG}{VG}
\newcommand{\md}[1]{\ (\operatorname{mod} #1)}

\newcommand{\cupdot}{\sqcup}



\newcommand{\cA}{{\cal A}}
\newcommand{\cB}{{\cal B}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cG}{{\cal G}}
\newcommand{\cH}{{\cal H}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cJ}{{\cal J}}
\newcommand{\cL}{{\cal L}}
\newcommand{\cM}{{\cal M}}
\newcommand{\cP}{{\cal P}}
\newcommand{\cR}{{\cal R}}
\newcommand{\cS}{{\cal S}}
\newcommand{\cT}{{\cal T}}
\newcommand{\cU}{{\cal U}}
\newcommand{\cV}{{\cal V}}



\newcommand{\bS}{{\bf S}}
\newcommand{\bone}{\mathbb{1}}



\newcommand{\NN}{\mathbb{N}}



\newcommand{\tF}{\widetilde{F}}
\newcommand{\tG}{\widetilde{G}}

\newcommand{\Sec}[1]{\hyperref[sec:#1]{\S\ref*{sec:#1}}} \newcommand{\Eqn}[1]{\hyperref[eqn:#1]{(\ref*{eqn:#1})}} \newcommand{\Fig}[1]{\hyperref[fig:#1]{Fig.\,\ref*{fig:#1}}} \newcommand{\Tab}[1]{\hyperref[tab:#1]{Tab.\,\ref*{tab:#1}}} \newcommand{\Thm}[1]{\hyperref[thm:#1]{Theorem\,\ref*{thm:#1}}} \newcommand{\Lem}[1]{\hyperref[lem:#1]{Lemma\,\ref*{lem:#1}}} \newcommand{\Prop}[1]{\hyperref[prop:#1]{Prop.~\ref*{prop:#1}}} \newcommand{\Cor}[1]{\hyperref[cor:#1]{Corollary~\ref*{cor:#1}}} \newcommand{\Def}[1]{\hyperref[def:#1]{Definition~\ref*{def:#1}}} \newcommand{\Alg}[1]{\hyperref[alg:#1]{Alg.~\ref*{alg:#1}}} \newcommand{\Ex}[1]{\hyperref[ex:#1]{Ex.~\ref*{ex:#1}}} \newcommand{\Clm}[1]{\hyperref[clm:#1]{Claim~\ref*{clm:#1}}} 


\def\yes{{\sf YES }}
\def\no{{\sf NO }}
\def\Lip{{\sf Lip}}
\def\poly{{\sf poly}}

\newcommand{\ab}{\hbox{ab}}
\newcommand{\abs}[1]{{|#1|}}
\newcommand{\proj}{{\mathsf{proj}}}
\newcommand{\drop}{\hbox{drop}}
\newcommand{\dist}[2]{\eps_{#1,#2}}
\newcommand{\hM}{\widehat{M}}
\newcommand{\hX}{\widehat{X}}
\newcommand{\davg}{\overline{d}}
\newcommand{\hatd}{\widehat{d}}
\newcommand{\otilde}{\widetilde{O}}
\newcommand{\const}{10}
\newcommand{\hash}{\hbox{hash}}

\newcommand{\adderr}{\Delta}
\newcommand{\mulerr}{\eps}

\newcommand{\C}{\hbox{C}}
\newcommand{\tC}{\widetilde{\hbox{C}}}
\newcommand{\ct}{\hbox{ct}}
\newcommand{\tg}{\hbox{TG}}
\newcommand{\cdf}{\hbox{C}}
\newcommand{\seen}{{\hbox{seen}}}
\newcommand{\rd}{{\hbox{red}}}
\newcommand{\loss}{{\hbox{loss}}}
\newcommand{\hN}{\widehat{N}}
\newcommand{\mult}{k}

\newcommand{\degdist}{{\tt headtail}}
\newcommand{\update}{{\tt update}}
\newcommand{\est}{{\tt estimate}}

\newcommand{\flo}[1]{\lfloor #1 \rfloor}

\newcommand{\Sesh}[1]{{\color{red} Sesh says: #1}}
\newcommand{\Andrew}[1]{{\color{red} Andrew says: #1}}
\newcommand{\Olivia}[1]{{\color{red} Olivia says: #1}}

\hyphenation{op-tical net-works semi-conduc-tor}

\author{Olivia Simpson\thanks{Work was done while the author
was an intern at Sandia National Laboratories, Livermore.}
\\ {\tt osimpson@ucsd.edu}\\
University of California, San Diego
\and C. Seshadhri
 \\ {\tt scomandu@ucsc.edu}\\
University of California, Santa Cruz
\and Andrew McGregor
\\ {\tt mcgregor@cs.umass.edu}\\
University of Massachusetts, Amherst
}

\title{Catching the head, tail, and everything in between: a streaming algorithm for the degree distribution}
\date{}

\begin{document}

\maketitle

\begin{abstract}
The degree distribution is one of the most fundamental graph properties of interest
for real-world graphs. It has been widely observed in numerous domains that graphs
typically have a \emph{tailed} or \emph{scale-free} degree distribution. While the average
degree is usually quite small, the variance is quite high and there are vertices
with degrees at all scales. We focus on the problem of approximating the degree distribution
of a large streaming graph, with small storage. We design an algorithm \degdist{}, whose main novelty
is a new estimator of infrequent degrees using truncated geometric random
variables. We give a mathematical analysis of \degdist{}
and show that it has excellent behavior in practice. We can process streams with millions
of edges with storage less than  and get extremely accurate approximations
for \emph{all} scales in the degree distribution.

We also introduce a new notion of \emph{Relative Hausdorff} distance between tailed
histograms. Existing notions of distances between distributions are not suitable, since they ignore
infrequent degrees in the tail. The Relative Hausdorff distance measures deviations at all scales,
and is a more suitable distance for comparing degree distributions. By tracking this new measure,
we are able to give strong empirical evidence of the convergence of \degdist.
\end{abstract}

\section{Introduction}

Graphs are a natural abstraction for any data set with entities and relationship between them. Popular examples include online social networks such as Facebook and Twitter;  transportation networks; biological networks such as protein-protein interaction and metabolic networks; and communication networks such as the internet and telephone and email networks.
Many of these graphs are most naturally represented by
a \emph{stream of edges}. Especially for social and communication networks, each edge has an associated timestamp, and the graph
is basically an aggregate of all these edges over some time window.
Such streams are typically quite massive; social networks like Facebook and Twitter can generate billions of communication links in a day~\cite{KwLe10,FB}.
A publicly available HTTP request dataset has billions of requests~\cite{Meiss08WSDM}. The scale of these data sizes has led
to interest in \emph{small-space streaming algorithms}. Such algorithms accurately compute specific properties
of the total graph, using a memory footprint that is orders of magnitude smaller in size.

Arguably, one of the most important properties of real-world networks is the \emph{degree distribution}. Seminal papers
in massive graph analysis studied precisely this quantity~\cite{BarabasiAlbert99,FFF99,BrKu+00}.
The study of degree distributions
is probably the birthplace of real-world network analysis. It has been found to be relevant for graph modeling, network resilience, and algorithmics~\cite{CoEr+00,NeStWa01,PeFlLa+02,Ne03,Mi03,ChFa06,SeKoPi11}.
One of the key discoveries of network analysis is the presence
of \emph{scale-free} or heavy-tailed degree distributions. The average degree of a node is usually small,
but there are nodes with degrees at all scales. The very notion of a \emph{scale-free network} has entered the common parlance
because of its relevance to network analysis~\cite{Wiki-scale}.

\subsection{Problem statement} \label{sec:problem}

The input is a stream of edges  without any repetitions.
The graph created by these edges is denoted . For convenience,
we set , though the labels may be from some arbitrary discrete universe.
We do not assume that the algorithm knows  and , the number of vertices
and edges respectively.
Each edge is represented by a pair  of vertex labels.

For vertex ,  denotes its degree (the number of neighbors of ).
We set  to be the number of vertices of degree , and  to be the number
of vertices of degree at least . In math, .
It is convenient for us to work with unnormalized raw counts, so we deal
with histograms rather than distributions.
We denote the sequence  by the \emph{degree histogram} (dh) and
 is the \emph{complementary cumulative degree histogram}\footnote{This is often
called the cumulative degree distribution, but that is counter to the standard
definition for probability distributions.} (ccdh).
When  is normalized by , it is called the degree distribution.
We focus on the ccdh, instead of the dh. Typically, the dh is quite noisy in real data, and
the ccdh has the added benefit of being monotonically decreasing. (Focus on the ccdh is standard
for fitting procedures~\cite{ClShNe09}.)

We study the problem of approximating the ccdh of  using a \emph{small-space one-pass streaming algorithm}.
Such an algorithm has some limited memory, denoted . It sees the edges in stream order,
and on seeing edge , updates the memory . The algorithm cannot access older edges,
and  is typically order of magnitudes smaller than the size of the stream. At the end of the stream,
the algorithm reports a sequence , an approximation to the ccdh of .

We make no assumption on the ordering of edges.
We do not consider edge deletions or edge repetitions. (This is the standard
model used in most work on practical streaming algorithms.)



\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{skitter_ccdd}
        \caption{as-Skitter: , , storage }
        \label{fig:skitter-ccdh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{livejournal_tailspace_04.png}
        \caption{com-LiveJournal: , , storage }
        \label{fig:livejournal_tailspace_04}
        \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{orkut_tailspace_04.png}
        \caption{com-Orkut: , , storage }
        \label{fig:orkut_tailspace_04}
    \end{subfigure}
    \caption{The output ccdh of \degdist{} on three different input graphs from
the SNAP~\cite{snap} collection. In each case, the storage is less  of the stream
    (and less than  of the number of vertices). Observe the near identical match
    with the true ccdh.}
    \label{fig:ccdh}
\end{figure*}

\subsection{Challenges}

\textbf{How does a small-space algorithm estimate the degree distribution at all scales?}
 The degree distribution involves degrees at ``all" scales:
many low degree vertices, some intermediate degree vertices, and few very high degree vertices.
Look at~\Fig{skitter-ccdh} for the ccdh of a router topology network. The average degree is ,
but there are vertices with degrees up to .
The count of low degree vertices is easy to estimate, since a simple random sample of vertices gives a good estimate.
Intermediate and high degrees pose a problem. There are few such vertices but it is critical to sample
their count accurately. There is a huge literature on estimating distribution properties
of a stream of items: frequent items, distribution moments, distinct items, etc.~\cite{IndykW05,KaneNW10,cormode2008frequent}.
(We discuss in depth later.) But these only give specific properties of the distribution. None of these methods can get frequency estimates at \emph{all scales}, ranging continuously from (frequent)
low degrees to (infrequent) high degrees.

\textbf{How to quantitatively compare (cumulative) degree distributions?} How do we actually assert that our algorithm
is any good?
One can use standard statistical distance measures like Kolmogorov-Smirnov.
Yet these measures typically ignore
the tail since it contains a negligible fraction of vertices. Consider the following examples. We take a clique
of  vertices and a clique of  vertices. It is natural to say that their degree distributions are quite close,
but no popular existing measure would assert that. On the other end, consider a star with  edges, and a matching with 
edges. The degree distribution only differs at one ``point", the vertex of degree . Yet we would consider
the degree distributions to be fundamentally different. Most statistical measures would say they are similar, since
they differ at only a single outlier.

An intuitive notion of similarity is closeness in log-log plots, but how do we quantify such a concept? One might
try to approximate degree distributions by closed-form, but fitting procedures are notoriously tricky
for tailed distributions and subject to much error~\cite{ClShNe09}.

\subsection{Main results}


\textbf{The algorithm \degdist:} Our main contribution is a new small-space algorithm \degdist{} that estimates
the ccdh of an input graph stream. The novelty is a new estimator for infrequent
degree counts, which is combined with standard sampling to give
ccdh estimates at all scales. We represent the sampling of \degdist{} through certain \emph{truncated geometric}
random variables. An analysis of their behavior provides the right ``correction" factors
to infer the ccdh from our sampling.
We provide a detailed mathematical analysis of \degdist{} explaining why it accurately
estimates the ccdh. Our analysis falls short of a complete proof, and we rely
on some heuristic arguments for the full argument.

\textbf{Relative Hausdorff distance:} We introduce a new notion of distance between
ccdhs (technically, between any two histograms) called the \emph{Relative Hausdorff (RH) distance}.
This distance avoids the pitfalls of standard measures, and is able to capture the closeness
at \emph{all scales}. Intuitively, a small RH-distance implies that \emph{every} point in one ccdh
is ``close" (up to relative error) to some point in the other ccdh. Put another way, both ccdhs
agree at all scales, and agree on outliers. While this condition is quite stringent, RH distance
is flexible enough to allow for minor errors. It gives a concrete way of quantifying the quality
of \degdist, and empirically establishing convergence of our estimate.

\textbf{Empirical behavior of \degdist:} We run \degdist{} on a wide variety of public graph datasets.
It gives excellent estimates of the ccdh in all our tests, for storage less than 1\% of the stream.
We show example outputs in \Fig{ccdh}, for three different input graphs. In each case, observe
the near perfect match with the true ccdh, at \emph{all} degrees.
We compute the RH distance for numerous runs and demonstrate convergence of \degdist{}'s output
with increasing storage. In all our runs, storage around 1\% of the stream is sufficient for
excellent match in ccdhs (and also for low RH-distance).



\subsection{Related Work}
\label{sec:relatedwork}
Note that we can frame our problem in terms of general histogram estimation.
If one views the input as a stream of vertex labels, then the dh (and ccdh)
is the histogram of label frequencies. There is much work on understanding frequencies in a discrete
stream, but as we detail below, none of this work solves the problem of estimating the ccdh.

Finding frequent items, aka ``heavy hitters," is a classic problem in the data stream model.
Cormode and Hadjieleftheriou \cite{cormode2008frequent} compare three of the most important algorithms: the
\emph{frequent} algorithm~\cite{demaine2002frequency, karp2003simple,BerindeICS10}, the
\emph{lossy counting} algorithm~\cite{manku2002approximate}, and the \emph{space
saving} algorithm~\cite{metwally2005efficient}.\footnote{Other popular algorithms such as CountSketch~\cite{charikar2002finding} and
CountMin~\cite{cormode2005improved} enable frequent items to be identified when the frequency of an item may be incremented and decremented.}
For large degrees, these approaches will give accurate results, but the error term dwarfs the degree at smaller scales. We demonstrate this empirically in Section \ref{sec:expresults}.
Much work has been done in approximating frequency moments~\cite{AlonMS99,IndykW05,KaneNW10,cormode2008frequent}, but they do not
give an estimate for multiple scales. Nor has this work been implemented in practice for large data sets.

Rather than just finding frequent items, Korn et al.~\cite{korn2006modeling} attempt to estimate the entire distribution of elements in the stream. However, in contrast to our work, their approach assumes that the distribution comes from a parameterized family of distributions, e.g., the distribution is Zipfian, and then focuses on estimating the relevant parameters. This approach is only applicable for graphs where the degree distribution is already relatively well understood.
Despite much study and claims, there are \emph{no} conclusive closed-form formulae for real-world degree distributions.
The classic power law fitting work of Clauset et al.~\cite{ClShNe09} argues why most previous methods are not statistically robust,
and how one needs strong independence assumptions to get rigorous results. Therefore, \degdist{} makes no closed form assumption
on the input stream.

Over the last ten years, there has been a growing body of work focused on processing graphs in the data stream model. See~\cite{mcgregor2014graph} for a summary of recent work on graph streaming and sketching. This work has included problems such as the number of triangles and related quantities such as the transitivity coefficient \cite{jha2013space,PavanTTW13,ahmed2014graph}, estimating the connectivity properties of a graph \cite{GuhaMT15}, and solving combinatorial problems such as computing large matchings \cite{KapralovKS14,McGregor05}.
Cormode and Muthukrishnan considered estimating properties of the degree
distribution in multigraphs but not the distribution itself\cite{CormodeM05}.

Closest to this work is the series of graph sampling papers by Ahmed et al.~\cite{ayman2013ads,ahmed12socialnets,ahmed2014network,ahmed2014graph}.
Their work focuses on estimating many properties (as opposed to a single property) with a fixed sampling method, and they study various sampling schemes.
The results on estimating ccdhs typically use 20-30\% of the stream, with weaker empirical results~\cite{ayman2013ads}.
The recent Graph Sample and Hold framework gives extremely strong results for triangle counting~\cite{ahmed2014graph}, but is
not applied for the ccdh.
This technique is closely related to an approach for estimating frequency moments
\cite{AlonMS99,braverman2013approximating}. Our sampling approach is also similar, and our main
contribution is in the actual estimation procedure.








\section{The algorithm}

The algorithm \degdist{} has two parts: \update{} and \est.
The procedure \update{} is called for every edge in the stream, and simply
updates the data structures. The procedure \est{} is called at the end
of the stream to get an estimate of . In what follows,
the subscript  refers to ``head" and  is ``tail".

The algorithm \degdist{} requires two parameters,  and , which
are probabilities. These decide the storage requirements of the algorithm,
as explained later. For convenience, we will assume these are global variables,
and will not pass them around to each function.

We will assume the existence of a hash function  that maps strings
uniformly to .

{\bf Data Structures:} There are two sets of vertices  and ,
and corresponding maps  and .
Again, we assume these are global variables.

{\bf The procedure \update:} This updates the data structures for each edge in the stream.
Consider edge  in the stream.
If , the  is incremented (analogously for ). Now for the critical
difference between  and . If  and if ,
then  is added to . If : we insert  to  with probability .
(The entire operation above is also done for .)
Note the difference: for , we essentially flip a random coin for the vertex. For , we flip a coin for the edge.
Intuitively,  is maintaining a uniform random set of vertices. On the other hand,
 maintains sample of vertices biased towards higher degree.

{\bf The procedure \est:} This procedure uses  to output
an estimate  for the ccdh of . We set  to be the number
of vertices in  with  value of  (similarly for ).
One can think of this as the ``observed" degree distribution. The scaling of
 is straightforward: we simply consider  to be an estimate
of .  By summing these appropriately, we get an estimate (the head
estimate) of .


For , we first do an additive ``correction". So we set ,
where  is a correction factor. The explanation of this factor is
provided in Section~\ref{sec:math}.
Then, we do a biased scaling and consider  as an estimate of .
Again, by taking partial sums, we have an estimate (the tail estimate) of .

Observe that we have two different estimates of . We prove in our mathematical analysis
that the former is accurate for the head of the distribution, while the latter is appropriate
for the tail. This distinction is made by , which is chosen to ensure
that the first estimate has low variance. Hence, for all degrees less than , we
use the head estimate, and for the remaining, we use the tail estimate.

\medskip
We now give a formal description of the algorithm.



\begin{algorithm}
\caption{\degdist} \label{alg:degdist}
\DontPrintSemicolon
Initialize empty sets  and  and empty mappings  and .\;
For each edge  in the stream,\;
\ \ \ \ Call \update.\;
Call \est{} to get output estimate for .\;
\end{algorithm}

\begin{algorithm}
\caption{\update} \label{alg:update}
\DontPrintSemicolon
If , increment .\;
If : if , insert  in  and set .\;
If , increment .\;
If : with probability , insert  in  and set .\;
(Repeat above steps for .)
\end{algorithm}

For fixed , we define  to be:



\begin{algorithm}
\caption{\est} \label{alg:est}
\DontPrintSemicolon
Let  be the number of vertices in  with count exactly . (Similarly, define ).\;
For all counts , set .\;
For all counts :\;
\ \ \ \ Set .\;
\ \ \ \ Set .\;
Set  to be largest  such that .\;
For all degrees :\;
\ \ \ \ If , set .\;
\ \ \ \ If , set .\;
\end{algorithm}

\section{Mathematical Analysis} \label{sec:math}

We abstract out the behavior of the algorithm in a series of claims.
We stress that all our theorems are independent of graph stream order, and hence
\est{} works for all orderings.

\begin{definition} \label{def:geo} For any positive integer  and , the \emph{truncated
geometric distribution}  has the pdf: , .
\end{definition}

Observe that as , this is a standard geometric random variable.

\begin{lemma} \label{lem:samp-head} For every ,  is inserted in  independently with probability . Conditioned on , .
\end{lemma}

\begin{proof} We assume that  is a uniform random function, so  is uniformly distributed in .
The probability that  is exactly . Observe that if , then 
is inserted in  at the very first occurrence of  in the stream. Hence , whenever .
\end{proof}

\begin{lemma} \label{lem:samp-tail} For every ,  is inserted in  independently with probability . Conditioned on , , where .
\end{lemma}

\begin{proof} There are  occurrences of  in the stream. The probability
of  being added in the th occurrence is . When this happens,
. The probability that  is never added is .
Conditioned on  being added to , the probability of  being added
in the th occurrence is exactly .
So  is distributed as .
\end{proof}

\begin{lemma} \label{lem:geo} The expected value of  is .
\end{lemma}

\begin{proof} Using the bound for the sum of an arithmetico-geometric series:

\end{proof}

This expression is exactly (up to rounding) .
Conditioned on ,  is  minus a ``loss" term, which is precisely
the expression in \Lem{geo}. That should hopefully explain the use of  in our algorithm.
We make the (admittedly wrong) assumption that \emph{every} vertex of degree  in 
``loses" exactly the expected loss. In other words, we assume that  is .
To infer the number of degree  vertices in , we add back the expected loss
to each vertex in . That is why we set .

\medskip

It is fairly easy to bound the space and running time of \degdist.

\begin{theorem} \label{thm:time} The expected space used by \degdist{} is .
The expected running time of \update{} is , and the expected running time of \est{} is .
\end{theorem}

\begin{proof} We will store all sets as hash tables, to ensure  updates.
By \Lem{samp-head}, each vertex is added to  with probability . Hence, the expected size
of  is . For each edge in the stream, we potentially add a vertex
to  with probability . Hence, the expected size of  is . (This is a gross
upper bound, and a refined bound based on \Lem{samp-tail} would be .)

The processing of \update{} only requires addition in set and count increments,
and requires  time. The procedure \est{} runs in time linear in the sets  and .
\end{proof}


\subsection{The estimators} \label{sec:est}

For the analysis of our estimators, we need to introduce various error parameters. Natually,
the actual implementation \est{} simply sets these to be fixed constants, so we make
slight modifications and assumptions for convenience of analysis.

Let  be an error parameter, and let  be a sufficiently large constant.
\begin{asparaitem}
    \item We set  to be the largest  such that .
    (In the implementation, we hardcoded  to be .)
    \item We assume that  is chosen so that .
\end{asparaitem}

\medskip

We begin with the analysis of the head estimator, which is a straightforward Chernoff bound application.

\begin{lemma} \label{lem:head} For all , .
With probability , for all , .
\end{lemma}

\begin{proof} Fix some . Note that the head estimator
is used for . Also,  is precisely the number of vertices of degree at least  in .
For convenience, denote this by , and observe that it is monotonically decreasing in .
By \Lem{samp-head}, each vertex is added independently to  with probability .
Thus, .
Note that  is precisely , so .

Since  is itself a random variable, we need a little care to prove the lemma.
Observe that  is well-defined for all , and is the sum of Bernoulli random
variables. By a multiplicative Chernoff bound (refer to Theorem 1.1 in~\cite{DuPa}), .
Furthermore, by an alternate bound, if , then .

When , apply the first bound.
When , apply the second bound with .
Finally, we apply the union bound over all errors, which a calculation shows to be .
Hence, for any  where , .
So,  must be smaller than any such degree. Thus, for all ,
, and the first Chernoff bound gives the desired
concentration.
\end{proof}

The more challenging part is to analyze the tail estimator. We fall short of giving a complete
proof that it works. Nonetheless, we provide
some mathematical evidence of its correctness. We provide a high level explanation
of the math that follows. We warn the reader that we shall switch between estimates for 
and .

The weakness of the head estimator is made clear in the proof of the previous lemma.
The Chernoff bounds says that the error probability of estimating of  is roughly .
This goes to  as  becomes smaller than . That is precisely what happens
in the tail of the degree distribution, which contains fewer vertices of higher degree.
In general, mild fluctuations in estimates for low degree vertices is ok (there are many of them),
but even a little wagging in the tail estimates creates significant error.

But high degree vertices are more likely to be in  by \Lem{samp-tail}.
Let  denote the subset of degree  vertices in .
We show in \Lem{highdeg} how to get an estimate of  from ,
where the error probabilities are roughly .
\emph{Note the extra  factor}. As long as , we can
hope for concentration. In other words, even though high degree vertices are infrequent,
it is provably possible to get accurate estimates for these counts.

Unfortunately, it is not clear how to estimate , since 
is quite different from . As mentioned earlier, we
make the (admittedly erroneous) assumption that ,
based on \Lem{samp-tail} and \Lem{geo}. This is used to predict the actual degree
of , based on . While this assumption is wrong because the truncated
geometric distribution has large variance, in practice, it works quite well.

In \est, the proxy for  is given by . We show
that the ``ccdh" (or partial sums) of  approximates those of .
In other words, we can a get a rough approximation for the number of vertices
of degree at least  in . This is what is proven in \Thm{tc} and the subsequent calculations.

We now proceed with the formal proofs. The following lemma provides an appropriate concentration bound
for estimating  from .

\begin{lemma} \label{lem:highdeg} For all , . For all
 and sufficiently small : with probability at least ,
.
\end{lemma}

\begin{proof} Every degree  vertex is added to  with probability  (for convenience,
denote this by ).
Linearity of expectation proves that .
Note that  is the sum of Bernoulli random variables, each with expectation .
By the original Chernoff-Hoeffding bound~\cite{Ch52}, ,
where  denotes the KL-divergence. With some manipulations,

Now we use . A calculation yields
 for sufficiently small .
Hence, the expression above is bounded below by:

An analogous bound holds for the upper tail, and a union bound completes the proof.
\end{proof}

Hence, we would like to estimate  and divide by  to get estimates for
 (where  is large). Our estimate for  is , and this
scaling is precisely what is done in \est.

\begin{definition} \label{def:geocdf}
\begin{asparaitem}
    \item : The cdf of , formally .
    \item .
    \item .
\end{asparaitem}
\end{definition}

Indeed, we will show that the ``ccdh" of
 is somewhat approximated by that of .

\begin{theorem} \label{thm:tc} .
\end{theorem}

\begin{proof} Note that  is monotonically increasing in .
Any  such that  will be counted as part of , for some
. The quantity , conditioned in , is distributed as .
The probability of the loss being most  is exactly .

By \Lem{highdeg}, 
\end{proof}

\subsection{Making sense of \Thm{tc}} \label{sec:tc}

Fix  and .
Consider  as a function of , and suppose it had value  for , and value  for .
Think of this as the ideal value for this function.
Then, by \Thm{tc}, , which
would be exactly what we want. We prove that the ``coefficients" 
behave like a step function with a transition roughly at . So 
is a sort of smoothed version of .

We begin with some approximations for . It is useful
to think of the limit as 
and reparametrize as .
By \Lem{geo},


Thus, . Now consider some .




Clearly, as  becomes large, this expression goes to . The minimum possible
value of  is  (equivalently, ), for which
the expression is . It behaves roughly like a step function, with a transition point (roughly)
at . As  becomes large, the transition point is , close
to .
When  is small, the extra  additive terms
ensures the transition is closer to . Of course, as  becomes smaller, the function looks less
like a sharp transition function.
This is shown in \Fig{step}. We plot  according to \Eqn{cdf} for . The red vertical
line is  (so ), and we draw dashed vertical lines corresponding to value  and .
The width between the dashed lines is a rough measure of the error in approximation.
Observe how it is fairly close to a step function for , and is a coarser approximation for .


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{logisticC-k5}
        \caption{.}
        \label{fig:logisticC-k5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{logisticC-k10}
        \caption{.}
        \label{fig:logisticC-k10}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering \includegraphics[width=\textwidth]{logisticC-k100}
        \caption{.}
        \label{fig:logisticC-k100}
    \end{subfigure}
    \caption{Plots of  according to~\Eqn{cdf} for different values of . Note that  is set to . In each
    plot, the thin vertical line is , and the dashed and dotted lines
correspond to values of  and , respectively.}
    \label{fig:step}
\end{figure*}






Hence,  is much further from ,
and \est{} provides worse results. But we set . So for degrees
close to , we do not use the tail estimator.

\section{The Relative Hausdorff distance} \label{sec:rh}

One of the main challenges in experimentally validating the behavior of \est{} is
in defining a distance between ccdhs. As we hinted earlier, existing statistical distances
do not capture ``similarity" of ccdhs.
Motivated by concerns (detailed below), we define a new notion of distance between ccdhs (technically,
between cumulative complementary histograms).
This is inspired by the geometric notion of Hausdorff distance between subsets of a metric space.
We say a ccdh is non-trivial
if it contains some non-zero point.

\begin{definition} \label{def:hauss} Let  and  be non-trivial ccdhs. Fix non-negative numbers .
The distributions  and  are \emph{-close by Relative Hausdorff (RH) distance} if:

(An analogous condition holds with  and  switched.)

The \emph{RH}-distance between  and  (denoted ) is  FG(\eps,\eps).
\end{definition}

Note that the -distance can be greater than .
For  and , if  and  are -close,
they are also -close. Since  and  are non-trivial, we can set  to be large enough so
that for some ,  and  are -close. Thus, the RH distance always exists.
If , then  and  are identical.

Observe that RH distance tolerates error both in degree and frequency, which is very important for
comparing degree distributions.
The RH distance exactly captures the notion of being close in log-scale, but is a much more stringent condition.
It forces \emph{all} points in  to be close to some point in  (and vice versa).
All ``outlier" and tail behavior in  must be approximated in . For RH-close ccdhs, the maximum degrees
must be close, and furthermore, there must be approximate agreement for frequencies at all scales.

To understand numerics, we think it is useful think of an RH-distance  to be quite small.
Suppose  for a true ccdh  and our algorithm output . This means that
for every reported point  is within 5\% of some , where  is within 5\% of 
(and vice versa). Any RH distance greater than  is very large, since we only get closeness
when .

\subsection{Problems with KS-statistic}

Fix two ccdhs  and . A standard comparison metric is the Kolmogorov-Smirnov (KS) statistic, ,
where  are normalized as distributions. (So  is the fraction of vertices
with degree at least .)

We discuss specific problems with the KS statistic and show how RH avoids these pitfalls.
(The exact same issues also holds for normed distances, so we do not explicitly calculate these.)

\textbf{Comparing cliques:} Let  be the ccdh of an -clique and  be the ccdh of an -clique.
So , , , and all other values are .
The KS-statistic is actually  (which is extremely large), since  but .
This is inconsistent with our intuitive notion that these degree distributions are similar.
The RH distance is , since it allows for error in degree and frequency.

\textbf{Star vs matching:} Let  be the ccdh of a star with  vertices, and 
be the ccdh of a matching (disjoint edges) with  vertices. (Assume  is even.)
So , , , and other values are .
We also have , and all other values are . The values of  that are 
are insignificant compared to the dominant . A calculation shows ,
though we should probably consider them different.
On the other hand, . The ``outlier"  forces the  to be ,
since  for .

\textbf{Ignoring the tail:} Let  be the ccdh of the as-Skitter graph, as plotted in \Fig{skitter-ccdh}.
Let  be the same ccdh up to degree  and zero afterwards. In other words,  is identical to 
up to the ``tail" starting at degree . The fraction of vertices with degree  is at most .
A calculation shows that . So ignoring a large portion of the tail still yields
small KS-distance. The RH-distance is , since  needs to be large to handle
the tail of .

\section{Experimental Results} \label{sec:expresults}
We implemented the algorithm in Python and performed experiments on a Samsung
NP-QX411L laptop with an Intel Core i5-2450M 2.5GHz four core processor and
5.7GB of memory.  To simulate a stream, we convert a graph to a list of edges
stored in a text file, and read the file one line at a time.  In the case that
the graph is directed, we treat it as undirected by considering each edge as an
unordered pair of vertices.  Note that this may imply multi- or parallel edges,
though we calculate degrees for the actual ccdh respecting this notion.

We test the algorithm on a number of graphs from the SNAP~\cite{snap} and
KONECT~\cite{konect} collections, the statistics of which are summarized in
Table~\ref{table:manygraphs}.  We use the as-Skitter graph on 1.7M nodes and 11M
edges as a case study.

We use the phrase \emph{storage of \degdist} to indicate the total storage .
As explained in \Thm{time}, this depends on  and .

\subsection{Convergence of \degdist} \label{sec:conv}
We demonstrate how increasing the storage of \degdist{} leads to convergence of
the ccdh. We fix the as-Skitter graph. We increase the storage by letting 
range from  to  in increments of , and  range from 
to  in increments of .  For each setting of  and , we
perform five independent runs of \degdist. We also run ten independent runs
fixing .  For each such run, we compute the RH distance
between the output of \degdist{} with the true ccdh. The results are shown
in~\Fig{skitter_scatter}. Observe how the RH distance goes to zero as the
storage increases.  In particular, \degdist{} outputs a ccdh with RH distance as
small as  using 230K space.
\begin{figure}
\centering \includegraphics[width=3.0in]{skitter_scatter}
\caption{RH distance as the storage of \degdist{} increase.}
\label{fig:skitter_scatter}
\end{figure}

We do a more nuanced study of how  and  affect convergence.
In this experiment, we fix a value  and vary  in
increments of .  We repeat this process for .  The RH distances of the runs are plotted in
Figure~\ref{fig:skitter_summary_varytail}.  Each line in the plot corresponds to
a fixed  value, and the RH distances are plotted against .  We point
out that an RH distance of about  is achieved with head and tail
probabilities as small as , respectively, resulting in a total
sample size of 82K or  of the edge stream.
For each fixed , increasing  initially decreases the RH distance, but it eventually
converges to a non-zero value. This is because all the error is coming from the head estimate.
As we increase , the convergence value goes down to zero, as expected.
\begin{figure}
\centering \includegraphics[width=3.0in]{skitter_summary_varytail}
\captionsetup{width=0.9\textwidth}
\caption{RH distance of the estimate output by our algorithm as  and 
vary.  Each line in the plot correponds to a fixed value for , and plots
the RH distance as  varies.  A near optimal RH value is achived with  and , which yielded sample sets with .}
\label{fig:skitter_summary_varytail}
\end{figure}

\subsection{Results for various graphs}
Here we demonstrate the quality of the estimates output by \degdist~on a variety
of graphs.  Each of the graphs are from the SNAP graph collection~\cite{snap}
with the exception of the youtube and youtube-friendship graphs which are from
the KONECT~\cite{konect} collection.  The node and edge set sizes of each graph
are given in the second and third columns of Table~\ref{table:manygraphs},
respectively.  For each graph we include the storage of the
algorithm and the RH distance of the estimate for two example runs.
The storage is less than  in almost at runs, and certainly less than .
Observe how the RH distance is usually less than . In our worst examples,
(soc-Pokec and com-Orkut), the RH distance is less than . We stress that RH distance
is a rather stringent condition, since it requires closeness of the estimate
at \emph{all} degrees.

In \Fig{ccdh} of the introduction, we have plotted the actually ccdh and the output
of \degdist{} for three of these graphs.
Observe the near identical match in all examples.

\begin{table}
\small
\centering
\begin{tabular}{l|c|c|r|l}
\hline
Graph &  &  & Space & RH distance\\
\hline\hline
youtube & 1.1M & 3M & 21K & \\
& & & 90K & \\\hline
wiki-Talk & 2.3M & 5M & 38K & \\
& & & 74K & \\\hline
youtube-friendship & 3M & 9M & 80K & \\
& & & 196K & \\\hline
as-Skitter & 1.7M & 11M & 31K & \\
& & & 69K & \\\hline
soc-Pokec & 1.6M & 30M& 75K & \\
& & & 212K & \\\hline
com-LiveJournal & 4M & 34M& 335K & \\
& & & 467K & \\\hline
com-Orkut & 3M & 117M& 273K & \\
& & & 387K & \\\hline
\end{tabular}
\caption{Performance of \degdist~for a number of graphs.}
\label{table:manygraphs}
\end{table}

\subsection{Errors at different scales}\label{sec:differentscales}
Here we investigate how well \degdist{} performs at different scales.
Specifically, we measure the error of a ccdh estimate at each degree.  Let 
be the ccdh of the as-Skitter graph, and  be the \degdist{} output.
The RH distance is maximized over all degrees, so we do a more detailed
analysis of the estimate errors.
We fix a value for  and for each
degree  compute the minimum value  such that  where  and vice versa.
In words, we are ``opening up'' the definition of RH-distance and looking at the
profile for every degree.

We performed a run of \degdist{} with  and  for the as-Skitter
graph. This used a storage of 31K ( of stream). We then plot in \Fig{allscales}
the corresponding  values with  set to  .
The red `\textsf{x}' markers denote the -values for \degdist{} (the
other markers are explained later). Observe how the  values
are quite small throughout, and peak at degree  to roughly .
In this case, \degdist{} achieves an RH-distance of about  with 31K space.

\begin{figure}
\centering \includegraphics[width=3.0in]{epserr1allfour}
\captionsetup{width=0.9\textwidth}
\caption{RH distances at different degrees.  We plot the -distance for
.  The red `\textsf{x}' markers correspond to an estimate output
by~\degdist{} using a storage of 31K.  The estimate is -far from
the true ccdh.  The rest of the plots correspond to combinations of the head
estimator using 17K space and the heavy hitter algorithms using 34K space for a
total of 51K space.  The \emph{lossy counting} estimate is -far from
the true ccdh, the \emph{space saving} estimate -far and the
\emph{frequent} estimate is -far from the true ccdh.}
\label{fig:allscales}
\end{figure}



\subsection{Comparing to other methods}\label{sec:compare}

While there is no existing small-space algorithm that has demonstrable convergence
to the ccdh, there are numerous algorithms to only capture the tail.
These are classic ``heavy hitters" algorithms: the \emph{frequent} algorithm~\cite{demaine2002frequency,
  karp2003simple,BerindeICS10}, the \emph{lossy counting}
algorithm~\cite{manku2002approximate}, and the \emph{space saving}
algorithm~\cite{metwally2005efficient}. We study the performance of these methods.
For convenience, we use ``head estimator" to denote the algorithm that simply
takes uniform samples of vertices and uses their degrees to estimate the full ccdh.
This is basically what \degdist{} employs for .


We fix the as-Skitter graph, and set the storage used by these algorithms to 35K. (Note that with storage 31K, \degdist{} gives an estimate
with RH-distance less than .) We show the resulting estimates of these algorithms
in \Fig{compare}. Not surprisingly, none of these algorithms give reasonable estimates
for , where .

At the face of it, the above algorithms perform reasonably well on the tail. The head estimator
(which is quite simple) seems to work well for the head. Could we just combine these algorithms,
and outperform \degdist? We show that this is not the case. Crucially,
none of these algorithms actually get accurate estimates even
at the moderate to high degrees, despite the apparent closeness in the log-log plot of \Fig{compare}.

We convert the existing algorithms for the full ccdh, by combining with the head estimator.
Pick (say) the algorithm \emph{frequent}.
We first run the head estimator with 20K space. We choose an appropriate , where we apply the head
estimator for , and \emph{frequent} for .
We pick the  that minimizes the RH distance to .
We do the same for each of \emph{frequent}, \emph{space saving}, and \emph{lossy counting}.
Note that we are being extra generous to the competing methods. First, the total
storage used is about 50K. Furthermore, we choose the  to minimize RH distance,
while \degdist{} chooses it based on a fixed formula.

The RH distance we achieved was  (\emph{frequent}),  (\emph{space saving}), and  (\emph{lossy counting}).
All of these used storage 50K. In contrast, \degdist{} had RH distance of  with 31K storage.
We measure the errors at all scales in~\Fig{allscales}, for all these algorithms. This is exactly
using the explanation in previous section, by setting , and plotting the 
values for all the estimates.

We immediately see how the -values (errors) for all the competing procedures are much higher than \degdist.
Indeed, for degrees around , the errors of the other procedures are extremely high, despite higher storage.
We see that \degdist{} handily beats all the procedures, at pretty much all scales simultaneously.
In \Fig{frequent_head}, we plot the output ccdh for the head estimator combined with \emph{frequent}.
As expected from \Fig{allscales}, we see a fair amount of fluctuation from the true ccdh in the the intermediate
to high degrees. We stress that a small fluctuation in a log-log plot is actually a fairly large error
in the RH measure.

For completeness, we increase the storage of the competing methods to get RH distance of around .
For all the other algorithms, we require storage more than 150K to get comparable
error to what \degdist{} gives with 31K storage.


\begin{figure*}
\centering
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{compare}
    \captionsetup{width=0.9\textwidth}
    \caption{ccdh estimates output by the \emph{frequent}, \emph{lossy counting},
and \emph{space saving} algorithms each using a storage of 35K.\\}
    \label{fig:compare}
\end{minipage}\begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{frequent_head}
    \captionsetup{width=0.9\textwidth}
    \caption{ccdh estimate output by the head estimator combined with the
\emph{frequent} algorithm using a storage of 50K.  The RH distance
is .}
    \label{fig:frequent_head}
\end{minipage}
\end{figure*}












\subsection{Results for different stream orderings}
As stated previously, our algorithms do not assume any stream order.  In this
section we test the performance of the algorithm when provided the stream in
different orderings.  We use six different orderings in total.  The first three
are different random orderings.  The second three are each edgelists (that is,
all the edges adjacent to a particular node are read in sequence), but the
orderings of the nodes are different.  In one, we read the nodes of highest
degree first, in another we read the nodes in increasing order of degree, and in
the last we consider a random ordering of the nodes.  In each experiment we let
 and .  The standard deviation of the RH distances for
each ordering is .  Table~\ref{table:ordering} summarizes the RH distance
of estimated ccdhs with different stream orderings.

\begin{table}
\small
\centering
\begin{tabular}{l|l}
\hline
Ordering & RH distance\\
\hline\hline
Random1 & \\\hline
Random2 & \\\hline
Random3 & \\\hline
Edgelist: Decreasing order of degree & \\\hline
Edgelist: Increasing order of degree & \\\hline
Edgelist: Random & \\\hline
\end{tabular}\captionsetup{width=0.9\textwidth}
\caption{Performance of \degdist~for different stream orderings.  The first
three are different random stream orderings.  The second three are edgelists
permuted by the nodes.  In each trial .}
\label{table:ordering}
\end{table}














\section*{Acknowledgment}

The authors would like to thank Tammy Kolda, Ali Pinar, and David Mayer for useful discussions.
Much of this work was done in Sandia National Laboratories, Livermore, and funded by the DARPA
GRAPHS program.











\bibliographystyle{IEEEtran}
\bibliography{streaming-degree,streaming_triangles}

\end{document}
