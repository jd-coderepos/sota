\documentclass[12pt]{article}



\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{amsthm}


\newcommand {\R}   {{\rm I\!R}}
\newcommand {\uu}  { {\bf u} }
\newcommand {\xx}  { {\bf x} }
\newcommand {\yy}  { {\bf y} }
\newcommand {\rr}  { {\bf r} }
\newcommand {\EE}  { {\bf E} }
\newcommand {\HH}  { {\bf H} }
\newcommand {\JJ}  { {\bf J} }
\newcommand {\veps} {\varepsilon}
\newcommand {\Ro} { {\cal R}}
\newcommand {\lag} { {\cal L}}
\newcommand {\sci} { {\rm i}}
\newcommand {\qq}  { {\bf q} }
\newcommand {\pp}  { {\bf p} }
\newcommand {\mm}  { {\bf m} }
\newcommand {\vv}  { {\bf v} }
\newcommand {\ff}  { {\bf f} }
\newcommand {\bb}  { {\bf b} }
\newcommand {\cc}  { {\bf c} }
\newcommand {\dd}  { {\bf d} }
\newcommand {\blambda}  { {\boldsymbol \lambda} }
\newcommand {\bmu}  { {\boldsymbol \mu} }
\newcommand {\zero}  { {\bf 0} }
\newcommand {\gb}  { {\bf g} }
\newcommand {\bnabla} { { \boldsymbol \nabla} }
\newcommand {\btheta} { { \boldsymbol \theta} }
\newcommand {\balpha} { { \boldsymbol \alpha} }
\newcommand {\bxi} { { \boldsymbol \xi} }
\newcommand {\fuu}  { {\frac{\partial (f_u^T\lambda)}{\partial u}} }
\newcommand {\fum}  { {\frac{\partial (f_u^T\lambda)}{\partial m}} }
\newcommand {\fmm}  { {\frac{\partial (f_m^T\lambda)}{\partial m}} }
\newcommand {\sg}{{\sigma}}
\newcommand{\hf}{\frac12}
\newcommand{\hx}[1]{{\ensuremath{h^x_{\scriptscriptstyle #1}}}}
\newcommand{\hy}[1]{{\ensuremath{h^y_{\scriptscriptstyle #1}}}}
\newcommand{\hz}[1]{{\ensuremath{h^z_{\scriptscriptstyle #1}}}}
\newcommand{\x}[1]{\ensuremath{x_{\scriptscriptstyle #1}}}
\newcommand{\y}[1]{\ensuremath{y_{\scriptscriptstyle #1}}}
\newcommand{\z}[1]{\ensuremath{z_{\scriptscriptstyle #1}}}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\E}{\vec{E}}
\newcommand{\A}{\vec{A}}
\renewcommand{\H}{\vec{H}}
\newcommand{\J}{\vec{J}}
\newcommand{\F}{\vec{F}}
\newcommand{\s}{\vec{s}}
\newcommand{\sig}{\sigma}
\newcommand{\hsig}{\widehat \sigma}
\newcommand{\hJ}{\widehat{\vec{J}}}
\newcommand{\nn}{\vec{n}}
\renewcommand{\div}{\nabla\cdot\,}
\newcommand{\grad}{\ensuremath {\vec \nabla}}
\newcommand{\curl}{\ensuremath{{\vec \nabla}\times\,}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\rme}{\rm{e}}
\DeclareMathOperator*{\argmin}{arg\,min}                   \newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newcommand{\tia}[1]{ {\bf #1} }

\sloppy




\begin{document}

\title{Simultaneous Source for non-uniform data variance and missing data}

\author{{E.\ Haber\thanks{Department of Mathematics
\& Department of Earth and Ocean Science, The University of British Columbia, Vancouver, BC, Canada, V6T-1Z4,  Phone: +1 (604) 822-9068, Fax: +1 (604) 822-2545}}  \ and {M. Chung\thanks{
Department of Mathematics, Virginia Tech
474 McBryde, Stanger Street, Blacksburg, VA 24061, USA,
Phone: +1 (540) 231-3446     Fax : +1 (540) 231-5960 }}}

\maketitle

\begin{abstract}
The use of simultaneous sources in geophysical inverse problems has revolutionized the ability to deal with large scale data sets that are obtained from multiple source experiments. However, the technique breaks when the data has non-uniform standard deviation or when some data are missing. In this paper we develop, study, and compare a number of techniques that enable to utilize advantages of the simultaneous source framework for these cases. We show that the inverse problem can still be solved efficiently by using these new techniques. We demonstrate our new approaches on the Direct Current Resistivity inverse problem.
\end{abstract}

{\bf keywords:} simultaneous sources, inverse problems, stochastic programming, Direct Current Resistivity\\

\section{Introduction}

In this paper we investigate efficient methods for parameter estimation in partial differential equations (PDEs) with multiple right hand sides. The goal is to infer quantitative information (physical properties) of a media from indirect measurements. Our main focus are geophysical experiments, where a combination of sources and receivers are used. However, application such as electromagnetic imaging and electrical impedance tomography facing similar challenges \cite{smvoz,brocea,devony,na1,jc1}. Sources produce signals that are omitted into a media to be investigated. Receivers collect (measure) data obtained by signal probing the media. If the interaction between the media, sources and receivers can be described by a linear PDE and  sources are used with  receivers then the measured data is an  matrix , that can be approximated by

Here,  is an  matrix that is obtained by a discretization of a differential operator that depends on parameters , i.e., the media's properties. The matrix  of size  represents a discretization of the sources, and  is an  matrix that represents the discretization of the receivers. Furthermore  is an  random matrix of the noise, assumed to be independent  and normal distributed with zero mean, but not necessarily with the same standard deviation. In Equation~\eqref{eq:for} the operator  denotes the Hadamard product. The  matrix  typically has three forms.
\begin{itemize}
\item[(a)] First, ,   is a matrix that consists of entries that are all ones -- all receivers share all sources -- multiplied by the inverse standard deviation  which is identical for all data.

\item[(b)] Second,  is a matrix of non-negative numbers . In this case all receivers share all sources but each datum has a different standard deviation.

\item[(c)] Third,  is a sparse matrix. In this case some receivers share some of the sources but the data is incomplete, that is, not all source-receiver combination are recorded. Clearly, a value 
in the matrix  is a special case of~(b), where some data has infinite standard deviation.
\end{itemize}
We refer to  as the ``variance matrix''. Notice that  is not the covariance matrix but it carries the entries of the diagonal of the covariance matrix (that is the variance). The entries of data matrix, , correspond to the  receiver and the  source normalized by the standard deviation of each datum.

Various other structures of the matrix  may occur and are application dependent.  While the above cases~(a)--(c) seems to be specific, many applied problems fall into the above category, applications such as seismic imaging, electromagnetic imaging, electrical impedance tomography, diffraction tomography \cite{smvoz,brocea,devony,na1,jc1} and more.

Consider some measured data . Our goal is to obtain a ``reasonable'' estimate of the parameters  that approximately give rise to the observed data. To obtain such an estimate we consider the output least squares approach. That is, we consider the regularized optimization problem

where  is some regularization term that is used to obtain an estimate of  and incorporates some a-priori information such as smoothness or sparsity. Here,  is a regularization parameter and  denotes the Frobenius norm. The choice of the Frobenius norm rises naturally from a statistical point of view, where the minimization problem can be interpreted as a maximum likelihood approach with a-priori information.



It is straight forward to calculate the gradient of  with respect to  (see \cite{hao} for details). In a nutshell, let ,  and  be the  columns of ,  and  respectively,

the solution of the forward problem for the  source, and let

be a sparse matrix containing the derivatives of  with respect to  for a fixed vector . The gradient of  is given by


In order to compute the objective function~\eqref{eq:opt} and its gradient~\eqref{eq:dopt} we essentially need to solve the following two linear systems

The first problem (the forward) has  right hand sides while the second (the adjoint) has  right hand sides. For small scale problems, this computation can be easily done using either a Cholesky (if the system is symmetric) or a LU factorization (for nonsymmetric systems). However, for large scale problems this computation becomes much more challenging. Indeed, solving large scale linear systems with multiple right hand sides is a difficult task that requires both efficient iterative solvers and large amount of memory. Even if powerful solvers such as multigrid methods can be utilized, the effective cost in computational speed and memory still increases linearly with the number of sources. For problems with many right hand sides such computation can become prohibitively expensive, especially for problems that stem from 3D applications.

\bigskip

A recent approach to solve such a problem that works well in the case that , a matrix with all entries  -- thus can be neglected ( is factored into the regularization parameter) -- is obtained using the identity

where  denotes the Euclidian norm,  is the expected value, and  is a random variable with zero mean and standard deviation of , see \cite{HaberChungHerrmann2011}. Using this identity, problem~\eqref{eq:opt} turns into a stochastic programming problem of the form

The key observation when solving the problem \eqref{stopro} is that {\em given a single realization , the evaluation of the objective function and the gradient, that correspond to that sample requires only a {\bf single} inversion of the matrix }.
Stochastic programming techniques attempt to use as little samples as possible to approximate minimal expected values. It is straight forward to apply efficient stochastic programing algorithms to solve problem~\eqref{stopro}, that requires significantly less inversions of the matrix  than standard methods mentioned above.
This approach, known also as {\bf``simultaneous source''}, was first suggested in the context of waveform inversion \cite{RohmbergGeop2010,krebs09ffw,neelamani08dos,LeeuwenAravkinHerrmann2011} and then explained and analyzed by means of stochastic programming in \cite{HaberChungHerrmann2011}. It has been shown to be highly effective in dealing with multiple sources.

Nonetheless, when  the above approach loose its advantages and other techniques are required. This is due to the fact that the Hadamard product does not commute with the matrix vector product and thus computing  requires first to compute  and only then apply the result to . It is therefore impossible to linearly combine sources and the approach  breaks. One approach that has been used in the literature mentioned above is to ignore the non-uniform standard deviation and to work with . This can be seen as a weighted least square approach. If the data has similar standard deviations this approach works well, however, when the data has standard deviations that range over a large dynamical spectrum the approach tend to produce unsatisfactory results. This is because small data values are basically ignored while large data values are fitted. In research areas such as geophysical applications small data values are crucial. Roughly speaking, large data values rise close to the source and thus do not contain much information about the media. On the other hand, small magnitude data contain more information about the media. Ignoring  this data may lead to serious artifacts in the reconstruction (see \cite{parker,taran} and our numerical experiments).

The goal of this paper is to explore techniques that give rise to efficient computations for problem when  is not just of the form . We discuss a number of possible approximations and show that it is possible to generate efficient algorithms for these cases as well. We use the Direct Current (DC) Resistivity example to demonstrate the effectiveness of our approach.

The paper is structured as follows. In Section~\ref{sec2} we present possible extensions of the simultaneous sources approach and discuss their properties. In Section~\ref{sec3} we discuss efficient computations of these techniques. In particular we explore the use of the Sample Average Approximation (SAA) as a stochastic optimization technique. In Section~\ref{sec4} we investigate the numerical efficiency of the different approaches and we summarize our work in Section~\ref{sec5}.

\section{Numerical treatment of the non-standard form}
\label{sec2}

In the previous section we reviewed the case where  in which the original deterministic problem is converted into a stochastic programming problem with the feature that only a single matrix inversion is needed for every realization of the random vector. In this section we discuss a number of techniques that enable us to work with a matrix  that does not all contain 's. We discuss the different conditions that enable each technique to work.

\subsection{Data completion}
\label{meth1}
Let us assume that the matrix  contains only entries of  and 's. Here, entries  mirror the fact that the  receiver is not connected to the  source and therefore missing and has infinite uncertainty.
The idea is to treat this problem as a missing data problem, where the complete data is given by

and the observed data is connected to the complete data by

where  is a sparse matrix.

It is possible to estimate missing data by matrix completion techniques. Interpolation of data is a major area of research (see \cite{TradUlrychSacchi2001} and reference within). For instance, in seismology, efficient methods for data interpolation are based on sparse recovery. Many other fields have a data-specific approach. Here, we present a simple data interpolation technique based on the solution of a reduced forward problem.

\bigskip

In some cases it is possible to compute the solution to the forward problem at reduced cost.
For example, if  is constant or, in many geophysical applications, admits a 1D or 2D structure
it is possible to compute the solution to the forward problem in a low cost  \cite{wardhow}.
Here we assume that it is possible to solve the 1D or 2D problems and that the solution is readily available. Let the reduced forward problem be

Here,  is the reduced (say 1D or 2D)
model and   is a reduced forward
model (that is the 1D or 2D forward modeling matrix). The reduced data 
therefore corresponds to the simple 1 or 2D model and we use this data to replace
the missing data.
Thus, we set

 We then use   to solve the optimization problem

that can be done by using  the algorithms proposed in~\cite{HaberChungHerrmann2011}.

Completing and using the data in this way has a distinctive advantage. Assuming for a moment, that we do not have any measured data. Then, the corresponding completed data is simply  which has a solution . That is, this data completion can be seen as a form of regularization, pulling the solution to a simple 1D or 2D ``reasonable'' model.

There is one obvious disadvantage to this approach. The matrix  contains some ``invented'' data. The missing data can be far from the true data and thus significantly  bias the solution. Nonetheless, the approach is straight forward and as we see in numerical experiments in Section~\ref{sec4} may lead to very reasonable results.

\subsection{The case of low rank }
\label{meth2}
In some cases it is possible to decompose  into low rank matrices, or at least, to approximate  by a matrix of small rank. Consider the case

where  and  are  and  matrices respectively and with . Using the low rank representation of  we can now prove the following result.
\begin{lem}
Let  be an  matrix with columns  and let  and  be an  and  matrices respectively, with columns  and . Finally, let  be a vector of size , then

\end{lem}
\begin{proof}
Statement~\eqref{eq:lemma} can be shown by simple transformations

\end{proof}


This identity implies that it is possible to obtain a stochastic representation to the misfit of the form

By using the identity~\eqref{eq:optLR} it is possible to obtain a stochastic approximation that extends the case
 using  matrix solves. Here,  is the rank of   and we solve the stochastic programming problem


In other cases, where  is not low rank, it is possible to approximate  by a low rank matrix and use the above decomposition as an efficient alternative to the original misfit function~\eqref{eq:opt}. The advantage of this approach is that it has the same structure of the efficient algorithm used in previous work on simultaneous sources~\cite{HaberChungHerrmann2011}. Its disadvantage is that if the rank of  is not low  then this approach looses its computational advantages over~\eqref{eq:opt}.

\subsection{Stochastic Approximation of the data matrix}
\label{meth3}
For problems where the rank of  is large and matrix completion fails, it is possible to obtain a different stochastic process that approximates the misfit function. It is straight forward to verify that

Using this approximation we can replace the deterministic problem~\eqref{eq:opt} with the stochastic programming problem


It is important to note that the problem~\eqref{absExp} is very different than the previous stochastic programming problems. The main difference is that the expected value is {\em inside} the norm rather than outside. Such stochastic programming problems are less common. However, it is possible to prove under certain conditions, that stochastic programming techniques converge to an optimal solution of the problem (see~\cite[Ch.~4]{shapiroBook}).

\subsection{Working with a subset of sources}
\label{meth4}
Finally, we consider a simple method that originated from a randomized Kaczmarz method~\cite{strohmer2009randomized}, which solves large linear systems and is used for instance in methods for  tomography such as OSEM~\cite{HudsonLarkin1994}. Let  be an equally distributed random variable. Then, the original problem~\eqref{eq:opt} is equivalent to the following stochastic programming problem

where the expected value is on the index of the source . This approach was proposed in \cite{KeesAscher2011} for the solution of the DC Resistivity problem using level sets. The approach is easy to implement, however, it may converge rather slowly if sources are not combined, especially if each source is sensitive only to a small part of the domain.

\section{Solving the optimization problem using stochastic programming techniques}
\label{sec3}

In this section we shortly discuss the solution of the stochastic programming problems~\eqref{eq:optj},~\eqref{absExp},~\eqref{clropt}, and~\eqref{stoproComp}. All proposed reformulations (besides~\eqref{absExp}) have the structure

where  is the misfit function for a single realization.
We use variants of the Sample Average Approximation (SAA) method~\cite{LinderothShapiroWright2006}, which we adopt for the problems at hand. The idea is to replace the expected value with a sum minimizing

Notice that in statistical methods problem~\eqref{eq:bayes} can be viewed as a \emph{risk minimization problem} and~\eqref{eq:empirical} as its \emph{empirical} pendant.
Obviously, an empirical approach is useful only if the number of samples , which implies that the number of matrix inversions in the stochastic programming implementation is substantially smaller than the number of matrix inversions needed to solve the deterministic problem~\eqref{eq:opt}.

\emph{Stochastic Approximation} (SA) methods are another class to target stochastic optimization problem such as problem~\eqref{eq:bayes}, see~\cite{JuditskyLanNemirovskiShapiro2009}. However, here we prefer to use SAA methods for the following two reasons.
\begin{itemize}
\item First, SAA methods can be used with optimization methods that utilize curvature information such as Gauss-Newton algorithms. Contrary, SA methods are limited to first order methods, since to the best of our knowledge no convergence theory exists for higher order methods. Thus, even if the number of realizations for SAA methods is larger, the solution can be obtained in much fewer iterations when utilizing higher order methods. In this work we use the Gauss-Newton method that is know to have faster convergence compared to steepest descent methods \cite{pratt1999,hao}.
\item Since the results obtained by SAA methods are unbiased (or have a very small Bias), it is simple to use SAA in parallel architecture, with minimal communication and averaging the results at the end. This is unlike SA methods when parallelization requires communication is needed after each iteration.
\end{itemize}

\bigskip

Notice that the regularization parameter , which is often crucial for the inverse solution, is a-priori unknown. To overcome this problem we use a process of continuation of the regularization
parameter \cite{hao}. We start with a large value of , solve the optimization problem and continue to reduce  until the misfit reaches a desired value.

The process of reducing  can be done independent of choosing the sample size. However, this, in general, is inefficient. If the sample size is too small then reducing  does not lead to an approximate inverse solution. On the other hand, if the sample size is large then the solution of each optimization problem for each  is expensive.

We therefore use another ``enhancement'' for the SAA technique, that we found to be efficient. We combine the process of reducing the value of  with an adaptive approach that determines the sample size by continuation. That is, we start with a very small sample size and a large  and solve the optimization problem~\eqref{eq:empirical}. We then reduce  and increase the sample size {\em simultaneously}.
For each optimization problem we use ``hot starts''. That is, we start from the solution of the previous optimization problem. The process is terminated when the inverse solution obtains the target misfit {\em and} changes little while changing the sample size. This method is summarized in Algorithm~1.
\begin{algorithm}
\caption{Stochastic Programming for Inverse Problems}
\begin{algorithmic}[1]
\STATE Choose parameters ,  and 
\STATE Initialize, set , and 
\WHILE {not converged}
\STATE Choose 
\STATE{Solve optimization problem~\eqref{eq:empirical} for 
with initial guess  }
\IF{  }
\STATE{ }
\ENDIF
\IF{  }
\STATE{ break}
\ENDIF
\STATE{Set }
\STATE 
 \ENDWHILE
 \end{algorithmic}
\label{al:al1}
\end{algorithm}

Obviously, the foremost computational costs of the algorithm is in step~5, where we solve an optimization problem for . Since we are using the Gauss-Newton method, we found that only a few iterations (1-3) are required in order to converge. Moreover, the number of iterations computed with the largest sample size is very small.

\section{Numerical comparisons}
\label{sec4}

In this section we perform a numerical comparisons between the approaches discussed earlier in Section~\ref{sec2}.

\paragraph{Model Problem.} As a model problem we consider the DC Resistivity inversion, where the forward problem is a discretization of the static Maxwell's equations (see \cite{hao2,na})

This is a common problem in geophysical exploration \cite{wardhow}. The goal is to recover the conductivity  from measurements of the electric field .


\paragraph{Experimental settings.}

We use the SEG salt model which is a common model for benchmarking geophysical inverse problems \cite{Alkhalifah1998}. The experimental setting is shown in Figure~\ref{fig1}. The model consists of a large salt body with conductivity of \,S/m embedded within a layered media with conductivity that ranges from  to \,S/m.
\begin{figure}
\begin{center}
\includegraphics[width=10cm]{segModel.pdf}
\caption{The SEG model used for the numerical experiments and the experimental setting for a single source. Electric lines transmit current into the media and the resulting electric and or magnetic fields are recorded by the receivers.
\label{fig1}}
\end{center}
\end{figure}
The sources are line sources and we assume to have 800 dipole line sources,  equally spaced that are pointing in the -direction and the same number of  dipoles pointing in the -direction. The receivers are assumed to measure 2 components of the electric field on the surface. We assume to have  receivers spread over the surface uniformly. For this setting we have  data points.

\paragraph{Discretization \& Forward Problem.} For the experiments we use a stretched mesh of  cells where we assume that the conductivity of each cell is constant. We use a staggered grid (Yee's method) for the discretization of the forward problem in first order form \cite{yee,mm89,na,ha} which leads to the linear system

where  is the discretization of the -operator and  is an averaging matrix. For the grid at hand, each forward problem (for a single source) consists of solving a linear system with  unknowns. This forward problem is solved using the direct solver MUMPS, see \cite{MUMPS}. The computation of the forward problem in this environment takes about 1 minute. The matrix  in this application is symmetric positive semidefinite and since we are able to compute and store the Cholesky factors we reuse them for the computation of the adjoint problem and for the Gauss-Newton iteration.
For the calculation of Jacobians and other related inverse problem quantities see \cite{hao2}.

\paragraph{Further Settings.} For the numerical experiments we set ,  and .
We use Tikhonov regularization

where we choose  as a discretization of the corresponding gradient matrix.

\paragraph{Setup \& Computations.} To compare the computational costs of the different techniques we count the number of forward calculations. Since over  of the computations are in this phase this is an accurate indicator for the overall computational costs of each algorithm. We perform~5 types of experiments corresponding to  different types of the matrix . In the first case we choose . This is the simplest case, where no data is missing and all data share the same standard deviation. The value of  is chosen as  of the average value of the data. In the second experiment we change the data's standard deviation to be  of each datum.
In the third, forth, and fifth experiments we use , , and  of the data, randomly selected, with standard deviations
that are  of each datum.

More details about the numerical experiments are summarized below.
\begin{itemize}
\item The Gauss-Newton method uses a conjugate gradient (CG) solver that iterates up to  iterations. Each iteration involves calculating the forward and the adjoint problems.
\item The matrices  that correspond to this experiment do not admit low rank and their singular values decay exponentially from  to . We therefore used a space of  and  vectors for our calculations.
\item For the source-subset method we start with  sources and increase the number of sources if needed. We have found to use up to  sources at a time lead to satisfactory results.
\end{itemize}
\paragraph{Results.} The results of the numerical experiments are summarize in Table~\ref{tab1}. We record the total number of Gauss-Newton iterations as well as the number of forward problems solve. The recovery error (relative error) is also recorded and it is computed
as .

In Figure~\ref{fig2} we slice through the true solution and the recovered solutions obtained by the different methods.
\begin{figure}
\begin{center}
\begin{tabular}{cc}
 \includegraphics[width=8cm]{trueModelSlices.pdf} &
 \includegraphics[width=8cm]{uniforSigmaSlices.pdf} \\
(A) True model &  (B) Full data  uniform \\
\includegraphics[width=8cm]{nonUniformSigmaSlices.pdf} &
\includegraphics[width=8cm]{nonUniformSigma70Slices.pdf}  \\
(C) Full data  non-uniform &  (D)  70\% data  non-uniform \\
\includegraphics[width=8cm]{nonUniformSigma40Slices.pdf} &
\includegraphics[width=8cm]{nonUniformSigma10Slices.pdf}  \\
(E)  40\% data  non-uniform &  (F)  10\% data  non-uniform \\
\end{tabular}
\caption{The true model and the resulting inverted data obtained for different choices of .
\label{fig2}}
\end{center}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|cccc|}
\hline
Method                     & GN Iterations & \# forward calculation & Recovery error \\
\hline
\multicolumn{4}{|c|}{ Uniform } \\
\hline
Low rank(1)            &         34                   &              2043                       &        0.28       \\
Stochastic approx  &         43                  &             9789                        &        0.29      \\
Subset                      &        54                   &            19446                        &       0.28        \\
\hline
\multicolumn{4}{|c|}{Standard deviation } \\
\hline
Low rank(5)            &         33            &              11254                   &        0.19       \\
Low rank(10)          &         28           &              17232                    &        0.18       \\
Stochastic approx  &       56             &             20242                    &      0.19         \\
Subset                     &       55            &              21897                    &       0.18        \\
\hline
\multicolumn{4}{|c|}{Same as  with  of the data missing} \\
\hline
Data completion    &       35                &             2154                        &     0.29          \\
Low rank(5)            &        32            &              10913                   &        0.21       \\
Low rank(10)          &         27           &              16617                    &        0.20       \\
Stochastic approx  &       54             &             19519                    &      0.21         \\
Subset                     &       55            &              22011                    &       0.19        \\
\hline
\multicolumn{4}{|c|}{Same as  with  of the data missing} \\
\hline
Data completion    &       35                &             2142                        &     0.29          \\
Low rank(5)            &        31            &              10572                   &        0.24       \\
Low rank(10)          &         24           &              14771                    &        0.23       \\
Stochastic approx  &       54             &             19581                    &      0.25         \\
Subset                     &       55            &              22032                    &       0.22        \\
\hline
\multicolumn{4}{|c|}{Same as  with  of the data missing} \\
\hline
Data completion    &       35                &             2201                        &     0.31          \\
Low rank(5)            &        28            &               9548                   &        0.28       \\
Low rank(10)          &         21           &              12925                    &        0.28       \\
Stochastic approx  &       51             &             18493                    &      0.30         \\
Subset                     &       50            &              20029                    &       0.27        \\
\hline
\end{tabular}
\caption{Comparison between different recovery techniques. The data completion
correspond to Section~\ref{meth1}, the low rank corresponds to Section~\ref{meth2}, the
stochastic approximation corresponds to Section~\ref{meth3} and the subset corresponds to Section~\ref{meth4}.}
 \label{tab1}
\end{center}
\end{table}

The numerical experiments reveal some interesting observations.
\begin{itemize}
\item First, we note that the number of forward problems solved is roughly equal to 2 times the average number of realizations times the number of Gauss-Newton iterations times the average of inner CG iterations plus one,

Thus, reducing the number of realizations plays a crucial role in the computational cost.
\item If we assign a constant standard deviation to each datum, small  data in magnitude are not fitted well and as a result, the reconstruction has lower resolution (data not shown). This demonstrates the need for the development of the techniques suggested in this paper, see Figure~\ref{fig2}.
\item
Assigning each datum an appropriate standard deviation leads to the best results but tends
to also be the most expensive.
\item The  subset approach    yield the best recovery closely followed by the  low rank approach.
Nonetheless, the low-rank approach is substantially cheaper that the subset approach.
\item Reducing the number of data  effected the inversion however, the resulting models are
still very reasonable. This implies that the data admits redundancy. The question rises how to effectively
collect data that has less redundancies.
\item As the number of data is reduced, fewer iterations are needed. This is not surprising as it is
easier to fit the data when there are fewer measurements.
\end{itemize}


\section{Conclusions and Summary}
\label{sec5}

 In this paper we study the question of the so called simultaneous source with nonuniform standard deviation
 and with missing data. The technique offers significant computational saving over the traditional
 deterministic Gauss-Newton method. The saving is obtained by turning the problem into a stochastic
programming problem and then using a version of the Stochastic Average Approximation (SAA) to solve
the problem coupled with adaptive increase of the sample size. The paper study a number of different
techniques that allow for non-uniform standard deviation.

We have conducted extensive experiments on the DC resistivity inverse problem.
In terms of quality we have found that the nonlinear Kaczmartz iteration yields the best reconstruction
but with the highest cost. Similar results but with much lower cost are obtained by low-rank approximation to the variances matrix . Data completion using a reduced model seem to also yield reasonable results.

The application of our approach to other inverse problems in geophysics and medical physics is straight
forward and can lead to significant saving in many applications.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
