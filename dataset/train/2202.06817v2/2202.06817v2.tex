\documentclass[11pt,draftclsnofoot,onecolumn]{IEEEtran}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabulary}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{array}
\usepackage{tabula rx}

\newcommand{\edit}[1]{{\color{red} #1}}    

\begin{document}

\title{\LARGE\underline{\textbf{Revisions for PAPER ID: TPAMI-2022-02-0287}}}

\author {\flushright \textbf{Seokju Cho, Sunghwan Hong and Seungryong~Kim}

\flushright Korea University, Seoul, Korea
}

\maketitle
\vspace{-1.5cm}
\noindent Dear Associate Editor and Reviewers,
\newline
\newline
\noindent 
We would like to thank the associate editor and reviewers for their valuable comments. 
We revised this paper in an attempt to address the reviewers' comments and also corrected some typos.
\newline
\noindent The main changes of the revised paper are as follows:

\begin{enumerate}
	\item We have added a "Visual correspondence and its applications" paragraph in Related Work Section.
	\item We have added a "Comparison to Volumetric Aggregation with Transformer (VAT)" Section, where we summarize the main differences between the proposed method and VAT.
	\item We have added additional results in Table 2 and Table 3, and included an analysis in Section 5.4.1.
	\item We have added additional results in Table 5, and included an analysis in Section 5.4.3.
	\item We have added additional results and analysis in Table 7 and Section 5.5.1, respectively.
	\item We have added additional results and analysis in Table 8 and Section 5.5.2, respectively.
	\item We have added  Table 10 and a "Extension to few-shot segmentation" Section.
	\item We have added  Table 11 and a "Extension to Visual Localization" Section.
	\item We have added  Figure 16 and a "Limitations" Section.
	\item We have fixed the minor typos and comments pointed out by the reviewers and proofread the paper.
\end{enumerate}

\noindent Detailed responses and revisions for each comment are attached, followed by a copy of the revised manuscript with major changes highlighted in blue. 
\newline
\newline
Sincerely,
\newline
\noindent Sunghwan Hong

\newpage

\hspace{1.4cm}\LARGE\underline{\textbf{Revisions for PAPER ID: TPAMI-2022-02-0287}}
\newline
\newline

\noindent \large\textbf{Reviewer: 1}

\begin{longtable}{|p{16cm}|}
	\hline 
	
	
	\textbf{
	- The main concern is that the main idea and framework proposed in the manuscript are very similar to the concurrent work of [A]. It needs to focus on differentiating their method with the concurrent work of [A] as both work look very similar.}\\
	
	[A] Hong et al., “Cost Aggregation Is All You Need for Few-Shot Segmentation” arXiv preprint\\
	

	\hline
\end{longtable}
- We thank the reviewer for the suggestion. We agree that explicitly differentiating our approach to [A] would help to prevent potential misunderstandings that our work and VAT [A] are very similar. Before our response, we would like to inform the reviewer that an ECCV'22 version for VAT [B] is publicly available on arXiv, which the major modifications are made to its presentations and some minor changes in performance, while the architecture remains unchanged. In this revision, we compare our work to [B], since the concerns that reviewer has seems to be related to the use of hypercorrelation, early convolution and hierarchical processing, and evaluation on the same datasets, which remain unchanged in [B]. 

- As a summary, for this comment, we have added additional texts in Section 4.3 to extend the reasoning behind the use of the convolutional embedding module and differentiate ours to VAT in terms of implementation. We have added a new Section (Section 4.6) to summarize the differences between this work and VAT [B]. Moreover, we have added the results of VAT in Table 1 and added  regarding texts in Section 5.3. We have added a new result in Table 5, substantiating that the relative positioning bias can significantly enhance the generalization power, hench affecting the performance. Finally, we have added a new table (Table 10) to show quantitative comparison on FSS-1000 dataset, and we have added more detailed analysis in Section 5.5.7.  

- We wish to highlight the differences from three aspects: motivation, architecture and experiments. As detailed in Section 4.6, the motivations behind the designs of each architecture differ as their main target tasks are different, \textit{i.e.,} few-shot segmentation and semantic correspondence. Although they may share similar challenges, \textit{e.g.,} background clutters and intra-class variations, at high-level, VAT [B] designs its architecture to enhance its generalization power to handle new classes beyond training samples tailored for few-shot segmentation, while CATs++ redesigns the components of the standard Transformer for efficient computations tailored for semantic correspondence.

- In terms of architecture, although the proposed method and VAT [B] share the use of hypercorrelation, early convolution and transformer aggregator, a key contribution that differentiates the proposed method to other Transformer-based architecture is that CATs++ reformulates the standard QKV projections and FFN, and proposes  affinity-aware convolutional projections and volumetric convolutional feed-forward network, aiming to reduce the memory and computational complexity that are typically high when the input is high dimensional correlation maps. To reduce the complexity, VAT [B] adopts shifting window multi-head self-attention that is designed to reduce the computational complexity of a global multi-head self-attention module, which is quadratic to , to , where  is a fixed hyperparameter that denotes the number of evenly partitioned patches within a local window. These  design goals lead to notable differences in the overall memory and computational loads, as evidenced in Table 10.


- Also, how VAT [B] defines the tokens differs from CATs++, since VAT [B] considers both spatial structure of a correlation map, while CATs++ defines the tokens separately for each spatial structure, but in parallel. Concretely, VAT [B] defines the channel dimension for each token as , while CATs++ defines it as . We believe that this can naturally affect the performance and we argue that it is responsible for the performance gap between VAT [B] and CATs++ for certain extent. 

- Importantly, we investigate the effects of relative positioning bias and added the results in Table 5. From this, we find that relative positioning bias enhances the generalization power, which can significantly affect the performance for few-shot segmentation. This explains a  portion of performance gap between VAT [B]  and those of ours on FSS-1000. For the semantic matching task, which CATs++ outperforms VAT, we wish to highlight that the way how the proposed approach integrates convolutions, appearance embeddings and transformer aggregator is highly effective for this task.

- For the design of decoder, while CATs++ employs soft-argmax function to estimate the final flow maps, VAT [B] proposes a decoder that inevitably faces large memory consumption and computations. Although this design of VAT [B] allows noise filtering within correlation maps and benefit from higher-resolution spatial structure during the progressive up-sampling of correlation map within its decoder, this introduces highly non-trivial computational burdens, while CATs++ mitigates this by integrating appearance embeddings within its transformer aggregator and attempts to preserve fine-details through hierarchical processing with the proposed transformer aggregator. We believe that each of these apparent differences contribute to their performance and efficiencies, which clearly differentiate both architectures. 


- Furthermore, in this work, in terms of experiments, we have provided extensive ablation study and analysis in Section 5.4 and Section 5.5. More specifically, the effectiveness of each components is validated in Table 2 and Table 3, the efficiencies of the proposed modules are substantiated in Table 4, justification of the use of Transformer in Table 5. We have also explored the effects of varying feature backbone, the number of encoders,  input image resolutions and alpha thresholds  in Table 6, Fig. 11, Fig. 12 and Fig. 15, respectively. The effectiveness of the proposed data augmentation strategy is validated in Table 7 and justification of serial/parallel processing is shown in Table 8. Finally, the memory and run-time comparison to other works are summarized in Table 9. We also show visualization of robustness to domain and class shift in Fig. 13. Finally, we additionally evaluate the proposed approach for visual localization task and reports the results in Table 11, as well as the newly added Section 5.5.8.   Combining these, we believe that they clearly differentiate this work to VAT [B]. 

[B] Hong et al., "Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation" ECCV'22. 
\newpage

\begin{longtable}{|p{16cm}|}
	\hline 
        \textbf{-  In Tab.3, why ablation study is performed with CATs? Usually the study is performed with the best model.}\\
		\textbf{- As seen from the table, (III) convolutional embedding, (IV) efficient transofmer aggregator and (V) appearance modeling are mainly effective techniques so instead of stacking up these three, combination of the modules will provide more intuitive ablation study.}\\
	\hline 
\end{longtable}

- We agree that providing different combinations will clearly provide more intuitive ablation study. Thanks for the suggestion! However, we wish to highlight that our intention was to show the effectiveness of each component \textbf{without} having influenced by the backbone network, which has much more number of learnable parameters than the proposed modules and it might have confused the actual effectiveness. In practice, we observed that fine-tuning the feature backbone leads to significant performance boosts, which CHM, PMNC, MMNet and CHMNet also benefit from this. We argue that to validate the effectiveness of the models, feature backbone network should be frozen, but recent works fine-tune the backbone networks to boost the performance, which we had to follow to ensure a fair comparison. 

- However, we agree that it would be much more informative to also provide results of CATs and CATs++ with feature backbone fine-tuned.  Following the reviewer's suggestion, we conducted additional experiments and updated Table 2 and Table 3. From the results, we find that as we stack up each component, generally, performance improved. However, we also observe a performance drop for appearance modelling, which may seem trivial for only a 0.4  drop, but we believe that this could indicate that effectiveness may have been influenced by the backbone network, since we observed a consistent performance increase when backbone network is not fine-tuned. Note that we also added a regarding analysis in Section 5.4.1. Moreover, in addition to the reviewer's comment, we have added new results to Table 7, where we show the effects of data augmentation, to show the effects of adopting data augmentation for both cases where backbone network is frozen or fine-tuned.

-  Furthermore we also included additional results that represent different combinations of the components, in Table 3, and an analysis in Section 5.4.1. We wish to highlight that we now consider all the valid combinations. Thanks for the suggestion.  


\newpage

\begin{longtable}{|p{16cm}|}
	\hline 





\textbf{-
In Tab.9, can CATs architecture can be implemented in both serial and parallel ways? If not why are there two columns?}\\

	\hline
\end{longtable}



- Thanks for the suggestion. We agree that providing the results for adopting  serial and parallel processing simultaneously with adequate analysis would make the ablation study more intuitive. To this end, we conducted additional experiements and included the results in Table 8 and regarding analysis in Section 5.5.2. 

- From the results, we conjecture that whether the network makes use of convolutions yields different results. More specifically, while token-mixing convolutional operations mix both spatial dimensions of correlation maps, self-attention only considers either source or target spatial dimension, and this makes self-attention suitable to serial processing and convolutions suitable to parallel processing. Note that through parallel processing, CATs++ aggregates local contexts of both spatial dimensions and the following self-attention impart them to all pixels, which allows the best results. We thus argue that employing serial processing for CATs++ may complicate the learning process by performing redundant aggregations, while it can benefit CATs as shown in the Table 8. 



\newpage
\begin{longtable}{|p{16cm}|}
	\hline 

	
	\textbf{- In related work, I recommend including how the semantic matching is applicable in computer vision problem like visual tracking so as to emphasize the importance of the sematic correspondence problem as fundamental mid-level problem as well as benefits of faster inference time in matching (Tab.9). }\\

	
	\hline
\end{longtable}
- We thank the reviewer for the suggestion. We agree that having an additional Section introducing the applications of establishing correspondence would give to the reader a good intuitions for the importance of the task. We have added a Sub-Section, "Visual correspondence Applications" at page 3, Section 2.2. Moreover, we have included the benefits of having faster inference time in matching in Section 5.5.3 at page 15. 

\newpage
\begin{longtable}{|p{16cm}|}
	\hline 

	\textbf{- In literature, FFN in multi-head self-attention is described as multi-layer perceptron so FFN in Eqn.(4) and (11) may mislead leaders. }\\
	
	
	\hline
\end{longtable}

- We thank the reviewer for the clarification. We assume that the reviewer meant to refer to FFN \textbf{after} MHSA, since MHSA refers to a module for attention mechanisms in parallel. We agree that Equation 4 and 11 may mislead the readers, since we originally denoted as  and . This may bring misunderstanding that there are 2 FFNs, when our meaning is actually the first linear transformation and the second linear transformation \textbf{within} the FFN. Analogously, for equation 11, our meaning for  amd  is the first and the second convolutional transformations. We thus corrected the notations to  and  for linear transformations   and  for convolutional transformations. We also included additional clarifications below equation 4 and 11.

- Just in case if the reviewer's meaning might have been to correct the notation FFN to MLP in Equation 4 and 11 and we did not properly address the reviewer's concern,  we respectfully refer the reviewer to the sentence above equation 2, where we explicitly mention "passing the normalized values to an MLP, which
is also known as Feed-Forward Network (FFN).".





\newpage
\noindent \large\textbf{Reviewer: 2}
\begin{longtable}{|p{16cm}|}
	\hline 
	
		
	\textbf{- Show some failure cases, e.g how does the proposed method cope with non-Lambertian surfaces/objects.  }\\
	
	\textbf{- If images in an input pair are not relevant (showing completely different objects), such that there are no correspondences between views, what would the method return in this case? }\\

	
	
	\hline
\end{longtable}
- We agree that including limitations in the paper will make our paper even stronger. As the reviewer suggested, we collected several images with non-Lambertian objects and passed to the proposed method to see how our method copes with them. Surprisingly, we found that the proposed method performs quite well on them as evidenced at a newly added Figure 14. We suspect that since there exist some non-lambertian objects in SPair-71k dataset (bottles), the proposed method does not struggle to find correspondences. Thanks for the suggestion. 

- Regarding the limitations and failure cases, we have added a Section and a figure to show the limitations of the proposed method. In figure 16, we visualized failure cases, which include when there are no common objects between the input images, and when accurate correspondence fields are not established due to extreme geometric deformations. In Section 5.5.9, we also discussed limitations of the proposed method, which include its incapability to process higher resolutions, since it directly processes correlation maps, and as the reviewer pointed out, when there are no common objects in the input images. 


\newpage


\begin{longtable}{|p{16cm}|}
	\hline 
	\textbf{
	- It was not very clear to me how the height (h) and width (w) of feature maps (the feature extraction part) were selected?  }\\

	\hline
\end{longtable}

- We thank the reviewer for the question. We included an additional sentence, "We down-sample the features to fixed  and  for efficiency.", in Section 3.2. For the specific value we chose for them in practice, we mention in implementation detail Section that "a sequence of selected features is resized to  = 16 and  = 16.". Since we resize the image pairs to 256  256, the smallest feature resolutions after feeding into ResNet-101 would be 16  16, which is the resolution we chose to adjust.

\newpage

	
\begin{longtable}{|p{16cm}|}
	\hline 
	\textbf{
	- I very much appreciate a thorough evaluation of the method for semantic correspondence (the problem, the proposed method is developed for) but it would be quite interesting to see how the method performs for visual localization (e.g. the Aachen Day-Night dataset) where accurate correspondences are crucial.  }\\
	\hline
\end{longtable}


- We agree with the reviewer's suggestion to extend the evaluation of the proposed approach to visual localization task, since this seems indeed interesting. We have added a new table, showing the quantitative comparison to other works on Aachen Day-Night dataset. Moreover, we added a new Section (Section 5.5.8) regarding this. Note that it is not trivial to do SfM 3D reconstruction with dense matching, and it has long been dominated by sparse matching methods. Unlike PDC-Net or LOFTR, who design their own ways for detector-free matching, e.g., match selection strategy, our approach would have to undergo non-trivial implementations and devise a strategy to evaluate on visual localization tasks without hurting the performance. To this end, we instead used Superpoint for the keypoint detection stage, and evaluated the matches using the proposed methods. 

- From the results, we observed that the proposed approach struggles to find accurate correspondences, which we believe that this is highly likely due to the resolution of the output flow maps. Unlike other works, which include PDCNet (H/4), LOFTR (H/2), DualRC-Net (H/4) and many other sparse matching methods, our approach outputs the flow maps at relatively low resolution; CATs outputs at 1616 and CATs++ outputs at 3232, in practice. This may have led to missing fine-details, an important aspect for the visual localization task. We wish to mention that although CATs undergo the same training and evaluation procedure to CATs++, due to its extremely low output flow resolution, we were unable to evaluate on Aachen day-night dataset, as CATs was unable to find sufficient number of correct matches to build 3D models and localize.    

- Nevertheless, although a new state-of-the-art is not attained, a promising future direction is revealed, which is to capture fine details. As a future direction, we could extend to include a module that captures the missing details of the predicted flow. For example, local cropping around the coarse correspondence may be exploited to find fine details. We believe that it is a promising direction to consider an approach to balance the memory consumption and the output resolution.  Thanks for the suggestion!



\end{document}
