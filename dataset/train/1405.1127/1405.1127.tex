\documentclass{sig-alternate-10pt}

\usepackage{subfigure}
\usepackage{url}
\usepackage{paralist}
\def\figurename{Fig.}
\def\UrlBreaks{\do\A\do\B\do\C\do\D\do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X\do\Y\do\Z\do\do\^\do\_\do\`\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z\do\0\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\.\do\@\do\\\do\/\do\!\do\_\do\|\do\;\do\>\do\do\,\do\?\do\'\do+\do\=\do\#}
\special{papersize=8.5in,11in}

\begin{document}
\title{Scale Congestion Control to Ultra-High Speed Ethernet}


\numberofauthors{4}
\author{Wanchun Jiang, Fengyuan Ren, Xin Yue, Chuang Lin}


\maketitle

\thispagestyle{empty}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother



\subsection*{Abstract}
Currently, Ethernet is broadly used in LAN, datacenter and enterprise networks, storage networks, high performance computing networks and so on. Along with the popularity of Ethernet comes the requirement of enhancing Ethernet with congestion control. On the other hand, Ethernet speed extends to  and  recently, and even  in the near future. The ultra-high speed requires congestion control algorithms to adapt to the broad changes of bandwidth, and highlights the impacts of small delay by enlarging the bandwidth delay product. The state-of-art standard QCN is heuristically designed for the  and  Ethernet, and unaware of the challenges accompanying the ultra-high speed. 

To scale congestion control to ultra-high speed Ethernet, we propose the Adaptive Sliding Mode (ASM) congestion control algorithm, which is simple, stable, has fast and smooth convergence process, can tolerate the impacts of delay and adapt to the wide changes of bandwidth. Real experiments and simulations confirm these good properties and show that ASM outperforms QCN. Designing ASM, we find that the derivative of queue length is helpful to rate adjustment because it reflects the difference between bandwidth and aggregated sending rate. We also argue for enforcing congestion control system staying at the congestion boundary line, along which it automatically slides to stable point. These insights are also valuable to develop other congestion control algorithms in ultra-high speed networks. 

\category{C.2.2}{Computer Systems Organization}{Computer Communication Networks}[Network Protocols] 
\terms{Design, Algorithms, Standardization}
\keywords{Ultra-high Speed Ethernet, Congestion Control, Sliding Mode Motion, Derivative of Queue Length}


\section{Introduction}

Up to now, Ethernet has been very popular in LAN, datacenter and enterprise networks, storage networks, High Performance Computing (HPC) networks, carrier and service provider networks and so on. With the popularity of big date, multiple cores servers and wireless devices, higher speed is required by applications such as huge scientific data transfer, and at positions such as the aggregation and core layer of datacenter and enterprise networks, the aggregation of the wireless access in carrier and service provider network~\cite{roadmap}. The Moore's law also predicts that servers need  I/O bandwidth in 2017~\cite{commercial}. To solve this problem, the standards of  and  Ethernet have been ratified recently~\cite{100G}, and the  Ethernet is under development~\cite{400G}. Currently, ESnet5 and Internet2 have deployed  network for scientific experiments~\cite{ESnet5}, companies such as Cisco, Brocade, Extreme and Huawei become suppliers of  Ethernet~\cite{extreme}, and the  Ethernet is expected to be available for commercial use in the near future~\cite{commercial}. The ultra-high speed would in turn expand the popularity of Ethernet.


Along with the popularity, however, also come new requirements making congestion management indispensable to ultra-high speed Ethernet. Congestion management is the cornerstone for Ethernet to satisfy the low latency requirement of HPC traffic and the lossless requirement of block-based storage traffic. Moreover, Ethernet congestion management would be helpful to improve the performance of upper layer protocols such as iSCSI~\cite{iSCSI} and iWARP~\cite{iWARP}. In addition, congestion management also takes an important role in enhancing Ethernet as the unified switch fabric of data center networks . Nowadays, Ethernet congestion management has been standardized by IEEE 802.1Qbb~\cite{802.1Qbb} and IEEE 802.1Qau~\cite{802.1Qau}. Specifically, IEEE 802.1Qbb develops the Priority-based Flow Control (PFC) mechanism, which pause the incoming traffic within the same priority to prevent dropping packets caused by transient congestion. To eliminate the long-lived congestion, IEEE 802.1Qau defines the end-to-end congestion control mechanism composed by a rate-based and queue-based framework, and a standardized congestion control algorithm named Quantized Congestion Notification (QCN). Both PFC and QCN have been supported in commercial devices such as FocalPoint FM6000 series~\cite{FM6000} and Juniper QFX3500~\cite{QFX}.


In this paper, we focus on scaling congestion control algorithm to ultra-high speed Ethernet. The impacts of ultra-high speed on the performance of congestion control are twofold: 1) The available bandwidth can changes in a wide range. Consequently, congestion control algorithms should be elaborately designed to adapt to the broad changes of bandwidth. 2) The ultra-high speed will enlarge the Bandwidth Delay Product (BDP) and correspondingly highlight the impacts of delay even if it's small. Although the good performance of QCN under the  and  Ethernet has been shown with simulations, experiments and theoretically analysis ~\cite{sim2,qcn-netfpga, average}, QCN is unaware of these challenges and accordingly unable to scale to ultra-high speed Ethernet (see \S2.3).


We stipulate that the congestion control algorithm for ultra-high speed Ethernet should be simple and stable, converge to stable state quickly  and smoothly, tolerate the impacts of delay and adapt to the broad changes of bandwidth. The Adaptive Sliding Mode (ASM) congestion control algorithm, which is proposed to replace QCN for ultra-high speed Ethernet in this paper, achieves these goals by enforcing congestion control system staying at the congestion boundary line, regardless of Ethernet speed, with the help of the derivative of queue length. Consequently, the congestion control system slides automatically along this boundary line to the stable point, being insensitive to the change of parameters of rate adjustment rules and  delays. In other words, ASM can tolerant the impacts of delay and its convergence process is smooth. Moreover, the sliding process and the approaching process are separated in ASM, and the approaching process can be accelerated independently for short response time. 


We demonstrate a hardware implementation of ASM on the NetFPGA platform~\cite{netfpga}, and use it to verify our simulator on the NS2 platform. Experiments and simulations confirm the theoretical good properties of ASM and show that ASM outperforms QCN, especially with ultra-high speed links.

The key contributions in this paper are not only the ASM algorithm itself, but also the insights on scaling congestion control algorithms to  ultra-high speed networks. We focus on the trajectory of congestion control algorithm on the plane of queue length and aggregated sending rate. We also emphasize the value of the derivative of queue length in determining the direction of trajectory, because it directly reflects the difference between link capacity and aggregated sending rate. Furthermore, we argue for enforcing the trajectory moving straightforwardly to the stable point regardless of link speed, e.g., along the congestion boundary line which has a fixed angle to the efficiency line. In contrast, classic algorithms such as AIMD~\cite{AIMD} and GAIMD~\cite{GAIMD} oscillate up and down around the efficiency line. And adjusting sending rate in proportion to the extent of congestion makes the trajectory cycling around the state point on the plane of queue length and aggregated sending rate. 


In the rest paper, we discuss congestion control of ultra-high speed Ethernet in \S2 and present the origin of designing ASM in \S3. We then show the ASM algorithm in detail and theoretically analyze its performance in \S4, and evaluate ASM in \S5. Finally, we discuss related work in \S6 and conclude in \S7.




\section{Congestion Control of Ultra-high Speed Ethernet}
We firstly give a brief introduction about the framework of Ethernet congestion control, then show the challenges of designing congestion control algorithm for ultra-high speed Ethernet, and finally discuss whether QCN can address these challenges or not.


\begin{figure}
\centering
\includegraphics[width=2.8in]{figure/framework.eps}
\caption{Framework defined by IEEE 802.1Qau}
\label{structure}
\end{figure}

\subsection{Framework}
IEEE 802.1Qau has been developing the congestion control mechanism for Ethernet since 2006 . With the lessons learned from designing other congestion control mechanisms in history, and taking the special environment of link layer into consideration, the researchers developed a rate-based framework, together with several congestion control algorithms, including the final standard QCN. 

Briefly speaking, the framework is composed by two parts, as shown in . One is called \textit{Congestion Point (CP)}, which refers to congested switches. The responsibility of CP is to detect congestion, sample incoming packets, and generate and send feedback packets. At CP, the extent of congestion is represented by information of queue length.  The other one is called \textit{Reaction Point (RP)}, which refers to sources. RP takes charge of parsing feedback packets, and correspondingly adjusting its sending rate according to the feedback state information. In general, the rate adjustment is implemented by the rate limiters at Network Interface Cards (NICs) or edge switches.

Generally speaking, the primary goal of the congestion control algorithms for Ethernet is to tightly hold the instantaneous queue length  around the predefined target point . The target queue length  is elaborately chosen to avoid overflowing or draining of the buffer, which are corresponding to packets loss and link underutilization, respectively. Ideally, if  is always small, then the  can be set small to reduce the queuing delay and reserve buffer room for burst traffic without degrading link utilization.


\subsection{Challenges}
To vividly illustrate the challenges in ultra-high speed Ethernet, we firstly present a graphic view of congestion control algorithms. Obviously, the core of congestion control algorithm for Ethernet is the interaction between the queue length at CP and the sending rate at RP. Thus, we put the queue length  and the aggregated sending rate  together. At any given time , the state of the congestion control system can be represented by point  in the plane of queue length and aggregated sending rate, which is called Q/R plane for convenience of subsequent discussion. Connecting all state points with arrows showing the direction that time  increases, we can obtain the trajectory describing the dynamic behaviors of congestion control system. The red trajectory in  is an example and the corresponding dynamic queue variation is illustrated below.


\subsubsection{Congestion Detection}
In the framework defined by IEEE 802.1Qau, the queue length is chosen as the congestion indicator. However, using queue length as the only congestion indicator is one-sided because the extent of congestion is also related with the aggregated sending rate . More intuitively, lines  and  (link capacity) divide the Q/R plane into four regions, but we can only judge that region \Rmnum{1} in  is underloaded and region \Rmnum{3} is congested. It is hard to determine whether the rest regions are congested or not.

In history, either line  is used as the boundary to judge congestion or not in Active Queue Management (AQM) schemes or line  is considered as the efficiency line of the AIMD regulation algorithm. In the high speed Ethernet, these simple rules for congestion detection will face challenges since the regions except for \Rmnum{1} and \Rmnum{3} can't be roughly regarded as congested or not. 



\subsubsection{Rate Adjustment}
The direction of trajectory in the Q/R plane has the following attributes. Since 

the trajectory of congestion control system can only point to right above the line , and point to left when . 

Constrained by this law, the rate adjustment rules decide the concrete direction of trajectory. More specifically, the slope  of the tangent of trajectory at any point  satisfies 

as shown in . Actually,  tells how fast RP adjusts its sending rate.


Eq. (\ref{slop}) implies that rate adjustment rules must associate with bandwidth and aggregated sending rate, or else the slope  varies with the changes of bandwidth. For example, when the additive increase mechanism is employed and the link capacity is huge,  is constant in the underloaded region \Rmnum{1}, and accordingly  referring to Eq. (\ref{slop}). It means that trajectory would move towards line , which implies empty buffer and even link underutilization, in the underloaded region \Rmnum{1}. This explain why TCP suffers from serious link underutilization in high speed networks~\cite{HS-TCP}. To maintain desired slop  such that trajectory is able to reach the stable point, rate adjustment rules or  should adapt to the changes of bandwidth. As the available bandwidth is unknown and can change in a large range, designing rate adjustment rules for ultra-high speed Ethernet becomes challenging.


\begin{figure}
\centering
\includegraphics[width=2.2in]{figure/switch-line.eps}
\caption{Dynamic Behaviors of Congestion Control System in Q/R Plane}
\label{switch-line}
\end{figure}


\subsubsection{Delay}
Contradicted to what has been known that large delay will ruin the stability of congestion control systems, small delay is always ignored. However, ultra-high speed Ethernet will increase the BDP and accordingly highlight the impacts of small delay. For example, when the bandwidth is  and the delay is , the BDP is . In other words, the error  of queue length observed by end hosts can be expressed as

where  denote the delay, is in the order of megabits. This is an unavoidable error to all congestion control algorithms for ultra-high speed Ethernet. Consequently, tolerating such a large error is challenging in designing congestion control algorithms. 


\textbf{Remark} There are also other errors, whose impacts are magnified by the ultra-high speed similar to delay, such as the measurement error of the queue length in hardware implementation and the absence of feedback information in the sampling interval. Obviously, the amplitudes of these errors are generally smaller than delay. Thus, we just consider delay in this work.



\subsection{Understanding of QCN} The state-of-art algorithm QCN is heuristically designed for the  and  Ethernet. Although the congestion detection is improved in QCN, it is unaware of the rest challenges and accordingly can't scale to ultra-high speed Ethernet as we will discuss below. For the convenience of discussion, we would list the basic operations in QCN at first and more details are in . 

\subsubsection{Basic Operations}
In QCN, whether the system is congested or not is judged according to the boundary line  (denoted by ) as shown in , where 

and 

 denotes the variation of the queue length in the sampling interval,  is the sampling probability and  is a weight. QCN employs multiple decrease by utilizing the feedback information and binary search algorithm for rate increase, complemented with correction mechanisms including AI, HAI, Target Rate Reduction and Extra Fast Recovery.

Let  denote the current sending rate and  denote the target sending rate recorded before rate decrease.
\\
\textbf{Rate Decrease (RD)} On the arrival of feedback packet, RP assigns the value of  to  and then decreases . 

where  is a constant chosen for , i.e., the sending rate won't decrease more than  in one adjustment.
\\
\textbf{Fast Recovery (FR)} After the \textbf{RD} phase, RP does binary search. More specifically, RP increases its sending rate  every  seconds 

Parameter  is acquiescently set to be the time of sending  data, which is measured by hardware implemented counter or timer. This behavior will be repeat five times by default, if no feedback messages arrive. 
\\
\textbf{Active Increase (AI)}
After the five times repetition, the binary search is persisted but the target rate  will be increased and the time interval of each cycle is halved. Namely

where  is a constant unit.
\\
\textbf{Hyper-Active Increase (HAI)}
By default,  is set to be  for the  Ethernet. But when both the counter and the timer indicate the QCN system has entered into the \textbf{AI} phase,  is increased to . This is called Hyper-Active Increase.
\iffalse
\\
\textbf{Target Rate Reduction}
If  is large than  by 10 times,  is decreased by a factor of 8. 
\\
\textbf{Extra Fast Recovery}
When RP receives consecutive feedback packets in one cycle, it just do RD once.
\fi



\subsubsection{Congestion Detection}
In the Q/R plane, except for the congested region \Rmnum{3} and the underloaded region \Rmnum{1}, the rest parts are divided into four regions by QCN using lines ,  and , as shown in . In region \Rmnum{5},  is greatly larger than  even though . And in region \Rmnum{4},  is greatly larger than  even though . Therefore, both regions \Rmnum{4} and \Rmnum{5} are treated as congested. Similarly, the regions \Rmnum{2} and \Rmnum{6} are treated as underloaded. Consequently, line  becomes the congestion boundary line. Note that to solve the challenge of congestion detection, QCN does not explicitly declare whether  or  dominates, but makes trade-off between queue offset and  with an adjustable weight  referring to Eq. (\ref{feedback}) and (\ref{deltaq}). However, this good congestion judgment method is not used continually in the rate adjustment of QCN, as we will show subsequently.  



\subsubsection{Rate Adjustment}
As the sending rate will be decreased in congested regions, the tangent of the trajectory of QCN can only point to lower right in regions \Rmnum{3} and \Rmnum{5}, and lower left in region \Rmnum{4}, as shown in . Constrained by this law, the actual direction of trajectory is decided by concrete rate adjustment rules. Referring to Eq. (\ref{decrease}), there is

in congested regions \Rmnum{3},\Rmnum{4} and \Rmnum{5}. Similarly, the trajectory of QCN can only point to upper right in region \Rmnum{2}, and upper left in region \Rmnum{1} and \Rmnum{6}. In the  cycle of \textbf{FR} phase, there is

where  is the aggregated target sending rate, referring to Eq. . Totally, the speed of rate adjustment in the \textbf{RD} phase and the \textbf{FR} phase is in proportion to the extent of congestion  and associated with the aggregated sending rate.

Besides, when the trajectory approaches to line , there are  referring to Eq. (\ref{decrease-slope}) and (\ref{increase-slope}), and accordingly  in Eq. (\ref{slop}). Consequently, the trajectory keeps moving from region \Rmnum{5} to region \Rmnum{3}, and from region \Rmnum{6} to region \Rmnum{1}. Referring to the constraints on the direction of trajectory in region \Rmnum{1}, \Rmnum{2}, \Rmnum{3}, \Rmnum{4}, the trajectory of QCN probably cycles around the stable point , just as the red trajectory shown in . Correspondingly, the queue length oscillates up and down, and eventually converges to target point .

As a step further, the binary search method is initially used to find proper sending rate  in BIC-TCP~\cite{BIC}, where the target sending rate recorded in \textbf{RD} phase is generally considered as larger than the bandwidth. In QCN, the binary search fails to find  in the \textbf{FR} phase when previous \textbf{RD} is done in region \Rmnum{4} such that the target sending rate  referring to Eq. (\ref{decrease}). In this condition, QCN needs the \textbf{AI} phase. But the parameters of the  \textbf{AI} phase can not adapt to the broad changes of bandwidth due to the existence of the constant . Even for the  Ethernet, QCN still needs the \textbf{HAI} phase to adjust parameters as aforementioned. When the speed of Ethernet increases to , where the available bandwidth can change in a huge range, it becomes very hard to set proper parameters for QCN. Previous work  also shows that the performance of QCN is sensitive to the changes of both parameters setting and link speed. Therefore, the parameters setting of QCN is cumbersome and can not adapt to the link bandwidth with broad variation. 

The above problem occurs because the rate adjustment goal of QCN to  although whether system is congested or not is judged according to Eq. (\ref{feedback}). Consequently, the speed of rate adjustment of QCN would be the same at three points  shown in , which are of the same distance to line  or are judged to be of the same extent of congestion . But obviously, points  denote different extent of congestion and the difference among them is lost when QCN computes the weighted sum according to Eq. . While these points can be treat differently to improve the performance of congestion control algorithm as discussed in next section. 

In sum, the rate adjustment of QCN is in proportion to the extent of congestion. But it can not adapt to the ultra-high speed, and the external expression of this issue is that the parameters setting of QCN becomes hard with the increase of bandwidth.


\subsubsection{Delay}
QCN is heuristically designed and the impacts of the delay is not taken into consideration. Recent work  shows the upper boundary of delay, which is in the order of hundreds of microseconds when the link capacity is , for the stable QCN system is large enough in data center networks. Following the work in , we deduce a proposition on the lower boundary of delay where QCN becomes unstable (see Appendix A). When the link capacity reaches , this lower boundary becomes the order of dozens of microseconds. Hence, QCN is easy to becomes unstable with the  Ethernet due to large delay. Moreover, the larger the bandwidth, the smaller the lower boundary of delay. Therefore, QCN can not scale to ultra-high speed Ethernet due to the impacts of delay.




\section{Origin of Design}
Above challenges place some specialized requirements on Ethernet congestion control. Here we summarize requirements and then show the key ideas steaming from above understanding of QCN to meet these requirements in ASM. These serve as the origin of our design.

An ideal congestion control algorithm for ultra-high speed Ethernet should have the following properties. First, it should be able to converge to the stable point  from any initial state, which is always referred as the stability requirement of the congestion control system. Second, the convergence process should be fast and smooth~\cite{Metrics}, which are measured by the response time and largest amplitude of oscillation, as shown in . These are two basic properties of all congestion control algorithms. Third, the congestion control algorithm should be robust to various errors, including delay. In this way, the buffer occupancy can always stay at the target queue length  with small oscillation, and accordingly  can be set small enough to reduce the queuing delay and reserve spare buffer for burst traffic to reduce the packets loss ratio without degrading link utilization. The last but not the least important, the above properties should not be destroyed by the wide changes of available bandwidth, namely the congestion control algorithm can scale to ultra-high speed Ethernet. With these properties, the congestion control system can admire prefect performance, i.e.,  high throughput, low packets loss ratio and low queuing delay. 


Inspired by above understanding of QCN on the Q/R plane, we are aware that  is not only useful in congestion detection, as what QCN has done, but also helpful to scale congestion control algorithm to ultra-high speed Ethernet, because  reflects bandwidth and  aggregated sending rate referring to Eq. (\ref{deltaq}). More specifically, with the help of , we are able to choose rate adjustment rules such that trajectory of congestion control system moves towards any desired direction, i.e.,  or compute the speed of rate adjustment by multiplying the desired tangent  of trajectories by , referring to Eq. .


But what is the desired direction? Intuitively, it is expected that the trajectory moves straightforwardly to the stable point  from any initial points. But this can only be done in regions \Rmnum{2}, \Rmnum{4}, \Rmnum{5} and \Rmnum{6}, because the trajectory is constrained to point to left and right respectively in region \Rmnum{1} and \Rmnum{3}, as shown in \S2.2.2. Taking the physical interpretation of trajectory into consideration, it is natural to expect the trajectory moves along the boundary line  of congestion  to the stable point. In other words, the congestion control system should be held around line . Note that once the congestion control system keeps staying at line , the trajectory automatically slides along it to the stable point because the queue length changes with  on line .


As a step further, we find that the above expected behavior is similar to a special motion pattern called sliding mode motion~\cite{Itkis}, whose characteristic is insensitive to the impacts of errors. Consequently, congestion control algorithm designed in this way can address aforementioned challenges, namely adapt to the changes to bandwidth with the help of , approach to the stable point quickly and smoothly, and be robust to the impacts of delay in the sliding mode motion. Subsequently, we describe the design of ASM in detail. 


\section{Design of ASM}
We present the detailed Adaptive Sliding Mode (ASM) algorithm at first, and then show why it can meet the aforementioned goals by using  for its rate adjustment to form the sliding mode motion. Note that we just update the congestion control algorithm but follow the framework defined by IEEE 802.1Qau. 

\subsection{Algorithm}
ASM is also composed by CP and AP. \newline
\textbf{Congestion Point} Similar to QCN, the switch monitors its queue length, ``samples'' incoming packets periodically with probability , and generates feedback packets. The main difference is twofold. First, the offset of the queue length  and the variation of the queue length  are carried in the feedback packets separately. Second, feedback packet is also generated for rate increase. \newline
\textbf{Reaction Point} 
Different from QCN, whose goal is to adjust the aggregated sending rate at , ASM enforces congestion control system staying at boundary line . Thus, ASM focuses on the direction of trajectories rather than whether the sending rate is increased or decreased, just like in QCN. To control the direction of trajectories, ASM always needs the information of  as discussed before. Specifically, let  denote the current sending rate, the concrete rate adjustment algorithm of ASM is as follows.

where coefficients  and  are

Here  are constants and  is given in Eq. . Note that  is used for rate adjustment independently in Eq. (\ref{adjustment}), but QCN losses the concrete value of  in computing  according to Eq. (\ref{feedback}) and uses  only for rate decrease. 


\begin{figure}
\centering
\includegraphics[width=2.1in]{figure/asm.eps}
\caption{Possible Trajectories of ASM}
\label{asm-trajectory}
\end{figure}

\subsection{Trajectory of ASM}
Subsequently, we show that the trajectory of ASM on the Q/R plane will endorse the features described in \S3 if coefficients  are properly tuned.

To obtain the trajectory of ASM, we can describe ASM by the following differential equations. (see Appendix B for the detailed deduction)

where  and  is the number of sources. The trajectories of such a second order ordinary differential equation with different characteristic roots have been listed in . To enforce the trajectory of ASM staying at the boundary line , we choose proper coefficients as follows such that trajectory moves towards line  in both sides of it, as  and , or  and  illustrated in . Namely

This inequality is equivalent to 

Note that  is sufficient to inequalities (\ref{sliding}), or in other word  is indispensable for enforcing the trajectory of ASM at the congestion boundary line. When inequalities (\ref{sliding}) are satisfied, we can know the distribution of the roots of the characteristic equation of , and accordingly know the trajectory of ASM referring to . The detail is summarized in Appendix C. 


Specifically, when  , i.e., in regions \Rmnum{1},\Rmnum{2},\Rmnum{3} and \Rmnum{4}, the trajectory of ASM is spiral as the red line shown in . When , i.e., in regions \Rmnum{5} and \Rmnum{6}, the trajectory of ASM is parabola with asymptotic lines  and , as the blue line shown in . Consequently, the trajectory of ASM can approach to the boundary line  quickly from whatever initial state. Once reaching line , the trajectory will move back and forth between regions \Rmnum{2} and \Rmnum{5}, or between regions \Rmnum{4} and \Rmnum{6}, depending on different feedback information and the corresponding rate adjustment rules , as illustrated in . For example, started from region \Rmnum{3}, the trajectory reaches line  in region \Rmnum{4} spirally. After that, due to various factors such as the delay and the errors discussed in \S2.2.3, the trajectory will pass over line , go into region \Rmnum{6} at little and stay at point . Subsequently, the other rate adjustment rule with parameters  and  takes effect in \Rmnum{6} and forces the trajectory moving from point  back to region \Rmnum{4} at point  along a parabola. The switching is repeated uninterruptedly, and the trajectory slides along line  to the stable point. This motion pattern is called sliding mode motion.

In the sliding mode motion, the trajectory or the dynamic of congestion control system is equivalently described by Eq. , whose solution is 

Namely, the queue length  converges to the target point  exponentially along line  in the sliding mode motion. Similarly, the sliding mode motion can also appear between region \Rmnum{2} and region \Rmnum{5}. \\
\textbf{Remark} Note that rate adjustment rules just force the trajectory of congestion control system staying at the boundary line  of congestion. The trajectory can not stay at any other fixed points but \textit{automatically} slide along line  to the stable point  because the queue length changes with  on line .


\subsection{Convergence and Stability}
From the trajectory of ASM, we can know that ASM can converge to the stable point starting from any initial stat, namely ASM is stable. 

Moreover, the convergence process is composed by two parts: the approaching process refers to ASM moving from any initial point to the boundary line , and the sliding mode motion where ASM slides along  to the stable point. Note that the approaching process is directly controlled by the rate adjustment rules, while the sliding mode motion of ASM is equivalently controlled by Eq. , which has nothing to do with the coefficients of the rate adjustment rules. Consequently, these two parts can be designed separately.

More specifically, the approaching process can be accelerated by setting large coefficients for rate adjustment rules, while small coefficients are required in the sliding mode motion. In general, we suggest the following adaptive parameters setting guidelines.
\begin{itemize}
\item First, all parameters should satisfy inequalities . 
\item Large parameters  and  are set initially.
\item When  is smaller than a bound , i.e., when ASM is going to enter into the sliding mode motion, small parameters  and  are employed.
\item When ASM is close to the stable point, namely , the large parameters  and  are reused in case ASM passively deviates stable point again.
\end{itemize}


With adaptive parameters setting method, the approaching process of ASM is accelerated by large coefficients. And the converging speed of queue length is exponential in sliding mode motion referring to Eq. . Therefore, the whole convergence speed of ASM will be greatly improved. In addition, since the trajectory of ASM reaches the stable point directly, the amplitude of the oscillations of queue length can be kept small.


\subsection{Impacts of Delay}
The above analysis describes only the ideal behaviors of ASM, where the impacts of delay are ignored. Surprisingly, the behaviors of ASM, even accounting for the impacts of delay, are still a good approximation of the ideal sliding mode motion.

More specifically, we obtain Proposition \ref{Proposition2}, which shows that the impacts of delay can be split into three parts. (see Appendix D). The first one is about the congestion detection, namely the feedback information is delayed. Correspondingly, the slope of the boundary line  slightly changes, as shown in Eq. (\ref{switching}). The second one is related to the rate adjustment. The computing of sending rate based on old feedback information can be treat as a slight change of the coefficients of rate adjustment rules, and the precise amplitude of the parameters drift is given in Eq. (\ref{para-drift}) and (\ref{amplitude}). The third one is that the computed sending rate takes effect with some lag. This can be treat as error on the result of rate adjustment, which is bounded by inequality (\ref{error}).

Proposition \ref{Proposition2} also explains why the impacts of delay with determined bound can be tolerated by ASM. First, according to , the boundary line is enlarged to be a boundary region called quasi-sliding region as illustrated in . Correspondingly, the sliding mode motion is replaced by the quasi-sliding mode motion, which is approximately equivalent to the ideal sliding mode motion. In other words, ASM can still slide to the stable point and the sliding process is equivalently described by Eq.  in both motion patterns. Second, note that Eq.  does not involves any of the drifted parameters used in rate adjustment rules . Although the parameters drift would result in a slight change of the direction of trajectory in the approaching process, but the motion pattern of the trajectory persists and the trajectory is always able to reach the boundary region. In total, the impacts of parameters drift can be tolerant by ASM. On the other hand, the drifted parameters can also be compensated through proper parameters setting. Finally, when the impacts of bounded error  given in Eq. (\ref{error}) are taken into consideration, ASM can still enter into the sliding mode motion and slide to the stable point outside the region  drawn in . With proper parameters setting, the range of  becomes small enough according to Eq. (\ref{H0}) and correspondingly the queue length stays close enough to the stable point. Thus, bounded disturbance  is tolerable by ASM. 

\begin{figure}
\centering 
\includegraphics[width=2in]{figure/sliding-mode-motion.eps}
\caption{Impacts of Delay on ASM}
\label{smm}
\end{figure}



\subsection{Deployment}
Although ASM has the above theoretical advantages, some issues should be considered carefully in deploying ASM. \\
\textbf{CPID}
As multiple bottleneck links exist, the feedback packets of ASM should also involve an identification of the congested switch, called CPID. When the sending rate is decreased, the sender saves the CPID. And the sending rate is increased only when the stored CPID matches the CPID embedded in the feedback packet. In this way, ASM can ignore the feedback information of underloaded switches. Because each packet passes only several switches and one feedback packet is generated when  packets pass one switch (assume ), the number of feedback packets is naturally smaller than that in the case of acknowledging each packet. Thus, the feedback mechanism is light and acceptable.\\
\textbf{Sampling}
As the sampling interval may become smaller than the propagation delay in ultra-high speed Ethernet, a feedback packet may be generated before the former feedback packet arriving the sender. Moreover, when the traffic is burst, these two feedback packets probably go to the same sender. Consequently, one of the feedback packet becomes redundant. In our implementation, the destination address of the feedback packet is recorded and then the incoming packet, whose source address is the same as the recorded destination address, will not be sampled. In this way, two consecutive feedback packets to the same sender can be avoided. \\
\textbf{Weight}
Different rate adjustment rules of ASM work depending on the sign of  and . Referring to Eq.  and , we can know that the range of the region corresponding to state  would be very small in . In reality, this region may be skipped over and the sliding mode motion would never occur, because the sampling and the rate adjustment are discrete. Therefore, the weight  should be set relatively large to enlarge the width of the region corresponding to state .\\
\textbf{Parameters Setting} The analysis in \S4.2 suggests the parameters setting is guided by inequalities . But Eq.  and  should also be taken into account to reduce both the impacts of parameters drift and the range of region . Obviously, all the inequalities suggest large  and , and small  and .

In the hardware implementation, the concrete values of parameters  and  are also decided by the number of bits used to represent the queue length, because the amount of the rate changed is also in linear proportion with  and , as indicated by Eq. .


\iffalse
\textbf{Quantization}
In the hardware implementation, the feedback information is quantized into several bits. In general, the quantization is linearized. For example, when the buffer size is  and the number of bits used to represent the instantaneous queue length is , the quantized queue length changes  when the real queue length changes . However, this quantized result is not balanced to the rate adjustment algorithm. Assume the target queue length is , the quantized queue length is smaller than the target point only when it takes value in , while larger than the target point when it takes value in . It is better to let the rate adjustment algorithm see that quantized queue length is smaller than the target point when it takes value in , and vice versa. Or else, in the rate adjustment, the parameters need to be very small for the condition that the queue length is smaller than the target point, and vice versa.\\
\fi


\section{Evaluation}
We evaluate ASM using both simulations on the ns2 platform and hardware implementations on the NetFPGA platform . The hardware implementation is used to confirm the basic motion pattern of ASM and validates our simulator. As our NetFPGA can only handle traffic at the speed of  and each card has only four Ethernet ports, simulations dominate the evaluation of ASM.


\subsection{NetFPGA-based Experiments}
The NetFPGA card is developed for fast prototyping of advanced network protocol. We implement the CP of ASM based on the Ethernet Switch project and implement the RP of ASM based on the Packet Generator projects . In hardware implementation, both  and  are quantized into 8 bits, the buffer size is  and the default parameters are that  and , , ,  are set such that the sending rate changes no more than , ,  of the link capacity respectively in each rate adjustment. 




\begin{figure}
\centering
\subfigure[Queue Length and Rate]{
    \centering \includegraphics[width=1.4in]{figure/time.eps}
}
\subfigure[Trajectories]{
    \centering \includegraphics[width=1.4in]{figure/queue-rate.eps}
}
\caption{Sliding Mode Motion of ASM}
\label{sliding-ASM}
\end{figure}


\subsubsection{Sliding Mode Motion of ASM}
Experiment is conducted with 3-source dumbbell topology,  Ethernet and  propagation delay at each link. Parameter  for the convenience of observation. When all the three sources concurrently start long lived flows at the speed of , the dynamics of queue length and the aggregated sending rate at the congested switch are shown in (a). Obviously, both the queue length and the aggregated sending rate converge to the stable point quickly. Putting queue length and aggregated sending rate together, we obtain the trajectory of ASM on the Q/R plane, as shown in (b). The trajectory approaches to the stable point directly and finally chatters around the stable point, similar to the ideal trajectory of sliding mode motion. Therefore, the sliding mode motion of ASM is confirmed by experiment. 


\subsubsection{Simulator Validation}
To simulate the hardware implementation, we manually do quantization of the queue length and the sending rate, and add some corresponding random factors. Simulators use the same parameters setting as experiments. The simulation results are also involved in .
The differences between experiment result and simulation result are twofold. First, the slopes of the lines, along which ASM approaches to the stable state, are different. This is probably because the interval of measurement, where queue length is read out from NetFPGA to its hosted compute every , is comparable to the time taken for ASM to converge to the stable point in experiment. Second, the amplitude of the oscillations of queue length is slightly large in experiment. Such oscillation is expected because the experiment involves many random factors, even though major factors have been considered in the implementation of simulator. Except these differences, the experimental result and the simulation result are identical in . Therefore, we believe that the following simulation results are also trustworthy.



\subsection{NS2-based Simulations}
Due to the limited number of ports and link speed, we evaluate ASM with simulation subsequently. QCN is implemented as the comparison algorithm on the ns2 platform by following the pseudo code version 2.3 .
\begin{figure*}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=2in]{figure/expr2-smallqueue.eps}
\caption{Ability of maintaining small queue length}
\label{small-queue}
\end{minipage}
\vspace{0.1cm}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=2in]{figure/expr5-convergence.eps}
\caption{Convergence}
\label{convergence}
\end{minipage}
\vspace{0.1cm}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=1.9in]{figure/expr3-parameter.eps}
\caption{Average queue length with different combination of parameters}
\label{parameter}
\end{minipage}
\end{figure*}
\begin{figure*}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=2in]{figure/expr4-bandwidth.eps}
\caption{Different Bandwidth}
\label{bandwidth}
\end{minipage}
\vspace{0.1cm}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=2in]{figure/delay.eps}
\caption{Different Delay with  Ethernet}
\label{delay}
\end{minipage}
\vspace{0.1cm}
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=1.9in]{figure/parking-result.eps}
\caption{Dynamics of queue length at switches with the changes of congested switch}
\label{park-result}
\end{minipage}
\end{figure*}
\textbf{Small Queue Length}
As the direct goals of ASM is to maintain small queue length for low queuing delay, high throughput and so on, we set the target queue length . The dumbbell topology is still used and the number of sources is increased to . The dynamic of queue length and throughput at the bottleneck link are drawn in . Obviously, ASM can always ensure nonempty buffer and does not loss throughput, while QCN frequently drains its buffer and loss throughput.  \\
\textbf{Convergence}
To show the convergence of ASM, we let  sources start at . Two of them start at the beginning, the rest three sources start subsequently one by one every  latter. At the  second, sources are finished one by one every . The instantaneous throughput at the bottleneck link is shown in . ASM can always maintain high throughput with the incoming or finishing of traffic, while QCN suffers from degrading of throughput. It implies that ASM can converges to the stable point more quickly than QCN with smaller oscillation. \\
\textbf{Parameters} We double or halve parameters , ,  respectively, obtaining  different combinations of these parameters. When each combination is used, we gather the average queue length. As shown in , the average queue length changes a little even parameters of ASM are changed in a large range. In other words, ASM is insensitive to the changes of parameters. \\
\textbf{Bandwidth}
We modify the link capacity from  to , and the dynamic of queue length and the throughput are presented in . With the changes of bandwidth, the amplitude of oscillation almost keeps unchanged, and there is no throughput degradation. Therefore, ASM can adapt to the broad changes of bandwidth.
\\
\textbf{Delay}
We change the propagation delay with  Ethernet, the dynamics of queue length at the congested switch are shown in . Obviously, both ASM and QCN can maintain small oscillation of the queue length when the delay is extremely small. But when the propagation delay of each link is increased to , namely when RTT is , QCN becomes unstable and the buffer of congested switch is emptied frequently. This result agrees with the conclusion in Proposition . While ASM can tolerate the impacts of delay, as claimed in \S4.4.

\noindent\textbf{Multiple Bottlenecks}
Subsequently, we will test the performance of ASM on the classic parking lot topology, where multiple bottlenecks exist. As shown in , all links are of capacity , flows  start at  and are terminated at  respectively, and flows  start randomly at any time in  and last for one second. Obviously, the congested switches change frequently in this condition. The dynamics of the queue length at switches are shown in . According to the simulation results, ASM can always rapidly response to the changes of congested switches.

\begin{figure}
\centering
\includegraphics[width=2.5in]{figure/parking.eps}
\caption{Parking Lot Topology}
\label{park-lot}
\end{figure}





\iffalse
Next, the performance of ASM with different link capacities is explored. Let flows  start at  and terminate at ,  respectively. When the capacity of link  is shrunk to , the corresponding dynamics of the queue length at switches are shown in \figurename \ \ref{park-bandwidth}(a). When the capacities of all the links are changed to , corresponding dynamics of the queue length at switches are shown in \figurename \ \ref{park-bandwidth}(b). Obviously, simulation results shown that ASM can always enter into the sliding mode motion and maintain extremely stable queue, regardless of the link capacity. Therefore, ASM would also be able to maintain small oscillation, as we have shown in previous subsection. 
\begin{figure}
\centering
\subfigure[ is ]{
    \centering \includegraphics[width=1.4in]{figure/parking-result-new.eps}
}
\subfigure[]{
    \centering \includegraphics[width=1.4in]{figure/parking-result-100G.eps}
}
\caption{Dynamics of queue length at switches with different link capacity}
\label{park-bandwidth}
\end{figure}

\noindent\textbf{FatTree Topology}
Finally, we will test the performance of ASM on the FatTree topology, which is specifically developed for DCN.
Nowadays, the speed of Ethernet will increase to ultra-high and Ethernet is enhanced to be the unified fabric for all the traffics in data centers, called DCE. Notifying that small queuing delay is emphasized in recent work and required by the HPC traffic, we argue for setting the target queue length of the CM schemes small value. Namely the CM schemes for DCE should maintain small stable queue. We also show that current CM schemes can not scale to the ultra-high speed DCE, because the impacts of the delay, the rate adjustment granularity and the other errors stand out, as the link capacity increases to ultra-high. In this paper, the ASM scheme is designed for the ultra-high speed DCE. ASM approaches the target queue length directly along the switching line, instead of spirally as traditional CM schemes do. Theoretically, benefiting from the advantages of the sliding mode motion, ASM can tolerate the drifting of parameters and the bounded error, which are equivalent to the impacts of the delay, achieve proper rate adjustment granularity through adaptive parameters setting, and replace QCN easily by replace only the rate adjustment algorithm. Small scale experiments and simulations verify the good properties and the scalability of ASM under kinds of conditions.
\fi



\section{Related Work}
As we focus on scaling congestion control algorithm to ultra-high speed in this paper, we mainly compare ASM with the related congestion control algorithms. 

The initial goal of congestion control is throughput. Consequently, traditional TCP keeps on filling the buffer of congested switches until packets are dropped. This method achieves as higher throughput as possible, but suffers from large queuing delay and large packets drop ratio. As low queuing delay and small packets loss ratio are eventually included as goals of congestion control, Active Queue Management (AQM) schemes, such as RED  and PI , are developed to help TCP to control the queue length of congested switches at the target points. This target point is always not small to meet the trade-off among throughput, queuing delay and packets loss ratio~\cite{Metrics}. QCN also works in this way. However, smaller queuing delay is desired with the popularity of high speed Ethernet in data center networks, HPC networks and storage networks. Recent work for data center networks such as DCTCP~\cite{DCTCP} and HULL~\cite{HULL} have tried to lower their target queue length as much as possible, even if the throughput is decreased a little. To ensure high throughput, low queuing delay and small packets loss ratio for ultra-high speed Ethernet, we focus on limiting the oscillation of queue length so as to loose contradictions among these goals.


Furthermore, to limit the oscillation of queue length, it is important to ensure congestion control system converging to the stable point quickly and smoothly. But in general, there is a trade-off between response time and smoothness . The ultra-high speed make it hard to take this trade-off by greatly shortening the finish time of small flows and enlarge the oscillation. Besides, both response time and smoothness are desired by congestion control algorithms for ultra-high speed Ethernet. Or else, either small flows finish in the slow converge process or packets are dropped due to large oscillation. ASM solves this problem by decoupling response time and amplitude of oscillation to same extent, with the help of sliding mode motion, which is independent to its approaching process.


Traditional rate adjustment algorithms can be roughly divided into two categories. One category computes proper sending rate directly. RCP~\cite{RCP} and XCP~\cite{XCP} are of this category. The key of these algorithms is to collect enough information for precise computing, and the drawback is that the overhead of collecting all information is always high. The other category probes for proper sending rate . The classic AIMD algorithm and its extensions are of this category. The characteristic of these algorithms is that a rate (or window size) decrease rule is used to response to congestion and a rate increase rule is used to probe proper sending rate step by step when these is no congestion. However, the length of each step is hard to be decided due to the bandwidth variation. For example, traditional TCP increases its window constantly, and suffers from poor performance in high speed networks. To adapt to the high speed, GAIMD~\cite{GAIMD}, Scalable TCP~\cite{S-TCP}, High Speed TCP~\cite{HS-TCP}, TCP-Illinois~\cite{illinois} and DCTCP~\cite{DCTCP} changes the factors of AIMD as functions of different congestion indicators such as packet loss ratio, RTT and sequence of ECN bits, and BIC-TCP even replaces addictive increase by binary search. In these ways, the length of step is changed dynamically, generally in proportion to the extend of congestion. However, lacking of information of bandwidth, they always step over the proper sending rate  and accordingly oscillate up and down around the efficiency line. Correspondingly, they cycle around the stable point in the Q/R plane, as discussed in \S2.3.3. In contrast, ASM changes its rate adjustment target from efficiency line  to the boundary line , and approaches to the stable point directly along the boundary line regardless of link speed by utilizing , which reflects the difference between link capacity and aggregated sending rate, to control the direction of its trajectory on the Q/R plane.



We also aware that the congestion control system can evolute itself even without any rate adjustment rules. For example, without rate adjustment rules, the queue length increases when , and vice versa. This evolution is not emphasized previously but utilized in sliding mode motion. What we have to do is enforce the trajectory of ASM staying at the congestion boundary line with the help of  in rate adjustment, and then the trajectory of ASM slides automatically to the stable point because . 


The sliding mode motion has been used in~\cite{SMVS, PengYan} to analyze the nonlinearity of TCP and design packets marking or dropping rules. But we use it to design rate adjustment algorithm in this paper. 


\iffalse
\begin{figure}
\centering
\includegraphics[width=2.8in]{figure/tcp.eps}
\caption{Trajectories of Congestion Control Algorithms}
\label{tcp}
\end{figure}

\subsection{Delay}

\subsection{Other Related Work}
ASM is designed using the method of sliding mode control. Previously, this method has been used in~\cite{SMVS, PengYan} to analyze the nonlinearity of TCP and design packets marking or dropping rules. But rate adjustment algorithm is designed in this paper. 
\fi

\section{Conclusion}
The ultra-high speed places two challenges on congestion control of Ethernet. One is that the rate adjustment algorithm should adapt to the broad changes of bandwidth. The other is that the impacts of small delay are magnified by the ultra-high speed. The state-of-art algorithm QCN is unaware of these challenges and accordingly can not scale to ultra-high speed Ethernet. 

We stipulate that congestion control of Ethernet should maintain small oscillation of queue length for high throughput, low queuing delay and small packets loss ratio. Utilizing  in rate adjustment, ASM can stay at the congestion boundary line and straightforwardly moves along it to the stable point regardless of link speed. Employing sliding mode motion, ASM decouples the response time and the smoothness of convergence process, and accordingly accelerate the convergence process without enlarging the amplitude of oscillations. Moreover, the impacts of the delay on ASM is equal to the combination of the impacts of parameters shift, boundary line drift and a bounded disturbance on the rate adjustment result, all of which are tolerable. Experiments and simulations confirm the good properties of ASM and show that ASM outperforms QCN. 

We emphasize that analyzing congestion control algorithm on the Q/R plane, utilizing the derivative of queue length in rate adjustment and enforcing congestion control system moving along the congestion boundary line to stable point are also helpful to scale other congestion control algorithms to ultra-high speed networks.




\bibliographystyle{abbrv}
\bibliography{sigproc}  



\appendix
\section{Impacts of Delay on QCN}
\newtheorem{theorem}{Proposition}
\begin{theorem}
When the delay  satisfies 

where , 
the QCN system is unstable
\label{Proposition1}
\end{theorem}
\begin{proof}
As shown in , the characteristic equation of the differential equations describing QCN system is  where

and , , , , , , , , ,  and . Parameter , ,  and  stand for the link capacity, the number of senders, the sampling probability and the delay, respectively. Define , , and , there is . Let  represents the 0-dB crossover frequency, there is  and , which is equivalent to 

Thus, there is

Solving inequality , there is

and accordingly 

Therefore, according to the Bode stability criterion~\cite{stability}, QCN is unstable.
\end{proof}
With the recommended parameters in~\cite{average}, the lower boundary of the delay in Eq.  is approximately  for  Ethernet link, and  for  Ethernet link. In other words, delay large than  would result in unstable QCN system when the speed of Ethernet increases to . Therefore, the delay should be taken more seriously in designing CN scheme.



\section{Fluid-Flow Model of ASM}
The rate adjustment algorithm  of ASM can be modeled by differential equation

Focus on the bottleneck link and assume that all  sources are identical, i.e.,  and

For the simplification of expression, we make linear transformation

Combining Eq. (\ref{rate}), (\ref{queue-n}), (\ref{transform}) and (\ref{deltaq}), we can obtain the fluid flow model of the ASM system 

Obviously, Eq.  can also be written as the form of Eq. .

\section{Trajectories of ASM}
Referring to Eq.  and (\ref{deltaq}), there are 

And referring to Eq.  and (\ref{fluid-flow}), there are

when . Substituting Eq.  into Eq. (\ref{feedback-dirative}), we know that  is equivalent to Eq. .

On the other hand, the roots of the characteristic equation of differential equations  is

Referring to inequality  and~\cite{Itkis}, we can know that  are complex with negative real parts when  and the trajectory of ASM is spiral in regions \Rmnum{1}, \Rmnum{2}, \Rmnum{3} and \Rmnum{4}, as the red line shown in . When ,  are two distinct negative real number and the trajectory of ASM is parabola with asymptotic line  and , which denote lines 

and

respectively, as the blue line shown in .

\section{Impacts of Delay on ASM}
\begin{theorem}
The impacts of the delay are equivalent to the impacts of the combination of a drifted parameters, a drifted boundary line  and an external disturbance  on the rate adjustment result, all of which are tolerable by ASM.
\label{Proposition2}
\end{theorem}
\begin{proof}
The proof of this proposition is composed by three parts. The first part is preliminary. The second part is mainly the changes of the fluid flow model of ASM, which deconstructs the impacts of delay. Additional result of the second part is the explanation why the impacts of drifted parameters and drifted boundary line are tolerable by ASM. The third part explain why the external disturbance  on the rate adjustment result is tolerable in ASM.

\textbf{Part I}
Assume the delay is  and define , , , , , , , ,  and . There is

and

From inequalities  and , we have

Therefore, the boundary of  can be deduced as follows.

where  and . With the same method, we can obtain , where .

\textbf{Part II}
The ideal fluid flow model of ASM  can be rewritten as

where  denotes the sign of . When the delay  is considered, the boundary line of ASM becomes

and the fluid flow model of ASM becomes

Define

there is

Accordingly, the fluid flow model  becomes

Define , there are

Comparing Eq. (\ref{adjust-basic}) with Eq. (\ref{algorithm1}), we can know that the impacts of the delay is equivalent to the impacts of both disturbance  and that parameters ,  and  are changed to be ,  and , respectively. As a step further, the change from both  and  to both  and  is equivalent to that parameters  and  are changed to  and  respectively, where

And the largest amplitude of parameters drift is 

According to~\cite{Itkis}, the drifted boundary line changes the sliding mode motion of ASM to be quasi-sliding mode motion, which has the same characteristic as the ideal sliding mode motion. Namely, ASM are still insensitive to the parameters drift of rate adjustment rules and can slide to the stable point in the quasi-sliding region. Moreover, the parameters drift can also be compensated through proper parameters setting.


\textbf{Part III}
When  is taken into consideration, the fluid-flow model of the ASM system becomes Eq.  and the existence of the sliding mode motion is guaranteed by inequality , which is equivalent to

Define region  as , where the width  is  

\end{proof}


\end{document}
