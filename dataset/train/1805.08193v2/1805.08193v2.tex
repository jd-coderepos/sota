\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{nips_2018}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    


\usepackage[table]{xcolor}
\definecolor{mydarkred}{rgb}{0.6,0,0}
\definecolor{mydarkgreen}{rgb}{0,0.6,0}
\usepackage[colorlinks,
linkcolor=mydarkred,
citecolor=mydarkgreen]{hyperref}
\usepackage{url}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[algo2e,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{multicol}

\usepackage{caption}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{url}
\newtheorem{remark}{Remark}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\title{Masking: A New Perspective of Noisy Supervision}



\author{
  Bo Han\thanks{The first two authors (Bo Han and Jiangchao Yao) made equal contributions. The implementation is available at https://github.com/bhanML/Masking.} , Jiangchao Yao, Gang Niu, Mingyuan Zhou,\\
  \textbf{Ivor W. Tsang, Ya Zhang, Masashi Sugiyama}\\label{eq:objective}
\begin{split}
\ln{P(\tilde{y}|x)} \geq \mathbb{E}_{Q(s)} \left[ \underbrace{\ln{\sum_{y} P(\tilde{y}|y,s)P(y|x)}}_{\text{previous model}} - \ln{\left(Q(s_o)\slash\underbrace{P(s_o)}_{\text{structure prior}}\right)}\Bigg|_{s_o=f(s)} \right],\\
\end{split}
\label{eq:tsigmoid}
f(s) = \frac{1}{1+\exp(-\frac{s-\alpha}{\beta})}, \text{\quad where },
\label{eq:elbo}
\begin{split}
\ln{P(\tilde{y}|x)}
& = \ln{\int_s\sum_y P(\tilde{y}|y,s)P(y|x)P(s)ds} \\
& = \ln{\int_s\sum_y P(\tilde{y}|y,s)P(y|x)\frac{Q(s)P(s)}{Q(s)}ds} \\
& \geq \mathbb{E}_{Q(s)} \left[ \ln{\sum_y P(\tilde{y}|y,s)P(y|x)} - \ln{\frac{Q(s)}{P(s)}}\right] \\
& = \mathbb{E}_{Q(s)} \left[ \ln{\sum_y P(\tilde{y}|y,s)P(y|x)} - \ln{\frac{Q(s_o)\frac{ds_o}{ds}}{P(s_o)\frac{ds_o}{ds}}}\bigg|_{s_o = f(s)}\right] \\
& = \mathbb{E}_{Q(s)} \left[ \ln{\sum_y P(\tilde{y}|y,s)P(y|x)} - \ln{\left(Q(s_o)\slash P(s_o)\right)}\bigg|_{s_o = f(s)}\right], \\
\end{split}

where  is the mapping function to transform the noise transition matrix  into its structure .

\section{Network structures and training settings}\label{app:net}
As we have explained in the paper, our GAN-like structure consists of three modules, generator, discriminator and reconstructor. We respectively illustrate their network configuration as follows.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/generator.pdf}
\caption{The network configuration of the generator module.}
\label{fig:generator}
\end{center}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/discriminator.pdf}
\caption{The network configuration of the discriminator module.}
\label{fig:discriminator}
\end{center}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/reconstructor.pdf}
\caption{The network configuration of the reconstructor module.}
\label{fig:reconstructor}
\end{center}
\end{figure}
The learning rate is initialized as 0.1, and decreases with the operation  with the staircase in tensorflow. The corresponding decay factor is set . The learning rate for generator and discriminator is fixed with .

\end{document}
