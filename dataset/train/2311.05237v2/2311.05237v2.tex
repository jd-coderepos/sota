\documentclass{bmvc2k}

\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{url}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}
\captionsetup[table]{skip=10pt}
\captionsetup[figure]{skip=10pt}
\usepackage{bbding}
\usepackage{xcolor}
\usepackage{array}

\title{Widely Applicable Strong Baseline for Sports Ball Detection and Tracking}

\addauthor{Shuhei Tarashima}{tarashima@acm.org}{1,2}
\addauthor{Muhammad Abdul Haq}{muhabdulhaq@gmail.com}{2}
\addauthor{Yushan Wang}{yushanwang218@gmail.com}{2}
\addauthor{Norio Tagawa}{tagawa@tmu.ac.jp}{2}

\addinstitution{
 Innovation Center\\
 NTT Communications Corporation\\
 Tokyo, Japan
}
\addinstitution{
 Faculty of Systems Design\\
 Tokyo Metropolitan University\\
 Tokyo, Japan
}

\runninghead{Tarashima \etal}{Widely Applicable Strong Baseline for SBDT}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}
\def\ie{\emph{i.e}\bmvaOneDot}

\begin{document}

\maketitle

\begin{abstract}
In this work, we present a novel Sports Ball Detection and Tracking (SBDT) method that can be applied to various sports categories.
Our approach is composed of (1) high-resolution feature extraction, (2) position-aware model training, and (3) inference considering temporal consistency, all of which are put together as a new SBDT baseline.
Besides, to validate the wide-applicability of our approach, we compare our baseline with 6 state-of-the-art SBDT methods on 5 datasets from different sports categories.
We achieve this by newly introducing two SBDT datasets, providing new ball annotations for two datasets, and re-implementing all the methods to ease extensive comparison.
Experimental results demonstrate that our approach is substantially superior to existing methods on all the sports categories covered by the datasets.
We believe our proposed method can play as a Widely Applicable Strong Baseline (WASB) of SBDT, and our datasets and codebase will promote future SBDT research.
Datasets and codes are available at \url{https://github.com/nttcom/WASB-SBDT}.
\end{abstract}
\section{Introduction}
\label{sec:intro}
Sports ball trajectory depicted in Figure \ref{fig:task} is an important statistic for analytics of various sports such as
badminton \cite{wang+2022aaai},
baseball \cite{shum+2004icme},
basketball \cite{fu+2011vcip},
golf \cite{huang+2012icppw},
soccer \cite{theagarajan+cvpr2018,sarkar+2019cvprw},
tennis \cite{pingali+2000icpr},
table tennis \cite{desai+2005prmi},
and
volleyball \cite{cheng+2016icassp}.
Several commercial systems like Hawk-Eye\footnote{\url{https://www.hawkeyeinnovations.com/track}} and KINEXON\footnote{\url{https://kinexon.com/technology/ball-tracking/}} have already been successfully introduced to professional leagues, but they usually require high-cost installation.
Computer vision techniques can be an alternative approach to obtain ball trajectories from easily available video data.
However, this Sports Ball Detection and Tracking (SBDT) task is challenging due to the small size of a sports ball, its high speed, occlusion, blending in with surroundings, and camera motion \cite{yu+2003acmmm}.
\par
This SBDT task can uniformly be defined through various ball-games.
Therefore, {\it wide applicability} is an important property to be equipped by good SBDT methods.
However, while there are extensive literatures of SBDT methods proposed in the last two decades, most of them cannot be directly applied to different domains, since they are tailor-made for specific sports
(\eg,
badminton \cite{chen+2007tst},
baseball \cite{shum+2004icme},
basketball \cite{chen+2009jvcir,chakraborty+2011sic,chakraborty+2012icspcc,chakraborty+2013indicon,chakraborty+2013jo},
golf \cite{lyu+2015icia,lyu+2017ijsr},
soccer \cite{ohno+1999mfi,ohno+2000icpr,yu+2003acmmm,yu+2003icme,yu+2003icme2,choi+2004smvp,tong+2004icpr,yu+2004acmmm,choi+2005iciap,li+2005avss,liang+2005pcm,yu+2005icme,liu+2006ivc,ren+2006eccvw,shimawaki+2006icpr,yu+2006tmm,ishii+2007pcm,liang+2007tce,misu+2007icassp,yu+2007icme,yu+2007avc,ariki+2008icme,huang+2008icpr,pallavi+2008jvcir,ren+2008tcsvt,zhu+2008civr,beetz+2009ijcss,dorazio+2009avss,dorazio+2009tcsvt,dorazio+2009cviu,kim+2009cgiv,miura+2009cviu,ren+2009cviu,zhu+2009tmm,rao+2015iccsp},
tennis \cite{pingali+2000icpr,lepetit+2003cvpr,yu+2004icip,kittler+2005ia,yan+2005bmvc,kittler+2007iciap,yu+2007vcip,ekinci+2008vis,yan+2008tpami,conaire+2009icdsp,yu+2009cviu,teachabarikiti+2010icca,wong+2010ics,huang+2011avsp,almajai+2012dir,huang+2012apsipa,zhou+2013icassp,yan+2014cvs,archana+2015pcs,zhou+2015tmm,reno+2016tishw,yuan+2016scis},
table tennis \cite{zaveri+2004icme,desai+2005prmi,abed+2006acivs,chen+2006cesa,zhang+2010tim,zhang+2011zus,glover+2014icra,myint+2015mva},
volleyball \cite{chen+2007icassp,chen2012mta,cheng+2015pcm,cheng+2016icassp}).
Recent approaches \cite{komorowski+2019mva,komorowski+2020visapp,zandycke+2019mmsports,huang+2019avss,sun+2020icpai,liu+2022cvprw} based on Convolutional Neural Networks (CNNs) can potentially be used for different ball-games, but unfortunately in their works evaluations are limited to almost one sports category.
\par
Here we aim at building a new state-of-the-art (SOTA) SBDT method widely applicable to various sports categories.
To achieve this goal, we will make the following contributions:
\begin{itemize}
    \item While current SOTAs \cite{komorowski+2019mva,komorowski+2020visapp,zandycke+2019mmsports,huang+2019avss,sun+2020icpai,liu+2022cvprw} successfully solve the SBDT task on limited sports domains, we found that there is room for improvement with respect to (1) high-resolution feature extraction, (2) model training being aware of tiny ball position, and (3) inference which takes temporal consistency of ball position into account. We propose a series of solutions to ameliorate these drawbacks of existing methods, and put them together into a new SBDT approach.
    \item Different from the most SBDT works that evaluate their methods for almost one sports category, we use 5 datasets from different sports categories (\ie, badminton, basketball, soccer, tennis, volleyball) to compare our approach with 6 SOTA SBDT methods \cite{komorowski+2019mva,zandycke+2019mmsports,sun+2020icpai,liu+2022cvprw}. We establish this experimental protocol by introducing two novel datasets, providing new manual annotations for two datasets, and re-implementing all the existing methods. Experimental results demonstrate that our method substantially outperforms all the SBDT methods on all the datasets used in our evaluation.
\end{itemize}
\noindent
These contributions indicate that our proposed approach can play as a Widely Applicable Strong Baseline (WASB) of SBDT.
Also, we make datasets and codebases publicly available, which we believe promotes future SBDT research.
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth, page=2]{figures/teaser3.pdf}
\caption{Exemplar ball trajectories extracted from soccer, tennis, badminton, volleyball and basketball videos, respectively. Best viewed in color.}
\label{fig:task}
\vspace*{-5mm}
\end{figure*}
\section{Related Work}
\label{sec:related}
Roughly speaking, classical SBDT methods
\cite{lepetit+2003cvpr,yu+2003acmmm,yu+2003icme,yu+2003icme2,tong+2004icpr,yu+2004icip,zaveri+2004icme,desai+2005prmi,kittler+2005ia,liang+2005pcm,yan+2005bmvc,abed+2006acivs,chen+2006cesa,ren+2006eccvw,shimawaki+2006icpr,yu+2006tmm,chen+2007tst,chen+2007icassp,yu+2007icme,yu+2007avc,yu+2007vcip,ariki+2008icme,huang+2008icpr,pallavi+2008jvcir,yan+2008tpami,zhu+2008civr,beetz+2009ijcss,zhu+2009tmm,kim+2009cgiv,miura+2009cviu,wong+2010ics,teachabarikiti+2010icca,chakraborty+2011sic,chakraborty+2012icspcc,chakraborty+2013indicon,chakraborty+2013jo,zhou+2013icassp,yan+2014cvs,archana+2015pcs,lyu+2015icia,zhou+2015tmm,lyu+2017ijsr}
are based on {\it tracking-by-detection} paradigm:
Ball candidates are first detected from each video frame, then a true trajectory is recovered by associating the candidates through time.
The most typical ball candidate detector is a temporal background subtraction.
However, this approach can easily be contaminated by non-ball moving objects like players, even though it requires careful tuning to the target domain.
\par
Recent methods \cite{reno+2018cvprw,kamble+2018oer,voeikov+2020cvprw,komorowski+2020visapp,komorowski+2019visapp,komorowski+2019mva,zandycke+2019mmsports,wu+2020iet,ghasemzadeh+2021bmvc,huang+2019avss,sun+2020icpai,liu+2022cvprw} significantly ameliorate the above issue by employing encoder-decoder CNN models.
For example, DeepBall \cite{komorowski+2019visapp,komorowski+2019mva} is composed of a variant of fully convolutional networks \cite{liu+2016eccv}, in which intermediate multi-scale features are fused in a decoder to extract high-resolution heatmaps representing ball positions.
BallSeg \cite{zandycke+2019mmsports} is a modification of ICNet \cite{zhao+2018eccv}, so that two consecutive frames can be fed into the model to capture ball dynamics.
TrackNet and its variants \cite{huang+2019avss,sun+2020icpai,liu+2022cvprw} are based on U-Net \cite{ronneberger+2015miccai} architecture, following a multiple-in multiple-out (MIMO) design to efficiently capture ball movement.
Usually, training these models inevitably confronts high foreground-background class imbalance, due to the small ball size appeared in sports videos.
Existing methods address this issue by adapting the focal loss \footnote{The WBCE loss proposed in \cite{sun+2020icpai} is equivalent to the focal loss.} \cite{lin+2017iccv},
the combo loss \cite{taghanaki+cmig} or hard negative mining technique \cite{liu+2016eccv}.
Notice that in these recent methods, ball dynamics are considered only within frames that are combined in the same batch.
\par
We argue that, in recent methods described above, there is room for improvement with respect to (1) high-resolution feature extraction, (2) model training being aware of tiny ball position, and (3) inference which takes temporal consistency of ball position into account.
In the next section, we introduce solutions to improve these potential drawbacks.
\section{Widely Applicable Strong Baseline (WASB)}
\label{sec:method}
Following the majority of the SBDT literature\footnote{Some exceptional works like \cite{zhang+2022arxiv} define a ball position as a bounding box.}, our goal is to detect a $(x,y)$-coordinate of ball location from each image in a given video clip.
Similar to the recent works \cite{komorowski+2019mva,komorowski+2019visapp,zandycke+2019mmsports,sun+2020icpai,huang+2019avss,liu+2022cvprw}, we solve this problem by training a neural network that predicts heatmaps representing ball positions in input images.
At inference time, ball positions are determined by post-processing the heatmaps.
In the followings we detail our model, training and inference, all of which are put together into our proposed Widely Applicable Strong Baseline (WASB) for SBDT.
\subsection{High-Resolution Feature Extraction Model}
\label{sec:method:cnn}
Here we build a model that can produce heatmaps of the same spatial resolution $H \times W$ with an input tensor.
Recent works \cite{komorowski+2019mva,komorowski+2019visapp,zandycke+2019mmsports,sun+2020icpai,huang+2019avss,liu+2022cvprw} demonstrate the importance of a high-resolution and semantically-rich feature representation to precisely detect tiny sports balls.
In their methods, heatmaps are generated by combining highly-semantic but low-resolution decoder outputs with intermediate features produced by encoders to complement their spatial resolution.
We argue that, however, this encoder-decoder architecture can be a drawback for SBDT, since features to be combined lack one of the two required perspectives.
\par
\begin{wrapfigure}{r}[0pt]{0.4\textwidth}
\vspace*{-5mm}
\centering
\includegraphics[width=0.4\textwidth, page=2]{figures/hrms.pdf}
\caption{High-Resolution Modules (HRMs) of our SBDT method.}
\label{fig:hrms}
\end{wrapfigure}
Based on this observation, in this work we propose to employ a CNN module that can produce semantically-rich representation without losing spatial resolution:
Specifically, we adopt a high-resolution feature extraction method proposed by a series of HRNet works \cite{wang+2021tpami,yu+2021cvpr}.
HRNet consists of a stem block and multi-stage high-resolution modules (HRMs), where in each new stage one high-to-low resolution convolution block is incrementally added.
The information across resolutions is exchanged repeatedly, which allows us to obtain a highly-semantic representation while keeping spatial resolution.
In this paper we instantiate our HRMs following the small HRNet design\footnote{\url{https://github.com/HRNet/HRNet-Image-Classification}} illustrated in Figure \ref{fig:hrms}:
There are 4 stages and each stage consists of parallel sequences of residual blocks \cite{he+2016cvpr} followed by a multi-resolution fusion.
\par
If we directly follow the HRNet \cite{wang+2021tpami,yu+2021cvpr}, the feature fed into HRMs is down-sized to one-fourth by the stem block ({\it cf.} Figure \ref{fig:arch} (a)).
To make the resolution of intermediate representations higher, we propose to remove strides from the stem block and feed a tensor with higher spatial resolution to the HRMs, which are illustrated in Figure \ref{fig:arch} (b) and (c).
Notice that computational complexity increases when strides are removed.
We specifically adopt the model shown in Figure \ref{fig:arch} (c) by default, since it achieves higher SBDT performance with reasonable sacrifice of inference efficiency ({\it cf.} \S \ref{sec:eval:ablation}).
\par
To capture temporal dynamics of fast-moving sports balls, we follow the MIMO design like \cite{sun+2020icpai,liu+2022cvprw}:
$N$ consecutive frames are concatenated along the channel dimension then the resulting $H \times W \times 3N$ tensor is fed into our model, which generates the corresponding $N$ heatmaps of the same spatial resolution with the input (\ie, $H \times W \times N$).
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth, page=2]{figures/arch3.pdf}
\caption{(a) In the original stem design of HRNet \cite{wang+2021tpami,yu+2021cvpr}, spatial resolution of an input is reduced to one-fourth to be fed into HRMs. Alternatively, we propose to remove strides from the stem so that the resolution of intermediate features to be higher, as shown in (b) and (c). $N$ is the number of frames. We use (c) by default based on the ablation result in \S \ref{sec:eval:ablation}.}
\label{fig:arch}
\vspace*{-5mm}
\end{figure*}
\subsection{Position-Aware Model Training}
\label{sec:method:train}
To train SBDT models, we need to prepare ground truth (GT) maps from 2D ball positions, then optimize the model parameters by minimizing a loss between model predictions and GT maps.
Given a GT ball position
$\mathbf{p}^{GT} \in \mathbb{R}^{2}$
in an image, existing methods \cite{komorowski+2019mva,komorowski+2019visapp,zandycke+2019mmsports,sun+2020icpai,huang+2019avss,liu+2022cvprw} generate a {\it binary} GT map $\mathbf{y}^{bin}$ based on the following Equation \ref{eq:hm1}:
\begin{wrapfigure}{r}[0pt]{0.5\textwidth}
\vspace*{-2mm}
\centering
\begin{tabular}{cc}
\begin{minipage}[t]{0.2\textwidth}
\centering
\includegraphics[width=0.9\textwidth, page=2]{figures/hm.pdf}
\end{minipage} &
\begin{minipage}[t]{0.2\textwidth}
\centering
\includegraphics[width=0.9\textwidth, page=3]{figures/hm.pdf}
\end{minipage}  \\
(a) & (b)
\end{tabular}
\caption{An exemplar (a) binary ground-truth (GT) map and (b) real-valued GT map.}
\label{fig:hm}
\vspace*{-3mm}
\end{wrapfigure}
\begin{equation}
y_{\mathbf{p}}^{bin} =
\begin{cases}
1 & \text{if} \;\;\; \| \mathbf{p} - \mathbf{p}^{GT} \| \leq d \\
0 & \text{otherwise},
\end{cases}
\label{eq:hm1}
\end{equation}
where $y_{\mathbf{p}}^{bin}$ is the value of the GT map at location $\mathbf{p} \in \mathbb{R}^{2}$ and $d$ is a distance threshold set differently between methods.
An exemplar binary GT map is illustrated in Figure \ref{fig:hm} (a).
The focal loss \cite{lin+2017iccv} or the combo loss \cite{taghanaki+cmig} is used to train models, all of which only supports binary maps as GT.
However, we argue that resulting prediction of existing methods tends to be less sensitive to the exact ball position, since the ball position is made obscure through the GT map generation process.
\par
To overcome this limitation, we propose a novel training scheme to make the resulting model more aware of the exact ball position.
Specifically, we first generate a {\it real-valued} GT map $\mathbf{y}^{real}$ based on the following Equation \ref{eq:hm}:
\begin{equation}
y_{\mathbf{p}}^{real} =
\begin{cases}
\min \Bigl( C \cdot \exp \bigl( - \frac{ \| \mathbf{p} - \mathbf{p}^{GT} \|^{2} }{ d^{2} } \bigr), 1 \Bigr) & \text{if} \;\;\; \| \mathbf{p} - \mathbf{p}^{GT} \| \leq d \\
0       & \text{otherwise},
\end{cases}
\label{eq:hm}
\end{equation}
where $y_{\mathbf{p}}^{real}$ is the value of the real-valued GT map at $\mathbf{p}$, while $C$ is determined so that the non-zero minimum value is set to a pre-defined value $c_{min}$.
We illustrate an exemplar real-valued GT map in Figure \ref{fig:hm} (b).
With this real-valued GT map, we optimize our model parameters by minimizing the following quality focal loss \cite{li+2020neurips,li+2020arxiv}:
\begin{equation}
L = \sum_{\mathbf{p}} \Bigl[ - | y_{\mathbf{p}} - \sigma_{\mathbf{p}} |^{\beta} \Bigl\{ ( 1-y_{\mathbf{p}} ) \log  ( 1-\sigma_{\mathbf{p}} ) + y_{\mathbf{p}} \log \sigma_{\mathbf{p}} \Bigr\} \Bigr].
\label{eq:loss}
\end{equation}
$\sigma_{\mathbf{p}}$ is the sigmoid output of the model prediction at $\mathbf{p}$ and $\beta$ is a parameter to control the down-weighting rate.
Equation \ref{eq:loss} is equivalent to the focal loss \cite{lin+2017iccv} if GT is binary.
\par
\vspace{1mm}
\begin{wrapfigure}{r}[0pt]{0.4\textwidth}
\vspace*{-4mm}
\centering
\includegraphics[width=0.4\textwidth, page=2]{figures/mining.pdf}
\caption{Exemplar hard-to-localize samples found in our HLSM. In (c) and (e), a green circle represents a GT while a red one is a prediction.}
\label{fig:mining}
\end{wrapfigure}
\noindent \textbf{Hard-to-Localize Sample Mining (HLSM)}.
We empirically found that applying this position-aware GT map generation to {\it all} the training data does not statistically improve the SBDT performance.
Alternatively, we propose to apply the real-valued GT map generation scheme only to {\it hard-to-localize} samples through mining such hard examples during training.
The procedure is very simple:
After each pre-defined epoch,
we perform inference ({\it cf.} \S \ref{sec:method:infer}) with the latest model parameters over all the training sequences to find images in which predicted ball positions are far from GT positions.
For all the found images, GT are generated with Equation \ref{eq:hm}, then the model is further tuned in remaining epochs.
We show 3 hard-to-localize examples found in the above mining process in Figure \ref{fig:mining} (a).
Since their background is noisy, our model trained with {\it binary} GT maps yields blurry heatmaps as shown in (b), leading to incorrect localization or miss detection like (c).
However, through further training with {\it real-valued} GT maps, our model is able to generate clearer heatmaps as illustrated in (d), which results in more precise localization as shown in (e).
\subsection{Inference}
\label{sec:method:infer}
We first describe a baseline inference algorithm.
Given a video clip that consists of $T$ images, $N$ consecutive images are sampled in order with no overlaps (\ie, sampling step size is set to $N$), and they are preprocessed into a tensor which is fed into our trained model to produce $N$ heatmaps.
Each heatmap is binarized with a threshold $0.5$ to find connected components (\ie, blobs), and for each blob a candidate 2D ball position is estimated with its confidence.
In this baseline the ball position is computed as a geometric center and the confidence is defined as a blob size.
A ball position with the highest confidence is chosen as an inference result for each image,
while a ball is not detected if there is no blob found.
In the followings we introduce 3 simple techniques to improve this baseline inference:
\par
\vspace{1mm}
\noindent \textbf{Ball Position as a Center of Heatmap (CoH).}
We found that heatmap values in a blob can be clues to precisely estimate a ball position.
We propose to compute a ball position as the center of heatmap values, and define its confidence as a sum of heatmap values in the blob.
\par
\vspace{1mm}
\noindent \textbf{Online Tracking.}
Relying only on a detection confidence within an image could be error-prone, especially when ball-like objects appear.
We thus propose to introduce the idea of online tracking to take both detection confidence and temporal consistency into account.
Specifically, for image at $t+1$ we detect candidates using a generated heatmap, while we also predict the ball position from tracked ball positions in the previous frames.
Candidates farther from the predicted ball position than a threshold are filtered out, then a candidate with the highest confidence in the remaining candidates is selected as an inference result at $t+1$.
Following the local motion model \cite{zhou+2013icassp,zhou+2015tmm}, we compute a predicted ball position $\hat{\mathbf{p}}$ at $t+1$ as follows:
\begin{equation}
\hat{\mathbf{p}}_{t+1} = \mathbf{p}_{t} + \mathbf{v}_{t} + \frac{\mathbf{a}_{t}}{2}, \;\; \mathbf{v}_{t} = \mathbf{p}_{t} - \mathbf{p}_{t-1} + \mathbf{a}_{t}, \;\; \mathbf{a}_{t} = \mathbf{p}_{t} - 2 \mathbf{p}_{t-1} + \mathbf{p}_{t-2}.
\label{eq:pred}
\end{equation}
Notice that we exploit temporal information to just filter out inconsistent detection candidates:
Different from classical methods, we do not use filtering algorithms such as Kalman filter \cite{yu+2003icme,yu+2003icme2,yu+2003acmmm,yu+2004icip,zaveri+2004icme,kittler+2005ia,liang+2005pcm,chen+2006cesa,ren+2006eccvw,yu+2006tmm,yu+2007icme,yu+2007vcip,kim+2009cgiv,chakraborty+2013indicon} and particle filter \cite{yan+2005bmvc,abed+2006acivs,ariki+2008icme,huang+2008icpr,zhu+2008civr,beetz+2009ijcss,zhu+2009tmm}, since any performance improvement was not observed with them.
\par
\vspace{1mm}
\noindent \textbf{Oversampling.}
We also found that different MIMO sampling of the same image leads to produce diverse detection candidates.
In this work we propose to oversample the same image in different MIMO combinations, then use all the resulting candidates in the following selection step.
In \S \ref{sec:eval}, we report the results in case the step size is set to 1.
Notice that this technique may slow down inference, which is also investigated in our experiments.
\section{Dataset and Codebase}
\label{sec:dataset}
\subsection{SBDT Datasets}
\label{sec:dataset:dataset}
To evaluate the wide-applicability of SBDT algorithms, in this work we use 5 SBDT datasets from different sports categories, which are detailed in the followings.
Among them, \textcolor{blue}{Basketball} and \textcolor{blue}{Volleyball} are newly introduced datasets for SBDT, while the ground truths of \textcolor{blue}{Basketball} and \textcolor{blue}{Soccer} are newly annotated by us.
Statistics are summarized in Table \ref{tab:dataset}.
\par
\vspace{1mm}
\noindent \textbf{\textcolor{blue}{Soccer}} \cite{dorazio+2009avss}.
This dataset\footnote{\url{https://pspagnolo.jimdofree.com/download/}} was originally introduced for soccer ball and player tracking from six synchronized videos, and has been used in some SBDT works \cite{komorowski+2019visapp,komorowski+2020visapp,wang+2014cviu}.
Following \cite{komorowski+2019visapp,komorowski+2020visapp}, we use the first four video clips for training and the remaining two clips for testing.
However, we found that ball annotations provided in the original dataset are collapsed and do not localize ball position correctly.
Therefore, in this work we manually re-annotate ball position to all the frames and use the resulting annotation for training and testing.
\par
\vspace{1mm}
\noindent \textbf{Tennis} \cite{huang+2019avss}.
This dataset was introduced along with the TrackNet work \cite{huang+2019avss}, but was not used in its experiment.
Since there is no common usage for this dataset, we propose to use all the clips included in the first 7 games
as a training set, and the remainings as a testing set.
\par
\vspace{1mm}
\noindent \textbf{Badminton} \cite{sun+2020icpai}.
This dataset was introduced by the TrackNetV2 work \cite{sun+2020icpai}.
Following the dataset split defined by the authors, we use all the clips from 26 matches as a training set and the remaining 3 matches as a testing set.
\par
\noindent \textbf{\textcolor{blue}{Volleyball}}.
We introduce this dataset for the first time in the SBDT literature, by adapting video clips presented by \cite{Ibrahim+cvpr2016} and the corresponding ball annotations provided by \cite{perez+2022pr}.
We follow the manner of \cite{Ibrahim+cvpr2016} to split this dataset into training and testing sets.
Notice that in 3.7\% (178 / 4,830) of video clips any ball does not appear.
\par
\vspace{1mm}
\noindent \textbf{\textcolor{blue}{Basketball}}.
This dataset is also introduced for the first time in the SBDT literature.
We adapt the video clips provided by \cite{yan+2020eccv}, but there is no public ball annotations for this.
Therefore, we manually annotated ball positions to 45\% (81/181 games) of the whole video clips, resulting in 275,328 annotated images composed of 3,824 video clips.
Currently, this is the largest SBDT dataset.
Notice that the average ball displacement between consecutive frames is the largest among the five datasets
({\it cf.} Table \ref{tab:dataset}).
Also, camera frequently moves and zooms in rapidly to follow where play happens, which causes a complex ball trajectory in a video.
\begin{table}[t]
\centering
\scalebox{0.73}{
\begin{tabular}{@{}l|c|c|cccc|cccc@{}}
\toprule
                                                & &     & \multicolumn{4}{c|}{Train}                        & \multicolumn{4}{c}{Test}                \\
                                                & resolution & FPS & games & clips & frames & disp.[pixel]           & games & clips & frames & disp. \\ \midrule
\textcolor{blue}{Soccer} \cite{dorazio+2009avss} & $1920\times1080$ & 25 & 1 & 4 & 11994 & $10.4 \pm 10.0$ & 1 & 2 & 5999 & $15.7 \pm 13.0$ \\
Tennis \cite{huang+2019avss}                    & $1280\times720$ & 30  & 7        & 65       & 14160     & $15.3 \pm 13.0$ & 3        & 30       & 5675      & $13.6 \pm 10.2$ \\
Badminton \cite{sun+2020icpai}                  & $1280\times720$ & 30  & 26       & 172      & 78558     & $11.8 \pm 12.2$ & 3        & 29       & 12656     & $12.5 \pm 12.9$ \\
\textcolor{blue}{Volleyball} & $1280\times720$ & N/A & 39 & 3493 & 143213 & $14.4 \pm 11.4$ & 16 & 1337 & 54817 & $15.1 \pm 11.5$ \\
\textcolor{blue}{Basketball} & $1920\times1080$ & N/A & 70 & 3392 & 244224 & $33.7 \pm 21.8$ & 11 & 432 & 31104 & $33.9 \pm 21.4$ \\
\bottomrule
\end{tabular}
}
\caption{Summary of 5 SBDT datasets used in our evaluation. Among them, \textcolor{blue}{Volleyball} and \textcolor{blue}{Basketball} are newly introduced in this work. Also, for \textcolor{blue}{Soccer} and \textcolor{blue}{Basketball} we provide novel frame-wise manual annotations of 2D ball position. In this table, ``resolution'' represents the majority of image resolution in the dataset and ``disp.'' represents the average ball displacement in pixel between consecutive frames. Notice that frame per second (FPS) of \textcolor{blue}{Volleyball} and \textcolor{blue}{Basketball} are unknown (\ie, N/A), since they are not provided by adapted image sequences.}
\label{tab:dataset}
\end{table}
\subsection{Codebase of Existing SBDT Methods}
\label{sec:eval:baseline}
Most existing SBDT implementations have not been made public.
While a few exceptions exist\footnote{\url{https://nol.cs.nctu.edu.tw:234/open-source/TrackNetv2}}\footnote{\url{https://nol.cs.nctu.edu.tw:234/open-source/TrackNet}}, unfortunately they are strongly tied up with particular datasets, thus difficult to be applied to others.
Therefore, here we re-implement state-of-the-art SBDT methods to perform comparison on various SBDT datasets.
In particular, we implemented \textbf{DeepBall} \cite{komorowski+2019mva,komorowski+2020visapp}, \textbf{BallSeg} \cite{zandycke+2019mmsports}, \textbf{TrackNetV2} \cite{sun+2020icpai} and \textbf{MonoTrack} \cite{liu+2022cvprw}.
For DeepBall, since its original model is very small ($< 0.1$M parameters), we built a variant by simply increasing intermediate feature dimension, which is called \textbf{DeepBall-Large} in the followings.
Also, we deployed an unpublished variant\footnote{\url{https://github.com/Chang-Chia-Chi/TrackNet-Badminton-Tracking-tensorflow2}} of TrackNetV2, where residual connection and transposed convolution are additionally employed.
We call this variant as \textbf{ResTrackNetV2}.
\par
Notice that while we basically followed the settings proposed by authors, for some methods minor modifications were made for performance improvement.
We provide these implementation details in Appendix \ref{appendix:imple}.
\par
We report the performances of our SOTA re-implementations in Table \ref{tab:bench}.
It shows that the accuracy of our TrackNetV2 \cite{sun+2020icpai} implementation on the Badminton dataset is 85.6, while Table IV in \cite{sun+2020icpai} shows that the original implementation scores 85.2, which indicates the correctness (or, superiority) of our TrackNetV2 implementation.
Unfortunately, such a validation cannot be performed for the remaining five methods:
The original DeepBall \cite{komorowski+2019visapp} was evaluated on the Soccer dataset, but its original annotation is collapsed ({\it cf}. \S \ref{sec:dataset:dataset}), which makes the validation intractable.
For BallSeg \cite{zandycke+2019mmsports}, neither its specific architecture is presented nor the benchmark is publicly available.
The MonoTrack paper \cite{liu+2022cvprw} does not explain their experimental protocol at all, and the remaining two (DeepBall-Large and ResTrackNet) are simple extensions of existing methods proposed by us, which have no reference implementations.
\begin{table*}[t]
\scalebox{0.49}{
\centering
\begin{tabular}{@{}lc||cccc|cccc|cccc|cccc|cccc@{}}
\toprule
            &           & \multicolumn{4}{c|}{Soccer} & \multicolumn{4}{c|}{Tennis} & \multicolumn{4}{c|}{Badminton} & \multicolumn{4}{c|}{Volleyball} & \multicolumn{4}{c}{Basketball} \\
            & \# param. & F1 $\uparrow$ & Acc. $\uparrow$ & AP $\uparrow$ & FPS $\uparrow$ & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS \\ \midrule
DeepBall \cite{komorowski+2019visapp,komorowski+2019mva} & 0.1M & 44.5 & 92.7 & 26.3 & 44.6 & 47.4 & 32.3 & 47.0 & 52.1 & 52.4 & 38.6 & 60.0 & 57.1 & 64.4 & 50.7 & 49.2 & \textcolor{teal}{21.1} & 0.0 & 12.9 & 0.0 & 30.3 \\
DeepBall-Large & 1.0M & 44.9 & 89.5 & 34.0 & 42.0 & 46.7 & 31.6 & 35.1 & 47.7 & 50.6 & 36.8 & 59.5 & 53.0 & 70.4 & 57.5 & 56.5 & \textcolor{teal}{21.1} & 57.2 & 47.5 & 36.6 & 30.9 \\
BallSeg \cite{zandycke+2019mmsports} & 12.7M & 36.1 & 92.6 & 20.0 & \textcolor{teal}{64.8} & 71.7 & 57.5 & 56.8 & \textcolor{teal}{62.7} & 79.9 & 72.2 & 68.4 & 75.0 & 19.5 & 17.5 & 8.5 & 18.2 & 16.8 & 20.5 & 5.3 & 29.5 \\
TrackNetV2 \cite{sun+2020icpai} & 11.3M & \textcolor{blue}{86.6} & \textcolor{teal}{97.7} & 77.2 & \textcolor{red}{66.0} & 89.4 & 81.4 & 80.6 & 55.3 & 90.5 & 85.6 & 83.6 & \textcolor{red}{77.0} & 83.6 & 73.8 & 72.3 & 17.6 & 78.8 & 69.3 & 64.6 & 28.0 \\
ResTrackNetV2 & 1.2M & 84.6 & 97.4 & 75.5 & 56.2 & 90.3 & 82.8 & 81.7 & 59.0 & 89.4 & 84.0 & 82.2 & 71.3 & 84.2 & 74.7 & \textcolor{blue}{74.7} & \textcolor{red}{28.6} & 77.9 & 68.2 & \textcolor{blue}{66.0} & \textcolor{red}{38.2} \\
MonoTrack \cite{liu+2022cvprw} & 2.9M & 85.2 & 97.4 & \textcolor{blue}{78.6} & 58.0 & \textcolor{blue}{92.1} & \textcolor{blue}{85.9} & \textcolor{blue}{87.3} & \textcolor{red}{64.1} & \textcolor{blue}{90.9} & \textcolor{blue}{85.9} & \textcolor{blue}{84.9} & \textcolor{teal}{75.5} & \textcolor{blue}{85.1} & \textcolor{blue}{75.9} & 72.1 & 19.7 & \textcolor{teal}{80.8} & \textcolor{teal}{71.3} & 65.3 & \textcolor{teal}{32.1} \\ \midrule
\rowcolor[gray]{0.9}
WASB (Ours, Step=3) & 1.5M & \textcolor{red}{88.3} & \textcolor{red}{97.9} & \textcolor{teal}{83.6} & 55.7 & \textcolor{teal}{94.0} & \textcolor{teal}{89.0} & \textcolor{teal}{91.0} & 58.2 & \textcolor{teal}{91.6} & \textcolor{teal}{87.0} & \textcolor{teal}{88.5} & 70.4 & \textcolor{teal}{86.5} & \textcolor{teal}{77.9} & \textcolor{teal}{79.9} & 18.0 & 80.6 & \textcolor{teal}{71.3} & \textcolor{teal}{71.5} & 30.2 \\
\rowcolor[gray]{0.9}
WASB (Ours, Step=1) & 1.5M & \textcolor{teal}{88.2} & \textcolor{red}{97.9} & \textcolor{red}{86.2} & 23.6 & \textcolor{red}{95.6} & \textcolor{red}{91.8} & \textcolor{red}{94.2} & 35.2 & \textcolor{red}{93.1} & \textcolor{red}{89.0} & \textcolor{red}{91.6} & 34.3 & \textcolor{red}{88.0} & \textcolor{red}{80.0} & \textcolor{red}{83.2} & 15.8 & \textcolor{red}{82.6} & \textcolor{red}{73.4} & \textcolor{red}{77.1} & 22.3 \\ \bottomrule
\end{tabular}
}
\caption{Benchmark results of SBDT methods on 5 SBDT datasets. We set the distance threshold $\tau=4$ [pixel] to compute F1, Accuracy (Acc.) and Average Precision (AP), all of which are shown as percentages. Red values are the best while green values are the second-best among all the methods. Blue values are the best in existing methods.}
\label{tab:bench}
\end{table*}
\section{EVALUATION}
\label{sec:eval}
Here we report quantitative evaluations of our proposed method, WASB, using the datasets and codebases established in \S \ref{sec:dataset}.
Qualitative results are presented in Appendix \ref{appendix:result}.
\subsection{Evaluation Metrics}
\label{sec:eval:metric}
We evaluate SBDT models using F1, Accuracy (Acc.) and Average Precision (AP).
With a distance threshold $\tau$ [pixel], for each frame we calculate the distance between a predicted ball position and a ground truth to classify the prediction into true positive, true negative, false positive or false negative.
F1 and Acc. can be directly computed with the results, while AP is computed over all the positive results with prediction confidences.
\subsection{Implementation Details}
\label{sec:eval:imple}
Following TrackNetV2 and its variants \cite{sun+2020icpai,liu+2022cvprw}, $N$ ({\it cf}. \S \ref{sec:method:cnn}) is set to 3 and each image is resized to $288 \times 512$ to be fed into our model.
We train our model from scratch with Adam optimizer \cite{kingma+2015iclr} for 30 epochs.
The batch size is set to 8 for both training and testing.
To generate GT maps, $d$ ({\it cf}. \S \ref{sec:method:train}) is set to 2.5 while $c_{min}$ is set to 0.7.
We run HLSM ({\it cf}. \S \ref{sec:method:train}) at the beginning of epoch 20, while we didn't observe performance improvement with more trials.
We performed all the following experiments on an Ubuntu server with 4 V100 GPUs.
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth, page=1]{figures/comparison.pdf}
\caption{F1 (first row), Accuracy (second row) and Average Precision (third row) of SBDT methods with different distance threshold $\tau$ [pixel] on 5 SBDT datasets.}
\label{fig:benchmark2}
\end{figure*}
\subsection{Main Results}
\label{sec:eval:comp}
Table \ref{tab:bench} shows the benchmark results of SBDT methods on our datasets, using the fixed distance threshold $\tau=4$ [pixel].
For our proposed WASB, we show the results where the step size is set to 3 (\ie, no oversampling) and 1 ({\it cf.} \S \ref{sec:method:infer}).
We can clearly see that WASB results dominate the best and the second-best SBDT performance over the most metrics in sports categories covered by our datasets.
Also, with respect to AP, our best models significantly outperform the best existing methods by 7.8 \textasciitilde 16.8 \%.
Notice that WASB is not the fastest among the methods.
However, it can still be processed over 30 FPS on 4 out of 5 datasets, which is reasonable efficiency for real-time inference.
\par
Figure \ref{fig:benchmark2} shows F1, Accuracy and AP scores of SBDT methods with different distance thresholds.
Interestingly, the performances of DeepBall \cite{komorowski+2019mva,komorowski+2020visapp} and BallSeg \cite{zandycke+2019mmsports} heavily depend on the dataset,
while TrackNetV2 \cite{sun+2020icpai}, ResTrackNetV2 and MonoTrack \cite{liu+2022cvprw} stably yield good results through the datasets.
Compared to these methods, WASB consistently achieves higher performance with most of the threshold settings on all the sports categories, which indicates the wide-applicability of our approach.
\subsection{Ablation Studies}
\label{sec:eval:ablation}
Table \ref{tab:ablation:model} shows the ablation results with respect to the model design discussed in \S \ref{sec:method:cnn}.
As expected, removing strides can contribute to improving the model performance through the datasets.
Also as anticipated, removing strides from the stem seems to slow down inference.
However, the actual impact is not so severe, and in some cases (\eg, volleyball) we do not observe the degradation of efficiency.
\par
Table \ref{tab:ablation:misc} represents the ablation results to evaluate the techniques introduced in \S \ref{sec:method:train} and \S \ref{sec:method:infer}.
We can see that each technique complementarily ameliorate the SBDT performance with a few exceptions (\eg, online tracking does not contribute on the Soccer and Badminton datasets).
Interestingly, {\it even without any techniques}, our method is superior to the best of existing methods ({\it cf.} first row in Table \ref{tab:ablation:misc}).
This indicates the superiority of our high-resolution feature extraction model to existing approaches.
\subsection{Limitation}
\label{sec:eval:limitation}
As with the most of existing SBDT methods, our method, WASB, assumes a ball-game video as an input, and predicts at most one ball location (\ie, a $(x,y)$-coordinate) for each frame.
Therefore, one apparent limitation is that WASB cannot be applied to sports in which multiple balls are used simultaneously
(\eg, billiards \cite{rea+2004ivr}).
Our method can be applied to videos both captured by fixed cameras and including camera motion, which is validated with our Basketball dataset ({\it cf}. \S \ref{sec:dataset:dataset}).
While there are no theoretical limitations with respect to frame resolution and frame rate, our validation is limited to standard frame resolutions (\eg, HD, FHD) and frame rates (\eg, 25-30 FPS).
\section{CONCLUSION}
\label{sec:conc}
In this paper we proposed a Widely Applicable Strong Baseline (WASB) for Sports Ball Detection and Tracking (SBDT).
Extensive experiments on 5 SBDT datasets from different sports categories demonstrate that our WASB achieves substantially better performance than 6 state-of-the-art (SOTA) SBDT methods on all the datasets.
We achieve this by introducing two novel SBDT datasets, providing two new manual annotations, and re-implementing all the SOTA methods.
In the future research, we explore to make our baseline more efficient while keeping its performance.
Extending SBDT datasets (\eg, dataset scale, sports category) is also an interesting research direction.
\begin{table*}[t]
\centering
\scalebox{0.51}{
\begin{tabular}{@{}lc||cccc|cccc|cccc|cccc|cccc@{}}
\toprule
            &           & \multicolumn{4}{c|}{Soccer} & \multicolumn{4}{c|}{Tennis} & \multicolumn{4}{c|}{Badminton} & \multicolumn{4}{c|}{Volleyball} & \multicolumn{4}{c}{Basketball} \\
            & \# param. & F1 $\uparrow$ & Acc. $\uparrow$ & AP $\uparrow$ & FPS $\uparrow$ & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS & F1 & Acc. & AP & FPS \\ \midrule
Figure \ref{fig:arch} (a) & 1.5M & 81.7 & 96.9 & 71.7 & 85.7 & 85.5 & 75.4 & 75.6 & 56.7 & 86.8 & 80.4 & 80.3 & 77.1 & 84.3 & 74.7 & 77.0 & 17.6 & 77.4 & 67.3 & 67.1 & 30.8  \\
Figure \ref{fig:arch} (b) & 1.5M & 86.4 & 97.6 & 79.0 & 76.7 & 91.9 & 85.4 & 86.7 & 60.3 & 90.5 & 85.5 & 86.2 & 76.2 & 85.0 & 75.8 & 77.2 & 17.9 & 80.4 & 71.0 & 71.4 & 28.7 \\
Figure \ref{fig:arch} (c) & 1.5M & 88.3 & 97.9 & 83.6 & 55.7 & 94.0 & 89.0 & 91.0 & 58.2 & 91.6 & 87.0 & 88.5 & 70.4 & 86.5 & 77.9 & 79.9 & 18.0 & 80.6 & 71.3 & 71.5 & 30.2 \\ \bottomrule
\end{tabular}
}
\caption{Ablations with respect to the model design ({\it cf.} \S \ref{sec:method:train}). Notice that in all the cases we do not adapt oversampling ({\it cf.} \S \ref{sec:method:infer}) for inference. }
\label{tab:ablation:model}
\end{table*}
\begin{table*}[t]
\centering
\scalebox{0.6}{
\begin{tabular}{@{}cccc||ccc|ccc|ccc@{}}
\toprule
 & & & & \multicolumn{3}{c|}{Soccer} & \multicolumn{3}{c|}{Tennis} & \multicolumn{3}{c}{Badminton} \\
HLSM (\S \ref{sec:method:train}) & CoH (\S \ref{sec:method:infer}) & Online Tracking (\S \ref{sec:method:infer}) & Step=1 (\S \ref{sec:method:infer}) & F1  $\uparrow$ & Acc. $\uparrow$       & AP $\uparrow$ & F1 & Acc. & AP & F1 & Acc. & AP \\ \midrule
\multicolumn{4}{c||}{( The best scores of existing methods ({\it cf.} Table \ref{tab:bench})) } & 85.2 & 97.7 & 78.6 & 92.1 & 85.9 & 87.3 & 90.9 & 85.9 & 84.9 \\ \midrule
           &            &             &            & 87.3 & 97.7 & 80.1 & 93.1 & 88.1 & 88.5 & 91.1 & 86.3 & 85.5 \\
\Checkmark &            &             &            & 87.8 & 97.8 & 81.1 & 93.7 & 88.6 & 89.4 & 91.4 & 86.6 & 86.2 \\
\Checkmark & \Checkmark &             &            & 88.3 & 97.9 & 83.6 & 93.9 & 88.8 & 90.8 & 91.6 & 87.0 & 88.5 \\
\Checkmark & \Checkmark & \Checkmark  &            & 88.3 & 97.9 & 83.6 & 94.0 & 89.0 & 91.0 & 91.6 & 87.0 & 88.5 \\
\Checkmark & \Checkmark & \Checkmark  & \Checkmark & 88.2 & 97.9 & 86.2 & 95.6 & 91.8 & 94.2 & 93.1 & 89.0 & 91.6 \\ \bottomrule
\end{tabular}
}
\caption{Ablation results with respect to our proposed training ({\it cf.} \S \ref{sec:method:train}) and inference ({\it cf.} \S \ref{sec:method:infer}) schemes on the Soccer, Tennis and Badminton datasets.}
\label{tab:ablation:misc}
\end{table*}

\appendix
\section{Details of Existing SBDT Methods}
\label{appendix:imple}
As is mentioned in \S 4.2, we re-implemented 6 state-of-the-art (SOTA) sports ball detection and tracking (SBDT) algorithms in our codebase, 4 of which have been proposed in the recent literature \cite{komorowski+2019mva,komorowski+2020visapp,zandycke+2019mmsports,sun+2020icpai,liu+2022cvprw} and the remaining 2 of which are their variants.
We basically followed the default implementation settings proposed by authors, meanwhile we found that their performance can be boosted by simple modifications.
In the following we describe the details of SOTA SBDT methods including modifications made by us.
\par
\vspace{1mm}
\noindent \textbf{DeepBall} \cite{komorowski+2019mva,komorowski+2020visapp}. This is a small convolutional neural network (CNN) that is originally proposed to detect a soccer ball.
Unfortunately, its official implementation has not been publicly available.
DeepBall takes a single frame to produce the heatmap representing ball position via aggregating multi-scale intermediate feature maps.
At inference time, a ball position is determined by simply detecting a peak from the heatmap.
Model training is performed by minimizing the pixel cross-entropy (CE) loss between model predictions and ground truth (GT) binary maps.
The GT binary map is produced by setting a true ball position and its nearest neighbours as foreground.
Adam optimizer \cite{kingma+2015iclr} is used to train the model, and hard negative mining \cite{liu+2016eccv} is employed to mitigate the effect of foreground-background class imbalance.
Notice that we directly followed the above settings for our re-implementation.
\par
\vspace{1mm}
\noindent \textbf{DeepBall-Large}.
Through the re-implementation of DeepBall, we found that the original model is too small ($< 0.1$M parameters) to be applied to other ball-game datasets ({\it cf.} Table 2 in our main body).
To increase the model capacity, we made the following two modifications to the original DeepBall model: (1) The depths of block \{1, 2, 3\} are increased from \{8, 16, 32\} to \{48, 96, 192\}, (2) a kernel size of the stem is set to 3.
Here we call the resulting variant of DeepBall as DeepBall-Large.
Its model training is the same with the original.
\par
\vspace{1mm}
\noindent \textbf{BallSeg} \cite{zandycke+2019mmsports}.
This is a variant of ICNet \cite{zhao+2018eccv} originally proposed to detect a basketball.
Its official implementation has not been publicly available.
BallSeg takes two consecutive frames by concatenating a frame of interest with its difference to another frame.
The model is trained using the Stochastic Gradient Descent (SGD) applied on the pixel-wise CE loss.
Since the specific ICNet architecture used to build BallSeg is not described in the original paper, we chose to adapt the smallest model provided in the official ICNet repository\footnote{\url{https://github.com/hszhao/ICNet}}.
Also, we found that model training is failed when the proposed loss and optimizer are used.
Instead, we employed the focal loss \cite{lin+2017iccv} and Adam optimizer \cite{kingma+2015iclr} to successfully train BallSeg, then evaluated the performance of resulting models in our experiments ({\it cf.} \S 4 in our manuscript).
\par
\vspace{1mm}
\noindent \textbf{TrackNetV2} \cite{sun+2020icpai}.
This is a UNet-based \cite{ronneberger+2015miccai} SBDT model originally proposed to detect a shuttlecock from badminton videos.
The authors proposed multiple-in multiple-out (MIMO) design to efficiently capture ball dynamics:
Three consecutive frames are concatenated along the channel dimension, then the resulting tensor is fed into the model that generates corresponding three heatmaps.
The model is trained using the Adadelta \cite{zeiler2012arxiv} optimizer applied on the the focal loss \cite{lin+2017iccv}.
Though its official implementation has been public\footnote{\url{https://nol.cs.nctu.edu.tw:234/open-source/TrackNetv2}}, unfortunately it is strongly tied up with the badminton dataset thus is difficult to adapt to other sports datasets.
Therefore, we re-implemented TrackNetV2 following the above settings while being applicable it to various sports datasets.
\par
\vspace{1mm}
\noindent \textbf{ResTrackNetV2}.
We found that there is a public SBDT repository\footnote{\url{https://github.com/Chang-Chia-Chi/TrackNet-Badminton-Tracking-tensorflow2}} that extends TrackNet \cite{huang+2019avss} by introducing residual connections \cite{he+2016cvpr}.
Based on this idea, we also added a residual connection to each encoder/decoder block in TrackNetV2 \cite{sun+2020icpai} to promote the model training.
Also, we decreased the channel dimension of encoder/decoder blocks, which results in almost one-tenth model parameters compared to the original TrackNetV2.
Here we call this variant as ResTrackNetV2.
We trained this model with the same manner with TrackNetV2.
\par
\vspace{1mm}
\noindent \textbf{MonoTrack} \cite{liu+2022cvprw} is another variant of TrackNetV2 \cite{sun+2020icpai}, which removes some convolution layers while adding skip connections.
One notable difference from TrackNetV2 is that they adopt the combo loss \cite{taghanaki+cmig} in model training.
Since its official implementation has not been publicly available, we also re-implemented this method following settings described in \cite{liu+2022cvprw}.
\section{Qualitative Results and Error Analysis}
\label{appendix:result}
Figure \ref{fig:qualitative} shows typical SBDT results of our proposed method, WASB ({\it cf.} \S 3 in our manuscript).
These results demonstrates that WASB correctly track balls from video clips of different sports categories.
Interestingly, we can see that sports balls can be tracked from video clips with very different viewpoints (\eg, (d) Volleyball), and also from video clips including fast camera motion (\eg, (e) Basketball).
\begin{figure}[htbp]
\begin{tabular}{c}
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=2]{figures/qualitative.pdf}
\end{minipage}
\\
(a) Soccer
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=3]{figures/qualitative.pdf}
\end{minipage}
\\
(b) Tennis
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=4]{figures/qualitative.pdf}
\end{minipage}
\\
(c) Badminton
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=5]{figures/qualitative.pdf}
\end{minipage}
\\
(d) Volleyball
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=6]{figures/qualitative.pdf}
\end{minipage}
\\
(e) Basketball
\end{tabular}
\caption{Exemplar qualitative results of our proposed method on each sports category in our dataset collection. A red circle represents a detection result while a light blue circle represents a ground truth ball position. The ball trajectory is overlaid on the first frame in each video clip. Best viewed in color.}
\label{fig:qualitative}
\end{figure}
\par
Figure \ref{fig:error} shows some error modes of our proposed method.
For example, the result (a) (\ie, Soccer) represents a false positive, while the result (e) (\ie, Basketball) shows a false negative.
We can see that in (a) the model detection is not precisely aligned due to the noisy background (\eg, player shoes), while in (e) a ball cannot be detected because it is blurry and ambiguous.
The results (b), (c) and (d) (\ie, Tennis, Badminton, Volleyball) also represent false positives.
Interestingly, however, in these examples model detections (red circles) seem to capture true ball positions (light blue) more correctly than manually annotated ground truths.
There results indicate a potential of WASB surpassing human ball localization performance.
\begin{figure}[htbp]
\begin{tabular}{c}
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=2]{figures/error.pdf}
\end{minipage}
\\
(a) Soccer
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=3]{figures/error.pdf}
\end{minipage}
\\
(b) Tennis
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=4]{figures/error.pdf}
\end{minipage}
\\
(c) Badminton
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=5]{figures/error.pdf}
\end{minipage}
\\
(d) Volleyball
\\
\begin{minipage}[t]{1.0\hsize}
\centering
\includegraphics[keepaspectratio, scale=0.25, page=6]{figures/error.pdf}
\end{minipage}
\\
(e) Basketball
\end{tabular}
\caption{Exemplar error modes of our proposed method. A red circle represents a detection result while a light blue circle represents a ground truth ball position. Results in the second column is the zoom of yellow rectangle areas in the first column, and the third column shows the corresponding heatmaps produced by our model. Best viewed in color.}
\label{fig:error}
\end{figure}

\newpage

\bibliography{refs}

\end{document}
