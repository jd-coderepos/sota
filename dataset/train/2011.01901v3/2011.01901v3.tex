\documentclass{bmvc2k}



\title{Learning visual representations for transfer learning by suppressing texture - Supplementary}

\addauthor{Shlok Mishra}{shlokm@umd.edu}{1}
\addauthor{Anshul Shah}{ashah95@jhu.edu}{2}
\addauthor{Ankan Bansal}{ankan@umd.edu}{1}
\addauthor{Janit Anjaria}{janit92@gmail.com}{1}
\addauthor{Jonghyun Choi}{jc@yonsei.ac.kr}{3}
\addauthor{Abhinav Shrivastava}{abhinav@cs.umd.edu}{1}
\addauthor{Abhishek Sharma}{abhisharayiya@gmail.com}{4}
\addauthor{David Jacobs}{djacobs@umiacs.umd.edu}{1}

\addinstitution{
 University of Maryland, College Park
}
\addinstitution{
Johns Hopkins University
}
\addinstitution{
Yonsei University
}
\addinstitution{
Axogyan AI
}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{rotating}
\usepackage{colortbl}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{balance}
\usepackage{combelow}
\usepackage{tabularx}
\definecolor{Gray}{gray}{0.85}
\definecolor{darkgreen}{rgb}{0.0, 0.8, 0.0}
\definecolor{Lightgray}{gray}{0.90} \newcolumntype{a}{>{\columncolor{Lightgray}}c}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[stable]{footmisc}

\usepackage{multirow}   
\usepackage{makecell}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,eqparbox,array}

\usepackage{xcolor,colortbl}


\usepackage{url}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\runninghead{Mishra et.al }{Improving transfer learning by suppressing texture}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}


\begin{document}
\maketitle


\section{Pix2Pix for suppressing texture}
In addition to using classical tools, we also apply recent Image-to-Image translation models like Pix2Pix \cite{isola2018imagetoimage} to suppress texture. We train pix2pix model to produce images that are similar in style to anisotropic diffusion. Pix2pix model also helps in capturing different variations of texture, e.g., the amount of smoothing,  according to the target task. The source domain for Pix2Pix model is normal ImageNet images, and the target domain is texture suppressed images. We only apply Pix2Pix model on Perona-Malik diffusion \cite{Perona1990ScaleSpaceAE}, which is our best performing anisotropic diffusion method. After adding Pix2Pix models and doing an end-to-end training for ImageNet classification, we are able to achieve better results and outperform Stylzied-ImageNet on \texttt{top-1} accuracy. We call this model  Pix2Pix-Anistropic model. For MoCoV2 we train using ten different anisotropic diffusion variations where the iterations are randomly picked between 10 to 20. Since we are already able to capture these variations and also due to the huge compute requirements and time to train SSL and Pix2Pix models, we only show these results in the supervised learning setting and not for MoCoV2. 
In Fig \ref{fig:pix2pix} we show our complete pipeline of training pix2pix model. We pretrain the pix2pix network on ImageNet dataset for 8 epochs and use this network for the end to end classification.

\begin{figure}[h]
\includegraphics[width=\linewidth]{images/pix2pix.pdf}
\caption{The figure shows pipeline for training Pix2Pix model with the classification loss. The input to the Pix2Pix network is normal ImageNet image and output is Anisotropic image. Both of these images are used further for the classification task. }
\label{fig:pix2pix}
\end{figure}

\section{Results on Medical imaging task:}
We also show  improved results on CheXpert and Chest14 datasets in Table \ref{2D} (last row). We see that our approach leads to an improvement on this challenging task. This shows the general nature of our approach and its applicability to medical imaging.


\begin{table*}[!b]
	\centering
	\caption{2D tasks: pretraining using Chest14 or CheXpert. We build on top of \cite{Zhou2021PreservationalLI} and add anistropic diffused images in the same setup as \cite{Zhou2021PreservationalLI}. We see improved performances for both CheXpert and Chest14 datasets.}
	{
    \resizebox{0.8\linewidth}{!}{ 
	\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
	\toprule
	\multirow{}{Method} & \multicolumn{5}{c|}{Chest14$\rightarrow$Chest14} & \multicolumn{7}{c}{CheXpert$\rightarrow$Chest14} \\ \cline{2-13}
	& 9.5:0.5 & 9:1 & 8:2 & 7:3 & 6:4 & 10\% & 20\% & 30\% & 40\% & 50\% & 60\% & 100\% \\
	\hline
	TS & 61.8 & 68.1 & 71.5 & 73.4 & 75.4 & 68.1 & 71.5 & 73.4 & 75.4 & 77.5 & 79.1  & 80.9\\
	\hline
	IN & 70.5 & 73.6 & 75.3 & 76.9 & 78.0 & 73.5 & 76.3 & 78.4 & 79.0 & 79.5 & 79.7 & 81.0 \\
	\hline
	MG & 66.4 & 70.0 & 73.9 & 76.1 & 77.3 & 70.1 & 73.9 & 75.5 & 76.5 & 77.6 & 79.3 & 80.8 \\
	\hline
	SG & 66.5 & 70.2 & 74.3 & 76.7 & 77.6 & 69.7 & 73.8 & 75.6 & 77.3 & 77.3 & 79.6 & 81.3 \\
	\hline
	C2L & 71.7 & 74.1 & 76.4 & 77.5 & 79.0 & 73.1 & 77.0 & 78.5 & 79.1 & 79.8 & 80.2 & 81.5 \\
	\hline
	PCRL & {74.1} & {76.2} & {78.8} & {79.0} & {79.9} & {75.8} & {77.6} & {79.8} & {80.8} & {81.2} & {81.7} & {83.1} \\
	\hline
	PCRL + Anisotropic & \textbf{75.8} & \textbf{77.5} & \textbf{80.3} & \textbf{80.1} & \textbf{80.5} & \textbf{76.9} & \textbf{78.1} & \textbf{80.7} & \textbf{82.2} & \textbf{82.2} & \textbf{82.9} & \textbf{84.2} \\
	\bottomrule
	\end{tabular}}
	\label{2D}}	
\end{table*}

\section{Analysis Continued}
We now show few more experiments which show qualitatively and quantitatively how our model is less dependent on texture information.


\subsection{Visual Analysis:}
We now visually analyze the results by the saliency maps, which are produced by different networks. We use GradCam \cite{Selvaraju2016GradCAMVE} to calculate the saliency maps. In Fig \ref{fig:correct_incorrect}, we show the saliency maps produced by networks trained using the combined dataset and the original ImageNet dataset. 
We observe that Pix2Pix-Anisotropic model has saliency maps that spread out over a bigger area and that include the outlines of the objects. This suggests that it attends less to texture and more to overall holistic shape. In contrast, ImageNet trained models have narrower saliency maps that miss the overall shape and focus on localized regions.
In Fig.~\ref{fig:correct_incorrect}(a-e), we present the examples where the Pix2Pix-Anisotropic model gives the correct prediction, and the ImageNet model fails.
For example in Fig.~\ref{fig:correct_incorrect}(e), we observe that the network trained on ImageNet alone is not focusing on the whole bird and is only focusing on the body to make the decision; whereas the one trained with Pix2Pix-Anisotropic ImageNet is focusing on complete bird to make a decision. We include more saliency maps on Sketch-ImageNet, and cases where ImageNet trained models are correct and our model fails in the supplementary material.

 
\begin{figure*}[t!]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{cccccc}
\\
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Anisotropic\\Correct}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000061.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000293.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000317.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000329.jpeg}\\
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering ImageNet\\Incorrect}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000061.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000293.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000317.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000329.jpeg}
\\
&(a)&(b)&(c)&(d)&(e)\\
\\


\end{tabular}
}
\caption{Saliency maps using GradCam. The text on the left of the row indicates whether the Anisotropic model or ImageNet model was used. The figure shows the saliency maps where the Anisotropic model gave correct predictions, and the ImageNet model gave wrong predictions. The failure of the ImageNet model might be due to it not attending to the whole object.}
  \label{fig:correct_incorrect}
\end{figure*} 


\subsection{Results after removing high-frequency components:}
CNNs tend to exploit the high-frequency components in an image which makes them less robust \cite{wang2020high,ilyas2019adversarial}. In this section, we show that our proposed Pix2Pix-Anisotropic model is less dependent on high-frequency components. To remove high-frequency components from an image, we first obtain the image spectrum using DFT. 
We then incrementally vary the masked region's size to retain the more/less high frequency components with $r$ denoting the mask region's side.
We can see from  Figure \ref{fig:remove_high_frequency_component} as we remove the highest frequency components, our model performs better than the baseline by +2.1\%. The same difference in normal RGB images is 0.8\%. Hence this shows that our model has less reliance on the highest frequency components.
As we remove more high-frequency components, the error increases for both models, but we are still consistently better than the baseline.





















\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/sketch_imagenet.pdf}
\caption{Examples of images from Sketch-ImageNet. Images have very little or no texture, which implies texture will have little to no impact on object classification.}
\label{fig:sketch_images}
\end{figure*}




\begin{figure}
\includegraphics[width=\linewidth]{images/stylized_plot.png}
\caption{Training plots for the both Stylized ImageNet and Standard ImageNet. The plot shows that the model trained on Stylized ImageNet quickly overfits by finding shortcuts after around 6000 steps. Therefore, it gives poor performance on downstream tasks by relying on texture based shortcuts.
}
\begin{table}
\begin{center}
\caption{Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard ImageNet}
\begin{tabular}{ccc} 
\toprule
Method    & Entropy & Mean Highest probability \\
\midrule
Anistropic ImageNet     & 0.81 & 0.93   \\
Standard ImageNet   & 1.88 & 0.59   \\
\bottomrule
\end{tabular}
\label{tab: Confidence_Experiments}
\end{center}
\end{table}



\label{fig:logs_stylized}
\end{figure}


\begin{table*}[t!]
\footnotesize
\setlength\tabcolsep{0.1pt}

\begin{center}
\caption{We show additional experiments on dataset ImageNet-C\cite{Hendrycks2019BenchmarkingNN}, which evaluates model robustness to common corruptions. We can see that by focussing less on texture our model is consistently more robust than the baseline Resnet model pre-trained on ImageNet data.}
\label{tab:ImageNet_C}
\begin{tabular}{@{}l c |c c c c | c c c c | c c c  c | c c c c@{}}
\multicolumn{3}{c}{} & \multicolumn{3}{c}{Noise} & \multicolumn{4}{c}{Blur} & \multicolumn{4}{c}{Weather} & \multicolumn{4}{c}{Digital} \\
\cline{1-18}
Network  & \multicolumn{1}{c|}{\,\textbf{mCE}\,} & \scriptsize{Gauss.}
    & \scriptsize{Shot} & \scriptsize{Impulse} & \scriptsize{Defocus} & \scriptsize{Glass} & \scriptsize{Motion} & \scriptsize{Zoom} & \scriptsize{Snow} & \scriptsize{Frost} & \scriptsize{Fog} & \scriptsize{Bright} & \scriptsize{Contrast} & \scriptsize{Elastic} & \scriptsize{Pixel} & \scriptsize{JPEG}\\ \hline 
Baseline(ImageNet)    & 76.7 & 80 &	82 & 83 & 75 & 89 & 78&	80&	78&	75&	66&	57&	71&	85&	77 & 77\\
Anistropic(Ours)  & 74.85 &	72.70 &	76.65 &	78.27 & 73.73 &	87.97	& 75.99&	79.70 & 80.33  &	78.02  &	68.05  &	58.30  &	71.34  &	82.64  &	70.88  & 68 \\
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{center}
\end{table*}
\subsection{Confidence of Models:}
In this section we compare the confidence and entropy of Anistropic Model and ImageNet model when both the models have given correct predictions. To find confidence, we generate the probability scores of correct class. After this we calculated the mean of correct probability scores on both the models.  As we can see from Table \ref{tab: Confidence_Experiments} that Anistropic ImageNet has larger mean which means that Anistropic ImageNet has better confidence as compared to Standard ImageNet.
We also calculate the entropy of output probability distribution from both the models. We can see from Table \ref{tab: Confidence_Experiments} Anistropic ImageNet has lower entropy scores as compared to Standard ImageNet.

\subsection{Training with Stylized ImageNet}
Figure \ref{fig:logs_stylized} shows the training curves for both Stylized ImageNet and Standard ImageNet. We see that the model quickly saturates when using Stylized ImageNet. This leads to a low performance on downstream tasks. Our hypothesis is that the model rapidly learns to exploit some regularities in the texture introduced by the GANs to easily solve the MoCoV2 and Jigsaw tasks. This means that the self-supervised model has the tendency to take shortcuts in the presence of regular textures. The aim of our paper has been to investigate such short-coimings and provide appropriate solutions to the issues.



\subsection{Results on model robustness}
In this section, we show that suppressing texture not only helps in achieving better results but also makes our model more robust to common image corruptions.
 
 
\begin{figure}
\centering
\includegraphics[ width=0.8\linewidth]{images/gold_combined_plotly_1.png}
\caption{Results on Label corruption task. The bottom two line corresponds to when the gold fraction is 0.05, and the top two lines correspond to the case when the gold fraction is 0.1.   We can see that our model consistently outperforms the baseline with a larger improvement upon increasing the corruption probability.
}
\label{fig:label_corruption}
\end{figure}

\begin{figure}[t]
    \centering
\includegraphics[width=0.8\linewidth]{images/high_frequency_plot.png}
\caption{Results after removing high-frequency components from images. We can see that our proposed anisotropic filtering leads to models that are less reliant on high-frequency components in an image. When we remove the highest frequency components from the image(far-right), out model, outperforms baseline by close to 2.1\%. As we remove more high-frequency components, both the model suffers, but our model consistently performs better.}
\label{fig:remove_high_frequency_component}
\end{figure}

 
\textbf{Label Corruption Task:}
We show results on the label corruption task in Fig \ref{fig:label_corruption}. In this task, \cite{hendrycks2019selfsupervised} only some fraction(gold fraction) of data is clean, and the label of the rest of the data is corrupted with some probability. The task is to use both of these datasets for supervised classification.
 We use CIFAR100 as our dataset, and we augment the CIFAR100 dataset with Anisotropic diffused images. We create a dataset double the size of the original dataset and use it for the task of Label Corruption \cite{hendrycks2019selfsupervised}. 
We can see from the results that we have consistent improvement over the baseline. With an increase in corruption probability, our results improve, even more, implying that focusing on higher-level features helps even more in extreme corruptions.

\textbf{Results on ImageNet-C:}
We show additional experiments on dataset ImageNet-C\cite{Hendrycks2019BenchmarkingNN}, which evaluates model robustness to common corruptions. ImageNet-C[1] dataset has a total of 15 corruptions, as shown in Table \ref{tab:ImageNet_C}. 
We improve(+1.85\%) upon the baseline Resnet model pre-trained on ImageNet data and perform especially well in the cases of noise corruptions since we train on anisotropic images, which removes high-frequency noise from images.




\section{Jigsaw Task}
In addition to MoCoV2 we also show results with Jigsaw pretext task. \subsection{Method}



The goal in Jigsaw is to infer correct ordering of the given regions from an image. Following \cite{Noroozi2016UnsupervisedLO}, the typical setting is to divide an image into nine non-overlapping square patches and randomly shuffle them. A CNN is trained to predict the original permutation of these patches in the image. Jigsaw++ \cite {Noroozi_2018_CVPR} extended this idea and replaced some patches with random patches. These patch based methods come with their own issues and there has been some recent effort to solve them. \cite {Mundhenk2017ImprovementsTC} describe an easy short-cut CNNs take that utilizes the chromatic aberration produced due to different wavelengths exiting the lens at different angles. The authors provided a solution to the problem of chromatic aberration by removing cues from the images and also allowing the color pattern to be partially preserved. They also address the problem of true spatial extent that network sees in patch based methods by yoking the patch jitter to create a random crop effect.

\subsection{Results using Jigsaw Pretraining}

The baseline model in the case of Jigsaw is pre-trained using the standard ImageNet dataset. Unlike \cite{Noroozi2016UnsupervisedLO}, we use ResNet18 as our backbone instead of Alexnet\cite{Krizhevsky2017ImageNetCW} to take advantage of deeper layers and capture better image representations. 
We obtained these results by building on top of a publicly available implementation\footnote{\href{https://github.com/bbrattoli/JigsawPuzzlePytorch}{https://github.com/bbrattoli/JigsawPuzzlePytorch}}.
Table \ref{tab:Stylzied_Experiments} shows results for Jigsaw models trained and tested on different datasets. We observe that the Jigsaw model trained on the Cartoon dataset outperforms the baseline methods by 2.52 mAP and Anisotropic ImageNet outperforms the baseline methods by 1.8 mAP on the PASCAL VOC image classification dataset.
On object detection Bilateral ImageNet outperforms the baseline Jigsaw model by 0.78 mAP. 
On semantic segmentation Anisotropic Imagenet outperforms the baseline Jigsaw models by 8.1 mAP. Traditionally semantic segmentation has been a difficult task for Self-Supervised methods \cite{Noroozi2016UnsupervisedLO,Caron2018DeepCF} and improvement of this order on semantic segmentation shows the effectiveness of removing texture.

We also show results on ImageNet classification as the downstream task. 
Due to its
large scale, it is usually infeasible to fine-tune the whole network for the final task every time.
Therefore, following prior work \cite{Caron2018DeepCF}, we only fine-tune a linear classifier. 
The inputs to this classifier are the features from a convolution layer in the network. Note that while fine-tuning for the final task, we keep the backbone frozen. Therefore, the performance of the linear classifier can
be seen as a direct representation of the quality of the features obtained from the CNN.  
We report
the results of this experiment on ImageNet in Table \ref{ref:Jigsaw_Imgagenet_Experiments}. 
Adding the Anisotropic ImageNet dataset to this model gives a further improvement of 0.5\%. 




\subsection{Jigsaw using Alexnet as backbone}
Our improvement when using Anisotropic ImageNet is not restricted to the backbone. Traditionally in Self-Supverised learning one of the most followed architectures is Alexnet \cite{Noroozi2016UnsupervisedLO, Doersch2015UnsupervisedVR, Caron2018DeepCF}. Following these methods, we also show results on Alexnet backbone.  
In Table. \ref{tab: Alexnet_Experiments} we show results on VOC Classification when using Alexnet as the backbone. We obtain an improvement of 0.67 mAP over the baseline. 


\subsection{Patch-wise Anisotropic Diffusion} 
In our best performing model, we considered all the patches for the jigsaw task to either come from the standard ImageNet or anisotropic diffusion filtered ImageNet. What if each of the 9 patches for the Jigsaw task could be either a standard patch or filtered patch?  For this experiment we randomly choose a patch from the standard dataset or the filtered dataset, with equal probability. This is a much more extreme form of data augmentation and considerably increases the difficulty of the task. We got an improvement of 0.6 mAP over the baseline model for the classification task. However this is 1.1 mAP lower then the doing Anistropic Diffusion on whole image. 
 

\begin{table*}
\begin{center}
\caption{Comparison of our approach with Jigsaw baseline methods. Using our best model, we improve 2.52 mAP in VOC classification , 0.78 mAP on VOC detection and 8.1 mAP on VOC semantic segmentation(SS) over the baseline models. Note that Stylized ImageNet performs poorly on VOC classification due to the visual shortcuts.}
\begin{tabular}{ccccc}  
\toprule
Method    & Dataset Size & VOC Cls. & VOC Det. & SS \\
\midrule
Baseline     & 1.2M & 74.82    & 61.98  & 27.1  \\
Stylized \cite{Geirhos2018ImageNettrainedCA}     & 1.2M & 13.81    & 28.13  &10.12   \\
Gaussian ImageNet & 2$\times$1.2M    & 75.49    & 62.39    &27.9  \\
Bilateral ImageNet & 2$\times$1.2M    & 74.55    & \textbf{62.74}    &28.9   \\
Only Anisotropic   & 1.2M & 74.52    & {61.85}  & 32.7     \\
Anisotropic ImageNet & 2$\times$1.2M    & {76.77}    & 61.59   &\textbf{35.2}    \\
Cartoon ImageNet & 2$\times$1.2M    & \textbf{77.34}    & 59.31   &  34.1\\
\bottomrule
\end{tabular}
\label{tab:Stylzied_Experiments}
\end{center}
\end{table*}
\begin{table*}[t!]
\caption{ImageNet classification by finetuning the last FC layer. Features from the conv layers are kept unchanged. This experiment helps evaluate the quality of features learnt by the convolutional layers.}
\vspace{-0.5em}
\begin{center}
{
\begin{tabular}{cccccc}  
\toprule
Method    & Dataset Size & VOC Cls & VOC Det. & ImageNet Cls. Acc \\
\midrule
Jigsaw Baseline     & 1.2M & 74.82    & 61.98  &26.17  \\
Jigsaw anisotropic   & 2$\times$1.2M& 76.77    & 61.59  & 26.67    \\
\bottomrule
\end{tabular}
\label{ref:Jigsaw_Imgagenet_Experiments}
}
\vspace{-1em}
\end{center}
\end{table*}
\begin{table*}
\begin{center}
\caption{Experiments with Alexnet as the backbone. Ideas of anisotropic diffusion filter can extend to other architectures like Alexnet. The Anistropic ImageNet model improves over the baseline by 0.67 mAP}
\begin{tabular}{cc} 
\toprule
Method    & VOC 2007 Classification \\
\midrule
Jigsaw Baseline(Our Implementation)     & 65.21    \\
Jigsaw anisotropic   & 65.88    \\
\bottomrule
\end{tabular}
\label{tab: Alexnet_Experiments}
\end{center}
\end{table*}









\section{Anistropic Images}
We show some more examples of Anistropic images obtained by applying Anisotropic diffusion filiters to images from ImageNet \cite{imagenet_cvpr09} in figures \ref{fig:example1} and \ref{fig:example2}.  Notice how the images lose texture information. This makes it more difficult for models to find shortcuts. This, in turn, leads to better semantic representations learned by the model which leads to higher performance on downstream tasks.




\section{Saliency Maps}
\subsection{Sketch-ImageNet Saliency Maps}
In Fig \ref{fig:sketch_saliency} we show some of saliency maps for Sketch-ImageNet images. We can see from saliency maps that Anistropic ImageNet has broader saliency map and has better coverage of the object as compared to ImageNet model. 
\subsection{Saliency Maps for Anistropic ImageNet and Standard ImageNet models}
We also show some addtional saliency maps in Figure \ref{fig:anis_correct}, Figure \ref{fig:both_correct}, Figure \ref{fig:both_wrong} and Figure \ref{fig:img_correct} corresponding to both the models. We can see from the figures that Anistropic ImageNet has in general diffused saliency maps.

\begin{table*}[t!]
\begin{center}
\caption{Results on Label corruption task. We can see that our model consistently outperforms the baseline with larger improvement upon increasing the corruption probability.}
\begin{tabular}{cccccc}  
\toprule
Corruption probability    & Gold Fraction & Baseline(Error) & Anistropic Model(Error) \\
\midrule
0.2 &	0.05 &	29.61 &\textbf{	28.75 } \\
0.2 &	0.1 &	28.31 &	\textbf{27.84} \\
0.4 &	0.05 &	31.92 &	\textbf{30.5} \\
0.4 &	0.1 &	32.59 &	\textbf{31.48} \\
0.6 &	0.05 &	39.04 &	\textbf{38.52} \\
0.6 &	0.1 &	36.75 &	\textbf{34.98} \\
0.8 &	0.05 &	54.23 &	\textbf{52.6} \\
0.8 &	0.1 &	44.31 &	\textbf{43} \\
1.0 &	0.05 &	75.05 &	\textbf{71.21} \\
1.0 &	0.1 &	51.19 &	\textbf{45.51} \\
\bottomrule
\end{tabular}
\label{tab:Label_Corruption}
\end{center}
\end{table*}

\section{Implementation details}
\paragraph{Training Details.}
With image classification as the downstream task, we train our network for 90,000 iterations with an
initial learning rate of 0.003 following \cite{Caron2018DeepCF}. 

For object detection we report our
results for Faster-RCNN \cite{Ren2015FasterRT} using our pre-trained model as backbone. 
We tune hyper-parameters using the validation set.  
For object detection, we follow the details of \cite{Ren2015FasterRT} to train a model; 10 epochs with an initial learning rate of 0.001.

For semantic segmentation, we report our results on FCN\cite{Shelhamer_2017} using our pretrained model as backbone. 
We train the FCN model for 30 epochs using an initial learning rate of 0.01.

\noindent\textbf{ImageNet.} 
We use ImageNet for all training and evaluation of image classification accuracy. For self-supervised learning, we follow \cite{Caron2018DeepCF,he2019momentum}; we train linear classifiers using features obtained from the final Residual block by freezing all convolutional layers. 
The performance of these linear classifiers is meant to evaluate the quality of the feature
representations learnt by the convolutional layers, since the backbone is completely frozen and only
fully-connected layers are being trained. 
We chose hyper-parameters using the validation set and report performance on the ImageNet validation set.

Note that since we use ImageNet to pre-train for self-supervised learning, there is no domain difference when we conduct inference on ImageNet, but with VOC there is.  
With the VOC results, we validate that the gain by our method is particularly large when there is domain shift.\\
\noindent\textbf{MoCo-v2 task.} For MoCo-v2 we use  higher lr of 0.3 as it gives better performance for our task.

\noindent\textbf{Jigsaw task.} 
In Jigsaw \cite{Noroozi2016UnsupervisedLO} the image is divided into 9 non-overlapping square patches. We select 1,000 from the 9! possible permutations. All of our primary experiments on Jigsaw use ResNet18 as the backbone \cite{He2015}. We train the Jigsaw task for 90 epochs, with an initial learning rate of 0.01. The learning rate is reduced by a factor of 0.1 after $(30,30,20,10)$ epochs. We use the same data augmentation as in \cite{Noroozi2016UnsupervisedLO}. In MoCo \cite{He2015}, we use ResNet50\cite{He2015} as the backbone, following the same procedure as mentioned in \cite{he2019momentum}.



\noindent\textbf{PASCAL VOC.} 
Following \cite{Caron2018DeepCF} and \cite{chen2020improved}, we evaluate image classification and object detection on the PASCAL VOC dataset \cite{Everingham2009ThePV}. 
It contains about 5,000 images in the train-val set belonging to 20 classes. 
Note that the image classification task is multi-label. Therefore, the metric used for evaluating both image classification and object detection is the mean Average Precision (mAP). \paragraph{Training Details for Object detection for COCO based metrics:}
We report object detection results on \cite{Ren2015FasterRT} C4 backbone which is finetuned end to end on VOC07+12 trainval dataset and evaluated on the VOC 07
test set using the COCO suite of metrics. 

\noindent\textbf{Other Details.}
We use 4 Nvidia GTX 1080 Ti for all experiments.
Pretraining on Jigsaw takes 3 days on the standard ImageNet dataset. The SGD optimizer with momentum was used for all experiments with momentum of 0.9 and weight decay of $5\times10^{-4}$. Cross-entropy loss was used for all experiments, mini-batch size was set to 256. Pretraining on MoCoV2 takes 6 days on 4 Nvidia P100 machines. We set all other hyperparamters following \cite{chen2020improved}.

\begin{table*}[t!]
\centering
\caption{Comparison between Stylized ImageNet and our Anisotropic ImageNet. Following \cite{Geirhos2018ImageNettrainedCA}, we use ResNet50 as our backbone. We finetune our models on only the ImageNet dataset. We can see that on ImageNet classification and object detection, Anisotropic ImageNet and Stylized ImageNet have very similar performance.}
\label{tab: Stylized_Classification_experiments}
\begin{tabular}{ccccc}  
\toprule
Method    & Finetune & Top-1 Accuracy & Top-5 Accuracy & OBJ Detection \\
\midrule
Stylized Imagenet     & - & 74.59    & 92.14 &  70.6   \\
Stylized Imagenet    & IN & 76.72    & 93.28 &  75.1  \\
Anisotropic Imagenet   & - & 68.38    & 87.19 & -\\
Anisotropic Imagenet   & IN & 76.71    & 93.26 &   74.27 \\

Cartoon Imagenet   & IN & 76.22    & 93.12 & 72.31   \\
\bottomrule
\end{tabular}
\end{table*}








\section{Label Corruption Task}
We show the full results of the label corruption task in Table \ref{tab:Label_Corruption}.

\begin{table*}
\begin{center}
\caption{Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard ImageNet}
\begin{tabular}{ccc} 
\toprule
Method    & Entropy & Mean Highest probability \\
\midrule
Anistropic ImageNet     & 0.81 & 0.93   \\
Standard ImageNet   & 1.88 & 0.59   \\
\bottomrule
\end{tabular}
\label{tab: Confidence_Experiments}
\end{center}
\end{table*}












\begin{figure*}[t!]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccccccc}
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Anisotropic}}}&
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Correct}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000262.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000994.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00001031.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00001114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000307.jpeg}\\
\rotatebox{90}{\parbox{0.15\linewidth}{\centering ImageNet}}&
\rotatebox{90}{\parbox{0.15\linewidth}{\centering Correct}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000262.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000994.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00001031.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00001114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000307.jpeg}
\\
&&(a)&(b)&(c)&(d)&(e)\\
\hline
\\
\\
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Anisotropic}}}&
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Correct}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000061.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000293.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000317.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_anis_model/ILSVRC2012_val_00000329.jpeg}\\
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering ImageNet}}}&
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Incorrect}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000061.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000114.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000293.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000317.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/anis_correct_img_model/ILSVRC2012_val_00000329.jpeg}
\\
&&(f)&(g)&(h)&(i)&(j)\\
\hline
\\
\\

{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Anisotropic}}}&
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Incorrect}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_anis_model/ILSVRC2012_val_00000265.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_anis_model/ILSVRC2012_val_00000427.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_anis_model/ILSVRC2012_val_00000456.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_anis_model/ILSVRC2012_val_00000828.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_anis_model/ILSVRC2012_val_00000869.jpeg}\\
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering ImageNet}}}&
{\rotatebox{90}{\parbox{0.15\linewidth}{\centering Correct}}}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_img_model/ILSVRC2012_val_00000265.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_img_model/ILSVRC2012_val_00000427.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_img_model/ILSVRC2012_val_00000456.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_img_model/ILSVRC2012_val_00000828.jpeg}&
\includegraphics[width=0.18\linewidth]{images/saliency_maps/imagenet_correct_img_model/ILSVRC2012_val_00000869.jpeg}
\\ 
&&(k)&(l)&(m)&(n)&(o)\\
\end{tabular}
}
\caption{Saliency maps on three different set of images. The text on the left of the row indicates whether Anisotropic model or ImageNet model was used. The first two rows show the saliency maps where both model gave correct predictions. We can see from saliency maps that the Anisotropic model has more diffused saliency maps. The second two rows show the saliency maps where Anisotropic model gave correct predictions and ImageNet model gave wrong predictions. The failure of ImageNet model might be due to it not attending to whole object. The last two rows show the saliency maps where Anisotropic model gives incorrect predictions and ImageNet model gives correct predictions. Even in this failure mode, the Anisotropic model gives diffused saliency maps.}
  \label{fig:correct_incorrect}
\end{figure*}



\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}
\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000262.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000994.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00001031.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00001114.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_anis_model/ILSVRC2012_val_00000307.jpeg}\\
\rotatebox{90}{Imagenet}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000262.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000994.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00001031.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00001114.jpeg}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/both_correct_img_model/ILSVRC2012_val_00000307.jpeg}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps when our technique gives the correct prediction and baseline approach gives incorrect label. The top row gives the saliency maps for our model and the bottom one shows the corresponding saliency maps for the model trained on imagenet alone. We can see from saliency maps that Anisotropic model has bigger saliency maps which might be the reason for the correct prediction.}
    \label{fig:correct_correct}

\end{figure*}






\section{Saliency Maps}
In Fig. \ref{fig:correct_incorrect} we show the saliency maps produced by networks trained using the combined dataset and the original ImageNet dataset. 
We use GradCam\cite{Selvaraju2016GradCAMVE} to calculate the saliency maps. We can see that Anisotropic ImageNet has saliency maps that spread out over a bigger area and that include the outlines of the objects.  This suggests that it attends less to texture and more to overall holistic shape.   In contrast, ImageNet trained models have narrower saliency maps that miss the overall shape and focus on localized regions, suggesting an attention to texture. In Fig. \ref{fig:correct_incorrect}(f-j) we show these for the case where the Anisotropic model gives the correct prediction and the ImageNet model fails.  For example in Fig. \ref{fig:correct_incorrect}(j), we see that the network trained on ImageNet alone is not focusing on the whole bird and is only focusing on the body to make the decision whereas the one trained with Anisotropic ImageNet is focusing on complete bird to make a decision.  We see a similar trend in the cases where both the models give the correct prediction (Fig. \ref{fig:correct_incorrect}(a-e)).  In the case where Anisotropic model makes incorrect predictions and ImageNet model (Fig. \ref{fig:correct_incorrect}(k-o)) is correct we see the saliency maps are still diffused, but we fail to capture the whole object leading to incorrect predictions. 



\begin{figure*}\centering
\centering
        \includegraphics[width==0.35\linewidth]{images/n01443537_24724.JPEG}
        \includegraphics[width=0.35\linewidth]{images/n01443537_24724_og.jpg}
\centering
        \includegraphics[width=0.35\linewidth]{images/n01443537_24724_og.jpg}
\\
\centering
        \includegraphics[width=0.35\linewidth]{images/n01532829_11283.JPEG}
\centering
        \includegraphics[width=0.35\linewidth]{images/n01532829_11283_og.jpg}
\\
\centering
        \includegraphics[width=0.35\linewidth,height=7cm]{images/n09835506_8865.JPEG}
\centering
        \includegraphics[width=0.35\linewidth,height=7cm]{images/n09835506_8865_og.jpg}
\caption{Original images (left)and images obtained after anisotropic diffusion (right). Most of the texture information in the images has been smoothed out by the filter while retaining the shape information. This forces the network to capture higher-level semantics without relying on low-level texture cues}
    \label{fig:example1}
    \end{figure*}
    


    
\begin{figure*}\centering
\centering
        \includegraphics[width=0.35\linewidth]{images/n02389026_16434.JPEG}
\centering
        \includegraphics[width=0.35\linewidth]{images/n02389026_16434_og.jpg}
\\
\centering
        \includegraphics[width=0.35\linewidth]{images/n02423022_11259.JPEG}
\centering
        \includegraphics[width=0.35\linewidth]{images/n02423022_11259_og.jpg}
\\
\centering
        \includegraphics[width=0.35\linewidth]{images/n02504013_1924.JPEG}
\centering
        \includegraphics[width=0.35\linewidth]{images/n02504013_1924_og.jpg}
\\
\centering
        \includegraphics[width=0.35\linewidth]{images/n03124043_9147.JPEG}
\centering
        \includegraphics[width=0.35\linewidth]{images/n03124043_9147_og.jpg}
\\
\caption{Original images (left)and images obtained after anisotropic diffusion (right). Most of the texture information in the images has been smoothed out by the filter while retaining the shape information. This forces the network to capture higher-level semantics without relying on low-level texture cues}
\label{fig:example2}\end{figure*}

\noindent







\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}
\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/anis_sktech_saliency_maps/n01440764sketch_5.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/anis_sktech_saliency_maps/n04487394sketch_13.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/anis_sktech_saliency_maps/n03041632sketch_50.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/anis_sktech_saliency_maps/n02488291sketch_19.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/anis_sktech_saliency_maps/n01440764sketch_3.JPEG}\\
\rotatebox{90}{ImageNet}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/imagenet_sktech_salinecy_maps/n01440764sketch_5.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/imagenet_sktech_salinecy_maps/n04487394sketch_13.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/imagenet_sktech_salinecy_maps/n03041632sketch_50.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/imagenet_sktech_salinecy_maps/n02488291sketch_19.JPEG}&
\includegraphics[width=0.15\linewidth]{images/saliency_maps/imagenet_sktech_salinecy_maps/n01440764sketch_3.JPEG}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps on few randomly selected images from Sketch-ImageNet. We can see from saliency maps that Anisotropic model has bigger saliency maps which might be the reason for the correct prediction.}
    \label{fig:sketch_saliency}

\end{figure*}



\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}
\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_anis_model/ILSVRC2012_val_00013035.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_anis_model/ILSVRC2012_val_00013182.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_anis_model/ILSVRC2012_val_00013151.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_anis_model/ILSVRC2012_val_00013149.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_anis_model/ILSVRC2012_val_00013227.JPEG}\\
\rotatebox{90}{ImageNet}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_img_model/ILSVRC2012_val_00013035.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_img_model/ILSVRC2012_val_00013182.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_img_model/ILSVRC2012_val_00013151.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_img_model/ILSVRC2012_val_00013149.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/anis_correct_img_model/ILSVRC2012_val_00013227.JPEG}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps when Anistropic Model had correct predictions and ImageNet model has wrong predictions.}
    \label{fig:anis_correct}

\end{figure*}


\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}
\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_anis_model/ILSVRC2012_val_00000242.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_anis_model/ILSVRC2012_val_00000228.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_anis_model/ILSVRC2012_val_00000028.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_anis_model/ILSVRC2012_val_00000087.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_anis_model/ILSVRC2012_val_00000310.JPEG}\\
\rotatebox{90}{ImageNet}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_img_model/ILSVRC2012_val_00000242.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_img_model/ILSVRC2012_val_00000228.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_img_model/ILSVRC2012_val_00000028.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_img_model/ILSVRC2012_val_00000087.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_wrong_img_model/ILSVRC2012_val_00000310.JPEG}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps when both model have wrong predictions.}
    \label{fig:both_wrong}

\end{figure*}

\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}

\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_anis_model/ILSVRC2012_val_00049181.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_anis_model/ILSVRC2012_val_00049000.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_anis_model/ILSVRC2012_val_00049712.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_anis_model/ILSVRC2012_val_00049166.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_anis_model/ILSVRC2012_val_00049253.JPEG}\\
\rotatebox{90}{ImageNet}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_img_model/ILSVRC2012_val_00049181.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_img_model/ILSVRC2012_val_00049000.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_img_model/ILSVRC2012_val_00049712.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_img_model/ILSVRC2012_val_00049166.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/both_correct_img_model/ILSVRC2012_val_00049253.JPEG}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps when both model have correct predictions.}
    \label{fig:both_correct}

\end{figure*}

\begin{figure*}[h!]\centering
\begin{tabular}{cccccc}

\rotatebox{90}{Anisotropic}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_anis_model/ILSVRC2012_val_00049896.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_anis_model/ILSVRC2012_val_00048493.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_anis_model/ILSVRC2012_val_00048898.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_anis_model/ILSVRC2012_val_00049174.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_anis_model/ILSVRC2012_val_00048744.JPEG}\\
\rotatebox{90}{ImageNet}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_img_model/ILSVRC2012_val_00049896.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_img_model/ILSVRC2012_val_00048493.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_img_model/ILSVRC2012_val_00048898.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_img_model/ILSVRC2012_val_00049174.JPEG}&
\includegraphics[width=0.15\linewidth]{images/salinecy_maps_supply/img_correct_img_model/ILSVRC2012_val_00048744.JPEG}\\
\\
&(a)&(b)&(c)&(d)&(e)\\
\end{tabular}
  \caption{Saliency maps when ImageNet model has correct predictions and Anistropic model has wrong predictions.}
    \label{fig:img_correct}

\end{figure*}
\bibliography{egbib}

\end{document}