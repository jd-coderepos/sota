\documentclass{article}







\usepackage[final, nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{pifont}\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.97}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{floatrow}
\usepackage{caption}
\usepackage{booktabs}
\floatsetup[table]{style=plaintop}
\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}[section]
\title{Transductive Information Maximization \\ For Few-Shot Learning}




\author{Malik Boudiaf\thanks{Corresponding author: malik.boudiaf.1@etsmtl.net} \\
  \'ETS Montreal \\
\And
  Ziko Imtiaz Masud \\
  \'ETS Montreal \\
  \And
  Jérôme Rony \\
  \'ETS Montreal \\
  \AND
  Jose Dolz \\
  \'ETS Montreal \\
  \And
  Pablo Piantanida \\
  CentraleSup\'elec-CNRS \\ Universit\'e Paris-Saclay 
  \And
  Ismail Ben Ayed \\
  \'ETS Montreal \\
}



\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\base}{{\text{base}}}
\newcommand{\test}{{\text{test}}}

\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\newcommand{\tr}[1]{{#1}^\top}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\CC}{\mathcal{C}}
\newcommand{\ceq}{\stackrel{\mathclap{\normalfont\mbox{c}}}{=}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\x}{\bm{x}} \newcommand{\w}{\bm{w}} \newcommand{\p}{\bm{p}} \newcommand{\q}{\bm{q}} \newcommand{\cc}{\bm{c}} \newcommand{\y}{\bm{y}} \newcommand{\z}{\bm{z}} \newcommand{\Dcos}{D^{\text{cos}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\pp}{\vec p}
\newcommand{\vo}{\vec 1}
\newcommand{\uu}{\vec u}
\newcommand{\xx}{\vec x}
\newcommand{\ff}{\vec f}
\newcommand{\aaa}{\boldsymbol{\alpha}}
\newcommand{\ttt}{\boldsymbol{\theta}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\def\real{{\mathbb R}}




%
 
\begin{document}

\maketitle
\begin{abstract}


We introduce Transductive Infomation Maximization (TIM) for few-shot learning. Our method maximizes the mutual information between the query features and their label predictions for a given few-shot task, in conjunction with a supervision loss based on the support set. Furthermore, we propose a new alternating-direction solver for our mutual-information loss, which substantially speeds up transductive-inference convergence over gradient-based optimization, while yielding similar accuracy. 
TIM inference is modular: it can be used on top of any base-training feature extractor. 
Following standard transductive few-shot settings, our comprehensive experiments\footnote{Code publicly available at \url{https://github.com/mboudiaf/TIM}} demonstrate that TIM outperforms state-of-the-art methods significantly across various datasets and networks, while used on top of a fixed feature extractor trained with simple cross-entropy on the base classes, without resorting to complex meta-learning schemes. It consistently brings between $2 \%$ and $5 \%$ improvement in accuracy over the best performing method, not only on all the well-established few-shot benchmarks but also on more challenging scenarios, with domain shifts and larger numbers of classes.


\end{abstract} 
\section{Introduction}
    Deep learning models have achieved unprecedented success, approaching human-level performances when trained on large-scale labeled data. However, the generalization of such models might be seriously challenged when dealing with new (unseen) classes, with only a few labeled instances per class. Humans, however, can learn new tasks rapidly from a handful of instances, by leveraging context and \emph{prior} knowledge. The few-shot learning (FSL) paradigm \cite{miller2000learning,fei2006one,matching_net} attempts to bridge this gap, and has recently attracted substantial research interest, with a large body of very recent works, e.g., \cite{can,dhillon2019baseline,leo,feat,liu2018learning,closer_look,team,kim2019edge,relation_net,inat_benchmark,gidaris2019boosting,prototypical_nets,maml}, among many others. In the few-shot setting, a model is first trained on labeled data with {\em base} classes. Then, model generalization is evaluated on few-shot {\em tasks}, composed of unlabeled samples from novel classes unseen during training (the {\em query} set), assuming only one or a few labeled samples (the {\em support} set) are given per novel class. 
    
    Most of the existing approaches within the FSL framework are based on the ``learning to learn'' paradigm or meta-learning \cite{maml,prototypical_nets,matching_net,relation_net,lee2019meta}, where the training set is viewed as a series of balanced tasks (or \textit{episodes}), so as to simulate test-time scenario. Popular works include prototypical networks \cite{prototypical_nets}, which describes each class with an embedding prototype and maximizes the log-probability of query samples via episodic training; matching network \cite{matching_net}, which represents query predictions as linear combinations of support labels and employs episodic training along with memory architectures; MAML \cite{maml}, a meta-learner, which trains a model to make it "easy" to fine-tune; and the LSTM meta-learner in \cite{ravi2016optimization}, which suggests optimization as a model for few-shot learning. A large body of meta-learning works followed-up lately, to only cite a few \cite{leo,tadam,Mishra18,relation_net,feat}.

    \subsection{Related work}
        \textbf{Transductive inference:} In a recent line of work, {\em transductive} inference has emerged as an appealing approach to tackling few-shot tasks \cite{dhillon2019baseline, can, kim2019edge,liu2018learning,team, nichol2018firstorder, prototype, Laplacian}, showing performance improvements over {\em inductive} inference. In the transductive setting\footnote{Transductive few-shot inference is not to be confused with semi-supervised few-shot learning \cite{tiered_imagenet, paper_to_please_R1}. The latter uses extra unlabeled data during meta-training. Transductive inference has access to exactly the same training/testing data as its inductive counterpart.}, the model classifies the unlabeled query examples of a single few-shot task at once, instead of one sample at a time as in inductive methods.
These recent experimental observations in few-shot learning are consistent with established facts in classical transductive inference \cite{vapnik1999overview,joachim99,z2004learning}, which is well-known to outperform inductive methods on small training sets. While \cite{nichol2018firstorder} used information of unlabeled query samples via batch normalization, the authors of \cite{liu2018learning} were the first to model explicitly transductive inference in few-shot learning. Inspired by popular label-propagation concepts \cite{z2004learning}, they built a meta-learning framework that learns to propagate labels from labeled to unlabeled instances via a graph. The meta-learning transductive method in \cite{can} used attention mechanisms to propagate labels to unlabeled query samples. More closely related to our work, the recent transductive inference of Dhillion et al. \cite{dhillon2019baseline} minimizes the entropy of the network softmax predictions at unlabeled query samples, reporting competitive few-shot performances, while using standard cross-entropy training on the base classes. The competitive performance of \cite{dhillon2019baseline} is in line with several recent inductive baselines \cite{closer_look,simpleshot,tian2020rethinking}, which reported that standard cross-entropy training for the base classes matches or exceeds the performances of more sophisticated meta-learning procedures. Also, the performance of \cite{dhillon2019baseline} is in line with established results in the context of semi-supervised learning, where entropy minimization is widely used \cite{grandvalet2005semi,miyato2018virtual,berthelot2019mixmatch}. It is worth noting that the inference runtimes of transductive methods are, typically, much higher than their inductive counterparts. For, instance, the authors of \cite{dhillon2019baseline} fine-tune all the parameters of a deep network during inference, which is several orders of magnitude slower than inductive methods such as ProtoNet \cite{prototypical_nets}. Also, based on matrix inversion, the transductive inference in \cite{liu2018learning} has a complexity that is cubic in the number of query samples.
            


        
        
        \textbf{Info-max principle:} While the semi-supervised and few-shot learning works in \cite{grandvalet2005semi,dhillon2019baseline} build upon Barlow's principle of entropy minimization \cite{Barlow1989Unsupervised}, our few-shot formulation is inspired by the general info-max principle enunciated by Linsker \cite{Linsker1988Self}, which formally consists in maximizing the Mutual Information (MI) between the inputs and outputs of a system. In our case, the inputs are the query features and the outputs are their label predictions. The idea is also related to info-max in the context of clustering \cite{clustering_infomax,HuICML17,jabi}. More generally, info-max principles, well-established in the field of communications, were recently used in several deep-learning problems, e.g., representation learning \cite{deep_infomax,cpc}, metric learning \cite{boudiaf2020metric} or domain adaptation \cite{liang2020we}, among other problems.   
        
        






    \subsection{Contributions}
        \begin{itemize}
            \item We propose Transductive Information Maximization (TIM) for few-shot learning. Our method maximizes the MI between the query features and their label predictions for a few-shot task at inference, while minimizing the cross-entropy loss on the support set.
            \item We derive an alternating-direction solver for our loss, which substantially speeds up transductive inference over gradient-based optimization, while yielding competitive accuracy.
            \item Following standard transductive few-shot settings, our comprehensive evaluations show that TIM outperforms state-of-the-art methods substantially across various datasets and networks,  while using a simple cross-entropy training on the base classes, without complex meta-learning schemes. It consistently brings between $2 \%$ and $5 \%$ of improvement in accuracy over the best performing method, not only on all the well-established few-shot benchmarks but also on more challenging, recently introduced scenarios, with domain shifts and larger numbers of ways. Interestingly, our MI loss includes a label-marginal regularizer, which has a significant effect: it brings substantial improvements in accuracy, while facilitating optimization, reducing transductive runtimes by orders of magnitude.

        \end{itemize}





 
\section{Transductive Information Maximization}

\subsection{Few-shot setting}
    	Assume we are given a labeled training set, $\mathcal{X}_\base\coloneqq \{\x_i, \y_i\}_{i=1}^{N_\base}$, where 
    	$\x_i$ denotes raw features of sample $i$ and $\y_i$ its associated one-hot encoded label. Such labeled set is often referred to as the {\em meta-training} or {\em base} dataset in the few-shot literature. Let  
    	$\mathcal{Y}_\base$ denote the set of classes for this base dataset. The few-shot scenario assumes that we are given a {\em test} dataset: $\mathcal{X}_\test\coloneqq\{\x_i, \y_i\}_{i=1}^{N_\test}$, with a completely new set of classes $\mathcal{Y}_{\test}$ such that $\mathcal{Y}_\base \cap \mathcal{Y}_\test = \emptyset$, from which we create randomly sampled few-shot {\em tasks}, each with a few labeled examples. 
Specifically, each $K$-way $N_{S}$-shot task involves sampling $N_S$ labeled examples from each of $K$ different classes, also chosen at random.
    	Let $\mathcal{S}$ denote the set of these labeled examples, referred to as the \textit{support} set with size   $|\mathcal{S}|=N_{{S}}\cdot K$.
    	Furthermore, each task has a {\em query} set denoted by $\mathcal{Q}$ composed of $ | \mathcal{Q}| =N_{{Q}} \cdot K$ unlabeled (unseen) examples from each of the $K$ classes. With models trained on the base set, few-shot techniques use the labeled support sets to adapt to the tasks at hand, and are evaluated based on their performances on the unlabeled query sets.

	
	\subsection{Proposed formulation}\label{sec:formulation}
	

    	We begin by introducing some basic notation and definitions before presenting  our overall Transductive Information Maximization (TIM) loss and the different optimization strategies for tackling it. For a given few-shot task, with a support set $\mathcal{S}$ and a query set $\mathcal{Q}$, let $X$ denote the random variable associated with the raw features within ${\mathcal{S} \cup \mathcal{Q}}$, and let $Y \in  \mathcal{Y}=\{1, \dots, K \}$ be the random variable associated with the labels. Let $f_{\boldsymbol{\phi}}: \mathcal{X} \longrightarrow \mathcal{Z} \subset \mathbb{R}^d$ denote the encoder (\emph{i.e.}, feature-extractor) function of a deep neural network, where $\boldsymbol{\phi}$ denotes the trainable parameters, and $\mathcal{Z}$ stands for the set of embedded features. The encoder is first trained from the base training set $\mathcal{X}_\base$ using the standard cross-entropy loss, without any meta training or specific sampling schemes. Then, for each specific few-shot task, we propose to minimize a mutual-information loss defined over the query samples.
    	
    	Formally, we define a soft-classifier, parametrized by weight matrix $\mathbf{W} \in \mathbb{R}^{K \times d}$, whose posterior distribution over labels given features\footnote{In order to simplify our  notations, we deliberately omit the dependence of posteriors $p_{ik}$ on the network parameters $(\boldsymbol{\phi},\mathbf{W})$. Also, $p_{ik}$ takes the form of \emph{softmax} predictions, but we omit the normalization constants.}, ${p_{ik} \coloneqq \mathbb{P}(Y=k|{ X=\x_i}; \mathbf{W}, \boldsymbol{\phi})}$, and marginal distribution over query labels, $\widehat{p}_k=\mathbb{P}(Y_\mathcal{Q}=k; \mathbf{W}, \bm{\phi})$, are given by:
    	\begin{equation}\label{eq:distance_based_classifier}
    		p_{ik} \propto \exp\left(-\frac{\tau}{2}\norm{\w_k - \z_i}^2\right),\quad  \textrm{ and } \quad   \widehat{p}_k = \frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} p_{ik} \,  
    	\end{equation}
    	where $\mathbf{W} \coloneqq [\w_1, \dots, \w_K]$ denotes classifier weights, $\z_i=\frac{f_{\boldsymbol{\phi}}(\x_i)}{\norm{f_{\boldsymbol{\phi}}(\x_i)}_2}$ the L2-normalized embedded features, and $\tau$ is a temperature parameter. 
    
         Now, for each single few-shot task, we introduce our empirical weighted mutual information between the query samples and their latent labels, which integrates two terms: The first is an empirical (Monte-Carlo) estimate of the conditional entropy of labels given the query raw features, denoted $\mathcal{\widehat{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$, while the second is the empirical label-marginal entropy, $\mathcal{\widehat{H}}(Y_\mathcal{Q})$.:
        \begin{align}
        \label{prior-aware-MI}
            \mathcal{\widehat{I}}_{\alpha}(X_\mathcal{Q}; Y_\mathcal{Q}) \coloneqq \underbrace{-\sum_{k=1}^K \widehat{p}_{k} \log \widehat{p}_{k}}_{\mathcal{\widehat{H}}(Y_\mathcal{Q}):~\text{marginal entropy}} +\ \alpha \ \underbrace{\frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}}\sum_{k=1}^K p_{ik} \log (p_{ik})}_{-\mathcal{\widehat{H}}(Y_\mathcal{Q}| X_\mathcal{Q}):~\text{conditional entropy}},
        \end{align}
        with $\alpha$ a non-negative hyper-parameter. Notice that setting $\alpha=1$ recovers the standard mutual information. Setting $\alpha<1$ allows us to down-weight the conditional entropy term, whose gradients may
        dominate the marginal entropy's gradients as the predictions move towards the vertices of the simplex. The role of both terms in \eqref{prior-aware-MI} will be discussed after introducing our overall transductive inference loss in the following, by embedding supervision from the task's support set. 


        We embed supervision information from support set $\mathcal{S}$ by integrating a standard cross-entropy loss $\textrm{CE}$ with the information measure in Eq. \eqref{prior-aware-MI}, which enables us to formulate our Transductive Information Maximization (\textbf{TIM}) loss as follows:
\begin{equation}\label{eq:tim_objective}
        	    \boxed{\min_{\mathbf{W}} \ \lambda \cdot \textrm{CE} -   \mathcal{\widehat{I}}_{\alpha}(X_\mathcal{Q}; Y_\mathcal{Q})} \quad
        	    \textrm{ with } \quad \textrm{CE} \coloneqq -\frac{1}{|\mathcal{S}|} \sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \log (p_{ik}),
        	\end{equation}
        where $\{y_{ik}\}$ denotes the $k^{th}$ component of the one-hot encoded label $\y_i$ associated to the $i$-th support sample. Non-negative hyper-parameters $\alpha$ and $\lambda$ will be fixed to $\alpha=\lambda=0.1$ in all our experiments. 
It is worth to discuss in more details the role (importance) of the mutual information terms in (\ref{eq:tim_objective}): 
    	\begin{itemize}[leftmargin=*]
    	    \item Conditional entropy $\mathcal{\widehat{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ aims at minimizing the uncertainty of the posteriors at unlabeled query samples, thereby encouraging the model to output {\em confident} predictions\footnote{The global minima of each pointwise entropy in the sum of $\mathcal{\widehat{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ are one-hot vectors at the vertices of the simplex.}. This entropy loss is widely used in the context of semi-supervised learning (SSL) \cite{grandvalet2005semi,miyato2018virtual,berthelot2019mixmatch}, as it models effectively the {\em cluster} assumption: The classifier's boundaries should not occur at dense regions of the unlabeled features \cite{grandvalet2005semi}. Recently, \cite{dhillon2019baseline} introduced this term for few-shot learning, showing that entropy fine-tuning on query samples achieves competitive performances. In fact, if we remove the marginal entropy $\widehat{\mathcal{H}}(Y_\mathcal{Q})$ in objective \eqref{eq:tim_objective}, our TIM objective reduces to the loss in \cite{dhillon2019baseline}. The conditional entropy $\widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ is of paramount importance but its optimization  requires special care, as its optima may easily lead to degenerate (non-suitable) solutions on the simplex vertices, mapping all samples to a single class. Such care may consist in using small learning rates and fine-tuning the whole network (which itself often contains several layers of regularization) as done in \cite{dhillon2019baseline}, both of which significantly slow down transductive inference.
    	    
    	    \item The label-marginal entropy regularizer $\widehat{\mathcal{H}}(Y_\mathcal{Q})$ encourages the marginal distribution of labels to be uniform, thereby avoiding degenerate solutions obtained when solely minimizing conditional entropy. Hence, it is highly important as it removes the need for implicit regularization, as mentioned in the previous paragraph. In particular, high-accuracy results can be obtained even using higher learning rates and fine-tuning only a fraction of the network parameters (classifier weights $\mathbf{W}$ instead of the whole network), speeding up substantially transductive runtimes. As it will be observed from our experiments, this term brings substantial improvements in performances (e.g., up to $10\%$ increase in accuracy over entropy fine-tuning on the standard few-shot benchmarks), while facilitating optimization, thereby reducing transductive runtimes by orders of magnitude.



    	   
\end{itemize}
        	
    	\subsection{Optimization}\label{sec:optim}
            At this stage, we consider that the feature extractor has already been trained on base classes (using standard cross-entropy). We now propose two methods for minimizing our objective (\ref{eq:tim_objective}) for each test task. The first one is based on standard Gradient Descent (GD). The second is a novel way of optimizing mutual information, and is inspired by the Alternating Direction Method of Multipliers (ADMM). For both methods:
            \begin{itemize}
                \item The pre-trained feature extractor $f_\phi$ is kept fixed. Only the weights $\mathbf{W}$ are optimized for each task. Such a choice is discussed in details in \autoref{sec:ablation_terms}. Overall, and interestingly, we found that fine-tuning only classifier weights $\mathbf{W}$, while fixing feature-extractor parameters $\phi$, yielded the best performances for our mutual-information loss.
\item For each task, weights $\mathbf{W}$ are initialized as the class prototypes of the support set: 
                \begin{align*}
                    \w_k^{(0)}= \frac{\sum_{i \in \mathcal{S}}y_{ik} \z_i}{\sum_{i \in \mathcal{S}}y_{ik}}
                \end{align*}
            \end{itemize}
            
    
            \textbf{Gradient descent (TIM-GD):} A straightforward way to minimize our loss in Eq. (\ref{eq:tim_objective}) is to perform gradient descent over $\mathbf{W}$, which 
we  update using all the samples of the few-shot task (both support and query) at once (i.e., no mini-batch sampling). This gradient approach yields our overall best results, while being one order of magnitude faster than the transductive entropy-based fine-tuning in \cite{dhillon2019baseline}. As will be shown later in our experiments, the method in \cite{dhillon2019baseline} needs to fine-tune the whole network (i.e., to update both $\phi$ and $\mathbf{W}$), which provides implicit regularization, avoiding the degenerate solutions of entropy minimization. However, TIM-GD (with $\mathbf{W}$-updates only) still remains two orders of magnitude slower than inductive closed-form solutions \cite{prototypical_nets}. In the following, we present a more efficient solver for our problem.
            


        		
    	    \textbf{Alternating direction method (TIM-ADM): }
        	    We derive an Alternating Direction Method (ADM) for minimizing our objective in \eqref{eq:tim_objective}. Such scheme yields substantial speedups in transductive learning (one order of magnitude), while maintaining the high levels of accuracy of TIM-GD. To do so, we introduce auxiliary variables representing latent assignments of query samples, and minimize a mixed-variable objective by alternating two sub-steps, one optimizing w.r.t classifier's weights $\mathbf{W}$, and the other w.r.t the auxiliary variables $\q$. 
\begin{prop}\label{prop:mi_adm}
    		        The objective in (\ref{eq:tim_objective}) can be approximately minimized via the following constrained formulation of the problem:
    		        \begin{align}\label{eq:mi_adm}
\min_{\mathbf{W}, \textbf{q}} \,\,  &\underbrace{-\frac{\lambda}{|\mathcal{S}|} \sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \log (p_{ik})}_{\mathrm{CE}} + \underbrace{\sum_{k=1}^K \widehat{q}_{k}\log {\widehat{q}_{k}}}_{\sim \widehat{\mathcal{H}}(Y_\mathcal{Q}) } \underbrace{-\frac{\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}}\sum_{k=1}^K q_{ik} \log (p_{ik})}_{\sim \, \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})} + \underbrace{\frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik}\log \frac{q_{ik}}{p_{ik}}}_{\text{Penalty} \, \equiv\, \mathcal{D}_{\mathrm{KL}}(\mathbf{q} \| \mathbf{p} )}
                        \nonumber \\
                        \text{s.t}  \quad & \sum_{k=1}^K q_{ik}=1, \quad q_{ik} \geq 0, \quad i \in \mathcal{Q}, \quad k \in \{1, \dots, K\},
    		        \end{align}
    		        
where $\textbf{q}=[q_{ik}] \in \mathbb R^{|\mathcal{Q}| \times K}$ are auxiliary variables,  $\textbf{p}=[p_{ik}] \in \mathbb R^{|\mathcal{Q}| \times K}$ and $\widehat{q}_k=\frac{1}{|\mathcal{Q}|}\sum\limits_{i \in \mathcal{Q}}q_{ik}$.
    		    \end{prop}
    		    \begin{proof}
    		    It is straightforward to notice that, when equality constraints $q_{ik} = p_{ik}$ are satisfied, the last term in objective (\ref{eq:mi_adm}), which can be viewed as a soft penalty for enforcing those equality constraints,  vanishes. Objectives (\ref{eq:tim_objective}) and (\ref{eq:mi_adm}) then become equivalent.
    		    \end{proof}
    		    Splitting the problem into sub-problems on $\mathbf{W}$ and $\textbf{q}$ as in Eq. \eqref{eq:mi_adm} is closely related to the general principle of ADMM (Alternating Direction Method of Multipliers) \cite{boyd}, except that the KL divergence is not a typical penalty for imposing the equality constraints\footnote{Typically, ADMM methods use multiplier-based quadratic penalties for enforcing the equality constraint.}.    		  The main idea is to \textbf{decompose the original problem into two easier sub-problems}, one over $\mathbf{W}$ and the other over $\q$, which can be alternately solved, each in closed-form. Interestingly, this KL penalty is important as it completely removes the need for dual iterations for the simplex constraints in \eqref{eq:mi_adm}, yielding closed-form solutions:
    		    


    		    \begin{prop}\label{prop:mi_adm_solution}
    		        ADM formulation in Proposition \ref{prop:mi_adm} can be approximately solved by alternating the following closed-form updates w.r.t auxiliary variables $\textbf{q}$ and classifier weights $\mathbf{W}$ ($t$ is the iteration index):
    		        \begin{align}
q_{ik}^{(t+1)} &\propto \quad \displaystyle \frac{ \left ( p_{ik}^{(t)} \right )^{1+\alpha}}{\displaystyle \left ( \sum_{i \in \mathcal{Q}} \left ( p_{ik}^{(t)} \right )^{1+\alpha} \right )^{1/2}}  \\
    		          \w_k^{(t+1)} &\leftarrow \frac{\displaystyle\frac{\lambda}{1+\alpha} \displaystyle\sum\limits_{i \in \mathcal{S}} \left ( y_{ik} \,\z_i + p_{ik}^{(t)} (\w_k^{(t)} - \z_i) \right ) +  \frac{|\mathcal{S}|}{|\mathcal{Q}|} \displaystyle\sum\limits_{i \in \mathcal{Q}} \left ( q_{ik}^{(t+1)} \z_i + p_{ik}^{(t)} (\w_k^{(t)} - \z_i) \right )}{\displaystyle\frac{\lambda}{1+\alpha} \displaystyle\sum\limits_{i \in \mathcal{S}} y_{ik} +  \frac{|\mathcal{S}|}{|\mathcal{Q}|} \sum\limits_{i \in \mathcal{Q}} q_{ik}^{(t+1)}}
    		        \end{align}
    		    \end{prop}
    		    \begin{proof}
    		    A detailed proof is deferred to the supplementary material. Here, we summarize the main technical ingredients of the approximation. Keeping the auxiliary variables $\textbf{q}$ fixed, we optimize
    		    a convex approximation of Eq. \eqref{eq:mi_adm} w.r.t $\mathbf{W}$. With $\mathbf{W}$ fixed, the objective is strictly convex w.r.t the auxiliary variables $\textbf{q}$ whose updates come from a closed-form solution of the KKT (Karush–Kuhn–Tucker) conditions. Interestingly, the negative entropy of auxiliary variables, which appears in the penalty term, handles implicitly the simplex constraints, which removes the need for dual iterations to solve the KKT conditions.
    		    \end{proof}


 
\section{Experiments}

    \textbf{Hyperparameters: } To keep our experiments as simple as possible, \textbf{our hyperparameters are kept fixed across all the experiments and methods (TIM-GD and TIM-ADM)}. The conditional entropy weight $\alpha$ and the cross-entropy weights $\lambda$ in Objective (\ref{eq:tim_objective}) are both set to $0.1$. The temperature parameter $\tau$ in the classifier is set to 15. 
In our TIM-GD method, we use the ADAM optimizer with the recommended parameters \cite{kingma2014adam}, and run 1000 iterations for each task. For TIM-ADM, we run 150 iterations.
    
    \textbf{Base-training procedure:} The feature extractors are trained following the same simple base-training procedure as in \cite{Laplacian} and using standard networks (ResNet-18 and WRN28-10), for all the experiments. Specifically, they are trained using the standard cross-entropy loss on the base classes, with label smoothing. The label-smoothing parameter is set to 0.1. We emphasize that base training does not involve any meta-learning or episodic training strategy. The models are trained for 90 epochs, with the learning rate initialized to 0.1, and divided by 10 at epochs 45 and 66. Batch size is set to 256 for ResNet-18, and to 128 for WRN28-10. During training, all the images are resized to $84 \times 84$, and we used the same data augmentation procedure as in \cite{Laplacian}, which includes random cropping, color jitter  and random horizontal flipping. 

    \textbf{Datasets: } We resort to 3 few-shot learning  datasets to benchmark the proposed models. As standard few-shot benchmarks, we use the \textbf{\textit{mini}-Imagenet} \cite{matching_net} dataset, with 100 classes split as in \cite{ravi2016optimization}, the \textbf{Caltech-UCSD Birds 200} \cite{cub} (CUB) dataset, with 200 classes, split following \cite{closer_look}, and finally the larger \textbf{\textit{tiered}-Imagenet} dataset, with 608 classes split as in \cite{tiered_imagenet}.


    \subsection{Comparison to state-of-the-art}\label{sec:benchmark}

        We first evaluate our methods TIM-GD and TIM-ADM on the widely adopted \textit{mini}-ImageNet, \textit{tiered}-ImageNet and \textit{CUB} benchmark datasets, in the most common 1-shot 5-way and 5-shot 5-way scenarios, with 15 query shots for each class. Results are reported in \autoref{tab:benchmark_results}, and are averaged over 10,000 episodes, following \cite{simpleshot}. We can observe that both TIM-GD and TIM-ADM yield state-of-the-art performances, consistently across all standard datasets, scenarios and backbones, improving over both transductive and inductive methods, by significant margins. 


        \begin{table}
        \centering
        \small
        \caption{Comparison to the state-of-the-art methods on \textit{mini}-ImageNet, \textit{tiered}-Imagenet and CUB. The methods are sub-grouped into transductive and inductive methods, as well as by backbone architecture. Our results (gray-shaded) are averaged over 10,000 episodes. "-" signifies the result is unavailable.}
        \begin{tabular}{lccccccccc}
            & & &\multicolumn{2}{c}{\textbf{\textit{mini}-ImageNet}} & \multicolumn{2}{c}{\textbf{\textit{tiered}-ImageNet}} & \multicolumn{2}{c}{\textbf{CUB}} \\
             Method & Transd. & Backbone & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot\\
             \toprule
             MAML \cite{maml}& \multirow{9}{*}{\xmark} & ResNet-18 & 49.6 & 65.7 & - & - & 68.4 & 83.5 \\
RelatNet \cite{relation_net}& & ResNet-18 & 52.5 & 69.8 &  - & - & 68.6 & 84.0 \\
             MatchNet \cite{matching_net}& & ResNet-18 & 52.9 & 68.9 &  - & - & 73.5 & 84.5 \\
             ProtoNet \cite{prototypical_nets}& & ResNet-18 & 54.2 & 73.4 &  - & - & 73.0 & 86.6 \\
MTL \cite{sun2019meta} & & ResNet-12 & 61.2 & 75.5 & - & - &  - & - \\
             vFSL \cite{variational_fsl} & & ResNet-12 & 61.2 & 77.7 & - & - & - & - \\
             Neg-cosine \cite{liu2020negative}& & ResNet-18 & 62.3 & 80.9 & - & - & 72.7 & 89.4 \\
             MetaOpt \cite{lee2019meta} & & ResNet-12 & 62.6 & 78.6 & 66.0 & 81.6 & - & -\\
             SimpleShot \cite{simpleshot} & & ResNet-18 & 62.9 & 80.0 & 68.9 & 84.6 & 68.9 & 84.0 \\
             Distill \cite{tian2020rethinking} & & ResNet-12 & 64.8 & 82.1 & 71.5 & 86.0 & - & -\\
             \midrule
             RelatNet + T \cite{can} & & ResNet-12 & 52.4 & 65.4 & - & - & - & -\\
             ProtoNet + T \cite{can} & & ResNet-12 & 55.2 & 71.1 & - & - & - & -\\
             MatchNet+T \cite{can} &  & ResNet-12 & 56.3 & 69.8 & - & - & - & -\\
             TPN \cite{liu2018learning} &  & ResNet-12 & 59.5 & 75.7 & - & - & - & -\\
             TEAM \cite{team}& & ResNet-18 & 60.1 & 75.9 & - & - & - & -\\
             Ent-min \cite{dhillon2019baseline}& & ResNet-12 & 62.4 & 74.5 & 68.4 & 83.4 & - & -\\
             CAN+T \cite{can}& & ResNet-12 & 67.2 & 80.6 & 73.2 & 84.9 & - & -\\
             LaplacianShot \cite{Laplacian}& & ResNet-18 & 72.1 & 82.3 & 79.0 & 86.4 & 81.0 & 88.7 \\
             \rowcolor{Gray} TIM-ADM & & ResNet-18 & 73.6 & \textbf{85.0} & \textbf{80.0} & \textbf{88.5} & 81.9 & 90.7 \\
             \rowcolor{Gray} TIM-GD & \multirow{-10}{*}{\cmark} & ResNet-18 & \textbf{73.9} & \textbf{85.0} & 79.9 & \textbf{88.5} & \textbf{82.2} & \textbf{90.8} \\
             \toprule
             LEO \cite{leo}& \multirow{5}{*}{\xmark} & WRN28-10 & 61.8 & 77.6 & 66.3 & 81.4 & - & -\\
             SimpleShot \cite{simpleshot}& & WRN28-10 & 63.5 & 80.3 & 69.8 & 85.3 & - & -\\
             MatchNet \cite{matching_net}& & WRN28-10 & 64.0 & 76.3 & - & - & - & -\\
             CC+rot+unlabeled \cite{gidaris2019boosting} & & WRN28-10 & 64.0 & 80.7 & 70.5 & 85.0 & - & -\\
             FEAT \cite{feat} & & WRN28-10 & 65.1 & 81.1 & 70.4 & 84.4 & - & -\\
             \midrule
             AWGIM \cite{guo2020attentive}&  & WRN28-10 & 63.1 & 78.4 & 67.7 & 82.8 & - & -\\
             Ent-min \cite{dhillon2019baseline} &  & WRN28-10 & 65.7 & 78.4 & 73.3 & 85.5 & - & -\\
             SIB  \cite{hu2020empirical} &  & WRN28-10 & 70.0 & 79.2 & - & - & - & -\\
             BD-CSPN \cite{prototype}&  & WRN28-10 & 70.3 & 81.9 & 78.7 & 86.92 & - & -\\
             LaplacianShot  \cite{Laplacian} &  & WRN28-10 & 74.9 & 84.1 & 80.2 & 87.6 & - & -\\
             \rowcolor{Gray} TIM-ADM & & WRN28-10 & 77.5 & 87.2 & 82.0 & 89.7 & - & -\\
             \rowcolor{Gray} TIM-GD & \multirow{-7}{*}{\cmark} & WRN28-10 & \textbf{77.8} & \textbf{87.4} & \textbf{82.1} & \textbf{89.8} & - & -\\
\bottomrule
        \end{tabular}
        \label{tab:benchmark_results}
        \end{table}
        
        \begin{table}
            \newlength\wexp
            \settowidth{\wexp}{\textbf{\textit{mini}-ImageNet} $\rightarrow$ \textbf{CUB}}
            \centering
            \small
            \caption{The results for the domain-shift setting \textit{mini}-Imagenet $\rightarrow$ CUB. The results obtained by our models (gray-shaded) are averaged over 10,000 episodes.}
            \begin{tabular}{lcccc}
& & \multicolumn{1}{c}{\textbf{\textit{mini}-ImageNet} $\rightarrow$ \textbf{CUB}}\\
                 Methods & Backbone & 5-shot\\
                 \toprule
                 MatchNet \cite{matching_net} & ResNet-18 & 53.1\\
                 MAML \cite{maml} & ResNet-18 & 51.3 \\
                 ProtoNet \cite{prototypical_nets} & ResNet-18 & 62.0 \\
                 RelatNet \cite{relation_net} & ResNet-18 & 57.7 \\
SimpleShot \cite{simpleshot} & ResNet-18 & 64.0 \\
                GNN \cite{tseng2020cross} & ResNet-10 & 66.9 \\
                Neg-Cosine \cite{liu2020negative} & ResNet-18 & 67.0 \\
                Baseline \cite{closer_look} & ResNet-18 & 65.6 \\
                LaplacianShot \cite{Laplacian} & ResNet-18 & 66.3 \\
                \rowcolor{Gray} TIM-ADM & ResNet-18 & 70.3 \\
                \rowcolor{Gray} TIM-GD & ResNet-18 & \textbf{71.0} \\
                 \bottomrule
            \end{tabular}
            \label{tab:domin_shift_results}
        \end{table}
         
    \subsection{Impact of domain-shift}   
        Chen et al. \cite{closer_look} recently showed that the performance of most meta-learning methods may drop drastically when a domain-shift exists between the base training data and test data. Surprisingly, the simplest 
        discriminative baseline exhibited the best performance in this case. Therefore, we evaluate our methods in this challenging scenario. To this end, we simulate a domain shift by training the feature encoder on \textit{mini}-Imagenet while evaluating the methods on \textit{CUB}, similarly to the setting introduced in \cite{closer_look}. TIM-GD and TIM-ADM beat previous methods by significant margins in the domain-shift scenario, consistently with our results in the standard few-shot benchmarks, thereby demonstrating an increased potential of applicability to real-world situations.

    \subsection{Pushing the meta-testing stage}
        
        Most few-shot papers only evaluate their method in the usual 5-way scenario. Nevertheless, \cite{closer_look} showed that meta-learning methods could be beaten by their discriminative baseline when more ways were introduced in each task. Therefore, we also provide results of our method in the more challenging 10-way and 20-way scenarios on \textit{mini}-ImageNet. These results, which are presented in \autoref{tab:more_ways}, show that TIM-GD outperforms other methods by significant margins, in both settings.
        \begin{table}
            \centering
            \caption{Results for increasing the number of classes on \textit{mini}-ImageNet. The results obtained by our models (gray-shaded) are averaged over 10,000 episodes.}
            \small
            \begin{tabular}{lcccccccc}
                & & \multicolumn{2}{c}{10-way} & \multicolumn{2}{c}{20-way} \\
                 Methods & Backbone & 1-shot & 5-shot & 1-shot & 5-shot\\
                 \toprule
                 MatchNet \cite{matching_net}& ResNet-18 & - & 52.3 & - & 36.8 \\
                 ProtoNet \cite{prototypical_nets}& ResNet-18 & - & 59.2 & - & 45.0 \\
                 RelatNet \cite{relation_net}& ResNet-18 & - & 53.9 & - & 39.2 \\
                 SimpleShot \cite{simpleshot}& ResNet-18 & 45.1 & 68.1 & 32.4 & 55.4 \\
                 Baseline \cite{closer_look}& ResNet-18 & - & 55.0 & - & 42.0 \\
                 Baseline++ \cite{closer_look}& ResNet-18 & - & 63.4 & - & 50.9 \\
                 \rowcolor{Gray} TIM-ADM & ResNet-18 & 56.0 & \textbf{72.9} & \textbf{39.5} & 58.8 \\
                 \rowcolor{Gray} TIM-GD & ResNet-18 & \textbf{56.1} & 72.8 & 39.3 & \textbf{59.5} \\
                 \bottomrule
            \end{tabular}
            \label{tab:more_ways}
        \end{table}

    \subsection{Ablation study}

        \textbf{Influence of each term: }\label{sec:ablation_terms} We now assess the impact of each term\footnote{The $\textbf{W}$ and $\textbf{q}$ updates of TIM-ADM associated to each configuration can be found in the supplementary material.} in our loss in Eq. (\ref{eq:tim_objective}) on the final performance of our methods. The results are reported in \autoref{tab:ablation_effect_terms}. 
        We observe that integrating the three terms in our loss consistently outperforms any other configuration. Interestingly, removing the label-marginal entropy, $\widehat{\mathcal{H}}(Y_\mathcal{Q})$, reduces significantly the performances in both TIM-GD and TIM-ADM, particularly when only classifier weights $\textbf{W}$ are 
        updated and feature extractor $\boldsymbol{\phi}$ is fixed. 
Such a behavior could be explained by the following fact: the conditional entropy term, $\mathcal{\widehat{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$, may yield degenerate solutions (assigning all query samples to a single class) on numerous tasks, when used alone. 
This emphasizes the importance of the label-marginal entropy term $\widehat{\mathcal{H}}(Y_\mathcal{Q})$ in our loss (\ref{eq:tim_objective}), which acts as a powerful regularizer to prevent such trivial solutions.
        
        \begin{table}[t]
           \small
           \centering
            \caption{Ablation study on the effect of each term in our loss in Eq. \eqref{eq:tim_objective}, when only the classifier weights are fine-tuned, i.e., updating only $\textbf{W}$, and when the whole network is fine-tuned, i.e., updating $\{\boldsymbol{\phi},\textbf{W}\}$. The results are reported for ResNet-18 as backbone. 
            The same term indexing as in Eq. \eqref{eq:tim_objective} is used here: $\widehat{\mathcal{H}}(Y_\mathcal{Q})$: Marginal entropy, $\widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$: Conditional entropy, $\mathrm{CE}$: Cross-entropy.}
           \resizebox{\textwidth}{!}{
           \begin{tabular}{ccccccccc}
                 & & & \multicolumn{2}{c}{\textbf{\textit{mini}-ImageNet}} & \multicolumn{2}{c}{\textbf{\textit{tiered}-ImageNet}} & \multicolumn{2}{c}{\textbf{CUB}}\\
                 Method & Param. & Loss & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\
                 \toprule
                 \multirow{4}{*}{TIM-ADM} & \multirow{4}{*}{$\{\textbf{W}\}$} & $\mathrm{CE}$ & 60.0 & 79.6 & 68.0 & 84.6 & 68.6 & 86.4 \\
                 & & $\mathrm{CE} + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & 36.0 & 77.0 & 48.1 & 82.5 & 48.5 & 86.5 \\
                 & & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q})$ & 66.7 & 82.0 & 74.0 & 86.5 & 74.2 & 88.3 \\
                 & & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q}) + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & \textbf{73.6} & \textbf{85.0} & \textbf{80.0} & \textbf{88.5} & \textbf{81.9} & \textbf{90.7} \\
                 \midrule
                 \multirow{4}{*}{TIM-GD} & \multirow{4}{*}{\{$\textbf{W} $\}}& $\mathrm{CE}$ & 60.7 & 79.4 & 68.4 & 84.3 & 69.6 & 86.3 \\
                 & & $\mathrm{CE} + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & 35.3 & 79.2 & 45.9 & 80.6 & 46.1 & 85.9 \\
                 & & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q})$ & 66.1 & 81.3 & 73.4 & 86.0 & 73.9 & 88.0 \\
                 & & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q}) + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & \textbf{73.9} & \textbf{85.0} & \textbf{79.9} & \textbf{88.5} & \textbf{82.2} & \textbf{90.8} \\
                 \midrule
                 \multirow{4}{*}{TIM-GD} & \multirow{4}{*}{$\{\boldsymbol{\phi}, \textbf{W}\}$}& $\mathrm{CE}$ & 60.8 & 81.6 & 65.7 & 83.5 & 68.7 & 87.7\\
                 & & $\mathrm{CE} + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & 62.7 & 81.9 & 66.9 & 82.8 & 72.6 & 89.0 \\
                 & & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q})$ & 62.3 & 82.7 & 68.3 & 85.4 & 70.7 & 88.8 \\
& & $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q}) + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & \textbf{67.2} & \textbf{84.7} & \textbf{73.0} & \textbf{86.8} & \textbf{76.7} & \textbf{90.5} \\
                 \bottomrule
            \end{tabular}}
            \label{tab:ablation_effect_terms}
        \end{table}


        \textbf{Fine-tuning the whole network vs only the classifier weights: }
            While our TIM-GD and TIM-ADM optimize w.r.t $\textbf{W}$ and keep base-trained encoder $f_{\boldsymbol{\phi}}$ fixed at inference, the authors of \cite{dhillon2019baseline} fine-tuned the whole network $\{\textbf{W}, \boldsymbol{\phi}\}$ when performing their transductive entropy minimization. To assess both approaches, we add to \autoref{tab:ablation_effect_terms} a variant of TIM-GD, in which we fine-tune the whole network $\{\textbf{W}, \boldsymbol{\phi}\}$, by using the same optimization procedure as in \cite{dhillon2019baseline}. We found that, besides being much slower, fine-tuning the whole network for our objective in Eq. \ref{eq:tim_objective} degrades the performances, as also conveyed by the convergence plots in \autoref{fig:convergence_methods}. 
Interestingly, when fine-tuning the whole network $\{\textbf{W}, \boldsymbol{\phi}\}$,  the absence of $\widehat{\mathcal{H}}(Y_\mathcal{Q})$ in the entropy-based loss $\mathrm{CE} + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ does not cause the same drastic drop in performance as observed earlier when optimizing with respect to $\textbf{W}$ only. We hypothesize that the network's intrinsic regularization (such as batch normalizations) and the use of small learning rates, as prescribed by \cite{dhillon2019baseline}, help the optimization process, preventing the predictions from approaching the vertices of the simplex, where entropy's gradients diverge.






    \subsection{Inference run-times}
        Transductive methods are generally slower at inference than their inductive counterparts, with run-times that are, typically, several orders of magnitude larger. In \autoref{tab:runtimes}, we measure the average adaptation time per few-shot task, defined as the time required by each method to build the final classifier, for a 5-shot 5-way task on \textit{mini}-ImageNet using the WRN28-10 network. Table \ref{tab:runtimes} conveys that our ADM optimization gains one order of magnitude in run-time over our gradient-based method, and more than two orders of magnitude in comparison to \cite{dhillon2019baseline}, which fine-tunes the whole network. Note that TIM-ADM still remains slower than the inductive baseline. Our methods were run on the same GTX 1080 Ti GPU, while the run-time of \cite{dhillon2019baseline} is directly reported from the paper. 



\begin{figure}[t]
            \centering
            \includegraphics[scale=0.4]{figures/runtimes/mini.pdf}
            \caption{Convergence plots for our methods on \textit{mini}-ImageNet with a ResNet-18. Solid lines are averages, while shadows are 95\% confidence intervals. Time is in logarithmic scale. \textbf{Left:} Evolution of the test accuracy during transductive inference. \textbf{Right:} Evolution of the mutual information between query features and predictions $\widehat{\mathcal{I}}(X_\mathcal{Q};Y_\mathcal{Q})$, computed as in Eq. (\ref{prior-aware-MI}), with $\alpha=1$.}
            \label{fig:convergence_methods}
        \end{figure}
        
        \begin{table}[t]
            \centering
                \begin{tabular}{lccc}
                    \multicolumn{4}{c}{\textbf{Run-times}} \\
                    \toprule
                    Method & Parameters & Transductive & Inference/task (s) \\
                    \toprule
                    SimpleShot \cite{simpleshot}& $\{\textbf{W}\}$ & \xmark & $9.0 \times 10^{-3}$ \\
                    \midrule
\rowcolor{Gray} TIM-ADM & $\{\textbf{W}\}$ & & $1.2 \times 10^{-1}$\\
                    \rowcolor{Gray} TIM-GD & $\{\textbf{W}\}$ & & $2.2 \times 10^{+0}$\\
                    Ent-min \cite{dhillon2019baseline} & $\{\boldsymbol{\phi}, \textbf{W}\}$ & \multirow{-4}{*}{\cmark} & $2.1 \times 10^{+1}$ \\
                    \bottomrule
                \end{tabular}
            \caption{Inference run-time per few-shot task for a 5-shot 5-way task on mini-ImageNet with a WRN28-10 backbone.}
            \label{tab:runtimes}
        \end{table}

 
\section{Conclusion and future work}

Our TIM inference establishes new state-of-the-art results on the standard few-shot benchmarks, as well as in more challenging scenarios, with larger numbers of classes and domain shifts. We used feature extractors based on a simple base-class training with the standard cross-entropy loss, without resorting to the complex meta-training schemes that are often used and advocated in the recent few-shot literature. TIM is modular: it could be plugged on top of any feature extractor and base training, regardless of how the training was conducted. Therefore, while we do not claim that the very challenging few-shot problem is solved, we believe that our model-agnostic TIM inference should be used as a strong baseline for future few-shot learning research. In future work, we target on giving a more theoretical ground for our proposed mutual-information objective, and on exploring further generalizations of the objective, e.g., via embedding domain-knowledge priors. Specifically, one of our theoretical goals will be to connect TIM's objective to the classifier's empirical risk on the query set, showing that the former could be viewed as a surrogate for the latter.

\section{Acknowledgements}
This research was supported by the National Science and Engineering Research Council of Canada (NSERC), via its Discovery Grant program.


 
 \section*{Broader impact}

    Due to the simplicity and efficiency of our method, we lower the barrier of entry to few-shot learning. In turn, we think that it will make a wider breadth of real-world applications tractable. The impact (positive or negative) on society is similar to that of any other few-shot method: being only a tool, its impact is entirely dependent on the final applications, and on the intentions of the people and institutions deploying it.
    
    In our strive towards finding simple and efficient formulations -- for instance, we stick to a standard cross-entropy, which not only eases implementation, but also avoid the huge memory consumption of more complex methods -- we believe our method can enable and empower persons and communities that are unable to afford the costly resources and infrastructures required. This may help level the playing field with larger and better funded entities. For instance, to be adapted to a new task, our TIM-ADM method requires a little more than a recent smartphone computational power. This could spawn a lot of fresh and new applications on edge devices, closer to the end-users, in real-time. \clearpage
\bibliographystyle{plain}
\bibliography{main}



\clearpage
\appendix
\section{Proofs}\label{sec:proofs}

    \textbf{Proof of Proposition \ref{prop:mi_adm}}
        \begin{proof}
            Let us start from the initial optimization problem:
            \begin{align}\label{eq:adm_proof_1}
	            \min_{\mathbf{W}}\quad & \sum_{k=1}^K \widehat{p_{k}} \log \widehat{p_{k}} - \frac{\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K p_{ik} \log p_{ik} - \frac{\lambda }{|\mathcal{S}|}\sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \log p_{ik}
            \end{align}
            We can reformulate problem (\ref{eq:adm_proof_1}) using the ADM approach, i.e., by introducing auxiliary variables $\q=[q_{ik}] \in \mathbb{R}^{|\mathcal{Q}| \times K}$ and enforcing 
            equality constraint $\q = \p$, with $\p=[p_{ik}] \in \mathbb{R}^{|\mathcal{Q}| \times K}$, in addition to pointwise simplex constraints: 
	        \begin{align}\label{eq:adm_proof_2}
	            \min_{\mathbf{W}, \q}\quad & \sum_{k=1}^K \widehat{q_{k}} \log \widehat{q_{k}} - \frac{\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik} \log p_{ik} - \frac{\lambda }{|\mathcal{S}|}\sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \log p_{ik} \nonumber \\
	            \text{s.t.}\quad & q_{ik}=p_{ik}, \quad i \in \mathcal{Q}, \quad k \in \{1, \dots, K\} \nonumber \\ 
	            & \sum_{k=1}^K q_{ik}=1, \quad i \in \mathcal{Q} \nonumber \\
	            & q_{ik} \geq 0, \quad i \in \mathcal{Q}, \quad k \in \{1,\dots,K\}
	         \end{align}
We can slove constrained problem (\ref{eq:adm_proof_2}) with a penalty-based approach, which encourages auxiliary pointwise predictions $\q_{i}=[q_{i1}, \dots, q_{iK}]$ to be close to our model's posteriors $\p_{i}=[p_{i1}, \dots, p_{iK}]$. To add a penalty encouraging equality constraints $\q_i = \p_i$, we use the Kullback–Leibler (KL) divergence, which is given by:
	         \[\mathcal{D}_{\mbox{\tiny KL}}(\q_i||\p_i) = \sum_{k=1}^K q_{ik} \log \frac{q_{ik}}{p_{ik}}\]
	         Thus, our constrained optimization problem becomes:
	         \begin{align}\label{eq:adm_proof_3}
	            \min_{\mathbf{W}, \q}\quad & \sum_{k=1}^K \widehat{q_{k}} \log \widehat{q_{k}} - \frac{\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik} \log p_{ik} - \frac{\lambda }{|\mathcal{S}|}\sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \log p_{ik} + \frac{1}{|\mathcal{Q}|}\sum_{i \in \mathcal{Q}} \mathcal{D}_{\mbox{\tiny KL}}(\q_i||\p_i) \nonumber \\
	            \text{s.t.}\quad & \sum_{k=1}^K q_{ik}=1, \quad i \in \mathcal{Q} \nonumber \\
	            & q_{ik} \geq 0, \quad i \in \mathcal{Q}, \quad k \in \{1,\dots,K\}
	         \end{align}
        \end{proof}
        
    
    
	\textbf{Proof of Proposition \ref{prop:mi_adm_solution}}
    	\begin{proof}
            Recall that we consider a softmax classifier over distances to weights $\mathbf{W}=\{\w_1, \dots, \w_K\}$. To simplify the notations, we will omit the dependence upon $\boldsymbol{\phi}$ in what follows, and write ${\z_i=\frac{f_{\boldsymbol{\phi}}(\x_i)}{\norm{f_{\boldsymbol{\phi}}(\x_i)}}}$, such that:
            \begin{align}
                p_{ik} = \frac{e^{-\frac{\tau}{2}\norm{\z_i - \w_k}^2}}{\sum_{j=1}^K e^{-\frac{\tau}{2}\norm{\z_i - \w_j}^2}}
            \end{align}
            Without loss of generality, we use $\tau=1$ in what follows. Plugging the expression of $p_{ik}$ into Eq. \eqref{eq:mi_adm}, and grouping terms together, we get:
            \begin{align}\label{eq:update_proof_1}
                (\ref{eq:mi_adm}) =& \sum_{k=1}^K \widehat{q_k} \log \widehat{q_k} - \frac{1+\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik} \log p_{ik}  - \frac{\lambda}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} \sum_{k=1}^K y_{ik} \log p_{ik} + \frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}}\sum_{k=1}^K q_{ik} \log q_{ik} \nonumber \\
                \begin{split}
                    =& \sum_{k=1}^K \widehat{q_k} \log \widehat{q_k} \\
                    & +\frac{1+\alpha}{2|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik} \norm{\z_i - \w_k}^2 + \frac{1+\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \log \left (\sum_{j=1}^K e^{-\frac{1}{2}\norm{\z_i - \w_j}^2} \right ) \\
                    & + \frac{\lambda}{2|\mathcal{S}|} \sum_{i \in \mathcal{S}}\sum_{k=1}^K y_{ik} \norm{\z_i - \w_k}^2 + \frac{\lambda}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} \log \left (\sum_{j=1}^K e^{-\frac{1}{2}\norm{\z_i - \w_j}^2} \right ) \\
                    & + \frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \sum_{k=1}^K q_{ik} \log q_{ik}
                \end{split}
            \end{align}
            Now, we can solve our problem approximately by alternating two sub-steps: 
            one sub-step optimizes w.r.t classifier weights $\mathbf{W}$ while auxiliary variables $\q$ are fixed; another sub-step fixes $\mathbf{W}$ and update $\q$.
            \begin{itemize}
                \item $\mathbf{W}$-update: Omitting the terms that do not involve $\mathbf{W}$, Eq. \eqref{eq:update_proof_1} reads:
    		        \begin{align}
    		            \begin{split}\label{eq:ce_reg_1}
    		            &\underbrace{\frac{\lambda}{2|\mathcal{S}|} \sum_{i \in \mathcal{S}} y_{ik} \norm{\z_i - \w_k}^2 + \frac{1+\alpha}{2|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} q_{ik} \norm{\z_i - \w_k}^2}_{\mathcal{C}: \text{convex}} \\
    		            +& \underbrace{\frac{\lambda}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} \log \left ( \sum_{j=1}^K e^{-\frac{1}{2}\norm{\z_i - \w_j}^2} \right ) + \frac{1+\alpha}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} \log \left ( \sum_{j=1}^K e^{-\frac{1}{2}\norm{\z_i - \w_j}^2} \right )}_{\Bar{\mathcal{C}}: \text{non-convex}}
    		            \end{split}
    		        \end{align}
    		        One can notice that objective (\ref{eq:update_proof_1}) is not convex w.r.t $\w_k$. Actually, it can be split into convex and non-convex parts as in Eq. \eqref{eq:ce_reg_1}. Thus, we cannot simply set the gradients to 0 to get the optimal $\w_k$. The non-convex part can be linearized at current solution $\w_k^{(t)}$ as follows:
    		        \begin{align}
    		            \Bar{\mathcal{C}}(\w_k) &\approx
    		            \Bar{\mathcal{C}}(\w_k^{(t)}) + \frac{\partial \Bar{\mathcal{C}}}{\partial \w_k} (\w_k^{(t)})^T (\w_k - \w_k^{(t)}) \nonumber \\
    		            & \ceq \frac{\lambda}{|\mathcal{S}|}\sum_{i \in \mathcal{S}} p_{ik}^{(t)} (\z_i - \w_k^{(t)})^T \w_k + \frac{1+\alpha}{|\mathcal{Q}|}\sum_{i \in \mathcal{Q}} p_{ik}^{(t)} (\z_i - \w_k^{(t)})^T \w_k
    		        \end{align}
    		       Where $\ceq$ stands for "equal, up to an additive constant". By adding this linear term to the convex part $\mathcal{C}$, we can obtain a strictly convex objective in $\w_k$, whose gradients w.r.t $\w_k$ read:
    		        \begin{align}
    		            \frac{\partial (\ref{eq:ce_reg_1})}{\partial \w_k} \approx & \frac{\lambda}{|\mathcal{S}|} [\sum_{i \in \mathcal{S}} y_{ik}(\z_i - \w_k) + p_{ik}^{(t)}(\z_i - \w_k^{(t)})] \ + \nonumber \\ 
    		            & \frac{1+\alpha}{|\mathcal{Q}|} [\sum_{i \in \mathcal{Q}} q_{ik}(\z_i - \w_k) + p_{ik}^{(t)}(\z_i - \w_k^{(t)})] 
    		        \end{align}
    		        Note that the approximation we do here is similar in spirit to concave-convex procedures, which are well known in optimization. Concave-convex techniques proceed as follows: for a function in the form of a sum of a concave term and a convex term, the concave part is replaced by its first-order approximation, while the convex part is kept as is. The difference here is that the part that we linearize in Eq. \eqref{eq:ce_reg_1} is not concave. 
    		        Setting the gradients above to 0 yields the optimal solution for the approximate objective. 
    		        
    		        Another solution to obtain a strictly convex objective would have been to discard the non-convex part $\Bar{\mathcal{C}}$. Very interestingly, in this case, one would recover $\w_k$ updates that would very much resemble the prototype updates of the K-means clustering algorithm (slightly modified to take into account the fact that for support points in $\mathcal{S}$ have labels). Note that the link between regularized K-means and mutual information maximization has been extensively explored in \cite{jabi}. Of course, in this case, the approximation is not as good as the first-order approximation above, and we found that omitting the non-convex part might decrease the performances significantly. 

                \item $\q$-update: With weights $\mathbf W$ fixed, the objective is convex w.r.t auxiliary variables $\q_i$ (sum of linear and convex functions) and the simplex 
                    constraints are affine. Therefore, one can minimize this constrained convex problem for each $\q_i$ by solving the
                    Karush-Kuhn-Tucker (KKT) conditions\footnote{Note that strong duality holds since the objective is convex and the simplex constraints are affine. This
                    means that the solutions of the (KKT) conditions minimize the objective.}. The KKT conditions yield closed-form solutions for both
                    primal variable $\q_i$ and the dual variable (Lagrange multiplier) corresponding to simplex constraint $\sum_{j=1}^K q_{ij}=1$.
                    Interestingly, the negative entropy of auxiliary variables, i.e., $\sum_{k=1}^K q_{ik} \log q_{ik}$,  which appears in the penalty term, handles implicitly non-negativity constraints $\q_i \geq 0$. In fact, this negative entropy acts as a barrier function, restricting the domain of each $\q_i$ to non-negative values, which avoids extra dual variables and Lagrangian-dual inner iterations for constraints $\q_i \geq 0$. As we will see, the closed-form solutions of the KKT conditions satisfy these non-negativity constraints, without explicitly imposing them. In addition to non-negativity, for each point $i$, we need to handle probability simplex constraints $\sum_{k=1}^K q_{ik}=1$. Let $\gamma_i \in \mathbb{R}$ denote the Lagrangian multiplier corresponding to this constraint. The KKT conditions correspond to setting the following gradient of the Lagrangian function to zero, while enforcing the simplex constraints:   
\begin{align}
    		            \frac{\partial (\ref{eq:mi_adm})}{\partial q_{ik}} &= - \frac{1+\alpha}{|\mathcal{Q}|} \log p_{ik} + \frac{1}{|\mathcal{Q}|} (\log \widehat{q_k} + 1) + \frac{1}{|\mathcal{Q}|}(\log q_{ik} + 1) + \gamma_i \\
    		            &= \frac{1}{|\mathcal{Q}|} \left (\log (\frac{q_{ik} \widehat{q_k}}{p_{ik}^{1+\alpha}}) + 2 \right ) + \gamma_i
    		        \end{align}
This yields: 
    		        \begin{align}
    		        \label{first-experession-qi}
    		            q_{ik} = \frac{p_{ik}^{1+\alpha}}{\widehat{q_k}}e^{-(\gamma_i |\mathcal{Q}|+2)}
    		        \end{align}
    		        Applying simplex constraint $\sum_{j=1}^K q_{ij}=1$ to \eqref{first-experession-qi}, Lagrange multiplier $\gamma_i$ verifies:
    		        \begin{align}
    		        \label{condition_wrt_gamma}
e^{-(\gamma_i |\mathcal{Q}|+2)} = \frac{1}{\displaystyle \sum_{j=1}^K \frac{p_{ij}^{1+\alpha}}{\widehat{q_j}}}
    		        \end{align}
    		        Hence, plugging \eqref{condition_wrt_gamma} in \eqref{first-experession-qi} yields:
    		        \begin{align}
    		        \label{second-experession-qi}
    		            q_{ik} = \frac{\displaystyle \frac{p_{ik}^{1+\alpha}}{\widehat{q_k}}}{\displaystyle \sum_{j=1}^K \frac{p_{ij}^{1+\alpha}}{\widehat{q_j}}}
    		        \end{align}
    		        Using the definition of $\widehat{q_k}$, we can decouple this equation:
    		        \begin{align}\label{eq:update_proof_2}
    		            \widehat{q_k} = \frac{1}{|\mathcal{Q}|} \sum_{i \in \mathcal{Q}} q_{ik} \propto \sum_{i \in \mathcal{Q}} \frac{p_{ik}^{1+\alpha}}{\widehat{q_k}}
    		        \end{align}
    		        which implies:
    		        \begin{align}
    		            \widehat{q_k} \propto \left (\sum_{i \in \mathcal{Q}} p_{ik}^{1+\alpha} \right )^{1/2}
    		        \end{align}
    		        Plugging this back in Eq. \eqref{second-experession-qi}, we get:
    		        \begin{equation}
    		        \label{third-experession-qi}
    		            q_{ik} \propto \frac{p_{ik}^{1+\alpha}}{\left (\displaystyle \sum_{i \in \mathcal{Q}} p_{ik}^{1+\alpha} \right )^{1/2}}
    		        \end{equation}
    		        Notice that $q_{ik} \geq 0$, hence the solution fulfils the positivity constraint of the original problem.
            \end{itemize}
        \end{proof}

\section{TIM algorithms}\label{sec:algorithms}
    
    In this section, we provide the pseudo-code for TIM's inference stage (both TIM-GD and TIM-ADM).
        \begin{algorithm}
            \SetKwInOut{Input}{Input}\SetKwInOut{Output}{output}
            \SetAlgoLined
            \Input{ Pre-trained encoder $f_{\bm{\phi}}$, Task $\{\mathcal{S}, \mathcal{Q}\}$, \# iterations $iter$, Temperature $\tau$, Weights $\{\lambda, \alpha\}$} \vspace{0.5em}
            $\z_i \leftarrow \frac{f_{\bm{\phi}}(\x_i)}{\norm{f_{\bm{\phi}}(\x_i)}_2}$ , $i \in \mathcal{S} \cup \mathcal{Q}$ \\
            $\w_k \leftarrow \frac{\sum_{i \in \mathcal{S}}y_{ik} \z_i}{\sum_{i \in \mathcal{S}}y_{ik}}$ , $k \in \{1, \dots, K\}$ \\
            \For{$i\leftarrow 0$ \KwTo $iter$}{
                $p_{ik} \leftarrow \exp\left(-\frac{\tau}{2}\norm{\w_k - \z_i}^2\right)$ , $i \in \mathcal{S} \cup \mathcal{Q}$ \\
                $p_{ik} \leftarrow \frac{p_{ik}}{\sum_{l=1}^K p_{il}}$ \\ 
                $q_{ik} \leftarrow \frac{p_{ik}^{1+\alpha}}{\left ( \sum_{i \in \mathcal{Q}} p_{ik}^{1+\alpha} \right )^{1/2}}$ , $i \in \mathcal{Q}$\\
                $q_{ik} \leftarrow \frac{q_{ik}}{\sum_{l=1}^K q_{il}}$ \\
      	        $\w_k \leftarrow \frac{\displaystyle\frac{\lambda}{1+\alpha} \displaystyle \sum\limits_{i \in \mathcal{S}} \left ( y_{ik} \,\z_i + p_{ik} (\w_k - \z_i) \right ) +  \frac{|\mathcal{S}|}{|\mathcal{Q}|}  \displaystyle\sum\limits_{i \in \mathcal{Q}} \left ( q_{ik} \z_i + p_{ik} (\w_k - \z_i) \right )}{\displaystyle\frac{\lambda}{1+\alpha} \displaystyle\sum\limits_{i \in \mathcal{S}} y_{ik} +  \frac{|\mathcal{S}|}{|\mathcal{Q}|} \sum\limits_{i \in \mathcal{Q}} q_{ik}}$
                
            }
            \KwResult{Query predictions $\hat{y}_{i} = \argmax_{k} p_{ik}$ , $i \in \mathcal{Q}$}
            \caption{TIM-ADM}
            \end{algorithm}
            \begin{algorithm}
            \SetKwInOut{Input}{Input}\SetKwInOut{Output}{output}
            \SetAlgoLined
            \Input{ Pre-trained encoder $f_{\bm{\phi}}$, Task $\{\mathcal{S}, \mathcal{Q}\}$, \# iterations $iter$, Temperature $\tau$, Weights $\{\lambda, \alpha\}$, Learning rate $\gamma$} \vspace{0.5em}
            $\z_i \leftarrow \frac{f_{\bm{\phi}}(\x_i)}{\norm{f_{\bm{\phi}}(\x_i)}_2}$ , $i \in \mathcal{S} \cup \mathcal{Q}$ \\
            $\w_k \leftarrow \frac{\sum_{i \in \mathcal{S}}y_{ik} \z_i}{\sum_{i \in \mathcal{S}}y_{ik} }$ , $k \in \{1, \dots, K\}$ \\
            \For{$i\leftarrow 0$ \KwTo $iter$}{
                $p_{ik} \leftarrow \exp\left(-\frac{\tau}{2}\norm{\w_k - \z_i}^2\right)$ \\
                $p_{ik} \leftarrow \frac{p_{ik}}{\sum_{l=1}^K p_{il}}$ \\ 
      	        $\w_k \leftarrow \w_k - \gamma \nabla_{\w_k} \mathcal{L}_{\textrm{TIM}}$
                
            }
            \KwResult{Query predictions $\hat{y}_{i} = \argmax_{k} p_{ik}$ , $i \in \mathcal{Q}$}
            \caption{TIM-GD}
            \end{algorithm}

\section{Summary figure}    
            	    
    We hereby provide a summarizing figure of the training and inference stages used in TIM.
	\begin{figure}[H]
        \centering
        \includegraphics[scale=0.22]{figures/few-shot_figure.png}
        \caption{Outline of TIM framework (best viewed in color). First, the feature extractor is trained with the standard cross-entropy on the base classes. Then, it is kept fixed at inference and weights $\mathbf{W}$ are optimized for by minimizing the cross-entropy on the support set $\mathcal{S}$, while maximizing the mutual information between features and predictions on the query set $\mathcal{Q}$.}
        \label{fig:pipeline}
    \end{figure}


\section{Details of ADM ablation}\label{sec:details_adm_ablation}

In \autoref{tab:adm_ablation_details}, we provide the $\mathbf{W}$ and $\q$ updates for each configuration of the TIM-ADM ablation study, whose results were presented in \autoref{tab:ablation_effect_terms}. The proof for each of these updates is very similar to the proof of Proposition \ref{prop:mi_adm_solution} detailed in \autoref{sec:proofs}. Therefore, we do not detail it here.  

\begin{table}[H]
    \centering
    \begin{tabular}{ccc}
         \textbf{Loss} & $\w_k$ \textbf{update} & $q_{ik}$ \textbf{update} \\
         \toprule
         $\mathrm{CE}$ & $\frac{\smashoperator[r]\sum_{i \in \mathcal{S}} y_{ik} \z_i }{\smashoperator[r]\sum_{i \in \mathcal{S}} y_{ik}}$ & N/A \\
         \midrule
$\mathrm{CE} + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & - & $\propto p_{ik}^{1+\alpha}$ \\
         \midrule
         $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q})$ & - & $\propto \frac{\smashoperator[r]p_{ik}}{\left ( \smashoperator[r]\sum_{i \in \mathcal{Q}}p_{ik} \right )^{1/2}}$ \\
         \midrule
         $\mathrm{CE} - \widehat{\mathcal{H}}(Y_\mathcal{Q}) + \widehat{\mathcal{H}}(Y_\mathcal{Q}|X_\mathcal{Q})$ & - & - \\
         \bottomrule
    \end{tabular}
    \caption{The $\bf W$ and $\q$-updates for each case of the ablation study. "-" refers to the updates in Proposition \ref{prop:mi_adm_solution}. "NA" refers to non-applicable.}
    \label{tab:adm_ablation_details}
\end{table}



%
 
\end{document}