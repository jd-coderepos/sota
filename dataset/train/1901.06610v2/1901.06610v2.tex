\documentclass[runningheads]{llncs}









\usepackage{graphicx} \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{flushend}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[section]{placeins}

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath, tabularx}

\begin{document}

\title{Hierarchical Attentional Hybrid Neural Networks for Document Classification}

\author{Jader Abreu \and
 Luis Fred\thanks{Authors contributed equally and are both first authors.} \and
David Mac\^{e}do \and
Cleber Zanchettin
}
\authorrunning{Abreu, J., Fred, L., et al.}

\institute{\textit{Centro} \textit{de} \textit{Inform\'{a}tica \\
\textit{Universidade} \textit{Federal} \textit{de} \textit{Pernambuco}\\
50.740-560, Recife, PE, Brazil}\\
\email{\{jaoa,lfgs,dlm,cz\}@cin.ufpe.br}}




\maketitle  

\begin{abstract}

Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classification tasks. The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical representation. The proposed method in this paper improves the results of the current attention-based approaches.

\keywords{Text classification \and Attention mechanisms \and Document classification \and Convolutional Neural Networks.}

\end{abstract}





\section{Introduction} 
\vspace{-2mm}
Text classification is one of the most classical and important tasks in the machine learning field. The document classification, which is essential to organize documents for retrieval, analysis, and curation, is traditionally performed by classifiers such as Support Vector Machines or Random Forests. As in different areas, the deep learning methods are presenting a performance quite superior to traditional approaches in this field \cite{c13}. Deep learning is also playing a central role in Natural Language Processing (NLP) through learned word vector representations. It aims to represent words in terms of fixed-length, continuous and dense feature vectors, capturing semantic word relations: similar words are close to each other in the vector space. 

In most NLP tasks for document classification, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. Much of these approaches do not select qualitative or informative words and sentences since some words are more informative than others in a document. Moreover, these models are frequently based on recurrent neural networks only \cite{c1}. Since CNN has leveraged strong performance on deep learning models by extracting more abundant features and reducing the number of parameters, we guess it not only improves computational performance but also yields better generalization on neural models for document classification.

A recent trend in NLP is to use attentional mechanisms to modeling information dependencies without regard to their distance between words in the input sequences. In \cite{c1} is proposed a hierarchical neural architecture for document classification, which employs attentional mechanisms, trying to mirror the hierarchical structure of the document. The intuition underlying the model is that not all parts of a text are equally relevant to represent it. Further, determining the relevant sections involves modeling the interactions and importance among the words and not just their presence in the text.



In this paper, we propose a new approach for document classification based on CNN, GRU \cite{c12} hidden units and attentional mechanisms to improve the model performance by selectively focusing the network on essential parts of the text sentences during the model training. Inspired by \cite{c1}, we have used the hierarchical concept to better representation of document structure. We call our model as Hierarchical Attentional Hybrid Neural Networks (HAHNN). We also used temporal convolutions \cite{c6}, which give us more flexible receptive field sizes. We evaluate the proposed approach comparing its results with state-of-the-art models and the model shows an improved accuracy.\-8mm]



\vspace{-1mm}

where \textit{d} is the dilatation factor, \textit{k} is the filter size, and  accounts for the past information direction. Dilation is thus equivalent to introducing a fixed step between every two adjacent filter maps. When \textit{d} = 1, a dilated convolution reduces to a regular convolution. The use of larger dilation enables an output at the top level to represent a wider range of inputs, expanding the receptive field. \
        \it{x_{it}} = W_ew_{it}, t \in [1,T], 
    
      \overrightarrow{h_{it}} = \overrightarrow{GRU}(\it{x_{it}}), t \in [1,T],
    
    \overleftarrow{h_{it}} = \overleftarrow{GRU}(\it{x_{it}}), t \in [T,1].
-6mm]



\noindent
\begin{tabularx}{\linewidth}{@{}XX@{}}
    
    &
    
\end{tabularx}

\vspace{-5mm}



\vspace{-2mm}

The model measures the importance of a word as the similarity of  with a word level context vector  and learns a normalized importance weight  through a softmax function. After that, the architecture computes the sentence vector   as a weighted sum of the word annotations based on the weights. The word context vector  is randomly initialized and jointly learned during the training process.

Given the sentence vectors , and the document vector, the sentence attention is obtained as:\
      \overrightarrow{h_{it}} = \overrightarrow{GRU}(\it{s_{i}}), i \in [1,L],
    
      \overleftarrow{h_{it}} = \overleftarrow{GRU}(\it{s_{i}}), i \in [L,1].
    -7mm]



 
\noindent
\begin{tabularx}{\linewidth}{XX}
    
    &
    
    
\end{tabularx}

\vspace{-5mm}

 

\vspace{-1mm}

In the above equation, \textit{v} is the document vector that summarizes all the information of sentences in a document. Similarly, the sentence level context vector  can be randomly initialized and jointly learned during the training process. The output of the sentence attention layer feeds a fully connected softmax layer. It gives us a probability distribution over the classes. The proposed method is openly available in the github repository \footnote{https://github.com/luisfredgs/cnn-hierarchical-network-for-document-classification}.\-12mm]

\begin{table}[!h]
\caption{Results in classification accuracies.}
\centering
\begin{tabular}{@{}p{0.28\textwidth}*{2}{p{\dimexpr0.37\textwidth-2\tabcolsep\relax}}@{}}
\toprule
\multicolumn{1}{c}{Method} & \multicolumn{2}{c}{Accuracy on test set}  \\
\cmidrule(r{4pt}){2-3} 
& Yelp 2018 (five classes) & IMDb (two classes)  \\
\midrule
VDNN \cite{c31} & 62.14 & 79.47 \\
HN-ATT  \cite{c1} & 72.73 &  89.02 \\
CNN  \cite{c13} & 71.81 &  91.34 \\
Our model with CNN & \textbf{73.28} & 92.26\\
Our model with TCN & 72.63 & \textbf{95.17} \\
\bottomrule
\end{tabular}
\label{tabela1}
\end{table}

\vspace{-4mm}

Table~\ref{tabela1} shows the experiment results comparing our results with related works. Note that HN-ATT \cite{c1} obtained an accuracy of 72,73\% in the Yelp test set, whereas the proposed model obtained an accuracy of 73,28\%. Our results also outperformed CNN \cite{c1} and VDNN \cite{c31}. We can see an improvement of the results in Yelp with our approach using CNN and varying window sizes in filters. The model also performs better in the results with IMDb using both CNN and TCN.

\subsection{Attention Weights Visualizations}

To validate the model performance in select informative words and sentences, we present the visualizations of attention weights in Figure \ref{fig:fig2}. There is an example of the attention visualizations for a positive and negative class in test reviews. Every line is a sentence. Blue color denotes the sentence weight, and red denotes the word weight in determining the sentence meaning. There is a greater focus on more important features despite some exceptions. For example, the word ``loving" and ``amazed" in Figure \ref{fig:fig2} (a) and ``disappointment" in Figure \ref{fig:fig2} (b).

\begin{figure*}[!ht]
  \center
  \begin{subfigure}{\linewidth}
    \center
      \includegraphics[scale=0.60]{vizualizations/attention/positive_1.PNG}
      \caption{A positive example of visualization of a strong word in the sentence.}  
  \end{subfigure}\par\medskip
  
  \begin{subfigure}{\linewidth}
   \center
   \includegraphics[scale=0.60]{vizualizations/attention/negative_1.PNG}
   \caption{A negative example of visualization of a strong word in the sentence.}
  \end{subfigure}\par\medskip
\vspace{-2mm}
  \caption{Visualization of attention weights computed by the proposed model}
  \label{fig:fig2}
\end{figure*}

\vspace{-4mm}

Occasionally, we have found issues in some sentences, where fewer important words are getting higher importance. For example, in Figure \ref{fig:fig2} (a) notes that the word ``translate" has received high importance even though it represents a neutral word. These drawbacks will be taken into account in future works.\-8mm] 







\begin{thebibliography}{9}
\bibitem{c4} BAHDANAU, Dzmitry; CHO, Kyunghyun; BENGIO, Yoshua. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
\bibitem{c6} BAI, Shaojie; KOLTER, J. Zico; KOLTUN, Vladlen. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.
\bibitem{c3} BOJANOWSKI, Piotr et al. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.
\bibitem{c12}CHO, Kyunghyun et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
\bibitem{c13}KIM, Yoon. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.
\bibitem{c1} YANG, Zichao et al. Hierarchical attention networks for document classification. In: Conf. North Am. Chapter of the Assoc. for Comp. Ling. 2016. p.1480-1489, San Diego, CA, USA.
\bibitem{c31} Conneau, Alexis, et al. "Very deep convolutional networks for text classification." arXiv preprint arXiv:1606.01781 (2016).
\end{thebibliography}

\end{document}
