\begin{table*}[!htbp]   
	\centering
	\setlength{\tabcolsep}{3mm}{  
	\begin{spacing}{1.1}
			\scalebox{0.75}{
				\begin{tabular}{|p{3cm}<{\centering}|p{1cm}<{\centering}||ccc||ccc||ccc|}
					\hline
					\multirow{2}{*}{Method} & \multirow{2}{*}{Data}           & \multicolumn{3}{c||}{AP\textsubscript{3D}(IoU=0.3)}                                 & \multicolumn{3}{c||}{AP\textsubscript{3D}(IoU=0.5)}                             & \multicolumn{3}{c|}{AP\textsubscript{3D}(IoU=0.7)}                  \\ \cline{3-11} 
				                                         	&                                               & \multicolumn{1}{c|}{Easy}     & \multicolumn{1}{c|}{Moderate}  & Hard               & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard                & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard           \\ \hline \hline
                      
					VeloFCN                 & LiDAR                                                   & /                             & /                              & /                      & 67.92                          & 57.57              & 52.56                     & 15.20                         & 13.66               & 15.98 \\
					\hline
					Mono3D                  & Mono                                                & 28.29                          &23.21                           &19.49               & 25.19                     & 18.20                         & 15.22               & 2.53                      & 2.31                          & 2.31           \\ 
					MF3D                    & Mono                                                &/                              &/                               &/                   & 47.88                     & 29.48                         & 26.44               & {10.53}                   & 5.69                          & 5.39           \\ 
					MonoGRNet               & Mono                                                & 72.17 &59.57 &46.08 &50.51 &36.97 &30.82 &13.88 &10.19 &7.62 \\
                    3DOP                    & Stereo                                                & 69.79                         &52.22                           &49.64               & 46.04                     & 34.63                         & 30.09               & 6.55                      & 5.07                          & 4.10           \\   \hline       
                    Ours (baseline)         & Mono                                            & 72.91                         & 55.72                          & 49.19              &48.34                      & 33.98                         & 28.67               & 13.77                     & 9.72                          & 9.29 \\ 
                
                    Ours                    & Stereo                                           & \textbf{78.26}                & \textbf{63.36}                 & \textbf{57.10}     &\textbf{59.51}             & \textbf{43.71}                & \textbf{37.99}      & \textbf{18.15}            & \textbf{14.26}                & \textbf{13.72} \\ \hline
				\end{tabular}  
			} 
			\end{spacing}
     }
	\caption{\textbf{3D detection performance.} Average Precision of 3D bounding boxes on KITTI~\cite{geiger2012kitti} validation set. The LiDAR based method VeloFCN~\cite{li2016} is listed for reference but not compared.
	}
	\label{tab:3dap} 
\end{table*}
   

\begin{table*}[!htbp]   
	\centering
	\setlength{\tabcolsep}{3mm}{  
	\begin{spacing}{1.1}
			\scalebox{0.75}{
				\begin{tabular}{|p{3cm}<{\centering}|p{1cm}<{\centering} ||ccc||ccc||ccc|}
					\hline
					\multirow{2}{*}{Method} & \multirow{2}{*}{Data}       & \multicolumn{3}{c||}{AP\textsubscript{BEV}(IoU=0.3)}                                 & \multicolumn{3}{c||}{AP\textsubscript{BEV}(IoU=0.5)}                             & \multicolumn{3}{c|}{AP\textsubscript{BEV}(IoU=0.7)}                  \\ \cline{3-11} 
				                                           &                                             & \multicolumn{1}{c|}{Easy}     & \multicolumn{1}{c|}{Moderate}  & Hard               & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard                & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard           \\ \hline \hline
                    VeloFCN                 & LiDAR                                   &  /                            & /                               & /                  & 79.68                    & 63.82                         & 62.80               & 40.14                     & 32.08                         & 30.47          \\
				    \hline
					Mono3D                  & Mono                                    & 32.76                         &25.15                           &23.65               & 30.50                     & 22.39                         & 19.16               & 5.22                      & 5.19                          & 4.13           \\ 
					MF3D                    & Mono                                     & /                             & /                              & /                  & 55.02                     & 36.73                         & 31.27               & 22.03                     & 13.63                         & 11.60           \\ 
					MonoGRNet               & Mono                                                &73.10 &60.66 &46.86 &54.21 &39.69 &33.06 &24.97 &19.44 &16.30 \\
                    3DOP                    & Stereo                                       & 71.41                         &57.78                           &51.91               & 55.04                     & 41.25                         & 34.55               & 12.63                     & 9.49                          & 7.59           \\          
                    \hline
                    Ours (baseline)         & Mono                                   & 74.18                         & 57.04                          & 50.17              & 52.72                     & 37.22                         & 32.16               & 21.91                     & 15.72                          & 14.32 \\
                   
                    Ours                    & Stereo                                  & \textbf{81.11}                & \textbf{65.25}                 & \textbf{58.15}     & \textbf{62.46}            & \textbf{45.99}                & \textbf{41.92}      & \textbf{29.22}            & \textbf{21.88}                 & \textbf{18.83} \\ \hline
				\end{tabular}  
			} 
			\end{spacing}
     }
	\caption{\textbf{BEV detection performance.} Average Precision of BEV bounding boxes on KITTI~\cite{geiger2012kitti} validation set. Different from typical 2D detection evaluation, the bounding boxes are orientated, \ie, not necessarily aligned on each axis.
	}
	\label{tab:bvap} 
\end{table*}

\begin{table*}[!htbp]   
	\centering
	\setlength{\tabcolsep}{3mm}{  
	\begin{spacing}{1.1}
			\scalebox{0.75}{
				\begin{tabular}{|p{3cm}<{\centering}|p{1cm}<{\centering} ||ccc||ccc||ccc|}
					\hline
					\multirow{2}{*}{Method} & \multirow{2}{*}{Data}       & \multicolumn{3}{c||}{AP\textsubscript{LOC}(\textless2m)}                                 & \multicolumn{3}{c||}{AP\textsubscript{LOC}(\textless1m)}                             & \multicolumn{3}{c|}{AP\textsubscript{LOC}(\textless0.5m)}                  \\ \cline{3-11} 
					                              &                                                        & \multicolumn{1}{c|}{Easy}     & \multicolumn{1}{c|}{Moderate}  & Hard               & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard                & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard           \\ \hline \hline
                      
					Mono3D                  & Mono                                    & 47.21                         &35.82                           &35.40                       & 17.74                     & 14.80                         & 14.05               & 4.78                      & 4.06                          & 4.03           \\ 
					MonoGRNet               & Mono                                                &83.09 &64.82 &55.79 &56.29 &42.29 &35.01 &28.38 &19.73 &18.06 \\
                    3DOP                    & Stereo                                       & 84.53                         &65.37                           &64.10                     & 60.84                     & 45.11                         & 40.35               & 27.53                     & 19.39                          & 17.64           \\          
                    \hline
                    Ours (baseline)         & Mono                                   & 75.50                         & 59.33                          & 52.92                      & 58.11                     & 41.05                         & 36.59               & 30.82                     & 21.25                          & 18.03 \\
                    Ours                        & Stereo                                  & \textbf{86.78}           & \textbf{72.17}              & \textbf{66.27}        & \textbf{69.15}         & \textbf{51.68}            & \textbf{47.68}  & \textbf{37.36}        & \textbf{27.42}             & \textbf{24.56} \\ \hline
				\end{tabular}  
			} 
			\end{spacing}
     }
	\caption{\textbf{3D localization performance.} Average Location Precision of 3D bounding boxes under different distance thresholds on KITTI~\cite{geiger2012kitti} validation set.}
	\label{tab:3dloc} 
\end{table*}








\begin{figure*}[ht] 
	\centering
	\scriptsize
	\includegraphics[width=0.98\linewidth]{figs/compare/compare.JPG}


\caption{\textbf{Qualitative comparison.} Orange bounding boxes are detection results, while the green boxes are ground truths. For our main method, we also visualize the projected 3D bounding boxes in image, i.e., the first and forth rows. The lidar point clouds are visualized for reference but not used in both training and evaluation. It is shown that the triangulation learning method can reduce missed detections and improve the performance of depth prediction at distant regions.}
	\label{fig:visualize3d}
\end{figure*}




\section{Experiment}
We evaluate the proposed network on the challenging KITTI dataset~\cite{geiger2012kitti}, which contains 7481 training images and 7518 testing images with calibrated camera parameters. Detection is evaluated in three regimes: easy, moderate and hard, according to the occlusion and truncation levels. We use the train1/val1 split setup in the previous works~\cite{chen2016monocular,chen2017multiview}, where each set contains half of the images. Objects of class  are chosen for evaluation. Because the number of cars exceeds that of other objects by a significant margin, they are more suitable to assess a data-hungry deep-learning network. We compare our baseline network with state-of-the-art monocular 3D detectors, MF3D~\cite{xu2018multifusion},  Mono3D~\cite{chen2016monocular} and MonoGRNet~\cite{qin2019monogr}. For stereo input, we compare our network with 3DOP~\cite{chen20153dop}. 

\paragraph{Metrics.}
For 3D detection, we follow the official settings of KITTI benchmark to evaluate the 3D Average Precision (AP\textsubscript{3D}) at different 3D IoU thresholds. To evaluate the bird's eye view (BEV) detection performance, we use the BEV Average Precision (AP\textsubscript{BEV}). We also provide results for Average Location Precision (AP\textsubscript{LOC}), in which a ground truth object is recalled if there is a predicted 3D location within certain distance threshold. Note that we obtain the detection results of Mono3D and 3DOP from the authors so as to evaluate them with different IoU thresholds. The evaluation of MF3D follows their original paper.




\vspace{-0.2cm}
\subsection{3D Object Detection}

\paragraph{Monocular Baseline.} 3D detection results are shown in \tab{\ref{tab:3dap}}. Our baseline network outperforms monocular methods MF3D~\cite{xu2018multifusion} and Mono3D~\cite{chen2016monocular} in 3D detection. For  scenarios, our AP\textsubscript{3D} exceeds MF3D~\cite{xu2018multifusion} by  at IoU = 0.5 and  at IoU = 0.7. For  scenarios the gap is smaller. Since the raw detection results of MF3D~\cite{xu2018multifusion} are not publicly available, detailed quantitative comparison cannot be made. But it is possible that a part of the performance gap comes from the object proposal stage, where MF3D~\cite{xu2018multifusion} proposes on the image plane, while we directly propose in 3D. In  and  cases where objects are occluded and truncated, 2D proposals can have difficulties retrieving the 3D information of a target that is partly unseen. In addition, the wide margin with Mono3D~\cite{chen2016monocular} reveals a better expressiveness of learnt features than handcrafted features.
\paragraph{TLNet.} By combining with the proposed TLNet, our method outperforms the baseline network and 3DOP~\cite{chen20153dop} across all 3D IoU thresholds in ,  and  scenarios. Under IoU thresholds of 0.3 and 0.5, stereo triangulation learning brings  improvement in AP\textsubscript{3D}. In comparison with 3DOP~\cite{chen20153dop}, our method achieves  better AP\textsubscript{3D} across all regimes. Note that 3DOP~\cite{chen20153dop} needs to estimate the pixel-level disparity maps from stereo images to calculate depths and generate proposals, which is erroneous at distant regions. Instead, the proposed method utilizes 3D anchors to construct geometric correspondence between left-right RoI pairs, then triangulates the targeted objects using coherent features. According to the curves in \fig{\ref{fig:apcurves}} (a), our main method has a higher precision than 3DOP~\cite{chen20153dop} and a higher recall than the baseline.

\subsection{BEV Object Detection}

\paragraph{Monocular Baseline.} Results are presented in \tab{\ref{tab:bvap}}. Compared with 3D detection, we still evaluate the same set of 3D bounding boxes, but the vertical axis is disregarded for BEV IoU calculation. The baseline keeps its advantages over MF3D in  and  scenarios. Note that MF3D uses extra data to train pixel-level depth maps. In  cases, objects are clearly presented, and pixel-level predictions with sufficient local features can obtain more accurate results in statistics. However, pixel-level depth maps with high resolution are unfortunately not always available, indicating their limitations in real applications.

\vspace{-0.2cm}
\paragraph{TLNet.} Not surprisingly, by use of TLNet, we outperform the monocular baseline under all IoU thresholds in various scenarios. At IoU threshold 0.3 and 0.5, the triangulation learning yields  increase in AP\textsubscript{BEV}. Our method also surpasses 3DOP by a notable margin, especially for strict evaluation criteria, \eg, under IoU threshold of 0.7, which reveals the high precision of our predictions. Such performance improvement mainly comes from two aspects: 1) the solid monocular baseline already achieves comparable results with 3DOP; 2) the stereo triangulation learning scheme further enhances the capability of our baseline model in object localization. \fig{\ref{fig:apcurves}} (b) further compares the Recall-Precision curves under IoU threshold of 0.3.



\subsection{3D Localization}
\paragraph{Monocular Baseline.} Results are shown in \tab{\ref{tab:3dloc}}. Our monocular baseline achieve better results than 3DOP~\cite{chen20153dop} under the strict distance threshold of 0.5m, indicating the high precision of our top predictions. 

\vspace{-0.1cm}
\paragraph{TLNet.} TLNet boosts the baseline performance marginally, especially for distance threshold of 2m and 1m, where there is  gain in AP\textsubscript{LOC}. According to the Recall-Precision curves in \fig{\ref{fig:apcurves}} (c), TLNet increases both the precision and recall. The maximum precision is close to  because most of the top predictions are correct.

\subsection{Qualitative Results}
3D bounding boxes predicted by the baseline network and our stereo method are presented in \fig{\ref{fig:visualize3d}}. In general, the predicted orange bounding boxes matches the green ground truths better when TLNet is integrated into the baseline model. As shown in (a) and (c), our method can reduce depth error, especially when the targets are far away from the camera. Object targets missed by the baseline in (b) and (f) are successfully detected. The heavily truncated car in the right-bottom of (d) is also detected, since the object proposals are in 3D, regardless of 2D truncation.






\begin{table}
	\centering
	\setlength{\tabcolsep}{3mm}{  
	    \begin{spacing}{1.1}
			\scalebox{0.8}{
			
				\begin{tabular}{|p{0.5cm}<{\centering}|p{1.6cm}<{\centering} |ccc|}
					\hline
					\multirow{2}{*}{IoU}            &\multirow{2}{*}{Method}                 & \multicolumn{3}{c|}{AP\textsubscript{3D}}                 \\ \cline{3-5}
					                                             &                                                        & \multicolumn{1}{c|}{Easy}     & \multicolumn{1}{c|}{Moderate}  & Hard    \\ \hline \hline
					                                          
                  \multirow{3}{*}{0.3}             &  Concat.                                          &  76.87                     & 60.81                          & 54.70                       \\          
                 
                                                                &  Add                                        & 75.74                         & 59.89                          & 53.57                     \\
                                                                &  Reweight                                             & \textbf{78.26}                & \textbf{63.36}                 & \textbf{57.10}         \\ \hline
                                                                
                  \multirow{3}{*}{0.5}             &  Concat.                                          & 56.32                        &41.20                           &36.41                \\          
                 
                                                                &  Add                                         &53.41                      & 41.61                         & 36.37                \\
                                                                &  Reweight                                              &\textbf{59.51}             & \textbf{43.71}                & \textbf{37.99}      \\ \hline
                                                                
                 \multirow{3}{*}{0.7}              &  Concat.                                          & 13.97                         &11.63                           &9.67                \\          
                 
                                                                &  Add                                       & 16.60                     & 13.59                         & 11.20             \\
                                                                &  Reweight                                             & \textbf{18.15}            & \textbf{14.26}                & \textbf{13.72}         \\ \hline 
				\end{tabular}  
				
			} 
			\end{spacing}
     }
	\caption{\textbf{Effect of reweighting on AP\textsubscript{3D}.} Three fusion methods in \fig{\ref{fig:fusion}} are compared, including concatenation, direct addition and the proposed reweighting strategy.}
	\label{tab:3dapnorew} 
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/persons.pdf}
    \caption{\textbf{Qualitative results for persons.}}
    \label{fig:vis_persons}
\end{figure}



\begin{figure}[!htbp] 
	\centering
	\scriptsize
	\begin{tabular}{c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c}
		\includegraphics[width=0.33\linewidth]{./figs/curves/3D_Easy} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/3D_Moderate} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/3D_Hard} \\
		\multicolumn{3}{c}{(a) 3D object detection at IoU threshold of 0.3. }  \\
		
		\includegraphics[width=0.33\linewidth]{./figs/curves/BV_Easy} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/BV_Moderate} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/BV_Hard} \\
		\multicolumn{3}{c}{(b) BEV object detection at IoU threshold of 0.3. }  \\
		
		\includegraphics[width=0.33\linewidth]{./figs/curves/LOC_Easy} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/LOC_Moderate} &
		\includegraphics[width=0.33\linewidth]{./figs/curves/LOC_Hard} \\
		\multicolumn{3}{c}{(c) 3D localization at distance threshold of 1m. }  \\
		
	\end{tabular}
	
	\caption{\textbf{Recall-precision curves.} }
	\label{fig:apcurves}

\end{figure}


\subsection{Ablation Study}
In TLNet, the small feature maps obtained by use of RoIAlign~\cite{he2017mrcn} are not directly fused. In order to focus more attention on the key parts of an object and reduce noisy signals, we first calculate the pairwise cosine similarity  as coherence score, then reweight corresponding channels by multiplying with . See \ref{subsubsec:tlnet} and \fig{\ref{fig:metriclearning}} for details. In the followings, we examine the effectiveness of reweighting by replacing it with other two fusion methods, \ie, direct concatenation and element-wise addition, as is shown in \fig{\ref{fig:fusion}}. 

Comparisons between their AP\textsubscript{3D} and AP\textsubscript{BEV} are presented in \tab{\ref{tab:3dapnorew}}. The evaluation corresponds with the empirical analysis in \ref{subsubsec:tlnet}. Since the left and right branches share their weights in the backbone network, the same channels in left and right RoIs are expected to extract the same kind of features. Some of these features are more suitable for efficient triangulation learning, which are strengthened by reweighting. Note that the coherence score  is not fixed for each channel, but is dynamically determined by the RoI pairs cropped out at specific locations.


	
	
\begin{figure}[!htbp]
	\centering
\includegraphics[width=1\linewidth]{./figs/fusion_function}
	\caption{\textbf{Feature fusion methods in TLNet.} (c) is the proposed strategy. It first computes the coherence score of each pair of channels, then reweights the channels and adds them together.}
	\label{fig:fusion}
\end{figure}
