\documentclass[11pt]{article}
\usepackage{amsmath,amsthm,amssymb,epsfig,sectsty,caption,color,times}
\usepackage[letterpaper,hmargin=0.95in,vmargin=1.1in]{geometry}
\usepackage[dvipdfm,bookmarks=false,linkbordercolor={0 1 1},citebordercolor={0 1 1}]{hyperref}

\usepackage{algorithm, algpseudocode}
\algrenewcommand\algorithmicrequire{\textbf{\quad Input:}}
\algrenewcommand\algorithmicensure{\textbf{\quad Output:}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}

\newcommand{\qedsymb}{\hfill{\rule{2mm}{2mm}}}
\renewenvironment{proof}{\begin{trivlist} \item[\hspace{\labelsep}{\bf \noindent Proof.\/}] }{\qedsymb\end{trivlist}}\newenvironment{proofof}[1]{\begin{trivlist} \item[\hspace{\labelsep}{\bf \noindent Proof of #1.\/}] }{\qedsymb\end{trivlist}}\newenvironment{MyEqn}[1]{\setlength\arraycolsep{2pt}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\alg}{\mathrm{ALG}}
\newcommand{\opt}{\mathrm{OPT}}

\sectionfont{\large} \subsectionfont{\normalsize}

\begin{document}
\begin{titlepage}
\title{Efficient Submodular Function Maximization\\under Linear Packing Constraints}
\author{
    Yossi Azar\thanks{Blavatnik School of Computer Science, Tel-Aviv
    University, Israel. Email: \href{mailto:azar@tau.ac.il}
    {\tt azar@tau.ac.il}. Supported in part by the Israel Science
    Foundation (grant No. 1404/10).}
    \and
    Iftah Gamzu\thanks{Blavatnik School of Computer Science, Tel-Aviv
    University, Israel and Computer Science Division, The Open University, Israel.
    Email: \href{mailto:iftah.gamzu@cs.tau.ac.il}{\tt iftah.gamzu@cs.tau.ac.il}}
}
\date{}
\maketitle

\begin{abstract}
We study the problem of maximizing a monotone submodular set
function subject to linear packing constraints. An instance of
this problem consists of a matrix , a
vector , and a monotone submodular set
function . The objective is to find
a set  that maximizes  subject to , where
 stands for the characteristic vector of the set . A
well-studied special case of this problem is when  is linear.
This special case captures the class of packing integer programs.

Our main contribution is an efficient combinatorial algorithm that
achieves an approximation ratio of , where  is the width of the packing
constraints. This result matches the best known performance
guarantee for the linear case. One immediate corollary of this
result is that the algorithm under consideration achieves constant
factor approximation when the number of constraints is constant or
when the width of the constraints is sufficiently large. This
motivates us to study the large width setting, trying to determine
its exact approximability. We develop an algorithm that has an
approximation ratio of  when . This result essentially matches the
theoretical lower bound of . We also study the special
setting in which the matrix  is binary and -column sparse. A
-column sparse matrix has at most  non-zero entries in each
of its column. We design a fast combinatorial algorithm that
achieves an approximation ratio of , that
is, its performance guarantee only depends on the sparsity and
width parameters.
\end{abstract}

\thispagestyle{empty}
\end{titlepage}


\section{Introduction} \label{sec:Intro} Let  be a set function, where . The function  is called \emph{submodular} if and only if
, for all . An alternative definition of submodularity is
through the property of decreasing marginal values. Given a
function  and a set , the function  is defined by . The value  is called the incremental marginal value
of element  to the set . The \emph{decreasing marginal
values} property requires that  is non-increasing function
of  for every fixed . Formally, it requires that  for all . Since the amount of
information necessary to convey an arbitrary submodular function
may be exponential, we assume a value oracle access to the
function. A \emph{value oracle} for the function  allows us to
query about the value of  for any set . Throughout the
rest of the paper, whenever we refer to a submodular function, we
shall also imply a \emph{normalized} and \emph{monotone} function.
Specifically, we assume that a submodular function  also
satisfies  and  whenever .

In this paper, we focus our attention on the problem (or rather
class of problems) of maximizing a monotone submodular set
function subject to linear packing constraints. Formally, the
input of this problem consists of a matrix , a vector , and a monotone submodular set
function . The objective is to find
a set  that maximizes  subject to , where
 stands for the characteristic vector of the set . We note
that the domain restrictions on the entries of  and  are
without loss of generality since arbitrary non-negative packing
constraints can be reduced to the above form by first eliminating
any element  for which there is some constraint  such that
, and then scaling the input (see, e.g., the
discussion in~\cite{Srinivasan99}). A well-studied special setting
of our problem is when the objective function  is
\emph{linear}, namely, there is a weight vector 
such that . This special setting
captures the class of packing integer programs, which models many
fundamental combinatorial optimization problems, including maximum
independent set, hypergraph matching, and disjoint paths.


\medskip \noindent {\bf Previous work.}
Submodular functions play an instrumental role in computer
science, economics, and operations research as they form a rich
class that is general enough to be valuable for applications, but
still has plenty of structure to allow positive results. These
properties seem to make submodular functions a natural candidate
of choice for objective functions in optimization problems.
Indeed, over the last few years, there has been a surge of
interest in understanding the limits of tractability of
optimization problems in which the classic linear objective
function was replaced by a submodular one.

There has been a long line of research on maximizing monotone
submodular functions subject to matroid and knapsack constraints.
Arguably, the most classic scenario is maximizing a submodular
function subject to a cardinality constraint, that is, . It is known that a simple greedy algorithm
achieves an approximation ratio of  for this
problem~\cite{NemhauserWF78}. Furthermore, this result is optimal
in two different ways: (i) given only oracle access to , one
cannot attain a better approximation ratio without asking
exponentially many value queries~\cite{NemhauserW78}, and (ii)
even if  has a compact representation, it is still NP-hard to
obtain a better approximation result~\cite{Feige98}. The greedy
approach and its variants has been shown to be useful in
additional constraint
structures~\cite{FisherNW78,KhullerMN99,ChekuriK04,GoundanS07}.
One relevant setting is maximizing a monotone submodular function
under a knapsack constraint~\cite{Wolsey82a}. A knapsack
constraint is essentially a single packing constraint, and may be
viewed as the weighted analog of a cardinality constraint.
Sviridenko~\cite{Sviridenko04} demonstrated that a greedy
algorithm with partial enumeration achieves an approximation
guarantee of  for this problem.

Another approach that has been proven effective in handling
submodular function maximization under different constraint
structures is based on approximately solving a continuous
fractional relaxation of the problem, followed by pipage or
randomized rounding. The pipage rounding technique was originally
developed by Ageev and Sviridenko~\cite{AgeevS04}, and was adapted
to submodular maximization scenarios by Calinescu, Chekuri,
P{\'a}l and Vondr{\'a}k~\cite{CalinescuCPV07}.
Vondr{\'a}k~\cite{Vondrak08} utilized the continuous relaxation
approach to achieve a tight -approximation for
maximizing a monotone submodular function subject to a matroid
constraint, and Kulik, Shachnai and Tamir~\cite{KulikST09} used
this approach to attain a -approximation
for maximizing a monotone submodular function under a constant
number of packing constraints. Later on, Chekuri, Vondr{\'a}k and
Zenklusen~\cite{ChekuriVZ10} presented a dependent randomized
rounding scheme that can be utilized to extend those results for
maximizing a monotone submodular function subject to one matroid
and constant number of packing constraints. Recently, Feldman,
Naor and Schwartz~\cite{FeldmanNS11a} presented a new unified
continuous relaxation approach that finds approximate fractional
solutions in both monotone and non-monotone scenarios.

\medskip \noindent {\bf Our contribution.}
Our main result is an efficient multiplicative updates algorithm
for maximizing a monotone submodular function subject to any
number of linear packing constraints. The approximation ratio of
our algorithm matches the best known performance guarantee for the
special case when the objective function  is linear, which is
achieved using the randomized rounding
technique~\cite{RaghavanT87,Raghavan88,Srinivasan99}. More
precisely, let  be the
\emph{width} of the packing constraints, we attain the following
result.

\begin{theorem} \label{th:MainResult1}
There is a deterministic polynomial-time algorithm that attains an
approximation guarantee of  for maximizing a
monotone submodular function under linear packing constraints.
\end{theorem}

It is worth noting that our combinatorial algorithm is
deterministic and efficient. Moreover, our technique is different
than the two leading approaches used in the past for submodular
maximization, namely, the greedy approach and the continuous
relaxation approach. Our algorithm is based on a multiplicative
updates method (see,
e.g.,~\cite{PlotkinST95,Young95,GargK07,AzarR06,BriestKV05}). This
method is known to be fruitful for approximately solving problems
that can be cast as linear and integer programs. Nevertheless, the
analysis of these algorithms relies heavily on primal-dual
results, which are not applicable in our submodular setting. We
believe that this new approach may be suitable for other
submodular optimization problems. We also like to remark that a
comparable approximation guarantee may be obtained using the
continuous relaxation approach applied with randomized
rounding~\cite{ChekuriV10}. However, in contrast with that
approach, our algorithm is deterministic, efficient and
combinatorial.

One immediate corollary of Theorem~\ref{th:MainResult1} is that
the algorithm under consideration achieves a constant factor
approximation when the number of constraints is constant or when
the width of the packing constraints is sufficiently large, say . This motivates us to study the large width
setting, trying to determine its exact approximability. The
following theorem summarizes our result in this context.

\begin{theorem} \label{th:MainResult2}
There is a deterministic polynomial-time algorithm that achieves
an approximation guarantee of  for
maximizing a monotone submodular function subject to linear
packing constraints when , for any
fixed .
\end{theorem}

We note that this result almost matches the theoretical lower
bound of , which already holds for maximizing a monotone
submodular function subject to a cardinality
constraint~\cite{NemhauserWF78,Feige98}. Specifically, the large
width setting captures the hard instances of that problem. We
remark that the -approximation in the submodular
setting stands in contrast with a -approximation
which can be achieved by randomized rounding when the objective
function is linear and the width is sufficiently large.

We also study the interesting special setting of the problem in
which the constraints matrix is binary, namely,  instead of . We demonstrate
how to fine-tune our algorithm and its analysis to achieve an
improved approximation guarantee of .
This result is formalized in Theorem~\ref{th:MainResult3}. We like
to emphasize that this result is optimal unless . Recently, Bansal et al.~\cite{BansalKNS10}
considered the special case of maximizing a submodular function
under \emph{-column sparse} packing constraints. In this
setting, the constraints matrix has at most  non-zero entries
in each column. They developed an algorithm whose approximation
ratio only depends on the sparsity and width parameters of the
input matrix. Specifically, they presented a -approximation algorithm that employs the continuous
relaxation approach in conjunction with randomized rounding and
alteration. We make a first step towards attaining their
performance guarantee in a deterministic and efficient way. We
present a fast combinatorial algorithm for the binary -column
sparse setting whose approximation ratio only depends on the
sparsity and width parameters of the input matrix. The following
theorem outlines this result.

\begin{theorem} \label{th:MainResult4}
There is a deterministic polynomial-time algorithm that achieves
an approximation guarantee of  for
maximizing a monotone submodular function under binary packing
constraints.
\end{theorem}


\medskip \noindent {\bf Other related work.}
The problem of maximizing a non-monotone submodular function
without any structural constraints is known to be both NP-hard and
APX-hard since it generalizes the maximum cut problem. Feige,
Mirrokni and Vondr{\'a}k~\cite{FeigeMV07} developed an algorithm
whose approximation ratio is . This result was iteratively
improved by Oveis Gharan and Vondr{\'a}k~\cite{GharanV11}, and
then by Feldman, Naor and Shwartz~\cite{FeldmanNS11b} to a ratio
of . Lee, Mirrokni, Nagarajan and Sviridenko~\cite{LeeMNS10}
presented a -approximation algorithm for
non-monotone submodular maximization subject to a constant number
of packing constraints. This result was iteratively improved by
Chekuri, Vondr{\'a}k and Zenklusen~\cite{ChekuriVZ11}, and then by
Feldman, Naor and Shwartz~\cite{FeldmanNS11a} to a ratio of
. Vondr{\'a}k~\cite{Vondrak09}, and very recently,
Dobzinski and Vondr{\'a}k~\cite{DobzinskiV12} developed general
approaches to derive inapproximability results in the value oracle
model.

Unlike submodular function maximization, the problem of minimizing
a submodular function can be performed efficiently, either by the
ellipsoid algorithm~\cite{GrotschelLS81} or through strongly
polynomial-time combinatorial
algorithms~\cite{Schrijver00,IwataFF01,Iwata03,Orlin07,Iwata08,IwataO09}.
Goemans, Harvey, Iwata and Mirrokni~\cite{GoemansHIM09} considered
the problem of explicitly constructing a function that
approximates a monotone submodular function while making a
polynomial number of oracle queries. They showed an essentially
tight -approximate solution. Recently, several
submodular analogues of classical combinatorial optimization
problems have been studied~\cite{SvitkinaF08,GoelKTW09,IwataN09}.
These submodular problems are commonly considerably harder to
approximate than their linear counterparts. For example, the
minimum spanning tree problem, which is polynomial-time solvable
with linear cost functions is -hard to approximate with
submodular cost functions~\cite{GoelKTW09}.


\section{Submodular Maximization with Linear Packing Constraints} \label{sec:GeneralPacking}
In this section, we develop a multiplicative updates algorithm for
the problem and analyze its performance. An important input
parameter of our algorithmic template is an update factor. This
parameter plays an essential role in achieving the desired
approximation guarantees in the two settings of interest. We first
consider the general problem, and demonstrate that there is an
update factor for which our algorithm attains an approximation
ratio of . In particular, this implies that
the algorithm achieves constant factor approximation for input
instances that have a large width, e.g., instances with . This motivates us to study this large width
setting, trying to determine its exact approximability. We match
(up to a disparity of~) the theoretical lower bound of
 using a different update factor and a refined analysis.


\subsection{The algorithm} \label{subsec:Algorithm}
The multiplicative updates algorithm, formally described below,
maintains a collection of weights that are updated in a
multiplicative way. Informally, these weights capture the extent
to which each constraint is close to be violated under a given
solution. The algorithm is built around one main loop. In each
iteration of that loop, the algorithm extends the current solution
with a non-selected element that minimizes a normalized sum of the
weights. When the loop terminates, the algorithm returns the
resulting solution in case it is feasible; otherwise, either the
last selected element or the resulting solution without that
element is returned, depending on their value. Recall that  is the incremental marginal value of
element  to the set , and  is the characteristic vector
of the set .

\begin{algorithm}
\caption{Multiplicative Updates}\label{cap:MultiplicativeUpdates}\begin{algorithmic}[1]
\Require A collection of linear packing constraints defined by  and  \Statex \qquad\; A monotone submodular set function  \Statex \qquad\; An update factor  \Ensure A subset of  \smallskip 
\State  \State \textbf{for}  to  \textbf{do}  \textbf{end for} \smallskip 
\While{ and } \label{alg:StopCond} \State Let  be the element with minimal  \State  \State \textbf{for}  to  \textbf{do}  \textbf{end for} \label{alg:WeightUpdate} \EndWhile \smallskip 
\State \textbf{if}  \textbf{then} return  \State \textbf{else if}  \textbf{then} return  \State \textbf{else} return  \textbf{end if} \end{algorithmic}
\end{algorithm}


\subsection{Analysis} \label{subsec:Analysis}
In the remainder of this section, we analyze the performance of
the algorithm. We begin by establishing several lemmas that hold
independently of the value of the update factor. Later on, we
consider specific update factors, and study their effect on the
approximation ratio of the algorithm. For ease of presentation, it
would be convenient to first introduce some notation and
terminology:
\begin{itemize}
\item Let  be a solution that maximizes the
submodular function subject to the linear packing constraints,
with value of .

\item Let  be the solution at the end of iteration  of the
algorithm, and note that  indicates the solution
at the beginning of the algorithm. Moreover, let 
denote the element selected at iteration  of the algorithm, and
let  be its incremental marginal
value to the solution. Finally, let  be the value of 
at the end of iteration  of the algorithm, and remark that
 is the value of  at the beginning of the
algorithm.

\item Let  and . Notice that
the algorithm may proceed to iteration  only if , and that . Also note that 
is the value which gave rise to the selection of element
 at iteration  of the algorithm.
\end{itemize}


\smallskip \noindent {\bf Correctness.}
We prove that the algorithm outputs a feasible solution. This is
achieved by demonstrating that the returned solution respects the
packing constraints.

\begin{lemma} \label{lemma:Feasibility}
The algorithm outputs a feasible solution.
\end{lemma}
\begin{proof}
Let us focus on the solution  when the main loop terminates.
Clearly, if  respects the packing constraints then the returned
solution also respects them. Thus, let us consider the case that
 is infeasible. We next argue that  became infeasible only
at the last iteration of the loop in which element  was
selected. Consequently, by inspecting the last two lines of the
algorithm, one can conclude that the returned solution must be
feasible as it is either  or \}.

For the purpose of establishing the previously mentioned argument,
let  be the first element that induces a violation in some
constraint. Specifically, suppose  induces a violation in
constraint  at iteration . Accordingly, , and theretofore,

where the last equality is due to the fact that . This implies that , and hence, by
inspecting the main loop stopping condition, we know that the loop
must have terminated immediately after element  was
selected.~
\end{proof}


\smallskip \noindent {\bf Approximation.}
We turn to analyze the approximation guarantee of the algorithm.
We begin by establishing a generic algebraic bound applicable for
any monotone submodular function and any arbitrary sequence of
element additions.

\begin{claim} \label{claim:SubmodularBound}
Given a submodular function , a set
collection , and a set  satisfying  then

\end{claim}
\begin{proof}
One should observe that for any ,

where the inequality follows by noticing that the function  is monotonically increasing for .
As a consequence, we obtain that
~
\end{proof}

We continue by bounding the value of the optimal solution using
the main parameters of the algorithm at the end of iteration
.

\begin{claim} \label{claim:OPTBound}
 in every
iteration .
\end{claim}
\begin{proof}
We know that the element selected at iteration 
minimizes the term  with respect to every .
This clearly implies that  for every  under
consideration. Rearranging the terms in this inequality, we can
bound the marginal value of each element  with respect to  as

Let  be the
set of elements selected by the optimal solution, but not selected
by the algorithm up to the end of iteration . Note that , and notice that

where the first inequality follows from the monotonicity of ,
and the last inequality holds as a result of its submodularity.
Specifically, the latter inequality is obtained using the
decreasing marginal values property. We now focus on bounding the
above right-hand side term. For this purpose, we utilize the bound
derived earlier on the marginal values of the elements in , and attain

where the last inequality follows by recalling that the elements
in  are a subset of the elements in the optimal solution, and
thus, constitute a feasible solution respecting all constraints.
As a result, .~
\end{proof}

We next demonstrate that the algorithm attains an approximation
guarantee of  when the update factor is
. Recall that  is the width of the constraints.

\begin{lemma} \label{lemma:Approx1}
The algorithm archives -approximation by
using .
\end{lemma}
\begin{proof}
Suppose the main loop terminates after  iterations. Notice that
when the loop terminates either  or . In the former case, one can easily infer that the
returned solution is -approximation to the optimal solution.
Specifically, if  is returned by the algorithm then the
outcome is clearly optimal since  consists of all elements,
and if one of  or  is returned then
the value of the solution is a -approximation since

where the last inequality uses the submodularity of . In fact,
one can easily validate that the above analysis also holds in case
that , which can happen since  may be
infeasible. Hence, in the remainder of the proof, we shall assume
that  and that the loop terminates with
.

We concentrate on upper bounding the value of . For
this purpose, we analyze the change in 
along the loop iterations. Observe that for any ,

The first inequality follows by plugging  and  to the inequality ,
which is known to be valid for any  and , and the last equality results from the definition of
. By Claim~\ref{claim:OPTBound}, we know that
 in
case . The latter condition clearly holds
since  by previous assumption, and  for any  under consideration. Therefore,

where the last inequality is due to the fact that . The resulting recursive definition can be used, in
conjunction with the base case , to upper bound
 by

Recall that we assumed that the loop terminated with . This lower bound on  can be utilized, together
with the upper bound on , to yield

where the last inequality is due to the
Claim~\ref{claim:SubmodularBound}. We note that  since
 is normalized and . Subsequently, one can
obtain that 
using simple algebraic manipulations. This can be further
simplified to  by
reutilizing the fact that . Notice that this
proves that the algorithm archives -approximation since the value of the returned solution
is at least . This follows from arguments similar to
those presented at the beginning of the proof.~
\end{proof}

We are now ready to complete the proof of the first main result of
the paper. We note that this result matches the best known
approximation guarantee for the case that the objective function
 is linear, achievable using the randomized rounding
technique~\cite{RaghavanT87,Raghavan88,Srinivasan99}.

\begin{proofof}{Theorem~\ref{th:MainResult1}}
By Lemma~\ref{lemma:Feasibility} and Lemma~\ref{lemma:Approx1}, we
know that when the algorithm uses an update factor of , it constructs a feasible solution which approximates the
optimal solution within a factor of .~
\end{proofof}

One immediate corollary of this theorem is that the algorithm
under consideration attains a constant approximation guarantee
when the number of constraints is constant or when the width is
sufficiently large, say . In particular, one
can reexamine the analysis presented in the proof of
Lemma~\ref{lemma:Approx1}, and deduce that the approximation ratio
of the algorithm approaches  for sufficiently large
's. A natural followup question is whether one can improve upon
this result. In what follows, we demonstrate that we can beat this
approximation ratio by a careful selection of the update factor.
We present a refined analysis that proves an approximation ratio
of  when . In particular, our analysis avoids the two-factor
loss due to the max-selection in the last two lines of the
algorithm.

\begin{lemma} \label{lemma:Approx2}
The algorithm achieves an approximation ratio of  by using  when  for any fixed
.
\end{lemma}
\begin{proof}
Suppose the main loop terminates after  iterations. Let us
consider the case that it terminates with . Note that this implies that . One can also argue that  is the returned solution
since it is feasible. The feasibility of  follows from
arguments similar to those presented in the proof of
Lemma~\ref{lemma:Feasibility}. Specifically, one can demonstrate
that if  violates some constraint  then . Obviously, the returned solution is
optimal as  consists of all elements. Hence, in the
remainder of the proof, we shall focus on the case that the loop
terminates with .

We next argue that solution constructed up to and not including
the last iteration, namely , achieves the claimed
approximation guarantee. Note that this implies that the returned
solution must also have the desired performance guarantee since
 is feasible. The feasibility of  also follows from
arguments similar to those exhibited in the proof of
Lemma~\ref{lemma:Feasibility}. Specifically, one can establish
that if  is infeasible then it became infeasible only at
the last iteration of the loop, and thus,  is feasible. We
turn to bound the value of . A lower bound can be
easily obtained by noticing that

and therefore, . Similarly to
the proof of Lemma~\ref{lemma:Approx1}, we derive an upper bound
on  by analyzing the change in 
along the loop iterations. Observe that for any ,

The first inequality follows from the fact that  for any , which can be derived from the
corresponding Taylor expansion. The last inequality is obtained by
using the fact that  to reason
that . Finally, the last equality results from
the definition of . By Claim~\ref{claim:OPTBound}, we
know that  when . The latter
condition clearly holds since  as  is a
feasible solution, and  for any 
under consideration. Therefore,

where the last inequality is due to the fact that . The resulting recursive definition can be used to upper
bound  by

where the last inequality holds since  by our assumption regarding the width of the
constraints. Recall that we previously demonstrated that
. This lower bound on
 can be utilized, together with the upper bound on
, to yield

where the last inequality is due to the
Claim~\ref{claim:SubmodularBound}. Note that  as 
is normalized and . Also notice that . Subsequently, one
can obtain that  using simple algebraic manipulations. The claimed
approximation ratio follows by noticing that

where the first inequality reuses the fact that  for any , and both inequalities assume that
, which is the interesting range of values for
.~
\end{proof}

We are now ready to complete the proof of the second principal
result of the paper. We note that this result almost matches the
theoretical lower bound of , which already holds for
maximizing a monotone submodular function subject to a cardinality
constraint~\cite{NemhauserWF78,Feige98}. In particular, our large
width setting captures the hard instances of the latter problem as
this problem can be solved in polynomial-time when  by enumerating over all sets of size at most .

\begin{proofof}{Theorem~\ref{th:MainResult2}}
Given an instance of the problem in which  for any fixed ,
Lemma~\ref{lemma:Feasibility} and Lemma~\ref{lemma:Approx2}
guarantee that employing the algorithm with an update factor of
 results in a feasible solution that
approximates the optimal solution within a factor of .~
\end{proofof}


\section{Submodular Maximization with Binary Packing Constraints} \label{subsec:0-1Packing}
In this section, we consider the special setting of monotone
submodular maximization under binary packing constraints, namely,
when  instead of . Note that we may assume without loss of generality
that  since each vector entry can be rounded down
to the nearest integer without any consequences whatsoever. This
natural setting has been considered in the past for linear
objective functions. Similarly to the general linear case, the
randomized rounding technique attains the best known approximation
guarantee in this case as well. In particular, it achieves an
approximation ratio of , which is
polynomially better than the general case. This outcome is also
known to be optimal unless ~\cite{ChekuriK04}. We can demonstrate that our
multiplicative updates approach from
Section~\ref{sec:GeneralPacking} can be utilized to obtain the
above-mentioned improved approximation guarantee for the
underlying setting. This requires a fine-tuning of the algorithm
and its analysis. We defer these technical details to
Appendix~\ref{appsec:0-1}.

We next develop a different multiplicative updates algorithm for
the special setting in which the constraints matrix is -column
sparse. In this case, the number of -value entries in each
column of the input matrix is at most . We prove that our
algorithm achieves an approximation guarantee that does not depend
on the number of rows , but only depends on the sparsity
parameter  and width parameter . More precisely, we
establish that the algorithm attains an approximation ratio of
.

\subsection{The algorithm} \label{subsec:SparseAlgorithm}
The multiplicative updates algorithm, formally described below,
maintains a collection of weights that capture the extent to which
each constraint is close to be violated under a given solution.
The algorithm is built around one main loop. In each iteration of
that loop, the algorithm considers a remaining element whose
marginal contribution to the current solution is maximal, and adds
it to the solution set if its corresponding sum of weights is
sufficiently small. In any case, the element under consideration
is removed from the list of remaining elements. When the loop
terminates, the algorithm returns the resulting solution. Recall
that  is the incremental marginal
value of element  to the set 

\begin{algorithm}
\caption{Column Sparse Multiplicative Updates}\label{cap:SparseMultiplicativeUpdates}\begin{algorithmic}[1]
\Require A collection of linear packing constraints defined by  and  \Statex \qquad\; A monotone submodular set function  \Statex \qquad\; An update factor  \Ensure A subset of  \smallskip 
\State ,  \State \textbf{for}  to  \textbf{do}  \textbf{end for} \smallskip 
\While{} \label{alg:SparseStopCond} \State Let  be the element with maximal  \State \textbf{if}  \textbf{then}  \State  \State \textbf{for}  to  \textbf{do}  \textbf{end for} \label{alg:SparseWeightUpdate} \EndWhile \smallskip 
\State return  \end{algorithmic}
\end{algorithm}


\subsection{Analysis} \label{subsec:Analysis}
In what follows, we analyze the performance of the algorithm. We
begin by establishing an algebraic bound applicable for any
monotone submodular function and any solution set of elements,
attained by an algorithm that considers the elements in a greedy
fashion. Note that our algorithm indeed considers the elements in
such fashion. We define the \emph{greedy elements sequence}
 of a
submodular function  and a set  as the ordered sequence of
elements considered by a greedy process whose outcome is .
Specifically, the greedy process is initialized with 
and . Then , it runs for  steps, where in each
step , it considers the element  that has a
maximum marginal value with respect to the current solution set
, and adds it to the solution set  of the next step
if . In any case, the element  is removed from
 to obtain the set  of remaining elements for the
next step. With this definition in mind, let  be the set of first  elements in the
sequence under consideration.


\begin{claim} \label{claim:SparseGreedyBound}
Given a submodular function , a set , their greedy elements sequence , and another set  satisfying 
for every  and a parameter , it holds
that .
\end{claim}
\begin{proof}
Let us assume without loss of generality that the greedy process
goes over the elements according to the order  to , namely,
, and so on. We note that this
assumption is valid since one can appropriately rename the
elements. Furthermore, let  and  be the respective elements of  and
 sorted in an increasing order. Let us suppose that
 is integral. We emphasize that this assumption is
merely for simplicity of presentation, as we demonstrate later. We
match between each element of  and  distinct elements
from . Specifically, each element  is matched to the
elements set .
Notice that every element of  is matched to an element of
; else, it must be that , but this
contradicts the fact that . We next argue that each . As a result, we attain that each

The last inequality holds since we known that when the element
 was considered by the greedy process, all the elements of
 were still available, and therefore, their marginal value
with respect to the solution  was no more than
the marginal value of the element . Consequently,

where both inequalities hold by the submodularity of . For the
purpose of establishing the previously mentioned argument, suppose
by way of contradicting that there is some  for which . Let us concentrate on the elements set
. Notice that , whereas . This implies that , a contradiction. We
conclude by noting that our assumption that  is integral
can be easily neglected. Specifically, one need to modify that
proof in such a way that a fractional part of an element from
 may be matched to an element form . Then, notice that at
most two fractional parts of an element of  are matched to
elements of , and those elements must appear before the element
of  in the greedy elements sequence.~
\end{proof}

We now turn to establish our main result for the special setting
of maximizing a monotone submodular function under -column
sparse packing constraints.

\begin{proofof}{Theorem~\ref{th:MainResult4}}
We first claim that the algorithm outputs a feasible solution,
namely, a solution that respects the packing constraints. Suppose
by way of contradiction that  is the first element that is
added to  and induces a violation in some constraint  at
iteration  of the main loop. Note that . Let
 be the solution at the end of iteration , and notice that
 since all the entries of 
are binary. This implies that  at the beginning
of the iteration in which  was considered, and thus,
. Inspecting the
selection rule, one can infer that  could not have been
selected.

We next demonstrate that the algorithm attains an approximation
guarantee of  when the update factor is
. Recall that  is the width of the
constraints, which is equal to  in our case.
Similarly to before, we denote by  a solution
that maximizes the submodular function subject to the linear
packing constraints. Let  be the
ordered sequence of elements considered by our algorithm, and note
that it is essentially the greedy elements sequence
. Moreover, let  be
the set of first  elements in that sequence,  be the elements of  in the optimal solution,  be the elements of  in our algorithm's solution,
and  be the
value of  at the end of iteration  of the algorithm. We
prove the two following claims:

\begin{claim}
For every ,

\end{claim}
\begin{proof}
We prove this claim by induction on . The induction base is
when the algorithm begins, i.e., when . It is easy to see
that both sides of the above expression are zero in this case. In
particular, notice that all the weights are initialized to .
Observe that in order to establish the induction step, it is
sufficient to demonstrate that when an element  is selected
at iteration  then . For
this purpose, notice that

where the inequality follows by plugging  and
 to the inequality ,
which is known to be valid for any  and . As a consequence, we attain that

where the last inequality holds since we know that (1) element
 is selected at iteration , and thus, , and (2) the packing constraints
are -column sparse, namely, the number of -value entries in
each column is at most , and hence, .
\end{proof}

\begin{claim}
For every ,

\end{claim}
\begin{proof}
Clearly, . Now, notice
that every element  was not selected by
our algorithm when it was considered in step  since
. since the
weights may only increase during the run of the algorithm, we can
infer that

where the last inequality holds by recalling that the set  is a subset of the optimal solution, and hence,
constitute a feasible solution respecting all constraints. As a
result, .~
\end{proof}

We can now utilize the above claims and get that for every ,

where the last equality holds as . Therefore, we
can employ Claim~\ref{claim:SparseGreedyBound} with , and attain that the solution of our
algorithm approximates the optimal solution to within a factor of
at least .~
\end{proofof}


\paragraph{Acknowledgments:}
The authors thank Chandra Chekuri, Ilan Cohen, Gagan Goel, and Jan
Vondr{\'a}k for valuable discussions on topics related to the
subject of this study.


\begin{thebibliography}{10}

\bibitem{AgeevS04}
A.~A. Ageev and M.~Sviridenko.
\newblock Pipage rounding: A new method of constructing algorithms with proven
  performance guarantee.
\newblock {\em J. Comb. Optim.}, 8(3):307--328, 2004.

\bibitem{AzarR06}
Y.~Azar and O.~Regev.
\newblock Combinatorial algorithms for the unsplittable flow problem.
\newblock {\em Algorithmica}, 44(1):49--66, 2006.

\bibitem{BansalKNS10}
N.~Bansal, N.~Korula, V.~Nagarajan, and A.~Srinivasan.
\newblock On {\it k}-column sparse packing programs.
\newblock In {\em Proceedings 14th International Conference on Integer
  Programming and Combinatorial Optimization}, pages 369--382, 2010.

\bibitem{BriestKV05}
P.~Briest, P.~Krysta, and B.~V{\"o}cking.
\newblock Approximation techniques for utilitarian mechanism design.
\newblock In {\em Proceedings 37th ACM Symposium on Theory of Computing}, pages
  39--48, 2005.

\bibitem{CalinescuCPV07}
G.~Calinescu, C.~Chekuri, M.~P{\'a}l, and J.~Vondr{\'a}k.
\newblock Maximizing a submodular set function subject to a matroid constraint.
\newblock In {\em Proceedings 12th International Conference on Integer
  Programming and Combinatorial Optimization}, pages 182--196, 2007.

\bibitem{ChekuriK04}
C.~Chekuri and S.~Khanna.
\newblock On multidimensional packing problems.
\newblock {\em SIAM J. Comput.}, 33(4):837--851, 2004.

\bibitem{ChekuriV10}
C.~Chekuri and J.~Vondr{\'a}k.
\newblock 2010.
\newblock Personal Communication.

\bibitem{ChekuriVZ10}
C.~Chekuri, J.~Vondr{\'a}k, and R.~Zenklusen.
\newblock Dependent randomized rounding via exchange properties of
  combinatorial structures.
\newblock In {\em Proceedings 51st Annual IEEE Symposium on Foundations of
  Computer Science}, pages 575--584, 2010.

\bibitem{ChekuriVZ11}
C.~Chekuri, J.~Vondr{\'a}k, and R.~Zenklusen.
\newblock Submodular function maximization via the multilinear relaxation and
  contention resolution schemes.
\newblock In {\em Proceedings 43rd ACM Symposium on Theory of Computing}, pages
  783--792, 2011.

\bibitem{DobzinskiV12}
S.~Dobzinski and J.~Vondr{\'a}k.
\newblock From query complexity to computational complexity.
\newblock In {\em Proceedings 44th ACM Symposium on Theory of Computing}, 2012.

\bibitem{Feige98}
U.~Feige.
\newblock A threshold of ln {\it n} for approximating set cover.
\newblock {\em J. ACM}, 45(4):634--652, 1998.

\bibitem{FeigeMV07}
U.~Feige, V.~S. Mirrokni, and J.~Vondr{\'a}k.
\newblock Maximizing non-monotone submodular functions.
\newblock In {\em Proceedings 48th Annual IEEE Symposium on Foundations of
  Computer Science}, pages 461--471, 2007.

\bibitem{FeldmanNS11b}
M.~Feldman, J.~Naor, and R.~Schwartz.
\newblock Nonmonotone submodular maximization via a structural continuous
  greedy algorithm.
\newblock In {\em Proceedings 38th International Colloquium on Automata,
  Languages and Programming}, pages 342--353, 2011.

\bibitem{FeldmanNS11a}
M.~Feldman, J.~Naor, and R.~Schwartz.
\newblock A unified continuous greedy algorithm for submodular maximization.
\newblock In {\em Proceedings 52nd Annual IEEE Symposium on Foundations of
  Computer Science}, pages 570--579, 2011.

\bibitem{FisherNW78}
M.~L. Fisher, G.~L. Nemhauser, and L.~A. Wolsey.
\newblock An analysis of approximations for maximizing submodular set functions
  {\sc ii}.
\newblock {\em Mathematical Programming Study}, 8:73--87, 1978.

\bibitem{GargK07}
N.~Garg and J.~K{\"o}nemann.
\newblock Faster and simpler algorithms for multicommodity flow and other
  fractional packing problems.
\newblock {\em SIAM J. Comput.}, 37(2):630--652, 2007.

\bibitem{GharanV11}
S.~O. Gharan and J.~Vondr{\'a}k.
\newblock Submodular maximization by simulated annealing.
\newblock In {\em Proceedings 22nd Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 1098--1116, 2011.

\bibitem{GoelKTW09}
G.~Goel, C.~Karande, P.~Tripathi, and L.~Wang.
\newblock Approximability of combinatorial problems with multi-agent submodular
  cost functions.
\newblock In {\em Proceedings 50th Annual IEEE Symposium on Foundations of
  Computer Science}, pages 755--764, 2009.

\bibitem{GoemansHIM09}
M.~X. Goemans, N.~J.~A. Harvey, S.~Iwata, and V.~S. Mirrokni.
\newblock Approximating submodular functions everywhere.
\newblock In {\em Proceedings 20th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 535--544, 2009.

\bibitem{GoundanS07}
P.~R. Goundan and A.~S. Schulz.
\newblock Revisiting the greedy approach to submodular set function
  maximization.
\newblock 2007.
\newblock Manuscript.

\bibitem{GrotschelLS81}
M.~Gr{\"o}tschel, L.~Lov{\'a}sz, and A.~Schrijver.
\newblock The ellipsoid method and its consequences in combinatorial
  optimization.
\newblock {\em Combinatorica}, 1(2):169--197, 1981.

\bibitem{Iwata03}
S.~Iwata.
\newblock A faster scaling algorithm for minimizing submodular functions.
\newblock {\em SIAM J. Comput.}, 32(4):833--840, 2003.

\bibitem{Iwata08}
S.~Iwata.
\newblock Submodular function minimization.
\newblock {\em Math. Program.}, 112(1):45--64, 2008.

\bibitem{IwataFF01}
S.~Iwata, L.~Fleischer, and S.~Fujishige.
\newblock A combinatorial strongly polynomial algorithm for minimizing
  submodular functions.
\newblock {\em J. ACM}, 48(4):761--777, 2001.

\bibitem{IwataN09}
S.~Iwata and K.~Nagano.
\newblock Submodular function minimization under covering constraints.
\newblock In {\em Proceedings 50th Annual IEEE Symposium on Foundations of
  Computer Science}, pages 671--680, 2009.

\bibitem{IwataO09}
S.~Iwata and J.~B. Orlin.
\newblock A simple combinatorial algorithm for submodular function
  minimization.
\newblock In {\em Proceedings 20th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 1230--1237, 2009.

\bibitem{KhullerMN99}
S.~Khuller, A.~Moss, and J.~Naor.
\newblock The budgeted maximum coverage problem.
\newblock {\em Inf. Process. Lett.}, 70(1):39--45, 1999.

\bibitem{KulikST09}
A.~Kulik, H.~Shachnai, and T.~Tamir.
\newblock Maximizing submodular set functions subject to multiple linear
  constraints.
\newblock In {\em Proceedings 20th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 545--554, 2009.

\bibitem{LeeMNS10}
J.~Lee, V.~S. Mirrokni, V.~Nagarajan, and M.~Sviridenko.
\newblock Maximizing nonmonotone submodular functions under matroid or knapsack
  constraints.
\newblock {\em SIAM J. Discrete Math.}, 23(4):2053--2078, 2010.

\bibitem{NemhauserW78}
G.~L. Nemhauser and L.~A. Wolsey.
\newblock Best algorithms for approximating the maximum of a submodular set
  function.
\newblock {\em Math. Operations Research}, 3(3):177--188, 1978.

\bibitem{NemhauserWF78}
G.~L. Nemhauser, L.~A. Wolsey, and M.~L. Fisher.
\newblock An analysis of approximations for maximizing submodular set functions
  {\sc i}.
\newblock {\em Mathematical Programming}, 14:265--294, 1978.

\bibitem{Orlin07}
J.~B. Orlin.
\newblock A faster strongly polynomial time algorithm for submodular function
  minimization.
\newblock In {\em Proceedings 12th International Conference on Integer
  Programming and Combinatorial Optimization}, pages 240--251, 2007.

\bibitem{PlotkinST95}
S.~A. Plotkin, D.~B. Shmoys, and \'{E}va Tardos.
\newblock Fast approximation algorithms for fractional packing and covering
  problems.
\newblock {\em Math. Operations Research}, 20:257--301, 1995.

\bibitem{Raghavan88}
P.~Raghavan.
\newblock Probabilistic construction of deterministic algorithms: Approximating
  packing integer programs.
\newblock {\em Journal of Computer and System Sciences}, 37(2):130--143, 1988.

\bibitem{RaghavanT87}
P.~Raghavan and C.~D. Thompson.
\newblock Randomized rounding: a technique for provably good algorithms and
  algorithmic proofs.
\newblock {\em Combinatorica}, 7(4):365--374, 1987.

\bibitem{Schrijver00}
A.~Schrijver.
\newblock A combinatorial algorithm minimizing submodular functions in strongly
  polynomial time.
\newblock {\em J. Comb. Theory, Ser. B}, 80(2):346--355, 2000.

\bibitem{Srinivasan99}
A.~Srinivasan.
\newblock Improved approximation guarantees for packing and covering integer
  programs.
\newblock {\em SIAM J. Comput.}, 29(2):648--670, 1999.

\bibitem{Sviridenko04}
M.~Sviridenko.
\newblock A note on maximizing a submodular set function subject to a knapsack
  constraint.
\newblock {\em Oper. Res. Lett.}, 32(1):41--43, 2004.

\bibitem{SvitkinaF08}
Z.~Svitkina and L.~Fleischer.
\newblock Submodular approximation: Sampling-based algorithms and lower bounds.
\newblock In {\em Proceedings 49th Annual IEEE Symposium on Foundations of
  Computer Science}, pages 697--706, 2008.

\bibitem{Vondrak08}
J.~Vondr{\'a}k.
\newblock Optimal approximation for the submodular welfare problem in the value
  oracle model.
\newblock In {\em Proceedings 40th Annual ACM Symposium on Theory of
  Computing}, pages 67--74, 2008.

\bibitem{Vondrak09}
J.~Vondr{\'a}k.
\newblock Symmetry and approximability of submodular maximization problems.
\newblock In {\em Proceedings 50th Annual IEEE Symposium on Foundations of
  Computer Science}, pages 651--670, 2009.

\bibitem{Wolsey82a}
L.~A. Wolsey.
\newblock Maximising real-valued submodular functions: Primal and dual
  heuristics for location problems.
\newblock {\em Math. Operations Research}, 7(3):410--425, 1982.

\bibitem{Young95}
N.~E. Young.
\newblock Randomized rounding without solving the linear program.
\newblock In {\em Proceedings 6th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 170--178, 1995.

\end{thebibliography}


\appendix
\section{Submodular Maximization with Binary Packing Constraints} \label{appsec:0-1}
We study the special setting of monotone submodular maximization
under binary packing constraints, that is, when  instead of . Note that we
assume without loss of generality that . We
demonstrate that our multiplicative updates approach from
Section~\ref{sec:GeneralPacking} can be utilized to attain an
improved approximation guarantee for the underlying setting.
Specifically, we prove the following theorem.

\begin{theorem} \label{th:MainResult3}
There is a deterministic polynomial-time algorithm that achieves
an approximation guarantee of  for
maximizing a monotone submodular function under binary packing
constraints.
\end{theorem}

Our approach for treating this case is identical to that of the
general case. We employ a multiplicative updates algorithm that is
identical to the algorithm presented for the general case with two
exceptions:
\begin{enumerate}
\item Line~\ref{alg:StopCond}: the first condition is changed to
 instead of .

\item Line~\ref{alg:WeightUpdate}: the weights update is changed
to  instead of .
\end{enumerate}

We now prove that the modified algorithm for the binary case
outputs a feasible solution and attains the claimed approximation
ratio. Essentially, these results follow the analogous proofs of
the general case with some minor adjustments.

\begin{lemma} \label{lemma:0-1-Feasibility}
The modified algorithm outputs a feasible solution.
\end{lemma}
\begin{proof}
Let us focus on the solution  when the main loop terminates.
Clearly, if  respects the packing constraints then the returned
solution also respects them. Thus, let us consider the case that
 is infeasible. We next argue that  became infeasible only
at the last iteration of the loop in which element  was
selected. Consequently, by inspecting the last two lines of the
algorithm, one can conclude that the returned solution must be
feasible.

For the purpose of establishing the previously mentioned argument,
let  be the first element that induces a violation in some
constraint. Specifically, suppose  induces a violation in
constraint  at iteration . This implies that  since all the entries of  are binary.
Therefore,

where the second equality is due to the fact that . This implies that ,
and hence, by inspecting the (modified) main loop stopping
condition, we know that the loop must have terminated immediately
after element  was selected.~
\end{proof}

\begin{lemma} \label{lemma:0-1-Approx}
The modified algorithm archives -approximation by using .
\end{lemma}
\begin{proof}
Suppose the main loop terminates after  iterations. Notice that
when the loop terminates either  or . One can easily demonstrate that in the
former case, and in fact whenever , the
returned solution is -approximation to the optimal one. Thus,
in the remainder of the proof, we shall assume that  and that the loop terminates with .

We concentrate on upper bounding the value of . For
this purpose, we analyze the change in 
along the loop iterations. Observe that for any ,

The first inequality can be obtained by plugging  and  to the
inequality , which is known to be valid for any
 and , while the last equality results
from the definition of . By
Claim~\ref{claim:OPTBound}, we know that  in case . The latter condition clearly holds since , and  for any  under
consideration. Therefore,

where the last inequality is due to the fact that . The resulting recursive definition can be used, in
conjunction with the base case , to upper bound
 by

Recall that we assumed that the loop terminated with . This lower bound on  can be utilized,
together with the upper bound on , to yield

where the last inequality is due to the
Claim~\ref{claim:SubmodularBound}. Noting that , one
can use simple algebraic manipulations and obtain that .~
\end{proof}

\end{document}
