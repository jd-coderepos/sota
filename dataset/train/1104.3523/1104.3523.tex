
\documentclass[twocolumn, compsocconf]{IEEEtran}
\usepackage{cite}

\usepackage{mdwmath}
\usepackage{mdwtab}

\usepackage{url}

\usepackage[latin1]{inputenc}

\usepackage{multirow}

\usepackage{ifpdf}
\ifpdf\usepackage[raiselinks=false,colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue,bookmarksopen=true,hyperfootnotes=false,pdftex]{hyperref}\else
\usepackage[raiselinks=false,colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue,bookmarksopen=true,hyperfootnotes=false,dvips]{hyperref}\fi

\usepackage{amsthm}
\usepackage{setspace}
\usepackage{calc}
\usepackage{xspace}
\usepackage{verbatim}

\usepackage{rotating}
\usepackage{epsfig}

\usepackage{amssymb, bm}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500
\usepackage{mathabx}
\usepackage{array}
\usepackage{fixltx2e}

\usepackage{pstricks}
\usepackage{pst-plot}
\usepackage{pst-eucl}

\usepackage{mathrsfs}
\usepackage[only,shortrightarrow]{stmaryrd}
\font\quotefonti=ptmri8t scaled\magstep1
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage{url}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]

\newtheorem{definition}{Definition}[section]
\newtheorem{rules}{Rule}\newtheorem{condition}{Condition}[section]

\newcommand{\comp}{\mbox{\ensuremath{\mspace{2mu}\circ\mspace{2mu}}}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\goto}{\rightarrow}

\newcommand{\reduc}{\ensuremath{\psi}}\newcommand{\reducMap}{\ensuremath{\psi}}\newcommand{\util}{\ensuremath{\mu}\xspace}
\newcommand{\demand}{\ensuremath{\eta}\xspace}
\newcommand{\exec}{\ensuremath{\chi}\xspace}
\DeclareMathOperator{\rank}{rank}
\newcommand{\dualMap}{\ensuremath{\varphi}}
\newcommand{\dual}{\ensuremath{\varphi}}
\newcommand{\packMap}{\ensuremath{\pi}}
\newcommand{\pack}{\ensuremath{\pi}}
\newcommand{\vareps}{\ensuremath{\varepsilon}\xspace}
\newcommand{\Nat}{\ensuremath{\mathbb{N}}\xspace}
\newcommand{\Real}{\ensuremath{\mathbb{R}}\xspace}

\newcommand{\liuLay}{\ensuremath{\mathcal{LL}}\xspace}

\newcommand{\sched}{\ensuremath{\Sigma}\xspace}
\newcommand{\unitPower}{\ensuremath{\mathscr{U}\!}\xspace}
\newcommand{\powSet}{\ensuremath{\mathscr{P}}\xspace}
\newcommand{\serv}{\ensuremath{\sigma}}
\newcommand{\clientOf}{\ensuremath{\Gamma}\xspace}
\newcommand{\futSet}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\servSet}{\ensuremath{\mathcal{T}}}
\newcommand{\jobSet}{\ensuremath{\mathcal{J}}}
\newcommand{\calF}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\procSet}{\ensuremath{\Pi}\xspace}
\newcommand{\taskSet}{\ensuremath{\Gamma}\xspace}
\newcommand{\ready}{\ensuremath{\mathbb{Q}}\xspace}

\def\taskdef(#1,#2,#3){\mbox{\ensuremath{\tau_{#1}\att(#2,#3)}}\xspace
}\newcommand{\att}{\ensuremath{\mspace{-5mu}:\mspace{-5mu}}}

\newcommand{\redc}[1]{\textcolor{red}{#1}}
\newcommand{\bluec}[1]{\textcolor{blue}{#1}}
\newenvironment{redpar}{\color{red}}{\color{red}}
\newenvironment{bluepar}{\color{blue}}{\color{blue}}

\newlength{\ytrans}
\newlength{\ytmpc}

\newlength{\xtmp}
\newlength{\ytmp}
\newlength{\xtmpa}
\newlength{\ytmpa}
\newlength{\xtmpb}
\newlength{\ytmpb}
\newcommand{\subscr}{}

\newlength{\execWidth}

\newcounter{proc}
\newcounter{step}

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{
An Optimal Real-Time Scheduling Approach:\\From Multiprocessor to Uniprocessor
}





\author{\IEEEauthorblockN{Paul Regnier, George Lima, Ernesto Massa\\}
  \IEEEauthorblockA{Computer Science Department --
    Distributed Systems Laboratory (LaSiD) --
    Federal University of Bahia, Brazil \\
    Email: \{pregnier, gmlima, ernestomassa\}@ufba.br}
}


\maketitle

\begin{abstract}
  An optimal solution to the problem of scheduling real-time tasks on a set of
  identical processors is derived. The described approach is based on solving an
  equivalent uniprocessor real-time scheduling problem. Although there are other
  scheduling algorithms that achieve optimality, they usually impose prohibitive
  preemption costs. Unlike these algorithms, it is observed through simulation
  that the proposed approach produces no more than three preemptions points per
  job.
\end{abstract}

\begin{IEEEkeywords}
  Real-Time, Multiprocessor, Scheduling, Server
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle

\section{Introduction}\label{sec:introduction}

\subsection{Motivation}\label{sec:motiv}

Scheduling  real-time tasks on  processors is a problem that has taken
considerable attention in the last decade. The goal is to find a feasible
schedule for these tasks, that is a schedule according to which no task misses
its deadlines. Several versions of this problem have been addressed and a number
of different solutions have been given. One of the simplest versions assumes a
periodic-preemptive-independent task model with implicit deadlines, PPID for
short. According to the PPID model each task is independent of the others, jobs
of the same task are released periodically, each job of a task must finish
before the release time of its successor job, and the system is fully
preemptive.
 
A scheduling algorithm is considered optimal if it is able to find a feasible
schedule whenever one exists. Some optimal scheduling algorithms for the PPID
model have been found. For example, it has been shown that if all tasks share
the same deadline \cite{McNaughton59}, the system can be optimally scheduled
with a very low implementation cost. The assumed restriction on task deadlines,
however, prevents the applicability of this approach. Other optimal algorithms
remove this restriction but impose a high implementation cost due to the
required number of task preemptions \cite{Baruah96, Cho06, Levin2010}. It is
also possible to find trade-offs between optimality and preemption cost
\cite{Andersson08, Massa10, Andersson08b, Bletsas09}.

Optimal solutions for the scheduling problem in the PPID model are able to
create preemption points that make it possible task migrations between
processors allowing for the full utilization of the system. As illustration
consider that there are three tasks, ,  and , to be
scheduled on two processors. Suppose that each of these tasks requires  time
units of processor and must finish  time units after they are released.
Also, assume that all three tasks have the same release time. As can be seen in
Figure \ref{fig:probSched}, if two of these tasks are chosen to execute at their
release time and they are not preempted, the pending task will miss its
deadline. As all tasks share the same deadline in this example, the approach by
McNaughton \cite{McNaughton59} can be applied, as illustrated in the figure. If
this was not the case, generating possibly infinitely many preemption points
could be a solution as it is shown by other approaches \cite{Baruah96, Cho06,
  Levin2010}. In this work we are interested in a more flexible solution.

\setlength{\execWidth}{0.37\psyunit}

\def\schedAxes(#1,#2){\setcounter{step}{#2}\multido{\nl=0+1}{\thestep}{\rput(\nl,-.25){\footnotesize \nl}}\multido{\i=0+1}{#1}{\setcounter{proc}{\i+1}\setlength{\ytmpa}{\i\psyunit+.14\psyunit}\setlength{\ytmpa}{\i\psyunit}\setlength{\ytmpb}{\i\psyunit-.07\psyunit}\setlength{\xtmpa}{#2\psxunit-0.5\psxunit}\psline[linewidth=0.5pt]{->}(0,\ytmpa)(\xtmpa,\ytmpa)\multido{\nt=0+1}{#2}{
      \psline[linestyle=solid,linewidth=0.5pt](\nt,\ytmpa)(\nt,\ytmpb)}}}\def\jobLocExec#1#2(#3,#4,#5){\setcounter{proc}{#5-1}\setlength{\xtmpa}{#3\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\xtmpb}{#3\psxunit+#4\psxunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth}\psset{linecolor=black,linestyle=solid}\psframe[linecolor=black,linestyle=solid,fillstyle=none]
  (\xtmpa,\ytmpa)(\xtmpb,\ytmpb)\ifthenelse{\equal{#1}{}}{}{}{\setlength{\xtmpa}{#3\psxunit+{#4\psxunit*\real{0.5}}}\setlength{\ytmpb}{\theproc\psyunit+0.5\execWidth}\ifthenelse{\equal{#2}{}}{\renewcommand{\subscr}{#1}}{\renewcommand{\subscr}{#1}}\rput(\xtmpa,\ytmpb){\footnotesize \subscr}}}\def\jobdeadline#1(#2,#3){\setcounter{proc}{#3-1}\setlength{\xtmpa}{#2\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth+0.35\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=-*,arrowsize=4pt]
  (\xtmpa,\ytmpa)(\xtmpa,\ytmpb)\ifthenelse{\equal{#1}{}}{}{\setlength{\ytmpa}{\theproc\psyunit}\uput{.3em}[270](\xtmpa,\ytmpa){\vphantom{}\footnotesize #1}}}\begin{figure}[tbh]
  \centering \subfloat[C misses its deadline]{\psset{xunit=0.8cm, yunit=1cm}\begin{pspicture*}(-.2,-0.4)(4.,1.8)\schedAxes(2,4)\jobLocExec{}(0, 2, 1)\jobLocExec{}(0, 2, 2)\jobLocExec{}(2, 1, 1)\jobdeadline{}(3,2)\jobdeadline{}(3,1)\end{pspicture*}
  }\hspace{.1cm}\subfloat[Correct schedule]{\psset{xunit=0.8cm, yunit=1cm}\begin{pspicture*}(-.2,-0.4)(4.,1.8)\schedAxes(2,4)\jobLocExec{}(0, 2, 1)\jobLocExec{}(1, 2, 2)\jobLocExec{}(0, 1, 2)\jobLocExec{}(2, 1, 1)\jobdeadline{}(3,2)\jobdeadline{}(3,1)\end{pspicture*}
   }\caption{ A deadline miss occurs in case (a), but not in case (b).}
   \label{fig:probSched}
\end{figure}

\subsection{Contribution}


In the present work, we define a real-time task as an infinite sequence of
jobs. Each job represents a piece of work to be executed on one or more
processors. A job is characterized by its release time , time after which it
can be executed, and its deadline , time by which it must be completed in
order for the system to be correct. Also, we assume that the deadline of a job
is equal to the release time of the next job of the same task. However,
differently from the PPID model, we do not assume that tasks are necessarily
periodic. Instead, we assume that tasks have a fixed-utilization, i. e. each job
of a task utilizes a fixed processor bandwidth within the interval between its
release time and deadline. For example, a job of a task with utilization  of processor requires  execution time. Note that according to the
PPID model, the value  is equal to the period of the periodic task, which
makes the model assumed in this paper slightly more general than the PPID model.

The proposed approach is able to optimally schedule a set of fixed-utilization
tasks on a multiprocessor system. The solution we describe does
not impose further restrictions on the task model and only a few preemption
points per job are generated. The idea is to reduce the real-time multiprocessor
scheduling problem into an equivalent real-time uniprocessor scheduling
problem. After solving the latter, the found solution is transformed back to a
solution to the original problem. This approach seems very attractive since it
makes use of well known results for scheduling uniprocessor systems.

Consider the illustrative system with 3-tasks previously given. We show that
scheduling this system on two processors is equivalent to scheduling another
3-task system with tasks ,  and  on one
processor. Each star task requires one unit of time and has the same deadline as
the original task, that is the star tasks represent the slack of the original
ones. As can be seen in Figure \ref{fig:dualSchedEx}, the basic scheduling rule
is the following. Whenever the star task executes on the transformed system, its
associated original task does not execute on the original system. For example,
when  is executing on the transformed system, task  is not
executing on the original system.

\begin{figure}[tbh]
  \centering \label{fig:validSched}
  \centering \setlength{\execWidth}{0.42\psyunit}

  \def\schedAxes(#1,#2){\setcounter{step}{#2}\multido{\nl=0+1}{\thestep}{\rput(\nl,-.25){\footnotesize \nl}}\multido{\i=0+1}{#1}{\setcounter{proc}{\i+1}\setlength{\ytmpa}{\i\psyunit+.14\psyunit}\setlength{\ytmpa}{\i\psyunit}\setlength{\ytmpb}{\i\psyunit-.07\psyunit}\setlength{\xtmpa}{#2\psxunit-0.5\psxunit}\psline[linewidth=0.5pt]{->}(0,\ytmpa)(\xtmpa,\ytmpa)\multido{\nt=0+1}{#2}{
        \psline[linestyle=solid,linewidth=0.5pt](\nt,\ytmpa)(\nt,\ytmpb)}}}\def\jobLocExec#1#2(#3,#4,#5){\setcounter{proc}{#5-1}\setlength{\xtmpa}{#3\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\xtmpb}{#3\psxunit+#4\psxunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth}\psset{linecolor=black,linestyle=solid}\psframe[linecolor=black,linestyle=solid,fillstyle=none]
    (\xtmpa,\ytmpa)(\xtmpb,\ytmpb)\ifthenelse{\equal{#1}{}}{}{}{\setlength{\xtmpa}{#3\psxunit+{#4\psxunit*\real{0.5}}}\setlength{\ytmpb}{\theproc\psyunit+0.5\execWidth}\ifthenelse{\equal{#2}{}}{\renewcommand{\subscr}{#1}}{\renewcommand{\subscr}{#1}}\rput(\xtmpa,\ytmpb){\footnotesize \subscr}}}\def\jobdeadline#1(#2,#3){\setcounter{proc}{#3-1}\setlength{\xtmpa}{#2\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth+0.35\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=-*,arrowsize=4pt]
    (\xtmpa,\ytmpa)(\xtmpa,\ytmpb)\ifthenelse{\equal{#1}{}}{}{\setlength{\ytmpa}{\theproc\psyunit}\uput{.3em}[270](\xtmpa,\ytmpa){\vphantom{}\footnotesize #1}}}\psset{xunit=1.cm, yunit=1.cm}\begin{pspicture*}(-1.2,-0.4)(4.,3)\schedAxes(3,4)\jobLocExec{}(0, 1, 3)\jobLocExec{}(1, 1, 3)\jobLocExec{}(2, 1, 3)
    \jobLocExec{}(0, 2, 1)\jobLocExec{}(0, 1, 2)\jobLocExec{}(2, 1, 1)\jobLocExec{}(1, 2, 2)\jobdeadline{}(3,3)\jobdeadline{}(3,2)\jobdeadline{}(3,1)\end{pspicture*}
  \caption{Scheduling equivalence of ,   on one
    processor and , ,  on two processors.}
  \label{fig:dualSchedEx}
\end{figure}


The illustrative example gives only a glimpse of the proposed approach and does
not capture the powerfulness of the solution described in this document. For
example, if the illustrative example had four tasks instead of three, the
scheduling rule could not be applied straightforwardly. For such cases, we show
how to aggregate tasks so that the reduction to the uniprocessor scheduling
problem is still possible. For more general cases, a series of system
transformation, each one generating a system with fewer processors, may be
applied. Once a system with only one processor is obtained, the well known EDF
algorithm is used to generate the correct schedule. Then, it is shown that this
schedule can be used to correctly generate the schedule for the original
multiprocessor system.

\subsection{Structure}

In the remainder of this paper we detail the proposed approach. The notation and
the assumed model of computation are described in Section
\ref{sec:model}. Section \ref{sec:server} presents the concept of servers, which
are a means to aggregate tasks (or servers) into a single entity to be
scheduled. In Section \ref{sec:virtualSched} it is shown the rules to transform
a multiprocessor system into an equivalent one with fewer processors and the
scheduling rules used. The correctness of the approach is also shown in this
section. Then, experimental results collected by simulations are presented in
Section \ref{sec:evaluation}. Finally, Section \ref{sec:relatedWork} gives a
brief summary on related work and conclusions are drawn in Section
\ref{sec:conclusion}.


\section{System Model and Notation}\label{sec:model}

\subsection{Fixed-Utilization Tasks}\label{sec:fixedUtilTask}

As mentioned earlier, we consider a system comprised of  real-time and
independent tasks, each of which defines an infinite sequence of released
jobs. More generally, a job can be defined as follows.

\begin{definition}[Job]
  A real-time job, or simply, job, is a finite sequence of instructions to be
  executed. If  is a job, it admits a release time, denoted , an
  execution requirement, denoted , and a deadline, denoted .
\end{definition}

In order to represent possibly non-periodic execution requirements, we introduce
a general real-time object, called fixed-utilization task, or task for short,
whose execution requirement is specified in terms of processor utilization
within a given interval. Since a task shall be able to execute on a single
processor, its utilization cannot be greater than one.

\begin{definition}[Fixed-Utilization Task]\label{dfn:fixedUtilTask}
  Let  be a positive real not greater than one and let  be a countable and
  unbounded set of non-negative reals. The fixed-utilization task  with
  utilization  and deadline set , denoted , satisfies the
  following properties: (i) a job of  is released at time  if and only
  if ; (ii) if  is released at time , then ; and (iii) .
\end{definition}

Given a fixed-utilization task , we denote  and
 its utilization and its deadline set, respectively.


As a simple example of fixed-utilization task, consider a periodic task 
characterized by three attributes: (i) its start time ; (ii) its period
; and (iii) its execution requirement . Task  generates an infinite
collection of jobs each of which released at  and with deadline at
, . Hence,  can be seen as a fixed-utilization task
with start time at , utilization  and set of deadlines
, which requires exactly
 of processor during periodic time intervals , for  in .  As will be clearer later on, the concept of
fixed-utilization task will be useful to represent non-periodic processing
requirements, such as those required by groups of real-time periodic tasks.


\subsection{Fully Utilized System}
\label{sec:fullyUtilSyst}

We say that a set of  fixed-utilization tasks fully utilizes a system
comprised of  identical processors if the sum of the utilizations of the 
tasks exactly equals . Hereafter, we assume that the set of 
fixed-utilization tasks fully utilizes the system.

It is important to mention that this assumption does not restrict the
applicability of the proposed approach.  For example, if a job  of a task is
supposed to require  time units of processor but it completes consuming
only  processor units, then the system can easily simulate 
of its execution by blocking a processor accordingly. Also, if the maximum
processor utilization required by the task set is less than , dummy tasks can
be created to comply with the full utilization assumption. Therefore, we
consider hereafter that the full utilization assumption holds and so each job
 executes exactly for  time units during .


\subsection{Global Scheduling} 

Jobs are assumed to be enqueued in a global queue and are scheduled to execute
on a multiprocessor platform , comprised of  identical
processors. We consider a global scheduling policy according to which tasks are
independent, preemptive and can migrate from a processor to another during their
executions. There is no penalty associated with preemptions or migrations.

\begin{definition}[Schedule]
  For any collection of jobs, denoted , and multiprocessor platform
  , the multiprocessor schedule  is a mapping from  to  with  equal
  to one if schedule  assigns job  to execute on processor  at
  time , and zero otherwise.
\end{definition}

Note that by the above definition, the execution requirement of a job  at
time  can be expressed as


\begin{definition}[Valid Schedule]
  A schedule  of a job set  is valid if (i) at any time, a
  single processor executes at most one job in ; (ii) any job in
   does not execute on more than one processor at any time; (iii) any
  job  can only execute at time  if  and .
\end{definition}

\begin{definition}[Feasible Schedule]
  Let  be a schedule of a set of jobs . The schedule
   is feasible if it is a valid schedule and if all the jobs in
   finish executing by their deadlines.
\end{definition}

We say that a job is feasible in a schedule  if it finishes executing by
its deadline, independently of the feasibility of . That is, a job can
be feasible in a non-feasible schedule. However, if  is feasible, then
all jobs scheduled in  are necessarily feasible. Also, we say that a job
 is active at time  if  and .  As a
consequence, a fixed-utilization task admits a unique feasible and active job at
any time.



\section{Servers}\label{sec:server}

As mentioned before, the derivation of a schedule for a multiprocessor system will
be done via generating a schedule for an equivalent uniprocessor system. One of
the tools for accomplishing this goal is to aggregate tasks into servers, which
can be seen as fixed-utilization tasks equipped with a scheduling mechanism.

As will be seen, the utilization of a server is not greater than one. Hence, in
this section we will not deal with the multiprocessor scheduling problem. The
focus here is on precisely defining the concept of servers (Section
\ref{sec:serverModel}) and showing how they correctly schedule the
fixed-utilization tasks associated to them (Section
\ref{sec:predictability}). In other words, the reader can assume in this section
that there is a single processor in the system. Later on we will show how
multiple servers are scheduled on a multiprocessor system.

\subsection{Server model and notations}
\label{sec:serverModel}

A fixed-utilization server associated to a set of fixed-utilization tasks is
defined as follows:


\begin{definition}[Fixed-Utilization Server]\label{dfn:server}
  Let  be a set of fixed-utilization tasks with total utilization given
  by
  
  A fixed-utilization server  associated to , denoted
  , is a fixed-utilization task with utilization
  , set of deadlines , equipped with a scheduling policy used to schedule
  the jobs of the elements in . For any time interval , where
  ,  is allowed to execute exactly for
   time units.
\end{definition}

Given a fixed-utilization server , we denote  the set of
fixed-utilization tasks scheduled by  and we assume that this set is
statically defined before the system execution. Hence, the utilization of a
server, simply denoted , can be consistently defined as equal to
. Note that, since servers are fixed-utilization tasks, we
are in condition to define the server of a set of servers. For the of sake of
conciseness, we call an element of  a client task of  and we
call a job of a client task of  a client job of .  If  is a server and
 a set of servers, then  and
.

For illustration consider Figure \ref{fig:servSet}, where  is a set
comprised of the three servers ,  and
 associated to the fixed-utilization tasks ,  and
, respectively. The numbers between brackets represent processor
utilizations. If  is the server in charge of scheduling
,  and , then we have  and .

\begin{figure}[t]
  \centering \psset{xunit=0.74cm, yunit=0.74cm}\begin{pspicture*}(0,-0.4)(7,3)\psset{fillstyle=none,linewidth=.5pt}\psellipse[fillcolor=lightgray](2.5,1.2)(1.8,1.4)\uput{0}[0](3.8,2.4){\vphantom{} }\uput{0}[0](1.2,1.9){\vphantom{} }\uput{0}[0](1.6,1.2){\vphantom{} }\uput{0}[0](1.3,0.5){\vphantom{} }\end{pspicture*}
  \caption{A three-server set. The utilization  of a server  or a set of
    server  is indicated by the notation  and
    , respectively. \label{fig:servSet}}
\end{figure}


As can be seen by Definition \ref{dfn:server}, the server  associated to
 may not have all the elements of . Indeed, the number of elements in  depends on a
server deadline assignment policy:

\begin{definition}[Server Deadline Assignment\label{dfn:deadlineAssignt}]
  A deadline of a server  at time , denoted , is given by
  the earliest deadline greater than  among all client jobs of  not yet
  completed at time . This includes those jobs active at  or the not yet
  released jobs at . More formally,
  
  where  is the set of all jobs of servers in .
\end{definition}

Note that by Definitions \ref{dfn:fixedUtilTask} and \ref{dfn:deadlineAssignt},
the execution requirement of a server  in any interval  equals
, where  and  are two consecutive deadlines in
. As a consequence, the execution requirement of a job  of a
server , released at time , equals  for all . The budget of  at any
time , denoted as , is replenished to  at all . The budget of a server represents the processing time available for
its clients. Although a server never executes itself, we say that a server 
is executing at time  in the sense that one of its client tasks consumes
its budget  at the same rate of its execution.

Recall from Section \ref{sec:fixedUtilTask} that a job of an fixed-utilization
task is feasible in a schedule  if it meets its deadline. However, the
feasibility of a server does not imply the feasibility of its client tasks. For
example, consider two periodic tasks  and
, with periods equal to 2 and 3 and
utilizations  and , respectively. Assume
that their start times are equal to zero. Consider a server  scheduling these
two tasks on a dedicated processor and let . Thus, the budget of  during  equals . Let  be a schedule of  and  in which  is
feasible.  The feasibility of server  implies that  acquires the processor
for at least  units of time during , since  is a deadline of
. Now, suppose that the scheduling policy used by  to schedule its client
tasks gives higher priority to  at time .  Then,  will
consume one unit of time before  begins its execution. Therefore, the
remaining budget  will be insufficient to complete  by
, its deadline. This illustrates that a server can be feasible while the
generated schedule of its clients is not feasible.


\subsection{EDF Server}
\label{sec:predictability}

In this section, we define an EDF server and shows that EDF servers are
predictable in the following sense.

\begin{definition}[Predictable Server]\label{dfn:predictableServer}
  A fixed-utilization server  is predictable in a schedule  if its
  feasibility in  implies the feasibility of all its client jobs.
\end{definition}

\begin{definition}[EDF Server]\label{dfn:edfServer}
  An EDF server is a fixed-utilization server , defined according to
  Definitions \ref{dfn:server} and \ref{dfn:deadlineAssignt}, which schedules
  its client tasks by EDF.
\end{definition}

For illustration, consider a set of three periodic tasks .  Since , we can define an EDF server 
to schedule  such that  and . Figure \ref{fig:budget} shows both the evolution of  during
interval  and the schedule  of  by  on a single
processor. In this figure,  represents the -th job of . Observe
here that . Indeed, deadlines
 of  and  of  are not in , since  and  are
completed at time  and , respectively.

It is worth noticing that the deadline set of a server could be defined to
include all deadlines of its clients. However, this would generate unnecessary
preemption points.

\begin{figure}[t]
  \centering \setlength{\execWidth}{0.37\psyunit}

\def\schedAxes(#1,#2){\setcounter{step}{((#2+1)/2)}\multido{\nl=0+2}{\thestep}{\uput{.5em}[270](\nl,0){\vphantom{}\footnotesize \nl}}\multido{\i=0+1}{#1}{\setcounter{proc}{\i+1}\setlength{\ytmpa}{\i\psyunit+.14\psyunit}\setlength{\ytmpa}{\i\psyunit}\setlength{\ytmpb}{\i\psyunit-.07\psyunit}\setlength{\xtmpa}{#2\psxunit-0.5\psxunit}\psline[linewidth=0.5pt]{->}(0,\ytmpa)(\xtmpa,\ytmpa)\multido{\nt=0+1}{#2}{
      \psline[linestyle=solid,linewidth=1pt](\nt,\ytmpa)(\nt,\ytmpb)}\setcounter{step}{(#2-1)*4}\multido{\nt=0+.25}{\thestep}{
      \psline[linestyle=solid,linewidth=0.5pt](\nt,\ytmpa)(\nt,\ytmpb)}}}\def\jobLocExec#1#2(#3,#4,#5){\setcounter{proc}{#5-1}\setlength{\xtmpa}{#3\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\xtmpb}{#3\psxunit+#4\psxunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth}\psset{linecolor=black,linestyle=solid}\psframe[linecolor=black,linestyle=solid,fillstyle=none]
  (\xtmpa,\ytmpa)(\xtmpb,\ytmpb)\ifthenelse{\equal{#1}{}}{}{}{\setlength{\xtmpa}{#3\psxunit+{#4\psxunit*\real{0.5}}}\setlength{\ytmpb}{\theproc\psyunit+0.5\execWidth}\ifthenelse{\equal{#2}{}}{\renewcommand{\subscr}{#1}}{\renewcommand{\subscr}{#1}}\rput(\xtmpa,\ytmpb){\scriptsize \subscr}}}\def\jobdeadline#1(#2,#3){\setcounter{proc}{#3-1}\setlength{\xtmpa}{#2\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth+0.35\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=-*,arrowsize=4pt]
  (\xtmpa,\ytmpa)(\xtmpa,\ytmpb)\ifthenelse{\equal{#1}{}}{}{\setlength{\ytmpa}{\theproc\psyunit+1.74\psyunit}\uput{.3em}[270](\xtmpa,\ytmpa){\vphantom{}\footnotesize #1}}}
\psset{xunit=.68cm, yunit=.64cm}\begin{pspicture*}(-.5,-1.)(12.7,7)\setlength{\ytrans}{2.7\psyunit}\setlength{\ytmpa}{\ytrans-0.1\psyunit}\setlength{\ytmpb}{\ytrans+3.7\psyunit}\uput{.01em}[90](.3,\ytmpb){\vphantom{}\footnotesize }\setlength{\ytmpb}{\ytrans+3.6\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=->,arrowsize=4pt](0,\ytmpa)(0,\ytmpb)\setlength{\ytmpb}{\ytrans+3.6\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=-<,arrowsize=4pt](3,\ytmpa)(3,\ytmpb)\psline[linestyle=solid,linewidth=0.5pt,arrows=-<,arrowsize=4pt](6,\ytmpa)(6,\ytmpb)\psline[linestyle=solid,linewidth=0.5pt,arrows=-<,arrowsize=4pt](8,\ytmpa)(8,\ytmpb)\psline[linestyle=solid,linewidth=0.5pt,arrows=-<,arrowsize=4pt](12,\ytmpa)(12,\ytmpb)\psline[linestyle=solid,linewidth=0.5pt,arrows=->](-0.1,\ytrans)(12.5,\ytrans)\uput{.5em}[270](0,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](2.25,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](3,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](5.25,\ytrans){\vphantom{}\footnotesize
    }\uput{.5em}[270](6,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](7.4,\ytrans){\vphantom{}\footnotesize
    }\uput{.5em}[270](8.1,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](11,\ytrans){\vphantom{}\footnotesize }\uput{.5em}[270](12,\ytrans){\vphantom{}\footnotesize }\setlength{\ytmpa}{\ytrans+2.25\psyunit}\uput{.3em}[180](0,\ytmpa){\vphantom{}\footnotesize }\setlength{\ytmpb}{\ytrans+1.5\psyunit}\uput{.3em}[180](0,\ytmpb){\vphantom{}\footnotesize }\setlength{\ytmpc}{\ytrans+3\psyunit}\uput{.3em}[180](0,\ytmpc){\vphantom{}\footnotesize }\psline[linestyle=solid,linewidth=1pt](0.0,\ytrans)(0.,\ytmpa)\psline[linestyle=solid,linewidth=1pt](0.,\ytmpa)(2.25,\ytrans)\psline[linestyle=solid,linewidth=1pt](2.25,\ytrans)(3,\ytrans)\psline[linestyle=dashed,linewidth=0.5pt](0,\ytmpb)(6,\ytmpb)\psline[linestyle=solid,linewidth=1pt](3,\ytrans)(3,\ytmpa)\psline[linestyle=solid,linewidth=1pt](3,\ytmpa)(5.25,\ytrans)\psline[linestyle=solid,linewidth=1pt](5.25,\ytrans)(6,\ytrans)\psline[linestyle=solid,linewidth=1pt](6,\ytrans)(6,\ytmpb)\psline[linestyle=solid,linewidth=1pt](6,\ytmpb)(7.5,\ytrans)\psline[linestyle=solid,linewidth=1pt](7.5,\ytrans)(8,\ytrans)\psline[linestyle=solid,linewidth=1pt](8.0,\ytrans)(8.,\ytmpc)\psline[linestyle=solid,linewidth=1pt](8.,\ytmpc)(11,\ytrans)\psline[linestyle=solid,linewidth=1pt](11,\ytrans)(12,\ytrans)\psline[linestyle=dashed,linewidth=0.5pt](0,\ytmpc)(8,\ytmpc)\schedAxes(1,13)\uput{.3em}[180](0,.3){\vphantom{}\footnotesize }\jobLocExec{}(0, 1, 1)\jobLocExec{}(1, 1, 1)\jobLocExec{}(2, 0.25, 1)\jobLocExec{}(3, 0.75, 1)\jobLocExec{}(3.75, 1, 1)\jobLocExec{}(4.75, 0.5, 1)\jobLocExec{}(6, 0.5, 1)\jobLocExec{}(6.5, 1, 1)\jobLocExec{}(8, 1, 1)\jobLocExec{}(9, 1, 1)\jobLocExec{}(10, 1, 1)\jobdeadline{}(3,1)\jobdeadline{}(4,1)\jobdeadline{}(6,1)\jobdeadline{}(8,1)\jobdeadline{}(9,1)\jobdeadline{}(12,1)\end{pspicture*}\caption{Budget management and schedule of an EDF server  with  and .}
\label{fig:budget}
\end{figure}

\begin{definition}\label{dfn:unitSet}
  A set  of fixed-utilization tasks is a unit set if
  . The server  associated to a unit set
   is a unit server.
\end{definition}

In order to prove that EDF servers are predictable, we first present some
intermediate results.

\begin{definition}\label{dfn:scaling}
  Let  be a server,  a set of servers with , and  a real such that . The
  -scaled server of  is the server with utilization 
  and deadlines equal to those of . The -scaled set of  is
  the set of the -scaled servers of server in .
\end{definition}

As illustration, consider  a set of servers with
, ,  and . The -scaled set of  is  with , , 
and .
  
\begin{lemma}\label{lem:scalingEquiv}
  Let  be a set of EDF servers with  and
   be its -scaled set. Define  and  as two EDF servers
  associated to  and  and consider that  and
   are their corresponding schedules, respectively. The schedule
   is feasible if and only if  is feasible.
\end{lemma}
  
\begin{IEEEproof}
  Suppose  feasible. Consider a deadline  in . Since 
  and  use EDF and ,  and  execute their
  client jobs in the same order. As a consequence, all the executions of servers
  in  during  must have a corresponding execution of a
  server in  during .

  Also, since  executes for  during  and , the execution time  of  during  satisfies
  .  Hence, a client job of  corresponding to an
  execution which completes in  before , completes before  in
  . Since  is feasible, this shows that  is feasible.

  To show that  is feasible if  is feasible the same reasoning
  can be made with a scale equal to 
\end{IEEEproof}

\begin{lemma}\label{lem:edfServSched}
  The schedule of a set of servers  produced by the EDF server  is feasible if and only if .
\end{lemma}

\begin{IEEEproof}
  The proof presented here is an adaptation of the proof of Theorem  from
  \cite{Liu73}. The difference between servers and tasks makes this presentation
  necessary.

  First, assume that . Let  be a time interval with
  no processor idle time, where  and  are two deadlines of servers in
  . By the assumed utilization, this time interval must exist. As the
  cumulated execution requirement within this interval is , a deadline miss must occur, which shows the necessary condition.

  Suppose now that  is the first deadline miss after time  and let 
  be the server whose job  misses its deadline at .  Let  be the start
  time of the latest idle time interval before . Assume that  if such
  a time does not exist. Also, let  be the earliest deadline in 
  after . Note that  otherwise no job would be released between
   and .
If  is not equal to zero, then the processor must be idle just before
  .  Indeed, if there were some job executing just before , it would be
  released after  and its release instant would be a deadline in
   occurring before  and after , which would contradict the
  definition of . Hence, only the time interval between  and  is to
  be considered.  There are two cases to be distinguished depending on whether
  some lower priority server executes within .

  \begin{figure}[h]
    \centering \psset{xunit=1cm, yunit=1cm}\begin{pspicture*}(0,-.6)(8,0.5)\psline[linestyle=solid,linewidth=0.5pt,arrows=->](0,0)(8,0)\multido{\nl=3.1+.1}{44}{\rput(\nl,0){\psline[linestyle=solid,linewidth=0.5pt](-.1,-.1)(.1,.1)} }\multido{\nl=0.1+.1}{9}{\rput(\nl,0){\psline[linestyle=solid,linewidth=0.5pt](-.1,-.1)(.1,.1)} }\uput{.5em}[270](2,0.7){ idle time}\uput{.5em}[270](1,-.1){}\psline[linestyle=solid,linewidth=1pt](1,-.14)(1,.14)\uput{.5em}[270](3,-.1){}\psline[linestyle=solid,linewidth=1pt](3.5,-.14)(3.5,.14)\uput{.5em}[270](3.5,-.1){}\psline[linestyle=solid,linewidth=1pt](3,-.14)(3,.14)\uput{.5em}[270](6.5,-.1){}\psline[linestyle=solid,linewidth=1pt](6.5,-.14)(6.5,.14)\uput{.5em}[270](7,-.1){}\psline[linestyle=solid,linewidth=1pt](7,-.14)(7,.14)\end{pspicture*}\caption{A deadline miss occurs for job  at time  and no job with
      lower priority than  executes before }
    \label{fig:deadlineMissA}
  \end{figure}

  \paragraph{Case 1} Illustrated by Figure \ref{fig:deadlineMissA}. Assume that
  no job of servers in  with lower priority than  executes
  within . Since there is no processor idle time between  and 
  and a deadline miss occurs at time , it must be that the cumulated
  execution time of all jobs in  released at or after  and
  with deadline less than or equal to  is strictly greater than
  . Consider servers  whose jobs have their release instants and
  deadlines within . Let  and  be the first release instant
  and the last deadline of such jobs, respectively.  The cumulated execution
  time of such servers during  equals .  As , , leading to a
  contradiction.
 
  \begin{figure}[h]
    \centering \psset{xunit=1cm, yunit=1cm}\begin{pspicture*}(0,-.6)(8,0.5)\psline[linestyle=solid,linewidth=0.5pt,arrows=->](0,0)(8,0)\multido{\nl=3.1+.1}{44}{\rput(\nl,0){\psline[linestyle=solid,linewidth=0.5pt](-.1,-.1)(.1,.1)} }\multido{\nl=0.1+.1}{9}{\rput(\nl,0){\psline[linestyle=solid,linewidth=0.5pt](-.1,-.1)(.1,.1)} }\uput{.5em}[270](2,0.7){ idle time}\uput{.5em}[270](1,-.1){}\psline[linestyle=solid,linewidth=1pt](1,-.14)(1,.14)\uput{.5em}[270](3,-.1){}\psline[linestyle=solid,linewidth=1pt](4,-.14)(4,.14)\uput{.5em}[270](4,-.1){}\psline[linestyle=solid,linewidth=1pt](3,-.14)(3,.14)\uput{.5em}[270](5.5,-.1){\vphantom{}}\psline[linestyle=solid,linewidth=1pt](5.5,-.14)(5.5,.14)\uput{.5em}[270](7,-.1){}\psline[linestyle=solid,linewidth=1pt](7,-.14)(7,.14)\end{pspicture*}\caption{ A deadline miss occurs for job  at time  and some lower
      priority job than  executes before }
    \label{fig:deadlineMissB}
  \end{figure}
  
  \paragraph{Case 2} Illustrated by Figure \ref{fig:deadlineMissB}. Assume that
  there exist client jobs of  with lower priority than  that execute
  within . Let  be the latest deadline after which no such jobs
  execute and consider  the release instant of . Since  misses its
  deadline, no job with lower priority than  can execute after . Thus, we
  must have . Also, there is no processor idle time in
  . Thus, for a deadline miss to occur at time , it must be that the
  cumulated execution time of all servers in  during  is
  greater than .

  Also, it must be that a lower priority job was executing just before .
  Indeed, if , a job with higher priority than , was executing just
  before , its release time  would be before  and no job with
  lower priority than  could have executed after , contradicting the
  minimality of . Thus, no job released before  and with higher
  priority than  executes between  and .  Hence, the jobs that
  contribute to the cumulated execution time during  must have higher
  priorities than  and must be released after . The cumulated
  requirement of such jobs of a server  is not greater than . Henceforth, since , the cumulated execution time of all servers during  cannot
  be greater than , reaching a contradiction.
\end{IEEEproof}

\begin{theorem}\label{thm:edfServPredict}
  An EDF server is predictable.
\end{theorem}

\begin{IEEEproof}
  Consider a set of servers  such that
   and assume that  is to be scheduled by an
  EDF server . Let  be the -scaled server set
  of . Hence, by Definition \ref{dfn:scaling}, we have
  . Let 
  be the EDF server associated to .  By Lemma \ref{lem:scalingEquiv},
  the schedule  of  by  is feasible if and only if the
  schedule  of  be  is feasible.  But,  schedules
  servers as EDF. Indeed, consider a release instant  of  at which the
  budget of  is set to . During the entire interval
  , the budget of  is strictly positive. This implies
  that  is not constrained by its budget during the whole interval . Thus,  behaves as if it has infinite budget and
  schedules its client servers according to EDF. Since, by Lemma
  \ref{lem:edfServSched}, a server set of utilization one is feasible by EDF,
  the schedule  produced by  is feasible and so is .
\end{IEEEproof}

It is worth saying that Theorem \ref{thm:edfServPredict} implicitly assumes that
server  executes on possibly more than one processor. The client servers of
 do not execute in parallel, though. The assignment of servers to processors
is carried out on-line and is specified in the next section.







\section{Virtual Scheduling}\label{sec:virtualSched}

In this section we present two basic operations, dual and packing, which are
used to transform a multiprocessor system into an equivalent uniprocessor
system. The schedule for the found uniprocessor system is produced on-line by
EDF and the corresponding schedule for the original multiprocessor system is
deduced straightforwardly by following simple rules. The transformation
procedure can generate one or more virtual systems, each of which with fewer
processors than the original (real) system.

The dual operation, detailed in Section \ref{sec:dualSched}, transforms a
fixed-utilization task  into another task  representing the slack
task of  and called the dual task of . That is  and the deadlines of  are equal to those of . As
 implies , the dual operation plays the
role of reducing the utilization of the system made of complementary dual tasks
as compared to the original system.

The packing operation, presented in Section \ref{sec:packing}, groups one or
more tasks into a server. As fixed-utilization tasks whose utilization do not
sum up more than  can be packed into a single server, the role of the packing
operation is to reduce the number of tasks to be scheduled.

By performing a pair of dual and packing operations, one is able to create a
virtual system with less processor and tasks. Hence, it is useful to have both
operations composed into a single one, called reduction operation, which will be
defined in Section \ref{sec:dualPackingSched}. As will be seen in Section
\ref{sec:reducCorrectness}, after performing a series of reduction operation,
the schedule of the multiprocessor system can be deduced from the (virtual)
schedule of the transformed uniprocessor system. Although a reduction from the
original system into the virtual ones is carried out off-line, the generation of
the multiprocessor schedule for the original system can be done on-line.
Section IV.E ilustrates the proposed approach with an example.

\subsection{Dual Operation}\label{sec:dualSched}

As servers are actually fixed-utilization tasks and will be used as a basic
scheduling mechanism, the dual operation is defined for servers.

\begin{definition}[Dual Server]\label{dfn:dualServer}
  Let  be a server with utilization  such that . The dual server of  is defined as the server  whose utilization
  , deadlines are equal to those of  and
  scheduling algorithm identical to that of . If  is a set of
  servers, then the dual set  of  is the set of servers
  which are duals of the servers in , i.e.  if and
  only if .
\end{definition}

Note that servers with utilization equal to  or  are not considered in
Definition \ref{dfn:dualServer}. This is not a problem since in these cases 
can straightforwardly be scheduled. Indeed, if  is a server with 
utilization, a processor can be allocated to  and by Theorem
\ref{thm:edfServPredict}, all clients of  meet their deadlines. In case that
 is a null-utilization server, it is enough to ensure that  never gets
executing.

We define the bijection  from a set of non-integer (neither zero nor one)
utilization servers  to its dual set  as the function
which associates to a server  its dual server , i.e .


\begin{definition}[Dual Schedule]\label{dfn:dualSchedule}
  Let  be a set of servers and  be its dual set.  Two
  schedules  of  and  of  are duals if,
  at any time, a server  in  executes in  if and only if
  its dual server  does not execute in .
\end{definition}

The following theorem relates the feasibility of a set of servers to the
feasibility of its dual set. It is enunciated assuming a fully utilized
system. However, recall from Section \ref{sec:fullyUtilSyst} that any system can
be extended to a fully utilized system in order to apply the results presented
here.

\begin{theorem}[Dual Operation]\label{thm:dualSched}
  Let  be a set of  servers
  with  and . The schedule  of
   on  processors is feasible if and only if its dual schedule
   is feasible on  processors.
\end{theorem}

\begin{IEEEproof}
  In order to prove the necessary condition, assume that a schedule of
   on  processors, , is feasible.  By Definition
  \ref{dfn:dualSchedule}, we know that  executes in  whenever
   does not execute in , and vice-versa. Now, consider the
  executions in  of a pair  and define a
  schedule  for the set  as follows:  always executes on the same processor in
  ;  executes in  at time  if and only if it
  executes at time  in ; and whenever  is not executing in
  ,  is executing in  on the same processor as
  .
  
  By construction, the executions of  and  in
   correspond to their executions in  and ,
  respectively. Also, in ,  and  execute on a single
  processor. Since  and  and  have
  the same deadlines, the feasibility of  implies the feasibility of
  . Since this is true for all pairs , we deduce that both
   and  are feasible. Furthermore, as by the definition
  of ,  processors are needed and by assumption 
  uses  processors,  can be constructed on  processors.

  The proof of the sufficient condition is symmetric and can be shown using
  similar arguments.
\end{IEEEproof}

Theorem \ref{thm:dualSched} does not establish any scheduling rule to generate
feasible schedules. It only states that determining a feasible schedule for a
given server set on  processors is equivalent to finding a feasible schedule
for the transformed set on  virtual processors.  Nonetheless, this theorem
raises an interesting issue. Indeed, dealing with  virtual processors
instead of  can be advantageous if . In order to illustrate this
observation, consider a set of three servers with utilization equal to
. Instead of searching for a feasible schedule on two processors, one can
focus on the schedule of the dual servers on just one virtual processor, a
problem whose solution is well known. In order to guarantee that dealing with
dual servers is advantageous, the packing operation plays a central role.

\subsection{Packing Operation}
\label{sec:packing}

As seen in the previous section, the dual operation is a powerful mechanism to
reduce the number of processors but only works properly if . If this is
not the case, one needs to reduce the number of servers to be scheduled,
aggregating them into servers. This is achieved by the packing operation, which
is formally described in this section.

\begin{definition}[Packed Server Set]\label{dfn:packedSet}
  A set of non-zero utilization servers  is packed if it is a
  singleton or if  and for any two distinct servers  and
   in , .
\end{definition}





\begin{definition}[Packing Operation]\label{dfn:packMap}
  Let  be a set of non-zero utilization servers. A packing operation
   associates a packed set of servers  to 
  such that the set collection  is a
  partition of .
\end{definition}



Note that a packing operation is a projection ()
since the packing of a packed set is the packed set itself.


An example of partition, produced by applying a packing operation on a set
 of  servers, is illustrated by the set  on the
top of Figure \ref{fig:dualSet}. In this example, the partition of  is
comprised of the three sets ,  and
. As an illustration of Definition \ref{dfn:packMap}, we
have, .

\begin{lemma}\label{lem:packMap}
  Let  be a set of non-zero utilization servers. If  is a
  packing operation on , then  and .
\end{lemma}

\begin{IEEEproof}
  A packing operation does not change the utilization of servers in 
  and so . To show the inequality,
  suppose that  with  natural and . As the utilization of a server is not greater than one, there
  must exist at least  servers in .
\end{IEEEproof}

The following lemma establishes an upper bound on the number of servers resulted
from packing an arbitrary number of non-zero utilization servers with total
utilization .

\begin{lemma}\label{lem:packReduc}
  If  is a set of non-zero utilization servers and  is
  packed, then .
\end{lemma}

\begin{IEEEproof}
  Let  and  for .  Since
   is packed, there exists at most one server in , say
  , such that . All other servers have utilization greater that
  .  Thus, . As , it
  follows that .
\end{IEEEproof}


\subsection{Reduction Operation}
\label{sec:dualPackingSched}

In this section we define the composition of the dual and packing operations. We
begin by noting that the following relation holds.

\begin{lemma}\label{lem:dualUtil}
  If  is a packed server set with more than one server, then
  .
\end{lemma}

\begin{IEEEproof}
  As  is packed, at least  servers have their
  utilization strictly greater than . Thus, at least all but one server in
   have utilization strictly less than
  . Hence, .
\end{IEEEproof}

According to Lemma \ref{lem:dualUtil}, the action of the dual operation applied
to a packed set allows for the generation of a set of servers whose total
utilization is less than the utilization of the original packed set, as
illustrated in Figure \ref{fig:dualSet}. Considering an integer utilization
server set , this makes it possible to reduce the number of servers
progressively by carrying out the composition of a packing operation and the
dual operation until  is reduced to a set of unit servers. Since this
server can be scheduled on a single processor, as will be shown later on, it is
known by Theorem \ref{thm:dualSched} that a feasible schedule for the original
multiprocessor systems can be derived. Based on these observations it is worth
defining a reduction operation as the composition of a packing operation and the
dual operation.

\begin{definition}
  A reduction operation on a set of servers , denoted
  , is the composition of the dual operation 
  (Definition \ref{dfn:dualSchedule}), with a packing operation 
  (Definition \ref{dfn:packMap}), namely .
\end{definition}

The action of the operator  on a set  of  servers is
illustrated in Figure \ref{fig:dualSet}.

\begin{figure}[t]
  \centering \psset{xunit=0.84cm, yunit=0.54cm}\begin{pspicture*}(-1,-2)(8.1,9.4)\psset{fillstyle=none,linewidth=.5pt}\uput{0}[0](6.5,8.8){\vphantom{} }\psellipse[fillstyle=none](3.7,5.8)(4.35,3.5)\psellipse[fillstyle=none](2.1,7.2)(1.4,1.4)\uput{0}[0](3.,8.5){\vphantom{} }\uput{0}[0](0.9,7.8){\vphantom{} }\uput{0}[0](1.5,7.2){\vphantom{} }\uput{0}[0](2.1,6.6){\vphantom{} }\psellipse[fillstyle=none](5.2,5.1)(1.8,1.7)\uput{0}[0](6.6,6.4){\vphantom{} }\uput{0}[0](4.7,6){\vphantom{} }\uput{0}[0](5.5,5.4){\vphantom{} }\uput{0}[0](3.5,5.4){\vphantom{} }\uput{0}[0](4.3,4.8){\vphantom{} }\uput{0}[0](5.1,4.2){\vphantom{} }\psellipse[fillstyle=none](1.6,4)(1.1,0.9)\uput{0}[0](2.1,5.1){\vphantom{} }\uput{0}[0](0.7,4.3){\vphantom{} }\uput{0}[0](1.3,3.7){\vphantom{} }\psbezier[linewidth=0.5pt,showpoints=false]{->}(1,6.)(0,4.5)(0,2.5)(0.6,0.5)
    \psbezier[linewidth=0.5pt,showpoints=false]{->}(2.5,3.2)(2.8,3)(3.4,1.5)(3.5,0.5)
    \psbezier[linewidth=0.5pt,showpoints=false]{->}(5.5,3.2)(5.7,2.5)(5.7,1.5)(5.5,0.5)
\uput{0}[0](6.5,1.5){\vphantom{} }\psellipse[fillstyle=none](3.6,0)(4.3,1.4)\uput{0}[0](0.,0){\vphantom{} }\uput{0}[0](5,0){\vphantom{} }\uput{0}[0](2.5,0){\vphantom{} }\end{pspicture*}\caption{Partition  of  into three subsets ,
     and  and
    image  of . The utilization  of a server 
    or a set of server  is indicated by the notation  and
    , respectively.}
  \label{fig:dualSet}
\end{figure}

\subsection{Reduction Correctness}
\label{sec:reducCorrectness}

The results shown in the previous sections will be used here to show how to
transform a multiprocessor system into an equivalent (virtual) uniprocessor
system by carrying out a series of reduction operations on the target system.
First, it is shown in Lemma \ref{lem:reducConv} that a reduction operator
returns a reduced task system with smaller cardinality.
Then, Lemma \ref{lem:threeSetInteger} and Theorem \ref{thm:reducConv} show that
after performing a series of reduction operations, a set of servers can be
transformed into a unit server, which, according to Theorem \ref{thm:reduction},
can be used to generate a feasible schedule on a uniprocessor system. Finally,
it is shown in Theorem \ref{thm:complexity} that time complexity for carrying
out the necessary series of reduction operations is dominated by the time
complexity of the packing operation.

\begin{lemma}\label{lem:reducConv}
  If  is a packed set of non-unit servers,
  
\end{lemma}

\begin{IEEEproof}
  Let . By the definition of , which is packed, there
  is at most one server  in  so that . This implies that at least  servers in  have their
  utilizations less than . Since servers in  are non-unit
  servers, their duals are non-zero-utilization servers. Hence, those dual
  servers can be packed up pairwisely, which implies that there will be at most
   servers after carrying out the packing operation.
  Thus, we deduce that .
\end{IEEEproof}

\begin{lemma}\label{lem:threeSetInteger}
  Let  be a packed set of non-unit servers. If  is
  an integer, then .
\end{lemma}


\begin{IEEEproof}
  If ,  would contain a unit server since
   is a non-null integer.  Nonetheless, there exist
  larger non-unit server sets. For example, let  be a set of
  servers such that each server in  has utilization
   and .
\end{IEEEproof}




\begin{definition}[Reduction Level and Virtual Processor]\label{dfn:virtalProc}
  Let  be a natural greater than one. The operator  is
  recursively defined as follows  and
  . The server
  system  is said to be at reduction level  and is to be
  executed on a set of virtual processors.
\end{definition}

Table \ref{tab:reducExample} illustrates a reduction of a system composed of
 fixed-utilization tasks to be executed on  processors. As can be
seen, two reduction levels were generated by the reduction operation. At
reduction level , three virtual processors are necessary to schedule the 
remaining servers, while at reduction level , a single virtual processor
suffices to schedule the  remaining servers.

The next theorem states that the iteration of the operator  transforms a
set of servers of integer utilization into a set of unit servers. For a given
set of servers , the number of iterations necessary to achieve this
convergence to unit servers vary for each initial server in , as shown
in Table \ref{tab:reducExample}.

\setlength{\tabcolsep}{0.5em}
\begin{table}
  \centering
\caption{Reduction Example of a Set of Servers.\label{tab:reducExample}}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
    \hline  \rule{0cm}{.4cm}
    & \multicolumn{10}{c|}{Server Utilization \rule{0cm}{.4cm}} \\
    \hline 
     \rule{0cm}{.35cm} & .6 & .6 & .6 & .6 & .6 & .8 & .6 & .6 & .5 & .5 \\
    \hline 
     \rule{0cm}{.35cm} & .6 & .6 & .6 & .6 & .6 & .8 & .6 & .6 & 
    \multicolumn{2}{c|}{1} \\
    \hline  
     \rule{0cm}{.35cm} & .4 & .4 & .4 & .4 & .4 & .2 & .4 & .4 \\
    \cline{1-9}
     \rule{0cm}{.35cm} & \multicolumn{2}{c|}{.8} & 
    \multicolumn{2}{c|}{.8} & .4 & \multicolumn{3}{c|}{1} \\
    \cline{1-9}
     \rule{0cm}{.35cm} & \multicolumn{2}{c|}{.2} & 
    \multicolumn{2}{c|}{.2} & .6 \\
    \cline{1-6} 
     & \multicolumn{5}{c|}{ \rule{0cm}{.35cm} 1}\\
    \cline{1-6}
  \end{tabular}
\end{table}

\begin{theorem}[Reduction Convergence]\label{thm:reducConv}
  Let  be a set of non-zero utilization servers.  If  is a
  packed set of servers with integer utilization, then for any element ,  is a unit server set for some level .
\end{theorem}

\begin{IEEEproof}
   can be seen as a partition comprised of two
  subsets, those that contain unit sets and those that do not.  Let  and
   be these sets, formally defined as follows: ; ; .  Also, for  define  and  as  and .  We first claim that while ,  and that  is integer.  We show
  the claim by induction on .

  \paragraph{Base case}
  As  is a packed set with integer utilization, it follows that
   is also integer since  is a unit set.  Consider  and
   the partition of .  As  and  have integer utilization,  is also
  integer.  Also, by Lemma \ref{lem:threeSetInteger},  and 
  is a packed set of servers, we deduce from Lemma \ref{lem:reducConv} that
  
  Therefore, , since  for .

  \paragraph{Induction step}
  Assuming the claim holds until , it can be shown that it holds for 
  analogously as it was done for the base case.
  \paragraph{Conclusion}
  By the claim there must exist  such that  since by Lemma
  \ref{lem:threeSetInteger} there is no  such that .  Hence,
   must belong to some  for some , which
  completes the proof.
\end{IEEEproof}
  

\begin{definition}[Proper Server Set]
  Let  be a reduction operation and
   be a set of servers with . A subset of
   is proper for  if there exists a level  such
   for all  and  in
  .
\end{definition}

Table \ref{tab:reducExample} shows three proper sets, each of which projected to
a unit server. Note that the partition of a task system in proper sets depends
on the packing operation. For instance, consider , , ,
. First, consider a packing operation 
which aggregates  and  into two unit servers
 and , then  is the
partition of  into two proper sets for . In this case, unit
servers are obtained with no reduction. Second, consider another packing
operation  which aggregates ,  and  into three non-unit servers ,  and . Then,  is the partition of  into one
proper set for . In this latter case, one reduction is necessary to
obtain a unit server at level one. The correctness of the transformation,
though, does not depend on how the packing operation is implemented.


\begin{theorem}[Reduction]\label{thm:reduction}
  Let  be a reduction and  be a proper
  set of EDF servers with  and  for some integer . If all servers are equipped with
  EDF, then the schedule  of  is feasible on
   processors if and only if the schedule  of  is feasible on a single virtual processor.
\end{theorem}

\begin{IEEEproof}[By transitivity between reduction levels]
  Consider the set of servers  and its
  reduction . By Theorem \ref{thm:reducConv},
  . Thus,  satisfies the
  hypothesis of Theorem \ref{thm:dualSched}. As a consequence, the schedule
   of  on  processors is feasible if and only if the schedule
   of  is feasible on
   processors.  As all servers in 
  are EDF servers, we conclude that the schedule  of
   on 
  processors is feasible if and only if the schedule  of
   is feasible on  processors.
\end{IEEEproof}

It is worth noticing that the time complexity of a reduction procedure is
polynomial. The dual operation computes for each task the utilization of its
dual, a linear time procedure. Also, since no optimality requirement is made for
implementing the packing operation, any polynomial-time heuristic applied to
pack fixed-utilization tasks/servers can be used. For example, the packing
operation can run in linear time or log-linear time, depending on the chosen
heuristic. As the following theorem shows, the time complexity of the whole
reduction procedure is dominated by the time complexity of the packing
operation.

\begin{theorem}[Reduction Complexity]\label{thm:complexity}
  The problem of scheduling  fixed-utilization tasks on  processors can be
  reduced to an equivalent scheduling problem on uniprocessor systems in time
  , where  is the time it takes to pack  tasks in 
  processors.
\end{theorem}

\begin{IEEEproof}
  Theorem \ref{thm:reducConv} shows that a multiprocessor scheduling problem can
  be transformed into various uniprocessor scheduling problems, each of which
  formed by a proper set. Let  be the largest value during a reduction
  procedure so that , where  is a
  proper set. Without loss of generality, assume that . It must
  be shown that . At each step, a reduction operation is carried
  out, which costs  steps for the dual operation plus .  Also, by Lemma
  \ref{lem:reducConv}, each time a reduction operation is applied, the number of
  tasks is divided by two. As a consequence, the time  to execute the
  whole reduction procedure satisfies the recurrence . Since  takes at least  steps, the solution of this recurrence
  is .
\end{IEEEproof}

\subsection{Illustration}\label{sec:illustration}

Figure \ref{fig:dualSchedComplex} shows an illustrative example produced by
simulation with a task set which requires two reduction levels to be
scheduled. Observe, for instance, that when  is
executing in , then both  and  do not execute in
, and both  and  execute in real schedule . On the
other hand, when  does not execute in
, then either --  and  -- or exclusive --  and
 -- executes in  and , respectively.


\begin{figure}[t]
  \centering \setlength{\execWidth}{0.3\psyunit}

  \def\schedlocaxes(#1,#2){\setcounter{step}{#2}\multido{\nl=0+1}{\thestep}{\rput(\nl,-.25){\footnotesize \nl}}\multido{\i=0+1}{#1}{\setcounter{proc}{\i+1}\setlength{\ytmpa}{\i\psyunit+.14\psyunit}\setlength{\ytmpa}{\i\psyunit}\setlength{\ytmpb}{\i\psyunit-.07\psyunit}\setlength{\xtmpa}{#2\psxunit-0.7\psxunit}\psline[linewidth=0.5pt]{->}(0,\ytmpa)(\xtmpa,\ytmpa)\multido{\nt=0+1}{#2}{
        \psline[linestyle=solid,linewidth=0.5pt](\nt,\ytmpa)(\nt,\ytmpb)}}}
  \setlength{\execWidth}{0.35\psxunit}\def\jobleg#1#2(#3,#4,#5){\setcounter{proc}{#5-1}\setlength{\xtmpa}{#3\psxunit}\setlength{\ytmpa}{\theproc\psyunit+0.3\psyunit}\setlength{\xtmpb}{#3\psxunit+#4\psxunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth+0.2\psyunit}\psset{fillcolor=#1, linecolor=black,linestyle=solid,linewidth=0.5pt}\psframe[linecolor=black,fillstyle=solid] (\xtmpa,\ytmpa)(\xtmpb,\ytmpb)\setlength{\ytmpa}{\theproc\psyunit+0.45\psyunit}\setlength{\xtmpa}{\xtmpb+0.1\psxunit}\rput[l](\xtmpa,\ytmpa){#2} }
  \def\joblocdead#1(#2,#3){\setcounter{proc}{#3-1}\setlength{\xtmpa}{#2\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth+0.27\psyunit}\psline[linestyle=solid,linewidth=0.5pt,arrows=-*,arrowsize=4pt]
    (\xtmpa,\ytmpa)(\xtmpa,\ytmpb)\ifthenelse{\equal{#1}{}}{}{\setlength{\ytmpa}{\theproc\psyunit+0.74\psyunit}\uput{.3em}[0](\xtmpa,\ytmpa){\vphantom{}\footnotesize #1}}}
  \def\jobcol#1#2(#3,#4,#5){\setcounter{proc}{#5-1}\setlength{\xtmpa}{#3\psxunit}\setlength{\ytmpa}{\theproc\psyunit}\setlength{\xtmpb}{#3\psxunit+#4\psxunit}\setlength{\ytmpb}{\theproc\psyunit+\execWidth}\psset{fillcolor=#1, linecolor=black,linestyle=solid,linewidth=0.5pt}\psframe[linecolor=black,fillstyle=solid] (\xtmpa,\ytmpa)(\xtmpb,\ytmpb)}
  \newcommand{\colA}{blue!50!white} \newcommand{\colB}{blue!90!white}
  \newcommand{\colAB}{blue!94!black}

  \newcommand{\colC}{red!50!white} \newcommand{\colD}{red!90!white}
  \newcommand{\colCD}{red!94!black}

  \newcommand{\colE}{yellow!97!black!80!white}
  \newcommand{\colEE}{yellow!97!black}

\psset{xunit=1.2cm, yunit=0.8cm}
  \begin{pspicture*}(-0.5,0)(8.5,2.1)\jobleg{\colA}{\footnotesize }(0, 0.2, 2)\jobleg{\colB}{\footnotesize }(1.2, 0.2, 2)\jobleg{\colC}{\footnotesize }(2.4, 0.2, 2)\jobleg{\colD}{\footnotesize }(3.6, 0.2, 2)\jobleg{\colE}{\footnotesize }(4.8, 0.2, 2)
    \jobleg{\colAB}{\footnotesize }(0,
    0.2, 1)\jobleg{\colCD}{\footnotesize }(2.4, 0.2, 1)\jobleg{\colEE}{\footnotesize }(4.8, 0.2,
    1)\end{pspicture*}
  \psset{xunit=1.2cm, yunit=1cm}
  \begin{pspicture*}(-0.8,-0.4)(8.5,6.)\schedlocaxes(6,7)
\rput[l](-0.74, 4.15){}\psbezier[linewidth=0.5pt]{-}(-0.3,4.2)(-0.1,4.4)(-0.3,5.3)(-0.1,5.5)
    \psbezier[linewidth=0.5pt]{-}(-0.3,4.2)(-0.1,4.)(-0.3,3.1)(-0.1,2.9)
    \psbezier[linewidth=0.5pt]{-}(-0.3,1.7)(-0.1,1.9)(-0.3,2.3)(-0.1,2.5)
    \psbezier[linewidth=0.5pt]{-}(-0.3,1.7)(-0.1,1.5)(-0.3,1.1)(-0.1,0.9)
    \psbezier[linewidth=0.5pt]{-}(-0.3,0.2)(-0.1,0.3)(-0.3,0.45)(-0.1,0.5)
    \psbezier[linewidth=0.5pt]{-}(-0.3,0.2)(-0.1,0.1)(-0.3,-0.05)(-0.1,-0.1)
\jobcol{\colE}(0, 1.2, 6)\jobcol{\colD}(1.2, 1.8, 6)\jobcol{\colC}(3, 0.2, 6)\jobcol{\colE}(3.2, 0.2, 6)\jobcol{\colA}(3.4, 1, 6)\jobcol{\colD}(4.4, 0.4, 6)\jobcol{\colE}(4.8, 1.2, 6)\jobcol{\colC}(0, 1.8, 5)\jobcol{\colB}(1.8, 0.2, 5)\jobcol{\colE}(2, 1, 5)\jobcol{\colD}(3, 0.6, 5)\jobcol{\colC}(3.6, 0.4, 5)\jobcol{\colC}(4, 1.2, 5)\jobcol{\colD}(5.2, 0.8, 5)\jobcol{\colA}(0, 2.6, 4)\jobcol{\colB}(2.6, 3.4, 4)\joblocdead{1}(2,6)\joblocdead{2}(3,6)\joblocdead{1}(4,6)\joblocdead{2}(6,5)\joblocdead{3}(4,5)\joblocdead{4}(6,4)\rput[l](-0.74, 1.65){}
\jobcol{\colB}(0, 1.8, 3)\jobcol{\colC}(1.8, 1.2, 3)\jobcol{\colE}(3, 0.2, 3)\jobcol{\colC}(3.2, 0.4, 3)\jobcol{\colD}(3.6, 0.8, 3)\jobcol{\colA}(4.4, 1.6, 3)\jobcol{\colD}(0, 1.2, 2)\jobcol{\colE}(1.2, 0.8, 2)\jobcol{\colB}(2, 0.6, 2)\jobcol{\colA}(2.6, 0.8, 2)\jobcol{\colE}(3.4, 0.6, 2)\jobcol{\colE}(4, 0.8, 2)\jobcol{\colD}(4.8, 0.4, 2)\jobcol{\colC}(5.2, 0.8, 2)\rput[l](-0.74, 0.15){}
\jobcol{\colEE}(0, 1.2, 1)\jobcol{\colCD}(1.2, 0.6, 1)\jobcol{\colAB}(1.8, 0.2, 1)\jobcol{\colEE}(2, 1, 1)\jobcol{\colCD}(3, 0.2, 1)\jobcol{\colEE}(3.2, 0.2, 1)\jobcol{\colAB}(3.4, 1, 1)\jobcol{\colCD}(4.4, 0.4, 1)\jobcol{\colEE}(4.8, 1.2, 1)
    \joblocdead{}(2,1)\joblocdead{}(3,1)\joblocdead{}(4,1)\joblocdead{}(6,1)\psset{linewidth=0.5pt,linestyle=dashed,dash=2pt 3pt}
    \psline(1.2,-0.1)(1.2,5.6) \psline(1.8,-0.1)(1.8,5.6) \psline(2,-0.1)(2,5.6)
    \psline(2.6,-0.1)(2.6,5.6) \psline(3,-0.1)(3,5.6) \psline(3.2,-0.1)(3.2,5.6)
    \psline(3.4,-0.1)(3.4,5.6) \psline(3.6,-0.1)(3.6,5.6)
    \psline(4.,-0.1)(4.,5.6) \psline(4.4,-0.1)(4.4,5.6)
    \psline(4.8,-0.1)(4.8,5.6) \psline(5.2,-0.1)(5.2,5.6)
  \end{pspicture*}
  \caption{ with , , ,  and . ,
     and  are the schedule on three processors, two virtual
    processors and one virtual processor of ,  and
    , respectively.}
  \label{fig:dualSchedComplex}
\end{figure}

\section{Assessment}\label{sec:evaluation}

We have carried out intensive simulation to evaluate the proposed approach. We
generated one thousand random task sets with  tasks each, . Hence a total of  thousands task sets were generated. Each
task set fully utilizes a system with  processors.  Although other
utilization values were considered, they are not shown here since they presented
similar result patterns. The utilization of each task was generated following
the procedure described in \cite{Emberson10a}, using the aleatory task generator
by \cite{Emberson10b}. Task periods were generated according to a uniform
distribution in the interval .

Two parameters were observed during the simulation, the number of reduction
levels and the number of preemption points occurring on the real multiprocessor
system. Job completion is not considered as a preemption point. The results were
obtained implementing the packing operation using the decreasing worst-fit
packing heuristic.

Figure \ref{fig:level} shows the number of reduction levels. It is interesting
to note that none of the task sets generated required more than two reduction
levels. For  tasks, only one level was necessary. This situation,
illustrated in Figure \ref{fig:dualSchedEx}, is a special case of Theorem
\ref{thm:dualSched}. One or two levels were used for  in . For
systems with more than  tasks, the average task utilization is low. This
means that the utilization of each server after performing the first packing
operation is probably close to one, decreasing the number of necessary
reductions.

The box-plot shown in Figure \ref{fig:ratio} depicts the distribution of
preemption points as a function of the number of tasks. The number of
preemptions is expected to increase with the number of levels and with the
number of tasks packed into each server. This behavior is observed in the
figure, which shows that the number of levels has a greater impact. Indeed, the
median regarding scenarios for  in  is below  and for those
scenarios each server is likely to contain a higher number of tasks. Further,
observe that the maximum value observed was  preemption points per job on
average, which illustrates a good characteristic of the proposed approach.

\begin{figure}[t]
\includegraphics{level}
  \caption{Fraction of task sets which requires 1 (crosshatch box) and 2 (empty
    box) reduction levels. 1000 task sets were generated for each point.}
  \label{fig:level}
\end{figure}

\begin{figure}[t]
\includegraphics{ratio}
  \caption{Distributions of the average number of preemptions per job, their
    quartiles, and their minimum and maximum values.}
  \label{fig:ratio}
\end{figure}

\section{Related Work}\label{sec:relatedWork}

Solutions to the real-time multiprocessor scheduling problem can be
characterized according to the way task migration is controlled. Approaches
which do not impose any restriction on task migration are usually called global
scheduling. Those that do not allow task migration are known as partition
scheduling. Although partition-based approaches make it possible using the
results for uniprocessor scheduling straightforwardly, they are not applicable
for task sets which cannot be correctly partitioned. On the other hand, global
scheduling can provide effective use of a multiprocessor architecture although
with possibly higher implementation overhead.

There exist a few optimal global scheduling approaches for the PPID model. If
all tasks share the same deadline, it has been shown that the system can be
optimally scheduled with a very low implementation cost
\cite{McNaughton59}. Removing this restriction on task deadlines, optimality can
be achieved by approaches that approximate the theoretical fluid model,
according to which all tasks execute at the steady rate proportional to their
utilization \cite{Baruah96}. However, this fluid approach has the main drawback
that it potentially generates an arbitrary large number of preemptions.

Executing all tasks at a steady rate is also the goal of other approaches
\cite{Cho06,Funaoka08}. Instead of breaking all task in fixed-size quantum
subtasks, such approaches define scheduling windows, called T-L planes, which
are intervals between consecutive task deadlines.
The T-L plane approach has been extended recently to accommodate more general
task models \cite{Funk10}. Although the number of generated preemptions has
shown to be bounded within each T-L plane, the number of T-L planes can be
arbitrarily high for some task sets.


Other approaches which control task migration have been proposed
\cite{Andersson08, Easwaran09, Kato09, Massa10}. They have been called
semi-partition approaches. The basic idea is to partition some tasks into
disjunct subsets. Each subset is allocated to processors off-line, similar to
the partition-based approaches. Some tasks are allowed to be allocated to more
than one processor and their migration is controlled at run-time. Usually, these
approaches present a trade-off between implementation overhead and achievable
utilization, and optimality can be obtained if preemption overhead is not
bounded.

The approach presented in this paper lie in between partition and global
approaches. It does not assign tasks to processors but to servers and optimality
is achieved with low preemption cost. Task migration is allowed but is
controlled by the rules of both the servers and the virtual schedule. Also, as
the scheduling problem is reduced from multiprocessor to uniprocessor, well
known results for uniprocessor systems can be used. Indeed, optimality for
fixed-utilization task set on multiprocessor is obtained by using an optimal
uniprocessor scheduler, maintaining a low preemption cost per task.


It has recently been noted that if a set with  tasks have their total
utilization exactly equal to , then a feasible schedule of these tasks on 
identical processors can be produced \cite{Levin09}. The approach described here
generalizes this result. The use of servers was a key tool to achieve this
generalization. The concept of task servers has been extensively used to provide
a mechanism to schedule soft tasks \cite{liu00}, for which timing attributes
like period or execution time are not known a priori. There are some server
mechanisms for uniprocessor systems which share some similarities with one
presented here \cite{DLS97,SB96}. To the best of our knowledge the server
mechanism presented here is the first one designed with the purposes of solving
the real-time multiprocessor scheduling problem.

\section{Conclusion}\label{sec:conclusion}

An approach to scheduling a set of tasks on a set of identical multiprocessors
has been described. The novelty of the approach lies in transforming the
multiprocessor scheduling problem into an equivalent uniprocessor
one. Simulation results have shown that only a few preemption points per job on
average are generated.

The results presented here have both practical and theoretical
implications. Implementing the described approach on actual multiprocessor
architectures is among the practical issues to be explored. Theoretical aspects
are related to relaxing the assumed task model, \textit{e.g.} sporadic tasks
with constrained deadlines. Further, interesting questions about introducing new
aspects in the multiprocessor schedule via the virtual uniprocessor schedule can
be raised. For example, one may be interested in considering aspects such as
fault tolerance, energy consumption or adaptability. These issues are certainly
a fertile research field to be explored.


\bibliographystyle{IEEEtran}
\bibliography{bib}

\end{document}
