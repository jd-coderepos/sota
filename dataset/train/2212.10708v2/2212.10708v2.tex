\section{Experiments}

\subsection{Dataset}

\begin{table}[t]
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
          & \# of examples & \# of relations & \# of entities   \\ \midrule
         FewRel & 56,000 & 80 & 72,954\\
         Wiki-ZSL & 94,383 & 113 & 77,623\\ \bottomrule
         & & & \\
    \end{tabular}}
    \begin{tabular}{ccccc}
    \toprule
          \multicolumn{2}{c}{}  &  &  \\
          FewRel  & Wiki-ZSL & \multicolumn{2}{c}{} & \\ \midrule
         70  & 103 & 5 & 5  \\
         65  & 98 & 10 & 5  \\
         60  & 93 & 15 & 5  \\ \bottomrule
    \end{tabular}
    \caption{Statistics of the datasets.  denotes the number of relations in each set, and  is the number of relations in the test set.}
    \label{tab:stat_data}
\end{table}

We evaluate our method on two datasets: FewRel \citep{han-etal-2018-fewrel} and Wiki-ZSL \citep{chen-li-2021-zs}. FewRel is designed primarily for few-shot relation extraction. Wiki-ZSL is a subset of Wiki-KB and targets zero-shot relation extraction. Both datasets are created using distant supervision, but FewRel has  additionally been filtered by humans.
We use dataset versions released by \citet{chia-etal-2022-relationprompt} which have been transformed for zero-shot triplet extraction.
We follow their zero-shot set up for training and evaluation as follows:
1) we keep relation types in training, validation, and test splits disjoint, 2) we evaluate different methods under different settings for the size of unseen relation types (), 3). To avoid experimental noise, we repeat experiments using different data folds wherein relation types are split with different random seeds: \{0, 1, 2, 3, 4\}. 
Table \ref{tab:stat_data} shows statistics of each dataset and setting.

 \begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/perf_graph.png}
    \caption{Results of each data fold in the single-triplet setting with =10. We experiment on three different random seeds. Each point is the accuracy (\%) of each evaluation and labeled numbers are an average of three results.  Final column shows average over all folds.}
    \label{fig:perf_graph}
\end{figure*}

\begin{table*}[ht]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{c|l|ccc|ccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Single-triplet (Accuracy (\%))} & \multicolumn{3}{c}{Multi-triplet (F1 score (\%))}  \\
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Model} & =5 & =10 & \multicolumn{1}{c}{=15} & \multicolumn{1}{c}{=5} & =10 & =15 \\ \midrule
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{FewRel}}} 
& TableSequence & 11.82 & 12.54 & 11.65 & 3.40 & 6.37 & 3.48 \\
& ZS-BERT+spanNER & 7.22 (±0.67) & 6.68 (±1.48) & 7.68 (±0.23) & 15.10	(±2.52)	&	12.14	(±2.33)	&	14.78	(±1.81) \\
& Seq2Seq & 22.02 (±1.07) & 16.71 (±0.82) & 11.91 (±0.70) & 26.02 (±2.81) & 17.12 (±1.86) & 13.02 (±0.96) \\
& RelationPrompt & 24.36 (±0.90) & 21.45 (±1.50) & 20.24 (±0.72) & 30.85 (±3.23) & 24.45 (±1.76) & 23.65 (±1.36) \\
& \textsc{ZETT} & \textbf{30.71} (±0.45) & \textbf{27.90} (±0.31) & \textbf{26.17} (±0.20) & \textbf{33.71} (±0.42) & \textbf{31.28} (±0.54) & \textbf{24.39} (±0.37) \\ \midrule
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Wiki-ZSL}}}
& TableSequence & 14.47 & 9.61 & 9.20 & 6.29 & 6.4 & 6.39 \\
& ZS-BERT+spanNER & 8.16 (±1.26) & 8.05 (±0.79) & 6.47 (±0.42) & 15.96 (±1.11) & 11.98 (±2.28) & 11.90 (±0.84) \\
& Seq2Seq & 14.73 (±1.30) & 9.94 (±0.46) & 7.05 (±0.44) & 30.71 (±4.31) & 19.70 (±1.90) & 11.52 (±3.32) \\
& RelationPrompt & 16.74 (±1.53) & 12.13 (±0.86) & 10.47 (±0.96) & \textbf{33.28} (±1.70) & 24.04 (±2.12) & 18.73 (±1.98)  \\
& \textsc{ZETT} & \textbf{21.49} (±0.44) & \textbf{17.27} (±0.31) & \textbf{12.78} (±0.42) & 31.17 (±0.69) & \textbf{24.87} (±0.32) & \textbf{21.21} (±0.35) \\ \bottomrule
\end{tabular}}
\caption{Accuracy in the single-triplet and F1 score in the multi-triplet settings.  denotes the number of unseen relations. All reported results are averaged over three different random seeds, where the result of each seed are averaged over five different data folds. Results of  are taken from \citet{chia-etal-2022-relationprompt}.}
\label{tab:main_results}
\end{table*}

\subsection{Experimental Settings}
\paragraph{Training}
We use the pre-trained T5-base\footnote{\url{https://huggingface.co/t5-base}}.
We fine-tune the model for 3 epochs with 64  batch size, and 3e-5 learning rate. We tune the parameters using the validation set. We provide details of the parameters in Appendix.

\paragraph{Inference}

At inference, ZETT generates entity spans given the input sentence concatenated with the relation template. We restrict the vocabulary to use tokens from the input sentence to ensure entities are spans from the input sentence.
We use a beam size of 4 to generate a maximum of 4 candidate entity pairs for each relation type. 
For the relation constraint, we set a threshold  and choose =0.85 based on the validation performance. 

\paragraph{Single- and Multi-triplet evaluation}
Each example in the datasets includes one or more triplets. We evaluate the models separately on single- and multi-triplet settings following the previous study~\cite{chia-etal-2022-relationprompt}. For the single-triplet setting, the examples have only one correct (gold) triplet, thus we use accuracy as the metric for evaluating performance.
In the multi-triplet setting, the number of gold triplets is two or more, thus we evaluate performance with a F1 score.
To retrieve positives in the multi-triplet setting, we set a threshold and output a candidate as a predicted positive example if its score is above this threshold. The thresholds for the multi-triplet evaluation are provided in Appendix.

\subsection{Baseline methods}
We compare the performance of ZETT with four existing methods for triplet extraction:
\textbf{1) ZS-BERT+spanNER} is a pipeline model, where two sub-tasks: relation classification and entity extraction are combined to extract triplets. We use ZS-BERT \citep{chen-li-2021-zs} for the relation classification model and implement span-based named entity recognition model using transformer encoder initialized with \texttt{roberta-base} checkpoint for entity extraction~\citep{fu-etal-2021-spanner}.
\textbf{2) TableSequence} \citep{wang-lu-2020-two} is a joint learning model with two separate encoders performing relation extraction and named entity recognition at the same time.
Since TableSequence is designed for supervised learning, we report the results of models trained on synthetic data from \citet{chia-etal-2022-relationprompt}.
\textbf{3) Seq2seq} \citep{chia-etal-2022-relationprompt} is an encoder-decoder  model based on the pre-trained BART-base \citep{lewis-etal-2020-bart}. The input to the encoder is a context, then the decoder generates a triplet as a sequence of structured template.
\textbf{4) RelationPrompt} \citep{chia-etal-2022-relationprompt} is an additionally fine-tuned model of the Seq2Seq on the synthetic data for the unseen relations.
\footnote{Since the synthetic data and pre-trained checkpoints for all experiments are not provided in the official implementation, we couldn't reproduce the results of RelationPrompt as same as reported in the paper. Thus, we re-trained the models with three different random seeds and report the average of them in Table \ref{tab:main_results}.}
They generate synthetic training datasets for unseen relations using GPT-2~\cite{radford2019language}\footnote{\url{https://huggingface.co/gpt2}} and fine-tune the Seq2seq-based triplet extraction model.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{l|cc|cc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Single-triplet (Accuracy ())} & \multicolumn{2}{c}{Multi-triplet (F1 score ())}  \\
\multicolumn{1}{c}{Model} & FewRel & \multicolumn{1}{c}{Wiki-ZSL} & \multicolumn{1}{c}{FewRel} & Wiki-ZSL \\ \midrule
RelationPrompt & 21.45 & 14.37 & 24.45 & 24.04 \\
\text{ZETT} w/ \textit{Manual} 
& 27.90 & 17.27 & 31.28 & 24.87 \\
\text{ZETT} w/ \textit{Paraphrased} (Random) & 24.74 (-3.16) & 14.49 (-2.78) & 28.86 (-2.42) & 23.39 (-1.48) \\
\text{ZETT} w/ \textit{Paraphrased} (Top 1) & 25.12 (-2.78) & 15.34 (-1.93) & 29.60 (-1.68) & 24.63 (-0.24) \\\bottomrule
\end{tabular}
\caption{Comparison of performance with paraphrased templates at =10.  is the performance difference over \text{ZETT} with manual templates.}
\label{tab:paraph_template}
\end{table*}

\section{Results}
\subsection{Automatic Evaluation}
\label{sec:main_results}
Table \ref{tab:main_results} shows the results of single- and multi-triplet evaluation settings on FewRel and Wiki-ZSL. As can be seen, ZETT consistently outperforms existing methods across different datasets under single-triplet setting. It achieves up to 6.45 and 5.14 higher accuracy than the existing state-of-the-art model, RelationPrompt on FewRel and Wiki-ZSL datasets, respectively. It also shows much lower variance in performance than RelationPrompt (see Figure \ref{fig:perf_graph}). In  the worst case, performance of  RelationPrompt differs by 7.1 points in accuracy (fold 0, =10, FewRel). We conjecture that the variance can be attributed to the varying quality of the synthesized dataset in every trial. On the the other hand, ZETT shows stable performance through all trials, consistently outperforming the existing methods. In the multi-triplet setting, ZETT achieves the best F1 score for different relation set sizes except on Wiki-ZSL with =5. We argue that this is mainly due to the biased distribution in the multi-triplet test sets; most examples are only for few relations. Therefore, performance loss in a particular relation results in significant drop in overall performance. We analyze the main causes of prediction failure in Section \ref{sec:error_analysis}. Overall, results of automatic evaluation show that having a simpler training process can yield more effective and stable performance on the task. 

\subsection{Human Evaluation}
\label{sec:human_eval}
{Both the test sets from our experiments are created using distant supervision and hence can be noisy and incomplete. In other words, they can include triplets that are not supported by an input text. Furthermore, it may not include all possible triplets from an input text. We, therefore, conduct human evaluation to better understand the performance of ZETT. We focus on WikiZSL since it is noisier and sample 200 contexts for manual annotation. We ask three CS graduates to annotate top-5 predictions for each context. An annotator labels a prediction correct if it is supported by the input text.
Three annotators labeled the triplets such that each triplet receives two annotations. 
We identified triplets labeled as \textit{True} by both annotators, showing a high agreement with a Cohen's Kappa coefficient of 0.75. Among the 1,000 triplets, we found 127 mislabeled instances--24 were \textit{False} (i.e., unsupported by input text) but originally labeled as \textit{True}, and 103 were \textit{True} but not covered in the original dataset.
We found that accuracy of ZETT increased from 18\% to 30.2\% on this manually annotated dataset. We will release the manually annotated dataset for future research.}

\section{Discussion} \label{sec:analysis}

\subsection{Robustness to choice of templates}
Since ZETT uses templates to generalize to unseen relations, the performance can be sensitive to the
wording of templates~\cite{sanh2022multitask}. In this section, we investigate how the performance varies depending on the template.
{To that end, we paraphrase the manually defined templates and compare the performance of ZETT with manual and paraphrased templates.}

We adopt the back-translation method from \citet{jiang-etal-2020-know}, which uses an English-German machine translation model to generate semantically-similar templates. {In our experiments, we used the translation model to generate 7 templates in German language for each manual template, and then back-translate each of them to 7 templates in English. As a result, we obtain 49 paraphrased templates per relation. Since many of them are duplicates, we evaluate on the most-frequent template, \textit{Paraphrased} (Top 1). We also compare with a randomly selected paraphrased template, \textit{Paraphrased} (Random). We will release the full set of paraphrased templates.}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l}
\toprule
\multicolumn{3}{l}{\textsc{Context}: \textcolor{blue}{Jimmy Jam} is the son of \textcolor{purple}{Cornbread Harris}, a Minneapolis blues and jazz musician.} \\
\multicolumn{3}{l}{\textsc{Gold Triplet}: (\textcolor{blue}{Jimmy Jam}, \textit{father}, \textcolor{purple}{Cornbread Harris})} \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{RP}} & \textsc{Model output} & Head Entity : Cornbread Harris , Tail Entity : Minneapolis blues, Relation : field of work . \\ \cline{2-3}
\multicolumn{1}{l|}{}                    & \textsc{Triplet} & (\textcolor{purple}{Cornbread Harris}, field of work, Minneapolis blues) \\ \midrule
\multirow{3}{*}{ZETT}  & \textsc{Template} & \texttt{<tail>} is a father of \texttt{<head>} \\ \cline{2-3}
                       & \textsc{Model output} & \texttt{<X>} Cornbread Harris \texttt{<Y>} Jimmy Jam \texttt{<Y>} \\ \cline{2-3}
                       & \textsc{Triplet} & (\textcolor{blue}{Jimmy Jam}, \textit{father}, \textcolor{purple}{Cornbread Harris}) \\ \midrule       
\multicolumn{3}{l}{} \\ \midrule
\multicolumn{3}{l}{\textsc{Context}: He participated in \textcolor{blue}{UEFA Euro 1972} for the \textcolor{purple}{Hungary national football team}.} \\
\multicolumn{3}{l}{\textsc{Gold Triplet}: (\textcolor{blue}{UEFA Euro 1972}, \textit{participating team}, \textcolor{purple}{Hungary national football team})} \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{RP}} & \textsc{Model output} & Head Entity : Hungary national football team , Tail Entity : UEFA Euro 1972, Relation : participating team . \\ \cline{2-3}
\multicolumn{1}{l|}{}                    & \textsc{Triplet} & (\textcolor{purple}{Hungary national football team}, \textit{participating team}, \textcolor{blue}{UEFA Euro 1972}) \\ \midrule
\multirow{3}{*}{ZETT}  & \textsc{Template} & \texttt{<tail>} is a participating team in \texttt{<head>} \\ \cline{2-3}
                       & \textsc{Model output} & \texttt{<X>} Hungary national football team \texttt{<Y>} UEFA Euro 1972 \texttt{<Y>} \\ \cline{2-3}
                       & \textsc{Triplet} & (\textcolor{blue}{UEFA Euro 1972}, \textit{participating team}, \textcolor{purple}{Hungary national football team}) \\ \bottomrule  
\end{tabular}}
\caption{Example \textsc{Model output} sequences and triplets from RelationPrompt (RP) and ZETT.}
\label{tab:analysis_eg}
\end{table*}

Table \ref{tab:paraph_template} compares the performance of ZETT with manual and paraphrased templates. {Since automatically paraphrased templates can be noisy in capturing the semantic meaning of a relation, we observe small performance drops when using ZETT with paraphrased templates. The performance drop is much smaller with \textit{Paraphrased} (Top 1). Even though the paraphrased templates are noisy, ZETT still consistently outperforms current state-of-the-art methods.}

\subsection{Ablation Study}
\label{sec:ablation_study}
Next, we conduct an ablation study to examine the importance of each generation setting and the relation constraint. The results are summarized in Table \ref{tab:ablation}. First, {we test without the vocabulary constraints that limits the vocabulary set to tokens that appear in the context. We observe a small drop in accuracy of up to 0.39 points. Second, we compare the performance of beam search with greedy decoding that only generates one sequence. We find that beam search improves accuracy by up to 1.62 points since it selects the best complete sequence of entity pair as opposed to selecting the best individual entity token in each position. Last, we observe that our proposed relation constraint, although simple, is effective in eliminating irrelevant relations and can improve accuracy by up to 3.56 points.}

\subsection{Qualitative Analysis}

{To further gain insights into the strengths and weaknesses of the models, we manually inspect examples where ZETT and RelationPrompt generate different triplets. Table \ref{tab:analysis_eg} shows some such examples. We observe that RelationPrompt often generates an incorrect triplet even for easy examples. For example, it fails to predict the correct relation \textit{father} even when a context (``Jimmy Jam is the son of Cornbread Harris'') clearly describes it. This can be attributed to noise in the synthesized training data for unseen relations. We find the training data for relation \textit{father} includes noisy examples such as ``The Last Days of the Lion-Cat is based on David Simon.''  (The Last Days of the Lion-Cat, \textit{father}, David Simon), where the context has no information about \textit{father}. Such noisy examples can propagate errors in the multi-step training process of RelationPrompt. ZETT, being a single-step process, is more robust to such errors.}

{We also find that RelationPrompt often fails to predict the correct order of entities. This is because the output sequence for generating a triplet "Head Entity: \texttt{<head>}, Tail Entity : \texttt{<tail>}, Relation : \texttt{<relation>}" does not encode any information about the order of entities. In contrast, ZETT can leverage the implicit information about entity types and their order encoded in relation templates. For instance, for the relation \textit{participating team}, the template ``\texttt{<tail>} is a participating team in \texttt{<head>}'' provides implicit information that \texttt{<head>} entity is a sports team and \texttt{<tail>} entity should be a contest or sports game. This information helps ZETT correctly predict the order of entities.}

\subsection{Error Analysis}
\label{sec:error_analysis}

To understand the limitations of ZETT, we perform a detailed analysis of errors on 200 examples. Incorrect ranking of relations contributed to the most frequent errors, accounting for 36\% of errors. Since ZETT relies on LM's probability , it tends to assign high scores to relations that are frequently used in common sentences. We find that relations such as \textit{occupation}, \textit{owned by} or \textit{work location} had higher scores than more rare relations such as \textit{contains administrative territorial entity} or \textit{place served by transport hub}.
Lack of discriminatory power over semantically similar relations contributed to 21\% of the errors. For example, relations such as \textit{headquarters location}, \textit{location of formation}, and \textit{work location}\, all represent the concept of location and are hard for the model to discriminate without any fine-tuning.
Lastly, we find that relation constraint sometimes excluded the correct relation, contributing to 17\% of the errors. For instance, when the context ``Ay was the penultimate Pharaoh of Ancient Egypt's 18th dynasty.'' and the gold triplet (Ay, \textit{country of citizenship}, Ancient Egypt) are given, even though the context contains the information of Ay's citizenship, our model predict as (Ay,	\textit{occupation}, Pharaoh) by determining \textit{country of citizenship} is irrelevant.
The rest of error types include failure of predicting entity spans, flipped entity pair, generation of null string, and labeling errors of datasets. We provide more detailed examples of error types in Appendix. Future work can focus on developing more effective score functions and relation classification techniques.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lcc}
    \toprule
         & FewRel & Wiki-ZSL \\\midrule
         ZETT & 27.90 & 17.27 \\
\quad w/o decoding vocab constraint & 27.76 & 16.88 \\
\quad w/o beam search & 26.87 & 15.65 \\
\quad w/o relation constraint & 24.34 & 16.61 \\ \bottomrule
    \end{tabular}
    \caption{Ablation study on =10 and single-triplet setting. The metric is accuracy.}
    \label{tab:ablation}
\end{table}

