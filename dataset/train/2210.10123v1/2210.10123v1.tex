\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}






\usepackage{tikz}
\newcommand{\mike}[1]{\textcolor{red}{#1}}
\newcommand{\david}[1]{\textcolor{orange}{#1}}
\newcommand{\bryan}[1]{\textcolor{blue}{#1}}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\!\operatorname{max}}\;}



\def\wacvPaperID{1342} 

\wacvalgorithmstrack   

\wacvfinalcopy 







\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi

\pagestyle{empty}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Interpolated SelectionConv for Spherical Images and Surfaces}

\author{David Hart, Michael Whitney, Bryan Morse\\
Brigham Young University\\ {\{davidmhart, mikeswhitney, morse\}@byu.edu}
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    We present a new and general framework for convolutional neural network operations on spherical (or omnidirectional) images. Our approach represents the surface as a graph of connected points that doesn't rely on a particular sampling strategy. Additionally, by using an interpolated version of SelectionConv, we can operate on the sphere while using existing 2D CNNs and their weights. Since our method leverages existing graph implementations, it is also fast and can be fine-tuned efficiently. Our method is also general enough to be applied to any surface type, even those that are topologically non-simple. We demonstrate the effectiveness of our technique on the tasks of style transfer and segmentation for spheres as well as stylization for 3D meshes. We provide a thorough ablation study of the performance of various spherical sampling strategies.
\end{abstract}

\section{Introduction}


Omnidirectional (or spherical) images are ever present in modern computer vision, from 360 videos for VR to HDRI images as lighting references in 3D graphics. Much research has gone into trying to bring the benefits of deep learning to this useful domain. A naive approach is to simply apply 2D Convolutional Neural Networks (CNNs) to a projection of the sphere, such as on the equirectangular image or a cubemap unfolding. However, all planar projections of a sphere lead to distortion of the 3D content. Thus, traditional 2D CNNs perform poorly in such cases.

To resolve this, two approaches have been well explored previously. The first  is to design the convolution operator uniquely for the sphere, looking at important properties such as rotation equivariance of the convolution operation~\cite{Cohen2019,Cohen2018,Chiyu2019}. 
The second is to try to transfer what has been learned from a 2D CNN to a specialized network that can operate on spherical images, either through distorting the convolution operator itself \cite{Coors2018,Tateno2018} or through special projections of the spherical image into the plane \cite{Eder2020,Zhang2019}. Both of these approaches have potential shortcomings. In the first, by redefining a convolution specific to the spherical domain, all training data must also be in the spherical domain, for which available datasets tend to be much smaller than their 2D counterparts. In the second approach, techniques are often restricted to particular samplings of the sphere (most often the icosphere \cite{Eder2020,Zhang2019}), which can only exist at specific resolutions, and transitions to these representations may require fine-tuning to properly adjust the CNN weights \cite{Zhang2019}. 


\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.30\linewidth]{Figures/segmentation_example.png} & 
\includegraphics[width=0.35\linewidth,trim={0 5cm 0 0},clip]{Supp_Figures/crow_ours.png}
\end{tabular}
\end{center}
   \caption{Interpolated SelectionConv can perform tasks such as segmentation on spheres (left) and 3D mesh stylization (right).
   }
\label{fig:intro}
\end{figure}

This work aims to begin to bridge the gap between these two approaches to operating on omnidirectional images. We propose a framework that builds on
our previous work of SelectionConv~\cite{SelectionConv}, a
graph-based approach for transferring weights from a 2D CNN. By using an edge-interpolated version of this method, we can create a graph of points on the sphere using any sampling scheme that we desire and still maintain the anisotropic behavior of convolution that is needed for transferring from 2D CNNs. Our 
new 
approach is general while remaining fast and efficient since it builds on previous graph network infrastructure.

We show that our method is a simple and effective way to bring 2D techniques into the spherical domain, while avoiding many of the drawbacks of regular SelectionConv and previous approaches. We also show that customizable sampling techniques may be more effective than the icosphere approach that has generally been used. Lastly, while we primarily focus on spherical images for this work, our method is general enough to operate on any surface or 3D mesh. 
Examples of tasks that can be performed with our method are shown in Fig.~\ref{fig:intro}.

In summary, our contributions are as follows:

\begin{itemize}
    \item We introduce an interpolation-based scheme for using SelectionConv in non grid-like spaces.
    \item We adapt Interpolated SelectionConv to work for surface point-cloud representations by modifying the selection function and clustering method.
    \item We demonstrate the effectiveness of our graph representation with customizable sampling techniques on the tasks of spherical stylization and segmentation. We also demonstrate stylization of 3D meshes. We achieve results comparable to state-of-the-art methods.
    \item We present a thorough ablation study of various spherical sampling and clustering techniques. \end{itemize}

\section{Related Works}

\subsection{SelectionConv} \label{sec:SelectionRelated}

This work builds heavily on the idea of selection-based convolution, which was first proposed in SelectionConv~\cite{SelectionConv}, 
a method for using CNNs on irregular image types such as cubemaps and masked images. 
In selection-based convolution, graph networks can be manipulated to be commensurate with 2D CNNs. This is done by preprocessing a graph's adjacency matrix into multiple adjacency matrices, where each matrix represents a cardinal or ordinal direction. By doing this, spatial structure is built back into the graph and weights can be transferred from a trained 2D CNN to a modified graph network.
Thus, graph structures can be designed for specific irregular image types and the same 2D networks can operate in these new domains.

In \cite{SelectionConv}, all experiments were done with graph representations that assume a single selection per target node. 
This allowed graph connections to go across seams for irregular images, but a single selection is not sufficient when building the graph in more general spaces such as arbitrary surfaces or point clouds.
We extend this approach by demonstrating how to effectively allow multiple selections per node to give interpolated values when selections lie between discrete spatial locations.
This is similar to techniques such as \cite{InterpConv}, but with the added benefit that we can orient the interpolation for each individual node on a surface. 


\subsection{Spherical CNNs} \label{sec:RelatedWork_Spherical}

Neural networks designed for spheres fall roughly into two categories. The first is projection-based CNNs that aim to learn on traditional 2D images and then utilize that information in some way on the sphere. The second is surface-based CNNs that have special convolution operations designed specifically for spheres.\\

\noindent \textbf{Projection-based CNNs}


The first approaches to applying CNNs to the sphere operated on projections of the sphere such as the equirectangular projection \cite{Lai2018}, cubemap projection \cite{Cheng2018,Monroy2018}, or even a combination of multiple projections \cite{Jiang2021,Su2017,Wang2020}. After the proposal of deformable convolution by Dai \etal \cite{Dai2017}, many researchers focused on deforming the convolution operator specifically for sphere projections \cite{Coors2018,Su2019,Tateno2018}. 

Lee \etal~\cite{Lee2019} later demonstrated the effectiveness of operating on an icosahedral approximation of the sphere. Zhang \etal~\cite{Zhang2019} adapted a similar representation, but allow for a more direct CNN operation by breaking the icosahedron into 5 planar projections. Eder \etal~\cite{Eder2020} increased performance on many spherical tasks by representing the sphere as a set of tangent images at the icosphere vertices. \\

\noindent \textbf{Surface-based CNNs}

Cohen \etal \cite{Cohen2019,Cohen2018} have proposed a convolution structure designed specifically for spheres and surfaces that is rotation and gauge equivariant. Representations have also been designed that utilize spherical harmonics~\cite{Esteves2018} or define the convolution in terms of a linear combination of differential operators \cite{Chiyu2019}. This line of work continues to expand to be used for more complex and general spherical networks, such as LSTMs~\cite{Xu2021}.

Our method aligns closely with projection-based approaches since it transfers weights from a 2D CNN and operates with oriented convolutions on the sphere. Our approach, however, builds on a framework that is not limited to spheres, and, like surface-based CNNs, is not constrained to a specific sampling technique.

\subsection{Sphere and Mesh Style Transfer}

Many spherical approaches demonstrate performance on tasks such object detection, semantic segmentation, and depth prediction. While we compare our method on the task of segmentation, we also demonstrate that our representation generates high-quality stylizations of the sphere. Ruder \etal~\cite{Ruder2018} were the first to demonstrate style transfer on a spherical image, but their method requires a slow optimization approach or fine-tuning a style network for spheres. Like \cite{SelectionConv}, our approach can stylize the sphere in a single feed-forward pass with any content and style image, but we do so with fewer artifacts than previous approaches.

Since our representation can easily be applied to general surfaces, we demonstrate style transfer for meshes as well. There have been many approaches to applying convolutions to surfaces and 3D meshes~\cite{Haim2019,Hanocka2019,Lahav2020,Sinha2016,Wiersma2020}, but these convolutions are trained on 3D data for 3D-specific tasks. Some methods have performed mesh style transfer~\cite{StyLit,SelectionConv,StyBlit,yin2021}, but these approaches are optimization-based, require professional reference renderings, use aligned texture maps, or have artifacts at seams. Our approach is simple, fast, and has no model or style prerequisites.

\section{Interpolated SelectionConv} \label{sec:Interpolation}

As discussed in Sec.~\ref{sec:SelectionRelated}, our method builds on the original SelectionConv \cite{SelectionConv}, which proposes a modified graph network that can distinguish between the edges of a graph during a convolution operation. 
Thus, if a graph is designed and processed for a specific type of irregular image, selection-based graph convolution will preserve the orientation and allow a traditional CNN to operate even in the irregular space. 

In order to distinguish the edges during a graph convolution, the graph must be preprocessed into multiple adjacency matrices. Selection-based convolution extends the common formulation of graph convolution to reflect these different spatially-relative adjacency matrices. Specifically,

\noindent where  represents a given direction/selection, 
 is the adjacency matrix for that selection,  is the current node activations, and  is the learned weights for that selection. Rather than a binary adjacency matrix, note that  is normalized so that multiple incoming edges can have the same selection, allowing for more general point-cloud-like structures.  This work further utilizes this normalization to break the restriction that each edge have a single selection.

In
our
original SelectionConv work, edges were given the selection of the cardinal or ordinal direction most closely aligned with the spatial relationship between the respective pixels or nodes. With general sampling patterns, however, node relationships do not lie so cleanly along specific axes. Thus, it is beneficial to be able to interpolate between multiple selections. To do this, we first assign multiple edges to a single node with each edge having a different selection (i.e., we assign the same source and target node to multiple adjacency matrices). Then, we assign each of those edges an edge weight that determines how much the spatial relationship between the nodes matches the given direction.

With this, we have two different senses of normalization. The first accounts for our introduced interpolation by normalizing all the assigned edges for a given source and target node, making the total of the interpolation weights across selections equal to one, or mathematically:

\noindent This guarantees that a single target node can not have an unbalanced amount of influence during the aggregation step. The second normalizes in the manner discussed in \cite{SelectionConv}, where each of the adjacency matrices is normalized individually so that for a given source node, the sum of all the target node edge weights equals one, or mathematically:


With this formulation, we are no longer constrained to discrete selections but can interpolate selection values within the convolution operation. There are various ways to interpolate a location's value given a set of points. We demonstrate two simple and effective ways to interpolate selection weights given source and target nodes.


\subsection{Angle-based Interpolation}\label{sec:Angle}

One way to practically determine the interpolation of a node between two selections is to use angles. Instead of simply choosing the selection in the direction that most closely matches a given edge, we use the angles between the given edge and the unit vectors for each selection direction (excluding the central selection). The two selections,  and , with the smallest angles are added to the respective adjacency matrices and the edge weights are equal to the convex combination of the two angles, or mathematically:


\noindent This process is illustrated in Fig.~\ref{fig:interpolation}.a.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.35\linewidth]{Figures/Interpolation-Angle.pdf}
& \includegraphics[width=0.35\linewidth]{Figures/Interpolation-BaryCenteric.pdf}
\\ a) Angular Interpolation &
b) Barycentric Interpolation
\end{tabular}
\end{center}
   \caption{Illustration of a) angular interpolation versus b) barycentric interpolation. In angular interpolation, the edge to each node is added to the adjacency matrix of the two selections it lies between. If a local distance/radius can be determined, barycentric interpolation can be used which also includes the central selection. 
   }
\label{fig:interpolation}
\end{figure}





\subsection{Barycentric Interpolation}

The angle-based interpolation is intuitive and would be expected to perform well for graph structures where nodes are spaced approximately equidistant from each other. However, another interpolation scheme can be used that accounts properly for distance from the source node. If an average or expected distance between nodes can be calculated, then the interpolation can be triangulated between three selections using barycentric interpolation. This process is illustrated in Fig.~\ref{fig:interpolation}.b.

Though the general form of barycentric interpolation requires inverse matrices or other time-consuming calculations, it can be dramatically simplified since the three points of interpolation will always lie along a perfect right triangle with two bases of length . In these circumstances, the three edge weights become



\noindent where  is the location of the target node relative to the source node. Further details and a proof of this formulation are shown in Appendix A of the supplemental material.

\section{The Spherical Graph}\label{sec:Spheres}

As noted in Sec.~\ref{sec:RelatedWork_Spherical}, many networks that operate on spherical images must first project the spherical data to some form. 
Ideally, rather than working on projections, it would be best to work on the spherical data directly without relying on projection. Thus, we generate our graph structure in 3D space using the spherical surface and adjust our selection function appropriately. We will now give the details of how we generate the exact graph structure.

\subsection{The Spherical Selection Function} \label{sec:SphericalFunction}

The original selection function used in \cite{SelectionConv} is

\noindent where  and  are two nodes of the graph,  and  are their respective positions, and   represents the set of unit vectors for each of the 8 cardinal/ordinal directions. In order to work on spheres, we modify this selection function by making selections relative to the surface of the sphere, using a local planar approximation to determine orientation.

To make a local planar approximation, we first need the normal  to each node position (which is conveniently the normalized version of the 3D location  for a sphere). Next, we generate a rotation matrix using the approximate up-vector of  and Graham-Schmidt orthogonalization. Specifically,



\noindent With this rotation matrix, edges from the node can be rotated relative to the surface. Then, the new z-coordinate is dropped, giving a local planar approximation of the coordinate relative to the surface. This dropping of z-coordinate can be represented by multiplying by a truncated identity matrix:


\noindent Thus, the final spherical selection function (for a single selection) becomes


\noindent Once the main selection is made, other selections and interpolation values can be determined according to Sec.~\ref{sec:Interpolation}. An illustration of our final graph structure is shown in Fig.~\ref{fig:sphericalgraph}.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.40\linewidth]{Figures/SphereGraph.png}
& \includegraphics[width=0.43\linewidth]{Figures/PlanarGraph.png}
\\
a) Spherical Graph &
b) Local Planar Projection
\end{tabular}
\end{center}
   \caption{Visualization of the graph connections being made in 3D space (a), and selections being made based on the local planar projection at each node (b).
   }
\label{fig:sphericalgraph}
\end{figure}

\subsection{Sampling}
Most spherical data is stored in the form of an equirectangular image, where the spherical data has been projected to a cylinder and then unfolded to the plane. This representation is easy to store, but the pixels do not represent locations that are equidistantly sampled across the sphere. Preferably, the algorithm should sample points from the image in such a way that their 3D locations are more consistently spaced across the surface.

For this work, we consider the following sampling strategies when comparing to the equirectangular baseline:
\begin{itemize}
\item \textbf{Fibonacci Spiral}: A common and fast approach for approximating equidistant points.
    \item \textbf{Icosphere}: Points generated by taking the icosahedron and subdividing iteratively.
    \item \textbf{Layering}: A sampling method described in \cite{sphereSampling}, where points are appropriately distanced in  based on equidistant  values.
\end{itemize}

\noindent Examples of each of these sampling schemes are shown in Fig.~\ref{fig:sampling}. We include the layering sampling algorithm here (Algorithm 1) for convenience, which we adapt from \cite{sphereSampling}.

\begin{algorithm}
\caption{Layering Sampling}
\begin{algorithmic}[1]
\REQUIRE , (i.e. number of sampling rows)
\STATE 
\STATE 
\STATE 
\STATE 
\STATE 

\FOR{each  in }
    \STATE 
    \STATE 
    
    \FOR{each  in }
        \STATE 
        \STATE Append point ()
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}




\begin{figure*}
\begin{center}
\begin{tabular}{ccccc}

\includegraphics[width=.14\linewidth,trim={75 60 70 60},clip]{Figures/equirec2D.png} &
\includegraphics[width=.14\linewidth,trim={75 60 70 60},clip]{Figures/random2D.png} &
\includegraphics[width=.14\linewidth,trim={75 60 70 60},clip]{Figures/fibonacci2D.png} &
\includegraphics[width=.14\linewidth,trim={75 60 70 60},clip]{Figures/icosphere2D.png} &
\includegraphics[width=.14\linewidth,trim={75 60 70 60},clip]{Figures/layering2D.png}
\\
\includegraphics[width=.16\linewidth,trim={120 70 100 100},clip]{Figures/equirec3D.png} &
\includegraphics[width=.16\linewidth,trim={120 70 100 100},clip]{Figures/random3D.png} &
\includegraphics[width=.16\linewidth,trim={120 70 100 100},clip]{Figures/fibonacci3D.png} &
\includegraphics[width=.16\linewidth,trim={120 70 100 100},clip]{Figures/icosphere3D.png} &
\includegraphics[width=.16\linewidth,trim={120 70 100 100},clip]{Figures/layering3D.png}
\\
Equirectangular &
Random &
Fibonacci Spiral &
Icosphere &
Layering

\end{tabular}
\end{center}
   \caption{ 
        The various sampling methods we test for building our graph on the spherical surface. The sampling points on the equirectangular image (top) correspond to particular points on the spherical surface (bottom).
    }
\label{fig:sampling}
\end{figure*}




\subsection{Clustering}

Nodes in the graph need to be clustered together for the downsampling stages of the CNNs. When operating on irregular images as in \cite{SelectionConv}, these could be easily determined by grouping pixels together on a 2D grid. However, determining clusters for a sphere based on the 2D equirectangular image removes the benefits of working in a 3D space. Thus, to cluster in 3D, we must resample the sphere at a lower resolution. Each current node is clustered to the resampled node of closest proximity. 

All of the previous sampling techniques (as well as random sampling) can be used, with an aim for a desired cluster size. For example, a stride of 2 in each dimension is commonly used in downsampling layers of CNNs. This would give a desired cluster size of 4, and each of the sampling methods would be used in the following ways:

\begin{itemize}
    \item \textbf{Random}: Randomly generate 1/4 as many points.
    \item \textbf{Fibonacci Spiral}: Resample with 1/4 as many points.
\item \textbf{Icosphere}: Complete one less subdivision of the icosahedron (since 1/4 as many points will be generated).
    \item \textbf{Layering}: Increase the distance between  values by 2 and resample.
\end{itemize}



\subsection{Customizable Resolution}

When 2D CNNs are trained, there is generally a fixed resolution that the input data is set too. Additionally, there is usually a small range of field-of-views in the cameras that were used to generate the training data. In a 2D sense, this intrinsically sets an expected size and scale that the network is expecting to see certain objects. While data augmentation and other techniques can make the network more flexible to some degree, it is important that our graph structures lie at relatively the same resolution and field-of-view that the network is expecting.

For example, the Stanford2D-3D-S dataset \cite{Armeni2017} that we evaluate on in Sec.~\ref{sec:Results} uses training images that have a field-of-view (FOV) between 45 and 75.
If a 2D CNN was trained on N  N images of that camera, the relative angle between points on our 3D surface in any direction should be approximately:


\noindent This spacing is handled by the sampling method we use for the sphere. As mentioned previously, most methods have relied on icosphere sampling for their approaches, but the icosphere is constructed through subdivisions and thus is limited to discrete resolutions that are quadruplings of the initial resolution. With a sampling approach such as layering, however, we can directly control the sampling density on the sphere and thus match the desired  more precisely. This gives it improved performance over the icosphere, which we demonstrate in Sec.~\ref{sec:Ablation}.




\section{Surface Graphs}

The approach described in Sec.~\ref{sec:Spheres} was specific to spheres, but with a few simple modifications, it can be applied to any general surface, specifically 3D meshes. To generate the graph on a surface, we start by randomly sampling the surface. For each node, the normal of the face it was sampled from and its respective UV coordinate are also stored. Next, edges connecting nearby points are made using a KNN approach (while also accounting for folds by culling points with disparate normals).

Then, to determine selections, the same selection process and selection function as Sec.~\ref{sec:SphericalFunction} can be used with two key differences. First, the approximate up-vector for the surface is either manually assigned (if a reasonable volume orientation exists) or is randomly generated. Second, the normal  used for the Graham-Schmidt orthogonalization is the stored face normal for each node. All other steps in the process remain the same for the graph generation. The surface is then resampled at a lower resolution to determine clustering nodes, and the whole process is repeated for the needed number of downsamplings.

As is shown in Sec.~\ref{sec:MeshResults}, one benefit of this approach is that, just like the sphere, we can customize the approximate resolution of the point cloud on the mesh by controlling the number of points in the initial sampling. Increasing the number of initial points increases the level of detail to which we operate on the mesh.



\section{Results} \label{sec:Results}
For the results presented, we use the layering method for both sampling and clustering discussed in Sec.~\ref{sec:Spheres} and the angular interpolation  presented in Sec.~\ref{sec:Angle}. 
We also use replicate padding in the networks to handle missing selections. 
We use this representation for its ability to customize the resolution on the sphere and since we found it to perform the best overall on all tasks. 
A more thorough analysis of the different sampling and clustering techniques, as well as a comparison between the interpolation techniques, is presented in Sec.~\ref{sec:Ablation}.





\subsection{Spherical Stylization}

We start with a clear visual example of the effectiveness of transferring weights from a 2D CNN to our graph structures without the need of any additional fine-tunning. We demonstrate style transfer with the approach proposed by Li \etal \cite{Li2019} on our spherical representations. 
SelectionConv~\cite{SelectionConv} achieved spherical style transfer by building a fully connected graph structure based on a cube map. While this approach improved the stylization over the naive method
of stylizing the equirectangular image directly, 
the uninterpolated nature of the graph can give artifacts
along pixels where a direction transition occurs. 
Our 
new
approach removes such artifacts, as shown in Fig.~\ref{fig:spherestyle}. This is possible because 
we 
can smoothly vary the selection between nodes in 3D space. 
Additionally, when reconstructing the equirectangular image, the relevant points are interpolated in 3D space, which gives a smooth and plausible stylization throughout the whole sphere.
Our new method is also similar to \cite{SelectionConv} in run time and memory usage.





\begin{figure*}
\begin{center}
\begin{tabular}{rcccc}
\raisebox{0.075\linewidth}{\rotatebox[origin=c]{90}{Spherical Image}} &
\includegraphics[width=.32\linewidth]{Figures/test360_ex3_detailed.jpg} &
\includegraphics[width=.16\linewidth]{Figures/reference_back.png} &
\includegraphics[width=.16\linewidth]{Figures/reference_top.png} &
\includegraphics[width=.16\linewidth]{Figures/reference_bottom.png}
\\
\raisebox{0.075\linewidth}{\rotatebox[origin=c]{90}{Naive}} &
\includegraphics[width=.32\linewidth]{Figures/ex3_naive.jpg} &
\includegraphics[width=.16\linewidth]{Figures/naive_back.png} &
\includegraphics[width=.16\linewidth]{Figures/naive_top.png} &
\includegraphics[width=.16\linewidth]{Figures/naive_bottom.png}
\\ 
\raisebox{0.075\linewidth}{\rotatebox[origin=c]{90}{SelectionConv~\cite{SelectionConv}}} &
\includegraphics[width=.32\linewidth]{Figures/ex3_cubemap.jpg} &
\includegraphics[width=.16\linewidth]{Figures/cubemap_back.png} &
\includegraphics[width=.16\linewidth]{Figures/cubemap_top.png} &
\includegraphics[width=.16\linewidth]{Figures/cubemap_bottom.png}
\\ 
\raisebox{0.075\linewidth}{\rotatebox[origin=c]{90}{Ours}} &
\includegraphics[width=.32\linewidth]{Figures/ex3_sphere.jpg} &
\includegraphics[width=.16\linewidth]{Figures/ours_back.png} &
\includegraphics[width=.16\linewidth]{Figures/ours_top.png} &
\includegraphics[width=.16\linewidth]{Figures/ours_bottom.png} 
\\
&
Equirectangular &
Back &
Top &
Bottom
\end{tabular}
\end{center}
    \caption{A 
    360 image
(1st row), its stylization when naively stylizing the equirectangular image (2nd row), using the cube-map graph setup in \cite{SelectionConv} (3rd row), and compared to our interpolated spherical representation (4th row). The equirectangular projection along with various views of the scene are presented. In the naive approach, note the vertical seam in the middle of the back view as well as the distortion in the top and bottom views. In the original SelectionConv results, note the artifacts in the top and bottom views along the seam connections (making an x shape). Those artifacts are removed with our new method. Public domain image courtesy of polyhaven.com.
    }
\label{fig:spherestyle}
\end{figure*}

\begin{figure}
\vspace{0.2in}
\begin{center}
\begin{tabular}{cccc}
\hspace{-0.12in}
\includegraphics[width=.21\linewidth]{Figures/teddy_side_detailed.png} &
\includegraphics[width=.21\linewidth]{Figures/teddy_side_naive.png} &
\includegraphics[width=.21\linewidth]{Figures/teddy_side_sc.png} &
\includegraphics[width=.21\linewidth]{Figures/teddy_side_ours.png} 
\\
a) Mesh &
b) Naive &
c) \cite{SelectionConv} &
d) Ours
\end{tabular}
\end{center}
   \caption{ 
When the texture map of the original mesh (a) is stylized naively (b), many artifacts are present along the UV seams. Stylizing with SelectionConv (c) removed some of those artifacts, but inconsistencies remain. 
Interpolated SelectionConv 
   (d) retains far greater consistency along seams.  
   }
\label{fig:teddystyle}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=.30\linewidth]{Figures/lime_detailed.png} &
\includegraphics[width=.24\linewidth]{Figures/lime_naive.png} &
\includegraphics[width=.24\linewidth]{Figures/lime_ours.png} 
\\
\includegraphics[width=.32\linewidth]{Figures/crate_detailed.png} &
\includegraphics[width=.26\linewidth]{Figures/crate_naive_detailed.png} &
\includegraphics[width=.26\linewidth]{Figures/crate_ours_detailed.png} 
\\
a) Mesh &
b) Naive &
c) Ours
\end{tabular}
\end{center}
   \caption{ Example stylizations of high quality meshes with 4K textures (a). Naively stylizing the texture map (b) is slow, leaves artifacts on seams, and provides little control for the level of detail for the stylization. Our approach (c) removes seam artifacts while allowing the user to control the number of sampling points, which in turn determines the speed and detail of the stylization. Public domain meshes courtesy of polyhaven.com. }
\label{fig:meshstylization}
\end{figure}


\subsection{Spherical Segmentation}

A common spherical image task is omnidirectional semantic segmentation. We evaluate our method on such a task using the Stanford 2D-3D-S dataset \cite{Armeni2017}. It consists of equirectangular images of various indoor scenes with 13 different classes. 

We first train a simple 6-layer deep U-Net architecture~\cite{UNet}, similar to the architecture used in \cite{Chiyu2019,Zhang2019}, 
and include depth information with the input to be consistent with state-of-the-art methods. 
We train on the provided precomputed views of the spherical image. Once the network has been trained in 2D space, we transfer over the weights to our graph representation with interpolated selections. We use the layering approach and scale the resolution to match the average  from the training data.

We can also fine-tune the graph representation on the full 3D data. These fine-tuning steps can be completed quickly since our graph structure only needs to be calculated once, then it can be reused in every successive iteration. Only the node feature values need to be updated.

The mean IOU for our method, both before and after fine-tuning in spherical space, is shown in Table~\ref{tab:segmentation}, along with comparisons to other previous works. We evaluate on the full scale equirectangular images () and follow the standard practice of weighting the metrics for each pixel by cosine of their latitude location, similar to \cite{Chiyu2019,Zhang2019}. 

As is shown, our method has performance comparable to state-of-the-art approaches, even after a direct transfer without fine-tuning. While \cite{Zhang2019} and \cite{Eder2020} have higher mean IOU scores,
we note that \cite{Zhang2019} must use a fine-tuned icosphere representation 
and that \cite{Eder2020} requires processing 80 different  planar projections to make a single spherical image, whereas our approach can evaluate in a single pass with a reusable graph structure between spherical images.




\begin{table}[]
    \centering
    \begin{tabular}{|cc|cc|}
        \hline
        Method & Input & mIOU & fine-tuned \\
        \hline
Cubemap \cite{SelectionConv} & RGB & 36.3\% & - \\
        UGSCNN \cite{Chiyu2019} & RGB-D & - & 38.3\%   \\
        GaugeNet \cite{Cohen2019} & RGB-D & - & 39.4\%   \\
        Sphere (Ours) & RGB-D & 39.9\%  & \textbf{41.4\%}  \\
        HexRUNet \cite{Zhang2019} & RGB-D & - & 43.3\%  \\
        TangentIms \cite{Eder2020} & RGB-D & 38.9\% & 51.9\% \\
        \hline
    \end{tabular}
    \caption{Mean intersection-over-union (mIOU) on the Stanford 2D-3DS dataset segmentation task \cite{Armeni2017}. The mIOU is shown for both direct transfer (left) and after fine-tuning (right). Our method performs comparably to state-of-the-art approaches.}
    \label{tab:segmentation}
\end{table}









\subsection{Mesh Stylization} \label{sec:MeshResults}

Lastly, we demonstrate the ability of our method to generalize to general surfaces by stylizing 3D meshes.
In \cite{SelectionConv}, a UV edge-pairing process was required to determine edges across seams in the texture map. This process failed to remove all artifacts along seam boundaries and was time consuming for high-polygon-count meshes. 
Our 
new 
approach avoids this computationally expensive step while achieving far better consistency along UV seams
since the edges and selections are made in 3D space, rather than on the topologically complex 2D texture map. A comparison of 
this
approach to the previous work is shown in Fig.~\ref{fig:teddystyle}. Additionally, our method allows users to control the number of initial sampling points, making it time invariant to the complexity of the mesh. This also provides a simple way for controlling the level of detail in the stylization on the mesh
while operating on high quality meshes and texture maps.
Examples of these benefits are shown in Fig.~\ref{fig:meshstylization} 
and in the supplemental material. 



\section{Ablation Study}\label{sec:Ablation}

\begin{table*}[]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        Sampling\textbackslash Clustering & Random & Equirectangular & Spiral & Icosphere & Layering \\
        \hline
        Equirectangular & 35.1\% / 35.0\%  & 38.1\% / 37.0\% & 38.9\% / 38.8\% & 38.7\% / 38.5\% & 40.7\% / 39.8\%  \\
        Spiral & 32.8\% / 32.5\%  & 32.8\% / 32.3\% & 35.7\% / 35.9\% & 36.3\% / 36.2\% & 37.9\% / 37.2\% \\
        Icosphere &  31.9\% / 30.8\% & 33.3\% / 31.9\% & 35.9\% / 35.4\%  & 36.5\% / 35.6\% & 38.1\% / 36.7\% \\
        Layering &  34.3\% / 33.6\% & 36.2\% / 34.3\% & 38.1\% / 37.5\% & 38.8\% / 38.2\%  & \textbf{39.9\%} / 38.7\%  \\
        \hline
    \end{tabular}
    \caption{Ablation study comparing mIOU results for the various sampling, clustering, and interpolation approaches on the Stanford2D-3D-S segmentation task. Angle-based interpolation results are shown on the left. Barycentric results are shown on the right. Results for random clustering were averaged over multiple configurations.}
    \label{tab:ablation}
\end{table*}



We presented the layering sampling and clustering technique (with angular interpolation) as our best representation of the sphere. However, one of the benefits of our method is that we are not constrained to any specific graph structure. Thus, we give the performance of the other possible representations of our sphere. 

We test the various sampling methods and clustering methods with the previously presented segmentation task without fine-tuning. Additionally, we compare barycentric interpolation versus angle-based interpolation for these various representations. The results are shown in Table~\ref{tab:ablation}.

The results show some interesting patterns. First, all methods perform worst when using random and equirectangular clustering. This is to be expected since the points will not be pooled together based on equidistant locations. Second, we note that spiral and icosphere sampling perform poorest overall since the spiral approach generates neighborhoods that tend to be more irregular and neither allows for customizing the resolution in a precise way. (We also note that icosphere performs more poorly than demonstrated elsewhere, but we attribute this to the fact that we do not explicitly modify the convolution operator to be hexagonal like many icosphere-specific methods~\cite{Lee2019,Zhang2019}.) Also,  barycentric interpolation performs slightly below  angle-based interpolation for most sampling and clustering representations. We attribute this to the blurring effect that comes from the larger number of edges, especially when compared to the angle-based representation when most points are reasonably spaced. Lastly, we note that layering clustering did the best for all methods. Equirectangular sampling with layering clustering even outperformed the layering/layering representation on the segmentation task, but we note that it used 25\% more nodes and edges, which makes it slower to run and harder to fine-tune.






\section{Conclusion}

We have presented a general framework for transferring weights from 2D convolutional networks to graph networks that can operate on spherical images and surfaces. These networks can be fine-tuned even further in their specific domains as needed. This approach allows for a simple and effective way to improve performance on spherical tasks without requiring large datasets that are specific to the spherical domain. We have also demonstrated possible applications for 3D meshes and have provided a thorough ablation study exploring sampling of spherical surfaces. \\


\noindent \textbf{Limitations}

The main limitation of our approach comes from the extra memory needed to store the graph edges while performing the convolution. Though we describe our method mathematically using adjacency matrices, the implementation actually uses memory-efficient edge indexes. However, this means that allowing multiple selections for the same edge increases the size of the edge index. Additionally, since the interpolation values also need to be stored, the memory needed for a graph with many nodes can become quite large. Since our focus was on direct transfer, this is usually not an issue and most graphs can still fit in GPU memory, but fine-tuning any task must be done with a very small batch size to be feasible.

Like the original SelectionConv, we can't use larger convolution kernel sizes because the edge hopping process is not differentiable. Thus the networks we used contained only  and  kernels. We note, however, that this limitation is shared with other previous methods~\cite{Chiyu2019, Zhang2019}. \\

\noindent \textbf{Future Work}

Though we demonstrated style transfer on meshes, the field of meshes and general surfaces is extensive and has many applications. More exploration and experiments using our interpolated SelectionConv in this area could be the study of future work. Looking into additional interpolation schemes and techniques for determining expected radial distances is also worth investigation.


\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Armeni2017}
I. {Armeni}, A. {Sax}, A.~R. {Zamir}, and S. {Savarese}.
\newblock {Joint 2D-3D-Semantic Data for Indoor Scene Understanding}.
\newblock {\em ArXiv e-prints}, Feb. 2017.

\bibitem{Cheng2018}
Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh Liu, and
  Min Sun.
\newblock Cube padding for weakly-supervised saliency prediction in 360°
  videos.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}. IEEE,
  June 2018.

\bibitem{Cohen2019}
Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling.
\newblock Gauge equivariant convolutional networks and the icosahedral {CNN}.
\newblock In {\em International Conference on Machine Learning}, pages
  1321--1330. PMLR, 2019.

\bibitem{Cohen2018}
Taco~S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling.
\newblock Spherical {CNN}s.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Coors2018}
Benjamin Coors, Alexandru~Paul Condurache, and Andreas Geiger.
\newblock {SphereNet}: Learning spherical representations for detection and
  classification in omnidirectional images.
\newblock In {\em European Conference on Computer Vision}, September 2018.

\bibitem{Dai2017}
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen
  Wei.
\newblock Deformable convolutional networks.
\newblock In {\em International Conference on Computer Vision}, pages 764--773.
  IEEE, 2017.

\bibitem{sphereSampling}
Markus Deserno.
\newblock How to generate equidistributed points on the surface of a sphere,
  2004.

\bibitem{Eder2020}
Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael Frahm.
\newblock Tangent images for mitigating spherical distortion.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  12426--12434. IEEE, 2020.

\bibitem{Esteves2018}
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas
  Daniilidis.
\newblock Learning {SO(3)} equivariant representations with spherical {CNN}s.
\newblock In {\em European Conference on Computer Vision}, pages 52--68, 2018.

\bibitem{StyLit}
Jakub Fi\v{s}er, Ond\v{r}ej Jamri\v{s}ka, Michal Luk\'{a}\v{c}, Eli Shechtman,
  Paul Asente, Jingwan Lu, and Daniel S\'{y}kora.
\newblock {StyLit}: Illumination-guided example-based stylization of {3D}
  renderings.
\newblock {\em ACM Trans. Graph.}, 35(4):92:1--92:11, 2016.

\bibitem{Haim2019}
Niv Haim, Nimrod Segol, Heli Ben-Hamu, Haggai Maron, and Yaron Lipman.
\newblock Surface networks via general covers.
\newblock In {\em International Conference on Computer Vision}. IEEE, October
  2019.

\bibitem{Hanocka2019}
Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel
  Cohen-Or.
\newblock {MeshCNN}: a network with an edge.
\newblock {\em ACM Transactions on Graphics (TOG)}, 38(4):1--12, 2019.

\bibitem{SelectionConv}
David Hart, Michael Whitney, and Bryan Morse.
\newblock {SelectionConv:} convolutional neural networks for non-rectilinear
  image data.
\newblock In {\em European Conference on Computer Vision}, October 2022.

\bibitem{Chiyu2019}
Chiyu~``Max" Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus,
  and Matthias Nie{\ss}ner.
\newblock Spherical {CNNs} on unstructured grids.
\newblock In {\em International Conference on Learning Representations
  (Poster)}, 2019.

\bibitem{Jiang2021}
Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang.
\newblock Unifuse: Unidirectional fusion for 360° panorama depth estimation.
\newblock {\em Robotics and Automation Letters}, 6(2):1519--1526, 2021.

\bibitem{Lahav2020}
Alon Lahav and Ayellet Tal.
\newblock Meshwalker: Deep mesh understanding by random walks.
\newblock {\em ACM Transactions on Graphics (TOG)}, 39(6):1--13, 2020.

\bibitem{Lai2018}
Wei-Sheng Lai, Yujia Huang, Neel Joshi, Christopher Buehler, Ming-Hsuan Yang,
  and Sing~Bing Kang.
\newblock Semantic-driven generation of hyperlapse from 360 degree video.
\newblock {\em Transactions on Visualization and Computer Graphics},
  24(9):2610--2621, 2018.

\bibitem{Lee2019}
Yeonkun Lee, Jaeseok Jeong, Jongseob Yun, Wonjune Cho, and Kuk-Jin Yoon.
\newblock Sphere{PHD}: Applying {CNN}s on a spherical {PolyHeDron}
  representation of 360° images.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  9173--9181. IEEE, 2019.

\bibitem{Li2019}
Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.
\newblock Learning linear transformations for fast arbitrary style transfer.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}. IEEE,
  2019.

\bibitem{InterpConv}
Jiageng Mao, Xiaogang Wang, and Hongsheng Li.
\newblock Interpolated convolutional networks for 3d point cloud understanding.
\newblock In {\em International Conference on Computer Vision}. IEEE, October
  2019.

\bibitem{Monroy2018}
Rafael Monroy, Sebastian Lutz, Tejo Chalasani, and Aljoscha Smolic.
\newblock {SalNet360}: Saliency maps for omni-directional images with {CNN}.
\newblock {\em Signal Process. Image Commun.}, 69:26--34, 2018.

\bibitem{UNet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock {U-Net}: Convolutional networks for biomedical image segmentation.
\newblock In Nassir Navab, Joachim Hornegger, William~M. Wells, and
  Alejandro~F. Frangi, editors, {\em Medical Image Computing and
  Computer-Assisted Intervention -- MICCAI 2015}, pages 234--241, Cham, 2015.
  Springer International Publishing.

\bibitem{Ruder2018}
Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
\newblock Artistic style transfer for videos and spherical images.
\newblock {\em International Journal of Computer Vision}, 126(11):1199--1219,
  2018.

\bibitem{Sinha2016}
Ayan Sinha, Jing Bai, and Karthik Ramani.
\newblock Deep learning {3D} shape surfaces using geometry images.
\newblock In {\em European Conference on Computer Vision}, pages 223--240.
  Springer, 2016.

\bibitem{Su2017}
Yu-Chuan Su and Kristen Grauman.
\newblock Learning spherical convolution for fast features from 360° imagery.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, page 529–539, Red Hook, NY, USA, 2017.
  Curran Associates Inc.

\bibitem{Su2019}
Yu-Chuan Su and Kristen Grauman.
\newblock Kernel transformer networks for compact spherical convolution.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  9442--9451. IEEE, 2019.

\bibitem{StyBlit}
Daniel S\'{y}kora, Ond\v{r}ej Jamri\v{s}ka, Ond\v{r}ej Texler, Jakub Fi\v{s}er,
  Michal Luk\'{a}\v{c}, Jingwan Lu, and Eli Shechtman.
\newblock {StyleBlit}: Fast example-based stylization with local guidance.
\newblock {\em Computer Graphics Forum}, 38(2):83--91, 2019.

\bibitem{Tateno2018}
Keisuke Tateno, Nassir Navab, and Federico Tombari.
\newblock Distortion-aware convolutional filters for dense prediction in
  panoramic images.
\newblock In {\em European Conference on Computer Vision}, September 2018.

\bibitem{Wang2020}
Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai.
\newblock Bifuse: Monocular 360 depth estimation via bi-projection fusion.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}. IEEE,
  June 2020.

\bibitem{Wiersma2020}
Ruben Wiersma, Elmar Eisemann, and Klaus Hildebrandt.
\newblock {CNN}s on surfaces using rotation-equivariant features.
\newblock {\em ACM Transactions on Graphics (ToG)}, 39(4):92--1, 2020.

\bibitem{Xu2021}
Yanyu Xu, Ziheng Zhang, and Shenghua Gao.
\newblock Spherical {DNNs} and their applications in 360° images and videos.
\newblock {\em Transactions on Pattern Analysis and Machine Intelligence}, PP,
  2021.

\bibitem{yin2021}
Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and Sanja Fidler.
\newblock {3DStyleNet}: Creating {3D} shapes with geometric and texture style
  variations.
\newblock In {\em International Conference on Computer Vision}. IEEE, 2021.

\bibitem{Zhang2019}
Chao Zhang, Stephan Liwicki, William Smith, and Roberto Cipolla.
\newblock Orientation-aware semantic segmentation on icosahedron spheres.
\newblock In {\em International Conference on Computer Vision}. IEEE, October
  2019.

\end{thebibliography}
 


\end{document}
