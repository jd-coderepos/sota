\documentclass{bmvc2k}



\usepackage{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\Crefname{figure}{Fig.}{Figs.}


\newcommand{\sinha}{\textcolor[rgb]{0.7,0.7,0.1}}
\newcommand{\onkar}{\textcolor[rgb]{0.3,0.7,1.0}}
\newcommand{\oha}{\textcolor[rgb]{0.1,0.7,0.1}}
\newcommand{\del}{\textcolor[rgb]{1.0,0,0}}

\usepackage{xparse}\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{wrapfig} \usepackage{xr}
\usepackage{cuted}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xr}
\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{graphicx}

\usepackage{subfiles}   


\newcommand{\xmark}{\ding{55}}

\setlength{\textfloatsep}{12pt}


\title{MILA: Memory-Based Instance-Level Adaptation for Cross-Domain Object Detection}

\addauthor{Onkar Krishna}{onkar.krishna.vb@hitachi.com}{1}
\addauthor{Hiroki Ohashi}{hiroki.ohashi.uo@hitachi.com}{1}
\addauthor{Saptarshi Sinha}{saptarshi.sinha@bristol.ac.uk}{2}

\addinstitution{
 Hitachi Ltd.\\
 Tokyo, Japan
}
\addinstitution{
 University of Bristol\\
 Bristol, UK
}

\runninghead{Onkar, Hiroki, Saptarshi}{MILA}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}



\begin{document}
\maketitle
\begin{abstract}













Cross-domain object detection is challenging, and it involves aligning labeled source and unlabeled target domains. 
Previous approaches have used adversarial training to align features at both image-level and instance-level.
At the instance level, finding a suitable source sample that aligns with a target sample is crucial. 
A source sample is considered suitable if it differs from the target sample only in domain, without differences in unimportant characteristics such as orientation and color, which can hinder the model's focus on aligning the domain difference. 
However, existing instance-level feature alignment methods struggle to find suitable source instances because their search scope is limited to mini-batches.
Mini-batches are often so small in size that they do not always contain suitable source instances.
The insufficient diversity of mini-batches becomes problematic particularly when the target instances have high intra-class variance.
To address this issue, we propose a memory-based instance-level domain adaptation framework. 
Our method aligns a target instance with the most similar source instance of the same category retrieved from a memory storage. 
Specifically, we introduce a memory module that dynamically stores the pooled features of all labeled source instances, categorized by their labels. Additionally, we introduce a simple yet effective memory retrieval module that retrieves a set of matching memory slots for target instances. 
Our experiments on various domain shift scenarios demonstrate that our approach outperforms existing non-memory-based methods significantly.







\end{abstract}


\section{Introduction}
\label{sec:intro}
Although recent object detection models have achieved success on public datasets~\cite{deng2009imagenet, lin2014microsoft, everingham2010Pascal}, they often suffer from a drop in performance when deployed in real-world use cases due to their inability to automatically generalize to unseen target environments. Retraining models on the target is a possible solution, but it is not viable due to the high cost of annotations.


To address this issue, Unsupervised Domain Adaptation (UDA) leverages knowledge transfer from a labeled source domain to an unlabeled target domain. 
The previous UDA approaches for object detection have focused on aligning the instance-level features extracted from source and target object proposals, which are intermediately generated by the detector model. However, most previous works~\cite{he2019multi, rezaeianaran2021seeking} ignore the category of the instances during the alignment, leading to negative knowledge transfer. 
To solve this, some recent works~\cite{tian2021knowledge, xu2020cross, zhang2021rpn, zheng2020cross} proposed category-to-category (C2C) alignment methods. They employ various techniques to identify instances with matching categories in a sampled mini-batch of source and target images, and then align these instances using either adversarial~\cite{rezaeianaran2021seeking, vs2021mega} or contrastive learning~\cite{zhang2021rpn, xu2020cross, zheng2020cross}. 

\begin{figure*}[t!]
  \includegraphics[width=\textwidth]{images/examples_bmvc_combined_a.pdf}
      \caption{(a) Examples of retrieved source instances by MILA and the C2C method for alignment. Our approach retrieves source instances with similar visual characteristics compared to existing C2C methods. (b) Comparison of similarity scores among selected cross-domain pairs for alignment using different methods reveals that MILA aligns a target instance only with a source instance that has a very high similarity score.} \label{fig:fig1}
\end{figure*}

















Even though C2C methods achieve better performance than vanilla instance alignment methods, we argue that aligning a target instance in a category to an arbitrary source instance in the same category is sub-optimal. 
We claim that it is important to align a target instance in a category to the source instance in the same category that is similar in terms of the {\it non-defining} visual characteristics.
We assume that the characteristics of an image in the view of the object detection task are divided into three groups: {\it defining characteristics}, which are indispensable for defining a category (\eg, shape), {\it non-defining characteristics}, which can be different and diverse within a category (\eg, orientation, color), and {\it domain specific characteristics}, which can be different within a category but shared within a domain (\eg, style).
In the domain adaptation process, it is important to focus on aligning domain specific features and aligning non-defining features are not necessary.
By finding a source instance that has similar non-defining characteristics with a target instance, a model is not disturbed by the unimportant differences and can focus solely on the difference in domains. 
The existing C2C methods often struggle to find a suitable source instance because they search a matching instance only within a mini-batch, which does not necessarily contain a suitable source instance since mini-batches are usually small in size and thus tend to lack diversity of the samples (see \Cref{fig:fig1}).


To address this issue, we propose memory-based instance-level adaptation (MILA).
We design MILA in such a way that it can learn alignment from the {\it `reliable'} matching pairs.
A pair of source and target instance is regarded as {\it reliable} when {\it (i)} their features are expected to well represent the categories and {\it (ii)} these features are similar enough so that a model can focus on domain differences.
To increase the chance of finding reliable matching pairs, MILA has four unique characteristics. 
(1) MILA has a memory module for storing source features, which is much larger than mini-batch and thus greatly increases chance to find a suitable source instance for the alignment (contribution to {\it (ii)}).
(2) MILA stores source features only when the model can correctly predict the category of the source instance using the features.
This is to guarantee the quality of the stored features (contribution to {\it (i)}).
(3) MILA uses a target instance for the alignment only when the category prediction confidence is sufficiently high.
This is to guarantee the quality of the target features (contribution to {\it (i)}).
(4) MILA assigns different weights to different matching pairs according to their similarities to emphasize more reliable pairs (contribution to {\it (ii)}).


Our main contributions are summarized into 3 points. 
(1) We are the first, to the best of our knowledge, to argue the importance of finding `reliable' pairs for the domain-adaptive object detection (DAOD) task, and provide with the empirical evidence for it (\Cref{fig:fig1}).
(2) We propose a dedicated design based on the memory module for increasing the chance of finding `reliable' pairs.
(3) We verify the effectiveness of the proposed method and its four characteristics by extensive experiments.
It achieved state-of-the-art results on five cross-domain object detection tasks, with significant relative improvements of 5.5\%, 4.1\%, and 4.0\% on the \textit{Sim10k}, \textit{Comic2k} and \textit{Watercolor2k} benchmark datasets, respectively.













\section{Related Works}
\label{sec:related}



\paragraph{Domain Adaptive Object Detection (DAOD).} DA-Faster~\cite{chen2018domain} proposed an early DAOD method that performs feature alignment at both
image-level and instance-level. MAF~\cite{he2019multi} and~\cite{xie2019multi} extended this idea to multi-layer feature adaptation of backbone network. SWDA~\cite{saito2019strong} suggests that aligning local features strongly is more effective than aligning global features strongly. CRDA~\cite{xu2020exploring} and MCAR~\cite{zhao2020adaptive} introduce a multi-label classifier upon backbone network to regularize the features. Recent methods~\cite{xu2020cross, he2020domain, li2020spatial, zhang2021rpn, vs2021mega, su2020adapting, zhu2019adapting} align instance-level features from object proposals using category-aware manner (C2C). They derive prototype representation of each category by aggregating multiple instances before inducing alignment. However, this causes loss of intra-class variance information and leads to suboptimal prototypes for alignment.
\paragraph{Memory Networks.}
The memory network~\cite{weston2014memory, sukhbaatar2015end} is a type of neural network module that utilizes an external memory to store and retrieve relevant information. It has been widely utilized in vision-related tasks, such as video object segmentation~\cite{rodriguez2019domain, oh2019video}, movie understanding~\cite{na2017read}, and visual tracking~\cite{yang2018learning} due to its ability to retain diverse knowledge types.


Memory networks have also been used in domain adaptation~\cite{memsac}, and DAOD~\cite{vs2021mega}. The closest to our work is MeGA-CDA~\cite{vs2021mega}, which utilizes memory modules for storing class-prototypes and use them to create category-specific attention maps for better C2C alignment between source and target instances.
Although MeGA-CDA is similar to MILA in a sense that both use memory module, their motivations differ significantly. 
MeGA-CDA is the method dedicated for C2C alignment and it aims to find source regions that correspond to a particular category within a limited search scope of each mini-batch, while MILA is designed to find most `reliable' source instance of a category for the alignment and exploit more from reliable pairs.
In other words, MeGA-CDA cares only categories, while MILA also cares specific instances in addition to categories.
MILA has many unique characteristics to increase the chance of finding reliable matching pairs, as mentioned in the introduction.
In fact, sometimes MeGA-CDA fails to find the appropriate source regions to match with a given target sample because there is no guarantee that a mini-batch always contains an object of a particular category.
In contrast, MILA can store wide variety of source instances of all the categories in the memory and therefore can always find a source instance of the same category as a given target instance.










\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{images/network.pdf}
  \caption{Network Overview: Mainly consist of memory module,  similarity-based memory retrieval module, and instance-level domain adaptation module}
  \label{fig:fig2}
\end{figure*}




\section{Method}



\subsection{ Problem Formulation}\label{sec:problem}
Given a labeled source dataset $D_S = \{x^S_i, y^S_i\}_{i=1}^{N_S}$ and an unlabeled target dataset $D_T = \{x^T_i\}_{i=1}^{N_T}$, the task of UDA is to transfer knowledge from $D_S$ to $D_T$ and predict accurate labels for $D_T$. Even though $D_S$ and $D_T$ have the same label space, they come from very different data distributions, which makes UDA a very challenging task. Since we focus on UDA for object detection, $y^S_{i} = \{b^S_{i,j}, c^S_{i,j}\}_{j=1}^{K^S_i}$ contains bounding box labels $b^S_{i,j}\in [0, 1]^4$ and category labels $c^S_{i,j} \in \{1, \dots, C\}$ for $K^S_i$ objects in image $x^S_i$, where there are $C$ different categories of objects.

\subsection{Overview}\label{sec:overview}



To overcome the limitations of existing C2C alignment methods discussed in \cref{sec:intro}, we present MILA. \Cref{fig:fig2} illustrates our proposed approach, which is built on Faster R-CNN, following prior works~\cite{saito2019strong, xu2020cross, li2022cross}. MILA includes three main modules: (1) a \textit{memory module} that stores instance-level features and category information from previously-seen source images, (2) a \textit{memory retrieval module} that retrieve most `reliable' source instances of the same category as a given target instance feature,
and (3) a \textit{similarity-weighted alignment} that scales target instance alignment based on similarity to retrieved source instances. Detailed descriptions of these modules are provided in the following sections.
We consider the two-stage Faster-RCNN to be made up of a feature extractor $f( ; \theta)$ followed by a region-proposal network $rpn(; \phi)$ and a detection head $det(;\psi)$, where $\theta, \phi$ and $\psi$ are learnable parameters. 


\subsection{Memory Module}\label{sec:memory module}
The memory module is used to save the instance-level features of source images extracted from the Faster R-CNN backbone in a memory bank. Specifically, for a source image $x^S_i$, $f$ computes the feature map as $f(x^S_i; \theta)$. 
The features for $j^{th}$ instance in the image can be extracted from $f(x^S_i; \theta)$ by pooling corresponding instance bounding box $b^S_{i,j}$ available in $y^S_i$. Therefore,
\begin{equation}
    \hat{F}^S_{i,j}= roipool\left(f(x^S_i; \theta), b^S_{i,j}\right) \hspace{1mm},
\end{equation}
where $roipool$ is the conventional region-of-interest (ROI) pooling function used in Faster R-CNN. The extracted instance features $\hat{F}^S_{i,j}$ is then filtered based on the classification accuracy as follows:

\begin{equation}
    F^S_{i,j} = \begin{cases} \hat{F}^S_{i,j} & \text{if } \hat{c}^S_{i,j} = c^S_{i,j} \\
discard & \text{otherwise}
\end{cases}
\end{equation}
where $\hat{c}^S_{i,j}$ is the predicted class of the source proposal $\hat{F}^S_{i,j}$ of original class label $c^S_{i,j}$.
The filtered instance features $F^S_{i,j}$ are category-wise stored in the memory bank, which is created as follows:
\begin{equation}
M(c) = \left\{\left\{\mathbf{I}\left(c^S_{i,j} = c\right)F^S_{i,j}\right \}_{j=1}^{K^S_i}\right\}_{i=1}^{N_S} \hspace{1mm} ,
\end{equation}
where $\mathbf{I}$ denotes the indicator function and $M(c)$ denotes all the stored instance features of category $c$, where $1 \leq c \leq C$.







During training, as $\theta$ gets updated and $f$ extracts more domain-aligned features, we propose dynamically updating the memory to store high-quality representations. Note that the memory $M$ is only created during training.



\subsection{Similarity-Based Memory Retrieval Module} \label{sec:retrieval module}
This module's objective is to retrieve `reliable' source instances that have similar features to the target instance, allowing the model to focus on domain differences.
The retrieval process is structured as follows: (1) Predicting the bounding boxes and their categories in the target image, and filtering out the inaccurate predictions. (2) Extracting the instance-level features from the predicted bounding boxes in the target images. (3) Retrieving the top-$K$ similar source instance features from memory $M$ that correspond to each target instance feature.








For $i^{th}$ target image $x^T_i$, the feature map is generated as $f(x^T_i; \theta)$. This feature map is then used by $rpn(;\phi)$ and $det(;\psi)$ to predict $\hat{y}^T_{i}$, which has $K^T_i$ instances with bounding box, category, and confidence score, i.e., 
\begin{equation}
    \hat{y}^T_{i} =\{b_{i,j}^T, c_{i,j}^T, s_{i,j}^T\}_{j=1}^{K^T_i} = det( rpn( f(x^T_i; \theta); \phi); \psi)
\end{equation}

To ensure MILA's effectiveness, it's crucial to filter out noisy predictions as they can hamper performance. Existing methods~\cite{li2022cross} use a fixed threshold $\delta$ for all classes to remove predictions with confidence scores ($s_{i,j}^T$) below $\delta$. However, due to class imbalance in the training data, lower confidence scores are often produced for underrepresented classes, making a fixed threshold impractical. To address this challenge, we assign category-specific thresholds $\delta_{c}$ based on the detection accuracy of each category in the source dataset inspired by \cite{sinha2020class}.
Note before filtering, we perform non-maximum suppression (NMS) $\sigma$ to remove duplicate bounding boxes, i.e.,

\begin{equation}
    \tilde y^T_{i} =  \left\{\sigma \left(b_{i,j}^T, c_{i,j}^T, s_{i,j}^T\right) \middle| s^T_{i,j} \geq \delta_{c}\right\}^{j=K^T_i}_{j=1}   \hspace{1mm},
\end{equation}
Once we have the filtered bounding boxes in $\tilde y^T_{i}$, we can extract instance-level features from $f(x^T_i; \theta)$ by pooling from the predicted bounding boxes $b^T_{i,j}$ ($j^{th}$ box for $i^{th}$ target image) as
\begin{equation}
    F^T_{i,j} = roipool(f(x^T_i; \theta), b^T_{i,j}) \hspace{1mm},
\end{equation}


From memory $M$, we select the source features to be used for the alignment with $F^T_{i,j}$ from the features that has the same category as the predicted category $c_{i,j}^T$.
The cosine similarity scores of $F^T_{i,j}$ with each source feature of the same category is computed as
\begin{equation}
    S(F^T_{i,j}, M(c^T_{i,j})_z) = \frac{F^T_{i,j} . M(c^T_{i,j})_z}{\parallel F^T_{i,j}\parallel \parallel M(c^T_{i,j})_z\parallel} \hspace{1mm},
\end{equation}
where $M(c)_z$ is the $z^{th}$ source instance feature stored in $M(c)$.
Now, we can easily retrieve top-$K$ most similar source instance features of the same category as the target instance $c^T_{i,j}$.
We call them positive samples and denote by $E^{+}_{i,j,k}$, where $1\leq k \leq K$ represents the index. Similarly, negative samples $E^{-}_{i,j,k}$ are obtained by randomly selecting 
one sample from each of the categories that are different from the category of the positive pairs. 

\subsection{Similarity-weighted Instance-level Domain Adaptation} \label{sec:alignment}
Once we retrieve positive and negative set of instances from source memory for a given target instance, we align them by applying a specially designed max-margin contrastive losses. 
For a target instance feature $F_{i,j}^{T}$ and $k$-th positive sample $E^{+}_{i,j,k}$, contrastive loss enforces them to come closer in latent space, whereas pushes the features apart for negative samples $E^{-}_{i,j,k}$.

\begin{equation}
\begin{gathered}
\mathcal{L}^{+}_{i,j} =\frac{1}{K}\sum_{k=1}^{K}S(F^T_{i,j}, E^+_{i,j,k})d_{pos}, \hspace{10pt}
\mathcal{L}^{-}_{i,j} = \frac{1}{C-1}\sum_{k=1}^{C-1}max(0, m - d_{neg}),\\
\mathcal{L}_{Ins} = \frac{1}{N^T}\sum_{i=1}^{N^T} \left( \frac{1}{K^T_i} \sum_{j=1}^{K^T_i}\mathcal{L}^{+}_{i,j} + \mathcal{L}^{-}_{i,j} \right). \\
\label{eqn:contrastive loss}
\end{gathered}
\end{equation}
Where $d_{pos}$ and $d_{neg}$ are the Euclidean distances of $F_{i,j}^{T}$ with $E^{+}_{i,j,k}$ and $E^{-}_{i,j,k}$, respectively. Recall that cardinality of the negative sample sets is $C-1$, i.e., $|E^{-}_{i,j,k}|=C-1$.


\subsection{Overall Objective}\label{sec:overall loss}
In addition to the instance-level alignment loss discussed earlier, we use supervised loss $\mathcal{L}_{Sup}$, unsupervised loss $\mathcal{L}_{Unsup}$, and discriminator loss $\mathcal{L}_{Dis}$.
$\mathcal{L}_{Sup}$ is the object detection loss optimized using labeled data from the source domain.
$\mathcal{L}_{Unsup}$ 
is the object detection loss computed on target domain images with pseudo labels generated using a similar approach as in~\cite{li2022cross}.
$\mathcal{L}_{Dis}$ is the loss of an image-level binary domain discriminator used in~\cite{he2019multi, xie2019multi}.
$\mathcal{L}_{Dis}$ is computed by a domain discriminator $D$ whose aim is to discriminate where the backbone feature is from ($d = 0$) or target ($d = 1$). 
$\mathcal{L}_{Dis}$ is formulated as follows:
\begin{align*}
  \MoveEqLeft[5] \mathcal{L}_{Dis} = \frac{1}{(N^T+N^S)} \sum_{i=1}^{N^T+N^S}- d\log D(f(x_{i}; \theta))-(1-d)\log(1-D(f(x_{i}; \theta))),
\end{align*}
Note that the gradients obtained from this loss term are used to update not only the parameters of the discriminator $D$, but also reversed by the gradient reversal layer (GRL) during backpropagation to update $\theta$. This helps $f(;\theta)$ to learn discriminative features that can confuse $D$.
Combining altogether, the overall objective function is as follows:
\begin{equation}
\mathcal{L}=\mathcal{L}_{Sup} + \lambda_1 \mathcal{L}_{Unsup}+ \lambda_2 \mathcal{L}_{Dis} + \lambda_3 \mathcal{L}_{Ins},
\label{eq:overall_loss}
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ are the hyperparameters to control the weight of each loss.



\begin{table}
\begin{minipage}[t]{.45\textwidth}
  \footnotesize	
  \centering
  \addtolength{\tabcolsep}{-3.0pt}
  \scalebox{0.7}{
    \begin{tabular}{l|cccccc|r}
      \toprule Method & bicycle & bird & car & cat & dog & person & mAP\\
      \midrule Source & 32.5 & 12.0 & 21.1 & 10.4 & 12.4 & 29.9 & 19.7 \\\midrule 
      DA-Faster~\cite{chen2018domain} & 31.1 & 10.3 & 15.5 & 12.4 & 19.3 & 39.0 & 21.2 \\SWADA~\cite{saito2019strong} & 36.4 & 21.8 & 29.8 & 15.1 & 23.5 & 49.6 & 29.4 \\MCAR~\cite{zhao2020adaptive} & 49.7 & 20.5 & 37.4 & 20.6 & 24.5 & 53.6 & 33.5 \\D-Adapt~\cite{jiang2021decoupled} & 52.4 & 25.4 & 42.3 & \textbf{43.7} & 25.7 & 53.5 & 40.5 \\\midrule Ours & \textbf{59.1} & \textbf{28.5} & \textbf{49.8} & 28.3 & \textbf{35.7} & \textbf{66.3} & \textbf{44.6}\\
      \midrule Oracle & 44.2 & 35.3 & 31.9 & 46.2 & 40.9 & 70.9 & 44.6\\
      
      \bottomrule \end{tabular}
    }
  \caption{Results on the Comic2k test set for \textbf{Pascal VOC$\xrightarrow{}$Comic2k} adaptation (ResNet-101).} \label{tab:table1_}
\end{minipage}\qquad
\begin{minipage}[t]{.45\textwidth}
  \footnotesize	
  \addtolength{\tabcolsep}{-3.0pt}
  \scalebox{0.72}{
    \begin{tabular}{l|cccccc|r}
      \toprule Method & bicycle & bird & car & cat & dog & person & mAP\\
      \midrule Source & 84.2 & 44.5 & 53.0 & 24.9 & 18.8 & 56.3 & 46.9\\ \midrule 
      SCL~\cite{shen2019scl} & 82.2 & 55.1 & 51.8 & 39.6 & 38.4 & 64.0 & 55.2 \\SWADA~\cite{saito2019strong} & 82.3 & 55.9 & 46.5 & 32.7 & 35.5 & 66.7 & 53.3 \\UMT~\cite{deng2021unbiased} & 88.2 & 55.3 & 51.7 & 39.8 & 43.6 & 69.9 & 58.1 \\AT~\cite{li2022cross} & 94.3 & 57.2 & 57.2 & 34.2 & 36.9 & 78.5 & 59.7 \\ \midrule Ours & \textbf{97.4} & \textbf{59.0} & \textbf{58.3} & \textbf{40.6} & \textbf{47.8} & \textbf{79.3} & \textbf{63.7}\\ \midrule Oracle & 51.8 & 49.7 & 42.5 & 38.7 & 52.1 & 68.6 & 50.6\\
      
      \bottomrule \end{tabular}
   }
  \caption{Results on the Watercolor2k test set for \textbf{Pascal VOC$\xrightarrow{}$Watercolor2k} adaptation (ResNet-101).} \label{tab:table3}
\end{minipage}
\end{table}












\section{Experiments}
\subsection{Datasets}
We performed extensive experiments on seven publicly available datasets covering five scenarios of domain shift. The datasets are Pascal VOC~\cite{everingham2010Pascal}, Clipart1k~\cite{inoue2018cross}, 
Watercolor2k~\cite{inoue2018cross}, 
Comic2k~\cite{inoue2018cross}, 
Sim10k~\cite{johnson2016driving}, Cityscapes~\cite{cordts2016cityscapes}, and FoggyCityscapes~\cite{sakaridis2018semantic}. Pascal VOC comprises 16,551 images of 20 categories of common objects from the real world. Clipart1k contains 1k comical images with the same 20 categories as Pascal VOC. Watercolor2k includes 1k training and 1k testing watercolor-style images, sharing six categories with Pascal VOC. Similarly, Comic2k consists of 1k training and 1k test images, sharing six categories with Pascal VOC. Sim10k contains 10,000 images with 58,701 bounding boxes of car categories, while both Cityscapes and FoggyCityscapes comprise 2,975 training images and 500 validation images with eight object categories. We evaluate the domain adaptation performance of different methods using the standard setting~\cite{saito2019strong, li2022cross} on the following five domain adaptation tasks: (i) Pascal VOC$\xrightarrow{}$Comic2k, (ii) Pascal VOC$\xrightarrow{}$Watercolor2k, (iii) Pascal VOC$\xrightarrow{}$Clipart1k, (iv) Sim10k$\xrightarrow{}$Cityscapes, and (v) Cityscapes$\xrightarrow{}$FoggyCityscapes.
Due to space constraints, we provide results for Pascal VOC$\xrightarrow{}$Clipart1k in supplementary materials.








\subsection{Implementation Details}\label{sec:implementation_details}
We adopt the Faster R-CNN model with either ResNet-101 or VGG16 architectures and implement it using Detectron2, following the approach in~\cite{saito2019strong, xu2020cross, tian2021knowledge, li2022cross}. We fine-tune the parameters of ResNet-101 from the model pre-trained on ImageNet~\cite{deng2009imagenet}, while VGG16 parameters are learned from scratch. The images are scaled by resizing the shorter side to 600 pixels while maintaining the aspect ratios, following the common practice~\cite{he2019multi}. We apply a set of strong and weak data augmentations as described in~\cite{li2022cross}. Our evaluation metric is the average precision (AP) for each class and the mean AP (mAP) over all classes.
For the hyperparameters, we set $\lambda_1$ to $1.0$, $\lambda_2$ to $0.1$, and $\lambda_3$ to $0.1$ unless otherwise stated. Margin $m$ in~\cref{eqn:contrastive loss} is set to $1.0$. We update the memory after every $1/3$ of an epoch and use stochastic gradient descent (SGD) with momentum 0.9 and a learning rate of 0.04 throughout the training stage, without any learning rate decay. We build on the code provided by the authors of~\cite{li2022cross} and follow their hyperparameter settings. The experiments are conducted on 4 Nvidia GPU V100 with a batch size of 8 or 4 depending on the dataset, using PyTorch.














































\subsection{Comparison with state-of-the-arts}
We compare the proposed MILA with the state-of-the-art DAOD methods, including SCL~\cite{shen2019scl}, SWADA~\cite{saito2019strong}, DM~\cite{kim2019diversify}, CRDA~\cite{xu2020exploring}, HTCN~\cite{chen2020harmonizing}, DA-Faster~\cite{chen2018domain}, MCAR~\cite{zhao2020adaptive}, D-Adapt~\cite{jiang2021decoupled},
MAF~\cite{he2019multi}, SCDA~\cite{zhu2019adapting}, CDN~\cite{li2020spatial}, MeGA-CDA~\cite{vs2021mega}, CADA~\cite{hsu2020every}, UMT~\cite{deng2021unbiased}, and Adaptive Teacher (AT)~\cite{li2022cross}.
Our implementation, as described in Sec.~\ref{sec:implementation_details}, builds upon the official code of \cite{li2022cross}. To ensure fairness, we compared our method with the best-reproduced results of AT, under the same conditions. The original results from~\cite{li2022cross} are in the supplementary material. `Source' is the baseline model without domain adaptation, while `Oracle' is trained and tested on the target domain.  








\begin{table}
\begin{minipage}[t]{.5\textwidth}
\centering 
  \footnotesize	
  \addtolength{\tabcolsep}{-3.5pt}
  \scalebox{0.6}{
  \begin{tabular}{l|cccccccc|r}
\toprule Method & bus & bicycle & car & mcycle & person & rider & train & truck & mAP\\
      \midrule Source (F-RCNN) & 20.1 & 31.9 & 39.6 & 16.9 & 29.0 & 37.2 & 5.2 & 8.1 & 23.5 \\\midrule 
      SCL~\cite{shen2019scl} & 41.8 & 36.2 & 44.8 & 33.6 & 31.6 & 44.0 & 40.7 & 30.4 & 37.9 \\DA-Faster~\cite{chen2018domain} & 35.3 & 27.1 & 40.5 & 20.0 & 25.0 & 31.0 & 20.2 & 22.1 & 27.6 \\SCDA~\cite{zhu2019adapting} & 39.0 & 33.6 & 48.5 & 28.0 & 33.5 & 38.0 & 23.3 & 26.5 & 33.8 \\SWDA~\cite{saito2019strong} & 36.2 & 35.3 & 43.5 & 30.0 & 29.9 & 42.3 & 32.6 & 24.5 & 34.3 \\DM~\cite{kim2019diversify} & 38.4 & 32.2 & 44.3 & 28.4 & 30.8 & 40.5 & 34.5 & 27.2 & 34.6 \\MTOR~\cite{cai2019exploring} & 38.6 & 35.6 & 44.0 & 28.3 & 30.6 & 41.4 & 40.6 & 21.9 & 35.1 \\MAF~\cite{he2019multi} & 39.9 &  33.9 & 43.9 & 29.2 & 28.2 & 39.5 & 33.3 & 23.8 & 34.0 \\iFAN~\cite{zhuang2020ifan} & 45.5 & 33.0 & 48.5 & 22.8 & 32.6 & 40.0 & 31.7 & 27.9 & 35.3 \\CRDA~\cite{xu2020exploring} & 45.1 & 34.6 & 49.2 & 30.3 & 32.9 & 43.8 & 36.4 & 27.2 & 37.4 \\HTCN~\cite{chen2020harmonizing} & 47.4 & 37.1 & 47.9 & 32.3 & 33.2 & 47.5 & 40.9 & 31.6 & 39.8 \\UMT~\cite{deng2021unbiased} & 56.5 & 37.3 & 48.6 & 30.4 & 33.0 & 46.7 & 46.8 & 34.1 & 41.7 \\AT~\cite{li2022cross} & 60.0 & 49.0 & 63.6 & 38.8 & 45.0 & \textbf{53.9} & 45.1 & 33.9 & 49.0 \\\midrule
      Ours & \textbf{61.4}  & \textbf{51.5} & \textbf{64.8} & \textbf{39.7} & \textbf{45.6} & \textbf{52.8} & \textbf{54.1} & \textbf{34.7} & \textbf{50.6} \\\midrule Oracle (F-RCNN) & 50.3 & 40.7 & 61.3 & 32.5 & 43.1 & 49.8 & 35.1 & 28.6 & 42.7\\
      \bottomrule \end{tabular}
    }
    \caption{Results on the FoggyCityscapes test set for \textbf{Cityscapes $\xrightarrow{}$ Foggy Cityscapes} adaptation (VGG-16).} 
   
   \label{tab:table2}
\end{minipage}\qquad
\begin{minipage}[t]{.45\textwidth}
  \footnotesize	
  \centering
  \addtolength{\tabcolsep}{-3.0pt}
  \scalebox{0.68}{
   \begin{tabular}{l|c|r}
      \toprule Method & Backbone & AP on Car\\
      \midrule DA-Faster~\cite{chen2018domain} &  & 38.9 \\
      BDC-Faster~\cite{saito2019strong} &  & 31.8 \\
      SWADA~\cite{saito2019strong} &  & 40.1 \\
      MAF~\cite{he2019multi} &   & 41.1 \\
      SCDA~\cite{zhu2019adapting} &   & 43.0 \\
      CDN~\cite{li2020spatial} &  VGG-16 & 49.3 \\
      MeGA-CDA~\cite{vs2021mega} &   & 44.8 \\
UMT~\cite{deng2021unbiased} &   & 43.1\\
\midrule 

      Source &   & 41.8 \\
      CADA~\cite{hsu2020every} &  & 51.2 \\
      D-adapt~\cite{jiang2021decoupled} & ResNet-101 & 51.9\\
      Ours &  & \textbf{57.4} \\
      \midrule Oracle &  & 70.4\\
      \bottomrule \end{tabular}
    }
   \caption{Results on the Cityscapes test set for \textbf{Sim10k $\xrightarrow{}$ Cityscapes} adaptation.} 
   \label{tab:table2_}
\end{minipage}\qquad
\end{table}












\paragraph{Adaptation between dissimilar domains.}
In our first set of experiments, we use the Pascal VOC as the source domain and Comic2k as the target domain, which has a very different style from Pascal VOC and contains many small objects. Table~\ref{tab:table1_} shows that MILA outperforms all the previous methods and improves mAP by 4.1 compared with the state-of-the-art. 
\\[2.5pt]
In the next set of experiments, we evaluate MILA on Watercolor2k, another target domain with a unique style. As shown in Table~\ref{tab:table3}, MILA achieves the highest average precision for all object categories, surpassing all previous methods by a significant margin. Additionally, MILA surpasses the oracle model on the Watercolor2k dataset by a considerable margin of $13.1\%$. 
These results consistently validate the effectiveness of aligning the most similar instances across domains in reducing the domain gap between different scenarios.


\paragraph{Adaptation between similar domains.}
The results of this setting are presented in Table~\ref{tab:table2}. MILA achieves the highest mAP in majority of the categories. Upon closer inspection, we observed that MILA achieved the largest performance gain of $+9.0\%$ in `train' class, which has the least number of instances, with only 504 training samples. This finding suggests that classes with fewer training instances specially benefit from the proposed memory module. We hypothesize that this is because it is generally more challenging to align less populated classes due to the difficulty of finding an appropriate alignment target. MILA helps to overcome this challenge by storing all the alignment targets in the memory.



\paragraph{Adaptation from synthetic to real images.}
We use Sim10k as the source domain and Cityscapes as the target domain. Following~\cite{jiang2021decoupled}, we evaluate on the validation split of the Cityscapes and report the mAP on car. Table~\ref{tab:table2_} shows that MILA scores the new state-of-the-art adaptation performance, achieving gain of 5.5 points on mAP.
































































































\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{images/all_vertical_final_arxiv.pdf}
  \caption{Visualization of instance pairs (a) Pascal VOC$\xrightarrow{}$Clipart1k (b) Pascal VOC$\xrightarrow{}$Comic2k (c) Sim10k$\xrightarrow{}$Cityscapes}
  \label{fig:fig100}
\end{figure*}



\subsection{Visualization of Instance Pairs}\label{sec:qualitative}
To visualize reliable matching pairs used in alignment, we present target instances retrieved by MILA in \cref{fig:fig100}.
In the second example of \cref{fig:fig100} (a), MILA retrieves a biker instance with a matching color scheme for the helmet and bike as the target instance. Similarly, in the fourth example (\cref{fig:fig100} (a)), MILA successfully identifies a person wearing a matching dress, showcasing its ability to capture subtle visual details essential for effective domain adaptation. 

On other datasets, MILA consistently demonstrates its capability to retrieve instances with remarkable visual similarities. For example, in \cref{fig:fig100} (b), the first and fifth example demonstrate MILA's ability to retrieve instances of the cat category that not only share similar color and orientation but also exhibit an overall appearance that closely matches the target instances.
In \cref{fig:fig100} (c), MILA retrieve car instances with matching color and orientation, as evident in examples 1, 2, and 6 for rear-facing target cars and examples 3, 4, and 5 for side-facing cars. In contrast, existing C2C methods retrieve source instances of car that display significant differences in color and orientation when compared to the target instances.


These visual examples highlight MILA's exceptional capability in identifying source instances with similar non-defining visual characteristics as the target compared to existing C2C methods. By focusing solely on domain differences and disregarding unimportant dissimilarities, our model achieves superior accuracy.
















\begin{wraptable}{l}{0.45\textwidth}
\footnotesize
\centering
\addtolength{\tabcolsep}{-4.0pt}
 \scalebox{0.9}{
\begin{tabular*}{0.45 \textwidth}{@{\extracolsep{\fill}\quad}l|ccccc}
\toprule
mAP & $m$ & $f$ & $s$ & $t$ & $u$ \\
\midrule
$44.6$ & \checkmark & \checkmark & \checkmark & \checkmark & \xmark \\
$40.3$ &\xmark & \xmark & \xmark & \xmark & \checkmark  \\
$42.6$ &\checkmark & \xmark & \checkmark & \checkmark & \xmark \\
$42.2$ & \checkmark & \checkmark & \xmark & \checkmark & \xmark   \\
$43.9$ &\checkmark & \checkmark & \checkmark & \xmark & \checkmark   \\


\bottomrule
\end{tabular*}
}
 \caption{Ablation study of different components. } \label{tab:tableab_}
\end{wraptable}



\subsection{Ablation Study}\label{sec:ablation}
In this section, we assess effectiveness of different components in our approach on the Pascal VOC $\rightarrow$ Comic2k dataset (see \cref{tab:tableab_}). We used our full model with all four characteristics discussed in ~\cref{sec:intro} and turned off each component one by one to evaluate their impact on the detection accuracy of the model.

First, we evaluate the \textbf{memory module's effectiveness ($m$)} by comparing the performance of our model with and without the memory module. We used a non-memory based instance alignment scheme~\cite{xu2020cross} instead and used the same contrastive loss function $\mathcal{L}_{Ins}$, but sampled positive and negative instances from the mini-batch instead of the memory module. Our results showed that MILA improves object detection accuracy by 4.3\% compared to the model without the memory module ($2^{nd}$ row), indicating the importance of the memory module in enhancing performance. \\
Secondly, we analyze the \textbf{effectiveness of source feature filtering ($f$)} to ensure the quality of the stored features in memory by checking their predicted category. We observed that the performance drops by $2.0$ points ($3^{rd}$ row) if we turn off this component and store all source features in memory without judging their quality.
\\
Thirdly, we evaluate the \textbf{effectiveness of similarity weighting of contrastive loss ($s$)} ($4^{th}$ row). We observe that if we use plain contrastive loss function, there is a drop in accuracy ($2.2\%$), which suggests that weighting our loss function by similarity helps to mitigate negative knowledge transfer when the aligned features are highly dissimilar. As a result, the detection accuracy improves.
\\
We also analyzed the \textbf{effectiveness of category-aware thresholding ($t$) and fixed thresholding ($u$)}. The model performed slightly better with category-imbalance aware thresholding, compared to the fixed thresholding. \\
In summary, our results demonstrate that all four components are critical for our approach, and turning them off results in decreased detection accuracy. Further analysis on the memory module and hyperparameters are available in the supplementary material.









\section{Conclusion}
In this paper, we propose a Memory-based Instance-Level Alignment (MILA) framework for cross-domain object detection. 
The proposed strategy with four unique characteristics enables the model in finding ‘reliable’ pairs for alignment in the domain-adaptive object detection (DAOD) task. As evident in the results, this can efficiently improve the adaptation of instances by only focusing on important visual characteristics that distinguish the two domains. Extensive experiments demonstrate that MILA achieves state-of-the-art performance for adapting object detectors on several benchmark datasets.



\bibliography{egbib}


\section{Supplementary Material}
\subfile{supplementary.tex}

\end{document}
