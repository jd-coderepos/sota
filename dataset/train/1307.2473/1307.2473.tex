\documentclass{article}
\usepackage{amsmath}
\usepackage{vmargin}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{bussproofs}
\usepackage{tensor}
\usepackage{xypic}
\usepackage{amsthm}
\usepackage{url}

\newcommand{\defeq}{\triangleq}

\newcommand{\ringunit}{\textbf 1}
\newcommand{\ringzero}{\textbf 0}
\newcommand{\aff}{\mathrm{Aff}_1^c}

\newcommand{\lexpt}{\mathsf{exp}}
\newcommand{\lvart}{\mathsf{var}}
\newcommand{\lcomt}{\mathsf{com}}
\newcommand{\lacct}{\mathsf{acc}}

\newcommand{\lcompc}{\mathsf{comp}}
\newcommand{\lopc}{\mathsf{op}}
\newcommand{\lasgc}{\mathsf{asg}}
\newcommand{\lderefc}{\mathsf{deref}}
\newcommand{\lifc}{\mathsf{if}}
\newcommand{\lnewc}{\mathsf{new}}
\newcommand{\lskipc}{\mathsf{skip}}

\newcommand{\sbr}[1]{\llbracket {#1}\rrbracket}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}
\newcommand{\mv}[1]{\mathsf{#1}}
\newcommand{\keep}{\downharpoonright}

\newcommand{\catplus}{\varoplus}
\newcommand{\cattimes}{\varodot}

\newcommand{\btr}{\blacktriangleright}
\newcommand{\constr}[2]{#1 \blacktriangleright #2}
\newcommand{\true}{\mathit{true}}

\begin{document}
\title{From bounded affine types to automatic timing analysis}
\author{
Dan R. Ghica \qquad Alex Smith\\
University of Birmingham
}
\maketitle
\begin{abstract}
Bounded linear types have proved to be useful for automated resource analysis and control in functional programming languages. In this paper we introduce an affine bounded linear typing discipline on a general notion of resource which can be modeled in a semiring. For this type system we provide both a general type-inference procedure, parameterized by the decision procedure of the semiring equational theory, and a (coherent) categorical semantics. This is a very useful type-theoretic and denotational framework for many applications to resource-sensitive compilation, and it represents a generalization of several existing type systems. As a non-trivial instance, motivated by our ongoing work on hardware compilation, we present a complex new application to calculating and controlling timing of execution in a (recursion-free) higher-order functional programming language with local store. 
\end{abstract}

\section{Resource-aware types and semantics}

The two important things about a computer program are what it computes and what resources it needs to carry out the computation successfully. Correctness of the input-output behavior of programs has been, of course, the object of much research from various conceptual angles: logical, semantical, type-theoretical and so on. Resource analysis has been conventionally studied for algorithms, such as time and space complexity, and for programs has long been a part of research in compiler optimization. 

An exciting development was the introduction of semantic~\cite{DBLP:conf/concur/Boudol93} and especially type theoretic~\cite{DBLP:conf/lics/Hofmann99a} characterizations of resource consumption in functional programming languages. Unlike algorithmic analyses, type based analysis are formal and can be statically checked for implementations of algorithms in concrete programming languages. Unlike static analysis, a typing mechanism is compositional which means that it supports, at least in principle, separate compilation and even a foreign function interface: it is an analysis based on signatures rather than implementations. 

Linear logic and typing, because of the fine-grained treatment of resource-sensitive structural rules, constitute an excellent framework for resource analysis, especially in its bounded fragment~\cite{girard1992bounded}, which can logically characterize polynomial time computation. Bounded Linear Logic (BLL) was subsequently extended to improve its flexibility while retaining poly-time~\cite{DBLP:conf/tlca/LagoH09} and further extensions to linear \emph{dependent} typing were used to completely characterize complexity of evaluation of functional programs \cite{DBLP:journals/corr/abs-1104-0193}. 

Although such analyses use \emph{time} as a motivating example, they can be readily adapted to other \emph{consumable} resources such as energy or network traffic. A slightly different angle on resource sensitivity is control of \emph{reusable} resources which can be allocated and de-allocated at runtime, the typical example of which is \emph{memory}, especially \emph{local} (stack-allocated) memory. A well-behaved program will leave the stack empty upon termination, so talking about the total usage of stack-allocated memory is meaningless. Also, talking about the total number of allocations (\emph{push}) on the stack is rarely interesting. What is interesting is that the maximum size of the stack, which is bounded on most architectures, is not exceeded. For reusable resources the relevant limits are, therefore, concerning the \emph{rate} at which the resource is used, for example power (as opposed to energy) or bandwidth (as opposed to total network traffic). In previous work, the first author used a BLL-like type system to bound the number of simultaneous concurrent threads in a parallel functional programming language in order to extract finite models~\cite{DBLP:journals/tcs/GhicaMO06}. This view of concurrent threads as a (reusable) resource proved to be instrumental in facilitating the compilation of functional-imperative programming languages directly into electronic circuits~\cite{DBLP:conf/popl/Ghica07} and is closely related (conceptually, if not formally) to the use of sub-linear runtime space restrictions~\cite{DBLP:conf/aplas/LagoS10}. 

As type systems become more sophisticated the burden on the programmer may increase correspondingly, unless type inference is used to automate the typing process. In the case of bounded linear types the bounds can be calculated fully automatically, by solving a system of numeric constraints~\cite{DBLP:conf/popl/GhicaS11}. In the case of dependent typing this procedure is not decidable, but reduction to constraint systems can still greatly simplify the typing burden~\cite{DBLP:conf/popl/LagoP13}. 

Resource-awareness can be usually captured quite well by operational models of programming languages or typing systems. This is a common feature of the work cited above. A notable exception is the use of game semantics as a \emph{denotational} framework for resource sensitivity, which was introduced by the first author~\cite{DBLP:conf/popl/Ghica05} and recently formulated in a more abstract denotational setting~\cite{lairdmmp13}. 

\section{Contribution and paper outline}

The first part of our present work generalizes bounded linear (or, rather, affine) type systems to an abstract notion of resource, so long as it can be modeled in a semiring. For this abstract type system we show how the problem of type inference can be reduced to a system of constraints based on the equational theory of the resource semiring. Provided this theory is decidable, a type inference algorithm automatically follows. Also for the abstract type system we give a simple categorical framework for which we prove the key result of \emph{coherence}. Because meaning is calculated inductively on the derivation of the typing judgment, and because these derivations are not unique, coherence is the property guaranteeing that all these interpretations are actually equal. Coherence for a categorical semantics is the analogue of a subject reduction lemma in an operational semantics, the basic guarantee of its well-formedness. 

The second part of our work presents a non-trivial application to timing analysis and automated pipelining of computations in a recursion-free functional programming language with local store. The key notion of resource is that of a \emph{schedule} of a computation, i.e.\ the multiset of \emph{stages}, as defined by the start and end of computation, at which a term undergoes execution. Mathematically, stages are contractive affine transformations representing a sub-interval of the unit interval, taken conventionally as the overall duration of execution of the entire program. The resource reading of duration makes good intuitive sense in our target application, automated pipelining, as each stage in a pipeline can be seen as a reusable resource which is either free or busy at any given time. Both the type inference and the categorical semantics are applicable to a variety of resource-sensitive type systems and semantics, generalizing prior work such as~\cite{DBLP:conf/popl/GhicaS11}. 

Finally, we give a game-semantic model for the (concrete) type system in order to justify it computationally. The game-semantic model is denotational therefore compositional by construction, and the categorical semantics ensures that it provides a reasonable interpretation. We do not provide a conventional operational semantics because the game semantics provides enough operational content to be directly usable in the definition of a compiler as proved practically by our previous work on hardware synthesis (\emph{loc.\ cit.}) and more formally in forthcoming work on constructing abstract machines from game semantics~\cite{fredrikssong13}. Moreover, the game semantics   provides an immediate model for foreign function interfaces, which is essential in the development of a useful compiler~\cite{DBLP:conf/memocode/Ghica11}.

\subsection{Related work}

The problem of calculating timing bounds for program execution has been studied extensively. In functional languages it is especially relevant for reactive~\cite{wan2001real} and syn\-chronous \cite{pilaud1987lustre} programming. A variety of methods have been proposed, from static analysis~\cite{liu1998automatic} to full dependent types~\cite{Crary:2000:RBC:325694.325716}. The defining feature of our work is the fact that it is type-based and offers fully automated inference, so requires no annotations or additional effort from the programmer. The application to pipelining is also suitable in terms of our restriction to recursion-free programming, as pipelining is most commonly used as an optimization for finite unfolding of recursive (or iterative) terms. 

\section{Bounded affine types, a general framework}\label{sub:agf}
 Types are generated by the grammar , where  is a fixed collection of base types and , where  is a semiring. We will always take  to bind strongest so we will omit the brackets. 

Let  be a list of identifiers  and types , annotated with semiring elements .  Let  be the set of free variables of term , defined in the usual way. The typing rules are:
\begin{center}
  \AxiomC{ }
  \RightLabel{Identity}
  \UnaryInfC{}
  \DisplayProof\1.5ex]
  \AxiomC{}
  \RightLabel{Abstraction}
  \UnaryInfC{}
  \DisplayProof\1.5ex]
  \AxiomC{}
  \RightLabel{Contraction}
  \UnaryInfC{}
  \DisplayProof
\end{center}
In \emph{Weakening} we have the side condition , and in  \emph{Application} we require .
In the \emph{Application} rule we use the notation


For the sake of simplicity we take operations in the semiring to be resolved \emph{syntactically} within the type system. So types such as  and  are taken to be syntactically equal. In the context of type-checking this is reasonable because ring actions are always constants that the type-checker can calculate with. If we were to allow resource variables, i.e.\ some form of resource-based polymorphism (cf.~\cite{DBLP:conf/tlca/LagoH09}) then a new structural rule would be required to handle type congruences induced by the semiring theory:
\begin{center}
  \AxiomC{}
  \AxiomC{}
  \RightLabel{Semiring}
  \BinaryInfC{}
  \DisplayProof
\end{center}
But in our current system this level of formalization is not worth the complication. 

\paragraph{Observation.} This is an affine type system where types are
decorated with resources taken from an arbitrary semiring. The new
rules are resource-oriented versions of contraction and application.
The similarity with BLL~\cite{girard1992bounded} and
SCC~\cite{DBLP:journals/tcs/GhicaMO06} is clear. If we instantiate  to resource
polynomials (and also remove weakening) we obtain BLL. If we instantiate  to the
semiring of natural numbers we get SCC. If  we obtain a conventional multiplicative affine type system. 
In Sec.~\ref{chap:pipes} we will see a much more complex resource semiring to control timing of execution. 

\subsection{Type inference}\label{sec:gti}
We present a bound-inference algorithm for the abstract system which works by creating a system of constraints to be solved, separately, by an SMT-solver that can handle the equational theory of the resource semiring. 
In the type grammar, for the exponential type  we allow  to stand for a concrete element of  or for a variable in the input program; the bound-inference algorithm will  produce a set of constraints such that every model of those  constraints gives rise to a typing derivation of the program without
  resource variables as variables are instantiated to suitable concrete values. Type judgments have form

where  is a set of equational constraints in the semiring. We also allow an arbitrary set of constants , which will allow the definition of concrete programming languages based on the type system. We allow each constant  to introduce arbitrary resource constraints 
\begin{center}\small
  \AxiomC{ }
 \UnaryInfC{}
  \DisplayProof\1.5ex]
  \AxiomC{}
 \UnaryInfC{}
  \DisplayProof\1.5ex]
\AxiomC{}
  \UnaryInfC{}
  \DisplayProof
\
\overline{\sigma=\sigma}&\stackrel{def}=\emptyset\\
\overline{J_1\cdot\theta_1\multimap\theta_1'=J_2\cdot\theta_2\multimap\theta_2'}&\stackrel{def}=
\{J_1=J_2\}\cup \overline{\theta_1=\theta_2}\cup \overline{\theta_1'=\theta_2'}.

\Lambda_{A,B,C}:A\otimes B\rightarrow C\simeq A\rightarrow B\multimap C,

  &J\cattimes(K\catplus L) \simeq J\cattimes K \catplus J\cattimes L&\text{(r-distributivity)}\\
  &(J\catplus K)\cattimes L \simeq J\cattimes L \catplus K\cattimes L&\text{(l-distributivity)}\\
  &J\cattimes 0 \simeq 0\cattimes J \simeq 0&\text{(zero)}.

  \delta_{J,K,A}: J\cdot A \otimes K\cdot A&\simeq(J\catplus K)\cdot A\label{eq:dis2}\\
  \pi_{R,R',A}: R\cdot(R'\cdot A)&\simeq (R\odot R')\cdot A \label{eq:sro}\\
  \zeta_A:0\cdot A &\simeq I\label{eq:zeroi}\\
  \iota_A:\mathbf 1 \cdot A &\simeq A\label{eq:one}
\label{eq:coh}
\xymatrix@C=12ex{
J{\cdot} A \otimes K{\cdot} A \otimes L{\cdot} A \ar[d]^{1_{J{\cdot}A}\otimes \delta_{K,L,A}} \ar[r]^-{\delta_{J,K,A}\otimes 1_{L{\cdot} A}} & (J\catplus K){\cdot} A\otimes L{\cdot} A \ar[d]^{\delta_{J\catplus K,L,A}}\\
J{\cdot} A \otimes (K\catplus L){\cdot} A \ar[r]^-{\delta_{J,K\catplus L, A}} & (J \catplus K\catplus L){\cdot} A 
}
\label{eq:nat}
\xymatrix{
J{\cdot}A\otimes K{\cdot} A \ar[d]^{J{\cdot}f\otimes K{\cdot}f} \ar[r]^{\delta_{J,K,A}}& (J\catplus K){\cdot} A\ar[d]^{(J\catplus K)\cdot f} \\
J{\cdot}B\otimes K{\cdot} B\ar[r]^{\delta_{J,K,B}} & (J\catplus K){\cdot} B 
}

\xymatrix{
B\ar[r]^-{1_B\otimes !_A}\otimes A \ar[d]^{f\otimes 1_A} & B\otimes I \ar[r]^{\rho_B}& B\ar[d]^f\\
C\otimes A \ar[r]^-{1_C\otimes !_A} & C\otimes I \ar[r]^{\rho_C} & C.
}

\sbr{J\cdot\theta\multimap\theta'}_{\mathcal G}= (\sbr J_{\mathcal R}\cdot\sbr\theta_{\mathcal G})\multimap\sbr{\theta'}_{\mathcal G}.

\sbr\Gamma=\sbr{x_1:J_1\cdot\theta_1,\ldots x_n:J_n\cdot \theta_n}
  = J_1\cdot\sbr{\theta_1}\otimes\cdots\otimes J_n\cdot\sbr{\theta_n}.

  & \sbr{x:\mathbf 1\cdot\theta\vdash x:\theta} = \iota_{\sbr\theta}  \\
  & \sbr{\Gamma,x:J\cdot\theta \vdash M:\theta'} = 1_{\sbr\Gamma}\otimes !_{J\cdot\sbr\theta};\rho_{\sbr\Gamma};\sbr{\Gamma\vdash M:\theta}   \\
  & \sbr{\Gamma\vdash\lambda x.M:J\cdot\theta\multimap\theta'}
  = \Lambda_{J\cdot\sbr\theta}\bigl( \sbr{\Gamma, x:J\cdot\theta\vdash M:\theta'} \bigr)\\
  & \sbr{\Gamma,J\cdot\Gamma'\vdash FM:\theta'}=(\sbr{\Gamma\vdash F:J\cdot\theta\multimap\theta'}\otimes
  J\cdot\sbr{\Gamma'\vdash M:\theta});\mathit{eval}_{J\cdot\sbr\theta,\sbr{\theta'}}\\
  & \sbr{\Gamma,x:(J+ K)\cdot\theta\vdash M[x/y]:\theta'}=  1_{\sbr\Gamma}\otimes\delta_{J,K, \theta};\sbr{\Gamma,x:J\cdot\theta,y: K\cdot\theta\vdash M:\theta}.
1.5ex]
\AxiomC{}
\RightLabel{Abs-weak}
\UnaryInfC{}
\DisplayProof
\end{center}
where, in both rules, .
We also introduce obviously admissible rules for contracting multiple (0, one or more) variables (labeled \emph{Contraction+}) and for weakening multiple (0, one or more) variables (labeled \emph{Weakening+}).

We denote sequents  by  and derivation
trees by . Let 

be a label on the sequents, indicating whether a sequent is the
product of the rule for identity, weakening, etc. If a sequent
 is the root of a derivation tree
 we write it  or . 

We say that a sequent is \emph{linear} if each variable in the environment  occurs freely in the term  exactly once. 
\begin{definition}
We say that a  derivation tree  is \emph{stratified} if and only if:
\begin{itemize}
\item the root is labeled ;
\item the node above the root is labeled ;
\item no other node is labeled by  or ;
\item all sequents in , except possibly for the root and the sequent above the root, are linear. 
\end{itemize}
\end{definition}
\begin{lemma}\label{lem:struni}
If a linear sequent has a stratified derivation tree then it is unique (up to renaming of variables). 
\end{lemma}
\begin{proof}
The last two rules (\emph{wk+} and \emph{co+}) bring the sequent to a linear form. 
In constructing the stratified derivation tree  of a linear sequent  the choice of what rules to apply is always uniquely determined by the structure of the term . 
\begin{description}
\item{:} The only possible rule is \emph{Application} and, since the term  is linear both  and  are linear and there is only one way  can be split. 
\item{:} We consider two cases:
\begin{itemize}
\item If  we infer the rule \emph{Abs-weak}. 
\item If  we use \emph{Abs-con} to give each occurrence of  in  a new (fresh) name.  
\end{itemize}
There are no other rules that would keep the derivation tree stratified. 
\item{:} The only possible rule is . 
\end{description}
All the choices above are unique (up to the choice of variable names in \emph{Abs-con}).
\end{proof}
We now show that any  derivation can be reduced to a stratified derivation through applying a series of meaning-preserving tree transformations, which we call \emph{stratifying rules}.

The Weakening rule commutes trivially with all other rules except Identity and Abstraction, if abstraction is on the weakened variable. In this case we replace the sequence of Weakening followed by Abstraction with the combined \emph{Abs-weak} rule.
The more interesting tree transformation rules are for Contraction. 

Contraction commutes with Application. There are two pairs of such rules, one for pushing down contraction in the function and one for pushing down contraction in the argument:
\begin{center}\small
\AxiomC{}
\UnaryInfC{}
\AxiomC{}
\BinaryInfC{}
\DisplayProof
\1.5ex]
\AxiomC{}
\AxiomC{}
\BinaryInfC{}
\UnaryInfC{}
\DisplayProof
\end{center}
Similarly for pushing down contraction from the argument side and similarly for rules involving weakening:
\begin{center}\small
\AxiomC{}
\AxiomC{}
\UnaryInfC{}
\BinaryInfC{}
\DisplayProof
\1.5ex]
\AxiomC{}
\AxiomC{}
\BinaryInfC{}
\UnaryInfC{}
\DisplayProof
\end{center}
Contraction also commutes with Abstraction, if the contracted and abstracted variables are distinct, : 
\begin{center}
\AxiomC{}
\UnaryInfC{}
\UnaryInfC{}
\DisplayProof
\1.5ex]
\AxiomC{}
\UnaryInfC{}
\UnaryInfC{}
\DisplayProof
\end{center}
The rule for swapping contraction and weakening is (types are obvious and we elide them for concision):
\begin{center}
\mbox{
  \AxiomC{}
  \UnaryInfC{}
  \UnaryInfC{}
  \DisplayProof}\mbox{
\AxiomC{}
  \UnaryInfC{}
  \UnaryInfC{}
  \DisplayProof}
\end{center}
\begin{proposition}\label{prop:syneq}The following judgments are syntactically equal

\end{proposition}
\begin{proof}
The proof of the first two statements is similar.
Because Application is linear it means that an identifier  occurs either in  or in , but not in both. Therefore  is either   or . This makes the terms syntactically equal. In any semiring, , which makes the environments equal.  Note that semiring equations are resolved syntactically in the type system, as pointed out at the beginning of this section.  For the third statement we know that .
\end{proof}
\begin{proposition}\label{prop:str}
If  is a  derivation and  is a tree obtained by applying a stratifying rule then  is a valid  derivation with the same root  and the same leaves. 
\end{proposition}
\begin{proof}
By inspecting the rules and using Prop.~\ref{prop:syneq}. 
\end{proof}
Stratifying transformations preserve meaning. The following more general proposition shows that in general the weakening rule can be pushed by any other rule without changing meaning.
\begin{lemma} \label{lem:streq}
If  is a stratifying rule then .
\end{lemma}
\begin{proof}
By inspecting the rules. Prop.~\ref{prop:str} states that the root
sequents are equal and the trees are well-formed. For WC (and the other rules involving the
stratification of Weakening) this is an immediate consequence of
Prop.~\ref{prop:wk}. For AL and AR the equality of the two sides is an
immediate consequence of symmetry in  and the
functoriality of the tensor . For CA the equality of the two sides is an instance of
the general property in a symmetric monoidal closed category that  for 
any , . 
\end{proof}
\begin{lemma}\label{lem:twk}
If  are   derivation trees consisting only of \emph{Contraction} and \emph{Weakening} with
a common root  then
. 
\end{lemma}
\begin{proof}
Weakening commutes with any other rule (Prop.~\ref{prop:wk}).
Changing the order of multiple contraction of the same variable
  uses the associativity coherence property in Eqn.~\ref{eq:coh}.
Changing the order in which different variables are contracted
  uses the naturality coherence property in Eqn.~\ref{eq:nat}.
\end{proof}
\begin{lemma}\label{lem:streq2}
If  is a  derivation there exists a stratified derivation
tree  which can be obtained from  by applying a
(finite) sequence of stratifying tree transformations. Moreover, .
\end{lemma}
\begin{proof}
The stratifying transformations push contraction and weakening through
any other rules and the derivation trees have finite height. If a contraction or weakening
cannot be pushed through a rule it means that the rule is an abstraction on the 
variable being contracted or weakened, and we replace the two rules with 
either \emph{Abs-con} or \emph{Abs-weak}. 

For the weakening and contractions pushed to the bottom of the tree the order
is irrelevant, according to Lem.~\ref{lem:twk}, therefore we replace them with
a \emph{Contraction+} and \emph{Weakening+} which perform all the required weakening
and contraction in one step each.
The result is a stratified  tree.

Then we
apply induction on the chain of stratifying rules using
Lem.~\ref{lem:streq} for every rule application and 
Lem.~\ref{lem:twk} for the final chain of weakening
and contractions.
\end{proof}
\begin{theorem}[Coherence]
For any   derivation trees  with common root ,  .
\end{theorem}
\begin{proof}
  Using Lem.~\ref{lem:streq},  must be effectively stratifiable into
   trees  with the same root. Using Lem.~\ref{lem:streq2},
   for
  . We first reduce  to a linear form using \emph{Contraction+} and \emph{Weakening+} then use Lem.~\ref{lem:struni},
  . 
\end{proof}


\section{Case study: automated pipelining}\label{chap:pipes}
Let us instantiate the abstract type system to a non-trivial resource-sensitive type system: automatic pipelining of computations.  This is interesting for two reasons. First we get to work with a complex resource semiring of \emph{execution schedules}. Second, for the type inference we show how the intrinsic constraints system generated over the resource semiring can be seamlessly combined with additional extrinsic constraints, in our case imposing a pipelining (first-in-first-out) discipline on the schedules. 

The concrete type system is an instance of the generic type system when  is taken to be the semigroup semiring (i.e.\ multisets) of one-dimensional contractive affine\footnote{The word ``affine'' has two distinct technical meanings, both standard: logical vs. algebraic. The overloading should be unambiguous in context.} transformations

We will use the notation  to represent some  as a multiset; we call  its \emph{stages} and  a \emph{schedule}.

Contractive affine transformations enable composition of timed functions in a natural way. Our view of timing is \emph{relative}: in a type  (brackets added for emphasis) we take the execution of the function to always be, by convention, the unit interval. This is a call-by-name language so each argument is re-evaluated when needed (to prevent needless re-evaluation we can use the store explicitly). The size of the multiset indicates that the function uses its argument  times. Contractive affine transformation , when applied to the unit interval, yields a sub-interval indicating the timing of execution of the -th use of the argument. Compositionality is given automatically by the fact that the product of contractive affine transformations is a a contractive affine transformation. Composing time represented as explicit intervals can be done but is more complicated. 

A \emph{contractive} affine transformation is represented
, where   and 
The factor  is a \emph{scaling factor}, representing the \emph{relative duration}
of a computation, and  is a \emph{phase}, representing a \emph{relative delay} for the same  computation. A one-dimensional affine transformation acting on the unit interval, in affine representation, can be used to represent the duration of the computation of one run of a term starting at  and ending at :


\begin{proposition}
If  then . 
\end{proposition}

When we refer to the timing of a computation, and it is unambiguous from context, we will sometimes use just  to refer to its action on the unit interval . For example, if we write  we mean , i.e.\   , i.e.  and .  If we write  we mean the Egli-Milner order on the two intervals, , i.e.\    and .  If we write  we mean the two intervals are disjoint, , etc.

Contractive affine transformations form a semigroup with matrix product as multiplication and unit element
.
The semiring of a semigroup  is  a natural construction from any semiring and any semigroup. In our case the semiring is natural numbers (), so the semigroup semiring is the set of finitely supported functions  with 

This is isomorphic to finite multisets over . We use interchangeably whichever representation is more convenient. 

\subsection{A concrete programming language}

A concrete programming language is obtained by adding a family of functional constants in the style of Idealized Algol~\cite{reynolds1997essence}. Let us call it PIA (Pipelined-IA). We take commands and integer expressions as the base types, 


Ground-type operators are provided with explicit timing information. For example, for commands we have a family of timed composition operators (i.e.\ schedulers):

The fact that  are contractive is a \emph{causality} constraint which says that each argument must execute within the interval in which the main body of the function is running which is, by convention, the unit interval. Sequential composition is a scheduler in which the arguments are non-overlapping, with the first argument completing before the second argument starts:
 where  and  (which we write ). Parallel composition is simply

with both arguments initiating and completing execution at the same time. Schedulers that are neither purely sequential nor parallel, but a combination thereof, are also possible. 

Arithmetic operators are also given explicit timings, but branching needs to be sequential.


Assignable variables are handled by separating read and write access, as is common for IA. Let the type of \emph{acceptors} be defined (syntactically) as , where  is a system-dependent constant (writing to memory cannot usually be instantaneous). There is no stand-alone  type in PIA, instead the readers and writers to a variable are bound to the same memory location by a block variable constructor with signature:

For programmer convenience -typed identifiers can be sugared into the language but, because the read and write schedules of access need to be maintained separately, the contraction rules become complicated (yet routine) so we omit them here. 

Finally, ground-type constants are 
 and 

In order to keep execution deterministic and timing predictable, no constants with data-dependent timing of execution can be allowed, such as recursion, iteration or semaphores. These restrictions are not onerous. Unbounded recursive (or iterated) executions cannot be in general pipelined, only finite unfoldings; we could handle this but it is a conceptually uninteresting complication. Semaphores are asynchronous computational features that also involve non-deterministic waiting for conditions to happen and cannot be pipelined. The language presented here must be understood as a sub-language of a larger ambient programming language, defining those computations that can be pipelined. 

\begin{example} \label{ex:incx}
The program , written functionally (while separating the reader and the acceptor) as  is typable. We give one possible way to annotate the constants with timing so that the term types:

which, written in a fully sugared notation, would be:

Note that addition here is given the schedule of a parallel operation with some arbitrary schedule .
\end{example}

\subsection{Type inference for automated pipelining}\label{sec:typin}\label{sec:pip}

Note that the recipe from Sec.~\ref{sec:gti} cannot be immediately applied because there is no (off-the-shelf) SMT solver for . We need to run the SMT in two stages: first we calculate the sizes of the multiset (as in SCC inference), which allows us to reduce constraints in  to constraints in . Then we map equations over  into real-number equations, which can be handled by the SMT solver. There is a final, bureaucratic, step of reconstructing the multi-sets from the real-number values. To fully automate the process we also use Hindley-Milner type inference to determine the underlying simple-type structure~\cite{milner1978theory}. 

Multiset size (SCC) type inference is presented in detail elsewhere~\cite{DBLP:conf/popl/GhicaS11}, but we will quickly review it here in the context of PIA. We first interpret schedules as natural numbers, representing their number of stages . Unknown schedules are variables, schedules with unknown stages but fixed size (such as those for operators) are constants. A type derivation results in a constraint system over  which can be solved by an SMT tool such as Z3~\cite{Z3}. More precisely, Z3 can attempt to solve the system, but it can be either unsatisfiable in some cases or unsolvable as nonlinear systems of constraints over  are generally undecidable. 

As a practical observation, solving this constraint using general-purpose tools will give an arbitrary solution, if it exists, whereas a ``small'' solution is preferable. In~\cite{DBLP:conf/popl/GhicaS11} we give a special-purpose algorithm guaranteed to produce solutions that are in a certain sense minimal. To achieve a small solution when using Z3 we set a global maximum bound which we increment on iterated calls to Z3 until the system is satisfied. 

The next stage is to instantiate the schedules to their known sizes, and to re-run the inference algorithm, this time in order to compute the stages. This stage proceeds according to the general type-inference recipe, resulting in a system of constraints over the  semiring, with the particular feature that all the sizes of all the multisets is known. We only need to specify the schedules for the constants:
\begin{center}
  \AxiomC{ }
 \UnaryInfC{}
  \DisplayProof\1.5ex]
  \AxiomC{ }
 \UnaryInfC{}
  \DisplayProof\1.5ex]
  \AxiomC{}
 \UnaryInfC{}
  \DisplayProof
\end{center}
 In the concrete system it is useful to characterize the resource usage of families of constants also by using constraints, which can be simply combined with the constraints (in the theory of the semiring) produced by the generic type inference algorithm. The language of constraints itself can be extended arbitrarily, provided that eventually we can represent it into the language of our external SMT solver, Z3. The constraints introduced by the language constants are motivated as follows:
\begin{description}
\item[op:] We prevent the execution of any of the two arguments to take the full interval, because an arithmetic operation cannot be computed instantaneously.
\item[if:] The execution of the guard must precede that of the branches.
\item[new:] The write-actions cannot be instantaneous.  
\end{description}

This allows us to translate the constraints into real-number constraints. Solving the system (using Z3) gives precise timing bounds for all types. However, this does not guarantee the fact that computations can be pipelined, it just establishes timings. 
In order to force a pipeline-compatible timing discipline we need to add extra constraints guaranteeing the fact that each timing annotation  is in fact a proper pipeline. Two stages  are \emph{FIFO} if they are Egli-Milner-ordered, . They are \emph{strictly FIFO}, written  if they are FIFO and they do not start or end at the same time, i.e. if  then  and . 

\newcommand{\pipe}{\mathsf{Pipe}}
\begin{definition}\label{def:pipe}
We say that a schedule  is a \emph{pipeline}, written , if and only if  (i.e.\  is a proper set) and for all ,  either  or  or . 
\end{definition}

Given a system of constraints  over , before solving it we augment it with the condition that every schedule is a proper pipeline: for any  used in , . Using the conventional representation (scaling and phase), the usual matrix operations and the pipelining definitions above we can represent  as a system of constraints over , and solve it using Z3. 

\paragraph{Implementation note.} For the implementation, we enforce arbitrary orders on the stages of the pipeline and, if that particular order is not satisfiable then a different (arbitrary) order is chosen and the process is repeated. However, spelling out the constraint for the existence of a pipelining order  for any schedule  would entail a disjunction over all possible such orders, which is  in the size of the schedule, for each schedule, therefore not realistic. However, if the systems of constraints have few constants and mostly unknowns, i.e.\ we are trying to find a schedule rather than accommodate complex known schedules, our experience shows that this pragmatic approach is reasonable. 

\paragraph{}
Ex.~\ref{ex:incx} is from a scheduling point of view quite trivial because no pipelining takes
place. We consider two more complex examples below. 

\begin{example}\label{ex:fx4}
Let us first consider the simple problem of using three parallel adders to compute the sum  when we know the timings of . Suppose , i.e.\ it is a two-stage pipeline where the execution of the argument takes half the time of the overall execution and have relative delays of 0.1 and 0.2 respectively. We have the choice of using three adders with distinct schedules  () so that the expression respects the pipelined schedule of execution of . The way the operators are associated is relevant: . Also note that part of the specification of the problem entails that the adders are trivial (single-stage) pipelines. Following the algorithm above, the typing constraints are resolved to the following:

In the implementation, the system of constraints has 142 variables and 357 assertions, and is solved by Z3 in circa 0.1 seconds on a high-end desktop machine.
\end{example}

\begin{example}\label{ex:fx4}
Let us now consider a more complex, higher-order example. Suppose we want to calculate the convolution () of a pipelined function () with itself four times. And also suppose that we want to use just two instances of the convolution operator , so we need to perform contraction on it as well. The simple type of the convolution operator is
 For hardware compilation this corresponds to the following circuit diagram:
\begin{center}
\includegraphics{convolution}
\end{center}
By  we denote the circuit implementing the function ,  the two instances of the convolution operator,  the four-way contraction of  and  the contraction of the convolution operation itself. Every port in this diagram must observe a pipelining discipline. 

The implementation of  and  are unknown, so we want to compute the timings for the term

The constraint system has 114 variables and 548 assertions and is solved by Z3 in 0.6 seconds on a high-end desktop machine. The results are:

\end{example}

\section{Timed games: semantics of PIA}\label{sec:gamhort}
Rather than give a conventional operational semantics to our programming language we define it denotationally, using game semantics. This has the technical advantage that the model is compositional by construction. Moreover, game semantics packs pertinent operational intuitions and can be effectively presented, therefore (arguably) not much is lost by eschewing the conventional syntax-oriented operational semantics. In support of this statement we mention our prior work on hardware~\cite{DBLP:conf/mpc/Ghica12} and distributed~\cite{fredrikssong13} compilation directly from the game-semantic model of a programming language. 
Game-semantic models are also well suited to modeling resource usage
explicitly by annotating moves with tokens representing resource
usage~\cite{DBLP:conf/popl/Ghica05}. We will use an annotated game
model here as well, starting from the game model of
ICA~\cite{DBLP:journals/apal/GhicaM08}. 

Note that since we are giving a denotational semantics the usual syntactic sanity checks (reduction preserves typing) do not apply. Instead, we must show that our model fits the categorical requirements of Sec.~\ref{sec:cf}. These requirements subsume and strengthen the syntactic sanity checks by lifting them to higher order terms and formulating them compositionally. 

This section assumes that the reader is familiar with the basic concepts of game
semantics. Tutorial introductions to game semantics are available, e.g.~\cite{DBLP:conf/lics/Ghica09}. 
For readability, all techincal proofs are given in a Sec.~\ref{sec:tec}.

\begin{definition}[Pre-arena]\label{def:prota}
A \emph{pre-arena}  is a tuple :
\begin{enumerate}
\item \label{i:moves}
  is a set of \emph{moves};
\item \label{i:timing}
 is a  \emph{timing function};
\item \label{i:label}
 is a \emph{labelling function};
\item \label{i:initial}
 such that  for some ;
\item \label{i:enable}
 is an \emph{enabling relation}, such that for any  there is an  such that  if and only if , and for any ,  implies 
\begin{enumerate}
\item \label{i:oqenable}
,
\item \label{i:qenable}
, 
\item \label{i:placehold}
if  then .
\end{enumerate}
\item \label{i:simult}
 is an equivalence relation such that for any  
\begin{enumerate}
\item \label{i:simtim}
if  then  for ,
\item \label{i:simjus}
if  then ,
\item \label{i:simena}
if  then .
\end{enumerate}
\end{enumerate}
\end{definition}
Some of the game-semantic concepts are conventional (move, opponent-proponent, question-answer, enabling, initial move) but some are specific to timed games. We will use  to stand for question, answer, opponent or proponent move if ambiguities are not introduced. We also use  to signify the set of \emph{final answers}, the answers to the initial questions in .

The key new concept particular to timed systems is that of timing (Def.~\ref{def:prota}.\ref{i:timing}), assigning each move a time in the unit interval. The arenas of timed games introduce the notion of \emph{alternative} moves, moves that are simultaneous (in the arena) but only one of which can occur in an actual play. Alternative moves are related by . Answers to the same question are alternatives (Def.~\ref{def:prota}.\ref{i:simena}) as are a move and its dummy counterpart (Def.~\ref{def:prota}.\ref{i:simtim}). 

One of the -alternatives in a collection of moves is the \emph{dummy move}.
The notion of ``dummy move'' (or \emph{non-move}, the label ) in Def.~\ref{def:prota}.\ref{i:label} corresponds to the principle that in a timed system observations are driven by timing: at any given moment we can observe a system to check whether it is producing any output or requesting any input. If that is the case this is modeled by a conventional, actual, move. But if that is not the case, especially if at a given time a move was possible or expected, the fact that no move occurred is relevant, and modeled by a dummy (non)move. If a dummy move enables another move, then that move must also be a dummy (Def.~\ref{def:prota}.\ref{i:placehold}). 

\begin{definition}[Arena]\label{def:arena}
	A \emph{precedence relation for arena} ,  is the minimum transitive relation such that:
\begin{enumerate}
\item if  then ;
\item if  then ;\label{def:arenat}
\item if  then ;
\item if  then ;
\item \label{i:fj} if  and  then .
\end{enumerate}
	An \emph{arena} is a pre-arena that has a well-founded precedence relation. 
\end{definition}
Precedence is consistent with timing but it has a finer grain: even moves with the same timing may have a precedence relation, which indicates causality. As in synchronous digital systems, just because two signals have the same timing (are on the same cycle) does not mean they are truly simultaneous, as time itself is only an abstract approximation. Within the same timing we are just unable to further discern the \emph{value} of the time but we can still observe the \emph{order} of the events. This distinction is essential in preventing causal loops in composition. The last requirement (Def.~\ref{def:arena}.\ref{i:fj}) is the language-dependent requirement that all children of a thread terminate before the parent (the \emph{Fork} and \emph{Join} rules in game semantics of ICA).

For any arena , two time intervals will play an important role, the time interval when a play \emph{may} execute  and the time when a play \emph{must} execute , defined as

A play in an arena may (must) execute after the earliest (latest) initial move and before the latest (earliest) final move. 
\begin{definition}
If  then the action of  on  is 
,
where  if 
\end{definition}
\begin{definition}\label{def:schedact}
If  is a schedule then the action of the schedule on the arena  is 

\end{definition}
Every stage in the schedule is allowed to act on the arena . Moreover, if a schedule occurs several times in the schedule then as many copies are created from the arena as required. The notion of ``distinct copies of the same arena" can be formalised using either explicit tags or nominal techniques, but we avoid this formalisation here, whenever possible, as it is generally the case in presentations of game-semantic models, in order to keep the technicalities at bay. 

Let  be the co-pairing of two functions and  be a function like  except that the  value are swapped. Let  be the (disjoint) union of two arenas. 
\begin{definition}
If  and  are arenas such that  then we define the \emph{arrow} arena as  where  is defined as .
\end{definition}
The arrow arena has the conventional definition in game semantics, except for the \emph{causality} requirement that  which ensures that all possible computations of the argument happens within the time bounds of the calling arena. Note that this condition is quite restrictive because it does not take into account the enabling relation, just the absolute earliest and latest possible moves in the arenas.
\begin{lemma}
If  and  are arenas then ,  and  are arenas. 
\end{lemma}

\begin{definition}[Play]\label{def:play}
A play  on an arena  is a sequence of \emph{distinct} moves of  such that 
\begin{enumerate}
\item\label{it:jus} for any ,  is initial or there is a unique  such that ;
\item\label{it:qa}  for any , there exists a unique  such that ;
\item\label{it:prec}  for any ,  occurs before  in ;
\item\label{it:sym}  
for any   there is a unique  such that . 
\end{enumerate}
The set of all legal plays of arena  is . 
\end{definition}
Some of the rules are common in game semantics, such as the existence of unique enablers (Def.~\ref{def:play}.\ref{it:jus}) and unique answers (Def.~\ref{def:play}.\ref{it:qa}). Clearly, temporal precedence must be consistent with move sequencing in the play (Def.~\ref{def:play}.\ref{it:prec}). The last condition (Def.~\ref{def:play}.\ref{it:sym}) requires that exactly one of a set of alternative moves occurs in a play. 

Also note that plays must consist of distinct moves in the arenas: the enabling relation is a directed acyclic graph and a play is a path in this DAG. This is why timing is associated with arenas rather than with plays.  

A \emph{position} of an arena  is a prefix of some play  in . A move  is \emph{legal} in a position  if  is a position. A \emph{position} of a set of plays  is a prefix of some play  in that set. 
\begin{definition}[Strategy]\label{def:strat}
A \emph{strategy}  on arena , written , is a set of plays on  such that 
\begin{enumerate}
\item (responsive) for any position  in  and legal -move  for  in ,  is a position in ;
\item (saturated) for any play  if  is a -move or  is a -move (or both) and  is a play then it is also in .
\end{enumerate}
\end{definition}
These conditions correspond to O-completeness and saturations, as used in the ICA game model. 

A move that (transitively) enables two moves is said to be a \emph{common enabler}. A common enabler that enables no other common enablers is said to be their \emph{last common enabler}. 
\begin{definition}\label{def:precs}
A \emph{precedence relation for strategy}  is a transitive well-founded relation  such that :
\begin{enumerate}
\item for any , if  then ;
\item if  then ;
\item if  then ;
\item for any position  of  and , if  then ;
\item for any  such that  and the last common enabler of  and  is not an -move then .
\end{enumerate}
\end{definition}
A strategy  is \emph{deadlock-free} if it has a precedence relation . From now on we only consider deadlock-free strategies. Deadlock-free strategies are interesting in their own right, and represent an alternative to conventional notions of termination (such as may, must or may-must) when timing is known. On the one hand termination analysis is simplified since the type (the arena) contains all the timing information. But on the other hand composition becomes more delicate as there is no room for the two strategies to wait for each other to perform certain common actions. Their synchronization needs to be on the nose, and for it to work at all it is essential that we rule out ``causal loops'' which may take an arbitrary amount of time to sort themselves out.  

For this reason we use precedence also on strategies. It is an order consistent with arena precedence () and preserved by alternative moves (), which prevents deadlock from two players waiting for each other.  The interesting requirement is the last one, which gives a term control over when it evaluates its arguments, which always have -moves as their last common enabler, but \emph{not} over when arguments it applies to functions are evaluated, which always have -moves as last common enablers. 

Let  be a play from which all moves not in  have been removed. Let . Let  be the set of sequences over . The interaction and composition of  strategies  are:

Let the interleaving of two strategies  be the set





\newcommand{\copycat}{c\!c}
\begin{definition}[Copycat]\label{def:cc}
  We define the \emph{copycat} 
  , as the set of all
  plays  such that for all ,
   if and only if . Moreover, 
   occurs before  in  if and only if  is an -move in~.
\end{definition}

\begin{theorem}\label{thm:games}
There exists a symmetric monoidal closed category with arenas  as objects and strategies  as morphisms where
\begin{enumerate}
\item identity  is the copycat strategy on ;
\item the tensor product is the disjoint union of arenas and interleaving of strategies, respectively;
\item the unit object is the empty arena (no moves);
\item the natural isomorphisms (associator, unitors, commutator) are (the obvious) copycat strategies;
\item currying is relabeling of moves in arenas induced by the obvious isomorphism between  and ;
\item the morphism  consists of two copycat behaviours between the  and  components, respectively.
\end{enumerate}
\end{theorem}
\subsection{Interpretation of PIA}
Let  be the discrete category with objects elements in ,  the additive operator of the semigroup semiring (Eq.~\ref{eq:add}),  the multiplicative operator of the semigroup semiring (Eq.~\ref{eq:mul}), and the unit  the additive unit (Eq.~\ref{eq:unit}). The associativity, distributivity and zero laws  follow from the semiring properties. 

Let  be the category of games from Thm.~\ref{thm:games}. The functor  is given in Def.~\ref{def:schedact} and it satisfies the associativity law given in Eqn.~\ref{eq:sro}. Any arena  and schedules  induce obviously isomorphic arenas
 . The strategy  is the one induced by the arena isomorphism and it satisfies all required coherence conditions. 

The arena of expressions (base type) is given by , where

In the concrete arena for expressions the initial question  (or its alternative dummy ) happen at 0 and the answer  (or the alternative dummy ) happen at 1. Note that we demand that an actual question receives an actual answer, not a dummy. 

The scheduling of commands  is interpreted by the strategy consisting of the unique play in arena  in which P does not play dummy moves unless responding to dummy O moves.

Operators  are interpreted by a strategy which is a set of plays, all with the same schedule as determined by their arena , in which the final P-answer is calculated as the corresponding arithmetical operation applied to the preceding O-answers. 

Branching , with , is a strategy in arena

defined as follows. The schedule constraint  ensures that O answers first in the  component, the guard. If it is not zero then a question is asked in the first  component and a dummy question in the second; the proper O-answer is then replicated as the final P-answer, while the dummy O-answer is ignored. Alternatively, if it is 0 then the question is asked in the second component and the dummy question in the first component, with the answer copied as final P-answer and the dummy answer ignored. 

The local variable binder  is interpreted in arena  in the same way as the local-variable strategy is interpreted in IA, in a history-sensitive way: whenever P answers in the  arena it is either with the same answer as the last answer in the  arena or with the last O-answer in the  arena, whichever is most recent.  

\subsection{Technical proofs}\label{sec:tec}

In this section, we show some of the main intermediate results and proofs demonstrating that the concrete category of games  is well defined and satisfies the required properties. 

This ancillary lemma is useful for proving further results about strategies:
\begin{lemma}\label{lem:stratprefix}
  For any (deadlock-free) strategies  on  and  on , and any positions  of ,  of  such that ,  then there is a sequence  such that  is a prefix of  and  is a prefix of .
\end{lemma}
\begin{proof}
  Let  be the minimum transitive relation on moves of , , and  such that  if  or . Assume for contradiction that there are two moves ,  such that  and ; both moves would obviously have to be moves of . Because , . Because , . If  and  do not have an -move as their last common enabler in , then because , we have , a contradiction. If they do have an -move as their last common enabler in , they must have the same move as their last common enabler in , where it is a -move, and thus because , , also a contradiction. Thus, the assumption is wrong; and so,  is well-founded.

  Assume for contradiction that for given arenas , , , that ,  are the counterexamples to the lemma with the largest total length. (They must be finitely long because the arenas contain finitely many questions, and all answers in a play must have a question enabling them.) Define  as the set of all moves legal in the respective arenas in  or in , or (transitively) justified by such a move. Choose a -least move  (such a move must be legal in  or , because otherwise, its enabler would be -less than it, and there must be such a move or else  and  are plays whose common moves appear in the same order and thus finding a suitable  is trivial). Without loss of generality, assume that either  is a move of , or an -move of  (the proof in the other cases is the same with  and , and  and , exchanged). Let  be a play of  with  as a prefix in which  or an alternative to it appears as early as possible. (Without loss of generality, assume that it is  that appears.) If  is not a prefix of , then there must be a move  immediately before  in ; then  (by the definition of m), so  (by the definition of ), so  with  and  exchanged is a play of  (by the definition of ), contradicting the assumption that  is chosen such that  appears as early as possible. Thus,  is a prefix of . If  is a move of , then we have  and  as a counterexample to the lemma, violating the assumption that  and  formed the counterexample with the largest total length. If  is a move of , then similarly we have  and  as a counterexample to the lemma ( is a prefix of a play in  because  is an -move of  and  is responsive), again violating the same assumption.

  Therefore, there cannot be a longest counterexample, and thus there cannot be any counterexample, to the lemma.
\end{proof}

To prove  a category, we need to show that it is closed under composition, that composition is associative, and that it has identities.

\begin{theorem}[Closure under composition]
  The composition  of two (deadlock-free) strategies  on ,  on  is a (deadlock-free) strategy on .
\end{theorem}
\begin{proof}
  We show that  is a set of plays on , and that it is responsive, saturated, and deadlock-free.

   is by definition a set of sequences of moves of . For each sequence:
  \begin{itemize}
    \item All moves must be distinct, because two identical moves from  would imply there were two identical moves in , and likewise for  and . Similar arguments shows that there is one move from each set of alternatives, and that each question enables exactly one answer.
    \item All moves must be either initial, or enabled by an earlier move in the sequence: 
      \begin{itemize}
      \item By construction of the arenas, moves of  cannot enable moves of  or C in the original arenas , , nor can moves of  enable moves of , nor moves of  enable moves of .
      \item Moves of  enabled by other moves of , and moves of  enabled by other moves of , in the initial arenas, will have both the move and enabler included in the same order in .
      \item Initial moves of  are initial moves of both  and  and so cannot be enabled in either arena.
      \item Initial moves of  (the only remaining case) are enabled by each initial move of , and so are enabled in  by whichever initial move of  happens to be included in the relevant play of .
      \end{itemize}
    \item The order of moves in  must be consistent with ; the timings must be in non-decreasing order, because otherwise either the order would be inconsistent with  or  respectively in a play of  or , or else  is the null arena (and thus no play of  contains any moves); and no moves can answer a move that is enabled by a move answered earlier in a sequence, using a similar argument to the above.
  \end{itemize}
  Therefore,  is a set of plays on .

  To see that  is responsive, consider a position  of , and an -move  legal in . Without loss of generality, assume that  is a move of  (the proof for  a move of  is similar). Let  be an element of  such that  is a prefix of ,  the shortest prefix of  where ,  be ,  be . Then because  is responsive,  is a position of ;  is a position of  by definition; and so by Lem.~\ref{lem:stratprefix},  is the prefix of some play in .

  For deadlock-freedom, we need to prove the existence of a . We claim that  defined in the proof of Lem.~\ref{lem:stratprefix} meets all the requirements to be such a relation. The first three requirements of Def.~\ref{def:precs} are obvious, and the last requirement is trivially met because the set of last common enablers of an -move and -move are the initial questions of  (which contains only -moves, and is nonempty except in the degenerate case where  has no nonempty plays), so we need only prove that if  and  is a position of , then  is also a position of . Let  be an element of  such that  is a prefix of ; let  be the shortest prefix of  such that ; and let  be the longest prefix of  such that . Then let , , and likewise for  and . Let , and .  is a position of , and because  is deadlock-free and  implies , it must be possible to repeatedly exchange the positions of a move of  in  and an immediately following move of  in  and still have a position of ; likewise for . The rearrangement is the same in both strategies, and so the rearranged  and  have their common moves in the same order.  by definition,  because  by assumption. And therefore, via Lem.~\ref{lem:stratprefix},  is a prefix of a play of .

To prove that  is saturated, we need to prove that if  is an -move and/or  is a
 -move, and , then if  is a play . The proof is
 along similar lines to the previous proof. Define  such that  for every
 -move  and -move  in , , and . Let  be the element of
  such that ; let  be the longest
 prefix and  the longest suffix of , such that 
 and , and define  such that . 
Then let , ,
 , and likewise for , , and . Let , and .
  is a position of , and because  is saturated and  implies  is a -move and/or  is an -move, it must be possible to repeatedly
 exchange the positions of a move of  in  and an immediately following move of  in  and still
 have a play of ; likewise for . As such, applying the same rearrangement to  leads to 
an interaction which forms a play of  if restricted to moves of , and a play of  if restricted
 to moves of . And thus, applying the same rearrangement to  gives a play of ,
 .

  Therefore,  is a strategy on .
\end{proof}

\begin{theorem}[Associativity]
   = .
\end{theorem}
\begin{proof}
  Define the three-way interaction  of
strategies , , , as .
,
because for each element of , there is by definition an
element of  corresponding to it. For the same reason,
.
Thus, .
\end{proof}


\begin{theorem}[Identity]
  The copycat strategy  for any arena  is in fact a strategy, and a left and right identity under strategy composition.
\end{theorem}
\begin{proof}
  By definition,  is a set of plays on .

  Because  is defined as containing all plays except those where specific -moves appear after specific -moves,  is trivially both saturated and responsive (the requirement is preserved by moving -moves earlier or -moves later, and cannot prevent -moves appearing unless they have already appeared in the play).

  To show deadlock-freedom, a suitable  is the least transitive relation where  for , and where  with  and  an -move. This relation is obviously well-founded, obviously respects precedence on the arena and alternatives, and obviously lists all pairs of moves that cannot be reversed. It also obeys the last common enabler rule, because the last common enablers of  and  are the initial moves of , which are -moves (except in the degenerate case where  has no initial moves, whose identity contains no moves in its plays and thus is trivially deadlock-free).

  To prove , consider the interaction . (This contains moves from two distinct copies of ; we label them  and  for clarity, with  on  and  on .) By the definition of , in the interaction , for each move of  there is a move of  and vice versa; and the -moves come first. Thus, for each play of , there is a play of  that contains the same moves (but not necessarily in the same order). However, the only changes to the ordering of the moves that are made are to move -moves later and/or -moves earlier. Thus, . Additionally, by replacing each -move  with  and each -move  with  in any play of , the resulting sequence is clearly an element of , and the play derived from it is clearly identical to the original play. Thus, . And so, . A similar argument can be used to prove that .
\end{proof}

\begin{theorem}
  Taking  on strategies to be interleaving of strategies, .
\end{theorem}
\begin{proof}
  We can decompose this condition into three simpler conditions, , , and . Each of these conditions becomes obvious upon replacing  and  with their definitions.
\end{proof}
\begin{theorem}[Unit object]
  With  as the empty arena,  for all arenas .
\end{theorem}
\begin{proof}
   has no moves, so its disjoint union with any arena is isomorphic to that arena.
\end{proof}
This proves that  is a monoidal category. Proving it to be also symmetric and closed requires proving several coherence constraints, but each of these constraints are requirements that relabelings are natural isomorphisms (which is obviously true), or that relabelings of the identity commute (which is also obviously true).

\allowdisplaybreaks 
The fact that  is a proper functor is immediate. We  prove that Eqn.~\ref{eq:sro} holds for  and :
\begin{theorem}
   for  and  a strategy.
\end{theorem}
\begin{proof}
  
\end{proof}

\section{Conclusion}

We have presented a bounded affine type system using an abstract resource semiring and gave a generic type inference and coherent categorical semantics for it. To illustrate its flexibility we used it to give a precise timing discipline to a recursion-free functional programming language with local state defined using a game-semantic model. The first, more theoretical, part of the paper is motivated by our desire to generalize our previous work on resource-sensitive type systems (such as SCC) and the results should be broadly applicable to many such systems. The second part is a highly non-trivial motivating application of the theory where schedules of execution are treated as a resource, and is driven by our interest in enhancing the \emph{Geometry of Synthesis} hardware compiler with transparent, automatic pipelining. It is hopefully obvious that the use of a generic type system and categorical semantics imposes a high degree of abstract discipline which is essential in managing a complex type system and its interpretation in a correspondingly complex semantics.

For future work, carefully injecting some data-dependencies into the type system would be highly desirable. Full-blown data dependency, especially in the presence of recursion, would make automatic type inference unfeasible. This goes beyond a mere decidability result. In type inference we outsource the heavy lifting to an external SMT solver and, so long as it can \emph{attempt} to solve the associated system of constraints with a decent chance of success (as determined by practical experiments) we are content. But when failure of inference due to computability issues is a matter of course (see e.g.~\cite{DBLP:conf/popl/LagoP13}) then it means that the type system is overly ambitious. Fortunately there is room for an interesting middle ground. To stay in the concrete context of precise timing, access to resources can be data dependent in (logically) simple ways even in the absence of recursion. An example is that of caching behavior: requesting an item of data the second consecutive time can be accomplished much faster than the first time around. 

The game semantics of Sec.~\ref{sec:gamhort} introduced a number of innovations which deserve to be studied in more depth.We did not attempt to prove (or even formulate) \emph{definability} in timed games, which is an interesting question. Also, although our game model is formulated for the concrete programming language directly, it is quite clear that much of its formulation is independent of the particular choice of resource semiring. The only place where the choice of the resource semiring (schedules) is important is in the Arena definition, Def.~\ref{def:arena}(\ref{def:arenat}), in which move ordering needs to be consistent with timing. A relaxation of this rule may lead to a generic  game model of the abstract type system. 

Finally, an efficient implementation of the pipelining mechanism in the hardware compiler requires the exploration of several possible ways in which detailed knowledge of timing can be exploited. The current implementation of the hardware compiler\footnote{See \url{http://veritygos.org}} is not compatible with pipelining because the circuit implementing contraction () can only be used sequentially. The new scheduled contraction operator  on the other hand can be used concurrently and can be given a finite-state implementation. The sizes of schedules () is known and finite and so is the order in which signals arrive, therefore their order can be used to determine signal routing. 

On the other hand, the timing information at our disposal is now much richer than simply knowing the order of events in the pipelines. We have full knowledge of the timing of each event; our timing is relative, but computing absolute timings from the relative timing information is quite easy. This means that our locally-synchronous-globally-asynchronous handshake protocol between components can be replaced by a globally-synchronous communication paradigm. Control signals indicating when data is available are now redundant, since this information is available at compile-time. Removing the handshake infrastructure is an interesting and appealing idea, but it is difficult to predict if it will lead to any performance improvements, since a new global clocking infrastructure needs to replace it. We will examine these questions in the near future. 


\paragraph{Acknowledgment.}
Sec.~\ref{sec:cf} benefited significantly from discussions with Steve Vickers. Olle Fredriksson and Fredrik Nordvall-Forsberg provided useful comments. The authors express gratitude for their contribution. 

\bibliographystyle{apalike}
\bibliography{manual}

\end{document}