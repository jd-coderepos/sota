\documentclass{ecai}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}




\begin{document}
\begin{frontmatter}

\title{Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method}

\author[A]{\fnms{}~\snm{Jiahao Qin}\thanks{qjh2020@liverpool.ac.uk}}
\author[A]{\fnms{}~\snm{Yitao Xu}}
\author[A]{\fnms{}~\snm{Zihong Luo}}
\author[A]{\fnms{}~\snm{Chengzhi Liu}}
\author[A]{\fnms{}~\snm{Zong Lu}}
\author[A]{\fnms{}~\snm{Xiaojun Zhang}}





\address[A]{Xi’an Jiaotong-Liverpool University}\orcid{https://orcid.org/0000-0002-0551-4647}


\begin{abstract}
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
\end{abstract}

\end{frontmatter}

\section{Introduction}
Multimodal data, such as images, audio, and text, is ubiquitous in the real world. The data for each model has different eigenvectors, However, each modality has its own set of eigenvectors, which are located in different subspaces \cite{lahat_multimodal_2015}. Consequently, they have different distributions and statistical properties, resulting in vectors with similar semantics being represented differently in different subspaces. Therefore, vectors with similar semantics represented in different subspaces will be completely different. This phenomenon is commonly known as the heterogeneity gap.

The heterogeneity gap is a significant issue as it can hinder the integrated use of multimodal data by subsequent machine learning modules \cite{lahat_multimodal_2015}. The traditional approach to addressing this issue is to transform data from different modalities into the same vector space, for example, through feature fusion or dimensionality reduction methods. 
 However, the disadvantage of this method is that the modal fusion is difficult, it is not easy to realize the cross-modal learning model, and the model cannot be extended to the single-modal scene \cite{cho_unifying_2021}.



 Cross-modal multimedia retrieval is a popular way to solve this problem. For example, Hardoon, Szedmak and Taylor \cite{hardoon_canonical_2004} map different modal data encodings to a common subspace and achieve good results on the corresponding datasets. Other researchers also achieve good results by similar methods \cite{peng_modality-specific_2018-1} \cite{chen_cross-modal_2022}. The main idea of this method is to map the features of different modes into a common low-dimensional vector space through a mapping function. In this vector space, the eigenvectors of each mode can be represented as similar vectors, which makes it convenient to perform tasks such as cross-modal retrieval and similarity calculation.

 This method has been widely used in multimodal data processing and has achieved certain results. For example, in text-image retrieval, this method can be used to map text descriptions and image features into a common subspace to achieve cross-modal retrieval \cite{cho_unifying_2021}. And in audio-image retrieval, this method can also be used to map audio and image features into a common subspace to achieve cross-modal retrieval \cite{yang2022multimodal} \cite{yuan2022exploring} \cite{zhang2007cross}.


 However, there are some problems with this approach. For example, due to the heterogeneous gap, the mapping relationship between different modes is difficult to learn, which leads to the poor effect of the mapping function \cite{guo_deep_2019}. In addition, due to the high dimensionality of the feature space, the computational complexity of the learning mapping function is also high \cite{houle_can_2010}. Therefore, how to effectively learn the mapping function and reduce the computational complexity is a problem that this method needs to solve.


We design a new alignment method: Alternative Telescopic Displacement(ATD), to overcome the above problems. We scale, rotate, and displace different feature spaces when fusing information features from different modalities. Displacement mapping is uncomplicated, and it can reduce the complexity of the model and prevent the gradient from disappearing.  Therefore, ATD could not only fully integrate the feature information between modalities and within modalities but also could reduce overfitting and prevent gradient disappearance. 

In addition, if a larger feature space is directly used to represent the features of multiple modalities, higher feature dimensions and more complex calculations will be inevitable. Before displacement mapping, we alternately select the features of one of the modalities for scaling and rotation transformation, which can reduce the dimensionality of the final alignment space. Also, we alternate between operations of stretching and rotating to add essential information about other modality features. In this way, we greatly avoid information loss.

\section{Related works}


Multimodal learning approach can be applied to many fields. For example, in computer vision, Velmurugan Sivakumar and Sampson \cite{subbiah_parvathy_optimal_2020} use multimodal medical image fusion for classification problems. They fuse multimodal medical images by shear wave transform and optimization models to obtain fused images. Their approach achieves superior performance with a fusion factor of 6.52 and a spatial frequency of 26.8. Melotti, Premebida and Goncalves \cite{melotti_multimodal_2020} proposed a multimodal approach to target recognition. They integrate the RGB images captured by the camera with the LIDAR projections to handle the target recognition problem. Their method has a more accurate and robust performance compared to the unimodal approaches. Jain and Wigington \cite{jain_multimodal_2019} proposed a multimodal fusion approach for document image classification. They combine text and image features by extracting the text content through OCR technique. Such an approach can obtain 92.3\% accuracy on the RVL-CDIP dataset. The multimodal approach is also applicable in the field of NLP. For instance, Jia et al. \cite{jia_scaling_2021} pre-trained visual language dual-modality models using a noisy dataset of over 1 billion image text pairs. They achieve SOTA in a wide range of tasks. Zhang et al. \cite{zhang_contrastive_2022} train image classifiers with image and text data pairs to obtain better image representations than baseline models in most cases. Liu, Wang and Li \cite{liu_towards_2022} integrate features from three modalities, text, audio and image, to obtain information about inter-modal consistency, which can make prediction more accurate.




 In addition to the areas developed from unimodal such as computer vision and natural language processing, Multimodal models also perform well in areas such as speech recognition and video classification, which are inherently multimodal in character.
 Paraskevopoulos et al. \cite{paraskevopoulos_multiresolution_2020} proposed a transformer-based combined audiovisual multimodal speech recognition model. The model uses cross-modal scaling dot product to achieve audio and visual fusion, which better adapts to the distribution characteristics of different modalities. Their experiments show that the model achieves about 50\% improvement in multi-resolution convergence speed and about 18\% improvement in word error rate. Eric and Hueber \cite{tatulli_feature_2017} propose a purely visual speech recognition method based on convolutional neural networks. The method uses CNN to extract features of ultrasound and video images and combines a Hidden Markov-Gaussian Mixture Model (HMM-GMM) decoder to achieve the fusion of modal information. This method achieves excellent performance in speech recognition accuracy, with an accuracy rate close to 84\%. Mroueh Marcheret and Goel \cite{mroueh_deep_2015} propose a multimodal speech recognition method based on a bilinear bimodal deep neural network (DNN). The method makes full use of the correlation between audio and visual modalities to further improve speech recognition accuracy. The Phone Error Rata (PER) was reduced to 34.03\%. The great results in several fields above have proven the superiority of multi-modal learning in deep learning research.

Multimodality can overcome a number of deficiencies in unimodal approaches, so multimodal approaches have superior performance over unimodal performance. A unimodal approach can only use information from one modality and cannot access information from other modalities. It may result in poor downstream task performance due to insufficient information and a lack of modal interaction \cite{lappe_cortical_2008} \cite{liang_expanding_2022} \cite{chen_cross-modal_2022}. However, multimodal methods can overcome the shortcomings of unimodal methods by integrating information from different modalities. So multimodal methods could obtain more accurate and robust results than unimodal methods. According to the survey by Baltrušaitis Ahuja and Morency \cite{baltrusaitis_multimodal_2019}, multimodal learning methods have been widely used in various fields such as vision, speech, and text. For example, Meng et al.  \cite{meng_depression_2013} introduced a multimodal learning method based on deep neural networks. The method can integrate information from multiple modalities, such as vision and speech, thus improving the performance of the model. In addition, based on a survey by Mohammad Soleymani et al. \cite{soleymani_survey_2017}, they conduct a series of studies on multimodal sentiment analysis tasks in social media. Their investigations find that more information and features can be obtained by analyzing and integrating information from multiple modalities, including text, images, and audio. These studies suggest that multimodal approaches can address a range of unimodality informational shortcomings and therefore perform better.
 



\section{Multimodal model with Alternating Telescopic Displacement modules}

\subsection{Overview}
Our bi-mode model comprises two modules: two Encoder modules that process different information modes, and an alignment module that merges the features of both modalities. The Encoder module is based on ResNet \cite{he_deep_2016} and LSTM \cite{hochreiter_long_1997} network structures, while the alignment module is based on ATD. The established framework is illustrated in Figure 1 and described in detail in this paper.

\begin{figure}[htp]
    \centering
    \includegraphics[width=9cm]{fullnetwork.pdf}
    \caption{The structure of our model. }
    \label{fig:network}
\end{figure}

During each training process, we first feed two types of data: image and numerical data. The model then extracts numeric features and image features from the corresponding input data through two different feature extraction modules. Next, the ATD module calculates a telescopic guiding value, which is used to scale the features of one of the modalities. This allows the two sequences with the same dimension to be asymmetrically combined and then mapped to a common spatial region using displacement mapping to form a consistent representation. Finally, the output of the ATD module is processed through a fully connected layer to obtain the final output result. The algorithm based on our model is presented below:
\begin{verbatim}

Algorithm 1:
Input:{Image data, Numeric data}
Output :{Final-output}

I1 = Linear(Image data)
I2, V = Linear(Numeric data)

Def ATD_Block(I1,I2,V):
    Guide = einsum(I1,I2)
    Out = einsum(Softmax(Guide),V)
    Out = Conv[Conv(Out) + I1] + I2
    return Out

Img-feature = Resnet-Encoder(image);
Data-feature = LSTM-Encoder(data);
Multimodel-out = ATD_Block(Img-feature,
                            Data-feature,
                            Data-feature);
                                    
Final-output = Linear(Multimodel-out)
    
\end{verbatim} 



\subsection{The Structure of the framework}
In this section, we will introduce the detailed architectural design of our proposed bimodal regression network framework for numeric and image data, which includes three main modules: the image feature extraction module, the LSTM-based digital feature extraction module, and the bimodal fusion module. It should be noted that in the following equations, x1 represents numeric data and x2 represents image features.

\subsection{Image feature extraction module: CNN}
A Convolutional Neural Network (CNN) \cite{lecun1998gradient} is a type of neural network commonly used in image processing tasks. It introduces local connectivity and weight sharing between layers through convolution and pooling operations, which can significantly reduce the number of parameters in the model and improve its convergence and generalization ability. In CNN, each convolutional layer processes the input data using convolution operations. The basic idea of convolution operation is to slide a convolution kernel over each pixel of an image, multiply each element in the kernel with the corresponding pixel value in the input image, and then sum up the products to obtain the output pixel value of the convolution operation.

In this paper, we use $H_1$ and $W_1$ to represent the height and width of the input image, respectively, while $H_2$ and $W_2$ denote the corresponding dimensions of the output image. $F$, $P$, and $S$ represent the size, padding, and stride of the convolutional kernel used in the CNN, respectively. Specifically, during each convolution operation, the input image of size $H_1 \times W_1$ is convolved with a kernel of size $F \times F$, resulting in an output image of size $H_2 \times W_2$, which can be adjusted using padding $P$ and stride $S$.






 The main structure of CNN includes convolutional layers, pooling layers and fully connected layers, and the structure of a typical CNN classification network is shown in the following figure:
 
\begin{figure}
\centering
\includegraphics[width=6cm]{cnn.pdf}
\caption{Convolution kernel extraction feature process--The convolution kernel slides over different partitions of the image and performs a convolution calculation to extract feature information}
\end{figure}










CNN can extract low or high-level features, and the level of extracted features increases with the depth of the network. The depth of the network has proven to be an important factor in achieving good performance. In our work, we intend to use a high-depth network to extract high-level features of images to obtain better classification results. However, gradient disappearance or gradient explosion becomes an obstacle for training deeper networks, which will lead to network failure to converge. The ResNet network structure can solve this problem to some extent by allowing the input information to be transferred directly to the output and added to the output layer so that the whole network only needs to learn the difference between the input and output, simplifying the learning difficulty, facilitating the convergence of the network, and protecting the integrity of the information. Presented below is the residual fork module of our image feature extraction network:

\begin{figure}
\centerline{\includegraphics[height=2in]{resnet.pdf}}
\caption{ResNet-- The residual block consists of two layers of $1\times 1$  and one layer of $3\times 1$ convolution. }
\end{figure}

\begin{eqnarray}
Output  & = & F(x)+x
\end{eqnarray}
For m-th block of the ResNet, it can be expressed mathematically as:
\begin{eqnarray}
x_{m+1} & = & x_m+F(x_m)\\
x_{m+2} & = & x_{m+1}+F(x_{m+1})\\
x_M & = & x_m+\sum\limits_{i=k}^{k-1}F(x_i)
\end{eqnarray}
The input image feature information $x$ of residual has two propagation paths, one path is directly transferred to the output, and three convolutional layers process the other path to obtain $F(x)$. We use $3\times 3$ convolution and two $1\times 1$  convolution kernels in residual fork structure to extract the image features while merging with the original image feature layer information output. The output $H(x)$ is expressed as $H(x) = F(x) + x$. When the output $x$ satisfies the need of a certain task in the forward propagation process, the $F(x)$ in the remaining residual fork layers automatically learns to become $0$, realizing the constant mapping: $x \longrightarrow H(x)$, which effectively solves the degradation problem of the network. Figure 5 shows the structure of the ResNet network used in this paper:
\begin{figure}[htp]
\centering
\includegraphics[width=6cm]{work.pdf}
\caption{ResNet network -- The network structure consists of a convolutional layer and six residual layers and extracts the obtained image features. The final output will go into the ATD module.}
\end{figure}



\subsection{Digital feature extraction module: LSTM}






Hochreiter and Schmidhuber proposed Long Short-Term Memory \cite{hochreiter_long_1997}, a network framework that can solve these problems by mitigating information loss within cells using three structures known as "gates" -- Forget Gate, Output Gate and Memory Gate -- composed of sigmoid neural network layers and element-wise operations to control how much information can pass through them. The strengths of an RNN mainly come from its unit-state, whereas LSTM can handle the loss of information within units through these gates to protect and maintain unit states while curbing gradient vanishing or exploding issues. Down below in this is a simple Figure 6. of the LSTM structure.

\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{lstm.pdf}
     \caption{The structure of LSTM --After expanding along the time axis, the hidden layer information of T=1 training will be passed to the next moment T=2. The three arrows to the right in the figure represent the transfer of the hidden layer state on the timeline.}
    \label{fig:network}
\end{figure}



We could see this in Figure 6. The forgetting gate (left part) is responsible for forgetting a part of the memory, then the current memory is retained in the memory gate (middle part), and finally, the output gate (right part) is used in filtered and updated the memory. The parameters within these gated units can be automatically learned during training through a backpropagation algorithm.

\begin{eqnarray}
f_t & = & \sigma(W_f\otimes[h_{t-1},x_t]+b_f) 
\end{eqnarray}
Forgetting Gate: The input value $x_t$ and output hidden state of the previous step $h_{t-1}$ are combined into a single vector, then passed through the sigmoid layer $\sigma$. Because the sigmoid function $\sigma$ compresses any input into the interval $(0,1)$, the different sigmoid $\sigma$ outputs determine how much of the information is remembered or forgotten. $\hat{C}_t$ represents the extent of $C_{t-1}$ need to be forgotten.
\begin{eqnarray}
\hat{C}_t & = &  tanh(W_C\otimes[h_{t-1},x_t]+b_C) 
\end{eqnarray}
Memory Gate: Use the $tanh$ function layer to extract the effective information of the input. $\hat{C}_t$ refers to how much information has been extracted from the current input.
\begin{eqnarray}
i_t & = & \sigma(W_f\otimes[h_{t-1},x_t]+b_f) 
\end{eqnarray}
Using the sigmoid $\sigma$ function to control how much of these memories are put into the cell state works as same as the Forgetting gate. $i_t$ represents the extent of  $\hat{C}_t$ need to be remembered. 
\begin{eqnarray}
C_t & = & f_t * C_{t-1}+i_t * \hat{C}_t 
\end{eqnarray}
The current cell state $C_t$ is determined by the forgetting gate$f_t * C_{t-1}$ and memory gate $i_t * \hat{C}_t$.
\begin{eqnarray}
o_t & = &\sigma(W_o\otimes[h_{t-1},x_t]+b_o) 
\end{eqnarray}
Output Gate: The input value $x_t$ and output hidden state of the previous step $h_{t-1}$ are combined into a single vector, then passed through the sigmoid layer $\sigma$.
\begin{eqnarray}
h_t & = & o_t * tanh(C_t) 
\end{eqnarray}
Compress and map the current cell state to the interval $(-1, 1)$ through the $tanh$ function.\\

The reasons we use $tanh$ function in Memory gate and Output gate are to prevent the vanishing gradient problem and easy to convex optimization, converge more quickly than others.







\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{lstmgate.pdf}
    \caption{The structure of LSTM gates --- there are three gates in one unit: the forgetting gate, the memory gate and the output gate.}
    \label{fig:network}
\end{figure}

\subsubsection{Alternative Telescopic Displacement}
We have extracted image and numeric features from the input data through the two modules mentioned above. In order to exploit the different features of the two modalities simultaneously, we design the Alternative Telescopic Displacement(ATD) module to fuse the features of the two modalities.

The main function of the ATD module is to alternately scale, rotate, and shift the eigenmatrices of the two modes. These could prevent us from getting a public representation space of a huge scale and getting stuck in more complex computations. Moreover, it can maintain a relatively simple mapping relationship with the original feature space. It works as follows:
\begin{figure}
    \centering
    \includegraphics[width=8cm]{atd.pdf}
    \caption{The structure of ATD module. }
    \label{fig:network}
\end{figure}

We perform Layernorm processing on the image $ID_1$ and the digital features $ID_2$ obtained from the two unimodal channels to obtain the normalized results. 

\begin{eqnarray}
{ID_1}_{norm} & = & \frac{{ID_1} - \mu_{ID_1}}{\sqrt{\sigma_{ID_1}^2 + \epsilon}}\\
{ID_2}_{norm} & = & \frac{{ID_2} - \mu_{ID_2}}{\sqrt{\sigma_{ID_2}^2 + \epsilon}}
\end{eqnarray}

Then we alternately select the feature matrix of one of the modalities and perform scaling and rotation transformations to reduce the dimension while keeping the feature representation unchanged.

Get the guide($ G $) of image identity $ ID_1 $ and series identity $ ID_2 $ by scaling the dot product. The value of $ G $ will be updated with the weight value update after backpropagation during training, meaning that the proportion of image and time-series data is different when each training is passed to the ATD module. The entire network will adjust the value of $ G $ with more training errors until the performance reaches the optimum.

\begin{eqnarray}
G & = & \frac{{ID_1}^T\cdot ID_2}{\sqrt{d}}
\end{eqnarray}

Computes the operation of taking the maximum value of Guide along the last dimension. This maximum value is then subtracted from each element of the Guide matrix.

\begin{eqnarray}
G_{b,h,i,j} & = & G_{b,h,i,j}-Max_{k=1}^n \left(G_{b,h,i,j}\right) 
\end{eqnarray}

$ b $ denotes the $ b $th sample in the batch, $h$ denotes the $ h $th head, and $ i $ and $ j $ denote the positions in the sequence, respectively. Finally, each element in $ G $ is compressed to the interval $[0, \infty)$, which ensures numerical stability after the softmax operation.


The Guide obtained through scaled dot-product is substituted into the Softmax function to obtain the ATD weights.
\begin{eqnarray}
 W & = & Softmax(\frac{G}{\sqrt{d_h}})
\end{eqnarray}

The output is the result of the operation of value and  ATD weights:
\begin{eqnarray}
 output & = & Softmax(\frac{G}{\sqrt{d_h}})\cdot V
\end{eqnarray}


Select the feature matrix of a single modality and apply scaling and rotation transformations to reduce the dimensionality while maintaining the feature representation unchanged.
\begin{eqnarray}
 ATD_{output} & = & F(output)+ID_1\\
 ATD_{output} & = & F(output)+ID_2
\end{eqnarray}


Finally, the normalized original features are displacement mapped with the interaction output of the same dimension.


















































\section{Experiments and Results}
We selected two different datasets for experiments, proving that the ATD module is not limited to a single specified task.

Besides, to prove that our experiments with multimodal data are adequate, we also conduct experiments with single modality data of the two datasets, respectively.

Finally, to verify that our proposed ATD module is a better alignment method, we use two other alignment methods to replace the ATD module in our model for experiments.

\subsection{Datasets}
 

ETT (electric Transformer Temperature) \cite{haoyietal-informer-2021}: This dataset records transformer temperature data for two years, and all data is divided into 1-hour classes. Each data point has a target temperature value and six power load characteristics.

The MIT-BIH Arrhythmia Database \cite{moody2001impact}: This dataset contains 48 excerpts of two-channel ambulatory ECG recordings taken from 47 subjects studied between 1975 and 1979 by the BIH Arrhythmia Laboratory. Twenty-three recordings were randomly selected from 4000 patients aged 24-30. Hour Holter recordings were collected from a mixed population of inpatients (approximately 60\%) and outpatients (approximately 40\%) at Beth Israel Hospital in Boston; the remainder 25 records were selected from the same group to include less common but clinically significant arrhythmias that would not be well represented in a small random sample.

We perform regression tasks on the ETT dataset and classification tasks on the MIT-BIH Arrhythmia dataset.

\subsection{Comparison with other methods}
Table 1 shows that the performance of our proposed method on the ETT dataset is better than the others.

\begin{table}
\begin{center}
{\caption{Comparison with previous state-of-the-art methods
on ETT test dataset.}\label{table1}}
\begin{tabular}{lcc}
\hline
\rule{0pt}{12pt}
&\multicolumn{2}{c}{Measurement metrics}\\
\cline{2-3}
\rule{0pt}{12pt}
Method &MAE &MSE
\\
\hline
\\[-6pt]
\quad NLinear\cite{zeng2022transformers} & 0.226 & 0.080\\
\quad FiLM\cite{zhou2022film} & 0.240 & 0.090\\

\quad SCINet\cite{liu2022scinet} & 0.127 & 0.029\\

\quad ATD (our)  & 0.058  & 0.006\\
\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

Experimental results on the ETT dataset show that our model has better performance. Compared with previous SOTA results, our results have the minimum MAE and MSE, meaning our model has more minor errors.



Table 2 shows that the performance of our proposed method on MIT-BIH-Arrhythmia dataset is state of the art.


\begin{table}
\begin{center}
{\caption{Comparison with previous state-of-the-art methods
on MIT-BIH Arrhythmia test dataset. }\label{table2}}
\begin{tabular}{lcc}
\hline
\rule{0pt}{12pt}
&\multicolumn{2}{c}{Measurement metrics}\\
\cline{2-3}
\rule{0pt}{12pt}
Method&Accuracy&F1
\\
\hline
\\[-6pt]
\quad 1D-CNN\cite{kiranyaz2015real}  &0.959 & 0.864   \\
\quad GANs\cite{shaker2020generalization}  &0.987 & 0.929  \\
\quad SR-based\cite{yamacc2022personalized}  &0.947   & 0.786  \\

\quad ATD (our)  &0.989 & 0.982  \\
\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

\section{Ablation studies}
\subsection{Single mode results}
We conduct experiments with unimodal data, and the results show that our ATD multimodal model has better performance. Table 3 shows that forecasting using only numerical data of ETT can not perform as well as ATD multimodal.
\begin{table}
\begin{center}
{\caption{Single mode result of ETT test dataset.}\label{table3}}
\begin{tabular}{lcc}
\hline
\rule{0pt}{12pt}
&\multicolumn{2}{c}{Measurement metrics}\\
\cline{2-3}
\rule{0pt}{12pt}
Method &MAE &MSE
\\
\hline
\\[-6pt]
\quad Numerical only & 0.187 & 0.0298\\

\quad ATD (multimodal)  & 0.058  & 0.006\\
\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

Table 4 shows the classification results using image and time-series signal data from the MIT-BIH Arrhythmia dataset. The results of ATD multimodal classification are better.

\begin{table}
\begin{center}
{\caption{Single mode result of MIT-BIH Arrhythmia test dataset. }\label{table4}}
\begin{tabular}{lcc}
\hline
\rule{0pt}{12pt}
&\multicolumn{2}{c}{Measurement metrics}\\
\cline{2-3}
\rule{0pt}{12pt}
Method &Accuracy&F1
\\
\hline
\\[-6pt]
\quad Image only  &0.518 & 0.479  \\
\quad Time-series only  &0.858 & 0.893  \\

\quad ATD (multimodal)  &0.989 & 0.982   \\
\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

\subsection{Results using other alignment methods}

LMF \cite{liu2018efficient} and Cross-attention \cite{lin2022cat} are popular multimodal alignment methods. We use these two modules to replace the ATD module in our model for experiments.

Table 5 and Table 6 present the results of experiments with other alignment methods. 
    
\begin{table}
\begin{center}
{\caption{Results of LFM method on two test datasets.}\label{table5}}
\begin{tabular}{lcccc}
\hline
\rule{0pt}{12pt}
&\multicolumn{4}{c}{Measurement metrics}\\
\cline{2-5}
\rule{0pt}{12pt}
Dataset &MAE &MSE &Accuracy&F1
\\
\hline
\\[-6pt]
\quad ETT & 0.337 & 0.116 & - & -\\
\quad MIT-BIH Arrhythmia & - & - & 0.912 & 0.905\\

\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
{\caption{Results of Cross-attention method on two test datasets.}\label{table6}}
\begin{tabular}{lcccc}
\hline
\rule{0pt}{12pt}
&\multicolumn{4}{c}{Measurement metrics}\\
\cline{2-5}
\rule{0pt}{12pt}
Dataset &MAE &MSE &Accuracy&F1
\\
\hline
\\[-6pt]
\quad ETT & 0.187 & 0.086 & - & -\\
\quad MIT-BIH Arrhythmia & - & - & 0.942 & 0.931\\

\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}

Compared with different alignment methods, ATD has apparent advantages in performance.

From the above table, compared with different alignment methods, ATD has apparent advantages in performance. 

ATD can fully fuse the information inside and outside the modal. Therefore, the experimental results of the ATD model on the ETT dataset and MIT-BIH Arrhythmia dataset have achieved better performance.



\section{Analysis and Discussion}
\subsection{Greater performance}
Compared with LMF, the performance of the model using ATD on the ETT dataset is improved by 82.8\% (MAE) and 48.3\% (MSE), and the performance of the model using ATD on the MIT-BIH-Arrythmia dataset is improved by 8.4\% (Acc) and 8.5\% (F1 score). Compared with ATD, the performance of the model using ATD on the ETT dataset is improved by 31.0\% (MAE) and 7.0\% (MSE), and the performance of the model using ATD on the MIT-BIH-Arrythmia dataset is improved by 5.0\% (Acc) and 5.5\% (F1 score).

Compared with the existing methods, ATD performs better, and ATD is more environmentally friendly. It has fewer parameters, is easier to train, and deploy on a personal PC or even some portable device.








\subsection{More environmentally friendly and easy to use}
The ATD module can significantly reduce the number of parameters of the overall model. Under the premise of using CNN and LSTM as encoders, The number of parameters of the model using ATD for modal alignment is only 70 million. The parameters of the model using LMF and cross-attention are 71 million and 160 million, respectively. Compared with LMF, ATD reduces the model parameters by 1 million, and the parameter reduction ratio is 1.4\%. Compared with cross-attention, ATD reduces the model parameters by 90 million, and the parameter reduction ratio is 56.3\%. With transformer and version-transformer as the encoder, The number of model parameters using ATD for modal alignment is 282 million, and the number of model parameters using LMF and cross-attention is 283 million and 370 million, respectively. Compared with LMF, ATD reduces the model by 1 million parameters with a parameter reduction ratio of 0.6\%, and compared with cross-attention, ATD reduces the model by 88 million parameters with a parameter reduction ratio of 24\%.
\begin{table}
\begin{center}
{\caption{The number of parameters for the two groups of encoders with three alignment methods.}\label{table3}}
\begin{tabular}{lccc}
\hline
\rule{0pt}{12pt}
&\multicolumn{3}{c}{Alignment methods}\\
\cline{2-4}
\rule{0pt}{12pt}
Encoders &ATD &LMF &Cross-attention
\\
\hline
\\[-6pt]
\quad Cnn and Lstm & 70 million & 71 million & 160 million\\

\quad Transformer and VIT  & 282 million  & 283 million & 370 million\\
\hline
\\[-6pt]

\end{tabular}
\end{center}
\end{table}


\section{Conclusions and future works}
This paper proposes a new method for fusing feature information from multiple modalities: Alternative Telescopic Displacement (ATD). ATD alternately selects the feature matrix of each mode and performs scaling, rotation and displacement transformations. It can align the feature information of multiple modalities in a common subspace. We use one multimodal model with ATD to achieve the best performance on two datasets.

In future works, we will work on improving the compatibility of the ATD module to ensure that it can efficiently work with any neural network architecture. In addition, we plan to apply the ATD module in more complex multimodal learning, such as multimodal unsupervised tasks.





\bibliography{ecai}
\end{document}
