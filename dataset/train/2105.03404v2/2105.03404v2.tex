
\section{Experiments}

In this section, we present experimental results for the \OURS architecture on image  classification and machine translation. 
We also study the impact of the different components of \OURS in ablation studies.
We consider three training paradigms for images:
\begin{itemize}[leftmargin=*]
\item
\emph{Supervised learning:} We train \OURS  from labeled images with a softmax classifier  and cross-entropy loss. This paradigm is the main focus of our work. 
\item \emph{Self-supervised learning:} We train the \OURS with the DINO method of Caron~\etal~\cite{caron2021emerging} that trains a network without labels by distilling knowledge from previous instances of the same network.
\item \emph{Knowledge distillation:} We employ the knowledge distillation procedure proposed by Touvron \etal~\cite{Touvron2020TrainingDI} to guide the supervised training of \OURS with a convnet. 
\end{itemize}


\subsection{Experimental setting}
\label{sec:experimental_setting}

\textbf{Datasets.} 
We train our models on the \ImNet-1k dataset~\cite{Russakovsky2015ImageNet12}, that contains 1.2M images evenly spread over 1,000 object categories. 
In the absence of an available test set for this benchmark, we follow the standard practice in the community by reporting performance on the validation set.
This is not ideal since the validation set was originally designed to select hyper-parameters.
Comparing methods on this set may not be conclusive enough because an improvement in performance may not be caused by better modeling, but by a better selection of hyper-parameters.
To mitigate this risk, we report additional results in transfer learning and on two alternative versions of \ImNet that have been built to have distinct validation and test sets, namely the \ImNet-real~\cite{Beyer2020ImageNetReal} and \ImNet-v2~\cite{Recht2019ImageNetv2} datasets. We also report a few data-points when training on \ImNet-21k. 
Our hyper-parameters are mostly adopted from Touvron et al.~\cite{Touvron2020TrainingDI,touvron2021going}. 




\textbf{Hyper-parameter settings.}
In the case of supervised learning, we train our network with the Lamb optimizer~\cite{you20lamb} with a learning rate of  and weight decay .
We initialize the LayerScale parameters as a function of the depth by following CaiT~\cite{touvron2021going}.
The rest of the hyper-parameters follow the default setting used in DeiT~\cite{Touvron2020TrainingDI}.
For the knowledge distillation paradigm, we use the same RegNety-16GF~\cite{Radosavovic2020RegNet} as in DeiT with the same training schedule.
The majority of our models take two days to train on eight V100-32GB GPUs.




\begin{table}[t]

    \caption{
\textbf{Comparison between architectures on \ImNet  classification.}
We compare different architectures  based on convolutional networks, Transformers and feedforward networks with comparable FLOPs and number of parameters. 
We report Top-1 accuracy on the validation set of \ImNet-1k with different measure of complexity: throughput, FLOPs, number of parameters and peak memory usage.
All the models use  images as input.
By default the Transformers and feedforward networks uses  patches of size , 
see Table~\ref{tab:ablation} for the detailed specification of our main models. 
The throughput is measured on a single V100-32GB GPU with batch size fixed to 32. 
For reference, we include the state of the art with \ImNet training only. 
    \label{tab:mainres}}
\centering
    \smallskip
    \scalebox{0.85}{
    \begin{tabular}{l|lcccccc}
        \toprule
        &  Arch.        & \#params & throughput & FLOPS & Peak Mem & Top-1\\
         &             & () & (im/s) & () & (MB) & Acc.\\
        \midrule
        \multirow{2}{*}{{\emph{State of the art}}} &
        CaiT-M48448~\cite{touvron2021going} & 356\pzo  & \dzo5.4 & \pzo329.6 & 5477.8 & 86.5  \\
        & NfNet-F6 SAM~\cite{Brock2021HighPerformanceLI}   & 438\pzo & \pzo16.0 & \pzo377.3& 5519.3& 86.5  \\
        \midrule
	\multirow{7}{*}{\emph{Convolutional networks}} 
	& EfficientNet-B3~\cite{tan2019efficientnet} & 12  & 661.8 & \tzo1.8  & 1174.0    & 81.1\\
	& EfficientNet-B4~\cite{tan2019efficientnet} & 19  & 349.4 & \tzo4.2 & 1898.9    & 82.6\\
	& EfficientNet-B5~\cite{tan2019efficientnet} & 30  & 169.1 & \tzo9.9 & 2734.9    & 83.3\\
	& RegNetY-4GF~\cite{radenovic2018fine}       & 21  & 861.0 & \tzo4.0 & \pzo568.4 & 80.0\\
	& RegNetY-8GF~\cite{radenovic2018fine}       & 39  & 534.4 & \tzo8.0 & \pzo841.6 & 81.7\\
	& RegNetY-16GF~\cite{radenovic2018fine}      & 84  & 334.7 & \dzo16.0 & 1329.6    & 82.9\\
        \midrule
       \multirow{3}{*}{\emph{Transformer networks}}   
         & DeiT-S~\cite{Touvron2020TrainingDI}   & 22  & 940.4 & \tzo4.6 & \pzo217.2 & 79.8\\
	 & DeiT-B~\cite{Touvron2020TrainingDI}       & 86  & 292.3 & \dzo17.5 & \pzo573.7 & 81.8\\
	 & CaiT-XS24~\cite{touvron2021going}         & 27  & 447.6 & \tzo5.4 & \pzo245.5 & 81.8\\
        \midrule
\multirow{3}{*}{\emph{Feedforward networks}}   
        & \OURS-S12 &  15  & 1415.1\pzo & \tzo3.0  & \pzo179.5 &  76.6 \\
        & \OURS-S24 &  30  & 715.4      & \tzo6.0  & \pzo235.3 &  79.4 \\
        & \OURS-B24 &  116\pzo &   231.3         & \dzo23.0 &  \pzo663.0 &   81.0 \\
        \bottomrule
    \end{tabular}}
\end{table}

\subsection{Main Results}

In this section, we compare \OURS with architectures based on convolutions or self-attentions with comparable size and throughput on \ImNet.

\textbf{Supervised setting.}
In Table~\ref{tab:mainres}, we compare \OURS with different convolutional and Transformer architectures. 
For completeness, we also report the best-published numbers obtained with a model trained on \ImNet alone. 
While the trade-off between accuracy, FLOPs, and throughput for \OURS is not as good as convolutional networks or Transformers, their strong accuracy still suggests that the structural constraints imposed by the layer design do not have a drastic influence on performance, especially when training with enough data and recent training schemes.



\begin{table}[t]
    \captionof{table}{\textbf{Self-supervised learning} with DINO~\cite{caron2021emerging}. Classification accuracy on \ImNet-1k val.
    ResMLPs evaluated with linear and -NN evaluation on ImageNet are comparable to convnets but inferior to ViT.  \smallskip } 

    \def \mysp {\hspace{6pt}}
     \centering \scalebox{0.85}
    {
    \begin{tabular}{lcccccc}
    \toprule
    Models                   &  ResNet-50 &  ViT-S/16 & ViT-S/8 &  ViT-B/16 & \OURS-S12 &   \OURS-S24 \\
    \midrule
    Params. () &   25        & 22       &  22     &  87      & 15       & 30\\
    FLOPS ()   &  4.1        &  4.6     & 22.4    & 17.5     & 3.0      & 6.0\\
    \midrule
    Linear                  &   75.3      & 77.0     &  79.7   & 78.2     &    67.5      & 72.8\\
    -NN                  & 67.5         & 74.5    & 78.3   & 76.1      &   62.6       &  69.4\\
    \bottomrule
    \end{tabular}}  
    \label{tab:ssl}
\end{table}



\textbf{Self-supervised setting. } 
We pre-train \OURS-S12 using the self-supervised method called DINO~\cite{caron2021emerging} during 300 epochs. 
We report our results in Table~\ref{tab:ssl}.
The trend is similar to the supervised setting: the accuracy obtained with \OURS is lower than ViT. Nevertheless, the performance is surprisingly high for a pure MLP architecture and competitive with Convnet in -NN evaluation. 
Additionally, we also fine-tune network pre-trained with self-supervision on \ImNet using the ground-truth labels.
Pre-training substantially improves performance compared to a \OURS-S24 solely trained with labels, achieving 79.9\% top-1 accuracy on \ImNet-val (+0.5\%). 


\textbf{Knowledge distillation setting.}
We study our model when training with the knowledge distillation approach of Touvron~\etal~\cite{Touvron2020TrainingDI}.
In their work, the authors show the impact of training a ViT model by distilling it from a RegNet. 
In this experiment, we explore if \OURS also benefits from this procedure and summarize our results in Table~\ref{tab:ablation} (Blocks ``Baseline models'' and  ``Training'').
We observe that similar to DeiT models, \OURS greatly benefits from distilling from a convnet. 
This result concurs with the observations made by d'Ascoli~\etal~\cite{d2019finding}, who used convnets to initialize feedforward networks.
Even though our setting differs from theirs in scale, the problem of overfitting for feedforward networks is still present on \ImNet. 
The additional regularization obtained from the distillation is a possible explanation for this improvement.




\subsection{Visualization \& analysis of the linear interaction between patches}

\textbf{Visualisations of the cross-patch sublayers.}
In Figure~\ref{fig:vizu}, we show in the form of squared images, the rows of the weight matrix from cross-patch sublayers at different depths of a \OURS-S24 model. 
The early layers show convolution-like patterns: the weights resemble shifted versions of each other and have local support. 
Interestingly, in many layers, the support also extends along both axes; see layer 7.
The last 7 layers of the network are different: they consist of a spike for the patch itself and a diffuse response across other patches with different magnitude; see layer 20.

\begin{figure}
\newcommand{\myvisu}[1]{{ \includegraphics[width=.237\linewidth]{figs/FilterVisu/layer_#1_w_fc1.pdf}}}
\small
\hspace{-1pt}
\begin{tabular}{@{}c@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }@{}}
       \myvisu{0} &  \myvisu{6}  &  \myvisu{9} & \myvisu{19} \\
       Layer 1 & Layer 7  &  Layer 10 & Layer 20 
\end{tabular}
\caption{
{\bf Visualisation of the linear layers in \OURS-S24.} 
For each layer we visualise the rows of the matrix  as a set of  pixel images, for sake of space we only show the rows corresponding to the 66 central patches. 
We observe patterns in the linear layers that share similarities with convolutions. 
In appendix~\ref{sec:vizu_unsup} we provide comparable visualizations for all layers of a ResMLP-S12 model. 
\label{fig:vizu}}
\end{figure}


\begin{figure}
\begin{minipage}{0.48\linewidth}
    \centering \includegraphics[width=0.99\linewidth]{figs/sparsity.pdf}
    \caption{\textbf{Sparsity of linear interaction layers.}
    For each layer (\textcolor{red!80}{linear} and \textcolor{blue!80}{MLP}), we show the rate of components whose absolute value is lower than 5\% of the maximum. 
    Linear interaction layers  are   sparser than the matrices involved in the per-patch  MLP. 
    \label{fig:sparsity}}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering \includegraphics[width=0.99\linewidth]{figs/imagenet_val-vs-v2.pdf}
    \caption{
    \textbf{Top-1 accuracy on \ImNet-V2 \vs  \ImNet-val.} 
    ResMLPs tend to overfit slightly more under identical training method. This is partially alleviated with by introducing more regularization (more data or distillation, see e.g., \OURS-B24/8-distil). \label{fig:overfitanalysis}}    
\end{minipage}
\end{figure}




\textbf{Measuring sparsity of the weights.} 
The visualizations described above suggest that the linear communication layers are sparse. 
We analyze this quantitatively in more detail in Figure~\ref{fig:sparsity}. 
We measure the sparsity of the matrix , and compare it to the sparsity of  and  from the per-patch MLP. 
Since there are no exact zeros, we measure the rate of components whose absolute value is lower than 5\% of the maximum value. 
Note, discarding the small values is analogous to the case where we normalize the matrix by its maximum and use a finite-precision representation of weights. 
For instance, with a 4-bits representation of weight, one would typically round to zero all weights whose absolute value is below 6.25\% of the maximum value. 

The measurements in Figure~\ref{fig:sparsity} show that all three matrices are sparse, with the layers implementing the patch communication being significantly more so. 
This suggests that they may be compatible with parameter pruning, or better, with modern quantization techniques that induce sparsity at training time, such as Quant-Noise~\cite{fan2020training} and DiffQ~\cite{defossez2021differentiable}. 
The sparsity structure, in particular in earlier layers, see Figure.~\ref{fig:vizu}, hints that we could implement the patch interaction linear layer with a convolution. 
We provide some results for convolutional variants in our ablation study. Further research on network compression is beyond the scope of this paper, yet we believe it worth investigating in the future. 

\textbf{Communication across patches } 
if we remove the linear interaction layer (linear  none), we obtain substantially lower accuracy (-20\% top-1 acc.) for a ``bag-of-patches'' approach. 
We have tried several alternatives for the cross-patch sublayer, which are presented in Table~\ref{tab:ablation} (block ``patch communication''). 
Amongst them, using the same MLP structure as for patch processing (linear  MLP), which we analyze in more details in the supplementary material. 
The simpler choice of a single linear square layer led to a better accuracy/performance trade-off -- considering that the MLP variant requires compute halfway between \OURS-S12 and \OURS-S24 -- and requires fewer parameters than a residual MLP block. 

The  visualization in Figure \ref{fig:vizu} indicates that many linear interaction layers look like convolutions. 
In our ablation, we  replaced the linear layer with different types of  convolutions. The depth-wise convolution does not implement interaction across channels  -- as  our linear patch communication layer --  and yields similar performance at a comparable number of parameters and FLOPs.
While  full  convolutions yield best results, they come with roughly double the number of parameters and FLOPs. 
Interestingly, the depth-separable convolutions combine accuracy close to that of full  convolutions with  a  number of parameters and FLOPs comparable to our linear layer. 
This suggests that  convolutions on low-resolution feature maps at all layers is an interesting alternative to the  common pyramidal design of convnets, where early layers operate at higher resolution and smaller feature dimension. 


\begin{table}[]
    \small \centering
    \scalebox{0.79}{
    \begin{tabular}{l|l@{\ \ }cc|c|l|cccc}
    \toprule
\multirow{2}{*}{Ablation}  &  \multirow{2}{*}{Model} & Patch & Params & FLOPs & \multirow{2}{*}{Variant} &  \multicolumn{3}{c}{top-1 acc. on \ImNet} \\
  &  & size &  &  & & val & real~\cite{Beyer2020ImageNetReal} & v2~\cite{Recht2019ImageNetv2} \\
  \midrule
  & \cellcolor{yellow!5} \OURS-S12 &  \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 15.4  & \cellcolor{yellow!5} 3.0  &  \cellcolor{yellow!5} 12 layers, working dimension 384  & \cellcolor{yellow!5} 76.6 &  \cellcolor{yellow!5} 83.3 & \cellcolor{yellow!5} 64.4 \\ 
Baseline models & \cellcolor{red!5}  \OURS-S24 & \cellcolor{red!5} 16 & \cellcolor{red!5} 30.0  & \cellcolor{red!5} 6.0  &  \cellcolor{red!5} 24 layers, working dimension 384    & \cellcolor{red!5} 79.4 & \cellcolor{red!5} 85.3 & \cellcolor{red!5} 67.9  \\ 
  &  \cellcolor{blue!3}\OURS-B24 & \cellcolor{blue!3} 16 & \cellcolor{blue!3} 115.7\pzo &\cellcolor{blue!3}  23.0\pzo &  \cellcolor{blue!3} 24 layers, working dimension 768    & \cellcolor{blue!3} 81.0 &  \cellcolor{blue!3} 86.1 & \cellcolor{blue!3} 69.0 \\
  \midrule
\multirow{1}{*}{Normalization}
  &  \cellcolor{yellow!5}\OURS-S12 & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 15.4 & \cellcolor{yellow!5} 3.0 &  \cellcolor{yellow!5} \texttt{Aff}  Layernorm & \cellcolor{yellow!5} 77.7 & \cellcolor{yellow!5} 84.1 & \cellcolor{yellow!5} 65.7  \\ 
  \midrule
\multirow{1}{*}{Pooling}
  &  \cellcolor{yellow!5}\OURS-S12 & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 17.7 & \cellcolor{yellow!5} 3.0 & \cellcolor{yellow!5} average pooling  Class-MLP & \cellcolor{yellow!5} 77.5 & \cellcolor{yellow!5} 84.0 & \cellcolor{yellow!5} 66.1 \\
  \midrule
\multirow{4}{1.9cm}{\begin{minipage}{1.4cm}Patch\\communication\ \end{minipage}}
  &  \cellcolor{yellow!5}\OURS-S12   & \cellcolor{yellow!5}16 & \cellcolor{yellow!5}14.9 & \cellcolor{yellow!5}2.8  & \cellcolor{yellow!5}linear  none &\cellcolor{yellow!5} 56.5  &  \cellcolor{yellow!5} 63.4 & \cellcolor{yellow!5} 43.1 \\ 
  &  \cellcolor{yellow!5}\OURS-S12   & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 18.6  & \cellcolor{yellow!5} 4.3  & \cellcolor{yellow!5} linear  MLP  & \cellcolor{yellow!5} 77.3  & \cellcolor{yellow!5}  84.0 & \cellcolor{yellow!5} 65.7 \\ 
  &  \cellcolor{yellow!5}\OURS-S12   & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 30.8 & \cellcolor{yellow!5} 6.0  & \cellcolor{yellow!5} linear   conv 3x3 & \cellcolor{yellow!5} 77.3 & \cellcolor{yellow!5} 84.4 & \cellcolor{yellow!5} 65.7\\ 
  &  \cellcolor{yellow!5}\OURS-S12   &\cellcolor{yellow!5}  16 & \cellcolor{yellow!5} 14.9 & \cellcolor{yellow!5} 2.8  & \cellcolor{yellow!5} linear  conv 3x3 depth-wise & \cellcolor{yellow!5} 76.3 & \cellcolor{yellow!5} 83.4 & \cellcolor{yellow!5} 64.6 \\ 
  &  \cellcolor{yellow!5}\OURS-S12   &  \cellcolor{yellow!5} 16 &  \cellcolor{yellow!5} 16.7 &  \cellcolor{yellow!5} 3.2  &  \cellcolor{yellow!5} linear  conv 3x3 depth-separable &  \cellcolor{yellow!5} 77.0 &  \cellcolor{yellow!5} 84.0 & \cellcolor{yellow!5}  65.5\\ 
  \midrule
\multirow{3}{*}{Patch size}  
  &   \cellcolor{yellow!5}\OURS-S12/14\,  &  \cellcolor{yellow!5} 14 &  \cellcolor{yellow!5} 15.6 &  \cellcolor{yellow!5} 4.0   &  \cellcolor{yellow!5} patch size 16161414 &  \cellcolor{yellow!5} 76.9 &  \cellcolor{yellow!5} 83.7 &  \cellcolor{yellow!5} 65.0  \\ 
  &   \cellcolor{yellow!5}\OURS-S12/8  &  \cellcolor{yellow!5} 8  &  \cellcolor{yellow!5} 22.1 &  \cellcolor{yellow!5} 14.0\pzo   & \cellcolor{yellow!5}  patch size 161688 &  \cellcolor{yellow!5} 79.1 &  \cellcolor{yellow!5} 85.2 &  \cellcolor{yellow!5} 67.2  \\ 
  &  \cellcolor{blue!3}\OURS-B24/8    & \cellcolor{blue!3} 8  & \cellcolor{blue!3} 129.1\pzo & \cellcolor{blue!3} 100.2\dzo & \cellcolor{blue!3} patch size 161688 & \cellcolor{blue!3} 81.0 &  \cellcolor{blue!3} 85.7 &\cellcolor{blue!3}  68.6 \\ 
  \midrule
\multirow{7}{*}{Training}  
  &  \cellcolor{yellow!5}\OURS-S12 & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 15.4 & \cellcolor{yellow!5} 3.0 & \cellcolor{yellow!5} old-fashioned (90 epochs)   &  \cellcolor{yellow!5} 69.2 & \cellcolor{yellow!5} 76.0 & \cellcolor{yellow!5} 56.1 \\ 
  &  \cellcolor{yellow!5}\OURS-S12 & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 15.4 & \cellcolor{yellow!5} 3.0 & \cellcolor{yellow!5} pre-trained SSL (DINO)    &\cellcolor{yellow!5} 76.5  & \cellcolor{yellow!5} 83.6 & \cellcolor{yellow!5} 64.5 \\ 
  &  \cellcolor{yellow!5}\OURS-S12 & \cellcolor{yellow!5} 16 & \cellcolor{yellow!5} 15.4 &\cellcolor{yellow!5} 3.0 & \cellcolor{yellow!5} distillation               &  \cellcolor{yellow!5} 77.8 & \cellcolor{yellow!5} 84.6 & \cellcolor{yellow!5} 66.0 \\ 
  &  \cellcolor{red!5} \OURS-S24 & \cellcolor{red!5} 16 & \cellcolor{red!5} 30.0 & \cellcolor{red!5} 6.0 & \cellcolor{red!5} pre-trained SSL (DINO)       &  \cellcolor{red!5} 79.9 & \cellcolor{red!5} 85.9 & \cellcolor{red!5} 68.6 \\ 
  &  \cellcolor{red!5} \OURS-S24 & \cellcolor{red!5} 16 & \cellcolor{red!5} 30.0 & \cellcolor{red!5} 6.0 & \cellcolor{red!5} distillation               &  \cellcolor{red!5} 80.8 & \cellcolor{red!5} 86.6 & \cellcolor{red!5} 69.8 \\ 
  &  \cellcolor{blue!3}\OURS-B24/8 & \cellcolor{blue!3} 8 & \cellcolor{blue!3} 129.1\pzo & \cellcolor{blue!3} 100.2\dzo & \cellcolor{blue!3} distillation     &  \cellcolor{blue!3} 83.6 & \cellcolor{blue!3} 88.4 & \cellcolor{blue!3} 73.4  \\ 
  &  \cellcolor{blue!3}\OURS-B24/8 & \cellcolor{blue!3} 8  &  \cellcolor{blue!3} 129.1\pzo &  \cellcolor{blue!3} 100.2\dzo &  \cellcolor{blue!3} pre-trained \ImNet-21k (60 epochs)  & \cellcolor{blue!3} {84.4} & \cellcolor{blue!3} {88.9} & \cellcolor{blue!3} {74.2}  \\ 
    \bottomrule
\end{tabular}}
    \smallskip
    \caption{\textbf{Ablation.} Our default configurations are presented in the three first rows. By default we train during 400 epochs. The ``old-fashioned'' is similar to what was employed for ResNet~\cite{He2016ResNet}: SGD, 90-epochs waterfall schedule, same augmentations up to variations due to  library used.\label{tab:ablation}}
\end{table}


\subsection{Ablation studies}
Table~\ref{tab:ablation} reports the ablation study of our base network and a summary of our preliminary exploratory studies. 
We discuss the ablation below and give more detail about early experiments in Appendix~\ref{sec:exploration}. 

\textbf{Control of overfitting.} 
Since MLPs are subject to overfitting, we show in Fig.~\ref{fig:overfitanalysis} a control experiment to probe for problems with generalization.
We explicitly analyze the differential of performance between the \ImNet-val and the distinct \ImNet-V2 test set. 
The relative offsets between curves reflect to which extent models are overfitted to \ImNet-val \wrt hyper-parameter selection. 
The degree of overfitting of our MLP-based model is overall neutral or slightly higher to that of other transformer-based architectures or convnets with same training procedure. 


\textbf{Normalization \& activation.}
Our network configuration does not contain any batch normalizations. 
Instead, we use the  affine per-channel transform \texttt{Aff}. 
This is akin to Layer Normalization~\cite{ba2016layer}, typically used in transformers, except that we avoid to collect any sort of statistics, since we do no need it it for convergence. 
In preliminary experiments with pre-norm and post-norm ~\cite{He2016IdentityMappings}, we observed that both choices converged.
Pre-normalization in conjunction with Batch Normalization could provide an accuracy gain in some cases, see Appendix~\ref{sec:exploration}.


We choose to use a GELU~\cite{Hendrycks2016GaussianEL} function. In Appendix~\ref{sec:exploration} we also analyze the activation function: 
ReLU~\cite{glorot11relu} also gives a good performance, but it was a bit more unstable in some settings. 
We did not manage to get good results with SiLU~\cite{Hendrycks2016GaussianEL} and HardSwish~\cite{Howard2019SearchingFM}. 

\textbf{Pooling.}  Replacing average pooling with Class-MLP, see Section~\ref{sec:method}, brings a significant gain for a negligible computational cost. We do not include it by default to keep our models more simple.  


\textbf{Patch size.} 
Smaller patches significantly increase the performance, but also increase the number of flops (see Block "Patch size" in Table~\ref{tab:ablation}).
Smaller patches benefit more to larger models, but only with an improved optimization scheme involving more regularization (distillation) or more data. 

\textbf{Training.} Consider the Block ``Training' in Table~\ref{tab:ablation}. ResMLP significantly benefits  from modern training procedures such as those used in DeiT. 
For instance, the DeiT training procedure improves the performance of ResMLP-S12 by  compared to the training employed for ResNet~\cite{He2016ResNet}\footnote{Interestingly, if trained with this ``old-fashion'' setting, ResMLP-S12 outperforms AlexNet~\cite{Krizhevsky2012AlexNet} by a margin. }. This is in line with recent work pointing out the importance of the training strategy over the model choice~\cite{Bello2021RevisitingRI,Radosavovic2020RegNet}.
Pre-training on more data and distillation also improve the performance of ResMLP, especially for the bigger models, \eg, distillation improves the accuracy of ResMLP-B24/8 by .



\textbf{Other analysis.} 
In our early exploration, we evaluated several alternative design choices. As in transformers, we could use positional embeddings mixed with the input patches. 
        In our experiments   we did not see any benefit from using these features, see Appendix~\ref{sec:exploration}.
        This observation suggests that our cross-patch sublayer provides sufficient spatial communication, and 
        referencing absolute positions obviates the need for any form of positional encoding.  


\subsection{Transfer learning} 
We evaluate the quality of features obtained from a \OURS architecture when transferring them to other domains. The goal is to assess if the features generated from a feedforward network are more prone to overfitting on the training data distribution.
We adopt the typical setting where we pre-train a model on \ImNet-1k and fine-tune it on the training set associated with a specific domain. We report the performance with different architectures on various image benchmarks in Table~\ref{tab:transfer}, namely CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning}, Flowers-102~\cite{Nilsback08}, Stanford Cars~\cite{Cars2013} and iNaturalist~\cite{Horn2019INaturalist}.  
We refer the reader to the corresponding references for a more detailed description of the  datasets.

We observe that the performance of our \OURS is competitive with the existing architectures, showing that pretraining feedforward models with enough data and regularization via data augmentation greatly reduces their tendency to overfit on the original distribution. Interestingly, this regularization also prevents them from overfitting on the training set of smaller dataset during the fine-tuning stage.
\def \mysp {\hspace{13pt}}
\begin{table}
    \hspace{-3pt}
    \scalebox{0.84}{
    \begin{tabular}{l@{\mysp}@{\mysp}c@{\mysp}@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c}
    \toprule
    Architecture              
     & {FLOPs}
           & {Res.}
        & {CIFAR}
        & {CIFAR}  
        & {Flowers102} 
        & {Cars} 
        & {iNat} 
        & {iNat} 
       \\
    \midrule
    EfficientNet-B7~\cite{tan2019efficientnet} & \pzo37.0B & 600 & 98.9 & 91.7  & 98.8 & 94.7 & \_ & \_  \\
    ViT-B/16~\cite{dosovitskiy2020image}  & \pzo55.5B & 384 & 98.1 & 87.1 & 89.5 & \_   & \_   & \_   \\
    ViT-L/16~\cite{dosovitskiy2020image}  & 190.7B    & 384 & 97.9 & 86.4 & 89.7 & \_   & \_   & \_   \\
    Deit-B/16~\cite{Touvron2020TrainingDI} & \pzo17.5B & 224 & 99.1 & 90.8 & 98.4 & 92.1 & 73.2 & 77.7  \\
    ResNet50~\cite{Touvron2020GrafitLF} & \dzo4.1B & 224 & \_ & \_ & 96.2 & 90.0 & 68.4 & 73.7  \\
    Grafit/ResNet50~\cite{Touvron2020GrafitLF} & \dzo4.1B & 224 & \_ & \_ & 97.6 & 92.7 & 68.5 & 74.6  \\
    \midrule
    ResMLP-S12      & \dzo3.0B & 224 & 98.1 & 87.0 & 97.4 & 84.6 & 60.2 & 71.0 \\
    ResMLP-S24       & \dzo6.0B &224 & 98.7 & 89.5 & 97.9 & 89.5 & 64.3  & 72.5  \\
    \bottomrule
    \end{tabular}}
    \smallskip
    \caption{
    \textbf{Evaluation on transfer learning.}
    Classification accuracy (top-1) of models trained on \ImNet-1k  for transfer to datasets covering different domains.
    The \OURS architecture takes  images  during training and transfer, while  ViTs and EfficientNet-B7 work with higher resolutions, see ``Res.'' column. 
    }
    \label{tab:transfer}
\end{table}



\subsection{Machine translation}



We also evaluate the ResMLP transpose-mechanism to replace the self-attention in the encoder and decoder of a neural machine translation system.
We train models on the WMT 2014 English-German and English-French tasks, following the setup from Ott \etal \cite{ott2018scaling}.
We consider models of dimension 512, with a hidden MLP size of 2,048, and with 6 or 12 layers. Note that the current state of the art employs much larger models: our 6-layer model is more comparable to the base transformer model from Vaswani \etal \cite{vaswani2017attention}, which serves as a baseline, along with pre-transformer architectures such as recurrent and convolutional neural networks. 
We use Adagrad with learning rate 0.2, 32k steps of linear warmup, label smoothing 0.1, dropout rate 0.15 for En-De and 0.1 for En-Fr.
We initialize the LayerScale parameter to 0.2. We generate translations with the beam search algorithm, with a beam of size 4.
As shown in Table~\ref{tab:wmt}, the results are at least on par with the compared architectures. 

\begin{table}[h]
    \captionof{table}{\textbf{Machine translation} on WMT 2014 translation tasks. We report tokenized BLEU on \emph{newstest2014}.    \label{tab:wmt}}
    \def \mysp {\hspace{6pt}}
     \centering \scalebox{0.9}
    {
    \begin{tabular}{lcccccc}
    \toprule
    Models &  GNMT~\cite{wu2016google} & ConvS2S~\cite{gehring2017convolutional} & Transf. (base)~\cite{vaswani2017attention} &  ResMLP-6 & ResMLP-12 &    \\
    \midrule
    \textsc{En-De} & 24.6 & 25.2 &  27.3  & 26.4 & 26.8  &  \\
    \textsc{En-Fr} & 39.9 & 40.5 &  38.1  & 40.3 & 40.6  &  \\
    \bottomrule
    \end{tabular}}
\end{table}