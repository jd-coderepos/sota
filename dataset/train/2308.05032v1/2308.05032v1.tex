

\section{Experiments}
\label{sec:experiments}

\noindent \textbf{Datasets and evaluation measures.} For the evaluation of methods, we employed two popular challenging benchmark datasets for Aerial Image Detection, namely the VisDrone \cite{zhu-VisDrone-2018} and DOTA \cite{xia-DOTA-2018} datasets. We used COCO style AP for assessing and comparing the performance of the methods \cite{mscoco-Lin-2014}. The AP of small, medium, and large objects are also reported, in particular, to understand the performance of methods for small-scale object detection in aerial images. 

\noindent \textbf{VisDrone.} This dataset contains  8,599 drone-captured images (6,471 for training, 548 for validation, and 1,580 for testing) with a resolution of about 2000 $\times$1500 pixels. The objects are from ten categories with 540k instances annotated in the training set, mostly containing different categories of vehicles and pedestrians observed when the drone is flying through the streets. It has an extreme class imbalance and scale imbalance making it an ideal benchmark for studying small object detection problems. As the evaluation server is closed now, following the existing works, we used the validation set for evaluating the performance.

\noindent \textbf{DOTA.} This dataset is comprised of satellite images. The images in this dataset have a resolution ranging from 800$\times$800 to 4000$\times$4000. Around 280k annotated instances are present in the dataset. The objects are from fifteen different categories, with movable objects such as planes, ships, large vehicles, small vehicles, and helicopters. The remaining ten categories are roundabouts, harbors, swimming pools, etc. The training and validation data contain 1411 images and 458 images, respectively. Since the images of DOTA are too large to be fed to the network directly, we extracted 1500$\times$1500 crops from the image by shifting 1000 pixels in a sliding window fashion.

\noindent \textbf{Implementation details.} The Detectron2 toolkit \cite{detectron2-wu-2019} was used to implement our CZ detector. The backbone detector used in our study is Faster RCNN \cite{faster_rcnn-Ren-2015}. We used Feature Pyramid Network (FPN) \cite{fpn-Lin-2017} backbone with ResNet50 \cite{resnet-He-2016}  pre-trained on ImageNet \cite{imagenet-Russakovsky-2015} dataset for our experimental validation. For data augmentation, we resized the shorter edge to one randomly picked from (800, 900, 1000, 1100, 1200), and applied horizontal flip with a 50\% probability. The model was trained on both datasets for 180k iterations. The initial learning rate is set to 0.01 and decayed by 10 at 70k iteration. For training, we used one NVIDIA A100 GPU with 40 GB of memory.


\subsection{Comparison with Different Percentage of Labeled Data}
We analyzed the effectiveness of our semi-supervised learning method by using partially labeled data from the train set of VisDrone and DOTA datasets. In particular, we used 1\%, 5\%, and 10\% randomly chosen data points from the train set as labeled data and the remaining as unlabeled for the semi-supervised training. There are five settings in the comparison; supervised baseline, supervised baseline with density crops (Supervised + Crop), semi-supervised with the mean teacher (SSOD), SSOD with density crops on labeled images (SSOD + Crop (L)), and SSOD with density crops on labeled and unlabeled images (SSOD + Crop (L + U)). These settings progressively assess the impact of the components of our density crop-guided semi-supervised object detection.

Table \ref{table:diff_sup_data_visdrone} presents the results for the VisDrone\cite{zhu-VisDrone-2018} dataset.  It compares the detection average precision values obtained using the COCO evaluation protocol \cite{mscoco-Lin-2014} for Intersection over Union (IoU) thresholds [0.5:0.05:0.95] (\textbf{AP}), and 0.5 (\textbf{$\textrm{AP}_{50}$}). It can be observed that \textbf{AP} is improved by more than 6\% in all cases with our density-guided SSOD over their supervised baseline. Compared to the vanilla mean-teacher method (SSOD), our density crop-guided SSOD shows an average improvement of more than 2\% on all metrics. Compared to 1\% and 5\% cases, with very limited labeled samples per class, 10\% shows a better boost in performance while leveraging density crops with SSOD. Another interesting result is that the improved performance with semi-supervised learning for 1\% settings is more than that of supervised training with 5\% labels and 2\% below with the 10\% labels. This is achieved with less than 100 labeled samples. $\textrm{AP}_{50}$ has a gain of more than 5\% compared to the vanilla mean-teacher when semi-supervised learning is performed with density crops in the 10\% setting.  

\begin{table*}
    \caption{Performance comparison of our density crop guided semi-supervised object detection with 1\%, 5\%, and 10\% labeled images on the VisDrone dataset. SSOD - semi-supervised detection with mean-teacher, Crop(L) - density crops on the labeled images, Crop (L + U) - density crops on the labeled and unlabeled images.}
    \centering
    \resizebox{.97\textwidth}{!}{
    \begin{tabular}{l||cc|cc|cc}
    \hline
    \textbf{Settings} & \multicolumn{2}{|c|}{1\% (\#Labeled =64)} & \multicolumn{2}{|c|}{5\% (\#Labeled =323)} & \multicolumn{2}{c}{10\% (\#Labeled =647)}\\
& \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} \\
     \hline \hline
     Supervised & 10.69$\pm0.20$ & 23.53$\pm0.17$ & 16.11$\pm0.14$ & 32.55$\pm0.16$ & 19.26$\pm0.13$ & 37.73$\pm0.22$ \\
     Supervised + Crop & 13.04$\pm0.23$ & 26.97$\pm0.16$ & 20.90$\pm0.28$ & 40.21$\pm0.29$ & 23.30$\pm0.17$ & 43.85$\pm0.31$ \\
     \hline
     SSOD & 15.30$\pm0.29$ & 29.16$\pm0.48$ & 21.89$\pm0.16$ & 40.58$\pm0.29$ & 24.39$\pm0.22$ & 43.05$\pm0.20$ \\
     SSOD + Crop (L) & 16.64$\pm0.24$ & 31.11$\pm0.18$ & 22.52$\pm0.12$ & 41.33$\pm0.24$ & 26.48$\pm0.19$ & 47.50$\pm0.14$ \\
     \rowcolor{mygray} SSOD + Crop (L + U) & \textbf{17.21}$\pm0.16$ & \textbf{31.22}$\pm0.21$ & \textbf{23.57}$\pm0.21$ & \textbf{42.34}$\pm0.19$ & \textbf{27.46}$\pm0.16$ & \textbf{48.95} $\pm0.14$ \\
     \hline  
    \end{tabular}
    }
    \label{table:diff_sup_data_visdrone}
\end{table*}

We also studied how the AP of small, medium, and large objects behave in the same five settings described above. Figure \ref{fig:vis_sml} shows the results. The trend here is similar to that of table \ref{table:diff_sup_data_visdrone}. Using density crops increases the detection accuracy both in supervised and semi-supervised settings. Compared to the supervised settings, the AP of all-sized objects increases by more than 5\% when semi-supervised learning is performed with density crops. The improvement over the vanilla mean-teacher is more than 3\% in most settings. The APs of small, medium, and large objects with fully supervised training using 100\% labeled data are 25.74, 42.93, and 41.44 respectively. It can be observed that our model with 10\% labeled data performs competitively with this fully supervised upper bound. 


\begin{figure}[t]
  \centering
  \includegraphics[height=6cm, width=0.49\textwidth]{images/vis_sml_plot.png} 
  \caption{Detection AP of small, medium, and large objects with different percentages of supervised data on the VisDrone dataset. FS: fully supervised, FS+C: fully supervised with crops, SS: vanilla mean-teacher, SS+C: mean-teacher with density crops on labeled images, SS+C+U: mean-teacher with density crops on all images.}
  \label{fig:vis_sml}
\end{figure}

We further verified this observation  by conducting the same type of study in the satellite images of the DOTA dataset. Table \ref{table:diff_sup_data_dota} shows the results. The magnitude of improvements is comparable to that of the VisDrone dataset. AP shows an average improvement above 2\% compared to the mean-teacher method. $\textrm{AP}_{50}$ has a gain of more than 3\% in this dataset compared to the mean-teacher. Also, the APs of small, medium, and large objects are studied in the same way as above. Figure \ref{fig:dota_sml} shows the results. APs of small, medium, and large objects with 100\% supervised data on the DOTA dataset are 15.66, 38.16, and 44.2 respectively. While for small objects, our method with 10\% labeled data is 3\% below the supervised upper bound, the gap is around 10\% for medium and large objects. This implies the boost from the density-guided training is more concentrated on the small objects. All of these experiments confirm the impact of each component in our model as well. The performance gain with our density-guided semi-supervised detector over the supervised baseline is significant and consistent.
\begin{comment}
\begin{table*}
    \centering
    \begin{tabular}{l||rrr|rrr|rrr|r}
    \hline
    \textbf{Settings} & \multicolumn{3}{|c|}{1\% (\#L=64)} & \multicolumn{3}{|c|}{5\% (\#L=323)} & \multicolumn{3}{|c|}{10\% (\#L=647)} & FPS\\
    \hline
     & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} &  \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} &  \\
     \hline
     Supervised & 5.56 & 12.39 & 3.92 & 14.55 & 25.52 & 14.27 & 19.18 & 34.44 & 18.40 & 0.50 \\
     Supervised + Dcrop & 6.80 & 14.19 & 5.51 & 15.97 & 28.98 & 15.74 & 20.43 & 36.50 & 20.27 & 0.92 \\
     \hline
     SSOD & 8.78 & 16.18 & 8.25 & 16.78 & 29.59 & 16.11 & 23.15 & 39.49 & 23.86 & 0.50 \\
     SSOD + Dcrop (L) & 9.74 & 18.44 & 8.96 & 18.42 & 31.62 & 18.26 &  24.34 & 42.30 & 24.29 & 0.92 \\
     SSOD + Dcrop (L + U) & \textbf{10.32} & \textbf{19.70} & \textbf{9.33} & \textbf{19.96} & \textbf{34.78} & \textbf{19.65} & \textbf{25.17} & \textbf{43.07} & \textbf{24.57} & 0.92\\
    \hline  
    \end{tabular}
    \caption{Performance comparison of our density crop guided semi-supervised object detection with 1\%, 5\%, and 10\% labeled images on the DOTA dataset. The detection speed is also reported in FPS. SSOD - semi-supervised detection with mean-teacher, Dcrop(L) - density crops on the labeled images, Dcrop (L + U) - density crops on the labeled and unlabeled images.}
    \label{table:diff_sup_data_dota}
\end{table*}
\end{comment}

\begin{figure}[t]
  \centering
  \includegraphics[height=6cm, width=0.49\textwidth]{images/dota_sml_plot.png} 
  \caption{Detection AP of small, medium, and large objects with different percentages of supervised data on the DOTA dataset. FS: fully supervised, FS+C: fully supervised with crops, SS: vanilla mean-teacher, SS+C: mean-teacher with density crops on labeled images, SS+C+U: mean-teacher with density crops on all images.}
  \label{fig:dota_sml}
\end{figure}


\begin{table*}
    \centering
    \caption{Performance comparison of our density crop guided semi-supervised object detection with 1\%, 5\%, and 10\% labeled images on the DOTA dataset. SSOD - semi-supervised detection with mean-teacher, Crop(L) - density crops on the labeled images, Crop (L + U) - density crops on the labeled and unlabeled images.}    
    \resizebox{.97\textwidth}{!}{
    \begin{tabular}{l||cc|cc|cc}
    \hline
    \textbf{Settings} & \multicolumn{2}{|c|}{1\% (\#Labeled=14)} & \multicolumn{2}{|c|}{5\% (\#Labeled=71)} & \multicolumn{2}{c}{10\% (\#Labeled =141)} \\
    \hline
     & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} &  \textbf{AP} & \textbf{$\textrm{AP}_{50}$} &  \textbf{AP} & \textbf{$\textrm{AP}_{50}$} \\
     \hline
     Supervised & 5.56$\pm0.26$ & 12.39$\pm0.49$ & 14.55$\pm0.11$ & 25.52$\pm0.24$ & 19.18$\pm0.18$ & 34.44$\pm0.36$ \\
     Supervised + Crop & 6.80$\pm0.19$ & 14.19$\pm0.41$ & 15.97$\pm0.18$ & 28.98$\pm0.35$ & 20.43$\pm0.15$ & 36.50$\pm0.51$ \\
     \hline
     SSOD & 8.78$\pm0.21$ & 16.18$\pm0.40$ & 16.78$\pm0.13$ & 29.59$\pm0.23$ & 23.15$\pm0.14$ & 39.49$\pm0.29$ \\
     SSOD + Crop (L) & 9.74$\pm0.18$ & 18.44$\pm0.26$ & 18.42$\pm0.16$ & 31.62$\pm0.25$ &  24.34$\pm$0.07 & 42.30$\pm0.11$ \\
      \rowcolor{mygray} SSOD + Crop (L + U) & \textbf{10.32}$\pm0.17$ & \textbf{19.70}$\pm0.22$ & \textbf{19.96}$\pm0.14$ & \textbf{34.78}$\pm0.24$ & \textbf{25.17}$\pm0.11$ & \textbf{43.07}$\pm0.21$ \\
    \hline  
    \end{tabular}}
    \label{table:diff_sup_data_dota}
\end{table*}

We also produced a qualitative comparison of the detection results from our semi-supervised model with that of its supervised baseline. Figure \ref{fig:det_comparison} shows the comparison on the DOTA (top two rows) and VisDrone (bottom two rows) datasets. The supervised baseline is shown at the top and the semi-supervised results at the bottom among each pair of rows. We can see that many tiny objects are getting detected with our density-guided semi-supervised detector. In the case of VisDrone datasets, the baseline detector is missing most of the small objects at the farther end of the camera, whereas our method with zoom-in capability is discovering them. In the DOTA dataset, the missing happens at a much higher rate as the images are very high in pixel resolution. Especially objects like small cars are mostly missed by the baseline detector on the DOTA dataset. But our method shows impressive results in detecting them.

\begin{figure*}
\centering
\begin{tabular}{c c @{\hspace{-0.3cm}} c @{\hspace{-0.3cm}} c @{\hspace{-0.3cm}} c}
\rotatebox{90}{\quad \quad Sup. DOTA} & \includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1014_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1380_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1269_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P0604_det_base.jpg}\\
\rotatebox{90}{\quad \quad SSOD+C DOTA} & \includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1014_det_ss.jpg} &
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1380_det_ss.jpg} &
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P1269_det_ss.jpg} & 
\includegraphics[width=0.25\linewidth, height=3.2cm]{images/comp/dota/P0604_det_ss.jpg}\\
\rotatebox{90}{\quad Sup. VisDrone} & \includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000001_04527_d_0000008_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000086_01443_d_0000004_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000154_00801_d_0000001_det_base.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000330_00201_d_0000801_det_base.jpg}\\
\rotatebox{90}{\quad SSOD+C VisDrone} & \includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000001_04527_d_0000008_det_ss.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000086_01443_d_0000004_det_ss.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000154_00801_d_0000001_det_ss.jpg} &
\includegraphics[width=0.25\linewidth, height=3.0cm]{images/comp/visdrone/0000330_00201_d_0000801_det_ss.jpg}\\
 \end{tabular}
 \caption{Qualitative comparison of detection results between supervised baseline and semi-supervised detector trained with density crops. More objects are detected with our semi-supervised zoom-in detector, especially the small ones. }
 \label{fig:det_comparison}
\end{figure*}

\subsection{Comparison with Other Semi-supervised Detectors}
As other density-based approaches for small object detection use an external module (and multi-stage training) for crop extraction, we cannot adapt them to the semi-supervised settings with mean-teacher. So, we choose the recently proposed scale-aware detection QueryDet \cite{querydet-Yang-2022} as it also accelerates small object detection with a detector itself. In particular, they proposed sparse querying on the high-resolution feature maps to improve small object detection. This is implemented on the feature pyramids within a detector, so we can wrap the mean-teacher training on top of this method. We used the VisDrone dataset with 10\% labels in this study. The result is shown in table \ref{table:other_ssod_comp}. Our method has an AP of more than 7\% compared to the QueryDet semi-supervised detector. The $\textrm{AP}_{s}$ is improved by 7\% whereas $\textrm{AP}_{m}$, $\textrm{AP}_{l}$ has an improvement of more than 10\%. While the semi-supervised QDet has an improvement of 3\% over its supervised baseline, our method has an improvement of 8\% over the supervised baseline. Note that the supervised baselines are different here because QueryDet proposed a method specific to the RetinaNet \cite{retinanet-Lin-2017} detector. This study also establishes the superiority of density-based detection over scale-aware training as well. 

\begin{table}
    \caption{Performance comparison with QueryDet \cite{querydet-Yang-2022} method for small object detection in the semi-supervised settings using 10\% labeled images on the VisDrone dataset.}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||rrr|rrr}
    \hline
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\ 
    \hline
    Sup. QDet\cite{querydet-Yang-2022} & 16.58 & 31.13 & 15.45 & 10.89 & 23.93 & 23.46 \\
    Sup. Ours & 19.26 & 37.73 & 17.48 & 12.94	& 26.85	& 26.65 \\
    \hline
    QDet\cite{querydet-Yang-2022} SSOD & 19.56 & 35.78 & 18.67 & 13.90 & 26.17 & 30.53 \\
    \rowcolor{mygray} Ours & \textbf{27.46} & \textbf{48.95} & \textbf{26.92} & \textbf{19.88} & \textbf{37.73} & \textbf{36.31} \\
    \hline
    \end{tabular}}
    \label{table:other_ssod_comp}
\end{table}

\subsection{Ablation Studies}
The improvement in performance when progressively adding density crops and semi-supervised detection is verified with the results in tables \ref{table:diff_sup_data_dota} and \ref{table:diff_sup_data_visdrone}. The results in these tables ablate the components of the proposed method extensively. Furthermore, the change in performance on objects of different sizes with these components is illustrated in figures \ref{fig:vis_sml} and \ref{fig:dota_sml}. The observations conclude that using density crops with the semi-supervised mean-teacher detector significantly improves the results over the basic supervised detector. As the impact of the components of the proposed method is verified with these experiments, we devote the ablation studies to consider other design choices and fine-grained analysis of the method's performance. We used the VisDrone dataset in this study.

\subsection{Inference}
The inference with density crops can be performed in two ways; taking the crop prediction directly from the model or running the cluster labeling algorithm with output detections. While the crop predictions are fast for inference, we observed that running the cluster labeling algorithm on the detection output is slightly more accurate. So one can choose the inference procedure among the two based on the speed vs accuracy trade-off of the downstream application. In the results reported so far, we used crop predictions directly from the model. To compare the performance of both we performed inference in two ways and reported the performance in table \ref{table:inf_comparison}. The VisDrone dataset with 10\% labels is used in this study. We can observe that while the improvement is small in AP, $\textrm{AP}_{50}$ has a gain of more than 1\%. We can also see that crop-labeled inference is improving the AP of small objects significantly, but at the same time, the AP of medium and large objects is declining. As the dataset is dominated by small objects, we still observe an overall improvement in performance. We also reported the detection speed in Frames Per Second (FPS). The FPS is only reduced by 5 frames when the expensive crop-labeled inference is used.


\begin{table}
    \caption{Results comparison of inference with predicted crops vs crops obtained by algorithm \ref{alg:crop_discovery} on detection boxes. The Visdrone dataset with 10\% labels is used in the study.}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||rrr|rrr|r}
    \hline  
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} & \textbf{FPS} \\  
    \hline
    \shortstack{Inference with \\ Predicted crops} & 27.46 & 48.95 & 26.92 & 19.88 & 37.73 & 36.31 & 12.45\\
    \shortstack{Inference with \\ labeled crops} & 27.78 & 50.02 & 27.12 & 20.99 & 36.32 & 35.84 & 7.17\\
    \hline
    \end{tabular}}
    \label{table:inf_comparison}
\end{table}

\subsection{Comparison with the Supervised Upper-bound}
In table \ref{table:sup_upperbound_ssod_comp}, we compare the results of our semi-supervised model with the fully supervised upper bound where 100\% images are labeled. The setting used here is images labeled 10\%. The lower bound of the performance when only the available 10\% labeled data is also provided. It can be observed that our method with 10\% labeled data is approximately 6\% points close to the upper bound, both in the AP and $\textrm{AP}_{s}$. $\textrm{AP}_{m}$ and $\textrm{AP}_{l}$ also show a similar trend. Therefore, it can be concluded that, by effectively leveraging unlabeled data, our method is able to achieve a performance close to the fully supervised upper bound, while using minimal labeled data points.

\begin{table}
    \caption{Performance comparison with the fully supervised upper-bound on the VisDrone dataset with 10\% labeled images.}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||rrr|rrr}
    \hline
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\ 
    \hline
    \emph{Lower bound} & & & & & & \\
    10\% Labeled & 19.26 & 37.73 & 17.48 & 12.94 & 26.85 & 26.65 \\
    \hline
    \emph{Semi-supervised (Ours)} & & & & & & \\
    10\% Lab. + 90\% Unlab. & 27.46 & 48.95 & 26.92 & 19.88 & 37.73 & 36.31 \\
    \hline
    \emph{Upper bound} & & & & & & \\
    100\% Labeled & 33.22 & 58.30 & 33.16 & 26.06 & 42.58 & 43.36 \\
    \hline
    \end{tabular}}
    \label{table:sup_upperbound_ssod_comp}
\end{table}


\subsection{computational cost}
Using unlabeled data for mean-teacher training comes with additional training costs. Exponentially averaged teacher weights must be learned with a small $\alpha$ value to have stable distillation. We used a 0.9996 following the standard practices \cite{unbiased-teacher-Liu-2021, humble-teacher-Yang-2021}. This results in many iterations for the mean-teacher training. In table \ref{table:cost_comparison}, we compared the training iterations and time for different settings. Inference time per image is also provided. Finding crops on unlabeled images is performed only after the pseudo labels on unlabeled images are converged. The augmentation then adds an additional set of crops to the training process. That is why the SS+C (L+U) setting is taking longer iterations. For inference, the difference when using crops is due to the second detection performed on the crops. Even though there is an effective increase in training and inference time, the improvement in detection performance is significant.

\begin{table}
    \caption{Comparison of the training and test time for fully supervised and semi-supervised methods with and without density crops. All settings are evaluated using one A100 GPU with the Visdrone dataset having 10\% labels.}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||rrrrr}
    \hline  
    \textbf{Settings} & \textbf{FS} & \textbf{FS+C} & \textbf{SS} & \textbf{SS+C (L)}  & \textbf{SS+C (L+U)} \\ 
    \hline
    \shortstack{Train iters} & 5k & 15k & 65k & 75k & 180k\\
    \shortstack{Train time in HH:MM} & 1:03 & 2:28 & 15:36 & 15:19 & 33:35\\
    \shortstack{Test time in s/image} & 0.0348 & 0.0661 & 0.0348 & 0.0661 & 0.0661 \\
    \hline
    \end{tabular}}
    \label{table:cost_comparison}
\end{table}


\subsection{Analysis of the Type of Errors}
To understand how the addition of semi-supervised learning and density crops affects the detector's abilities, we profiled different error types based on the TIDE \cite{tide-Bolya-eccv2020} evaluation protocol. Figure \ref{fig:tide_comparison} shows the comparison results. With the addition of density crops on a supervised detector, we observe the localization error reduces. Other types of errors remain mostly the same. With semi-supervised training using the vanilla mean-teacher method, the classification error reduces. Using density crops with semi-supervised learning is reducing the localization error  similar to the fully-supervised case and other errors remain the same mostly. Compared to fully supervised detectors, semi-supervised detectors reduce classification error, but they tend to miss objects too. This is probably due to the imbalance in object classes of this dataset such that dominant classes get more pseudo-labels on unlabeled images. This can result in rare class objects being missed on the unlabeled images.

\begin{figure*}
\centering
\begin{tabular}{@{\hspace{0.2cm}} c @{\hspace{0.3cm}} c @{\hspace{0.3cm}} c @{\hspace{0.3cm}} c}
\includegraphics[width=0.23\linewidth, height=6cm]{images/tide/10_vis_base.png} &
\includegraphics[width=0.23\linewidth, height=6cm]{images/tide/10_vis_crop.png} &
\includegraphics[width=0.23\linewidth, height=6cm]{images/tide/10_vis_ss_base.png} &
\includegraphics[width=0.23\linewidth, height=6cm]{images/tide/10_vis_ss_crop.png}\\
 (a) Supervised & (b) Supervised + Crop  & (c) SSOD & (d) SSOD + Crop \\
 \end{tabular}
 \caption{TIDE\cite{tide-Bolya-eccv2020} evaluation of detection results of the detectors trained with (a) supervised, (b) supervised with density crops, (c) vanilla semi-supervised and (d) semi-supervised with density crops modes. Error types are: \textbf{Cls}: localized correctly but classified incorrectly, \textbf{Loc}: classified correctly but localized incorrectly, \textbf{Both}: both cls and loc error, \textbf{Dupe}: duplicate detection error, \textbf{Bkg}: detected background as foreground, \textbf{Miss}: missed ground truth error.}
 \label{fig:tide_comparison}
\end{figure*}

