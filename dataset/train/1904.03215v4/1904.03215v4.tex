Here we provide additional experimental evaluations as well as details on the proposed datasets and the evaluated methods.

\section{Misclassification Detection}
\label{sec:misclassification}
Additionally to anomaly detection, we test some methods on the detection of misclassifications from the semantic segmentation output. Misclassification detection is another proxy classification task that correlates with uncertainty. However, misclassification mixes uncertainty from
\begin{itemize}[label={--},leftmargin=*, nosep]
    \item noise in the input (\emph{aleatoric} uncertainty) 
    \item model uncertainty
    \item shifts in data balance (softmax classification implicitly learns a prior distribution of the classes over the training set) 
\end{itemize}
Nevertheless, failure detection is an important problem for deployment on autonomous agents, e.g. as part of sensor fusion mechanisms, and misclassification detection is used in different related work~\cite{Hendrycks2016-ua,Lakshminarayanan2017-zk,Guo2017-kg,Jiang2018-rl} to benchmark uncertainty estimates.


\begin{figure*}
\centering
\def\iwidth{.19\linewidth}
\def\iheight{1.8}
\def\iiheight{3.65}
\begin{tikzpicture}
\node[rotate=90] at (-2,-\iheight) {\scriptsize\strut Fishyscapes Misclassifications};

\node[inner sep=0pt, label=\scriptsize{Input\strut}] at (0,0)
    {\includegraphics[width=\iwidth]{0038_rgb.jpg}};
\node[inner sep=0pt, label=\scriptsize{Prediction\strut}] () at (\iwidth,0)
    {\includegraphics[width=\iwidth]{0038_trainent_pred.png}};
\node[inner sep=0pt, label=\scriptsize{Void Entropy\strut}] () at (\iwidth*2,0)
    {\includegraphics[width=\iwidth]{0038_trainent.jpg}};
\node[inner sep=0pt, label=\scriptsize\strut Prediction] () at (\iwidth*3,0)
    {\includegraphics[width=\iwidth]{0038_bayesent_pred.png}};
\node[inner sep=0pt, label=\scriptsize\strut Bayesian Predictive Entropy] () at (\iwidth*4,0)
    {\includegraphics[width=\iwidth]{0038_bayesent.jpg}};
    
\node[inner sep=0pt] at (0,-\iheight)
    {\includegraphics[width=\iwidth]{0080_rgb.jpg}};
\node[inner sep=0pt] () at (\iwidth,-\iheight)
    {\includegraphics[width=\iwidth]{0080_trainent_pred.png}};
\node[inner sep=0pt] () at (\iwidth*2,-\iheight)
    {\includegraphics[width=\iwidth]{0080_trainent.jpg}};
\node[inner sep=0pt] () at (\iwidth*3,-\iheight)
    {\includegraphics[width=\iwidth]{0080_bayesent_pred.png}};
\node[inner sep=0pt] () at (\iwidth*4,-\iheight)
    {\includegraphics[width=\iwidth]{0080_bayesent.jpg}};
    
\node[inner sep=0pt] at (0,-\iiheight)
    {\includegraphics[width=\iwidth]{0123_rgb.jpg}};
\node[inner sep=0pt] () at (\iwidth,-\iiheight)
    {\includegraphics[width=\iwidth]{0123_trainent_pred.png}};
\node[inner sep=0pt] () at (\iwidth*2,-\iiheight)
    {\includegraphics[width=\iwidth]{0123_trainent.jpg}};
\node[inner sep=0pt] () at (\iwidth*3,-\iiheight)
    {\includegraphics[width=\iwidth]{0123_bayesent_pred.png}};
\node[inner sep=0pt] () at (\iwidth*4,-\iiheight)
    {\includegraphics[width=\iwidth]{0123_bayesent.jpg}};
\end{tikzpicture}
\caption{\textbf{Qualitative examples of misclassification detection.} Predictions correspond to the uncertainty maps to their right. Misclassifications are marked in black, while ignored \emph{void} pixels are marked in bright green. Better methods should assign a high score (dark) to misclassified pixels. While the different trainings clearly lead to different classification performances, none of the methods captures all the misclassified pixels.}
\label{fig:misclass-overview}
\end{figure*}

\PAR{Dataset.} We test misclassification detection on a diverse mixture of different data sources that introduce sources of uncertainty in the input. From Foggy Driving~\cite{foggycityscapes1}, we select all images. From Foggy Zurich~\cite{foggycityscapes2}, we map classes \emph{sky} and \emph{fence} to \emph{void}, as their labelling is not accurate and sometimes areas that are not visible due to fog are simply labelled \emph{sky}. For WildDash~\cite{Zendel2018-ru}, we use all images. For Mapillary Vistas~\cite{Neuhold2017-ca}, we sample 50 random images from the validation set and apply the label mapping described in Table~\ref{tab:mapillary}.\\
During evaluation all pixels labelled as \emph{void} are ignored.

\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{ll}
\toprule
mapillary label & used label\\
\midrule
\emph{construction--barrier--fence} & \emph{fence}\\
\emph{construction--barrier--wall} & \emph{wall}\\
\emph{construction--flat--road} & \emph{road}\\
\emph{construction--flat--sidewalk} & \emph{sidewalk}\\
\emph{construction--structure--building} & \emph{building}\\
\emph{human--person} & \emph{person}\\
\emph{human--rider--*} & \emph{rider}\\
\emph{nature--sky} & \emph{sky}\\
\emph{nature--terrain} & \emph{terrain}\\
\emph{nature--vegetation} & \emph{vegetation}\\
\emph{object--support--pole} & \emph{pole}\\
\emph{object--support--utility-pole} & \emph{pole}\\
\emph{object--traffic-light} & \emph{traffic light}\\
\emph{object--traffic-sign--front} & \emph{traffic sign}\\
\emph{object--vehicle--bicycle} & \emph{bicycle}\\
\emph{object--vehicle--bus} & \emph{bus}\\
\emph{object--vehicle--car} & \emph{car}\\
\emph{object--vehicle--motorcycle} & \emph{motorcycle}\\
\emph{object--vehicle--on-rails} & \emph{train}\\
\emph{object--vehicle--truck} & \emph{truck}\\
\emph{marking--*} & \emph{road}\\
anything else & \emph{void}\\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Mapping of Mapillary classes onto our used set of classes for misclassification detection.}
\label{tab:mapillary}
\end{table}

\begin{table}[ht]
    \centering
    \scriptsize{
    \setlength\tabcolsep{4pt}
    \begin{tabular}{ll|mcc}
    \toprule
    & & \multicolumn{3}{c}{FS Misclassification}\\
    
    method & score & max J & AP & mIoU\\
    \toprule
    Baseline & random uncertainty & 00.0 & 38.9 & 45.5 \\
    \midrule
    \multirow{2}{*}{Softmax}
        & max-probability & 43.6 & 67.4 & \multirow{2}{*}{45.5} \\
        & entropy  & 43.5 & 68.4 &\\
    \midrule
    \ac{ood} training & max-entropy & \textbf{44.3} & 71.3  & 35.8 \\
    \midrule
    \multirow{2}{*}{Bayesian DeepLab}
        & mutual information & 40.7 & 70.4  & \multirow{2}{*}{30.3}\\
        & predictive entropy  & 41.6 & \textbf{73.8} &\\
    \midrule
    Dirichlet DeepLab
        & prior entropy & 29.7 & 65.0 & 37.5 \\
    \midrule
    \multirow{2}{*}{\parbox[b]{8em}{kNN Embedding}}
        & density  & 40.7 & 68.0 & \multirow{2}{*}{45.5}\\
        & relative class density  & 31.7 & 58.0 & \\
    \bottomrule
    \end{tabular}
    }
    \vspace{5pt}
    \caption{\textbf{Misclassification Detection Results}. The gray column marks the primary metric.
    }
    \label{tab:misclassification_results}
    \vspace{-3mm}
\end{table}

\PAR{Evaluated Methods} From the methods evaluated on anomaly detection, we note that the void classifier produces meaningless results for misclassification detection since a high void output score produces the exact misclassification it is detecting. Furthermore, we did not evaluate the learned embedding density.

\PAR{Results} of our evaluation are presented in table~\ref{tab:misclassification_results} and qualitative examples in figure~\ref{fig:misclass-overview}. Differently from anomaly detection, the softmax score is expected to be a good indicator for classification uncertainty, and indeed shows competitive results. For Bayesian DeepLab, we find the predictive entropy to be a better indicator of misclassification, which was also observed by~\cite{Kendall2017-jy}.
The k\ac{nn} density shows results similar to the other methods, hinting that embedding-based methods cannot be entirely classified as \ac{ood}-specific, but may also be able to detect input noise that is very different from the training distribution. Overall, the experiments do not reveal a single method that performs significantly better than others.

\section{Details on the Methods}
\label{sec:details:methods}
In this section we provide implementation details on the evaluated methods to ease the reproducibility of the results presented in this paper.

\subsection{Semantic Segmentation Model}
We use the state-of-the-art model DeepLabv3+~\cite{Chen2018-bp} with Xception-71 backbone, image-level features, and dense prediction cell. When no retraining is required, we use the original model trained on Cityscapes\footnote{\url{https://github.com/tensorflow/models/blob/master/research/deeplab}}.

\subsection{Softmax}
ODIN~\cite{Liang2017-mj} applies input preprocessing and temperature scaling to improve the \ac{ood} detection ability of the maximum softmax probability. Early experiments on Fishyscapes showed that (i) temperature scaling did not improve much the results of this baseline, and (ii) input preprocessing w.r.t.\ the softmax score is not possible due to the limited GPU memory and the large size of the DeepLab model. As the maximum probability is anyway not competitive with respect to the other methods, we decided to not further develop that baseline.








\subsection{Bayesian DeepLab}
We reproduce the setup described by Mukhoti \& Gal~\cite{Mukhoti2018-af}. As such, we use the Xception-65 backbone pretrained on ImageNet, and insert dropout layers in its middle flow. We train for 90k iterations, with a batch size of 16, a crop size of , and a learning rate of  with polynomial decay.

\subsection{Dirichlet DeepLab}
Following Malinin \& Gales~\cite{Malinin2018-pl}, we interpret the output logits of DeepLab as log-concentration parameters  and train with the loss described by Equation~(\ref{eq:dirichlet}) and implemented with the TensorFlow Probability~\cite{dillon2017tensorflow} framework. For the first term, the target labels are smoothed with  and scaled by  to obtain target concentrations. To ensure convergence of the classifier, we found it necessary to downweight both the first and second terms by  and to initialize all but the last layer with the original DeepLab weigths.

We also tried to replace the first term by the negative log-likelihood of the Dirichlet distribution but were unable to make the training converge.

\subsection{kNN Embedding}
\PAR{Layer of Embedding.} As explained in Section~\ref{sec:method:baselines}, we had to restrict the kNN queries to one layer. A single layer of the network already has more than 10000 embedding vectors and we need to find k nearest neighbors for all of them. Querying over multiple layers therefore becomes infeasible. To select a layer of the network, we test multiple candidates on the FS Lost \& Found validation set. We experienced that our kNN fitting with \emph{hnswlib}\footnote{\url{https://github.com/nmslib/hnswlib}}~\cite{Malkov2016-fv} was not deterministic, therefore we provide the average performance on the validation set over 3 different experiments. Additionally, we had to reduce the complexity of kNN fitting by randomly sampling 1000 images from Cityscapes instead of the whole training set (2975 images).

For the kNN density, we provide the results for different layers in Table~\ref{tab:knn:layers}.

\begin{table}[htb]
\centering
\footnotesize
\begin{tabular}{lc}
\toprule
DeepLab Layer & \ac{ap} \\
\midrule
\scriptsize{\verb|xception_71/middle_flow/block1/unit_8|} &  \\
\scriptsize{\verb|xception_71/exit_flow/block2|}          &  \\
\scriptsize{\verb|aspp_features|}                         & \\
\scriptsize{\verb|decoder_conv0_0|}                       &  \\
\scriptsize{\verb|decoder_conv1_0|}                       &  \\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Parameter search of the \textbf{embedding layer for kNN density}. The AP is computed on the validation set of FS Lost \& Found. Based on these results, we use the layer \texttt{decoder\_conv0\_0} in all our experiments.}
\label{tab:knn:layers}
\end{table}

For class-based embedding, we perform a similar search for the choice of layer. The result can be found in Table~\ref{tab:classknn:layers}.

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{lc}
\toprule
DeepLab Layer & \ac{ap} \\
\midrule
\scriptsize{\verb|xception_71/middle_flow/block1/unit_8|} &  \\
\scriptsize{\verb|xception_71/middle_flow/block1/unit_10|} &  \\
\scriptsize{\verb|xception_71/exit_flow/block2|}          &  \\
\scriptsize{\verb|aspp_features|}                         & \\
\scriptsize{\verb|decoder_conv0_0|}                       &  \\
\scriptsize{\verb|decoder_conv1_0|}                       &  \\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Parameter search of the \textbf{embedding layer for class based relative kNN density}. The AP is computed on the validation set of FS Static. Based on these results, we use the layer \texttt{xception\_71/exit\_flow/block2} in all our experiments.}
\label{tab:classknn:layers}
\end{table}

\PAR{Number of Neighbors.}We select  according to Tables~\ref{tab:knn-k} and~\ref{tab:classknn-k}. All values are measured with the same kNN fitting. As the computational time for each query grows with , small values are preferable. Note that by definition, the relative class density needs a sufficiently high  such that not all neighbors are from the same class. 

\begin{table}[htb]
\centering
\footnotesize
\begin{tabular}{rc}
\toprule
k & \ac{ap} \\
\midrule
      1 & 42.3\\
      2 & 44.6 \\
      5 & 47.7\\
      10 & 50.9 \\
      20 & 52.2\\
      50 & 52.7 \\
      100 & 52.5\\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Parameter search for the \textbf{number of nearest neighbors for kNN embedding density}. As computing time increases with , we select .}
\label{tab:knn-k}
\end{table}

\begin{table}[htb]
\centering
\footnotesize
\begin{tabular}{rc}
\toprule
k & \ac{ap} \\
\midrule
      5  & 5.4\\
      10 & 6.7 \\
      20 & 7.9\\
      50 & 9.3 \\
      100 & 9.9\\
      200 & 10.0\\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Parameter search for the \textbf{number of nearest neighbors for the class based kNN relative density}. As computing time increases with , we select .}
\label{tab:classknn-k}
\end{table}

\subsection{Learned Embedding Density}
\label{sec:appendix:methods:learned-embedding-density}

\PAR{Flow architecture.} The normalizing flow follows the simple architecture of Real-NVP. We stack 32 steps, each one composed of an affine coupling layer, a batch normalization layer, and a fixed random permutation. As recommended by~\cite{Kingma2018-jp}, we initialize the weights of the coupling layers such that they initially perform identity transformations.

\PAR{Flow training.} For a given DeepLab layer, we export the embeddings computed on all the images of the Cityscapes training set. The number of such datapoints depends on the stride of the layer, and amounts to 22M for a stride of 16. We keep 2000 of them for validation and testing, and train on the remaining embeddings for 200k iterations, with a learning rate of , and the Adam optimizer. Note that we can compare flow models based on how well they fit the in-distribution embeddings, and thus do not require any \ac{ood} data for hyperparameter search.

\PAR{Layer selection.} \ac{ood} data is only required to select the layer at which the embeddings are extracted. The corresponding feature space should best separate \ac{ood} and \ac{id} data, such that \ac{ood} embeddings are assigned low likelihood. We found that it is critical to extract embeddings before ReLU activations, as some dimensions might be negative for all training points, thus making the training highly unstable. We show in Table~\ref{tab:learned:layers} the \ac{ap} on the FS Lost \& Found validation set for different layers. We first observe that we did not achieve training convergence for those layers that showed best results in the k\ac{nn} method. This may be due to the high dimensionality of these layers, and/or because the flow is not well suited to approximate these distributions. We also notice that overall layers in the encoder middle flow work best, while Mukhoti \& Gal~\cite{Mukhoti2018-af} insert dropout layers at this particular stage. While we do not know the reason behind their design decision, we hypothesize the they found these layers to best model the epistemic uncertainty.

\begin{table}[htb]
    \centering
    \footnotesize
    \begin{tabular}{lc}
      \toprule
      DeepLab layer & AP \\
      \midrule
      \scriptsize{\verb|xception_71/entry_flow/block5|} & 1.27 \\
      \scriptsize{\verb|xception_71/middle_flow/block1/unit_4|} & 2.14 \\
      \scriptsize{\verb|xception_71/middle_flow/block1/unit_6|} & 2.38 \\
      \scriptsize{\verb|xception_71/middle_flow/block1/unit_8|} & 2.41 \\
      \scriptsize{\verb|xception_71/middle_flow/block1/unit_10|} & 2.52 \\
      \scriptsize{\verb|xception_71/middle_flow/block1/unit_12|} & 2.22 \\
      \scriptsize{\verb|aspp_features|}                         & - \\
      \scriptsize{\verb|decoder_conv0_0|}                       & 0.16 \\
      \scriptsize{\verb|decoder_conv1_0|}                       & \textbf{2.77} \\
      \bottomrule
    \end{tabular}
    \vspace{3mm}
    \caption{Cross-validation of the \textbf{embedding layer for the learned density.} The AP is computed on the validation set of FS Lost \& Found. Based on these results, we use the layer \texttt{decoder\_conv1\_0} in all our experiments. We could not manage to make the training of the \texttt{aspp\_features} layer converge, most likely due to a very peaky distribution that induces numerical instabilities.}
    \label{tab:learned:layers}
\end{table}

\PAR{Effect of input preprocessing.} As previously reported by~\cite{Liang2017-mj,Lee2018-si}, we observe that this simple input preprocessing brings substantial improvements to the detection score on the test set. We show in Table~\ref{tab:learned:noise} the AP for different noise magnitudes .

\begin{table}[htb]
    \centering
    \footnotesize
    \begin{tabular}{ccc}
      \toprule
      \multirow{2}{*}{Noise } & \multicolumn{2}{c}{AP on FS Static} \\
      & validation & test \\
      \midrule
      None & 36.0 & 52.5 \\
      0.1 & 38.4 & - \\
      0.2 & 39.1 & - \\
      0.25 & \textbf{39.2} & \textbf{55.4} \\
      0.3 & \textbf{39.2} & - \\
      0.35 & \textbf{39.2} & - \\
      0.4 & 39.1 & - \\
      0.5 & 39.0 & - \\
      1.0 & 36.6 & - \\
      \bottomrule
    \end{tabular}
    \vspace{3mm}
    \caption{Cross-validation of the \textbf{input preprocessing for the learned density.} Based on these results, we apply noise with magnitude  in all our experiments.}
    \label{tab:learned:noise}
\end{table}