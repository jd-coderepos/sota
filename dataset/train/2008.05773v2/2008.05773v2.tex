\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amsfonts,amssymb}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{url}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{continuous speech separation with conformer}
\name{\begin{tabular}{c}Sanyuan Chen, Yu Wu,  Zhuo Chen,  Jian Wu, Jinyu Li, Takuya Yoshioka \\
		Chengyi Wang, Shujie Liu, Ming Zhou\sthanks{Emails: v-sanych, yuwu1, zhuc, wujian, jinyli, tayoshio, v-chengw, shujliu, mingzhou @microsoft.com}\end{tabular}}
\address{Microsoft Corporation}
\begin{document}
	\ninept
\maketitle
\begin{abstract}
		Continuous speech separation was recently proposed to deal with overlapped speech in natural conversations. While it was shown to significantly improve the speech recognition performance for multi-channel conversation transcription, its effectiveness has yet to be proven for a single-channel recording scenario. 
This paper examines the use of  Conformer architecture in lieu of recurrent neural networks for the separation model. 
Conformer allows the separation model to efficiently capture both local and global context information, which is helpful for speech separation. Experimental results using the LibriCSS dataset show that the Conformer separation model achieves state of the art results for both single-channel and multi-channel settings. Results for real meeting recordings are also presented, showing significant performance gains in both word error rate (WER) and speaker-attributed WER. 
		


\end{abstract}
\begin{keywords}
		Multi-speaker ASR, Transformer, Conformer, Continuous speech separation
	\end{keywords}
\section{Introduction}
	\label{sec:intro}
	
The advance in deep learning has drastically improved the accuracy and robustness of modern automatic speech recognition (ASR) systems in the past decade \cite{ li2018developing, movsner2019improving, 
		sun2019speaker, Li2020Comparison}, enabling various voice-based applications. However, when applied to acoustically and linguistically complicated scenarios such as conversation transcription \cite{watanabe2020chime,yoshioka2019advances}, the ASR systems still suffer from the performance limitation due to overlapped speech and quick speaker turn-taking, which break the usually assumed single active speaker condition.
	Additionally, the overlapped speech causes the so-called permutation problem \cite{hershey2016deep}, further increasing the difficulty of the  conversation transcription.
	
	Speech separation is often applied as a remedy for this problem, where the mixed speech is  processed by a specially trained separation network before ASR . 
	Starting from deep clustering (DC) \cite{hershey2016deep} and permutation invariant training (PIT) \cite{yu2017permutation, kolbaek2017multitalker}, various separation models have been shown effective in handling overlapped speech \cite{yoshioka2019advances,luo2019conv,luo2020dual,chen2020dual}. 
Among the network architectures proposed thus far, the Transformer \cite{chen2020dual} based approach achieved a promising result. Transformer was first introduced  for machine translation \cite{vaswani2017attention} and later extended to speech processing  \cite{dong2018speech}. A Transformer based speech separation architecture was proposed in \cite{chen2020dual}, 
	achieving the state of the art separation quality on the WSJ0-2mix dataset. It was also reported in \cite{chang2020end} that incorporating Transformer into an end-to-end multi-speaker recognition network yielded higher recognition accuracy. 
However, both studies were evaluated on artificially simulated data sets that only considered overlapped speech, assuming the utterance boundaries to be provided, which significantly differs from the real conversational transcription scenario \cite{yoshioka2019advances,chen2020continuous}.




	
	In this work, 
	inspired by the recent advances in transducer-based end-to-end ASR modeling, which has evolved from a recurrent neural network (RNN) transducer \cite{he2019streaming} to Transformer \cite{zhang2020transformer} and Conformer \cite{gulati2020conformer} transducers, we examine the use of the  Conformer architecture for continuous speech separation (CSS) \cite{yoshioka2018multi}. Unlike the prior speech separation studies, in CSS, the separation network continuously receives a mixed speech signal, performs separation, and routes each separated utterance to one of its output channels in a way that each output channel contains overlap-free signals. 
	This allows a standard ASR system trained with single speaker utterances to be directly applied to each output channel to generate transcriptions.
	The proposed system is evaluated by using the LibriCSS dataset \cite{chen2020continuous}, which consists of real recordings of long-form multi-talker sessions that were created by concatenating and mixing LibriSpeech utterances with various overlap ratios.
	Our proposed network significantly outperforms the RNN-based baseline systems, achieving the new state of the art performance on this dataset. 
Evaluation results on real meetings are also presented along with tricks for further performance improvement.  

	
	


	


	




	
	
	


	\section{Approach}
	\subsection{Problem Formulation}
	The goal of speech separation is to  estimate individual speaker signals from their mixture, where the source  signals may be overlapped with each other wholly or partially. The mixed signal is formulated as 
, 
	where  is the time index,  denotes the -th source signal, and  is the mixed signal. 
Following \cite{yoshioka2018multi}, 
	when  microphones are available, the model input to the separation model can be obtained as 
	 where  means a concatenation operation,  refers to the STFT of the -th channel,  is the inter-channel phase difference between the -th channel and the first channel, i.e.  with  being the phase of . 
	These features are normalized along the time axis. If , it reduces to a single channel speech separation task. 
	
	
	
	Following \cite{wang2014training, erdogan2017deep}, a group of masks  are estimated with a deep learning model  instead of  directly predicting the source STFTs. 
Each source STFT, , is obtained as , where  is an elementwise product. 
	For the multi-channel setting, 
	the source signals are obtained with adaptive minimum variance distortionless response (MVDR) beamforming~\cite{6516079}. 
	In this paper, we employ the Conformer  structure \cite{gulati2020conformer} as  to estimate the masks for (continuous) speech separation. 
	
	\subsection{Model structure}
	
	\begin{figure}[t]		
		\begin{center}
			\includegraphics[width=.8\columnwidth,  trim={0cm 2cm 0cm 0cm}]{ConformerEncoder.PNG}
		\end{center}
		
		\caption{Conformer architecture. There are three mask outputs, two for speakers and one for noise.}\label{fig:conformer} 
		\vspace{-4.5mm}
		
	\end{figure} 
	
	Conformer \cite{gulati2020conformer} is a state-of-the-art ASR encoder architecture, which inserts a convolution layer into a Transformer block  to increase the local information modeling capability of the traditional Transformer model \cite{vaswani2017attention}. The architecture of the Conformer is shown in Fig. \ref{fig:conformer}, where each block consists of a self-attention module, a convolution module, and a macron-feedforward module. A chunk of  over time frames and frequency bins is the input of the first block. Suppose that the input to the -th block is , the -th block output is calculaed as
	 where
	, , , and  denote the feed forward network, self-attention module, convolution module, and layer normalization, respectively. 
	In the self-attention module,  is linearly converted to  with three different parameter matrices. Then, we apply a multi-head self-attention mechanism
	
	 where   is the dimensionality of the feature vector,  is the number of the attention heads.  is the relative position embedding \cite{shaw2018self}, where  is the maximum chunk length and  is a vector representing the offset of  and  with  and  denoting the -th vector of  and the -th vector of , respectively.  
	The Convolution  starts with  a pointwise convolution and a gated linear unit (GLU), followed by a 1-D depthwise convolution layer with a Batchnorm \cite{ioffe2015batch} and a Swish activation. After obtaining the Conformer output, we further convert it to a mask matrix as .
	
	


	
	\subsection{Chunk-wise processing for continuous separation}
	\label{ssec:sliding-window}
	The speech overlap usually takes place in a natural conversation which may last for tens of minutes or longer. To deal with such long input signals, CSS generates a predefined number of signals where overlapped utterances are separated and then routed to different output channels. 
	
	
	To enable this, we employ the chunk-wise processing proposed in \cite{Yoshioka2018Unmix} at test time. 
	A sliding-window is applied as illustrated in Figure~\ref{fig:sliding_window}, which contains three sub-windows, representing the history ( frames), the current segment ( frames), and the future context ( frames).  
We move the window position forward by  frames each time, and compute the masks for the current  frames using the whole -frame-long chunk. 


	To further consider the history information beyond the current chunk, we also consider taking account of the previous chunks in the self-attention module. Following Transformer-XL \cite{dai2019transformer}, the Equation \ref{self_att} is rewritten as
	
	where  is obtained by the current chunk while  and  are the concatenations of the previous and current changes in the key and value spaces, respectively. The dimensionality of  depends on the number of the history chunks considered. 
	


	\begin{figure}[t]
		\centering
		\begin{tikzpicture}
		\draw (0,0 ) node[inner sep=0] {\includegraphics[height=2.5cm, clip]{sliding_window.png}};
		\end{tikzpicture}
		\caption{Chunk-wise processing is employed to enable streaming processing for continuous speech separation.
		}\label{fig:sliding_window}
		\vspace{-4.5mm}
	\end{figure}
	
	
	\section{EXPERIMENT}
	
	\begin{table*}[!t]
		\centering
		\caption{ Utterance-wise evaluation for seven-channel and single-channel settings. Two numbers in a cell denote \%WER of the \textbf{hybrid ASR model} used in LibriCSS \cite{chen2020continuous} and \textbf{E2E Transformer} based ASR model \cite{wang2019semantic}. 0S and 0L are utterances with short/long inter-utterance silence.}
		\label{tab:7ch_utt_result}
		
		\begin{tabular}{l|cccccc} 
			\toprule \hline
			\multirow{2}{*}{\textbf{System}} &
			\multicolumn{6}{c}{\textbf{Overlap ratio in \%}} \\ &
			0S & 0L & 10 & 20 & 30 & 40   \\ \hline
			No separation \cite{chen2020continuous} & 11.8/5.5 & 11.7/5.2 & 18.8/11.4 & 27.2/18.8 & 35.6/27.7 & 43.3/36.6 \\\hline
			&   \multicolumn{6}{c}{Seven-channel Evaluation} \\ 
			\hline
BLSTM & \textbf{7.0}/\textbf{3.1} & 7.5/\textbf{3.3} & 10.8/4.3 & 13.4/5.6 & 16.5/7.5 & 18.8/8.9 \\ 
Transformer-base & 8.3/3.4 & 8.4/3.4 & 11.4/4.1 & 12.5/4.8 & 14.7/6.4 & 16.9/7.2 \\					
			Transformer-large & 7.5/\textbf{3.1} &	7.7/3.4 &	10.1/\textbf{3.7} &	12.3/\textbf{4.8} &	14.1/5.9 &	16.0/6.3 \\
			
			Conformer-base & 7.3/\textbf{3.1} & \textbf{7.3}/\textbf{3.3} & \textbf{9.6}/3.9 & 11.9/\textbf{4.8} &	13.9/6.0 &	15.9/6.8 \\
			Conformer-large & 7.2/\textbf{3.1} & 7.5/\textbf{3.3} & \textbf{9.6}/\textbf{3.7} & \textbf{11.3}/\textbf{4.8} & \textbf{13.7}/\textbf{5.6} & \textbf{15.1}/\textbf{6.2} \\ \hline
			&   \multicolumn{6}{c}{Single-channel Evaluation}  \\ \hline
			
			BLSTM &  15.8/6.4 & 14.2/5.8 & 18.9/9.6 & 25.4/15.3 & 31.6/20.5 & 35.5/25.2 \\
			
Transformer-base & 13.2/5.5 & 12.3/5.2 & 16.5/8.3 & 21.8/12.1 & 26.2/15.6 & 30.6/19.3 \\
			Transformer-large & 13.0/\textbf{5.3} & 12.4/5.1 & 15.5/\textbf{7.4} & 20.1/11.1 & 24.6/\textbf{13.5} & 27.9/\textbf{17.0} \\
			
			Conformer-base & 13.8/5.6 & 12.5/5.4 & 16.7/8.2 & 21.6/11.8 & 26.1/15.5 & 30.1/18.9 \\
			Conformer-large & \textbf{12.9}/5.4 & 12.2/\textbf{5.0} & \textbf{15.1}/7.5 & \textbf{20.1}/\textbf{10.7} & \textbf{24.3}/13.8 & \textbf{27.6}/17.1 \\ \hline
			\bottomrule
		\end{tabular}
\end{table*}
	\subsection{Datasets}
	Our training dataset consists of 219 hours of artificially reverberated and mixed utterances that sampled randomly from WSJ1 \cite{wsj1}. Four different mixture types described in \cite{yoshioka2018multi} are included in the training set. To generate each training mixture, we randomly pick one or two speakers from  WSJ1 
	and convolve each with a 7 channel room impulse response (RIR) simulated with the image method \cite{imagemethod}. The reverberated signals are then rescaled and mixed with a source energy ratio  between -5 and 5 dB. In addition, we add simulated isotropic noise \cite{habets2007generating} with a 0--10 dB signal to noise ratio. The average overlap ratio of the training set is around 50\%. 
	
	LibriCSS is used for evaluation \cite{chen2020continuous}. 
	The dataset has 10 hours of seven-channel recordings of mixed and concatenated LibriSpeech test utterances. The recordings were made by playing back the mixed audio in a meeting room. 
	Two evaluation schemes are used: utterance-wise evaluation and continuous input evaluation. 
	In the former evaluation, the long-form recordings are segmented into individual utterances by using ground-truth time marks to evaluate the pure separation performance. 
	In the contuous input evaluation, systems have to deal with the unsegmented recordings and thus CSS is needed. 
	






	
	\subsection{Implementation details}
	We use BLSTM and Transformers as our baseline speech separation models.
	The BLSTM model has three BLSTM layers with 1024 input dimensions and 512 hidden dimensions, resulting in 21.80M parameters. There are three masks, two for speakers and one for noise. The noise mask is used to enhance the beamforming \cite{Yoshioka2018Unmix}.
	We use three sigmoid projection layers to estimate each mask.  Transformer-base and Transformer-large models with 21.90M and 58.33M parameters are our two Transformer-based baselines. The Transformer-base model consists of 16 Transformer encoder layers with 4 attention heads, 256 attention dimensions and 2048 FFN dimensions. The Transformer-large model consists of 18 Transformer encoder layers with 8 attention heads, 512 attention dimensions and 2048 FFN dimensions.
	
	As with the Transformer baseline models, we experiment with two Conformer-based models, 
	Conformer-base and Conformer-large. They have 22.07M and 58.72M parameters, respectively. The Conformer-base model consists of 16 Conformer encoder layers with 4 attention heads, 256 attention dimensions and 1024 FFN dimensions. The Conformer-large model consists of 18 Conformer encoder layers with 8 attention heads, 512 attention dimensions and 1024 FFN dimensions. 
	Both Conformer and Transformer are trained with the AdamW optimizer \cite{loshchilov2018decoupled}, where the weight decay is set to 1e-2.
	We set the learning rate to 1e-4 and use a warm-up learning schedule with a linear decay, in which the warmp-up step is 10,000 and the training step is 260,000. 
	
	We use two ASR models to evaluate the speech separation accuracy. One is the ASR model used in the original LibriCSS publication \cite{chen2020continuous}, which is a hybrid system using a BLSTM acoustic model and a 4-gram language model. The other one is one of the best open source end-to-end Transformer  based ASR models \cite{wang2019semantic}, which achieves 2.08\% and 4.95\% word error rates (WERs) for LibriSpeech test-clean and test-other, respectively. Following \cite{chen2020continuous}, we generate the separated speech signals with  spectral masking and mask-based adaptive minimum variance distortionless response (MVDR) beamforming for the single-channel and seven-channel cases, respectively. 
	For a fair comparison, we follow the LibriCSS setting for chunk-wise CSS processing, where , ,  are set to 1.2s, 0.8s, 0.4s respectively.
	


	\subsection{Results for  utterance wise evaluation}
	
	Table~\ref{tab:7ch_utt_result} shows the WER of the utterance wise evaluation for the seven-channel and single-channel settings. 
	Our Conformer models achieved state-of-the-art results. Compared with BLSTM,  Conformer-base
	yielded substantial WER gains for the 7-channel setting.
The fact that 
	the Conformer-base model outperformed Transformer-base for almost all the settings indicates  Conformer's superior local modeling capability. Also, the larger models achieved better performance in the highly overlapped settings. 
As regards the single-channel case, 
	while the overall WERs were higher, 
the trend was consistent between the single- and multi-channel cases, except for the non-overlap scenario. With the seven channel input, all models showed similar performance for 0S and 0L.
	On the other hand, when only one channel was used,  
	the self-attention models were markedly better. This could indicate that the seven-channel features contain sufficiently rich information for simpler networks to do the beamforming well. Meanwhile, the information in the single-channel signal is quite limited, requiring a more advanced structure.


	


	


	
	\subsection{Results for continuous input evaluation}
	
	\begin{table*}[!t]
		\centering
		
		\caption{ Continuous speech separation evaluation for seven-channel and single-channel settings. }
		\label{tab:7ch_cont_result}
		
		\begin{tabular}{l|cccccc}
			\toprule \hline
			\multirow{2}{*}{\textbf{System}} &
			\multicolumn{6}{c}{\textbf{Overlap ratio in \%}} \\ &
			0S & 0L & 10 & 20 & 30 & 40   \\ 
			\hline
			No separation \cite{chen2020continuous} & 15.4/12.7 & 11.5/5.7 & 21.7/17.6 & 27.0/24.4 & 34.3/30.9 & 40.5/37.5 \\\hline
			&   \multicolumn{6}{c}{Seven-channel Evaluation} \\ 
			\hline
BLSTM & 11.4/6.0 & \textbf{8.4}/4.1 & 13.1/7.0 & 14.9/7.9 & 18.7/11.5 & 20.5/12.3 \\ 
Transformer-base & 12.0/5.6 & 9.1/4.4 & 13.4/6.2 & 14.4/6.8 & 18.5/9.7 & 19.9/10.3 \\					
			Transformer-large & \textbf{10.9}/5.4 & 8.8/\textbf{4.0} & \textbf{12.6}/6.0 & 13.6/\textbf{6.7} & \textbf{17.2}/9.3 & \textbf{18.9}/10.2 \\
			
			Conformer-base & 11.1/5.6 & 8.7/\textbf{4.0} & 12.8/6.1 & 13.8/\textbf{6.7} & 17.6/9.4 & 19.6/10.4 \\
			Conformer-large & 11.0/\textbf{5.2} & 8.7/\textbf{4.0} & \textbf{12.6}/\textbf{5.8} & \textbf{13.5}/6.8 & 17.6/\textbf{9.0} & 19.6/\textbf{10.0} \\ 
Conformer-base & 11.4/5.4 & 8.7/4.1 & 13.2/6.2 & 13.6/6.7 & 17.8/9.5 & 20.0/10.8 \\
			Conformer-large &  11.0/5.2 & 8.8/4.1 & 12.9/5.8 & 13.7/6.7 & 17.5/9.4 & 19.8/10.6\\ \hline
			&   \multicolumn{6}{c}{Single-channel Evaluation}  \\ \hline
			
			BLSTM &  19.1/11.7 & 16.1/9.7 & 22.1/14.5 & 27.4/19.1 & 33.0/25.9 & 37.6/30.1 \\
			
Transformer-base & 13.8/7.1 & \textbf{11.5}/6.6 & 16.7/9.6 & 20.8/13.3 & 26.7/18.6 & 31.0/21.6 \\
			Transformer-large & \textbf{13.0}/7.2 & 12.3/6.9 & \textbf{15.8}/9.5 & \textbf{19.8}/\textbf{12.2} & \textbf{25.3}/16.9 & \textbf{28.6}/\textbf{19.3} \\
			
			Conformer-base & 14.1/7.7 & 13.0/7.1 & 17.4/10.6 & 21.9/13.7 & 27.4/18.7 & 32.0/22.4 \\
			Conformer-large & 13.3/\textbf{6.9} & 11.7/\textbf{6.1} & 16.3/\textbf{9.1} & 20.7/12.5 & 25.6/\textbf{16.7} & 29.3/\textbf{19.3} \\ \hline
			
			\bottomrule
		\end{tabular}
\end{table*}
	


	Table~\ref{tab:7ch_cont_result} shows the continuous input evaluation results. 
The Conformer and Transformer models performed consistently better than BLSTM, but their performance gap became smaller in the large overlap test-set. The relative WER gains obtained with 
	Conformer-base over BLSTM were 4 and 15 for the hybrid and transducer ASR sytems, respectively, which were smaller than those obtained for the utterance-wise evaluation. 
	A possible explanation is that the self-attention based methods are good at using global information while the chunk-wise processing limits teh use of the context information. 
	
	It is noteworthy that 0S results were much worse than those of 0L only in the continuous evaluation, which is consistent with the previous report \cite{chen2020continuous}.
	The 0S dataset contains much more quick speaker turn changes, imposing a challenge for both speech separation and ASR.  
	The self-attention-based models showed clear improvement over BLSTM, indicating that they are also helpful for dealing with turn-takings in natural conversations. 
	


	Table~\ref{tab:7ch_cont_result} also shows that 
	the Conformer models using 
	longer context information did not result in lower WERs especially in the large overlap ratio settings. 
	Two factors may have contributed to the performance degradation. 
	1) The unexpected noise may have been introduced from the use of the longer history, which may contain more speakers' voices. 2) Also, we did not consider the overlap regions of the adjacent windows during training, possibly making the training/testing gap greater and resulting in sub-optimal performance. We leave the training with overlap regions for the future work. 
	
	




	






	




	


	
	
	\subsection{Results on large scale real meetings}
	To further verify the effectiveness of our method, we further conduct an experiment on an internal real conversation corpus which consists of 15.8 hours of single channel recordings of daily group discussions, noted as the Real Conversation dataset. In this dataset, the per-meeting speaker number ranges from 3 to 22. We applied a modified version of the conversation transcription system of \cite{yoshioka2019advances}, where a large scale trained speech recognizer and speaker embedding extractor were included, 
to obtain  speaker attributed transcriptions.


	Compared with LibriCSS, those real meetings are significantly more complex with respect to the acoustics, linguistics, and inter-speaker dynamics. To deal with the real data challenges, 
	three improvements were made. Firstly, we increased the training data amount to 1500 hours.
	Additional clean speech samples were taken from a Microsoft internal corpus and they were mixed with the simulation setup as Section 3.1. 
	Secondly, the separation network sometimes generated a low volume residual signal from the redundant output channel for single speaker regions, which increased the word insertion errors. To mitigate this, we introduced a merging scheme, where the two channel outputs were merged when a single active speaker was judged to be present. 
The merger was triggered when only one masked channel had a significantly large energy. Lastly, to reduce the distortion introduced by the masking operation, we used single speaker signals corruped by background noise as a training target. This allowed the separation network to focus only on the separation task and leave the noise to the ASR model. 
	The WER and speaker attributed WER (SA-WER) were used for evaluation, where the latter assesses the combined quality of speech transcription and  speaker diarization \cite{yoshioka2019advances}.
	
	
	\begin{table}[!h]
		\centering
		\caption{Continuous evaluation on a real meeting dataset.}
		\label{tab:prod}


\begin{tabular}{l|c|cc}
			\toprule
			\textbf{system}& Data & WERR & SA-WERR   \\ 	
			\midrule
			Original  & N/A & 0 & 0 \\
			BLSTM & 219hr & -6.4\% & -18.8\% \\
			Conformer-base & 219hr & -7.2\% & -6.3\% \\
			Conformer-large& 219hr & -2.5 \% & 1.9 \% \\
			Conformer-base &1500hr & 9.5\% & 8.8\% \\
			Conformer-base-merge& 1500hr & 8.4\% & 10.13\% \\
			Conformer-base-merge-nlabel& 1500hr & 11.8\% & 13.7\% \\
			Conformer-large-merge-nlabel & 1500hr & 8.08\% & 18.4\% \\
			\bottomrule
		\end{tabular}
		
	\end{table}	
	
	
	Table  \ref{tab:prod} shows the WER and SA-WER reduction rates. 
	With the three improvements described above, the proposed model reduced the WER and SA-WER by   and  relative, respectively, compared with a system without the separation front-end. Although the BLSTM based network improved the recognition result for the LibriCSS dataset especially for the high overlap ratio settings, it largely degraded the speech recognition and speaker diarization performance on the Real Conversation dataset. Because the speech overlap happens only sporadially in real conversations, it is important for the separation model not to hurt the performance for less overlap cases. 
Thanks to the better modeling capacity, the Conformer based models significantly mitigates the performance degradation. In addition, it can be seen that each introduced step brought about consistent improvement for both performance metrics. 


	


	
	\section{Conclusion}
	In this work, we investigated the use of Conformer for continuous speech separation. The experimental results showed that it  outperformed  RNN-based models for both utterance-wise evaluation and continuous input evaluation. 
	The superiority of Conformer to Transformer was also observed. 
	This work is also the first to report 
	substantial WER and SA-WER gains from the speech separation in a single-channel real meeting transcription task. 
	The results indicate the usefulness of appropriately utilizing context information in the speech separation. 
	
	


	
\bibliographystyle{IEEEbib}
	\bibliography{strings,refs}
	
\end{document}