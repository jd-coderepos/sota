\section{Introduction}
A generative adversarial network (GAN)~\citep{goodfellow2014generative} is a popular approach for generative modeling. GANs have achieved remarkable performance in various domains such as image~\citep{brock2018large,karras2019style,karras2021alias}, audio~\citep{kumar2019melgan,donahue2019adversarial,kong2020hifi}, and video~\citep{tulyakov2018mocogan,hao2021gancraft}. The aim of GAN is to learn a target probability measure via a neural network, called a generator. To achieve this, a discriminator is introduced, and the generator and discriminator are optimized in a minimax way.


Here, we pose the question of whether GAN optimization actually makes the generator distribution close to the target distribution. For example, likelihood-based generative models such as variational autoencoders~\citep{kingma2013auto,higgins2017beta,zhao2019infovae,takida2022sq-vae}, normalizing flows~\citep{tabak2010density,tabak2013family,rezende2015variational}, and denoising diffusion probabilistic models~\citep{ho2020denoising,song2020denoising,nichol2021improved} are optimized via the principle of minimizing the exact Kullback--Leibler divergence or its upper bound~\citep{jordan1999introduction}. For GANs, in contrast, it is known that solving the minimization problem with the optimal discriminator is equivalent to minimizing a specific dissimilarity~\citep{goodfellow2014generative,sebastian2016f,lim2017geometric,miyato2018spectral}. Furthermore, GANs have been analyzed on the basis of the optimal discriminator assumption~\citep{chu2020Smoothness}.
However, real-world GAN optimizations hardly achieve maximization~\citep{fiez2022minimax}, and analysis of GAN optimization without that assumption is still challenging.

To analyze and investigate GAN optimization without the optimality assumption, there are approaches from the perspectives of training convergence~\citep{mescheder2017the,nagarajan2017gradient,mescheder2018training,sanjabi2018convergence,xu2020understanding}, loss landscape around the saddle points of minimax problems~\citep{farnia2020gans,berard2019closer}, and the smoothness of optimization~\citep{fiez2022minimax}. Although these studies have been insightful for GAN stabilization, there has been little discussion on whether trained discriminators indeed provide generator optimization with gradients that reduce dissimilarities.

\begin{table*}
  \centering
  \small
  \caption{
  Common GAN losses do not simultaneously satisfy all the sufficient conditions given in Theorem~\ref{th:main}. Thus, we propose the SAN to address one of the conditions, \textit{direction optimality}. Even if a direction  is the maximizer of the inner problems , it does not satisfy \textit{direction optimality} except in Wasserstein GAN (see Sec.~\ref{sec:proposed_method}).
\yuhta{The results in Appx.~\ref{sec:app_metrizable_conditions} empirically demonstrate that a discriminator trained on Wasserstein GAN tends not to satisfy \textit{separability}.}
\yuhta{The last condition of \textit{injectivity} depends  on the discriminator implementation (see Appx.~\ref{sec:app_metrizable_conditions} for empirical verification).}
  }
\begin{tabular}{cccc}
    \bhline{0.8pt}
         & Direction optimality & Separability & Injectivity \\
        \bhline{0.8pt}
        Wassertein GAN & \cmark & {\color{gray}weak} &  \\
        GAN (Hinge, Saturating, Non-saturating)  & {\color{black}\xmark} & {\color{gray}\cmark} &  \\
        SAN (Hinge, Saturating, Non-saturating)  & {\color{myred}\cmark} & {\color{gray}\cmark} &  \\
        \bhline{0.8pt}
  \end{tabular}
  \label{tb:if_gans_are_metrizable}
\end{table*}

In this paper, we provide a novel perspective on GAN optimization, which helps us to consider whether a discriminator is \textit{metrizable}.
\begin{definition}[Metrizable discriminator]
Let  and  be measures. Given an objective function  for , a discriminator  is - or -\textit{metrizable} for  and , if  is minimized only with  for a certain distance on measures, .
\end{definition}
To evaluate the dissimilarity with a given GAN minimization problem , we are interested in other conditions besides the discriminator's optimality.
Hence, we propose \textit{metrizable conditions}, namely, \textit{direction optimality}, \textit{separability}, and \textit{injectivity}, that induce -\textit{metrizable} discriminator. To achieve this, we first introduce a divergence, called functional mean divergence (FM), in Sec.~\ref{sec:def_fm}. We connect the FM with the minimization objective function of Wasserstein GAN. Then, we obtain the \textit{metrizable conditions} for Wasserstein GAN by investigating \yuhta{Question~\ref{qt:intuition}}.
We provide an answer to this question in Sec.~\ref{sec:fmh_is_distance} by relating the FM to the concept of sliced optimal transport~\citep{bonneel2015sliced,kolouri2019generalized}. Then, in Sec.~\ref{sec:main_theorem}, we formalize the proposed conditions for Wasserstein GAN and further extend the result to generic GAN.
\begin{question}
    \yuhta{Under what conditions is FM a distance?}
    \label{qt:intuition}
\end{question}

\yuhta{Based on} the derived \textit{metrizable conditions}, we propose the Slicing Adversarial Network (SAN) in Sec.~\ref{sec:proposed_method}. As seen in Table~\ref{tb:if_gans_are_metrizable}, we find that optimal discriminators for \yuhta{most existing GANs} do not satisfy \textit{direction optimality}. Hence, we develop a modification scheme for GAN maximization problems to enforce \textit{direction optimality} on our discriminator. Owing to the scheme's simplicity, GANs can easily be converted to SANs.
\yuhta{We} conduct experiments to verify our perspective and demonstrate that SANs are superior to GANs in certain generation tasks on synthetic and image datasets.
\yuhta{In particular, we confirmed a SAN improves state-of-the-art FID for conditional generation with StyleGAN-XL~\citep{sauer2022stylegan} on ImageNet 256256 despite the simple modifications.}





\section{Preliminaries}
\label{sec:background}

\subsection{Notations}
We consider a sample space  and a latent space . Let  be the set of all probability measures on , and let  denote the  space for functions .
Let  (or ) represent a probability measure with probability density function  (or ).
\yuhta{Denote .}
We use the notation of the pushforward operator , which is defined as  for  with a function  and a probability measure .
We denote the Euclidean inner product by .
Lastly,  denotes a normalized vector.


\subsection{Problem Formulation in GANs}
\label{ssec:problem_formulation}

Assume that we have data obtained by discrete sampling from a target probability distribution .
Then, we introduce a trainable generator function  with parameter  to model a trainable probability measure as \tkd{ with }
The aim of generative modeling here is to learn  so that it approximates the target measure as .
 
For generative modeling in GAN, we introduce the notion of a discriminator .
We formulate the GAN's optimization problem as a two-player game between the generator and discriminator with \yuhtb{} and , as follows:

Regarding the choices of  and , there are GAN variants~\citep{goodfellow2014generative,sebastian2016f,arjovsky2017wasserstein} that lead to different dissimilarities between  and  with the maximizer .
\yuhta{In this paper, we use a representation of the discriminator in an inner-product form}

where  and .
\yuhta{The form is naturally represented by a neural network}\footnote{In practice, the discriminator  is implemented as in \beqref{eq:critic_general}, e.g.,  with nonlinear layers , , and their weights }. 


\subsection{Wasserstein Distance and Its Use for GANs}
\label{ssec:wasserstein_distance_gan}

\yuhta{We consider the  Wasserstein- distance~\citep{villani2009optimal} between probability measures  and }

where  \yuhta{ and}  is the set of all coupling measures whose marginal distributions are  and .
The idea of Wasserstein GAN is to learn a generator by minimizing the Wasserstein-1 distance between  and . 
For this goal, one can adopt
the Kantorovichâ€“-Rubinstein (KR) duality representation to rewrite Eq.~\beqref{eq:def_wasserstein_distance} and obtain the following optimization problem:

\yuhta{where  denotes the class of 1-Lipschitz functions.}
\yuhta{In contrast}, we formulate an optimization problem for the generator as minimization of the right side of Eq.~\beqref{eq:max_wgan} w.r.t. the generator parameter:



\subsection{Sliced Optimal Transport}
\label{ssec:sliced_optimal_transport}


The Wasserstein distance is highly intractable when the dimension  is large~\citep{arjovsky2017wasserstein}. 
However, it is well known that \textit{sliced optimal transport} can be applied to break this intractability, by projecting the data on a one-dimensional space.
That is, for the  case, the Wasserstein distance has a closed-form solution:

\yuhta{where  denotes the quantile function for .}
The closed-form solution for a one-dimensional space prompted the emergence of the concept of sliced optimal transport.

In the original sliced Wasserstein distance (SW)~\citep{bonneel2015sliced}, a probability density function   on the data space  is mapped to a probability density function of  by the standard Radon transform~\citep{natterer2001mathematics,helgason2011radon} as , where  is the Dirac delta function and  is a direction. The sliced Wasserstein distance between  and  is defined as .
Intuitively, the idea behind this distance is to decompose high-dimensional distributions into an infinite number of pairs of tractable distributions by linear projections.


Various extensions of the sliced Wasserstein distance have been proposed~\citep{kolouri2019generalized,deshpande2019max,nguyen2021distributional}.
Here, we review an extension called augmented sliced Wasserstein distance (ASW)~\citep{chen2022augmented}. Given a measurable injective function , the distance is obtained via the spatial Radon transform (SRT), which is defined for any  and , as follows:

The ASW- is then obtained via the SRT in the same fashion as the standard \yuhta{SW}:

\yuhta{Equation~\beqref{eq:closed_form_W1}} can be used to evaluate the integrand in Eq.~\beqref{eq:def_asw}, which is usually evaluated via approximated quantile functions with sorted finite samples from  and .



\section{Formulation of Question~\ref{qt:intuition}}
\label{sec:def_fm}

Next, we introduce a divergence, the functional mean divergence, FM or FM, which is defined for a given functional space or function. Minimization of the FM can be formulated as an optimization problem involving , and we cast Question~\ref{qt:intuition} in this context.
In Sec.~\ref{sec:fmh_is_distance}, we provide an answer to that question, which in turn provides the \textit{metrizable conditions} with  in Sec.~\ref{sec:main_theorem}. 


\subsection{Proposed Framework: Functional Mean Divergence}
\label{ssec:fsw1_distance}

We start by defining the FM with a given functional space.
\begin{definition}[Functional Mean Divergence (FM)]
We define a family of \yuhta{FMs} as

Further, we denote an instance in the family as , where .
\end{definition}
By definition, the FM family includes the integral probability metric (IPM)~\citep{alfred1997integral}, which includes the Wasserstein distance in KR form as a special case.
\begin{proposition}
\textit{
For , .
}


 \label{pr:fm_is_distance}
\end{proposition}
The FM is an extension of the IPM to deal with vector-valued functional spaces.
Although the FM with a properly selected functional space yields a distance between target distributions, the maximization in Eq.~\beqref{eq:def_max_FM} is generally hard to achieve. Instead, we use the following metric, which is defined for a given function.
\begin{definition}[Functional Mean Divergence (FM)]
Given a functional space , we define a family of \yuhta{FMs} as

Further, we denote an instance in the family as , where .
\end{definition}
\yuhta{We are interested in Question~\ref{qt:formulation}, which is a mathematical formulation of Question~\ref{qt:intuition}.}
We give an answer to this question in Sec.~\ref{sec:fmh_is_distance}. Because optimization of the FM is related to  in Sec.~\ref{ssec:direction_optimality}, \yuhta{the conditions} in Question~\ref{qt:formulation} enable us to derive -\textit{metrizable conditions} in Sec.~\ref{sec:main_theorem}.
\begin{question}
\yuhta{Under what conditions for  are  distances?}
\label{qt:formulation}
\end{question}


\subsection{\textit{Direction Optimality} to Connect FM and }
\label{ssec:direction_optimality}

Optimization of the FM with a given  returns us to an optimization problem involving .
\begin{proposition}[\textit{Direction optimality} connects FM and ]
\textit{
Let  be on . For any , minimization of  is equivalent to optimization of . Thus, 

where  is the optimal solution (direction) given as follows:

} \label{pr:equivalence_FM_wgan}
\end{proposition}
Recall that we formulated the discriminator in the inner-product form~\beqref{eq:critic_general}, which is aligned with this proposition.
We refer to the condition for the direction in Eq.~\beqref{eq:optimal_direciton} as \textit{direction optimality}.
It is obvious here that, given function , the maximizer  becomes . 

From the discussion in this section, Proposition~\ref{pr:equivalence_FM_wgan} supports the notion that investigation of Question~\ref{qt:formulation} will reveal the -\textit{metrizable conditions}.




\begin{figure}[t]
\begin{minipage}[c]{0.45\columnwidth}
\centering
\includegraphics[width=.80\textwidth]{figure/outline_sec4.pdf}
    \caption{
    Outline of Sec.~\ref{sec:fmh_is_distance}. Proposition~\ref{pr:fm_is_distance_if_injective_and_separable} is a major step toward our main theorem.
    }
    \label{fig:outline_sec4}
\end{minipage}
\hspace{0.02\columnwidth}
\begin{minipage}[c]{0.51\columnwidth}
\centering
\includegraphics[width=.90\textwidth]{figure/separability.pdf}
   \caption{
   Illustration of Definition~\ref{def:separable_fswd}. (a) A separable function  for  and  satisfies  for . (b) For non-separable ,  depends on , which necessiates of evaluation of the sign to calculate  by Eq.~\beqref{eq:closed_form_W1} (see Remark~\ref{rm:intuition_seperability}).
   }
   \label{fig:separability}
\end{minipage}
\end{figure}


\section{\yuhta{Conditions for Metrizability: Analysis by Max-ASW}}
\label{sec:fmh_is_distance}

In this section, we provide an answer to Question~\ref{qt:formulation}.


\subsection{Strategy for Answering Question~\ref{qt:formulation}}
\label{ssec:strategy}

We consider the conditions of  for Question~\ref{qt:formulation} in the context of sliced optimal transport. To this end, we define a variant of sliced optimal transport, called maximum augmented sliced Wasserstein divergence (max-ASW) in Definition~\ref{def:max_asw}.
In Sec.~\ref{ssec:separability}, we first introduce a condition, called \textit{separable} condition, where divergences included in the FM family are also included in the max-ASW family.
In Sec.~\ref{ssec:injectivity}, we further introduce a condition, called \textit{injective} condition, where the max-ASW is a distance.
Finally, imposing these conditions on  brings us the desired conditions (see Fig.~\ref{fig:outline_sec4} for the discussion flow).
\begin{definition}[Maximum Augmented Sliced Wasserstein Divergence (max-ASW)]
    Given a functional space , we define a family of \yuhta{max-ASW}s as
    
    Further, we denote an instance of \yuhta{the family} as , where .
    \label{def:max_asw}
\end{definition}
Note that although the formulation in the definition includes the SRT, similarly to the ASW in Eq.~\beqref{eq:def_asw}, they differ in terms of the method of direction sampling (). 

\subsection{\textit{Seperability} for Equivalence of FM and Max-ASW}
\label{ssec:separability}

We introduce a property for the function , called \textit{separability} (refer to Remark~\ref{rm:intuition_seperability} for intuitive explanation).
\begin{definition}[Separable]
Given , let  be on , and let  be the cumulative distribution function of .
If  satisfies  for any ,  is separable for those probability measures.
We denote the class of all these separable functions for them as .
\label{def:separable_fswd}
\end{definition}
Under this definition, \textit{separable} functions connect the FM and the max-ASW as follows.
\begin{lemma}
\textit{
Given , every  satisfies .
} \label{lm:fm_included_in_max_asw}
\end{lemma}
\begin{remark}
For a general function ,  is not necessarily included in the max-ASW family, i.e.,  for general . Fig.~\ref{fig:separability} intuitively illustrates why \textit{separability} is crucial.
Given , calculation of the max-ASW via \yuhtb{Eq.~~\beqref{eq:closed_form_W1}} generally involves evaluation of the sign of the difference between the quantile functions. Intuitively, the equivalence between the FM and max-ASW distances holds if the sign is always positive regardless of ; otherwise, the sign's dependence on  breaks the equivalence. 
\label{rm:intuition_seperability}
\end{remark}



\subsection{\textit{Injectivity} for Max-ASW To Be A Distance}
\label{ssec:injectivity}

Imposing \textit{injectivity} on  guarantees that the induced max-ASW is indeed a distance.
\begin{lemma}
\textit{
Every  is a distance, where  indicates a class of all the injective functions in .
} \label{lm:max_asw_is_distance}
\end{lemma}
According to Lemmas~\ref{lm:fm_included_in_max_asw} and \ref{lm:max_asw_is_distance},  with a \textit{separable} and \textit{injective}  is indeed a distance since it is included in the family of max-ASW \textit{distances}.
\yuhta{With Proposition~\ref{pr:fm_is_distance_if_injective_and_separable}, we have now one of the answers to Question~\ref{qt:formulation}, which hints us our main theorem.}
\begin{proposition}
\textit{
Given , every  is indeed a distance.
} \label{pr:fm_is_distance_if_injective_and_separable}
\end{proposition}


\section{GAN Training Indeed Minimizes Distance?}
\label{sec:main_theorem}

\yuhta{W}e present Theorem~\ref{th:main}, which is our main theoretical result and gives sufficient conditions for the discriminator to be -metrizable. Then, we explain certain implications of the theorem.
\subsection{Metrizable Discriminator in GAN}
\label{ssec:metrizable_discriminator}
We directly apply the discussion in the previous section to , and by extending the result for Wasserstein GAN to a general GAN, we derive the main result.


First, a simple combination of Propositions~\ref{pr:equivalence_FM_wgan} and \ref{pr:fm_is_distance_if_injective_and_separable} yields the following lemma.

\begin{lemma}[-metrizable]
Given  \yuhta{with }, let  be . Then  is -metrizable. \label{lm:metrazability_wgan}
\end{lemma}
Lemma~\ref{lm:metrazability_wgan} provides the conditions for the discriminator to be -\textit{metrizable}.  

Next, we generalize this result to more generic minimization problems. The scope is minimization problems of general GANs that are formalized in the form  with .
We use the gradient of such minimization problems w.r.t. :

\yuhtb{where  is the derivative of  as listed in Table~\ref{tb:maximization_weighting}.}
By ignoring a scaling factor, Eq.~\beqref{eq:gradient_minJ} can be regarded as a gradient of \yuhtb{}, where \yuhtb{ is defined via }. To examine whether updating the generator with the gradient in Eq.~\beqref{eq:gradient_minJ} can minimize a certain distance between  and , we introduce the following lemma.
\begin{lemma}
\textit{
For any \yuhtb{} and a distance for probability measures ,  indicates a distance between  and .
} \label{lm:weighted_distance}
\end{lemma}
By leveraging Lemma~\ref{lm:weighted_distance} and applying Propositions~\ref{pr:equivalence_FM_wgan} and \ref{pr:fm_is_distance_if_injective_and_separable} to  and , we finally derive the -\textit{metrizable conditions} for  and , which is our main result.
\begin{theorem}[-Metrizability]
\textit{
Given a functional  with , let  and  satisfy the following conditions:
\begin{itemize}
  \setlength{\parskip}{0cm}
  \setlength{\itemsep}{0cm}
    \item
    \textbf{(Direction optimality)}
    \yuhtb{,}\item
    \textbf{(Separability)}
     is separable \yuhtb{for  and ,}
\item
    \textbf{(Injectivity)}
     is an injective function.
\end{itemize}
Then  is -metrizable  for  and .
}
%
 \label{th:main}
\end{theorem}

We refer to the conditions in Theorem~\ref{th:main} as \textit{metrizable conditions}.
According to the theorem, the discriminator  can serve as a distance between the generator and target distributions even if it is not the optimal solution to the original maximization problem . 

\subsection{Implications of Theorem~\ref{th:main}}
We are interested in the question of whether discriminators in existing GANs can satisfy the \textit{metrizable conditions}.
We summarize our observations in Table~\ref{tb:if_gans_are_metrizable}.

First, as explained in the next section, most existing GANs besides Wasserstein GAN do not satisfy \textit{direction optimality} with the maximizer  of . This fact inspires us to develop novel maximization objectives (see Sec.~\ref{sec:proposed_method}).
Second, it is generally hard to make the function  rigorously satisfy \textit{separability}, or verify that it is really satisfied. In \yuhta{Appx.~\ref{sec:app_metrizable_conditions}}, a simple experiment demonstrates that Wasserstein GAN tends to fail to learn separable functions.
Third, the property of \textit{injectivity} largely depends on the discriminator design. There are various ways to impose \textit{injectivity} on the discriminator. One way is to implement the discriminator with an invertible neural network. Although this topic has been actively studied~\citep{behrmann2019invertible,karami2019invertible,song2019mintnet}, such networks have higher computational costs for training~\citep{chen2022augmented}.
Another way is to add regularization terms to the maximization problem. For example, a gradient penalty (GP)~\citep{gulrajani2017improved} can promote injectivity by explicitly regularizing the discriminator's gradient. In contrast, simple removal of operators that can destroy \textit{injectivity}, such as ReLU activation, can implicitly overcome this issue. We empirically verify this discussion in Sec.~\ref{ssec:exp_synthetic} \yuhta{and Appx.~\ref{sec:app_metrizable_conditions}}.


\begin{figure}[t]
\centering
   \includegraphics[width=.98\textwidth] {figure/from_gan_to_san.pdf}
   \caption{
   \yuhtb{
   Converting GAN to SAN involves only simple modifications to discriminators.
   }
   }
   \label{fig:from_gan_to_san}
\vskip -0.1in
\end{figure}

\section{Slicing Adversarial Network}
\label{sec:proposed_method}
This section describes our proposed model, the Slicing Adversarial Network (SAN) to achieve the \textit{direction optimality} in Theorem~\ref{th:main}.
We develop the SAN by modifying the maximization problem on  to guarantee that the optimal solution  achieves \textit{direction optimality}. The proposed modification scheme is applicable to most existing GAN objectives and does not necessitate additional \yuhtb{exhaustive hyperparameter tuning} and computational complexity.


As mentioned in Sec.~\ref{ssec:metrizable_discriminator}, given a function , maximization problems in most GANs (besides Wasserstein GAN) cannot achieve \textit{direction optimality} with the maximum solution of , as reported in Table~\ref{tb:if_gans_are_metrizable}. 
We use hinge GAN as an example to illustrate this claim. The objective function to be maximized in hinge GAN is formulated as

Given , the maximizer  becomes , where  and  denote truncated distributions whose supports are restricted by conditioning  on  and , respectively. 
Because  is generally different from , the maximum solution does not satisfy \textit{direction optimality} for  and .

In line with the above discussion, we propose the following novel maximization problem:

where  indicates a stop-gradient operator and has zero partial derivatives. \yuhtb{We simply set  to 1 in our experiments.} The proposed modification scheme in Eq.~\beqref{eq:maximization_proposed} enables us to select any maximization objective for  with , while  enforces \textit{direction optimality} on .
\yuhta{
Hence, we can convert most GAN maximization problem to SAN problem using Eq.~\beqref{eq:maximization_proposed}.
}
\yuhtb{One can easily extend SAN to class conditional generation by introducing directions for respective classes (see Appx.~\ref{sec:app_conditional_san}).} 


\begin{table*}
  \centering
  \small
  \caption{Minimization problem and weighting function for direction optimization.}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{ccc}
    \bhline{0.8pt}
         & Minimization problem  & Weighting  \\
        \bhline{0.8pt}
        Wasserstein GAN / Hinge GAN &  & 1 \\
        Saturating GAN &  &  \\
        Non-saturating GAN &  &  \\
        \bhline{0.8pt}
  \end{tabular}
  \label{tb:maximization_weighting}
\end{table*}

One of SAN's strength is that the model can be trained with two small modifications to existing GAN implementations \yuhta{(see Fig.~\ref{fig:from_gan_to_san} and Appx.~\ref{sec:app_implementation})}.
The first is to implement the discriminator's last linear layer to be on a hypersphere as  with , where  is the parameter for the th layer. The second is to use the maximization problem defined in Eq.~\beqref{eq:maximization_proposed}.

\textbf{Comparison with other generative models.}~~
\yuhtb{First, SAN can be compared with GAN.
In GAN, other than Wasserstein GAN, the optimal discriminator is known to involve a dilemma: it induces a certain dissimilarity but leads to exploding and vanishing gradients~\citep{arjovsky2017wasserstein,lin2021spectral}.
In contrast, in SAN, the \textit{metrizable conditions} ensure that the optimal discriminator is not necessary to obtain a \textit{metrizable} discriminator.
Next, similarly to variants of the maximum SW~\citep{deshpande2019max,kolouri2019generalized}, the SAN's discriminator is interpreted as extracting features  and slicing them in the most distinctive direction . In the conventional methods, one-dimensional Wasserstein distances are approximated by sample sorting (see Remark~\ref{rm:intuition_seperability}). It has been reported that the generation performances get better when the batch size increases~\citep{nguyen2021distributional,chen2022augmented}. However, larger batch size leads to slower training speed, and the size is capped by memory constraints. In contrast, SANs remove the necessity of the approximation.}




\section{Experiments}
\label{sec:experiment}
We perform experiments with synthetic and image datasets (1) to verify our perspective on GANs as presented in Sec.~\ref{sec:main_theorem} in terms of \textit{direction optimality}, \textit{separability}, and \textit{injectivity}, and (2) to show the effectiveness of SAN against GAN.
For fair comparisons, we essentially use the same architectures in SAN and GAN. However, we modify the last linear layer of SAN's discriminators (see Sec.~\ref{sec:proposed_method}).


\begin{figure}[t]
\begin{minipage}[c]{0.55\columnwidth}
\centering
   \includegraphics[width=.83\textwidth]{figure/mog/mog_asgn_vs_gan.pdf}
   \vskip -0.1in
   \caption{
Comparison of the learned distributions (at 10,000 iterations) between GAN and SAN with various objectives.
\yuhta{In all cases, SANs cover all modes whereas mode collapse occurs in some GAN cases.}
   }
   \label{fig:exp_mog_asgn_vs_gan}
\end{minipage}
\hspace{0.02\columnwidth}
\begin{minipage}[c]{0.40\columnwidth}
\centering
   \includegraphics[width=.80\textwidth]{figure/mog/mog_inner_product.pdf}
   \vskip -0.1in
   \caption{
   Inner product of trained  and numerically estimated \textit{optimal direction} for  and  during training. The trained  were closer to the \textit{optimal direction} with SAN than with GAN.
   }
   \label{fig:exp_mog_inner_product}
\end{minipage}
\end{figure}



\begin{table}[t]
  \centering
  \caption{FID scores () on DCGAN.}
  \small
\begin{tabular}{l|cc|cc|cc}
    \bhline{0.8pt}
        \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{Hinge loss} & \multicolumn{2}{c|}{Saturating loss} & \multicolumn{2}{c}{Non-saturating loss}\\  \cline{2-7}
         & GAN & SAN & GAN & SAN & GAN & SAN \\
        \bhline{0.8pt}
        CIFAR10 & 24.07{\scriptsize0.56} & \textbf{20.23}{\scriptsize0.86} & 25.63{\scriptsize0.98} & \textbf{20.62}{\scriptsize0.94} & 24.90{\scriptsize0.21} & \textbf{20.51}{\scriptsize0.36}\\
        CelebA & 32.51{\scriptsize2.53} & \textbf{27.79}{\scriptsize1.60} & 37.33{\scriptsize1.02} & \textbf{28.16}{\scriptsize1.60} & 28.22{\scriptsize2.16} & \textbf{27.78}{\scriptsize4.59}\\
        \bhline{0.8pt}
  \end{tabular}
  \label{tb:results_dcgan}
\centering
  \caption{FID and IS results on CIFAR10 with the experimental setup of BigGAN~\citep{brock2018large}. Scores marked with  are results from our implementation, which is based on BigGAN author's PyTorch implementation. For reference, scores reported in their paper are put with .}
  \small
  \renewcommand{\arraystretch}{1.0}
  \begin{tabular}{l|cc|ccc}
    \bhline{0.8pt}
        \multirow{2}{*}{Metric} & \multicolumn{2}{c|}{Unconditional} & \multicolumn{3}{c}{Conditional} \\ \cline{2-6}
         & Hinge GAN & Hinge SAN & Hinge GAN& Hinge GAN & Hinge SAN \\
        \bhline{0.8pt}
        FID () & 17.16{\scriptsize1.34} & \textbf{14.45}{\scriptsize0.58} & 14.73 & 8.25{\scriptsize0.82} & \textbf{6.20}{\scriptsize0.27} \\
        IS () & 8.42{\scriptsize0.11} & \textbf{8.81}{\scriptsize0.04} & 9.22 & 9.05{\scriptsize0.05} & \textbf{9.16}{\scriptsize0.08} \\ 
        \bhline{0.8pt}
  \end{tabular}
  \label{tb:results_biggan}
\centering
  \caption{Numerical results for StyleGAN-XL~\citep{sauer2022stylegan} and StyleSAN-XL on ImageNet (256256). Scores marked with  are reported in their paper.}
  \small
  \renewcommand{\arraystretch}{1.0}
  \begin{tabular}{lcc}
    \bhline{0.8pt}
        Method & FID () & IS () \\
        \bhline{0.8pt}
        StyleGAN-XL & 2.30 & 265.12 \\
        StyleSAN-XL & \textbf{2.14} & \textbf{274.20} \\
        \bhline{0.8pt}
  \end{tabular}
  \vskip -0.1in
  \label{tb:results_stylegan_xl}
\end{table}

\subsection{Mixture of Gaussians}
\label{ssec:exp_synthetic}
\yuhta{To empirically investigate \textit{optimal direction}}, we conduct experiments on a mixture of Gaussian (MoG). \yuhta{Please refer to Appx.~\ref{sec:app_metrizable_conditions} for empirical verification of implications of Theorem~\ref{th:main} regarding \textit{separability} and \textit{injectivity}.} We use a two-dimensional sample space . The target MoG on  comprises eight isotropic Gaussians with variances  and means distributed evenly on a circle of radius . We use a 10-dimensional latent space  to model a generator measure.

We compare SAN and GAN with various objectives. As shown in Fig.~\ref{fig:exp_mog_asgn_vs_gan}, the generator measures trained with SAN cover all modes whereas mode collapse~\citep{srivastava2017veegan} occurs with hinge GAN and non-saturating GAN.
In addition, Fig.~\ref{fig:exp_mog_inner_product} shows a plot of the inner product of the learned direction  (or the normalized weight in GAN's last linear layer) and the estimated \textit{optimal direction} for  and .
Recall that there is no guarantee that a non-optimal direction  induces a distance.


\subsection{Image Generation}
\label{ssec:exp_image}
\yuhta{We apply SANs to several image generation tasks to show they scale beyond toy experiments.}


\textbf{DCGAN.}~~
We train SANs and GANs with various objective functions on CIFAR10~\citep{krizhevsky2009learning} and CelebA (128128)~\citep{liu2015deep}. We adopt the DCGAN architectures~\citep{radford2015unsupervised}, and for the discriminator, we apply spectral normalization~\citep{miyato2018spectral}. As reported in Table~\ref{tb:results_dcgan}, SANs outperform GANs in terms of the FID score in all cases.

\textbf{BigGAN.}~~
Next, we apply SAN to BigGAN~\citep{brock2018large} on CIFAR10. In this experiment, we calculate the Inception Score (IS)~\citep{salimans2016improved}, as well as the FID, to evaluate the sample quality by following the experiment in the original paper. As in Table~\ref{tb:results_biggan}, the adoption of SAN consistently improves the generation performance in terms of both metrics.

\textbf{StyleGAN-XL.}~~
Lastly, we apply our SAN training framework to StyleGAN-XL~\citep{sauer2022stylegan}, which is a state-of-the-art GAN-based generative model. We name the StyleGAN-XL model combined with our SAN training framework StyleSAN-XL. We train StyleSAN-XL on ImageNet (256256)~\citep{russakovsky2015imagenet} and report the FID and IS as with BigGAN. As shown in Table~\ref{tb:results_stylegan_xl}, SAN boosts even the generation performance of the state-of-the-art GAN-based generative model.
\anonymous{Our code for StyleSAN-XL is available at \url{https://github.com/sony/san}.}

\section{Conclusion}
We first proposed a novel perspective on GANs to derive sufficient conditions for the discriminator to serve as a distance between the data and generator probability measures. To this end, we introduced the FM and max-ASW families. By using a class of metrics that are included in both families, we derived the \textit{metrizable conditions} for Wasserstein GAN. We then extended the result to a general GAN. The derived conditions consist of \textit{direction optimality}, \textit{separability}, and \textit{injectivity}. 

By leveraging the theoretical results, we proposed SAN, in which a generator and discriminator are \yuhta{trained with a modified GAN training scheme}.
This model can impose \textit{direction optimality} on the discriminator \yuhta{despite the ease of modifications and the generality}. SANs experimentally outperformed GANs on synthetic and image datasets in terms of the sample quality and mode coverage.