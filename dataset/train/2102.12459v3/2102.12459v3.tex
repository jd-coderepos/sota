
\section{Experimental setup}
\label{sec:setup}

\paragraph{Datasets}
We evaluate our model on four standard NLP benchmarks.
\begin{itemize}
    \item \textsc{Enwik8}~\cite{hutter2012human} is a character-level language modeling dataset consisting of 100M tokens taken from Wikipedia. 
    The vocabulary size of this dataset about 200.
    We use the standard 90M/5M/5M splits as the training, dev and test sets, and report bits-per-character (BPC) as the evaluation metric.
\item \textsc{Wiki-103}~\cite{merity2016pointer} is a word-level language modeling dataset. The training data contains 100M tokens extracted from Wikipedia articles. 
    Following prior work, we use a vocabulary of 260K tokens, and adaptive embedding and softmax layers~\cite{grave2017efficient,baevski2018adaptive}.
\item \textsc{Billion word}~\cite{billionword} is one of the largest language modeling datasets containing 768M tokens for training.
    Unlike \textsc{Wiki-103} in which sentences in the same article are treated as consecutive inputs to model long context, the sentences in \textsc{billion word} are randomly shuffled.
    Following \citet{baevski2018adaptive}, we use a vocabulary of 800K tokens, adaptive embedding and softmax layers. 
    \item \textsc{IWSLT}'14 De$\rightarrow$En is a low-resource machine translation dataset consists of 170K translation pairs.
    We showcase SRU++ can be applied to other tasks such as translation.
    We follow the same setup of \citet{lin-etal-2020-autoregressive} and other previous work.
    The dataset uses a shared vocabulary of 14K BPE tokens.
\end{itemize}

\begin{table}[t!]
    \centering
    \begin{tabular}{lcr}
        \toprule
\bf Model & \bf Batch size $B\times M$ & \bf BPC $\downarrow$ \\
        \hline
        Trans-XL & 24$\times$512 & 1.06 \\
        SRU++ &  24$\times$512 & 1.03 \\
        SRU++ &  16$\times$768 & 1.02 \\
        \bottomrule
    \end{tabular}
    \caption{Test BPC of SRU++ and Transformer-XL on \textsc{Enwik8} dataset. We train SRU++ using the same setting as Transformer-XL base model. 
Numbers are smaller the better.
    $B$ is the number of sequence. $M$ is the unroll size (and additional context size).}
    \label{tab:enwik8_base}
\end{table}

\paragraph{Models}
All our language models are constructed with a word embedding layer, multiple layers of SRU++ and an output linear layer followed by softmax operation. 
We use single-head attention in each layer and 10 SRU++ layers for all our models.
We use the same dropout probability for all layers and tune this value according to the model size and the results on the dev set.
By default, we set the hidden dimension $d : d' = 4 : 1$.
We report additional analysis and tune this ratio for best results in Section~\ref{sec:results} and Appendix~\ref{sec:appendix:more_analyses}.


For simplicity, SRU++ does not use recent techniques that are shown useful to Transformer such as multi-head attention, compressed memory~\cite{Rae2020Compressive}, relative position~\cite{shaw-etal-2018-self,press2020shortformer}, nearest-neighbor interpolation~\cite{Khandelwal2020Generalization} and attention variants to handle very long context~\cite{sukhbaatar-etal-2019-adaptive,roy2020efficient}.

We compare with previous Transformer models that incorporate one or several these techniques.
However, we do not compare with results that use additional data or dynamic evaluation~\cite{Graves13,pmlr-v80-krause18a},
for a fair comparison between all models.


\begin{table}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Param & \bf BPC $\downarrow$ & \bf GPU hrs$\,\downarrow$\\
    \hline
    Trans-XL & 41M & 1.06 & 356$\ \,$\\
    SHA-LSTM & 54M & 1.07 & 28$^\dagger$\\
    \hline
    $k=1$ & \multirow{5}{*}{42M} & 1.022 & 37$^\dagger$\\
    $k=2$ & & 1.025 & 29$^\dagger$\\
    $k=5$ & & 1.032 & 24$^\dagger$\\
    $k=10$ & & 1.033 & 22$^\dagger$\\
    No attention & & 1.190 & 20$^\dagger$\\
    \bottomrule
    \end{tabular}
    \caption{Results of SRU++ on \textsc{enwik8} by enabling attention every $k$ layers. We adjust the hidden size so the number of parameters are comparable. $\dagger$ indicates mixed precision training.}
    \label{table:analyze_attention}
\end{table}

\paragraph{Optimization}
We use RAdam~\cite{liu2019radam} with the default $\beta$ values as our optimizer.
RAdam is a variant of Adam optimizer~\cite{Kingma:14adam} that is reported less sensitive to the choice of learning rate and warmup steps while achieving similar results at the end.
We use a fixed weight decay of 0.1 and an initial learning rate of 0.0003 in our experiments.
These values are selected based on \textsc{enwik8} dev set and used for other tasks.
See Appendix~\ref{sec:appendix:tuning_lr_wd} for more details.
We use a cosine learning rate schedule following~\citet{dai-etal-2019-transformer}.
We do not change the initial learning rate unless otherwise specified.
See Appendix~\ref{sec:appendix:train_details} for the detailed training configuration of each model.

Each training batch contains $B$ sequences (i.e. the batch size) and $M$ consecutive tokens for each sequence (i.e. the unroll size), which gives an effective size of $B\times M$ tokens per batch.
Following standard practice, the previous training batch is provided as additional context for attention, which results in a maximum attention length of $2\times M$.
For \textsc{Enwik8} and \textsc{Wiki-103} datasets, the training data is partitioned into $B$ chunks by concatenating articles and ignoring the boundaries between articles.
For \textsc{billion word} dataset, we follow~\citet{dai-etal-2019-transformer} and concatenate sentences to create the training batches.
Sentences are randomly shuffled and separated by a special token <s> indicating sentence boundaries.

\section{Results}
\label{sec:results}

\paragraph{Does recurrence improve upon attention-only model?}
We first conduct a comparison with the Transformer-XL model~\cite{dai-etal-2019-transformer} on \textsc{enwik8} dataset\footnote{\url{https://github.com/kimiyoung/transformer-xl/tree/master/pytorch}}.
Their base model consists of 41M parameters and 12 Transformer layers.
Following the official instructions, we reproduced the reported test BPC of 1.06 by training with 4 Nvidia 2080 Ti GPUs.
The training took about 4 days or a total of 360 GPU hours equivalently.

\begin{figure}[!t!]
\includegraphics[width=3.0in]{figures/example.v3.pdf}
    \caption{Dev BPC vs. total GPU hours used on \textsc{enwik8} for each model. 
Using automatic mixed precision (amp) and only one attention sub-layer achieves 16x reduction.
    To compute the dev BPC, the maximum attention length is the same as the unroll size $M$ during training.}
    \label{fig:enwik8_base}
\end{figure}

We train a 10-layer SRU++ model with 42M parameters. For a fair comparison, we use the same hyperparameter setting including the effective batch size, attention context length, learning rate and the number of training iterations as the Transformer-XL base model.
Notably, our base model can be trained using 2 GPUs due to less GPU memory usage. 
After training, we set the attention context length to 2048 for testing, similarly to the Transformer-XL baseline.
Table~\ref{tab:enwik8_base} presents the results.
Our model achieves a test BPC of 1.03, outperforming the baseline by a large margin.
This result suggests that combining recurrence and attention can greatly outperform an attention-only model.
We obtain a BPC of 1.02 by extending the attention context length from 512 to 768, while keeping the number of tokens per batch the same.

\paragraph{How much attention is needed?}
\citet{merity2019single} demonstrated that using a single attention layer with LSTM retains most of the modeling capacity compared to using multiple attention layers.
We conduct a similar analysis to understand how much attention is needed in SRU++.
To do so, we only enable attention every $k$ layers.
The layers without attention become the variant with dimension projection illustrated in Figure~\ref{fig:arch} (b).
Note that $k=1$ gives the default SRU++ model with attention in every layer, and $k=10$ means only the last layer has attention in a 10-layer model.

\begin{figure}[!t!]
\includegraphics[width=3.0in]{figures/attn_position.pdf}
\caption{
Analyzing where to apply attention.
We enable only one attention layer (top figure) or two (bottom figure) in the SRU++ model.
For the latter, we always apply attention in the last layer and move the location of the other.
X-axis is the layer index. The layer closest to the input embedding layer has index 1.
}
\label{fig:attn_position}
\end{figure}

\begin{table*}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Parameters $\downarrow$ & \bf Test BPC $\downarrow$ & \bf GPU days $\downarrow$\\
    \hline
    Longformer 30L~\cite{Beltagy2020Longformer} & 102M & 0.99 & 104$^\dagger$\\
    All-attention network 36L~\cite{sukhbaatar2019augmenting} & 114M & 0.98 & 64$\ \,$\\
    Transformer-XL 24L~\cite{dai-etal-2019-transformer}& 277M & 0.99 & - \\
      $\quad\circ$ $\ $ Compressive memory~\cite{Rae2020Compressive} & - & 0.97 & -\\
    Feedback Transformer~\cite{fan2020accessing} & 77M & 0.96 & -\\
    \hline
    SRU++ Base & 108M & 0.97 & 6$^\dagger$ \\
    $\quad\circ$ $\ $ only 2 attention layers ($k=5$) & 98M & 0.98 & 4$^\dagger$ \\
\hdashline
    SRU++ Large & 191M & 0.96 & 12$^\dagger$ \\
    $\quad\circ$ $\ $ $d=8\,d'$ & 195M & 0.95 & 13$^\dagger$\\
    \bottomrule
    \end{tabular}
    \caption{Comparison with top-performing models on \textsc{enwik8} dataset. We include the training cost (measured by the number of GPUs used $\times$ the number of days) if it is reported in the previous work. 
    Our results are obtained using an AWS p3dn instance with 8 V100 GPUs. 
    The reported training time of all-attention network is based on V100 GPUs while the training time of Longformer is based on RTX8000 GPUs (which is about 90\% speed of V100).
    $\dagger$ indicates mixed precision training.}
    
    
    \label{tab:enwik8_sota}
\end{table*}

\begin{table}[!t]
    \centering
    \begin{tabular}{lccc}
    \toprule
    \bf Ratio & \multicolumn{2}{@{~~~}c@{~~~~~}}{\bf Dimensions $d$, $\,d'$} & \bf Dev BPC $\downarrow$ \\
    \hline
    4 & ~3072 & 768 & 0.997 \\
    6 & ~3840 & 640 & 0.992 \\
    8 & ~4480 & 560 & 0.991 \\
    10 & ~5040 & 504 & 0.992 \\
    \bottomrule
    \end{tabular}
    \caption{Dev BPC on \textsc{enwik8} by changing the ratio $d:d'$ in the SRU++ model while fixing the number of parameters to 108M.}
    \label{table:analyze_ratio}
\end{table}

Table~\ref{table:analyze_attention} presents the results by varying $k$.
Our base model is the same 10-layer SRU++ model in Table~\ref{tab:enwik8_base}.
We see that using 50\% less attention ($k=2$) achieves almost no increase in test BPC.
Moreover, using only a single attention module ($k=10$) leads to a marginal loss of 0.01 BPC but reduces the training time by 40\%. 
Our results still outperform Transformer-XL model and single-headed attention LSTM~\cite{merity2019single} greatly by 0.03 BPC.
Figure~\ref{fig:enwik8_base} showcases the training efficiency of our model.
SRU++ is 5x faster to reach the dev BPC obtained by the Transformer-XL model.
Furthermore, using automatic mixed precision training and a single attention layer ($k=10$) achieves 16x reduction on training cost.

\paragraph{Where to use attention?}
Next, we analyze if the location of attention in SRU++ makes a non-trivial difference.
Figure~\ref{fig:attn_position} (top) compares the results by enabling attention in only one of the SRU++ layers. 
Applying attention in the first bottom layer achieves significantly worse result. 
We believe this is due to the lack of positional information for attention, since SRU++ does not use positional encoding.
Enabling attention in subsequent layers gives much better and comparable results because recurrence can encode positional information.

Moreover, SRU++ consistently achieves worse results by moving the attention to lower layer closer to the input embedding.
We also enable a second attention layer while fixing the first one in the 10th layer.
The corresponding results are shown in Figure~\ref{fig:attn_position} (bottom).
Similarly, SRU++ achieves worse results if the attention is added to one of the lower layers.
In contrast, results are comparable once the attention is placed in a high-enough layer.
These observations suggest that the model should first learn local features before attention plays a most effective role at capturing long-range dependencies.
More analyses can be found in Appendix~\ref{sec:appendix:more_analyses}.

\paragraph{Does the ratio $d:d'$ matter?}
Transformer models by default use a FFN dimension that is 4 times larger than the attention dimension~\cite{vaswani2017attention}.
We analyze the ratio of recurrence dimension $d$ to attention dimension $d'$ for SRU++.
A small value of $d'$ can reduce the amount of computation and the number of parameters used in attention layers but may limit the modeling capacity.
Table~\ref{table:analyze_ratio} compares the results of using different $d:d'$ ratio given a similar amount of model parameters.
We fix the model size to around 108M and use 10 SRU++ layers. 
Changing this ratio from 4 to a higher value gives better result.
The best dev result is obtained with a ratio of 8.

Given this observation, we report SRU++ result using a default ratio of 4 as well as a ratio of 8 in the subsequent result sections.
This ensures we conduct a comparison that uses a setup similarly to the default of Transformer models, but also showcases stronger results SRU++ can achieve.


\begin{table*}[!t!]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Parameters $\downarrow$ & \bf Test PPL $\downarrow$ & \bf GPU days $\downarrow$\\
    \hline
    All-attention network 36L~\cite{sukhbaatar2019augmenting} & 133M & 20.6 & -\\
    Feedback Transformer~\cite{fan2020accessing} & 139M & 18.2 & 214$\ \,$\\
    Transformer~\cite{baevski2018adaptive} & 247M & 18.7 & 22$^\dagger$\\
    Transformer-XL 18L~\cite{dai-etal-2019-transformer} & 257M & 18.3 & - \\
    $\quad\circ$ $\ $ Compressive memory~\cite{Rae2020Compressive} & - & 17.1 & -\\
    Routing Transformer~\cite{roy2020efficient} & - & 15.8 & -\\
    kNN-LM~\cite{Khandelwal2020Generalization} & - & 15.8 & -\\
    \hline
    SRU++ Base & 148M & 18.3 & 8$^\dagger$ \\
\hdashline
    SRU++ Large & 232M & 17.4 & 14$^\dagger$ \\
    $\quad\circ$ $\ $ $d=8\,d'$ & 234M & 17.1 & 15$^\dagger$\\   
    $\quad\circ$ $\ $ only 2 attention layers ($k=5$) & 225M & 17.3 & 11$^\dagger$\\   
\bottomrule
    \end{tabular}
    \caption{Comparison with top-performing models on \textsc{wiki-103} dataset. We include the training cost (measured by the number of GPUs used $\times$ the number of days) if it is reported in the previous work. The reported training costs are based on V100 GPUs. Our results are similarly obtained using an AWS p3dn instance with 8 V100 GPUs. $\dagger$ indicates mixed precision training.}
    \label{tab:wit103_sota}
\end{table*}
\begin{table}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf ~Param~ & \bf ~PPL$\,\downarrow$~ & \bf Days$\,\downarrow$\\
    \hline
    \multirow{3}{*}{Transformer} & \multirow{2}{*}{331M} & 25.6 & 57$^\dagger$\\
    & & 25.2 & 147$^\dagger$\\
    & 465M & 23.9 & 192$^\dagger$ \\
    \hline
    SRU++ & 328M & 25.1 & 36$^\dagger$\\
    SRU++ ($k=5$) & 465M & 23.5 & 63$^\dagger$\\
\bottomrule
    \end{tabular}
    \caption{Test perplexity and effective GPU days for training of SRU++ models and the Transformer models of~\citet{baevski2018adaptive} on \textsc{billion word} dataset.}
    \label{tab:1blm}
\end{table}

\paragraph{\textsc{enwik8}}
Table~\ref{tab:enwik8_sota} compares our model with other top-performing models on the \textsc{enwik8} dataset.
We train a base model with $d=3072$ and a large model with $d=4096$ using 400K training steps.
The unroll size and attention context length are set to 1024 during training and 3072 during evaluation.
To compare the computation efficiency we report the effective GPU days -- the number of GPUs multiplied by the number of days needed to finish training. 
Our base model achieves better BPC and uses a fraction of the training cost reported in previous work.
Furthermore, our large models achieve a new state-of-the-art result on this dataset, reaching a test BPC of 0.96 when $d=4\,d'$ and 0.95 when $d=8\,d'$. 


\paragraph{\textsc{Wiki-103}}
Table~\ref{tab:wit103_sota} presents the result of SRU++ models and other top results on the \textsc{Wiki-103} dataset. 
We train one base model with 148M parameters and a few large models which contain about 230M parameters.
As shown in the table, our base model obtains a test perplexity of 18.3 using 8 GPU days of training, about 3x reduction compared to the Transformer model in~\citet{baevski2018adaptive} and over 10x reduction compared to Feedback Transformer~\cite{fan2020accessing}.
Again, changing the hidden size ratio to $d=8\,d'$ improves the modeling capacity.
Our big model achieves a test perplexity of 17.1.
The required training cost remains significantly lower.

\paragraph{\textsc{billion word}}
We double our training iterations to 800K and use a learning rate of 0.0002 for the \textsc{billion word} dataset. 
We train a base model using $d=4096$, $d'=1024$ and an effective batch size of 65K tokens per gradient update.
We also train a large model by increasing the hidden size $d$ to 7616 and the batch size to 98K.
In addition, we use only 2 attention layers ($k=5$) for the large model.
Table~\ref{tab:1blm} reports the test perplexity and associated training cost.
Our base and large model obtain a test perplexity of 25.1 and 23.5 respectively, outperforming the Transformer model of~\citet{baevski2018adaptive} given similar model size.
Moreover, SRU++ achieves 3-4x training cost reduction and is trained using 8 GPUs.
In comparison, the Transformer model uses 32 or 64 V100 GPUs.

\begin{table}[!t]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \bf Model & \bf Speed$\uparrow$ & \bf PPL$\downarrow$\\
    \hline
    kNNLM~(\citeauthor{Khandelwal2020Generalization}) & 145 & 15.8\\
    Trans~(\citeauthor{baevski2018adaptive}) & 2.5k & 18.7\\
    Trans-XL~(\citeauthor{dai-etal-2019-transformer}) & 3.2k & 18.3 \\
    Shortformer~(\citeauthor{press2020shortformer}) & 15k & 18.2\\
    \hline
    SRU++ Large & 15k & 17.1\\
    SRU++ Large ($k=5$) & 22k & 17.3\\
    \bottomrule
    \end{tabular}
    \caption{Inference speed (tokens/second) on \textsc{Wiki-103} test set. Results of baselines are taken from~\citet{press2020shortformer}. We use a single V100 GPU, a batch size of 1 and maximum attention length 2560 for consistency.}
    \label{tab:inference_speed}
\end{table}

\paragraph{Inference speed}
Table~\ref{tab:inference_speed} compares the inference speed of SRU++ with other top-performing models on \textsc{wiki-103} test set.
We use a single V100 GPU for inference.
Our large model runs at least 4.5x faster than all baseline models except Shortformer~\cite{press2020shortformer}.
In addition, our model achieves 0.9-1.1 perplexity lower than Shortformer and runs 50\% faster when using 2 attention layers ($k=5$).


\paragraph{\textsc{IWSLT}}
Does SRU++ work well for other tasks?
We study this question by evaluating SRU++ on the \textsc{IWSLT}'14 De$\rightarrow$En translation task.
We use the open-sourced training and evaluation code of \citet{lin-etal-2020-autoregressive}.
The base model is an 8-layer Transformer model containing 20M parameters.
We train SRU++ models using 6 layers and $d=1024$, resulting in similar number of parameters.
We use the original settings such as learning rate and batch size, except that we use RAdam optimizer for consistency and increase the number of training epochs to 50.
Both architectures achieve much higher BLEU scores given more training epochs.\footnote{\citet{lin-etal-2020-autoregressive} reports a test BLEU of 35.2. We obtain 35.9 for the same Transformer model by training longer.}
Table~\ref{tab:iwslt} presents the test results.
Without additional hyperparameter tuning, SRU++ achieves 0.4 BLEU score higher and less training time compared to the Transformer model tuned in~\citet{lin-etal-2020-autoregressive}.

\begin{table}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf ~Param~ & \bf ~BLEU$\,\uparrow$~ & \bf Hrs$\,\downarrow$\\
    \hline
    Transformer & 20.1M & 35.9$\pm$0.1 & 10.5 \\
    SRU++ & 20.4M & 36.3$\pm$0.2 & 8.5\\
    SRU++ ($k=2$) & 19.6M & 36.1$\pm$0.1 & 7.5\\
    \bottomrule
    \end{tabular}
    \caption{Results on \textsc{IWSLT}'14 De$\rightarrow$En test set. We use a beam size of 5. BLEU scores and training time are averaged over 4 independent runs.}
\label{tab:iwslt}
\end{table}


\paragraph{Why does SRU++ reduce training cost in our experiments?}
Several factors contribute to the computation reduction observed in our experiments. 
First, combining attention and recurrence gives stronger modeling capacity.
As shown in our experiments, SRU++ often achieves comparable results using fewer layers and/or fewer parameters.
The required computation are much lower for shallower and smaller models. 

We also observe higher training efficiency, requiring fewer training steps and smaller training batch compared to several Transformer models.
For example, SRU++ uses a maximum effective batch size of 98K tokens and 800K training steps on the \textsc{billion word} dataset, while the Transformer model in comparison~\cite{baevski2018adaptive} uses 128K tokens and near 1000K steps.
The reduced batch size and gradient updates cut down the training cost.

Finally, model implementation is an important factor for computation saving.
Our implementation is highly efficient for two reasons.
First, the fast recurrence operation of SRU is a reusable module that is already optimized for speed~\cite{lei2018sru}.
Second, since recurrence encodes positional information, we can use simple single-head attention and remove positional encoding.

\begin{figure*}[!t]
\includegraphics[width=6.1in]{figures/profiling_both.pdf}
    \caption{Profiling of SRU++ and Transformer-XL: (a) forward time (in milliseconds) of small and large models and (b) forward time used in various types of time-consuming operations. We use a single GPU for profiling to avoid extra overhead such as data synchronization between GPUs. We use an unroll size / context length $M=512$ and $1024$ respectively for small and large models. All models use a batch size $B=16$ for profiling.}
    \label{fig:profiling}
\end{figure*}

On the contrary, advanced attention and positional encoding mechanism can generate non-trivial computation overhead.
To see this, we measure the running time of SRU++ and Transformer-XL using Pytorch Profiler. 
Figure~\ref{fig:profiling}~(a) shows the average model forward time of a single batch.
SRU++ runs 4-5x times faster compared to the Transformer-XL implementation. 
Figure~\ref{fig:profiling}~(b) breaks down the computation and highlights the most time-consuming operations in both models.
The matrix multiplications are one of the most expensive operations for both models.
Surprisingly, many operations in the relative attention of Transformer-XL are computationally expensive.
For example, the relative attention requires shifting the attention scores and adding up different attention score matrices.
Both require a lot of time but they are not needed in non-relative attention.
In addition, the last column shows the running time of tensor transpose operators needed by batch matrix-matrix multiplications in attention.
Again, the relative attention uses an order of magnitude more time compared to the simple single-head attention used in our model implementation.\footnote{Note that this high latency of tensor transpose might be caused by sub-optimal implementation choices such as a poor arrangement of tensor axes in the open-sourced model. There is room for improvement. Nevertheless, relative attention and positional encoding are reported to be non-trivially slower in other works~\cite{shaw-etal-2018-self,tian2021shatter}.}

