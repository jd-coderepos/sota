\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}


\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{color}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{soul}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage[normalem]{ulem}


\newdimen\abovecrulesep
\newdimen\belowcrulesep
\abovecrulesep=0pt
\belowcrulesep=0pt
\makeatletter
\patchcmd{\@@@cmidrule}{\aboverulesep}{\abovecrulesep}{}{}
\patchcmd{\@xcmidrule}{\belowrulesep}{\belowcrulesep}{}{}

\definecolor{demphcolor}{RGB}{144, 144, 144}
\definecolor{mygray}{gray}{0.4}
\definecolor{lightgray}{rgb}{0.9, 0.9, 0.9}
\newcommand{\demph}[1]{\textcolor{demphcolor}{#1}}
\newcommand{\cmark}{\color{mygray}\ding{51}}
\newcommand{\xmark}{\color{mygray}\ding{55}}


\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand\hshline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth 0.6pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\small}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{.5em\@plus1ex\@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}
\makeatother\def\x{}

\newcommand{\modelname}{TESTA\xspace}

\setstcolor{blue}
\def\revised{\textcolor{black}}
\def\mydemph{\demph}


\definecolor{dt}{HTML}{ADCAD8}
\definecolor{dt2}{HTML}{cddfe7}
\newcommand{\ots}[1]{\textcolor{dt}{#1}}
\newcommand{\otsmodel}[1]{\cellcolor{dt2}{#1}}

\newtheorem{remark}{\textbf{Remark}}
\newcommand{\m}[1]{\mathbf{#1}}
\def \mL{{\mathcal L}}
\def \mS{{\mathcal S}}
\def \mV{{\mathcal V}}
\def \z{{\bf z}}
\def \v{{\bf v}}
\def \c{{\bf c}}
\def \t{{\bf t}}
\def \x{{\bf x}}
\def \y{{\bf y}}
\def \p{{\bf p}}
\def \V{{\bf V}}
\def \A{{\bf A}}
\def \Q{{\bf Q}}
\def \K{{\bf K}}
\def \V{{\bf V}}
\def \bv{\bar{{\bf v}}}
\def \bt{\bar{{\bf t}}}
\def \mR{\mathbb{R}}
\def \mA{\mathbb{A}}
\def \mB{\mathbb{B}}

\newcommand{\acronym}[1]{\underline{\textbf{#1}}}
\newcommand{\hou}[1]{\textcolor{blue}{#1}}
\newcommand{\rsh}[1]{\textcolor{orange}{[#1 - rsh]}}


\title{\modelname: Temporal-Spatial Token Aggregation \\ for Long-form Video-Language Understanding}



\author{Shuhuai Ren,\quad Sishuo Chen,\quad Shicheng Li,\quad Xu Sun,\quad Lu Hou\\
National Key Laboratory for Multimedia Information Processing, 
\\School of Computer Science, Peking University\\
Center for Data Science, Peking University 
Huawei Noah's Ark Lab\\
\texttt{shuhuai\_ren@stu.pku.edu.cn} \quad 
  \texttt{\{lisc99, chensishuo, xusun\}@pku.edu.cn} \\ \texttt{houlu3@huawei.com} \\
}

\begin{document}
\maketitle
\begin{abstract}


Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. 
However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. 
These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. 
To tackle this issue, we propose an efficient method called \acronym{TE}mporal-\acronym{S}patial \acronym{T}oken \acronym{A}ggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame.
TESTA can reduce the number of visual tokens by  and thus accelerate video encoding. 
Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. 
We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. 
Experimental results show that TESTA improves computing efficiency by  times, and achieves significant performance gains from its scalability in processing longer input frames, e.g.,  R@1 on QuerYD and  R@1 on Condensed Movie.\footnote{Our code is available at \url{https://github.com/RenShuhuai-Andy/TESTA}.}

\end{abstract}

\section{Introduction}
Video-language modeling aims to learn semantic alignment between video and language in a joint representation space~\citep{Xu2021VideoCLIPCP, Lei2021LessIM} to facilitate downstream tasks including text-video retrieval, video question answering (VideoQA), and video captioning. 
Unlike text, which can be represented concisely as a sequence of words with dense semantics, video input consists of much longer sequences due to its 3D properties and the redundancy in space-time information~\citep{He2021MaskedAA, Tong2022VideoMAEMA}. 
In fact, the number of visual tokens processed by Transformer-based models~\citep{Fu2021VIOLETE, Cheng2022VindLUAR, Ye2022HiTeAHT, Li2021AlignAP, Wang2022OmniVLOF} can be over  more than text tokens.\footnote{For example, in the QuerYD dataset, a long-form video with  sampled frames at a resolution of  pixels generates around  visual tokens after patchification, while the corresponding caption contains only  text tokens.} 
This poses an efficiency bottleneck for video-language understanding, 
especially for long-form videos lasting more than 30 seconds~\citep{Wu2021TowardsLV, Sun2022LongFormVP}.




\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/fig1_tome_cmp.pdf}
\caption{Two blocks on the left compare ToMe~\citep{Bolya2022TokenMY} and our \modelname on three aspects: video token definition, aggregation method, and computation complexity. The block on the right illustrates \modelname's divided temporal aggregation (left) and spatial aggregation (right). Patches sharing the same inner and border colors are merged together. Our aggregation gradually reduces the number of frames and patches by averaging their features during the forward process of video encoding.} \label{fig:overview}
\end{figure*}


To encode long videos within limited computing budgets, previous approaches can be broadly categorized into two types: 
\textbf{(1) Sparse Sampling}~\citep{Lei2021LessIM, Sun2022LongFormVP, Lei2022RevealingSF}. This method reduces the number of visual tokens by sampling very few frames from the raw video.\footnote{For instance, sample  frames from more than  frames for ActivityNet Captions dataset~\citep{Krishna2017DenseCaptioningEI}.} 
However, sparse sampling sacrifices rich temporal dynamics and storyline information, which limits model performance. 
\textbf{(2) Offline Encoding}~\citep{Luo2021CLIP4ClipAE, Bain2022ACG}. It allows processing more frames within the same computation budgets by constraining the interaction between visual tokens. It first uses an off-the-shelf image encoder~\citep{Dosovitskiy2020AnII, Radford2021LearningTV} to encode each frame independently, then uses a temporal module to aggregate all the frame features. However, the frame features encoded offline may not be well adapted to downstream tasks in various domains. 
Additionally, the post-aggregation mechanism also prohibits the full fusion of frame features~\citep{Cheng2022VindLUAR}. 
Considering that both \textbf{sufficient input frames} and \textbf{full temporal-spatial modeling in an end-to-end manner} are pivotal for optimal performance, a natural question arises: \textit{Are there better approaches to achieve efficient video coding without compromising on either of these aspects?}

In this paper, we propose an efficient method named \acronym{TE}mporal-\acronym{S}patial \acronym{T}oken \acronym{A}ggregation (\modelname) inspired by Token Merging (ToMe)~\citep{Bolya2022TokenMY}. 
Specifically, \modelname samples input frames densely, but progressively aggregates similar visual tokens during video encoding to reduce the token number and computational overhead. 
As shown in Fig.~\ref{fig:overview}, our aggregation operates separately in temporal and spatial dimensions, allowing for the merging of similar frames as well as similar patches within each frame. This reduces ToMe's complexity from  to , making it more efficient for encoding longer videos. 
After aggregation, around  visual tokens can be reduced and thus the video encoding is accelerated. 
To achieve this, we use the bipartite matching algorithm. 
Specifically, we select a set of tokens and then find their most similar counterparts from the remaining set. 
Finally, we aggregate the features of these pairs through mean pooling. 
This aggregation-based mechanism has three advantages: 
\textbf{First}, it does not incorporate additional parameters and is amenable to parallelism, which significantly improves the training and inference efficiency; 
\textbf{Second}, our method (1) adaptively condenses video semantics rather than directly discarding input information, (2) retains full end-to-end spatiotemporal fusion, which both ensure the performance. 
\textbf{Third}, compared to convolution-based feature down-sampling methods~\citep{Liu2021VideoST, Li2021MViTv2IM}, our aggregation trajectory can be easily tracked and recovered. The aggregated tokens often correspond to higher-level semantics (e.g., objects, scenes, and events), making them more interpretable and even grounded in language. 



Building upon \modelname, we design a pre-trained video-language model with a temporal and spatial token aggregation module in each video encoder block. We evaluate our model on paragraph-to-video retrieval and long-form VideoQA tasks. 
When using an equal number of input frames, our model improves computing efficiency by  times while maintaining comparable performance. 
When accessing more frames, our model exhibits strong scalability and achieves significant performance gains compared to previous state-of-the-art methods (e.g.,  R@1 on QuerYD and  R@1 on Condensed Movie).  


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/arch_decoder.pdf}
\caption{Architecture of our pre-trained model and token aggregation algorithm of \modelname. We record the size of the input and output features in red. The circles in the left panel denote either patch tokens or frame tokens.}
\label{fig:arch}
\end{figure*}

\section{Related Work}
\label{sec:relate-work}

\paragraph{Video-Language Pre-trained Models.}
Benefitting from large-scale video-text datasets~\citep{Bain2021FrozenIT, Xue2021AdvancingHV} and advances in Transformer model design~\citep{Gorti2022XPoolCL, Ren2021LearningRA, Fu2021VIOLETE, Zellers2021MERLOTMN, Wang2022AllIO}, 
pre-trained Video-Language Models (VidLMs)~\citep{Chen2022LiteVLEV, Sun2022LongFormVP, Cheng2022VindLUAR} have demonstrated impressive performance in video-language understanding tasks. 
VidLMs typically comprise a video encoder and a text encoder, which encode video-text pairs into a shared feature space to learn the semantic alignment between video and language. 
Additionally, a text decoder can be added after the video encoder for tasks such as video captioning and VideoQA~\citep{Yan2022VideoTextMW, Zhang2020DCADC}.

\paragraph{Efficient Video Transformer.}
A Transformer-based video encoder typically pachifies each video into massive visual tokens, which will cause prohibitive computation costs for full self-attention with quadratic computational complexity. 
Therefore, research on efficient video Transformers has always been active. 
Representative work like TimeSFormer~\citep{Bertasius2021IsSA} and ViViT~\citep{Arnab2021ViViTAV} propose to factorize the spatial and temporal dimensions of the input, then separately apply spatial and temporal attention. 
Video Swin Transformer~\citep{Liu2021VideoST} keeps the joint temporal-spatial attention but restricts it within a local 3D window. 
Orthogonal to the advances of efficient Transformer architectures, our \modelname aggregates token features from the spatial and temporal dimensions, which reduces the size of input features for each Transformer block and can further boost the efficiency of video encoding. 

\paragraph{Feature Aggregation in Video Transformers.}
Existing feature aggregation methods can be broadly categorized into two branches. 
Temporally, frame features can be encoded by a pre-trained image encoder and aggregated using self-attention, joint-attention, or mean pooling for post-temporal modeling purposes~\citep{Bain2022ACG, Luo2021CLIP4ClipAE}. 
Spatially, previous work explored merging similar patches in the image or aggregating tokens into additional proxy tokens~\citep{Bolya2022TokenMY, Shi2023CrossGETCE, Cao2023PuMerPA, Xu2022GroupViTSS, Ryoo2021TokenLearnerWC, Marin2021TokenPI}. 
In contrast, we propose a unified mechanism to simultaneously aggregate frames and patches. Our method gradually aggregates features during video encoding, improving efficiency while ensuring sufficient interaction between features in both space and time. 



\section{Method}
\label{sec:method}
In this section, we first introduce our video-language pre-trained model and its architecture in \textsection~\ref{subsec:arch}. To improve the efficiency of encoding long-form videos, we propose a novel temporal-spatial token aggregation mechanism (\textsection~\ref{subsec:tsta}). Finally, we present the pre-training objectives in \textsection~\ref{subsec:obj}. 

\subsection{Model Architecture}
\label{subsec:arch}
Inspired by prevalent VidLMs~\citep{Li2022BLIPBL, Li2021AlignBF}, our model consists of three encoders and one decoder for video-language representation learning. 
Figure~\ref{fig:arch} shows the model architecture. 

\paragraph{Text Encoder.} The text encoder is a uni-modal encoder similar to BERT~\citep{Devlin2019BERTPO}. A \texttt{[CLS]} token is prepended at the beginning of the input text to represent its global feature.

\paragraph{Video-grounded Text Encoder.} This is a cross-modal encoder. Compared to the uni-modal text encoder, we add a cross-modal module to each encoder layer to enable information flow from video to language. We insert an \texttt{[ENC]} token before the input text to condense the cross-modal information from both video and language. 

\paragraph{Video-grounded Text Decoder.} This is a cross-modal decoder with causal self-attention for auto-regressive text generation. 

\paragraph{Video Encoder.} This is a uni-modal encoder. 
Given a raw video, the visual input  is a sequence of  RGB frames of size  sampled from the video. 
Each frame is split into  non-overlapping patches\footnote{The size of each patch is , and the  patches span the entire frame ().} following ViT~\citep{Dosovitskiy2020AnII}. 
To represent the global video feature, an additional \texttt{[CLS]} token is also used.  
Our video encoder is similar to TimeSFormer~\citep{Bertasius2021IsSA} with the Divided Space-Time Attention. 
Specifically, each video encoder block captures the temporal relations across frames using Temporal Attention and fuses the spatial information of objects, scenes, etc., within each frame using Spatial Attention. 
In contrast to TimeSFormer, we improve the efficiency of video encoding by equipping each video encoder block with a Temporal Aggregation Module and a Spatial Aggregation Module, which we will introduce in \textsection~\ref{subsec:tsta}. 

\subsection{Temporal-Spatial Token Aggregation}
\label{subsec:tsta}
Videos have heavy spatiotemporal redundancy~\citep{He2021MaskedAA, Tong2022VideoMAEMA}. On one hand, some activities (e.g., conversations) can persist across multiple frames with little visual variations. On the other hand, some scenes like background often contain numerous indistinguishable patches in each frame. Aggregating these similar frames and patches can simplify video feature representation and accelerate video encoding.  


Accordingly, we introduce a Temporal Aggregation Module (TAM) and a Spatial Aggregation Module (SAM), i.e., the yellow modules in Figure~\ref{fig:arch}. After each aggregation, TAM reduces  frames while SAM reduce  patches, 
where  and  are hyper-parameters to control the tradeoffs between performance and efficiency. 
TAM and SAM are incorporated into each block of the video encoder, aggregating tokens progressively to reduce their number. 
For the -th Transformer block, let  represents the input video feature, where , ,  denote the number of frames, the number of patches per frame, and the dimension of the token feature, respectively. 
The output video feature after temporal and spatial token aggregation is , resulting in a smaller size and reducing the computing burden for subsequent blocks. 
After the forward process with  encoder blocks, the final number of visual tokens is reduced to .  


\subsubsection{Objects for Aggregation}
\label{subsubsec:agg-obj}
Our video encoder based on \modelname involves two types of tokens for aggregation: \textit{patch tokens} and \textit{frame tokens}. Recall that each frame is divided into a sequence of patches, which are treated as patch tokens. To ensure a formally unified aggregation algorithm, we define frame tokens as pseudo tokens to represent each frame by averaging all the patch tokens within it.
When merging two frame tokens, the corresponding  patches  in frame-1 and  patches  in frame-2 are merged, resulting in  patches . 
As our aggregation strategy is agnostic to the token type, we refer to both patch tokens and frame tokens as ``tokens'' throughout the rest of the paper, without loss of generality.~\looseness=-1 

\subsubsection{Aggregation Strategy}
Recall that given a sequence of  tokens, our target is to reduce  tokens after each aggregation operation.\footnote{For temporal aggregation,  and , for spatial aggregation,  and .} 
To achieve this, we can greedily merge two tokens with the highest similarity and then repeat  times, or merge  tokens into  clusters using clustering algorithms such as k-means~\citep{Lloyd1982LeastSQ}. 
However, these iteration-based methods are not suited for parallelism and can slow down encoding speed~\citep{Bolya2022TokenMY}. 
Therefore, we resort to the bipartite matching method. 
We first partition the  tokens into two disjoint sets  and  with  and  tokens, respectively. The  tokens in the set  are selected elaborately as the tokens to be reduced. 
For each token in the set , we find its most similar token from the set , then merge them by averaging their features. As a result, the remaining  tokens in the set  form a new sequence as the output.  

For similarity calculation, we utilize the attention keys (K) of tokens as features and measure their similarity using cosine similarity. The attention keys contain summarized information intended for use in QKV self-attention, yielding accurate similarity measures~\citep{Bolya2022TokenMY}.


In practice, we introduce two aggregation algorithms, i.e., \textit{importance-based} aggregation and \textit{geometry-based} aggregation. 

\paragraph{Importance-based Aggregation.}
In this algorithm, we pick out the least important  tokens into the set  for aggregation, so as to minimize the negative effects of token reduction. 
The importance of the token  is measured by the following score function , which is defined as the product of the attention it receives from the other tokens :

where  is the attention score from token  to ,  and  represent Queries and Keys in self-attention, respectively.


\paragraph{Geometry-based Aggregation.}
In practice, we notice that adjacent tokens have a larger similarity and should be merged. However, these adjacent tokens also have similar importance scores and thus are prone to be grouped into the same set in importance-based strategy, which hinders their aggregation. 
To address this issue, we partition the  tokens in an alternative way inspired by \citet{Bolya2022TokenMY}, thus assigning adjacent tokens to different sets  and . 
As shown in the left panel in Figure~\ref{fig:arch}, for each token  in the set , we find its most similar token  from the set  to construct a pair  and record their similarity. 
After that, we select  pairs with the greatest similarity and merge the two tokens in the top- pairs. 
Finally, we concatenate the tokens in the two sets back into one sequence as the output.  

The above aggregation algorithms are parameter-free, and can be easily plugged into a Transformer-based video encoder. 
We conduct our aggregation during both training and testing. 
Although the token similarity calculation brings additional computing overhead, it is negligible compared to the efficiency gained by reducing token numbers. 


\subsubsection{Novelty over Token Merging}
\label{subsubsec:novelty-over-tome}
Our work is inspired by Token Merging (ToMe)~\citep{Bolya2022TokenMY}, which also proposes to reduce video tokens by merging similar ones. 
However, we differentiate ourselves from ToMe in two significant ways:

\paragraph{Video Token Definition.} ToMe uses joint space-time tokens ( cubes), while our \modelname defines frame tokens (representing entire frames) and patch tokens ( 2D patches) for decoupled aggregation. This tailored token design is more efficient for modeling long-form videos.

\paragraph{Aggregation Method.} ToMe performs global aggregation over all tokens, resulting in a complexity of . This becomes impractical for long-form video and causes out-of-memory issues beyond  frames. In contrast, \modelname uses divided aggregation in time and space, reducing complexity to . This allows efficient encoding of much longer videos (more than  frames under the same computation quota). The divided scheme also better captures spatial and temporal semantics, resulting in improved performance on long-form video understanding tasks (to be shown in ~\ref{subsec:cmp-tome}).

\subsection{Pre-training Objectives}
\label{subsec:obj}
We use the following three classic  pre-training objectives, i.e., video-text contrastive loss, video-text matching loss, and captioning loss. Please refer to Appendix~\ref{sec:pre-training} for more details.


\begin{table*}[t!]
\centering
    \tablestyle{7pt}{1.1} 
    \def \w{15pt}
    \begin{adjustbox}{max width=\linewidth}

    \begin{tabular}{lrrr|ccc|ccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{\#PT Data} & \multirow{2}{*}{\#Frame} & \multirow{2}{*}{GFLOPs } & \multicolumn{3}{c|}{QuerYD}& \multicolumn{3}{c}{Condensed Movie} \\
        \cmidrule(lr){5-7} \cmidrule(lr){8-10}
         &  &  & & R@1  & R@5  & R@10  & R@1  & R@5  & R@10  \\
        \midrule
        MoEE~\citep{Miech2018LearningAT} & - & - & - & 11.6 & 30.2 & 43.2 & 1.9 & 7.8 & 13.4 \\
        TeachText~\citep{Croitoru2021TeachTextCG} & - & - & - & 14.4 & 37.7 & 50.9 & 12.1 & 27.4 & 37.5 \\
        Frozen~\citep{Bain2021FrozenIT} & 5M & 32 & 1424 & 53.8 & 75.7 & 82.7 & - & - & - \\
        LF-VILA~\citep{Sun2022LongFormVP} & 8M & 32 & \textbf{298} & 69.7 & 85.7 & 90.3 & 13.6 & 32.5 & 41.8 \\
        VINDLU~~\citep{Cheng2022VindLUAR} & 25M & 32 & 745 & 67.8 & 86.3 & 81.8 & 18.4 & 36.4 & 44.3 \\
        \midrule
\rowcolor{dt!50}
        \modelname (Ours) & 5M & 32 & 420 & 77.0 & 91.3 & 94.6 & 21.5 & 42.4 & 50.7 \\
        \mydemph{\modelname w/o agg.} & \mydemph{5M} & \mydemph{32} & \mydemph{786} & \mydemph{79.7} & \mydemph{92.6} & \mydemph{95.5} & \mydemph{23.5} & \mydemph{45.4} & \mydemph{54.8} \\
        \rowcolor{dt!50}
        \modelname (Ours) & 5M & 96 & 1381 & \textbf{83.4} & \textbf{93.8} & \textbf{95.3} & \textbf{24.9} & \textbf{46.5} & \textbf{55.1} \\
        \mydemph{\modelname w/o agg.} & \mydemph{5M} & \mydemph{96} & \mydemph{2383} & \mydemph{84.2} & \mydemph{93.8} & \mydemph{95.1} & \mydemph{25.5} & \mydemph{46.8} & \mydemph{56.0} \\
        \bottomrule
    \end{tabular}
    




    \end{adjustbox}
\caption{Paragraph-to-video retrieval performance (Recall@) on QuerYD and Condensed Movie. \textbf{\#PT Data} refers to the number of video-text pairs used for pre-training.  indicates the results of our re-implementation. \textbf{\modelname w/o agg.} denotes fine-tuning our pre-trained model without activating the token aggregation modules, resulting in no reduction in token number. This serves as an upper bound for \modelname's performance. 
}
\label{table:retrieval-long-long-sota}
\end{table*}
 
\begin{table*}[t!]
\centering
    \tablestyle{7pt}{1.1} 
    \def \w{15pt}
    \begin{adjustbox}{max width=\linewidth}

        \begin{tabular}{lrrr|ccc|ccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{\#PT Data} & \multirow{2}{*}{\#Frame} & \multirow{2}{*}{GFLOPs } & \multicolumn{3}{c|}{DiDeMo} & \multicolumn{3}{c}{ActivityNet Caption}\\
        \cmidrule(lr){5-7} \cmidrule(lr){8-10} 
         &  &  &  & R@1  & R@5  & R@10  & R@1  & R@5  & R@10  \\
        \midrule
        TeachText~\citep{Croitoru2021TeachTextCG} & - & - & - & 21.6 & 48.6 & 62.9 & 23.5 & 57.2 & - \\
        ClipBERT~\citep{Lei2021LessIM} & 0.2M & 2 & 13 & 20.4 & 48.0 & 60.8 & 21.3 & 49.0 & 63.5 \\
        Frozen~\citep{Bain2021FrozenIT} & 5M & 4 & 178 & 31.0 & 59.8 & 72.4 & - & - & - \\
        LF-VILA~\citep{Sun2022LongFormVP} & 8M & 32 & 298 & 35.0 & 64.5 & 75.8 & 35.3 & 65.4 & - \\
        ALPRO~\citep{Li2021AlignAP} & 5M & 8 & 197 & 35.9 & 67.5 & 78.8 & - & - & - \\
        BridgeFormer~\citep{Ge2022BridgingVR} & 5M & 4 & 71 & 37.0 & 62.2 & 73.9  & - & - & - \\
        Singularity~\citep{Lei2022RevealingSF} & 5M & 32 & 589 & 47.4 & 75.2 & 84.0 & 43.0 & 70.6 & 81.3 \\
        HiTeA~\citep{Ye2022HiTeAHT} & 5M & 12 & 98 & 51.8 & 79.1 & 85.3 & 45.1 & 73.5 & 84.2 \\
        VINDLU~\citep{Cheng2022VindLUAR} & 5M & 4 & 93 & 54.6 & 81.3 & 89.0 & 51.1 & 79.2 & 88.4 \\
        \mydemph{All-in-one~\citep{Wang2022AllIO}} & \mydemph{138M} & \mydemph{3} & \mydemph{62} & \mydemph{32.7} & \mydemph{61.4} & \mydemph{73.5} & \mydemph{22.4} & \mydemph{53.7} & \mydemph{67.7} \\
        \mydemph{Clip4Clip~\citep{Luo2021CLIP4ClipAE}} & \mydemph{400M} & \mydemph{64} & \mydemph{282} & \mydemph{43.4} & \mydemph{70.2} & \mydemph{80.6} & \mydemph{40.5} & \mydemph{72.4} & \mydemph{-} \\
        \mydemph{X-CLIP~\citep{Ma2022XCLIPEM}} & \mydemph{400M} & \mydemph{64} & \mydemph{1086} & \mydemph{47.8} & \mydemph{79.3} & \mydemph{-} & \mydemph{46.2} & \mydemph{75.5} & \mydemph{-} \\
\mydemph{CLIP-ViP~\citep{Xue2022CLIPViPAP}} & \mydemph{100M} & \mydemph{12} & \mydemph{212} & \mydemph{50.5} & \mydemph{78.4} & \mydemph{87.1} & \mydemph{53.4} & \mydemph{81.4} & \mydemph{90.0} \\
\midrule
\modelname (Ours) & 5M &  32 & 420 & 57.7 & 83.3 & 89.4 & 51.7 & 79.1 & 87.6 \\
\modelname (Ours) & 5M &  96 & 1381 & \textbf{61.2} & \textbf{87.2} & \textbf{91.5} & \textbf{54.8} & \textbf{80.8} & \textbf{89.6} \\
        \bottomrule
\end{tabular}


\end{adjustbox}
\caption{Paragraph-to-video retrieval performance on DiDeMo and ActivityNet Caption. We gray out methods that use significantly more pre-training data for a fair comparison. The other notations are the same as those on Table~\ref{table:retrieval-long-long-sota}. }
\label{table:retrieval-long-sota}
\end{table*}
 
\section{Experiments}

\subsection{Implementation Details}
To pre-train our \modelname model, we start by initializing it with the BLIP (12-layer ViT-B/16) checkpoint~\citep{Li2022BLIPBL}, with the exception of the temporal attention, which is copied from the spatial attention weights. 
We use around 5M image-text and video-text pairs from two datasets for pre-training. 
See Appendix~\ref{sec:pre-training} for more details.~\looseness=-1 


For downstream fine-tuning, we uniformly sample either  or  frames, each with a resolution of  pixels ( patches per frame with a patch size of ). 
To achieve approximately a  reduction in computation cost, we employ different hyper-parameters for aggregation.
Specifically, for -frame inputs, we set  to  and  to , while for -frame inputs,  is  and  is . 
We use geometry-based aggregation by default since it achieves better performance. 
Please refer to Appendix~\ref{sec:fine-tuning} for more fine-tuning details. 


\subsection{Downstream Task Setups}
We finetune and evaluate \modelname on two downstream tasks of paragraph-to-video retrieval and long-form VideoQA. 
For paragraph-to-video retrieval, we use four datasets: DiDeMo~\citep{Hendricks2017LocalizingMI}, QuerYD~\citep{Oncescu2020QUERYDAV}, ActivityNet Captions~\citep{Krishna2017DenseCaptioningEI}, and Condensed Movie~\cite{Bain2020CondensedMS}.
For long-form VideoQA, we use ActivityNet-QA~\citep{Yu2019ActivityNetQAAD}. 
The details of these datasets are shown in Appendix~\ref{sec:dataset}. 


\subsection{Paragraph-to-Video Retrieval}
\begin{table}[t]
\centering
    \tablestyle{4pt}{1.1} 
    \def \w{20pt} 
    \resizebox{\linewidth}{!}{

    \begin{tabular}{lrc}
        \toprule
Method & \#PT Data & Accuracy (\%) \\
        \midrule
        LF-VILA~\citep{Sun2022LongFormVP} & 8M & 39.9 \\
        Singularity~\citep{Lei2022RevealingSF} & 5M & 41.8 \\
        \mydemph{VIOLET~\citep{Fu2021VIOLETE}} & \mydemph{183M} & \mydemph{38.9} \\
        \mydemph{JustAsk~\citep{Yang2020JustAL}} & \mydemph{69M}  & \mydemph{38.9} \\
        \mydemph{MERLOT~\citep{Zellers2021MERLOTMN}} & \mydemph{180M} & \mydemph{41.4}  \\
        \modelname (Ours) & 5M& \textbf{45.0} \\ 
        \bottomrule
    \end{tabular}

}
\caption{Accuracy (\%) on ActivityNet-QA. }
\label{table:videoqa-sota}
\end{table}
 \begin{table*}[t!]
\centering
\begin{adjustbox}{max width=\linewidth}

    \begin{tabular}{lrr|ccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{\#PT Data} & \multirow{2}{*}{GFLOPs } & \multicolumn{3}{c|}{QuerYD}& \multicolumn{3}{c|}{DiDeMo} & \multicolumn{3}{c}{ActivityNet Caption} \\
        \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12}
         &  &  & R@1  & R@5  & R@10  & R@1  & R@5  & R@10  & R@1  & R@5  & R@10  \\
        \midrule
        \mydemph{Clip4Clip~\citep{Luo2021CLIP4ClipAE}} & \mydemph{400M} & \mydemph{282} & \mydemph{50.0} & \mydemph{74.5} & \mydemph{83.3} & \mydemph{43.6} & \mydemph{71.3} & \mydemph{79.0} & \mydemph{25.0} & \mydemph{51.6} & \mydemph{65.7}  \\
        BLIP~\citep{Li2022BLIPBL} & 129M & 707 & 50.7 & 67.6 & 73.5 & 60.9 & 84.9 & 91.0 & 34.2 & 60.0 & 70.7 \\
\modelname (Ours) & 5M & 786 & \textbf{64.4} & \textbf{82.9} & \textbf{86.9} & \textbf{64.9} & \textbf{88.7} & \textbf{91.8} & \textbf{37.1} & \textbf{63.7} & \textbf{75.4} \\
\bottomrule
    \end{tabular}

    \end{adjustbox}
\caption{Zero-shot evaluation (32 frames) on paragraph-to-video retrieval performance. 
}
\label{table:zeroshot}
\end{table*}
 \begin{table*}[t!]
\centering
\begin{adjustbox}{max width=.9\linewidth}
\begin{tabular}{l|cccc|cc}
\toprule
\modelname                        & R@1   & R@5  & R@10  & Avg.  & GFLOPs  & Memory (GB)  \\ \midrule
No Aggregation            & 84.2 & 93.8 & 95.1 & 91.0   & 2382.5 & 19.2       \\  \midrule
\rowcolor{dt!50}
\multicolumn{7}{l}{\textit{(1) Token Aggregation v.s. Token Pruning (w/o training for both)}} \\
~ Token Pruning () & 71.0 & 86.1 & 90.6 &  82.6  &  \textbf{1380.9}  &   \textbf{12.6}      \\
~ Token Aggregation () & \textbf{79.2} & \textbf{91.8} & \textbf{95.3} & \textbf{88.8}   & 1381.4 & \textbf{12.6}        \\ \midrule
\rowcolor{dt!50}
\multicolumn{7}{l}{\textit{(2) Aggregation Strategy}} \\
~ Importance-based Aggregation & 80.2 & 91.7 & 94.6 & 88.9   & \textbf{1380.9} & 13.7        \\
~ Geometry-based Aggregation & \textbf{83.4} & \textbf{93.8} & \textbf{95.3} & 90.8   & 1381.4 & \textbf{12.6}        \\ \midrule
\rowcolor{dt!50}
\multicolumn{7}{l}{\textit{(3) Aggregation dimension}} \\
~ Only temporal () & 79.5 & 92.9 & \textbf{95.4} & 89.3   & \textbf{1303.9} & \textbf{11.5}        \\
~ Only spatial () & 81.4 & 93.3 & 95.1 & 89.9   & 1364.0 & 11.9        \\
~ Both temporal and spatial () & \textbf{83.4} & \textbf{93.8} & 95.3 & \textbf{90.8}   & 1381.4 & 12.6        \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation study on (1) token reduction method, (2) aggregation strategy, and (3) aggregation dimension. The results are reported on QuerYD with  frames. Avg. represents average recall across R@1, R@5, and R@10.}
\label{tab:ablation}
\end{table*} 
Table~\ref{table:retrieval-long-long-sota} demonstrates the performance of \modelname on two challenging and under-explored paragraph-to-video retrieval datasets, QuerYD and Condensed Movie, which involve videos with lengthy durations (over  seconds on average). For -frame video inputs, \modelname achieves Recall@1 of  on QuerYD and  on Condensed Movie, surpassing previous SOTA methods by  and , respectively. 
In terms of computational complexity, \modelname exhibits a significantly lower GFLOPs of  compared to Frozen~\citep{Bain2021FrozenIT} and VINDLU~\citep{Cheng2022VindLUAR}. 
While LF-VILA~\citep{Sun2022LongFormVP} operates with even fewer GFLOPs (), it necessitates feature aggregation within a fixed local window, which can  potentially undermine semantic integrity after concentration.  
In contrast, our model enables the adaptive merging of features with high similarity in the global scope, resulting in improved performance ( R@1 on average compared to LF-VILA). 

Given the importance of incorporating more input frames for long video understanding tasks, we finetune \modelname using -frame inputs and further promote R@1 to  on QuerYD and  on Condensed Movie. This exhibits strong scalability of our model (see Appendix~\ref{sec:tradeoff} for a detailed analysis). 
Additionally, we report the results of \modelname without token aggregation, which serves as an upper bound for \modelname's performance. 
Although preserving full visual tokens yields higher recall, it requires  times more GLFOPs compared to \modelname. 
As the number of input frames increases from  to , the GFLOPs of \modelname w/o agg. exceed , but the performance gain diminishes (only  R@1 on QuerYD). This indicates the superiority of our method in aggregating redundant tokens in long sequence inputs. 



Table~\ref{table:retrieval-long-sota} demonstrates model performance on  DiDeMo and ActivityNet Caption, which consist of shorter videos ( seconds on average) and are considered less challenging.  
For -frame inputs, \modelname with 5M pre-training data achieves  R@1 on DiDeMo, which even surpasses the models pre-trained with over 100M data. 
By increasing the number of frames to , \modelname achieves R@1 of  on DiDeMo and  on ActivityNet, outperforming previous SOTA methods by  and , respectively.~\looseness=-1 


\subsection{Long-Form Video Question-Answering}
Table~\ref{table:videoqa-sota} showcases the performance of \modelname on ActivityNet-QA (using -frame). 
The accuracy of \modelname is , which is  higher than the previous SOTA, Singularity~\citep{Lei2022RevealingSF}. 
This demonstrates that our method eliminates redundant information while integrating crucial visual cues to accurately answer the posed questions. 


\subsection{Zero-shot Generalizability}
In Table~\ref{table:zeroshot}, we show the zero-shot performance of pre-trained CLIP4clip, BLIP, and \modelname on three datasets (32 frames). 
Although our \modelname is initialized by the BLIP checkpoint, it consistently outperforms BLIP (as well as CLIP4clip) after our pre-training, achieving average improvements of , , and  on QuerYD, DiDeMo, and ActivityNet respectively. 
This indicates our substantial gains on long-form video datasets are not solely due to the strong BLIP checkpoint, but also owing to our temporal modeling and pre-training on video data. 

\subsection{Ablation Study}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/case_didemo_129.pdf}
\caption{Visualization of our temporal and spatial aggregation. Frames that are enclosed within the same red rectangle, as well as patches that share the same inner and border color, are merged together.}
\label{fig:vis}
\end{figure*}

We perform an extensive ablation study and analysis on various crucial components in our aggregation algorithm to examine their impacts. 

\paragraph{Token Aggregation v.s. Token Pruning.}
We first compare the performance and efficiency of token aggregation and token pruning~\citep{Rao2021DynamicViTEV}. 
Regarding pruning, we calculate the importance score (Eq.~\eqref{eq:importance}) for each token and prune the least important  tokens following previous methods~\cite{Goyal2020PoWERBERTAB}. 
We finetune our pre-trained model on QuerYD without token aggregation, then apply token aggregation and pruning in an off-the-shelf manner for test evaluation. 
The results are presented in the first block of Table~\ref{tab:ablation}. In comparison to the vanilla model (no aggregation), both pruning and aggregation decrease computation costs, with only  GFLOPs and  GPU memory. However, the performance degradation of our token aggregation is much smaller than that of pruning ( v.s.  in terms of average recall), suggesting that aggregation better preserves the valuable visual semantics within videos.

\paragraph{Ablation on the Aggregation Strategy.}
To investigate the effectiveness of different aggregation strategies, we report the performance of \modelname using importance-based and geometry-based aggregation methods. 
The results in the middle block of Table~\ref{tab:ablation} show that the simplest geometry-based aggregation method achieves the best Recall@1 of , outperforming the other method by . 
This confirms our hypothesis that adjacent tokens exhibit greater similarity and should be assigned to separate sets for aggregation. 

\paragraph{Ablation on the Aggregation Dimension.}
We compare the performance of three aggregation methods: (1) temporal only, (2) spatial only, and (3) both temporal and spatial. 
To ensure a roughly equal computational overhead, we adjust  and  accordingly. 
The results in the bottom block of Table~\ref{tab:ablation} show that performing token aggregation on a single dimension leads to excessive dilution of information, while the information in other dimensions becomes overly redundant. 
This imbalance hurts the performance of the model. Therefore, our approach, with incorporates both temporal and spatial aggregation, achieves the best outcomes. 

Additionally, Appendix~\ref{sec:more-ablation} discusses the impact of the number of reduced tokens  and . Appendix~\ref{sec:token-sim} analyzes the properties of aggregated tokens by probing their similarity. 

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figs/case_didemo_141_region.pdf}
\caption{Text grounding visualization. F denotes the  frame in the video and S denotes the  sentence in the caption. We calculate the similarity between the phrase query (in orange) and each region formed by our aggregation, then record the value in the region. The phrase queries can be grounded to their corresponding aggregated regions, achieving the highest similarity.}
\label{fig:vis-region}
\end{figure}

\begin{table}[t]
\centering
    \tablestyle{4pt}{1.1} 
    \def \w{20pt} 
    \resizebox{\linewidth}{!}{

    \begin{tabular}{@{}lccc|c@{}}
    \toprule
    Method & R@1   & R@5   & R@10  & GFLOPs  \\ \midrule
    ToMe  & 59.9        & 82.2        & 88.6   &   252        \\
    \modelname     & \textbf{62.4} & \textbf{85.6} & \textbf{91.1}  &   \textbf{228} \\ \midrule 
    ToMe w/o agg.     & 66.1        & 86.4        & 90.4  &   450        \\
    \modelname w/o agg.     & \textbf{75.0} & \textbf{91.1} & \textbf{93.8} &   \textbf{392} \\ \bottomrule
    \end{tabular}
    }
\caption{Paragraph-to-video retrieval performance (Recall@) on QuerYD ( frames). \textbf{w/o agg.} denotes fine-tuning without token aggregation; the only distinction lies in the attention mechanism, where ToMe employs global attention, while \modelname utilizes separate spatial-temporal attention.}
\label{table:tome}
\end{table}
 \subsection{Comparison to Token Merging}
\label{subsec:cmp-tome}
We directly compare the performance of ToMe~\citep{Bolya2022TokenMY} and \modelname by initializing both models from the BLIP pre-trained checkpoint and fine-tuning them on QuerYD. As we noted in \textsection~\ref{subsubsec:novelty-over-tome}, due to the extremely high computational complexity of ToMe's global attention, increasing the number of input frames can lead to out-of-memory issues without token aggregation (w/o agg.). Therefore, we limit the number of input frames to . Besides, We set the hyperparameter  (number of reduced tokens) to ensure matched GFLOPs. Specifically, for ToMe, , while for \modelname,  and . 
The results in Table~\ref{table:tome} illustrate \modelname's efficiency and effectiveness for long-form video understanding, which can be attributed to our tailored design for divided spatial-temporal modeling. In comparison to ToMe, our approach achieves higher recall with fewer GFLOPs, regardless of whether token aggregation is applied.

\subsection{Visualization}
Figure~\ref{fig:vis} provides a visualization of temporal and spatial aggregation on the DiDeMo dataset. \modelname effectively aggregates tokens with highly-similar semantics, demonstrating its strong interpretability. 
\textbf{From a temporal perspective}, \modelname aggregates a sequence of frames captured during continuous lens movement (first  frames). It also condenses similar frames of athletes waiting for the game (last  frames). 
\textbf{From a spatial perspective}, \modelname merges the patches belonging to the same \textit{scenes} (e.g., sky, baseball park) and the same \textit{objects} (e.g., billboard, back of the audience's head).
More examples can be found in Appendix~\ref{sec:more-vis}.

In Figure~\ref{fig:vis-region}, we further show that \modelname enables grounding of language to the aggregated visual tokens~\citep{Ren2023PromptPW, ren-etal-2023-delving}. 
Given the phrase query in the caption, it achieves the highest similarity of its oracle region formed by our aggregation, facilitating fine-grained alignment between phrases and regions. 


\section{Conclusion}
In this paper, we present \modelname, an efficient method for long-form video-language understanding. 
By aggregating similar frames and patches, \modelname effectively condenses video semantics and accelerates video encoding. 
Experimental results on paragraph-to-video retrieval and VideoQA tasks demonstrate that \modelname outperforms previous SOTA methods by a considerable margin. 


\section*{Limitations}
To facilitate future research, we analyze the limitations and possible solutions in our work. 
\textbf{(1)} Due to limited computing resources, we do not use long-form video pre-training datasets such as HD-VILA~\cite{Xue2021AdvancingHV} or incorporate \modelname in pre-training. We believe long video pre-training with \modelname could greatly improve pre-training efficiency and obtain a video-language model with better performance. 
\textbf{(2)} For aggregation efficiency, we only use video-side features to merge visual tokens. We believe that leveraging text signals for aggregation could make the final encoded features more suitable for downstream tasks. 
\textbf{(3)} Our model training only uses coarse objectives such as VTC, VTM, and CAP (Eq.~\eqref{eq:contrastive_loss}-\eqref{eq:cap_loss}) on video-text pairs. Considering \modelname can aggregate tokens into objects, scenes, events, etc., training with fine-grained alignment functions~\citep{Ren2021LearningRA, Wang2022DisentangledRL} could help some tasks like action localization and video object detection~\citep{Zhukov2019CrossTaskWS, Real2017YouTubeBoundingBoxesAL}, on which we will perform more explorations in future work. 


\section*{Acknowledgements}
We thank all the anonymous reviewers for their constructive comments, and Rundong Gao and Lei Li for their valuable suggestions in preparing the manuscript. 
This work is supported in part by a Huawei Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author of this paper. 






\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\label{sec:appendix}

\begin{table*}[t]
\small
\centering
\begin{tabular}{l l c r r r} 
\toprule
Dataset & Domain  & \#Video-Text Pairs & Avg. Len (sec) & Text Len & Duration (h) \\
\midrule
WebVid-2.5M~\cite{Bain2021FrozenIT} & open & ~2.5M & 18.0 & 12.0 & 13K \\
QuerYD~\cite{Oncescu2020QUERYDAV} & open  & ~~~~~2K  & 278.0 & 243.8 & 200\\
Condensed Movie~\cite{Bain2020CondensedMS} & movie  & ~~~34K  & 132.0 & 18.0  & 1.3K \\
DiDeMo~\cite{Hendricks2017LocalizingMI} & Flickr & ~~~10K & 28.0 & 29.2 &87 \\ 
ActivityNet Captions~\cite{Krishna2017DenseCaptioningEI} & action & ~~~20K & 180.0 & 48.3 & 849 \\
ActivityNet QA~\cite{Yu2019ActivityNetQAAD} & action & ~~~~~5K & 117.0 & 8.9 & 976 \\
\bottomrule
\end{tabular}
\caption{Statistics of video-language datasets.}
\label{tab:datasets}
\end{table*} 
\section{Pre-training Details}
\label{sec:pre-training}

\subsection{Pre-training Datasets.}
\label{subsec:pre-training-datasets}
We perform pre-training on two datasets: WebVid-2M ~\citep{Bain2021FrozenIT} containing 2.5M video-text pairs and Conceptual Captions (CC3M)~\citep{Changpinyo2021Conceptual1P} consisting of 3M image-text pairs.
We include CC3M to improve spatial representations of videos as suggested by ~\citet{Li2021AlignAP}.
We duplicate images from CC3M for  times to make static videos. 
For WebVid-2M, we randomly sample  frames for each video instance.  
Because a small fraction of video and image URLs from the original datasets are no longer available, the total number of pre-training samples is around 5M. 
In the pre-training phase, we do not perform token aggregation since the number of frames in the pre-training video data is relatively small.


\subsection{Detailed Pre-training Objectives.}
\label{subsec:pre-training-obj}
We use the following three classic pre-training objectives.

\paragraph{Video-Text Contrastive Loss.}
Given a batch of  video-text pairs, the contrastive objective aims to pull together the paired videos and texts while pushing apart the others with dissimilar semantics in the feature space. 
Let  and  represent the \texttt{[CLS]} feature of the video and text, respectively. The video-to-text contrastive loss  is:

where   is a learnable temperature parameter. Similarly, the text-to-video contrastive loss
 is:

The video-text contrastive loss is defined as:

In the implementation , the negative sample features are extracted from a queue of recent samples encoded by a momentum  encoder~\citep{he2020momentum}.
Moreover, a momentum distillation regularization loss~\citep{Li2021AlignBF} is added to  for the sake of the potential positives in the negative pairs.

\paragraph{Video-Text Matching Loss.}
Video-text matching aims to predict whether a pair of video and text is matched or not. 
For the -th video-text pair, 
we first obtain their joint video-text embedding of the \texttt{[ENC]} token from the video-grounded text encoder. 
We then use this embedding to generate a two-class probability , and calculate the video-text matching loss  as:

Here  is a one-hot vector representing the ground-truth label, and  is the cross-entropy loss. 
In the implementation of , we apply online contrastive hard negative mining~\citep{Li2021AlignBF}.
We refer readers to the ALBEF paper~\citep{Li2021AlignBF} for a comprehensive introduction to momentum distillation and  online contrastive hard negative mining.

\paragraph{Captioning Loss.} This objective activates the video-grounded text decoder to predict the precise tokenized caption  in an autoregressive way:

where  is the text length. 
Combining Eq.~\eqref{eq:contrastive_loss}-\eqref{eq:cap_loss}, the overall objective can be formulated as: 


\subsection{Hyperparameters.}
The model is pre-trained for  epochs with the Adam~\citep{adam} with a weight decay of 5e-2.
The batch size is  and the momentum queue size is . 
The pre-training is conducted on four nodes with  NVIDIA V100 GPUs ( GB memory per GPU) in total and each epoch lasts around  hours.
The learning rate is linearly warmed up from 1e-6 to 5e-6 in the first  steps and then gradually cosine decayed to 5e-7 in the remaining steps.
Temporally consistent random spatial augmentation~\citep{qian2021spatiotemporal} is applied and mixed precision is used for efficient training.

\section{Fine-tuning Details}
\label{sec:fine-tuning}
The downstream fine-tuning is conducted on  NVIDIA V100 GPUs. The learning rate is 1e-5 with a warmup ratio of . The batch size is  and the momentum queue size is . We fine-tune our model for  epochs with the Adam optimizer and a weight decay of . 
For paragraph-to-video retrieval,  we use  and   as training objectives.  
For evaluating paragraph-to-video retrieval models, we select the top 128 candidates based on the video-text feature similarity and then rerank the selected candidates by their pairwise VTM scores.
For video-QA, we use the cross-entropy loss for maximizing the generation probability of the correct answer and rank the candidates by their generation probabilities for evaluation.

\section{Downstream Datasets}
\label{sec:dataset}
We finetune and evaluate \modelname on two downstream tasks of paragraph-to-video retrieval and long-form VideoQA. The details of these datasets are shown in Table~\ref{tab:datasets}. 

For paragraph-to-video retrieval, we use 4 datasets of DiDeMo~\citep{Hendricks2017LocalizingMI}, QuerYD~\citep{Oncescu2020QUERYDAV}, ActivityNet Captions~\citep{Krishna2017DenseCaptioningEI}, and Condensed Movie~\cite{Bain2020CondensedMS}.
We evaluate text-to-video retrieval, where the text acts as the query, in terms of R@, which means the recall (\%) of the target video through  retrieval efforts. 

For long-form VideoQA, we use ActivityNet-QA~\citep{Yu2019ActivityNetQAAD}. The metric is accuracy (\%).

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figs/gflops_vs_r1.pdf}
\caption{Comparison of GFLOPs and Recall@1 on the QuerYD dataset. F denotes using  input frames for fine-tuning and evaluation. The curve of our \modelname is located in the upper left corner, indicating that our model achieves a better performance-cost tradeoff compared to other pre-trained models.}
\label{fig:tradeoff}
\end{figure}





\begin{figure*}[ht]
\centering
\begin{minipage}[b]{0.63\textwidth}
  \centering
\subfloat{
    \label{sfig:ablation-rt}
    \includegraphics[width=0.53\textwidth]{figs/ablation_r_t.pdf}} \subfloat{
    \label{sfig:ablation-rp}
    \includegraphics[width=0.47\textwidth]{figs/ablation_r_s.pdf}}
  \caption{Ablation on reduced the token number,  (temporal aggregation), and  (spatial aggregation). The average recall is represented by \textcolor{red}{red stars}, while GFLOPs are depicted by \textcolor{blue}{blue bars}. The dotted lines denote the results without any aggregation ( and ). All results are evaluated on QuerYD with  frames.}
  \label{fig:ablation-r}
\end{minipage}
\hspace{10pt} \begin{minipage}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figs/tradeoff.pdf}
    \caption{GFLOPs-Recall tradeoff on QuerYD. We record the performance (dots) of \modelname with various - configurations, and plot the trends (curve) by fitting the dots.}
    \label{fig:tradeoff}
\end{minipage}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=.9\textwidth]{figs/token_sim.pdf}
\caption{Cosine similarity between tokens from Set  and Set  in various video encoder blocks. The \textcolor{blue}{blue} color indicates frame tokens while the \textcolor{orange}{orange} color indicates patch tokens. For those tokens finally being aggregated, we plot their similarity in a dark color.}
\label{fig:token_sim}
\end{figure*}

\section{Recall-GFLOPs Tradeoff of Various Pre-trained Models}
\label{sec:tradeoff}
In Figure~\ref{fig:tradeoff}, we analyze the tradeoff between recall and GFLOPs for various pre-trained models. The curve of our \modelname is located in the upper left corner, indicating that our model achieves a superior Recall-GFLOPs tradeoff compared to other pre-trained models. 

Furthermore, Figure~\ref{fig:tradeoff} presents the model performance with different input frames.  Surprisingly, increasing the number of input frames from  to  has minimal impact on the performance of Singularity~\citep{Lei2022RevealingSF} and Frozen~\citep{Bain2021FrozenIT}, and even slightly reduced the recall of ALPRO~\citep{Li2021AlignAP} and VINDLU~\citep{Cheng2022VindLUAR}. 
In contrast, our \modelname exhibits linear improvement in performance with the number of input frames, demonstrating superior scalability.


\section{Ablation on the Number of Reduced Tokens}
\label{sec:more-ablation}

In our \modelname (\textsection~\ref{subsec:tsta}),  and  specify the number of tokens to be reduced for the temporal and spatial aggregation module, separately. To investigate the influence of these two hyper-parameters, we vary the number of  and , then report the average GFLOPs (blue bars) and recall (red star) on the QuerYD dataset. 
Figure~\ref{fig:ablation-r} illustrates the results. 
On one hand, GFLOPs decrease linearly as \footnote{Here we use  to refer to  or  for brevity.} increases, indicating that increasing the reduced token number can improve the efficiency of video encoding. 
On the other hand, merging too many tokens with large  (e.g., ) will lose semantic information in the final encoded video representation, thus leading to a declined average recall.~\looseness=-1 


We evaluate more cases with various  and  configurations, and plot the GFLOPs-Recall tradeoff in Figure~\ref{fig:tradeoff}. 
Based on these results and analysis, we determined the default configuration for our \modelname, i.e.,  and for -frame inputs, and  for -frame inputs. This configuration helps our model achieve approximately a  reduction in computation cost without significant performance decline.~\looseness=-1 



\section{Token Similarity Analysis}
\label{sec:token-sim}
We probe the properties of the aggregated tokens by analyzing their similarity. In Figure~\ref{fig:token_sim}, we count the average similarity between tokens from different blocks, different dimensions (frame tokens or patch tokens), and different aggregation results (aggregated or disaggregated). 


\textbf{For patch tokens} (in orange), the overall similarity between them is large (higher than ), indicating considerable spatial redundancy. Meanwhile, the aggregated patch tokens (in dark orange) have a very high similarity of , which ensures the semantic purity of the aggregated patch tokens. 

\textbf{While for frame tokens} (in blue), their similarity decreases as the number of blocks increases, which may yield aggregated frames with mixed and diverse semantics.
Nevertheless, recall that our frame token is a pseudo token (\textsection~\ref{subsubsec:agg-obj}) obtained by averaging patch features, which does not elaborately model frame semantics. Therefore, compared to patch tokens, the representation of frame token and their similarity measure needs improvement, which we regard as future work.

\section{More Visualization of Aggregation}
\label{sec:more-vis}
In this section, we provide more qualitative results of our \modelname for video-language understanding. Figure~\ref{fig:more-vis} shows another  case on the DiDeMo dataset. 
\modelname effectively aggregates tokens with highly-similar semantics, demonstrating its strong interpretability. 


\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/case_appendix.pdf}
\caption{More visualizations of our aggregation on DiDeMo. Frames that are enclosed within the same rectangle, as well as patches that share the same inner and border color, are merged together.}
\label{fig:more-vis}
\end{figure*}

\end{document}
