\documentclass[journal,twoside]{IEEEtran}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{float}
\usepackage{overpic}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{color}
\usepackage{arydshln}
\usepackage{makecell,rotating}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cite}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\myPara}[1]{\vspace{5pt}\noindent~\textbf{#1} \quad}
\usepackage[dvipsnames]{xcolor}

\usepackage{enumitem}
\setenumerate[1]{itemsep=4pt,partopsep=0pt,parsep=\parskip,topsep=4pt}
\setitemize[1]{itemsep=4pt,partopsep=0pt,parsep=\parskip,topsep=4pt}

\definecolor{citecolor}{RGB}{119,185,0}
\definecolor{citecolor1}{RGB}{66,168,235}
\usepackage{hyperref}
\hypersetup{colorlinks=true,citecolor=citecolor1,linkcolor=BrickRed,urlcolor=Thistle}


\definecolor{mygray}{gray}{.88}
\definecolor{mygrayd}{gray}{.94}

\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etc{\emph{etc}}
\def\etal{{\em et al.~}}

\DeclareMathOperator{\prob}{Prob}

\newtheorem{theorem}{\indent Theorem}[section]
\newtheorem{lemma}{\indent Lemma}
\newtheorem{proposition}{\indent Proposition}[section]
\newtheorem{corollary}{\indent Corollary}[section]
\newtheorem{property}{\indent Property}[section]
\newtheorem{definition}{\indent Definition}[section]
\newtheorem{remark}{\indent Remark}
\newtheorem{example}{\indent Example}[section]
\newtheorem{assumption}{\indent Assumption}[section]

\setcounter{page}{1}

\begin{document}

\title{Learning Visual Affordance Grounding from Demonstration Videos}

\author{Hongchen Luo, Wei Zhai, Jing Zhang, \IEEEmembership{Member, IEEE}, Yang Cao, \IEEEmembership{Member, IEEE}, and Dacheng Tao, \IEEEmembership{Fellow, IEEE}

\thanks{H. Luo, W. Zhai and Y. Cao are with the School of Information Science and Technology, at the University of Science and Technology of China, Anhui, China. (email: \{lhc12, wzhai056\}@mail.ustc.edu.cn, forrest@ustc.edu.cn).}


\thanks{J. Zhang and D. Tao are with the School of Computer Science, in the Faculty of Engineering, at the University of Sydney, Sydney, Australia. (email: \{jing.zhang1,dacheng.tao\}@sydney.edu.au).}

}


\maketitle

\begin{abstract}
Visual affordance grounding aims to segment all possible interaction regions between people and objects from an image/video, which is beneficial for many applications, such as robot grasping and action recognition. However, existing methods mainly rely on the appearance feature of the objects to segment each region of the image, which face the following two problems: (i) there are multiple possible regions in an object that people interact with; and (ii) there are multiple possible human interactions in the same object region. To address these problems, we propose a Hand-aided Affordance Grounding Network (HAG-Net) that leverages the aided clues provided by the position and action of the hand in demonstration videos to eliminate the multiple possibilities and better locate the interaction regions in the object. Specifically, HAG-Net has a dual-branch structure to process the demonstration video and object image. For the video branch, we introduce hand-aided attention to enhance the region around the hand in each video frame and then use the LSTM network to aggregate the action features. For the object branch, we introduce a semantic enhancement module (SEM) to make the network focus on different parts of the object according to the action classes and utilize a distillation loss to align the output features of the object branch with that of the video branch and transfer the knowledge in the video branch to the object branch. Quantitative and qualitative evaluations on two challenging datasets show that our method has achieved state-of-the-art results for affordance grounding. The source code will be made available to the public.
\end{abstract}

\begin{IEEEkeywords}
Visual affordance grounding, Learning from demonstrations, Weakly supervised learning, Deep learning.
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{F}{or} an object in the scene, we need to know what it is and what interactions it may have with people, and the possible interactions are the affordance of the object. The concept of affordance was introduced by the ecological psychologist Gibson \cite{theaff} in 1979 to describe the potential ``action possibilities'' that the environment can provide. The ``action possibilities'' reflect all possible interactions between the object and the agent. In particular, perceiving object affordance is a valuable capability and has a wide range of applications in action recognition, robot grasping, autonomous driving, scene understanding, \etc. \cite{zhang2020empowering,DBLP:journals/corr/abs-1807-06775,DBLP:journals/cviu/KjellstromRK11,qi2017predicting,vu2014predicting,yamanobe2017brief,liu2019auto}.

This paper focuses on the affordance grounding task, \ie, given an action label and an object image,  our goal is to generate the heatmap where the action happens. However, in Gibson's definition of affordance, ``it implies the complementarity of the animal and the environment'' \cite{theaff}. The multiple potential complementarities between the animal and the environment, leading to the possibility that affordance has multiple possibilities: (i) there are multiple possible regions in an object where people interact. As shown in Fig \ref{FIG:1} (a), the same object may imply two different interactions, ``pull'' and ``rotate'', and the interaction occurs at different locations; and (ii) there are multiple possible human interactions in the same region. As shown in Fig. \ref{FIG:1} (b), two different interactions, ``rotate'' and ``touch'', can happen at the same position on the same object. Existing methods \cite{DBLP:KoppulaS14,DBLP:conf/iros/NguyenKCT17,8460902,DBLP:conf/cvpr/SawatzkySG17} typically establish the mapping relationship between apparent features to affordance labels, which are not skilled at capturing the affordance-related context and perceiving the affordance multiple possibilities.

\begin{figure}[t]
	\centering
		\includegraphics[width=1\linewidth]{figs/figure_1.pdf}
	\caption{\textbf{The challenges of affordance grounding.} We show some examples of affordance with multiple possibilities. (a) Different interactions occur at different locations of the same object. (b) Different interactions occur at a similar position of the same object.}
	\label{FIG:1}
\end{figure}

At the same time, the study \cite{heft1989affordances} found that, in a particular scene, the affordance of the object is uniquely determined by the intention of the human action. For example, in a demonstration video, human hands and objects interact frequently, and human action intention can be inferred from hand position and action, providing an essential clue to solve the above two problems. For the first question, we can infer the region where the person interacts with the object by the hand's position, such as ``touch'' and ``pull'' in Fig. \ref{FIG:1} (a). For the second question, we eliminate the ambiguity of affordance by inferring human intention through human hand actions, such as (b) in Fig. \ref{FIG:1}, according to the hand action in the video, we can infer whether the object's affordance is ``touch'' or ``rotate''. To this end, we both consider the affordance clues provided by the position and the action of the hand and solve multiple possibilities of affordance grounding by establishing a mapping relationship between hand action intentions and object parts.

This paper presents a Hand-aided Affordance Grounding Network (HAG-Net) to learn the affordance of objects from demonstration videos. We use the clues provided by the position and the action of the hand to solve the two problems in the affordance grounding. The pipeline of our method is shown in Fig. \ref{FIG:2}. To better capture the affordance cues from the demonstration videos, we propose a hand-related selection network to select the keyframes which primarily reflect the interaction between the hand and the object.  During learning the object's affordance from the demonstration videos, we emphasize the features around the hand to provide valuable context to learn the region where the person interacts with the object. Instead of simply feeding the hand features directly into the network, we consider the occluded features and the objects surrounding the hand. Since the region of human-object interaction is related to the category of action, we design a semantic enhancement module (SEM) that considers both the action category and appearance features to jointly adjust the feature of the object image so that the network can better focus on the feature regions related to the action category. Finally, we consider both the position and the action of the hand to better transfer the knowledge of the video branch to the object branch. The contributions of this paper are as follows:

\begin{enumerate}

\item 
We propose a Hand-aided Affordance Grounding Network (HAG-Net) to use the clues provided by the hands in the demonstration videos. We leverage both the position of the hand and the action to eliminate the multiple possibilities in the affordance grounding task. 

\item 
We introduce the Semantic Enhancement Module (SEM), which utilizes the action category and appearance features to jointly modulate the feature of the object image to locate the relevant region of the interaction more effectively.

\item 
Experimental results on the two most challenging affordance grounding datasets have demonstrated the superiority of the proposed model against the state-of-the-arts.

\end{enumerate}

\section{Related Work}
\subsection{Affordance Grounding}
Affordance grounding is to detect the regions of the object where interactions may occur. Early works \cite{DBLP:KoppulaS14,DBLP:conf/iros/NguyenKCT17,8460902,2019Object} are mainly on image-based supervised segmentation tasks, outputting the affordance label of each pixel for a given image. However, these tasks rely on pixel-level labels, and these methods can not learn how humans interact with objects.

Another part of the early works represents affordance as a pose in which humans interact with objects. Yao \etal \cite{DBLP:conf/iccv/YaoMF13} used a clustering method to find that all possible object functionalities and represented it in the form of a pose in which human interaction with musical instruments. Wang \etal \cite{Wang_affordanceCVPR2017} used Variational-Auto Encoders (VAE) to construct their model to predict affordance poses. Li \etal \cite{3d-affordance} proposed a 3D generative model to predict physically plausible and physically feasible human poses in a given 3D scene. The difference from the above methods is that we focus on the possible interactions of the various parts of the object instead of the human pose. 

A large number of works solve the affordance grounding by learning the interactions between humans and objects in a video. Koppula \etal \cite{DBLP:KoppulaS14} proposed a generative model to ground the affordance into the form of the object's spatial position and time trajectory. Fang \etal \cite{demo2vec2018cvpr} used the feature embedding of the demonstration video to predict the interaction region of the target object and proposed the OPRA (Online Product Review dataset for Affordance) dataset. Nagarajan \etal \cite{interaction-hotspots} learned the interaction between humans and objects by observing videos. Unlike the above methods, we explicitly infer human action intentions from the cues provided by the position and action of a human hand to specify the uniqueness of affordance in a given scene, thus eliminating ambiguity due to the multiple possibilities of affordance.

\begin{figure}[t]
	\centering
		\includegraphics[width=1\linewidth]{figs/figure_2.pdf}
	\caption{\textbf{The pipeline of our method.} Our method first trains a hand-related selection network to select keyframes and then sends the pre-processed video frames to the Hand-aided Affordance Grounding Network (HAG-Net), trained using only action classes as supervisory signals.}
	\label{FIG:2}
\end{figure}

\begin{figure*}[t]
	\centering
		\begin{overpic}[width=1\linewidth]{figs/figure_3.pdf}
		    \put(18, -1.2){\textbf{(a)}}
		    \put(69, -1.2){\textbf{(b)}}
		    \put(6, 20.7){\colorbox{white}{\scriptsize\textbf{OPRA} \cite{demo2vec2018cvpr}}}
            \put(26, 20.7){\colorbox{white}{\scriptsize\textbf{EPIC} \cite{Damen2018EPICKITCHENS}}}
		\end{overpic}
	\caption{\textbf{Hand-aided mask}. (a) Some hand detection results, We use the trained YOLOv3 pruning model \cite{yolov3,Liu2017learning}  to detect the position of the hand on each frame of the two datasets of OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS}. (b) The shadow regions are the surrounding regions of the hand enhanced by hand-aided attention. We extend the bounding box of the hand to the surroundings and remove the regions occluded by the hand. The blue mask on the right side is the hand mask () that we input into the network.}
	\label{FIG:3}
\end{figure*}

\subsection{Hand and Affordance}
There is a large number of researches in the field of hand-based egocentric action recognition. Hand detection, segmentation, and tracking technologies \cite{DBLP:conf/cvpr/LiK13,DBLP:conf/cvpr/Betancourt14,DBLP:journals/cviu/BetancourtMBMRR17} continue to develop and their research results are applied to model actions and activities \cite{DBLP:conf/iccv/BambachLCY15,9060114}. Kjellstrom \etal \cite{DBLP:journals/cviu/KjellstromRK11} learned the affordance of objects from the human demonstration and extracted hand position, orientation, and articulated pose for embedding action features. Stark \etal \cite{DBLP:conf/icvs/StarkLZWS08} obtained the affordance clues from the interaction between the human hand and the objects in the trainset and then determined the functions of the objects according to the affordance cue features. Sun \etal \cite{DBLP:journals/ras/0004RL14} significantly improved the detection accuracy of the interactive objects by learning hand movement trajectories and statistical knowledge in training data. Song \etal \cite{DBLP:conf/icra/SongKOPABK13} predicted the human intention by observing the human/object interaction. Pieropan \etal \cite{DBLP:conf/icra/PieropanEK13} directly represent objects as their interactions with human hands for modeling human activity. In this paper, we use the clues provided by the hand to learn the affordance of the objects.

\subsection{Learning from Demonstrations}
By learning how humans interact with objects through demonstrations, the robot can perceive the affordance of the objects. Furthermore, when faces with objects in different environments, the robot can mimic human actions to interact with the objects through the knowledge learned from human demonstrations. In recent years, there has been a great deal of works on learning from demonstrations \cite{schulman2016learning,chu2016learning,aleotti2011part,zha2021contrastively,demo2vec2018cvpr}.  Schulman \etal \cite{schulman2016learning} proposed a method based on non-rigid trajectory transfer for adapting the demonstrated trajectory from the training geometry to the test geometry, enabling the robot to tie different types of knots in the rope automatically. Chu \etal \cite{chu2016learning} learned different haptic affordances by demonstrating learning to provide a robot with an example of successful interaction with a given object. Aleotti \etal \cite{aleotti2011part} performed the task of robot grasp planning well by combining the limited and automatic 3D shape segmentation of human demonstrators for object recognition and semantic modeling. Fang \etal \cite{demo2vec2018cvpr} proposed to learn from online product review videos and then transfer the knowledge from the videos to the target image to learn the relevant regions of human-object interaction. We also consider learning object affordance from demonstrator videos but only use the action labels of the videos as supervisory signals to learn the mapping relationships from static objects to the object features interacted in the videos.

\section{Method}
\subsection{Problem Description}
In this paper, our goal is to learn the affordance of objects through the demonstration videos while only using the action class as supervision without pixel-level labels. The complete process is shown in Fig. \ref{FIG:2}. Before learning affordance, we first detect the hand in each frame of the video. Some detection results are shown in Fig. \ref{FIG:3} and then introduce a specific selection network. It combines the clues provided by the position and action of the hand to select the keyframes where the hand interacts with the objects. After preprocessing the video datasets, we introduce a Hand-aided Affordance Grounding Network (HAG-Net), as shown in Fig. \ref{FIG:5}. In the affordance learning phase (train phase), we leverage the affordance cues of the object from the demonstration videos and transferring them to a static object. Finally, in the affordance grounding phase (test phase), we feed an object image and an action label into the Hand-aided Affordance Grounding Network (HAG-Net) and output a heatmap of the region on the object where the action may appear using visualization techniques.

\subsection{Pre-processing}
\label{3.1}

Since human-object interactions frequently occur in the demonstration videos, the hand position and actions can offer valuable clues for locating the affordance region of interest. Both OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} have no hand annotations, however, the large amount of existing work on object detection \cite{yolov3,chen2020recursive,ren2016faster} allows us to accurately detect the human hand. We choose the YOLOv3 pruning model \cite{yolov3,Liu2017learning} trained on the Oxford hand dataset \cite{DBLP:conf/bmvc/MittalZT11} to detect the hands in each frame of the two datasets. At most, two hands can be detected in each frame. If the number of detected hands is greater than two, we select the two hands with the highest confidence. Some of the detection results are shown in Fig. \ref{FIG:3} (a). As can be seen, the YOLOv3 model can accurately detect the hand.

\begin{figure*}[t]
	\centering
		\includegraphics[width=0.99\linewidth]{figs/figure_5.pdf}
	\caption{\textbf{The architecture of Hand-aided Affordance Grounding Network (HAG-Net).} (A) Learning object affordance from demonstration videos. (A-1) For video branches, we first extract features using ResNet50 \cite{DBLP:conf/cvpr/HeZRS16}, introduce Hand-aided attention to enhance the hand-related region features, and finally use LSTM to aggregate the features of video frames to obtain action classification results. For the object branch, HAG-Net uses the ResNet50 to extract features, which together with the action label are fed into the semantic enhancement module (SEM). Then, a projection layer transfers the affordance information from the demonstration video to the object image. Finally, we use the distillation loss to align the features of the object branch and those of the video branch. (A-2) The Semantic Enhancement Module (SEM) utilizes a combination of action categories and object apparent features to guide the network to enhance features in areas relevant to action interactions. (B) Affordance grounding. Given an object image and an action label, we use the Grad-CAM to activate the affordance regions of the feature map according to the action (affordance) label.}
	\label{FIG:5}
\end{figure*}

Since many video frames do not contain helpful information about the interaction between the human and the objects, which may cause an undesirable influence on the affordance grounding, therefore, we use a specific hand-related selection network to select the keyframes where the hand interacts with the object and filter out the frames whose intermediate states cannot provide affordance clues. The framework has a similar structure to the video branch in Fig. \ref{FIG:5} (A-1). Since hand position and action cues play an essential role in affordance grounding, we also introduce hand-aided attention in the keyframe selection process to make the network pay attention to hand-aided information. We send the feature maps after CNN into hand-aided attention to enhance the relevant features of human hands, thus making the selected frames more discriminative. We will introduce how hand-aided attention can make use of hand information better in Section \ref{hand-aided}. To capture the action information of the hands in the demonstration videos, we use the Long Short-Term Memory (LSTM) \cite{hochreiter1997long} to aggregate the temporal features. After training, we process the videos in both OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets. Specifically, we randomly choose a starting position and select eight consecutive frames, then feed into the network, and the frame with the highest confidence is selected. We only keep the frame when the action prediction is correct or the confidence is greater than a certain threshold (generally set to 0.3). We repeat this process and select several keyframes for each video. In this way, the retained frames are discriminative and can provide affordance-related information.

\subsection{Affordance Learning}
\label{3.2}
After preprocessing the video, we introduce the Hand-aided Affordance Grounding Network (HAG-Net). It is mainly divided into two parts, as shown in Fig. \ref{FIG:5}. The sub-figure (A) describes the process of affordance learning of objects from demonstration videos, and the sub-figure (B) describes the process of affordance grounding. This section mainly introduces the process of affordance learning, and we will describe the affordance grounding in Section \ref{3.4}. We divide the network into two branches during the training phase to learn how human interacts with the objects in the videos and transfer the knowledge to the object branch. 

\par For the video branch, let a video contain T frames  with an afforded action class . First, we use a ResNet50 \cite{DBLP:conf/cvpr/HeZRS16} backbone to extract the features of each frame, and the features of the video are represented as . We then input  into hand-aided attention (in Section \ref{hand-aided}) to obtain the enhanced feature . After enhancing the relevant features of the human hand, we input them into the LSTM \cite{hochreiter1997long} for aggregating the hand action features:

where  is the feature representation after aggregation. Finally,  is sent to a classifier to predict the action class. 

\par For the object branch, we first extract the features of the object image using the same ResNet50 backbone. Then, the semantic enhancement module  is introduced to enhance the features. Since there is a large gap between the feature of the interacting object and the static object, we use a projection layer  to learn the object feature representation  when the interaction occurs:

 is the feature map of the object image after ResNet50 \cite{DBLP:conf/cvpr/HeZRS16}.

Finally, we introduce a distillation loss () to align the feature space of static objects to the feature space of human-object interaction, from the affordance of the objects in the demonstration video to the static objects. To ensure that the feature representation of the object after the projection layer has the same classification performance as the video branch, we send the output of the projection layer to the same LSTM and action classifier and calculate the classification loss of the object branch. The total loss () is mainly composed of the following three parts:

where , , and  are hyper-parameters that balance the loss items.  and  represent the cross-entropy loss of video branch and object branch for action classification, respectively. 
In the following sections, we present the details of \textbf{Hand-Aided Attention}, \textbf{Semantic Enhancement Module}, \textbf{Projection Layer}, and \textbf{Distillation Loss}, respectively.

\myPara{Hand-Aided Attention.}
\label{hand-aided}
We use the results of Section \ref{3.1} to enhance the features of the hand regions. Since we need the context information of the hands and the objects, we expand the bounding box of the detected hand and exclude the hand regions that may occlude the object. The expansion and exclusion regions are controlled by parameters  () and  (). As shown in Fig. \ref{FIG:3} (b),  and  are the height and width of the hand bounding box, respectively. The expansion region is the bounding box enclosed by  and , while the exclusion region is the bounding box enclosed by  and . The hand mask () used in our network is shown in the right part of Fig. \ref{FIG:3} (b). We set  to 1 when calculating the hand-aided attention in the selection network and do not exclude the hand region. While in the affordance learning phase, since it will affect the extraction of affordance clues, we set  to 0.4 accordingly. After obtaining , we calculate the masked feature  by multiplying it with the feature map  of the video frame via an position-wise dot product operator :


Then the feature representation of each frame and the mask of the hand are respectively passed through L2 pooling layer () and added to obtain the enhanced feature :



where  and  are the feature representations of the video frame and the masked feature after pooling, respectively.

\label{SEM}
\myPara{Semantic Enhancement Module.}
 
Many previous studies \cite{singh2019hetconv,sun2019high,wu2018group} have demonstrated that the features learned by the grouping mechanism are more compact and have better representational power. Therefore, we introduce an Semantic Enhancement Module (SEM) to improve the feature representation capability of object branches by leveraging a group-wise feature enhancement mechanism. Furthermore, within each grouping, the feature of the object image is modulated jointly by action categories and global features, thus enabling the network to focus more on affordance-related features. Our SEM is shown in Fig. \ref{FIG:5} (A-2), the feature map of size  is divided into G groups along the channel dimension. The feature of the th position of the th group is represented as , . 

Besides, we use a fully connected layer  to map the one-hot action label to have  channels and use a Softmax layer to get the action class weights :

where  is the one-hot representation of action class. We also divide  into G groups.
Then, we use a global average pooling to aggregate the global statistical feature from  and add it with  to obtain the semantic vector  of the th group:


Then, we calculate the correlation coefficient  between  and  and normalize it at each position as follows:




where  means the element-wise multiplication operation,  and  represent the mean and standard deviation of the group of coefficients, respectively, and  is a regularization constant. After that, we introduce  and , which scale and shift the normalized value :

where  denotes the th group of parameters. Finally, we input  to a sigmoid function gate , and multiply it with the original feature representation  to get the enhanced feature representation :


\label{projection}
\myPara{Projection Layer and Distillation Loss.}

We introduce a projection layer and a distillation loss to align the feature representations of the object branch and the video branch, reducing the distance in feature space between the object in the static image and the object in the scene of human-object interaction in the video. During aligning the feature representation of interaction between humans and objects to that of static objects, we mainly focus on two factors: 1) the highest-confidence frame  in the input video contains the affordance clues of the hand's position, and 2) the average feature of the input contains the hand context. 

Specifically, in the process of obtaining the features of , we determine  using the following equation:

where  is the cross-entropy loss for classification using the hidden state of LSTM at time  (each frame of input). The average feature of the video is represented as :


We define a distillation loss () to adjust the projection layer so that it can align the feature representation of static object and that of interactive object:

where  is the L2 pooling operation,  and  are the loss weights.  is the key to ensure that the object image is mapped to the feature space of character interactions. 

\subsection{Affordance Grounding}
\label{3.4}
\par In Affordance Grounding process, the input is just a static object image, and our goal is to infer the interaction regions of all possible actions on this object image. For each action class , the model generates a heatmap of the position where the human interacts with the object, as shown in Fig. \ref{FIG:5} (B). In this paper, we use the Grad-CAM \cite{selvaraju2017grad} feature visualization technique to establish the mapping between action labels to regions related to human-object interaction. In the ablation study in Section \ref{ablation}, we also compare other advanced visualization methods and prove that Grad-CAM can achieve the best results. The detailed approach is as follows: 1) for a specific object image encoding  and action class , the sensitivity of the action relative to each channel of the last layer of the feature map is calculated, which can be regarded as the attention mask of the feature map of this layer; 2) unlike Grad-CAM, we directly combine it with the weighted linear combination of the last layer of the feature map instead of taking the gradient map to the mean value; and 3) we input it to the ReLU activation function for further processing. Using the ReLU function can help our model focus on the pixels that have a positive impact on a particular category:

where  is the element-wise multiplication operator,   is the th channel of the input embedding, and  is a two-dimensional attention mask.  represents the final interaction heatmap for the given action class.

\section{Experiments}
In our experiments, we explore the following questions: 
\begin{itemize}
    \item []
     \textbf{Does our method outperform other weakly or fully supervised methods on the affordance grounding task?} (In Section \ref{results_analysis})
    \item []
     \textbf{Does our method have excellent generalization performance on unseen objects?} (In Section \ref{results_analysis})
    \item []
     \textbf{What is the influence of each module of our model and different visualization strategies on affordance grounding?} (In Section \ref{ablation})
\end{itemize}
\par We chose the OPRA \cite{demo2vec2018cvpr} dataset for the third-person perspective, and the EPIC \cite{Damen2018EPICKITCHENS} dataset for the first-person perspective to evaluate the effectiveness of the model in terms of both objective metrics as well as subjective visualization. In the ablation study, we explore different visualization strategies and investigate the impact of each module of our model on affordance grounding.

\subsection{Experimental Setup}

\myPara{Datasets.}
Our main goal is to use human demonstration videos to learn how people interact with objects. We need datasets that contain a large number of demonstration videos in which people interact with various objects. To this end, we conducted our experiments on the following two datasets.

\begin{itemize}

\item [] 
\textbf{Online Product Review dataset for Affordance (OPRA)} \cite{demo2vec2018cvpr}: Fang \etal \cite{demo2vec2018cvpr} proposed the OPRA dataset, which aims to use demonstration videos for object affordance inference. Each sample contains a video, an object image, affordance class, and annotations of the interacting regions on the object image.

\item [] 
\textbf{EPIC-KITCHENS (EPIC)} \cite{Damen2018EPICKITCHENS}: The dataset contains a large number of egocentric videos of activities in kitchens. Each clip contains an action label and an object. Moreover, each frame has a bounding box of the object that interacts with the person. In this paper, we use the data annotated in \cite{interaction-hotspots}.

\end{itemize}

\myPara{Evaluation Metrics.}
To evaluate the results of different models comprehensively, we choose four metrics from \cite{bylinskii2018different}, including KLD \cite{bylinskii2018different}, SIM \cite{swain1991color}, AUC-J \cite{DBLP:conf/iccv/JuddEDT09}, NSS \cite{peters2005components}.

\begin{table*}[!t]
  \begin{center}
  \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{2.8pt}
   \caption{\textbf{Results of the proposed method and several weakly supervised methods}, including Saliency detection methods (Egogaze \cite{DBLP:conf/eccv/HuangCLS18}, Mlnet \cite{DBLP:conf/icpr/CorniaBSC16}, DeepgazeII \cite{DBLP:journals/corr/KummererWB16}, Salgan \cite{Pan_2017_SalGAN}) and Hotspot \cite{interaction-hotspots}, and two affordance detection methods using mask labels as strong supervision during training, including Demo2vec \cite{demo2vec2018cvpr} and Affsegnet \cite{interaction-hotspots}, on the OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets. , , , and  represent the four evaluation metrics KLD \cite{bylinskii2018different}, SIM \cite{swain1991color}, AUC-J \cite{DBLP:conf/iccv/JuddEDT09}, and NSS \cite{peters2005components}, respectively.  / indicates higher/lower results are better. 
   }
   \label{Table:1}
  \begin{tabular}{r|cccc|cccc|cccc|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\quad} & \multicolumn{8}{c|}{\textbf{Affordance Grounding}} & \multicolumn{8}{c}{\textbf{Generalization to Novel Objects}} \\ 
    \hline
    \multicolumn{1}{c|}{Dataset} & \multicolumn{4}{c|}{OPRA \cite{demo2vec2018cvpr}} & \multicolumn{4}{c|}{EPIC \cite{Damen2018EPICKITCHENS}} & \multicolumn{4}{c|}{OPRA \cite{demo2vec2018cvpr}} & \multicolumn{4}{c}{EPIC \cite{Damen2018EPICKITCHENS}}\\ 
    \hline
    \multicolumn{1}{c|}{Method} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} &  \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} \\ 
    \hline
    Center bias &  &  &  &  &  &  & 	&  &  &	 &  &  & 	 &  &  &   \\
   Egogaze \cite{DBLP:conf/eccv/HuangCLS18} & 	\cellcolor{gray!20} &  &  & 	 &	 \cellcolor{gray!20} &  &  &  & \cellcolor{gray!20} &  &	 &  & \cellcolor{gray!20} &  &  &     \\
   Mlnet \cite{DBLP:conf/icpr/CorniaBSC16}	&  &	\cellcolor{gray!20}	& \cellcolor{gray!35} &	\cellcolor{gray!35} &  & \cellcolor{gray!20} & \cellcolor{gray!20} &	\cellcolor{gray!20} &  & \cellcolor{gray!20} & \cellcolor{gray!35} &	\cellcolor{gray!35} &  & \cellcolor{gray!20} & \cellcolor{gray!35} &	\cellcolor{gray!20}   \\
   DeepgazeII \cite{DBLP:journals/corr/KummererWB16}	& \cellcolor{gray!50} & \cellcolor{gray!35} & \cellcolor{gray!20} & \cellcolor{gray!20} & \cellcolor{gray!50} & \cellcolor{gray!35} & \cellcolor{gray!35} & \cellcolor{gray!35} & \cellcolor{gray!35} & \cellcolor{gray!35} & \cellcolor{gray!20} & \cellcolor{gray!20} & \cellcolor{gray!35} & \cellcolor{gray!35} & \cellcolor{gray!20} & \cellcolor{gray!35}   \\
   Salgan \cite{Pan_2017_SalGAN}	& \cellcolor{gray!35} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!35} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!70} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!50} & \cellcolor{gray!70} & \cellcolor{gray!50} & \cellcolor{gray!50}    \\
   Hotspot \cite{interaction-hotspots} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!50} & \cellcolor{gray!70} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!70} & \cellcolor{gray!70} & \cellcolor{gray!50} & \cellcolor{gray!70} & \cellcolor{gray!70}   \\
   \hline
   Ours & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!70} & \cellcolor{gray!70}	 & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95} & \cellcolor{gray!95}    \\
   \hline
   Img2heatmap \cite{interaction-hotspots} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &    \\

   Demo2vec\cite{demo2vec2018cvpr} &  &  &  &  & - & - & - & - & - & - & - & - & - & - & - & - \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
  \end{table*}

\begin{figure*}[t]
	\centering
		\begin{overpic}[width=0.99\linewidth]{figs/figure_6.pdf}
          \put(21.5,59.3){\colorbox{white}{\textbf{OPRA} \cite{demo2vec2018cvpr}}}
          \put(71,59.3){\colorbox{white}{\textbf{EPIC} \cite{Damen2018EPICKITCHENS}}}
          \put(-0.5,52){\footnotesize \textbf{\rotatebox{90}{GT}}}
          \put(-0.5,41){\footnotesize \textbf{\rotatebox{90}{ Ours}}}
          \put(-0.5,31){\footnotesize \textbf{\rotatebox{90}{Hotspot}}}
          \put(1,31.5){\footnotesize \textbf{\rotatebox{90}{ \cite{interaction-hotspots}}}}
          \put(-0.5,22){\footnotesize \textbf{\rotatebox{90}{Saliency}}}
          \put(1,21){\footnotesize \textbf{\rotatebox{90}{ \cite{Pan_2017_SalGAN,DBLP:journals/corr/KummererWB16,DBLP:conf/icpr/CorniaBSC16,DBLP:conf/eccv/HuangCLS18}}}}
          \put(-0.5,10.5){\footnotesize \textbf{\rotatebox{90}{Img2heatmap}}}
          \put(1,13.5){\footnotesize \textbf{\rotatebox{90}{ \cite{interaction-hotspots}}}}
          \put(-0.5,1){\footnotesize \textbf{\rotatebox{90}{Demo2vec}}}
          \put(1,3){\footnotesize \textbf{\rotatebox{90}{ \cite{demo2vec2018cvpr}}}}
		\end{overpic}
		\caption{\textbf{Visualization of the affordance heatmaps generated by different methods,} including Hotspot \cite{interaction-hotspots}, Saliency (Egogaze \cite{DBLP:conf/eccv/HuangCLS18}, Mlnet \cite{DBLP:conf/icpr/CorniaBSC16}, DeepgazeII \cite{DBLP:journals/corr/KummererWB16} and Salgan \cite{Pan_2017_SalGAN}), Img2heatmap \cite{interaction-hotspots} and Demo2vec \cite{demo2vec2018cvpr}.
		}
	\label{FIG:6}
\end{figure*}

\begin{itemize}

\item []
\textbf{KLD }\cite{bylinskii2018different}: Kullback-Leibler Divergence (KLD) is used to measure the distribution difference between the prediction map and the target map. Given a prediction map  and a ground truth map ,  is computed as follows:

where  is a regularization constant.

\item []
\textbf{SIM }\cite{swain1991color}: The similarity metric (SIM) measures the similarity between the prediction map and the ground truth map. Given a prediction map  and a continuous ground truth map ,  is computed as the sum of the minimum values at each pixel, after normalizing the input maps:


\item []
\textbf{AUC-J }\cite{DBLP:conf/iccv/JuddEDT09}: AUC-Judd (AUC-J) is a variant of AUC proposed by Judd \etal \cite{DBLP:conf/iccv/JuddEDT09}. It measures the relative prediction map values at ground truth locations.

\item []
\textbf{NSS }\cite{peters2005components}: The Normalized Scanpath Saliency measures the correspondence between the prediction map and the ground truth, and it treats false positives and false negatives symmetrically. Given a prediction map  and a binary ground truth map ,  computes the average normalized prediction at ground truth locations:


\end{itemize}

\myPara{Implementation Details.}
Each video in the OPRA \cite{demo2vec2018cvpr} dataset has an object image associated with it, while the EPIC \cite{Damen2018EPICKITCHENS} dataset does not. Thus, for the EPIC dataset, we use the provided bounding box to crop the object from the video frame according to \cite{interaction-hotspots} and randomly select an image whose class matches the object class in the video. Due to the background of EPIC is more complex and the apparent features between the objects in the video and the objects in the object image are different, we replace L2 loss with triplet loss when calculating . The triplet can better utilize positive and negative samples for feature learning so that the same category is close in the space and different categories are far away in the space. Closing the gap of features belonging to the same category can help the network better focus on interacting objects and ignore irrelevant backgrounds. The  is calculated as follows:

\par We set  on the two datasets. For OPRA dataset, we set , , and . For EPIC dataset, we set , , and . We carry out all experiments on 1080Ti, and set the batch size and learning rate to 128 and 1e-4, respectively. During training the hand-related selection network, the video input is eight frames, while during affordance learning from demonstration videos, the video input is three frames. Furthermore, we set the stride of the last two residual stages of ResNet50 to 1 and using dilated convolutions in the convolutional layers. In this way, the output spatial resolution of ResNet50 is 1/8 of the input.

\subsection{Contenders}
We compare the performance of our method with several state-of-the-art methods on OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS}. It is worth to notice that we select a series of saliency detection models as the comparison methods, because the human visual system has the ability to quickly orient attention to the most informative parts of visual scenes and the research on salient object detection is derived from this ability of the human visual system to extract the most attention-grabbing objects from the image. These saliency detection models include Egogaze \cite{DBLP:conf/eccv/HuangCLS18}, Mlnet \cite{DBLP:conf/icpr/CorniaBSC16}, DeepgazeII\cite{DBLP:journals/corr/KummererWB16}, and Salgan\cite{Pan_2017_SalGAN}. The generated heatmaps by them represent the parts of the object first noticed by the human visual system. We directly use the trained saliency detection models for test. In addition, we also choose the Hotspot \cite{interaction-hotspots}, Demo2vec \cite{demo2vec2018cvpr}, and Img2heatmap \cite{interaction-hotspots} models, where our model, Hotspot, and the saliency detection models are all weakly supervised, while Demo2vec and Img2heatmap are fully supervised. All these methods are briefly described as follows: 

\begin{itemize}

\item [] 
\textbf{Center bias}: It generates a gaussian heatmap at the center of an image. Thus, it is a simple baseline for a dataset with features of central bias. 

\item [] 
\textbf{Egogaze}\cite{DBLP:conf/eccv/HuangCLS18}: It is a hybrid gaze prediction model that exploits both the visual saliency of bottom-up and task-dependent attention transition and is the first work to explore the attention transition model in the egocentric gaze prediction task and achieves state-of-the-art results in gaze prediction.

\item [] 
\textbf{Mlnet}\cite{DBLP:conf/icpr/CorniaBSC16}: Unlike previous works that predict saliency maps directly from the last layer of convolution neural network, the model fuses features extracted from different layers of the CNN. Their method contains three main blocks: feature extraction CNN, feature encoding network (weighting of low and high features), and a prior learning network. The method achieves promising results in all datasets for saliency detection.

\item []
\textbf{DeepgazeII}\cite{DBLP:journals/corr/KummererWB16}: Unlike other saliency models, DeepGazeII does not perform additional fine-tuning of the VGG features and only trains some output layers to predict saliency on top of VGG.

\item []
\textbf{Salgan}\cite{Pan_2017_SalGAN}: It introduces the adversarial training mechanism of GAN for salient object prediction, which consists of two main parts: one predicts saliency maps based on the input image, and the other discriminates whether the input is the prediction result or the grounding truth. It explores the application of GAN to salient object detection and achieves excellent results on relevant datasets. 

\item []
\textbf{Hotspot} \cite{interaction-hotspots}: It is a weakly supervised way to learn the affordance of an object through video, and affordance grounding is achieved only through action labels. 

\item []
\textbf{Demo2vec} \cite{demo2vec2018cvpr}: It is a fully supervised way to learn the affordance of objects by learning the coded representation in the demonstration video to predict the regions in the object image that also interact with each other. The network's input is a demonstration video and an object image, and the output is the action category of the demonstration video and the interacting regions.

\item []
\textbf{Img2heatmap} \cite{interaction-hotspots}: The encoder is a VGG16 \cite{DBLP:journals/corr/SimonyanZ14a} pre-trained on Imagenet \cite{DBLP:conf/cvpr/DengDSLL009}. The decoder is a mirrored VGG16, where the max-pooling is replaced by the upsampling operation. The network's final output is a heatmap of the same size as the input, and the number of channels is the same as the action class.

\end{itemize}

\subsection{Results Analysis}
\label{results_analysis}
In this section, we compare the ability of different models for affordance grounding on two datasets and their generalization abilities on unseen objects, as well as discuss the compatibility of our model for datasets with first- and third-person view videos. Finally, we discuss the effectiveness of our model for handling affordance with multiple possibilities.

\myPara{Affordance Grounding Results.} 
We compare our method with the state-of-the-art methods on OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets, the results are summarized in the left part of Table \ref{Table:1}. Our method surpasses all other weakly supervised methods in all metrics and is close to the supervised Demo2vec \cite{demo2vec2018cvpr} and Img2heatmap \cite{interaction-hotspots}. It proves that our method utilizes the affordance cues provided by hand position and action can achieve promising results. We also visualize the heatmaps generated by different methods in Fig. \ref{FIG:6}. Our method generates heatmaps that are closer to ground truth than those of Hotspot and saliency detection models. And there is no large response on object parts that are not related to actions. The results on some objects are even better than those of the supervised Img2heatmap \cite{interaction-hotspots} and Demo2vec \cite{demo2vec2018cvpr} methods. It shows that our method transfers the affordance cues from the hand to the static object, which can make the network pay more attention to the regions related to the affordance while suppressing the regions unrelated to the action.

\myPara{Generalization to Novel Objects.}
To verify the generalization ability of our method on new objects, we re-divide the datasets according to \cite{interaction-hotspots}. The results are shown in Table \ref{Table:1} (right part). On EPIC \cite{Damen2018EPICKITCHENS} dataset, our model outperforms all other methods in all metrics. On OPRA \cite{demo2vec2018cvpr} dataset, our method is superior to other methods in most metrics. It demonstrates that our method can predict the region of interaction on unseen objects. This is because the cues provided by our method from hand motion and position information enable the network to pay more attention to the local details of the particular affordance class of objects, thus having better generalization performance.

\myPara{Compare the Results of Merging Two Datasets.}
To verify whether our method can perform better on a more complex multi-view dataset, we merge the OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets. The experimental results are shown in Table \ref{Table:2}. Our method is superior to other methods in all metrics, suggesting that our method can also transfer hand position and action clues to static objects in a complex dataset. This is due to the fact that our method focuses more on hand-object interactions and is not too sensitive to changes in viewpoint and can be more robust to complex backgrounds.

\par We compared the results of ``Affordance grounding'', ``Generalization to novel objects'', and `` Merging two datasets'' for three task settings with 20 metrics, ranked the results of various methods on each metric in each task setting, and summarized the result as a matrix, where each element (i,j) indicates how many metrics the model has ranked the th. The results are shown in Fig. \ref{FIG:7}, from which we can see that our method achieves promising results in different task settings, with stronger robustness and generalization ability.

\begin{table}[t]
    \caption{\textbf{The results of different methods on the mixture of OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets}, , a dataest with images at different views.
    }
    \label{Table:2}
  \begin{center}
  \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{8pt}
  \begin{tabular}{r|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\textbf{Dataset}} & \multicolumn{4}{c}{\textbf{OPRA} \cite{demo2vec2018cvpr} \textbf{+} \textbf{EPIC} \cite{Damen2018EPICKITCHENS}} \\ 
    \hline
    \multicolumn{1}{c|}{Method} & \emph{} & \emph{} & \emph{} & \emph{} \\
    \hline
    Center bias &  &  &  &  \\	
    Egogaze\cite{DBLP:conf/eccv/HuangCLS18} &	 &  &  &    \\
    Mlnet\cite{DBLP:conf/icpr/CorniaBSC16} &  &  &  &   \\
    DeepgazeII\cite{Pan_2017_SalGAN} &  &  &  &   \\
    Salgan\cite{DBLP:journals/corr/KummererWB16} &  &  &  &    \\
    Hotspot\cite{interaction-hotspots} &  &  &  &  \\
     \hline
     \rowcolor{mygray}
    Ours &  &  &  &    \\    
     \hline
    Img2heatmap \cite{interaction-hotspots} & 1.444 & 0.358 & 0.769 & 0.811 \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
  
  \end{table}

\begin{figure}[t]
  \begin{minipage}[b]{1.0\linewidth}
    \begin{center}
      \begin{overpic}[width=1\linewidth]{figs/fig9.pdf}
          \put(29,2){\colorbox{white}{1}}
          \put(38,2){\colorbox{white}{2}}
          \put(47,2){\colorbox{white}{3}}
          \put(56,2){\colorbox{white}{4}}
          \put(65,2){\colorbox{white}{5}}
          \put(74,2){\colorbox{white}{6}}
          \put(83,2){\colorbox{white}{7}}
          \put(92,2){\colorbox{white}{8}}
          
          \put(18,75){\small Ours}
           \put(17,71){\small (\textcolor{red}{1.20})}
          \put(7,66){\small Hotspot \cite{interaction-hotspots} }
          \put(17,62){\small (\textcolor{red}{2.20})}
          \put(-1,57){\small Img2heatmap \cite{interaction-hotspots}}
          \put(17,53){\small (\textcolor{red}{3.45})}
          \put(9,48){\small Salgan \cite{Pan_2017_SalGAN}}
          \put(17,44){\small (\textcolor{red}{3.65})}
          
          \put(2,39){\small DeepgazeII \cite{DBLP:journals/corr/KummererWB16}}
          \put(17,35){\small (\textcolor{red}{4.90})}
          \put(10,30){\small Mlnet \cite{DBLP:conf/icpr/CorniaBSC16}}
          \put(17,26){\small (\textcolor{red}{5.75})}
          \put(6,21){\small Egogaze\cite{DBLP:conf/eccv/HuangCLS18}}
          \put(17,17){\small (\textcolor{red}{7.05})}
          \put(9,12){\small Center bias}
          \put(17,8){\small (\textcolor{red}{7.65})}
        \end{overpic}
    \end{center}
    \caption{\textbf{Rank list.} We rank the 20 results of different methods on the tasks of \textbf{``Affordance grounding''}, \textbf{``Generalization to novel object''}, and \textbf{``Merging two datasets''} in Table \ref{Table:1} and Table \ref{Table:2}, where the element (, ) indicates how many metrics that the model  are ranked the th. The \textcolor{red}{red} letter denotes the average rank. 
    }
    \label{FIG:7}
  \end{minipage}
\end{figure}
 
\begin{table}[t]
\caption{\textbf{Results of different methods on twelve most frequent classes in the EPIC \cite{Damen2018EPICKITCHENS} dataset.} The best performing method is highlighted in \textbf{bold}.
}
\label{Table:3}
  \begin{center}
  \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{8pt}
  \begin{tabular}{r|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\textbf{Dataset}} & \multicolumn{4}{c}{\textbf{EPIC} \cite{Damen2018EPICKITCHENS}} \\ 
    \hline
    \multicolumn{1}{c|}{Method} & \emph{} & \emph{} & \emph{} & \emph{} \\ 
    \hline
    Center bias &	 &  &  &  \\
    Egogaze\cite{DBLP:conf/eccv/HuangCLS18} &	 &  &  &    \\
    Mlnet\cite{DBLP:conf/icpr/CorniaBSC16} &  &  &  &   \\
    DeepgazeII\cite{Pan_2017_SalGAN} &  &  &  &   \\
    Salgan\cite{DBLP:journals/corr/KummererWB16} &  &  &  &    \\
    Hotspot \cite{interaction-hotspots} &	 &  &  &	  \\
    \hline
    \rowcolor{mygray}
    Ours   &  &  &  &  \\
    \hline
    Img2heatmap \cite{interaction-hotspots} &  &  &  &  \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
\end{table}

\begin{figure}[t]
	\centering
		\begin{overpic}[width=1\linewidth]{figs/figure_7.pdf}
		    \put(36, 46){\colorbox{white}{\textbf{(a) Hotspot} \cite{interaction-hotspots}}}
		    \put(42, 1){\colorbox{white}{\textbf{(b) Ours}}}
		\end{overpic}
	\caption{\textbf{The confusion matrices of Hotspot \cite{interaction-hotspots} and our method for action prediction on the EPIC dataset \cite{Damen2018EPICKITCHENS}.} We select eight most frequent classe.}
	\label{FIG:8}
\end{figure}

 \begin{figure*}[t]
	\centering
		\begin{overpic}[width=1\linewidth]{figs/fig_x.pdf}
		\put(23.5,-0.5){ \textbf{(a)}}
		\put(74,-0.5){ \textbf{(b)}}
		\end{overpic}
		\caption{\textbf{Visualization of heatmaps generated by our method when handling affordance with multiple possibilities.}
		(a) Different interactions may occur in different regions of the object. (b) Multiple possible interactions take place at the same location of the object.
		}
	\label{FIG:x}
\end{figure*}
  
\myPara{Comparison on 12 Most Frequent Classes of EPIC.}
Since the data distribution in the EPIC \cite{Damen2018EPICKITCHENS} dataset is highly unbalanced, e.g., the number of samples for some categories is less than one percent. The experimental results are more affected by the classes that contain a large number of samples. To verify the effectiveness of our method in the case of a large number of samples, we compare our method with others on the twelve most frequent classes. The experimental results are shown in Table \ref{Table:3}. Our method outperforms all other methods, implying that our method can better locate the region where people interact with objects in more samples.

\myPara{Comparison of the Action Classification Accuracy.}
Since our method and hotspot \cite{interaction-hotspots} only use action class as supervision during affordance learning, the accuracy of action recognition affects the results of affordance grounding. We compare the performance of action classification between ours and hotspot \cite{interaction-hotspots} on the EPIC \cite{Damen2018EPICKITCHENS} dataset. We only choose the eight most frequent classes and calculate the confusion matrix, as shown in Fig. \ref{FIG:8}. Our method performs better on ``open'', ``close'', ``take'', ``put'', and ``wash'', demonstrating that it can better distinguish different actions by combining hand position and action-related affordance cues to improve the result of action classification.

\begin{table*}[t]
\caption{\textbf{Comparison of different visualization approaches,} including Grad-CAM++ \cite{chattopadhay2018grad}, XGrad-CAM \cite{fu2020axiom}, and Grad-CAM \cite{selvaraju2017grad} for obtaining the object affordance region.
}
  \begin{center}
  \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{1.pt}
  \begin{tabular}{r|cccc|cccc|cccc|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\quad} & \multicolumn{8}{c|}{\textbf{Affordance Grounding}} & \multicolumn{8}{c}{\textbf{Generalization to Novel Objects}} \\ 
    \hline
    \multicolumn{1}{c|}{Dataset} & \multicolumn{4}{c|}{OPRA \cite{demo2vec2018cvpr}} & \multicolumn{4}{c|}{EPIC \cite{Damen2018EPICKITCHENS}} & \multicolumn{4}{c|}{OPRA \cite{demo2vec2018cvpr}} & \multicolumn{4}{c}{EPIC \cite{Damen2018EPICKITCHENS}}\\ 
    \hline
    \multicolumn{1}{c|}{Method} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} &  \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} \\ 
    \hline
   HAG-Net \& Grad-CAM++ \cite{chattopadhay2018grad} &  &  &  &   &  &  &  &  &  &  &  &  &  &  &  &  \\
   HAG-Net \& XGrad-CAM \cite{fu2020axiom} &  &  &  &   &  &  &  &  &  &   &  &  &  &  &  &  \\
   \hline
   \rowcolor{mygray}
   HAG-Net \& Grad-CAM (Ours) &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &     \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
  \label{Table:4}
  \end{table*}
  
\begin{figure*}[t]
	\centering
		\begin{overpic}[width=1\linewidth]{figs/fig_10.png}
          \put(23.6,44.4){\colorbox{white}{\textbf{OPRA} \cite{demo2vec2018cvpr}}}
          \put(71.3,44.4){\colorbox{white}{\textbf{EPIC} \cite{Damen2018EPICKITCHENS}}}
          \put(2,36){\footnotesize \textbf{\rotatebox{90}{GT}}}
          \put(0.5,24){\footnotesize \textbf{\rotatebox{90}{ HAG-Net \&}}}
          \put(2,24){\footnotesize \textbf{\rotatebox{90}{ Grad-CAM}}}
          \put(3.5,25){\footnotesize \textbf{\rotatebox{90}{ (Ours)}}}
          \put(0.5,14){\footnotesize \textbf{\rotatebox{90}{ HAG-Net \&}}}
          \put(2,14){\footnotesize \textbf{\rotatebox{90}{XGrad-CAM}}}
          \put(3.5,17){\footnotesize \textbf{\rotatebox{90}{ \cite{fu2020axiom}}}}
          \put(0.5,3){\footnotesize \textbf{\rotatebox{90}{ HAG-Net \&}}}
          \put(2,3){\footnotesize \textbf{\rotatebox{90}{Grad-CAM++}}}
          \put(3.5,6.5){\footnotesize \textbf{\rotatebox{90}{ \cite{chattopadhay2018grad}}}}
		\end{overpic}
		\caption{\textbf{Heatmaps by using different visualization approaches} including Grad-CAM++ \cite{chattopadhay2018grad}, XGrad-CAM \cite{fu2020axiom}, and Grad-CAM \cite{selvaraju2017grad} in our method.
		}
	\label{FIG:9}
\end{figure*}

\myPara{Affordance with Multiple Possibilities.} We also show the visual heatmap results to demonstrate that our model can address the challenge of multiple possibilities of affordance well. As shown in Fig. \ref{FIG:x} (a), different actions interact with different regions of same the object, and our model can localize to different regions of the object based on the action category. Fig. \ref{FIG:x} (b) shows that different interactions occur in the same region of the object, and our model can also accurately localize to the region associated with the affordance of the interaction. It demonstrates that our model can reason about human intentions through human actions and positions and well address the challenge of ambiguity in affordance.

\subsection{Ablation Studies}
\label{ablation}

\myPara{Impact of Visualization Strategy.}
In this section, we also compare two more advanced visualization methods, i.e., XGrad-CAM \cite{fu2020axiom} and Grad-CAM++ \cite{chattopadhay2018grad}, to verify the effectiveness of Grad-CAM \cite{selvaraju2017grad} used in our method for affordance grounding both subjectively and objectively.

\begin{itemize}

\item []
\textbf{XGrad-CAM }\cite{fu2020axiom}: This paper introduces two axioms: sensitivity \cite{sundararajan2017axiomatic} and consistency \cite{montavon2018methods}, and makes XGrad-CAM satisfy these two constraints. XGrad-CAM is a visualization method with class discriminative ability to highlight the relevant regions belonging to the class, which is calculated as follows:



\item []
\textbf{Grad-CAM++ }\cite{chattopadhay2018grad}: Grad-CAM++ is a more generalized approach based on Grad-CAM and is capable of predicting better visual interpretation of CNN models and better visual localization. Furthermore, it provides a way to measure the importance of each pixel in the feature map to the overall decision of the CNN by introducing a pixel weighting of the gradients of the output to get better visualization results, which is calculated as follows:




\end{itemize}

The experimental results are shown in Table \ref{Table:4}. As can be seen, using Grad-CAM in our method outperforms both of the remaining two more advanced visualization methods. In the affordance grounding task setting, it outperforms XGrad-CAM \cite{fu2020axiom} and Grad-CAM++ \cite{chattopadhay2018grad} by 13.3\% and 18.9\% in terms of NSS \cite{peters2005components} metrics on the OPRA \cite{demo2vec2018cvpr} dataset, respectively. On the EPIC \cite{Damen2018EPICKITCHENS} dataset, it exceeds XGrad-CAM by 14.6\% and surpasses Grad-CAM++ by 30.8\% in terms of NSS metrics. These results demonstrate that our method can obtain more accurate region localization. Some of the visualization results are shown in Fig. \ref{FIG:9}. The heatmaps generated by XGrad-CAM and Grad-CAM++ are large and contain a large number of irrelevant interaction regions, while our results can better focus on affordance-related regions, probably because their calculations focus more on the object as a whole, whereas our method can retain local details to better focus on affordance-related local regions of the object.

\begin{table*}[t]
\caption{\textbf{Ablation results on both OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets.} For details please refer Section \ref{ablation}.}
  \label{Table:5}
  \begin{center}
  \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{12.1pt}
  \begin{tabular}{r|cccc|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\textbf{Dataset}} & \multicolumn{4}{c|}{\textbf{OPRA} \cite{demo2vec2018cvpr}} & \multicolumn{4}{c}{\textbf{EPIC} \cite{Damen2018EPICKITCHENS}} \\ 
    \hline
    \multicolumn{1}{c|}{Method} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} & \emph{} \\ 
    \hline
    Random (a) &  &  & 	&  &  &	 &	  &  \\
    Random (b) &  &  &  &  &  &	 &	  &  \\
    w/o hand-aided attention (a) &  &		&  &	 &  & 	 &  &	 \\
    w/o hand-aided attention (b) &  &  &	 &  & 	 &   &  &   \\
    w/o hand-aided attention (c) &  &  &  &  &  & 	&  &	   \\
    w/o select frame &  &  &  &  &  &  &  &  \\
    \hline
    Max score   &  &  &  &  &  &  &  &  \\
    Average    &  &  &  &  &  &  &  &  \\
    \hline
    w/o SEM &  &		&  & 	 &  &  &	 &  \\
    \hline
    \rowcolor{mygray}
    Ours &  &	 &  &  &  &  &  &     \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
 \end{table*}

\myPara{Effectiveness of Different Modules.}
In this section, We investigate the impact of each module in our method on affordance grounding.

\begin{itemize}
\item []
\textbf{Random (a)}: In the preprocessing phase, a single frame is randomly chosen from the input and the process is iterated three times. We also remove the hand-aided attention during the affordance learning phase.

\item []
\textbf{Random (b)}: In preprocessing, a single frame is randomly chosen from the input and this process is iterated three times. We keep the network structure for affordance learning as in Fig. \ref{FIG:5} (A).

\item []
\textbf{w/o hand-aided attention (a)}: We remove the hand-aided attention in the selection network during the preprocessing process, and also remove the hand-aided attention in the HAG-Net.

\item []
\textbf{w/o hand-aided attention (b)}: We remove the hand-aided attention in the selection network during the preprocessing process, but keep it for affordance learning as shown in Fig. \ref{FIG:5}.

\item []
\textbf{w/o hand-aided attention (c)}: There is no change in the pre-processing stage. But during the affordance learning phase, the hand-aided attention of HAG-Net is removed.

\item[]
\textbf{w/o select frame}: We input 8 video frames into our HAG-Net without selecting keyframes.

\item []
\textbf{Max frame}: We use the frame with the highest confidence when calculating the distillation loss.

\item []
\textbf{Average frames}: We take the average of the input three frames when calculating the distillation loss.

\item []
\textbf{w/o SEM}: We remove the SEM from the object branch of the affordance learning network.
\end{itemize}

\par The ablation study results are shown in Table \ref{Table:5}. From the top rows we can see that using hand-aided attention matters for affordance grounding, e.g., our model achieves relative improvements of up to 8.1\%, and 13.3\% (NSS) compared to random (a) on the OPRA \cite{demo2vec2018cvpr} and EPIC \cite{Damen2018EPICKITCHENS} datasets, respectively. It proves that the position and the action of the hand provide essential clues for the affordance of learning objects from the demonstration videos. In the setting without selecting keyframes, where we input the original video frames into the network for training, it can still achieve good results but falls behind of ours. For example, our method achieves a gain of 1.5\% NSS on OPRA and 5.6\% NSS on EPIC over the model ``w/o select frame'', respectively. These results demonstrate that the pre-processing step of selecting keyframes enables the network to learn more critical cues, leading to better results.

\par Compared to using only the frame with the highest confidence, our method achieves relative improvements of 6.5\% and 8.2\% (NSS) on OPRA and EPIC datasets, respectively. Furthermore, compared to using only the input average features, our method achieves relative improvements of 7.1\% and 8.0\% (NSS) on the two datasets, respectively. These results validate that calculating the distillation loss of the object and video branches from the highest confidence frame and the average feature of the input can better transfer the affordance cues of the video to the static objects. Compared with the model without SEM, our method achieves 3.4\% and 4.4\% (NSS) relative improvements on OPRA and EPIC, respectively. SEM can allow the object branch to focus on different parts based on appearance and action class information, thereby improving the results of affordance grounding.

\myPara{Effectiveness of Hand-aided Attention in Other Models.} We also incorporate the preprocessing strategy as well as hand-aided attention in the hotspot \cite{interaction-hotspots} model to explore the impact of hand cues for affordance grounding. The results are shown in Table \ref{Table:6}, from which we can see that levearging the human hand information can also improve the performance of the existing methods. For example, using the preprocessed frames as input to the Hotspot brings a relative improvement of 1.7\%, while adding hand-aided attention to the affordance learning process of Hotspot brings a relative 1.9\% improvement. However, the performance is still inferior to ours, which is mainly because we also leverage affordance labels to modulate the features of the target image and consider the affordance clues from hand actions and positions. Consequently, our method can better locate the human-object interactions and learn affordance-related contextual information efficiently.

\section{Conclusion and Discussion}

In this paper, we propose a novel Hand-aided Affordance Grounding Network (HAG-Net) for learning the affordance of objects from demonstration videos. By leveraging the clues offered by the position and action of the hand, we address two challenging problems in affordance grounding: (i) the same object has multiple possible interaction regions; and (ii) the same region has multiple possible interactions. Experiments on two public datasets demonstrate that our method achieves state-of-the-art results for affordance grounding.

\myPara{Weakness}
Although our HAG-Net has achieved good affordance grounding performance, there are still some limitations that should be addressed in the future work. Firstly, it is not an end-to-end solution, where a separate pre-processing stage is needed. In the future, we plan to devise an end-to-end affordance grounding method, which can optimize the selection of video frames containing affordance-related information and select hand context features in the same framework. Secondly, since the training efficiency of LSTM is low, we hope to explore the latest transformer structure in the future to improve the training efficiency as well as the performance, which has been proved to be an efficient model to handle sequential data \cite{vaswani2017attention,khan2021transformers}, \cite{girdhar2021anticipative}. 

\begin{table}[t]
\caption{\textbf{Ablation study results of exploring hand-aided attention and the preprocessing strategy} in the hotspot \cite{interaction-hotspots} model on the OPRA \cite{demo2vec2018cvpr} dataset.}
  \begin{center}
 \small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{5.pt}
  \begin{tabular}{r|cccc}
    \hline\toprule
    \multicolumn{1}{c|}{\textbf{Dataset}} & \multicolumn{4}{c}{\textbf{OPRA} \cite{demo2vec2018cvpr}} \\
    \hline
    Method &  &  &   &  \\
    \hline
    Hotspot \cite{interaction-hotspots} &  &  &   &  \\
    Select frame \& Hotspot  \cite{interaction-hotspots}  &  &   &  &  \\
    Hotspot \cite{interaction-hotspots} \& hand attention &  &  &    &     \\
   \hline
   \rowcolor{mygray}
   Ours &  &  &  &   \\
    \hline\bottomrule
    \end{tabular}
    \end{center}
  \label{Table:6}
  \end{table}

\myPara{Potential Applications}
There may be some potential applications of our method as listed below.
\begin{itemize}
    
    \item [1)]
    Our method can provide candidate operation areas for robot grasping. The problem of vision-based robot grasping has been a hot research topic in the field of robotics \cite{du2021vision,fang2020learning,mahler2017dex}. Our approach can learn human grasping habits by observing human-object interactions, enabling the robot to mimic human actions in selecting relevant areas that can be manipulated.
    
    \item [2)]
    Our method can assist visual perception agent to comprehensively understanding of the scene \cite{DBLP:conf/cvpr/ZhuZZ15,Wang_affordanceCVPR2017}. For example, the agent need to know the semantic category of each part in a scene as well as how it interacts with people and feedbacks the environment. Moreover, it can deliver a goal-oriented understanding of the object and the environment.
    
    \item [3)]
    Visual affordance grounding can also be used in virtual reality \cite{fujinawa2017computational} applications, where the object affordance heatmap can highlight the regions where human interacts with an object, together with some virtually displayed information, e.g., affordance indicators or warning signs.
    
    \item[4)]
    In the physical domain, object affordance information can be used to improve object design and manufacture. For example, objects can be made according to human-object interaction characteristics and habits to facilitate possible human-object interactions \cite{zhao2018characterizes}.

\end{itemize}

\myPara{Future Research Directions} There are several promising directions for future research on this task.

\begin{itemize}
    \item [1)]
    \textbf{Dataset}: In the future, we will consider constructing a more extensive and richer dataset that contains videos from both first-person and third-person perspectives \cite{sigurdsson2018charades}, complex backgrounds, diverse interactions,  various object classes, and human pose annotations \cite{zhang2020towards}.
    
    \item [2)]
    \textbf{Generalization}: It is also worthwhile to further investigate how to improve the generalization ability \cite{zhu2019one,zhu2020one,zhu2020self,zhu2021self,wang2020deep} of affordance grounding methods, e.g., locating the affordance-related regions precisely among unseen objects, which is of practical importance \cite{Luo2021one}.
    
    \item [3)]
    \textbf{Compatibility}: First-person perspective videos can provide a unique viewpoint of people's interactions with objects, attention, and even intentions. In contrast, third-person videos observe human actions from an objective perspective. Such a difference poses a significant challenge for one-third-person video compatibility \cite{yu2020first,sigurdsson2018actor}. Therefore, learning better affordance grounding ability from a dataset containing both first-person and third-person videos is worth further study.
    
    \item [4)]
    \textbf{Transferability}: Although third-person videos are more accessible, in cases such as robot manipulation where the agent observes the environment and objects from the first-person perspective, it is necessary to transfer the knowledge learned from third-person videos to first-person scenarios. However, transferring affordance grounding knowledge between different perspectives \cite{li2021ego} is still under-explored and of practical meaning.
    
    \item [5)]
    \textbf{Multi-Information Assistance}: The affordance grounding task is related to many visual factors and to obtain better results in practice we need to investigate affordance grounding methods with the aid of other information, such as the introduction of texture cues \cite{zhai2019deep,zhai2020deep}, depth cues \cite{zhao2020monocular}, human-related information \cite{zhang2020towards,he2020grapy}, \etc.
    
\end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{IEEEtranbib}


\end{document}
