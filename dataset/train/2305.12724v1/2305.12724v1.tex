\documentclass{article}








\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}       \usepackage{subcaption}     \usepackage{tikz}           \usepackage{pgfplots}       

\title{Bridging the Gap Between End-to-end and Non-End-to-end Multi-Object Tracking}






\author{Feng Yan, Weixin Luo, Yujie Zhong, Yiyang Gan, Lin Ma \\
  Meituan \\
  \texttt{} \\
}


\begin{document}


\maketitle

\def\thefootnote{}\footnotetext{These authors contributed equally to this work.}\def\thefootnote{\arabic{footnote}}


\begin{abstract}
Existing end-to-end Multi-Object Tracking (e2e-MOT) methods have not surpassed non-end-to-end tracking-by-detection methods. One potential reason is its label assignment strategy during training that consistently binds the tracked objects with tracking queries and then assigns the few newborns to detection queries. With one-to-one bipartite matching, such an assignment will yield an unbalanced training, \textit{i.e.}, scarce positive samples for detection queries, especially for an enclosed scene, as the majority of the newborns come on stage at the beginning of videos. Thus, e2e-MOT will be easier to yield a tracking terminal without renewal or re-initialization, compared to other tracking-by-detection methods. To alleviate this problem, we present Co-MOT, a simple and effective method to facilitate e2e-MOT by a novel coopetition label assignment with a shadow concept. Specifically, we add tracked objects to the matching targets for detection queries when performing the label assignment for training the intermediate decoders. For query initialization, we expand each query by a set of shadow counterparts with limited disturbance to itself. With extensive ablations, Co-MOT achieves superior performance without extra costs, \textit{e.g.}, 69.4\% HOTA on DanceTrack and 52.8\% TETA on BDD100K. 
Impressively, Co-MOT only requires 38\% FLOPs of MOTRv2 to attain a similar performance, resulting in the 1.4 faster  inference speed. 

\end{abstract}


\section{Introduction}
Multi-Object tracking (MOT) is traditionally tackled by a series of tasks, \textit{e.g.}, object detection~\cite{zou2023object, tan2020efficientdet, redmon2016you, ge2021yolox}, appearance Re-ID~\cite{zheng2016person, li2018harmonious, bertinetto2016fully}, motion prediction~\cite{lefevre2014survey, welch1995introduction}, and temporal association~\cite{kuhn1955hungarian}. The sparkling advantage of this paradigm is task decomposition, leading to an optimal solution for each task. However, it lacks global optimization for the whole pipeline. 

Recently, end-to-end Multi-Object Tracking (e2e-MOT) via Transformer such as MOTR~\cite{zeng2022motr} and TrackFormer~\cite{meinhardt2022trackformer} has emerged, which performs detection and tracking simultaneously in unified transformer decoders. Specifically, tracking queries realize identity tracking by recurrent attention over time. Meanwhile, detection queries discover newborns in each new arriving frame, excluding previously tracked objects, due to a Tracking Aware Label Assignment (TALA) during training. However, we observe an inferior performance for e2e-MOT due to poor detection, as it always yields a tracking terminal, shown in Figure~\ref{Fig.resultsOfMOTR}. MOTRv2~\cite{zhang2022motrv2} consents to this conclusion, which bootstraps performance by a pre-trained YOLOX~\cite{ge2021yolox} detector, but the detector will bring extra overhead to deployment. 

In this paper, we present a novel viewpoint for addressing the above limitations of e2e-MOT: \textbf{detection queries are exclusive but also conducive to tracking queries}. To this end, we develop a COopetition Label Assignment (COLA) for training tracking and detection queries. Except for the last Transformer decoder remaining the competition strategy to avoid trajectory redundancy, we allow the previously tracked objects to be reassigned to the detection queries in the intermediate decoders. Due to the self-attention between all the queries, detection queries will be complementary to tracking queries with the same identity, resulting in feature augmentation for tracking objects with significant appearance variance. Thus, the tracking terminal problem will be alleviated. 

Besides TALA, another drawback in Transformer-based detection as well as tracking is one-to-one bipartite matching used, which cannot produce sufficient positive samples, as denoted by Co-DETR~\cite{zong2022detrs} and HDETR~\cite{jia2022detrs} that introduces one-to-many assignment to overcome this limitation. Differing from these remedies with one-to-many auxiliary training, we develop a \textbf{one-to-set matching strategy with a novel shadow concept}, where each individual query is augmented with multiple shadow queries by adding limited disturbance to itself, so as to ease the one-to-set optimization. The set of shadow queries endows Co-MOT with discriminative training by optimizing the most challenging query in the set with the maximal cost. Hence, the generalization ability will be enhanced.

We evaluate our proposed method on multiple MOT benchmarks, including DanceTrack~\cite{sun2022dancetrack}, BDD100K\cite{yu2020bdd100k} and MOT17~\cite{milan2016mot16}, and achieve superior performance. The contributions of this work are threefold: i) we introduce a coopetition label assignment for training tracking and detection queries for e2e-MOT with high efficiency; ii) we develop a one-to-set matching strategy with a novel shadow concept to address the hungry for positive training samples and enhance generalization ability; iii) Our approach achieves superior performance on multiple benchmarks, while it functions as an efficient tool to bridge the gap between end-to-end and non-end-to-end MOT.


\begin{figure*}[t] \centering \includegraphics[width=0.9\textwidth]{images/motivation.png} \caption{Visualization of tracking results in DanceTrack0073~\cite{sun2022dancetrack} and MOT17-09~\cite{milan2016mot16} videos. The first row displays the tracking results from MOTR~\cite{zeng2022motr}, where all individuals can be correctly initialized at the beginning (\#237 and \#302). However, heavy occlusion appears in the middle frames (\#238 and \#312), resulting in inaccurate detection (indicated by yellow boxes). The tracking of yellow targets finally terminates in \#239 and \#322 frames. The second row shows MOTR's detection results, in which tracking queries are removed during the inference process. Targets in different frames are accurately detected.} \label{Fig.resultsOfMOTR} \end{figure*}

\section{Related Works}

\textbf{Tracking by detection}: Most tracking algorithms are based on the two-stage pipeline of tracking-by-detection: Firstly, a detection network is used to detect the location of targets, and then an association algorithm is used to link the targets across different frames. However, the performance of this method is greatly dependent on the quality of the detection. SORT~\cite{bewley2016simple} is a widely used object tracking algorithm that utilizes a framework based on Kalman filters~\cite{welch1995introduction} and the Hungarian algorithm~\cite{kuhn1955hungarian}; Deep SORT~\cite{wojke2017simple} incorporates reid features extracted by a deep neural network to improve the accuracy and robustness of multi-object tracking based SORT~\cite{bewley2016simple}. After, new batch of joint detection and reid methods are proposed, \textit{e.g.}, JDE~\cite{wang2020towards}, FairMOT~\cite{zhang2021fairmot}; Recently, ByteTrack~\cite{zhang2022bytetrack}, OC-SORT~\cite{cao2022observation}, Strongsort \cite{du2023strongsort}, BoT-SORT\cite{aharon2022bot} are proposed, that have further improved the tracking performance by introducing the strategy of matching with low-confidence detection boxes. While these methods show improved performance, they often require significant parameter tuning and may be sensitive to changes in the data distribution. Additionally, some approaches may require more advanced techniques such as domain adaptation or feature alignment to effectively handle domain shift issues. 

\textbf{End-to-end tracking}: With the recent success of Transformer in various computer vision tasks, several end-to-end object tracking algorithms using Transformer encoder and decoder modules are proposed, such as MOTR~\cite{zeng2022motr} and TrackFormer~\cite{meinhardt2022trackformer}. These approaches demonstrate promising results in object tracking by directly learning the associations between object states across time steps. MOTRv2~\cite{zhang2022motrv2} introduces the use of pre-detected anchor boxes from a YOLOX~\cite{ge2021yolox} detector to indirectly achieve state-of-the-art performance in multi-object tracking.

\textbf{One-to-many label assignment}: DETR\cite{carion2020end}, being a pioneer in employing transformers for computer vision, utilizes a one-to-one label assignment strategy to achieve end-to-end object detection. During training, DETR\cite{carion2020end} leverages Hungarian matching to compute the global matching cost and thereby assigns each ground-truth box to a unique positive sample. Researchers shifte focus towards enhancing the performance of DETR\cite{carion2020end}, with most efforts concentrated on developing new label assignment techniques. For example, DN-DETR\cite{li2022dn} building on Deformable DETR\cite{zhu2020deformable}, breaks away from the traditional one-to-one matching strategy by introducing noisy ground-truth boxes during training.  DINO\cite{zhang2022dino} builds upon the successes of DN-DETR\cite{li2022dn} and DAB-DETR\cite{liu2022dab} to achieve an even higher detection performance, putting it at the forefront of current research. Group-DETR\cite{chen2022group} takes a simpler approach by adopting a group-wise one-to-many label assignment that explores multiple positive object queries. This approach resolves the slow convergence issue often associated with Transformers, Its methodology is similar to the hybrid matching scheme used in H-DETR~\cite{jia2022detrs}. CO-DETR\cite{zong2211detrs} introduces multiple additional parallel branches during training to achieve one-to-many allocation. This training scheme helps to overcome the limitations of one-to-one matching and allows for more flexible and accurate object detection in complex real-world scenarios.





\section{Method}



\subsection{Motivation}


\begin{figure}[t]
    \centering
    \caption{The detection performance (mAP) of MOTR (v2) on DanceTrack validation dataset. \checkmark means whether the tracking queries are used in the training or inference phase. All the decoded boxes of both tracking if applicable and detection queries are treated as detection boxes for evaluation on mAP. We separately evaluate the detection performance for six decoders. For analysis, please refer to the motivation section.}
    \begin{tabular}{c@{\hspace{1pt}}c@{\hspace{1pt}}c c c c@{\hspace{1pt}}c cccccc}
        \toprule
            && methed                       && Training  & Inference && 1     & 2     & 3     & 4     & 5     & 6 \\
        \cmidrule{1-3}       \cmidrule{5-6}              \cmidrule{8-13}
        (a) && MOTR\cite{zeng2022motr}      &&\checkmark &\checkmark && 41.4  & 42.4 & 42.5 & 42.5  & 42.5 & 42.5  \\
        (b) && MOTR\cite{zeng2022motr}      &&\checkmark &           && 56.8 & 60.1 & 60.5 & 60.5 & 60.6 & 60.6  \\
        (c) && MOTR\cite{zeng2022motr}      &&           &           && 57.3 & 62.2 & 62.9 & 63.0 & 63.0 & 63.0  \\
        (d) && MOTRv2\cite{zhang2022motrv2} &&\checkmark &\checkmark && 67.9  & 70.2 & 70.6 & 70.7 & 70.7 & 70.7  \\
        (e) && MOTRv2\cite{zhang2022motrv2} &&\checkmark &           && 71.9  & 72.1 & 72.1 & 72.1 & 72.1 & 72.1  \\
\bottomrule
    \end{tabular}
    \label{Tab.mAPofMOTR}
\end{figure}






To explore the shortcomings of current end-to-end methods in tracking, we conduct an in-depth study of the effectiveness on DanceTrack\cite{sun2022dancetrack} validation and MOT17 \cite{milan2016mot16} test dataset by analyzing MOTR~\cite{zeng2022motr}, which is one of the earliest proposed end-to-end multiple-object tracking methods. In Figure~\ref{Fig.resultsOfMOTR}, we show MOTR's tracking results in some frames of video, \textit{e.g.}, DanceTrack0073 and MOT17-09. In the left three columns of the first row, the 3rd person (in the yellow box) is tracked normally in \#237 image. However, in \#238 image, due to an inaccurate detection, the bounding box is not accurately placed around that person (the box is too large to include a person on the left side). In \#239 image, the tracking is completely wrong and associated with the 2nd person instead. In the right three columns of the first row, the 2nd person (in the yellow box) is successfully detected and tracked in \#302 image. However, in \#312 image, this person is occluded by other people. When the person appears again in \#322 image, she is not successfully tracked or even detected. To determine whether the tracking failure is caused by the detection or association of MOTR, we visualized MOTR's detection results in the second row. We remove the tracking queries during inference, and the visualization shows that all persons are accurately detected. This demonstrates that the detection will deteriorate due to the nearby tracked objects, though TALA used in training ensures that the detection with the same identity of tracked objects will be suppressed.

We further provide quantitative results of how the queries affect each other in Table~\ref{Tab.mAPofMOTR}. All the decoded boxes of both tracking and detection queries are treated as detection boxes so that they can be evaluated by the mAP metric commonly used for object detection. We can see from the table that the vanilla MOTR (a) has a low mAP 42.5\%, but it increases by 18.1\% (42.5\% vs 60.6\%) when removing tracking queries during inference (b). Then we retrain MOTR as a sole detection task by removing tracking queries (c) and mAP further increases to 66.1\% (+5.5\%). That means the DETR-style MOT model has a sparking capability of detection but still struggles with the temporal association of varied appearance, which is the crucial factor of MOT. 

We also observe an excellent detection performance (70.7\%) for MOTRv2, which introduces a pretrained YOLOX detector. Removing tracking queries during inference brings a slight improvement (1.4\%) for mAP, which means MOTRv2 has almost addressed the poor detection issue with high-quality detection prior from YOLOX. \textbf{However, the introduced YOLOX brings extra computational burden, unfriendly to deployment. In contrast, we intend to endow the end-to-end MOT model with its own powerful detection capability, rather than introducing any extra pretrained detector.}

\subsection{Tracking Aware Label Assignment}





Here we revisit the Tracking Aware Label Assignment (TALA) used to train end-to-end Transformers such as MOTR~\cite{zeng2022motr} and TrackFormer~\cite{meinhardt2022trackformer} for MOT. At the moment ,  queries are categorized to two types:  tracking queries  and  detection queries , where . All the queries will self-attend each other and then cross-attend the image feature tokens via  decoders, and the output embeddings of the -th decoder are denoted as  and . At the moment , there are  ground truth boxes. Among them,  previously tracked objects, denoted as , are assigned to  tracking queries, where  as some objects disappear. Formally, -th tracking embedding  will be assigned to the same identity with the previous timestamp if still alive at this moment, otherwise zero (disappearing). Besides,  newborn objects, denoted as , are assigned to  detection queries. Specifically, the Hungarian matching algorithm is used to find the optimal pairing between  and  for each decoder, by a cost function (), that takes into account the class scores and box overlapping. Where  represents the focal loss for classification,  represents the  cost of the bounding box, and  represents the Generalized Intersection over Union cost. 











\begin{figure*}[t] \centering \includegraphics[width=0.95\textwidth]{images/COMOT.png} \caption{The CO-MOT framework includes a CNN-based backbone network for extracting image features, a deformable encoder for encoding image features, and a deformable decoder that uses self-attention and cross-attention mechanisms to generate output embeddings with bounding box and class information. The queries in the framework use set queries as units, with each set containing multiple shadows that jointly predict the same target. Detection queries and tracking queries are used for detecting new targets and tracking existing ones, respectively. To train CO-MOT, S-COLA and S-TALA are proposed for training only.} \label{Fig.main2} \end{figure*}

\subsection{Overall Architecture}
The entire CO-MOT framework is illustrated in Figure~\ref{Fig.main2}. During the forward process, the features of an image in a video are extracted by the backbone and fed into the deformable~\cite{zhu2020deformable} encoder to aggregate information. Finally, together with the detection and tracking queries, they are used as the inputs of the  layer decoders  ( in this paper by default) to detect new targets or track the already tracked targets. It is worth noting that queries contain  position () and embedding () as we use deformable attention~\cite{zhu2020deformable}. Here  is the number of shadow queries for each set, and we will introduce the shadow set concept in the following section. All the queries predict  target boxes, where  queries in a set jointly predict the same target. To train CO-MOT, we employ the COLA and TALA on the different decoders, along with the one-to-set label assignment strategy.

\subsection{Coopetition Label Assignment}
Unlike TALA, which only assigns newborn objects to detection queries, we advocate a novel COopetition Label Assignment (COLA). Specifically, we assign  tracked objects  to detection queries as well in the intermediate decoders, \textit{i.e.}, , which is illustrated in Figure~\ref{Fig.main2}. As shown in the output of the first decoder, the track queries continue to track the 3rd and 4th person. The detection queries not only detect the 1st and 2nd newborns but also detect the 3rd and 4th people. Note that we remain the competition assignment for the -th decoder to avoid trajectory redundancy during inference. Thanks to the self-attention used between tracking and detection queries, detection queries with the same identity can enhance the representation of the corresponding tracking queries (\textit{e.g.} grey 3rd helps blue 3rd). 


















\subsection{Shadow Set }



In densely crowded scenes, objects can be lost or mistakenly tracked to other objects due to minor bounding box fluctuations. We conjecture that one query for one object is sensitive to prediction noises. Inspired by previous works such as Group-DETR\cite{chen2022group} and H-DETR~\cite{jia2022detrs}, we propose the one-to-set label assignment strategy for multi-object tracking, which is significantly different from the one-to-many manner. During the tracking, an object is no longer tracked by a single query but by a set of queries, where each member of the set acts as a shadow of each other. Tracking queries are rewritten as  and detection queries are rewritten as . The total number of queries is . When a particular query in the set tracks the object incorrectly, the other shadows in the same set help it continue tracking the object. In the experiments, this strategy prove effective in improving tracking accuracy and reducing tracking failures in dense and complex scenes.


\textbf{Initialization.}  and , which represents position and embedding of the -th shadow query in the -th set, are initialized, which significantly affects the convergence and the final performance. 
In this paper, we explore three initialization approaches: i) : random initialization; ii) : initializing all shadows in the same set with one learnable vector, \textit{i.e.},  and , where  and  are learnable embeddings with random initialization; iii) : adding Gaussian noises  and  to  and , respectively, in the previous approach. 
In the experiment, we set  and  to 1e-6. Although the variance between each shadow in the same set is subtle after initialization, it expands to 1e-2 at the end of training. The last approach provides the similarity for helping optimization and diversity to improve tracking performance. 

\textbf{Training.} We propose a shadow-based label assignment method (S-COLA or S-TALA) to ensure that all objects within a set are matched to the same ground truth object. Take S-COLA as an example, we treat the set as a whole, and select one of them as a representative based on criteria to participate in subsequent matching. Specifically, for tracking queries , the tracked target in the previous frame is selected to match with the whole set; For detection queries , we first calculate the cost function () of all detection queries with respect to all ground truth. We then select the representative query by a strategy  (\textit{e.g., } Mean, Min, and Max) for each set, resulting in .  is then used as an input for Hungarian matching to obtain the matching results between the sets and newborns. Finally, the other shadows within the same set share the representative's matching result.

\textbf{Inference.} We determine whether the -th shadow set tracks an object by the confidence score of the selected representative. Here we adopt a different strategy  (\textit{e.g., } Mean, Min, and Max) for representative sampling. When the score of the representative is higher than a certain threshold  , we select the box and score predictions of the shadow with the highest score as the tracking outputs and feed the entire set to the next frame for subsequent tracking. Sets that do not capture any object will be discarded. 

















\section{Experiment}






\begin{table}[htbp]
\caption{ Comparison to state-of-the-art methods on different dataset. Please pay more attention to the metrics with *. }
    \begin{subtable}[b]{0.45\textwidth}
        \centering
        \small
        \caption{Comparison to existing methods on the DanceTrack test set. Best results are marked in bold.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{1pt}}}
            \toprule
                                                    & HOTA & DetA & AssA & MOTA & IDF1 \\
            \hline
            \multicolumn{6}{c}{Non-End-to-end}  \\
            CenterTrack~\cite{stone2000centertrack} & 41.8 & 78.1 & 22.6 & 86.8 & 35.7 \\
            FairMOT~\cite{zhang2021fairmot}         & 39.7 & 66.7 & 23.8 & 82.2 & 40.8 \\
            ByteTrack~\cite{zhang2022bytetrack}     & 47.7 & 71.0 & 32.1 & 89.6 & 53.9 \\
            GTR~\cite{zhou2022global}               & 48.0 & 72.5 & 31.9 & 84.7 & 50.3 \\
            QDTrack~\cite{fischer2022qdtrack}       & 54.2 & 80.1 & 36.8 & 87.7 & 50.4 \\
            OC-SORT~\cite{cao2022observation}       & 55.1 & 80.3 & 38.3 & 92.0 & 54.6 \\
            TransTrack~\cite{sun2020transtrack}     & 45.5 & 75.9 & 27.5 & 88.4 & 45.2 \\
            \hline
            \multicolumn{6}{c}{End-to-end} \\
            MOTR~\cite{zeng2022motr}                & 54.2 & 73.5 & 40.2 & 79.7 & 51.5 \\
            MOTRv2~\cite{zhang2022motrv2}           & \textbf{69.9} & \textbf{83.0} & \textbf{59.0} & \textbf{91.9} & 71.7 \\
            \hline
            CO-MOT(ours)                            & 69.4 & 82.1 & 58.9 & 91.2 & \textbf{71.9} \\
            \bottomrule
        \end{tabular}
        \label{Tab.DanceTrack}
        \vspace{0.1cm}
        \caption{Comparison to existing methods on the BDD100K validation set. Best results are marked in bold.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{8pt}}c@{\hspace{8pt}}c@{\hspace{8pt}}c@{\hspace{8pt}}c@{\hspace{1pt}}}
            \toprule
                                                & TETA  & LocA  & AssocA    & ClsA \\
            \hline
            DeepSORT~\cite{wojke2017simple}     & 48.0  & 46.4  & 46.7      & 51.0 \\
            QDTrack~\cite{fischer2022qdtrack}   & 47.8  & 45.8  & 48.5      & 49.2 \\
            TETer~\cite{li2022tracking}         & 50.8  & 47.2  & 52.9      & 52.4 \\
            MOTRv2~\cite{zhang2022motrv2}       & \textbf{54.9}  & \textbf{49.5}  & 51.9      & 63.1 \\
            \hline
            CO-MOT(ours)                        & 52.8  & 38.7  & \textbf{56.2}      & \textbf{63.6} \\
            \bottomrule
        \end{tabular}
        \label{Tab.BDD100K}
    \end{subtable}
\hspace{0.05\linewidth}
    \begin{subtable}[b]{0.45\textwidth}
        \centering
        \small
        \caption{Comparison to existing methods on the MOT17 test dataset. Best results of end-to-end methods are marked in bold.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{1pt}}}
            \toprule
                                                        & HOTA & DetA & AssA & MOTA & IDF1 \\
            \hline
            \multicolumn{6}{c}{Non-End-to-end}  \\
Tracktor++~\cite{bergmann2019tracking}      & 44.8 & 44.9 & 45.1 & 53.5 & 52.3 \\
            CenterTrack~\cite{stone2000centertrack}     & 52.2 & 53.8 & 51.0 & 67.8 & 64.7 \\
            TraDeS~\cite{wu2021track}                   & 52.7 & 55.2 & 50.8 & 69.1 & 63.9 \\
            QuasiDense~\cite{pang2021quasi}             & 53.9 & 55.6 & 52.7 & 68.7 & 66.3 \\
            TransTrack~\cite{sun2020transtrack}         & 54.1 & 61.6 & 47.9 & 74.5 & 63.9 \\
            GTR~\cite{zhou2022global}                   & 59.1 & 57.0 & 61.6 & 71.5 & 75.3 \\
            FairMOT~\cite{zhang2021fairmot}             & 59.3 & 60.9 & 58.0 & 73.7 & 72.3 \\
            CorrTracker~\cite{wang2021multiple}         & 60.7 & 62.9 & 58.9 & 76.5 & 73.6 \\
            Unicorn~\cite{yan2022towards}               & 61.7 & /    & /    & 77.2 & 75.5 \\
            GRTU~\cite{wang2021general}                 & 62.0 & 62.1 & 62.1 & 74.9 & 75.0 \\
            MAATrack~\cite{stadler2022modelling}        & 62.0 & 64.2 & 60.2 & 79.4 & 75.9 \\
            ByteTrack~\cite{zhang2022bytetrack}         & 63.1 & 64.5 & 62.0 & 80.3 & 77.3 \\
            OC-SORT~\cite{cao2022observation}           & 63.2 & /    & 63.2 & 78.0 & 77.5 \\
            QDTrack~\cite{fischer2022qdtrack}           & 63.5 & 62.6 & 64.5 & 77.5 & 78.7 \\
            BoT-SORT\cite{aharon2022bot}                & 64.6 & /    & /    & 80.6 & 79.5 \\
            Deep OC-SORT\cite{maggiolino2023deep}       & 64.9 & /    & /    & 80.6 & 79.4 \\
            P3AFormer~\cite{zhao2022tracking}           & /    & /    & /    & 81.2 & 78.1 \\
            \hline
            \multicolumn{6}{c}{End-to-end} \\
            TrackFormer~\cite{meinhardt2022trackformer} & /    & /    & /    & 65.0 & 63.9 \\
            MOTR~\cite{zeng2022motr}                    & 57.8 & \textbf{60.3} & 55.7 & \textbf{73.4} & 68.6 \\
MeMOT~\cite{cai2022memot}                   & 56.9 & /    & 55.2 & 72.5 & 69.0 \\
            \hline
            CO-MOT(ours)                                & \textbf{60.1} & 59.5 & \textbf{60.6} & 72.6 & \textbf{72.7} \\
            \bottomrule
        \end{tabular}
        \label{Tab.MOT17}
    \end{subtable}
\end{table}


\subsection{ Datasets and Metrics}

\textbf{Datasets.} We validate the effectiveness of our approach on different datasets, including DanceTrack~\cite{sun2022dancetrack}, MOT17~\cite{milan2016mot16}, and BDD100K~\cite{yu2020bdd100k}. Each dataset has its unique characteristics and challenges.

The DanceTrack~\cite{sun2022dancetrack} dataset is used for multi-object tracking of dancers and provides high-quality annotations of dancer motion trajectories. This dataset is known for its significant difficulties such as fast object motion, diverse object poses, and object appearances that are similar to one another.

The MOT17~\cite{milan2016mot16} dataset is a commonly used multi-object tracking dataset, and each video contains a large number of objects. The challenges of this dataset include high object density, long-period occlusions, varied object sizes, dynamic camera poses, and so on. Additionally, this dataset provides various scenes, such as indoor, outdoor, and city centers.

The BDD100K~\cite{yu2020bdd100k} dataset is a large-scale autonomous driving scene recognition dataset that is used for scene understanding in autonomous driving systems. This dataset provides multiple object categories, such as cars, pedestrians, etc. It can be used to evaluate our model's performance in multi-object tracking across different object categories. The challenges of this dataset include rapidly changing traffic and road conditions, diverse weather conditions, and lighting changes.

\textbf{Metrics.} To evaluate our method, we use the Higher Order Tracking Accuracy (HOTA) metric ~\cite{luiten2021hota}, which is a higher-order metric for multi-object tracking. Meantime We analyze the contributions of Detection Accuracy (DetA), Association Accuracy (AssA), Multiple-Object Tracking Accuracy (MOTA), Identity Switches (IDS), and Identity F1 Score (IDF1). For BDD100K, to better evaluate the performance of multi-class and multi-object tracking, we use the Tracking Every Thing Accuracy (TETA)~\cite{li2022tracking}, Localization Accuracy (LocA), Association Accuracy (AssocA), and Classification Accuracy(ClsA) metrics. 























\subsection{Implementation Details}

Our proposed label assignment and shadow concept can be applied to any e2e-MOT method. For simplicity, we conduct all the experiments on MOTR~\cite{zeng2022motr}. It uses ResNet50 as the backbone to extract image features and uses a Deformable encoder and Deformable decoder to aggregate features and predict object boxes and categories. We also use the data augmentation methods employed in MOTR, including randomly clipping and temporally flipping a video segment. To sample a video segment for training, we use a fixed sampling length of 5 and a sampling interval of 10. The dropout ratio in attention is zero. We train all experiments on 8 V100-16G GPUs, with a batch size of 1 per GPU. For DanceTrack and BDD100k, we train the model for 20 epochs with an initial learning rate of 2e-4 and reduce the learning rate by a factor of 10 every eight epochs.  We use 60 initial queries for a fair comparison with previous work. For MOT17, we train the model for 200 epochs, with the learning rate reduced by a factor of 10 every 80 epochs.  We use 300 initial queries due to the large number of targets to be tracked. Unless otherwise specified, all the experiments on the DanceTrack dataset use Crowdhuman~\cite{shao2018crowdhuman} for joint training.





\subsection{Comparison with state-of-the-art methods}





\begin{table}[t]
    \centering
    \small
    \caption{Ablation studies of our proposed CO-MOT on the DanceTrack validation set. Please pay more attention to the metrics with *.}
    \begin{subtable}[t]{0.6\textwidth}
        \centering
        \small
        \caption{Ablation study on individual CO-MOT components. As components are added, the tracking performance improves gradually.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{1pt}}}
            \toprule
                & \#Q     & COLA      & Shadow    & HOTA  & DetA  & AssA  & MOTA  & IDF1\\
            \hline
            (a)   & 180     &           &           & 63.8  & 77.2  & 52.9  & 87.6  & 66.1\\
            (b)   & 60      &           &           & 63.7  & 77.5  & 52.7  & 87.3  & 65.2\\
            (c)   & 60      & \checkmark&           & 64.9  & 77.8  & 54.5  & 87.4  & 66.9 \\
(d)   & 60*3    &           &\checkmark & 64.7  & 76.8  & 54.7  & 86.1  & 67.4\\
            (e)   & 60*3    & \checkmark& \checkmark& 65.9  & 77.6  & 56.2  & 87.1  & 68.8\\
            \bottomrule
        \end{tabular}
        \label{Tab.Components}
        \caption{ Effect of different  and  combinations.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{3pt}}  c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}} c@{\hspace{3pt}} c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}} c@{\hspace{3pt}} c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{1pt}}}
            \toprule
               &\multicolumn{3}{c}{max}&&\multicolumn{3}{c}{mean}&&\multicolumn{3}{c}{min}\\
                        \cmidrule{2-4}          \cmidrule{6-8}          \cmidrule{10-12}
               & min   & mean  & max   && min  & mean  & max   && min  & mean  & max \\ 
            \midrule
            HOTA    & 57.6  & 56.4  & 55.1  && 56.7 & 55.2  & 52.0  && 57.5 & 55.9  & 51.5 \\
            DetA        & 70.7  & 69.3  & 65.4  && 70.6 & 66.5  & 59.0  && 70.8 & 66.4  & 59.3 \\ 
            AssA        & 47.3  & 46.1  & 46.7  && 45.9 & 46.1  & 46.1  && 46.8 & 47.2  & 45.0 \\ 
            \bottomrule
        \end{tabular}
        \label{Tab.fTraAndInf}
    \end{subtable}
    \hspace{0.05\linewidth}
    \begin{subtable}[t]{0.3\textwidth}
        \centering
        \caption{Effect of initialization methods for shadow queries and number of shadows on the DanceTrack validation set. : initialization methods for shadow queries, : number of shadows.}
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{1pt}}}
            \toprule
                            &               & HOTA      & DetA      & AssA      \\
            \midrule
                            &     & 64.9          & 77.4      & 54.6      \\
                     &    & 65.3          & 77.8      & 55.0      \\
                            &     & 63.8          & 77.5      & 52.8      \\
            \midrule
            \midrule
                            & 1             & 64.9          & 77.8      & 54.5      \\
                            & 2             & 65.5          & 77.7      & 54.5      \\
                       & 3             & 65.9          & 77.6      & 56.2      \\
                            & 4             & 65.3          & 77.4      & 54.4      \\
                            & 5             & 65.3          & 77.8      & 55.0      \\
                            & 6             & 64.4          & 76.7      & 54.2      \\
            \bottomrule
        \end{tabular}
        \label{Tab.SizeOfSet}
    \end{subtable}
    \label{Tab.AblationStudies}
\end{table}


\textbf{DanceTrack.} Our method presents promising results on the DanceTrack test set, as evidenced by Table \ref{Tab.DanceTrack}. Without bells and whistles, our method achieve an impressive HOTA score of 69.4\%. In comparison with tracking-by-detection methods, such as ByteTrack~\cite{zhang2022bytetrack}, OC-SORT~\cite{cao2022observation}, and QDTrack~\cite{fischer2022qdtrack}, our approach stands out with a significant improvement in a variety of tracking metrics. For example, compared to OC-SORT~\cite{cao2022observation}, CO-MOT improves HOTA, DetA, and AssA by 14.3\%, 1.8\%, and 20.6\%, respectively. This remarkable performance demonstrates the effectiveness of the end-to-end method for object tracking in complex scenarios. Our approach can avoid tedious parameter adjustments and ad hoc fusion of two independent detection and tracking modules. It realizes automatic learning of data distribution and global optimization objectives. Compared to other end-to-end methods, such as MOTR~\cite{zeng2022motr}, CO-MOT outperforms them by a remarkable margin (\textit{e.g.,} 15.2\% improvement on HOTA compared to MOTR). \textbf{Note that CO-MOT has a comparable performance with MOTRv2~\cite{zhang2022motrv2} which introduces an extra pre-trained YOLOX detector to MOTR. We claim that MOTR with our novel label assignment and shadow queries, namely CO-MOT, is enough to realize a promising performance.}


\textbf{BDD100K.} Table \ref{Tab.BDD100K} shows the results of different tracking methods on the BDD100K validation set. To better evaluate the multi-category tracking performance, we adopt the latest evaluation metric TETA, which combines multiple factors such as localization, association and classification. Compared with DeepSORT~\cite{wojke2017simple}, QDTrack~\cite{fischer2022qdtrack}, and TETer~\cite{li2022tracking}, although the LocA was considerably lower, we achieve superior performance on TETA with an improvement of 2\% (52.8\% vs 50.8\%), which is benefited from the strong tracking association performance revealed by the AssocA (56.2\% vs 52.9\%). Compared with MOTRv2, CO-MOT slightly falls behind on TETA, but its AssocA (56.2\%) is much better than MOTRv2 (51.9\%).


\textbf{MOT17.} Table \ref{Tab.MOT17} shows the results of the MOT17 test set. Compared to the end-to-end methods, such as TrackFormer~\cite{meinhardt2022trackformer}, MOTR~\cite{zeng2022motr} and MeMOT~\cite{cai2022memot}, we still have  significant improvement on HOTA. Although it is inferior to non-end-to-end methods, we conjecture that the insufficient amount of MOT17 training data cannot be able to fully train a Transformer-based MOT model. 


\subsection{Ablation Study}




\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                    legend style={
                        at={(0.02,0.98)}, anchor=north west, font=\tiny, },
                    xlabel={\#decoder},
                    ylabel={Attention weight(\%)},
                    xmin=0.8, xmax=6.2,
                    ymin=0, ymax=35,
                    xtick={1, 2, 3, 4, 5, 6},
                    ytick={0, 10, 20, 30},
width=1.0\textwidth,
                    height=0.6\textwidth,
axis x line=bottom, axis y line=left, nodes near coords, nodes near coords style={
                        font=\tiny, anchor=south, },
                    ylabel style={yshift=-12pt},
                ]
                \addplot[smooth,mark=*,blue] 
                plot coordinates { 
                     (1,2.50)
                     (2,2.55)
                     (3,17.32)
                     (4,30.74)
                     (5,16.02)
                     (6,22.68)
                 };\addlegendentry{D2T}\addplot[smooth,mark=triangle,cyan] 
                plot coordinates {
                     (1,0.10)
                     (2,11.85)
                     (3,29.33)
                     (4,19.07)
                     (5,21.14)
                     (6,4.03)
                 };
                 \addlegendentry{T2T}
                 \addplot[smooth,mark=star,red] 
                 plot coordinates {
                     (1,99.36/60)
                     (2,61.85/60)
                     (3,62.08/60)
                     (4,64.07/60)
                     (5,53.08/60)
                     (6,75.55/60)
                 };
                 \addlegendentry{MD2T}
            \end{axis}
        \end{tikzpicture}
        \caption{The attention weights between different types of queries on different decoders.}
        \label{Fig.attnWeight} \end{minipage}
    \hspace{0.05\linewidth}
    \begin{minipage}[t]{0.45\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                    legend style={
                        at={(1,0)}, anchor=south east, font=\tiny, },
                    legend image post style={mark size=3},
                    xlabel={FLOPs(G)},
                    ylabel={HOTA(\%)},
                    xmin=0, xmax=500,
                    ymin=45, ymax=75,
                    xtick={0, 100, 200, 300, 400, 500},
                    ytick={45, 50, 60 ,70, 75},
width=1.0\textwidth,
                    height=0.6\textwidth,
                    axis x line=bottom, axis y line=left, ylabel style={yshift=-12pt},
                ]
                \addplot[
                    color=green,
                    mark=*,
                    mark size=40/10
                    ]
                    plot[only marks]
                    coordinates {
                        (173, 54.2)
                    };
                \addlegendentry{MOTR(19 FPS)}
                \addplot[
                    color=blue,
                    mark=*,
                    mark size=(99.1+40)/10
                    ]
                    plot[only marks]
                    coordinates {
                        (281.9+173, 69.9)
                    };
                \addlegendentry{MOTRv2(14 FPS)}
                \addplot[
                    color=red,
                    mark=*,
                    mark size=40/10
                    ]
                    plot[only marks] 
                    coordinates {
                        (173, 69.4)
                    };
                \addlegendentry{CO-MOT(19 FPS)}
            \end{axis}
        \end{tikzpicture}
        \caption{Efficiency comparison for CO-MOT and other end-to-end methods on the DanceTrack teset set.}
        \label{Fig.flops} \end{minipage}
\end{figure}



\textbf{Component Evaluation of CO-MOT.} Based on the results shown in Table\ref{Tab.Components}, we examine the impact of different components of the CO-MOT framework on tracking performance, as evaluated on the DanceTrack validation set. Through experimental analysis by combining various components, we achieve significant improvements over the baseline (65.9\% vs 63.7\%). To begin with, we reduce the number of queries for fair comparison as we will introduce shadow sets that double or triple the number of queries. We observe almost the same performance of the model when using (a) 60 initial queries or (b) 180 initial queries. Then, by introducing the COLA strategy to the baseline (b), we observe an improvement of 1.2\% on HOTA and 1.8\% on AssA, without any additional computational cost. By incorporating the concept of shadow into the baseline (a), HOTA is improved by 0.9\% and AssA is improved by 1.8\%.



\textbf{COLA.} It is also evident from Table \ref{Tab.Components} that both COLA and Shadow have minimal impact on DetA, which is detection-related. However, they have a significant impact on AssA and HOTA, which are more strongly related to tracking. We observe an improvement of 3.8\% (56.5\% vs 52.7\%) on AssA and 2.5\% (66.2\% vs 63.7\%) on HOTA. On the surface, our method seems to help detection as it introduces more matching objects for detection, but it actually helps tracking. 

To answer this question, we demonstrate the attention weights between detection and tracking queries in Figure \ref{Fig.attnWeight}. The horizontal and vertical axes denote the attention weights after self-attention between different types of queries on different decoder layers. These weights roughly indicate the contribution of one query to another. In our model, there are a total of 6 decoder layers. T2T represents the contribution of a tracking query to itself. D2T represents the contribution of a detection query predicting the same object to a tracking query. Two bounding boxes with an IOU greater than 0.7 are treated as the same object. MD2T represents the average contribution of all detection queries to a specific tracking query, which serves as a reference metric. Note that the normalized attention weights are with a sum of 1.

From Figure \ref{Fig.attnWeight}, it is evident that detection queries make a significant contribution (more than 15\%) to their corresponding tracking queries in decoder layers where , even greater than the T2T for \#4 and \#6 decoders and much higher than the MD2T for all the decoders. This indicates that detection queries pass on the rich semantic information they represent to their corresponding tracking queries, which in turn can be utilized by the tracking queries to improve their tracking accuracy.

\textbf{Shadow Set.} Table \ref{Tab.SizeOfSet} and Table \ref{Tab.fTraAndInf} list ablation experiments related to three hyperparameters of shadow, which are the number of shadows, initialization method of shadows, and representative sampling strategies  and . 
To choose the appropriate option for  and , we first set  to 5 and train the model only on the DanceTrack training set for 5 epochs using  without COLA. Then we try different combinations of  and . It can be seen from Table \ref{Tab.fTraAndInf} that the combination of  and  yields the best results. That means we use the most challenging query in the set to train the model, leading to discriminative representation learning.
To determine the initialization method, we also fix  with COLA and find that the best results are achieved using . For , there is a considerable variation between different shadows within the same set due to random initialization, making convergence difficult and resulting in inferior results. 
Finally, we try different values of  and find that the best results are achieved when . When  is too large, we observe that convergence becomes more difficult, and the results deteriorate.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/limitation_big.png}
        \caption{}
        \label{fig:subfig1}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/limitation_small.png}
        \caption{}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{Failed cases are often due to the failure to detect the target.}
    \label{fig:limitaiton}
\end{figure}

\subsection{Efficiency Comparison}
In Figure \ref{Fig.flops}, efficiency comparisons on DanceTrack test dataset are made between CO-MOT and MOTR(v2). The horizontal axis represents FLOPs (G) and the vertical axis represents the HOTA metric. The size of the circles represents the number of parameters (M). It can be observed that our model achieves comparable HOTA with MOTRv2 while maintaining similar FLOPs and number of parameters with MOTR. The runtime speed of CO-MOT is much faster (1.4Ã—) than MOTRv2's. Thus, our approach is effective and efficient, which is friendly for deployment as it does not need an extra detector.

\subsection{Limitations}
Despite the introduction of COLA and Shadow, which improve the tracking effect of MOTR~\cite{zeng2022motr}, the inherent data-hungry nature of the Transformer model means that there is not a significant improvement in smaller datasets like MOT17~\cite{milan2016mot16}. As shown in Figure ~\ref{fig:subfig1}, a prominently visible target has not been detected, but this issue has only been observed in the small MOT17 dataset. And due to the scale problem, the detection and tracking performance is poor for small and difficult targets in Figure ~\ref{fig:subfig2}. In order to further improve the effect, it is necessary to increase the amount of training data or use a more powerful baseline such as DINO~\cite{zhang2022dino}.

\section{Conclusion}
This paper proposes a method called CO-MOT to bridge the gap between end-to-end and non-end-to-end multi-object tracking.  We investigate the issues in the existing end-to-end MOT using Transformer and find that the label assignment can not fully explore the detection queries as detection and tracking queries are exclusive to each other. Thus, we introduce a coopetition alternative for training the intermediate decoders. Also, we develop a shadow set as units to augment the queries, mitigating the unbalanced training caused by the one-to-one matching strategy. Experimental results show that CO-MOT achieves significant performance gains on multiple datasets in an efficient manner. We believe that our method as a plugin significantly facilitates the research of end-to-end MOT using Transformer.










\bibliographystyle{abbrv}
\bibliography{refe.bib}


\end{document}
