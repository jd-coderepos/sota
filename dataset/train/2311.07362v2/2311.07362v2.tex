\pdfoutput=1


\documentclass[11pt]{article}



\usepackage[final]{acl}


\usepackage{times}
\usepackage{booktabs}
\usepackage{authblk}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}
\usepackage{microtype}

\usepackage{inconsolata}



\usepackage{subfigure}
\usepackage{placeins}
\title{\includegraphics[scale=0.1]{volcano_emoji.png} {\Ours}: Mitigating Multimodal Hallucination through \\ Self-Feedback Guided Revision}

\makeatletter
\renewcommand\AB@affilsepx{\hspace{0.5cm} \protect\Affilfont}
\makeatother

\author[1,2\thanks{\hspace{0.15cm}Work done during internship at KAIST AI}]{\textbf{Seongyun Lee}}
\author[2]{\textbf{Sue Hyun Park}}
\author[3]{\textbf{Yongrae Jo}}
\author[2]{\textbf{Minjoon Seo}}
\affil[1]{Korea University}
\affil[2]{KAIST AI}
\affil[3]{LG AI Research \protect \3ex] \texttt{sy-lee@korea.ac.kr \{suehyunpark, minjoon\}@kaist.ac.kr yongrae.jo@lgresearch.ai}}
\renewcommand*{\Authands}{\hspace{1cm}}
\renewcommand*{\Authsep}{\hspace{1cm}}
\newcommand{\Ours}{\textsc{Volcano}}
\renewcommand*{\subsectionautorefname}{}

\begin{document}
\maketitle
\begin{abstract}
Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce \textbf{{\Ours}}, a multimodal self-feedback guided revision model. {\Ours} generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. {\Ours} effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that {\Ours}'s feedback is properly grounded on the image than the initial response. This indicates that {\Ours} can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release {\Ours} models of 7B and 13B sizes along with the data and code at \href{https://github.com/kaistAI/Volcano}{https://github.com/kaistAI/Volcano}.
\end{abstract}
\section{Introduction}
\label{sec:intro}
Large multimodal models (LMMs) enable instruct-tuned large language models (LLMs) to comprehend the visual features conveyed by vision encoders with the help of substantial image-text or video-text pairs \citep{alayrac2022flamingo, liu2023improved, liu2023visual, chen2023shikra, peng2023kosmos2, dai2023instructblip, zhu2023minigpt4, ye2023mplugowl, li2023otter, zhang2023llamaadapter, su2023pandagpt, maaz2023videochatgpt}. Recently, with the introduction of fine-tuning methods such as visual instruction tuning, LMMs are evolving into assistants capable of understanding the world through multiple channels, akin to humans \citep{liu2023improved, liu2023visual}.\begin{figure}[t!]
\includegraphics[width=1.0\linewidth]{figure1_v5.pdf}
\caption{\textbf{Overview of {\Ours}.} This example illustrates the process undertaken by {\Ours} for a question in the MMHal-Bench dataset. Before giving the response, {\Ours} goes through a \textit{critique-revise-decide} process. It critiques its initial response with natural language feedback, revises the response based on the feedback, and decides whether to accept the revised answer.}
\label{fig:figure1}
\end{figure} \\
Despite the impressive performance on various benchmark tasks and qualitative outcomes observed, these models grapple with an issue called \textit{multimodal hallucination}, where they produce responses that do not align with the visual information given in the question.
Recent work \citep{zhai2023halleswitch} demonstrates that multimodal hallucinations can occur when the vision encoder fails to ground images accurately. In other words, LMMs tend to rely more on their own parametric knowledge than on provided visual features, causing them to respond with guesses and generate multimodal hallucinations. \citet{wang2023evaluation} empirically show that the model attends to the previous tokens more than image features when it generates hallucinated tokens. \\
In this paper, we propose a novel method that utilizes natural language feedback to enable the model to correct hallucinated responses by offering detailed visual information. Building on this hypothesis, we introduce \textbf{{\Ours}}\footnote{We call our model {\Ours} because it frequently erupts \textit{LLaVA}}, a multimodal self-feedback guided revision model. {\Ours} is trained to first generate an initial response based on the given image and question, then sequentially revises it until it determines that no more improvement is required. We collect our multimodal feedback and revision data for training using proprietary LLMs. \\
To verify the efficacy of {\Ours} in reducing multimodal hallucination, we evaluate its performance on multimodal hallucination benchmarks \citep{sun2023aligning, li2023evaluating, liu2023mitigating}. The results demonstrate consistent performance improvements across all benchmarks. Notably, when compared to previous works aiming at mitigating multimodal hallucination \citep{zhou2023analyzing, sun2023aligning, yin2023woodpecker}, {\Ours} showcases an 24.9\% enhancement, underscoring its effectiveness in addressing the challenge. Furthermore, on multimodal understanding benchmarks \citep{liu2023mmbench, yu2023mmvet}, {\Ours} is also effective in understanding and reasoning about visual concepts. \\
Through qualitative analysis, we find that the generated feedback attends on the image with higher intensity and disperses the attention widely across the image. These findings explain that feedback carries fine-grained visual information and suggest that even if the vision encoder fails to properly ground, the feedback can still guide LLMs to improve upon a hallucinated response, supporting our claim. \\
Our work's contributions can be summarized as follows:
\begin{description}
\item 1. We introduce {\Ours}, a self-feedback guided revision model that effectively mitigates multimodal hallucination. It achieves state-of-the-art on multimodal hallucination benchmarks and multimodal understanding benchmarks.
\item 2. Our qualitative analysis shows that {\Ours}'s feedback is effectively rooted on the image, conveying rich visual details. This underscores that feedback can offer guidance and reduce multimodal hallucination, even when a vision encoder inadequately grounds the image
\item 3. We open-source {\Ours} (7B \& 13B), along with the data and code for training.
\end{description}
\section{Related work}
\label{sec:related}
\subsection{Multimodal hallucination} 
\label{subsec:hal}
Unlike language hallucination where fabrication of unverifiable information is common \citep{10.1145/3571730,zhang2023sirens, li2023halueval}, the majority of multimodal hallucination occurs within verifiable information given the input visual content. Multimodal hallucination is mostly studied as a form of object hallucination where a generation contains objects inconsistent with or absent from the target image \citep{rohrbach-etal-2018-object, 9706727, li2023evaluating, liu2023mitigating, zhai2023halleswitch}, with misrepresentations of a scene or environment being documented until recently \citep{sun2023aligning}. To uncover the cause of failure in grounding, previous works analyze either the visual or language side. \citet{zhai2023halleswitch} pinpoints the lack of preciseness in visual features produced by the vision encoder. Other studies \citep{li2023evaluating, liu2023mitigating, wang2023evaluation} focus on the tendency of LLMs to generate words more in line with common language patterns rather than the actual visual content. The error may be further exacerbated by autoregressive text generation \citep{rohrbach-etal-2018-object, zhang2023language, zhou2023analyzing}. \\
\subsection{Learning from feedback}
\label{subsec:feedback}
Learning from feedback can align LLMs to desired outcomes, for instance to better follow instructions via human preference feedback \citep{ouyang2022training}, preference feedback generated by AI itself \citep{lee2023rlaif, dubois2023alpacafarm}, or even fine-grained feedback \citep{wu2023finegrained, lightman2023lets}. Compared to preference and fine-grained feedback which provide scalar values as training signals, natural language feedback provides more information \citep{scheurer2022training, ma2023eureka} and has been effective for language models to correct outputs, especially for \textit{self-correction} \citep{welleck2022generating, pan2023automatically}. Inspired by successful iterative self-refining language models \citep{madaan2023selfrefine, selfee2023, shinn2023reflexion}, to the best of our knowledge, we are the first to achieve improvement in multimodal modals through iterative self-feedback guided refinement.\\
\subsection{Mitigating multimodal hallucination} 
\label{subsec:mitigating}
Previous methods for mitigating multimodal hallucinations have varied in their focus, including enhancing the quality of instruction tuning data,  model training methodologies, and implementing post-hoc refinements. LRV-Instruction dataset \citep{liu2023mitigating} ensures the balance of both negative and positive instructions and VIGC \citep{wang2023vigc} iteratively generates and corrects instructions to reduce hallucinated samples in training data. Adapting reinforcement learning from human feedback (RLHF) to train a single reward model as in LLaVA-RLHF \citep{sun2023aligning} or training multiple or even without no reward models as in FDPO \citep{gunjal2023detecting} has proven effective as well. LURE \citep{zhou2023analyzing} trains a revision model to detect and correct hallucinated objects in base model's response. Woodpecker \citep{yin2023woodpecker} breaks down the revision process into multiple subtasks where three pre-trained models apart from the base LMM are employed for the subtasks. \\
Unlike models using reinforcement learning, our approach does not require reward model training. Also, contrary to revision-only methods, our method trains a model to \textit{self}-revise, eliminating the need of extra modules. Furthermore, we introduce natural language feedback prior to the revision process. This feedback serves a dual purpose: it revisits the visual features for enhanced clarity and specifically pinpoints the hallucinated elements that require correction, thereby enriching the information available for more effective revision.
\section{{\Ours}}
\label{sec:volcano}
\begin{algorithm}[t]
\caption{Feedback guided self-revision}
\begin{algorithmic}[1]
\State \textbf{Input:} \textit{model }, \textit{image }, \textit{question }
\State \textit{}
\State \textit{}
\For{up to 3 iterations}
    \State \textit{} \hspace{0.3cm}
    \State \textit{} \hspace{0.3cm}
    \State \textit{} \hspace{0.3cm}
    \If {}
        \State \textbf{break}
    \Else
        \State \textit{}
    \EndIf
\EndFor
\State \textbf{return} \textit{}
\end{algorithmic}
\label{alg:algorithm}
\end{algorithm}
{\Ours} employs a single LMM to generate initial responses, feedback, and revisions, as well as decisions to accept revisions. It follows a sequential procedure of an iterative critique-revision-decide loop. In section \ref{subsec:iterative}, we introduce the process by which {\Ours} self-revises its responses iteratively. Section \ref{subsec:data} describes the collection of multimodal feedback and revision data used to train {\Ours}. Finally, section \ref{subsec:imple} provides detailed information about the models and data used in our study. The overall process is explained in Algorithm \autoref{alg:algorithm} and illustrated in \autoref{fig:figure2}.
\begin{figure*}[ht]
\includegraphics[width=1.0\linewidth]{figure2_v5.pdf}
\caption{\textbf{Overall process of {\Ours}.} {\Ours} is a multimodal self-feedback guided revision model that takes an image and a question and then generates an improved response based on the self-feedback.}
\label{fig:figure2}
\end{figure*}
\subsection{Iterative self-revision}
\label{subsec:iterative}
{\Ours} employs a single model to generate improved responses through a sequential process of four stages. First, similar to other LMMs, it generates an initial response  for the image  and question  and initializes the best response  with . This stage is performed only once in the process of creating the final response. Second, it generates feedback  based on the  (\textbf{stage 1}). Using this feedback, it self-revises the  (\textbf{stage 2}). Since there is no guarantee that the revised response  will be better than the existing , there is a need to determine which response is better for the given  and . At this point, {\Ours} is given the , , and both responses, and it goes through the process of deciding which response is better (\textbf{stage 3}). The order of  and  in stage 3 is randomized to prevent the positions from affecting the results \citep{wang2023large}. If the model decides that  is better than , then  is updated with  and the procedure from stage 1 to stage 3 is repeated, with the predetermined maximum number of iterations. Otherwise, the loop is early-stopped, and  is selected as the final output. 
\begin{figure}[h!]
\includegraphics[width=1.0\linewidth]{figure3_v2.pdf}
\caption{\textbf{Data collection.}}
\label{fig:figure3}
\end{figure}
\subsection{Data collection}
\label{subsec:data}
To train {\Ours}, we collect initial responses for visual questions from an open-source LMM and generate feedback and revisions using a proprietary LLM as shown in \autoref{fig:figure3} \citep{akyürek2023rl4f, madaan2023selfrefine, selfee2023, wang2023shepherd, kim2023prometheus}.
\\
Since current proprietary LLMs cannot process images, we provide object details in text and image captions as a proxy for image. For each data instance, we feed the proprietary LLM image information consisting of object details and captions, question, initial response, and gold answer as reference answer, allowing the model to evaluate the given inputs and produce feedback. \\
The proprietary LLM might exploit the gold answer to generate the feedback, which can cause potential inaccuracies in feedback during inference time when it is not provided. To avoid this, we give the LLM clear prompts to use text-formatted image details when generating feedback. When constructing the revision data, we set up the system to predict the existing gold answer as the output, using the feedback data, image, question, and initial response obtained from the previous steps as input, without involving any separate model generation process.
\begin{table*}[ht]
\centering
\small
\begin{tabular}{lcc|cc|ccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{MMHal-Bench} & \multicolumn{2}{c|}{POPE} & \multicolumn{3}{c}{GAVIE} \\
& Score  & Hal rate  & Acc  & F1  & Acc score  & Rel score  & Avg score  \\
\midrule
MiniGPT-4 7B & - & - & 68.4 & 74.5 & 4.14 & 5.81 & 4.98 \\
mPLUG-Owl 7B & - & - & 51.3 & 67.2 & 4.84 & 6.35 & 5.6 \\
InstructBLIP 7B & 2.1 & 0.58 & 71.5 & 80.0 & 5.93 & 7.34 & 6.64 \\
LLaVA-SFT+ 7B & 1.76 & 0.67 & 81.6 & 82.7 & 5.95 & 8.16 & 7.06 \\
LLaVA-RLHF 7B & 2.05 & 0.68 & 81.8 & 81.5 & 6.01 & 8.11 & 7.06 \\
LLaVA-SFT+ 13B & 2.43 & 0.55 & 83.2 & 82.8 & 5.95 & 8.2 & 7.09 \\
LLaVA-RLHF 13B & 2.53 & 0.57 & 83.1 & 81.9 & 6.46 & 8.22 & 7.34 \\
\midrule
LLaVA-1.5 7B & 2.42 & 0.55 & 86.1 & 85.1 & 6.42 & 8.2 & 7.31 \\
LLaVA-1.5 13B & 2.54 & 0.52 & 86.2 & 85.2 & 6.8 & 8.47 & 7.64 \\
{\Ours} 7B & 2.6 & 0.49 & 88.2 & \textbf{87.7} & 6.52 & 8.4 & 7.46 \\
{\Ours} 13B & \textbf{2.64} & \textbf{0.48} & \textbf{88.3} & \textbf{87.7} & \textbf{6.94} & \textbf{8.72} & \textbf{7.83} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of multimodal hallucination benchmarks.}   The MMHal-Bench score is measured on a 0-5 scale. Hallucination rate (Hal rate) is measured as the proportion of scores less than 3. Additionally, GAVIE's Acc score (Accuracy score) and Rel score (Relevancy score) are measured on a 0-10 scale, with Avg score representing the average of Acc and Rel scores. Detailed evaluation results for each benchmark by question type are in \autoref{tab:Table6} and \autoref{tab:Table7}.}
\label{tab:Table1}
\end{table*}
\subsection{Implementation details}
\label{subsec:imple}
\textbf{Data} To construct multimodal feedback and revision data, we utilize the LLaVA-SFT-127k dataset \citep{sun2023aligning}. We only use the first turn of each instance in the dataset. When finetuning {\Ours}, we use the llava-1.5-mix665k as the visual instruction dataset \citep{liu2023improved}. \\
\textbf{Model} For proprietary LLM, we employ OpenAI's gpt-3.5-turbo\footnote{\href{https://platform.openai.com/docs/models/gpt-3-5}{\texttt{gpt-3.5-turbo}}}. We use the LLaVA-SFT+ 7B model\footnote{\href{https://huggingface.co/zhiqings/LLaVA-RLHF-7b-v1.5-224}{\texttt{LLaVA-RLHF-7b-v1.5-224}}} to generate the initial response when creating feedback data and LLaVA-1.5 7B\footnote{\href{https://huggingface.co/liuhaotian/llava-v1.5-7b}{\texttt{llava-v1.5-7b}}} and 13B\footnote{\href{https://huggingface.co/liuhaotian/llava-v1.5-13b}{\texttt{llava-v1.5-13b}}} as backbone models of {\Ours} \citep{liu2023improved, liu2023visual}.
\section{Experiments}
\label{sec:exp}
\subsection{Benchmarks}
\label{subsec:bench}
\textbf{Multimodal hallucination benchmarks} We use POPE \citep{li2023evaluating}, GAVIE \citep{liu2023mitigating}, and MMHal-Bench \citep{sun2023aligning} as our multimodal hallucination benchmarks. POPE and GAVIE are benchmarks for assessing object-level hallucinations in images. POPE comprises 9k questions asking if a specific object is present or not in an image. GAVIE is composed of 1k questions evaluating how accurately the response describes the image (accuracy) and how well the response follows instructions (relevancy) using GPT-4. MMHal-Bench aims to evaluate the overall hallucination of LMMs, consisting of realistic open-ended questions. It comprises 96 image-question pairs across 8 question categories and 12 object topics. GPT-4 evaluates an overall score by comparing the model's response to the correct answer based on the given object information. If the overall score is less than 3, it is considered to have hallucinations. \\
\textbf{Multimodal understanding benchmarks} We use MM-Vet \citep{yu2023mmvet} and MMBench \citep{liu2023mmbench} as benchmarks to measure the general performance of LMMs. MM-Vet is a benchmark consisting of 16 tasks designed to evaluate LMM's ability in complex multimodal tasks. It has about 218 instances. GPT-4 measures the score by comparing the LMM's response to the gold answer. MMBench comprises 4,377 multiple-choice questions aimed at assessing visual perception and visual reasoning. We utilize a dev split of MMBench in this study.
\subsection{Baselines}
\label{subsec:baselines}
We use Openflamingo \citep{awadalla2023openflamingo}, MiniGPT-4 \citep{zhu2023minigpt4}, mPLUG-Owl \citep{ye2023mplugowl}, InstructBLIP \citep{dai2023instructblip}, Otter \citep{li2023otter}, LLaVA-SFT+, and LLaVA-RLHF \citep{sun2023aligning} as baseline models. For the multimodal hallucination corrector baseline, we employ LURE \citep{zhou2023analyzing} and Woodpecker \citep{yin2023woodpecker}. LURE utilize MiniGPT-4 13B as its backbone model. Woodpecker use GPT-3.5-turbo as its corrector, Grounding DINO \citep{liu2023grounding} as its object detector and BLIP-2-FlanT5-XXL \citep{li2023blip2} for its VQA model.
\subsection{Results}
\label{subsec:results}
\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{MMHal-Bench} \\
& Score  & Hal rate   \\
\midrule
LURE & 1.9 & 0.58  \\
Woodpecker & 1.98 & 0.54  \\
{\Ours} 7B  & \textbf{2.6} & \textbf{0.49} \\
\midrule
LLaVA-RLHF 7B & 2.05 & 0.68  \\
{\Ours}\textsuperscript{--} 7B  & \textbf{2.19} & \textbf{0.59}  \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of competitive test.} {\Ours}\textsuperscript{--} 7B is a model fine-tuned with multimodal feedback and revision data on LLaVA-SFT+ 7B.}
\label{tab:Table2}
\end{table}
\begin{table}[t]
\centering
\small
\begin{tabular}{lc|c}
\toprule
\multirow{2}{*}{Model} & \multicolumn{1}{c|}{MMBench} & \multicolumn{1}{c}{MM-Vet} \\
& Acc  & Acc  \\
\midrule
Openflamingo 9B & 6.6 & 24.8\\
MiniGPT-4 13B & 24.3 & 24.4 \\
InstructBLIP 14B & 36.0 & 25.6 \\
Otter 9B & 51.4	& 24.7  \\
LLaVA-SFT+ 7B & 52.7 & 30.4 \\
LLaVA-RLHF 7B & 52.7 & 29.8 \\
LLaVA-SFT+ 13B & 59.6 & 36.1 \\
LLaVA-RLHF 13B  & 59.6 & 36.4 \\
\midrule
LLaVA-1.5 7B & 59.9 & 31.2 \\
LLaVA-1.5 13B & 67.7 & 36.1 \\
{\Ours} 7B & 62.3 & 32.0 \\
{\Ours} 13B & \textbf{69.4} & \textbf{38.0} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of multimodal benchmarks.} The detailed evaluation results for each benchmark by question type are in \autoref{tab:Table8} and \autoref{tab:Table9}.}
\label{tab:Table3}
\end{table}
\textbf{{\Ours} achieves the best performance in the multimodal hallucination benchmarks.} As shown in \autoref{tab:Table1}, {\Ours} consistently outperforms the base model, LLaVA-1.5 and other existing LMMs in the multimodal hallucination benchmark. It show strong performance in benchmarks that measures scores using proprietary LLMs (MMHal-Bench, GAVIE) and a benchmark evaluating with conventional metrics like accuracy and F1 score (POPE). Notably, results from GAVIE demonstrate that {\Ours} not only provides accurate answers for a given image but also enhances its ability to follow instructions. \\
\textbf{Natural language self-feedback is effective in revising responses.} \autoref{tab:Table2} shows {\Ours}'s effectiveness by comparing it with previous studies designed to tackle multimodal hallucination. It reduces hallucination more than LURE and Woodpecker, which try to revise responses without feedback. This suggests that specific feedback is crucial for correcting multimodal hallucination. Unlike the two methods that need a separate model to revise, {\Ours} efficiently gives better responses with just one model. In addition, Woodpecker converts visual information into text and feeds it to the proprietary LLM corrector. Its improvement in hallucination is less significant compared to {\Ours}. From this, we find that for reducing multimodal hallucination, it is effective to convey visual features directly to the corrector model. When compared to LLaVA-RLHF, which reduces LLM hallucination using RLHF, {\Ours} consistently performs better. LLaVA-RLHF 7B employs LLaVA-SFT+ 7B as its core architecture. To ensure a fair comparison, we fine-tune this model using multimodal feedback and revision data, resulting in the development of a {\Ours}\textsuperscript{--} 7B. The result shows that giving natural language feedback, which the model can directly understand, is more powerful than providing feedback in scalar value form. \\
\textbf{{\Ours} is also effective for general multimodal understanding tasks.} As multimodal hallucination decreases, it is expected that the LMM can answer user questions about images more accurately. In this sense, we anticipate that {\Ours} would score high in benchmarks measuring general LMM's performance. To prove this, we evaluate {\Ours} on benchmarks assessing LMM's complicated visual reasoning and perception capabilities (\autoref{tab:Table3}). It achieves superior performance compared to existing LMMs. Notably, as shown in \autoref{tab:Table8}, when measuring the math score related to a model's arithmetic capability, {\Ours} 13B impressively scored about twice as high as LLaVA-1.5 13B.
\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{MMHal-Bench} \\
& Score  & Hal rate   \\
\midrule
Only prediction  & 2.45 & 0.52  \\
No decision & 2.33	& 0.56  \\
{\Ours} 7B & \textbf{2.6} & \textbf{0.49} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of module ablation.}   The "Only prediction" is the result of performing only stage 1 for {\Ours} 7B. "No decision" is the outcome of completing stages 1 and 2.}
\label{tab:Table4}
\end{table}
\subsection{Ablation studies}
\label{subsec:ablation}
\textbf{Module ablation} We test the influence of each stage in reducing multimodal hallucination. As shown in \autoref{tab:Table4}, when we skip iterative self-revision and only use the initial response as the final response, it scores lower than going through both processes. Surprisingly, even after just completing stage 1 and without self-revision, it still scores higher than the base model LLaVA-1.5 7B. This shows that merely fine-tuning with multimodal feedback and revision data can effectively reduce the hallucination rate. We observe a decrease in performance when the revised response is given as the final output without executing stage 3, compared to when a decision is made. This highlights the role of stage 3 in decreasing hallucination as it can prevent unnecessary revisions. This also suggests that while it is hard for the model to produce the right answer initially, distinguishing between right and wrong answers is relatively easier. \\
\noindent \textbf{Iteration ablation} We test how the number of max iterations affects the {\Ours}'s performance. As shown in \autoref{tab:Table5}, as the max iteration count increased, the hallucination rate decreased. This demonstrates that multiple revisions can refine the answers. However, there also exists a trade-off: as the iteration count goes up, the inference time also increases.\\
\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{MMHal-Bench} \\
& Score  & Hal rate   \\
\midrule
Iter 1  & 2.54 & 0.51  \\
Iter 2 & 2.58 & 0.5  \\
Iter 3 ({\Ours} 7B) & \textbf{2.6} & \textbf{0.49} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of iteration ablation.}}
\label{tab:Table5}
\end{table}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure5_v6.pdf}
    \caption{\textbf{Case study of image feature attention in initial response and feedback generation.} For the heatmaps above, the intensity of the highlight behind each token corresponds to the magnitude of attention weight from the token to image features, with darker highlights indicating higher attention weights. For the heatmaps below, values at or above the 0.995-th quantile are represented with the maximum color intensity on the colorbar.}
    \label{fig:figure5}
\end{figure*}
\FloatBarrier
\section{Qualitative analysis}
\label{sec:qualitative}
We qualitatively analyze how feedback from {\Ours} is effective in reducing multimodal hallucination. Using results on MMHal-Bench where {\Ours} 7B revision is selected as the final answer, we compare the visual information content between initial response and feedback, focusing on amount (\ref{subsec:amount}) and coverage (\ref{subsec:coverage}). 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figure4_v3.pdf}
    \caption{\textbf{Image feature attention in initial response and feedback generation.} Attention weights are averaged across instances in MMHal-Bench where {\Ours}'s revision enhance the initial response.}
    \label{fig:figure4}
\end{figure}
\subsection{Amount of visual information}
\label{subsec:amount}
Through manual inspection, we observe that the initial response often correctly identifies object-level information but frequently misinterprets details such as object attributes or relationships between objects. On the contrary, we discover that feedback tends to describe the image contents more comprehensively. \\
To delve deeper into this phenomenon, we take inspiration from \citet{wang2023evaluation} by visualizing how attention weights connect output tokens to input image features during the generation of both initial responses and feedback. Specifically, we focus on the top-3 attention weights across hidden layers and attention heads. These weights are averaged to form a consolidated view. As there is a difference in the initial response length and feedback length, we choose the minimum  of the two and averaged top- weights from the output.\footnote{This approach is chosen based on experiments with different aggregation methods---max, mean, and top-k-mean pooling. We find that the top-3 configuration provided the clearest visualization for our analysis.} \\
As shown in \autoref{fig:figure4}, image features are more strongly attended by feedback compared to initial response. Interestingly, even though attention to input would be more dispersed when generating feedback due to the inclusion of the initial response as additional input, an increased concentration on large areas of image features is visible. This suggests that visual information is largely contained in the feedback text, supporting our manual observation beforehand.
\subsection{Coverage of visual information}
\label{subsec:coverage}
We further investigate the coverage of information to identify whether the visual information correctly aligns with both global and local image features. We perform a case study on an instance that asks the color of a pot (\autoref{fig:figure5}). The initial response incorrectly answers "red" while the feedback makes it clear that the answer should be "silver". \\
The correction can be explained by the difference in distribution of attention to image features during each generation. 
Based on the global features visualization, when {\Ours} generates the initial response, it only focuses on features corresponding to the pot. When generating feedback, {\Ours} attends to the entire image including the areas corresponding to the pot and red berries in the it. Specifically, the local features visualization show that in the process of improving the initial response, it indeed focuses on the exact areas of the image corresponding to key color descriptors "red" and "silver" when generating these words. From these findings, we infer that {\Ours} can grasp a more holistic view of the image and distinguish information in local features at the same time. \\
In summary, existing LMMs may generate answers based on their prior knowledge if the visual features lack clarity, leading to multimodal hallucination. We suggest that {\Ours} can alleviate multimodal hallucination as it is capable of acquiring fine-grained visual information from its feedback. The feedback can effectively encompass a sufficient quantity of a broad spectrum of image features.
\section{Conclusion}
\label{sec:conclusion}
In our work, we suggest a novel approach that utilizes feedback as visual signals to direct the model to refine responses that do not accurately reflect the image. Building on this approach, we present {\Ours}, a multimodal self-feedback guided revision model. {\Ours} has not only achieved state-of-the-art results on a multimodal hallucination benchmark but also demonstrated its effectiveness by improving performance compared to baseline models on multimodal understanding benchmarks. Through qualitative analysis, we demonstrate that the feedback produced by {\Ours} is well-grounded on the image, which means that it can provide the model with rich visual information. This helps reducing multimodal hallucination.
\section*{Limitations}
\label{sec:limit}
In this study, we demonstrate through evaluation and analysis in benchmarks that {\Ours} can effectively alleviate multimodal hallucination. However, it requires more time to execute as it needs to call the model multiple times, compared to directly generating a response. To address this, we introduce stage 3, which allows for early stopping, thereby reducing the execution time.
\section*{Acknowledgements}
\label{sect:ack}
We thank Seungone Kim, Seonghyun Ye, Doyoung Kim and Miyoung Ko for helpful discussion and valuable feedback on our work.
\bibliography{acl_latex}

\appendix
\section{Appendix}
\label{sec:appendix}
\subsection{Detailed results}
In this section, we describe the detailed results from the benchmarks used in our work. The benchmarks are designed to evaluate the performance of LMMs from multiple perspectives, encompassing various sub-tasks and types of questions. For MMHal-Bench, the questions are categorized into 8 types: Attribute, Adversarial, Comparison, Counting, Relation, Environment, Holistic, and Other (\autoref{tab:Table6}). POPE evaluates three types of questions: random, popular, and adversarial (\autoref{tab:Table7}). MM-Vet is composed of sub-tasks designed to measure 6 LMM capabilities: Recognition, OCR (Optical Character Recognition), Knowledge, Language generation, Spatial awareness, and Math (\autoref{tab:Table8}). MMBench is structured to evaluate across L-1, L-2, and L-3 dimensions. We followed previous works and conducted evaluations for the L-2 dimension. The L-2 dimension tasks include Coarse Perception (CP), Fine-grained Single-instance Perception (FP-S), Fine-grained Cross-instance Perception (FP-C), Attribute Reasoning (AR), Relation Reasoning (RR), and Logic Reasoning (LR) (\autoref{tab:Table9}).
\subsection{Prompts}
\textbf{Prompt for generating multimodal feedback} We introduce the prompt used in generating our multimodal feedback dataset. For a LLM that cannot see images, we included the image contents in the form of text within the prompt, allowing it to provide feedback as if it had seen the image and initial response. We utilized object information and a gold caption as the image contents. In instances where no objects are present in the dataset, we didn't use a separate object detector to prevent the model's errors from propagating into the feedback. Instead, only the gold caption is provided in such cases. Additionally, to avoid erroneously generating feedback that suggests the presence of hallucination merely due to the use of different expressions, even when the initial response aligns sufficiently with the image information but uses different terms from the gold answer, we crafted the prompt to treat synonyms or paraphrases as correct answers. Drawing inspiration from previous research \citep{kim2023prometheus}, we structured the prompt to ensure that it encapsulates these aspects well. \\
\textbf{Prompts for inference at each stage} For all prompts, we did not explicitly provide an image feature prompt. Instead, the image features are concatenated with the question during the tokenization process before being input to the model. Additionally, the prompt for the decision process is based on the work of \citep{liu2023improved}.
\subsection{Computation}
For this research, we used an NVIDIA A100-SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training {\Ours} 7B required 8 GPUs and took a total of 15 hours, while training {\Ours} 13B took 30 hours. While the time taken to evaluate each dataset varies, {\Ours} takes about 2 to 3 times longer to complete the entire process compared to existing baselines that only generate responses. \subsection{Hyperparameters}
We used a batch size of 128, a learning rate of 2e-5, and trained for 1 epoch. The maximum length is set to 2048, with no weight decay. We employed a cosine scheduler for learning rate adjustments, with a warmup ratio of 0.03. Additionally, we incorporated gradient checkpointing and used deepspeed zero stage 3. The maximum number of iterations for self-revision is 3. When generating responses, we utilized greedy decoding following LLaVA-1.5.
\begin{table*}[t]
\tiny
\centering
\begin{tabular}{lcccccccc|cc}
\toprule
Model  & Attribute  &	Adversarial  & 	Comparison  &	Counting  &	Relation  &	Environment 	& Holistic	 & Other  & Score  & Hal rate \\
\midrule
Kosmos-2	&	2	&0.25&	1.42&	1.67&	1.67&	2.67&	2.5&	1.33 & 1.69& 	0.68\\
IDEFIC 9B	&	1.58&	0.75&	2.75&	1.83&	1.83&	2.5&	2.17&	1.67 &1.89&	0.64\\
IDEFIC 80B	&	2.33&	1.25&	2&	2.5&	1.5&	3.33&	2.33&	1.17 &2.05&	0.61\\
InstructBLIP 7B	&	3.42&	2.08&	1.33&	1.92&	2.17&	3.67&	1.17&	1.08 &2.1	&0.58\\
InstructBLIP 13B&	2.75&	1.75&	1.25&	2.08&	2.5&	\textbf{4.08}&	1.5&	1.17 &	2.14	&0.58\\
LLaVA-SFT+ 7B&	2.75&	2.08&	1.42&	1.83&	2.17&	2.17&	1.17&	0.5 &	1.76&	0.67\\
LLaVA-RLHF 7B&	2.92&	1.83&	2.42&	1.92&	2.25&	2.25&	1.75&	1.08 &	2.05&	0.68\\
LLaVA-SFT+ 13B&	3.08&	1.75&	2&	\textbf{3.25}&	2.25&	3.83&	1.5&	1.75 & 2.43&	0.55\\
LLaVA-RLHF 13B&	3.33&	\textbf{2.67}&	1.75&	2.25&	2.33&	3.25&	2.25&	\textbf{2.42} &	2.53&	0.57\\
\midrule
LLaVA-1.5 7B&	3.17&	1.25&	3.17&	2.5&	2.33&	3.17&	1.5&	2.25 &	2.42&	0.55\\
LLaVA-1.5 13B&	\textbf{3.5}&	2&	2.67&	2.33&	1.67&	3.33&	2.58&	2.25 &	2.54&	0.52\\
{\Ours} 7B&	3.42&	2.42&	3.08&	1.75&	\textbf{2.75}&	3.75&	1.33&	2.33 &	2.6&	0.49\\
{\Ours} 13B&	3&	1.75&	\textbf{3.42}&	1.67&	2.33&	3.75&	\textbf{2.75}&	\textbf{2.42} &	\textbf{2.64}&	\textbf{0.48}\\
\bottomrule
\end{tabular}
\caption{\textbf{Results of MMHal-Bench}}
\label{tab:Table6}
\end{table*}

\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{lccc|ccc|ccc|cc}
\toprule
\multirow{2}{*}{Model}  & \multicolumn{3}{c|}{Random} &	\multicolumn{3}{c|}{Popular} & \multicolumn{3}{c|}{Adversarial} & \multicolumn{2}{c}{Overall}\\
& Acc  & F1  & Yes (\%) & Acc  & F1  & Yes (\%) & Acc  & F1  & Yes (\%) & Acc  & F1  \\
\midrule
Shikra & 86.9 & 86.2 & 43.3 & 84 & 83.2 & 45.2 & 83.1 & 82.5 & 46.5 & 84.7 & 84.0 \\
InstructBLIP & 88.6 & 89.3 & 56.6	& 79.7	& 80.2	& 52.5&	65.2	&70.4	&67.8	&77.8	&80.0\\
MiniGPT-4&	79.7	&80.2&	52.5	&69.7&	73	&62.2&	65.2&	70.4&	67.8&	71.5	&74.5\\
mPLUG-Owl&	54	&68.4	&95.6&	50.9	&66.9&	98.6&	50.7&	66.8	&98.7&	51.9	&67.2\\
LLaVA-SFT+ 7B&	86.1	&85.5	&44.5	&82.9&	82.4	&47.2	&80.2&	80.1&	49.6	&83.1&	82.7\\
LLaVA-RLHF 7B&	84.8&	83.3&	39.6&	83.3	&81.8&	41.8	&80.7	&79.5	&44	&82.9&	81.5 \\
LLaVA-SFT+ 13B&	86&	84.8&	40.5&	84&	82.6&	41.6&	82.3	&81.1&	43.5&	84.1&	82.8\\
LLaVA-RLHF 13B&	85.2	&83.5	&38.4&	83.9&	81.8	&38	&82.3&	80.5	&40.5&	83.8&	81.9 \\
\midrule
LLaVA-1.5 7B	&88.2	&87.3&	41.9&	87.3	&86.2	&41.8&	85.2	&84.2	&44&	86.9	&85.9 \\
LLaVA-1.5 13B&	88&	87.1	&41.7&	87.4	&86.2	&41.3	&85.5&	84.5	&43.3	&87.0	&85.9 \\
{\Ours} 7B&	89.9	&89.4	&43.9	&\textbf{88.5}&	\textbf{87.9}	&45.1	&86.2&	85.7&	46.6	&88.2&	\textbf{87.7} \\
{\Ours} 13B& \textbf{90.2}& \textbf{89.7}& 44.3& 88.1& 87.4& 44.5& \textbf{86.6}& \textbf{86.1}& 46.7& \textbf{88.3}& \textbf{87.7}\\										
\bottomrule
\end{tabular}
\caption{\textbf{Results of Pope}}
\label{tab:Table7}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{lcccccc|c}
\toprule
Model & rec  & ocr & know & gen & spat & math & total \\
\midrule
Transformers Agent (GPT-4)	& 18.2 & 3.9 & 2.2&	3.2	&12.4	&4&	13.4 \\
MiniGPT-4-8B&	27.4&	15&	12.8&	13.9	&20.3&	7.7&	22.1 \\
BLIP-2-12B	&27.5&	11.1	&11.8&	7&	16.2&	5.8	&22.4 \\
MiniGPT-4-14B&	29.9&	16.1&	20.4&	22.1&	22.2&	3.8&	24.4 \\
Otter-9B	&27.3&	17.8	&14.2&	13.8&	24.4&	3.8&	24.7 \\
OpenFlamingo-9B&	28.7&	16.7	&16.4	&13.1	&21	&7.7	&24.8 \\
InstructBLIP-14B&	30.8	&16	&9.8	&9	&21.1	&10.5	&25.6 \\
InstructBLIP-8B	&32.4	&14.6	&16.5	&18.2	&18.6	&7.7	&26.2 \\
LLaMA-Adapter v2-7B	3&8.5	&20.3	&\textbf{31.4}	&\textbf{33.4}	&22.9	&3.8	&31.4 \\
\midrule
LLaVA-1.5 7B	&37	&21	&17.6&	20.4&	24.9	&7.7&	31.2 \\
LLaVA-1.5 13B	&40.6	&28	&23.5	&24.4	&\textbf{34.7}	&7.7	&36.1 \\
{\Ours} 7B	&36.7	&23.5	&18.2	&22	&27.6	&3.8	&32 \\
{\Ours} 13B	&\textbf{42.9}	&\textbf{30.4}	&24.5	&29.2	&32.7	&\textbf{15}	&\textbf{38} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of MM-Vet}}
\label{tab:Table8}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{lcccccc|c}
\toprule
Model & LR  &	AR 	& RR 	& FP-S & 	FP-C &	CP & 	Overall \\
\midrule
OpenFlamingo & 6.7 & 8 & 0 & 6.7 & 2.8	& 2	& 4.6 \\
OpenFlamingo v2	& 4.2	& 15.4	& 0.9	& 8.1	& 1.4	& 5	& 6.6 \\
MMGPT	& 2.5	& 26.4	& 13	& 14.1	& 3.4	& 20.8	& 15.3 \\
VisualGLM	& 10.8	& 44.3	& 35.7	& 43.8	& 23.4	& 47.3	& 38.1 \\
LLaMA-Adapter	& 11.7	& 35.3	& 29.6	& 47.5	& 38.6	& 56.4	& 41.2 \\
µ-G2PT	& 13.3	& 38.8	& 40.9	& 46.5	& 38.6	& 58.1	& 43.2 \\
mPLUG-Owl	& 16.7	& 53.2	& 47.8	& 50.2	& 40.7	& 64.1	& 49.4 \\
Otter	& 32.5	& 56.7	& 53.9	& 46.8	& 38.6	& 65.4	& 51.4 \\
Shikra	& 25.8	& 56.7	& 58.3	& 57.2	& 57.9	& 75.8	& 58.8 \\
Kosmos-2	& \textbf{46.7}	& 55.7	& 43.5	& 64.3	& 49	& 72.5	& 59.2 \\
PandaGPT	& 10	& 38.8	& 23.5	& 27.9	& 35.2	& 48.3	& 33.5 \\
MiniGPT-4	& 20.8	& 50.7	& 30.4	& 49.5	& 26.2	& 50.7	& 42.3 \\
InstructBLIP	& 19.1	& 54.2	& 34.8	& 47.8	& 24.8	& 56.4	& 44 \\
\midrule
LLaVA-1.5 7B	& 30.8	& \textbf{73.1}	& 53.9	& 67	& 57.2	& 77.2	& 59.9 \\
LLaVA-1.5 13B	& 41.7	& 69.7	& 63.5	& 70	& 59.3	& 80.2	& 67.7 \\
{\Ours} 7B	& 30.8	& 65.2	& 59.1	& 67.7	& 54.5	& 72.8	& 62.3 \\
{\Ours} 13B	& 38.3	& 70.6	&\textbf{67}	& \textbf{72.4}	& \textbf{62.8}	& \textbf{82.2}	& \textbf{69.4} \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of MMBench}}
\label{tab:Table9}
\end{table*}
\begin{figure*}[h]
\includegraphics[width=1.0\linewidth]{figure_appendix_prompt1.pdf}
\caption{\textbf{Prompt for generating multimodal feedback}}
\end{figure*}
\begin{figure*}[h]
\includegraphics[width=1.0\linewidth]{figure_appendix_prompt2.pdf}
\caption{\textbf{Prompts for inference at each stage}}
\end{figure*}

\end{document}