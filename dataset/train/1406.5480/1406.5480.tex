\documentclass[runningheads, envcountsame, a4paper]{llncs}\usepackage{etex}
\usepackage[pst,vaucanson-g]{xcolor}
\usepackage{vaucanson-g}
\usepackage{pstricks}
\usepackage{pst-tree,pst-node}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{array}
\newcommand{\defproblem}[3]{
  \vspace{2mm}
\noindent\fbox{
  \begin{minipage}{0.96\textwidth}
  #1\\
  {\bf{Input:}} #2  \\
  {\bf{Output:}} #3
  \end{minipage}
  }
  \vspace{2mm}
}

\usepackage[algo2e, noend, noline, boxed]{algorithm2e}
  \renewcommand{\CommentSty}[1]{\textnormal{#1}\unskip}\SetKwComment{tcm}{\{}{\}}
   \newenvironment{myfunction}[2][htbp]
  {\setlength{\algomargin}{.2cm}
    \begin{center}
    \begin{minipage}{#2}
    \begin{function}[#1]
    \small
     \let\Par=\par
       \def\par{\endgraf\vspace{.1cm}}
           \SetKw{To}{to}\SetKw{Downto}{downto}\SetKw{Or}{or}\SetKwFor{Algo}{Function}{}{}\vspace{.15cm}}
   {\let\par=\Par
     \end{function}\end{minipage}\end{center}}
  \newenvironment{myalgorithm}[2][htbp]
  {\setlength{\algomargin}{.2cm}
    \begin{center}
    \begin{minipage}{#2}
    \begin{algorithm2e}[#1]
    \small
     \let\Par=\par
       \def\par{\endgraf\vspace{.1cm}}
           \SetKw{To}{to}\SetKw{Downto}{downto}\SetKw{Or}{or}\SetKwFor{Algo}{Algorithm}{}{}\vspace{.15cm}}
   {\let\par=\Par\end{algorithm2e}\end{minipage}\end{center}}

\newcommand{\ACSM}{\textsc{ApproximateCircularStringMatching}}
\def\dd{\mathinner{.\,.}}
\newcommand{\cO}{\mathcal{O}}
\begin{document}
\frontmatter          \title{Average-case Optimal Approximate Circular String Matching}

\author{Carl Barton\inst{1}
\and Costas S.\ Iliopoulos\inst{1,2}
\and Solon P.\ Pissis\inst{1}\thanks{Supported by a London Mathematical Society grant (no. 51303).}
}

\institute{Department of Informatics, King's College London, The Strand, London, UK \\ 
\email{\{carl.barton,costas.iliopoulos,solon.pissis\}@kcl.ac.uk} \\ 
Department of Mathematics \& Statistics,
University of Western Australia, 35 Stirling Highway, Perth, Australia \\ }

\titlerunning{Average-Case Optimal Approximate Circular String Matching}
\authorrunning{C. Barton, C. S. Iliopoulos, and S. P. Pissis}

\toctitle{Average-Case Optimal Approximate Circular String Matching}
\tocauthor{Carl~Barton, Solon~P.~Pissis and Costas~S.~Iliopoulos}
\maketitle

\begin{abstract}
Approximate string matching is the problem of finding all factors of a text  of length 
that are at a distance at most  from a pattern  of length . 
Approximate circular string matching is the problem of finding all factors of 
that are at a distance at most  from  {\em or} from any of its rotations. 
In this article, we present a new algorithm for approximate circular string matching under the edit distance model with optimal average-case search time .
Optimal average-case search time can also be achieved by the algorithms for multiple approximate string matching (Fredriksson and Navarro, 2004) using
 and its rotations as the set of multiple patterns. Here we reduce the preprocessing time and space requirements compared to that approach.
\keywords{algorithms on automata and words, average-case complexity, average-case optimal, approximate string matching}
\end{abstract}

\section{Introduction}
In order to provide an overview of our results and algorithms,
we begin with a few definitions, generally following~\cite{CHL07}.
We think of a \textit{string}  of \textit{length}  as an array
, where every , , is a \textit{letter}
drawn from some fixed \textit{alphabet}  of size .
By a {\em -gram} we refer to any string .
The \textit{empty string} of length  is denoted by .
A string  is a \textit{factor} of a string  if there exist two strings  and , such that .
Consider the strings , and , such that . If , 
then  is a \textit{prefix} of . If , then  is a \textit{suffix} of .
Let  be a non-empty string of length  and  be a string. 
We say that there exists an \textit{occurrence} of  in , or, more simply, that 
\textit{occurs in} , when  is a factor of .
Every occurrence of  can be characterised by a position in . Thus we say that  occurs at the
\textit{starting position}  in  when .
Given a string  of length  and a string  of length , the \emph{edit distance}, 
denoted by , is defined as the minimum total cost of operations 
required to transform one string into the other. For simplicity, we only count the number of edit operations, 
considering the cost of each to be ~\cite{levelshtein-66-binary}.
The allowed edit operations are as follows:
\begin{itemize}
	\item \emph{Insertion}: insert a letter in , not present in ; 
	\item \emph{Deletion}: delete a letter in , present in ;  
	\item \emph{Substitution}: replace a letter in  with a letter in ; . 
\end{itemize}

\noindent We write  if the edit distance between 
and  is at most . Equivalently, if , we say that
 and  have at most  {\em differences}. 
We refer to the \textit{standard dynamic programming matrix} of  and  as the matrix defined by

\noindent


\noindent Similarly we refer to the \textit{standard dynamic programming algorithm} as the algorithm to compute the 
edit distance between  and  through the above recurrence in time . 
Given a non-negative integer threshold  for the edit distance, this can be computed in time ~\cite{editd}.
We say that there exists an \textit{occurrence} of  in  with at most  differences, or, more simply, that 
\textit{occurs in}  with at most  differences, when  and  is a factor of .

A circular string of length  can be viewed as a traditional linear string which has the left- and right-most symbols 
wrapped around and stuck together in some way. Under this notion, the same circular string can be seen as  different 
linear strings, which would all be considered equivalent. Given a string  of length , we denote 
by , , the -th \textit{rotation} of  and .
Consider, for instance, the string ; this string has the following rotations:
, , , , 
, , .

This type of structure occurs in the DNA of viruses, bacteria, 
eukaryotic cells, and archaea. In~\cite{G97}, it was noted that, due to this, algorithms on 
circular strings may be important in the analysis of organisms with such structure. For instance, circular strings have been studied before 
in the context of sequence alignment. In~\cite{Lee:2010:FOA:1875737.1875765,circ09}, algorithms for multiple circular 
sequence alignment were presented. Here we consider the problem of finding occurrences of a pattern  of length  with circular structure 
in a text  of length  with linear structure. 
This is the problem of \emph{circular string matching}.

The problem of exact circular string matching has been considered in~\cite{Lot05}, where an -time algorithm was presented. 
The approach presented in~\cite{Lot05} consists of preprocessing  by constructing 
a \emph{suffix automaton} of the string , by noting that every rotation of  is a factor of . 
Then, by feeding  into the automaton, the lengths of the longest factors of  occurring in  can be found 
by the links followed in the automaton in time . 
In~\cite{Grabowski}, an average-case optimal algorithm for exact circular string matching was presented and it was also shown that the average-case lower bound for single string matching of  also holds 
for circular string matching. Very recently, in~\cite{Chen03032013}, the authors presented two fast average-case algorithms 
based on word-level parallelism. The first algorithm requires average-case time , where  is the 
number of bits in the computer word. The second one is based on a mixture of word-level parallelism and -grams. 
The authors showed that with the addition of -grams, and by setting , an average-case optimal time 
of  is achieved.  
Indexing circular patterns~\cite{Iliopoulos:2008:ICP:1787651.1787658} based on the construction of \emph{suffix tree}---have also been considered. 

The aforementioned algorithms for the exact case have the disadvantage that
they cannot be applied in a biological context since single nucleotide polymorphisms and errors introduced 
by wet-lab sequencing platforms might have occurred in the sequences; also it is not clear whether they could easily be adapted to deal with the approximate case. 
For the rest of the article, we assume that each position in the text  is uniformly randomly drawn from , 
and consider the following problem.

\defproblem{\ACSM}{a pattern  of length , a text  of length , and an integer threshold }{all factors  of  such that , }

Similar to the exact case~\cite{Grabowski}, it can be shown that the average-case lower bound for single 
approximate string matching of ~\cite{Chang} also holds for approximate circular string matching 
under the edit distance model.
Recently, we have presented average-case -time algorithms for approximate circular string matching which are also very efficient in practice~\cite{1748-7188-9-9}. 
In~\cite{aproxcir}, an algorithm with  average-case search time was presented.
To achieve average-case optimality, one could use the algorithms for multiple approximate string matching, 
presented in~\cite{Fredriksson:2004:ASM:1005813.1041513}, for matching the  rotations of  with
 average-case search time, only if  and . 
Therefore the focus of this article is on a more {\em direct} algorithm which also improves on the preprocessing time and space complexity.

\textbf{Our Contribution.} 
In this article, we present a new average-case optimal algorithm for approximate circular string matching, under the edit distance model, that reduces 
the preprocessing time and space requirements compared to previous algorithms with optimal average-case search time. These savings are around  or more in all cases.
\section{Algorithm}
\label{sec:algo}
In this section, we present our algorithm for approximate circular string matching under the edit distance model. 
The presented algorithm consists of two distinct schemes: the {\em searching} scheme, which determines if the currently considered text window potentially has a valid occurrence; in case the window \textit{may} contain a valid occurrence, we are required to check the window for valid occurrences of the pattern or any of its rotations; this is done through the {\em verification} scheme.

Intuitively, the algorithm considers a {\em sliding window} of length  of the text, and reads -grams backwards from the end of the window until it is likely to have found enough {\em differences} to skip the entire window. That is, we wish to make the probability of a verification being triggered sufficiently unlikely whilst also ensuring we can shift the window a reasonable amount.

The rest of this section is structured as follows. We first present an efficient incremental string comparison technique which forms the basis of the verification scheme. 
We then present the searching scheme of our algorithm which requires a preprocessing step. In fact, this preprocessing step is similar to the verification scheme. Finally, we show how plugging these schemes together results in a new average-case optimal algorithm for approximate circular string matching.

\subsection{Verification scheme}

The verification scheme of our algorithm is based on incremental string comparison techniques. 
First we give an introduction to these techniques; and then explain how we use them in the verification scheme.
The incremental string comparison problem was introduced in the pioneering work of Landau {\em et al}~\cite{LMS98}. 
The authors considered the following problem: given the edit distance between two strings  and , 
how can the edit distance between  and  or  be efficiently derived, where  is an additional letter. 
Given a threshold on the number of differences , they solve this problem and allow prepending and appending of letters in time  per operation. 
Later the authors of~\cite{Hsu:2009:FAG:1696924.1697033} considered a generalisation of this problem with the aim of computing all maximal gapped palindromes in a string. 
The problem considered is a generalisation of the incremental string comparison problem considered in~\cite{LMS98} as it considers how to efficiently derive the edit distance when prefixes are deleted and letters are prepended to  or . The solution proposed in~\cite{Hsu:2009:FAG:1696924.1697033} also has a 
time complexity of  per operation. 
The solution for the generalised incremental string comparison problem forms the basis of our verification step. The technique lends itself more naturally to circular string matching due to the increased flexibility it provides. We begin by recalling some of the main results from~\cite{Hsu:2009:FAG:1696924.1697033} required for our algorithm.

The main idea in both~\cite{LMS98} and~\cite{Hsu:2009:FAG:1696924.1697033} is the efficient computation of the so-called \textit{-waves}. 
In the standard dynamic programming matrix for two strings  and , we say that a cell  is on the diagonal  {\em iff} . For each diagonal, we may have a lowest cell with value ; if  and  then  is this cell for diagonal . The -wave, for all , is the position of all these cells across all diagonals, that is, a list  of length , where each entry is a pair  such that  and . Note that the -th wave can only contain entries on diagonal zero and the  diagonals either side of it, so for  every wave has size . 
Both incremental string comparison techniques show some bounds on the possible values of the cells on -waves and how to 
efficiently compute them. 
These -waves define the entire dynamic programming matrix due to the monotonicity properties of the matrix. For any diagonal , if we know the position of the lowest cell on  with 
value  and , then we also know the value of every cell between these two cells: it must be . So given the -waves of the matrix, for all , we have all the information that is in the standard dynamic programming matrix. The key result from our perspective is the following.

Let  denote the string obtained by 
concatenating  and , where .
Let  denote the string obtained by deleting the 
prefix of length  of .
Let  denote the standard dynamic 
programming matrix for strings  and 
, where .

\begin{theorem}[\cite{Hsu:2009:FAG:1696924.1697033}]
The -wave, -wave,  , and -wave of matrix  can be computed in time .
\label{the:Hsu}
\end{theorem}

If a window of the text triggers a verification then we have a window of length  such that there exist some -grams of the window that occur in  or its rotations with at most  differences in total. When we verify a window, we check for occurrences of pattern  starting at every position in the window. For each position, we may have a factor of length at most  representing an occurrence, meaning we must consider a factor  of the text of length  which we refer to as a \textit{block}. This ensures we avoid missing any occurrences at the  starting positions as .

For each possible starting position , , we compute the -wave, -wave,  , and -wave for  
and , the suffix of  starting at position . To check if we have an occurrence, we must check the -wave . 
We iterate through each entry in the -wave ; and if  has missing entries or contains entries on the last row of the matrix, then  occurs in  with at most  differences. 

Similarly we can check for the occurrences of the rotations of  using the incremental string comparison techniques.
We are now ready to outline the verification scheme, denoted by function . Given the pattern  of length , an integer threshold , and a block 
of length  of the text , function  finds all factors  of  such that , .
If any diagonal has no entry on the -wave then that diagonal reached the last row of the matrix with less than  differences; this means  occurs in  with less than  differences.
  \begin{myfunction}[H]{10 cm}
  \Algo{}{
      Compute the edit distance between  and  with at most  differences using the standard dynamic programming algorithm\;
      Check for any occurrences using , and if found, {\bf report} an occurrence at position 0\;
      \ForEach{}{
       \ForEach{}{
       Construct rotation  of  by removing the first letter of  and appending it to the end of \; 
       Compute the edit distance between  and  using the incremental string comparison techniques\;
       Check for any occurrences using , and if found, {\bf report} an occurrence at the current position  being checked\;
       }
     }
  }
  \end{myfunction}
\begin{lemma}
Given the pattern  of length , an integer threshold , and string  of length , function  requires time .
\label{lem:ver}
\end{lemma}
\begin{proof}
Computing the edit distance between  and  with at most  differences takes time  using the standard dynamic programming algorithm.
By Theorem~\ref{the:Hsu}, computing the edit distance between all the rotations of the pattern and  for a single position in  requires ; 
and there are  positions in . In total, the time is , that is . \qed
\end{proof}
\subsection{Searching scheme}
The searching scheme of the presented algorithm requires the preprocessing and indexing of the pattern . 
We first present the preprocessing required and then present the searching technique itself.
\subsubsection*{Preprocessing.}
We build a -gram index in a similar way as that proposed by Chang and Marr in~\cite{Chang}. Intuitively, we wish to determine the minimum possible edit distance between every -gram and any factor of  or its rotations. Equivalently we find the minimum possible edit distance between every -gram and any {\em prefix} of a factor of length  of  and the suffixes of length 1 to  of  or its rotations. An index like this allows us to lower bound the edit distance between a window of the text and  or its rotations without computing the edit distance between them. To build this index, we generate every string of length  on , and find the minimum edit distance between it and all prefixes of factors of length  of  or its rotations. This information can easily be stored by generating a numerical representation of the -gram and storing the minimum edit distance in an array at this location. If we know the numerical representation, we can then look up any entry in constant 
time. 

We determine the edit distance using the preprocessing scheme, denoted by function , which is 
similar to the verification scheme (function ). 

Given the string  of length , function  finds the minimum edit distance between every -gram on , generated in increasing order, and any factor  of length  of  and its suffixes of length 1 to .

\begin{lemma}\label{lemm:pre}
Given the string  of length  on , , and , 
function   requires time  and space .
\end{lemma}

\begin{proof}
The time required for initialising array  is .
The time required for computing the edit distance between  and  is  using the standard dynamic programming algorithm. 
By Theorem~\ref{the:Hsu}, computing the edit distance between all -grams of  and  requires time .
There exist  possible -grams on  and so, in total, the time complexity is . Keeping array  in memory requires space . \qed
\end{proof}

  \begin{myfunction}[H]{11 cm}
  \Algo{}{
      \;
      \;
      \ForEach{}{
Compute the edit distance between  and  using the standard dynamic programming algorithm. Set  equal to the minimum edit distance between  and any prefix of  using \;
       \ForEach{}{
       \;
       Compute the edit distance  between  and  using the incremental string comparison techniques. Set  equal to the minimum edit distance between  and any prefix of  using \;
       \lIf{}{}
     }
       \;
       \;
    }
    \Return{}\;
  }
  \end{myfunction}

\subsubsection*{Searching.}
In the search phase we wish to read backwards enough -grams from a window of size  that the probability we must verify the window is small and the amount we can shift the window by is sufficiently large.
We now recall some important lemmas from~\cite{Chang} that we will use in the analysis of our algorithm.


\begin{lemma}[\cite{Chang}]\label{lem:prob}
The probability that two -grams on , one being uniformly random, have a common subsequence of length  is 
at most , where  and . The probability decreases exponentially for , which holds if .
\label{lem:Chang1}
\end{lemma}
\begin{lemma}[\cite{Chang}]
If  is a -gram occurring with less than  differences in a given string , ,  has a common subsequence of length  with some -gram of .
\label{lem:Chang2}
\end{lemma}
By Lemmas~\ref{lem:Chang1} and~\ref{lem:Chang2}, we know that the probability of a random -gram occurring in a string of length  with less than  differences is no more than  as we have  -grams in the string. For circular string matching this is not sufficient. To ensure that we have the -grams of all possible rotations of pattern , we instead consider the string  and extract the -grams from . We may have up to  -grams, but to simplify the analysis we assume we have  and so the probability becomes . 

In the case when we read  -grams, we know that with probability at most  we have found less than  differences. This does not permit us to discard the window if all -grams occur with at most  differences. To fix this, we instead read  -grams. If any -gram occurs with less than  differences, we will need to verify the window; but if they all occur with at least  differences, we must exceed the threshold  and can shift the window. When shifting the window we have the case that we shift after verifying the window and the case that the differences exceed  so we do not verify the window. If we have verified the window, we can shift past the last position we checked for an occurrence: we can shift by  positions. If we have not verified the window, as we read a fixed number of -grams, we know the minimum-length shift we can make is one position past this point. The length of this shift is at least  positions. This 
means 
we will have at most  windows. The previous statement is only true assuming , as then the denominator is positive. From there we see that we also have the condition that  can be at most , 
where , so the denominator will be . This puts a slightly stricter condition on , that is, .

We can see that, for each window, we verify with probability at most , where  
and . So the probability that a verification is triggered is



\noindent Because by Lemma~\ref{lem:ver}, verification takes time , then per window, the expected cost is



\noindent We wish to ensure that the probability of verifying a window is small enough that the average work done is no more than the work we must do if we skip a window without verification. When we do not verify a window, we read  -grams and shift the window. This means that we read  letters. So a sufficient {\em condition} is the following:



\noindent Or equivalently the below expression, where  is the constant of proportionality:



\noindent By rearranging and setting  we get the condition on the value of  below:



\noindent From the condition on  we can see that it is sufficient to pick , so asymptotically on  we get the following:



\noindent Therefore, for sufficiently large , the below condition is sufficient for optimality, where :



\noindent For this analysis to hold we must be able to read the required number of -grams to ensure the probability of verifying a window is small enough to negate the work of doing it. Note that the above probability is the probability that at least one of -grams match with less than  differences. To ensure we have enough unread random -grams in the window for Lemma~\ref{lem:Chang2} to hold in the above analysis the window must be of size . Now we consider the case where . If we have just verified a window then we have enough new random -grams and our analysis holds. If we have just shifted then we know that all the -grams we previously read matched with at least  differences and we have between 1 and  -grams and the probability that one of these matches with less than  difference is less than in the analysis above so it holds.

The condition  implies a condition on , it must be the case that . This condition on  is weaker than our previous condition on , so to determine the {\em error ratio} , we use the stronger condition. Additionally, from Lemma \ref{lem:prob}, we know that .  So we must pick a value for  subject to .
This inequality implies a limit on the error ratio for which our algorithm is optimal. Clearly it must be the case that  for . 
Rearranging the inequality implies the following sufficient condition on our error ratio: 



\noindent From here we can factorise and divide everything by 2 to get the following:



\noindent So asymptotically on  we have:



\noindent Note that this technique can work for any ratio which satisfies .
For any ratio below this, pick a large enough value for  such that asymptotically on  the algorithm will work in the claimed search time.
By choosing a suitable value for  and  we obtain the following result.


\begin{theorem}
The problem \ACSM\ can be solved in optimal average-case search time .
\end{theorem}

\section{Comparison with Existing Algorithms}

To the best of our knowledge, the only other algorithms to achieve optimal average-case search time for approximate circular string matching are the algorithms presented in~\cite{Fredriksson:2004:ASM:1005813.1041513} for multiple approximate string matching. In the analysis of the algorithms in~\cite{Fredriksson:2004:ASM:1005813.1041513} it is assumed that all patterns are random. In~\cite{doi:10.1142/S0129054106004455} the authors re-analyse their algorithms for the problem of circular string matching with the same preprocessing and space costs. 
In this section, we analyse these results and compare them with our own. 
We refer to the algorithm presented in Section~\ref{sec:algo} as . Due to the constant  in the value of  from Lemma \ref{lem:Chang1}, the exact preprocessing and space costs for these algorithms depend on the chosen value for . It is however possible to determine the minimum savings we make based on the value of  used in all algorithms.

Applying the algorithms in~\cite{Fredriksson:2004:ASM:1005813.1041513} to approximate circular string matching requires a reduction to multiple approximate string matching for matching the  rotations of . The first algorithm in~\cite{Fredriksson:2004:ASM:1005813.1041513} has the following time complexity:



\noindent By setting  this matches our search time and the result is valid when , , and we have  space available, where  is subject to the constraint: 



\noindent Again by setting  this becomes
 and the preprocessing time is . We will refer to this algorithm as . The second algorithm, presented in~\cite{Fredriksson:2004:ASM:1005813.1041513}, has the same preprocessing cost and requires space . We will refer to this algorithm as . The important difference between \textsf{FN1} and \textsf{FN2} comes in the condition on  which is slightly lower for \textsf{FN2}:



\noindent Again, setting  this becomes:



To simplify the comparison between these approaches, we will ignore the factor of , and simply say that the value of  for algorithm  is greater than or equal to . This is lower than the sufficient requirement, so any saving we make using this value must be at least as good or better in reality.

First let us consider \textsf{FN1}. The preprocessing requirement of  is , so before any savings made due to the value of  for , we have reduced the preprocessing cost by a factor of . Given the condition on  for , it is clear that even in the worst case, when ,  will make a saving of at least  on the value of . This corresponds to an additional saving of  in preprocessing time bringing the total to  and  in space. 
In the case of , we make a saving of at least  on the value of . This corresponds to a total saving of  in preprocessing time and  in space. It should be noted that this is a pessimistic analysis of the savings as we have assumed  and , although it must hold that . Note that the standard dynamic programming algorithm can be used with runtime  for verification and  for preprocessing. The speed-ups mentioned in the previous section remain significant as we assumed
that . We still achieve a preprocessing speed up of at least  and  against \textsf{FN1} and \textsf{FN2}, respectively.
Table~\ref{tab:comparison} corresponds to this analysis. 

\begin{table}[h]
\begin{center}
\caption{Comparison of average-case optimal approximate circular string matching algorithms}
\label{tab:comparison}
\begin{tabular}{|l|l|l|l|l|}
\hline Algorithm & Error Ratio () & Space & Preprocessing Time & Condition on  \\ \hline
\textsf{FN1} &  &  &  & \\ \hline
\textsf{FN2} &  &  &  &  \\ \hline 
\textsf{BIP} &  &  &  & \\ \hline
\end{tabular}
\end{center}
\end{table}

\section{Final Remarks}

In this article, we presented a new average-case optimal algorithm for approximate circular string matching. 
To the best of our knowledge, this algorithm is the first average-case optimal algorithm specifically designed for this problem. 
Other average-case optimal algorithms exist but with higher preprocessing and space requirements than the presented algorithm. 
Additionally the considered problem is solved in a more direct fashion, that is, with no reduction to multiple approximate string matching 
by taking greater advantage of the similarity of the rotations of the pattern. 

\noindent Our immediate target is twofold: 
\begin{itemize}
\item first, we plan on tackling the problem of multiple approximate circular string matching. We will try to generalise the approach we have taken here to see if it leads to an average-case optimal 
algorithm in this case. 
\item second, we plan on implementing the presented algorithm. We will then compare the respective implementation to other average- and worst-case approaches.
\end{itemize}

\bibliographystyle{splncs03}
\bibliography{references}
\end{document}
