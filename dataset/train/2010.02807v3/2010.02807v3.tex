
\pdfoutput=1
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{amsmath,bm,amsfonts,dsfont,bbm}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{color}
\usepackage{booktabs}
\usepackage[inline]{enumitem}
\usepackage{xspace}
\usepackage{mdframed}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage[activate={true,nocompatibility}, kerning=true,spacing=true, stretch=5,shrink=25]{microtype}

\aclfinalcopy \def\aclpaperid{1518} 

\newcommand\Mark[1]{\textsuperscript#1}


\author{Shubham Toshniwal\Mark{1},
Sam Wiseman\Mark{1},
Allyson Ettinger\Mark{2},
Karen Livescu\Mark{1},
Kevin Gimpel\Mark{1}
\\
\Mark{1}Toyota Technological Institute at Chicago\\
\Mark{2}Department of Linguistics, University of Chicago\\textrm{ES}(e) = [\min_{\textrm{ENT}(x) = e}\textrm{START}(x), \max_{\textrm{ENT}(x) = e}\textrm{END}(x)]
s_c(x_i, e_j) &\!=\! f_c([x_i; e_j; x_i \odot e_j; g(x_i, e_j)]) \!+\! s_m(x_i)\\
s_c^{\mathit{top}} &\!=\! \max_{j=1 \dotsc M} s_c(x_i, e_j)\\
e^{\mathit{top}} &\!=\! \underset{{j=1 \dotsc M}}{\arg\max}\; s_c(x_i, e_j)

            &d = \arg\min [f_r(e_1), \dotsc, f_r(e_M), f_r(x_i), s_m(x_i)]
        
        where  is a learned feedforward neural network. If  then then the model evicts the previous entity  and reinitialize it to .
        Otherwise if  then the model ignores  due to limited capacity.
        Finally if  then the model predicts the mention to be invalid.
        \item \textbf{Rule-based Bounded Memory (\lru)}
        The Least Recently Used (LRU) principle is a popular choice among memory models~\citep{rae2016scaling, santoro2016one}.
        While \learned  considers all potential entities for eviction, with \lru  this choice is restricted to just the LRU entity, i.e., the entity whose mention was least recently seen.
        The rest of the steps are similar to the \learned model.
    \end{enumerate}
\end{enumerate}
 
\paragraph{Training}
All the models are trained using teacher forcing.
The ground truth decisions for bounded memory models are chosen to maximize the number of mentions tracked by the model (details in Appendix~\ref{sec:app_gt}).
Finally, the training loss is calculated via the addition of the cross-entropy losses for the two steps of mention clustering.
 \section{Experimental Setup}

\label{sec:res_litbank}
\begin{table}[t]

\centering{
\small{
\caption{Results for LitBank (CoNLL F1).}
\label{tab:res_litbank}

\begin{tabular}{lcc}
\toprule
Model & Dev F1 & Test F1 \\\midrule
\unbounded & 77.1 & 76.5 \\
\learned &  & \\
\hspace{0.1in} 5 cells & 71.9 &  70.3 \\
\hspace{0.1in} 10 cells & 75.0 & 74.7 \\
\hspace{0.1in} 20 cells & 75.7 & 75.1 \\
\lru & & \\
\hspace{0.1in} 5 cells & 58.5 & 57.8 \\
\hspace{0.1in} 10 cells & 69.9 & 69.0 \\
\hspace{0.1in} 20 cells & 75.3 & 74.4 \\\midrule
\citet{bamman2019annotated} & - & 68.1 \\
\bottomrule
\end{tabular}
}
}
\end{table}
 
\subsection{Datasets}

\paragraph{LitBank}  is a recent coreference dataset for literary texts~\citep{bamman2019annotated}.
The dataset consists of prefixes of 100 novels with an average length of 2100 words.
Singletons are marked and used for evaluation.  Evaluation is done via 10-fold cross-validation over 80/10/10 splits.\footnote{\url{https://github.com/dbamman/lrec2020-coref/tree/master/data}}

\paragraph{OntoNotes}  consists of 2802/343/348 documents in the train/development/test splits, respectively \cite{pradhan2012conll}.
The documents span 7 genres and have an average length of 463 words.
Singletons are not marked in the dataset.



\subsection{Hyperparameters}
\label{sec:hyperparam}

Document encoding is done using the SpanBERT\textsubscript{LARGE} model of \citet{joshi2020span} which was finetuned for OntoNotes.\footnote{
From the original models, we stripped out just the SpanBERT part which is available at \url{https://huggingface.co/shtoshni/spanbert_coreference_large}
}  The SpanBERT model is not further finetuned.
The other model parameters are trained using the Adam optimizer \citep{kingma2014adam} with an initial learning rate of  which is linearly decayed.
For span representation, we use the embedding function described in \citet{lee-etal-2017-end}.
For OntoNotes we follow the setup of \citet{xia2020revisiting}. We differ, however, in training all the model parameters, except SpanBERT, from scratch.
The models are trained for a maximum of 15 epochs for OntoNotes, and 25 epochs for LitBank. For both the datasets, the training stops if dev performance doesn't improve for 5 epochs. For more details see Appendix~\ref{sec:app_hyperparam}.
 \section{Results}


\begin{table}[t]
\centering{
\small{
\caption{Results for OntoNotes (CoNLL F1) .}
\label{tab:res_ontonotes}

\begin{tabular}{lcc}
\toprule
Model & Dev F1 & Test F1 \\\midrule
\unbounded & 78.4 & 78.1  \\
U-MEM* & 79.6 & 79.6 \\
\learned &  & \\
\hspace{0.1in} 5 cells  & 74.0 & 73.3  \\
\hspace{0.1in} 10 cells  & 77.1 & 76.8  \\
\hspace{0.1in} 20 cells  & 78.1 & 78.2  \\
\lru & & \\
\hspace{0.1in} 5 cells   & 69.8 & 69.6  \\
\hspace{0.1in} 10 cells   & 75.9 & 75.5  \\
\hspace{0.1in} 20 cells   & 78.2 & 77.8  \\\midrule
U-MEM* (\citealp{xia2020revisiting}) & 79.7 & 79.4 \\\midrule
\citet{joshi2020span} & 80.1 & 79.6 \\
\citet{wu2019coreference} & 83.4 & 83.1 \\
\bottomrule
\end{tabular}
}
}
\end{table}

Tables \ref{tab:res_litbank} and~\ref{tab:res_ontonotes} show results of all the proposed models for LitBank and OntoNotes respectively. Detailed results with the performance on different coreference metrics is presented in Table~\ref{tab:full_litbank} for LitBank, and Table~\ref{tab:full_ontonotes} for OntoNotes (Appendix~\ref{sec:misc}).

As expected, the bounded memory models improve with increase in memory.
For both the datasets, the \learned model with 20 memory cells is competitive with the \unbounded model though the gap between them for LitBank is non-trivial. 
Among the bounded memory models, the \learned model is significantly better than \lru for lower numbers of memory cells. We analyze the reasons for this in the next section.

For OntoNotes, the U-MEM* model easily outperforms the \unbounded model which is trained to ignore all non-gold mentions.
These non-gold mentions also include singletons in case of OntoNotes. 
Thus, the  \unbounded model essentially has to predict if a non-coreferent mention will be coreferent with future mentions or not. 
U-MEM* avoids this difficult problem by adding all non-coreferent mentions to memory.
Since in OntoNotes singletons are removed during evaluation, the U-MEM* model is not penalized for predicting singletons corresponding to invalid mentions, and otherwise.
Note that the U-MEM* model doesn't make sense for LitBank where singletons are used for evaluation. The initial empirical results also confirmed that decisively. 


Between the two datasets, we see that the increase in memory results in larger improvement for LitBank.
We also establish a new state-of-the-art for LitBank with the \unbounded model.
For OntoNotes, the U-MEM* model matches the performance of a similar model by  \citet{xia2020revisiting}.
Remarkably, the two U-MEM* models almost match the performance of the computationally and memory intensive span-ranking model of \citet{joshi2020span} whose finetuned SpanBERT document encoder is used by these two models.  
We expect gains by further finetuning the SpanBERT model and learning a parameterized global entity representation, but we leave them for future work.

 \section{Analysis}
In this section we analyze the behavior of the three memory models on LitBank and OntoNotes.


\begin{table}[t!]

\centering{
\small{

\caption{Peak memory and inference time statistics for the LitBank cross-validation split . Note that the training memory statistics depend on document truncation and sampling probability of invalid mentions. The models in this table are trained without document truncation and sample 20\% of invalid mentions during training.}\label{tab:mem_stats}
\begin{tabular}{l ccc}
\toprule
\multirow{2}{*}{Model} & Peak training   & Peak inference  & Inference  \\
     & mem. (in GB) &  mem. (in GB) &   time (in s)\\\midrule
\unbounded              &  11.6   &    3.1 &  29.25 \\
\learned    &      &                &            \\
\hspace{0.1in} 5 cells &  \phantom{1}8.0     &  3.2   &  27.31 \\
\hspace{0.1in} 10 cells &  \phantom{1}8.4    &  3.2   &  27.44 \\
\hspace{0.1in} 20 cells &  \phantom{1}9.1   &   3.2   &  27.86 \\
\lru & &  &  \\
\hspace{0.1in} 5 cells &  \phantom{1}8.0   &  3.2   & 26.19   \\
\hspace{0.1in} 10 cells &  \phantom{1}8.3   & 3.2   & 26.50  \\
\hspace{0.1in} 20 cells &  \phantom{1}8.9   &  3.2  & 26.19  \\\bottomrule
\end{tabular}
}
}
\end{table}



\begin{table}[t!]

\centering{
\small{

\caption{Comparison of number of entities in memory.}
\label{tab:num_ents}

\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{LitBank} & \multicolumn{2}{c}{OntoNotes} \\
     & Avg & Max & Avg & Max\\\midrule
\unbounded  & 80.8  & 160  & 16.0   &  \phantom{1}83 \\
U-MEM*  & -  & -  & 173   &  962 \\
\learned    &      &                &           & \\
\hspace{0.1in} 5 cells & \phantom{1}5.0  & \phantom{11}5   & \phantom{1}4.6       & \phantom{11}5 \\
\hspace{0.1in} 10 cells & 10.0 & \phantom{1}10 & \phantom{1}7.8 & \phantom{1}10 \\
\hspace{0.1in} 20 cells & 20.0 & \phantom{1}20 & 12.1      & \phantom{1}20 \\
\lru & &  & & \\
\hspace{0.1in} 5 cells & \phantom{1}5.0 & \phantom{11}5    & \phantom{1}4.6       & \phantom{11}5   \\
\hspace{0.1in} 10 cells & 10.0 & \phantom{1}10 & \phantom{1}7.9 & \phantom{1}10 \\
\hspace{0.1in} 20 cells & 20.0 & \phantom{1}20 & 11.9      & \phantom{1}20 \\\bottomrule
\end{tabular}
}
}


\end{table}


\paragraph{Memory Utilization}
Table~\ref{tab:mem_stats} compares the memory and inference time statistics for the different memory models for the LitBank cross-validation split zero.\footnote{Peak memory usage estimated via \texttt{torch.cuda.max\_memory\_allocated()}}
For training, the bounded memory models are significantly less memory intensive than the \unbounded model.
The table also shows that the bounded memory models are faster than the \unbounded memory model during inference (inference time calculated by averaging over three runs).
This is because the number of entities tracked by the \unbounded memory model grows well beyond the maximum of 20 memory slots reserved for the bounded models as shown in Table~\ref{tab:num_ents}.

Surprisingly, for inference we see that the bounded models have a slightly larger memory footprint than the \unbounded model.
This is because the document encoder, SpanBERT, dominates the memory usage during inference (as also observed by \citealp{xia2020revisiting}).
Thus the peak memory usage during inference is determined by the mention proposal stage rather than the mention clustering stage.
And during the mention proposal stage, the additional parameters of bounded memory models, which are loaded as part of the whole model, cause the slight uptick in peak inference memory.
Note that using a cheaper encoder or running on a sufficiently long document, such as a book,
can change these results.




\paragraph{Number of Entities in Memory}
Table~\ref{tab:num_ents} compares the maximum number of entities kept in memory by the different memory models for the LitBank cross-validation dev sets and the OntoNotes dev set.
As expected, the \unbounded model keeps more entities in memory than the bounded memory models on average for both datasets.
For LitBank the difference is especially stark with the  \unbounded model tracking about 4/8 times more entities in memory on average/worst case, respectively. 
The difference between the \unbounded and U-MEM* model is striking, with U-MEM* tracking more than 10 times the entities of \unbounded in both the average and worst case.
Also, while some OntoNotes documents do not use even the full 5 memory cell capacity, all LitBank documents fully utilize even the 20 memory cell capacity.
This is because LitBank documents are more than four times as long as OntoNotes documents, and LitBank has singletons marked. These results also justify our initial motivation that with long documents, the memory requirement will increase even if we only keep the entity representations.



\paragraph{\learned vs. \lru}

\begin{table}[t!]
\centering{
\small{
\caption{Average number of mentions ignored by the two bounded memory models.}
\label{tab:num_ignored}


\addtolength{\tabcolsep}{-3pt}
\begin{tabular}{c cccc}
\toprule
Memory & \multicolumn{2}{c}{LitBank} & \multicolumn{2}{c}{OntoNotes} \\
size     & \learned & \lru & \learned & \lru\\\midrule
\phantom{1}5 & \phantom{1}4.5  & 70.0   & 0.3       & 3.7 \\
10 & \phantom{1}0.0 & 14.2 & 0.0 & 0.4 \\
20 & \phantom{1}0.0 & \phantom{1}0.4 & 0.0  & 0.1 \\\bottomrule
\end{tabular}
}
}
\end{table}

Table~\ref{tab:num_ignored} compares the number of mentions ignored by \learned and \lru.
The \learned model ignores far fewer mentions than \lru.
This is because while the \lru model can only evict the LRU entity, which might not be optimal,
the \learned model can choose any entity for eviction.
These statistics combined with the fact that the \learned model typically outperforms \lru mean
that the \learned model is able to anticipate which entities are important and which are not.





\begin{table}[t!]

\centering{
\small{
\caption{Error Analysis for OntoNotes dev set. CE=Conflated Entities, DE=Divided Entity, EM=Extra Mention, EE=Extra Entity, MM=Missing Mention, ME=Missing Entity.}
\label{tab:err_analysis}
\begin{tabular}{l c c c c c c}
\toprule
Model &  CE & DE & EM & EE & MM & ME \\\midrule
\unbounded & 853& 496& 515& 904& 545& \phantom{1}603 \\
U-MEM* & 754& 466& 504& 816& 527& \phantom{1}583 \\
\learned    &      &     &      &    &   &  \\
\hspace{0.1in}  5 cells & 706& 381& 340& 972& 844& 1116 \\
\hspace{0.1in}  10 cells & 757& 425& 340& 868& 655& \phantom{1}894 \\
\hspace{0.1in}  20 cells &  752& 396& 402& 859& 613& \phantom{1}799 \\
\lru    &      &     &      &    &   & \\
\hspace{0.1in}  5 cells & 672& 369& 365& 1146& 923& 1359 \\
\hspace{0.1in}  10 cells & 722& 393& 403& 986& 696& \phantom{1}931 \\
\hspace{0.1in}  20 cells & 713& 420& 380& 833& 559& \phantom{1}853 \\\bottomrule
\end{tabular}
}
}


\end{table}

\paragraph{Error Analysis}
Table~\ref{tab:err_analysis} presents the results of automated error analysis done using the Berkeley Coreference Analyzer~\citep{kummerfeld2013error} for the OntoNotes dev set.
As the memory capacity of models increases, the errors shift from missing mention, missing entity, and divided entity categories, to conflated entities, extra mention, and extra entity categories.
The \learned model outperforms \lru in terms of tracking more entities.  \section{Conclusion and Future Work}
We propose a memory model which tracks a small, bounded number of entities.
The proposed model guarantees a linear runtime in document length, and in practice significantly reduces peak memory usage during training.
Empirical results on LitBank and OntoNotes show that the model is competitive with an unbounded memory version and outperforms a strong rule-based baseline.
In particular, we report state of the art results on LitBank.
In future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory.


\section*{Acknowledgments}


We thank David Bamman for help with the LitBank setup, and Patrick Xia for answering questions about their coreference model. We also thank the anonymous EMNLP reviewers for their valuable feedback.
This material is based upon work supported by the National Science Foundation under Award Nos.~1941178 and 1941160.
 

\begin{figure*}[h]
\centering
\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/litbank_active_chains.pdf}
        \caption{LitBank.}
        \label{fig:litbank_active}
    \end{subfigure}~
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ontonotes_active_chains.pdf}
        \caption{OntoNotes Training Set.}
        \label{fig:ontonotes_active}
    \end{subfigure}
    \caption{Histograms of Maximum Active Entities for documents in LitBank and OntoNotes.}
    \label{fig:active_chains}
\end{figure*}


\begin{figure*}[h]
\centering
\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/litbank_entity_spread.pdf}
        \caption{LitBank.}
        \label{fig:litbank_spread}
    \end{subfigure}~
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ontonotes_entity_spread.pdf}
        \caption{OntoNotes Training Set.}
        \label{fig:ontonotes_spread}
    \end{subfigure}
    \caption{Histograms of Entity Spread as fraction of document length for LitBank and OntoNotes.}
    \label{fig:entity_spread}
\end{figure*}
 
\bibliography{0-main}
\bibliographystyle{acl_natbib}
\newpage



\appendix
\section{Appendix}



\subsection{Maximum Active Entities}
Figure~\ref{fig:active_chains} visualizes the histograms of length of Entity Spread (ES), defined in Section~\ref{sec:active},  as a fraction of document length for documents in LitBank and OntoNotes. For LitBank we only visualize the entity spread of non-singleton clusters because otherwise the histogram is too skewed towards one.
Figure~\ref{fig:entity_spread} visualizes the histograms of Maximum Active Entity Count (MAE), defined in Section~\ref{sec:active}, for documents in LitBank and OntoNotes.




\begin{table}[!ht]

\centering{
\small{
\caption{Hyperparameter options for OntoNotes with preferred choices highlighted in bold.}
\label{tab:hyperparam_ontonotes}
\begin{tabular}{ll}
\toprule
Parameter & Range \\\midrule
Dropout & \{0.4\}\\
FFNN hidden layer & \{3000\}\\
FFNN \# of hidden layers & 1\\
Document Encoding & \{Independent, \textbf{Overlap}\} \\
Label Smoothing & \{0.0, 0.01, 0.1\} \\
Sampling Prob. Invalid Mentions & \{0.25, 0.5, 0.75, 1.0\} \\
Max. \# of BERT Segments & \{\textbf{3}, 5\} \\
Non-coreferent entity weight & \{1.0\}\\
\bottomrule
\end{tabular}
}
}
\end{table}


\subsection{Model Details}
\label{sec:app_hyperparam}

\paragraph{Hyperparameter Choices}
We stick with the hyperparameters for feedforward neural network (FFNN) size and depth from~\citet{joshi2020span}. We didn't do much exploration with dropout but with the limited experiments our finding was that there was little separating dropout probability of 0.3 and 0.4. Among choices for how to segment document into BERT windows, we found overlapping windows to work better than independent BERT windows. Two very important hyperparameters that affect peak memory usage during training are: (a) maximum number of BERT segments, and (b) sampling probability of invalid mentions. Truncating the document by selecting a chosen maximum number of contiguous BERT segments essentially caps the length of documents during training. And the second hyperparameter of sampling invalid mentions  controls the number of invalid mentions, which happens to be the overwhelming category of proposed mentions, the model sees during training. We also explore two hyperparameters for the cross-entropy loss of the first step of mention clustering: (a) label smoothing for regularization, and (b) weight of the non-coreferent term in the cross-entropy loss. 




\paragraph{Specific Hyperparameter Choices for OntoNotes}
We didn't see any gain by increasing the maximum number of BERT segments from 3 to 5 in our initial experiments. The \unbounded and bounded models preferred lower sampling probabilities for invalid mentions but no clear winner in label smoothing weight. The U-MEM* model preferred low label smoothing weight and higher sampling probabilities for invalid mentions. 


\paragraph{Specific Hyperparameter Choices for LitBank}
Initial experiments with cross validation splits \{0, 1, 2\} showed that models preferred maximum number of BERT segments to be 5 in comparison to 3. This might be because most of the LitBank documents are really long, and training on a maximum of 3 BERT segments might lead to a bigger mismatch between training and inference. 
Another hyperparameter that proved important for LitBank was the non-coreferent entity weight of . Due to explosion of combinations driven by the fact that there are 10 cross validation splits, we didn't explore label smoothing for LitBank. 



\begin{table}[t]

\centering{
\small{
\caption{Hyperparameter options for LitBank with  preferred choices highlighted in bold.}
\label{tab:hyperparam_litbank}
\begin{tabular}{ll}
\toprule
Parameter & Range \\\midrule
Dropout & \{0.3\}\\
FFNN hidden layer & \{3000\}\\
FFNN \# of hidden layers & 1\\
Document Encoding & \{Independent, \textbf{Overlap}\} \\
Label Smoothing & \{0.0\} \\
Sampling Prob. Invalid Mentions & \{0.25, 0.5, 0.75, 1.0\} \\
Max. \# of BERT Segments & \{3, \textbf{5}\} \\
Non-coreferent entity weight & \{1.0, \textbf{2.0}\}\\
\bottomrule
\end{tabular}
}
}
\end{table}


\subsection{Ground Truth Generation}
\label{sec:app_gt}
In this section we explain how the ground truth action sequence is generated corresponding to the predicted mention sequence.
The ground truth for \unbounded model is fairly straight forward.
For the bounded memory models, we keep growing the number of entities till we hit the memory ceiling.
For all the entities in memory, we maintain the number of mentions remaining in the ground truth cluster.
For example, a cluster with a total of five mentions, two of which have already been processed by the model, has three remaining mentions.

Suppose now a mention corresponding to a currently untracked entity comes in and the memory is already at full capacity.
Then for the \learned model, we compare the number of mentions of this new entity (along with the current mention) against the number of mentions remaining for all the entities currently being tracked. If there are entities in memory with number of remaining mentions less than or equal to the number of mentions of this currently untracked entity, then the untracked entity replaces the entity with the least number of remaining mentions. Ties among the entities with least number of remaining mentions are broken by the least recently seen entity. If there's no such entity in the memory, then the mention is ignored.
For the \lru model, the comparison is done in a similar way but is limited to just the LRU entity.

\begin{table}[!t]
    \centering{
    \small{

    \caption{Number of model parameters (in millions).}
    \label{tab:num_param}

    \begin{tabular}{lll}
        \toprule
                & LitBank & OntoNotes \\\midrule
    \unbounded  & 37.36 & 37.42\\
    \learned    & 46.83 & 46.95\\
    \lru        & 46.83 & 46.95\\\bottomrule
\end{tabular}
}
}
\end{table}


\subsection{Miscellany}
\label{sec:misc}
\begin{table*}[!ht]

\centering{
\caption{Detailed results of the proposed models on the aggregated LitBank cross-validation test set.}
\label{tab:full_litbank}

\begin{tabular}{l c c c c c c c c c c}
\toprule
Model & \multicolumn{3}{c}{MUC} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \\
 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Avg.\ F1\\\midrule
 \unbounded & 90.8 & 85.7 & 88.2  & 80.0 & 72.1 & 75.9  & 65.1 & 66.0 & 65.5 &  76.5\\
\learned \\
\hspace{0.1in} 5 cells & 90.9 & 80.0 & 85.1  & 77.4 & 64.0 & 70.1  & 57.8 & 53.8 & 55.7 &  70.3\\
\hspace{0.1in} 10 cells & 90.0 & 84.6 & 87.2  & 78.1 & 70.8 & 74.2  & 64.2 & 61.1 & 62.6 &  74.7\\
\hspace{0.1in} 20 cells & 90.3 & 85.0 & 87.6  & 79.2 & 70.9 & 74.8  & 64.1 & 62.0 & 63.0 &  75.1\\
\lru \\
\hspace{0.1in} 5 cells & 91.4 & 74.6 & 82.2  & 75.7 & 51.1 & 61.0  & 52.2 & 21.3 & 30.3 &  57.8\\
\hspace{0.1in} 10 cells & 91.1 & 81.3 & 85.9  & 78.5 & 62.1 & 69.3  & 56.3 & 47.8 & 51.7 &  69.0\\
\hspace{0.1in} 20 cells & 90.5 & 85.1 & 87.7  & 79.7 & 69.8 & 74.4  & 61.1 & 61.0 & 61.1 &  74.4\\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[t]

\centering{
\caption{Detailed results of the proposed models on the OntoNotes 5.0 test set.}
\label{tab:full_ontonotes}
\begin{tabular}{l c c c c c c c c c c}
\toprule
Model & \multicolumn{3}{c}{MUC} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \\
 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Avg.\ F1\\\midrule
\unbounded &  84.6 &  84.1 &  84.3  &  77.2 &  76.2 &  76.7  &  72.5 &  74.3 &  73.4  & 78.1  \\
U-MEM* &  85.5 & 85.1 &  85.3 & 78.7 & 77.3 & 78.0  &  74.2 & 76.5 &  75.3  & 79.6  \\

\learned   \\
\hspace{0.1in} 5 cells &  76.4 &  86.2 &  81.0  &  66.4 &  78.4 &  71.9  &  62.0 &  72.7 &  66.9  & 73.3  \\
\hspace{0.1in} 10 cells &  81.7 &  85.9 &  83.8  &  72.8 &  77.9 &  75.3  &  67.0 &  76.4 &  71.4  & 76.8  \\
\hspace{0.1in} 20 cells &  83.2 &  86.2 &  84.7  &  74.8 &  78.9 &  76.8  &  70.0 &  76.7 &  73.2  & 78.2  \\
\hspace{0.1in} 30 cells &  83.8 &  85.6 &  84.7  &  76.1 &  78.2 &  77.1  &  70.4 &  77.1 &  73.6  & 78.5  \\
\lru \\

\hspace{0.1in} 5 cells  &  72.0 &  85.7 &  78.3  &  60.1 &  78.9 &  68.2  &  57.0 &  68.9 &  62.4  & 69.6  \\
\hspace{0.1in} 10 cells  &  80.1 &  85.7 &  82.8  &  70.5 &  78.3 &  74.2  &  66.0 &  73.4 &  69.5  & 75.5  \\
\hspace{0.1in} 20 cells  &  82.8 &  85.9 &  84.3  &  74.8 &  78.3 &  76.5  &  68.0 &  77.4 &  72.4  & 77.8  \\
\hspace{0.1in} 30 cells  &  84.0 &  85.2 &  84.6  &  76.2 &  78.2 &  77.2  &  72.1 &  75.6 &  73.8  & 78.5  \\\bottomrule
\end{tabular}
}
\end{table*}


\paragraph{Computing Infrastructure \& Runtime}
All the models for a single cross validation split of LitBank can be trained within 4 hours. Training on OntoNotes finishes within 12-20 hours.
The U-MEM* model where all invalid mentions are seen during training is the only configuration that requires 24GB memory GPUs, all other configurations can be trained on 12GB memory GPUs.


\paragraph{Number of model parameters.}

Table~\ref{tab:num_param} shows the number of trainable parameters for all the model and dataset combinations. \learned and \lru have additional parameters in comparison to \unbounded for predicting a score corresponding to the number of remaining mentions for an entity.
Comparing across datasets, the OntoNotes models  have a few additional parameters than their LitBank counterparts for modeling the document genre.


\paragraph{Evaluation Metric Code.}
We use the coreference scorer Perl script available at \url{https://github.com/conll/reference-coreference-scorers}.
We also use the Python implementation by Kenton Lee available at \url{https://github.com/kentonl/e2e-coref/blob/master/metrics.py}.
The two scripts can have some rounding differences.

\begin{table}[t!]

\centering{
\small{
\caption{Spearman correlation of F1 score with document length and \# of entities in OntoNotes dev set.}
\label{tab:corr}

\begin{tabular}{l c c }
\toprule
Model & Document Length & \# of Entities \\\midrule
\unbounded  & -0.31  & -0.28 \\
U-MEM* & -0.28 & -0.25 \\
\learned   &          & \\
\hspace{0.1in} 5 cells & -0.36 & -0.37 \\
\hspace{0.1in} 10 cells & -0.34 & -0.33 \\
\hspace{0.1in} 20 cells & -0.34 & -0.31 \\

\lru & &  \\
\hspace{0.1in} 5 cells & -0.37 & -0.41   \\
\hspace{0.1in} 10 cells & -0.29 & -0.30 \\
\hspace{0.1in} 20 cells& -0.31 & -0.29 \\\bottomrule
\end{tabular}
}
}
\end{table}
\paragraph{Effect of Document Length and Number of Entities.}
Table~\ref{tab:corr} and ~\ref{tab:corr_litbank} presents the Spearman correlation of document F1 score with  document length and number of entities in the document.
The correlations are mostly negative because the task becomes more challenging with an increase in document length and entities, though for LitBank the length of the document  doesn't seem to be a great indicator of the hardness of the task. 
The increase in memory capacity for bounded models results in less negative correlations, suggesting improved performance for challenging documents.




\begin{table}[t!]

\centering{
\small{
\caption{Spearman correlation of F1 score with document length and \# of entities in aggregated LitBank cross-validation dev set.}
\vspace{0.00in}
\label{tab:corr_litbank}
\begin{tabular}{l c c }
\toprule
Model & Document Length & \# of Entities \\\midrule
\unbounded  & -0.07 & -0.36 \\
\learned   &          & \\
\hspace{0.1in} 5 cells & \phantom{-}0.01 & -0.23 \\
\hspace{0.1in} 10 cells & -0.06 & -0.35 \\
\hspace{0.1in} 20 cells & -0.03 & -0.32 \\

\lru & &  \\
\hspace{0.1in} 5 cells & -0.00 & -0.41  \\
\hspace{0.1in} 10 cells & \phantom{-}0.01 & -0.36 \\
\hspace{0.1in} 20 cells& -0.02 & -0.33 \\\bottomrule
\end{tabular}
}
}
\end{table}


 
\end{document}
