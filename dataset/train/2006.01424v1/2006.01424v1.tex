\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{relsize}
\usepackage{siunitx}
\usepackage{array,multirow,adjustbox}
\usepackage{pifont}
\usepackage{color}

\newcommand{\widthscale}{0.13}
\newcommand{\widthscalefive}{0.145}

\definecolor{orange}{rgb}{1,0.5,0}
\newcommand{\sh}[1]{\textcolor{orange}{#1}}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\z}[1]{\textcolor{blue}{#1}}
\newcommand{\qa}[1]{\textcolor{red}{Q:#1}}
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Image Super-Resolution with Cross-Scale Non-Local Attention \\ and Exhaustive Self-Exemplars Mining}

\author{Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, \\
Thomas S. Huang, Humphrey Shi\\
\\
{\small IFP Group, UIUC, Horizon Robotics, University of Oregon}}



\maketitle
\thispagestyle{empty}

\begin{abstract}
Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range feature-wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: \textbf{cross-scale feature correlation}. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks. Our code will be available at: \href{https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention}{https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention}
\end{abstract}
 
\section{Introduction}
Single image super resolution (SISR) aims at recovering a high-resolution (HR) image from its low-resolution (LR) counterpart. SISR has numerous applications in the areas of satellite imaging, medical imaging, surveillance monitoring and high-definition display and imaging \textit{etc} \cite{demirel2011discrete, yu2017computed,zhou2018survey,zhou2020image,zou2011very}. The mapping between LR and HR image is not bijective, which yields more possibilities for a faithful and high-quality HR recovery. Due to this ill-posed nature, SISR remains challenging in the past decades. 


Early efforts in traditional methods provide good practices for resolving SISR. By fully using the intrinsic property of the LR images, they mostly focus on local prior and non-local prior for patch matching and reconstruction. Specifically, local prior based methods, like bilinear or bicubic interpolation, reconstruct pixels merely by the weighted sum of neighbour ones. To go beyond the local limitation, methods based on non-local mean filtering \cite{protter2008generalizing, zhang2012single} start to globally search similar patches over the whole LR image. 

\begin{figure}[t]
    \centering
      \includegraphics[width=0.9\linewidth]{./introduction/intro.png}
\caption{Visualization of most engaged patches captured by our Cross-Scale Non-Local (CS-NL) attention. Cross-scale similarities widely exist in natural images. Multiple high-resolution (HR) patches from the low-resolution (LR) image itself significantly improve target patch super-resolution.}
    \label{fig:match}
\end{figure}
The non-local search for self-similarity can be further extended to \textit{cross-scale} cues. It has been verified that cross-scale patch similarity widely exists in natural images \cite{glasner2009super,zontak2011internal}. Intuitively, in addition to non-local pixel-to-pixel matching, pixels can also be matched with larger image patches. The natural cross-scale feature correspondence makes us search high-frequency details directly from LR images, leading to more faithful, accurate and high-quality reconstructions.








Since the first deep learning-based method \cite{dong2014learning} was proposed, discriminative learning based methods make it possible to use large-scale external image priors for SISR. Compared with traditional methods, they tend to have better feature representation ability, faster inference speed, end-to-end trainable paradigm \cite{goodfellow2016deep,krizhevsky2012imagenet}, and significant performance improvement. To further take the advantages of deep SISR, for several years, efforts \cite{ fan2017balanced, kim2016accurate,lim2017enhanced,zhang2018image,zhang2018residual,yu2018wide,fan2019scale} have been made on increasing the depth or width of the networks to increase the receptive field or improve the feature representation. However, the essence of the solutions was not changed, but locally finding external similar patches. It yields great limitations of deep SISR. SISR performance was boosted right after the non-local attention modules  \cite{dai2019second,liu2018non,zhang2019residual} were proposed. They explored non-local self-similarity property and embedded the non-local modules into the deep network.



What should be the next progress for deep SISR? One intuitive idea is following the traditional methods to explore the non-local \textit{cross-scale} self-similarity in deep networks. Recently, Shocher et al. \cite{shocher2018zero} proposed a zero-shot super-resolution (ZSSR) network to learn the high-frequency details from a pair of down-sampled LR and LR itself using one single test LR image. The essence of ZSSR is an implicit cross-scale patch matching approach using a light-weight network. However, inferring with ZSSR requires additional training time for each new LR image, which is not elegant and efficient enough for practical applications.



Following the successful path of non-local attention modules, in this paper, we are seeking ways of incorporating \textit{cross-scale} non-local attention scheme into the deep SR network. Specifically, we propose a novel Cross-Scale Non-Local (CS-NL) attention module, learning to mine long-range dependencies between LR features to larger-scale HR patches within the same feature map, as shown in Figure \ref{fig:match}. After that, we integrate the previous local prior, In-Scale Non-Local (IS-NL) prior and the proposed Cross-Scale Non-Local prior into a Self-Exemplars Mining (SEM) module, and fuse them with multi-branch mutual-projection. Finally, we embed the SEM module into a recurrent framework for image super-resolution task.












In summary, the main contributions of this paper are three-fold:
\begin{itemize}
    \item The core contribution of the paper is to propose the first Cross-Scale Non-Local (CS-NL) attention module in deep networks for SISR task. We explicitly formulate the pixel-to-patch and patch-to-patch similarities inside the image, and demonstrate that additionally mining cross-scale self-similarities greatly improves the SISR performance.
    \item We then propose a powerful Self-Exemplar Mining (SEM) cell to fuse information recurrently. Inside the cell, we exhaustively mine all the possible intrinsic priors by combining local, in-scale non-local, and the proposed cross-scale non-local feature correlations, and embrace rich external statistics learned by the network.
    \item The newly proposed recurrent SR network achieves the state-of-the-art performance on multiple image benchmarks. Extensive ablation experiments further verify the effectiveness of the novel network.
\end{itemize} 
\section{Related Works}


\paragraph{Self-Similarity in Image SR}
The fact that small patches tend to recur within and across scale of a same image has been verified for most natural images \cite{glasner2009super, zontak2011internal}. Since then, a category of self-similarity based approaches has been extensively developed and achieves promising results. Such algorithms utilize the cross-scale information redundancy of a given image as a unique source for reconstruction without relying on any external examples \cite{freedman2011image,freeman2002example,glasner2009super,huang2015single,michaeli2013nonparametric,singh2014super,yang2013fast}. In the pioneering work, Glasner \textit{et al.} \cite{glasner2009super} proposed to jointly exploit repeating patches within and across image scales by integrating the idea of multiple image SR and example-based SR into a unified framework. Furthermore, Freedman \textit{et al.} \cite{freedman2011image} effectively assumed that similar patches exist in an extremely localized region and thus can greatly reduce computation time. Following this fashion, Yang \textit{et al.} \cite{yang2013fast} proposed a very fast regression model that focused on only in-place cross-scale similarity. To handle appearance variations in the scene, Huang \textit{et al.} \cite{huang2015single} enlarged the internal dictionary by modeling geometric transformations. The idea of internal data repetition has also been applied to solve SR with blur and noisy images \cite{michaeli2013nonparametric,singh2014super}.

\paragraph{Deep CNNs for Image SR} 

The first work that introduced CNN to solve image SR was proposed by \cite{dong2014learning}, where they interpret the three consecutive convolution layers as corresponding extraction, non-linear mapping and reconstruction step in sparse coding. Kim \textit{et al.} \cite{kim2016accurate} proposed a very deep model VDSR with more than 16 convolution layers benefiting from effective residual learning. To further unleash the power of deep CNNs, Lim \textit{et al.} \cite{lim2017enhanced} integrated residual blocks into the SR framework to form a very wide model (EDSR) and a very deep model (MDSR). As the network goes as deep as hundreds of layers, Zhang \textit{et al.} \cite{zhang2018residual} utilized densely connected blocks with global feature fusion to effectively exploit hierarchical features from all intermediate layers. Besides extensive efforts spent on designing wider and deeper structures, algorithms with attention modules \cite{dai2019second,liu2018non,zhang2018image,zhang2019residual} were proposed to further enhance representation power of deep CNNs by exploring feature correlations along either spatial or channel dimension. 

\paragraph{Non-Local Attention in Deep Networks}
In recent years, there is an emerging trend of applying non-local attention mechanism to solve various computer vision problems. In general, non-local attention in deep CNNs allows the network to concentrate more on informative areas. Wang \textit{et al.} \cite{wang2018non} initially proposed non-local neural network to seek semantic relationships for high-level tasks, such as image classification and object detection. On the contrary, non-local attention for image restoration is based on non-local similarities prior. Methods, such as NRLN \cite{liu2018non}, RNAN \cite{zhang2018image} and SAN \cite{dai2019second}, incorporate non-local operation into their networks in order to make better use of image structural cues, by considering long-range feature correlations. As such, they achieved considerable performance gain.

However, existing non-local approaches for image restoration only explored feature similarities at the same scale, while ignoring abundant internal LR-HR exemplars across scales, leading to relatively low performance. It is known that the internal HR correspondences contain more relevant high-frequency information and stronger predictive power. To this end, we propose Cross-Scale Non-Local (CS-NL) attention by exploring cross-scale feature correlations. 


 \section{Cross-Scale Non-Local (CS-NL) Attention}\label{sec:csnl}

In this section, we formulate the proposed cross-scale non-local attention, and compare it with the existing in-scale non-local attention. 
\paragraph{In-Scale Non-Local (IS-NL) Attention}
Non-local attention can explore self-exemplars by summarizing related features from the whole images.
Formally, given image feature map , the non-local attention is defined as

where ,  and  are pairs of coordinates of .  is feature transformation function, and  is correlation function to measure similarity that is defined as

where  and  are feature transformations. Note that the pixel-wise correlation is measured in the same scale.



\begin{figure}[t]
	\centering
		\includegraphics[clip, trim=0 2.2cm 0 2.2cm,width=\linewidth]{model/ssa2t.pdf}
	\caption{The proposed Cross-Scale Non-Local (CS-NL) attention module. The bottom green box is for patch-level cross-scale similarity-matching. The upper branch shows extracting the original HR patches in LR image. }
	\label{fig:attention}
\end{figure}

\paragraph{Cross-Scale Non-Local (CS-NL) Attention}



The above formulation can be easily extended to a cross-scale version referring to Figure \ref{fig:attention}. Instead of measuring the pixel-wise mutual correlation as the in-scale non-local module, the proposed cross-scale attention is designed to measure the correlation between low-resolution pixels and larger-scale patches in the LR image. To super-resolve the LR image, the Cross-Scale Non-Local (CS-NL) attention directly utilizes the patches matched to each pixel within this LR image.  







Hence, for super-resolution purposes, cross-scale non-local attention is built upon in-scale attention by finding candidates in features  downsampled by scaling factor . The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity. Downsampling operation in this paper is bilinear interpolation.

Suppose the input feature map is  (), to compute pixel-patch similarity, we need to first downsample  to  () and find pixel-wise similarity between  and , and finally use corresponding  patches in  to super-resolve pixels in , thus the output  will be .
Cross-scale attention can be adapted from Eq.\ref{eq:nla} as

where  now is the feature patch of size  located at . We obtain the weighted-averaged features  directly from the feature patches  extracted from the input feature maps. Intuitively, with the cross-scale attention, we can mine more faithful and richer high-frequency details from the original intrinsic image resources.














\begin{figure*}
   \centering
		\includegraphics[clip, trim=0cm 2cm 0cm 3cm, width=0.9\linewidth]{model/model.pdf}
\caption{The recurrent architecture with the proposed Self-Exemplars Mining (SEM) cell. Inside SEM, it fuses features learned from a proposed Cross-Scale Non-Local (CS-NL) attention, with others from the In-Scale Non-Local (IS-NL) and the local paths.} 
    \label{fig:model}
\end{figure*}


\paragraph{Patch-Based Cross-Scale Non-Local Attention}
Feature-wise affinity measurement can be problematic. First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features. Take the average pooling as an example, an original region representing a HR \textit{window} and its flipped version have exactly the same high-level features. Therefore, it is likely that many erroneous matches will be synthesized to HR tensors. Besides, adjacent target regions (e.g.  and ) are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.   



Based on the above analysis, we generalize to empirically implement our Cross-Scale Non-Local (CS-NL) attention using another patch-wise matching. Therefore, Eq.\ref{eq:csa1} is generalized to,

and Eq.\ref{eq:pbcsa1} will be identical to Eq.\ref{eq:csa1} if . The measured correlations are efficiently extended to patch-level, and regions in the output feature map  are now densely overlapped due to patch-based matching. 


 \section{Methodology}

The proposed network architecture is shown in Figure \ref{fig:model}. It is basically a recurrent neural network, with each recurrent cell called Self-Exemplars Mining (SEM) fully integrating local, in-scale non-local, and a newly proposed Cross-Scale Non-Local (CS-NL) priors. In this section, we introduce them in a bottom-up manner.

\subsection{CS-NL Attention Module}

Figure \ref{fig:attention} illustrates the newly-proposed Cross-Scale Non-Local (CS-NL) attention module embedded into the deep networks. As formulated in section \ref{sec:csnl}, we apply a patch-level cross-scale similarity-matching in the CS-NL attention module. Specifically, suppose we are conducting an s-scale super-resolution with the module, given a feature map  of spatial size , we first bilinearly downsample it to  with scale , and match the  patches in  with the downsampled  candidates in  to obtain the softmax matching score. Finally, we conduct deconvolution on the score by weighted adding the patches of size  extracted from . The obtained  of size , will be  times super-resolved than . \subsection{Self-Exemplars Mining (SEM) Cell}

\paragraph{Multi-Branch Exemplars}
Inside the Self-Exemplars Mining (SEM) cell, we exhaustively mine all the possible intrinsic priors, and embrace rich external image priors. Specifically, we mine the image self-similarities and learn the new information using a multi-branch structure, including the conventional Local (L) and In-Scale Non-Local (IS-NL) branches, and also the newly proposed CS-NL branch.  

The local branch, in Figure \ref{fig:model}, is a simple identical pathway connecting the convolutional features to the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from \cite{dai2019second} and a deconvolution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in \cite{dai2019second}, we divide the feature maps into region grids, where the inter-dependencies are captured independently in each grid. This reduces the computation burden. 












\paragraph{Mutual-Projected Fusion}
\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\linewidth]{./model/projection_new_new.pdf}
	\caption{Mutual-projected fusion. Downsample and upsample operations are implemented using stride convolution and stride deconvolution, respectively.}
	\label{fig:fusion}
\end{figure}
While three-branch structure in SEM generates three feature maps by independently exploiting each information sources from LR images, how to fuse these separate tensors into a comprehensive feature map remains unclear. One possible solution is simply adding or concatenating them together, as widely used in previous works \cite{lim2017enhanced,liu2018non,zhang2019residual,zhang2018residual}. In this paper, we proposed a mutual-projected fusion to progressively combine features together. The algorithm procedure is illustrated in Figure \ref{fig:fusion}.







To allow the network to concentrate on more informative features, we first compute the residual  between two features from IS-NL  and CS-NL  branch, and then after a single convolution layer  on , the features are added back to  to obtain .



Intuitively, the residual feature  represents the details existing in one source while missing in the other. Such inter-residual projection allows the network to focus on only the distinct information between sources while bypassing the common knowledge, thus improves the discriminative ability of the network.





Motivated by the traditional Image SR and recent DBPN \cite{haris2018deep}, we adopt the back-projection approach to incorporate local information to regularize the feature and correct reconstruction errors. Following \cite{haris2018deep}, the final fused feature  is computed by,

where  is the feature maps of the Local branch,  is a stride convolution to down-sample , and  is a stride deconvolution to upscale the feature maps.

The mutual-projected operation guarantees a residual learning while fusing different feature sources, enabling a more discriminative feature learning compared with trivial adding or concatenating. 


 \subsection{Recurrent Framework}
The repeated SEM cells are embedded into a recurrent framework, as shown in Figure \ref{fig:model}. At each iteration , the hidden unit  of SEM is directly the fused feature map , and the output unit  is the computed by  going through a two-layer CNN. Note that the initial features  are directly computed by the LR image  through only two convolutional layers.   







Later on, the extracted deep SR features  from each iteration  are concatenated together into a wide tensor and mapped to the SR image  via one single convolution operation. The network is trained solely with  reconstruction loss.


%
 

 
\section{Experiments}
\subsection{Datasets and Evaluation Metrics}
Following \cite{lim2017enhanced,zhang2019residual,zhang2018residual}, we use 800 images from DIV2K \cite{timofte2017ntire} dataset to train our models. For testing,  we report the performance on five standard benchmark datasets: Set5 \cite{bevilacqua2012low}, Set14 \cite{zeyde2010single}, B100 \cite{martin2001database}, Urban100 \cite{huang2015single} and Manga109 \cite{matsui2017sketch}. For evaluation, all the SR results are first transformed into YCbCr space and evaluated by PSNR and SSIM \cite{wang2004image} metrics
on Y channel only.

\subsection{Implementation details}
We set the recurrence number of SEM as 12 following \cite{liu2018non}. For the Cross-Scale Non-Local (CS-NL) attention in SEM, we set patch size  and stride  for dense sampling. We use  as filter size for all convolution layers except for those in attention blocks where the kernel size is . The filter size for stride convolution and deconvolution in SEM are set to be equal at each scale, e.g., ,  and  for scale factor 2, 3, 4, respectively. All intermediate features have channel  except for those embedded features in attention module, where . The last convolution layer in SEM has 3 convolution filters that transfer a deep SR feature to an RGB image. 

During training, we crop 16 images with patch size  to form a input batch. The training examples are augmented by random rotating \ang{90}, \ang{180}, \ang{270} and horizontal flipping. To optimize our model, we use ADAM optimizer \cite{kingma2014adam} with , , and 1e-8. The initial learning rate is set to 1e-4 and reduced to half every 150 epochs. The training stops at 500 epochs. We implement the model using PyTorch, and train it on Nvidia V100 GPUs. 

\subsection{Comparisons with State-of-the-arts}
\begin{table*}[thbp]\setlength{\tabcolsep}{7pt}
\small
\center
\begin{center}
\caption{Quantitative results on benchmark datasets. Best and second best results are colored with \textcolor{red}{red} and \textcolor{blue}{blue}.}
\label{tab:results_psnr_ssim_x2348}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Scale} &  \multicolumn{2}{c|}{Set5} &  \multicolumn{2}{c|}{Set14} &  \multicolumn{2}{c|}{B100} &  \multicolumn{2}{c|}{Urban100} &  \multicolumn{2}{c|}{Manga109}  
\\
\cline{3-12}
&  & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM 
\\
\hline
\hline


LapSRN~\cite{lai2017deep} & 2 
& 37.52
 & 0.9591
  & 33.08
   & 0.9130
    & 31.08
     & 0.8950
      & 30.41
       & 0.9101
        & 37.27
         & 0.9740
                   
\\
MemNet~\cite{tai2017memnet} & 2 
& 37.78
 & 0.9597
  & 33.28
   & 0.9142
    & 32.08
     & 0.8978
      & 31.31
       & 0.9195
        & 37.72
         & 0.9740
                   
\\
EDSR~\cite{lim2017enhanced} & 2 
& 38.11
 & 0.9602
  & 33.92
   & 0.9195
    & 32.32
     & 0.9013
      & 32.93
       & 0.9351
        & 39.10
         & 0.9773
                   
\\
SRMDNF~\cite{zhang2018learning} & 2 
& 37.79
 & 0.9601
  & 33.32
   & 0.9159
    & 32.05
     & 0.8985
      & 31.33
       & 0.9204
        & 38.07
         & 0.9761
                   
\\
DBPN~\cite{haris2018deep} & 2 
& 38.09
 & 0.9600
  & 33.85
   & 0.9190
    & 32.27
     & 0.9000
      & 32.55
       & 0.9324
        & 38.89
         & 0.9775        
\\
RDN~\cite{zhang2018residual} & 2 
& 38.24
 & 0.9614
  & 34.01
   & 0.9212
    & 32.34
     & 0.9017
      & 32.89
       & 0.9353
        & 39.18
         & 0.9780
         
\\


RCAN~\cite{zhang2018image} & 2 
& {38.27}
 & {0.9614}
  & \color{red}{34.12}
   & \color{blue}{0.9216}
    & \color{blue}{32.41}
     & \color{blue}{0.9027}
      & \color{red}{33.34}
       & \color{blue}{0.9384}
        & \color{red}{39.44}
         & \color{blue}{0.9786}
\\       
NLRN~\cite{liu2018non}& 2 
& {38.00}
 & {0.9603}
  & {33.46}
   & {0.9159}
    & {32.19}
     & {0.8992}
      & {31.81}
       & {0.9249}
        & {--}
         & {--}
\\
SRFBN~\cite{li2019feedback}& 2 
& {38.11}
 & {0.9609}
  & {33.82}
   & {0.9196}
    & {32.29}
     & {0.9010}
      & {32.62}
       & {0.9328}
        & {39.08}
         &{0.9779}
\\
 OISR~\cite{he2019ode} & 2 
& {38.21}
 & {0.9612}
  & {33.94}
   & {0.9206}
    & {32.36}
     & {0.9019}
      & {33.03}
       & {0.9365}
        &--
         & --      
\\
SAN~\cite{dai2019second} & 2 
& \color{red}{38.31}
 & \color{red}{0.9620}
  & \color{blue}{34.07}
   & {0.9213}
    & \color{red}{32.42}
     & \color{red}{0.9028}
      & {33.10}
       & {0.9370}
        & {39.32}
         & \color{red}{0.9792}
\\
CSNLN (ours) & 2 
& \color{blue}{38.28}
 & \color{blue}{0.9616}
  & \color{red}{34.12}
   & \color{red}{0.9223}
    & {32.40}
     & {0.9024}
      & \color{blue}{33.25}
       & \color{red}{0.9386}
        & \color{blue}{39.37}
         & {0.9785}
\\

                 
\hline
\hline

LapSRN~\cite{lai2017deep} & 3 
& 33.82
 & 0.9227
  & 29.87
   & 0.8320
    & 28.82
     & 0.7980
      & 27.07
       & 0.8280
        & 32.21
         & 0.9350
                   
\\
MemNet~\cite{tai2017memnet} & 3 
& 34.09
 & 0.9248
  & 30.00
   & 0.8350
    & 28.96
     & 0.8001
      & 27.56
       & 0.8376
        & 32.51
         & 0.9369
                   
\\
EDSR~\cite{lim2017enhanced} & 3 
& 34.65
 & 0.9280
  & 30.52
   & 0.8462
    & 29.25
     & 0.8093
      & 28.80
       & 0.8653
        & 34.17
         & 0.9476
                   
\\
SRMDNF~\cite{zhang2018learning} & 3 
& 34.12
 & 0.9254
  & 30.04
   & 0.8382
    & 28.97
     & 0.8025
      & 27.57
       & 0.8398
        & 33.00
         & 0.9403
                   
\\
RDN~\cite{zhang2018residual} & 3 
& 34.71
 & 0.9296
  & 30.57
   & 0.8468
    & 29.26
     & 0.8093
      & 28.80
       & 0.8653
        & 34.13
         & 0.9484
         
\\
RCAN~\cite{zhang2018image}& 3 
& \color{blue}{34.74}
 &\color{blue}{0.9299}
  & \color{blue}{30.65}
   & \color{red}{0.8482}
    & \color{blue}{29.32}
     & \color{blue}{0.8111}
      & \color{blue}{29.09}
       &\color{blue}{0.8702}
        & \color{blue}{34.44}
         &\color{blue}{0.9499}
         
\\
NLRN~\cite{liu2018non}& 3 
& {34.27}
 &{0.9266}
  & {30.16}
   &{0.8374}
    & {29.06}
     & {0.8026}
      & {27.93}
       & {0.8453}
        & {-}
         & {-}
\\
SRFBN~\cite{li2019feedback}& 3 
& {34.70}
 &{0.9292}
  & {30.51}
   &{0.8461}
    & {29.24}
     & {0.8084}
      & {28.73}
       & {0.8641}
        & {34.18}
         & {0.9481}
\\
OISR~\cite{he2019ode}& 3 
& {34.72}
 &{0.9297}
  & {30.57}
   &{0.8470}
    & {29.29}
     & {0.8103}
      & {28.95}
       & {0.8680}
        & {-}
         & {-}
\\
SAN~\cite{dai2019second} & 3 
& \color{red}{34.75}
 &\color{red}{0.9300}
  & {30.59}
   &\color{blue}{0.8476}
    &\color{red}{29.33}
     & \color{red}{0.8112}
      & {28.93}
       & {0.8671}
        & {34.30}
         & {0.9494}
        
\\
CSNLN (ours) & 3
& \color{blue}{34.74}
 & \color{red}{0.9300}
  & \color{red}{30.66}
   & \color{red}{0.8482}
    & \color{red}{29.33}
     & {0.8105}
      & \color{red}{29.13}
       & \color{red}{0.8712}
        & \color{red}{34.45}
         & \color{red}{0.9502}
\\
\hline
\hline
LapSRN~\cite{lai2017deep} & 4 
& 31.54
 & 0.8850
  & 28.19
   & 0.7720
    & 27.32
     & 0.7270
      & 25.21
       & 0.7560
        & 29.09
         & 0.8900
                   
\\
MemNet~\cite{tai2017memnet} & 4 
& 31.74
 & 0.8893
  & 28.26
   & 0.7723
    & 27.40
     & 0.7281
      & 25.50
       & 0.7630
        & 29.42
         & 0.8942
                   
\\
EDSR~\cite{lim2017enhanced} & 4 
& 32.46
 & 0.8968
  & 28.80
   & 0.7876
    & 27.71
     & 0.7420
      & 26.64
       & 0.8033
        & 31.02
         & 0.9148
                   
\\
SRMDNF~\cite{zhang2018learning} & 4 
& 31.96
 & 0.8925
  & 28.35
   & 0.7787
    & 27.49
     & 0.7337
      & 25.68
       & 0.7731
        & 30.09
         & 0.9024
                   
\\
DBPN~\cite{haris2018deep} & 4 
& 32.47
 & 0.8980
  & 28.82
   & 0.7860
    & 27.72
     & 0.7400
      & 26.38
       & 0.7946
        & 30.91
         & 0.9137
         
\\
RDN~\cite{zhang2018residual} & 4 
& 32.47
 & 0.8990
  & 28.81
   & 0.7871
    & 27.72
     & 0.7419
      & 26.61
       & 0.8028
        & 31.00
         & 0.9151
         
\\
RCAN~\cite{zhang2018image}& 4 
& {32.63}
 & {0.9002}
  & {28.87}
   &\color{red}{0.7889}
    & {27.77}
     & \color{blue}{0.7436}
      &\color{blue} {26.82}
       & \color{blue}{0.8087}
        &\color{blue}{31.22}
         & \color{blue}{0.9173}
         
                   
\\
NLRN~\cite{liu2018non}& 4 
& {31.92}
 & {0.8916}
  & {28.36}
   & {0.7745}
    & {27.48}
     & {0.7306}
      & {25.79}
       & {0.7729}
        & {-}
         & {-}
\\
SRFBN~\cite{li2019feedback} & 4 
& {32.47}
 & {0.8983}
  & {28.81}
   & {0.7868}
    & {27.72}
     & {0.7409}
      & {26.60}
       & {0.8015}
        & {31.15}
         & {0.9160}
\\
OISR~\cite{he2019ode} & 4 
&{32.53}
 &{0.8992}
  &{28.86}
   & {0.7878}
    &{27.75}
     & {0.7428}
      & {26.79}
       & {0.8068}
        & {-}
         & {-}
\\
SAN~\cite{dai2019second} & 4 
& \color{blue}{32.64}
 &\color{blue}{0.9003}
  &\color{blue}{28.92}
   &\color{blue}{0.7888}
    &\color{blue}{27.78}
     & \color{blue}{0.7436}
      & {26.79}
       & {0.8068}
        & {31.18}
         & {0.9169}
\\
CSNLN (ours)  & 4 
& \color{red}{32.68}
 & \color{red}{0.9004}
  & \color{red}{28.95}
   & \color{blue}{0.7888}
    & \color{red}{27.80}
     & \color{red}{0.7439}
      & \color{red}{27.22}
       & \color{red}{0.8168}
        & \color{red}{31.43}
         & \color{red}{0.9201}
         

\\
\hline             
\end{tabular}
\end{center}
\end{table*}
 
\begin{figure*}[htbp]
\newlength\fsdttwofigBD
	\setlength{\fsdttwofigBD}{-5.0mm}
	\scriptsize
	\centering
	\begin{tabular}{cc}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
				\includegraphics[width=0.229\textwidth]{./experiment/picture1/hr_draw.png}
				\\
				 Urban100 ():
				\\
				img\_005


			\end{tabular}
		\end{adjustbox}
		\hspace{-2.3mm}
		\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/hrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/bicubiccrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/lapsrncrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/edsrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/dbpncrop.png} 
				\\
				HR \hspace{\fsdttwofigBD} &
				Bicubic \hspace{\fsdttwofigBD} &
				LapSRN~\cite{lai2017deep} \hspace{\fsdttwofigBD} &
				EDSR~\cite{lim2017enhanced} \hspace{\fsdttwofigBD} &
				DBPN~\cite{haris2018deep}
				\\
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/oisrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/rdncrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/rcancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/sancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture1/ourcrop.png}  
				\\ 
				OISR~\cite{he2019ode} \hspace{\fsdttwofigBD} &
				RDN~\cite{zhang2018residual} \hspace{\fsdttwofigBD} &
				RCAN~\cite{zhang2018image} \hspace{\fsdttwofigBD} &
				SAN~\cite{dai2019second}  \hspace{\fsdttwofigBD} &
				Ours 
			 \hspace{\fsdttwofigBD} 
				\\
			\end{tabular}
		\end{adjustbox}
		\vspace{0.5mm}
		\\

		\begin{adjustbox}{valign=t}
\begin{tabular}{c}
				\includegraphics[width=0.229\textwidth]{./experiment/picture3/hr_draw.png}
				\\
				 Urban100 ():
				\\
				img\_078


			\end{tabular}
		\end{adjustbox}
		\hspace{-2.3mm}
		\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/hrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/bicubiccrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/lapsrncrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/edsrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/dbpncrop.png} 
				\\
				HR \hspace{\fsdttwofigBD} &
				Bicubic \hspace{\fsdttwofigBD} &
				LapSRN~\cite{lai2017deep} \hspace{\fsdttwofigBD} &
				EDSR~\cite{lim2017enhanced} \hspace{\fsdttwofigBD} &
				DBPN~\cite{haris2018deep}
				\\
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/oisrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/rdncrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/rcancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/sancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture3/ourcrop.png}  
				\\ 
				OISR~\cite{he2019ode} \hspace{\fsdttwofigBD} &
				RDN~\cite{zhang2018residual} \hspace{\fsdttwofigBD} &
				RCAN~\cite{zhang2018image} \hspace{\fsdttwofigBD} &
				SAN~\cite{dai2019second}  \hspace{\fsdttwofigBD} &
				Ours 
			 \hspace{\fsdttwofigBD} 
				\\
			\end{tabular}
		\end{adjustbox}
		\vspace{0.5mm}
		\\
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
				\includegraphics[width=0.229\textwidth]{./experiment/picture2/hr_draw.png}
				\\
				Urban100 ():
				\\
img\_047
			\end{tabular}
		\end{adjustbox}
		\hspace{-2.3mm}
		\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/hrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/bicubiccrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/lapsrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/edsrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/dbpncrop.png} 
				\\
				HR \hspace{\fsdttwofigBD} &
				Bicubic \hspace{\fsdttwofigBD} &
				LapSRN~\cite{lai2017deep} \hspace{\fsdttwofigBD} &
				EDSR~\cite{lim2017enhanced} \hspace{\fsdttwofigBD} &
				DBPN~\cite{haris2018deep}
				\\
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/oisrcrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/rdncrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/rcancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/sancrop.png} \hspace{\fsdttwofigBD} &
				\includegraphics[width=\widthscalefive \textwidth]{./experiment/picture2/ourcrop.png}  
				\\ 
				OISR~\cite{he2019ode} \hspace{\fsdttwofigBD} &
				RDN~\cite{zhang2018residual} \hspace{\fsdttwofigBD} &
				RCAN~\cite{zhang2018image} \hspace{\fsdttwofigBD} &
				SAN~\cite{dai2019second}  \hspace{\fsdttwofigBD} &
				Ours
				\\
				
				\\
			\end{tabular}
		\end{adjustbox}
	\end{tabular}
	\caption{
		Visual comparison for  SR on Urban100 dataset. For all the shown examples, especially the images with repeated edges or structures, our method perceptually out-performs other state-of-the-arts by a large margin.
	}
	\label{tab:table 4}
\vspace{-3mm}
\end{figure*}
 To verify the effectiveness of the proposed model, we compare our approach with 11 state-of-the-art methods, which are LapSRN \cite{lai2017deep}, SRMDNF \cite{zhang2018learning}, MemNet \cite{tai2017memnet}, EDSR \cite{lim2017enhanced}, DBPN \cite{haris2018deep}, RDN \cite{zhang2018residual}, RCAN \cite{zhang2018image}, NLRN\cite{liu2018non}, SRFBN \cite{li2019feedback}, OISR \cite{he2019ode} and SAN \cite{dai2019second}. 

\paragraph{Quantitative Evaluations}
In Table \ref{tab:results_psnr_ssim_x2348}, We report the quantitative comparisons for scale factor 2, 3 and 4. Compared with other methods, our CS-NL-embedded recurrent model achieved the best performance on multiple benchmarks for almost all scaling factors. It worth noting that our model significantly outperforms NLRN, which is the first proposed in-scale non-local network for image restoration. 

Our method has better performance when the scaling factor is larger. For  settings, our CS-NL embedded model achieves the state-of-the-art PSNR for all the testing benchmarks. In particular, for Urban100 and Manga109 dataset, our model outperforms previous state-of-the-art approaches by 0.4 dB and 0.2 dB, respectively. These datasets contains abundant repeated patterns, such as edges and small corners. Therefore, the superior performance demonstrates the effectiveness of our attention in exploiting internal HR hints. We claim that cross-scale intrinsic priors are indeed effective for a more faithful reconstruction.
\paragraph{Qualitative Evaluations}
The qualitative evaluations on Urban100 dataset are shown in Figure \ref{tab:table 4}. The proposed model is proven to be more effective for images with repeated high-frequency features like windows, lines, squares, etc. For example, in the figure of building, LR image contains plenty of window features covering long-range of spatial-frequency. Directly utilizing those cross-scale self-exemplars from the images will be significantly better than searching for in-scale features or external patches in the training set. For all the shown examples, our method perceptually out-performs other state-of-the-arts by a large margin.
\begin{table}[h]
\footnotesize
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
        &EDSR & DBPN & RDN & RCAN & SAN & CSNLN \\
         \hline
         Para. & 43M & \color{blue}{10M} & 22.3M & 16M & 15.7M & \color{red}{3M}\\
         \hline
         PSNR & 38.11 & 38.09 & 38.24 & 38.27 & \color{red}{38.31} &\color{blue}{38.28}
         \\ \hline
    \end{tabular}
    \caption{Model size and performance comparsion on Set5 (2) .}
    \label{tab:table model}
\end{table}

\noindent\textbf{Model Size Analysis}
We report the model size and performance for recently competitive SR methods in Table \ref{tab:table model}. Comparing with others, our model has the least parameters, which only needs 20\% parameters of RCAN and SAN, but achieves the second best result. Therefore, our CSNLN obtains better parameter efficiency in comparison with other methods, by effectively mining internal HR hints. 

\subsection{Ablation Study}
\begin{figure*}[t]
 \centering
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img004x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/4down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/4up.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img024x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/24down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/24up.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img030x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[width = 1\textwidth]{./experiment/attention/30down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/30up.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img038x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/38down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/38up.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img052x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[width = 1\textwidth]{./experiment/attention/52down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/52up.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/img062x4.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/62down.png}
 \end{minipage}
 \begin{minipage}{0.105\textwidth}
    \includegraphics[ width = 1\textwidth]{./experiment/attention/62up.png}
 \end{minipage}
 \\
 \centering
 \vspace{1.5mm}
\caption{Comparisons of correlation maps of CS-NL attention and IS-NL attention. For each group of three columns, the left one is the input image, the middle one shows the in-scale attention, and the right one depicts the cross-scale attention. one can see that the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeated structures in the images, demonstrating its effectiveness for exploiting internal HR information. }
\label{fig:att}
\end{figure*}

\begin{table*}[]\setlength{\tabcolsep}{8pt}
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline
    Local (L) branch &  & \cmark &&  &\cmark &\cmark & &\cmark  \\
    \cline{1-1}
         In-Scale Non-Local (IS-NL) Branch & & &\cmark &  &\cmark &  & \cmark & \cmark
         \\
         \cline{1-1}
         Cross-Scale Non-Local (CS-NL) branch&  &  & &\cmark  & &\cmark &\cmark & \cmark \\
         \hline
         PSNR & 33.32 &33.47 &33.52 &33.51 &33.62 &33.64 &33.57 &\textbf{33.74}\\
         \hline
    \end{tabular}}
\caption{Ablation study on the branch features in SEM. We report the PSNR results on Set14 (2) after 200 epochs. With an additional CS-NL branch, the performance becomes 33.74dB compared with the one without CS-NL, 33.62dB.}
    \vspace{-1mm}
    \label{tab:table 1}
\end{table*}

\begin{table}[]
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabularx}{\linewidth}{|l|Y|Y|Y|}
    \hline
         Attention Patch Size & 11 & 33 & 55 \\
         \hline
         PSNR & 33.67 &\textbf{33.74} &33.61\\
         \hline
    \end{tabularx}}
\caption{Effects of patch size for matching.}
    \vspace{-1mm}
    \label{tab:table 2}
\end{table}

\begin{table}[]
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabularx}{\linewidth}{|l|Y|Y|Y|}
    \hline
         Fusion & addition & concatenation & Mutual Projection  \\
         \hline
         PSNR &33.69 &33.62 &\textbf{33.74} \\
         \hline
    \end{tabularx}}
\caption{Comparison of fusion operators.}
    \label{tab:table 3}
\end{table}


\paragraph{Cross-Scale v.s. In-Scale Attention}
The key difference between our cross-scale non-local attention and the in-scale one is to allow network to benefit from abundant internal HR hints with different scales. To verify it, we visualize its correlation maps on 6 images from Urban100 \cite{huang2015single}, and compare it with in-scale non-local attention. 

As shown in Figure \ref{fig:att}, these images contain extensive recurrences of small patterns both within scale and across scale. It is interesting to point out that once the image contains repeated edges, such redundant recurrences are not limited to where high scale patterns appear, but also can be found in-place or even in the area that pattern tends to slightly shrink. For example, the HR appearance of a small corner can be simply found by properly zooming out. All these recurrences contain valuable high frequency information for reconstruction. As shown in Figure \ref{fig:att}, the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeating structures in the images, demonstrating its effectiveness for exploiting internal HR information. 

\paragraph{Self-Exemplars Mining Module} 
To demonstrate the effectiveness of our proposed Self-Exemplars Mining (SEM) module, we construct a baseline model by removing all branches, resulting in a fully convolutional recurrent network (RNN). To keep the total parameters same as other variants, we set 10 convolution layers in the recurrent block. As shown in Table \ref{tab:table 1}, the basic RNN achieves 33.32 dB on Set14 (). Results in first 4 columns demonstrate the effectiveness of individual branch, as each of them brings improvement over the baseline. Furthermore, from last 4 columns, we find that combining these branches achieves the best performance. For example, when cross-scale non-local branch is added, the performance is improved from 33.47 dB to 33.64 dB. When both local branch and non-local branch are added to the network, the best performance is achieved by further adding cross-scale non-local branch, resulting in a improvement from 33.62 dB to 33.74 dB. 

These facts indicate that the cross-scale correlations learned by our attention can not be captured by either simple convolution or previous in-scale attention module, demonstrating that our CS-NL attention is of crucial importance for fully exploiting information from LR images.

\paragraph{Patch-Based Matching v.s. Pixel-Based Matching} In practical implementation, we compute patch-wise correlation rather than pixel-wise correlation. Here we investigate the influence of patch size  in CS-NL attention. We compare the patch size of ,  and , where  is equivalent to pixel-wise matching. As shown in Table \ref{tab:table 2}, the performance peak is at , which is higher than pixel based matching, indicating that a small patch can serve as a better region descriptor. However, when using a larger patch size, the performance is worse than the pixel-based matching. This is mainly because larger patches mean additional constraint on the content when evaluating similarity, and therefore it becomes harder to find well-matched correspondences. All these results show that it is necessary to choose a proper patch size for effectively computing correlations in CS-NL attention.

\paragraph{Mutual-Projected Fusion} We show the effectiveness of our mutual-projected fusion by comparing it with other commonly used fusion strategies, e.g., feature addition and concatenation. As shown in Table \ref{tab:table 3}, it can be found that our projection based fusion obtains the best result. By replacing the addition and concatenation with mutual projection, the performance improves about 0.05 dB and 0.12 dB. These results demonstrate the effectiveness of our fusion module in progressively aggregating information. 



 \section{Conclusion}
In this paper, we proposed the first Cross-Scale Non-Local (CS-NL) attention module for image super-resolution deep networks. With the novel module, we are able to sufficiently discover the widely existing cross-scale feature similarities in natural images. Further integrating it with local and the previous in-scale non-local priors, while embracing the abundant external information learned by the network, our recurrent model achieved state-of-the-art performance for multiple benchmarks. Our experiments suggest that exploring cross-scale long-range dependencies will greatly benefit single image super-resolution (SISR) task, and possibly is also promising for general image restoration task.  
\paragraph{Acknowledgments}
This work is in part supported by IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
