
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  

\IEEEoverridecommandlockouts                              \overrideIEEEmargins


\usepackage[lined,ruled,vlined]{algorithm2e}

\usepackage{mathtools}

\usepackage{bm}
\usepackage{srcltx}
\usepackage{cite}    
\usepackage{verbatim}
\usepackage{float}
\usepackage{color}
\usepackage{acronym}\usepackage{subcaption}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{mathtools}



\usepackage[lined,ruled,vlined]{algorithm2e}

\usepackage{amssymb,latexsym,amsfonts,amsmath,amscd} 
\usepackage{paralist}
\usepackage[left=20mm, right=20mm, top=20mm,
  bottom=20mm]{geometry}
 \newtheorem{definition}{Definition}
 \newtheorem{problem}{Problem}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\usepackage{tikz}
\usetikzlibrary{shapes,snakes}



\usepackage{placeins}
\usetikzlibrary{arrows,fit,shapes,automata}
\usetikzlibrary{positioning,fit,calc,shapes}
\usetikzlibrary{decorations.fractals}
\usetikzlibrary{decorations.markings}


\newtheorem{theorem}{Theorem}
 \newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}


\acrodef{mitl}[MITL]{metric interval temporal logic}
\acrodef{sde}[SDE]{stochastic differential equation}
\acrodef{dfta}[DFTA]{deterministic finite-state timed automaton}
\acrodef{mdp}[MDP]{Markov decision process}
\newcommand*{\longhookrightarrow}{\ensuremath{\lhook\joinrel\relbar\joinrel\rightarrow}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}_{\ge 0}}
\newcommand{\Occ}{\mathsf{Occ}}


\newcommand{\calV}{\mathcal{V}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calM}{\mathcal{M}}


\newcommand{\calAP}{\mathcal{AP}}
\newcommand{\pb}{\mathsf{PB}}

\newcommand{\dt}{\mathsf{DT}}

  \newcommand{\sink}{\mathsf{sink}}
  \newcommand{\truev}{\top}

  \newcommand{\init}{\mathsf{Init}}


 \newcommand{\nat}{\mathbb{N}}
\providecommand{\abs}[1]{\left|#1\right|}
\providecommand{\norm}[1]{\lVert#1\rVert}


\title{\LARGE \bf Computational methods for stochastic control with
  metric interval temporal logic
  specifications \author{Jie Fu and Ufuk Topcu}
\thanks{J. Fu and U. Topcu are with the Department of Electrical and
  Systems Engineering, University of Pennsylvania, Philadelphia, PA
  19104, USA  {\tt\small jief, utopcu@seas.upenn.edu}.}}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
  This paper studies an optimal control problem for continuous-time
  stochastic systems subject to reachability objectives specified in a
  subclass of metric interval temporal logic specifications, a
  temporal logic with real-time constraints. We propose a
  probabilistic method for synthesizing an optimal control policy that
  maximizes the probability of satisfying a specification based on a
  discrete approximation of the underlying stochastic system.  First,
  we show that the original problem can be formulated as a stochastic
  optimal control problem in a state space augmented with finite
  memory and states of some clock variables. Second, we present a
  numerical method for computing an optimal policy with which the
  given specification is satisfied with the maximal probability in
  point-based semantics in the discrete approximation of the
  underlying system. We show that the policy obtained in the discrete
  approximation converges to the optimal one for satisfying the
  specification in the continuous or dense-time semantics as the
  discretization becomes finer in both state and time.  Finally, we
  illustrate our approach with a robotic motion planning example.
\end{abstract}
\section{Introduction}
\label{sec:intro}
Stochastic optimal control is an important research area for analysis
and control design for continuous-time dynamical systems that operate
in the presence of uncertainty. However, existing stochastic control methods cannot be readily applied
to handle complex temporal logic specifications with real-time
constraints, which are of growing interest to the design of autonomous
and semiautonous systems
\cite{pnueli1992temporal,koymans1990specifying,fainekos2009temporal,wongpiromsarn2012receding}. In
this paper, we propose a numerical method for stochastic optimal
control with respect to a subclass of metric temporal logic
specifications. Particularly, given a specification encoding desirable
properties of a continuous-time stochastic system, the task is to
synthesize a control policy such that if the system implements the
policy, then the probability of a path satisfying the formula is
maximized.





Metric temporal logic (MTL) is one of many real-time logics that not
only express the relative temporal ordering of events as linear
temporal logic (LTL), but also the duration between these events. For
example, a surveillance task of a mobile robot, infinitely revisiting
region 1 and 2, can be expressed in LTL. But tasks with quantitative
timing constraints, for instance, visiting region 2 within 5 minutes
after visiting region 1, require the expressive power of MTL. For
system specifications in LTL and its untimed variants, methods have
been developed for quantitative verification of discrete-time
stochastic hybrid systems
\cite{abate2010approximate,abate2011quantitative}, control design of
continuous-time and discrete-time linear stochastic systems
\cite{lahijanian2009probabilistic,Maria2015}.  For MTL and its
variants, a specification-guided testing framework is proposed in
\cite{Abbas2014} for verification of stochastic cyber-physical
systems.  Reference \cite{Karaman2008} proposes a solution to the
vehicle routing problem with respect to MTL specifications. Reference
\cite{JunLiu2014} develops an abstraction technique and a method of
transforming MTL formulas to LTL formulas. As a result, existing
synthesis methods for discrete deterministic systems with LTL
constraints can be applied to design switching protocols for
continuous-time deterministic systems in dynamical environment subject
to MTL constraints. Reference \cite{VasuHSCC} proposes a reactive
synthesis method to non-deterministic systems with respect to
maximizing the robustness of satisfying a specification in signal
temporal logic, which is a subclass of MTL. The robustness of a path
is measured by the distance between this path and the set of paths
that satisfy the specification.
Our work differs from existing ones in both the problem formulation
and control objective.  We deal with systems with stochastic dynamics,
rather than non-deterministic systems \cite{VasuHSCC,JunLiu2014}.  We
consider reachability objectives specified in \ac{mitl}, which is a
subclass of MTL. The optimality of control design is evaluated by the
probability of satisfying the given specification. The synthesis
method is with respect to quantitative criteria (the probability of
satisfying the formula), not qualitative criteria (whether the formula
is satisfied).

Our solution approach utilizes the \emph{Markov chain approximation
  method} \cite{kushner2001numerical} to generate a discrete
abstraction in the form of a \ac{mdp} approximating the
continuous-time stochastic system. Based on a product operation
between the discrete abstraction and a finite-state automaton that
represents the desirable system property, a near optimal policy with
respect to the probability of satisfying the formula in the
\emph{point-based semantics} of \ac{mitl} \cite{henzinger1991temporal}
can be computed by solving an optimal planning problem in the
\ac{mdp}. We show that as the discretization gets finer in both state
space and time space, the optimal control policy in the abstract
system converges to the optimal one in the original stochastic system
with respect to the probability of satisfying the \ac{mitl} formula in
the \emph{continuous or dense-time semantics} \cite{Alur1996}.













\section{Preliminaries and problem formulation}
\label{sec:prelim}



\subsection{The system model and timed behaviors}


We study stochastic dynamical systems in continuous time.
The state of the system evolves according to the \ac{sde}

where  and
 are continuous and bounded
functions given  and  as compact state and input
space;  is an -valued, -Wiener
process which serves as a ``driving noise'' and is defined on the
probability space ;  is an -valued,
-adapted, measurable process also defined on
 and  is an \emph{admissible control
  law}, i.e., a -valued, -adapted, measurable process
defined on . We say  solve the SDE in \eqref{sde} provided that

holds for all time .











We introduce a labeling function that relates a sample path of the
\ac{sde} in \eqref{sde} to a \emph{timed behavior}. Let  be
a finite set of atomic propositions and 
be a labeling function that maps each state  to a set of
atomic propositions that evaluate true at that state.


A \emph{time interval}  is a convex set 
where , the symbol `' can be one of
`', `', and the symbol `' can be one of `',
`', and . For a time interval of the above form,  and
 are left and right end-points, respectively.  A time interval is
empty if it contains no point. A time interval is \emph{singular} if
 and it contains exactly one point.
\begin{definition} \cite{Furia2006}
  A \emph{dense-time behavior} over an infinite-time domain
   and a set  of atomic propositions is a
  function  which maps every
  time instant  to a set  of atomic
  propositions that hold at .
\end{definition}
Given a continuous sample path  of
the stochastic process , the timed behavior  of this
sample path is , for all .



\begin{definition}
  \cite{Furia2006} Let  be a dense-time behavior and
   be a positive real number, referred to
  as the \emph{sampling interval}. The \emph{canonical sampling}
   of the timed behavior  is
  defined such that 
\end{definition}







\subsection{Specifications}


We introduce metric interval temporal logic \cite{Alur1996}, a
subclass of MTL, to express system specifications.
\begin{definition}[Metric interval temporal logic]
Given a set  of atomic propositions, the formulas of \ac{mitl}
are built from  by Boolean connectives and time-constrained
versions of the \emph{until} operator  as follows.

where ,  is a \emph{nonsingular} time interval with
integer end-points, and ,  are unconditional true and
false, respectively.
\end{definition}



\paragraph*{Dense-time semantics of \ac{mitl}} Given a timed behavior
, we define  with respect to an \ac{mitl}
formula  at time  inductively as follows:
\begin{itemize}
\item  where  if and only if ;
\item  where ;
\item  if and only if  and ;
\item  if and only if there
  exists  such that  and for all
  , ;
\end{itemize}
We write  if . We also define
temporal operator 
(eventually,  will hold within interval  from now) and
 (for all
points within ,  holds.)


\subsection{Timed automata}
An \ac{mitl} formula can be translated into equivalent
non-deterministic timed automaton \cite{Alur1996}. We consider a
fragment of \ac{mitl} which can be translated into equivalent
\emph{deterministic} timed automaton.




Let  be a finite alphabet.  are
the sets of finite and infinite words (sequences of symbols) over
. A \emph{(infinite) timed word} \cite{Alur1994183} over
 is a pair , where
 is an infinite word
and  is an infinite \emph{timed sequence},
which satisfies \begin{inparaenum}[1)]
\item \emph{Initialization}: ;
\item \emph{Monotonicity}:  increases strictly monotonically;
  i.e., , for all ;
\item \emph{Progress}: For every  and
  , there exists some , such that
  .
\end{inparaenum}
The conditions ensure that there are finitely many symbols (events) in
a bounded time interval, known as \emph{non-Zenoness}.  We also write
.








Before the introduction of timed automata, we introduce \emph{clock}
and \emph{clock} constraints: Let  be a finite set of clocks,
. We define a set  of clock
constraints over  in the following manner. Let  be a
non-negative integer, and  be
a comparison operator,

where  are clocks.
\begin{definition}\cite{Alur1994183}
  A \emph{deterministic timed automaton} is a tuple
   where  is a
  finite set of states,  is a finite set of alphabet with
  the set  of atomic propositions,  is the initial
  state,  is a finite set of accepting states,  is a finite set
  of clocks. The transition function
   is
  deterministic and interpreted as follows: If
   then  allows a transition from
   to  when the set  of atomic propositions
  evaluate true and the clock constraint  is
  met. After taking this transition, the clocks in  are
  reset to zero, while other clocks remain unchanged.
\end{definition} 







For each clock , we denote  the range of that
clock.  For notational convenience, we define a \emph{clock vector}
 where the -th entry  of the clock vector 
is the value of clock , for . Given
, let
.  We use  for the
clock vector  where  for all  and
 the set of all possible clock
vectors in . Note that a clock vector is essentially a
\emph{clock valuation} defined in \cite{Alur1994183}.

A \emph{configuration} of  is a pair  where  is a
state and  is a clock vector. A transition
 being taken from the configuration 
after  time units is also written as
 where
, and  if
, otherwise .


A \emph{run} in  on a timed word
 is an infinite alternating
sequence of configurations and delayed transitions
,
with  and 
for , subject to the following conditions:
\begin{enumerate}

\item There exists  and  such
  that ,
   and  for
  all  and  for all .
\item For each , there exist  and
   such that 
  satisfies the clock constraint ,
   is defined and
   for all 
  and  for all .
\end{enumerate} 


We consider reachability objectives: A run  on a timed word 
is \emph{accepting} if and only if 
where  is the set of states in  occurring in
. The set of timed words on which runs are accepted by 
is called the language of , denoted .

\begin{example}
As a simple example of timed automata, let  and 
the specification formula is . The
reachability specification can be expressed with a deterministic timed
automaton  in Figure~\ref{fig:reach}. The set of final
states is .  The timed automaton accepts a timed word with
a prefix , i.e.,
 since

and  is accepting. It does not accept
,
 for an arbitrary timed sequence
, or  because either 
is evaluated true when  or , or it is never true over an
infinite timed sequence.
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{timedautomaton}
\caption{Timed automaton  for
  . Only one clock  is used. A
  transition labeled  is taken if and only if both
   and  evaluate true. A transition labeled
   is taken if and only if both  and
   evaluate true and along with taking the transition, the
  clock  is reset to .}
\label{fig:reach}
\end{figure}
\end{example}














\subsection{Problem formulation}
Given a sampling interval  and a timed behavior
, we map the canonical sampling
 of  to a timed word
 such
that for any , . We say that the timed
behavior  satisfies the formula  in the \emph{point-based
  semantics} under the sampling interval , denoted
, if and only if  is
accepted in the timed automaton  that expresses
. The sampling interval 
determines a sequence of positions (time instances)  in the timed behavior. With  being a positive infinitesimal, any position in a timed behavior

appears in the timed sequence of the timed word . Thus, we say that the timed behavior

satisfies 
in the \emph{continuous or dense-time semantics}, i.e., , if and only if . A formal definition of satisifiability of MTL
formulas over dense-time and point-based semantics is given in
\cite{Bouyer2009} and the relation between these two semantics has
been studied in \cite{Furia2006}.










We say that a sample path of the \ac{sde} in \eqref{sde} satisfies an
\ac{mitl} formula  in the dense-time semantics
(resp. point-based semantics under the sampling interval ) if
its timed behavior satisfies  in the dense-time semantics
(resp. point-based semantics under the sampling interval
). Formally, let  where  be a sample
path of the stochastic process . We have that
 is
equivalent to .







Given a stochastic process  and an admissible control law
 that solve the \ac{sde} in \eqref{sde}, the probability of
satisfying a formula  in the system under the control law
 is the sum of probabilities of continuous sample paths of
 that satisfy the formula  in the dense-time or
point-based semantics (with respect to a given sampling interval).

\begin{problem}
\label{problem}
Given an \ac{sde} in \eqref{sde} and a timed automaton

expressing an \ac{mitl} formula , compute a control input
 that maximizes the probability of satisfying  in
the dense-time semantics.
\end{problem}




\section{Main result}
In this section, we first show that for the \ac{sde} in \eqref{sde},
Problem~\ref{problem} can be formulated as a stochastic optimal
control problem in a system derived from the \ac{sde} with an
augmented state space for capturing relevant properties with respect
to its \ac{mitl} specification. Then, we introduce a numerical scheme
that computes an optimal policy in a discrete-approximation of the
\ac{sde} in \eqref{sde} with respect to the probability of satisfying
the specification in the point-based semantics. The numerical scheme
is based on the so-called Markov chain approximation method
\cite{kushner2001numerical}.  We prove that such a policy converges to
a solution to Problem~\ref{problem} as the discretization gets finer.


We make two assumptions.
\begin{assumption}
\label{assumption0}
  The state space  and the clock vector space  are bounded.
\end{assumption}
This condition ensures a finite number of states in the discrete
approximation. In certain cases, we might also require  to be
bounded in order to approximate the input space with a finite set.


\begin{assumption}\label{assumption1}
   and  are bounded, continuous, and Lipschitz
  continuous in state , while  is uniformly so in .
\end{assumption}
Assumption~\ref{assumption1} ensures that the \ac{sde} in \eqref{sde}
has a unique solution for a given controller .





\subsection{Characterizing the reachability probability}
For reachability objectives in \ac{mitl}, Problem~\ref{problem} is
also referred to as the \emph{probabilistic reachability problem}.













A state in  is called a \emph{product state},
following from the fact that it is a state in a product construction
between the stochastic process for the controlled stochastic system
and the timed automaton expressing the specification. We define a
projection  such that for a given tuple ,  is the
-th element in the tuple. The projection  is extended to
sequences of tuples in the usual way:
 where  is a tuple and  is
a sequence of tuples.

Let . For a stochastic process
, we derive a \emph{product stochastic process}
 where  is a random
variable describing the product state. The process  satisfies the following conditions.
\begin{itemize}
\item  where  and
  .
\item For any time , let
  .
  If , then let
  ,  for
   and  for . Moreover,
  for all ,
  .
\end{itemize}

Alternatively, given a sample path ,
, suppose that at time  the configuration in
 is , the labeling 
and the clock vector  trigger a transition precisely
at time  and between the interval ,
no transition is triggered. Then, the configuration in 
changes from  to  also at time  provided
that .
Moreover, for any time  during the time interval
, the state in the specification automaton
remains to be  and each clock increases by  as the time passes.

For a measurable function  that maps sample paths in the process
 into reals, we write  for the expected value of
 when the initial state is .


The following lemma is an immediate consequence of the derivation
procedure for the product stochastic process.
\begin{lemma}
\label{lma:relate}
  Given a set , let  denote
  the probability of a sample path in the stochastic process
   starting from  and satisfying 
  in the dense-time semantics and  is the probability of
  reaching the set  in the derived product stochastic process
   with . It holds that
  
\end{lemma}
By Lemma~\ref{lma:relate}, we can define a value function in the product
stochastic process to characterize the probability of satisfying
 in the dense-time semantics.
\paragraph*{\bf Dense-time reachability probability}
The probability of
reaching  from a product state  under a controller
 is denoted as .
We construct a reward function  such
that  where  is the indicator function, i.e.,
 if , and  otherwise.
Then,  is evaluated by the value function

where  is a random variable describing the stopping time such that
.  





The optimal value function is defined as
, where  is the set of all
admissible control policies for the \ac{sde} in \eqref{sde}.


So far, we have shown that given  that solve the
\ac{sde} in \eqref{sde}, the probability that a sample path in the
stochastic process  satisfies the \ac{mitl} formula
 in the dense-time semantics can be represented by the value
 in the derived product stochastic process  under
the reward function .

\subsection{Markov chain approximation}
In this section, we employ the methods in \cite{kushner2001numerical}
to compute locally consistent Markov chains that approximate the SDE
in \eqref{sde} under a given control policy.





































Given an approximating parameter , referred to as the \emph{spatial
  step}, we obtain a discretization of the bounded state space,
denoted by , which is a finite set of discrete points
approximating . Intuitively, the spatial step  characterizes the
distance between neighboring and introduces a partition of . The
set of points in the same set of the partition is called an
\emph{equivalent class}.  For each , the set of points in
the same equivalent class of  is denoted
.  We call  the
representative point of . 




We define an \ac{mdp}  where
 is the discrete state space.  is the input space, which can
be infinite.   is the
transition probability function (defined later in this section).  The
initial state is  such that the initial state  of
the \ac{sde} in \eqref{sde} satisfies .

\begin{definition} \cite{kushner2001numerical} Let  be
  the interpolation interval at step  for . Let 
  and  for  be
  interpolation times.  The \emph{continuous interpolations}
   of the stochastic processes
   and  under the
  interpolation times  are
  
\end{definition}

Given a policy , let  be the
induced Markov chain from  by such a policy.  It is shown that if
a certain condition is satisfied by the spatial step and the
interpolation times, the continuous interpolations of
 and  converges to processes
 and  which solve the \ac{sde} in \eqref{sde}.




\begin{theorem}
\label{thm:stateconverge}
\cite{kushner2001numerical} Suppose Assumption \ref{assumption1}
holds.  For any policy , let the chain induced from
 by this policy be .  Let 
denote the conditional expectation given
. Then, for all
 and , the chain  satisfies the
\emph{local consistency condition}:
  
where  is the difference and
 is an appropriate interpolation interval for
 and .  As , the continuous
interpolations  of 
and  under the interpolation times
 computed from the interpolation intervals
, , converge in distribution to
 which solve the \ac{sde} in
\eqref{sde}.\end{theorem}





Given a spatial step , under the local consistency condition we
construct the \ac{mdp}  over the discrete state space  by
computing the transition probability function  from the
parameters of the \ac{sde} (see \cite{kushner2001numerical} for the
details). If the diffusion matrix  is diagonal, then the
transition probabilities are:

and

where  is the unit vector in the -th direction and
.





\subsection{Optimal planning with the discrete approximation}
In this section, we construct a product \ac{mdp} from a discrete
approximation of the original system and the timed automaton
expressing the system specification.  Then, an optimal planning
problem is formulated in a product \ac{mdp} for computing a
near-optimal policy for the \ac{sde} in \eqref{sde} with respect to
the probability of satisfying the \ac{mitl} specification in the
point-based semantics.  






Given the timing constraints in \ac{mitl}, we consider an explicit
approximation method that discretizes both the continuous state space
and time. Particularly, instead of computing potentially varying
interpolation intervals, we choose a constant interpolation interval
, referred to as the \emph{time step}. For the local
consistency condition to hold, it is required that for a given
, 

Furthermore,  is used as the parameter to discretize the clock
vector space . Let

be the discretized space for the range  of clock .
The discretized clock vector space is
.  Since both 
and  are bounded, sets  and  are both
finite.  The method is ``explicit'' given the fact that the advance of
clock values are explicit: At each step , if the clock is not reset
to , then its value is increased by the interpolation interval
. 

 


Let  denote a tuple of spatial and time steps. Next, we
construct a product \ac{mdp}

where  is the discrete product
state space,  is the input space,
 is the transition
probability function, defined as follows. Let  and
. For any ,  if and
only if . Otherwise
. The initial state is  with
.

\begin{assumption}
\label{assume:equivalent}
There exists a spatial step  and a choice of
representative points from  such that for all  and all
, . 
\end{assumption}
\begin{lemma}
  \label{lma1}Under Assumption~\ref{assume:equivalent}, given
   that solve the SDE in \eqref{sde} and a
  discretization  of the state space, we construct a discrete
  chain  as follows:
   with  and
  ; for all
  ,  where  is the
  representative point to which  belongs, i.e.,
  , and
  .
  The following two statements hold.
\begin{enumerate}
\item For all , the range of the random variable  is
  .
\item The probability of a continuous sample path in
   satisfying  in the point-based semantics
  under the sampling interval  equals the probability of a
  discrete sample path in the chain  hitting the
  set . That is,

\end{enumerate}
\end{lemma}

\begin{proof}
  To show the first
  statement, initially,  is a vector of zeros, which is in
  . Suppose that at the -th sampling step
  , at the next sampling step, for any clock
  , either the value of  is increased by  or
  it is reset to  depending on the current state in the automaton,
  the clock vector and the current labeling of state in .  If the
  value of  is reset to ,
  . Otherwise,
  .  By
  induction, all possible clock vectors we can encounter at the
  sampling times are in the set . Thus, the range of a
  random variable  for any  is , which is a
  subset of .


  Let  with  be a sample path of
  the process  and  be the
  corresponding sample path of the chain  given
  the construction method above. Remind that 
  satisfies  in the point-based semantics under the sampling
  interval  if and only if the timed word ,
  where , is accepted in .
  Since  for all , let
  , we have that the timed word
   by
  Assumption~\ref{assume:equivalent} . Let the run on the timed word
   be  such that
  .
  By construction, it holds that  and
   for all . By definition of the
  acceptance condition in ,
   is accepted in  if and
  only if , which is equivalent to say
  that for some ,  and
  for all , . Since in the
  first statement we have shown that the range of  for all
   is ,
   and the proof for the
  second statement is complete.
\end{proof}


Lemma~\ref{lma1} characterizes the probability of satisfying the
specification in point-based semantics under the sampling interval
 with the probability of reaching a set  in the product
\ac{mdp} . Given the objective of maximizing the probability of
reaching a set in an \ac{mdp}, there exists a memoryless
and deterministic policy such that by following this policy, from any
state, the probability of reaching the set is maximized
\cite{Baier2008}.

We introduce a state  into the product \ac{mdp}  and
modify  such that for all  and all ,
, and for all ,
, while the other transition probabilities
remain unchanged. The product \ac{mdp}  with the augmented
state set and the modified transition probability function is denoted
.  The reward function
 is defined by
.  Let  be a
memoryless and deterministic policy in  and  the
set of all such policies in .  The value function of
policy  is

where  is the Markov chain induced from
 with policy .  Thus, the optimal value function
, and the dynamic programming
equation is obtained: For ,




Given the optimal policy
 that achieves the
maximum value of  for all  in the modified product
\ac{mdp} , we derive a policy 
by letting . By the definition of reward
function and the modified product \ac{mdp}, policy  maximizes
the probability of hitting the set  in .

A policy  is implemented in the original system in
\eqref{sde} in the following manner. The initial product state is
 with
. At each sampling
time , let the current product state be
. We compute  such that
. Note that at the sampling time the clock vector is
always in  by Lemma~\ref{lma1} and thus
, for which  is defined. Then, we apply a
constant input  during the time interval
. At the next sampling time ,
according to the current state , we compute the state in
 and the clock vector such that
.  Hence, the new product
state is  and a constant control input for the interval
 is obtained in the way we just described.




\begin{remark}
  When the input space  for the product \ac{mdp} is bounded, in the
  numerical method for the reward maximization problem in the product
  \ac{mdp}, in general we also discretize the input space  with
  some discretization parameter . Let  be the
  discretized input space. Given the optimal policy  for the
  product \ac{mdp} and the optimal policy  in the
  product \ac{mdp} with the input space , one can derive
  the bound on 
  as a function of , which converges to  as
  
  \cite{fleming2006controlled,kushner2001numerical}.  Thus, with both
  discretized state and input space, the implemented policy is
  near-optimal for the SDE in \eqref{sde} with respect to the
  probability of satisfying the \ac{mitl} specification in the
  point-based semantics.
\end{remark}

 \subsection{Proof of convergence}
Based on Theorem~\ref{thm:stateconverge}, we show that the optimal
 policy synthesized in the product \ac{mdp} converges to the
 optimal policy that achieves the maximal probability of satisfying
 the \ac{mitl} specification in the dense-time semantics as the
 discretization in both state space and time space get finer.
\begin{theorem}
  Given a discretization parameter  where 
  satisfies the local consistency condition in
  \eqref{eq:constraintdelta} with respect to the spatial step , it
  holds that
  
where ,  and
.
\end{theorem}

\begin{proof}
  First, it is noted by the local consistency condition and the
  constraint on  in \eqref{eq:constraintdelta},  is a
  decreasing function of  and when ,
  .
 

  For a given  where  satisfies the constraint
  in \eqref{eq:constraintdelta} with respect to , let
   be a policy in the product \ac{mdp}
   and  be the induced Markov chain.
  According to Theorem~\ref{thm:stateconverge}, when
  ,  and the continuous
  interpolations of  and
   converge in distribution to
   and  that solve the SDE in \eqref{sde}. By the
  determinism in the transition function of the timed automaton and
  the labeling function, as , 
  also converges in distribution to , which is the
  product stochastic process derived from .
  According to the definition of reward functions  and , since
  , we have ,
   and 
  converges to .


  Now given the optimal control policy 
  obtained for the product \ac{mdp} , let
   be the Markov chain induced by  in
  . We have that
  
  by the optimality of the value function .  On the other
  hand, let  be the optimal
  control policy in the continuous-time stochastic system.  We
  construct a policy  for the
  product \ac{mdp}  such that the action 
  is taken at the step .  We have
   by the
  optimality of .  Since
  ,
  it is inferred that
  . Therefore,
  .
\end{proof}
\section{Example}

\begin{figure*}[!t]
\centering
    \begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{3dview}
\caption{3D-view}
\label{fig:3d-view}
\end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{xy-view}
\caption{x-y view}
\label{fig:xy-view}
\end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{xt-view}
\caption{x-t view}
\label{fig:xt-view}
\end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{yt-view}
\caption{y-t view}
\label{fig:yt-view}
\end{subfigure}
\vspace{-2ex}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{valuefunction}
\caption{Value function}
\label{fig:valuefunction}
\end{subfigure}
\label{fig:simulation}
\caption{(a) -- (d) Total 20 sample paths for the robot starting at
   with 3D view, x-y plane view, x-t plane view and
  y-t plane view. The sample path stops whenever the specification is
  satisfied, or it does not meet the specification due to time
  constraints or hitting the walls.  Most of the sample paths that
  fail to satisfy the specification in the point-based semantics reach
  the region  prior to the rd time units after visiting
  . (e) The value function for the robot with the initial state
  , the initial state in the specification
  timed automaton and the initial clock vector .}
\end{figure*}
This section illustrates the method using a motion planning example
for a robot modeled as a stochastic Dubin's car. The dynamics of the
system are described by the \ac{sde}
 
where  is the coordinate and heading angle of
the robot,  is the linear velocity and  is the
angular velocity input. In this example,  is fixed and
, and  is a 3-dimensional Wiener
process on the probability space .

The workspace of the robot is depicted in Figure~\ref{fig:xy-view},
with two regions  and  of importance. The workspace is
constrained by the walls
.

The objective of the robot is to maximize the probability of visiting
region  within the first  time units and after visiting ,
reaching  between the rd and th time units, while avoiding
hitting the walls. We define atomic propositions , ,
which evaluates true when the robot is in region . An atomic
proposition  evaluates true if the robot hits the surrounding
walls.  The \ac{mitl} formula describing the specification is
.
Given an initial state , we want to find an optimal policy
that maximizes the probability of  being satisfied. We select
a spatial step  to obtain a uniform
discretization of the state space . Given the choice of , the
time step  is chosen to be  time units for the local
consistency condition to hold for all state and control input
pairs. The number of states in the \ac{mdp}  is  and the
number of product states in the modified product \ac{mdp}
 is  (after trimming unreachable states). Remind
that the value iteration is polynomial in the size of the \ac{mdp}
.  The implementation are in
MATLAB\textsuperscript{\textregistered} on a desktop with Intel(R)
Core(TM) processor and 16 GB of memory. The computation of the product
\ac{mdp} takes  minutes and the value iteration converges after
 iterations with a pre-specified error tolerance of . Each
iteration takes about  minutes. In the value iteration we also
approximate the input space  with a finite set  where
 is the discretization parameter for the input space.




Since the product state space of the example is -dimensional, we
select to plot the optimal value  for the states with the initial
heading angle , the initial state of the timed automaton
and initial clock vector  in
Figure~\ref{fig:valuefunction}. Figures~\ref{fig:3d-view},
\ref{fig:xy-view}, \ref{fig:xt-view}, and \ref{fig:yt-view} show the
sample paths starting from  for a time interval
 from different perspectives. The optimal value  with
 is , which is the approximately
maximal probability for satisfying  in the point-based
semantics under the sampling interval  in the system with initial
state . In simulation, there are  paths
(marked in blue) out of  sample paths that satisfy the
specification in the point-based semantics. 





The drawback of the explicit approach is scalability. In order to
compute a control policy with a finer approximation, we need to reduce
the spatial step  as well as the time step  for the local
consistency condition to hold. The product state space becomes very
large for a fine discretization. For example, if  is chosen to be
,  has to be chosen below  time units
and for the simple example, the product \ac{mdp} has  states
after trimming. We did not carry out the computation for  given
this finer discretization since it is very time consuming. We discuss
the limitation and possible solutions to deal with the issue of
scalability in Section~\ref{sec:conclude}. 

\section{Conclusions and future work}
\label{sec:conclude}
This paper proposes a numerical method based on the Markov chain
approximation method for stochastic optimal control with respect to a
subclass of quantitive metric temporal logic specifications. We show
that as the discretization gets finer, the optimal control policy in
the discrete abstract system with respect to satisfying the \ac{mitl}
specification in the point-based semantics converges to the optimal
policy in the original system with respect to the dense-time semantics
for satisfying the \ac{mitl} formula. The approach can be easily
extended to bounded-time MTL formulas including signal temporal logic
formulas. In the future work, we aim to investigate the error bounds
introduced by the proposed discrete approximation method. On the other
hand, since scalability is a critical issue in the explicit
approximation method, we will also investigate a solution approach
based on implicit approximation \cite{kushner2001numerical}. With
implicit approximation method, we can potentially reduce the size of
discrete abstract system by treating the clock vector as a state
variable, whose discretization parameters are pre-defined and
potentially different from the interpolation interval. Parallel
algorithms and distributed planning for large-scale \ac{mdp}s are also
considered to handle the issue of scalability.









\bibliographystyle{ieeetran} 
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{pnueli1992temporal}
Z.~Manna and A.~Pnueli, \emph{The Temporal Logic of Reactive and Concurrent
  Systems: Specifications}.\hskip 1em plus 0.5em minus 0.4em\relax Springer
  Science \& Business Media, 1992, vol.~1.

\bibitem{koymans1990specifying}
R.~Koymans, ``\BIBforeignlanguage{English}{Specifying real-time properties with
  metric temporal logic},'' \emph{\BIBforeignlanguage{English}{Real-Time
  Systems}}, vol.~2, no.~4, pp. 255--299, 1990.

\bibitem{fainekos2009temporal}
G.~E. Fainekos, A.~Girard, H.~Kress-Gazit, and G.~J. Pappas, ``Temporal logic
  motion planning for dynamic robots,'' \emph{Automatica}, vol.~45, no.~2, pp.
  343--352, 2009.

\bibitem{wongpiromsarn2012receding}
T.~Wongpiromsarn, U.~Topcu, and R.~M. Murray, ``Receding horizon temporal logic
  planning,'' \emph{IEEE Transactions on Automatic Control}, vol.~57, no.~11,
  pp. 2817--2830, 2012.

\bibitem{abate2010approximate}
A.~Abate, J.-P. Katoen, J.~Lygeros, and M.~Prandini, ``Approximate model
  checking of stochastic hybrid systems,'' \emph{European Journal of Control},
  vol.~16, no.~6, pp. 624--641, 2010.

\bibitem{abate2011quantitative}
A.~Abate, J.-P. Katoen, and A.~Mereacre, ``Quantitative automata model checking
  of autonomous stochastic hybrid systems,'' in \emph{{ACM international
  conference on Hybrid Systems: Computation and Control}}, 2011, pp. 83--92.

\bibitem{lahijanian2009probabilistic}
M.~Lahijanian, S.~B. Andersson, and C.~Belta, ``A probabilistic approach for
  control of a stochastic system from {LTL} specifications,'' in \emph{IEEE
  Conference on Decision and Control}, 2009, pp. 2236--2241.

\bibitem{Maria2015}
M.~Svorenova, J.~Kretinsky, M.~Chmelik, K.~Chatterjee, I.~Cerna, and C.~Belta,
  ``Temporal logic control for stochastic linear systems using abstraction
  refinement of probabilistic games,'' in \emph{ACM international conference on
  Hybrid Systems: Computation and Control}, 2015, to appear.

\bibitem{Abbas2014}
H.~Abbas, B.~Hoxha, G.~Fainekos, and K.~Ueda, ``Robustness-guided temporal
  logic testing and verification for {Stochastic Cyber-Physical Systems},'' in
  \emph{IEEE Annual International Conference on Cyber Technology in Automation,
  Control, and Intelligent Systems}, 2014, pp. 1--6.

\bibitem{Karaman2008}
S.~Karaman and E.~Frazzoli, ``{Vehicle routing problem with metric temporal
  logic specifications},'' in \emph{IEEE Conference on Decision and Control},
  2008, pp. 3953--3958.

\bibitem{JunLiu2014}
J.~Liu and P.~Prabhakar, ``Switching control of dynamical systems from metric
  temporal logic specifications,'' in \emph{IEEE International Conference on
  Robotics and Automation}, 2014, pp. 5333--5338.

\bibitem{VasuHSCC}
V.~Raman, A.~Donze, D.~Sadigh, R.~Murray, and S.~A. Seshia, ``Reactive
  synthesis from signal temporal logic specifications,'' in \emph{ACM
  international conference on Hybrid Systems: Computation and Control}, 2015,
  to appear.

\bibitem{kushner2001numerical}
H.~J. Kushner and P.~Dupuis, \emph{Numerical Methods for Stochastic Control
  Problems in Continuous Time}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2001, vol.~24.

\bibitem{henzinger1991temporal}
T.~A. Henzinger, ``The temporal specification and verification of real-time
  systems,'' Ph.D. dissertation, Citeseer, 1991.

\bibitem{Alur1996}
\BIBentryALTinterwordspacing
R.~Alur, T.~Feder, and T.~A. Henzinger, ``The benefits of relaxing
  punctuality,'' \emph{Journal of the ACM}, vol.~43, no.~1, pp. 116--146, Jan.
  1996. [Online]. Available: \url{http://doi.acm.org/10.1145/227595.227602}
\BIBentrySTDinterwordspacing

\bibitem{Furia2006}
C.~A. Furia and M.~Rossi, ``A theory of sampling for continuous-time metric
  temporal logic,'' \emph{ACM Transactions on Computational Logic}, vol.~12,
  no.~1, p.~8, 2010.

\bibitem{Alur1994183}
R.~Alur and D.~L. Dill, ``A theory of timed automata,'' \emph{Theoretical
  Computer Science}, vol. 126, no.~2, pp. 183 -- 235, 1994.

\bibitem{Bouyer2009}
P.~Bouyer, ``{Model-checking timed temporal logics},'' \emph{Electronic Notes
  in Theoretical Computer Science}, vol. 231, pp. 323--341, 2009.

\bibitem{Baier2008}
C.~Baier and J.-P. Katoen, \emph{Principles of Model Checking (Representation
  and Mind Series)}.\hskip 1em plus 0.5em minus 0.4em\relax The {MIT} Press,
  2008.

\bibitem{fleming2006controlled}
W.~H. Fleming and H.~M. Soner, \emph{Controlled Markov Processes and Viscosity
  Solutions}.\hskip 1em plus 0.5em minus 0.4em\relax Springer Science \&
  Business Media, 2006, vol.~25.

\end{thebibliography}


\end{document}