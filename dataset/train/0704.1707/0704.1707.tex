\documentclass{llncs}
\usepackage{url}
\usepackage{bussproofs}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\pagestyle{plain}



\numberwithin{equation}{section}

\newcommand{\pair}[2]{\langle #1, #2\rangle}

\newcommand{\Pred}{{\mathcal{P}}}
\newcommand{\Succ}{{\mathcal{S}}}

\newcommand{\Lg}[1]{\mathtt{#1}}

\newcommand{\ExAnd}{\wedge}
\newcommand{\ExOr}{\vee}
\newcommand{\ExNot}{\neg}
\newcommand{\ExTop}{\top}
\newcommand{\ExBot}{\bot}
\newcommand{\ExImp}{\rightarrow}

\newcommand{\Fml}{\mathit{Fml}}
\newcommand{\Atoms}{\mathit{Atoms}}



\newcommand{\sequent}{\vdash}




\newcommand{\urule}[3]{
                                                                                        \AxiomC{#2}
                \LeftLabel{}        \UnaryInfC{#3}  
        \DisplayProof
}

\newcommand{\uruleSideCond}[4]{
        #1
}

\newcommand{\brule}[4]{
                                                                                        \AxiomC{#2}
                                                                                        \AxiomC{#3}
                \LeftLabel{}        \BinaryInfC{#4} 
        \DisplayProof
}

\newcommand{\bruleEx}[4]{
                                                                                        \AxiomC{#2}
                                                                                        \AxiomC{#3}
                \dashedLine \LeftLabel{}    \BinaryInfC{#4} 
        \DisplayProof
}


\newcommand{\bruleSideCond}[5]{
        #1
}

\newcommand{\bruleSideCondEx}[5]{
        #1
}


\newcommand{\Imp}{\rightarrow}
\newcommand{\Force}{\vDash}
\newcommand{\WeakImpD}{\hspace{0.1cm}-\hspace{-0.2cm}<}
\newcommand{\WeakImpDOne}{-\hspace{-0.05cm}<}
\newcommand{\WeakImp}{
\begin{picture}(10,10)
     \put(1,0){}
     \put(7,0){}
   \end{picture}
   \;\; 
}
\newcommand{\WeakNot}{\sim \hspace{-0.1cm}}
\newcommand{\NotForce}{\nvDash}
\def\Reject{=\joinrel\mathrel|}
\newcommand{\Or}{\vee}
\newcommand{\MetaImp}{\Rightarrow}
\newcommand{\MetaBiImp}{\Leftrightarrow}

\newcommand{\LabWhtBox}[1]{[#1]}
\newcommand{\LabWhtDia}[1]{\langle#1\rangle}

\newcommand{\FBox}{\LabWhtBox{F}}
\newcommand{\FDia}{\LabWhtDia{F}}
\newcommand{\PBox}{\LabWhtBox{P}}
\newcommand{\PDia}{\LabWhtDia{P}}


\newcommand{\K}{(\mathrm{K)}}
\newcommand{\Bottom}{\perp}
\newcommand{\Top}{\top}

\newcommand{\stacked}[2]{
\genfrac{}{}{0pt}{}{#1}{#2} 
 }

\newcommand{\mycal}[1]{
        {\cal{#1}}
}

\newcommand{\stackedLeft}[2]{
\begin{array}{l} #1 \\ #2 \end{array}
}



\newcommand{\SequentKK}[9] {
\left. \left. 
                \stacked{#1}{#2}
                \stacked{#3}{#4}
                \stacked{#5}{#6}
          \left( \stacked{#7}{#8} \right) \right|\right|
        \sequent #9
}

\newcommand{\SequentK}[7]{
        \left. \left. \begin{array}{l} \hspace{-5pt} \stacked{#1}{#2} \stacked{#3}{#4} \stacked{#5}{#6} \hspace{-5pt}\end{array} \right|\right| \sequent #7
}

\newcommand{\Sequent}[4]{
        \left. \left. \begin{array}{l} \hspace{-5pt} \stacked{#1}{#2}  \hspace{-5pt}\end{array} \ \right|\right| #3 \sequent #4
}

\newcommand{\SequentSimple}[2]{
        #1 \sequent #2
}

\newcommand{\SequentAny}{
        \Sequent{\Succ}{\Pred}{\Gamma}{\Delta}
}

\newcommand{\SequentAnyNamed}{
        \alpha = \SequentAny
}

\newcommand{\SequentAnyNamedParam}[1]{
        #1 = \SequentAny
}

\newcommand{\SequentEmptyShort}[2]{
\left. \left. \begin{array}{l} \hspace{-5pt} \epsilon \hspace{-5pt}\end{array} \right|\right| #1 \sequent #2
}

\newcommand{\SequentEmpty}[2]{
        \Sequent{\epsilon}{\epsilon}{#1}{#2}
}

\newcommand{\SideCond}[1]{
        \scriptstyle{#1}
}

\newcommand{\Model}{
        \mycal{M}=\langle \mycal{W}, \mycal{R}, \vartheta \rangle
}

\newcommand {\BiInt}{
        \Lg{BiInt}
}

\newcommand {\GHPC}{
        \Lg{GHPC}
}

\newcommand {\Int}{
        \Lg{Int}
}

\newcommand {\DualInt}{
        \Lg{DualInt}
}

\newcommand {\SFour}{
        \Lg{S4}
}

\newcommand {\SFive}{
        \Lg{S5}
}

\newcommand {\KtSFour}{
        \Lg{Kt.S4}
}

\newcommand {\KK}{
        \Lg{K}
}

\newcommand{\GBiInt}{\mathbf{GBiInt}}

\newcommand{\GKtSFour}{\mathbf{GKt.S4}}

\newcommand{\GKtTwo}{\mathbf{GKt2}}

\newcommand{\tree}[1]{
        {\mycal{#1}}
}


\newcommand{\true}{\mbox{true}}
\newcommand{\false}{\mbox{false}}

\newcommand {\myspecial}{
        SPECIAL
}



\newcommand{\IdRule}{(Id)}
\newcommand{\FalseLeftRule}{(\Bottom_L)}
\newcommand{\TrueRightRule}{(\Top_R)}
\newcommand{\AndLeftRule}{(\ExAnd_L)}
\newcommand{\AndRightRule}{(\ExAnd_R)}
\newcommand{\OrRightRule}{(\ExOr_R)}
\newcommand{\OrLeftRule}{(\ExOr_L)}

\newcommand{\ImpRightRule}{({\Imp_R})}
\newcommand{\ImpRightRuleI}{({\Imp_R^I})}
\newcommand{\ImpRightRuleKeep}{({\Imp_R}Keep)}
\newcommand{\ImpRightRuleClear}{({\Imp_R}Clear)}
\newcommand{\ImpLeftAllRule}{(\Imp_{L})}
\newcommand{\ImpLeftExistingRule}{(\Imp_{L}Ex)}

\newcommand{\WeakImpLeftRule}{({\WeakImp_L})}
\newcommand{\WeakImpLeftRuleI}{({\WeakImp_L^I})}
\newcommand{\WeakImpLeftRuleD}{({\WeakImpD_L}\frac{Keep}{Clear})}
\newcommand{\WeakImpLeftRuleKeep}{({\WeakImp_L}Keep)}
\newcommand{\WeakImpLeftRuleClear}{({\WeakImp_L}Clear)}
\newcommand{\WeakImpRightAllRule}{(\WeakImp_{R})}
\newcommand{\WeakImpRightExistingRule}{(\WeakImp_{R}Ex)}
\newcommand{\WeakImpRightExistingRuleD}{(\WeakImpD_{R}Ex)}


\newcommand{\SpecialRightRule}{({\bigwedge_R})}
\newcommand{\SpecialLeftRule}{({\bigvee_L})}
\newcommand{\ReturnRule}{(Ret)}

\newcommand{\entails}{\hspace{-0.05cm}\Vdash_{_{\BiInt}}\hspace{-0.1cm}}

\newcommand{\combineVars}{
\combineVarsParam{\Omega}{\Theta}{\theta}
}

\newcommand{\combineVarsParam}[3]{
combine(#1, #2) := \left\{ \begin{array}{lll}
                                                                                                                                                                \{ #2   \}                      & \mbox{if}     &               #1 = \epsilon \\ 
                                                                                                                                                                \displaystyle{\bigcup_{#3_i \in #2} \{ \{ #2  \cup #3_i \} \}}          & \mbox{if}     &                       #1 \ne \epsilon \\
                                                                                                                                                                \end{array}\right.
}

\newcommand{\varsTrans}[5]{
#1 := 
\left\{ \begin{array}{ll}
        #2                                                                              & \mbox{if }    #5 = \epsilon \\                                                                                                                                                                
        #3                                                                              & \mbox{if}     \text{ right prem created} \\ 
        #4                                                                              & \mbox{otherwise} \\
        \end{array}\right.
}



\newcommand{\RuleDefReturn}
{
        \uruleSideCond{\ReturnRule}
        {}
        {}
        {\text{where no other rule is applicable}}
}


\newcommand{\RuleDefId}
{
        \urule{\IdRule}
        {}
        {}
}

\newcommand{\RuleDefFalseLeft}
{
        \urule{\FalseLeftRule}
        {}
        {}
}

\newcommand{\RuleDefTrueRight}
{
        \urule{\TrueRightRule}
        {}
        {}
}


\newcommand{\RuleDefImpLeftAll}
{
        \bruleSideCond{\ImpLeftAllRule}
        {}
        {}
        {}
        {}
}

\newcommand{\RuleDefExclRightAll}
{
        \bruleSideCond{\WeakImpRightAllRule}
        {}
        {}
        {}
        {}
}

\newcommand{\RuleDefImpRightI}
{
        \urule{\ImpRightRuleI}
        {}
        {}
}

\newcommand{\RuleDefExclLeftI}
{
        \urule{\WeakImpLeftRuleI}
        {}
        {}
}



\newcommand{\RuleDefImpRight}
{
        \bruleSideCondEx{\ImpRightRule}
        {}
        {}    
{}
{\text{\ \ \ \ \ \ \ \ \ right prem created only if } \Pred_1 \ne \epsilon \And \forall \Pi_i \in \Pred_1 . \Pi_i \not\subseteq \{ \Delta, \varphi \ExImp \psi \}}
}

\newcommand{\RuleDefSpecialRight}
{
        \urule{\SpecialRightRule}
        {}
        {}
}


\newcommand{\RuleDefExclLeft}
{
        \bruleSideCondEx{\WeakImpLeftRule}
        {}
        {}  
        {}
{\text{\ \ \ \ \ \ \ \ \ \ \ \ right prem created only if } \Succ_1 \ne \epsilon \And \forall \Sigma_i \in \Succ_1 . \Sigma_i \not\subseteq \{ \Gamma, \varphi \WeakImp \psi \}}        
}

\newcommand{\RuleDefSpecialLeft}
{
        \urule{\SpecialLeftRule}
        {}
        {}
}

\newcommand{\RuleDefAndRightBlocked}
{
        \brule{\AndRightRule}
        {}
        {}
        {}
}

\newcommand{\RuleDefAndLeftBlocked}
{
        \urule{\AndLeftRule}
        {}
        {}
}

\newcommand{\RuleDefOrRightBlocked}
{
        \urule{\OrRightRule}
        {}
        {}
}

\newcommand{\RuleDefOrLeftBlocked}
{
        \brule{\OrLeftRule}
        {}
        {}
        {}
}





\begin{document}
\title{A Cut-free Sequent Calculus for\\  Bi-Intuitionistic Logic: \\ Extended Version}
\author{Linda Buisman \and Rajeev Gor\'{e}}
\institute{
   The Australian National University\\
   Canberra ACT 0200, Australia
   \and
   Logic and Computation Programme\\
   Canberra Research Laboratory, NICTA\thanks{National ICT Australia is
    funded by the Australian Government's Dept of Communications,
    Information Technology and the Arts and the Australian Research
    Council through Backing Australia's Ability and the ICT Centre of
    Excellence program.}, Australia\\
   \email{\{Linda.Buisman|Rajeev.Gore\}@anu.edu.au}
}
\maketitle

\begin{abstract}
  Bi-intuitionistic logic is the extension of intuitionistic logic
  with a connective dual to implication. Bi-intuitionistic logic was
  introduced by Rauszer as a Hilbert calculus with algebraic and
  Kripke semantics. But her subsequent ``cut-free'' sequent calculus
  for  has recently been shown by Uustalu to fail
  cut-elimination.  We present a new cut-free sequent calculus for
  , and prove it sound and complete with respect to its Kripke
  semantics. Ensuring completeness is complicated by the interaction between
  implication and its dual, similarly to future and past modalities in
  tense logic. Our calculus handles this interaction using extended sequents
  which pass information from premises to conclusions using variables
  instantiated at the leaves of failed derivation trees.
  Our simple termination argument allows our calculus to be used for
  automated deduction, although this is not its main purpose.
\end{abstract}

\section{Introduction}

Propositional intuitionistic logic () has connectives , ,  and , with  often defined as .  has a well-known Kripke semantics, where a possible world  makes  true if every successor  that makes  true also makes  true.  also has an algebraic semantics in terms of Heyting algebras, and there is a well-known embedding from  into the classical modal logic .  is constructive in that it rejects the Law of Excluded Middle: that is,  is not a theorem of .

Propositional dual intuitionistic logic () has connectives , ,  and , with  often defined as .  also has Kripke semantics, where a possible world  makes  true if there exists a predecessor  where  holds, but  does not hold: that is,  \textit{excludes} . Thus, the  connective of  is dual to implication in .  also has algebraic semantics in terms of Brouwer algebras \cite{mckinsey1946}. There is a less well-known embedding from  into .  is para-consistent in that it rejects the Law of Non-contradiction: that is,  is -satisfiable. Various names have been used for : coimplication \cite{wolter98,uustalu2006a}, subtraction \cite{crolard2001,crolard2004}, pseudo-difference \cite{rauszer1980}, explication \cite{rauszer1974}. We refer to it as exclusion.

Bi-intuitionistic logic (), also known as subtractive logic and Heyting-Brouwer logic, is the union of  and , and it is a conservative extension of both.  was first studied by Rauszer \cite{rauszer1974,rauszer1980}.
 is an interesting logic to study, since it combines the constructive aspects of  with the para-consistency of .
While every -theorem is also a -theorem, adding  connectives introduces a non-constructive aspect to the logic -- the disjunction property does not hold for  formulae if they contain . Note that  differs from intuitionistic logic with constructive negation, also known as constructible falsity \cite{nelson1949}, where the disjunction property does hold.

While the proof theory of  and  separately has been studied extensively and there are many cut-free sequent systems for  (for example, \cite{gentzen1935,dyckhoff1992,dragalin1988}) and  (for example, \cite{urbas1996,czermak1977}), the case for  is less satisfactory. Although Rauszer presented a sequent calculus for  in \cite{rauszer1974} and ``proved'' it cut-free, Uustalu has recently given a counter-example \cite{uustalu2004} to her cut-elimination theorem: the formula  is -valid, but cannot be derived in Rauszer's calculus without the cut rule. Similarly, Uustalu's counterexample shows that Crolard's sequent calculus \cite{crolard2001} for  is not cut-free. Uustalu's counterexample fails in both Rauszer's and Crolard's calculi because they limit certain sequent rules to singleton succedents or antecedents in the conclusion, and the rules do not capture the interaction between implication and exclusion.

Uustalu and Pinto have also given a cut-free sequent-calculus for  in \cite{uustalu2006a}. Since only the abstract of this work has been published so far, we have not been able to examine their sequent rules, or verify their proofs. According to the abstract \cite{uustalu2006a} and personal communication with Uustalu \cite{uustalu2006}, his calculus uses labelled formulae, thereby utilising some semantic aspects, such as explicit worlds and accessibility, directly in the rules. Hence a traditional cut-free sequent calculus for  is still an open problem.

We present a new purely syntactic cut-free sequent calculus for
. We avoid
Rauszer's and Crolard's restrictions on the 
antecedents and succedents for certain rules by basing our rules on
Dragalin's  \cite{dragalin1988} which allows multiple formulae
on both sides of sequents. To maintain intuitionistic soundness, we
restrict 
the \emph{premise} of the implication-right
rule
to a singleton in the succedent. Dually, the
premise of our exclusion-left rule is restricted to a singleton in the
antecedent.
But using Dragalin's calculus and its dual does not give us
 completeness. We therefore follow Schwendimann
\cite{schwendimann98}, and use sequents which pass relevant
information from premises to conclusions using variables instantiated
at the leaves of failed derivation trees. We then recompute parts of our derivation trees using the new information, similarly to the restart technique of \cite{horrocks1998}. Our calculus thus uses a
purely syntactic addition to traditional sequents, rather than resorting to a semantic
mechanism such as labels. Our termination argument also relies on two
new rules from \'{S}vejdar \cite{svejdar2006}.

If we were interested only in decision procedures, we could obtain a decision procedure for  by embedding it
into the tense logic  \cite{wolter98}, and using tableaux for description logics with inverse roles 
\cite{horrocks1998}. However, an embedding into  provides no proof-theoretic insights into  itself. Moreover, the restart technique of Horrocks et al. \cite{horrocks1998} involves
non-deterministic expansion of disjunctions, which is complicated by
inverse roles. Their actual implementation avoids this non-determinism by keeping a global view of the whole counter-model under construction. In contrast, we handle this non-determinism by syntactically encoding it using variables and extended formulae, neither of which have a semantic content. Our purely syntactic approach is preferable for proof-theoretic reasons, since models are never explicitly involved in the proof system: see Remark~\ref{proofVsCountermodel}.

The rest of the paper is organized as follows. In Section~\ref{syntaxSemantics}, we define the syntax and semantics
of .  In Section~\ref{ourCalculus}, we introduce our sequent calculus  and give an example derivation of Uustalu's interaction formula. We prove the soundness and completeness of  in Sections~\ref{soundness}
and~\ref{completeness} respectively.  In Section \ref{sec:conclusion}, we outline further work.

\section{Syntax and Semantics of }\label{syntaxSemantics}

In this section we introduce the syntax and semantics of .

\begin{definition}[Syntax]
The formulae of  are defined as:

We refer to the set of atoms as , and we refer to the set of  formulae as .
\end{definition}

The connectives  and  are those of intuitionistic logic, and the connectives  and  are those of dual intuitionistic logic. The connectives  and  are from both.

\begin{definition}[Length]
The length of a  formula  is defined as:

\end{definition}

We use the language of classical first-order logic when reasoning about  at the meta-level.

\begin{definition}[Frame]
A  frame is a pair , where:
	\begin{enumerate}
		\item  is a non-empty set of worlds;
		\item  is the binary accessibility relation;
		\item  is reflexive, i.e.,  ;
		\item  is transitive, i.e.,  .
	\end{enumerate}
\end{definition}

\begin{definition}[Model]\label{model}
A  model is a triple , where:
	\begin{enumerate}
		\item  is a  frame;
		\item The truth valuation  is a function , which tells us the truth value of an atom at a world;
		\item The persistence property holds: \\
			;
		\item\label{top} ;
		\item\label{bottom} .
	\end{enumerate}
\end{definition}

\begin{definition}[Forcing of atoms]
Given a model , a world  and an atom , we write  if .
We pronounce  as ``forces'', and we pronounce  as ``rejects''.
\end{definition}

\begin{definition}[Forcing of formulae]\label{forcing}
Given a model , a world  and formulae , we write:

\end{definition}

From the semantics, it can be seen that the connectives  and  can be derived from  and  respectively. Therefore from now on we restrict our attention to the connectives , , ,  only.

\begin{lemma}
The persistence property also holds for formulae, that is:

\end{lemma}
\begin{proof}
By induction on the length of . 
\end{proof}

\begin{lemma}
The reverse persistence property holds:

\end{lemma}
\begin{proof}
Reverse persistence follows from persistence, because the truth valuation is binary. That is, suppose for a contradiction that 
Then  and  together with the persistence property give us , which contradicts .
\end{proof}

We write  to mean the empty set. Given two sets of formulae  and , we write  for . Given a set of formulae  and a formula , we write  for .

\begin{definition}
Given a model , a world  and sets of formulae  and , we write:

As a corollary, for any world , we vacuously have  and .
\end{definition}

\begin{definition}[Consequence]\label{consequence}
Given two sets  and  of formulae,  means:

We write  to mean that it is not the case that , that is:

Thus  means that  is falsifiable.
\end{definition}

We wish to prove  by failing to falsify . By Definition~\ref{consequence},  means that there exists a  model  that contains a world  such that  and . We therefore try to construct the model using a standard \textit{counter-model construction} approach: see \cite{gallier1986}. We shall start with an initial world  and assume that  and , and then systematically decompose the formulae in  and . The procedure will either:
	\begin{itemize}
		\item lead to a contradiction and therefore conclude that it cannot be the case that  and , therefore  holds, OR
		\item construct the counter-model successfully and therefore demonstrate that it is possible that  and , therefore  does not hold.
	\end{itemize}

\section{Our Sequent Calculus }\label{ourCalculus}

We now present , a Gentzen-style sequent calculus for . The sequents have a non-traditional component in the form of variables that are instantiated at the leaves of the derivation tree, and passed back to lower sequents from premises to conclusion. Note that the variables are not names for Kripke models and have no semantic content.

\subsection{Sequents}

First, we introduce an extended syntax that will help us in the presentation of some of our sequent rules.

\begin{definition}[Extended Syntax]\label{extendedSyntax}
The extended  formulae are defined as follows:
\begin{enumerate}
	\item If  is a  formula, then  is an extended  formula,
	\item If  and  are sets of sets of  formulae, then  and  are extended  formulae.
\end{enumerate}
If  and \\ , then from every extended  formula we can obtain a  formula as follows:

\end{definition}

From now on,  we implicitly treat extended  formulae as their  equivalents. The following semantics follows directly from Definition~\ref{extendedSyntax}:

\begin{definition}[Semantics of Extended Syntax]\label{extendedSemantics}
Given a  model , and a world , we write:

We can now extend the definition of forcing and rejecting to extended  formulae in the obvious way. If  and  are sets of extended  formulae viewed as their  equivalents, and  is an extended  formula viewed as its  equivalent, then:

\end{definition}

\begin{definition}[Sequent]
A  sequent is an expression of the form  and consists of the following components:
	\begin{description}
		\item[Left hand side (LHS):] , a set of extended  formulae;
		\item[Right hand side (RHS):] , a set of extended  formulae;
		\item[Variables:] , , each of which is a set of sets of formulae.
	\end{description}
\end{definition}

\noindent We shall sometimes use  to refer to sequents, ignoring the variable values for readability. We shall only do that in cases where the values of the variables are not important to the discussion. Note that the variables do not contain extended  formulae.

We now define the meaning of a sequent in terms of the counter-model under construction.

\begin{definition}[Falsifiability]\label{falsifiability}
A sequent  is falsifiable [at  in ] if and only if there exists a  model  and  such that  and .
\end{definition}

\begin{definition}[Variable conditions]
We say the variable conditions of a sequent  hold if and only if  is falsifiable at  in some model  and the following conditions hold:
		\begin{description}
			\item[-condition:] \textbf{S}uccessor condition \\
				
			\item[-condition:] \textbf{P}redecessor condition  \\
								
		\end{description}		
\end{definition}

\begin{lemma}\label{notFalsifiable}
A sequent  is not falsifiable if and only if .
\end{lemma}
\begin{proof}
Applying the negation of Definition \ref{falsifiability} to  gives .\end{proof}

\subsection{Sequent Rules}

\begin{definition}[Sequent Rule]
A sequent rule is of one of the forms
	
where ,  for , are sequents. The rule consists of the following components:
	\begin{description}
		\item[Conclusion:] , written below the horizontal line;
		\item[Premise(s):] Optional, , written above the horizontal line;
		\item[Name:] Written to the left of the horizontal line;
		\item[Side conditions:] Optional, written underneath the rule;
		\item[Branching:] Universal (indicated by a solid line) or existential (indicated by a dashed line); explained shortly.							
	\end{description}
\end{definition}

To achieve completeness and termination for , we combine a number of ideas from various existing systems for , as well as use variables for updating worlds with relevant information received from successors and predecessors. Our rules can be divided into two groups: traditional (Fig.~\ref{figTraditionalRules}) and non-traditional (Fig.~\ref{figNonTraditionalRules}).

Our \textbf{traditional} rules (Fig.~\ref{figTraditionalRules}) are
based on Dragalin's  \cite{dragalin1988} for  because we
require multiple formulae in the succedents and antecedents of
sequents for completeness; we have added symmetric rules for the
 connective . The main difference
is that our  rule 
and the symmetric  
carry their principal formula and all side formulae into the premises.
Our rules for  and  also carry their principal formula
into their premises to assist with termination.  Note that there are
other approaches to a terminating sequent calculus for , e.g.,
Dyckhoff's contraction-free calculi \cite{dyckhoff1992}, or history
methods by Heuerding et al.  \cite{heuerding1998} and Howe
\cite{howe1998}. These methods are less suitable when the interaction
between  and  formulae needs to be considered, since
they erase potentially relevant formulae too soon during backward
proof search. Moreover, we found it easier to prove semantic
completeness with our loop-checking method than with history-based
methods since both \cite{heuerding1998} and \cite{howe1998} prove
completeness using syntactic transformations of derivations.
Consequently, while  is sound and complete for the 
(and ) fragment of , it is unlikely to be as
efficient on the fragment as these specific calculi.

\begin{figure}[t]
	\begin{tabular}{cc}
	\multicolumn{2}{c}{
		\scriptsize{\RuleDefId \ \ \ \RuleDefFalseLeft \ \ \ \RuleDefTrueRight}
	}
	\3em]	
	\scriptsize{\RuleDefOrRightBlocked} & \scriptsize{\RuleDefOrLeftBlocked}
	\3em]
	\multicolumn{2}{c}{
		\scriptsize{\RuleDefExclRightAll}
	}
	\3em]	
	\scriptsize{\RuleDefImpRightI} & \scriptsize{\RuleDefExclLeftI}
	\4em]
	\multicolumn{2}{c}{
		\scriptsize{\RuleDefExclLeft}
	}
	\3em]
	\multicolumn{2}{c}{
		\begin{minipage}{\textwidth}
		For every universally branching rule with premises  and conlusion , \\
		apply the rule only if: 
		\end{minipage}
	}
	\ \SequentAny }							
				\LeftLabel{} \UnaryInfC{}

			\LeftLabel{}	\BinaryInfC{}	

			\AxiomC{}
										
		\LeftLabel{} \dashedLine \BinaryInfC{}	
\end{prooftree}
}}

Where  is:

{\scriptsize{\begin{prooftree}
					\AxiomC{}			
					\LeftLabel{} \UnaryInfC{}			

					\AxiomC{}			
					\LeftLabel{} \UnaryInfC{}			

					\LeftLabel{} \BinaryInfC{}			

					\AxiomC{}
					\LeftLabel{} \UnaryInfC{}			
					
					\AxiomC{}			
					\LeftLabel{} \UnaryInfC{}			
							
					\LeftLabel{} \BinaryInfC{}

				\LeftLabel{} \BinaryInfC{}		\end{prooftree}}}

In this case, the  rule in  has two premises, since the returned  variable contains two sets of formulae. Since only the left premise of the  rule is derivable, the conclusion is not derivable. Thus, the open branch corresponding to the bolded member  of  remains open. If we did not return both variable choices from the left sibling of , then we might mistakenly derive  without seeing this open branch.
\end{example}

\begin{lemma}
If a -tree  rooted at  is a derivation then .
\end{lemma}
\begin{proof}
By induction on the longest branch in .
\end{proof}

\subsection{Termination Proof}\label{sec:termination}

We first show that proof search in  terminates because the subsequent soundness proof relies on our ability to receive the variables from the left premises of transitional rules.

\begin{definition}
The rules of  are categorised as follows:
	\begin{description}
		\item[Operational:] ;
		\item[Logical:] 
		\begin{description}
			\item[Static:] , , , , , , , , , , ;
			\item[Transitional:] , ;
			\item[Special:] , .			
		\end{description}
	\end{description}
\end{definition}

The intuition behind the classification of the logical rules is that the static rules add formulae to the current world in the counter-model, the transitional rules create new worlds and add formulae to them, and the special rules decompose variables returned from non-derivable leaves. We shall prove this formally for each rule later. The classification justifies the following search strategy.

\begin{figure}[t]
\textbf{Function} Prove \\
Input: sequent  \\
Output: Derivable ( or )
\begin{enumerate}
	\item If  applicable to  then
		\begin{enumerate}
			\item Return 
		\end{enumerate}
	\item Else if any special or static rule  applicable to  then
		\begin{enumerate}
			\item Let  be the premises of 
			\item Return 
		\end{enumerate}
	\item Else for each transitional rule  applicable to  do
		\begin{enumerate}
			\item Let  and  be the premises of 
			\item If  then return 
		\end{enumerate}
	\item Endif
	\item Return .	
\end{enumerate}
\caption{Proof search strategy. Note that we have left out the variables for simplicity.  is true iff  is true for all premises  for , and  is true iff  is true for some premise  for .}
\label{strategyFig}
\end{figure}


\begin{definition}[Strategy]\label{strategy}
The strategy defined in Figure~\ref{strategyFig} is used when applying the rules of our sequent calculus in backward proof search. Note that we have left out the variables for simplicity.
\end{definition}

\begin{definition}[Subformulae]\label{sf}
For a  formula, we define the subformulae as follows, where  and :

For a set  of extended  formulae, we define .
\end{definition}

Note that the subformulae of  and  do not include the conjunctions and disjunctions implicit in their  equivalents.

\begin{definition}[LEN]
Let  be a lexicographic ordering of sequents:

\end{definition}

\begin{definition}
Given a -tree  and a branch  in , we say that  is \textbf{forward-only} if  contains only applications of static and special rules,  and the right premises of . Similarly,  is \textbf{backward-only} if  contains only applications of static and special rules,  and the right premises of . A branch is \textbf{single-directional} if it is either forward-only or backward-only. Finally, a branch contains \textbf{interleaved} left premises of transitional rules if it contains a sequence  such that  is the left premise of ,  is the left premise of , and  is the left premise of .
\end{definition}

\begin{lemma}\label{forwardOnlyFinite}
Every forward-only branch of any -tree is finite.
\end{lemma}
\begin{proof}
We show that on every such branch, the length of a sequent defined according to  increases.

Consider a rule , and a backwards application of  to some , which yields  premises , where .

We show that if  is a static rule, then for all premises , we have :
\begin{description}
	\item[\rm{, , :}] Then ;
	\item[\rm{:}] Then  and ;	
	\item[\rm{:}] Then for the left premise,  and , and for the right premise, ;
	\item[\rm{:}] Then for the left premise, , and for the right premise,  and .
\end{description}

We now show the cases for .
Even though the right premise of  and  itself is not greater than the conclusion, we show that the lemma holds on the overall  branch, since according to the strategy we immediately apply  or , thus increasing the length of the premise according to .
\begin{description}
	\item[\rm{:}] For every  rule application:
		\begin{enumerate} 
			\item Consider the left premise . We know that according to our strategy, the  rule has already been applied and thus , so  is applied only if . Therefore, for the left premise, we have ;
			\item\label{rightPremImp} Consider the right premise . It is created only if

			That is, every member of  introduces new formulae to the RHS. But recall that . According to our strategy, the  rule will be immediately applied to  in , giving  premises  where . By \ref{conditionForRightPrem}, we will then have  for all . We also have  for all . Therefore, according to the lexicographic ordering, we have  for all the premises .
		\end{enumerate}			
	\item[\rm{:}] Since the  rule is only used in conjunction with the right premise of the  rule, see case \ref{rightPremImp} above;						
	\item[\rm{:}] For every  rule application:
		\begin{enumerate} 
			\item The assumption of the lemma does not apply to the left premise;
			\item The case for the right premise is dual to the case for  above.
		\end{enumerate}		
	\item[\rm{:}] By symmetry with the case for  above;			
\end{description}
Since the length of a sequent defined according to  increases on every forward-only branch as shown above, and since  has the subformula property, eventually no more formulae can be added to a sequent on a forward-only branch, and the branch will terminate.
\end{proof}

\begin{lemma}\label{backwardOnlyFinite}
Every backward-only branch of any -tree is finite.
\end{lemma}
\begin{proof}
By symmetry with Lemma~\ref{forwardOnlyFinite}.
\end{proof}

\begin{lemma}\label{infMustBeInterleaved}
If a -tree contains an infinite branch, then the branch contains an infinite number of interleaved left premises of transitional rules.
\end{lemma}
\begin{proof}
By Lemmas \ref{forwardOnlyFinite} and \ref{backwardOnlyFinite}, single-directional branches must eventually terminate. Thus, a potential infinite loop must involve an infinite number of interleaved left premises of transitional rules  and .
\end{proof}

\begin{definition}[Degree]
The degree of a  formula  is defined as:


Thus, the degree of  is the number of  and  connectives in .

The degree of a sequent  is defined as:

\end{definition}

Note that we have deliberately defined the degree of a sequent as the sum of the degrees of subformulae, because it allows us to make the following observations, which will be crucial in the main termination proof.

\begin{corollary}\label{cannotIncrease}
Since  has the subformula property, the degree of a sequent can never increase in backward proof search. In other words, no  rule can increase the degree of a sequent.
\end{corollary}

\begin{corollary}\label{mustDecrease}
Given two sequents  and , if , then . That is, removing some formula  from a sequent during backward proof search decreases the degree of the sequent if  is not a subformula of any other formula in the sequent, since  no longer contributes to the sum of degrees of subformulae.
\end{corollary}

\begin{theorem}[Termination]\label{termination}
Every -tree constructed according to the strategy of Definition~\ref{strategy} is finite.
\end{theorem}
\begin{proof}
Suppose for a contradiction that there exists an infinite -tree . Since every rule has a finite number of premises, i.e., finite branching, then by K\"{o}nig's lemma an infinite tree can only be obtained by having a branch of infinite length. Thus,  has an infinite branch . By Lemma~\ref{infMustBeInterleaved},  must contain an infinite number of interleaved left premises of transitional rules, as shown below:
\begin{prooftree}
				\AxiomC{}
				\UnaryInfC{}											

				\AxiomC{}													
				\UnaryInfC{}
		
			\LeftLabel{}	\dashedLine \BinaryInfC{}											
			\UnaryInfC{}
	 		\UnaryInfC{}													 		

			\AxiomC{}													
			\UnaryInfC{}

		\LeftLabel{} \dashedLine \BinaryInfC{}	

		\UnaryInfC{}													
		\UnaryInfC{}											

		\AxiomC{}													
		\UnaryInfC{}
	\LeftLabel{} \dashedLine \BinaryInfC{}
	\UnaryInfC{}															
\end{prooftree}

Let  be some formula such that , that is,  is one of the subformulae with the maximum degree. In particular, this means that  is not a subformula of any formula with a larger degree. We shall now show that .

There are two cases:
\begin{description}
	\item[:]	Then  or . In both cases, .
	\item[:] Then it cannot be the case that  or , since then , contradicting our assumption that . Therefore, either:
	\begin{itemize}
		\item  and all its occurrences in subformulae disappear from the sequent at the premise of , in which case , or
		\item  is moved to the RHS of the sequent by applying the  rule to some formula . However, since , it again contradicts our assumption that .	
	\end{itemize} 
\end{description}

We have shown that for some formula  we have  and . Also, by the subformula property of  we have . Together with  and , this means . Then by Corollary~\ref{mustDecrease} we have . Note that the steps indicated by vertical ellipses () are arbitrary, since by Corollary~\ref{cannotIncrease} no rule can increase the degree of a sequent.
	
Since we have , we know that every sequence of interleaved transitional rule applications must decrease the degree of the sequent. This can only happen a finite number of times, until no more transitional rules are applicable. Therefore our assumption was wrong, and no branch  can be infinite. Therefore, every -tree is finite.
\end{proof}

\section{Soundness}\label{soundness}

\subsection{Proof Outline}

Instead of the traditional approach of showing that each rule application preserves validity downwards, we use the notion of falsifiability and show that each rule application preserves falsifiability upwards. We then use Lemma~\ref{notFalsifiable} to make the connection between falsifiability and validity.

Also, our addition of variables to the calculus introduces a two-way flow of information in the  trees, and this complicates the usually simple soundness proof.

We separate the notion of soundness into two: \textit{local soundness}, applicable locally to a single rule application, and \textit{global soundness}, which takes into account the propagation of variables from the leaves down to some node, and possible instances of the operational  rule. Note that locality here refers to locality in the  trees, not locality in the underlying Kripke models. We use the notions of static and transitional rules to classify the rules according to this latter notion.

\subsection{Local soundness}

\begin{definition}[Local soundness]
A logical rule in  is locally sound if and only if:
\begin{itemize}
	\item For rules with \textbf{universal} branching: if the conclusion is falsifiable, then some premise is falsifiable;
	\item For rules with \textbf{existential} branching: if the conclusion is falsifiable, then all premises are falsifiable.
\end{itemize}
\end{definition}

We shall now show that each static and special rule is locally sound, and we shall then use induction on the height of a derivation tree to extend our proof to arbitrary trees containing static rules, special rules, transitional rules and the operational  rule.

\begin{lemma}\label{staticRuleSoundness}
Each static and special rule of  is locally sound.
\end{lemma}
\begin{proof}
We consider each static and special rule in turn. We assume that the conclusion is falsifiable, and show that some premise is falsifiable.
\begin{enumerate}
	
	\item  \\ \RuleDefId \1em]
		The conclusion of this rule is never falsifiable, because by Property~\ref{bottom} of Definition~\ref{model}, no  model can contain a world  such that .
	
	\item  \\ \RuleDefTrueRight \1em]
		Since the conclusion is falsifiable by assumption, we know from Definition \ref{falsifiability} that there exists a world  such that:
		\begin{description}
			\item[\rm{(i)}]  and
			\item[\rm{(ii)}] .
		\end{description}
		From the semantics of  in , (b) implies that either:
		\begin{description}
			\item[\rm{(ii.1)}]  or
			\item[\rm{(ii.2)}] .
		\end{description}
		
		To show that some premise of the  rule is falsifiable, we need to show that there exists a world  such that some premise is falsifiable at . We let .
		
		Then case (ii.1) together with (i) gives us that the left premise is falsifiable, or case (ii.2) together with (i) gives us that the right premise is falsifiable.
		
	\item  \\ \RuleDefOrLeftBlocked \1em]
		Since the conclusion is falsifiable by assumption, we know from Definition \ref{falsifiability} that there exists a world  such that:
		\begin{description}
			\item[\rm{(i)}]  and
			\item[\rm{(ii)}] 
		\end{description}

		To show that the premise of the  rule is falsifiable, we need to show that there exists a world  such that the premise is falsifiable at . We let .		
		
		From the semantics of  in , (ii) implies that  and . Together with (i), this means that the premise is falsifiable.

	\item  \\ \RuleDefAndLeftBlocked \1em]
		Since the conclusion is falsifiable by assumption, we know from Definition \ref{falsifiability} that there exists a world  such that:
		\begin{description}
			\item[\rm{(i)}]  and
			\item[\rm{(ii)}] .
		\end{description}
		From the semantics of  in , (i) implies that for all successors , we have  or .
		
		By reflexivity of , this applies to  too, so we have:
		\begin{description}
			\item[\rm{(i.1)}]  or
			\item[\rm{(i.2)}] .
		\end{description}

	To show that some premise of the  rule is falsifiable, we need to show that there exists a world  such that some premise is falsifiable at . We let .

		Then items (i), (ii) and (i.1) give us that the left premise is falsifiable, or items (i), (ii) and (i.2) give us that the right premise is falsifiable.

	\item  \\ \RuleDefExclRightAll \1em]
		Since the conclusion is falsifiable by assumption, we know from Definition \ref{falsifiability} that there exists a world  such that:
		\begin{description}
			\item[\rm{(i)}]  and
			\item[\rm{(ii)}] .
		\end{description}
	From the semantics of  in , (ii) implies that there exists a successor  such that:
		\begin{description}
			\item[\rm{(iii)}]  and 		
			\item[\rm{(iv)}]  and 
			\item[\rm{(v)}] .
		\end{description}	
	Then, by the reverse persistence property of , and (iii) and (v), we have:
		\begin{description}
			\item[(vi)] .
		\end{description}

		To show that the premise of the  rule is falsifiable, we need to show that there exists a world  such that the premise is falsifiable at . We let .
		
		Then items (i), (ii) and (vi) give us that the premise is falsifiable.
	
	\item  \\ \RuleDefExclLeftI \1em]
		Since the conclusion is falsifiable by assumption, we know from Definition \ref{falsifiability} that there exists a world  such that:
		\begin{description}
			\item[\rm{(i)}]  and
			\item[\rm{(ii)}] .
		\end{description}
	From the semantics of  (recall Definition~\ref{extendedSemantics}), (i) implies that:
		\begin{description}
			\item[\rm{(iii)}] for some , we have .
		\end{description}

		To show that some premise of the  rule is falsifiable, we need to show that there exists a world  such that this premise is falsifiable at . We let .
	
		Then items (i), (ii) and (iii) give us that the -th premise containing  is falsifiable at .
	
	\item  \\ \RuleDefSpecialRight \\exists \Sigma_m \in \Succ_m . \forall w \in \mycal{W} . w_0 \mycal{R} w \MetaImp w \Force \Sigma_m.\exists \Sigma \in \bigcup_1^n \Succ_i . \forall w \in \mycal{W} . w_0 \mycal{R} w \MetaImp w \Force \Sigma.\label{predLeft}
	\exists \Pi \in \Pred_1 . \forall w \in \mycal{W} . w \mycal{R} w_1\MetaImp w \Reject \Pi
\label{succConcl}
	\exists \Sigma \in \Succ . \forall w \in \mycal{W} .w_0 \mycal{R} w \MetaImp w \Force \Sigma
\exists \Sigma_2 \in \Succ_2 . \forall w \in \mycal{W} . w_0 \mycal{R} w \MetaImp w \Force \Sigma_2\SequentAny is not derivable, then  is consistent for all values of  and .
\end{corollary}

\begin{remark}\label{keepChoices}
  As usual, every sequent
  has a set of one or more ``saturations'' due to the branching of
  , , etc., rules. The usual approach is
  to non-deterministically choose one of the non-derivable premises of
  each such rule. However, in the presence of the inverse relation, a
  branch that appears open may close once we return variables to a
  lower sequent. Therefore, we need to temporarily keep all the
  non-derivable premises, since we do not know which of the open
  branches will stay open when we return to a lower sequent.
\end{remark}

\begin{lemma}\label{saturationProcedure}
For each finite non-derivable sequent , there is an effective procedure to construct a finite set  of finite saturated sequents, with  for all .
\end{lemma}
\begin{proof}
Since  is non-derivable, we know from Corollary~\ref{nonderIsConsistent} that  is consistent. Then from Corollary~\ref{consNonId} we know that the , ,  rules are not applicable to . Let . While some static rule  is applicable to a leaf of , extend  by applying  to the leaf to obtain new leaves. Keep the non-derivable leaves only; by Corollary~\ref{nonderIsConsistent} they are consistent. By Theorem~\ref{termination}, the saturation process will eventually terminate; let  be the final leaves of . Since the formulae in each premise are always subformulae of the conclusion, we have that  for all .
\end{proof}

\subsection{Model Graphs and Satisfiability Lemma}

We shall use model graphs as an intermediate structure between -trees and  models.

\begin{definition}\label{defModelGraph}
A model graph for a sequent  is a finite  frame  such that all  are saturated sequents  and all of the following hold:
	\begin{enumerate}
		\item  and  for some , where ;
		\item\label{p_impRight} if  then  with  and  and ;
		\item\label{p_weakImpLeft} if  then  with  and  and ;
		\item\label{p_impLeft} if  and  then  or ;
		\item\label{p_weakImpRight} if  and  then  or ;
		\item\label{p_pers} if  and  then ;
		\item\label{p_revpers} if  and  then .	
	\end{enumerate}
\end{definition}

\noindent We now show that given a model graph, we can use it to construct a  model.

\begin{lemma}\label{modelGraphToModel}
If there exists a model graph  for , then there exists a  model  such that for some , we have  and . We call  the counter-model for .
\end{lemma}
\begin{proof}
Since we already have a  frame , we need to define a valuation  in order to construct a  model :
\begin{enumerate}
	\item For every world  and every atom , let .
	\item For every world  and every atom , let . 
\end{enumerate}

Then properties \ref{p_pers} and \ref{p_revpers} of Definition~\ref{defModelGraph} ensure persistence and reverse persistence respectively.

We now need to show that for every world , we have  and ; we can do this by simple induction on  the length of the formulae in .

Now let  be the world in the model graph such that  and . Since our proof by induction has shown that for every world , we have  and , then in particular, we have that  and . Then, since we have that  and , we also have  and .
\end{proof}

\subsection{Main Completeness Proof}

\begin{figure}[t]
\textbf{Procedure} MGC \\
Input: sequent  \\
Output: model graph , variables  and 
\begin{enumerate}
        \item Let  be the result of saturating  using Lemma~\ref{saturationProcedure};
        \item\label{eachSaturatedVersion} For each  do 
                \begin{enumerate}
                \item Let ; let ;
                \item\label{futureWorldConstruction} For each non-blocked  and while  do
                \begin{enumerate}
                        \item\label{startSuccConstr} Apply  to  and obtain a left premise ;
                        \item Let ;
                        \item If  then
                        \begin{enumerate}
                                \item\label{chooseSuccessor} Let  be the root of the connected component  from ;
                                \item\label{create} Let ; add  to , and put .
                        \end{enumerate}
                        \item\label{newInfo} else
                        \begin{enumerate}
                                \item\label{delete} Let ; let ; 
                                \item\label{addVars} Invoke the right premise of  to obtain ;
                                \item Apply  to  to obtain  non-derivable premises ;
                                \item For each , , let ;
                                \item\label{endSuccConstr} Let , and  and ;
                        \end{enumerate}                         
                \end{enumerate}
                \item\label{pastWorldConstruction} For each non-blocked  and while  do
                \begin{enumerate}
                        \item Perform a symmetric procedure to Steps~\ref{startSuccConstr} to \ref{endSuccConstr}.
                \end{enumerate}
                \item\label{finalVars} If  then let  and .
                \end{enumerate}
        \item Return 
\end{enumerate}
\caption{Model Graph Construction Procedure}
\label{algConstruction}
\end{figure}

We now show how to construct a model graph for
 from a
consistent .
Recall from Remark~\ref{keepChoices} that we need to keep a number of
independent versions of worlds because of the choices arising due to
disjunctive non-determinism. We do this by storing one or more
independent connected-components  in the constructed model
graph , and the indices (sorts) of worlds
and relations tell us the connected-component of the graph to which
they belong.
We write  to relabel the
connected component  with sort  to
a connected component  with sort .
Similarly, we also label each member of the variables  and
, so we can later extract the member 
with sort , corresponding to the component
of  with sort . We write
-neighbour to mean -predecessor or
-successor.

Our algorithm in Fig.~\ref{algConstruction} starts by saturating the
root world to obtain one or more saturated ``states''. For each
``state'' , it recursively creates all the
-neighbours and saturates them, and so on.  If during the
construction of any -neighbour, new information is returned
from the higher sequents (Step~\ref{newInfo}), then we delete the
entire subtree (connected component of sort ) rooted at ,
and recreate  using the new information
(Step~\ref{addVars}). This re-creates all the
-neighbours of . Otherwise, if none of the
-neighbours of  return any new information, or
there are no -neighbours for , then
Step~\ref{finalVars} instantiates the variables and returns from the
recursion. In the latter case, the ``state''  already has
all the required information it can possibly receive from any
-neighbours, thus  is final. Note the duality:
new information {\em from} a single -neighbour means that
all of the members of a variable were new, while new information {\em
  at} a ``state''  means that some -neighbour
returned new information.

When we return from , we form the union of the components of the
model graph and the variables from the different ``states'', so that
the caller of  can extract the appropriate component at
Step~\ref{chooseSuccessor}.

\begin{remark}\label{proofVsCountermodel}
Note that while the counter-model construction procedure keeps the whole counter-model in memory, this procedure is only used to prove the completeness of . Our procedure for checking the validity of  formulae (Fig.~\ref{strategyFig}) does not need the whole counter-model, and explores one branch at a time, as is usual for sequent/tableaux calculi.
\end{remark}

\begin{theorem}[Completeness]
 is complete: if  is not derivable, then there exists a counter-model for .
\end{theorem}

\begin{proof}
Suppose  is not derivable, then by Corollary~\ref{nonderIsConsistent} we have that  is consistent. We construct a model graph for  using the procedure given in Figure~\ref{algConstruction}, and obtain . We let  be any connected component of . We now show that  satisfies the properties of a model graph from Definition~\ref{defModelGraph}:
	\begin{enumerate}
		\item 
          and  for some : This holds because  is one of the saturated
          sequents obtained from .  Moreover,
          if we delete the original  at Step~\ref{delete}, a
          final version of  is created at Step~\ref{create} which
          is never deleted.
          
		\item\label{succExists} if  then  with  and  and : This holds because we have either created 
          using  at Step~\ref{create}, or had 
          fulfill the role of this successor by reflexivity if
           was blocked.
          
		\item if  then there exists some  with  and  and : \\
			By symmetry with property~\ref{succExists}.
			
		\item\label{impLeftAll} if  and 
                  then  or : In our construction, there are three
                  ways of obtaining , so we need to
                  show that for each case, the property holds. We
                  first show that :
                        \begin{enumerate}
                                \item  was created by applying  to  on some . Then  also contains .
                                \item  was created by applying  to some . Then, when the final version of  was created,  was added to the  variable at Step~\ref{finalVars}. There are two cases:
                                \begin{itemize}
                                        \item The right premise  of  was invoked at . Then  was added to  at  by the symmetric process to Step~\ref{addVars}. Thus the updated  also contains .
                                        \item The right premise of  was not invoked at . This means that , and the -th version of 's predecessor  is chosen at the symmetric process to Step~\ref{chooseSuccessor}. But since Step~\ref{finalVars} at  assigns , then we have  and thus .
                                \end{itemize}
                                \item , and  by reflexivity. Then , so .
                        \end{enumerate}         
                        In all cases, saturation for  will then ensure that  or .
		\item if  and  then  or : \\
			By symmetry with property \ref{impLeftAll}.
		\item\label{forwardPersistence} if  and  then : \\
			By similar argument to property \ref{impLeftAll}.
		\item if  and  then :	\\	
			By symmetry with property \ref{forwardPersistence}.
	\end{enumerate}

\noindent We can obtain a counter-model for  from  via Lemma \ref{modelGraphToModel}.
\end{proof}

\begin{definition}
A di-tree is a directed graph such that if the direction of the edges is ignored, it is a tree.
\end{definition}

\begin{theorem}
Every falsifiable  sequent can be falsified by a model whose frame is a di-tree, consisting of reflexive points.
\end{theorem}
\begin{proof}
From Lemmas \ref{forwardOnlyFinite} and \ref{backwardOnlyFinite}, we know that the construction of new successors for  and predecessors for  stops when either there are no rejected -formulae or forced -formulae in the current world, or the current world already forces  and rejects . In the latter case, the world itself fulfills the role of the successor or predecessor by reflexivity, and no new successors or predecessors are created.

The reason we are able to avoid proper cycles is the persistence and reverse persistence properties of , used in the  and  rules.

Consider the  case. Every time some  appears on the RHS of a sequent , we first add  to the RHS to obtain  using the  rule, since by reverse persistence the current world must reject everything that some successor world rejects. Now that  is on the RHS, we need to apply the  rule to create the -successor  only if  is not already on the LHS. For if , then the successor  that fulfills  can be the current world itself. So there is no point creating it explicitly.
\end{proof}

\begin{corollary}
 is characterised by finite rooted reflexive and transitive di-trees of reflexive points.
\end{corollary}

\section{Conclusions and Future Work}\label{sec:conclusion}

Our cut-free calculus for  enjoys terminating backward
proof-search and is sound and complete w.r.t Kripke semantics.  A
simple Java implementation of  is available at
\url{http://users.rsise.anu.edu.au/~linda}.
The next step is to add a cut rule to , and prove cut
elimination syntactically. We are also extending our work to the modal logic , and the tense
logic . Our approach of existential branching and inter-premise communication bears some similarities to hypersequents of Pottinger and Avron~\cite{avron1996}. It would be interesting to investigate this correspondence further. From an automated deduction perspective,  is the first step towards an efficient decision procedure for . The next task is to analyse the computational complexity of  and investigate which of the traditional optimisations for tableaux systems are still applicable in the intuitionistic case.

We would like to thank the anonymous reviewers for their suggestions.

\begin{thebibliography}{10}

\bibitem{avron1996}
Arnon Avron.
\newblock The method of hypersequents in the proof theory of propositional
  non-classical logics.
\newblock In Wilfrid Hodges, Martin Hyland, Charles Steinhorn, and John Truss,
  editors, {\em Logic: from foundations to applications. Proc. Logic
  Colloquium, Keele, UK, 1993}, pages 1--32. Oxford University Press, New York,
  1996.

\bibitem{crolard2001}
T.~Crolard.
\newblock Subtractive logic.
\newblock {\em Theor. Comp. Sci.}, 254(1--2):151--185, March 2001.

\bibitem{crolard2004}
T.~Crolard.
\newblock A formulae-as-types interpretation of {S}ubtractive {L}ogic.
\newblock {\em Journal of Logic and Computation}, 14(4):529--570, August 2004.

\bibitem{czermak1977}
J.~Czermak.
\newblock A remark on {G}entzen's calculus of sequents.
\newblock {\em Notre Dame Journal of Formal Logic}, 18(3):471--474, 1977.

\bibitem{dragalin1988}
A.~Dragalin.
\newblock {\em Mathematical Intuitionism: Introduction to Proof Theory},
  volume~68 of {\em Translations of Mathematical Monographs}.
\newblock Cambridge Univ. Press, 1988.

\bibitem{dyckhoff1992}
R.~Dyckhoff.
\newblock Contraction-free sequent calculi for intuitionistic logic.
\newblock {\em The Journal of Symbolic Logic}, 57(3):795--807, September 1992.

\bibitem{gallier1986}
J.~H. Gallier.
\newblock {\em Logic for Computer Science, Foundations of Automated Theorem
  Proving}.
\newblock Computer Science and Technology Series. Harper \& Row, 1986.

\bibitem{gentzen1935}
G.~Gentzen.
\newblock Untersuchungen {\"u}ber das {L}ogische {S}chliessen.
\newblock {\em Mathematische Zeitschrift}, 39:176--210 and 405--431, 1935.
\newblock English translation in \cite{szabo1969}.

\bibitem{gore1999}
R.~Gor\'e.
\newblock Tableau methods for modal and temporal logics.
\newblock In D'Agostino at~al, editor, {\em Handbook of Tableau Methods}, pages
  297--396. Kluwer, 1999.

\bibitem{heuerding1998}
A.~Heuerding, M.~Seyfried, and H.~Zimmermann.
\newblock Efficient loop-check for backward proof search in some non-classical
  propositional logics.
\newblock In {\em Analytic Tableaux and Related Methods}, volume 1071 of {\em
  LNAI}, pages 210--225, 1996.

\bibitem{horrocks1998}
I.~Horrocks, U.~Sattler, and S.~Tobies.
\newblock A {PS}pace-algorithm for deciding -satisfiability.
\newblock Technical Report LTCS-98-08, LuFG Theoretical Computer Science, RWTH
  Aachen, 1998.

\bibitem{howe1998}
J.~M. Howe.
\newblock {\em Proof search issues in some non-classical logics}.
\newblock PhD thesis, University of St Andrews, 1998.

\bibitem{mckinsey1946}
J.~C.~C. McKinsey and A.~Tarski.
\newblock On closed elements in closure algebras.
\newblock {\em Annals of Mathematics}, 47(1):122--162, 1946.

\bibitem{nelson1949}
D.~Nelson.
\newblock Constructible falsity.
\newblock {\em Journal of Symbolic Logic}, 14(2):16--26, 1949.

\bibitem{rauszer1974}
C.~Rauszer.
\newblock A formalization of the propositional calculus of {H-B} logic.
\newblock {\em Studia Logica}, 33:23--34, 1974.

\bibitem{rauszer1980}
C.~Rauszer.
\newblock An algebraic and {K}ripke-style approach to a certain extension of
  intuitionistic logic.
\newblock {\em Dissertationes Mathematicae}, 168, 1980.
\newblock Institute of Mathematics, Polish Academy of Sciences.

\bibitem{schwendimann98}
S.~Schwendimann.
\newblock A new one-pass tableau calculus for {PLTL}.
\newblock In {\em Analytic Tableaux and Related Methods}, volume 1397 of {\em
  LNAI}, pages 277--292, 1998.

\bibitem{svejdar2006}
V.~{\'S}vejdar.
\newblock On sequent calculi for intuitionistic propositional logic.
\newblock {\em Commentationes Mathematicae Universitatis Carolinae},
  47(1):159--173, 2006.

\bibitem{szabo1969}
M.~E. Szabo, editor.
\newblock {\em The Collected Papers of Gerhard Gentzen}.
\newblock Studies in Logic and the foundations of Mathematics. North-Holland,
  Amsterdam, 1969.

\bibitem{urbas1996}
I.~Urbas.
\newblock Dual-intuitionistic logic.
\newblock {\em Notre Dame Journal of Formal Logic}, 37(3):440--451, Summer
  1996.

\bibitem{uustalu2004}
T.~Uustalu.
\newblock Personal communication.
\newblock via email, 2004.

\bibitem{uustalu2006}
T.~Uustalu.
\newblock Personal communication.
\newblock via email, 2006.

\bibitem{uustalu2006a}
T.~Uustalu and L.~Pinto.
\newblock Days in logic '06 conference abstract.
\newblock Online at \url{http://www.mat.uc.pt/~kahle/dl06/tarmo-uustalu.pdf},
  accessed on 27th October 2006, 2006.

\bibitem{wolter98}
F.~Wolter.
\newblock On logics with coimplication.
\newblock {\em Journal of Philosophical Logic}, 27(4):353--387, 1998.

\end{thebibliography}
\end{document}