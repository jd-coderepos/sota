\section{Experiments}
\subsection{Can Face Parsing Mask Help?}\label{sec:exp:masks}
As a motivational example, we first test whether existing age estimation methods can benefit from facial parts segmentation. This is done by simply stacking the face parsing masks to the input image and using the resulted 14-channel tensor as the input to the models. During this experiment, we re-train three state-of-the-art methods, Dex, DLDL-V2 and MV-Loss, with the modified 14-channel input and test the models on IMDB-Clean. 
From Table~\ref{tab:stack-helps} we observe that by taking the stacked representation as input, all three models can achieve better performance in terms of both MAE and CS$_5$.
\begin{table}[h]
\renewcommand{\arraystretch}{1.3}
    \caption{Stacking images and face masks helps (evaluated on IMDB-Clean).}
    \label{tab:stack-helps}
    \centering
    \begin{tabular}{l|c|c}
    \toprule
        Method & MAE $\downarrow$ & CS$_5$(\%) $\uparrow$\\ \hline
        Dex~\cite{rotheDEXDeepEXpectation2015} &5.34 & 58.31 \\ 
        Dex with stacked input &  5.29 & 58.61\\\hline
        DLDL-V2~\cite{gaoAgeEstimationUsing2018} & 5.19  & 54.28 \\
        DLDL-V2 with stacked input & 5.12 & 55.14 \\\hline
        MV-Loss~\cite{panMeanVarianceLossDeep2018} & 5.27 & 53.97 \\
        MV-Loss with stacked input & 5.13 & 59.74 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Which Face Parsing Features to Use?}
We study which face parsing features are more informative for age estimation. 
We remove the face parsing attention module in FP-Age and use take face parsing features directly as input. 
We use four kinds of features as input: 1) low-level; 2) high-level; 3) stacking low and high; and 4) stacking low, high and mask.

From Table~\ref{tab:which-features}, we observe that using high-level features gives worse performance than using low-level features. This is consistent with earlier research~\cite{hanDemographicEstimationFace2015} which argues that local features are more informative as they capture ageing patterns around the facial regions, such as the dropping skin around the eyes, and the wrinkles around the mouth. On the other hand, due to the dilated convolutions in RTNet, the high-level features capture a larger perceptive field and thus the details can be lost. Stacking low-level and high-level features gives better performance which shows that these two types of features are complementary and combining them can help age estimation network. 

We also observe that adding mask further improves the model. This can be attributed to the fact that face mask contains semantics about different regions and adding it as an explicit attention mechanism helps the model to effortlessly locate these regions and extract ageing patterns. Furthermore, our face parsing attention module yields better results than simple stacking, which we further investigate in \sectionautorefname~\ref{sec:FPA}.
\begin{table}
    \caption{Using Different Face Parsing Features for Age Estimation on \imdbc.}
    \label{tab:which-features}
    \centering
    \begin{tabular}{l|c|c}
    \toprule
        Features from RTNet & MAE $\downarrow$ & CS$_5$(\%) $\uparrow$ \\ \hline
        Low-level &5.01 & 60.97 \\ 
        High-level &5.24 & 58.30 \\ 
        Stacking Low and High &4.96 & 61.01 \\ 
        Stacking Low, High and Masks&4.90 & 61.84 \\ \hline
        Full Model (with Face Parsing Attention) &4.68  &63.78 \\  
    \bottomrule
    \end{tabular}
\end{table}







\subsection{How about Other Feature Extractors?}
To validate the choice of the face parsing network as the feature extractor, we replace it with other CNN-based feature extractors and compare the performance of these variants. 

We adopted various generic feature extractors that are commonly used in transfer learning tasks as a replacement of face parsing network. The feature extractors include variants from the families of ResNet~\cite{resnet}, ResNeXt~\cite{xie2017resnext}, MobileNetV3~\cite{howard2019mobilenetv3}, FBNet~\cite{wu2019fbnet} and InceptionV4~\cite{szegedy2017inceptionv4}. Their weights have been pre-trained on the ImageNet dataset and remained frozen during the training for age estimation.

We also adopted a state-of-the-art face recognition feature network, ArcFace~\cite{deng2018arcface}, for feature extraction. The backbone of ArcFace is a customised, improved version of ResNet and it has been pre-trained on the large scale MS1M~\cite{guo2016ms1m} dataset for face recognition. The pre-trained weights remained frozen during the training for age estimation.

To ensure fair comparison, we did not use Face Parsing Attention in our model. All feature extractors adopted the same strategy for stacking deep and shallow semantic features. The age estimation sub-network and all other hyper-parameters remain the same as FPAge.

\tableautorefname~\ref{tab:which-feature-extractor} shows the results of using different pre-trained feature extractors on \imdbc. Our first observation is that the features of ResNet50 performed the best among ImageNet pre-trained models though being less accurate on image classification tasks. Moreover, all ImageNet pre-trained models obtained MAEs larger than 7. This in turn suggests generic features encoded in the CNNs for image classification are not directly transferable to solve the age estimation problem. 

Our second observation is that
face recognition features resulted better performance than generic features, meaning that the encoded details for distinguishing between identities are more transferable to the age estimation problem.

Finally, face parsing features have given the best performance among all evaluated backbones. This suggests that face parsing networks, designed to classify each pixel in a face, are able to encode the most informative details for age estimating.



\begin{table}[h]
    \caption{Performance of Different Pre-trained Feature Extractors on \imdbc.}
    \label{tab:which-feature-extractor}
    \centering
    \begin{tabular}{l|c|c|c|c}
    \toprule
        Feature extractor & Pre-train Data & \# Params & MAE $\downarrow$ & CS$_5$(\%) $\uparrow$ \\ \hline
        FBNet-C & ImageNet  & \textbf{2.9} M & 7.45 & 42.64 \\
        InceptionV4& ImageNet  & 41.1 M & 7.43 & 42.59 \\
        ResNeXt50& ImageNet & 23.0 M & 7.26 & 44.13 \\
        MobileNetv3-L& ImageNet  & 3.0 M & 7.24 & 44.11 \\
        ResNeXt101& ImageNet & 86.7 M & 7.17 & 44.28 \\ 
        ResNet101& ImageNet & 42.5 M & 7.12 & 44.63 \\
        ResNet50& ImageNet & 23.5 M & 7.10 & 44.59 \\
        ArcFace~\cite{deng2018arcface}& MS1M~\cite{guo2016ms1m} & 30.7 M & 5.96 & 52.19 \\ \hline
        Ours (w/o FPA) & iBugMask~\cite{linRoITanhpolarTransformer2021} & 27.3 M & 4.96 & 61.01 \\ 
        Ours (full) & iBugMask~\cite{linRoITanhpolarTransformer2021} & 27.3 M & \textbf{4.68} & \textbf{63.78} \\ 
    \bottomrule
    \end{tabular}
\end{table}






\subsection{But Aren't There Other Attentions?}
To validate the usefulness of the proposed Face Parsing Attention~(FPA) module, we compare it with three generic CNN attention modules, Squeeze-Excitation (SE)~\cite{Hu_2018_CVPR}, Convolutional Block Attention Module (CBAM)~\cite{Woo_2018_ECCV}, and Simple and Parameter Free Attention Module~(SimAM)~\cite{SimAM}. 
To ensure fair comparison, all attention modules are applied on the high-level features. Other components and all other hyper-parameters remain the same as FPAge.

\tableautorefname~\ref{tab:which-attention-module} shows that, when applied on the same face parsing features, the proposed FPA has achieved the lowest MAE on \imdbc. Moreover, FPA is directly derived from the face parsing map and acts as a probe to understand what the netword has learned, which we investigate in \sectionautorefname~\ref{sec:FPA}. 

\begin{table}[h]
    \caption{Applying Different Attention Modules on Face Parsing Features on \imdbc.}
    \label{tab:which-attention-module}
    \centering
    \begin{tabular}{l|c|c}
    \toprule
        Attention & MAE $\downarrow$ & CS$_5$(\%) $\uparrow$ \\ \hline
        Squeeze-Excitation~\cite{Hu_2018_CVPR} & 4.86 & 61.47 \\
        CBAM\cite{Woo_2018_ECCV} & 4.83 & 62.04 \\
        SimAM\cite{SimAM} & 4.82 & 62.03 \\
        \hline
        Face Parsing Attention~(ours) & \textbf{4.68} & \textbf{63.78} \\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Ablation Study}

We conduct ablation study on the overall \fpage~model to understand the contribution of each component. We have evaluated five variants on \imdbc.  We replaced the face parsing network with ResNet50 which has the similar number of parameters. Next, we either removed the FPA module or replace it with the Squeeze-Excitation~\cite{Hu_2018_CVPR} module. 

Table~\ref{tab:ablation-study} shows that the biggest improvement comes from adopting the face parsing network as the feature extractor with MAEs improved from above 7 to 4.96. Moreover, the proposed FPA has reduced the MAE from 4.96 to 4.68, an improvement of 0.28 compared with 0.1 by the Squeeze-Excitation module.

\begin{table}[h]
    \caption{Ablation Study on \imdbc.}
    \label{tab:ablation-study}
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule
        Feature Extractor & Attention & MAE $\downarrow$ & CS$_5$(\%) $\uparrow$ \\ \hline
        ResNet50 & - & 7.10 & 44.59 \\
        ResNet50 & Squeeze-Excitation & 7.00 & 45.50 \\
        Face Parsing Network & - & 4.96 & 61.01 \\
        Face Parsing Network & Squeeze-Excitation &4.86 & 61.47\\ \hline
        Face Parsing Network  & FPA & \textbf{4.68} & \textbf{63.78}\\
        
    \bottomrule
    \end{tabular}
\end{table}
\subsection{What Did FPA Learn Really?}\label{sec:FPA}
To provide a clearer picture of the function of the proposed face parsing attention module, we study the 11-class activation output of the Sigmoid layer. Specifically we show the mean and standard deviations of the activations for images in the IMDB-Clean dataset in \figurename~\ref{fig:se_act}. 

We observe that the network consistently gives higher attention weights to most inner facial regions, especially eyes (``l-eye'' and ``r-eye'') and mouth (``upper-lip'', ``i-mouth'', and ``lower-lip''). This is in line with the observations reported in \cite{hanDemographicEstimationFace2015}.
Interestingly, it can also be seen that the ``background`` class contributes more than the ``skin'' class. This could be attributed to the fact that the face parsing network classifies objects like ``beard'', ``glasses'' and ``accessories'' as ``background'', and such context information could give hints about the person's age.

We have also performed the same test on separate age groups and observed the importance of different facial regions follows the same trend as shown in \figurename~\ref{fig:se_act}. This means that the face parsing attention allows the model to focus on informative regions that are universally important for judging different ages.
Although there are some works such as ~\cite{yiAgeEstimationMultiscale2014,Angeloni_2019_ICCV, liao2017local, peiAttendedEndtoEndArchitecture2020}  that used attention, we are the first to present the evidence that the network attends to specific facial parts and that such attention modelling improves age estimation.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/fig_se_activations_imdb_clean.pdf}
    \caption{Attention weights for facial regions induced by the face parsing attention module on IMDB-Clean.}
    \label{fig:se_act}
\end{figure}

\subsection{Effectiveness of IMDB-Clean}
We conduct experiments on the effectiveness of the proposed IMDB-Clean. Specifically, we train 6 models on three datasets, \ie~IMDB-Clean, IMDB-WIKI and CACD. We then directly test them on KANFace without any fine-tuning. For IMDB-WIKI, we randomly sampled $300,000$ images for training; for the other two datasets, we used their provided training splits.
Table~\ref{tab:imdb-clean-effectiveness} shows the cross-dataset evaluation results on KANFace. We observe that 1) all models have improved when they are trained on our IMDB-Clean; 2) our model outperforms other methods when trained on IMDB-Clean and IMDB-WIKI, and is comparable to DLDL-V2 when trained on CACD. 
\begin{table}[ht]
	\caption{Effectiveness of IMDB-Clean (Testing dataset: KANFace~\cite{georgopoulosInvestigatingBiasDeep2020}). }\label{tab:imdb-clean-effectiveness}
	\begin{center}
	\begin{threeparttable}
	\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{l|c|c|c|c|c|c}
    		\toprule
			 \multirow{2}{*}{}& \multicolumn{6}{c}{Trained on}  \\ 
			& \multicolumn{2}{c|}{IMDB-Clean} & \multicolumn{2}{c|}{IMDB-WIKI} & \multicolumn{2}{c}{CACD}
			\\ 
			\midrule \hline
            Method & MAE & CS$_5$(\%) & MAE & CS$_5$(\%) & MAE & CS$_5$(\%) 
            \\ 
            \hline 
			DLDL\cite{gaoDeepLabelDistribution2017}  & \textbf{9.84} &\textbf{37.37}   & 12.19 & 27.20 &11.66 &29.20  \\
			DLDL-V2\cite{gaoAgeEstimationUsing2018}   &\textbf{8.05} & \textbf{41.74} & 11.46 & 28.83 &10.88 & 30.66   \\
			Dex\cite{rotheDeepExpectationReal2018}   & \textbf{7.91} & \textbf{42.30} & 11.70 & 20.91 & 11.90 & 28.62   \\
			M-V Loss\cite{panMeanVarianceLossDeep2018}   & \textbf{7.71} & \textbf{43.31}& 11.95 & 28.30  & 11.30&   29.07 \\
			OR-CNN\cite{niuOrdinalRegressionMultiple2016}   & \textbf{7.71} & \textbf{47.51} & 11.10 & 33.07 & 11.18  & 32.90 \\
			\hline
			FP-Age~(ours)   & \textbf{6.81} & \textbf{48.49} & 10.83 & 29.63 & 10.91 &  30.27  \\
			\bottomrule
		\end{tabular}
		\end{adjustbox}
	\end{threeparttable}
	\end{center}
\end{table}

\subsection{Comparison to the State-of-the-arts} \label{sec:compare-sotas}
\subsubsection{Intra-Dataset Evaluation}
In this section, the performance of the proposed FP-Age is compared with the state-of-the-art age estimation methods under the intra-dataset evaluation protocol. Three benchmarks are used: IMDB-Clean, Morph and CACD. On IMDB-Clean, we train all the models from scratch on the same training set and test them on the testing set. For Morph and CACD, we only train our own models and compare the performance with the reported values for the other methods on the testing set.


The benchmarking results are shown in Table~\ref{tab:imdb-clean-results}. It can be seen that our model achieves state-of-the-art results on \imdbc~dataset. When all model are trained under the same settings, our model achieves $4.68$ in terms of MAE and $63.78\%$ in terms of CS$_5$. Additionally, the results show that \imdbc~is quite challenging compared to other datasets, such as Morph where the state-of-the-art MAEs have achieved below $2$. We provide significance testing analysis in Appendix~\ref{appendix:t-test} which shows our results are significantly better than the other methods.

From Table~\ref{tab:morph-results-s3}, it can be seen that our model achieves state-of-the-art results on Morph dataset. When directly trained on Morph, our model achieves $2.04$ in terms of MAE and $92.8\%$ in terms of CS$_5$. When pre-trained on IMDB-Clean and fine-tuned the weights on Morph, \fpage~achieves a MAE of $1.90$ and a  CS$_5$ of $93.7\%$, which is the new state-of-the-art result.

Table~\ref{tab:cacd-results} shows the results on the CACD dataset. Following the training protocols of CACD~\cite{shenDeepRegressionForests2018}, we train our models with both the training set and the validation set, and report the MAE values on the testing set. Our model achieves $4.50$ when trained on CACD-train and $5.62$ when trained on CACD-val. Similar to above experiments, when pre-trained on IMDB-Clean, our model achieves $4.33$ and $4.95$ .  
\subsubsection{Cross-Dataset Evaluation} To test the generalisation ability of different models, we conduct experiments on a cross-dataset evaluation protocol. Our results are compared with $9$ advanced models: SSRNet, C3AE, SVRT, DLDL, DLDL-V2, Coral, Dex, MV-Loss, and OR-CNN. We train all models on IMDB-Clean and test them on $4$ different testing datasets without fine-tuning. The reuslts are summarised in Table~\ref{tab:cross-db-res}. It can be seen that when all models are trained on \imdbc, the proposed \fpage~achieves the best results on most of evaluation datasets.
\begin{table}[tb] 
	\caption{Intra-Dataset Evaluation on \imdbc.}\label{tab:imdb-clean-results}
	\begin{center}
    	\begin{threeparttable}
    		\begin{tabular}{l|c|c|c}
    			\toprule
    			{Method} & {MAE} $\downarrow$  & {CS$_5$(\%)} $\uparrow$ & {Year}\\
    			\midrule
    			\midrule
    	    	OR-CNN~\cite{niuOrdinalRegressionMultiple2016} & 5.85
& 49.72 & 2016 \\		
    			DLDL~\cite{gaoDeepLabelDistribution2017}  & 6.04 & 56.94 & 2017 \\
    			SSRNet~\cite{ssrnet2018} &7.08 & 27.87 & 2018 \\
    			Dex~\cite{rotheDeepExpectationReal2018}  & 5.34 & 58.61 & 2018 \\
    			M-V Loss\cite{panMeanVarianceLossDeep2018} & 5.27 & \textit{59.74} & 2018 \\
    		    DLDL-V2~\cite{gaoAgeEstimationUsing2018}  & \textit{5.19} & 54.28 & 2018 \\
    			SVRT~\cite{imScaleVaryingTripletRanking2019} & 5.85 & 49.72 & 2019 \\
    			C3AE~\cite{zhangC3AEExploringLimits2019} &6.75 & 47.98 & 2019 \\
    			\midrule
    			FP-Age~(ours) & \textbf{4.68}\tnote{$\ddagger$} & \textbf{63.78} &{-} \\
    			\bottomrule
    		\end{tabular}
    	
	
    	\begin{tablenotes}
    	\item \textbf{Bold} indicates the best and \textit{italic} the second
    	\item $\ddagger$ Our results are statistically significant according to paired t-test and Bonferroni corrections (See Appendix~\ref{appendix:t-test})
    	\end{tablenotes}
    	\end{threeparttable}
	\end{center}
\end{table}


\begin{table}[tb] 
	\caption{Intra-Dataset Evaluation on Morph~\cite{ricanekMORPHLongitudinalImage2006}.}\label{tab:morph-results-s3}
	\begin{center}
    	\begin{threeparttable}
    		\begin{tabular}{l|c|c|c}
    			\toprule
    			{Method} & {MAE} $\downarrow$  & {CS$_5$(\%)} $\uparrow$&{Year} \\
    			\midrule
    			\midrule
    			Human workers~\cite{niuOrdinalRegressionMultiple2016} & 6.30 & 51.0  & 2015 \\
    	    	OR-CNN~\cite{niuOrdinalRegressionMultiple2016} & 3.34 & 81.5 & 2016 \\		
    			DLDL~\cite{gaoDeepLabelDistribution2017}  & 2.42 & - & 2017 \\
    			ARN~\cite{agustssonAnchoredRegressionNetworks2017} & 3.00 & - & 2017 \\	    			Ranking-CNN~\cite{chenUsingRankingCNNAge2017}\tnote{$\ast$}  & 2.96 & 85.2 & 2017 \\
    			M-V Loss\cite{panMeanVarianceLossDeep2018} & 2.41 & 91.2 & 2018 \\
    			DLDL-V2~\cite{gaoAgeEstimationUsing2018}\tnote{$\dagger$}  & {1.97} & - & 2018 \\
    			BridgeNet~\cite{liBridgeNetContinuityAwareProbabilistic2019}\tnote{$\ast$} & 2.38 & - & 2019 \\
    			C3AE~\cite{zhangC3AEExploringLimits2019}\tnote{$\ast$} & 2.75 & - & 2019 \\
    			AVDL~\cite{wenAdaptiveVarianceBased2020}\tnote{$\ast$} & \textit{1.94} & - & 2020 \\
    			PML~\cite{dengPMLProgressiveMargin2021} & {2.15} & -  &  {2021} \\ 
    			DRF~\cite{shenDeepDifferentiableRandom2021} & 2.14 & 91.3 & 2021 \\ 
    			\midrule
    			FP-Age~(ours) & 2.04& \textit{92.8} &{-} \\
    			FP-Age\tnote{$\ddagger$}~~(ours) & \textbf{1.90} & \textbf{93.7} &{-} \\
    			\bottomrule
    		\end{tabular}
    	
	
    	\begin{tablenotes}
    	\item \textbf{Bold} indicates the best and \textit{italic} the second
    	\item $^\ast$ pre-trained on IMDB-WIKI
    	\item $^\dagger$ pre-trained on MS-Celeb-1M
    	\item $\ddagger$ pre-trained on the proposed IMDB-Clean 
    	\end{tablenotes}
    	\end{threeparttable}
	\end{center}
\end{table}

\begin{table}[tb]
	\caption{Intra-Dataset Evaluation (MAEs) on CACD~\cite{chenFaceRecognitionRetrieval2015}.}\label{tab:cacd-results}
	\begin{center}
	\begin{threeparttable}
		\begin{tabular}{l|c|c|c}
    		\toprule
			\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Trained on} &\multirow{2}{*}{Year} \\
			& CACD-train & CACD-val & \\
			\midrule \midrule
			Dex\cite{rotheDeepExpectationReal2018} & 4.78 & 6.52 & 2018 \\
			DLDLF~\cite{shenDeepRegressionForests2018} & 4.67 & 6.16 & 2018 \\
			DRF~\cite{shenDeepDifferentiableRandom2021} &4.61 & 5.63 & 2021 \\
			\hline
			FP-Age~(ours) & 4.50 & 5.62 & -\\
			FP-Age\tnote{$\ddagger$}~~(ours) & \textbf{4.33} &  \textbf{4.95} & - \\ \bottomrule
		\end{tabular}
    \begin{tablenotes}
    \item $\ddagger$ pre-trained on the proposed IMDB-Clean 
    \end{tablenotes}
	\end{threeparttable}
	\end{center}
\end{table}

    	
	




\begin{table*}[ht] 
	\caption{Cross-Dataset Evaluation (Training set: IMDB-Clean).}\label{tab:cross-db-res}
	\begin{center}
    		\begin{threeparttable}
		\begin{tabular}{l|c|c|c|c|c|c|c|c}
    		\toprule
            & \multicolumn{2}{c|}{FG-Net~\cite{fgnet2002}} 
            & \multicolumn{2}{c|}{Morph~\cite{ricanekMORPHLongitudinalImage2006}} 
            &
            \multicolumn{2}{c|}{KANFace~\cite{georgopoulosInvestigatingBiasDeep2020}} & \multicolumn{2}{c}{CACD-test~\cite{chenFaceRecognitionRetrieval2015}}
			\\ 
			\midrule \hline
            Method & MAE & CS$_5$(\%) & MAE & CS$_5$(\%) & MAE & CS$_5$(\%) 
            & MAE & CS$_5$(\%) 
            \\ 
            \hline 
			SSRNet\tnote{$\ast$}~~\cite{ssrnet2018} & 12.04 & 19.86 & 7.12 & 40.77 & 11.36 & 30.11 & 11.76&22.01 
			\\
			C3AE\tnote{$\ast$}~~\cite{zhangC3AEExploringLimits2019} & 11.23 
			& 27.34 
			& 7.03 
			& 41.81
			& 10.41
			& 31.71
			& 12.71
			& 16.14
			\\
			SVRT\tnote{$\ast$}~~\cite{imScaleVaryingTripletRanking2019} & 9.77 & 23.75 & 5.87 &43.71 &10.89 & 27.55 & 11.73 & 14.37 
			\\
			DLDL\tnote{$\dagger$}~~\cite{gaoDeepLabelDistribution2017}  & 11.40 & 24.05 &6.07 & 33.06 & 9.84 &37.37 &6.53 &  55.12 
\\
			Coral\tnote{$\ast$}~~\cite{Cao2020coral}    
			& 6.12
			& 45.61
			& 6.13
			& 42.33
			& 7.88
			& 39.01
			& 12.58
			& 11.38
			\\
			Dex\tnote{$\dagger$}~~\cite{rotheDeepExpectationReal2018}  & 6.52 & 41.52 &  5.63 & 53.03 & 7.91 & 42.30 & 6.08 & 55.94 
			\\
			DLDL-V2\tnote{$\dagger$}~~\cite{gaoAgeEstimationUsing2018}   & 6.65 & 42.41  & 5.10 & 55.64 &8.05 & 41.74  & 5.92 & 57.39 
			\\
			M-V Loss\tnote{$\dagger$}~~\cite{panMeanVarianceLossDeep2018}   & 6.49  & 42.12  &4.99 &56.94 & 7.71 & 43.31 & 5.88 & 57.22 
			\\
			OR-CNN\tnote{$\dagger$}~~\cite{niuOrdinalRegressionMultiple2016} 
			& 6.44
			& 40.72
			&  5.04
			& 60.87
			& 7.71
			& 47.51
			& 5.83
			& \textbf{62.47}
			\\
			\hline
			Ours\tnote{$\dagger$}   & \textbf{5.60} &  \textbf{48.80} & \textbf{4.67} & \textbf{60.54} & \textbf{6.81} & \textbf{48.49} & \textbf{5.60} & 60.91 \\ \bottomrule

		\end{tabular}
    	\begin{tablenotes}
    	\item $^\ast$ inputs are pre-processed with 5-point face alignment 
    	\item $^\dagger$ inputs are pre-processed with RoI Tanh-polar Transform~\cite{linRoITanhpolarTransformer2021}
    	\end{tablenotes}
	\end{threeparttable}

	\end{center}
\end{table*}

