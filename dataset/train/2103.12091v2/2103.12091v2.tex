\section{Experimental Evaluation}
\subsection{Datasets}
The {NYU-v2} dataset~\cite{silberman2012indoor} is used to evaluate our approach in the depth estimation task. We use 120K RGB-Depth pairs with a resolution of $480\times640$ pixels, acquired with a Microsoft Kinect device from 464 indoor scenes. We follow the standard train/test split as previous works~\cite{eigen2014depth}, using 249 scenes for training and 215 scenes (654 images) for testing. It is also used to evaluate our approach in the surface normal task, which includes 795 training images and 654 testing images.

The {KITTI} dataset~\cite{Geiger2013IJRR} is a large-scale outdoor dataset created for various autonomous driving tasks. We use it to evaluate the depth estimation performance of our proposed model. Following the standard training/testing split proposed by Eigen~\etal~\cite{eigen2014depth}, we specifically use 22,600 frames from 32 scenes for training, and 697 frames from the rest 29 scenes for testing.

The {ScanNet} dataset~\cite{dai2017scannet} is a large RGB-D dataset for 3D scene understanding. We employ it to evaluate the surface normal performance of our proposed model. ScanNet dataset is divided into 189916 for training and 20942 for test with file lists provided in \cite{dai2017scannet}.

\subsection{Evaluation Metrics}
\par\noindent\textbf{Evaluation Protocol on Monocular Depth Estimation.}
Following the standard evaluation protocol as in previous works~\cite{eigen2015predicting,eigen2014depth,wang2015towards}, the following quantitative evaluation metrics are adopted in our experiments: 
\begin{itemize}
\item Abs relative error (abs-rel): 
\( \frac{1}{K}\sum_{i=1}^K\frac{|\tilde{d}_i - d_i^\star|}{d_i^\star} \); 
\item Squared Relative difference (sq-rel):
\( \frac{1}{K}\sum_{i=1}^K\frac{||\tilde{d}_i - d_i^\star||^2}{d_i^\star} \); 
\item root mean squared error (rms): 
\( \sqrt{\frac{1}{K}\sum_{i=1}^K(\tilde{d}_i - d_i^\star)^2} \);
\item mean log10 error (log-rms): 
\( \sqrt{\frac{1}{K}\sum_{i=1}^K \Vert \log_{10}(\tilde{d}_i) - \log_{10}(d_i^\star) \Vert^2 }\);
\item accuracy with threshold $t$: percentage (\%) of $d_i^\star$, subject to $\max (\frac{d_i^\star}{\tilde{d}_i}, \frac{\tilde{d}_i}{d_i^\star}) = 
\delta < t~(t \in [1.25, 1.25^2, 1.25^3])$.
\end{itemize}
Where $\tilde{d}_i$ and $d_i^\star$ is the ground-truth depth and the estimated depth at pixel $i$ respectively; $K$ is the total number of pixels of the test images.

\par\noindent\textbf{Evaluation Protocol on Surface Normal Estimation.} For the evaluation of surface normal estimation, we utilize five standard evaluation metrics~\cite{fouhey2013data},  such as median angle distance between prediction and ground-truth for valid pixels, and the fraction of pixels with angle difference with ground-truth less than $11.25^{\circ}$ . 

\subsection{Implementation Details} 
The proposed VISTA-Net is implemented in~\textit{Pytorch}. The experiments are conducted on four Nvidia Tesla V100 GPUs, each with 36~GB memory. The ResNet-50 architecture pretrained on ImageNet~\cite{deng2009imagenet} is considered in the experiments for initializing the backbone network of our encoder network. For parameters of transformer, T-layers, Hideen size and attention mluti-head are set to 12, 768 and 12 respectively.

For the the monocular depth estimation task and surface normal prediction task, the learning rate is set to $10^{-4}$ with weight decay of 0.01. The Adam optimizer is used in all our experiments with a batch size of 16 for all tasks. 
The total training epochs are set to 50 for depth prediction and 20 for surface normal.  We train our network on a random crop of size 352$\times$704 for KITTI dataset, 416$\times$512 for NYU dataset for depth prediction while
the input image size is uniformly set to 320$\times$256 for surface normal prediction.

\subsection{Experimental Results and Analysis}

\noindent \textbf{Monocular Depth Estimation.} 
Comparative results on KITTI dataset are shown in Table~\ref{tab:depth_kitti}. We propose a comparison with the state of the art models such as \cite{ranjan2019competitive,bian2019unsupervised,spencer2020defeat,cheng2020s,godard2019digging,tiwari2020pseudo,johnston2020self,klingner2020self,shu2020feature,guizilini20203d,fu2018deep,yin2019enforcing,gonzalez2020forget,lee2019big}. 
Our method performs favorably versus all previous fully- and self-supervised methods, achieving the best results on the majority of the metrics. 
Our approach employs the supervised setting using single monocular images in the training and testing phase.
Specifically, Bts needs to focal length to refine the finial depth output, which is not the same setting as ours and thus not directly comparable to ours. It can support our standpoint that adding linear transformer make networks improve their ability to capture long-range dependencies.
In order to demonstrate the competitiveness of our approach in an indoor scenario, the proposed method is also evaluated by NYU depth dataset. The results are shown in Table~\ref{tab:depth_nyu}, compared with the the state of the art methods like \cite{xu2018pad,li2017two,wang2020cliffnet,laina2016deeper,xu2017multi,lee2020multi,xia2020generating,fu2018deep,lee2019big,yin2019enforcing,huynh2020guiding}.
Similar to the experiments on KITTI, it outperforms both state of the art approaches and previous methods based on attention mechanism \cite{xu2017multi,xu2018pad,huynh2020guiding}. Figure~\ref{fig:vis_kitti} is shown a qualitative comparison of our method with DORN~\cite{fu2018deep}. The significant improvement parts are marked by the red box. 
Results indicate that our method generates more precise boundaries from distant stuff like vehicles and traffic signs and near stuff like humans.
Fig.~\ref{fig:vis_nyu} shows a similar comparison done on NYU dataset. Owing to applying transformer, the corners of the room are more distinguishable. It can support our standpoint that adapting linear transformer make the CNN backbone network enhance the ability to capture long-range dependencies. 

\begin{table*}[]
\centering
\caption{Depth Estimation: KITTI dataset.K: KITTI. CS: CityScapes~\cite{cordts2016cityscapes}. CS$\to$K: CS pre-training. D: Depth supervision. M, Se, V, S: Monocular, segmentation, video, stereo.}
\label{tab:depth_kitti}
\resizebox{0.8\textwidth}{!}{\begin{tabular}{lccccccccc}
\toprule[1.2pt]
\multirow{2}{*}{Method}& \multirow{2}{*}{Sup} & \multirow{2}{*}{Data} & \multicolumn{4}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\ \cmidrule{4-10} 
 & &  & abs rel & sq rel & rms & log rms & $\delta \textless 1.25$ & $\delta \textless 1.25^2$ & $\delta \textless 1.25^3$ \\
\midrule
CC~\cite{ranjan2019competitive}& M+Se & K & 0.140 & 1.070 & 5.326 & 0.217 & 0.826 & 0.941 & 0.975 \\
Bian~\etal \cite{bian2019unsupervised}& M+V & K+CS & 0.137 & 1.089 & 5.439 & 0.217 & 0.830 & 0.942 & 0.975 \\
DeFeat~\cite{spencer2020defeat}& M & K & 0.126 & 0.925 & 5.035 & 0.200 & 0.862 & 0.954 & 0.980 \\
$S^3$Net~\cite{cheng2020s}& M+Se & K & 0.124 & 0.826 & 4.981 & 0.200 & 0.846 & 0.955 & 0.982 \\
Monodepth2~\cite{godard2019digging}& M & K & 0.115 & 0.903 & 4.863 & 0.193 & 0.877 & 0.959 & 0.981 \\
pRGBD~\cite{tiwari2020pseudo}& M & K & 0.113 & 0.793 & 4.655 & 0.188 & 0.874 & 0.960 & 0.983 \\
Johnston~\etal \cite{johnston2020self}& M & K & 0.106 & 0.861 & 4.699 & 0.185 & 0.889 & 0.962 & 0.982 \\
SGDepth~\cite{klingner2020self}& M+Se & K+CS & 0.107 & 0.768 & 4.468 & 0.180 & 0.891 & 0.963 & 0.982 \\
Shu~\etal \cite{shu2020feature}& M & K & 0.104 & 0.729 & 4.481 & 0.179 & 0.893 & 0.965 & 0.984 \\
DORN~\cite{fu2018deep}& D & K & 0.072 & 0.307 & 2.727 & 0.120 & 0.932 & 0.984 & 0.994 \\
Yin~\etal \cite{yin2019enforcing}& M & K & 0.072 & - & 3.258 & 0.117 & 0.938 & 0.990 & 0.998 \\
PackNet~\cite{guizilini20203d}& V & K+CS & 0.071 & 0.359 & 3.153 & 0.109 & 0.944 & 0.990 & 0.997 \\
FAL-net~\cite{gonzalez2020forget}& S & K+CS & 0.068 & 0.276 & 2.906 & 0.106 & 0.944 & 0.991 & 0.998 \\
Bts \cite{lee2019big}& M & K & 0.061 & 0.261 & 2.834 & 0.099 & 0.954 & 0.992 & 0.998 \\
 \midrule
Baseline & M & K & 0.106 & 0.753 &  3.981 & 0.104 & 0.888  & 0.967  & 0.986\\
ours w/ VIT& M & K & 0.064 & 0.258 & 2.761 & 0.099 & 0.955 & 0.993 & 0.999\\
ours w/ AGD+VIT(full)& M & K & \textbf{0.064} & \textbf{0.252} & \textbf{2.755} & \textbf{0.098} & \textbf{0.956} & \textbf{0.994} & \textbf{0.999} \\
\bottomrule[1.2pt]

\end{tabular}}
\end{table*}

\begin{figure*}[!t]
\centering
\subfigure[Image]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img8.jpg}\\


    \end{minipage}}\subfigure[GT]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt8.jpg}\\ 


    \end{minipage}}\subfigure[DORN]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn8.jpg}\\  


    \end{minipage}}\subfigure[ours w/ AGD]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans8.jpg}\\ 
\end{minipage}}\subfigure[Ours(full)]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours8.jpg}\\
\end{minipage}}\centering
\caption{Qualitative examples on KITTI dataset.}
\label{fig:vis_kitti}
\end{figure*}




\begin{table}[]
\centering
\caption{Depth Estimation: NYU dataset.}
\resizebox{1\linewidth}{!}{
\label{tab:depth_nyu}
\begin{tabular}{lcccccc}
\toprule[1.2pt]
\multirow{2.5}{*}{Method} & \multicolumn{3}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\ \cmidrule{2-7} 
 & rel & log10 & rms  & $\delta \textless 1.25$ & $\delta \textless 1.25^2$ & $\delta \textless 1.25^3$ \\ 
 \midrule
    PAD-Net~\cite{xu2018pad} & 0.214 & 0.091 & 0.792 & 0.643 & 0.902 & 0.977 \\
    Li~\etal~\cite{li2017two}& 0.152 & 0.064 & 0.611 & 0.789 & 0.955 & 0.988\\
    CLIFFNet~\cite{wang2020cliffnet} & 0.128 &	0.171 &	0.493 &	0.844 &	0.964 &	0.991\\
    Laina~\etal \cite{laina2016deeper} & 0.127 & 0.055 & 0.573 & 0.811 & 0.953 & 0.988\\
    MS-CRF~\cite{xu2017multi} & 0.121 & 0.052 & 0.586 & 0.811 & 0.954 & 0.987\\
    Lee~\etal \cite{lee2020multi} & 0.119 & 0.050 &	- &	0.870 &	0.974 & 0.993\\
    Xia~\etal \cite{xia2020generating} & 0.116 & - & 0.512 & 0.861 & 0.969 & 0.991 \\
    DORN~\cite{fu2018deep} & 0.115 & 0.051 & 0.509 & 0.828 & 0.965 & 0.992 \\
    Bts \cite{lee2019big} & 0.113 & 0.049 & 0.407 & 0.871 & 0.977 & 0.995 \\
Yin~\etal \cite{yin2019enforcing} & 0.108 & 0.048 & 0.416 & 0.875 & 0.976 & 0.994 \\
    Huynh~\etal \cite{huynh2020guiding} & 0.108 & -	& 0.412 & 0.882 & 0.980 & \textbf{0.996}\\
    \midrule
    baseline & 0.118 & 0.051 & 0.414 & 0.866 & 0.979 & 0.995 \\
ours w/ VIT & 0.109 & 0.047 & 0.388 & 0.887 & 0.981 & 0.996\\
    ours w/ AGD+ VIT(full)  & \textbf{0.106} & \textbf{0.045} & \textbf{0.365} & \textbf{0.900} & \textbf{0.983} & \textbf{0.996}\\
\bottomrule[1.2pt]
\end{tabular}}
\end{table}

\begin{figure*}[!t]
\centering
\subfigure[Image]{
    \begin{minipage}{0.19\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img1.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/img8.jpg}\\
    \end{minipage}}\subfigure[GT]{
    \begin{minipage}{0.19\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt1.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/gt8.jpg}\\ 
    \end{minipage}}\subfigure[DORN]{
    \begin{minipage}{0.19\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn1.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/dorn8.jpg}\\  
    \end{minipage}}\subfigure[ours w/ AGD]{
    \begin{minipage}{0.19\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans1.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/wotrans8.jpg}\\ 
    \end{minipage}}\subfigure[Ours(full)]{
    \begin{minipage}{0.19\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours1.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.8in]{images/nyu_depth_vis/ours8.jpg}\\ 
    \end{minipage}}\centering
\caption{ Qualitative examples on NYU dataset.}
\label{fig:vis_nyu}
\end{figure*}


\noindent\textbf{Surface Normal Estimation.}
We compare VISTA-Net with the state-of-the-art RGB-based methods, including GeoNe~\cite{qi2018geonet}, VPLNet~\cite{wang2020vplnet}, FrameNet~\cite{huang2019framenet} and Do~\etal~\cite{do2020surface}.
For fair comparison, we report our result in two different train condition. Because of limited space, only median angle and $11.25^{\circ}$ are compared in Table~\ref{tab:sn_nyu} while detail comparison is shown in supplementary. Our method outperforms the state-of-the-art on median angle and $11.25^{\circ}$. 


\begin{table}[]
\caption{Surface Normal Estimation: NYU dataset.}
\label{tab:sn_nyu}
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{l|c|c|cc}
\toprule[1.2pt]
method & Training Data & Testing Data & median $\downarrow$ & $11.25^{\circ} \uparrow$ \\
\midrule
Li~\etal \cite{li2015depth} & \multirow{7}{*}{NYU} & \multirow{11}{*}{NYU} & 27.8 & 19.6 \\
Chen~\etal \cite{chen2017surface} &  &  & 15.8 & 39.2 \\
Eigen~\etal \cite{eigen2015predicting} &  &  & 13.2 & 44.4 \\
SURGE \cite{wang2016surge} &  &  & 12.2 & 47.3 \\
Bansal~\etal \cite{bansal2016marr} &  &  & 12.0 & 47.9 \\
GeoNet~\cite{qi2018geonet} &  &  & 12.5 & 46.0 \\
Ours &  &  & \textbf{11.8} & \textbf{48.2} \\\cmidrule{1-2} \cmidrule{4-5} 
FrameNet~\cite{huang2019framenet} & \multirow{4}{*}{Scannet} &  & 11.0 & 50.7 \\
VPLNet~\cite{wang2020vplnet} &  &  & 9.8 & 54.3 \\
Do~\cite{do2020surface} &  &  & \textbf{8.1} & 59.8 \\
Ours &  &  & \textbf{7.8} & \textbf{61.7}\\
\bottomrule[1.2pt]
\end{tabular}}
\end{table}

\begin{figure*}[!t]
\centering

\centering
\caption{ Qualitative examples on NYU surface normal dataset.}
\label{fig:vis_nyu_sn}
\end{figure*}








\par\noindent\textbf{Ablation Study }

\begin{table}[]
\caption{ab: NYU.}
\label{tab:ab_scale}
\resizebox{\linewidth}{!}{\begin{tabular}{llcccccc}
\toprule[1.2pt]
 &  & \multicolumn{3}{l}{Error (lower is better)} & \multicolumn{3}{l}{Accuracy (higher is better)} \\
 \cmidrule{3-8}
\multirow{-2}{*}{$f_e$} & \multirow{-2}{*}{$f_r$} & rel & log10 & rms & a1 & a2 & a3 \\
\midrule

- & - & 0.118 & 0.051 & 0.414 & 0.866 & 0.979 & 0.995 \\
$f_4$ & $f_5 $ & 0.108 & \textbf{0.045} & 0.366 & 0.897 & 0.982 & 0.996 \\
$f_3,f_4$ & $f_5 $ & \textbf{0.106} & \textbf{0.045} & \textbf{0.365} & \textbf{0.900} & \textbf{0.983} & \textbf{0.996} \\
$f_2,f_3,f_4$ & $f_5 $ & 0.107 & \textbf{0.045} & 0.366 & 0.899 & \textbf{0.983} & \textbf{0.996}\\
\bottomrule[1.2pt]
\end{tabular}}
\end{table}

\begin{figure}[!t]
\centering
\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att_img1.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att1_img1.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att1_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att1_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att1_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att2_img1.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att2_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att2_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att2_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att3_img1.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att3_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att3_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.5in]{images/att/nyu_att3_img5.jpg}\\
\end{minipage}}\centering
\caption{Qualitative attention examples of monocular depth prediction on the NYU dataset. First column is original image and next three columns is different fusion attention }
\label{fig:vis_att}
\end{figure}