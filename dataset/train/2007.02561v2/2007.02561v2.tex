\section{Experiments}
\label{sec:experiment}
In this section, we demonstrate the effectiveness of our LfF algorithm proposed in Section~\ref{sec:method}, and introduce our newly constructed biased action recognition dataset, coined BAR. 
All experimental results in this section support that LfF successfully trains a debiased classifier, with the knowledge that the bias attribute is learned earlier than the target attribute (instead of explicit supervision for the bias present in the dataset).
The classifiers trained by LfF consistently outperforms the vanilla classifiers (trained without any debiasing procedure) on both the unbiased and bias-conflicting evaluation set. 

For the experiments in this section, we use MLP with three hidden layers, ResNet-20, and ResNet-18 \citep{he2016deep} for the Colored MNIST, Corrupted CIFAR-10, and \{CelebA, BAR\} datasets, respectively. All results reported in this section are averaged over three independent trials. We provide a detailed description of datasets we considered in Appendix~\ref{app:datasets}, \ref{app:biased_action_dataset} and experimental details in Appendix~\ref{app:experimental}.

\subsection{Controlled experiments}

\begin{table}[t!]
\centering
\caption{
Accuracy evaluated on unbiased samples for the Colored MNIST and Corrupted CIFAR-10$^{1,2}$ datasets with varying ratio of bias-aligned samples. We denote bias supervision type by \protect\emptycircle{0.1} (no supervision), \protect\smpriority{50} (bias-tailored supervision), and \protect\smpriority{100} (explicit bias supervision). Best performing results are marked in bold.
}
\label{tab:ratio}
\scalebox{0.88}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2.5}{*}{Dataset} 
& \multirow{2.5}{*}{Ratio (\%)} 
& Vanilla 
& Ours
&
& HEX
& REPAIR
& Group DRO
\\
\cmidrule{3-4}
\cmidrule{6-8}
&
& \emptycircle{0.15}
& \emptycircle{0.15}
&  
& \priority{50}
& \priority{100}
& \priority{100}
\\
\midrule
\multirow{4}{*}{\makecell{Colored\\MNIST}} 
& 95.0 
& 77.63\stdv{0.44}
& \textbf{85.39}\stdv{0.94}
&  
& 70.44\stdv{1.41}
& 82.51\stdv{0.59}
& 84.50\stdv{0.46}
\\
& 98.0 
& 62.29\stdv{1.47}
& \textbf{80.48}\stdv{0.45}
&  
& 62.03\stdv{0.24}
& 72.86\stdv{1.47}
& 76.30\stdv{1.53}
\\
& 99.0 
& 50.34\stdv{0.16}
& \textbf{74.01}\stdv{2.21}
&  
& 51.99\stdv{1.09}
& 67.28\stdv{1.69}
& 71.33\stdv{1.76}
\\
& 99.5 
& 35.34\stdv{0.13}
& \textbf{63.39}\stdv{1.97}
&  
& 41.38\stdv{1.31}
& 56.40\stdv{3.74}
& 59.67\stdv{2.73}
\\
\midrule
\multirow{4}{*}{\makecell{Corrupted\\CIFAR-10$^1$}} 
& 95.0 
& 45.24\stdv{0.22}
& \textbf{59.95}\stdv{0.16}
&  
& 21.74\stdv{0.27}
& 48.74\stdv{0.71}
& 53.15\stdv{0.53}
\\
& 98.0 
& 30.21\stdv{0.82}
& \textbf{49.43}\stdv{0.78}
&  
& 17.81\stdv{0.29}
& 37.89\stdv{0.22}
& 40.19\stdv{0.23}
\\
& 99.0 
& 22.72\stdv{0.87}
& \textbf{41.37}\stdv{2.34}
&  
& 16.62\stdv{0.80}
& 32.42\stdv{0.35}
& 32.11\stdv{0.83}
\\
& 99.5 
& 17.93\stdv{0.66}
& \textbf{31.66}\stdv{1.18}
&  
& 15.39\stdv{0.13}
& 26.26\stdv{1.06}
& 29.26\stdv{0.11}
\\
\midrule
\multirow{4}{*}{\makecell{Corrupted\\CIFAR-10$^2$}} 
& 95.0 
& 41.27\stdv{0.98}
& \textbf{58.57}\stdv{1.18}
&  
& 19.25\stdv{0.81}
& 54.05\stdv{1.01}
& 57.92\stdv{0.31}
\\
& 98.0 
& 28.29\stdv{0.62}
& \textbf{48.75}\stdv{1.68}
&  
& 15.55\stdv{0.84}
& 44.22\stdv{0.84}
& 46.12\stdv{1.11}
\\
& 99.0 
& 20.71\stdv{0.29}
& \textbf{41.29}\stdv{2.08}
&  
& 14.42\stdv{0.51}
& 38.40\stdv{0.26}
& 39.57\stdv{1.04}
\\
& 99.5 
& 17.37\stdv{0.31}
& 34.11\stdv{2.39}
&  
& 13.63\stdv{0.42}
& 31.03\stdv{0.42}
& \textbf{34.25}\stdv{0.74}
\\
\bottomrule
\end{tabular}
}
\end{table} \begin{table}[t!]
\centering
\caption{
Accuracy evaluated on bias-conflicting samples for the Colored MNIST and Corrupted CIFAR-10$^{1,2}$ datasets with varying ratio of bias-aligned samples. We denote bias supervision type by \protect\emptycircle{0.1} (no supervision), \protect\smpriority{50} (bias-tailored supervision), and \protect\smpriority{100} (explicit bias supervision). Best performing results are marked in bold.
}
\label{tab:ratio_skew}
\scalebox{0.88}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2.5}{*}{Dataset} 
& \multirow{2.5}{*}{Ratio (\%)} 
& Vanilla 
& Ours
&
& HEX
& REPAIR
& Group DRO
\\
\cmidrule{3-4}
\cmidrule{6-8}
&
& \emptycircle{0.15}
& \emptycircle{0.15}
&  
& \priority{50}
& \priority{100}
& \priority{100}
\\
\midrule
\multirow{4}{*}{\makecell{Colored\\MNIST}} 
& 95.0 
& 75.17\stdv{0.51}
& \textbf{85.77}\stdv{0.66}
&  
& 67.75\stdv{1.49}
& 83.26\stdv{0.42}
& 83.11\stdv{0.41}
\\
& 98.0 
& 58.13\stdv{1.63}
& \textbf{80.67}\stdv{0.56}
&  
& 58.80\stdv{0.28}
& 73.42\stdv{1.42}
& 74.28\stdv{1.93}
\\
& 99.0 
& 44.83\stdv{0.18}
& \textbf{74.19}\stdv{1.94}
&  
& 46.96\stdv{1.20}
& 68.26\stdv{1.52}
& 69.58\stdv{1.66}
\\
& 99.5 
& 28.15\stdv{1.44}
& \textbf{63.49}\stdv{1.94}
&  
& 35.05\stdv{1.46}
& 57.27\stdv{3.92}
& 57.07\stdv{3.60}
\\
\midrule
\multirow{4}{*}{\makecell{Corrupted\\CIFAR-10$^1$}} 
& 95.0 
& 39.42\stdv{0.20}
& \textbf{59.62}\stdv{0.03}
&  
& 14.09\stdv{0.31}
& 49.99\stdv{0.92}
& 49.00\stdv{0.45}
\\
& 98.0 
& 22.65\stdv{0.95}
& \textbf{48.69}\stdv{0.70}
&  
& 9.34\stdv{0.41}
& 38.94\stdv{0.20}
& 35.10\stdv{0.49}
\\
& 99.0 
& 14.24\stdv{1.03}
& \textbf{39.55}\stdv{2.56}
&  
& 8.37\stdv{0.56}
& 33.05\stdv{0.36}
& 28.04\stdv{1.18}
\\
& 99.5 
& 10.50\stdv{0.71}
& \textbf{28.61}\stdv{1.25}
&  
& 6.38\stdv{0.08}
& 26.52\stdv{0.94}
& 24.40\stdv{0.28}
\\
\midrule
\multirow{4}{*}{\makecell{Corrupted\\CIFAR-10$^2$}} 
& 95.0 
& 34.97\stdv{1.06}
& \textbf{58.64}\stdv{1.04}
&  
& 10.79\stdv{0.90}
& 54.46\stdv{1.02}
& 54.60\stdv{0.11}
\\
& 98.0 
& 20.52\stdv{0.73}
& \textbf{48.99}\stdv{1.61}
&  
& 6.60\stdv{7.23}
& 44.63\stdv{0.75}
& 42.71\stdv{1.24}
\\
& 99.0 
& 12.11\stdv{0.29}
& \textbf{40.84}\stdv{2.06}
&  
& 5.11\stdv{0.59}
& 38.81\stdv{0.20}
& 37.07\stdv{1.02}
\\
& 99.5 
& 10.01\stdv{0.01}
& \textbf{32.03}\stdv{2.51}
&  
& 4.22\stdv{0.43}
& 31.45\stdv{0.28}
& 30.92\stdv{0.86}
\\
\bottomrule
\end{tabular}
}
\end{table} 



\textbf{Baselines.}
For controlled experiments, we compare the performance of our algorithm with other debiasing algorithms, either presuming a particular bias type or requiring access to additional labels containing information about bias attributes.
We consider HEX proposed by \citet{wang2018learning}, which attempts to remove texture bias using the bias-specific knowledge. We also consider REPAIR and Group DRO proposed by \citet{li2019repair} and \citet{sagawa2019distributionally}, requiring explicit labeling of the bias attributes. We provide a detailed description of the employed baselines in Appendix \ref{app:baselines}.

\textbf{Ratio of the bias-aligned samples.} 
In the first set of experiments, we vary the ratio of bias-aligned samples in the training dataset by selecting from $\{95.0\%, 98.0\%, 99.0\%, 99.5\%\}$. We experiment on Colored MNIST, and Corrupted CIFAR-10$^{1,2}$ datasets with $(a_t, a_b) = (\texttt{Digit}, \texttt{Color})$, and $(a_t, a_b) = (\texttt{Object}, \texttt{Corruption})$, respectively. 

In Table~\ref{tab:ratio}, \ref{tab:ratio_skew}, we report the accuracy evaluated on unbiased samples and bias-conflicting samples. We observe that the proposed method significantly outperforms the baseline on all levels of ratio for bias-aligned samples. Most notably, LfF achieves 41.37\% accuracy on the unbiased evaluation set for the Corrupted CIFAR-10$^{1}$ dataset with 99\% bias-aligned samples, while the vanilla model only achieves 22.72\%. In addition, we report the accuracy of unbiased evaluation set and bias-conflicting samples with varying levels of difficulty for the bias attributes in Appendix~\ref{app:additional}.





\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{figure/ablation_legends.png}
    \\
    \subfigure[Bias-\{aligned, conflicting\} Colored MNIST]{
        \includegraphics[width=0.235\textwidth]{figure/mnist_accuracy_aligned_wo_legend.png}
        \includegraphics[width=0.235\textwidth]{figure/mnist_accuracy_skewed_wo_legend.png}
    }
    \subfigure[Bias-\{aligned, conflicting\} Corrupted CIFAR-10$^{1}$]{
        \includegraphics[width=0.235\textwidth]{figure/cifar_accuracy_aligned_wo_legend.png}
        \includegraphics[width=0.235\textwidth]{figure/cifar_accuracy_skewed_wo_legend.png}
    }
    \vspace{-.1in}
    \caption{
    Learning curves of vanilla, biased, and debiased model for Colored MNIST and Corrupted CIFAR-10$^{1}$. For each dataset, the left and the right plots correspond to curves for the bias-aligned samples and the bias-conflicting samples, respectively. }
    \vspace{-.15in}
    \label{fig:learning_curve}
\end{figure*}   

 
\textbf{Detailed analysis of failure-based debiasing.}
We further analyze the specific details of our algorithm. To be precise, we first investigate the accuracies of the vanilla model and the biased, debiased models $f_{B}, f_{D}$ trained by our algorithm for the bias-aligned and bias-conflicting samples. In Figure~\ref{fig:learning_curve}, we plot the training curves of each model. We observe both the vanilla model and the biased model easily achieve 100\% accuracy for the bias-aligned samples. On the other hand, the vanilla model shows 50\% accuracy for the bias-conflicting samples, and the biased model performs close to random guessing. From this result, we can say that our intentionally biased model only exploits the bias attribute without learning the target attribute. 

To compare our debiased model to the vanilla model, we start by observing the accuracy gap between the bias-aligned and bias-conflicting samples. As described before, while the vanilla model easily achieves 100\% accuracy for the bias-aligned samples, it cannot achieve similar performance for the bias-conflicting samples. In contrast, our debiased model shows consistent performance (about 80\%) for both the bias-aligned and bias-conflicting samples. As a result, it indicates that our debiased model successfully learns the \textit{intended} target attribute, while the predictions of the vanilla model heavily rely on the \textit{unintended} bias attribute.

\begin{table}[t!]
\centering
\caption{
Accuracy evaluated on the unbiased samples and bias-conflicting samples of the Colored MNIST and Corrupted CIFAR-10$^{1, 2}$ dataset for ablation of the proposed algorithm. We denote our algorithm using vanilla model as biased model by Ours$^\dagger$. Best performing results are marked in bold.
}
\label{tab:ablation}
\scalebox{0.88}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2.5}{*}{Dataset} 
& 
& \multicolumn{3}{c}{Unbiased} 
&
& \multicolumn{3}{c}{Bias-conflicting} 
\\
\cmidrule{3-5}
\cmidrule{7-9}
& 
& Vanilla
& Ours$^\dagger$
& Ours
&  
& Vanilla 
& Ours$^\dagger$
& Ours
\\
\midrule
Colored MNIST
& 
& 50.34\stdv{0.16}
& 49.90\stdv{1.67}
& \textbf{74.01}\stdv{2.21}
&  
& 44.83\stdv{0.18}
& 44.44\stdv{1.83}
& \textbf{74.19}\stdv{1.94}
\\
Corrupted CIFAR-10$^1$
& 
& 22.72\stdv{0.87}
& 25.15\stdv{0.63}
& \textbf{41.37}\stdv{2.34}
&  
& 14.24\stdv{1.03}
& 17.21\stdv{0.69}
& \textbf{39.55}\stdv{2.56}
\\
Corrupted CIFAR-10$^2$
& 
& 20.71\stdv{0.29}
& 22.90\stdv{0.47}
& \textbf{41.29}\stdv{2.08}
&  
& 12.11\stdv{0.29}
& 14.89\stdv{0.66}
& \textbf{40.84}\stdv{2.06}
\\
\bottomrule
\end{tabular}
}
\vspace{-.1in}
\end{table}
 
\textbf{Contribution of GCE loss.}
We also test a variant of our algorithm, where we train the biased model with standard cross entropy instead of GCE. As observed in Figure~\ref{fig:learning_curve}, the model trained with standard cross entropy, denoted by Vanilla, not only exploits the bias attribute but also partially learns the target attribute. Therefore, one can expect that using such a CE-trained model as the biased model can hurt debiasing ability of our algorithm. In Table~\ref{tab:ablation}, we report the test performance of our debiased model trained with the CE-trained biased model instead of the GCE-trained model, denoted as Ours$^\dagger$. As expected, using the CE-trained model to compute relative difficulty does not help debiasing model. 




\subsection{Real-world experiments}

\begin{table}[t!]
\centering
\caption{
Accuracy evaluated on the unbiased samples and bias-conflicting samples for the CelebA dataset with \texttt{Gender} as the bias attribute.
}
\label{tab:celeba}
\scalebox{0.88}{
\begin{tabular}{lcccccccc}
\toprule
\multirow{2.5}{*}{Target attribute} 
& 
& \multicolumn{3}{c}{Unbiased} 
&
& \multicolumn{3}{c}{Bias-conflicting} 
\\
\cmidrule{3-5}
\cmidrule{7-9}
& 
& Vanilla
& Ours 
& Group DRO
&  
& Vanilla 
& Ours 
& Group DRO
\\
\midrule
\texttt{HairColor}
& 
& 70.25\stdv{0.35}
& 84.24\stdv{0.37}
& 85.43\stdv{0.53}
&  
& 52.52\stdv{0.19}
& 81.24\stdv{1.38}
& 83.40\stdv{0.67}
\\
\texttt{HeavyMakeup}
& 
& 62.00\stdv{0.02}
& 66.20\stdv{1.21}
& 64.88\stdv{0.42}
& 
& 33.75\stdv{0.28}
& 45.48\stdv{4.33}
& 50.24\stdv{0.68}
\\
\bottomrule
\end{tabular}
}
\end{table}


%
 \textbf{CelebA.}
The CelebA dataset \citep{liu2015faceattributes} is a multi-attribute dataset for face recognition, equipped with 40 types of attributes for each image. Among 40 attributes, we find that \texttt{Gender} and \texttt{HairColor} attributes have a high correlation. Moreover, we observe the attribute \texttt{Gender} is used as a cue for predicting the attribute \texttt{HairColor}. Therefore, we use \texttt{HairColor} 
as the target and \texttt{Gender} 
as the bias attribute. Similarly, we do the same thing for \texttt{HeavyMakeup} as the target and \texttt{Gender} as the bias attribute.



In Table~\ref{tab:celeba}, we observe our algorithm consistently outperforms the vanilla model while the vanilla model suffers from gender bias existing in the real-world dataset. The accuracy gap between the vanilla model and our model is larger for the bias-conflicting samples, which indicates the vanilla model fails to learn the intended target attribute, instead exploits the biased statistic of the dataset. 
Notably, our model is comparable to Group DRO, which requires explicit labeling for the bias attribute (unlike ours), and even outperforms on the unbiased evaluation set when the target attribute is \texttt{HeavyMakeup}. This is because Group DRO aims to maximize the worst-case group accuracy, not overall unbiased accuracy.




\begin{table}[t!]
\centering
\caption{
Class-wise action recognition accuracy on BAR evaluation set. Best performing results are marked in bold.
}
\label{tab:action}
\scalebox{0.88}{
\begin{tabular}{lccccccccc}
\toprule
\texttt{Action} & & \texttt{Climbing} & \texttt{Diving} & \texttt{Fishing} & \texttt{Racing} & \texttt{Throwing} & \texttt{Vaulting} & & {Average} 
\\
\cmidrule{1-8} \cmidrule{10-10}
Vanilla
& 
& 59.05\stdv{17.48}
& 16.56\stdv{1.58} 
& 62.69\stdv{3.64} 
& 77.27\stdv{2.62} 
& 28.62\stdv{2.95} 
& 66.92\stdv{7.25} 
& & 51.85\stdv{5.92}




\\

ReBias
&
& {77.78\stdv{8.32}}
& {\textbf{51.57}\stdv{4.54}}
& {54.76\stdv{2.38}}
& {80.56\stdv{2.19}}
& {28.63\stdv{2.71}}
& {65.14\stdv{7.21}}
& & {59.74\stdv{1.49}}

\\

Ours
&
& \textbf{79.36}\stdv{4.79} 
& 34.59\stdv{2.26} 
& \textbf{75.39}\stdv{3.63} 
& \textbf{83.08}\stdv{1.90} 
& \textbf{33.72}\stdv{0.68} 
& \textbf{71.75}\stdv{3.32} 
& & \textbf{62.98}\stdv{2.76}



\\
\bottomrule
\end{tabular}
}
\end{table} 


\textbf{Biased action recognition dataset.}
To verify effectiveness of our proposed scheme in a realistic setting, we construct a place-biased action recognition (BAR) dataset with training and evaluation set. We settle six typical action-place pairs by inspecting imSitu dataset \citep{yatskar2016Imsitu}, which provides action and place labels. We assign images describing these six typical action-place pairs to the training set and, otherwise, the evaluation set of BAR.
BAR is publicly available\footnote{\url{https://github.com/alinlab/BAR}}, and a detailed description of BAR is in Appendix~\ref{app:biased_action_dataset}.

BAR aims to resolve the lack of realistic evaluation benchmark for debiasing schemes. Since previous debiasing schemes have tackled certain types of bias, they have assumed the correlation between the target and bias attribute to be tangible, which indeed is not available in the case of real-world settings. Such an assumption also makes it hard to verify whether one's scheme can be applied to a wide range of realistic settings. BAR instead offers evaluation set which is constructed based on the intuition that ``a majority of samples that do not match typical action-place pairs are bias-conflicting.'' To demonstrate, the evaluation set consists of samples not matching the settled six typical pairs.
In the end, we can use this evaluation set, which would be similar to a set of bias-conflicting samples to verify our algorithm's effectiveness.

Table~\ref{tab:action} illustrates the test accuracy on the BAR evaluation set. LfF outperforms the vanilla classifier for all action classes. This indicates that LfF encourages the model to train on relatively hard training samples, thereby leading to debiasing the model. 
In addition, LfF outperforms ReBias \citep{bahng2019learning}, which is also free from explicit labeling on the bias attribute, for most action classes except \texttt{Diving}.
























\begin{figure*}[t!]
\centering
    
    \subfigure[\texttt{Climbing}]{
        \includegraphics[width=0.145\textwidth]{figure/climbing_2x2.png}
    }
    \subfigure[\texttt{Diving}]{
        \includegraphics[width=0.145\textwidth]{figure/diving_2x2.png}
    }
    \subfigure[\texttt{Fishing}]{
        \includegraphics[width=0.145\textwidth]{figure/fishing_2x2.png}
    }
    \subfigure[\texttt{Racing}]{
        \includegraphics[width=0.145\textwidth]{figure/racing_2x2.png}
    }
    \subfigure[\texttt{Throwing}]{
        \includegraphics[width=0.145\textwidth]{figure/throwing_2x2.png}
    }
    \subfigure[\texttt{Vaulting}]{
        \includegraphics[width=0.145\textwidth]{figure/pole_vaulting_2x2.png}
    }
    
    
    \caption{
    Illustration of BAR images of six typical action-place pairs settled as (\texttt{Climbing}, \texttt{RockWall}), (\texttt{Diving}, \texttt{Underwater}), (\texttt{Fishing}, \texttt{WaterSurface}), (\texttt{Racing}, \texttt{APavedTrack}), (\texttt{Throwing}, \texttt{PlayingField}), and (\texttt{Vaulting}, \texttt{Sky}). The images with red border lines belong to BAR evaluation set, and others belong to BAR training set.
    }
\label{fig:action_place_combination}
\end{figure*}
 




