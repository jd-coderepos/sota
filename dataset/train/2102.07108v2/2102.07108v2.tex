\section{Experiments}
\label{sec.experiments} 


\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{Figure_encodings_comparison_crop.pdf}
\caption{Comparison between CATE and other architecture encoding schemes under different subroutines on NAS-Bench-101: \emph{sample random architecture} (top left), \emph{perturb architecture} (top middle, top right), and \emph{train predictor model}
(bottom left, bottom middle, bottom right). It reports the test error of $200$ independent runs given $150$ queried architectures.}
	\label{fig.nas_encodings_comprison}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{Figure_sota_comparison_crop.pdf}
\caption{Comparison between CATE and SOTA NAS methods on NAS-Bench-101 (left) and NAS-Bench-301 (right). It reports the test error of $200$ independent runs. The error bars denote the variance of the test error. The number of queried architectures is set to $150$ for NAS-Bench-101 and $100$ for NAS-Bench-301.}
	\label{fig.nas_sota_comprison}
\end{figure*}

We describe two NAS benchmarks used in our experiments.

\textbf{NAS-Bench-101.}
The NAS-Bench-101 search space \cite{pmlr-v97-ying19a} consists of  $423,624$ architectures. Each architecture has its pre-computed validation and test accuracies on CIFAR-10. The cell includes up to $7$ nodes and at most $9$ edges with the first node as input and the last node as output. The intermediate nodes can be either 1$\times$1 convolution, 3$\times$3 convolution, or 3$\times$3 max pooling. We use the number of network parameters as the computational attribute $\mathbf{P}$ for architecture pair sampling. We set $\delta$ to $2,000,000$ and $K$ to 2. The ablation studies on $\delta$ and $K$ are summarized in Section \ref{sec:ablation}. We split the dataset into 95\% training and 5\% held-out test sets for pre-training.

\textbf{NAS-Bench-301.}
NAS-Bench-301 \cite{julien2020nas301} is a new surrogate benchmark on the DARTS \cite{liu2018darts} search space that is much larger than NAS-Bench-101. It 
was created by fully training $60,000$ architectures that is \emph{stratified by the NAS methods}\footnote{We suggest referring to C.2 in ~\citep{julien2020nas301} for a detailed description on the data collection.} with a good coverage and then fitting a surrogate model that can estimate the accuracy (with noise) at epoch $100$ and the training time for any of the remaining $10^{18}$ architectures.
To convert the DARTS search space into one with the same input format as NAS-Bench-101, we add a summation node to make nodes represent operations and edges represent data flow. Following \cite{liu2018progressive}, we use the same cell for both normal and reduction cell, allowing roughly $10^9$ DAGs without considering graph isomorphism. More details about the DARTS/NAS-Bench-301 and a cell transformation example are included in Appendix \ref{sec.301-cell}. 
We randomly sample $1,000,000$ architectures in this search space, and use the same data split used in NAS-Bench-101 for pre-training. We use network FLOPs as the computational attribute $\mathbf{P}$ for architecture pair sampling. We set $\delta$ to $5,000,000$ and $K$ to 1. Since some NAS methods we compare against use the same GIN \cite{xu2018how} surrogate model used in NAS-Bench-301, to ensure fair comparison, we thus followed \cite{julien2020nas301} to use XGB-v1.0 and LGB-runtime-v1.0 which utilizes gradient boosted trees \cite{Chen:2016:XST:2939672.2939785,NIPS2017_6449f44a} as the regression model. 

\textbf{Model and Training.}
We use a $L = 12$ layer Transformer encoder and a $L_c = 24$ layer cross-attention Transformer encoder, each has 8 attention heads. The hidden state size is $d_h =d_c = 64$ for all the encoders. The hidden dimension is $d_{ff} = 64$ for all the feed-forward layers.
We employ AdamW \cite{loshchilov2017decoupled} as our optimizer.
The initial learning rate is 1e-3.
The momentum parameters are set to 0.9 and 0.999.
The weight decay is 0.01 for regular layer and 0 for dropout and layer normalization.
We trained our model with batch size of 1024 on NVIDIA Quadro RTX 8000 GPUs. It takes around 4GB GPU memory for NAS-Bench-101 and 9GB GPU memory for NAS-Bench-301. The validation loss converges well after 10 epochs of pretraining, which takes 1.2 hours on NAS-Bench-101 and 7.5 hours on NAS-Bench-301. 

\subsection{Comparison with Different Encoding Schemes} \label{enc:comp}
In our first experiment, we compare CATE with eleven architecture encoding schemes under three major encoding-dependent subroutines described in Section \ref{application} on NAS-Bench-101.
These encoding schemes include (1-3) one-hot/categorical/continuous adjacency matrix encoding  \cite{pmlr-v97-ying19a},  (4-6) one-hot/categorical/continuous path encoding and (7-9) their corresponding truncated counterparts \cite{white2019bananas}, (10) D-VAE \cite{zhang2019d}, and (11) arch2vec \cite{yan2020arch}. For continuous encodings, we use L2 distance as the distance metric. To examine the effectiveness of the encoding schemes themselves, we compare different encoding schemes under the same search subroutine. 

Figure \ref{fig.nas_encodings_comprison} illustrates our results. For each subroutine, we show the top-five best-performing encoding schemes. Overall, despite there is no overall best encoding, we found that CATE is among the top five across all the subroutines.

Specifically, for \emph{sample random architecture} subroutine, random search using adjacency matrix encoding performs the best. 
The random search using continuous encodings performs slightly worse than the adjacency encodings possibly due to the discretization loss from vector space into a fixed number of bins of same size before the random sampling. 

For \emph{perturb architecture} subroutine, CATE is on par with or outperforms adjacency encoding and path encoding because it is pre-trained to preserve strong computation locality information. This advantage allows the evolution or local search to find architectures with similar performance in local neighborhood more easily. Interestingly, we observe very small deviation using local search with CATE. This indicates that it always converges to some certain local minimums across different initial seeds. Since NAS-Bench-101 already exhibits locality in edit distance, encoding computation makes architectures even closer in terms of accuracy and thus benefits the local search. 

For \emph{train predictor model} subroutine, we have four observations: 
1) Adjacency matrix encodings perform less effective with neural predictor and DNGO. It is possibly that edit distance cannot fully reflect the closeness of architectures w.r.t their actual performance. 
2) Path encoding performs well with neural predictor but worse than other encodings with Bayesian optimization.
3) D-VAE and arch2vec, two encodings learned via variational autoencoding, perform well only with some certain NAS methods. It could be attributed to their challenging training objective which easily leads to overfitting. 
4) CATE is competitive with neural predictor and outperforms all the other encodings with Bayesian optimization. This is because neighboring computation-aware encodings correspond with similar accuracies. Moreover, the training objective in CATE is more efficient compared to the standard VAE loss \cite{kingma2013autoencoding} used by D-VAE and arch2vec.

\subsection{Comparison with Different NAS Methods}

In our second experiment, we compare the neural architecture search performance based on CATE encodings with state-of-the-art NAS algorithms on NAS-Bench-101 and NAS-Bench-301. Existing NAS algorithms contain one or more encoding-dependent subroutines. 
We consider six NAS algorithms that contain one encoding-dependent subroutine: random search (RS) \cite{Li2019RandomSA} (\emph{sample random arch.}), regularized evolution (REA) \cite{real2019regularized} (\emph{perturb arch.}), local search (LS) \cite{white2020local} (\emph{perturb arch.}), DNGO \cite{snoek2015scalable} (\emph{train predictor}), BOHAMIANN \cite{NIPS2016_springenberg} (\emph{train predictor}), arch2vec-DNGO \cite{yan2020arch} (\emph{train predictor}), and two NAS algorithms that contain more than one encoding-dependent subroutine: BOGCN \cite{shi2019multiobjective} (\emph{perturb arch.}, \emph{train predictor}) and BANANAS \cite{white2019bananas} (\emph{sample random arch.}, \emph{perturb arch.}, \emph{train predictor}). 
We compare these eight existing NAS algorithms with CATE-DNGO: a NAS algorithm  based on CATE encodings with the DNGO subroutine (\emph{train predictor}), and CATE-DNGO-LS: a NAS algorithm based on CATE encodings with the combination of DNGO and LS subroutines (\emph{train predictor}, \emph{perturb arch.}) as described in Section \ref{application}.




\begin{table}[t]

\centering
\begin{adjustbox}{width=1.0\columnwidth}
{\small
\begin{tabular}{@{}l|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{NAS methods}} & \multicolumn{1}{c}{\textbf{NAS-Bench-101}} & \multicolumn{1}{c}{\textbf{NAS-Bench-301}}\\
\midrule 
Prev. SOTA \cite{white2019bananas} & 5.92 & 5.35  \\ 
CATE-DNGO-LS (ours) & \textbf{5.88} & \textbf{5.28} \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Comparison between CATE and state-of-the-arts: Final test error [\%] given $150$ queried architectures on NAS-Bench-101 and $100$ queried architectures on NAS-Bench-301. The result is averaged over $200$ independent runs.}
\label{table:nasbench101-nasbench301-query-comparison}
\end{table}


Figure \ref{fig.nas_sota_comprison} and Table \ref{table:nasbench101-nasbench301-query-comparison} summarize our results. We have three major findings from Figure \ref{fig.nas_sota_comprison}:
1) Architecture encoding matters especially in the large search space. The right plot shows that CATE-DNGO and CATE-DNGO-LS in DARTS search space not only converge faster but also lead to better final search performance given the same budgets.
2) Local search (LS) is a strong baseline in both small and large search spaces. As mentioned in Section \ref{enc:comp}, performing LS using CATE leads to better results compared to other encodings.
3) NAS algorithms that use more than one encoding-dependent subroutine in general perform better than NAS algorithms with just one subroutine. Specifically, BOGCN and BANANAS that have multiple subroutines perform better than the single-subroutine NAS algorithms such as REA, DNGO, and BOHAMIANN.
Moreover, CATE-DNGO-LS leads to the best performing result in both NAS-Bench-101 and NAS-Bench-301 search spaces.
Meanwhile, the improvement of CATE-DNGO-LS versus CATE-DNGO shrinks in larger search space, indicating that the larger search space is more challenging to encode.


NAS-Bench-301 uses a surrogate model trained on $60$k architectures to predict the performance of all the other architectures in the DARTS search space. The performance of the other architectures, however, can be inaccurate. Given that, we further validate the effectiveness of CATE-DNGO-LS in the \textit{actual} DARTS search space by training the queried architectures from scratch. We set the budget to $100$ and $300$ queries, separately. Each queried architecture is trained for $50$ epochs with a batch size of $96$, using $32$ initial channels and $8$ cell layers. The average validation error of the last $5$ epochs is computed as the label. These values are chosen to be close to the proxy model used in DARTS. It takes about $3.3$ GPU days to finish the search with $100$ quries and $10.3$ GPU days with $300$ queries. See Figure \ref{fig:cell} for the best found cells. To ensure fair comparison, we compare CATE-DNGO-LS to methods \cite{liu2018darts,Li2019RandomSA,yan2020arch,white2019bananas} that use the common test evaluation script which is to train for $600$ epochs with cutout and auxiliary tower. 

Table \ref{table:cifar10-comparison} summarizes our results.
As shown, CATE-DNGO-LS (small budget) achieves competitive performance ($2.55\%$ avg. test error) with much less search cost and CATE-DNGO-LS (large budget) achieves superior performance ($2.46\%$ avg. test error) with similar search cost compared to other sampling-based search methods \cite{yan2020arch,white2019bananas} in the actual DARTS search space. This is consistent with our observation in NAS-Bench-301. We report the transfer learning results on ImageNet \cite{imagenet_cvpr09} in Table \ref{table:imagenet-comparison}. 



\begin{figure}[t]
	\centering
	\includegraphics[width=0.48\textwidth]{cate_small_resize.pdf}
	\includegraphics[width=0.48\textwidth]{cate_large_resize.pdf}
\caption{
    Top: Best found cell from CATE-DNGO-LS given the budget of 100 samples. 
	Bottom: Best found cell from CATE-DNGO-LS given the budget of 300 samples. 
	}
	\label{fig:cell}
\end{figure}


\begin{table}[t]
\centering
\begin{adjustbox}{width=1.0\columnwidth}
{\small
\begin{tabular}{@{}l|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{NAS Methods}} & \multicolumn{1}{c}{\textbf{Avg. Test Error}} & \multicolumn{1}{c}{\textbf{Params}} & \multicolumn{1}{c}{\textbf{Search Cost}}\\
\multicolumn{1}{l}{\textbf{}} & \multicolumn{1}{c}{\textbf{(\%)}} & \multicolumn{1}{c}{\textbf{(M)}} & \multicolumn{1}{c}{\textbf{(GPU days)}}\\
\midrule 
RS \cite{Li2019RandomSA}  & 3.29 $\pm$ 0.15 & \textbf{3.2} & 4  \\ 
DARTS \cite{liu2018darts}  & 2.76 $\pm$ 0.09 & 3.3 & 4  \\ 
BANANAS \cite{white2019bananas}  & 2.67 $\pm$ 0.07 & 3.6 & 11.8 \\ 
arch2vec-BO \cite{yan2020arch} & 2.56 $\pm$ 0.05 & 3.6 & 9.2 \\
\midrule
CATE-DNGO-LS (small budget) & 2.55 $\pm$ 0.08 & 3.5 &  \textbf{3.3} \\
CATE-DNGO-LS (large budget) & \textbf{2.46 $\pm$ 0.05} & 4.1 &  10.3 \\ 
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{NAS results in DARTS search space using CIFAR-10.}
\label{table:cifar10-comparison}
\end{table}


\begin{table}[t]
\centering
\begin{adjustbox}{width=1.0\columnwidth}
{\small
\begin{tabular}{@{}l|c|c|c@{}}
\toprule
\multicolumn{1}{l}{\textbf{NAS Methods}} & \multicolumn{1}{c}{\textbf{Params}} & \multicolumn{1}{c}{\textbf{Mult-Adds}} & \multicolumn{1}{c}{\textbf{Top-1 Test Error}}\\
\multicolumn{1}{l}{\textbf{}} & 
\multicolumn{1}{c}{\textbf{(M)}} & \multicolumn{1}{c}{\textbf{(M)}} & \multicolumn{1}{c}{\textbf{(\%)}}\\
\midrule 
SNAS \cite{Xie2018SNAS}  & 4.3 & 522 & 27.3  \\ 
DARTS \cite{liu2018darts} & 4.7 & 574 & 26.7 \\
BayesNAS \cite{pmlr-v97-zhou19e} & \textbf{4.0} & \textbf{440} & 26.5 \\
\textit{arch2vec}-BO \cite{yan2020arch} & 5.2 & 580 & 25.5  \\ 
BANANAS (ours) & 5.1 & 576 & 26.3  \\
\midrule
CATE-DNGO-LS (small budget) & 5.0 & 556 & 26.1 \\  
CATE-DNGO-LS (large budget) & 5.8 & 642 & \textbf{25.0} \\  
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Transfer learning results on ImageNet.}
\label{table:imagenet-comparison}
\end{table}








\subsection{Generalization to Outside Search Space}
In our third experiment, inspired by \cite{white2020study}, we evaluate the generalization ability of CATE beyond the search space on which it was trained. The training search space is designed as a subset of NAS-Bench-101, where each included architecture has 2 to 6 nodes and 1 to 7 edges. The test search space is disjointed from the training search space and includes architectures with 6 nodes and 7 to 9 edges. There are $10,026$ and $60,669$ non-isomorphic graphs in the training and test space respectively. The CATE encodings are pre-trained using the training space and are used to conduct architecture search in the test space. We compare CATE with the adjacency matrix encoding because it was shown in \cite{white2020study} to have \emph{the best generalization capability} compared to other encodings. A simple 2-layer MLP with hidden size $128$ is used as the neural predictor for both encodings. 

Figure \ref{fig.oo} shows the validation error curve of the test search space given the number of $150$ sample budget across $500$ independent runs. As shown, CATE outperforms adjacency matrix encoding by a large margin.
This indicates that CATE can better contextualize the computation information compared to fixed encodings, which generalizes better when adapting to outside search space. Moreover, the padding scheme in our encoder allows us to handle architectures with different numbers of nodes.

\subsection{Ablation Studies}
\label{sec:ablation}
Finally, we conduct ablation studies on different hyperparameters involved in CATE. We use CATE-DNGO as the NAS method and report the final NAS test error [\%] given $150$ queried architectures on NAS-Bench-101. The result is averaged over $200$ independent runs.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{Figure_oo_crop.pdf}
\caption{Performance on the out-of-training search space. It reports the validation error of $500$ independent runs.}
	\label{fig.oo}
\end{figure}

\textbf{Architecture Pair Sampling Hyperparameters.}
We plot the histogram of model parameters on NAS-Bench-101 in Figure \ref{fig.histogram}. As shown, the architectures are neither normally nor uniformly distributed in this search space in terms of model parameters. This motivates us to use a sliding window-based architecture pair selection to avoid the unbalanced sampling as proposed in Section \ref{pretraining}. The choice of $\delta$ and $K$ and their effects on the downstream NAS are summarized in Table \ref{table:pair_sampling_choice}. 
We found that strong computation locality (\emph{i.e.} small $\delta$) usually leads to better results. The choice of neighborhood size $K$ does not have a significant effect on NAS performance. Therefore, we choose small $K$ for faster pretraining. For NAS-Bench-301, we use the FLOPs as the computational attributes $\mathbf{P}$ and observe the same trend as in NAS-Bench-101 on the selection of $\delta$ and $K$. We report the results in Appendix \ref{ablation:hp}. 


\begin{table}[t] 
\begin{adjustbox}{width=0.76\columnwidth,center}
\scriptsize{
\begin{tabular}{c|c|c|c|c} 
\hline
\diagbox[]{$\delta$}{K} & 1 & 2 & 4 & 8 \\ \hline
$1\times10^{6}$  & 6.02 & 5.95 & 5.99 & 5.95  \\ 
$2\times10^{6}$ & 6.02 & \textbf{5.94} & 6.04 & 5.96  \\
$4\times10^{6}$  & \textbf{5.94} & 6.03 & 6.05 & 5.99  \\
$8\times10^{6}$  & 6.05 & 6.04 & 6.11 & 6.04  \\
\hline
\end{tabular} 
}
\end{adjustbox}
\caption{Effects of $\delta$ and $K$ on architecture pair sampling.}
\label{table:pair_sampling_choice}
\end{table} 






\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{Figure_hist_nas101_crop.pdf}
\caption{Histogram of model parameters on NAS-Bench-101.}
\label{fig.histogram}
\end{figure}


\textbf{Transformer Hyperparameters.} We studied the effect of the number of cross-attention Transformer blocks $L_c$ and the hidden dimension of the feed-forward layer $d_{ff}$ on CATE. We fix $\delta$ and $K$ for pre-training as mentioned in Section \ref{sec.experiments}. The downstream NAS result is summarized in Table \ref{table:transformer_choice}. It shows that larger $L_c$ and $d_{ff}$ usually lead to better NAS performance, which indicates that deep contextualized representations are beneficial to downstream NAS.  

\begin{table}[ht] 
\begin{adjustbox}{width=0.71\columnwidth,center}
\scriptsize{
\begin{tabular}{c|c|c|c} 
\hline
\diagbox[]{$d_{ff}$}{$L_c$} & 6 & 12 & 24 \\ \hline
64  & 6.07 &  5.99 & 5.95  \\ 
128 & 6.01 & \textbf{5.94} & 5.95   \\
256  & 5.97 & \textbf{5.94} & \textbf{5.94}  \\
\hline
\end{tabular} 
}
\end{adjustbox}
\caption{Effects of $L_c$ and $d_{ff}$ on pretraining CATE.}
\label{table:transformer_choice}
\end{table} 



\textbf{Choice of Mask Type.} We studied pretraining CATE with direct/indirect dependency mask and summarize its downstream NAS results in Table \ref{table:mask_type}. CATE trained with indirect dependency mask outperforms the direct one in both benchmarks, indicating that capturing long-range dependency helps preserve computation information in the encodings.



\begin{table}[ht]
\centering
\begin{adjustbox}{width=0.81\columnwidth}
{\small
\begin{tabular}{@{}c|c|c@{}}
\toprule
\multicolumn{1}{c}{\textbf{Mask type}} & \multicolumn{1}{c}{\textbf{NAS-Bench-101}} & \multicolumn{1}{c}{\textbf{NAS-Bench-301}} \\ 
\midrule 
Direct  & 6.03 & 5.35  \\ 
Indirect  & \textbf{5.94} & \textbf{5.30}  \\ 
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Direct/Indirect dependency mask selection.}
\label{table:mask_type}
\end{table}