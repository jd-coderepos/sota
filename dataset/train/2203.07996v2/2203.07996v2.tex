\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \usepackage{amsmath, xparse}
\usepackage{booktabs}
\usepackage{array}
\usepackage{makecell}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{multirow}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\def\arraystretch{1.4}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[dvipsnames]{xcolor}
\newcommand{\zhouhan}[1]{[\textcolor{red}{zhouhan: #1}]}
\newcommand{\Changed}[1]{[\textcolor{blue}{changed: #1}]}

\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}



\setcounter{secnumdepth}{0} 





\iffalse
\title{AAAI Press Formatting Instructions \\for Authors Using \LaTeX{} --- A Guide}
\author{
Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
\textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\




    2275 East Bayshore Road, Suite 160\\
    Palo Alto, California 94303\\
publications22@aaai.org
}
\fi

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\title{Leveraging Uni-Modal Self-Supervised Learning for\\Multimodal Audio-visual Speech Recognition}
\author {
Paper ID: 5200; SNLP: Speech and Multimodality
}
\affiliations {
}


\usepackage{bibentry}


\begin{document}

\maketitle

\begin{abstract}
Training Transformer-based models demands a large amount of data, while obtaining parallel aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled uni-modal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. 
In this work, we successfully leverage uni-modal self-supervised learning to promote the multimodal AVSR. 
In particular, we first train audio and visual encoders on a large-scale uni-modal dataset, then we integrate components of both encoders into a larger multimodal framework which learns to recognize paired audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from uni-modal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level AVSR tasks. 
Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted AVSR dataset Lip Reading Sentences 2 (LRS2) by a large margin, with a relative improvement of 30\%.  



\end{abstract}

\section{Introduction}
Audio-Visual Speech Recognition (AVSR) is a speech recognition task that leverages both an audio input of human voice and an aligned visual input of lip motions. It has been one of the successful application fields that involve multiple modalities in recent years, and has attracted a lot of research attention. 

Due to the limited amount of labeled parallel multi-modal data and the difficulty of recognition from the visual inputs, existing AVSR models tend to use extra data to increase the performance of the system, in a form of inserting an extra supervised learning stage in the training process. 

For the visual front-end, many existing methods rely on an extra sequence level classification to bootstrap its learning on visual features. For example, \citet{avsrhybrid, convseq2seq} train their visual front-end on LRW \cite{LRW} before learning on the AVSR task. \citet{deepavsr, LRS3} chunks the MV-LRS data \cite{chung2017lip} into pieces of words and pre-train the model through classification. VoxCeleb \cite{chung2018voxceleb2} are also used in \citet{afouras2020asr} for the same purpose. Due to the difficulty of transcribing text from visual inputs, learning an effective visual front-end could still be notoriously hard, even with these extra supervised learning tasks. Sometimes curriculum learning is required to transplant the learned visual front-end into AVSR task \cite{deepavsr}. End-to-end learning of large-scale AVSR data hasn't been successful until recently \cite{e2econformer}. On the other hand, there are much fewer efforts in developing extra tasks for the audio front-end, since it is much easier than its visual counterpart. 

Despite all the above efforts to involve extra tasks or datasets, utilizing unlabelled data hasn't been well explored yet. \citet{shukla2020visually} is among the few attempts in this facet, in which cross-modality setting is utilized by predicting lip motions of videos from audio inputs. Their proposed learning schemes yield strong emotion recognition results but are relatively weak in speech recognition. In another scenario, self-supervised learning in uni-modality has been well established as a paradigm to learn general representations from unlabelled examples, such as in natural language processing \cite{brown2020language, devlin2018bert}, speech recognition \cite{wav2vec2}, and computer vision \cite{mocov1, simclr, byol}.

In this work, we rely on a simple but effective approach, which is to utilize unlabelled uni-modal data by using pre-trained models that are trained through self-supervised learning. Specifically, we use \citet{wav2vec2} pre-trained on the large LibriLight \cite{librilight} dataset as our audio front-end. For visual front-end, we found that it is not as straight-forward for it to leverage pre-trained models, as we have to substitute the first ResNet block in MoCo v2 \cite{mocov2} by 3-D convolution layer and fine-tune it through LRW. In total, our approach doesn't require a curriculum learning stage, and the overall training time has been decreased. 

Experimental results show that our new front-ends significantly outperform previous ones by a big margin in both audio-only and visual-only settings, and a new state-of-the-art has been achieved in the final AVSR setting. To our best knowledge, this is the first work that successfully applies uni-modal pre-trained models in the multimodal setting of AVSR. We also ensure this research is reproducible by publishing our codes at \url{anonymized_url}.

\section{Related Work}



\subsection{Audio-Visual Speech Recognition}
The earliest work on AVSR could be dated back to around two decades ago, when \citet{early_hmm_avsr} showed hand-crafted visual feature improves HMM-based ASR systems. The first modern AVSR system is proposed in \citet{deepavsr} where deep neural networks are used. The field has been rapidly developing since then. Most of the works are devoted into the architectural improvements, for example, \citet{convseq2seq} proposed temporal focal block and spatio-temporal fusion, and \citet{lee2020audio} explored to use cross-modality attentions with Transformer. 

The other line of research focuses on a more diversified learning scheme to improve AVSR performance. \citet{li2019improving} uses a cross-modal student-teacher training scheme. \citet{paraskevopoulos2020multiresolution} proposes a multi-task learning scheme by making the model to predict on both character and subword level. Self-supervised learning has also been explored in \citet{shukla2020visually}, where the cross-modality setting is utilized by predicting frames of videos from audio inputs. 

The end-to-end learning of AVSR systems are first seen in \citet{tao2020end}, albeit in a much simpler dataset than LRS2. More recent work \cite{e2econformer} has made end-to-end learning on LRS2 possible by using a Conformer acoustic model and a hybrid CTC/attention decoder.


\subsection{Self-Supervised Learning}


Self-supervised learning has been chased in recent years since its ability to learn general representations of data through simple tasks that don't require labeling. Contrastive learning \cite{cl} has become the most impactful learning scheme in this field. In natural language processing, uni-or bi-directional language modelling \cite{brown2020language, devlin2018bert} have been used to significantly increase performances on various tasks. In audio speech processing, contrastive predictive coding \cite{wav2vec2} has been proven to be powerful in speech recognition. In the visual domain, Earlier works create self-supervised tasks through image processing based methods, such as distortion \cite{distortion},colorization \cite{colorization} and context prediction \cite{patches}. More recently, contrastive learning emerged as a paradigm of self-supervised learning, which results in a group of more expressive general visual representations, such as MoCo \cite{mocov1, mocov2}, SimCLR \cite{simclr}, BYOL \cite{byol}, etc.






\section{Architecture}
The overall architecture of our model is shown in Fig. \ref{fig:framework}. The audio-visual model is comprised of four components, the front-ends and back-ends for both modalities, the fusion module, and the decoders. 

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig/modelarch.pdf}
\caption{Overall architecture of our AVSR model.}
\label{fig:framework}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig/pipeline.pdf}
\caption{Training pipeline of the model. Yellow blocks represent new parameters that are randomly initialized, while green blocks represent parameters that are inherited from last training stage. Blue blocks are model parts that got discarded after their corresponding training stage. }
\label{fig:pipeline}
\end{figure*}

\subsection{Front-ends}
\subsubsection{Visual Front-end}
Visual front-end serves as a component to capture the lip motion and reflect the lip position differences in its output representations. A naive way to apply pre-trained models in the visual front-end is to directly feed the RGB channels of each frame as input. However, since frames within a same clip in AVSR are largely similar in their contents while most pre-trained models in vision target at learning general representations reflecting the content of the whole image, this approach will result in similar outputs for all the frames, collapsing the informative lip position differences between frames. 

To overcome the aforementioned problem while still being able to utilize the pre-trained model, we truncate the first convolutional layer in MoCo v2 \cite{mocov2}, which is pre-trained on ImageNet \cite{imagenet}, and replace it by a layer of 3-D convolution. The outputs of 3-D convolution layer are intentionally made identical to the input of the first ResBlock in MoCo v2 (see Table \ref{tb:visualdim}), thus providing a compatible interface to transfer higher layers of MoCo v2 into this task. On the other hand, we also adopt the common practice to convert the RGB input image to gray-scale before feeding it into the model, as it prevents the model from learning chromatic aberration information.





\subsubsection{Audio Front-end}
The audio front-end is rather straight-forward. We use wav2vec 2.0 \cite{wav2vec} pre-trained on Libri-Light \cite{librilight}, like it is normally used for ASR tasks, both the 1-D convolution layers and the stacked Transformer encoder layers are transferred into our audio front-end. The audio front-end takes as input raw audio wave of 16kHz, and produces one vector representation every 20ms. The audio feature dimensions are shown in Table \ref{tb:audiodim}.

\subsection{Back-ends}
Since the visual frames are in 25 FPS and the wav2vec 2.0 outputs are around 49 Hz\footnote{The odds are due to the larger receptive fields of wav2vec 2.0 1-D convolution layers, which we circumvent by properly prefixing and suffixing the audio sequence and truncate the trailing audio vector. Thus a perfect 1:2 ratio of visual frames and audio front-end outputs are ensured. }, one should note that there is 2x difference in the frequency of frame-wise visual and audio representations at the output of their front-ends. In the back-end, we use 1-D convolution layers on the time dimension combined with Transformer encoder layers to provide single modality temporal modeling, as well as adjusting the features to have the same frequency. 


\subsubsection{Visual Back-end}
The incoming MoCo v2 output to the visual back-end has a feature dimension of 2048, at a frequency of 25 vectors per second. In the visual backend, we keep this frequency while reducing the feature size to 512. See Table \ref{tb:visualdim}. For positional encodings of the Transformer, we use fixed positional encoding in the form of sinusoidal functions. 


\begin{table}[h]
    \small
    \centering
\begin{tabularx}{\linewidth}{c|c|c}
    \toprule
    Stage                    & Modules                & \makecell{Input image sequence \\ }\\
    \hline
    \multirow{2}*{Front-end} & 3-D convolution layer  & \makecell{} \\
    \cline{2-3}
    ~                        & MoCo v2                & \makecell{}           \\
    \hline
    \multirow{2}*{Back-end}  & 1-D convolution layers & \makecell{}            \\
    \cline{2-3}
    ~                        & Transformer Encoder    & \makecell{}            \\
    \bottomrule
\end{tabularx}
    \caption{The feature dimension of visual stream. The dimensions of features are denoted by .  denote the number of visual frames.}
    \label{tb:visualdim}
\end{table}

\begin{table}[h]
    \small
    \centering
\begin{tabularx}{\linewidth}{c|c|c}
    \toprule
    Stage                   & Modules                & \makecell{Input audio waveform          \\ }\\
    \hline
    Front-end               & wav2vec 2.0            & \makecell{}          \\
    \hline
    \multirow{2}*{Back-end} & 1-D convolution layers & \makecell{} \
p_{\text{CTC}}(\mathbf{y}|\mathbf{x})\approx \prod_{t=1}^{T} p(y_t|\mathbf{x})

p_{\text{CE}}(\mathbf{y}|\mathbf{x}) = \prod_{l=1}^L p(y_l|y_{<l}, \mathbf{x})

\label{eq:lossfunction}
\mathcal{L}=\lambda \log p_{\text{CTC}}(\mathbf{y}|\mathbf{x})+(1-\lambda)\log p_{\text{CE}}(\mathbf{y}|\mathbf{x})

    \hat{\mathbf{y}}=\mathop{\arg\max}_{\mathbf{y}\in\hat{\mathcal{Y}}}\{\alpha\log p_{\text{CTC}}(\mathbf{y}|\mathbf{x})+(1-\alpha)\log p_{\text{CE}}(\mathbf{y}|\mathbf{x})\}
    \label{eq:lmeq}

    p_{\text{ctc}}(h|X)=\sum\limits_{v\in (\mathcal{U})^+}p_{\text{ctc}}(h\cdot v|X)

where  denotes all possible symbol sequences except the empty. The CTC probability can be computed by keeping the forward hypothesis probabilities  and , where the superscripts  and  represents all CTC paths end with a non- or  symbol, respectively.

The decoding algorithm is also a beam search with width  and hyperparameter  control the relative weight given to CTC and attention decoding.  is a set of symbols excluding , and a same token is used to represent  and  in our implementation.

\section{Decoding Examples}
\begin{table}[h!]
\small
\begin{tabularx}{\linewidth}{X}
    \toprule
    \textit{\texttt{AO:}} WHATEVER YOU \underline{ASK}\\
    \textit{\texttt{AV:}} WHATEVER YOU ARE\\
    \hline
    \textit{\texttt{AO:}} TRAVEL THREE MILES \underline{URBER} WEST AND YOU DO GET MORE FOR YOUR MONEY HERE\\
    \textit{\texttt{AV:}} TRAVEL THREE MILES FURTHER WEST AND YOU DO GET MORE FOR YOUR MONEY HERE\\
    \hline
    \textit{\texttt{AO:}} IT COULD BE YOUR PASSPORT \underline{FOR} A SMALL FORTUNE\\
    \textit{\texttt{AV:}} IT COULD BE YOUR PASSPORT TO A SMALL FORTUNE\\
    \hline
    \textit{\texttt{AO:}} \underline{WHAT} TO THINK FOR THEMSELVES\\
    \textit{\texttt{AV:}} NOT TO THINK FOR THEMSELVES\\
    \hline
    \textit{\texttt{AO:}} NOT \underline{THE} SUBJECT \underline{MATTERING}\\
    \textit{\texttt{AV:}} NOT FOR SUBJECT MATTER\\
    \hline
    \textit{\texttt{AO:}} I WOULDN'T SAY I'M \underline{THE} STAR\\
    \textit{\texttt{AV:}} I WOULDN'T SAY I'M A STAR\\
    \hline
    \textit{\texttt{AO:}} \underline{CRISPAS} PUDDING THAT NOBODY REALLY LIKES\\
    \textit{\texttt{AV:}} CHRISTMAS PUDDING THAT NOBODY REALLY LIKES\\
    \hline
    \textit{\texttt{AO:}} \underline{BUT} AT THE SAME TIME\\
    \textit{\texttt{AV:}} AT THE SAME TIME\\
    \hline
    \textit{\texttt{AO:}} BEING MY OWN\\
    \textit{\texttt{AV:}} BEING \textbf{ON} MY OWN\\
    \hline
    \textit{\texttt{AO:}} AT ONE POINT\\
    \textit{\texttt{AV:}} \textbf{SO} AT ONE POINT\\
    \bottomrule
\end{tabularx}
    \caption{AO (audio-only) and AV (audio-visual) decoding examples. Underline denotes substitutions and insertions error; Bold denotes deletions error.}
    \label{tb:AOAV_match}
\end{table}

Table \ref{tb:AOAV_match} is examples of sentences that audio-only model fails to predict while audio-visual model correctly predicts. The visual modality enhances the model from a wide range of error cases. 

\section{Preprocessing Example}

\begin{figure*}[ht!]
    \centering
    \subfloat[Landmarks detected by dlib. Green dots are 68 landmarks, frames without landmarks are ones that dlib fail to detect.]
    {
        \includegraphics[width=\textwidth]{fig/preprocess/detect.png}
        \label{fig:detect}
    }\\
    \subfloat[Landmarks after linear interpolation.]
    {
        \includegraphics[width=\textwidth]{fig/preprocess/interpolate.png}
        \label{fig:interpolate}
    }\\
    \subfloat[Faces smoothed with a window width of 12 and aligned to a neural reference frame using a similarity transformation.]
    {
        \includegraphics[width=\textwidth]{fig/preprocess/transpose.png}
        \label{fig:transpose}
    }\\
    \subfloat[Mouth ROIs cropped using a bounding box of .]
    {
        \includegraphics[width=\textwidth]{fig/preprocess/cut.png}
        \label{fig:cut}
    }
    \caption{Preprocessing example to illustrate the process to generate mouth ROIs.}
    \label{fig:preprocessexample}
\end{figure*}

The input images are sampled at 25 FPS and resized to  pixels. We crop a  mouth ROI from each frame. Fig. \ref{fig:preprocessexample} shows the process to generate. 


\end{document}
