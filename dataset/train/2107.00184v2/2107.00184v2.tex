\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{nicefrac}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[normalem]{ulem}


\hyphenpenalty = 4000
\tolerance = 2000


\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}


\newcommand{\diag}[1]{ \text{diag}\!\left(#1\right)}
\newcommand{\R}{\mathbb{R}}

\newcommand{\parabegin}[1]{\vspace{3px}\noindent\textbf{#1}}



\title{Bilinear Scoring Function Search \\for Knowledge Graph Learning}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}

\author{Yongqi Zhang,~\IEEEmembership{Member,~IEEE}
	Quanming Yao,~\IEEEmembership{Member,~IEEE}
James T. Kwok,~\IEEEmembership{Fellow,~IEEE}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Y. Zhang was with 4Paradigm Inc. Beijing, China.
	E-mail: zhangyongqi@4paradigm.com
	\IEEEcompsocthanksitem Q. Yao was with Department of Electronic Engineering,
	Tsinghua University and 4Paradigm Inc. Beijing, China.
	E-mail: qyaoaa@connect.ust.hk
\IEEEcompsocthanksitem 
	J.T. Kwok was with Department of Computer Science,
	Hong Kong University of Science and Technology.
	Hong Kong, China.
	E-mail: jamesk@cse.ust.hk
}\thanks{The code is public available at https://github.com/AutoML-Research/AutoSF. 
	Corresponding author: Quanming Yao.}
}



\begin{document}	
	
\IEEEtitleabstractindextext{
\begin{abstract}
Learning embeddings
for entities and relations in knowledge graph (KG)
have benefited many downstream tasks.
In recent years,
scoring functions, 
the crux of KG learning,
have been human designed 
to 
measure the plausibility of triples
and
capture different kinds of relations in KGs.
However, 
as relations exhibit intricate patterns that are hard to infer before training,
none of them consistently perform the best on benchmark tasks. 
In this paper,
inspired by the recent success of automated machine learning (AutoML), 
we search bilinear scoring functions
for different KG tasks 
through the AutoML techniques.
However,
it is non-trivial to explore domain-specific information here.
We first set up a search space for AutoBLM
by analyzing existing scoring functions.
Then, we propose a progressive algorithm (AutoBLM) 
and an evolutionary algorithm (AutoBLM+),
which are further accelerated by filter and predictor 
to deal with the domain-specific properties for KG learning.
Finally,
we perform extensive experiments on benchmarks in 
KG completion, multi-hop query, and entity classification tasks.
Empirical results
show that the searched scoring functions
are KG dependent, new to the literature, 
and outperform the existing scoring functions.
AutoBLM+ is better than  AutoBLM 
as the evolutionary algorithm can
 flexibly explore better structures in the same budget.
\end{abstract}
	
\begin{IEEEkeywords}
Automated machine learning,
Knowledge graph,
Neural architecture search,
Graph embedding
\end{IEEEkeywords}
}

\maketitle


\section{Introduction}
\label{sec:intro}

\IEEEPARstart{T}{he} knowledge
 graph (KG)
\cite{singhal2012introducing,nickel2016review,wang2017knowledge} is a 
graph in which the nodes represent entities, 
the edges are the relations between
entities, and
the facts
are represented by triples of the form \textit{(head entity, relation, tail entity)} (or  in short).
The KG has been found useful in a lot of 
data mining and machine learning
applications and tasks,
including question answering~\cite{ren2019query2box},
product
recommendation~\cite{zhang2016collaborative},
knowledge graph completion~\cite{nickel2011three,yang2014embedding},
multi-hop query~\cite{hamilton2018embedding,ren2019query2box},
and entity classification~\cite{kipf2016semi}.

In a KG, 
plausibility  of a fact
 
is given by , where  is the
{\em scoring function\/}. 
Existing 's are
custom-designed by human experts,
and can be categorized into the following three families:
(i) translational distance models (TDMs)
\cite{bordes2013translating,wang2014knowledge,fan2014transition,sun2019rotate},
which 
model the relation embeddings as translations from the head entity embedding
to the tail entity embedding;
(ii)
bilinear model (BLMs)
\cite{nickel2011three,yang2014embedding,trouillon2017knowledge,nickel2016holographic,liu2017analogical,kazemi2018simple,lacroix2018canonical,zhang2019quaternion},
which
model the interaction between entities and relations
by a bilinear product between the entity and relation embeddings; and (iii)
neural network models (NNMs) 
\cite{dong2014knowledge,
dettmers2017convolutional,guo2019learning,schlichtkrull2018modeling,vashishth2019composition},
which
use neural networks to capture the interaction.
The scoring function
can significantly impact KG learning performance \cite{nickel2016review,wang2017knowledge,lin2018knowledge}.
Most TDMs are less expressive and have poor empirical performance~\cite{wang2017knowledge,wang2018evaluating}.
NNMs are powerful but have
large numbers of parameters and may overfit the training triples.
In comparison, 
BLMs  
are more advantageous in that they are 
easily customized to be expressive,
have linear complexities w.r.t. the numbers of entities/relations/dimensions,
and have state-of-the-art performance~\cite{lacroix2018canonical}.
While a number of BLMs have been proposed,
the best BLM is often dataset-specific.


Recently, 
automated machine learning (AutoML)~\cite{quanming2018auto,automl_book}
has demonstrated 
its power in many machine learning tasks such as 
hyperparameter optimization (HPO)~\cite{feurer2015efficient}
and neural architecture search
(NAS)~\cite{zoph2017neural,liu2018darts,elsken2019neural}.
The models discovered have better performance than those
designed by
humans, and 
the amount of human effort required is
significantly reduced.
Inspired by its success,
we propose 
in this paper 
the use of AutoML for the design  of
KG-dependent scoring functions.
To achieve this, one has to 
pay careful consideration to the
three main components in 
an AutoML algorithm:
(i) search space, 
which identifies important properties of the learning models
to search;
(ii) search algorithm, which ensures that
finding a good model in this space
is efficient;
and 
(iii) evaluation method,
which offers feedbacks to the search algorithm.

In this paper,
we make the following contributions in achieving these goals:
\begin{itemize}[leftmargin = 10px,itemsep=1px]
\item 
We design a search space
of scoring functions, which includes all the
existing BLMs.
We further analyze properties of this search space,
and provide conditions for a candidate scoring function
to be expressive,
degenerate, and equivalent to another.


\item 
To explore the above search space properties 
and reduce the computation cost in evaluation,
we design 
a filter to remove degenerate and equivalent structures,
and a performance predictor with specifically-designed symmetry-related features (SRF) 
to select promising structures.

\item 
We customize a progressive algorithm (AutoBLM)
and an evolutionary algorithm (AutoBLM+)
that,
together with the filter and performance predictor,
allow flexible exploration of new BLMs.
\end{itemize}

Extensive experiments are 
performed on the 
tasks
of KG completion, multi-hop query and entity classification.
The results
demonstrate that the models obtained by AutoBLM and
AutoBLM+
outperform the start-of-the-art 
human-designed 
scoring functions.
In addition,
we show that the customized progressive and evolutionary algorithms are 
much less expensive than popular search algorithms
(random search, Bayesian optimization
and reinforcement learning)
in finding a good scoring function.


\vspace{3px}
\noindent
\textit{Differences with the Conference Version.}
Compared to 
the preliminary version published in ICDE 2020~\cite{zhang2020autosf},
we have made the following important extensions:
\begin{enumerate}[leftmargin=*]
\item \textit{Theory.}
We add new analysis to the designed 
search space based on bilinear models.
We theoretically prove
when the candidates in the search space can be expressive (Section~\ref{ssec:unifiedBLM}),
degenerate (Section~\ref{sssec:degenerate})
and equivalent structures (Section~\ref{sssec:equiv}).


\item 
\textit{Algorithm.}
We extend the search algorithm with the evolutionary algorithm (Section~\ref{ssec:evolution}),
i.e., AutoBLM+.
The evolutionary strategy in Algorithm~\ref{alg:evolution} 
can explore better in the search space,
and can also leverage the filter 
and predictor to deal with the domain-specific properties.

\item
\textit{Tasks.}
We extend AutoBLM and AutoBLM+ to two new tasks,
namely, multi-hop query (Section~\ref{sec:exp:hop})
and entity classification in (Section~\ref{sec:exp:cls}).
We show that the search problem can be well adopted to these new scenarios,
and achieve good empirical performance.

\item \textit{Ablation Study.}
We conduct more experiments on the performance 
(Section~\ref{exp:kgc:performance} and \ref{exp:kgc:ogb}) and analysis (Section~\ref{exp:alg:compare})
of the new search algorithm,
analysis on the influence of   
(Section~\ref{sec:exp:varyK}),
and the problem of parameter sharing (Section~\ref{sec:exp:ps})
to analyze the design schemes in the search space and search algorithm.



\end{enumerate}






\parabegin{Notations.}
In this paper, vectors are denoted by lowercase boldface, and matrix by uppercase boldface.
The important notations are listed in Table~\ref{tab:notations}.

\begin{table}[ht]
	\centering
	\vspace{-7px}
	\caption{Notations used in the paper.}
	\label{tab:notations}
	\vspace{-10px}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c|C{6.3cm}}
		\toprule
   &   set of entities, relations, triples \\
		 & number of entities, relations, triples \\ 
		 & triple of head entity, relation and tail entity \\
		 & embeddings of , , and  \\ 
		 & scoring function for triple  \\ 
		 & -dimensional real/complex/hypercomplex space \\
		 & square matrix based on relation embedding  \\
		 & triple product  \\
		 & -norm of vector  \\
		 & real part of complex vector  \\
		 & conjugate of complex vector   \\
		\bottomrule
	\end{tabular}
	\vspace{-10px}
\end{table}


\section{Background and Related Works}
\label{sec:relworks}


\subsection{Scoring Functions for Knowledge Graph (KG)}
\label{ssec:kg}

A knowledge graph 
(KG)
can be represented by a third-order tensor ,
in which 
 if
the corresponding triple  
exists
in the KG, and 0
otherwise.
The {\em scoring function}  measures plausibility of the triple .
As introduced in Section~\ref{sec:intro},
it is
desirable 
for a scoring function 
to be 
able to represent  any of the
symmetric, anti-symmetric, general asymmetric and inverse
KG relations  
in Table~\ref{tab:relations}.

\begin{table}[t]
	\centering
	\caption{Popular properties in KG relations.}
	\label{tab:relations}
	\setlength\tabcolsep{1.5pt}
	\renewcommand{\arraystretch}{1.08}
	\vspace{-10px}
	\begin{tabular}{c|c|c} \toprule
		property & examples in WN18/FB15k & constraint on \\ \midrule
		symmetry
		&  \texttt{isSimilarTo}, \texttt{spouseOf} 
		&     
		\\  anti-symmetry
		&  \texttt{ancestorOf}, \texttt{isPartOf} 
		&    
		\\general asymmetry
		& \texttt{locatedIn}, \texttt{profession} 
		&     
		\\  inverse 
		&  \texttt{hypernym}, \texttt{hyponym}  
		& 		
		\\ 
		\bottomrule
	\end{tabular} 
	\vspace{-12px}
\end{table}


\begin{definition}[Expressiveness~\cite{trouillon2017knowledge,wang2017multi,balavzevic2019tucker}] 
	\label{def:express}
	A scoring function 
   is {\em fully expressive\/} if
for any KG  and the corresponding tensor , one can find an instantiation 
	
of 
	the scoring function  such that
	, .
\end{definition}



Not all scoring functions are fully expressive.
For example, consider a KG with two people \textit{A}, \textit{B}, and 
a relation ``OlderThan''. Obviously, we can have
either \textit{(A, OlderThan, B)} or \textit{(B, OlderThan, A)}, but not both.
The scoring function , where
 are -dimensional embeddings of  and ,
respectively, cannot be fully expressive
since
.

On the other hand,
while expressiveness  indicates
the ability of  to fit a given KG,
it may not 
generalize well when inference on different KGs.
As real-world KGs can be very sparse~\cite{singhal2012introducing,wang2017knowledge},
a scoring function with a large amount of trainable parameters may overfit the training triples.
Hence, it is also desirable that the scoring function has only a manageable number of parameters.




In the following, we review the three main types of scoring functions, namely,
translational distance model (TDM),
neural network model (NNM), and
biLinear model (BLM).
As will be seen,
many TDMs (such as TransE~\cite{bordes2013translating} and TransH~\cite{wang2014knowledge}) cannot model the symmetric relations well~\cite{wang2017knowledge,ji2020survey}.
Neural network models,
though fully expressive,
have large numbers of parameters.
This not only prevents the model from generalizing well on unobserved triples in a sparse KG,
but also increases the training and inference costs~\cite{dettmers2017convolutional,lacroix2018canonical,vashishth2019composition}.
In comparison,
BLMs 
(except DistMult)
can model all relation pattens in Table~\ref{tab:relations} and are fully expressive.
Besides,
these models 
(except RESCAL and TuckER)
have moderate complexities
(with the number of parameters 
linear in  and ).
Therefore,
we consider BLM as a better choice, and it will be our focus in this paper.


\begin{table*}[ht]
	\centering
	\caption{The representative BLM scoring functions. 
		For each scoring function we show the definitions, 
		expressiveness in Definition~\ref{def:express},
		the ability to model all common relation patterns in Table~\ref{tab:relations} (``RP'' for short),
		and the number of parameters.}
	\vspace{-10px}
	\label{tab:scofun}
	\setlength\tabcolsep{8pt}
	\renewcommand{\arraystretch}{1.3}
	\begin{tabular}{c |  c | c | c | c}
		\toprule
		scoring function & definition & expressiveness    &  RP  & \# parameters                                                        \\ \midrule
		RESCAL~\cite{nickel2011three}                                                                               &     &     &    &    \\
		              DistMult~\cite{yang2014embedding}      &      &   &  &               \\
ComplEx~\cite{trouillon2017knowledge}/HolE
		\cite{nickel2016holographic} & Re         &     &  &     \\
		Analogy~\cite{liu2017analogical}  &  + Re         &     &  &     \\
		{SimplE~\cite{kazemi2018simple}/CP~\cite{lacroix2018canonical}}                                           &  +                  &   &    &                            \\
		 QuatE~\cite{zhang2019quaternion}                                                                            &                                                                                                                                                         &    &   &                            \\
		TuckER~\cite{balavzevic2019tucker}                                                                          &                                                                                                                                &   &    &                         \\ \bottomrule
	\end{tabular} 
	\vspace{-7px}
\end{table*}



\parabegin{Translational Distance Model (TDM).}
Inspired by analogy results in word embeddings~\cite{bengio2013representation}, 
scoring functions in TDM 
take the relation  as a translation from  to .
The most representative TDM is TransE~\cite{bordes2013translating}, with .
In order to handle one-to-many, many-to-one and many-to-many relations,
TransH~\cite{wang2014knowledge} and TransR~\cite{fan2014transition} introduce additional vectors/matrices to map
the entities to a relation-specific hyperplane.
The more recent
RotatE~\cite{sun2019rotate} treats the relations as rotations in a
complex-valued space:
,
where 
and  is the Hermitian product~\cite{trouillon2017knowledge}.
As discussed in~\cite{wang2017multi},
most TDMs are not fully expressive. For example,
TransE and TransH cannot model symmetric relations.

\parabegin{Neural Network Model (NNM)}.
NNMs take the 
entity and relation 
embeddings 
as input, and
output a probability for the triple  using a neural network.
Earlier works are
based on multilayer perceptrons~\cite{dong2014knowledge} 
and neural tensor networks~\cite{socher2013reasoning}.
More recently,
ConvE~\cite{dettmers2017convolutional}
uses the convolutional network to 
capture interactions among embedding dimensions.
By sampling relational paths~\cite{guu2015traversing} from the KG,
RSN 
\cite{guo2019learning}
and Interstellar \cite{zhang2020interstellar}
use the recurrent network~\cite{hochreiter1997long} to recurrently combine the head entity and
relation 
with a step-wise scoring function.
As the KG is a graph,
R-GCN~\cite{schlichtkrull2018modeling} 
and CompGCN~\cite{vashishth2019composition} use the graph convolution network
\cite{kipf2016semi}
to aggregate entity-relation compositions layer by layer.
Representations 
at the final layer 
are then used to compute the scores.
Because of the use of an additional neural network,
NNM requires more parameters and has larger model complexity.


\parabegin{BiLinear Model (BLM).}
BLMs model the KG relation as a bilinear product between entity embeddings.
For example, RESCAL~\cite{nickel2011three} 
defines  as:
,
where , and 
.
To avoid overfitting, DistMult~\cite{yang2014embedding} requires
 to be 
diagonal, and
 reduces to a triple product:
.
However, it can only model symmetric relations.
To capture anti-symmetric relations,
ComplEx~\cite{trouillon2017knowledge} uses complex-valued embeddings  
with 
,
where 
 is the Hermitian product in complex space~\cite{trouillon2017knowledge}.
HolE~\cite{nickel2016holographic} uses the circular correlation instead of the dot product,
but is shown to be equivalent to ComplEx~\cite{hayashi2017equivalence}.

Analogy~\cite{liu2017analogical} decomposes the head embedding  into a real part 
and a complex part
.
Relation embedding  (resp. tail embedding ) is similarly decomposed into 
a real part  (resp. )
and a complex part
 (resp. ).  is then written
as:
,
which can be regarded as a combination of DistMult and ComplEx.
To simultaneously model the forward triplet  and its inverse ,
SimplE~\cite{kazemi2018simple} / CP~\cite{lacroix2018canonical} 
similarly
splits the embeddings to a forward part 
()
and a backward part ():
.
To allow more interactions among embedding dimensions,
the recent QuatE~\cite{zhang2019quaternion} 
uses embeddings in the hypercomplex space () to 
model

where
 
is the Hamilton product.
By using the Tucker decomposition~\cite{tucker1966some},
TuckER~\cite{balavzevic2019tucker} proposes a generalized bilinear model 
and introduces more parameters in the core tensor :
,
where
 is the tensor product along the th mode.
A summary of 
these BLMs is  
in
Table~\ref{tab:scofun}.




\subsection{Common Learning Tasks in KG}


\subsubsection{KG Completion}
\label{sec:app1}

KG
is naturally incomplete~\cite{singhal2012introducing}, and
KG completion is a representative task in KG learning \cite{wang2017knowledge,nickel2011three,bordes2013translating,kazemi2018simple,trouillon2017knowledge,yang2014embedding,dettmers2017convolutional}.  
Scores on the observed triples are maximized, while those on the non-observed  triplets are minimized.
After training,
new triples 
can be added
to the KG
by entity prediction with either a missing head
entity

or a missing tail entity
~\cite{wang2017knowledge}.
For each kind of query,
we enumerate all the entities 
and compute the corresponding scores  or
.
Entities with larger scores are more likely to be true facts.
Most of the models in Section~\ref{ssec:kg} can be directly used for KG completion.


\subsubsection{Multi-hop Query}
\label{sec:app2}

In KG completion, we predict queries  with length one, i.e., -hop query.  In practice, there can be multi-hop queries with lengths larger than one~\cite{guu2015traversing,hamilton2018embedding,wang2017knowledge}.  For example, one may want to predict ``\textit{who is the sister of Tony's mother}".  
To solve this problem, we need to solve the length-2 query problem  with the relation composition operator .

Given the KG ,
let 
,
corresponding to the relation ,
be a binary function
.
The multi-hop query 
is defined
as follows.
\begin{definition}[Multi-hop query~\cite{hamilton2018embedding,ren2019query2box}]
	The multi-hop query  with length 
	is defined as
	
	where  is the conjunction operation,  is the starting entity,  is the entity to predict,
	and  are intermediate entities that connect the conjunctions.
\end{definition}

Similar to KG completion,
plausibility of a query  
is measured
by
a scoring function 
\cite{guu2015traversing,hamilton2018embedding}:

where  is a relation-specific matrix of the th relation.
The key is on how to model the composition of relations in the embedding space.
Based on TransE~\cite{bordes2013translating},
TransE-Comp~\cite{guu2015traversing}
models the composition operator as addition, and 
defines
the scoring function 
as
.
Diag-Comp~\cite{guu2015traversing}
uses the multiplication operator in DistMult~\cite{yang2014embedding} to define
,
where .
Following RESCAL~\cite{nickel2011three},
GQE~\cite{hamilton2018embedding} 
performs
the composition with a product of relational matrices
, as:
.
More recently,
Query2box~\cite{ren2019query2box} models the composition of relations as a projection of box embeddings
and defines an entity-to-box distance to measure the score.



\subsubsection{Entity Classification}
\label{sssec:entclass}

Entity classification aims at predicting the labels of the unlabeled entities.
Since the labeled entities are few, a common approach is to 
use a graph convolutional network
(GCN)~\cite{kipf2016semi,gilmer2017neural} 
to aggregate neighborhood information.
The GCN
operates on the local neighborhoods of each entity
and aggregates the representations 
layer-by-layer as:

where 
 contains all the training triples,
 is the activation function,

are 
the 
layer-
representations of  and the  neighboring entities ,
respectively,
and  are weighting matrices sharing across different entities
in the th layer.

GCN does not encode relations in edges. To alleviate this problem,
R-GCN~\cite{schlichtkrull2018modeling} and CompGCN~\cite{vashishth2019composition}
encode relation  
and entity  
together by a
composition function :

where 
 is the representation of relation  
at the th layer.
The composition function  
can significantly impact performance~\cite{vashishth2019composition}.
R-GCN 
uses the composition operator in RESCAL~\cite{nickel2011three}, and
defines
,
where  is a relation-specific weighting matrix in the th layer.
CompGCN,
following TransE~\cite{bordes2013translating}, DistMult~\cite{yang2014embedding}
and HolE~\cite{nickel2016holographic}, 
defines
three operators:
subtraction , 
multiplication
 where
 is the 
element-wise product,
and circular correlation  where .


\subsection{Automated Machine Learning (AutoML)}
\label{sec:automl}

Recently,
automated machine learning (AutoML)~\cite{automl_book,quanming2018auto}
has 
demonstrated its advantages
in the design of better machine learning models.
AutoML is often formulated as a bi-level optimization
problem~\cite{colson2007overview}, in which
model parameters 
are updated
from the training data in the inner loop, while
hyper-parameters are tuned from the validation data in the outer loop.
There are three important components
in AutoML~\cite{automl_book,quanming2018auto,bender2018understanding}:
\begin{enumerate}[leftmargin=10.9px]
	\item
	\textit{Search space:}
This identifies important properties of the learning models to search.
The search space should be large enough to cover most manually-designed models,
while specific enough to ensure that the search will not be too expensive.
	
\item \textit{Search algorithm}: 
A search algorithm is used to search for good solutions in the designed space.  
Unlike convex optimization problems, there is no universally efficient optimization tool.

\item \textit{Evaluation}: 
Since the search aims at improving performance, evaluation is needed 
to offer feedbacks to the search algorithm.
The evaluation procedure should be fast and the signal should be accurate.
\end{enumerate}



\subsubsection{Neural Architecture Search (NAS)}
\label{ssec:nas}

Recently, 
a variety of NAS algorithms have been developed to facilitate efficient search
of deep networks
\cite{elsken2019neural,automl_book,zoph2017neural}.
They can generally be divided into 
model-based approach and sample-based approach~\cite{quanming2018auto}.
The model-based approach builds a surrogate model for all candidates in the
search space,
and selects candidates with promising performance
using methods such as 
Bayesian optimization~\cite{feurer2015efficient},
reinforcement learning~\cite{zoph2017neural,pham2018efficient},
and gradient descent~\cite{liu2018darts,yao2019differentiable}.
It requires evaluating a large number of architectures 
for training the surrogate model
or requires a differentiable objective w.r.t. the architecture.
The sample-based approach
is more flexible and
explores new structures
in the search space 
by using heuristics
such as 
progressive algorithm~\cite{liu2018progressive} 
and evolutionary algorithm~\cite{real2019regularized}.


As for evaluation,
parameter-sharing~\cite{liu2018darts,pham2018efficient,yao2019differentiable} 
allows faster architecture evaluation by
combining architectures in the whole search space
with the same set of parameters.
However,
the obtained results can
be sensitive to initialization, which hinders reproducibility.
On the other hand,
 stand-alone methods 
\cite{zoph2017neural,liu2018progressive,real2019regularized}
train 
and evaluate 
the different models 
separately.
They are slower but more reliable.
To improve its efficiency,
a predictor can be used to select promising architectures~\cite{liu2018progressive}
before it is fully trained.

\begin{figure*}[ht]
	\centering
	\small 
	\subfigure[DistMult.] {
	\includegraphics[height=3.1cm]{figure/distmult.pdf}}
	\quad
	\subfigure[SimplE.] {
		\includegraphics[height=3.1cm]{figure/simple.pdf}}
	\quad
	\subfigure[ComplEx.] {
		\includegraphics[height=3.1cm]{figure/complex.pdf}}
	\quad
	\subfigure[Analogy.] {
		\includegraphics[height=3.1cm]{figure/analogy.pdf}}
	\quad
	\subfigure[QuatE.] {
		\includegraphics[height=3.1cm]{figure/quate.pdf}}
	\vspace{-10px}
\caption{The forms of  for representative
BLMs (best viewed in color).  Different colors correspond to 
different parts of 
	(red for , blue for , yellow for , gray for ).
Solid lines mean positive values, while 
dashed lines mean negative values. 
The empty parts have value zero.
}
\label{fig:graphsf}
\vspace{-10px}
\end{figure*}



\section{Automated Bilinear Model}
\label{sec:search}

In the last decade, KG learning has been improving with new scoring function designs.
However, as different KGs
may have different properties,
it is unclear
how 
a proper scoring function 
can be designed 
for a particular KG.
This raises the question:
\textit{Can we automatically design a scoring function for a given KG}?
To address this question,
we first  provide a unified view of
BLMs, 
and then
formulate the 
design of scoring function 
as an AutoML problem:
AutoBLM
(``automated bilinear model").


\subsection{A Unified View of BLM}
\label{ssec:unified}

Recall 
from Section~\ref{ssec:kg} 
that a BLM may operate in  the
real/complex/hypercomplex space.
To write the different BLMs in the same form,
we first unify them to the same representation space.
The idea is to partition each of the 
embeddings 
to 
 equal-sized chunks, as
 and
.
The BLM 
is then written
in terms of .

\begin{itemize}[leftmargin=*]
\item DistMult~\cite{yang2014embedding}, which uses
.
We simply split 
  
(and analogously  and )
into 4 parts as , where
 for . Obviously, 

\item SimplE~\cite{kazemi2018simple} / CP~\cite{lacroix2018canonical},
which uses
.
We split  
(and analogously  and )
into 2 parts as  (where
),
and similarly  as 
(and analogously  and ). Then,


\item ComplEx~\cite{trouillon2017knowledge} / HolE~\cite{nickel2016holographic},
which uses
, where
  are complex-valued.
Recall that any complex vector 
 is of the form ,
where  is the real part and
 is the imaginary part.
 Thus,
 
We split
 
(and analogously  and )
into 2 parts  (where
),
and  similarly 
(and analogously  and ).
Then,


\item Analogy~\cite{liu2017analogical}, which uses
.
We split 
 
(and analogously  and )
into 2 parts
 (where
),  and similarly
 
(and analogously  and )
into 2 parts
 (where
).
Then,


\item QuatE~\cite{zhang2019quaternion}, which uses
.
Recall that any hypercomplex vector 
is of the form , where
. Thus, 

\end{itemize}
As ,
all the above BLMs can be written in the form of a bilinear function
 
where\footnote{With a slight abuse of notations, we still use  to denote the dimensionality after this transformation.}
,
and

is a matrix
with  blocks,
each block being either , or .
Figure~\ref{fig:graphsf} shows graphically
the  for the BLMs considered.

\subsection{Unified Bilinear Model}
\label{ssec:unifiedBLM}

Using the above unified representation, 
the design of BLM becomes designing  in (\ref{eq:bilinear}).

\begin{definition}[Unified BiLinear Model] 
	\label{def:unify}
The desired scoring function is  of the form

where
 
is called the {\em structure matrix}.
Here, we define , and .
\end{definition}
It can be easily seen that this covers all the BLMs 
in Section~\ref{ssec:unified}
when .  
Let  
be a matrix with  blocks, with its -th block:

The form in (\ref{eq:uni1}) can be written more compactly as
 
A graphical illustration is shown in Figure~\ref{fig:autosf}.

\begin{figure}[ht]
\centering
\vspace{-4px}
\includegraphics[width=0.9\columnwidth]{figure/s2r.pdf}
\vspace{-8px}
\caption{A graphical illustration of 
the proposed form of .}
\vspace{-4px}
\label{fig:autosf}
\end{figure}


The following Proposition gives a necessary and sufficient condition for the
BLM  with scoring function
in (\ref{eq:uni})
to be fully expressive.
The proof is in Appendix~\ref{app:expBLMs}.

\begin{prop} \label{pr:expBLMs}
Let
 
Given an 
in (\ref{eq:A}),
the bilinear model with scoring function 
\eqref{eq:uni}
is fully expressive
if
\begin{enumerate}
\item  
such that 
 is symmetric (i.e.,
),
and
\item  
such that 
 is skew-symmtric
(i.e.,
).
\end{enumerate}
\end{prop}
Table~\ref{tab:conditions} shows examples 
		of    and   
for the existing BLMs  (ComplEx, HolE, Analogy, SimplE, CP, and QuatE), thus justifying that they are fully expressive.

\begin{table}[ht]
	\centering
\caption{Example
		   (resp. ) for 
the 
two conditions
in Proposition~\ref{pr:expBLMs}.}
	\label{tab:conditions}
	\vspace{-10px}
	\setlength\tabcolsep{8pt}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c|c|c}
		\toprule
		model &    &    \\
		\midrule
		ComplEx / HolE &   &  \\
		Analogy  &   &  \\
		SimplE / CP &   &  \\
		QuatE  &     &   \\
		\bottomrule
	\end{tabular}
\vspace{-10px}
\end{table}


\subsection{Searching for BLMs}
\label{ssec:searSFs}

Using the family of unified BLMs in Definition~\ref{def:unify} as the search
space  for structure matrix
,
the search for a good data-specific BLM 
can be formulated
as
the following 
AutoML problem.

\begin{definition}[Bilinear Model Search (AutoBLM)]
\label{def:autoSF}
Let  be
a KG embedding model (where 
 includes the
entity embedding matrix
 and relation embedding matrix ,
and 

is the structure matrix)
and  be the performance measurement
of  on triples 
(the higher the better).
The AutoBLM
 problem is formulated as:

where

contains all the possible choices of ,

is the training set, and 
 is the
validation set.
\end{definition}
As a bi-level optimization problem,
we first train the model to obtain  (converged model parameters) 
on the training set  by 
\eqref{eq:autosf},
and then search for a better  (and consequently a
better relation matrix 
)
based on its performance  on the validation set 
in \eqref{eq:autosf:l1}.
Note that the objectives
in (\ref{eq:autosf:l1}) and 
(\ref{eq:autosf})
are non-convex, and
the search space is large
(with  candidates, as can be seen from
(\ref{eq:A})).
Thus, solving \eqref{eq:autosf} can be expensive and
challenging.




\subsection{Degenerate and Equivalent Structures}
\label{ssec:challange}

In this section, 
we introduce properties specific to the proposed search space .
A careful exploitation of these would be key to an efficient
search.



\subsubsection{Degenerate Structures}
\label{sssec:degenerate}

Obviously, not all structure matrices 
in (\ref{eq:A})
are
equally good.
For example, if all the nonzero blocks 
in 
are in the first column,
 will be zero
for all head embeddings with .
These structures should be avoided.

\begin{definition}[Degenerate structure]
Matrix
 
is degenerate
if (i) there exists 
such that ;
or (ii) there exists 
such that .
	\label{def:degenerate}
\end{definition}
With a degenerate ,
the triple  is always non-plausible for every nonzero
head embedding  or relation embedding ,
which limits expressiveness of the scoring function.
The following Proposition
shows that
it is easy to check whether  is degenerate.
Its proof is in Appendix~\ref{app:degenerate}.
\begin{prop} \label{pr:degenerate}
 is not degenerate if and only if

and
.
\end{prop}

Since   is very small (which is equal to 4 here), the above conditions are
inexpensive to check. Hence,
we can efficiently filter out degenerate 
's
and avoid wasting
time in training and evaluating these structures.







\subsubsection{Equivalence}
\label{sssec:equiv}

In general, two different 's can have the same performance (as measured by 
 in Definition~\ref{def:autoSF}).
This is captured in the following notion of equivalence.
If a group of 's are equivalent, we only need to evaluate one of them.

\begin{definition}[Equivalence]
	\label{def:equiv}
Let 

  and

.
If  but


for all 
,
then 
 is equivalent to 
(denoted ).	

\end{definition}

The following
Proposition
shows several conditions for two structures to be equivalent.
Its proof is in Appendix~\ref{app:equiv}.
Examples are shown in Figure~\ref{fig:equiv}.

\begin{figure}[t]
	\centering
	\vspace{-3px}
	\subfigure[Analogy. \label{fig:ana}]
	{\includegraphics[height=3.1cm]{figure/analogy.pdf}}
	\qquad\quad
	\subfigure[Permuting rows and columns. \label{fig:cond1}]
	{\includegraphics[height=3.1cm]{figure/equiv1.pdf}}
	
	\subfigure[Permuting values. \label{fig:cond2}]
	{\includegraphics[height=3.1cm]{figure/equiv2.pdf}}
	\qquad\quad
	\subfigure[Flipping signs. \label{fig:cond3}]
	{\includegraphics[height=3.1cm]{figure/equiv3.pdf}}
	\vspace{-8px}
	\caption{Illustration of  for Analogy (Figure~\ref{fig:ana}) and
		three example equivalent structures
		based on the conditions in Proposition~\ref{pr:equiv}.
		Figure~\ref{fig:cond1} permutes the index  of rows and columns in  to ;
		Figure~\ref{fig:cond2} permutes the values  in  to ;
		Figure~\ref{fig:cond3} flips the signs of values  in  to .}
	\label{fig:equiv}
	\vspace{-8px}
\end{figure}

\begin{prop} \label{pr:equiv}
Given an  in (\ref{eq:A}), construct
 such
	that
	
	if ,
	and 0
otherwise.\footnote{Intuitively,
in , the indexes of nonzero values 
in its 
	-th
row 
indicate positions of elements in  whose absolute values are 
	.}
Two structure matrices
 and  are equivalent
if any one of the following conditions 
is satisfied.
\begin{enumerate}[label=(\roman*)]
	\item 
	Permuting rows and columns: There
exists a permutation matrix  such that .
\item 
Permuting values:
There exists a permutation matrix  such
that
		;
		\item Flipping signs: There exists a sign vector 
		such that 
.
\end{enumerate}
\end{prop}


There are  possible permutation matrices for conditions
(i) and (ii), and  
possible sign vectors for condition (iii). Hence, one has to check
a total of  combinations.



\section{Search Algorithm}
\label{sec:progred}

In this section, 
we 
design efficient 
algorithms 
to search  for
the structure matrix 
 
in (\ref{eq:A}).
As discussed in Section~\ref{ssec:nas},
 the
model-based approach
requires a proper surrogate model for such a complex space.
Thus,
we will focus on the sample-based approach, particularly on 
the progressive algorithm 
and evolutionary algorithm.
To search efficiently, 
one needs to (i) ensure that each new  is neither 
degenerate 
nor equivalent to an already-explored structure; and (ii) the
scoring function  obtained from the new  is likely
to have high
performance.
These can be achieved by designing an efficient filter
(Section~\ref{ssec:filter}) and performance predictor
(Section~\ref{sssec:predictor}).
Then, we introduce two search algorithms: progressive search
(Section~\ref{ssec:progressive})
and evolutionary
algorithm
(Section~\ref{ssec:evolution}).




\subsection{Filtering Degenerate and Equivalent Structures}
\label{ssec:filter}

Algorithm~\ref{alg:filter} shows the filtering procedure.
First,
step~\ref{step:filt:rank}
removes
degenerate structure matrices 
by
using the conditions in Proposition~\ref{pr:degenerate}.
Step~\ref{step:filt:gen} then generates a set of  structures that are equivalent to 
(Proposition~\ref{pr:equiv}).
 is filtered out
if any of its equivalent structures appears in the set 
 containing
structure matrices that have
already been explored.
As  is small,
this filtering 
cost 
is very low
compared with the cost of model training in \eqref{eq:autosf}.

\begin{algorithm}[ht]
	\caption{
		Filtering degenerate and equivalent structure matrices. The
		output is ``False" if the input structure matrix
		 is to be filtered out.}
	\label{alg:filter}
	\small
	\begin{algorithmic}[1]
		\REQUIRE : a  structure matrix, : a set of structures.
		\STATE \textbf{initialization:} .
		\STATE \textbf{if}  or 
		,
		\textbf{then} . \label{step:filt:rank}
		\STATE 
		generate a set of equivalent structures  by
		enumerating permutation matrices 's and sign vectors 's. \label{step:filt:gen}
		\FOR{ in }
		\STATE \textbf{if} ,  \textbf{then} ,
		and exit the loop.
		\label{step:filt:equiv}
		\ENDFOR
\RETURN .
	\end{algorithmic}
\end{algorithm}


\subsection{Performance Predictor}
\label{sssec:predictor}


After collecting  structures 
in , 
we construct a predictor  to estimate the goodness of each 
.
As mentioned in Section~\ref{sec:automl}, 
search efficiency depends heavily on  how to 
evaluate the candidate models.

A highly efficient approach is parameter sharing, 
as is popularly used in
one-shot neural architecture search (NAS)~\cite{pham2018efficient,liu2018darts}.
However,
parameter sharing
can be problematic
when used to predict the performance 
of scoring functions.
Consider 
the following two 
's: (i)
 is a  matrix of all 's, and so , and
(ii)
 is a  matrix of all 's, and so .
When parameter sharing is used, it is likely that the performance predictor will output different
scores for
 
and .
However, 
from 
Proposition~\ref{pr:equiv},
by setting 
in condition (iii),
we have
 and thus they indeed have the same performance.
This problem 
will also be empirically demonstrated in Section~\ref{sec:exp:ps}.
Hence, instead,
we train and evaluate the models separately as in the 
stand-alone NAS evaluation 
\cite{zoph2017neural,liu2018progressive}.

\begin{algorithm}[ht]
	\caption{Construction of the symmetry-related feature (SRF) vectors.}
	\small
	\label{alg:srf}
	\begin{algorithmic}[1]
		\REQUIRE structure matrix .
		\STATE \textbf{initialization:} .
		\FOR{}
		\IF{}
		\STATE ;
		\STATE ;
		\\ // \textit{for symmetric case}
		\STATE \textbf{if} {} 
		\textbf{then} ; \label{step:srf:sym}
		\\ // \textit{for skew-symmetric case}
		\STATE \textbf{if} {} 
		\textbf{then} ;  \label{step:srf:ssym}
		\ENDIF
		\ENDFOR
		\RETURN .
	\end{algorithmic}
\end{algorithm}

Recall  from Section~\ref{ssec:kg}
that 
it is desirable for
the scoring function
to be fully expressive.
Proposition~\ref{pr:expBLMs} shows that this requires looking for 
a  
such that  is symmetric and
a  
such that  is skew-symmetric.
This motivates us to examine 
each of the 
 
's in  (defined in (\ref{eq:C}))  and see whether it 
leads to a symmetric or skew-symmetric
.
However, 
directly using all these  choices as features to a performance predictor 
can be computationally expensive.
Instead,
empirically we find  
that
the following two features can be used to group
the scoring functions:
(i) number of zeros in :
;
and (ii)
number of nonzero absolute values in :
.
The possible choices
is reduced 
to  (groups of scoring functions).
We keep two 
	symmetry-related feature (SRF)
	as
	 and .
If  is symmetric (resp. skew-symmetric) for any
 in ,
the 
entry in  (resp. ) 
corresponding 
to  
is set to 1.
The construction process is also shown in Algorithm~\ref{alg:srf}.
Finally,
the SRF vector is 
composed with  and ,
which vectorize the values in  and ,
and
fed as input 
to a two-layer MLP
for performance prediction.

\begin{algorithm}[t]
	\caption{Progressive search algorithm (AutoBLM).}
	\label{alg:greedy}
	\small
	\begin{algorithmic}[1]
		\REQUIRE
		: number of top structures;
		: number of generated structures;
		: number of structures selected by ;
		:  number of nonzero elements in initial structures;
		filter  and performance predictor .
		\STATE \textbf{initialization:} 
		, create a candidate set ;
		\label{step:greedyinit}
		\FOR{each } \label{step:Kstart}
		\STATE \textbf{if}  from Algorithm~\ref{alg:filter} is true \\ \textbf{then} ;
		\STATE \textbf{if} , break loop;
		\ENDFOR  \label{step:Kend}
		\STATE \textit{train} and \textit{evaluate} all 's in ;
		\STATE add 's to  and record the performance in ;
		\STATE update predictor  with records in .
		\REPEAT
		\STATE ;  \label{step:greedyb}
		\STATE ;
		\REPEAT	 \label{step:gen-start}
		\STATE randomly select a top- structure ; \label{step:top-struct}
		\STATE randomly generate , , 
		and form  with
;
		\label{step:gen}
		\STATE \textbf{if}  from Algorithm~\ref{alg:filter} is true \\ \textbf{then} ;
\label{step:filter}
		\UNTIL{}  \label{step:gen-end}
		
		\STATE select top- 's in  based on the \textit{predictor} ;
		\label{step:predict}    
		
		\STATE \textit{train} embeddings and \textit{evaluate} the performance of 's;
		\label{step:train}
		
		\STATE add 's in  and record the performance in ;
		\label{step:record}
\STATE update the predictor  (the following commented out)
		 with (, );
		\label{step:update} 
		\UNTIL{budget is exhausted or ;}
		\STATE select the top- structures in  based on performance in  to form the set .
		\RETURN .
		\label{step:return}
	\end{algorithmic}
\end{algorithm}


\subsection{Progressive Algorithm}
\label{ssec:progressive}

To explore the search space  in \eqref{eq:space},
the simplest approach 
is by direct sampling.
However,
it can be expensive as the
space
is large.
Note from \eqref{eq:uni1} that the 
complexity of  is controlled by
the number of nonzero elements in . 
Inspired by~\cite{liu2018progressive},
we 
propose in this section a progressive algorithm
that starts with
's having only a few
nonzero elements, 
and
then gradually expands the search space
by allowing more nonzeros.

The procedure,
which is called AutoBLM,
is in Algorithm~\ref{alg:greedy}.
Let  be an
 with
 nonzero elements, and the corresponding BLM be .
In step~\ref{step:greedyinit}, we initialize  to some  and create an empty candidate set .
As 's with fewer than  nonzero elements are degenerate
(Proposition~\ref{pr:degenerate}),
we
use . 
We first sample 
positions of  
 
nonzero elements, and
then
randomly assign them
values in .
The other entries
are set 
to zero.

Steps~\ref{step:Kstart}-\ref{step:Kend}
	filter away degenerate and equivalent structures.
The number of nonzero elements  is then increased by 
(step~\ref{step:greedyb}).
For each such ,
steps~\ref{step:gen-start}-\ref{step:gen-end}
greedily  select a top-performing 
structure (evaluated based on the mean reciprocal rank (MRR)~\cite{wang2017knowledge} 
performance on ) in ,
and
generate   candidates.
All the candidates 
are checked by the filter
 (Section~\ref{ssec:filter}) 
to avoid degenerate 
or equivalent solutions.
Next, the
predictor  in Section~\ref{sssec:predictor}
selects
the top- 's,
which are then trained and evaluated in step~\ref{step:train}.
The training data for  is collected with the recorded structures 
and performance
in  at step~\ref{step:update}.
Finally,
the top- structures in  evaluated by the corresponding performance
in  are returned.



\begin{algorithm}[t]
	\caption{Evolutionary search algorithm. (AutoBLM+).}
	\label{alg:evolution}
	\small
	\begin{algorithmic}[1]
		\REQUIRE 
		: number of top structures;
		: number of generated structures;
		: number of structures selected by ;
		: number of nonzero elements in initial structures;
		filter , and performance predictor .
		\STATE \textbf{initialization:} ; \label{step:evol:init}
		\FOR{each }
		\STATE \textbf{if}  from Algorithm~\ref{alg:filter} is true  \textbf{then} ;
		\STATE \textbf{if} , break loop;
		\ENDFOR 
		\STATE \textit{train} and \textit{evaluate} all 's in ; \label{step:evol:endinit}
		\STATE add 's to  and record the performance in ;
		\REPEAT	\label{step:evo-init}
		
		\STATE update predictor  with records in .
		\REPEAT
\STATE ;
		\STATE \textbf{mutation:} sample  and mutate to ; \textbf{or}
		\STATE \textbf{crossover:} sample , and use crossover to generate ;
		\label{step:evo-co}
\label{step:evo-mu}
\STATE \textbf{if}  is true by Algorithm~\ref{alg:filter}, 
		\\ \textbf{then} ;
		\label{step:evo-filter}
		\UNTIL{}; \label{step:evo-end}
		\STATE  
		select top- structures  in  based on the 
		the \textit{predictor} ;
\label{step:evo-pred}
		\FOR{
			each top- structure
			 
		}
		\STATE  \textit{train} embeddings and \textit{evaluate} the performance of ;
		\label{step:evo-train}
		\STATE \textbf{survive:} update  with  if  is better than the worst structure in ;
		\label{step:evo-update}
		\ENDFOR
		\STATE add 's in  and record the performance in ;
		\UNTIL{budget is exhausted;}
		\RETURN .
	\end{algorithmic}
\end{algorithm}


\subsection{Evolutionary Algorithm}
\label{ssec:evolution}

While progressive search 
can be efficient,
it may not fully explore the search space and can lead to 
sub-optimal solutions~\cite{tropp2004greed}.
The progressive search can only generate structures from fewer non-zero elements
to more ones.
Thus, it can not visit and adjust the structures with fewer non-zero elements
when  is increased.
To address these problems, we consider in this section the use of evolutionary
algorithms~\cite{back1996evolutionary}.


The procedure,
which is called AutoBLM+,
is in Algorithm~\ref{alg:evolution}.
As in Algorithm~\ref{alg:greedy},
we start with structures having  nonzero elements.
Steps~\ref{step:evol:init}-\ref{step:evol:endinit}
initializes a set  of  non-degenerate and non-equivalent structures.
The main difference with 
Algorithm~\ref{alg:greedy}
is in steps~\ref{step:evo-init}-\ref{step:evo-end},
in which new structures are generated 
by mutation and crossover.
For a given structure ,
mutation changes the value of 
each entry to another one in  
with 
a small 
probability .
For crossover,
given two structures   and ,
each entry of the new structure
has equal probabilities
to be selected from 
the corresponding entries in 
  or .
After mutation or crossover,
we check if the newly generated  
has to be filtered out. 
After  structures are collected,
we 
use the performance predictor  in Section~\ref{sssec:predictor}
to select the top- structures.
These are then trained and evaluated for actual performance.
Finally, structures in  with performance 
worse than the newly
evaluated ones are replaced
(step~\ref{step:evo-update}).





\section{Experiments}

In this section,
experiments are performed on a number of KG tasks.
Algorithm~\ref{alg:full} shows the general 
procedure for each task. First,
we find a good hyper-parameter setting 
to train and evaluate different structures
(steps~\ref{step:before-for}-\ref{step:before-best}).
Based on the observation that the performance ranking of scoring functions is
consistent across different 's (details are in Appendix~\ref{sssec:transfer}),
we set  to a smaller value ()
to reduce model training time.
The search algorithm 
is then used
to obtain the set  of top-
structures (step~\ref{step:search}).
Finally,
the hyper-parameters 
are fine-tuned
with a larger , and 
the best structure
selected 
(steps~\ref{step:after}-\ref{step:after-endfor}).
Experiments are run on a RTX 2080Ti GPU with 11GB memory.  All algorithms are implemented in python~\cite{paszke2017automatic}.

\begin{algorithm}[ht]
	\caption{Experimental procedure for each KG task.
Here,  denotes
the hyper-parameters 
.}
	\label{alg:full}
	\small
	\begin{algorithmic}[1]
		\STATE // \textbf{stage 1}: \textit{configure hyper-parameters for scoring function search}.
			
		\FOR{} \label{step:before-for}
		\STATE fix , randomly select ,  and ;
		
		\STATE train \textit{SimplE} with , and evaluate the validation MRR; 
		\label{step:before-run}
		
\ENDFOR


		\STATE  
		select the best hyper-parameter setting ; \label{step:before-best} 
	 
		\STATE // \textbf{stage 2}: \textit{search scoring function} 
		\STATE 
		using hyper-parameter setting ,
		obtain the set  of top- structures from Algorithm~\ref{alg:greedy} or Algorithm~\ref{alg:evolution};
		\label{step:search}
		
		\STATE
		// \textbf{stage 3}: \textit{fine-tune the obtained scoring function}
		\FOR{
		(
for YAGO3-10)}
		\label{step:after}
			\STATE randomly select a structure ;
			\STATE randomly select , , 
			,
			and ;
			
			\STATE train the KG learning model with 
			structure  and hyper-parameter setting 
			
			
		\ENDFOR \label{step:after-endfor}
		
		\STATE 
select the best structure . 
		 
\end{algorithmic}
\end{algorithm}


\subsection{Knowledge Graph (KG) Completion}
\label{ssec:KGC}

In this section, we perform experiments 
on KG completion as introduced in Section~\ref{sec:app1}.  
we use the full multi-class log-loss~\cite{lacroix2018canonical},
which is more robust and
has better performance than negative sampling \cite{lacroix2018canonical,zhang2020autosf}.

\subsubsection{Setup}
\label{sssec:kgcsetup}

\parabegin{Datasets.}
Experiments are performed on
the following popular benchmark 
datasets:
WN18, FB15k, WN18RR, FB15k237, YAGO3-10,
ogbl-biokg and ogbl-wikikg2
(Table~\ref{tab:dataset}).
WN18 and
FB15k 
are introduced in~\cite{bordes2013translating}.
WN18 is
a subset of the lexical database WordNet~\cite{miller1995wordnet},
while FB15k is
a subset of the Freebase KG~\cite{bollacker2008freebase} for human knowledge.
WN18RR~\cite{dettmers2017convolutional} and FB15k237~\cite{toutanova2015observed}
are obtained
by removing the near-duplicates and inverse-duplicate relations
from
WN18 and FB15k.
YAGO3-10 is
created by~\cite{dettmers2017convolutional}, and
is a subset of 
the semantic KG
YAGO~\cite{suchanek2007yago},
which
unifies WordNet and Wikipedia. 
The
ogbl-biokg and ogbl-wikikg2 datasets are from
the open graph benchmark (OGB) \cite{hu2020open}, which
contains
realistic and large-scale datasets for graph learning.
The ogbl-biokg dataset is a biological KG
describing interactions among proteins, drugs, side effects and functions.
The ogbl-wikikg2 dataset is extracted from the Wikidata knowledge base \cite{vrandevcic2014wikidata}
describing relations among entities in Wikipedia.


\begin{table}[ht]
	\centering
	\vspace{-7px}
	\caption{Statistics of the KG completion datasets.}
	\vspace{-10px}
	\label{tab:dataset}
	\setlength\tabcolsep{4pt}
	\begin{tabular}{c|ccccc}
		\toprule
		& & &  \multicolumn{3}{c}{number of samples}\\ 
		data set                 & \#entity & \#relation &  training & validation &
		testing  \\ \midrule
		WN18~\cite{bordes2013translating}     &  40,943  &     18     &  141,442  &  5,000  & 5,000    \\
		FB15k~\cite{bordes2013translating}    &  14,951  &   1,345    &  484,142  & 50,000  & 59,071 \\
		WN18RR~\cite{dettmers2017convolutional} &  40,943  &     11     &  86,835   &  3,034  & 3,134   \\
		FB15k237~\cite{toutanova2015observed}   &  14,541  &    237     &  272,115  & 17,535  & 20,466  \\
		YAGO3-10~\cite{mahdisoltani2013yago3}   & 123,188  &     37     & 1,079,040 &  5,000  & 5,000    \\ 
		\hline
		ogbl-biokg & 94k & 51 & 4,763k  & 163k  & 163k \\
		ogbl-wikikg2 & 2500k  & 535  & 16,109k &  429k  & 598k \\ 
		\bottomrule	
	\end{tabular}
\vspace{-7px}
\end{table}




\begin{table*}[ht]
	\caption{Testing performance of MRR, H@1 and H@10 on KG completion.
		The best model is highlighted in bold and the second best is underlined.
``--'' means that results are not reported in those papers or their code on that
		data/metric is not available.
		CompGCN  uses the entire KG in each iteration and  so runs out of memory  on the
		larger data sets of WN18, FB15k and YAGO3-10.}
	\label{tb:comparison}
	\centering
\setlength\tabcolsep{4pt}
	\vspace{-10px}
	\begin{tabular}{cc|ccc|ccc|ccc|ccc|ccc}
		\toprule
		& &                \multicolumn{3}{c|}{WN18}                &               \multicolumn{3}{c|}{FB15k}                &               \multicolumn{3}{c|}{WN18RR}               &              \multicolumn{3}{c|}{FB15k237}              &              \multicolumn{3}{c}{YAGO3-10}               \\ 
		\multicolumn{2}{c|}{model}                  &        MRR        &       H@1        &       H@10       &        MRR        &       H@1        &       H@10       &        MRR        &       H@1        &       H@10       &        MRR        &       H@1        &       H@10       &        MRR        &       H@1        &       H@10       \\ \midrule
		(TDM)  
		&    TransH     &       0.521       &       ---        &       94.5       &       0.452       &       ---        &       76.6       &       0.186       &       ---        &       45.1       &       0.233       &       ---        &       40.1       &        ---        &       ---        &       ---        \\
		&       RotatE       &       0.949       &       94.4       &       95.9       &       0.797       &       74.6       &       88.4       & {0.476} &       42.8       &  \underline{57.1}   &       0.338       &       24.1       &       53.3       &        0.488        &       39.6        &       66.3        \\ 
		&	PairE   &   --- & --- & --- &  0.811  &  76.5   &  89.6  &  --- &   --- &    ---  &  0.351  &   25.6  &   54.4   &   ---   &   ---   &    --- \\
		\midrule
		(NNM)  
		& ConvE   &       0.942       &       93.5       &      {95.5}      &       0.745       &       67.0       &       87.3       &       0.46        &       39.        &      {48.}       &      {0.316}      &       23.9       &       49.1       &       0.52        &       45.        &      {66.}       \\
		&       RSN        &       0.94        &       92.2       &       95.3       &        ---        &       ---        &       ---        &        ---        &       ---        &       ---        &       0.28        &       20.2       &       45.3       &        ---        &       ---        &       ---        \\ 
		&       Interstellar        &       ---        &       ---       &       ---       &        ---        &       ---        &       ---        &        0.48        &       44.0        &       54.8       &       0.32        &       23.3       &       50.8       &        0.51        &      42.4        &       66.4        \\ 
		& CompGCN   &  --- & --- & --- & --- & --- & --- & 0.479 & 44.3 & 54.6 & 0.355 & 26.4 & 53.5 & --- & --- & --- \\ \midrule
		(BLM)  &   TuckER     &  \textbf{0.953}   &  \textbf{94.9}   &       95.8       &       0.795       &       74.1       &       89.2       &       0.470       & {44.3} &       52.6       & {0.358} & {26.6} &       54.4       &        ---        &       ---        &       ---        \\
		&                DistMult                 &       0.821       &       71.7       &       95.2       &       0.775       &       71.4       &       87.2       &       0.443       &       40.4       &       50.7       &      {0.352}      &       25.9       &       54.6       &       0.552       &       47.1       &       68.9       \\
		&                SimplE/CP                &       0.950       &       94.5       & \underline{95.9} &       0.826       & {79.4} &      {90.1}      &      {0.462}      &       42.4       &       55.1       &      {0.350}      &       26.0       &       54.4       &       0.565       & {49.1} &      {71.0}      \\ 
		&                HolE/ComplEx                 &       0.951       &       94.5       &       95.7       &  {0.831} &       79.6       & {90.5} &      {0.471}      &       43.0       &       55.1       &       0.345       &       25.3       &       54.1       & {0.563} & {49.0} &       70.7       \\
		&                 Analogy                 &       0.950       &       94.6       &       95.7       &       0.816       &       78.0       &   {89.8} &      {0.467}      &       42.9       &      {55.4}      &       0.348       &       25.6       & {54.7} &       0.557       &       48.5       & {70.4} \\
		&    QuatE      &       0.950       &       94.5       &       95.9       &       0.782       &       71.1       &       90.0       & {0.488} &       43.8       &  \textbf{58.2}   &       0.348       &       24.8       &       55.0       &        0.556       &       47.4        &       70.4        \\  \midrule
		\multicolumn{2}{c|}{AutoBLM}           & \underline{0.952} & \underline{94.7} &  \textbf{96.1}   &  \underline{0.853}   &  \underline{82.1}   &  \underline{91.0}   &  \underline{0.490}   &  \underline{45.1}   & {56.7} &  \underline{0.360}   &  \underline{26.7}   &  \underline{55.2}   &  \underline{0.571}   &  \underline{50.1}   &  \textbf{71.5}   \\ 
		\multicolumn{2}{c|}{AutoBLM+}       &     \underline{0.952}   &    \underline{94.7}     &    \textbf{96.1}     &    \textbf{0.861}  &  \textbf{83.2}  &   \textbf{91.3}   &  \textbf{0.492}       & 	\textbf{45.2}	   &  	{56.7}	   &  	\textbf{0.364}	   &    \textbf{27.0}	 &   	\textbf{55.3}   &  \textbf{0.577}  &   \textbf{50.2}    &  \textbf{71.5}  \\ \bottomrule
	\end{tabular}
\vspace{-5px}
\end{table*}


\begin{figure*}[ht]
	\centering
{\includegraphics[height=3.5cm]{figure/WN18_perf.png}}\qquad
{\includegraphics[height=3.5cm]{figure/FB15k_perf.png}}\qquad
{\includegraphics[height=3.5cm]{figure/WN18RR_perf.png}}
	
{\includegraphics[height=3.5cm]{figure/FB15k237_perf.png}}\qquad
{\includegraphics[height=3.5cm]{figure/YAGO_perf.png}}
	
	\vspace{-10px}
	\caption{Convergence of the testing MRR versus running time 
		on the KG completion task.}
	\vspace{-10px}
	\label{fig:curve}
\end{figure*}




\parabegin{Baselines.}
For AutoBLM and AutoBLM+, 
we 
select the structure for evaluation 
from the set returned by 
Algorithm~\ref{alg:greedy}
or \ref{alg:evolution}
based on the 
MRR performance on
the validation set.

For WN18, FB15k, WN18RR, FB15k237, YAGO3-10,
AutoBLM and AutoBLM+
are compared
with the following popular
 human-designed 
KG embedding models\footnote{Obtained from
\url{https://github.com/thunlp/OpenKE} and \url{https://github.com/Sujit-O/pykg2vec}}:
(i) TDM, including
TransH~\cite{wang2014knowledge}, RotatE~\cite{sun2019rotate}
and PairE~\cite{chao2021pairre};
(ii)
NNM, including
ConvE~\cite{dettmers2017convolutional},
 RSN~\cite{guo2019learning}
 and CompGCN~\cite{vashishth2019composition};
(iii)
BLM, including
TuckER~\cite{balavzevic2019tucker},
Quat~\cite{zhang2019quaternion},
DistMult~\cite{yang2014embedding},
ComplEx~\cite{trouillon2017knowledge},
HolE~\cite{nickel2016holographic},
Analogy~\cite{liu2017analogical}
SimplE~\cite{kazemi2018simple},
and
CP~\cite{lacroix2018canonical}.
We do not
compare
with NASE~\cite{kou2020nase} as
its code is not publicly available.

For ogbl-biokg and ogbl-wikikg2 \cite{hu2020open}, we 
compare with
the models reported in the
OGB leaderboard\footnote{\url{https://ogb.stanford.edu/docs/leader_linkprop/}},
namely,
TransE~\cite{bordes2013translating},
		RotatE,
		PairE,
		DistMult, and
		ComplEx.




\parabegin{Performance Measures.}
The learned  
is evaluated in the context
of link prediction.  
Following 
~\cite{yang2014embedding,trouillon2017knowledge,liu2017analogical,kazemi2018simple,dettmers2017convolutional,wang2017knowledge},
for each triple ,
we first take  as the query and obtain the filtered rank on the head 

where
 are the training,
validation, and
test 
sets, respectively.
Next
we take   
as the query 
and obtain the filtered rank on the tail 

The following metrics
are computed from both the head and tail ranks on all triples:
(i) Mean reciprocal ranking (MRR):

and
(ii) 
H@: 
ratio of ranks no larger than , i.e., 

where  if  is true, otherwise . 
The larger the MRR or H@, the better is the embedding.
Other metrics 
for the completion task 
\cite{wang2019evaluating,tabacof2019probability}
can also be adopted here.

For ogbl-biokg and ogbl-wikikg2 \cite{hu2020open},
we only use the MRR as
the H@ is not reported by the baselines in the OGB leaderboard. 



\begin{figure*}[ht]
	\centering
\subfigure[WN18.]
	{\includegraphics[width=0.31\columnwidth]{figure/wn18.pdf}
	\label{fig:wn18}}
	\qquad
\subfigure[FB15k.]
	{\includegraphics[width=0.31\columnwidth]{figure/fb15k.pdf}
	\label{fig:fb15k}}
\qquad
	\subfigure[WN18RR. \label{fig:model1}]
	{\includegraphics[width=0.31\columnwidth]{figure/wn18rr.pdf}
	\label{fig:wn18rr}}
	\qquad
	\subfigure[FB15k237.]
	{\includegraphics[width=0.31\columnwidth]{figure/fb15k237.pdf}
	\label{fig:fb15k237}}
	\qquad
	\subfigure[YAGO3-10. \label{fig:model2}]
	{\includegraphics[width=0.31\columnwidth]{figure/yago.pdf}
	\label{fig:yago}}
	
	\vspace{-10px}
	\caption{Graphical illustration of the BLMs obtained by 
		AutoBLM (top) and
		AutoBLM+ (bottom) on the KG
		completion task (Section~\ref{exp:kgc:performance}).
		Different colors correspond to 
		different parts of (red), (blue), (yellow), (gray).
		Solid lines mean positive values, while 
		dashed lines mean negative values. 
		The empty parts have value zero.}
	\label{fig:searchedsf}
	\vspace{-10px}
\end{figure*}


\parabegin{Hyper-parameters.}
The search algorithms 
have the following hyper-parameters:
(i) : number of candidates generated after filtering;
(ii) : number of scoring functions selected by the predictor; 
(iii) : number of top structures selected in 
Algorithm~\ref{alg:greedy}
(step~\ref{step:top-struct}),
or the
number of structures survived in  in Algorithm~\ref{alg:evolution};
and
(iv) : number of nonzero elements in the initial set.
Unless otherwise specified,
we use 
, 
, 

and .
For the evolutionary algorithm,
the mutation and crossover operations are selected with
equal probabilities. 
When mutation is selected,
the value of each entry has a mutation probability of .
A budget is used to terminate the algorithm. This is set to 256 structures
on WN18, FB15k, WN18RR, FB15k-237,
128 on YAGO3-10,
64 on ogbl-biokg,
and 32 on ogbl-wikikg2.

We follow~\cite{lacroix2018canonical,trouillon2017knowledge} 
to use Adagrad~\cite{duchi2011adaptive} as optimizer.
The Adagrad
hyper-parameters 
are selected
from the following
ranges:
learning rate
 in , 
-penalty  in ,
batch size  in ,
and dimension  in .


\subsubsection{Results
on WN18, FB15k, WN18RR, FB15k237, YAGO3-10}
\label{exp:kgc:performance}

{\bf Performance.}
Table~\ref{tb:comparison}
shows
the testing results on
WN18, FB15k, WN18RR, FB15k237, and YAGO3-10.
As can be seen,
there is no clear winner
among the baselines.
On the other hand,
AutoBLM
performs consistently well.
It outperforms the baselines on FB15k, WN18RR, FB15k237 and YAGO3-10,
and is the first runner-up on WN18.
AutoBLM+
further improves AutoBLM on FB15k, WN18RR, FB15k237 and YAGO3-10.


\noindent
{\bf 
Learning curves.}
Figure~\ref{fig:curve}
shows the learning curves of 
representative models in each type of scoring functions,
including: RotatE in TDM;
ConvE and CompGCN in NNM; and 
DistMult, SimplE/CP, ComplEx/HolE, Analogy, QuatE 
and the proposed AutoBLM/AutoBLM+
in BLM.
As can be seen,
NNMs 
are much slower and inferior than 
BLMs.
On the other hand, 
AutoBLM+
has 
better performance 
and comparable time as the other BLMs.




\begin{table}[t]
	\centering
	\vspace{-5px}
	\caption{Testing MRR on applying the BLMs obtained
		from a source dataset (row) to a target dataset (column). 
		Bold numbers indicate the best performance each dataset for the models searched 
		by AutoBLM and AutoBLM+ respectively.}
	\label{tb:sf-dependent}
	\setlength\tabcolsep{1.5pt}
	\vspace{-10px}
	\begin{tabular}{cc|ccccc}
		\toprule
		& &	WN18	& 	FB15k	& 	WN18RR	&	FB15k237	& 	YAGO3-10		\\
		\midrule
		\multirow{5}{*}{AutoBLM} & WN18		& 	\textbf{0.952}			& 0.841	&	0.473	&	0.349	&	0.561	\\
		& FB15k		& 	0.950	&	\textbf{0.853}   &	0.470	&	0.350	&	0.563	\\
		& WN18RR	& 	0.951	&	 0.833	&	\textbf{0.490}	&	0.345	&	0.568	\\
		& FB15k237	& 	0.894	&	0.781	&	0.462	&	 \textbf{0.360}	&	0.565	\\
		& YAGO3-10	& 	0.885	&	0.835	&	0.466	& 0.352    &	\textbf{0.571}	\\
		\midrule
		\multirow{5}{*}{AutoBLM+} & WN18		& 	\textbf{0.952}	 	&	0.848	&	0.482	&	0.350	&	0.564	\\
		& FB15k		& 	0.951	&	\textbf{0.861 }  &		0.479	&	0.352	&	0.563	\\
		& WN18RR	& 		0.947	&	0.841	&	\textbf{0.492}	&	0.347	&	0.551	\\
		& FB15k237	& 	0.860	&	0.821	&	0.463	&	 \textbf{0.364}	&	0.546	\\
		& YAGO3-10	& 	0.951	&	0.833	&	0.469	&	0.345	&	\textbf{0.577}	\\
		\bottomrule
	\end{tabular}	
	\vspace{-8px}
\end{table}



\noindent
{\bf Data-dependent BLM structure.}
Figure~\ref{fig:searchedsf} 
shows the BLMs
obtained by 
AutoBLM and AutoBLM+. As can be seen,
they are different from 
the human-designed BLMs in Figure~\ref{fig:graphsf}
and are also different from each other.
To demonstrate that these data-dependent structures also have different accuracies on the same dataset,
we take the 
BLM obtained by AutoBLM (or AutoBLM+) on a source dataset and then 
evaluate it
on a different target dataset.
Table~\ref{tb:sf-dependent} shows the testing MRRs obtained (the trends
for H@1 and H@10  are
similar).
As can be seen,
the different BLMs perform differently on the same dataset, 
again confirming the
need for data-dependent structures.











\subsubsection{Results
on 
ogbl-biokg and ogbl-wikikg2}
\label{exp:kgc:ogb}




Table~\ref{tb:comparison:ogb} shows the testing MRRs of
the baselines (as reported in the OGB leaderboard),
the BLMs obtained by AutoBLM and AutoBLM+.
As can be seen,
AutoBLM and 
AutoBLM+ achieve significant gains on the testing MRR 
on both datasets, 
even though
fewer model parameters are needed for AutoBLM+.
The searched structures are provided in Figure~\ref{fig:structure:ogb}
in Appendix~\ref{app:figures}.

\begin{table}[ht]
	\vspace{-7px}
	\caption{Testing MRR and number of parameters on ogbl-biokg and ogbl-wikikg2.
The best performance is indicated in boldface.}
	\label{tb:comparison:ogb}
	\centering
		\setlength\tabcolsep{8pt}
	\vspace{-10px}
	\begin{tabular}{c|cc|cc}
		\toprule
		& \multicolumn{2}{c|}{ogbl-biokg} & \multicolumn{2}{c}{ogbl-wikikg2} \\
		model    & MRR & \# params & MRR  & \# params \\
		\midrule
		TransE   & 0.745      & 188M      & 0.426         & 1251M     \\
		RotatE   & 0.799  & 188M      & 0.433        & 1250M     \\
		PairE    	& 0.816     & 188M      & 0.521         & 500M      \\
		DistMult & 0.804      & 188M      & 0.373        & 1250M     \\
		ComplEx  & 0.810      & 188M      & 0.403         & 1250M     \\
		\midrule
		AutoBLM  &  {0.828}    &  188M   &   {0.532}  &  500M \\
		AutoBLM+ & \textbf{0.831}     & 94M       & \textbf{0.546}    & 500M     \\
		\bottomrule
	\end{tabular}
\vspace{-8px}
\end{table}


\subsubsection{Ablation Study 1: Search Algorithm Selection}
\label{exp:alg:compare}

First,
we 
study the following 
search algorithm
choices.
\begin{enumerate}[label=(\roman*)]
\item Random, which
samples each element 
of  
	independently and uniformly 
	from ;
	
\item {Bayes}, which 
	selects each element of  
	from 
	by
performing hyperparameter optimization using the Tree
	Parzen estimator \cite{bergstra2011algorithms} and Gaussian mixture model
	(GMM);
	
	\item Reinforce,
	which
	generates the  elements in 
	by using a LSTM~\cite{hochreiter1997long} 
	recurrently 
	as in NAS-Net~\cite{zoph2017neural}.
	The LSTM is optimized with REINFORCE~\cite{williams1992simple};
	
	\item AutoBLM (no Filter, no Predictor, ) with initial ;
	
	\item AutoBLM+ (no Filter, no Predictor, ) with initial .
\end{enumerate}

For a fair comparison, we do not use the filter and performance
predictor
in the proposed 
	AutoBLM and
	AutoBLM+ here.
All structures selected by each of the above algorithms are trained and evaluated
with the same hyper-parameter settings in step~\ref{step:before-best} of Algorithm~\ref{alg:full}.
Each algorithm evaluates 
a total
of 256 structures.

\begin{figure}[ht]
	\vspace{-3px}
	\centering
	\includegraphics[height=3.4cm]{figure/WN18RR_alg.png}\hfill
\includegraphics[height=3.4cm]{figure/FB15k237_alg.png}
	\vspace{-10px}
	\caption{Comparison of different search algorithms.}
	\label{fig:automl}
	\vspace{-4px}
\end{figure}

Figure~\ref{fig:automl}
shows the mean 
validation MRR of the top 
 structures
w.r.t. clock time during the search process.
As can be seen,
AutoBLM (no Filter, no Predictor, ) and 
AutoBLM+ (no Filter, no Predictor, ) outperform the rest at the later stages.
They have
poor initial performance as they start with structures having few nonzero elements,
which can be degenerate.
This will be further demonstrated in the next section.





\subsubsection{Ablation Study 2: 
Effectiveness of  the
Filter}
\label{exp:filter}

Structures with more nonzero elements are more likely to satisfy the
two conditions in 
Proposition~\ref{pr:degenerate}, and 
thus less likely to be
degenerate.
Hence,
the filter is expected to be particularly useful
when there are few nonzero elements 
in the structure.
In this experiment,
we demonstrate this by
comparing
AutoBLM/AutoBLM+ with and without
the filter.
The performance predictor is always enabled.


\begin{figure}[ht]
	\centering
\includegraphics[height=3.4cm]{figure/WN18RR_filt1.png}\hfill
	\includegraphics[height=3.4cm]{figure/WN18RR_filt2.png}
	
	\includegraphics[height=3.4cm]{figure/FB15k237_filt1.png}\hfill
	\includegraphics[height=3.4cm]{figure/FB15k237_filt2.png}
	\vspace{-10px}
	\caption{Comparison of the effect of filter.}
	\vspace{-10px}
	\label{fig:filter}
\end{figure}

Figure~\ref{fig:filter}
shows the mean 
validation 
MRR 
of the top  structures
w.r.t. clock time.
As expected,
when the filter is not used, using a larger  
will be more likely to have non-degenerate
structures  and thus
better performance, especially at the initial stages.
When the filter is used, the performance of both 
 
settings are improved. In particular,
with ,
the initial search space is simpler and leads to
better performance.


\subsubsection{Ablation Study 3: Performance Predictor}
\label{exp:predictor}

In this experiment, we
compare
the following 
AutoBLM/AutoBLM+
variants: (i)
AutoBLM (no-predictor) and AutoBLM+ (no-predictor), which simply
randomly select  structures for evaluation (in step~17 of
Algorithm~\ref{alg:greedy} and step 16 of
Algorithm~\ref{alg:evolution}, respectively);
(ii)
AutoBLM (Predictor+SRF) and AutoBLM+ (Predictor+SRF),
using the proposed SRF (in Section~\ref{sssec:predictor}) as input features to the performance predictor;
and (iii) AutoBLM (Predictor+1hot) and AutoBLM+ (Predictor+1hot),
which 
map each of
the  entries in  (with values in )
to 
a simple ()-dimensional
one-hot vector, and then 
use these as features to the performance predictor.
The resultant feature vector is thus -dimensional, which is
much longer
than the -dimensional
SRF 
representation.

\begin{figure}[ht]
	\centering
	\vspace{-5px}
	\includegraphics[height=3.4cm]{figure/WN18RR_pred.png}\hfill
	\includegraphics[height=3.4cm]{figure/FB15k237_pred.png}
	\vspace{-10px}
	\caption{Effectiveness of the performance predictor.}
	\vspace{-2px}
	\label{fig:predictor}
\end{figure}

Figure~\ref{fig:predictor} shows the mean validation MRR of the top 
structures w.r.t. clock time.
As can be seen,
the use of performance predictor improves the  results over
AutoBLM (no-Predictor) and AutoBLM+ (no-Predictor).
The SRF features also perform better than the one-hot features,
as the one-hot features are higher-dimensional and more difficult to learn.
Besides,
we observe that  
AutoBLM+ performs better than 
AutoBLM,
 as
it can 
more flexibly
explore  the search space.
Thus,
in the remaining ablation studies,
we will only focus on 
AutoBLM+.






\subsubsection{Ablation Study 4: Varying }
\label{sec:exp:varyK}

As  increases,
the search space, which has a size of 
(Section~\ref{ssec:searSFs}), increases dramatically.
Moreover, the SRF also needs to enumerate a lot more 
() vectors in .
In this experiment,
we demonstrate the dependence on  by running
AutoBLM+ with
.
To ensure that  is divisible by ,
we set .
Figure~\ref{fig:ks} shows
the top-8 mean MRR performance on the validation set of the searched models
versus clock time.
As can be seen, the best performance 
attained by different 's
are similar.
However,
 runs slower.

\begin{figure}[ht]
	\centering
	\vspace{-3px}
	\includegraphics[height=3.4cm]{figure/WN18RR_K.png}
	\hfill
	\includegraphics[height=3.4cm]{figure/FB15k237_K.png}
	
	\vspace{-10px}
	\caption{Comparison of different  values.}
	\vspace{-5px}
	\label{fig:ks}
\end{figure}



Table~\ref{tab:Ktime} shows
the running time of the
filter, performance predictor (with SRF features),
training and evaluation in Algorithm~\ref{alg:evolution}
with different 's.
As can be seen,
the costs of filter and performance predictor increase a lot with ,
while the model training and evaluation time are relatively stable for different 's.

\begin{table}[ht]
	\centering
	\vspace{-5px}
	\caption{Running time (in minutes) of different components in Algorithm~\ref{alg:evolution}.}
	\label{tab:Ktime}
	\vspace{-10px}
	\setlength\tabcolsep{4pt}
	\begin{tabular}{c|c|cccc}
		\toprule
		dataset &    &  filter  & performance predictor  & train  & evaluate \\
		\midrule
		\multirow{3}{*}{WN18RR} & 3  & 0.04 & 1 & 1217  & 152 \\
		& 4  & 1.4 &  23 &  1231 & 156 \\
		& 5  & 90 & 276 & 1252 & 161 \\  
		\midrule
		\multirow{3}{*}{FB15k237} & 3  & 0.04& 1 & 714 & 178  \\
		& 4  & 1.5 & 22 & 721 & 181 \\
		& 5  & 91 & 283 &  728 & 186 \\  
		\bottomrule
	\end{tabular}
\vspace{-10px}
\end{table}


\subsubsection{Ablation Study 5: Analysis of Parameter Sharing}
\label{sec:exp:ps}

As mentioned in 
Section~\ref{sssec:predictor},
parameter sharing may not reliably predict the model performance.
To demonstrate this,
we empirically compare the parameter-sharing approach, which 
shares parameter  
(where  is the entity embedding matrix and 
 is the
relation embedding matrix
in Section~\ref{ssec:kg})
and the stand-alone approach, which trains each model separately.
For parameter sharing,
we randomly sample a  
in each training  iteration
from the set of top candidate structures 
( in Algorithm \ref{alg:greedy} or  in Algorithm \ref{alg:evolution}),
and then update parameter .
After one training epoch,
the sampled structures 
are evaluated. 
After 500 training epochs,
the top-100 's 
are output.
For the stand-alone approach,
the 100 's 
are separately trained and evaluated.

\begin{figure}[ht]
	\centering
	\vspace{-5px}
	\includegraphics[height=3.4cm]{figure/WN18RR_share.png}
	\hfill
	\includegraphics[height=3.4cm]{figure/FB15k237_share.png}
	
	\vspace{-10px}
	\caption{
		MRRs of structures as estimated by the parameter-sharing approach and stand-alone
		approach.}
	\label{fig:share}
	\vspace{-6px}
\end{figure}

Figure~\ref{fig:share} shows the 
MRR estimated by parameter-sharing versus 
the true MRR obtained by individual model training.
As can be seen,
structures that have high
estimated MRRs 
(by parameter sharing)
do not truly have 
high MRRs.
Indeed, the Spearman's rank correlation coefficient\footnote{\url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}
between the two sets of
MRRs is negative
( 
on WN18RR  
and  
on FB15k237).
This demonstrates that 
the one-shot approach, though faster,
cannot find good structures.


\subsection{Multi-Hop Query}
\label{sec:exp:hop}

In this section, we perform experiment on 
multi-hop query
as introduced in Section~\ref{sec:app2}.  
The 
entity and relation
embeddings 
are optimized
by maximizing the scores on positive queries 
and minimizing the scores on negative queries, which 
are generated by replacing  with an incorrect entity.
On evaluation,
we rank the scores of queries   of all 
to obtain the ranking of ground truth entities.


\subsubsection{Setup}

Following~\cite{ren2019query2box},
we use the FB15k and FB15k237 datasets
in Table~\ref{tab:dataset}.
Evaluation is based  on two-hop (2p) and three-hop 
(3p)
queries.
Interested readers are referred to~\cite{ren2019query2box} 
for a more detailed description on query generation.
For FB15k,
there are 273,710 queries in the training set,
8,000 non-overlapping queries in the validation and testing sets.
For FB15k237,
there are 143,689 training queries,
and 5,000 queries for validation and testing.
The setting of the search algorithms' hyper-parameters are 
the same as in Section~\ref{ssec:KGC}.
For the learning hyper-parameters,
we search the dimension ,
and the other hyper-parameters are the same as those in Section~\ref{ssec:KGC}.
We use the MRR performance on the validation set
to search for structures as well as hyper-parameters.
For performance evaluation,  
we follow 
\cite{hamilton2018embedding,ren2019query2box},
and use 
the testing Hit@3 and MRR.

We compare with the following baselines:
(i) TransE-Comp~\cite{guu2015traversing}
(based on
TransE); (ii)
Diag-Comp~\cite{guu2015traversing}
(based on DistMult); 
(iii) GQE~\cite{hamilton2018embedding}, which
uses a 
 trainable 
matrix  
for composition,
and can be regarded as a composition based on RESCAL~\cite{nickel2011three}; and
(iv)
Q2B~\cite{ren2019query2box},
which is a recently proposed box embedding method.


\subsubsection{Results}

Results are
shown in Table~\ref{tab:exp:query}.
As can be seen, among the baselines,
TransE-Comp, Diag-Comp and GQE are inferior
to Q2B.
This shows that the general scoring functions
cannot be directly applied to model the complex interactions
in multi-hop queries.
On the other hand,
AutoBLM and
AutoBLM+ have better performance as
they can adapt to the different tasks with different matrices .
The obtained structures can be found in Appendix \ref{app:figures}.

\begin{table}[ht]
	\vspace{-5px}
	\caption{Testing performance of H@3 and MRR on multi-hop query task. Results of 's are copied from \cite{ren2019query2box}.}
	\label{tab:exp:query}
	\vspace{-10px}
	\centering
	\setlength\tabcolsep{2.5pt}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c|cc|cc|cc|cc}
		\toprule
		& \multicolumn{4}{c|}{FB15K}                    & \multicolumn{4}{c}{FB15K237}                       \\ 
		& \multicolumn{2}{c|}{2p} & \multicolumn{2}{c|}{3p} & \multicolumn{2}{c|}{2p}	
		& \multicolumn{2}{c}{3p} \\ 
		& H@3        & MRR       &   H@3       &  MRR     &  H@3        & MRR        &  H@3        &  MRR         \\ \midrule 
		TransE-Comp~\cite{guu2015traversing}	&  27.3     &   .264    & 15.8 &  .153     &  19.4 &  .177   & 14.0 & .134  \\ 
		Diag-Comp~\cite{guu2015traversing}   &   32.2   &	  .309  &  27.5   & .266  &   19.1   &  .187   & 15.5  &   .147         \\  
		GQE~\cite{hamilton2018embedding}                      & 34.6       & .320      & 25.0      &  .222   & 21.3       &  .193     & 15.5      &  .145          \\ 
		Q2B~\cite{ren2019query2box}     & 41.3       & .373      & 30.3     &    .274     & 24.0     &    .225     & 18.6      &  .173        \\ \midrule
		AutoBLM			& 41.5	& .402	&  29.1 &	.283  &	 23.6	&  .232	 &	  18.2	&  .180	\\  
		AutoBLM+        &   \textbf{43.2}   &    \textbf{.415}  &   \textbf{30.7}   &    \textbf{.293}   &     \textbf{24.9}     &      \textbf{.248}   &  \textbf{19.9}     &  	\textbf{.196}    \\ \bottomrule
	\end{tabular}
\vspace{-10px}
\end{table}


\subsection{Entity Classification}
\label{sec:exp:cls}

In this section, we perform experiment on 
entity classification
as introduced in 
Section~\ref{sssec:entclass}.


\subsubsection{Setup}

After aggregation for  layers,
representation 
at the last layer 
is transformed by a multi-layer perception (MLP) to
,
where
 
is 
the intermediate layer dimension,
and 
is the number of classes.
The parameters,
including embeddings of entities, relations,
, 's and the MLP,
are optimized by minimizing the cross-entropy loss 
on the labeled entities:
,
where  is the set of labeled entities,
 indicates whether the th entity belongs to class ,
and  is the th dimension of .

Three graph datasets 
are used 
(Table~\ref{tab:stat:entclass}):
AIFB, an affiliation graph;
MUTAG, 
a bioinformatics graph;
and BGS, 
a geological graph.
More details can be found in~\cite{ristoski2016rdf2vec}.
All entities do not have attributes.
The entities' and relations'
trainable embeddings
are used 
as input to the GCN.

\begin{table}[ht]
	\centering
	\vspace{-5px}
	\caption{Data sets used in entity classification.
	Sparsity is computed as .}
	\label{tab:stat:entclass}
	\vspace{-10px}
	\setlength\tabcolsep{2.5pt}
	\begin{tabular}{c|ccccccc}
		\toprule
		dataset & \#entity     & \#relation & \#edges   & \#train & \#test & \#classes & sparsity \\ \midrule
		AIFB    & 8,285   & 45  & 29,043  &  140     & 36     & 4     &  9.4e-6\\
		MUTAG   & 23,644  & 23  & 74,227  &  272     &   68   & 2   &  5.7e-6  \\
		BGS     & 333,845 & 103 & 916,199 &   117    &  29    & 2     & 8.0e-8 \\ \bottomrule
	\end{tabular}
\vspace{-5px}
\end{table}

The following five models
are compared:
(i) GCN~\cite{kipf2016semi},
with ,
does not leverage relations of the edges;
(ii) 
R-GCN~\cite{schlichtkrull2018modeling}, with 
;
(iii) CompGCN~\cite{vashishth2019composition} with 
 (-/*/) ,
in which the operator (subtraction/multiplication/circular correlation as
discussed in Section~\ref{sssec:entclass})
is chosen based on 
5-fold cross-validation;
(iv) AutoBLM; and (v) AutoBLM+.
oth 
AutoBLM and AutoBLM+
use the searched structure  to form 
.

Setting of the hyper-parameters  are the same as in
Section~\ref{ssec:KGC}.
As for the learning hyper-parameters,
we search the embedding dimension  
from , 
learning rate from  with Adam as the optimizer~\cite{kingma2014adam}.
For the GCN structure,
the hidden size is the same as the embedding dimension,
the dropout rate 
for each layer is from .
We search for 50 hyper-parameter settings
for each dataset based on the 5-fold classification accuracy.


For performance evaluation, we use 
the testing accuracy.
Each model runs 5 times, and then the average testing accuracy reported.



\subsubsection{Results}

Table~\ref{tab:classification}
shows the average testing accuracies.
Among the baselines, R-GCN is slightly better than CompGCN on the AIFB dataset,
but worse on the other two sparser datasets.
By searching the composition operators,
AutoBLM and AutoBLM+  outperform all the baseline methods.
AutoBLM+ is better than AutoBLM 
since it can find better structures 
with the same budget 
by the evolutionary algorithm.
The structures obtained are in Appendix \ref{app:figures}.


\begin{table}[ht]
	\centering
	\vspace{-5px}
	\caption{Classification accuracies (in \%) on entity classification task. Values marked ``*'' are copied from~\cite{vashishth2019composition}.}
	\label{tab:classification}
	\vspace{-10px}
	\begin{tabular}{c|ccc}
		\toprule
		 dataset   &        AIFB         &        MUTAG        &         BGS         \\ \midrule
 GCN     &   86.67    &   68.83     &   73.79     \\
R-GCN    &   92.78      &   74.12    &   82.97    \\
CompGCN   &  90.6   &  85.3   &   84.14    \\
AutoBLM  &   95.55 	&   85.00   &  84.83   \\
AutoBLM+ & \bf{96.66} & \bf{85.88} & \bf{86.17} \\ \bottomrule
	\end{tabular}
\vspace{-12px}
\end{table}


\section{Conclusion}
\label{sec:conclusion}

In this paper, we propose AutoBLM and AutoBLM+, 
the algorithms to automatically design and discover better scoring functions for KG learning.
By analyzing the limitations of existing scoring functions,
we setup the problem as searching relational matrix for BLMs.
In AutoBLM,
we use a progressive search algorithm
which is enhanced by a filter and a predictor with domain-specific knowledge,
to search in such a space.
Due to the limitation of progressive search,
we further design an evolutionary algorithm,
enhanced by the same filter and predictor,
called AutoBLM+.
AutoBLM and AutoBLM+ can efficiently design scoring functions
that outperform existing ones on tasks
including 
KG completion,
multi-hop query
and entity classification from the large search space.
Comparing AutoBLM with AutoBLM+,
 AutoBLM+ can
 design better scoring functions
with the same budget.


\section*{Acknowledgment}
This work was supported by the National Key Research and Development Plan under Grant 2021YFE0205700, the Chinese National Natural Science Foundation Projects 61961160704, and the Science and Technology Development Fund of Macau Project 0070/2020/AMJ.

\bibliographystyle{plain}
\bibliography{bib}

\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figure/bio/yongqi.png}}]{Yongqi Zhang}
	(Member, IEEE)
is a senior researcher in 4Paradigm.
He obtained his Ph.D. degree at the Department of Computer Science and Engineering of Hong Kong University of Science and Technology (HKUST) in 2020 and received his bachelor degree at Shanghai Jiao Tong University (SJTU) in 2015.
He has published five top-tier conference/journal papers as first-author, including NeurIPS, ACL, WebConf, ICDE, VLDB-J.
His research interests focus on knowledge graph embedding,
automated machine learning and graoh learning.
He was a Program Committee for AAAI 2020-2022, IJCAI 2020-2022, CIKM 2021, KDD 2022, ICML 2022, and a reviewer for TKDE and NEUNET.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figure/bio/quanming.png}}]{Quanming Yao} (Member, IEEE)
is a tenure-track assistant professor at Department of Electronic Engineering, Tsinghua University. Before that, he spent three years from a researcher to a senior scientist in 4Paradigm INC, where he set up and led the company's machine learning research team.  He is a receipt of Wunwen Jun Prize of Excellence Youth of Artificial Intelligence (issued by CAAI), the runner up of Ph.D. Research Excellence Award (School of Engineering, HKUST), and a winner of Google Fellowship (in machine learning).
Currently,
his main research topics are Automated Machine Learning (AutoML) and neural architecture search (NAS).
He was an Area Chair for ICLR 2022, IJCAI 2021 and ACML 2021; 
Senior Program Committee for IJCAI 2020 and AAAI 2020-2021; and a guest editor of IEEE TPAMI AutoML special issue in 2019.
\end{IEEEbiography}



\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figure/bio/kwok.png}}]{James T. Kwok} (Fellow, IEEE)
	received the Ph.D. degree in computer science from The Hong Kong University of Science and Technology in 1996. 
	He is a Professor with the Department of Computer Science and Engineering, Hong Kong
	University of Science and Technology. His research interests include machine
	learning, deep learning, and artificial intelligence. He received the
	IEEE Outstanding 2004 Paper Award and the Second Class Award in Natural Sciences by
	the Ministry of Education, China, in 2008. 
	He is serving as an Associate Editor for the IEEE Transactions on Neural Networks and Learning Systems, Neural Networks, Neurocomputing, Artificial Intelligence Journal, International Journal of Data Science and Analytics, Editorial Board Member of Machine Learning,
	Board Member, and Vice President for Publications of the Asia Pacific Neural Network
	Society. He also served/is serving as Senior Area Chairs / Area Chairs of major machine learning /
	AI conferences including NIPS, ICML, ICLR, IJCAI, AAAI and ECML.
\end{IEEEbiography}

\cleardoublepage
\appendices


\section{Proofs}
\label{app:proof}


Denote . 
For vectors and matrix, we use the uppercase  italic bold letters, such as 
, to denote matrices,
use uppercase normal bold letters, such as , to denote tensors,
lowercase bold letters, such as  to denote vectors,
and normal characters to indicate scalers, such as .
 are generally used to indicate index.
, still a matrix or vector, are the -th block of -chunk split,
while  is a scalar in the -th dimension of vector .
 indicates the -th row of matrix ,
and  indicates the -th column.
For , 
we use  with  to indicate the -th block,
while  with  
to indicate the element in the -th row and -th column of .
 means the smallest integer that is equal or larger than .


\subsection{Proposition~\ref{pr:expBLMs}}
\label{app:expBLMs}

Here,
we first show some useful lemmas
in Appendix~\ref{app:lem1},
then we prove 


\subsubsection{Auxiliary Lemmas}
\label{app:lem1}

Recall that 
 
We firstly show that 
any symmetric matrix can be factorized as a bilinear form if 
 
in Lemma~\ref{lm:sym};
any skew-symmetric matrix can be factorized as a bilinear form if

in Lemma~\ref{lm:ssym}.

\begin{lemma}
\label{lm:sym}
If ,
there exist  and  
such that any symmetric matrix .
\end{lemma}

\begin{proof}
For any real symmetric matrix ,
it can be decomposed as~\cite{horn2012matrix}

where  is an orthogonal matrix 
and  
is a diagonal matrix with  elements.
If ,
we discuss with two cases: 
1) there exist some non-zero elements in the diagonal;
2) all the diagonal elements are zero.

To begin with,
we evenly split
the relation embedding into  parts as
,
and
the entity embedding matrix into  blocks as
.
Then for the two cases:

\begin{enumerate}[leftmargin=*]
	\item If there exist non-zero elements in the diagonal,
	i.e. ,
	we denote any one of the index in the diagonal as  with .
	Then, 
	we have
	
	and .
	Next,
	we assign  with
	
	And  with
	
	where  returns the diagonal elements in the diagonal matrix 
	and 
	
	
	Based on \eqref{eq:lemma1:gsym}, \eqref{eq:lemma1:E1} and \eqref{eq:lemma1:g1},
	we have 
	
	
	\item If all the diagonal elements are zero,
	there must be some non-zero elements in the non-diagonal indices,
	i.e. 
	.
	We denote any one of the index in the non-diagonal indices as 
	with  and .
	Then we have
	
	and .
	
	Similarly,
	we assign 
	 with
	
	And  with
	
	which leads to
	
	Based on \eqref{eq:lemma1:gsym}, \eqref{eq:lemma1:E2} and \eqref{eq:lemma1:g2},
	we have
	
\end{enumerate}
Hence, 
there exist  and  
such that any symmetric matrix .
\end{proof}







\begin{lemma}
\label{lm:ssym}
If ,
there exist  and  
such that any skew-symmetric matrix .
\end{lemma}

\begin{proof}
First,
if ,
all the elements in the diagonal should be zero,
i.e. , with .
Therefore,
there must be some non-zero elements in the non-diagonal indices,
i.e.,
.
We denote any one of the index in the non-diagonal indices as
 with  and .
Then we have

and .
Next,
we assign 
 with

And  with

which leads to



Since  is skew-symmetric,
we have

Based on \eqref{eq:lemma2:E}, \eqref{eq:lemma2:g} and \eqref{eq:lemma2:gssym}
we have


Hence,
there exist  and  
such that any symmetric matrix .
\end{proof}




Based on Lemma~\ref{lm:sym} and~\ref{lm:ssym},
we prove the following lemma for any real-valued square matrices.


\begin{lemma}
	\label{lm:matrix}
	If
	1) ),
	and
	2) ),
	then
	there exist  and 
	that any matrix 
	can be written as 
	
	where , 
	.
\end{lemma}

\begin{proof}
In Lemma~\ref{lm:sym} and Lemma~\ref{lm:ssym},
we prove that any symmetric matrix can be factorized as   
with  and  
and any skew-symmetric matrix can be factorized as  

with
 and  .
In this part,
we show any square matrix 
can be split as the sum of a particular  and a particular 
and it can be factorized
in the bilinear form with the composition of  and .

We firstly composite
 in the proof of Lemma~\ref{lm:sym}
and 
 in the proof of Lemma~\ref{lm:ssym}
into
 and  .
The basic idea is to add the symmetric part into odd rows and skew-symmetric part into even rows.
Specifically,
we define the rows of 
 as

And the relation embedding  is element-wise set as

Based on the form of 
we can have 
where each element is formed by corresponding element in 
or  as

Refer to the notation introduced at the beginning of Appendix~\ref{app:proof}, 
 represents the -th element in the matrix,
while  represents the -th block in previous parts.
Note that
such a construction of  will not violate the structure matrix .
The construction of \eqref{eq:lemma:matrix-E}, \eqref{eq:lemma:matrix-r} 
are graphically illustrated in the left part of Figure~\ref{fig:comp-matrix},
which leads to the construction of \eqref{eq:lemma:matrix-g} in the right part.

\begin{figure}[ht]
	\centering
	\includegraphics[height=3.6cm]{figure/comp-matrix}
		\vspace{-5px}
	\caption{Graphical illustration of the composed embeddings. The blue parts are from ,  or  and the red parts are from ,  or . The white spaces in  are zero values.}
	\label{fig:comp-matrix}
\end{figure}

Given any matrix ,
it can be split into a symmetric part  and a skew-symmetric part ,
i.e.,

Based on 
\eqref{eq:lemma:matrix-E},
\eqref{eq:lemma:matrix-r},
\eqref{eq:lemma:matrix-g},
and \eqref{eq:lemma:split},
,
we have 

with  and  in \eqref{eq:lemma:matrix-r}.
\eqref{eq:lemma:g2} and \eqref{eq:lemma:g3} are 0 since  is zero when  and  are not simultaneously even or odd.
From \eqref{eq:lemma:g5} to \eqref{eq:lemma:g1}, we let  and 
to get the odd part in \eqref{eq:lemma:matrix-E} and \eqref{eq:lemma:matrix-g}.
From \eqref{eq:lemma:g6} to \eqref{eq:lemma:g4}, we let  and 
to obtain the even part in \eqref{eq:lemma:matrix-E} and \eqref{eq:lemma:matrix-g}.
\end{proof}	


Finally,
we show a Lemma
which bridges
3 order tensor  with 
bilinear scoring function of form \eqref{eq:uni}.
Given any KG with tensor form ,
we denote 
as the -th slice in the lateral of , i.e. ,
corresponding to relation .

\begin{lemma}
\label{lm:tensor}
Given any KG with tensor form ,
and structure matrix .
If all the 's can be
independently expressed by a unique entity embedding matrices 

and relation embedding ,
i.e. ,
with ,
then there exist entity embedding  
and relation embedding  
such that
.
\end{lemma}

\begin{proof}
We show that computing 
can be independently expressed by  for each relation .
Specifically,
we define the rows of entity embedding in  as

And each element in the relation embedding  as 

which leads to the element in   as


\begin{figure}[ht]
	\centering
	\includegraphics[height=3.5cm]{figure/comp-tensor}
	\vspace{-5px}
	\caption{Graphical illustration of the composed embeddings. Different colors represent components from different embeddings. White colors mean the zero values and gray colors represent spaces with mixed colors.}
	\label{fig:comp-tensor}
\end{figure}

The construction of \eqref{eq:lemma:tensor-E}, \eqref{eq:lemma:tensor-r} and \eqref{eq:lemma:tensor-g} can be graphically illustrated in Figure~\ref{fig:comp-tensor}.
Under these definitions,
we can get that each element  can be expressed with

The step to get \eqref{eq:exp:stepsum} depends on Lemma~\ref{lm:matrix}.
Eq.~\eqref{eq:exp:stepsum} to \eqref{eq:exp:allsum} depends on \eqref{eq:lemma:tensor-E} and \eqref{eq:lemma:tensor-g}.
\end{proof}


\subsubsection{Proof of Proposition \ref{pr:expBLMs}}


\begin{proof}
Based on Lemma~\ref{lm:matrix} and Lemma~\ref{lm:tensor},
if
\begin{enumerate}[leftmargin=*]
	\item  
	such that 
	 is symmetric,
	and
	\item  
such that 
	 is skew-symmetric,
\end{enumerate}
with 
,
then
given any KG with the tensor form ,
there exist entity embedding 
and relation embedding  
such that for all , we have
 
with .
Thus,
scoring function
\eqref{eq:uni} is fully expressive once condition 1) and 2) hold.
\end{proof}




\subsection{Proposition~\ref{pr:degenerate}}
\label{app:degenerate}

\subsubsection{Auxiliary Lemmas}


First,
we introduce two lemmas from matrix theory about the rank of Kronecker product
and the solution of equation group.

\begin{lemma}[\cite{horn2012matrix}]
\label{lem:kron}
Denote  as the Kronecker product,
given two matrices , we have
.
\end{lemma}

\begin{lemma}[\cite{horn2012matrix}]
\label{lem:zero}
Given ,
there is no non-zero solution  for
the equation group ,
if and only if .
\end{lemma}

Note that the definition of degenerate structure 
 is that
	
	or 
	.
To proof that  is not degenerate if and only if 

and
,
we prove its converse-negative proposition
in Lemma~\ref{lem:con-neg}.

\begin{lemma}
	\label{lem:con-neg}
	
	or 
	.
	if and only if
	 or .
\end{lemma}

This can be decomposed into two separate parts
in Lemma~\ref{lem:case1} and~\ref{lem:case2}.


\begin{lemma}
\label{lem:case1}

if and only if .
\end{lemma}

\begin{proof}
To begin with, 
we show the 
relationship between the rank of 
and the rank of .
If we assign ,
then the -th block will be 
with the identity matrix .
Using Kronecker product,
we can write  as a Kronecker product,

where  here represents the Kronercker product 
and  is a  matrix formed by the signs of elements in  .
Then, 
based on Lemma~\ref{lem:kron},
we have 

and
,
.


\begin{itemize}[leftmargin=*]
\item 	
First, we show the \textbf{sufficient condition},
i.e.,
if , 
we have
.

Since  is not full rank,
we have 
based on \eqref{eq:grank2}.
Then based on Lemma~\ref{lem:zero},	
for all 
there exists 
that 

This leads to .
Thus, 
if ,
.

\item
Then we show the
\textbf{necessary condition},
i.e.,
if ,
we have
.

We assign
,
and a set of  with .
Then, 
this will lead to the following equation group


We proof the necessary  condition by contraction here.
Assume , then 
based on \eqref{eq:grank1}.
Then, based on Lemma~\ref{lem:zero},
there is no  such that 
the above equation group is satisfied.
Thus, 
the assumption that  is wrong.
Therefore,
if ,
we have
.
\end{itemize}

Based on the proof of sufficient and necessary conditions,
we have

if and only if .
\end{proof}


\begin{lemma}
	\label{lem:case2}
	
	if and only if 
	.
\end{lemma}

\begin{proof}
	\begin{itemize}[leftmargin=*]
		\item First, we show the sufficient condition, i.e.,
		if ,
		we have .
		
		Given the -chunk representation of ,
		if    ,
		we assign 
			
		Then,
		 is always true 
		since .
		This leads to 
		with .
		As a result,
		.
		Therefore,
		if ,
		we have .
		
		\item Then, we show the \textbf{necessary condition}, i.e.,
		if ,
		we have .
		
		We can enumerate 
		as the set of unit vectors with one dimension as 
		and the remaining to be .
		Then from 
		we derive 
		since any element is .
		Specially,
		we have that each block in  is
		
		If the number of unique values of set  is ,
		we will have .
		This is in contrary to the fact that
		.
		Thus there must 
		.
		

	\end{itemize}
\noindent
Thus, 
we have 
if and only if 
.
\end{proof}

By combining Lemma~\ref{lem:case1} and Lemma~\ref{lem:case2},
we can show Lemma~\ref{lem:con-neg} that

or 
.
if and only if
 or .
Since
the original statement is equal to the converse-negative proposition,
Proposition~\ref{pr:degenerate} is proved.


\subsubsection{Proof of Proposition~\ref{pr:degenerate}}

\begin{proof}
Proposition~\ref{pr:degenerate},
is equivalent to the statement that
 is degenerate if and only if 

and .
From Definition~\ref{def:degenerate},
 is degenerate means
\begin{enumerate}[leftmargin=*]
	\item ; or
	\item .
\end{enumerate}
Here, 
Lemma~\ref{lem:case1} proves 1) and Lemma~\ref{lem:case2} proves 2).
Thus,
we can conclude that
 is non-degenerate,
if and only if 
and 
.
\end{proof}






\subsection{Proposition~\ref{pr:equiv}}
\label{app:equiv}

We denote the -chunk representation of the embeddings as 

with 
and 

with .
Besides,
given the permutation matrix ,
we denote 
 
and 
if  
for .

\subsubsection{Auxiliary Lemmas}


\begin{lemma}
\label{lem:optimal}
If below two conditions hold,
then .
\begin{itemize}[leftmargin=*]
\item given the optimal embedding 
 and  for 
there exist  and  such that  always hold;

\item 
given the optimal embedding 
 and  for ,
there exist  and  such that  always hold.
\end{itemize}
\end{lemma}

\begin{proof}
	Denote 
	and 	
	Then, 
	from Definition~\ref{def:equiv},
	we have 
	
	and 
	     .
	
	If given the optimal embedding 
	 and  for the scoring function 
	there exist  and  such that ,
	we will have
	
	
	Similarly,
	if given the optimal embedding 
	 and  for the scoring function 
	there exist  and  such that ,
	we have
	
	Based on \eqref{eq:p-opt} and \eqref{eq:p'-opt},
	we have ,
	namely .
\end{proof}


\subsubsection{Proof of Proposition~\ref{pr:equiv}}

\begin{proof}
The key point of this proof is that,
there exist corresponding operations on the optimal embedding such that the score of equivalent structures
can always be the same,
i.e.,
.
\begin{enumerate}[label=(\roman*),leftmargin=15px]
\item 
We can permute the corresponding chunks in the entity embeddings 
to get the same scores.
If there exists a permutation matrix  that ,
we will have  and 
for . 


Given  as the optimal embeddings trained by ,
we set 
with .
In this way,
we always have 

In the third to fourth line, we set  and .

Similarly,
given  as the optimal embeddings trained by ,
we can set 
with  .
Thus, we always have

In the third to fourth line, we set  and .
Finally,
based on Lemma~\ref{lem:optimal},
we have .

	
\item 
We can permute the corresponding chunks in relation embedding
to get the same scores.

If there exists a permutation matrix  that 
,
we will have 

and 
.

Given 
as the optimal embeddings trained by ,
we can set  with
.
In this way,
we alway have 	

with 


Similarly,
given  as the optimal embeddings trained by ,
we can set 
with  .
In this way,
we always have

with .
Finally,
based on Lemma~\ref{lem:optimal},
we have .

\item 
We can flip the signs of corresponding chunks in relation embedding
to get the same scores.

If there exists a sign vector 
that
,
we will have
 and  with 
and .

Given 
as the optimal embedding trained by 
,
we can set  with
.
In this way,
we always have

\begin{figure*}[ht]
	\centering
	\subfigure[ogbl-biokg (AutoBLM)]{\includegraphics[width=0.31\columnwidth]{figure/ogbl-biokg-2}}
	\qquad
	\subfigure[ogbl-wikikg2 (AutoBLM)]{\includegraphics[width=0.31\columnwidth]{figure/ogbl-wikikg2-2}}
	\qquad\qquad
	\subfigure[ogbl-biokg (AutoBLM+)]{\includegraphics[width=0.31\columnwidth]{figure/ogbl-biokg}}
	\qquad
	\subfigure[ogbl-wikikg2 (AutoBLM+)]{\includegraphics[width=0.31\columnwidth]{figure/ogbl-wikikg2}}
	\vspace{-10px}
	\caption{A graphical illustration of  identified by AutoBLM and AutoBLM+ 
		on the large-scale KG completion task with ogbl-biokg and ogbl-wikikg2 datasets.}
	\label{fig:structure:ogb}
\end{figure*}


\begin{figure*}[ht]
	\begin{center}
{\includegraphics[width=0.31\columnwidth]{figure/fb15kn2-2}}
		\qquad
{\includegraphics[width=0.31\columnwidth]{figure/fb15kn3-2}}
		\qquad\qquad
{\includegraphics[width=0.31\columnwidth]{figure/fb237n2-2}}
		\qquad
{\includegraphics[width=0.31\columnwidth]{figure/fb237n3-2}}

		\subfigure[FB15k-2p.]
		{\includegraphics[width=0.31\columnwidth]{figure/fb15kn2}}
		\qquad
		\subfigure[FB15k-3p.]
		{\includegraphics[width=0.31\columnwidth]{figure/fb15kn3}}
		\qquad\qquad
		\subfigure[FB15k237-2p.]
		{\includegraphics[width=0.31\columnwidth]{figure/fb237n2}}
		\qquad
		\subfigure[FB15k237-3p.]
		{\includegraphics[width=0.31\columnwidth]{figure/fb237n3}}
		\vspace{-10px}
		\caption{A graphical illustration of  identified by AutoBLM
			(top) and AutoBLM+ (bottom) on the multi-hop query task.}
		\label{fig:s1}
	\end{center}
\end{figure*}


\begin{figure*}[ht]
	\centering
{\includegraphics[width=0.31\columnwidth]{figure/aifb-2}}
	\qquad\qquad
{\includegraphics[width=0.31\columnwidth]{figure/mutag-2}}
	\qquad\qquad
{\includegraphics[width=0.31\columnwidth]{figure/bgs-2}}
	
	
	\subfigure[AIFB.]
	{\includegraphics[width=0.31\columnwidth]{figure/aifb}}
	\qquad\qquad
	\subfigure[MUTAG.]
	{\includegraphics[width=0.31\columnwidth]{figure/mutag}}
	\qquad\qquad
	\subfigure[BGS.]
	{\includegraphics[width=0.31\columnwidth]{figure/bgs}}
	\vspace{-10px}
	\caption{A graphical illustration of  identified by
		AutoBLM (top) and AutoBLM+ (bottom) on the entity classification task.}
	\label{fig:s2}
	\vspace{-10px}
\end{figure*}


with .

Similarly,
given  as the optimal embeddings trained by ,
we can set 
with  .
In this way,
we always have

Finally,
based on Lemma~\ref{lem:optimal},
we have .
\end{enumerate}
\end{proof}


\section{Relation distribution in different datasets}
\label{app:rel:distribution}

Following \cite{rossi2020knowledge},
if more than half of the training triples 
of a relation 
have
inverse triples 
(i.e.,
),
 is considered as symmetric.
If there exists no inverse triplet
(i.e., ),
 is anti-symmetric.
Relations that are neither 
symmetric nor
anti-symmetric
are general asymmetric.
Finally, a relation  belongs to the inverse type
if .


As can be seen, the four datasets have very 
different distributions and thus properties.
As demonstrated in neural architecture search
\cite{elsken2019neural,liu2018darts,zoph2017neural},
different datasets need different neural architectures.
The architectures discovered have better performance than those
designed by
humans.
Hence,
the scoring functions should also be data-dependent, 
as demonstrated empirically
in Section \ref{exp:kgc:performance}.

\begin{table}[ht]
	\centering
	\caption{Distribution of relation types in the testing set.}
	\label{tb:rel-distribution}
	\setlength\tabcolsep{3.1pt}
	\vspace{-10px}
	\begin{tabular}{c|ccc|c}
		\toprule
		& symmetry & anti-symmetry & general asymmetry & inverse \\  \midrule
		WN18     & 23.4\%    & 72.1\%         & 4.5\%      & 4.5\%   \\
		FB15k    & 9.3\%     & 5.2\%          & 85.5\%     & 74.9\%  \\
		WN18RR   & 37.4\%    & 59.0\%         & 3.6\%      & 0.0\%   \\
		FB15k237 & 3.0\%     & 8.5\%          & 88.5\%     & 10.5\%  \\
		YAGO3-10 & 3.4\%     & 0.7\%          & 95.9\%     & 8.2\%  \\
		\bottomrule
	\end{tabular}
\end{table}


\section{Consistent performance under different dimensions}
\label{sssec:transfer}


In this section, we perform the KG completion experiment in Section~\ref{ssec:KGC}.
First, we
collect the first
100 structures 's 
(with )
of AutoBLM+ in Section~\ref{ssec:KGC}
and measure the corresponding 
validation MRR
performance 
in step~\ref{step:evo-train} of Algorithm~\ref{alg:evolution}.
We then
increase the embedding dimensionality to , 
retrain and re-evaluate these structures.
Figure~\ref{fig:transfer}
compares the validation MRRs obtained with
 and 
on the WN18RR and FB15k-237 data sets.
As can be seen,
the two sets of MRRs
are correlated,
especially for the top performed ones.
The Spearman's rank correlation coefficient
on WN18RR is
  and on FB15k-237 is .
\begin{figure}[H]
	\centering
	\includegraphics[width=0.48\columnwidth]{figure/WN18RR_transfer}
	\hfill
	\includegraphics[width=0.50\columnwidth]{figure/FB15k237_transfer}
	
	\vspace{-10px}
\caption{Validation MRRs of the structures with  and .}
	\label{fig:transfer}
\end{figure}


\section{Models Obtained by AutoBLM and AutoBLM+}
\label{app:figures}


Figures~\ref{fig:structure:ogb},
\ref{fig:s1} and
\ref{fig:s2} 
show the structures obtained by AutoBLM and AutoBLM+
on the tasks of 
KG completion (Section~\ref{ssec:KGC}) for the  ogbl-biokg and ogbl-wikikg2
datasets, 
multi-hop query (Section~\ref{sec:exp:hop}) and
entity classification (Section~\ref{sec:exp:cls}), respectively.


\end{document}
