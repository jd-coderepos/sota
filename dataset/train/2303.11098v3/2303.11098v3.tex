\documentclass[letterpaper]{article} \usepackage[submission]{aaai23}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2023.1)
}



\setcounter{secnumdepth}{0} 




\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{emoji}
\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      



\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[labelformat=simple]{subcaption}
\usepackage{array}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{pgfgantt}
\usepackage{multirow}
\usepackage{array,booktabs}
\usepackage{tabularray}
\usepackage{soul}

\usepackage{svg}
\usepackage{pgfplots}
\usepackage{multirow, tabularx}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{adjustbox}
\usepackage{xcolor}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{stackengine}
\usepackage{listings}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\black}[1]{{\color{black}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\definecolor{light_red}{rgb}{0.921,0.78039,0.83137}
\definecolor{dark_red}{rgb}{0.729,0.459,0.459}
\newcommand{\highlight}[1]{{\color{dark_red}#1}}

\newcommand{\lpnorm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\sidenote}[1]{{\color{red}#1}}
\newcommand{\Real}{{\rm I\!R}}
\newcommand{\p}[1]{{\noindent \textbf{#1}}}
\newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
\newcommand{\renyi}{{r\'enyi}}
\newcommand{\Renyi}{{R\'enyi}}
\newcommand{\functional}{function\;}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0,1.0,1.0}
\definecolor{link}{rgb}{1.0,1.0,1.0}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    float=tp,
  floatplacement=tbp
}
\lstset{style=mystyle}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}
\renewcommand{\lstlistingname}{Algorithm}
\DeclareCaptionFormat{mylst}{\hrule#1#2#3\hrule}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}
\newcommand\redcancel[2][red]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

\definecolor{light_orange}{RGB}{252,219,191}
\definecolor{dark_orange}{RGB}{237,139,55}
\definecolor{light_green}{RGB}{111,193,174}
\definecolor{dark_green}{RGB}{67,154,134}
\definecolor{cyan}{RGB}{123,250,241}

\newcommand{\roy}[1]{\textbf{[Roy]}  \; \red{#1} }
\newcommand{\comment}[1]{\red{#1} }

\definecolor{xarch}{rgb}{1.0, 0.9, 0.9}
\definecolor{sarch}{rgb}{0.94, 0.97, 1.0}
\definecolor{light_green}{rgb}{0.72, 0.85, 0.85}
\definecolor{light_red}{rgb}{1.0, 0.9, 0.9}

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{fit}

\usepackage{amssymb}\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcounter{nodecount}
\newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node  (\arabic{nodecount}) {#1};}

\tikzstyle{every picture}+=[remember picture,baseline]
\tikzstyle{every node}+=[anchor=base,minimum width=0.4cm,align=center,text depth=.25ex,outer sep=1.5pt]
\tikzstyle{every path}+=[thick, rounded corners]




\title{A closer look at the training dynamics of knowledge distillation}
\author{
Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
\textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\




    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
publications23@aaai.org
}

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
\textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


\usepackage{bibentry}


\begin{document}

\maketitle

\begin{abstract}
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a \% top-1 accuracy with DeiT-Ti on ImageNet.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Deep neural networks have achieved remarkable success in various applications, ranging from computer vision~\cite{Krizhevsky2012ImageNetNetworks} to natural language processing~\cite{Vaswani2017AttentionNeed}. However, the high computational cost and memory requirements of deep models have limited their deployment in resource-constrained environments. Knowledge distillation is a popular technique to address this problem through transferring the knowledge of a large teacher model to that of a smaller student model. This technique involves training the student to imitate the output of the teacher, either by directly minimizing the difference between intermediate features or by minimizing the Kullback-Leibler (KL) divergence between their soft predictions. Although knowledge distillation has shown to be very effective, there are still some limitations related to the computational and memory overheads in constructing and evaluating the losses, as well as an insufficient theoretical explanation for the underlying core principles.

To overcome these limitations, we revisit knowledge distillation from both a function matching and metric learning perspective. We perform an extensive ablation of three important components of knowledge distillation, namely the distance metric, normalisation, and projector network. Alongside this ablation we provide a theoretical perspective and unification of these design principles through exploring the underlying training dynamics. Finally, we extend these principles to a few large scale vision tasks, whereby we achieve comparable or improved performance over state-of-the-art. The most significant result of which pertains to the data-efficient training of transformers, whereby a performance gain of 2.2\% is achieved over the best-performing distillation methods that are designed explicitly for this task.
Our main contributions can be summarised as follows.
\begin{itemize}
    \item We explore three distinct design principles from knowledge distillation, namely the projection, normalisation, and distance function. In doing so we demonstrate their \textbf{unification and coupling} with each other, both through \textbf{analytical means and by observing the training dynamics}.
    \item We show that a \textbf{projection layer implicitly encodes relational information from previous samples}. Using this knowledge we can remove the need to explicitly construct correlation matrices or memory banks that will inevitably incur a significant memory overhead.
    \item We propose a \textbf{simple recipe for knowledge distillation using a linear projection, batch normalisation, and a  function}. These three design choices can attain competitive or improved performance to state-of-the-art for image classification, object detection, and the data efficient training of transformers. 
\end{itemize}



\begin{figure}[ht]
    \centering
    \resizebox{1.\columnwidth}{!}{\includegraphics[width=1.\linewidth]{figures/distillation_overview_v2.pdf}
    }
    \caption{\textbf{Training pipeline for knowledge distillation on a classification task.} Left and right show representation distillation with and without the addition of a logit distillation loss respectively. For clarity, the normalisation of arguments for the representation loss  have been omitted.}
    \label{fig:distillation_overview}
\end{figure}

\section{Related Work}
\label{sec:related_work}

\paragraph{Knowledge Distillation}
Knowledge distillation is the process of transferring the knowledge from a large, complex model to a smaller, simpler model. Its usage was originally proposed in the context of image classification~\cite{Hinton2015DistillingNetwork} whereby the soft teacher predictions would encode relational information between classes. Spherical KD~\cite{Guo2020ReducingDistillation} extended this idea by re-scaling the logits, prime aware adaptive distillation~\cite{Zhang2020Prime-AwareDistillation} introduced an adaptive weighting strategy, while DKD~\cite{Zhao2022DecoupledDistillation} proposed to decouple the original formulation into target class and non-target class probabilities.

Hinted losses~\cite{Romero2015FitNets:Nets} were a natural extension of the logit-based approach whereby the intermediate feature maps are used as hints for the student. Attention transfer~\cite{Zagoruyko2019PayingTransfer} then proposed to re-weight this loss using spatial attention maps. ReviewKD~\cite{Chen2021DistillingReview} addressed the problem relating to the arbitrary selection of layers by aggregating information across all the layers using trainable attention blocks. Neuron selectivity transfer~\cite{Huang2017LikeTransfer}, similarity-preserving
KD~\cite{Tung2019Similarity-preservingDistillation}, and relational KD~\cite{Park2019RelationalDistillation} construct relational batch and feature matrices that can be used as inputs for the distillation losses. Similarly FSP matrices~\cite{Yim2017ALearning} were proposed to extract the relational information through a residual block. In contrast to this theme, we show that a simple projection layer can implicitly capture most relational information, thus removing the need to construct any expensive relational structures.

Representation distillation was originally proposed alongside a contrastive based loss~\cite{Tian2019ContrastiveDistillation} and has since been extended using a Wasserstein distance~\cite{Chen2020WassersteinDistillation}, information theory~\cite{Miles2022InformationDistillation}, graph theory~\cite{Ma2022DistillingAlignment}, and complementary gradient information~\cite{Zhu2021ComplementaryDistillation}. Distillation also been empirically shown to benefit from longer training schedules and more data-augmentation~\cite{Beyer2022KnowledgeConsistent}, which is similarly observed with HSAKD~\cite{Yang2021HierarchicalDistillation} and SSKD~\cite{Xu2020KnowledgeSelf-supervision}.
Distillation between CNNs and transformers has also been a very practically motivated task for data-efficient training~\cite{Touvron2021TrainingAttention} and has shown to benefit from an ensemble of teacher architectures~\cite{Ren2022Co-advise:Distillation}. However, we show that just a simple extension of some fundamental distillation design principles is much more effective.

\paragraph{Self-Supervised Learning}
Self-supervised learning (SSL) is an increasingly popular field of machine learning whereby a model is trained to learn a useful representation of unlabelled data. Its popularity has been driven by the increasing cost of manual labelling and has since been crucial for training large transformer models. Various pretext tasks have been proposed to learn these representations, such as image inpainting~\cite{He2022MaskedLearners}, colorization~\cite{Zhang2016ColorfulColorization}, or prediction of the rotation~\cite{Gidaris2018UnsupervisedRotations} or position of patches~\cite{Doersch2015UnsupervisedPrediction, Carlucci2019DomainPuzzles}. SimCLR~\cite{Chen2020ARepresentations} approached self-supervision using a contrastive loss with multi-view augmentation to define the positive and negative pairs. They found a large memory bank of negative representations was necessary to achieve good performance, but would incur a significant memory overhead. MoCo~\cite{He2020MomentumLearning} extending this work with a momentum encoder, which was subsequently extended by MoCov2~\cite{Chen2020ImprovedLearning} and MoCov3~\cite{Chen2021AnTransformers}.


Asymmetric architectures were proposed as an alternative to contrastive learning, whereby no negative samples are needed. Most noticeable works in this area are BYOL~\cite{Grill2020BootstrapLearning} and SimSiam~\cite{Chen2021ExploringLearning} which both use stop gradients to avoid representation collapse. DirectPred~\cite{Tian2021UnderstandingPairs} provided a theoretical understanding of this non-contrastive SSL setting. They introduced a series of conceptual insights into the functional roles of many crucial ingredients used. In doing so they derived a simple linear predictor with competitive performance to some much more complex predictor architectures. In our work we explore knowledge distillation from a similar perspective as DirectPred but observe some unique observations and results pertaining to this distillation setting.

Feature decorrelation is another approach to SSL that avoids the need for negative pairs to address representation collapse. Barlow twins~\cite{Zbontar2021BarlowReduction} proposed to compute a cross-view correlation matrix and minimise its distance to the identity. This loss was later decomposed into a variance, invariance, and correlation regularisation terms in VICReg~\cite{Bardes2022VICReg:Learning}. Both contrastive learning and feature decorrelation have since been extended to dense prediction tasks~\cite{Bardes2022VICRegL:Features} and unified with knowledge distillation~\cite{Miles2023MobileVOS:Distillation}.

\begin{figure}
\centering
\includegraphics[width=.95\linewidth]{figures/correlation.pdf}
\caption{\textbf{Correlation between input-output features using different projector architectures.} All projector architectures considered will gradually decorrelate the input-output features. Although this decorrelation is attributed to the layer removing irrelevant information, it can degrade the efficacy of distilling through to the student backbone.}
\label{fig:input_output_decorrelation}
\end{figure}

\section{Understanding the Training Dynamics}
\label{sec:understanding_kd}

Knowledge distillation (KD) is a technique used to transfer knowledge from a large, powerful model (teacher) to a smaller, less powerful one (student). In the classification setting, it can be done using the soft teacher predictions as pseudo-labels for the student. Unfortunately, this approach does not trivially generalise to non-classification tasks~\cite{Liu2019StructuredSegmentation} and the classifier may collapse a lot of information~\cite{Tishby2015DeepPrinciple} that can be useful for distillation.
Another approach is to use feature maps from the earlier layers for distillation~\cite{Romero2015FitNets:Nets, Zagoruyko2019PayingTransfer}, however, its usage presents two primary challenges: the difficulty in ensuring consistency across different architectures~\cite{Chen2021DistillingReview} and the potential degradation in the student's downstream performance for cases where the inductive biases of the two networks differ~\cite{Tian2019ContrastiveDistillation}
A compromise, which strikes a balance between the two approaches  discussed above, is to distill the representation directly before the output space. Representation distillation has been successfully adopted in past works~\cite{Tian2019ContrastiveDistillation, Zhu2021ComplementaryDistillation, Miles2022InformationDistillation} and is the focus of this paper. The exact training framework used is described in figure \ref{fig:distillation_overview} alongside an extension to incorporates a logit distillation loss. The projection layer shown was originally used to simply match the student and teacher dimensions~\cite{Romero2015FitNets:Nets}, however, we will show that its role is much more important and it can lead to significant performance improvements even when the two feature dimensions match. The two representations are also typically both followed by some normalisation scheme as a way of appropriately scaling the gradients. However, we find this normalisation has a more interesting property in its relation to what information is encoded in the learned projector weights.

In this work we provide a theoretical perspective to motivate some simple and effective design choices for knowledge distillation. In contrast to the recent works~\cite{Tung2019Similarity-preservingDistillation, Miles2022InformationDistillation}, we show that an explicit construction of complex relational structures, such as feature kernels~\cite{He2022FeatureDistillation} is not necessary. In fact, most of this structure can learned implicitly through the tightly coupled interaction of a learnable projection layer and an appropriate normalisation scheme.
In the following we discuss the importance of multi-view augmentation in distillation. We subsequently investigate the training dynamics of the projection layer with the normalisation scheme.  We explore the impact and trade-offs that arise from the architecture design of the projector. Finally, we propose a simple modification to the distance metric to address issues arising from a large capacity gap between the student and teacher models. \\

\noindent\textbf{The projection weights encode relational information from previous samples.} The projection layer plays a crucial role in KD as it provides an implicit encoding of previous samples and its weights can the capture the relational information needed to transfer information regarding the correlation between features. We observe that even a single linear projector layer can provides significant improvement in accuracy (see Supplementary). This improvement suggests that projections role in distillation can be described more concisely as being an encoder of essential information needed for the distillation loss itself. Most recent works propose a manual construction of some relational information to be used as part of a loss~\cite{Park2019RelationalDistillation, Tung2019Similarity-preservingDistillation}, however, we posit that an implicit and learnable approach is much more effective. To explore this phenomenon in more detail, we consider the update equations for the projector weights and its training dynamics. Without loss in generality, consider a simple L2 loss and a linear projection layer without any bias term.


\begin{figure*}
\centering
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/no_evolvement_singular_vals.pdf}
\caption*{(a) no normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/l2_evolvement_singular_vals.pdf}
\caption*{(b) L2 normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/bn_evolvement_singular_vals.pdf}
\caption*{(c) batch normalisation.}
\end{minipage}
\caption{\textbf{Evolution of singular values of the projection weights  under three different representation normalisation schemes.} The student is a Resnet-18, while the teacher is a ResNet-50. The three curves shows the evolution of singular values for the projector weights when the representations undergo no normalisation, L2 normalisation, and batch norm respectively.}
\label{fig:singular_values_evolution}
\end{figure*}


Where  and  are the \textbf{s}tudent and \textbf{t}eacher representations respectively, while  is the matrix representing the linear \textbf{p}rojection. Using the trace property of the Frobenius norm, we can then express this loss as follows:

Taking the derivative with respect to , we can derive the update rule 

which can be further simplified

where  and  denote self and cross correlation matrices that capture the relationship between features. Due to the capacity gap between the student network and the teacher network, there is no linear projection between these two representation spaces. Instead, the projector will converge on an approximate mapping that we later show is governed by the normalisation being employed. However, we note that, unlike in self-supervised learning, this solution will never be unstable since representation collapse is not possible - the teacher is frozen and we jointly trained with the ground truth. 

\underline{Whitened features:} consider using self-supervised learning in conjunction with distillation whereby the student features are whitened to have perfect decorrelation~\cite{Ermolov2020WhiteningLearning}, or alternatively, they are batch normalised and sufficiently regularised with a feature decorrelation term~\cite{Bardes2022VICReg:Learning}. In this setting, the fixed point solution for the projection weights will be symmetric and will capture the cross relationship between student and teacher features.









Other normalisation schemes, such as those that jointly normalise the projected features and the teacher features, will have a much more involved analysis but will unlikely provide any additional insight on the dynamics of training itself. Thus, we propose to explore the training trajectories of the projector weights singular values. This will help quantify how the projector is mapping the student features to the teachers space. We cover this in the next section along with additional insights into what is being learned and distilled. \\

\textbf{The choice of normalisation directly affects the training dynamics of .}
Equation \ref{eqn:projector_update} shows that the projector weights can encode relational information between the student and teacher's features. This suggests redundancy in explicitly constructing and updating a large memory bank of previous representations~\cite{Tian2019ContrastiveDistillation}. By considering a weight decay  and a learning rate , the update equation can be given as follows:



By setting  we can see that the projection layer will reduce to a moving average of relational features, which is very similar to the momentum encoder used by CRD~\cite{Tian2019ContrastiveDistillation}. Other works suggest to extract relational information on-the-fly by constructing correlation or gram matrices~\cite{Miles2022InformationDistillation, Peng2019CorrelationDistillation}. We show that this is also not necessary and more complex information can be captured through careful design of the projector architecture. We also demonstrate that, in general, the use of a projector will scales much more favourably for larger batch sizes and feature dimensions. We also note that the handcrafted design of kernel functions~\cite{Joshi2021OnNetworks, He2022FeatureDistillation, Miles2022InformationDistillation} may not generalise to large scale or complex real-world datasets.



From the results in table \ref{table:rebuttal_normalisation}, we observe that when fixing all other settings, the choice of normalisation can significantly affect the student's performance. To explore this in more detail, we consider the training trajectories of  under  different normalisation schemes. We find that the choice of normalisation not only controls the training dynamics, but also the fixed point solution (see equation \ref{eqn:projector_update}). We argue that the efficacy of distillation is dependent on how much relational information can be encoded in the learned weights and how much information is lost through the projection. To jointly evaluate these two properties we show the evolution of singular values of the projector weights during training. The results can be seen in figure \ref{fig:singular_values_evolution} and show that the better performing normalisation methods (table \ref{table:rebuttal_normalisation}) are shrinking far fewer singular values towards zero. This shrinkage can be described as collapsing the input along some dimension, which will induce some information loss and it is this information loss that degenerates the efficacy of distillation.



\begin{table}[htp]
    \centering
    \resizebox{1.\linewidth}{!}{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lccc}
    \toprule
    Description & RegNet-Y  MBv2 & ViT  MBv3 & ConvNext  EffNet-b0 \\
    \midrule
    No distillation & 50.89 & 54.13 & 64.48 \\
    Cosine Similarity & 52.91 & 54.65 & 64.23 \\
    Group Norm & 55.63 & 59.08 & 65.52 \\
    Batch Norm & \textbf{56.09} & \textbf{59.28} & \textbf{67.95} \\
    \midrule










    
\end{tabular}     }
    \caption{\textbf{Normalisation ablation} for distillation across a range of architecture pairs on ImageNet-1K 20\% subset.} 
    \label{table:rebuttal_normalisation}
\end{table}

\begin{table}[htp]
    \centering
    \resizebox{1.\linewidth}{!}{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{cccccccc}
    \toprule
Teacher  Student & 1.0 & 1.5 & 2.0 & 2.5 & 3.0 & 4.0 & 5.0\\
    \midrule
    ResNet50  ResNet18 & 61.74 & 62.18 & 62.23 & 62.84 & 63.10 & 63.32 & \textbf{63.40} \\
    ConvNext  EffNet-b0 & 65.52 & 66.69 & 66.61 & 67.10 & 67.72 & \textbf{68.51} & 67.69 \\
    \midrule
\end{tabular}



%
     }
    \caption{\textbf{Ablating the importance of .} Distillation is generally robust for various values of , but consistently optimal in range 4-5 across various architecture pairs.}
    \label{table:alpha}
\end{table}

\begin{table}[htp]
    \centering
    \resizebox{1.\linewidth}{!}{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lcc|c}
    \toprule
    Description & ViT  MBv3 & ConvNext  EffNet-b0 & ResNet-50  ResNet-18 \\
    \midrule
    
    wo/ LogSum & 59.28 & 67.95 & 70.03 \\
    w/ LogSum & \textbf{59.80} & \textbf{68.51} & \textbf{71.29} \\
    \midrule
\end{tabular}     }
    \caption{\textbf{LogSum ablation} across various architecture pairs. \textbf{Left:} 20\% subset. \textbf{Right:} Full ImageNet. The soft maximum function provides consistent improvement across both the CNNCNN and ViTCNN distillation settings.} 
    \label{table:logsum}
\end{table}

\textbf{Larger projector networks learn to decorrelate the input-output features.}
One natural extension of the previous observations is to use a larger projector network to encode more information relevant for the distillation loss. Unfortunately, we observe that a trivial expansion of the projection architecture does not necessarily improve the students performance. To explain this observation we evaluate a measure of decorrelation between the input and output features of these projector networks. The results can be seen in figure \ref{fig:input_output_decorrelation} and we can see that the larger projectors learn to decorrelate more and more features from the input. This decorrelation can lead to the projector learning features that are not shared with the student backbone, which will subsequently diminish the effectiveness of distillation. These observations suggest that there is an inherent trade-off between the projector capacity and the efficacy of distillation. We note that the handcrafted design of the projector architecture is a motivated direction for research~\cite{Chen2022ImprovedEnsemble, Navaneet2021SimReg:Distillation}.
However, in favour of simplicity, we choose to use a linear projector for all of the larger scale evaluations. \\

\begin{table*}[ht!]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{lcccc|ccccccc}
    \toprule
    Teacher & wrn40-2 & wrn40-2 & resnet56 & resnet32×4 & vgg13 & ResNet50 & ResNet50 & resnet32×4 & resnet32×4 & wrn40-2\\
    Student & wrn16-2 & wrn40-1 & resnet20 & resnet8×4 & MobileNetV2 & MobileNetV2 & vgg8 & ShuffleV1 & ShuffleV2 & ShuffleV1 \\
    \midrule
    Teacher & 76.46 & 76.46 & 73.44 & 79.63 & 75.38 & 79.10 & 79.10 & 79.63 & 79.63 & 76.46 \\
    Student & 73.64 & 72.24 & 69.63 & 72.51 & 65.79 & 65.79 & 70.68 & 70.77 & 73.12 & 70.77 \-.5ex]
    \tabnode{KD}~\cite{Hinton2015DistillingNetwork} & 75.94 & 75.32 & 71.10 & 75.84 & 70.79 & 71.29 & 75.75 & 77.80 & 78.43 & 78.00 \\
    \;\;CRD~\cite{Tian2019ContrastiveDistillation} & 77.27 & \textbf{76.15} & 72.21 & 77.69 & 71.65 & 72.03 & 75.73 & 78.57 & 79.01 & 78.54 \\
    \;\;DKD~\cite{Zhao2022DecoupledDistillation} & 74.96 & 75.89 & 70.95 & 77.52 & 72.01 & 73.30 & 76.88 & \textbf{79.71} & \textbf{80.08} & 77.86 \\





\;\;Our Method & \textbf{77.61} & 76.04 & \textbf{72.25} & \textbf{78.37} & \textbf{72.82} & \textbf{73.51} & \textbf{77.08} & 78.99 & 79.86 & \tabnode{\textbf{78.79}} \\
    


















\end{tabular}

\begin{tikzpicture}[overlay]
\node[draw=dark_green,fill=light_green,fill opacity=.2,draw opacity=1,rounded corners = 1ex,fit=(1)(2),inner sep = 0pt,text opacity=1] {};
\end{tikzpicture}

\begin{tikzpicture}[overlay]
\node[draw=dark_orange,fill=light_orange,fill opacity=.2,draw opacity=1,rounded corners = 1ex,fit=(3)(4),inner sep = 0pt,text opacity=1] {};
\end{tikzpicture}     }
    \caption{\textbf{KD between Similar and Different Architectures.} Top-1 accuracy (\%) on CIFAR100. \textbf{Bold} is used to denote the best results. All reported models are trained using pairs of augmented images. Those reported in the \textcolor{dark_orange}{orange} box use RandAugment~\cite{Cubuk2020Randaugment:Space} strategy, while those in the \textcolor{dark_green}{green box} use pre-defined rotations, as used in SSKD.  denotes reproduced results in a new augmentation setting using the authors provided code.}
    \label{table:cifar100} \end{table*}

\textbf{The soft maximum function can address distilling across a large capacity gap.}
 When the capacity gap between the student and the teacher is large, representation distillation can become challenging. More specifically, the student network may have insufficient capacity to perfectly align these two spaces and in attempting to do so may degrade its downstream performance. To addresses this issue we explore the use of a soft maximum function which will soften the contribution of relatively close matches in a batch. In this way the loss can be adjusted to compensate for poorly aligned features which may arise when the capacity gap is large. The family of functions which share these properties can be more broadly defined through a property of their gradients. In favour of simplicity, we use the simple  function throughout our experiments.

 

where  is a smoothing factor. We also note that other functions, such as the , with a temperature parameter , have been used in SimCLR and CRD to a similar effect. Table~\ref{table:rebuttal_normalisation} shows the importance of feature normalisation across a variety of student-teacher architecture pairs. Batch normalisation provides the most consistent improvement that even extends to the Transformer  CNN setting. In table~\ref{table:logsum} we highlight the importance of the  function, which is most effective in the large capacity gap settings, as evident from the 1\% improvement for R50  R18. Table~\ref{table:alpha} provides an ablation of the importance of the  parameter, whereby we observe that the performance is relatively robust to a wide range of values, but consistently optimal in the range 4-5.







\section{Benchmark Evaluation}
\label{sec:benchmark_evaluation}

\textbf{Implementation details.}
We follow the same training schedule as CRD~\cite{Tian2019ContrastiveDistillation} for both the CIFAR100 and ImageNet experiments. For the object detection, we use the same training schedule as ReviewKD~\cite{Chen2021DistillingReview}, while for the data efficient training we use the same as Co-Advice~\cite{Ren2022Co-advise:Distillation}. All experiments were performed on a single NVIDIA RTX A5000. When using batch normalisation for the representations, we removed the affine parameters and set .

\subsection{Classification on CIFAR100 and ImageNet}
\label{sec:cifar_and_imagenet}
Experiments on the CIFAR-100 classification task~\cite{Krizhevsky2009LearningImages} consist of 60K 32×32 RGB images across
100 classes with a 5:1 training/testing split. Table \ref{table:cifar100} shows the results for several student-teacher pairings. To enable a fair evaluation, we have only included the methods that use the same teacher weights provided by SSKD\cite{Xu2020KnowledgeSelf-supervision}. In these experiments we use an MLP projector with a hidden size of 1024 and no additional KL divergence loss. We confirm that not only is the choice of augmentation critical for good performance~\cite{Beyer2022KnowledgeConsistent} on this dataset, but applying our principles can attain state-of-the-art across most architecture pairs. The most significant improvements pertain to the cross-architecture experiments or where the capacity gap is large. We provide two sets of experiments with and without introducing a wider set of augmentations. In both settings we maintain the same optimiser, learning rate scheduler, and training duration.



The ImageNet~\cite{Russakovsky2014ImageNetChallenge} classification uses 1.3 million images that are classified into 1000 distinct classes. The input size are set to 224 x 224, and we employed a typical augmentation procedure that includes cropping and horizontal flipping. We used the torchdistill library with the standard configuration, which involves 100 training epochs using SGD and an initial learning rate of 0.1, which is decreased by a factor of 10 at epochs 30, 60, and 90. The results can be seen in table \ref{table:imagenet} and although the choice of architectures is not in favour of our method since the capacity gap is small, we are still able to attain competitive performance. Other methods, such as ICKD~\cite{Liu2021ExploringDistillation} or SimKD~\cite{Chen2022KnowledgeClassifier} either modify the original training settings or modify the underlying student architecture, and so have been omitted from this evaluation.

\subsection{Data efficient training for transformers}
\label{sec:deit}
Transformers have emerged as a viable replacement for convolution-based neural networks (CNN) in visual learning tasks. Despite the promise of these models, their performance will suffer when there is insufficient training data available, such as in the case of ImageNet. DeiT~\cite{Touvron2021TrainingAttention} was the first to address this problem through the use of knowledge distillation. Although the authors show improved alignment with the teacher, we believe this fails to capture why less data is needed.

We posit that the distillation process encourages the student to learn layers which are "more" translational equivariant in attempt to match the teacher's underlying function. Although this is the principle that motivates using an ensemble of teacher models with different inductive biases~\cite{Ren2022Co-advise:Distillation}, there is still no thorough demonstration on if the inductive biases are actually being transferred. In this section we attempt to address this gap by introducing a measure of equivariance. We show that applying our distillation principles to this task can achieve significant improvements over state-of-the-art as a result of transferring more of the translational equivariance.

The results of these experiments are shown in table \ref{table:data_efficient_training_transformers}. We use the exact same training methodology as co-advice~\cite{Ren2022Co-advise:Distillation} and choose to use batch normalisation, a linear projection layer, and  as the parameters for distillation. We observe a significant improvement over both DeiT and CivT when the capacity gap is large. However, as the capacity gap diminishes, and the student approaches the same performance as the teacher, this improvement is much less significant. Multiple factors, such as the soft maximum function and the batch normalisation, will be contributing to this observed result. However, the explanation is more concisely described by the fact that our distillation loss transfers more translational equivariance, which is discussed in the next section.

\paragraph{Cross architecture distillation can implicitly transfer inductive biases.} CNNs use convolutions, which are spatially local operations, whereas transformers use self-attention, which are global operations. We expect that a benefit of this cross-architecture distillation setting is that the students learn to be "more" spatially equivariant in an attempt to match the teachers underlying function. It is this strong inductive bias that can reduce the amount of training data needed. A layer is translation equivariant if the following property holds:



In other words, if we take a translated input  and pass it through a layer , the result should be equivalent to first applying  to  and then performing the translation. A natural measure of equivariance can then be the difference between the left and right-hand side of this equation \ref{eqn:equivariance}.



\begin{table}[H]
    \centering
    \resizebox{1.\columnwidth}{!}{\begin{tabular}{lccc}
    \toprule
    Network & acc@1	& acc@5	& \#params \\
    \midrule
    RegNety 160~\cite{Radosavovic2020DesigningSpaces} & 82.6 & 96.4 & 84M \\

\midrule
    DeiT-Ti~\cite{Touvron2021TrainingAttention} & 72.2 & 91.1 & 5M\\
    \Lsh KD~\cite{Touvron2021TrainingAttention} & 74.5 & 91.9 & 6M \\
    \Lsh Co-advise~\cite{Ren2022Co-advise:Distillation} & 74.9 & - & 6M \\
    \Lsh USKD~\cite{yang2023knowledge} & 75.0 & - & 6M \\
\Lsh Our Method & \textbf{77.2} & \textbf{93.7} & 6M \\
\midrule
    Swin-Ti & - & - & 29M \\ \vspace{0.3em}
    \Lsh DIST~~\cite{huang2022knowledge} & 82.3 & - & 29M \\
    DeiT-S~\cite{Touvron2021TrainingAttention} & 79.8 & 95.0 & 22M \\
    \Lsh KD~\cite{Touvron2021TrainingAttention} & 81.2 & 95.4 & 22M \\
    \Lsh Co-advise~\cite{Ren2022Co-advise:Distillation} & 82.0 & - & 22M \\
    \Lsh USKD~\cite{yang2023knowledge} & 80.8 & - & 22M \\
\Lsh Our Method & \textbf{82.1} & \textbf{96.0} & 22M \\
\midrule
\end{tabular}     }
    \caption{\textbf{Data-efficient training of transformers} on Imagenet with DeiT. All models are trained for 300 epochs using Mixup augmentation~\cite{Zhang2017Mixup:Minimization}.  uses additional training data and a pretrained Swin-L teacher.}
\label{table:data_efficient_training_transformers}
\end{table}

\begin{table*}[htp]
    \centering
\begin{tabular}{c|cc|ccccc|c}
    \toprule
& Teacher & Student & AT & KD & CC & CRD & ReviewKD & Ours \\
\midrule
    acc@1 & 26.69 & 30.25 & 29.30 & 29.34 & 30.04 & 28.62 & 28.39 & \textbf{28.37} \\
    acc@5 & 8.58 & 10.93 & 10.00 & 10.12 & 10.83 & 9.51 & 9.42 & \textbf{9.41} \\





    \midrule
\end{tabular} \caption{\textbf{Top-1 and Top-5 error rates (\%) on ImageNet.} ResNet18 as student, ResNet34 as teacher.}
    \label{table:imagenet}
\end{table*}

We evaluate this measure on a block of self-attention layers by first removing the distillation and class tokens and then rolling the patch tokens to recover the spatial dimensions. This operation can then be performed on the input and output tensors before applying a translation. Table \ref{table:equivariance_measure} shows this measure of equivariance after training with and without distillation. In general, we observe that the distilled models do in fact learn to preserve spatial locality between feature maps, which aligns with the function matching perspective for distillation (see figure \ref{table:equivariance_measure}). We also find that our distillation method can transfer a lot more of this equivariance property to the student. Although CivT-S does learn this spatial locality to some extent, it is much less significant than using our feature based distillation, despite both attaining a similar level of performance. Although feature map losses may be able to transfer even more of this equivariance, we believe it may degrade the benefit of using transformers in the first place. For example, although we observe that most self-attention blocks (trained using distillation) do preserve a lot of the spatial locality, there is still some global context between patch tokens that is still being preserved.

\begin{table}[H]
    \centering
\begin{tabular}{lc}
    \toprule
    Network & 	\\
    \midrule
    DeIT~\cite{Touvron2021TrainingAttention} &  \\
    \Lsh Co-advise~\cite{Ren2022Co-advise:Distillation} &  \\
    \Lsh Our Method &  \\
    \bottomrule
\end{tabular} \caption{\textbf{Measure of translational equivariance of a DeiT-S transformer model trained with and without distillation.} Reported measure was computed after every epoch and averaged over 256 images. These results confirm that distillation can transfer explicit inductive biases from the teacher.}
    \label{table:equivariance_measure}
\end{table}

\subsection{Object Detection on COCO}
\label{sec:object_detection}

We extend the application of our method to object detection, whereby we employ a similar approach as used in the classification task by distilling the backbone output features of both the student and teacher networks. To evaluate the efficacy of our method, we conduct experiments on the widely-used COCO2017 dataset~\cite{Lin2014MicrosoftContext} under the same settings provided in ReviewKD. We then further demonstrate the applicability of our distillation principles on the more recent and efficient YOLOv5 model~\cite{Zhu2021TPH-YOLOv5:Scenarios}. In both cases we show improved student performance on the downstream task, whereby competitive performance is achieved with ReviewKD despite being significantly simpler and cheaper to integrate into a given distillation pipeline. Our method even outperforms FPGI~\cite{Wang2019DistillingImitationb}, which is directly designed for detection.

\begin{table}[ht]
    \centering
    \resizebox{1.\linewidth}{!}{\begin{tabular}{lcc}
    \toprule
    Model & mAP (50-95) & mAP 50 \\
\midrule
    YOLOv5m (teacher) & 64.1 & 45.4 \\
YOLOv5s & 56.8 & 37.4 \\

    \Lsh Our Method & \textbf{57.3} & \textbf{37.5} \\
    
    \midrule
    & mAP & AP50 \\
    \midrule
    
    Faster R-CNN w/ R50-FPN (teacher) & 40.22 & 61.02 \\
Faster R-CNN w/ MV2-FPN & 29.47 & 48.87 \\
    \Lsh KD~\cite{Hinton2015DistillingNetwork} & 30.13 & 50.28 \\
    \Lsh FitNet~\cite{Romero2015FitNets:Nets} & 30.20 & 49.80 \\
    \Lsh FPGI~\cite{Wang2019DistillingImitationb} & 31.16 & 50.68 \\
    \Lsh ReviewKD~\cite{Chen2021DistillingReview} & \textbf{33.71} & \textbf{53.15} \\
\Lsh Our Method & \underline{32.92} & \underline{52.96} \\

    \bottomrule
    
\end{tabular}     }
    \caption{\textbf{Object detection on COCO.} (top) We report the standard COCO metric of mAP averaged over IOU thresholds in [0.5 : 0.05 : 0.95] along with the standard PASCAL VOC’s metric~\cite{Everingham2010TheChallenge}, which is the average mAP@0.5. (bottom) For the R-CNN results, we report the mAP and AP50 metrics to enable a consistent comparison with ReviewKD.}
    \label{table:yolo}
\end{table} 

\section{Conclusion}
\label{sec:conclusion}
In this paper, we revisited knowledge distillation from a function matching and metric learning perspective. We performed an extensive ablation on the most effective and scaleable components of distillation and in doing so provide a theoretical perspective to understand these observations. By extending these principles to a wide range of tasks, we achieve competitive or improved performance to state-of-the-art across image classification, object detection, and data efficient training of transformers. Although we do not aim to necessarily propose a new method, we do provide an effective theory to approach the capacity gap problem in distillation. We also significantly reduce the complexity and memory consumption's of existing pipelines through unifying a new underlying principle of distillation. Looking ahead to future research in this area, we expect to see the joint development of more sophisticated normalisation schemes and projection networks, which will encode more complex and informative features for the distillation process. 

\paragraph{Code Reproducibility.} To facilitate the reproducibility of results, we will release all the training code and pre-trained weights. The ImageNet experiments are also performed using the popular \textit{torchdistill}~\cite{Matsubara2020TorchdistillDistillation} framework, while the CIFAR100 and data-efficient training code is based on those provided by CRD~\cite{Tian2019ContrastiveDistillation} and co-advice~\cite{Ren2022Co-advise:Distillation} respectively.

\bibliography{references, more_references}





\end{document}
