\documentclass[12pt]{article}

\usepackage{tikz}  \usepackage{pifont}\usepackage[symbol*]{footmisc}

\usepackage{hyperref}\hypersetup{breaklinks,ocgcolorlinks,
   colorlinks=true,linkcolor=[rgb]{0.45,0.0,0.0},citecolor=[rgb]{0,0,0.45}
}
\usepackage{titlesec}
\titlelabel{\thetitle. }

\usepackage{ntheorem}
\usepackage{wide}\usepackage{color}\usepackage{xspace}\usepackage{euscript}\usepackage{amstext}
\usepackage{paralist}\usepackage{amsmath}\usepackage{amssymb}\usepackage{graphicx}
\usepackage{theorem}\usepackage{salgorithm}


\theoremseparator{.}






\newcommand*\circled[1]{\footnotesize\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=0.2pt] (char) {#1};}}



\DefineFNsymbols*{stars}[text]{{\ding{172}} {\ding{173}} {\ding{174}} {\ding{175}} {\ding{176}} {\ding{177}} {\ding{178}} {\ding{179}} {\ding{180}} {{\protect\circled{{\tiny 10}}}}{{\protect\circled{{\tiny 11}}}}{{\protect\circled{{\tiny 12}}}}{{\protect\circled{{\tiny 13}}}}{{\protect\circled{{\tiny 14}}}}{{\protect\circled{{\tiny 15}}}}{{\protect\circled{{\tiny 16}}}}{{\protect\circled{{\tiny 17}}}}{{\protect\circled{{\tiny 18}}}}{{\protect\circled{{\tiny 19}}}}{{\protect\circled{{\tiny 20}}}}{{\protect\circled{{\tiny 21}}}}{{\protect\circled{{\tiny 22}}}}}






\providecommand{\tildegen}{{\protect\raisebox{-0.1cm}
      {\symbol{'176}\hspace{-0.01cm}}}}

\providecommand{\SarielWWWPapersAddr}
        {http://www.uiuc.edu/\string~sariel/papers}

\providecommand{\SarielWWWPapers}
    {http://www.uiuc.edu/\tildegen{}sariel/papers}

\providecommand{\urlSarielPaper}[1]{
     \href{\SarielWWWPapersAddr/#1}
            {\SarielWWWPapers{}/#1}}
\providecommand{\urlSarielPaperExt}[2]
{\href{\SarielWWWPapersAddr/#1}
            {\SarielWWWPapers{}/#2}}

\definecolor{blue25}{rgb}{0,0,0.25}
\newcommand{\emphic}[2]{\textcolor{blue25}{\textbf{\emph{#1}}}\index{#2}}
\newcommand{\emphi}[1]{\emphic{#1}{#1}}


\newcommand{\obslab}[1]{\label{observation:#1}}
\newcommand{\obsref}[1]{Observation~\ref{observation:#1}}

\newcommand{\assumplab}[1]{\label{assumption:#1}}
\newcommand{\assumpref}[1]{Assumption~\ref{assumption:#1}}

\providecommand{\lemlab}[1]{\label{lemma:#1}}
\providecommand{\lemref}[1]{Lemma~\ref{lemma:#1}}

\providecommand{\lempntlab}[1]{\label{lemmapnt:#1}}
\providecommand{\lempntref}[1]{(\ref{lemmapnt:#1})}
\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\alglab}[1]{\label{algorithm:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{algorithm:#1}}
\newtheorem{theorem}{Theorem}[section] \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{assumption}[theorem]{Assumption}
\newcommand{\Term}[1]{\textsf{#1}}
\newcommand{\TermI}[1]{\Term{#1}\index{#1@\Term{#1}}}
\newcommand{\BFS}{\textsf{BFS}\xspace}
\newcommand{\PTAS}{\TermI{PTAS}\xspace}
\newcommand{\aftermathA}{\par\vspace{-\baselineskip}}
\theoremstyle{remark}{\theorembodyfont{\rm} \newtheorem{remark}[theorem]{Remark}}
\newenvironment{proof}{\trivlist\item[]\emph{Proof}:}{\unskip\nobreak\hskip 1em plus 1fil\nobreak \rule{2mm}{2mm}\parfillskip=0pt\endtrivlist}
\newcommand{\thmlab}[1]{{\label{theo:#1}}}
\newcommand{\thmref}[1]{Theorem~\ref{theo:#1}}
\newcommand{\thmrefpage}[1]{Theorem~\ref{theo:#1}}
\newcommand{\eqnlab}[1]{\label{equation:#1}}
\newcommand{\eqnref}[1]{(\ref{equation:#1})}
\newcommand{\eqlab}[1]{\label{equation:#1}}
\newcommand{\Eqref}[1]{Eq.~(\ref{equation:#1})}
\newcommand{\Eqrefpage}[1]{Eq.~(\ref{equation:#1})}
\newcommand{\seclab}[1]{{\label{section:#1}}}
\newcommand{\secref}[1]{Section~\ref{section:#1}}
\newcommand{\remlab}[1]{\label{remark:#1}}
\newcommand{\remref}[1]{Remark~\label{remark:#1}}
\newcommand{\deflab}[1]{\label{defn:#1}}
\newcommand{\defref}[1]{Definition~\ref{defn:#1}}
\newcommand{\defrefpage}[1]{Definition~\ref{defn:#1}}
\providecommand{\Comment}[1]{\textcolor[named]{RedViolet}{\texttt{{\bf //} #1}}}


\newcommand{\MakeVBig}{\rule[-.2cm]{0cm}{0.62cm}}
\newcommand{\MakeBig}{\rule[-.2cm]{0cm}{0.4cm}}
\newcommand{\MakeSBig}{\rule[0.0cm]{0.0cm}{0.35cm}} \newcommand{\brc}[1]{\left\{ {#1} \right\}}
\newcommand{\sep}[1]{\,\left|\, {#1} \MakeBig\right.}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}
\newcommand{\pbrc}[2][\!\!]{#1\left[ {#2} \MakeBig \right]}


\newcommand{\ceiling}[1]{\left \lceil {#1} \right \rceil}
\newcommand{\norm}[1]{\left\lVert {#1} \right \rVert}
\newcommand{\normX}[2]{\left\lVert {#1} \right \rVert_{#2}}

\newcommand{\normY}[2]{\left\lVert {#1} - {#2} \right \rVert}

\newcommand{\normP}[1]{\norm{#1}_{\oplus}}

\newcommand{\distP}[2]{\normP{{#1}- {#2}}}
\newcommand{\distPk}[3]{\mathsf{d}_{#3}\pth{#2,#1}}
\newcommand{\distNN}[2]{\mathsf{d}\pth{#1,#2}}
\newcommand{\minDistPk}[2]{r_{\mathrm{opt}}\pth{{#1},{#2}}}
\newcommand{\radius}[1]{\mathrm{radius}({#1})}

\newcommand{\abs}[1]{\left | {#1} \right |}
\newcommand{\floor}[1]{\left\lfloor {#1} \right\rfloor}
\newcommand{\nfrac}[2]{{#1}/{#2}}
\newcommand{\cardin}[1]{\left\lvert {#1} \right\rvert}
\newcommand{\order}[1]{O\pth{#1}}
\newcommand{\approxorder}[1]{\widetilde{O}\pth{#1}}
\newcommand{\ordereq}[1]{\Theta \left ( {#1} \right )}
\newcommand{\ordergeq}[1]{\Omega \left ( {#1} \right )}


\newcommand{\eps}{{\varepsilon}}\newcommand{\divides}{|}

\newcommand{\DDM}[2]{F}
\newcommand{\aDDM}[2]{{F}_1}
\newcommand{\ADDM}[2]{F_2}
\newcommand{\IdxSet}{\mathcal{I}}

\newcommand{\Hp}[1]{H_{#1}}
\newcommand{\wt}[1]{w_{#1}}
\newcommand{\wtX}[1]{w\pth{#1}}
\newcommand{\FSF}{\mathcal{F}_{\mathsf{sg}}}
\newcommand{\SarielThanks}[1]{\thanks{Department of Computer
      Science; 
      University of Illinois; 
      201 N. Goodwin Avenue;
      Urbana, IL, 61801, USA;
      {\tt sariel\atgen{}illinois.edu}; {\tt
         \url{http://www.illinois.edu/\string~sariel/}.} #1}}
\newcommand{\NirmanThanks}[1]{\thanks{Department of Computer
      Science; 
      University of Illinois; 
      201 N. Goodwin Avenue;
      Urbana, IL, 61801, USA;
      {\tt \si{nkumar5}\atgen{}illinois.edu}; {\tt
         \url{http://www.illinois.edu/\string~\si{nkumar5}/}.} #1}}
\newcommand{\atgen}{\symbol{'100}}

\providecommand{\si}[1]{#1}
\providecommand{\Matousek}{Matou{\v s}ek\xspace}


\newcommand{\WSPD}{\TermI{WSPD}\xspace}
\newcommand{\ANN}{\TermI{ANN}\xspace}
\newcommand{\NNTerm}{\TermI{NN}\xspace}
\newcommand{\NND}{\TermI{NND}\xspace}
\newcommand{\KANN}{\TermI{ANN}\xspace}
\newcommand{\KANND}{\TermI{ANND}\xspace}
\newcommand{\JL}{\TermI{JL}\xspace}
\newcommand{\AVD}{\TermI{AVD}\xspace}
\newcommand{\etal}{\textit{et~al.}\xspace}
\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\diameter}[1]{\mathsf{diam}\pth{ {#1} }}
\newcommand{\PntSet}{\mathsf{P}}
\newcommand{\PntSetQ}{\mathsf{Q}}
\newcommand{\query}{\mathtt{q}}
\newcommand{\dkann}{\alpha}

\newcommand{\pnt} {\mathsf{p}}
\newcommand{\pntA}{\mathsf{u}}
\newcommand{\pntB}{\mathsf{v}}
\newcommand{\pntC}{\mathsf{s}}

\newcommand{\distS}[2]{\mathsf{d}\pth{#1,#2}}
\newcommand{\distSP}[2]{\mathsf{d}_{\oplus}\pth{#1,#2}}

\newcommand{\dist}[2]{\norm{{#1}- {#2}}}
\newcommand{\distX}[2]{\normX{{#1}{#2}}{1}}

\newcommand{\distY}[2]{\normY{#1}{#2}}

\newcommand{\distXS}[2]{\mathsf{d}_\oplus\pth{#1,#2}}
\newcommand{\ballA}{\mathsf{b}}
\newcommand{\ballB}{\mathsf{u}}
\newcommand{\Ring}[3]{\mathsf{R}\pth{#1,#2,#3}}
\newcommand{\BallSet}{\mathcal{B}}
\newcommand{\ctrA}{\mathsf{c}}
\newcommand{\radA}{\mathsf{r}}

\newcommand{\ctrB}{\mathsf{c_1}}
\newcommand{\radB}{\mathsf{r_1}}

\newcommand{\ctrC}{\mathsf{c_2}}
\newcommand{\radC}{\mathsf{r_2}}

\newcommand{\ctrD}{\mathsf{c_3}}
\newcommand{\radD}{\mathsf{r_3}}

\newcommand{\kdist}{w}
\newcommand{\distA}{t}
\newcommand{\CtrSetA}{\mathcal{C}}
\newcommand{\BallA}{\mathsf{B}}
\newcommand{\CellSetA}{\mathcal{X}}
\newcommand{\CellSetB}{\mathcal{U}}
\newcommand{\CellSetAVD}{\mathcal{W}}
\newcommand{\CellSetC}{\mathcal{S}}
\newcommand{\QtreeA}{\mathcal{T}}
\newcommand{\QtreeB}{\mathcal{V}}
\newcommand{\Nnds}{\mathcal{N}}
\newcommand{\cellA}{\mathsf{\Box}}
\newcommand{\nndist}{x}
\newcommand{\distB}{y}
\newcommand{\Scube}{U}
\newcommand{\UnitCube}{[0,1]^d}
\newcommand{\regionA}{\mathsf{R}}
\newcommand{\mapped}[1]{#1'}
\newcommand{\ldist}{\ell}
\newcommand{\adist}{a}
\newcommand{\CellB}{\mathcal{V}}
\newcommand{\repres}[1]{#1_{\mathsf{rep}}}
\newcommand{\knnrepX}[1]{\mathrm{nn}_k\pth{#1}}
\newcommand{\DFS}{\Algorithm{DFS}\xspace}
\newcommand{\algkANNQuery}{\Algorithm{\si{algkANNQuery}}\xspace}
\newcommand{\algQC}{\Algorithm{\si{QuorumCluster}}\xspace}
\renewcommand{\th}{th\xspace}
\newcommand{\Otilde}{\widetilde{O}}

\newcommand{\NN}[2]{\mathsf{nn}\pth{#1, #2}}\newcommand{\NNk}[3]{\mathsf{nn}_{#1}\pth{#2, #3}}\newcommand{\dNNk}[3]{\mathsf{d}_{#1}\pth{#2, #3}}\newcommand{\Cell}{\Box}
\newcommand{\CenterSet}{\mathcal{C}}
\newcommand{\Grid}{\mathsf{G}\index{grid}}
\newcommand{\ball}[2]{\mathsf{ball}\pth{#1,#2}}
\newcommand{\gridSet}[2]{\boxplus\pth{#1, #2}}

\newcommand{\num}{\alpha}
\newcommand{\ds}{\displaystyle}
\newcommand{\QTree}{\EuScript{T}\index{quadtree}}
\newcommand{\repX}[1]{\mathrm{nn'}\pth{#1}}
\newcommand{\pntrepX}[1]{\mathbf{p}\pth{#1}}
\newcommand{\remove}[1]{}
\newcommand{\qtreenode}{\nu}
\newcommand{\reg}[1]{\mathrm{rg}_{#1}}
\newcommand{\rvector}{\mathbf{b}}
\newcommand{\rep}[1]{\mathrm{rep}_{#1}}
\newcommand{\adknn}[1]{\mathrm{\beta}_k\pth{#1}}
\newcommand{\approxkd}{R}
\newcommand{\approxfactor}{f}

\newcommand{\lo}{\mathsf{l}}\newcommand{\hi}{\mathsf{h}}\newcommand{\levelX}[1]{\mathrm{l{v}l}\pth{#1}}


\newcommand{\NNd}{\textsf{N{}N}\xspace}
\newcommand{\ceil}[1]{\left\lceil {#1} \right\rceil}

\newcommand{\constA}{\zeta_1}
\newcommand{\constB}{c}
\newcommand{\constC}{\zeta_2}
\newcommand{\constD}{\zeta_3}
\newcommand{\ts}{\hspace{0.6pt}}

\newlength{\savedparindent}
\newcommand{\SaveIndent}{\setlength{\savedparindent}{\parindent}}
\newcommand{\RestoreIndent}{\setlength{\parindent}{\savedparindent}}
\newcommand{\XSays}[2]{{
      {\fbox{\tt
            #1:} }
      #2
      \marginpar{#1}
      {\fbox{\tt
            end}}
      }
   }
\newcommand{\sariel}[1]{{\XSays{Sariel}{#1}}}
\newcommand{\Sariel}[1]{{\XSays{Sariel}{#1}}}

\newcommand{\numA}{z}
\newcommand{\DS}{\mathcal{D}}\newcommand{\Array}{\mathcal{X}}

\newlength{\ppicwd}
\newcommand{\padbox}[1]{\settowidth{\ppicwd}{#1}\begin{minipage}{1.01\ppicwd}\smallskip {#1}\smallskip \end{minipage}
}

\newcommand{\wx}{\tau}

\newcommand{\hdim}{d}
\newcommand{\Phalf}[1]{\PntSet_{\leq n/2}\pth[]{#1}}
\newcommand{\Pk}[2][\!]{\PntSet_{\leq k}\pth[#1]{#2}}
\newcommand{\PkExt}[4][\!]{{#2}_{\leq #3}\pth[#1]{#4}}
\newcommand{\Family}{\mathcal{H}}\newcommand{\dhalf}[2][\!]{\mathsf{d}_{n/2}\pth[#1]{#2}}
\newcommand{\dk}[2][\!]{\mathsf{d}_{k}\pth[#1]{#2}}
\newcommand{\BadProb}{\varphi}
\newcommand{\prob}{\rho}
\newcommand{\RangeSpace}{\mathsf{S}}
\newcommand{\GroundSet}{\textsf{X}}\newcommand{\pbrcS}[1]{\left[ {#1} \right]}
\newcommand{\Dim}{\delta}\newcommand{\Set}{N}
\newcommand{\DistX}[2]{d_{#1}\pth{#2}}
\newcommand{\MeasureChar}{\overline{m}}
\newcommand{\sMeasureChar}{\overline{s}}
\newcommand{\Measure}[1]{\MeasureChar\pth{#1}}
\newcommand{\sMeasureX}[2]{\overline{s}_{#2}\pth{#1}}
\newcommand{\pbrcx}[1]{\left[ {#1} \right]}
\newcommand{\Prob}[1]{\mathop{\mathbf{Pr}}\!\pbrcx{#1}}

\newcommand{\X}{\EuScript{X}}\newcommand{\R}{\EuScript{R}}\newcommand{\range}{\mathsf{r}}\newcommand{\VC}{\Term{VC}\xspace}
\newcommand{\RSample}{\mathsf{R}}

\newcommand{\Error}{\EuScript{E}}
\providecommand{\Merigot}{M{\' e}rigot}




\begin{document}

\title{Down the Rabbit Hole: Robust Proximity Search and Density
   Estimation in Sublinear Space\footnote{Work on this paper was partially supported by NSF AF awards
      CCF-0915984 and CCF-1217462. A preliminary version of this paper appeared in FOCS 2012
      \cite{hk-drhrp-12}.}}


\author{Sariel Har-Peled\SarielThanks{}\and Nirman Kumar\NirmanThanks{}}

\date{\today}



\maketitle

\setfnsymbol{stars}

\begin{abstract}
    For a set of  points in , and parameters  and ,
    we present a data structure that answers -\ANN queries
    in logarithmic time.  Surprisingly, the space used by the
    data-structure is ; that is, the space used is
    sublinear in the input size if  is sufficiently large. Our
    approach provides a novel way to summarize geometric data, such
    that meaningful proximity queries on the data can be carried out
    using this sketch. Using this, we provide a sublinear space
    data-structure that can estimate the density of a point set under
    various measures, including:
    \begin{inparaenum}[(i)]
        \item sum of distances of  closest points to the query
        point, and
        \item sum of squared distances of  closest points to the
        query point.
    \end{inparaenum}
    Our approach generalizes to other distance based estimation of
    densities of similar flavor.

    We also study the problem of approximating some of these
    quantities when using sampling. In particular, we show that a
    sample of size  is sufficient, in some restricted
    cases, to estimate the above quantities. Remarkably, the sample
    size has only linear dependency on the dimension.
\end{abstract}


\section{Introduction}

Given a set  of  points in , the \emphi{nearest
   neighbor} problem is to construct a data structure, such that for
any \emph{query} point  it (quickly) finds the closest point
to  in .  This is an important and fundamental
problem in Computer Science \cite{sdi-nnmlv-06, c-tpfgn-08,
   ai-nohaa-08, c-nnsms-06}. Applications of nearest neighbor search
include pattern recognition \cite{fh-dandc-49, ch-nnpc-67},
self-organizing maps \cite{k-som-01}, information retrieval
\cite{swy-vsmai-75}, vector compression \cite{gg-vqsc-91},
computational statistics \cite{dw-nnmd-82}, clustering
\cite{dhs-pc-01}, data mining, learning, and many others.  If one is
interested in guaranteed performance and near linear space, there is
no known way to solve this problem efficiently (i.e., logarithmic
query time) for dimension .

A commonly used approach for this problem is to use Voronoi
diagrams. The \emphi{Voronoi diagram} of  is the
decomposition of  into interior disjoint closed cells, so that
for each cell  there is a unique single point 
such that for any point  the
nearest-neighbor of  in  is .  Thus, one can
compute the nearest neighbor of  by a point location query in
the collection of Voronoi cells.  In the plane, this approach leads to
 query time, using  space, and preprocessing time
.  However, in higher dimensions, this solution leads to
algorithms with \emph{exponential} dependency on the dimension. The
complexity of a Voronoi diagram of  points in  is
 in the worst case.  By requiring
slightly more space, Clarkson \cite{c-racpq-88} showed a
data-structure with query time , and
 space, where  is a
prespecified constant (the  notation here hides constants
that are exponential in the dimension). One can tradeoff the space
used and the query time \cite{am-rsps-93}. Meiser \cite{m-plah-93}
provided a data-structure with query time  (which
has polynomial dependency on the dimension), where the space used is
.  Therefore, even for moderate dimension,
the exact nearest neighbor data-structure uses an exorbitant amount of
storage.  It is believed that there is no efficient solution for the
nearest neighbor problem when the dimension is sufficiently large
\cite{mp-p-69}; this difficulty has been referred to as the ``curse of
dimensionality''.

\paragraph{Approximate Nearest Neighbor (\ANN).}

In light of the above, major effort has been devoted to develop approximation
algorithms for nearest neighbor search \cite{amnsw-oaann-98,
   im-anntr-98, kor-esann-00, sdi-nnmlv-06, c-tpfgn-08, ai-nohaa-08,
   \si{c-nnsms-06}, \si{him-anntr-12}}. In the 
\emphi{-approximate nearest neighbor} problem (the \ANN problem), 
one is additionally given an  approximation parameter  and one 
is required to find a point  such that 
. In  dimensional 
Euclidean space, one can answer \ANN queries, in  time
using linear space \cite{amnsw-oaann-98, h-gaa-11}. Because of the
 in the query time, this approach is only efficient in
low dimensions. Interestingly, for this data-structure, the
approximation parameter  need not be specified during the
construction, and one can provide it during the query.  An alternative
approach is to use Approximate Voronoi Diagrams (\AVD), introduced by
Har-Peled \cite{h-rvdnl-01}, which is a partition of space into regions
of low total complexity, with a representative point for each region,
that is an \ANN for any point in the region. In particular, Har-Peled
showed that there is such a decomposition of size , see also \cite{him-anntr-12}. 
This allows \ANN queries to be answered in  time.  Arya and Malamatos \cite{am-lsavd-02} showed how to
build \AVD{}s of linear complexity (i.e., ). Their
construction uses \WSPD (Well Separated Pair Decomposition)
\cite{ck-dmpsa-95}. Further tradeoffs between query time and space usage for
\AVD{}s were studied by Arya \etal \cite{amm-sttan-09}.


\paragraph{-nearest neighbor.}
A more general problem is the -nearest neighbors problem where one
is interested in finding the  points in  nearest to the
query point . This is widely used in pattern recognition,
where the majority label is used to label the query point. In this paper, 
we are interested in the more restricted problem of approximating the
distance to the \th nearest neighbor and finding a data point
achieving the approximation. We call this problem the 
\emphi{-approximate nearest neighbor} (-\ANN) 
problem. This problem is widely used for density
estimation in statistics, with 
\cite{s-desda-86}. It is also used in meshing (with ), or to compute
the local feature size of a point set in  \cite{r-draqt-95}.
The problem also has applications in non-linear dimensionality
reduction; finding low dimensional structures in data -- more
specifically low dimensional submanifolds embedded in Euclidean
spaces. Algorithms like ISOMAP, LLE, Hessian-LLE, SDE and others,
use the -nearest neighbor as a subroutine \cite{t-mmpo-98,
   bslt-gagem-00, ms-trn-94, ws-ulms-04}.

\paragraph{Density estimation.}
Given distributions  defined over , and a
query point , we want to compute the \emph{a
   posteriori} probabilities of  being generated by one of
these distributions. This approach is used in unsupervised learning as
a way to classify a new point. Naturally, in most cases, the
distributions are given implicitly; that is, one is given a large
number of points sampled from each distribution. So, let  be such
a distribution, and  be a set of  samples. To estimate the
density of  at , a standard Monte Carlo technique is to
consider a ball  centered at , and count the number of
points of  inside .  Specifically, one possible
approach that is used in practice \cite{dhs-pc-01}, is to find the
smallest ball centered at  that contains  points of
 and use this to estimate the density of .  The
right value of  has to be chosen carefully -- if it is too small,
then the estimate is unstable (unreliable), and if it is too large, it
either requires the set  to be larger, or the estimate is too
``smoothed'' out to be useful (values of  that are used in practice
are ), see Duda \etal \cite{dhs-pc-01} for more
details. To do such density estimation, one needs to be able to answer,
approximate or exact, -nearest neighbor queries.

Sometimes one is interested not only in the radius of this ball
centered at the query point, but also in the distribution of the
points inside this ball. The average distance of a point inside the
ball to its center, can be estimated by the sum of distances of the sample
points inside the ball to the center. Similarly, the variance of this
distance can be estimated by the sum of squared distances of the
sample points inside the ball to the center of the ball.  As mentioned, 
density estimation is used in manifold learning and surface
reconstruction. For example, Guibas \etal \cite{gmm-wkd-11} recently
used a similar density estimate to do manifold reconstruction.


\paragraph{Answering exact -nearest neighbor queries.}

Given a point set , computing the partition
of space into regions, such that the  nearest neighbors do not
change, is equivalent to computing the \emphi{\th order Voronoi
   diagram}.  Via standard lifting, this is equivalent to computing
the first  levels in an arrangement of hyperplanes in 
\cite{a-vdsfg-91}. More precisely, if we are interested in the
\th-nearest neighbor, we need to compute the -level in this
arrangement.

The complexity of the  levels of a hyperplane arrangement in
 is  \cite{cs-arscg-89}. The exact complexity of the \th-level is not
completely understood and achieving tight bounds on its complexity is
one of the long-standing open problems in discrete geometry
\cite{m-ldg-02}. In particular, via an averaging argument, in the
worst case, the complexity of the \th-level is . As such, the
complexity of \th-order Voronoi diagram is  in two
dimensions, and  in three dimensions.

Thus, to provide a data-structure for answering -nearest neighbor
queries exactly and quickly (i.e., logarithmic query time) in ,
requires computing the -level of an arrangement of hyperplanes in
. The space complexity of this structure is prohibitive
even in two dimensions (this also effects the preprocessing time).
Furthermore, naturally, the complexity of this structure increases as
 increases.  On the other end of the spectrum one can use
partition-trees and parametric search to answer such queries using
linear space and query time (roughly) 
\cite{m-ept-92, c-opt-10}. One can get intermediate results using
standard space/time tradeoffs \cite{ae-rsir-98}.

\paragraph{Known results on approximate -order Voronoi diagram.}
Similar to \AVD, one can define a \AVD for the -nearest neighbor.
The case  is the regular approximate Voronoi diagram
\cite{h-rvdnl-01, am-lsavd-02, amm-sttan-09}. The case  is the
furthest neighbor Voronoi diagram. It is not hard to see that it has a
constant size approximation (see \cite{h-caspm-99}, although it was
probably known before). Our results (see below) can be interpreted as
bridging between these two extremes.

\paragraph{Quorum clustering.}
Carmi \etal \cite{cdhks-gqsa-05} describe how to compute efficiently a
partition of the given point set  into clusters of  points each,
such that the clusters are compact. Specifically, this quorum clustering
computes the smallest ball containing  points, removes
this cluster, and repeats, see \secref{quorum} for more details. Carmi
\etal \cite{cdhks-gqsa-05} also describe a data-structure that can
approximate the smallest cluster. The space usage of their data structure is
, but it cannot be directly used for our
purposes. Furthermore, their data-structure is for two dimensions and
it cannot be extended to higher dimensions, as it uses additive
Voronoi diagrams (which have high complexity in higher dimensions).




\section*{Our results.}

We first show, in \secref{const}, how to build a data-structure
that answers -\ANN queries in time , 
where the input is a
set of  points in . Surprisingly, the space used by this
data-structure is .  This result is surprising as the
space usage \emphi{decreases} with . This is in sharp contrast to
behavior in the exact version of the \th-order Voronoi diagram
(where the complexity increases with ). Furthermore, for
super-constant  the space used by this data-structure is
sublinear. For example, in some applications the value of  used is
, and the space used in this case is a tiny
fraction of the input size.  This is a general reduction showing that
such queries can be reduced to proximity search in an appropriate
product space over  points computed carefully.

In \secref{sec:avd}, we show how to construct an \emph{approximate}
-order Voronoi diagram using space  (here
 is an approximation quality parameter specified in
advance). Using this data-structure one can answer
-\ANN queries in 
time. See \thmref{ann:main} for the exact result.


\paragraph{General density queries.}
We show in \secref{applications}, as an application of our
data-structure, how to answer more robust queries.  For
example, one can approximate (in roughly the same time and space as
above) the sum of distances, or squared distances, from a query point
to its  nearest neighbors. This is useful in approximating density
measures \cite{dhs-pc-01}. Surprisingly, our data-structure can be
used to estimate the sum of any function  defined over the
 nearest neighbors, that depends only on the distance of these
points from the query point. Informally, we require that  is
monotonically increasing with distance, and it is (roughly) not
super-polynomial. For example, for any constant , our
data-structure requires sublinear space (i.e., ), and given a query point , it can
-approximate the quantity , where  is the set of  nearest points 
in  to . The query time is logarithmic.

To facilitate this, in a side result, that might be of independent
interest, we show how to perform point-location queries in 
compressed quadtrees of total size  simultaneously in  time (instead of the naive  query time), without
asymptotically increasing the space needed.

\paragraph{If  is specified with the query.}
In \secref{qtree:algo}, given a set  of  points in
, we show how to build a data-structure, in  time
and using  space, such that given a query point and parameters
 and , the data-structure can answer -\ANN 
queries in 
time. Unlike previous results, this is the first data-structure where
\emph{both}  and  are specified during the query
time. The data-structure of Arya \etal \cite{amm-sttas-05}
required knowing  in advance. Using standard techniques
\cite{amnsw-oaann-98} to implement it, should lead to a simple and
practical algorithm for this problem.



\paragraph{If  is not important.}
Note, that our main result can not be done using sampling. Indeed,
sampling is indifferent to the kind of geometric error we care
about. Nevertheless, a related question is how to answer a
-\ANN query if one is allowed to also approximate .
Inherently, this is a different question that is, at least
conceptually, easier. Indeed, the problem boils down to using sampling
carefully, and loses much of its geometric flavor. We show to solve
this variant (this seems to be new) in \secref{sampling}. Furthermore,
we study what kind of density functions can be approximated by such an
approach. Interestingly, the sample size needed to provide good
density estimates is of size  (which is sublinear in
), and surprisingly, has only linear dependency on the
dimension. This compares favorably with our main result, where the
space requirement is exponential in the dimension.


\paragraph{Techniques used.}
We use quorum clustering as a starting point in our solution. In
particular, we show how it can be used to get a constant
factor approximation to the approximate -nearest neighbor distance
using sublinear space.  Next, we extend this construction and combine
it with ideas used in the computation of approximate Voronoi
diagrams. This results in an algorithm for computing approximate
-nearest neighbor Voronoi diagram.  To extend this data-structure
to answer general density queries, as described above, requires a
way to estimate the function  for relatively few values
(instead of  values) when answering a query. We use a coreset
construction to find out which values need to be approximated.
Overall, our work combines several known techniques in a non-trivial
fashion, together with some new ideas, to get our new results.

For the sampling results, of \secref{sampling}, we need to use some
sampling bounds that are not widely known in Computational Geometry.


\paragraph{Paper organization.}
In \secref{prelim} we formally define the problem and introduce some
basic tools, including quorum clustering, which is a key insight into
the problem at hand.  The ``generic'' constant factor algorithm is
described in \secref{const}.  We describe the construction of the
approximate -order Voronoi diagram in \secref{sec:avd}.  In
\secref{applications} we describe how to construct a data-structure to
answer density queries of various types. In \secref{qtree:algo} we
present the data-structure for answering -nearest neighbor queries
that does not require knowing  and  in advance.  The
approximation via sampling is presented in \secref{sampling}.
We conclude in \secref{conclusions}.


\section{Preliminaries}
\seclab{prelim}

\subsection{Problem definition}

Given a set  of  points in  and a number , , consider a point  and order the points of 
by their distance from ; that is,

where . The point
 is the \emphi{\th-nearest
   neighbor} of  and  is the \emphi{\th-nearest neighbor
   distance}. The nearest neighbor distance (i.e., ) is
.  The global minimum of
, denoted by , is the radius of
the smallest ball containing  points of . 

\begin{observation}\obslab{1:Lipschitz}For any ,  and a set , we have that .
\end{observation}
Namely, the function  is
-Lipschitz.
The problem at hand is to preprocess  such that given a query
point  one can compute  quickly.  The standard
\emphi{nearest neighbor problem} is this problem for . In the
\emphi{-approximate nearest neighbor} (-\ANN) problem, given
,  and , one wants to find a point , such that .




\subsection{Basic tools}

For a real positive number  and a point , define  to be the grid point
. We call  the \emphi{width} or \emphi{sidelength} of
the \emphi{grid} . Observe that the mapping 
partitions  into cubic regions, which we call grid
\emphic{cells}{cell}.

\begin{defn}
    A cube is a \emphi{canonical cube} if it is contained inside the
    unit cube , it is a cell in a grid , and  is
    a power of two (i.e., it might correspond to a node in a quadtree
    having  as its root cell).  We will refer to such a grid
     as a \emphic{canonical grid}{canonical!grid}. Note, that
    all the cells corresponding to nodes of a compressed quadtree are
    canonical.
\index{grid!canonical} \deflab{canonical:grid}
\end{defn}


For a ball  of radius , and a parameter , let
 denote the set of all the canonical cells
intersecting , when considering the canonical grid with
sidelength . Clearly,
.

A ball  of radius  in , centered at a point ,
can be interpreted as a point in , denoted by
. For a regular point , its corresponding image under this transformation is the
\emphi{mapped} point .

Given point  we will
denote its Euclidean norm by .  We will consider a point
 to
be in the product metric of  and endowed with the
product metric norm

It can be verified that the above defines a norm and the following
holds for it.
\begin{lemma}\lemlab{p:norm}For any  we have .
\end{lemma}
The distance of a point to a set under the  norm is
denoted by .



\begin{assumption}\assumplab{k:div:n}We assume that  divides ; otherwise one can easily add fake
    points as necessary at infinity.
\end{assumption}

\begin{assumption}\assumplab{points:in:cube}We also assume that the point set 
    is contained in , where
    . This can be achieved by scaling and
    translation (which does not affect the distance ordering).
    Moreover, we assume the queries are restricted to the unit cube
    .
\end{assumption}

\subsubsection{Quorum clustering}
\seclab{quorum}

\begin{figure*}[t]
    \centerline{
       \includegraphics{quorum}
    }
    \caption{Quorum clustering for  and .}
    \figlab{fig:quorum}
\end{figure*}


Given a set  of  points in , and a number , where , we start with the smallest ball 
that contains  points of , that is . Let . Continue on the set of points 
by finding the smallest ball that contains  points of , and so on.  Let
 denote the set of balls
computed by this algorithm and let . See
\figref{fig:quorum} for an example. Let  and  denote
the center and radius respectively, of , for .  A slight symbolic perturbation can guarantee that
\begin{inparaenum}[(i)]
    \item each ball  contains exactly  points of
    , and
    \item all the centers , are
    distinct points.
\end{inparaenum}
Observe that .  Such a partition of 
into  clusters is a \emphi{quorum clustering}. An
algorithm for computing it is provided in Carmi \etal
\cite{cdhks-gqsa-05}. We assume we have a black-box procedure
\algQC{} \cite{cdhks-gqsa-05} that computes an
\emphi{approximate} quorum clustering. It returns a list of balls,
.  The
algorithm of Carmi \etal \cite{cdhks-gqsa-05} computes such a sequence
of balls, where each ball is a -approximation to the smallest
ball containing  points of the remaining points. The following is
an improvement over the result of Carmi \etal \cite{cdhks-gqsa-05}.

\begin{lemma}\lemlab{q:clustering}Given a set  of  points in  and parameter ,
    where ,
    one can compute, in  time, a sequence of  balls,
    such that, for all , we have \smallskip
    \begin{compactenum}[\rm \quad(A)]
        \item For every ball  there is an
        associated subset  of  points of
        , that it covers.
        \item The ball  is a
        -approximation to the smallest ball covering  points in
        ; that is, .
    \end{compactenum}
\end{lemma}

\begin{proof}
    The guarantee of Carmi \etal is slightly worse -- their algorithm
    running time is . They use a dynamic
    data-structure for answering  queries, that report how many
    points are inside a query canonical square. Since they use
    orthogonal range trees this requires  time per
    query. Instead, one can use dynamic quadtrees. More formally, we
    store the points using linear ordering \cite{h-gaa-11}, using any
    balanced data-structure. A query to decide the number of points
    inside a canonical node corresponds to an interval query (i.e.,
    reporting the number of elements that are inside a query interval),
    and can be performed in  time. Plugging this
    data-structure into the algorithm of Carmi \etal
    \cite{cdhks-gqsa-05} gives the desired result.
\end{proof}



\section{A -\ANN in sublinear space}
\seclab{const}

\begin{lemma} \lemlab{5:approx}Let  be a set of  points in ,  be a
    number such that , , , be the list of balls
    returned by \algQC{}, and let  . We have
    that .
\end{lemma}

\begin{proof}
    For any  we have .  Since
    , we have
    .  As such, .
    
    For the other direction, let  be the first index such that
     contains a point of
    , where  is the set of  points of
     assigned to . Then, we have
    
    where ,  is a -approximation to
    , and the last inequality follows
    as 
    is a set of size  and .
    Then,
    
    as the distance from  to any  satisfies  by the triangle inequality.
    Putting the above together, we get
    
    \aftermathA \aftermathA \end{proof}

\begin{theorem}\thmlab{const:main}Given a set  of  points in , and a number  such that , one can build a data-structure,
    in  time, that uses  space, such that
    given any query point , one can compute, in
     time, a -approximation to
    .
\end{theorem}
\begin{proof}
    We invoke \algQC{} to compute the clusters
    , for . For , let . We preprocess the set  for
    -\ANN queries (in  under the Euclidean norm). 
    The preprocessing time for the
    \ANN data structure is , 
    the space used is  and the query time is 
     \cite{h-gaa-11}.
    
    Given a query point  the algorithm computes a
    -ANN to , denoted by
    , and returns
     as the approximate
    distance.
    
    Observe that, for any , we have  by \lemref{p:norm}.
    As such, the returned distance to  is a
    -approximation to ; that is,
    
    By \lemref{5:approx}, . Namely,
    
    implying the claim.
\end{proof}

\begin{remark}
    The algorithm of \thmref{const:main} works for any metric
    space. Given a set  of  points in a metric space, one
    can compute  points in the product space induced by adding
    an extra coordinate, such that approximating the distance to the
    \th nearest neighbor, is equivalent to answering \ANN queries
    on the reduced point set, in the product space.
\end{remark}


\section{Approximate Voronoi diagram for }
\seclab{sec:avd}

Here, we are given a set  of  points in , and our
purpose is to build an \AVD that approximates the -\ANN distance,
while using (roughly)  space.


\subsection{Construction}

\subsubsection{Preprocessing}
\begin{figure*}[t]
    \centerline{
       \includegraphics{grid}
    }
    \caption{Quorum clustering, immediate environs and grids. }
    \figlab{fig:grid}
\end{figure*}

\SaveIndent
\begin{compactenum}[(A)]
    \RestoreIndent
    \item Compute a quorum clustering for  using
    \lemref{q:clustering}. Let the list of balls returned be .
    
    \item Compute an exponential grid around each quorum
    cluster. Specifically, let
    
    be the set of grid cells covering the quorum clusters and their
    immediate environ, where  is a sufficiently large
    constant, see \figref{fig:grid}.
    
\item Intuitively,  takes care of the region of space
    immediately next to a quorum cluster\footnote{That is, intuitively, if the query point falls into one
       of the grid cells of , we can answer a query in
       constant time.}.  For the other regions of space, we can apply
    a construction of an approximate Voronoi diagram for
    the centers of the clusters (the details are somewhat more
    involved). To this end, lift the quorum clusters into points in
    , as follows
    
    where ,
    for .  Note, that all points in
     belong to  by
    \assumpref{points:in:cube}.  Now build a -\AVD for
     using the algorithm of Arya and Malamatos
    \cite{am-lsavd-02}. The \AVD construction provides a list of
    canonical cubes covering  such that in the smallest
    cube containing the query point, the associated point of
    , is a -\ANN to the query
    point. (Note, that these cubes are not necessarily disjoint. In
    particular, the smallest cube containing the query point 
    is the one that determines the assigned approximate nearest
    neighbor to .)
    
    
    Clip this collection of cubes to the hyperplane 
    (i.e., throw away cubes that do not have a face on this
    hyperplane). For a cube  in this collection, denote by
    , the point of  assigned to it.
    Let  be this resulting set of canonical -dimensional
    cubes.
    
    
    \item Let  be the space decomposition resulting from
    overlaying the two collection of cubes, i.e.  and
    .  Formally, we compute a compressed quadtree 
    that has all the canonical cubes of  and  as
    nodes, and  is the resulting decomposition of space
    into cells. One can overlay two compressed quadtrees representing
    the two sets in linear time \cite{bhst-sqgqi-10, h-gaa-11}.  Here,
    a cell associated with a leaf is a canonical cube, and a cell
    associated with a compressed node is the set difference of two
    canonical cubes. Each node in this compressed quadtree contains
    two pointers -- to the smallest cube of , and 
    to the smallest cube of , that contains it. This
    information can be computed by doing a \BFS on the tree.
    
    For each cell  we store the following.
    \begin{compactenum}[\qquad(I)]
        \item An arbitrary representative point .
        
        \item The point  that is
        associated with the smallest cell of  that contains
        this cell. We also store an arbitrary point, , that is one of the  points belonging to the
        cluster specified by .
        
        \item A number  that satisfies
        , and a point
         that realizes this
        distance. In order to compute  and
         use the data-structure of
        \secref{qtree:algo} (see \thmref{q:tree:main}) or
        the data-structure of Arya \etal \cite{amm-sttas-05}.
    \end{compactenum}
\end{compactenum}

\subsubsection{Answering a query}


Given a query point , compute the leaf cell (equivalently the
smallest cell) in  that contains  by performing a
point-location query in the compressed quadtree .  Let
 be this cell. Return

as the approximate value to . 
Return either
 or  depending on which
of the two distances  or
 is smaller
(this is the returned approximate value of
), as the approximate \th-nearest neighbor.

\subsection{Correctness}

\begin{lemma} \lemlab{algub}Let  and . Then the
    number computed by the algorithm is an upper bound on
    .
\end{lemma}

\begin{proof}
    By \obsref{1:Lipschitz}, .  Now, let .  We have, by \lemref{5:approx}, that
     As the returned value
    is the minimum of these two numbers, the claim holds.
\end{proof}

\begin{lemma}\lemlab{a:m:easy}Consider any query point , and let  be
    the smallest cell of  that contains the query point.
    Then, .
\end{lemma}

\begin{proof}
    Observe that the space decomposition generated by  is a
    refinement of the decomposition generated by the Arya and Malamatos
    \cite{am-lsavd-02} \AVD construction, when applied to
    , and restricted to the  dimensional subspace
    we are interested in (i.e., ). As such, 
    is the point returned by the \AVD for this query point
    before the refinement, thus implying the claim.
\end{proof}



\subsubsection{The query point is close to a quorum cluster of the
   right size}

\begin{lemma}\lemlab{lipschitz}Consider a query point , and let 
    be any set with , such that . Then, for any , we have \end{lemma}\begin{proof}
    By \obsref{1:Lipschitz}, we have
    
    The other direction follows by a symmetric argument.
\end{proof}

\begin{lemma}\lemlab{small:cell}If the smallest region  that contains
     has diameter , then the algorithm returns a
    distance which is between  and
    .
\end{lemma}
\begin{proof}
    Let  be the representative stored with the cell.
    Let  be the number returned by the algorithm.  By
    \lemref{algub} we have that .  Since the algorithm returns the minimum of two numbers, one
    of which is , we have by \lemref{lipschitz},
    
    establishing the claim.
\end{proof}

\begin{defn}
    Consider a query point .  The first quorum
    cluster  that intersects
     is the \emph{anchor
       cluster} of . The corresponding \emphi{anchor point} is
    .


    \deflab{anchor:p:t}
\end{defn}

\begin{lemma}\lemlab{anchor}For any query point , we have that
    \begin{compactenum}[(i)]
        \item the anchor point  is well defined,
        \item ,
        \item for  we have ,
        and
        \item .
    \end{compactenum}
\end{lemma}
\begin{proof}
    Consider the  closest points to  in . As
     it must
    be that  intersects
    some .  Consider the first cluster 
    in the quorum clustering that intersects
    .  Then
     is by definition the anchor point and we
    immediately have .  Claim
    (ii) is implied by the proof of \lemref{5:approx}.  Finally, as
    for (iv), we have  and
    the ball around  of radius 
    intersects , thus implying that
    .
\end{proof}



\begin{lemma}\lemlab{large:anchor}Consider a query point . If there is a cluster
     in the quorum clustering computed, such that
     and
    , then the output of the algorithm is
    correct.
\end{lemma}

\begin{proof}
    We have
    
    Thus, by construction, the expanded environ of the quorum cluster
     contains the query point, see
    \Eqrefpage{clusters:around:q}. Let  be the smallest integer
    such that . We have that,
    .  As such,
    if  is the smallest cell in  containing the
    query point , then
    
    by \Eqrefpage{clusters:around:q} and if .  As
    such, , and the claim follows by \lemref{lipschitz}.
\end{proof}

\subsubsection{The general case}

\begin{lemma}\lemlab{correct}The data-structure constructed above returns
    -approximation to , for any
    query point .
\end{lemma}

\begin{proof}
    Consider the query point  and its anchor point . By \lemref{anchor}, we have  and . This implies that
    
    Let the returned point, which is a -\ANN for
     in , be , where . We have that
    . In particular,
    
    and .
    
    
    Thus, if  or
     we are done,
    by \lemref{large:anchor}.  Otherwise, we have
    
    as  is a 
    approximation to . As
    such,
    
    As  we
    have, by the triangle inequality, that
    
    
    By \Eqref{rel:to:anchor} and \Eqref{int:anchor} we have
    
    By the above and as , we have
    
    Since the algorithm returns for  a value that is at most
    , the result is
    correct.
\end{proof}



\subsection{The result}


\begin{theorem}\thmlab{ann:main}Given a set  of  points in , a number  such that , and  sufficiently small,
    one can preprocess , in  time, where
     and . The space used by the
    data-structure is .  This data structure answers
    a -\ANN query in  time.  The data-structure also
    returns a point of  that is approximately the desired
    -nearest neighbor.
\end{theorem}

\begin{proof}
    Computing the quorum clustering takes time  by
    \lemref{q:clustering}.  Observe that . From the
    construction of Arya and Malamatos \cite{am-lsavd-02}, 
    we have  (note, that since
    we clip the construction to a hyperplane, we get  in the
    bound and not ). A careful implementation of this
    stage takes time . Overlaying the two compressed
    quadtrees representing them takes linear time in their size, that
    is .
    
    The most expensive step is to perform the -\ANN
    query for each cell in the resulting
    decomposition of , see \Eqrefpage{in:cell} (i.e.,
    computing  for each cell ). Using the data-structure of \secref{qtree:algo}
    (see \thmref{q:tree:main}) each query takes  time (alternatively, we could use the
    data-structure of Arya \etal \cite{amm-sttas-05}), As such, this
    takes
    
    time, and this bounds the overall construction time.
    
    The query algorithm is a point location query followed
    by an  time computation and
    takes time .
    
    Finally, one needs to argue that the returned point of 
    is indeed the desired approximate -nearest neighbor. This
    follows by arguing in a similar fashion to the correctness proof;
    the distance to the returned point is a -
    approximation to the \th-nearest neighbor distance.  We omit the
    tedious but straightforward details.
\end{proof}


\subsubsection{Using a single point for each \AVD cell}

The \AVD generated can be viewed as storing two points in each cell
 of the \AVD. These two points are in , and for a
cell , they are
\begin{compactenum}[\quad(i)]
    \item the point , and
    \item the point .
\end{compactenum}
The algorithm for  can be viewed as
computing the nearest neighbor of  to one of the above
two points using the  norm to define the
distance. Using standard \AVD algorithms we can subdivide each such cell 
 into  cells 
to answer this query approximately. By using this finer subdivision 
we can have a single point inside each cell for which the closest 
distance is the approximation to . This 
incurs an increase by a factor of  
in the number of cells.

\subsection{A generalization -- weighted version of  \ANN}
\seclab{main:weighted}

We consider a generalization of the -\ANN
problem.  Specifically, we are given a set of points , a weight  for each , and a number .  
Given a query  and weight , its
\emphi{-\NNTerm} distance to , is the 
minimum  such that the
closed ball  contains points of  of total
weight at least . Formally, the -\NNTerm distance for  is

where . A
\emphi{-approximate -\NNTerm distance} is a distance ,
such that  and a 
\emphi{-approximate -\NNTerm} is a point of 
that realizes such a distance. The 
\emphi{-\ANN problem} is to preprocess , 
such that a -approximate -\NNTerm
can be computed efficiently for any query point .

The -\ANN problem is the special case 
for all  and . Clearly, the function 
 is also a -Lipschitz function of its
argument. If we are given  at the time of preprocessing, 
it can be verified that the -Lipschitz property is enough to
guarantee correctness of
the \AVD construction for the -\ANN problem. However, we 
need to compute a  quorum clustering, where
now each quorum cluster has weight at least . A slight modification
of the algorithm in \lemref{q:clustering} allows this. Moreover, for the
preprocessing step which requires us to solve the -\ANN
problem for the representative points, one can use the algorithm of 
\secref{qtree:weighted}. We get the following result,
\begin{theorem}\thmlab{q:tree:main:w}Given a set of  weighted points  in , a number
     and  sufficiently small, one can preprocess
     in  time,
    where  and  and 
    . 
    The space used by the
    data-structure is .  This data
    structure answers a -\ANN query in  time.  The data-structure
    also returns a point of  that is a -approximation
    to the -nearest neighbor of the query point.
\end{theorem}


\section{Density estimation}
\seclab{applications}


Given a point set , and a query point , consider the point  .
This is a point in , and several problems in Computational
Geometry can be viewed as computing some interesting function of
. For example, one could view the nearest neighbor
distance as the function that returns the first coordinate of
. Another motivating example is a geometric version of
discrete density measures from Guibas \etal \cite{gmm-wkd-11}. In
their problem one is interested in computing . In this section, we show
that a broad class of functions (that include ), can be
approximated to within , by a data structure requiring
space .


\subsection{Performing point-location in several quadtrees simultaneously}

\begin{lemma}\lemlab{lowest:color}Consider a rooted tree  with  nodes, where the nodes are
    colored by  colors (a node might have several
    colors). Assume that there are  pairs of such
     associations. One can preprocess the
    tree in  time and space, such that given a query leaf  of
    , one can report the nodes  in 
    time. Here,  is the lowest node in the tree along the path
    from the root to  that is colored with color .
\end{lemma}


\begin{proof}
    We start with the naive solution -- perform a \DFS on , and
    keep an array  of  entries storing the latest node of
    each color encountered so far along the path from the root to the
    current node. Storing a snapshot of this array  at each
    node would require  space. But then one can answer a query
    in  time. As such, the challenge is to reduce the required
    space.
    
    To this end, interpret the \DFS to be a Eulerian traversal of the
    tree. The traversal has length , and every edge traveled
    contains updates to the array . Indeed, if the \DFS
    traverses down from a node  to a child node , the updates
    would be updating all the colors that are stored in , to
    indicate that  is the lowest node for these colors. Similarly,
    if the \DFS goes up from  to , we restore all the colors
    stored in  to their value just before the \DFS visited
    . Now, the \DFS traversal of  becomes a list of 
    updates. Each update is still an  operation.  This is
    however a technicality, and can be resolved as follows.  For each
    edge traveled we store the updates for all colors separately, each
    update being for a single color.  Also each update entry stores
    the current node, i.e. the destination of the edge traveled.  The
    total length of the update list is still , as follows from a
    simple charging argument, and the assumption about the number of
     pairs.  We simply charge each
    restore to its corresponding ``forward going'' update, and the
    number of forward going updates is exactly equal to the number of
     pairs.  For each leaf we store its
    last location in this list of updates.
    
    So, let  be this list of updates. At each \th update, for
     for some integer , store a snapshot of the array of
    colors as updated if we scan the list from the beginning till this
    point. Along with this we store the node at this point and 
    auxiliary information allowing us to compute the next update i.e.
    if the snapshot stored is between all updates at this node. 
    Clearly, all these snapshots can be computed in 
    time, and require  space.
    
    Now, given a query leaf , we go to its location in the list
    , and jump back to the last snapshot stored. We copy this
    snapshot, and then scan the list from the snapshot till the location
    for . This would require re-doing at most  updates, 
    and can be done in  time overall.
\end{proof}


\begin{lemma}\lemlab{sim:point:location}Given  compressed quadtrees  of total
    size  in , one can preprocess them in 
    time, using  space, such that given a query point ,
    one can perform point-location queries in all  quadtrees,
    simultaneously for , in  time.
\end{lemma}
\begin{proof}
    Overlay all these compressed quadtrees together. Overlaying  quadtrees 
    is equivalent to
    merging  sorted lists \cite{h-gaa-11} and can be done in  time. Let  denote the resulting compressed
    quadtree. Note that any node of , for ,
    must be a node in .
    
    Given a query point , we need to extract the  nodes in
    the original quadtrees , for , that contain
    the query point (these nodes can be compressed nodes).  So, let
     be the leaf node of  containing the query point
    .  Consider the path  from the root to the node 
    . We are interested in the lowest node of  that
    belongs to , for . To this end, color all
    the nodes of  that appear in , by color , for
    . Now, we build the data-structure of
    \lemref{lowest:color} for . We can use this data-structure to
    answer the desired query in  time.
\end{proof}
\subsection{Slowly growing functions}


\begin{figure*}[t]
    \centerline{
       \begin{tabular}{|l|l|}
           \hline
            & The class of slowly growing functions, 
           see \defref{slow}.\\\hline  &
           A function in  or a monotonic increasing function from
            to .\\\hline
            & \padbox{}\\\hline
 
           &
           \padbox{}
\\\hline  & \padbox{, is a coreset,}see \lemref{di-coreset}.\\\hline \padbox{} &  are associated 
           weights for coreset elements.\\\hline  &\padbox{}
           \\\hline
       \end{tabular}
    }
    \caption{Notations used.}
    \figlab{notn:table}
\end{figure*}

\begin{defn}
    \deflab{slow}A monotonic increasing function  is
    \emphi{slowly growing} if there is a constant , such
    that for  sufficiently small, we have , for all . The constant  is
    the \emphi{growth constant} of . The family of slowly growing
    functions is denoted by .
\end{defn}


Clearly,  includes polynomial functions,
but it does not include, for example, the function .  We assume
that given , one can evaluate the function  in constant time.
In this section, using the \AVD construction 
of \secref{sec:avd}, we show how to approximate any function  
that can be expressed as

where .  See \figref{notn:table} for a summary of the
notations used in this section.
\begin{lemma}\lemlab{easy-obs}Let  be a monotonic increasing function. Now,
    let . Then, for any
    query point , we have that , where
    . 
\end{lemma}

\begin{proof}
    The first inequality is obvious. As for the second inequality,
    observe that  is a monotonically
    increasing function of , and so is
    . We are dropping the smallest
     terms of the summation  that is
    made out of  terms. As such, the claim follows.
\end{proof}


The next lemma exploits a coreset construction, so that we have to
evaluate only few terms of the summation.


\begin{lemma}\lemlab{di-coreset}Let  be a monotonic increasing function.  There
    is a set of indices , and integer weights ,
    for , such that:\smallskip \begin{compactenum}[\quad(A)]
        \item .
        \item For any query point , we have that
         is a good estimate for
        ; that is,
        , where
       .
    \end{compactenum}
    \smallskip Furthermore, the set  can be computed in  time.
\end{lemma}

\begin{proof}
    Given a query point  consider the function  defined as . Clearly, since , it follows that  is a monotonic increasing
    function.  The existence of  follows from Lemma  in
    {Har-Peled}'s paper \cite{h-cdic-06}, as applied to -approximating the function ; that is,
    .
\end{proof}


\subsection{The data-structure}
We are given a set of  points , a function
, an integer  with , and  sufficiently
small. We describe how to build a data-structure to approximate
.
\subsubsection{Construction}
In the following, let , 
where  is the growth constant of  (see
\defref{slow}).  Consider the coreset  from
\lemref{di-coreset}.  For each  we compute, using
\thmref{ann:main}, a data-structure (i.e., a compressed quadtree)
 for answering -\ANN queries for . 
We then overlay all these
quadtrees into a single quadtree, using \lemref{sim:point:location}.


\paragraph{Answering a Query.}
Given a query point , perform a simultaneous point-location
query in , by using , as described in
\lemref{sim:point:location}.  This results in a 
approximation  to , for , and takes  time, where  is the size of
, and .  We return , where  is the weight associated
with the index  of the coreset of \lemref{di-coreset}.


\paragraph{Bounding the quality of approximation.}
We only prove the upper bound on . The proof for the lower
bound is similar.  As the  are 
approximations to  we have,
, for , and it follows from definitions that,

for .  Therefore,

Using \Eqref{eq:1} and \lemref{di-coreset} it follows that,

Finally, by \Eqref{eq:2} and \lemref{easy-obs} we have,

Therefore we have, , as desired.


\paragraph{Preprocessing space and time analysis.}
We have that .  Let
.  By \thmref{ann:main} the total
size of all the s (and thus the size of the resulting
data-structure) is

Indeed, the maximum of the terms involving  is
 and .  By
\thmref{ann:main} the total time taken to construct all the  is

where .  The time to construct
the final quadtree is , but this is subsumed by the
construction time above.


\subsubsection{The result}
Summarizing the above, we get the following result.
\begin{theorem}
    \thmlab{thm:appln:ddm}Let  be a set of  points in .  Given any slowly
    growing, monotonic increasing function 
    (i.e , see \defref{slow}), an integer  with 
    , and ,
    one can build a data-structure to approximate
    . Specifically, we have:
    \begin{compactenum}[\qquad\rm (A)]
        \item The construction time is ,
        where .
        \item The space used is , where .
        \item For any query point , the data-structure
        computes a number , such that , where
        .
        \item The query time is .
    \end{compactenum}
    (The  notation here hides constants that depend on .)
\end{theorem}

\section{\ANN queries where  and  are part of the query}
\seclab{qtree:algo}

Given a set  of  points in , we present a
data-structure for answering -\ANN queries, 
in time . Here  and  are not known
during the preprocessing stage, but are specified during query
time. In particular, different queries
can use different values of  and . Unlike our main result,
this data-structure requires linear space, and the amount of space used 
is independent of 
and . Previous data-structures required knowing  in
advance \cite{amm-sttas-05}.


\subsection{Rough approximation}


Observe that a fast constant approximation to
 is implied by \thmref{const:main} if 
is known in advance.  We describe a polynomial approximation when 
is not available during preprocessing.  We sketch the main ideas; our
argument closely follows the exposition in {Har-Peled}'s book
\cite{h-gaa-11}.

\begin{lemma}\lemlab{polyapprox}Given a set  of  points in , one can preprocess
    it, in  time, such that given any query point
     and  with , one can find, in  time, 
    a number 
    satisfying . The result
    is correct with high probability i.e. at least , 
    where  is an arbitrary constant.
\end{lemma}

\begin{proof}
    By an appropriate scaling and translation ensure that . Consider a compressed quadtree
    decomposition  of  for ,
    whose shift  is a random vector in . By a 
    bottom-up traversal, compute, for each
    node  of , the axis parallel bounding box  of
    the subset of  stored in its subtree, and the number
    of those points. 
    
    Given a query point , locate the
    lowest node  of  whose region contains
     (this takes  time, see \cite{h-gaa-11}). By
    performing a binary search on the root to  path 
    locate the lowest node  whose subtree contains
     or more points from .  The algorithm returns
    , the distance of the query point to the furthest point
    of , as the approximate distance.
    
    To see that the quality of approximation is as claimed, 
    consider the ball
     centered at  with radius . Next,
    consider the smallest canonical grid having side length  (thus, ).  Randomly
    translating this grid, we have with probability , that the ball  is contained inside a
    canonical cell  of this grid. This implies that
    the diameter of  is bounded by ,
    Indeed, if the cell of  is contained in ,
    then this clearly holds. Otherwise, if  is contained in
    the cell , then  must be a
    compressed node, the inner portion of its cell is contained in
    , and the outer portion of the cell can not contain any
    point of . As such, the claim holds.
    
    Moreover, for the returned distance , we have that
    
    \aftermathA
\end{proof}

An alternative to the argument used in \lemref{polyapprox}, is to use
two shifted quadtrees, and return the smaller distance returned by the
two trees. It is not hard to argue that in expectation the returned
distance is an -approximation to the desired distance (which then
implies the desired result via Markov's inequality). One can also
derandomize the shifted quadtrees and use  quadtrees
instead \cite{h-gaa-11}.

We next show how to refine this approximation.

\begin{lemma}\lemlab{app:refine}Given a set  of  points in , one can preprocess
    it in  time, so that given a query point
    , one can output a number 
    satisfying, , in  time. Furthermore, one can return a point  such that .
\end{lemma}

\begin{proof}
    Assume that .  The algorithm of \lemref{polyapprox} returns the
    distance  between  and some point of ;
    as such we have,
    .  We start with a compressed quadtree
    for  having  as the root. We look at
    the set of canonical cells  with side length at least
    , that intersect the ball .
    Clearly, the \th nearest neighbor of  lies
    in this set of cubes. The set  can be computed in  time using cell queries \cite{h-gaa-11}.
    
    
    For each node  in the compressed quadtree there is a
    \emphi{level} associated with it. This is . The root has level  and it
    decreases as we go down the compressed quadtree. Intuitively,
     is the depth of the node if it was a node in a regular
    quadtree.
    
    We maintain a queue of such canonical grid cells. Each step in the
    search consists of replacing cells in the current level with their
    children in the quadtree, and deciding if we want to descend
    a level. In the \th iteration, we replace every node of
     by its children in the next level, and put them into the
    set .
    
    We then update our estimate of
    . Initially, we set . For every node , we
    compute the closest and furthest point of its cube (that is the
    cell of this node) from the query point (this can be done in
     time). This specifies a collection of intervals  one
    for each node .  Let  denote the number of points
    stored in the subtree of . For a real number , let  denote the total number of points in the intervals,
    that are to the left of , contains , and are to the right of
    , respectively. Using median selection, one can compute in
    linear time (in the number of nodes of ) the minimum  such
    that . Let this value be . Similarly, in
    linear time, compute the minimum  such that , and let this value be . Clearly, the desired distance
    is in the interval .
    
    The algorithm now iterates over . If  is strictly
    to the left of ,  is discarded
    (it is too close to the query and can not contain the \th
    nearest neighbor), setting . Similarly, if
     is to the right of  it can be thrown away. The algorithm 
    then moves to the next iteration.
    
    The algorithm stops as soon as the diameter of all the cells of
     is smaller than .  A
    representative point is chosen from each node of  (each node of the
    quadtree has an arbitrary representative point precomputed for it out
    of the subset of points stored in its subtree), and 
    the furthest such point is returned as the - 
    approximate  nearest
    neighbor. To see that the returned answer is indeed correct,
    observe that 
    and , which implies the
    claim. The distance of the returned point from  is
    in the interval , where  and . This interval
    also contains . As such,  is
    indeed the required approximation.
    
    Since we are working with compressed quadtrees, a child node might
    be many levels below the level of its parent. In particular, if a
    node's level is below the current level, we freeze it and just
    move it on the set of the next level. We replace it by its
    children only when its level has been reached.
    
    The running time is clearly . Let
     be the diameter of the cells in the level being handled
    in the \th iteration. Clearly, we have that . All the cells of  that survive must intersect the
    ring with inner and outer radii  and  respectively, 
    around . By a
    simple packing argument, . As long as , we have that , as . This clearly holds for the
    first  iterations. It can be verified that once
    this no longer holds, the algorithm performs at most  additional iterations, as then
     and the
    algorithm stops. Clearly, the s in this
    range can grow exponentially, but the last one is
    . This implies that , as desired.
\end{proof}


\subsection{The result}

\begin{theorem}\thmlab{q:tree:main}Given a set  of  points in , one can preprocess
    them in  time, into a data structure of size
    , such that given a query point , an integer 
    with  and  one can compute, in  time, a number  such that
    . The data-structure also
    returns a point  such that
    .
\end{theorem}

\subsection{Weighted version of -\ANN}
\seclab{qtree:weighted}

We now consider the weighted version of the -\ANN problem
as defined in \secref{main:weighted}. Knowledge of the threshold weight 
 is not required at the time of preprocessing.
By a straightforward adaptation of the arguments in this section we get
the following.
\begin{theorem}\thmlab{q:tree:main:w:2}Given a set  of  weighted points in  one can
    preprocess them, in  time, into a data structure
    of size , such that one can efficiently answer
    -\ANN queries.
    Here a query  is made out of
    \begin{inparaenum}[(i)]
        \item a query point , 
        \item a weight , and
        \item an approximation parameter .
    \end{inparaenum}
    Specifically, for such a query, one can compute, in  time, a number  such that
    . The data-structure also
    returns a point  such that
    .
\end{theorem}


\section{Density and distance estimation via sampling}
\seclab{sampling}


In this section, we investigate the ability to approximate density
functions using sampling. Note, that sampling can not handle our basic
proximity result (\thmref{ann:main}), since sampling is indifferent to
geometric error. Nevertheless, one can get meaningful results, that
are complementary to our main result, giving another intuition why it
is possible to have sublinear space when approximating the -\NNTerm
and related density quantities.


\subsection{Answering -\ANN}

\subsubsection{Relative approximation}

We are given a \emphi{range space} , where  is a set of
 objects and  is a collection of subsets of , called
\emphi{ranges}. In a typical geometric setting,  is a subset of
some infinite ground set  (e.g.,  and
 is a finite point set in ), and , where  is a
collection of subsets (i.e., \emphi{ranges}) of  of some
simple shape, such as halfspaces, simplices, balls, etc.

The \emphi{measure} of a range , is , and its \emphi{estimate} by a
subset  is .  We are interested in range spaces that have
bounded \VC dimension, see \cite{h-gaa-11}. More specifically, we are
interested in an extension of the classical -net and
-approximation concepts.

\begin{defn}
    For given parameters , a subset 
    is a \emphi{relative -approximation} for 
    if, for each , we have
    \begin{compactenum}[\quad(i)]
        \item , if .
        
        \item , if
        .
    \end{compactenum}
\end{defn}

\begin{lemma}[\cite{hs-rag-11, h-gaa-11}]\lemlab{relative}For a range space with \VC dimension , a random sample of
    size , is a
    relative -approximation with probability .
\end{lemma}

\subsubsection{Sampling the -\ANN}

So, let  be a set of  points in ,  and , be prespecified parameters. The range space of balls in
 has \VC dimension , as follows by a standard lifting
argument, and Radon's theorem \cite{h-gaa-11}. Set , and
compute a random sample  of size

This sample is a relative -approximation
with probability , and assume that this indeed
holds.

\paragraph{Answering a -\ANN query.}
Given a query point , let  be its
-\NNTerm in , where .  Return  as the
desired -\ANN.


\paragraph{Analysis.}
Let , and consider the ball .  We have that

If , then by the relative approximation
definition, we have that . But this implies that
, which is a contradiction.

As such, we have that . Again, by the
relative approximation definition, we have that , and this in turn implies that


as .

\paragraph{The result.} 
Of course, there is no reason to compute the exact -\NNTerm in
. Instead, one can compute the -\ANN in
 to the query. In particular, using the data-structure of
\thmref{q:tree:main}, we get the following.

\begin{lemma}\lemlab{random:sample:k:ann}Given a set  of  points in , and parameters 
    , and . Consider a random sample
     from  of size . One can build a data-structure in
     time, using  space, such that for any query
    point , one can compute a -\ANN in , by answering -\NNTerm or
    -\ANN query on , where .

    Specifically, the query time is , and
    the result is correct for all query points with probability ; that is, for the returned point , we have
    that .
\end{lemma}  

\begin{remark}
    \begin{inparaenum}[(A)]
        \item If one plugs the random sample into \thmref{ann:main},
        then one gets a data-structure of size , that can answer -\ANN in logarithmic time.

        \item Once computed, the data-structure of
        \lemref{random:sample:k:ann} works for approximating any
        -\ANN, for any , by
        computing the -\ANN on , where .
    \end{inparaenum}
\end{remark}


\subsection{Density estimation via sampling}

\subsubsection{Settings}
\seclab{settings:density}

Let  be a set of  points in , and let  be a
parameter.  In the following, for a point , let 
be the set of  points closest to  in .  For such a
query point , we are interested in estimating the
quantity

Since we care only about approximation, it is sufficient to
approximate the function without the square root. Formally, a
-approximation  to
, yields the approximation  to
, and this is a -approximation to the
original quantity, see \cite[Lemma 4.6]{ahv-aemp-04}. Furthermore, as
in \defrefpage{slow}, we can handle more general functions than
squared distances. However, since we are interested in random
sampling, we have to assume something additional about the
distribution of points.

\begin{defn}\deflab{well:behaved}For a point-set , and a parameter ,
    the function  is a \emphi{well-behaved}
    distance function, if \smallskip \begin{compactenum}[\quad(i)]
        \item  is monotonically increasing, and
        \item for any point , there exists a
        constant , such that .
    \end{compactenum}
    \smallskip A set  of functions is \emphi{well-behaved} if the above
    holds for any function in  (with the same constant
     for all the functions in ).
\end{defn}

As such, the target here is to approximate

where  is a well-behaved distance function. 

\subsubsection{The estimation algorithm}

Let  be a random sample from  of size , where
, and  is a prespecified confidence
parameter. Given a query , compute the quantity

where . Return this as the required estimate to
, see \Eqref{halving}.


\subsubsection{Analysis}

We claim that this estimate is good, with good probability for all
query points.  Fix a query point , and let  be the prespecified approximation parameter.  For the sake of
simplicity of exposition, we assume that
 -- this can be
achieved by dividing  by the right constant, and applying
our analysis to this modified function. In particular,
, for all . For any , let

Consider a value . The sublevel set of all points ,
such that , is the union of
\begin{inparaenum}[(i)]
    \item a ball centered at , with 
    \item a complement of a ball (also centered at  of radius
    .
\end{inparaenum}
(i.e., its the complement of a ring.)  This follows as  is a
monotonically increasing function. As such, consider the family of
functions

This family has bounded pseudo-dimension (a fancy way to say that the
sublevel sets of the functions in this family have finite \VC
dimension), which is  in this case, as every range is the union
of a ball and a ball complement \cite[Section 5.2.1.1]{h-gaa-11}.
Now, we can rewrite the quantity of interest as

where  (here  is a function of
).  Note, that by our normalization of , we have that , for any . We
are now ready to deploy a sampling argument. We need a generalization
of -approximation due to Li \etal \cite{lls-ibscl-01}, see also
\cite{h-gaa-11}.

\begin{theorem}[\cite{lls-ibscl-01}]
    \thmlab{SAMPLING}Let  be parameters, let  be a range space, and let  be
    a set of functions from  to , such that
    the pseudo-dimension of  is .  For a random
    sample \index{random sample}  (with repetition) from
     of size , we have
    that
    
    with probability .
\end{theorem}

Lets try to translate this into human language. In our case,
. For the following argument, we fix the query
point , and the distance .  The measure
function is

which is the desired quantity if one set , and
 -- see \Eqref{F:q}.  For the
sample , the estimate is

where .  Now, by the normalization of ,
we have that  and
.  The somewhat mysterious distance function, in
the above theorem, is

Setting 

the condition in the theorem is

as an easy but tedious calculation shows.  This is more or less the
desired approximation, except that we do not have  at
hand. Conceptually, the algorithm first estimates , from the
sample, see \Eqref{estimate}, by computing the \th nearest
neighbor to the query in , and then computes the estimate
using this radius. Formally, let ,
and observe that as , we have  

In particular, the error between the algorithm estimate, and theorem
estimate is 


Now, by \lemref{relative},  is a relative
-approximation, with probability , where  is a sufficiently large constant
(its exact value would follow from our analysis). This implies that
the ball centered at  of radius , contains between
 points of . This in
turn implies that number of points of  in the ball of radius
 centered at  is in the range
. This in turn
implies that the number of ``heavy'' points in the sample 
is relatively small. Specifically, the number of points in 
that are in the ball of radius  around , but not in the
concentric ball of radius  (or vice versa) is

By the well-behaveness of , this implies that the contribution of
these points is marginal compared to the ``majority'' of points in
; that is, all the points in  that are the \th
nearest-neighbor to , for , have
weight at least , where  is the maximum value
of  on any point of
. That is, we have

Similarly,  we have 

if .  We thus have that\vspace{-0.5cm}
by \Eqref{amazing}.  That is, the returned approximation has small
error.

The above analysis assumed both that the sample  is a
relative -\si{approxima}\-\si{tion} (for
balls), and also complies with \thmref{SAMPLING}, for the range space,
where the ranges are a complement of a single ring, for the parameters
set in \Eqref{parameter:setting}. Clearly, both things hold with
probability , for the size of the sample taken by the
algorithm. Significantly, this holds for all query points.

\subsubsection{The result}

\begin{theorem}
    Let  be a set of  points in , ,  and  be parameters. Furthermore, assume that we
    are given a well-behaved function  (see
    \defrefpage{well:behaved}). Let  be a random sample of
     of size . Then, with probability , for all query points , we have
    that for the quantity
    
    we have that ,
    where .  Here, 
    denotes the set of  nearest-neighbor to  in
    .
\end{theorem}

The above theorem implies that one can get a 
multiplicative approximation to the function , for all
possible query points, using  space. Furthermore, the above
theorem implies that any reasonable density estimation for a point-set
that has no big gaps, can be done using a sublinear sample size; that
is, a sample of size roughly , which is (surprisingly)
polynomial in the dimension. This result is weaker than
\thmref{thm:appln:ddm}, as far as the family of functions it handle,
but it has the advantage of being of linear size (!) in the dimension.
This compares favorably with the recent result of \Merigot{}
\cite{m-lbfkd-13}, that shows an exponential lower bound
 on the complexity of such an
approximation for a specific such distance function, when the
representation used is (essentially) additive weighted Voronoi diagram
(for ). More precisely, the function \Merigot{} studies has the
form of \Eqrefpage{estimate:sq}. However, as pointed out in
\secref{settings:density}, up to squaring the sample size, our result
holds also in this case.



\section{Conclusions}
\seclab{conclusions}

In this paper, we presented a data-structure for answering
-\ANN queries in  where  is a constant. Our
data-structure has the surprising property that the space required is
. One can verify that up to noise this is the best one
can do for this problem. This data-structure also suggests a natural
way of compressing geometric data, such that the resulting sketch can
be used to answer meaningful proximity queries on the original
data. We then used this data-structure to answer various proximity
queries using roughly the same space and query time.  We also
presented a data-structure for answering -\ANN queries
where both  and  are specified during query time. This
data-structure is simple and practical. Finally, we investigated what
type of density functions can be estimated reliably using random
sampling.

There are many interesting questions for further research.
\begin{compactenum}[\qquad (A)]
    \item In the vein of the authors recent work \cite{hk-annsl-11},
    one can verify that our results extends in a natural way
    to metrics of low doubling dimensions (\cite{hk-annsl-11}
    describes what an approximate Voronoi diagram is for doubling
    metrics). It also seems believable that the result would extend to
    the problem where the data is high dimensional but the queries
    arrive from a low dimensional manifold.
    
    \item It is natural to ask what one can do for this problem in
    high dimensional Euclidean space. In particular, can one get query
    time close to the one required for approximate nearest neighbor
    \cite{im-anntr-98, him-anntr-12}. Of particular interest is getting a query time
    that is sublinear in  and  while having subquadratic space
    and preprocessing time.
    
    \item The dependency on  in our data-structures may
    not be optimal. One can probably get space/time tradeoffs, as done by
    Arya \etal \cite{amm-sttan-09}.
\end{compactenum}


\subsection*{Acknowledgments.}

The authors thank Pankaj Agarwal and Kasturi Varadarajan for useful
discussions on the problems studied in this paper.






\bibliographystyle{alpha}\bibliography{shortcuts,geometry}

\end{document}
