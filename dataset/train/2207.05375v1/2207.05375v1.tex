\section{Experiments}
\label{sec:experiments}

\subsection{Implementation details}
The spatial-temporal layer consists of 3 dilated convolution layers with a dilation rate of 1, 2, and 5, respectively. We also adopt two separate transformers~\cite{dosovitskiy2020image} to model long-term spatial and temporal information. The decoder of the 2D branch only contains a LayerNorm~\cite{ba2016layer} and a linear layer, thus the features encoded by the prior can represent a full motion. The lifting network is also a transformer. We rely on PyTorch~\cite{paszke2019pytorch} to implement the model and use AdamW~\cite{loshchilov2017decoupled} optimizer with a learning rate of 1e-4 for training. The batchsize of all experiments is 32. The model is trained on a single NVIDIA RTX 3090 GPU with 24GB memory for 45 epochs. We use a joint regressor in LSP format~\cite{johnson2010clustered} to obtain 3D joints and calculate errors for the predicted mesh.

\begin{table}
    \caption{Quantitative comparison with state-of-the-art methods. Our method obtains good results and achieves the best performance in some metrics on occluded datasets.  means the image-based method.  denotes the method that explicitly considers the occlusion problem. Total Params is the total number of model parameters including 2D detection~\cite{cao2017realtime} or feature extraction~\cite{kolotouros2019learning}.}
    \label{tab:experiment}
    \vspace{-3mm}
    \begin{center}
        \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{l|c|c c c|c c c| c c c}
            \noalign{\hrule height 1.5pt}
            \begin{tabular}[l]{l}\multirow{2}{*}{Method}\end{tabular} &\begin{tabular}[l]{l}\multirow{2}{*}{\begin{tabular}[1]{c}Total\\Params\end{tabular}}\end{tabular} &\multicolumn{3}{c|}{OcMotion} &\multicolumn{3}{c|}{3DPW}  &\multicolumn{3}{c}{3DPW-OC}\\
            & &MPJPE &PA-MPJPE &Accel. &MPJPE &PA-MPJPE &PVE &MPJPE &PA-MPJPE &Accel.\\
            \noalign{\hrule height 1pt}
            HMMR~\cite{kanazawa2019learning}    &29.8M  &-- &-- &-- &116.5 &72.6 &139.3 &-- &-- &--   \\
            SPIN~\cite{kolotouros2019learning}    &27.0M   &88.2 &56.7 &47.0 &96.9 &59.2 &116.4 &105.0 &71.3 &44.6   \\   
            OCHMR~\cite{khirodkar2022occluded}  &-- &--  &-- &-- &89.7 &58.3  &107.1  &-- &-- &--\\
            LASOR~\cite{yang2022lasor} &-- &-- &-- &-- &-- &57.9 &-- &-- &-- &--  \\
            VIBE~\cite{kocabas2020vibe}         &48.3M  &89.6 &58.6 &44.5 &93.5 &56.5 &113.4 &98.3 &69.7 &39.0   \\ 
            TCMR~\cite{choi2021beyond}     &108.9M  &95.8  &62.6  &24.3  &95.0 &55.8 &111.5 &90.3 &63.0 &\textbf{8.0}   \\
            OOH~\cite{zhang2020object} &33.0M  &83.0 &55.0 &48.6 &86.7 &55.2 &105.2 &90.4 &57.0 &45.3 \\
            MEVA~\cite{luo20203d}     &92.0M &88.8 &59.9 &29.0 &86.9 &54.7 &-- &91.4 &63.5 &17.8   \\
            ROMP~\cite{sun2021monocular}  &29.0M &79.4  &48.1 &57.2 &85.5  &53.3  &103.1 &--  &66.5  &--\\
            PARE~\cite{kocabas2021pare}   &32.9M   &81.1 &52.0 &43.6 &\textbf{82.9} &52.3 &\textbf{99.7} &90.5 &56.6 &40.9 \\
            Wan~\etal~\cite{wan2021encoder}     &-- &-- &-- &-- &88.8 &50.7 &104.5 &-- &-- &--   \\ 
            Chen~\etal~\cite{chen2021self}     &51.4M &-- &-- &-- &85.8 &\textbf{50.4} &100.6 &-- &-- &--   \\ 
            \textbf{Ours}~(w/o OcMotion) &56.3M &72.1 &44.9 &24.2 &83.7 &51.8 &110.4 &90.1 &54.5 &16.6 \\
            \textbf{Ours} &56.3M &\textbf{58.3} &\textbf{36.1} &\textbf{23.2} &83.7 &51.7 &110.1 &\textbf{89.4} &\textbf{53.4} &16.6 \\
            \noalign{\hrule height 1.5pt} 
            \end{tabular}
        }
\end{center}
\vspace{-10mm}
\end{table}

\noindent\textbf{Metrics}. We adopt the metrics in previous works~\cite{kocabas2020vibe,luo20203d,choi2021beyond} to evaluate our method. The Mean Per Joint Position Error (MPJPE) and the MPJPE after rigid alignment of the prediction with ground truth using Procrustes Analysis (PA-MPJPE) are used for measuring joint positions. The Per Vertex Error (PVE) and acceleration error (Accel.) are applied to evaluate mesh quality and motion smoothness.

\subsection{Dataset}
We follow previous works~\cite{kocabas2020vibe,choi2021beyond} to set the training data for a fair comparison. The spatial-temporal prior is trained on 2D~(PoseTrack~\cite{andriluka2018posetrack}, InstaVariety~\cite{kanazawa2019learning} and PennAction~\cite{zhang2013actemes}) and 3D human motion datasets~(Human3.6M~\cite{ionescu2013human3} and MPI-INF-3DHP~\cite{mehta2017monocular}). We use Human3.6M and MPI-INF-3DHP to train the lifting network. The method is evaluated on 3DPW~\cite{von2018recovering}, Human3.6M, and MPI-INF-3DHP. In addition, we use 3DPW-OC, the occluded sequences from the entire dataset selected by~\cite{zhang2020object}, to demonstrate the effectiveness of our approach in occluded cases. We also report the results with and without OcMotion training. The sequences \textit{0013, 0015, 0017, 0019} in OcMotion with 6 views are used for testing, and the rest are adopted for training.


\begin{figure*}
    \begin{center}
    \includegraphics[width=1\linewidth]{Figures/qualitative_results.pdf}
    \end{center}
    \vspace{-6mm}
    \caption{Our method achieves good performance in both occluded and non-occluded cases.}
    \label{fig:quali_results}
    \vspace{-7mm}
\end{figure*}


\subsection{Comparison to state-of-the-art results}
We first compared our method to state-of-the-art approaches on OcMotion dataset. To the best of our knowledge, OcMotion is the first video dataset designed for the object-occluded human mesh recovery task. We conducted experiments on this dataset to demonstrate the superiority of our method in occluded cases. As shown in~\tabref{tab:experiment}, since previous methods do not explicitly consider the occlusion problem, our method significantly outperforms previous video-based methods on all metrics. In addition, PARE~\cite{kocabas2021pare} and OOH~\cite{zhang2020object} are image-based methods and are designed for the occluded human reconstruction task. Benefited from the spatial-temporal information, our method can obtain more robust results. Moreover, OOH~\cite{zhang2020object} uses UV map representation, though it can explicitly describe an occluded human, the resampled meshes show a lot of artifacts~\figref{fig:results}~(e). Furthermore, we found that video-based methods show more motion jitters and have higher acceleration errors in occluded cases. In contrast, the spatial-temporal prior obtains features of a full motion and avoids the ambiguities induced by occlusions. With the prior, our method achieves the best performance in terms of acceleration error and produces more temporally coherent results~\figref{fig:qualitative_video} on the occlusion dataset. 


\begin{figure*}
    \vspace{-4mm}
    \begin{center}
    \includegraphics[width=1\linewidth]{Figures/results.pdf}
    \end{center}
    \vspace{-8mm}
    \caption{Qualitative comparison among the methods that utilize temporal information~(b, c, d) and explicitly consider the occlusion problem~(e, f). Our method is more robust to occlusions than other methods.}
    \label{fig:results}
    \vspace{-8mm}
\end{figure*}


Another strength of our method is the good generalization ability. Since we represent the human motion in a 2D map, the background of the image cannot affect the model performance, thus the trained model is insensitive to environmental changes. Besides, the self-supervised training on a large amount of synthetic data makes the model robust to various occlusions. We conduct some experiments on 3DPW dataset to demonstrate the advantages. Our model is trained on indoor 3D datasets but can also achieve satisfactory performance in outdoor scenarios. Specifically, \cite{wan2021encoder} and \cite{chen2021self} also develop attention modules to exploit temporal cues and produce the best results. The results in~\tabref{tab:experiment} demonstrate that our method can also obtain similar results as state-of-the-arts on the non-occluded dataset. To further show the performance in more challenging occluded environments, we follow~\cite{zhang2020object} to evaluate the method on 3DPW-OC dataset, which is a subset of 3DPW that contains occlusions. We have the same training data as VIBE, MEVA, and TCMR. Although VIBE, MEVA, and TCMR achieve excellent results on 3DPW, they are sensitive to the occlusion~\figref{fig:results}~(b,c,d), while our method is robust. Moreover, TCMR relies on the past and future features to regress SMPL parameters for the current frame, and it may produce over-smoothed motion~(refer to supplemental video). With the spatial-temporal prior, our method can obtain coherent and highly dynamic motions.


We conduct experiments on Human3.6M dataset to further demonstrate the effectiveness of our method. As shown in~\tabref{tab:h36m}, our method achieves state-of-the-art in protocol 1 of Human3.6M in terms of MPJPE, which outperforms VIBE by 4.1mm. In addition, in~\tabref{tab:experiment} and~\tabref{tab:h36m}, LASOR~\cite{yang2022lasor} and Pose2Mesh~\cite{choi2020pose2mesh} also recover human mesh from 2D detections, and our method can obtain more accurate results.

We also report the number of model parameters to show the efficiency of our approach. The results in~\tabref{tab:experiment} show the total parameters including 2D pose detection~\cite{cao2017realtime} and feature extraction~\cite{kolotouros2019learning} of different methods. We have fewer parameters than TCMR and MEVA. Specifically, the lifting network in our method contains 19.2M parameters. The 2D detection consumes most of the computations, which can be replaced with more compact models. To further demonstrate the runtime efficiency, we report the model parameters without the 2D detection and feature extraction among the two-stage methods in~\tabref{tab:h36m}. Our model can achieve the best performance with the fewest parameters. For current implementation with~\cite{cao2017realtime}, the inference FPS is 32 on a single NVIDIA RTX 3090 GPU, which is acceptable for real-time applications. Furthermore, when regressing from the 2D pose inputs, our model runs at 286 FPS, which is significantly faster than common 2D pose detectors. The inference speed will not be the bottleneck for real-world implementation.

\begin{table}
    \vspace{-5mm}
    \caption{We conduct quantitative comparisons on Human3.6M following the protocols defined in~\cite{kanazawa2018end}. In the column of \#Params, we also compare the number of model parameters without 2D detection and feature extraction among two-stage methods. MACs is the estimated multiplyâ€“accumulate operations. Our method achieves competitive performance as state-of-the-art methods with higher runtime efficiency.}
    \label{tab:h36m}
    \vspace{-2mm}
    \begin{center}
        \resizebox{0.65\linewidth}{!}{
            \begin{tabular}{l|c|c|c c|c c c}
            \noalign{\hrule height 1.5pt}
            \begin{tabular}[l]{l}\multirow{2}{*}{Method}\end{tabular} &\begin{tabular}[l]{l}\multirow{2}{*}{\#Params}\end{tabular} &\begin{tabular}[l]{l}\multirow{2}{*}{MACs}\end{tabular} &\multicolumn{2}{c|}{Protocol 1} &\multicolumn{3}{c}{Protocol 2}\\
            & & &MPJPE &PA-MPJPE &MPJPE &PA-MPJPE &Accel.\\
            \noalign{\hrule height 1pt}
            HMMR~\cite{kanazawa2019learning}    &-- &--  &-- &-- &-- &56.9 &--\\
            MEVA~\cite{luo20203d}    &65.0M &1.31G   &73.4 &51.9 &76.0 &53.2  &15.3\\
            PARE~\cite{kocabas2021pare}  &-- &--  &78.5 &55.1   &71.6 &49.9 &32.3         \\
            Pose2Mesh~\cite{choi2020pose2mesh}   &76.3M &3.81G   &-- &--   &64.9 &47.0 &--         \\
            DSD-SATN~\cite{sun2019human}  &-- &--  &--  &--  &59.1  &42.4 &--\\
            VIBE~\cite{kocabas2020vibe}   &21.3M &0.45G   &68.8 &49.5   &65.9 &41.5 &27.3  \\
            OOH~\cite{zhang2020object} &-- &--  &74.7 &53.3  &61.8 &41.2 &35.3  \\
            TCMR~\cite{choi2021beyond}     &81.9M  &1.29G   &79.8 &56.8  &62.3  &41.1 &\textbf{5.3} \\
            Chen~\etal~\cite{chen2021self}    &-- &--  &-- &-- &58.9 &\textbf{38.7} &--\\
            Wan~\etal~\cite{wan2021encoder}     &-- &--  &-- &-- &\textbf{56.3} &\textbf{38.7} &--\\
            \textbf{Ours}~(w/o OcMotion) &\textbf{19.2M} &0.49G    &\textbf{64.7} &\textbf{46.3} &59.7 &40.1 &13.0 \\
            \noalign{\hrule height 1.5pt} 
            \end{tabular}
        }
\end{center}
\vspace{-10mm}
\end{table}


\subsection{Ablation}


\noindent\textbf{Self-supervised prior.}
We ablate the self-supervised spatial-temporal prior to reveal the properties of this module in occluded human motion capture. We found that the occlusion token can promote the prior to learn better motion representation in self-supervised learning. In \tabref{tab:Ablation}, we remove the token and use constant value 0 to represent the occluded parts as~\cite{zhang2020object}. Using the prior learned with the constant value even produces a worse performance. The experimental results demonstrate the importance of the occlusion token. We then compared the model with and without the prior. The two strategies also use the occlusion augmentation technique~\cite{sarandi2018robust}. The results show that the prior significantly improves the joint and vertex accuracy. Directly recovering 3D human motion from occluded 2D poses without the prior is a highly ill-posed problem, and the same occluded 2D pose can map to various 3D poses. However, with the self-supervised training on many 2D motions and synthetic occlusions, the model learns the prior knowledge of converting an occluded motion to a highly dynamic full motion. The results show that with the assistance of the learned prior, the occluded human motion capture can obtain more accurate and coherent motions.

\begin{table}
    \vspace{-6mm}
    \caption{Ablation studies on different key components on OcMotion dataset. The spatial-temporal layer~(ST layer) and the self-supervised prior improve the motion capture in both joint accuracy and motion smoothness in occluded cases. + denotes adding the corresponding module on the temporal model.}
    \label{tab:Ablation}
    \vspace{-3mm}
    \begin{center}
        \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{l|c|c|c c c c}
            \noalign{\hrule height 1.5pt}
            Method &\#Params &MACs &MPJPE&PA-MPJPE &PVE &Accel.\\
            \noalign{\hrule height 1pt}
            temporal &19.1M &0.46G &70.5 &44.8 &79.6 &40.6 \\
            + smoothness loss &19.1M &0.46G &72.6  &45.4   &80.1   &27.4  \\
            + spatial &19.1M &0.47G &65.4  &41.1   &76.4   &40.4  \\
            + ST layer &19.2M &0.49G &63.6 &39.8 &73.7 &38.6  \\
            + ST layer + self-supervised prior (w/o occlusion token) &19.2M &0.49G &66.7 &43.2 &78.9 &37.1   \\
            + ST layer + self-supervised prior &19.2M &0.49G &58.2 &36.1 &67.5 &36.4   \\
            + ST layer + self-supervised prior + smoothness loss &19.2M &0.49G &58.3 &36.1 &67.1 &23.2  \\
            + ST layer + self-supervised prior + smoothness loss + gt 2D pose &19.2M &0.49G   &40.5 &23.8 &45.5 &16.3  \\
            \noalign{\hrule height 1.5pt} 
            \end{tabular}
        }
\end{center}
\vspace{-10mm}
\end{table}

\noindent\textbf{Spatial-temporal layer.}
The joint-level spatial-temporal information is essential for the occlusion problem. We first compared the temporal model without the spatial relations in~\tabref{tab:Ablation}. In addition, VIBE, TCMR, and MEVA also rely on temporal information and do not consider the kinematic features. The two comparisons in~\tabref{tab:Ablation} and~\tabref{tab:experiment} turn out the same conclusion that the model which only focuses on the temporal relation cannot achieve good results in the occluded cases. We then added a separate transformer module like~\cite{zheng20213d} in the temporal model to exploit the kinematic relation to assist the motion capture. Since the model cannot consider joint-level spatial-temporal correlation in the same stage, the performance is inferior to the spatial-temporal layer. 

\begin{figure*}
    \vspace{-6mm}
    \begin{center}
    \includegraphics[width=1\linewidth]{Figures/qualitative_video.pdf}
    \end{center}
    \vspace{-8mm}
    \caption{Qualitative results on consecutive frames in occlusion scenario. More results can refer to our supplemental video.}
    \label{fig:qualitative_video}
    \vspace{-8mm}
\end{figure*}



\noindent\textbf{Sensitivity to occlusions.}
We analyzed the sensitivity to the occlusion of our method to further demonstrate the effectiveness of the proposed components by synthesizing additional occlusions on OcMotion dataset. As shown in~\figref{fig:curve}, we first synthesize occluded images with different occlusion ratios to simulate severely occluded cases. Since the OcMotion dataset contains a lot of real occlusions, we synthesize realistic occlusions among the occlusion ratios from 0\% to 50\%. We use OpenPose~\cite{cao2017realtime} to detect visible 2D poses and conduct evaluations of different models on the synthetic data. The results show that the temporal approach without joint-level spatial-temporal correlation is sensitive to the occlusions, while the model with self-supervised prior is robust. With the proposed modules, our method is insensitive to various occlusions.

\begin{figure}
    \vspace{-4mm}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{Figures/curve.pdf}
    \end{center}
    \vspace{-8mm}
    \caption{We synthesize additional realistic occlusions on OcMotion with different occlusion ratios. The curve on different occlusion data shows that our method is more robust to variant occlusion proportions and different occlusion types.}
    \label{fig:curve}
    \vspace{-6mm}
\end{figure}




