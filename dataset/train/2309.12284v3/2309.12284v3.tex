\documentclass{article} 
\usepackage[numbers,sort&compress,square]{natbib}
\usepackage{conference, times}
\usepackage{tablefootnote}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{linkcolor=[rgb]{0.7,0.1,0.1}}
\hypersetup{citecolor=[rgb]{0.4,0.15,0.95}}
\newcommand{\wy}[1]{\textcolor{cyan}{[Weiyang: #1]}}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{adjustbox}

\renewcommand{\captionlabelfont}{\footnotesize}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.94}
\usepackage{pifont}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage{colortbl}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{colortbl}
\usepackage{nicematrix}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{multirow}

\usepackage{amsthm}
\usepackage{amsmath,amsfonts,bm}
\usepackage{nicefrac}
\usepackage{array}
\usepackage{wrapfig}

\usepackage{hyperref}
\usepackage{url}
\usepackage{pbox}





\definecolor{bluex}{rgb}{0.27, 0.42, 0.81}
\definecolor{purplex}{HTML}{9564bf}
\definecolor{red3}{HTML}{C52A20}
\definecolor{red2}{HTML}{B36A6F}
\definecolor{red1}{HTML}{FFb5b5}
\definecolor{purple}{HTML}{B36A6F}
\definecolor{darkyellow}{HTML}{D5BA82}
\definecolor{blue1}{HTML}{508AB2}
\definecolor{blue2}{HTML}{C4E4E3}
\definecolor{green1}{HTML}{A1D0C7}
\definecolor{green2}{HTML}{BFF6BA}
\definecolor{green3}{HTML}{028100}
\definecolor{teal}{HTML}{508AB2}
\definecolor{purple1}{HTML}{8d3a94}

\usepackage{pifont}\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\usepackage[listings]{tcolorbox}
\tcbuselibrary{listings,theorems}
\newtcolorbox{mybox}{colback=white!5!white,colframe=black!75!black, left=.05in, right=.05in}

\newtcbtheorem[number within=section]{exmp}{Example}{colback=green2!5,colframe=blue1,fonttitle=\bfseries, left=.02in, right=.02in,bottom=.02in, top=.02in}{exmp}
\newtcbtheorem[]{trainprompt}{Prompt}{colback=green2!5,colframe=blue1,fonttitle=\bfseries, left=.02in, right=.02in,bottom=.02in, top=.02in}{trainprompt}
\newtcbtheorem[]{prompt}{Backward Prompting}{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}
\newtcbtheorem[number within=section]{thm}{Theorem}{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}
\newtcbtheorem[number within=section]{corr}{Corollary}{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}
\newtcbtheorem{method}{Method}{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\egg}{EGG}
\DeclareMathOperator{\eer}{EER}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\softmax}{softmax}


\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\sign}{sign}
\newcommand{\acc}{\textsf{Acc}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\bU}{\mathbb{U}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bP}{\mathbb{P}}

\newcommand{\metaloss}{\mathcal{L}_{\text{meta}}}
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}

\newcommand{\bone}{{\bf 1}}
\newcommand{\bzero}{{\bf 0}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\sspan}{\text{span}}
\newcommand*{\escape}[1]{\texttt{\textbackslash#1}}

\newcommand{\uniform}{{\textsf{Uniform}}}

\newcommand{\hA}{\mathcal{A}}
\newcommand{\hB}{\mathcal{B}}
\newcommand{\hC}{\mathcal{C}}
\newcommand{\hD}{\mathcal{D}}
\newcommand{\hE}{\mathcal{E}}
\newcommand{\hF}{\mathcal{F}}
\newcommand{\hG}{\mathcal{G}}
\newcommand{\hH}{\mathcal{H}}
\newcommand{\hI}{\mathcal{I}}
\newcommand{\hJ}{\mathcal{J}}
\newcommand{\hK}{\mathcal{K}}
\newcommand{\hL}{\mathcal{L}}
\newcommand{\hM}{\mathcal{M}}
\newcommand{\hN}{\mathcal{N}}
\newcommand{\hO}{\mathcal{O}}
\newcommand{\hP}{\mathcal{P}}
\newcommand{\hQ}{\mathcal{Q}}
\newcommand{\hR}{\mathcal{R}}
\newcommand{\hS}{\mathcal{S}}
\newcommand{\hT}{\mathcal{T}}
\newcommand{\hU}{\mathcal{U}}
\newcommand{\hV}{\mathcal{V}}
\newcommand{\hW}{\mathcal{W}}
\newcommand{\hX}{\mathcal{X}}
\newcommand{\hY}{\mathcal{Y}}
\newcommand{\hZ}{\mathcal{Z}}

\newcommand{\bI}{\mathbb{I}}

\newcommand{\va}{{\bf a}}
\newcommand{\vb}{{\bf b}}
\newcommand{\vc}{{\bf c}}
\newcommand{\vd}{{\bf d}}
\newcommand{\ve}{{\bf e}}
\newcommand{\vf}{{\bf f}}
\newcommand{\vg}{{\bf g}}
\newcommand{\vh}{{\bf h}}
\newcommand{\vi}{{\bf i}}
\newcommand{\vj}{{\bf j}}
\newcommand{\vk}{{\bf k}}
\newcommand{\vl}{{\bf l}}
\newcommand{\vm}{{\bf m}}
\newcommand{\vn}{{\bf n}}
\newcommand{\vo}{{\bf o}}
\newcommand{\vp}{{\bf p}}
\newcommand{\vq}{{\bf q}}
\newcommand{\vr}{{\bf r}}
\newcommand{\vs}{{\bf s}}
\newcommand{\vt}{{\bf t}}
\newcommand{\vu}{{\bf u}}
\newcommand{\vv}{{\bf v}}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vz}{{\bf z}}

\newcommand{\vA}{{\bf A}}
\newcommand{\vB}{{\bf B}}
\newcommand{\vC}{{\bf C}}
\newcommand{\vD}{{\bf D}}
\newcommand{\vE}{{\bf E}}
\newcommand{\vF}{{\bf F}}
\newcommand{\vG}{{\bf G}}
\newcommand{\vH}{{\bf H}}
\newcommand{\vI}{{\bf I}}
\newcommand{\vJ}{{\bf J}}
\newcommand{\vK}{{\bf K}}
\newcommand{\vL}{{\bf L}}
\newcommand{\vM}{{\bf M}}
\newcommand{\vN}{{\bf N}}
\newcommand{\vO}{{\bf O}}
\newcommand{\vP}{{\bf P}}
\newcommand{\vQ}{{\bf Q}}
\newcommand{\vR}{{\bf R}}
\newcommand{\vS}{{\bf S}}
\newcommand{\vT}{{\bf T}}
\newcommand{\vU}{{\bf U}}
\newcommand{\vV}{{\bf V}}
\newcommand{\vW}{{\bf W}}
\newcommand{\vX}{{\bf X}}
\newcommand{\vY}{{\bf Y}}
\newcommand{\vZ}{{\bf Z}}

\newcommand{\bc}{{\underline{c}}}
\newcommand{\uc}{{\bar{c}}}

\newcommand{\fD}{\mathfrak{D}}
\newcommand{\fP}{\mathfrak{P}}

\DeclareMathOperator{\ber}{\textsf{Ber}}
\DeclareMathOperator{\var}{\textsf{Var}}
\DeclareMathOperator{\trace}{\textsf{Trace}}
\DeclareMathOperator{\kl}{\textsf{KL}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\hbeta}{\textsf{Beta}}
\DeclareMathOperator{\ssim}{\textsf{sim}}

\newcommand{\fracsam}{\text{\%SAM}}

\newcommand{\vomega}{{\boldsymbol \omega}}
\newcommand{\vgamma}{{\boldsymbol \gamma}}
\newcommand{\vnu}{{\boldsymbol \nu}}
\newcommand{\vtau}{{\boldsymbol \tau}}
\newcommand{\vpsi}{{\boldsymbol \psi}}
\newcommand{\vtheta}{{\boldsymbol \theta}}
\newcommand{\vbeta}{{\boldsymbol \beta}}
\newcommand{\vmu}{{\boldsymbol \mu}}
\newcommand{\vepsilon}{{\boldsymbol \epsilon}}
\newcommand{\vPsi}{{\boldsymbol \Psi}}
\newcommand{\vSigma}{{\boldsymbol \Sigma}}
\newcommand{\vxi}{{\boldsymbol \xi}}
\newcommand{\vzeta}{{\boldsymbol \zeta}}
\newcommand{\valpha}{{\boldsymbol \alpha}}

\newcommand*\numcircledmod[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\newcommand*\ceil[1]{\lfloor {#1} \rfloor}
\newcommand*\cboxa[1]{\colorbox[HTML]{c8e9c8}{#1}}
\newcommand*\cboxb[1]{\colorbox[HTML]{c5dced}{#1}}
\newcommand*\magentabox[1]{\colorbox{magenta!20}{#1}}
\newcommand*\cyanbox[1]{\colorbox{cyan!20}{#1}}
\newcommand*\grayboxa[1]{\colorbox{lightgray!30}{#1}}
\newcommand*\grayboxb[1]{\colorbox{lightgray!70}{#1}}
\newcommand*\grayboxc[1]{\colorbox{lightgray!10}{#1}}
\newcommand*\demobox[1]{\pbox{.7\textwidth}{#1}}

\newcommand\Mark[1]{\textsuperscript#1}



\def\boxit#1{\smash{\fboxsep=0pt\llap{\rlap{\fbox{\strut\makebox[#1]{}}}~}}\ignorespaces
}

\newcommand{\TODO}{{\color{red}[TODO] }}
 
\title{\fontsize{15.25pt}{\baselineskip}\selectfont\vspace{-6mm} {MetaMath}: Bootstrap Your Own Mathematical\\Questions for Large Language Models}


\author{Longhui Yu\textsuperscript{1,*} \quad Weisen Jiang\textsuperscript{2,3,*} \quad Han Shi\textsuperscript{4,\textdagger} \quad Jincheng Yu\textsuperscript{3,4} \quad Zhengying Liu\textsuperscript{4} \\ 
\textbf{Yu Zhang\textsuperscript{2} \quad James T. Kwok\textsuperscript{3} \quad Zhenguo Li\textsuperscript{4} \quad Adrian Weller\textsuperscript{1,5} \quad Weiyang Liu\textsuperscript{1,6,\textdagger}}
\
    \small
    \hD_{\text{AnsAug}} = \{(q_i, r_i^{(j)}, a_i^{(j)}): a_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{AnsAug}}\}.
    
\small
\hD_{\text{rephrase}} = \{(\hat{q}_i, \hat{r}_i^{(j)}, \hat{a}_i^{(j)}): \hat{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{rephrase}}\}.

    \small
    \hD_{\text{SV}} = \{(\tilde{q}_i^{(j)}, \tilde{r}_i^{(j)}, \tilde{a}_i^{(j)}): \tilde{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{SV}}\}.
    
	\small
		\hD_{\text{FOBAR}} = \{(\bar{q}_i^{(j)}, \bar{r}_i^{(j)}, \bar{a}_i^{(j)}): \bar{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{FOBAR}}\}.
	
        \small
		\hD_{\text{MetaMathQA}} = \hD_{\text{AnsAug}}  \cup \hD_{\text{rephrase}} \cup \hD_{\text{SV}} \cup \hD_{\text{FOBAR}}.
	
        \small
		\mathcal{L}(\vtheta)= \sum_{(q, r, a) \in \hD_{\text{MetaMathQA}} } \log \bP(r \mid  q;\vtheta).
		\label{eq:mle}
	
 Although we only consider LLaMA-2 here, MetaMathQA can also be used to finetune other LLMs.

\newpage
\begin{table}[!t]
\footnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.2}
    \begin{tabular}{c|cccccc|cccccc}
        \multirow{2}{*}{Method}& \multicolumn{6}{c|}{GSM8K}& \multicolumn{6}{c}{MATH}\\
         & AnsAug & Rep. & SV  & FOBAR  
        & GSM8K & MATH & AnsAug & Rep. & SV  & FOBAR  
        & GSM8K & MATH\\\midrule
        SFT \citep{touvron2023llama} & \xmark & \xmark & \xmark & \xmark & 41.6 & 3.0 & \xmark & \xmark & \xmark & \xmark & 13.8 & 4.7\\
        \multirow{4}{*}{MetaMath}& \cmark & \xmark & \xmark & \xmark & 59.6 & 4.4 & \cmark & \xmark & \xmark & \xmark & 28.4 & 12.9 \\
        & \xmark & \cmark & \xmark & \xmark & 59.7 & 4.4 & \xmark & \cmark & \xmark & \xmark & 30.4 & 12.4 \\
        & \cmark & \cmark & \xmark & \xmark & 60.6 & 4.4& \cmark & \cmark & \xmark & \xmark & 29.1 & 15.3 \\
        & \cmark & \cmark & \cmark & \cmark & \textbf{64.4} & \textbf{5.7}& \cmark & \cmark & \cmark & \cmark & \textbf{34.6} & \textbf{17.7}\\
    \end{tabular}
   \vspace{-1.5mm}
\caption{\footnotesize Effect of different question augmentation with {LLaMA-2-7B} finetuned on GSM8K or MATH.}\label{exp:abl-effect-aug}
\vspace{-2mm}
\end{table}

\vspace{-.5mm}
\section{Experiments and Results}
\vspace{-.5mm}
    \subsection{Experimental Setup}\label{sec:expt-setup}
\vspace{-.5mm}
    \setlength{\columnsep}{9pt}
    \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt}
	\renewcommand{\arraystretch}{1.22}
    \begin{tabular}{c|ccccc}
    \specialrule{0em}{0pt}{-15pt}
     Dataset   & AnsAug & Rephrasing & SV & FOBAR & Overall \\
    \midrule
    MetaMathQA-GSM8K & 80K & 80K & 40K & 40K & 240K \\
    MetaMathQA-MATH  & 75K & 50K & 15K & 15K & 155K \\
    MetaMathQA  & 155K  & 130K  & 55K  & 55K & 395K \\
    \specialrule{0em}{0pt}{-7pt}
    \end{tabular}
    \caption{\footnotesize Number of samples in the proposed MetaMathQA.}\label{exp:dataset}
    \vspace{-1mm}
    \end{wraptable}
    
    \textbf{Datasets.}
    We use two popular
    mathematical reasoning benchmarks:
    \begin{enumerate*}[(i), series = tobecont, itemjoin = ~~]
    \item GSM8K \citep{cobbe2021training} is a dataset consisting of high-quality grade school math 
    problems, containing 7,473 training samples and 1,319 testing samples;
    and 
    \item MATH \citep{hendrycks2021measuring}
    dataset consists of high school math competition problems that span 
    seven subjects including 
    Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra,
    and Precalculus.
    It contains 7,500 and 5,000 samples for training and testing, respectively.
    \end{enumerate*}
    Questions in GSM8K \citep{cobbe2021training} take between 2 and 8 steps to reach the answer, while 
    MATH
    is much more challenging.
    \vspace{-1.mm}

\textbf{Models.}
    We use the current state-of-the-art open-source model
    LLaMA-2
    \citep{touvron2023llama},
    including three different parameter sizes:
    7B, 
    13B,
    and 
    70B,
    as the base model for fine-tuning.
    GPT-3.5-Turbo is used for rephrasing questions as well as 
    generating answers in all four augmentations,
    where the temperature is set to 0.7 as in \cite{wang2023selfconsistency}.
    The LLaMA-2-7B and LLaMA-2-13B are trained by fully fine-tuning. LLaMA-2-70B is finetuned by QLoRA~\citep{dettmers2023qlora} for computational efficiency. More experimental details can be seen in Appendix~\ref{expdetalis}.
    
    \textbf{Baselines.}
    The proposed methods are compared with 
    \begin{enumerate*}[(i), series = tobecont, itemjoin = \quad]
    \item closed-source models such as GPT-3.5-Turbo~\citep{gpt3-5-turbo}, PaLM~\citep{chowdhery2022palm};
    \item open-source models such as LLaMA-1~\citep{touvron2023llama1}, LLaMA-2~\citep{touvron2023llama};
    \item Supervised Fine-Tuning (SFT),
    which uses the training set of the original GSM8K or MATH datasets;
    \item Rejection sampling Fine-Tuning (RFT) \citep{yuan2023scaling}
    generates and collects correct reasoning paths as 
    augmented data for fine-tuning; 
    \item WizardMath \citep{luo2023wizardmath}
    which generates samples and trains two reward models using ChatGPT \footnote{\url{https://openai.com/}} to select samples for fine-tuning. 
    \end{enumerate*}

    \textbf{Diversity Gain.} We use the diversity gain \citep{bilmes2022submodularity} to measure to what extent a new dataset added to a basic dataset can improve the overall data diversity. For a base dataset  with  samples, and a new dataset  with M samples, the diversity gain is defined as:  relative to  as: , where  is the feature extractor and we use the OpenAI Embedding API \textit{text-embedding-ada-002} for feature extraction. For Figure~\ref{fig:Accuracy Saturation}, we change the data size of base data and select a fixed set of 20K new data points that the model has not encountered to form .

    \vspace{-1mm}
    \subsection{Results on GSM8K and MATH}
    \vspace{-1.5mm}

    Table~\ref{exp:dataset} illustrates the detailed description of our MetaMathQA collection and 
    Table \ref{exp:main-expt}
    shows the testing accuracy on GSM8K and MATH.
    As can be seen,
    for open-source models with 
    1-10B parameters,
    MetaMath achieves the state-of-the-art performance.
    Compared to the previous best LLM, MetaMath achieves a large improvement of 11.6\% on GSM8K and 9.1\% on MATH in testing accuracy,
    showing that finetuning on our MetaMathQA data is effective.

    
    
     
\newpage
\setlength{\columnsep}{14pt}
    \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \hspace{-2.4mm}
    \footnotesize
    \setlength{\tabcolsep}{6.5pt}
    \renewcommand{\arraystretch}{1.15}
\begin{tabular}{l|ccc}
    \specialrule{0em}{0pt}{-1pt}
    Model& \#params & GSM8K & MATH \\\midrule
    \multicolumn{4}{c}{\textit{closed-source models}}  \\
    GPT-4 \citep{gpt4} & - & 92.0 & 42.5 \\
    GPT-3.5-Turbo \citep{gpt3-5-turbo} & -& 80.8 & 34.1 \\
    PaLM \citep{chowdhery2022palm} & 8B & 4.1 & 1.5 \\ 
    PaLM \citep{chowdhery2022palm} & 62B & 33.0 & 4.4 \\ 
    PaLM \citep{chowdhery2022palm} & 540B & 56.5 & 8.8 \\ 
    PaLM-2 \citep{anil2023palm} & 540B & 80.7 & 34.3 \\
    Flan-PaLM 2 \citep{anil2023palm} & 540B & 84.7 & 33.2 \\
    Minerva \citep{lewkowycz2022solving} & 8B & 16.2 & 14.1 \\
    Minerva \citep{lewkowycz2022solving} & 62B & 52.4 & 27.6\\
    Minerva \citep{lewkowycz2022solving} & 540B & 58.8 & 33.6\\\midrule
    \multicolumn{4}{c}{\textit{open-source models (1-10B)}}  \\
    LLaMA-1 \citep{touvron2023llama1} & 7B & 11.0 & 2.9 \\
    LLaMA-2 \citep{touvron2023llama} & 7B & 14.6 & 2.5 \\
    MPT \citep{MosaicML2023Introducing} & 7B & 6.8 & 3.0 \\
    Falcon \citep{penedo2023refinedweb} & 7B & 6.8 & 2.3 \\
    InternLM \citep{2023internlm} & 7B & 31.2 & - \\
    GPT-J \citep{gpt-j} & 6B & 34.9 & - \\
    ChatGLM 2 \citep{zeng2022glm} & 6B & 32.4 & - \\
    Qwen \citep{qianwen} & 7B & 51.6 & - \\
    Baichuan-2 \citep{baichuan2} & 7B & 24.5 & 5.6 \\
    SFT \citep{touvron2023llama} & 7B & 41.6 & - \\
    RFT \citep{yuan2023scaling} & 7B & 50.3 & - \\
    WizardMath \citep{luo2023wizardmath} & 7B & 54.9 & 10.7 \\ \rowcolor{Gray}
    MetaMath  & 7B & \textbf{66.5}& \textbf{19.8} \\\midrule
    \multicolumn{4}{c}{\textit{open-source models (11-50B)}}  \\
    LLaMA-1 \citep{touvron2023llama1} & 13B & 17.8 & 3.9 \\
    LLaMA-1 \citep{touvron2023llama1} & 33B & 35.6 & 7.1 \\
    LLaMA-2 \citep{touvron2023llama} & 13B & 28.7 & 3.9 \\
    LLaMA-2 \citep{touvron2023llama} & 34B & 42.2 & 6.2\\
    MPT \citep{MosaicML2023Introducing} & 30B & 15.2 & 3.1 \\
    Falcon \citep{penedo2023refinedweb} & 40B & 19.6 & 2.5 \\
    GAL \citep{taylor2022galactica} & 30B & - & 12.7  \\
    Vicuna \citep{vicuna2023} & 13B & 27.6 & - \\
    Baichuan-2 \citep{baichuan2} & 13B & 52.8 & 10.1\\
    SFT \citep{touvron2023llama} & 13B & 50.0 & - \\
    RFT \citep{yuan2023scaling} & 13B & 54.8 & - \\
    WizardMath \citep{luo2023wizardmath} & 13B & 63.9 & 14.0  \\ \rowcolor{Gray}
    MetaMath  & 13B & \textbf{72.3} & \textbf{22.4} \\\midrule
    \multicolumn{4}{c}{\textit{open-source models (51-70B)}} \\
    LLaMA-1 \citep{touvron2023llama1} & 65B & 50.9 & 10.6 \\
    LLaMA-2 \citep{touvron2023llama} & 70B & 56.8 & 13.5 \\
    RFT \citep{yuan2023scaling}             & 70B & 64.8 & - \\
    WizardMath \citep{luo2023wizardmath}    & 70B & 81.6 & 22.7  \\ \rowcolor{Gray}
    MetaMath               & 70B & \textbf{82.3} & \textbf{26.6} \\
    \specialrule{0em}{-5pt}{0pt}
    \end{tabular}
\caption{\footnotesize Comparison of testing accuracy to existing LLMs on GSM8K and MATH. Due to the computing resource limitation, we finetune MetaMath-70B using QLoRA \citep{dettmers2023qlora}.}
    \label{exp:main-expt}
    \vspace{-12mm}
    \end{wraptable}

%
     
    As for LLMs with 11-50B parameters,
    the proposed MetaMath performs the best.
    Particularly,
    on both GSM8K and MATH,
    MetaMath achieves higher accuracy than SFT, RFT, and WizardMath
    by a large margin (+7\%), demonstrating 
    the effectiveness of the MetaMath data in improving mathematical reasoning ability.
    Furthermore,
    for LLMs with 51-70B parameters,
    again, 
    MetaMath achieves the highest testing accuracy.
    Particularly,
    MetaMath is better than GPT-3.5-Turbo on GSM8K, 
    which is used for generating augmented data for finetuning.

    \vspace{-3.2mm}
    
	\subsection{Effect of Augmentations}
 \vspace{-1.75mm}
	
	In this section,
	we conduct experiments 
	to study
	the effect of augmentations in MetaMath.
	We first finetune the {LLaMA-2-7B}
	model on augmented GSM8K (MetaMath-GSM8K) data, 
	and test the finetuned model 
	on GSM8K and MATH.
	Table \ref{exp:abl-effect-aug}
	shows the testing accuracy of
	different combinations of augmentations.
	As can be seen, 
	on GSM8K,
	the models trained on answer augmentation (AnsAug) or rephrasing  augmentation 
	achieve much higher
	accuracy than SFT, which is only trained on 
	the training set.
	Combing answer augmentation and rephrasing  augmentation data
	for fine-tuning leads to a slightly higher accuracy,
	which is further improved by about 4\% through merging the FOBAR and SV augmentation data.
	As for MATH,
	MetaMath trained only on MetaMahQA-GSM8K data
	performs better than SFT, suggesting its effectiveness in generalizing to
	unseen mathematical tasks. 
	
	
    We also conduct an experiment
    by fine-tuning
    {LLaMA-2-7B} on the MetaMathQA-MATH data
    then evaluate the model on GSM8K and MATH.
    Table \ref{exp:abl-effect-aug}
    shows the testing accuracy.
    Again,
    MetaMath trained on AnsAug or rephrasing augmentation data 
    performs much better
    than SFT.
    Furthermore,
    merging all augmented data together for
    fine-tuning is better than merging AnsAug and rephrasing augmentation data, demonstrating 
    the effectiveness of SV and FOBAR augmentation data
    in improving mathematical reasoning ability.
    Moreover,
    for the unseen GSM8K task,
    MetaMath trained on MetaMathQA-MATH data is significantly better
    than SFT (+20\%).

    \begin{figure}[!t]
    \begin{minipage}{0.44\linewidth}
    \vspace{-1.4em}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/ppl_box.pdf}
    \vspace{-1.85em}
	\caption{\footnotesize  Lower perplexity of MetaMathQA.}
	\label{fig:perplexity}
    \vspace{-0.5em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.54\linewidth}
    \vspace{-1.4em}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/table5.pdf}
    \vspace{-2.18em}
	\caption{\footnotesize  Accuracy correlates positively with diversity.}
	\label{fig:DiversityGains}
    \vspace{-0.5em}
    \end{minipage}
    \end{figure}


        \begin{figure}[b]
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/diff_bar.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize Combing RFT~\citep{yuan2023scaling} dataset with our MetaMathQA leads to a performance drop.}
	\label{fig:Less is More}
    \vspace{-0.7em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/gsm8k_backward.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize The accuracy gap between GSM8K and GSM8K-Backward.}
	\label{fig:reverse}
    \vspace{-0.7em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/question_length.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize Testing accuracy on questions with short length, medium length and long length.}
	\label{fig:erroranalysis}
    \vspace{-0.7em}
    \end{minipage}
    \end{figure}
    
    \subsection{Discussion from a Perplexity Perspective}    

    According to the Superficial Alignment Hypothesis proposed by ~\citet{zhou2023lima}, the capability of a model is rooted in pretraining, and data from downstream tasks acts to activate the inherent ability of LLMs that has been learned during pretraining. There are two important questions that arise from such a hypothesis: (i) \textit{what} kind of data is most effective at activating possible latent knowledge, and (ii) \textit{why} is one dataset better than another at such activation? Our empirical results suggest that, in the mathematical tasks we consider, our MetaMathQA dataset may serve as a superior activator of mathematical knowledge. Yet, \textit{why} MetaMath yields superior performance than training on the data of correct answer-only or GSM8K CoT is unclear. We speculate that perhaps it is the simplicity of the data that matters. As shown in Figure~\ref{fig:perplexity}, we compute the perplexity~\citep{wang2023making,marion2023less} for the under-finetuned {LLaMA-2-7B} model, in terms of answer-only data, GSM8K CoT, and the subsections of {MetaMathQA} data. The perplexity of {MetaMathQA} is significantly lower than the other two datasets. This highlights its inherently easy-to-learn nature, which may be more conducive to eliciting bolstered problem-solving abilities from an LLM. This is also aligned with the findings with TinyStories~\citep{eldan2023tinystories}, where short and easy story data can help LLMs generate content fluently.
    
    \subsection{Discussion from a Diversity perspective}
As shown in Figure~\ref{fig:Accuracy Saturation}, naively prompting GPT-3.5-Turbo for answer augmentation leads to a clear accuracy saturation. After accuracy saturation, increasing the AnsAug data only yields a limited performance gain. For instance, using 80K answer augmentation data to train a LLaMA-2 7B model leads to a 59.6\% accuracy, adding new 20K AnsAug data would only take 0.1\% performance gain. This is due to the homogeneity of the additional samples, contributing to a diversity gain of only 0.05 (shown in Figure~\ref{fig:DiversityGains}). In comparison, adding the same amount of data generated by question bootstrapping leads to a significant performance boost, which is due to the noticeable diversity gain brought by question bootstrapping. As shown in Figure~\ref{fig:DiversityGains}, adding 20K data from Rephrasing, FOBAR, or SV takes an increasing diversity gain, thus causing a 0.4\%, 2.3\%, and 2.6\% accuracy gain, respectively. This experiment demonstrates a positive correlation (the Pearson coefficient is 0.972) between the diversity brought by the bootstrapping methods and accuracy. This is also aligned with the success of MetaMath, which is trained with the diverse MetaMathQA dataset including 4 kinds of data reflecting both the forward and backward reasoning paths.

\subsection{Evaluating the Reversal Mathematical Capability}
The Reversal Curse~\citep{berglund2023reversal}, where LLMs trained from a sentence ``A is B" are not able to generalize to answer ``B is A", also aligns with the observation in this paper that LLMs lack backward mathematical reasoning ability. To evaluate the backward mathematical capability, we propose a GSM8K-Backward test set, including 1270 backward questions by using SV and FOBAR to augment the original GSM8K test set (as shown in Example~\ref{exmp:sv-example} and Example~\ref{exmp:fobar}). 
Figure~\ref{fig:reverse} shows the accuracy comparison of different 7B mathematical LLMs between the GSM8K and GSM8K-Backward datasets. As can be seen, existing LLMs struggle to solve mathematical problems in backward rationales and our MetaMath has a significant improvement on both datasets. Specifically, the ways where different LLMs solve the backward mathematical problem are illustrated through examples in Appendix~\ref{app:reversestudy}.

    \vspace{-1mm}
    \subsection{Reasoning Paths with Incorrect Answer Can Also Be Useful}
    \vspace{-1mm}

    \setlength{\columnsep}{9pt}

        \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt}
	\renewcommand{\arraystretch}{1.15}
    \begin{tabular}{c|c}
     \specialrule{0em}{0pt}{-14pt}
        Data & Accuracy \\\midrule
        GSM8K~\citep{cobbe2021training}  &  41.6  \\
        Incorrect Answers   & 43.6 \\
        Correct Answers  & \textbf{52.2}  \\
        \specialrule{0em}{0pt}{-5pt}
    \end{tabular}
    \caption{\footnotesize Testing accuracy on GSM8K of {LLaMA-2-7B} trained on different data.}
    \label{table:incorrect} 
    \vspace{-0.2cm}
    \end{wraptable}
    
    We conduct experiments on GSM8K using {LLaMA-2-7B} to study whether the answer augmentation samples with incorrect answers are helpful for finetuning the LLM. We randomly choose 7,473 reasoning paths with incorrect answers from the generated answers, and we ensure that the size is the same as that of the original training set. From Table~\ref{table:incorrect}, we observe that the model finetuned on the augmented data with incorrect answers is actually better than SFT, which is counter-intuitive. We hypothesize that although the final answer is incorrect, some intermediate reasoning steps are correct (see Example \ref{exmp:incorrect}). These reasoning steps can still be useful supervision signals. Our results are also aligned with~\citep{lightman2023let}, where they discover the importance of intermediate process supervision for reasoning.
    
    

    
    \begin{exmp}{A Reasoning Path with Incorrect Answer}{incorrect}\small
    \textbf{Question:} Tonya is in a hamburger eating contest. Each hamburger is 4 ounces. Last year the winner ate 84 ounces. How many hamburgers does she have to eat to beat last year's winner? {\color{red3}(Ground-truth answer is )}
    
    \textbf{Generated Answer:} If each hamburger is 4 ounces and last year's winner ate 84 ounces, then Tonya needs to eat 84/4 = 21 hamburgers to beat last year's winner. The answer is: 21
    \end{exmp}

\vspace{-.5mm}  
\subsection{More Data is not Always Better}
\vspace{-.5mm}
    
    There are also previous works that augment mathematical reasoning data for fine-tuning~\citep{luo2023wizardmath,yuan2023scaling}. An interesting question is whether combining existing augmented datasets with our {MetaMathQA} can improve the overall mathematical problem-solving performance. We select the RFT~\citep{yuan2023scaling} dataset as the external dataset. Figure~\ref{fig:Less is More} shows that merging the RFT data into {MetaMathQA} actually hurts the performance,
    indicating that the RFT data may not be beneficial to {MetaMath}. Such a phenomenon is consistently observed in the {MetaMathQA} dataset under different sizes (from 20K to 100K), and the added RFT dataset is about 47K. The performance drop implies that more augmented data does not always help the generalization. 
    

        \begin{exmp}{Case Study in Test Set of GSM8K}{case-study}\small
    \textbf{Question:}
    Darrell and Allen's ages are in the ratio of 7:11, If their total age now is 162, calculate Allen's age 10 years from now. {\color{red3}(Ground-truth answer is )}
   
    \textbf{SFT Answer:}
    The total ratio representing their ages is 7+11=7+11=1818. The fraction representing Darrell's age is 7/18, and since their total age now is 162, Darrell's age is 7/18*162 = 7/18*162=6060. The fraction representing Allen's age is 11/18, and since their total age now is 162, Allen's age is 11/18*162 = 11/18*162=8484. Ten years from now, Allen's age will be 84 + 10 = 84+10=9494.
    
    \textbf{RFT Answer:}
    The total ratio representing Darrell and Allen's ages is 7+11 = 7+11=1818. The fraction representing Darrell's age from the total ratio is 7/18, and since their total age is 162, Darrell is 7/18*162 = 100 years old. Allen's age, from the total ratio, is 11/18, and since Darrell is 100, Allen is 11/18*109 = 11/18*100=6666 years old. Ten years from now, Allen will be 66 + 10 = 76 years old.

    \textbf{WizardMath Answer:}
    We know Darrell and Allen's ages are in the ratio of 7:11, which means the total ratio representing their ages is 7+11 = 18. If their total age now is 162, we can calculate Darrell's age by using the ratio. Specifically, Darrell's age is 7/18 of 162, which is 7/18*162 = 49 years old.  ...  The answer is: 76.

    \textbf{Our Answer:}
    The ratio of Darrell's age to Allen's age is 7:11. Let's assume Darrell's age is 7x and Allen's age is 11x. The total age of Darrell and Allen is 7x + 11x = 18x. We are given that the total age is 162, so 18x = 162. Dividing both sides by 18, we get x = 9. Therefore, Allen's age is 11x = 11 * 9 = 99. 10 years from now, Allen's age will be 99 + 10 = 109.
    \end{exmp}


\vspace{-1mm}
\subsection{Error Analysis}
\vspace{-1mm}

We have demonstrated that -- across multiple scales -- our MetaMath models can achieve stellar problem-solving performance. Yet, it is important to consider the characteristics of problems that induce errors in MetaMath and existing open-source mathematical models. In particular, we consider the relationship between question length and model performance. To investigate, we divide the GSM8K test set into three equally-sized subsets based on the different lengths of questions and calculate the accuracy of the models over each subset. We find in Figure~\ref{fig:erroranalysis} that, MetaMath and related methods struggle under longer questions. However, excitingly, MetaMath always obtains superior performance. We see the study of improving model performance with longer question lengths -- for instance, by further augmenting the MetaMathQA dataset -- as ripe grounds for future work. 
	
	
\vspace{-1.5mm}
\section{Concluding Remarks}
\label{sec:conclusion}

 \vspace{-1.5mm}
 In this paper, we focus on improving the mathematical problem-solving abilities of open-source LLMs. By bootstrapping mathematical questions on GSM8K and MATH, we present a high-quality and diverse dataset {MetaMathQA}, involving forward reasoning and backward reasoning samples. Our family of LLMs finetuned on {MetaMathQA}, called {MetaMath}, have achieved state-of-the-art on mathematical benchmarks among all open-source LLMs. Remarkably, {MetaMath-7B} reaches  on GSM8K and  on MATH, surpassing previous open-source LLMs by a significant margin. Our work further emphasizes the importance of the characteristics of the training data on boosting LLM problem-solving capabilities.


\section*{Acknowledgement}

The authors would like to sincerely thank Katherine M. Collins from University of Cambridge for her valuable insights and suggestions.

	
	
\bibliography{paper}
\bibliographystyle{conference.bst}

 \clearpage
 \newpage
    \appendix
    \section{Prompts}

    \subsection{Rephrasing Prompts}
    \label{sec:rephrase-prompt}
    
    \begin{exmp}{Prompt for Rephrasing GSM8K Questions}{Rephrase-prompt-all}\small
    	\textit{You are an AI assistant to help me rephrase questions.
    	Follow the given examples.} \\
    	
    	\textbf{Question:} Olivia has \3 each. How much money does she have left? \\
    	\textbf{Rephrase the above question:} What is the amount of money that Olivia has left after purchasing five bagels for \23? \\
    	
    	\textbf{Question:} Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? \\
    	\textbf{Rephrase the above question:} After losing 23 golf balls on Tuesday and an additional 2 on Wednesday, how many golf balls does Michael have left if he initially had 58 golf balls? \\
    	
    	\textbf{Question:} Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?
    	
    	\textbf{Rephrase the above question:} Angelo and Melanie need to study 2 chapters in their textbook and 4 worksheets for their upcoming test. They have planned to dedicate 3 hours for each chapter and 1.5 hours for each worksheet. They can study for a maximum of 4 hours each day, taking into account 10-minute breaks every hour, 3 10-minute snack breaks per day, and 30 minutes for lunch. How many days do they need to study in total over the next week to complete their study plan?\\
    	
    	\textbf{Question:} Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? \\
    	\textbf{Rephrase the above question:} If Leah had 32 chocolates and her sister had 42, and they both consumed 35 chocolates, what is the total number of chocolates that they have left? \\
    	
    	
    	\textbf{Question:} There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? \\
    	\textbf{Rephrase the above question:} If there were initially nine computers in the server room and five more computers were added each day from Monday to Thursday, what is the current total number of computers in the server room? \\
    	
    	\textbf{Question:} Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? \\
    	\textbf{Rephrase the above question: }If Jason initially had 20 lollipops and now has 12 after giving some to Denny, how many lollipops did he give to Denny? \\
    	
    	\textbf{Question:} Sam bought a dozen boxes, each with 30 highlighter pens inside, for \3 per package. He sold the rest of the highlighters separately at the rate of three pens for \10 per box. He repackaged five of these boxes into sets of six highlighters and sold them for \2. What is the total profit he made in dollars? \\
    	
    	\textbf{Question:} There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? \\
    	\textbf{Rephrase the above question:} If there were initially 15 trees in the grove and the grove workers are planning to plant more trees today, resulting in a total of 21 trees, how many trees did the workers plant today? \\
    	
    	\textbf{Question:} {\color{red3}\{Q\}} \\
    	\textbf{Rephrase the above question:}
    \end{exmp}
    \newpage


    \begin{exmp}{Prompts for Rewriting Question with Answer into a Declarative Statement}{sv}
    \small
    \textit{You are an AI assistant to help me rewrite question into a declarative statement when its answer is provided.
        Follow the given examples and rewrite the question.} \\
    
    \textbf{Question:} How many cars are in the parking lot? The answer is: 5. \\
    \textbf{Result:} There are 5 cars in the parking lot. 
    
    ...
    
    \textbf{Question:} {\color{red3}\{Q\}} The answer is: {\color{red3}\{A\}}. \\
    \textbf{Result:} 
    \end{exmp}
    
    \subsection{Experimental Details}
    \label{expdetalis}
    \textbf{Training Details.}
    For the fully fine-tuning setting, we use the AdamW optimizer to train the model with 3 epochs and the batch size is 128. We use 8 NVIDIA A100 GPUs to train the 7B and 13B models, the learning rate is set as 2e-5 with a 3\% learning rate warmup. For the 70B model QLoRA fine-tuning, the LoRA rank and alpha are 96 and 16, with a 0.05 dropout between the two matrices. The LoRA matrices are append in both the attention layer and the mlp layer. We use the same AdamW optimizer but with a 1e-4 learning rate and without a learning rate warmup. The Training Prompt~\ref{trainprompt:trainprompt} are basically from Alpaca~\citep{alpaca}, where the instruction is replaced by the MetaMathQA question.
    
    \begin{trainprompt}{Training Prompt}{trainprompt}\small
    \textit{Below is an instruction that describes a task.
    Write a response that appropriately completes the request.\textbackslash n\textbackslash n\#\#\#
    Instruction:\textbackslash n\{instruction\}\textbackslash n\textbackslash n\#\#\# Response:}
    \end{trainprompt}
    \begin{trainprompt}{Evaluation Prompt}{evaluationprompt}\small
    \textit{Below is an instruction that describes a task.
    Write a response that appropriately completes the request.\textbackslash n\textbackslash n\#\#\#
    Instruction:\textbackslash n\{instruction\}\textbackslash n\textbackslash n\#\#\# Response: Let's think step by step.}
    \end{trainprompt}

    \textbf{Evaluation Prompting.} Different from the few-shot prompting evaluation for closed-source models, we find that zero-shot prompting is better for finetuned LLMs, which also saves more inference costs. Hence, MetaMath uses the zero-shot Evaluation Prompt~\ref{trainprompt:evaluationprompt} for GSM8K and MATH, where the instruction is replaced by the testing question. We set the temperature as 0 for fine-tuned LLaMA model.

    \textbf{Answer Extraction.} Different from the~\citet{wei2022chain}, where they use complex string rules to extract the final answer. In line with WizardMath~\cite{luo2023wizardmath}, MetaMath only extracts the string behind \textit{The answer is: } as the final answer. To teach the model this extraction method, we append \textit{The answer is: \{gold answer\}} to the end of answers in the MetaMathQA dataset, where the gold answer is replaced by the respective question's answer.

\newpage
    \subsection{How do different LLMs solve reversal mathematical problems?}
    \label{app:reversestudy}
    
    \begin{exmp}{Case study about Reverse Mathematical problem}{reverse-study}\small
    \textbf{Question:}
    A robe takes 2 bolts of blue fiber and x\% that much white fiber. It takes a total of 3 bolts. What is the value of unknown variable x? {\color{red3}(Ground-truth answer is )}
   
    \textbf{SFT Answer:} We know that the robe takes 2 * x\% = 3 bolts.
So 2 * x\% = 3. So x\% = 3/2 = 60\%. So x = 60. \#\#\#\# 60. The answer is: 60

    \textbf{RFT Answer:} We know that the robe takes 2 bolts of blue fiber and x\% that much white fiber, so it takes 2 * 0.01 = 0.02 bolts of white fiber. So in total, it takes 2 + 0.02 = <<2+0.02=2.02>>2.02 bolts of blue and white fiber. Since we know that it takes 3 bolts in total, we can write the equation 2.02 = 3. We can solve for x by multiplying both sides by 3, getting 6.04 = 3 * 0.02. We can then write the equation 6.04 = 3, and solve for x by dividing both sides by 6.04, getting x = 0.02. \#\#\#\# 2 The answer is: 2

    \textbf{WizardMath Answer:} Step 1: Define the variables. Let x\% of the blue fiber be represented by the variable y. Then, the robe takes 2y bolts of blue fiber. Step 2: Set up the equation. Since the robe takes a total of 3 bolts, we can write the equation as follows: 2y + xy = 3 Step 3: Simplify the equation. We can simplify the equation by combining the terms with y: 2y + xy = 3 3y + xy = 3 Step 4: Solve for x. To solve for x, we need to isolate xy on one side of the equation. We can do this by subtracting 3y from both sides: 3y + xy - 3y = 3 - 3y xy = 3 - 3y Step 5: Find the value of x. Now, we need to find the value of x. We know that xy = 3 - 3y, and we want to express y in terms of x. To do this, we can divide both sides of the equation by x: (3 - 3y) / x = y Step 6: Solve for y. Now, we can solve for y by multiplying both sides of the equation by x: 3 - 3y = xy 3 - 3y = x(3 - 3y) Step 7: Simplify the equation. We can simplify the equation by expanding and combining like terms: 3 - 3y = 3 - 3y Step 8: Find the value of x. Now, we can see that y = 0, which means that x\% of the blue fiber is 0\%. Since we are dealing with a percentage, the only possible value for x is 0. Therefore, the value of unknown variable x is 0. Final Answer: x = 0. The answer is: x

    \textbf{Our Answer:} The robe takes 2 bolts of blue fiber. It takes x\% of that much white fiber, which means it takes (x/100) * 2 bolts of white fiber. In total, it takes 3 bolts. So, we can write the equation as: 2 + (x/100) * 2 = 3 Let's simplify and solve for x: 2 + (2x/100) = 3 To isolate x, we subtract 2 from both sides of the equation: 2 + (2x/100) - 2 = 3 - 2 (2x/100) = 1 To solve for x, we multiply both sides of the equation by 100/2: (2x/100) * (100/2) = 1 * (100/2) x = 50 The value of x is 50. \#\#\#\# 50 The answer is: 50

    \end{exmp}

\end{document}
