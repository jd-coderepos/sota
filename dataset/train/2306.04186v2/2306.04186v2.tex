
\section{Results}
\subsection{Ablation Study}
Ablation experiments are conducted using ATST-Clip  and ATST-Frame with the linear evaluation protocol, due to their low computational cost.

\subsubsection{Ablations on ATST-Clip}
\label{sec:ablation}


We separately evaluate the effectiveness of the Transformer encoder and the proposed view creation strategy. Table~\ref{tab:segments} shows the results. The result of BYOL-A is also given, which uses a CNN encoder and a single 1-second segment. Our models use single or two segments, with a length of 1 second or 6 seconds. For a fair comparison, when the segment length is set to 1 second, we split audio clips into 1-second chunks for downstream tasks.

\textbf{Transformer Encoder:} With the same view creation strategy, i.e. creating two views from a 1-second segment, our model (line 2 in Table \ref{tab:segments}) outperforms BYOL-A, especially for the two speech tasks (SPCV2 and VOX1). Speech involves more long-term semantic information, and Transformer is more suitable than CNN for learning these long-term dependencies. 

\textbf{View Creation Strategy:} As shown in Table \ref{tab:segments}, when the segment length is set to 1 second, using one single segment is better than using two segments. This phenomenon is consistent with the claim made in BYOL-A \cite{niizumi_byol_2021} that the two segments may be too different to be identified as the same sample. However, two views created from a single segment may share too much semantic content, thus leading our model to find an easy solution.
When the segment length is increased to 6 seconds, the performance measures of AS-20K, VOX1 and US8K are systematically increased, no matter whether using one or two segments. This is partially due to the capability of learning long-term dependencies of the Transformer encoder.
In addition, for the 6-second case, using two segments exhibits superior performance over using one segment. The possible reasons are: the two segments can be rationally identified as the same sample as they share a small portion of overlap, and meanwhile they are different enough to increase the task difficulty and thus leads the model to learn a more generalized representation.

\begin{table*}[ht]

  \centering
  \begin{tabular}{l|cc|ccccc|c}
    \toprule
    \textbf{Method}                        & \textbf{Segments} & \textbf{\makecell{length of                                                                                                 \\segment (s)}}   &   \makecell{AS-20K\\mAP (\%)}    & \makecell{SPCV2\\Acc (\%)}   & \makecell{VOX1\\Acc (\%)}
                                           & \makecell{NSYNTH                                                                                                                                \\Acc (\%)} & \makecell{US8K\\Acc (\%)}&\makecell{Average\\Acc (\%)}  \\
    \midrule
    BYOL-A \cite{niizumi_byol_2021}        & single            & 1                           & -             & 92.2          & 40.1          & 74.1          & 79.1          & 71.4          \\
    \midrule
    \textbf{\multirow{4}{*}{ATST-Clip}} & single            & 1                           & 21.0          & 94.3          & 52.3          & 73.8          & 79.3          & 74.9          \\
                                           & two               & 1                           & 19.1          & 91.3          & 50.0          & 74.3          & 76.6          & 73.1          \\
                                           & single            & 6                           & 25.7          & \textbf{94.0} &        &         & 80.9          & 76.5          \\
                                           & two               & 6                           & \textbf{27.9} & 93.6          & \textbf{61.9} & \textbf{75.3} & \textbf{82.0} & \textbf{78.2} \\
    \bottomrule
  \end{tabular}
  \caption{Ablation studies on ATST-Clip. Linear evaluation results are shown. "Average" is taken over the last four tasks.}
  \label{tab:segments}
\end{table*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{img/length.eps}
\caption{Acc/mAP of ATST-Clip as a function of segment length, Acc/mAP of each task is normalized into the range of [0,1]. "Avg" denotes averaging over all tasks. }
  \label{fig:length}
\end{figure}

Fig. \ref{fig:length} shows the normalized performance of each task as a function of segment length, where two segments are used. We can observe that as the segment length increases, the performance metrics continue to improve until they reach a maximum at 6 seconds. This further verifies our findings: i) when Transformer encoder is used, increasing the segment length helps to learn more information; ii) when two segments are used, the segment length should be set to make the segments share a proper amount of overlap, and have a proper difficulty for matching them as the same sample.

\subsubsection{Ablations on ATST-Frame}

\begin{table*}[!t]
  \centering
  \begin{threeparttable}


    \begin{tabular}{c|cccc|ccccc|c}
      \toprule
                    & \multicolumn{4}{c|}{} & \multicolumn{5}{c|}{Downstream Tasks}       &                                                                                      \\

      \textbf{Configuration} &  \textbf{\makecell{Augmented\\views}}                                  & \textbf{\makecell{Mask\\teacher}}     & \textbf{\makecell{Mask\\student}}     & \textbf{\makecell{Mask\\strategy}} & \makecell{AS-20K\\mAP (\%)} & \makecell{SPCV2\\Acc (\%)} & \makecell{VOX1\\Acc (\%)} & \makecell{NSYNTH\\Acc (\%)} & \makecell{US8K\\Acc (\%)} & Average \\

      \midrule
      A             &  0 &            & \checkmark & Group         & 8.0    & 63.3  & 25.8 & 56.8   & 60.1     &42.8      \\
      B             &   1                       &            & \checkmark & Random        & 18.0   & 85.2  & 43.7 & 69.9   & 76.1   &  58.6      \\

      C             & 1                            & \checkmark & \checkmark & Group         & 22.5   & 88.2  & 47.0 & 72.9   & 72.9   & 60.7        \\
      D             & 1                           &            & \checkmark & Group         & 28.1   & 92.3  & \bf{67.0} & 72.5   & \bf{84.0} & \bf{68.8}    \\
      E             & 2                           &            & \checkmark & Group         &  \bf{28.5}   & \bf{92.5}  & 59.6 & \bf{74.7}   & 83.4 & 67.7    \\
      \bottomrule
    \end{tabular}
    \caption{Ablation studies on ATST-Frame. Linear evaluation results are shown. }
    \label{tab:abframecom}
  \end{threeparttable}

\end{table*}

\begin{table*}[ht]

  \centering
  \begin{tabular}{l|ccc|ccccc|c}
    \toprule
    \textbf{Method}                        & \textbf{Symmetrical} & \textbf{Loss}  & \textbf{\makecell{Augmented \\branch}}   &   \makecell{AS-20K\\mAP (\%)}    & \makecell{SPCV2\\Acc (\%)}   & \makecell{VOX1\\Acc (\%)}
                                           & \makecell{NSYNTH                                                                                                                                \\Acc (\%)} & \makecell{US8K\\Acc (\%)}&Average  \\
    \midrule
    \textbf{\multirow{3}{*}{ATST-Frame}} & True     &      & Teacher \& Student                          & 28.1          & 92.3          & 67.0          & 72.5          & 84.0          & 68.8          \\
                                           &  False    &   & Teacher                           & 6.4          & 77.8          &   18.0         & 63.0          & 67.1          & 46.7         \\
                                           & False      &    &    Student                           & 22.5          & 87.2 &  50.2 &        68.7        & 80.1          & 61.7          \\
    \bottomrule
  \end{tabular}
  \caption{Ablation studies on the symmetrical loss of ATST-Frame.  "Augmented branch" denotes the branch taking as input the augmented view. Linear evaluation results are shown. }
  \label{tab:nonsymmetric}
\end{table*}

\begin{table*}[ht]

  \centering
  \begin{tabular}{l|cc|ccccc|c}
    \toprule
    \textbf{Method}                        & \textbf{Strategy} & \textbf{\makecell{Frequency \\ warping}}   &   \makecell{AS-20K\\mAP (\%)}    & \makecell{SPCV2\\Acc (\%)}   & \makecell{VOX1\\Acc (\%)}
                                           & \makecell{NSYNTH                                                                                                                                \\Acc (\%)} & \makecell{US8K\\Acc (\%)}&Average  \\
    \midrule
    \textbf{\multirow{3}{*}{ATST-Frame}} & Frame-wise         &   True                          & 28.1          & 92.3          & 67.0          & 72.5          & 84.0          & 68.8          \\
                                           &  Frame-wise              &  False                           & 23.5          & 89.4          &  56.9        & 69.0          &   79.7       & 63.7         \\
                                           &  Patch-wise              &  True                           & 16.3          & 77.8          &  35.9        & 71.6          & 75.2          & 55.4         \\
                                           & Patch-wise            & False                           & 23.3          & 80.8 & 48.6 & 70.2                & 79.5          & 60.5         \\
    \bottomrule
  \end{tabular}
  \caption{Ablation studies on comparison of frame-wise and patch-wise strategy. Linear evaluation results are shown. }
  \label{tab:patchwise}
\end{table*}


\begin{table*}[ht]

  \centering
  \begin{tabular}{l|cc|ccccc|c}
    \toprule
    \textbf{Method}                        & \textbf{Symmetrical} & \textbf{\makecell{Augmented \\branch}}   &   \makecell{AS-20K\\mAP (\%)}    & \makecell{SPCV2\\Acc (\%)}   & \makecell{VOX1\\Acc (\%)}
                                           & \makecell{NSYNTH                                                                                                                                \\Acc (\%)} & \makecell{US8K\\Acc (\%)}&Average  \\
    \midrule

    \textbf{ATST-Frame} & True         &   Teacher \& Student                          & 28.1          & 92.3          & 67.0          & 72.5          & 84.0          & 68.8          \\
    \midrule
    \textbf{\multirow{3}{*}{ATST-Frame-data2vec}} & False        &   None                      & 24.1          & 92.0          & 58.4         & 73.0         & 81.4          & 65.8          \\
                                           &  False              &  Student                          & 18.6          & 89.3          & 43.7         & 71.5          & 76.0          & 59.8         \\
                                           & True            & Teacher \& Student                           & 21.6          & 90.7 &  52.3 &       70      &  78.4          & 62.6         \\
    \bottomrule
  \end{tabular}
  \caption{Ablation studies on comparison with data2vec-style training target. "ATST-Frame-data2vec" denotes ATST-Frame with data2vec-style\cite{baevski_data2vec_2022} training target. "Augmented branch" denotes the branch takes as input the augmented view. Linear evaluation results are shown. }
  \label{tab:data2vec}
\end{table*}



Table \ref{tab:abframecom} shows the ablation results on the effectiveness of ATST-Frame components. 

\textbf{Data augmentation:} Compared with the no augmentation case (configuration A in Table \ref{tab:abframecom}), using data augmentation (configuration B, C, D, E) brings a significant performance improvement on all tasks. This means data augmentation is able to properly increase the task difficulty and to encourage the model to learn more meaningful audio representations. Augmenting two views (for both teacher and student branches, configuration E) leads to a large task difficulty, and achieves better results on three out of five tasks. Only augmenting one view (for either student or teacher branch, configuration D) achieves slightly worse performance than augmenting two views on AS-20K and SPCV2, but much better performance on VOX1, thus has a better average result. This is consistent with our observations in the ablation studies of ATST-Clip that better performance can be achieved with a balanced difficulty of the pre-training task. 

\textbf{Masking:}
In configuration C, both the student and teacher branches are masked with the same time index, thus the two branches need to predict the masked frames from unmasked frames, and the predictions should be matched.
In configuration D, only the student branch is masked, while the teacher branch sees the whole audio clip. We can see that masking both branches performs worse than masking only the student branch. The possible reasons are i) the teacher branch provides more meaningful guidance for the student branch when seeing the frames that are not visible to the student branch; ii) the teacher encoder consistently sees unmasked input for pre-training and downstream tasks. As for the masking strategy, group masking that forces N adjacent frames to be masked together performs better than random masking \addnote[ConfigurationB]{1}{(Configuration B)}. This is consistent with the observations in the speech pre-training works\cite{baevski_wav2vec_2020,baevski_data2vec_2022}.

Based on the above analysis, the proposed ATST-Frame is set up with configuration D. Unless noted, the following experiments of ATST-Frame use configuration D by default. 

\addnote[symmetricloss]{1}{\textbf{The symmetrical loss:} In ATST-Frame, augmentation is applied to one of the two views. As the symmetrical loss is used, both the teacher branch and the student branch see the augmented view during training. We conduct experiments by using only  or  to evaluate which one of teacher and student branches is more important to see the augmented view. 
The results are shown in Table \ref{tab:nonsymmetric}, which shows that it is more important for the student branch than the teacher branch to see the augmented view, and using the symmetrical loss outperforms the case that only one branch sees the augmented view. }

\addnote[patch-wise]{1}{\textbf{Patch-wise strategy and frequency warping:} ATST-Frame uses frame-wise strategy for log-mel spectrogram, while other works\cite{gong_ssast_2022,baade_mae-ast_2022} have reported that patch-wise strategy exhibits better performance than frame-wise strategy on sound event/scene classification task, as sound events/scenes have complex frequency
structure, which can be better captured by the frequency split
of patch-wise models \cite{gong_ssast_2022}.
To testify the frame-wise strategy of our ATST-Frame model, we further conduct experiments using patch-wise strategy in the framework of ATST-Frame. Specifically, we organize the log-mel spectrogram into patches in the size of 16 frequency bins  16 frames, which leads to the same number of tokens as ATST-Frame for a 64-bin log-mel spectrogram. For patch-wise models, frequency warping conflicts with the principle of the patch-wise loss, as it distorts the patch correspondences. Therefore, we conducted experiments both with or without using frequency warping. Note that Mixup is always used. The results are shown in \ref{tab:patchwise}. Without using frequency warping, the frame-wise model noticeably performs better than the patch-wise model on speech tasks (SPCV2 and VOX1), which is consistent with the observations in other works\cite{gong_ssast_2022,baade_mae-ast_2022}. However, we do not observe the advantage of patch-wise strategy on sound event/scene classification (AS-20K and US8K), where patch-wise strategy and frame-wise strategy are comparable. The frame-wise model significantly benefits from frequency warping for all tasks whereas the patch-wise model does not. Frequency warping (FW) encourages learning FW-invariant representations, which may help to learn the spectral pattern, even for the complex spectral structure of sound events/scenes. 
Overall, within the framework of ATST-Frame, the frame-wise strategy is suitable for both speech and sound events/scenes, and frequency warping helps to largely improve the performance. 
}

\addnote[data2vec]{1}{\textbf{Using the training target of data2vec:} 
We apply the training target of data2vec \cite{baevski_data2vec_2022} to our ATST-Frame model (referred to as ATST-Frame-data2vec). Specifically, the last 8 blocks of the teacher encoder are averaged to form the training target; the projectors are removed; the predictor is replaced with a linear projection. Although the original data2vec does not use data augmentation and symmetrical loss, 
we also test how will data augmentation and symmetrical loss perform when used to ATST-Frame-data2vec. The results are reported in Table \ref{tab:data2vec}. It can be seen that the data2vec target performs well, but it does not benefit from data augmentation. }









\subsection{Results on Clip-level Downstream Tasks}

\subsubsection{Linear Evaluation Results}

Table \ref{tab:le_result} shows the linear evaluation results on six tasks.
For a fair comparison, we compare with other methods that also use Audioset for pre-training and have also reported the linear evaluation results in their papers, including TRILL \cite{shor2020towards}, COLA \cite{saeed_contrastive_2020}, BYOL-A \cite{niizumi_byol_2021}, BYOL-A-v2\cite{niizumi_byol_2023}, SF NFNET-F0\cite{wang_towards_2022} and M2D\cite{niizumi_masked_2023}. 
The proposed ATST-Clip is developed based on BYOL-A and BYOL-A-V2, using a Transformer encoder and a new view creation strategy. It can be seen that ATST-Clip noticeably outperforms BYOL-A and BYOL-A-V2 on all tasks, which indicates that our modifications are very effective. On average, the proposed models and the recently proposed M2D model perform better than other models. The performance of the proposed models and M2D are comparable, as M2D performs better on SPCV2, NSYNTH and US8K with small advantages, while the proposed ATST-Frame performs better on VOX1. Among the two proposed models, ATST-Clip outperforms ATST-Frame for all the tasks except for VOX1. ATST-Clip is dedicated to learning clip-level representation, its embedding is more representative for the audio clip than the one obtained by averaging the frame-level embeddings of ATST-Frame. However, the drawbacks of ATST-Frame are not significant., which means the average of its frame-level embeddings is still an effective clip-level representation. 





\def\a{true}
\def\b{false}
\setlength\tabcolsep{3pt}
\begin{table}[t]

  \centering

  \footnotesize
  \scalebox{0.85}{
    \begin{threeparttable}
      \begin{tabular}{l|cccccc}
        \toprule
        \textbf{Method}                    & \makecell{AS-20K                                          \\mAP (\%)}   & \makecell{SPCV2\\Acc (\%)}    & \makecell{VOX1\\Acc (\%)}
                                           & \makecell{NSYNTH                                          \\Acc (\%)} & \makecell{US8K\\Acc (\%)} & \makecell{FSD50K \\ mAP (\%)} \\
        \midrule
        TRILL \cite{shor2020towards}       & -                & -      & 17.9   & -      & -   & -        \\
        COLA \cite{saeed_contrastive_2020} & -                &  &  &  & -     & -      \\
        BYOL-A \cite{niizumi_byol_2021}    & -                &  &  &  & 79.1        \\
        BYOL-A-V2 \cite{niizumi_byol_2023} & -                & 93.1   & 57.6   & 73.1   & 79.7 & 44.8 \\
        SF NFNet-F0\cite{wang_towards_2022} & - & 93.0 & 64.9 & \bf{78.2} & - \\
        M2D\cite{niizumi_masked_2023} &- &\textbf{95.4} & 73.1 & 76.9 & \textbf{87.6} & - \\
        \midrule
{ATST-Clip (ours)}          & \bf{33.8}             & 95.1   & 72.0   & 76.2   & 85.8 & \bf{58.5}
        \\



        {ATST-Frame (ours)}         & 33.0             & 94.9   & \bf{77.4}   & 75.9   & 85.8 & 55.1
        \\




        \bottomrule
      \end{tabular}



    \end{threeparttable}
  }




  \caption{ Linear evaluation results on clip-level downstream tasks. The scores of comparison models are quoted from their papers. }
  \label{tab:le_result}
\end{table}

\begin{table*}[ht]

  \centering
  \footnotesize
  \scalebox{1.0}{
    \begin{threeparttable}
      \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Method}                                & \# Param  & \makecell{Pre-training\\data} & \makecell{AS-2M                                 \\ mAP (\%)} & \makecell{AS-20K\\mAP (\%)}    & \makecell{SPCV2\\Acc (\%)}   & \makecell{VOX1\\Acc (\%)} & \makecell{FSD50K \\ mAP (\%)} & \makecell{NSYNTH \\ Acc (\%)} 
        \\
        \midrule
        \multicolumn{5}{l}{\textbf{Supervised Methods} }                                                             \\
        PANN \cite{kong2020panns}                       & 81M &         & 43.9            & 27.8 & -      & -      & -  & -  \\


        PSLA \cite{pascual_learning_2019}                &14M &         & 44.4            & 31.9 & -      & -      & 55.4 & - \\

        AST \cite{gong_ast_2021}                         & 86M&         & 45.9            & 34.7 & 98.1   & -      & -   & - \\
        HTS-AT \cite{chen_hts-at_2022}  & 31M & & 47.1 & - & 98.0 &  - & - & - \\
        PassT \cite{koutini_efficient_2022} & 86M& & 47.1 & - & -& - & 65.3 & -  \\
        KD-AST \cite{gong_cmkd_2022}         &  86M          &         & 47.1            & -    & -      & -      & 62.9 & - \\
        \midrule[0.2pt]
        \multicolumn{5}{l}{\textbf{Self-supervised Methods} }                                                        \\

  
        SSAST-PATCH\cite{gong_ssast_2022} & 89M & AS+LS & - & 31.0 & 98.0 & 64.2 & - & -  \\
        SSAST-FRAME\cite{gong_ast_2021} &89M & AS+LS &- & 29.2 &98.1 & 80.8 & - & - \\
        Conformer-Based\cite{srivastava_conformer-based_2022} &88M & 67K hours *        & 41.5               & 27.6 & -      & -   &-  & -       \\


        MAE-AST-PATCH\cite{baade_mae-ast_2022}    &86M & AS+LS   & -               & 30.6 & 97.9   & -   & -  & -    \\
        MAE-AST-FRAME\cite{baade_mae-ast_2022}    &86M & AS+LS   & -               & 23.0 & 98.0   & 63.3   & -  & -   \\
        ASiT\cite{atito_asit_2022}                      &85M & AS      & -               & 35.2 & \textbf{98.8}   & 63.1  & -   & -     \\
        data2vec\cite{baevski_data2vec_2022} & 94M & AS & -&34.5&-&-&- & -\\
        MaskSpec\cite{10095691}  & 86M & AS & 47.1 & 34.7 & 97.6 & - & - & -  \\
        MSM-MAE\cite{niizumi_masked_nodate}  & 86M & AS & - & 36.7 & 98.4 & 95.3 &  -  & - \\

        Audio-MAE (local) \cite{huang_masked_2023} &86M & AS      & 47.3            & 37.0 & 98.3   & 94.8   & - & -     \\
        BEATs \cite{chen_beats_2022}          & 90M& AS      & 48.0            & 38.3 & 98.3   & -      & -   & -  \\        
       
                       BEATs \cite{chen_beats_2022} **          & 90M& AS      & 48.6            & 38.9 & 98.1  & -      & -   & -   \\
        M2D\cite{niizumi_masked_2023}                   & 86M & AS      & -               & 37.4 & 98.5   & 94.4   & -  & -  \\

        \midrule

\multicolumn{5}{l}{\textbf{Ours} } \\                              
        {ATST-Clip }              & 86M         & AS      & 45.2            & 37.9 & 98.0   & 95.5   & 63.4 & 78.6 
        \\
\vspace{1mm}
        {ATST-Frame }             &   86M       & AS      & 48.0            &39.0 & 98.1   & 97.3   & 61.8 & 79.2 \\
        
        


    
         
         {ATST-C2F} ** &86M & AS & \bf{49.7} & \bf{40.5} & 98.4& \bf{97.5} & \bf{65.5} & 79.2  \\
         
        \textcolor{mygray}{ATST-F2C **}  &\textcolor{mygray}{86M} & \textcolor{mygray}{AS} & \textcolor{mygray}{46.8} & \textcolor{mygray}{39.0} & \textcolor{mygray}{98.1}& \textcolor{mygray}{95.5} & \textcolor{mygray}{64.6} & \textcolor{mygray}{\bf{79.8}}  \\
       


        \bottomrule
      \end{tabular}
      \begin{tablenotes}
\item{*} Self-hold dataset\cite{srivastava_conformer-based_2022}.
      \item{} Results are quoted from M2D\cite{niizumi_masked_2023}.
      \item{**} Perform knowledge distillation across two models at the finetuning stage.
      \end{tablenotes}


    \end{threeparttable}
  }
  \caption{ Finetuning results on clip-level downstream tasks. The scores of comparison models are quoted from their papers. AS and LS denote AudioSet and Librispeech\cite{panayotov2015librispeech}, respectively.  }
  \label{tab:finetune_result}
\end{table*}

\subsubsection{Fine-tuning Results}

Linear evaluation cannot fully reflect the capabilities of pre-trained models, as normally the models can be further fine-tuned with the data  of downstream tasks. Fine-tuning experiments are conducted on the tasks of multi-label audio event classification (AS-2M, AS-20K and FSD50K), Spoken command recognition (SPCV2), speaker identification (VOX1) and musical instrument classification (NSYNTH). We compare with two groups of prior methods: supervised methods and self-supervised methods. The results are shown in Table \ref{tab:finetune_result}. 

\textbf{ATST-Frame outperforms ATST-Clip.} After fine-tuning, the performance of ATST-Frame is better than ATST-Clip on five out of six tasks. As mentioned above, ATST-Frame does not explicitly learn clip-level representation during pre-training. However, fine-tuning allows the adjustment of the pre-trained parameters to fit a specific downstream task. 
ATST-Frame is pre-trained by maximizing the agreement of frame-level embeddings, which is more fine-grained and challenging compared with ATST-Clip. This may help ATST-Frame to learn more sophisticated knowledge and network parameters (such as the self-attention parameters), which happen to be a better initial setting for fine-tuning even on clip-level downstream tasks.

\textbf{Comparison with supervised methods.} The proposed ATST-Frame outperforms the supervised methods on AS-2M, AS-20K and SPCV2. The proposed ATST-C2F model further improves the performance, and outperforms the supervised methods on all tasks. This is encouraging for the field of audio self-supervised learning, as we no longer need to annotate audio data for pre-training when we want to further scale up the dataset. Compared with supervised pre-training, self-supervised pre-training does not suffer from the problem of inaccurate and erroneous labels.


\textbf{Comparison with other self-supervised methods.} Compared with other self-supervised methods, the proposed ATST-Frame achieves comparable or better performance on all tasks. In particular, compared with the recent state-of-the-art self-supervised method BEATs\cite{chen_beats_2022}, ATST-Frame achieves the same performance on AS-2M, and better performance on AS-20K. This indicates that ATST-Frame is more effective with less fine-tuning data than BEATS. 

\textbf{Combination through knowledge distillation.} 
ATST-Clip and ATST-Frame learn complementary features in the pre-training stage. Combining ATST-Clip and ATST-Frame through knowledge distillation can further improve the performance, as shown by the results of ATST-C2F in Table \ref{tab:finetune_result}. Even though ATST-Clip performs worse than ATST-Frame on AS-2M and AS-20K, as a teacher, it still successfully helps to fine-tune ATST-Frame to achieve better performance. 
Performing knowledge distillation the other way around, i.e. from ATST-Frame to ATST-Clip, does not perform well on most of the tasks. 
\addnote[sophisticated]{1}{
This verifies that ATST-Frame conveys more fine-grained and semantically complicated information than ATST-Clip
}, and ATST-Frame should be used as the final model.
BEATs\cite{chen_beats_2022} also performs knowledge distillation across models at the fine-tuning stage, specifically, it uses the fine-tuned BEATs model as a teacher to fine-tune the final BEATs model. Thence, BEATs can be regarded as a fair comparison with the proposed ATST-C2F model.


\subsection{Results on Frame-level Downstream Task - DESED}
\label{sec:dcase}

\begin{table}[h]
  \centering
  \footnotesize
  \scalebox{1}{
    \begin{threeparttable}
        \begin{tabular}{lcccc}
        \toprule 
        \textbf{Method} &  (ms) & \textbf{learning rate} & \textbf{} & \textbf{}\\
        \midrule
        \multicolumn{5}{l}{\textbf{Linear Evaluation}} \\
        BYOL-A-V2-40ms \cite{niizumi_byol_2023}      & 40 & 0.01 & 0.024 & 0.181  \\
        SSAST-FRAME-20ms \cite{gong_ssast_2022}      & 20 & 0.1 & 0.028 & 0.166 \\
        SSAST-FRAME-40ms \cite{gong_ssast_2022}      & 40 & 0.1 & 0.096 & 0.266 \\
        SSAST-PATCH     \cite{gong_ssast_2022}       & 160 & 0.1 & 0.179 & 0.315  \\
        MAE-AST-FRAME-20ms \cite{baade_mae-ast_2022} & 20 & 0.1 & 0.031 & 0.234 \\
        MAE-AST-FRAME-40ms \cite{baade_mae-ast_2022} & 40 & 0.1 & 0.081 & 0.293 \\
        MAE-AST-PATCH   \cite{baade_mae-ast_2022}    & 160 & 0.1 & 0.225 & 0.442 \\
        Audio-MAE (local) \cite{huang_masked_2023}   & 160 & 0.1 & 0.218 & 0.401 \\
        BEATs \cite{chen_beats_2022}       & 160 & 0.1 & 0.177 & 0.358 \\
        M2D \cite{niizumi_masked_2023}               & 160 & 0.1 & 0.234 & 0.438 \\
        \midrule
        \multicolumn{5}{l}{\textbf{Ours}} \\
        ATST-Clip                            & 40 & 0.1 & 0.115 & 0.293   \\
        ATST-Frame                           & 40 & 0.1 & \textbf{0.304} & \textbf{0.507} \\
        \midrule
        \midrule
        \multicolumn{5}{l}{\textbf{Finetuning}} \\
        BYOL-A-V2-40ms \cite{niizumi_byol_2023}      & 40 & 0.01 & 0.030 & 0.219 \\
        SSAST-FRAME-20ms \cite{gong_ssast_2022}      & 20 & 0.05 & 0.046 & 0.235 \\
        SSAST-FRAME-40ms \cite{gong_ssast_2022}      & 40 & 0.1 & 0.132 & 0.325 \\
        SSAST-PATCH     \cite{gong_ssast_2022}       & 160 & 0.1 & 0.236 & 0.459 \\
        MAE-AST-FRAME-20ms \cite{baade_mae-ast_2022} & 20 & 0.05 & 0.123 & 0.346 \\
        MAE-AST-FRAME-40ms \cite{baade_mae-ast_2022} & 40 & 0.1 & 0.235 & 0.418 \\
        MAE-AST-PATCH   \cite{baade_mae-ast_2022}    & 160 & 0.05 & 0.281 & 0.573 \\
        Audio-MAE (local) \cite{huang_masked_2023}   & 160 & 0.1 & 0.254 & 0.509 \\
        BEATs \cite{chen_beats_2022}       & 160 & 0.1 & 0.282 & 0.584 \\
        M2D \cite{niizumi_masked_2023}               & 160 & 0.1 & 0.267 & 0.500 \\
        \midrule
        \multicolumn{5}{l}{\textbf{Ours}} \\
        ATST-Clip                          & 40 & 0.05 & 0.223 & 0.422      \\
        ATST-Frame                         & 40 & 0.01 & \textbf{0.361} & 0.581 \\
        ATST-C2F                         & 40 & 0.1 & 0.357 & \textbf{0.607}  \\
        \textcolor{mygray}{ATST-F2C}                         & \textcolor{mygray}{40} & \textcolor{mygray}{0.1} & \textcolor{mygray}{0.259} & \textcolor{mygray}{0.445} \\
        \bottomrule
        \end{tabular}     
    \end{threeparttable}
  }
  \caption{Results on the frame-level downstream task, DESED.  {} stands for temporal resolution.}
  \label{tab:linear_SED}
\end{table}










\subsubsection{Comparison Methods}

We compare with six SSL pre-trained models: BYOL-A-v2 \cite{niizumi_byol_2023}, SSAST \cite{gong_ssast_2022}, MAE-AST \cite{baade_mae-ast_2022}, Audio-MAE \cite{huang_masked_2023}, BEATs \cite{chen_beats_2022} and M2D \cite{niizumi_masked_2023}. 
Sound event detection requires to perform frame-level multi-class classification. As mentioned in Section \ref{sec:sedsetup}, the proposed models can be directly used for this task by adding a linear classifier on top of their frame-level representations, with a temporal resolution of 40 ms per frame. The comparison models are pre-trained either frame-wisely or patch-wisely. The frame-wise models, e.g. SSAST and MAE-AST, can also be directly used for this task, with a temporal resolution of 20 ms per frame. According to the setup of the proposed models, we also evaluate SSAST and MAE-AST with a temporal resolution of 40 ms per frame, by applying average pooling to the frame-level representations.  
As for the patch-wise models, we average the patch-level representations for each time interval to obtain the interval/frame-level representations, except for M2D, since its authors propose to concatenate instead of average the patch-level representations \cite{niizumi_masked_2023}. 
Note that, the patch-wise models have a coarser temporal resolution, i.e. 160 ms per frame.



All the pre-trained models are fine-tuned by ourselves using the SED supervised dataset. For linear evaluation, pre-trained models are frozen, and only the two dense layers of the linear classifier are trained. In fine-tuning experiments, for all the Transformer-based models, we unfreeze the last Transformer block. For BYOL-A-v2, we unfreeze the entire model for fair comparison such that the amount of the trainable parameters of each model are similar. The best learning rate for each model has been carefully searched, which is also given in Table \ref{tab:linear_SED}.

\subsubsection{Results Analysis}

\addnote[DESEDTraining2]{1}{Table \ref{tab:linear_SED} shows the results. These results are deviated from the results reported in the DCASE challenge, that is because of the different experimental setups as discussed in Sec.~\ref{sec:DESEDTraining}. The objective of this study is to conduct fair  comparison between different SSL models, instead of pursuing the SOTA performances on this dataset.} It can be seen that, as expected, relative to linear evaluation, the performance of all models can be improved by fine-tuning with the SED dataset. As the fine-tuning setup is more practically important than linear evaluation, we mainly analyze the fine-tuning results in the following, and most of the analyses are valid for the linear evaluation results as well. 

BYOL-A-V2 does not achieve reasonable performance, possibly due to its limited capacity for sequential processing with a two-layer CNN architecture.
For SSAST \cite{gong_ssast_2022} and MAE-AST \cite{baade_mae-ast_2022}, increasing the temporal resolution of their frame-wise models from 20 ms to 40 ms largely improves the performance. The possible reasons are that the frame-level representations get more stable when averaging two frames, and meanwhile, the 40 ms temporal resolution is still fine enough for tracking the time variation of sound events. This could also be because the characteristics of one event cannot be well represented without sufficiently long frames. However, the performances of their frame-wise models still largely lag behind their patch-wise models. This is consistent with the observations in \cite{gong_ssast_2022,baade_mae-ast_2022} that, the patch-wise models are more suitable for sound events, while the frame-wise models are more suitable for speech signals. Sound events have more complex frequency structure, which can be better captured by the frequency split of patch-wise models. Among the comparison models, BEATs performs the best in terms of both  and , and MAE-AST-PATCH achieves close performance with BEATs. 

The proposed ATST-Clip does not work well, as it is trained for learning global representation, which does not automatically lead to good frame-level representations. By leveraging the proposed frame-level training criterion and thus learning better frame-level representations, ATST-Frame largely improves the performance over ATST-Clip.  Compared with the best comparison model, i.e. BEATs, ATST-Frame achieves much better , and similar . 
 emphasizes the time localization accuracy of sound events, thence the better  of ATST-Frame means a better temporal detection performance, which is possibly due to the finer temporal resolution of ATST-Frame compared with BEATs, i.e. 40 ms versus 160 ms. 
 emphasizes the recognition accuracy of sound events. The similar  of ATST-Frame and BEATs reflect the similar representation quality of them. This is consistent with the results on the clip-level AS-2M task, ATST-Frame also performs similarly with BEATs as shown in Table \ref{tab:le_result}. It is important to note that, the good performance of ATST-Frame conflicts with the observations in \cite{gong_ssast_2022,baade_mae-ast_2022} that patch-wise models are more suitable for sound events than frame-wise models. As discussed in the ablation study, the success of ATST-Frame is possibly attributed to the frequency warping operation, which helps to capture the complex frequency structure of sound events. 








Knowledge distillation is also applied to combine ATST-Clip and ATST-Frame. The results of ATST-C2F show that, taking ATST-Clip as a teacher for fine-tuning ATST-Frame,  can be further improved, while  is slightly decreased. This means the knowledge learned by ATST-Clip  is still complementary for improving the accuracy of frame-level representations, but will slightly blur the time localization.    



\begin{table}[tb]
  \centering
  \footnotesize
  \scalebox{1}{
      \begin{tabular}{l|c|cc|cc}
        \toprule 
        \multirow{3}{*}[-0.5em]{\textbf{Method}} & 
        \multirow{3}{*}[-0.3em]{\textbf{\makecell{Learning\\ rate}}} & 
        \multicolumn{2}{c|}{\textbf{}} &
        \multicolumn{2}{c}{\textbf{}} \\
        \cmidrule{3-6}
        &
        & w/o  & with
        & w/o  & with \\          
        &
        & var-pen & var-pen
        & var-pen & var-pen \\
        \midrule 
        \multicolumn{3}{l}{\textbf{Linear Evaluation}} \\
        BYOL-A-V2-40ms \cite{niizumi_byol_2023}      & 0.5 & 0.087 & 0.0 & 0.083 & 0.0 \\
        SSAST-PATCH     \cite{gong_ssast_2022}       & 0.5 & 0.048 & 0.0 & 0.067 & 0.0 \\
        MAE-AST-PATCH   \cite{baade_mae-ast_2022}    & 0.5 & 0.116 & 0.0 & 0.185 & 0.0 \\
        Audio-MAE (local) \cite{huang_masked_2023}   & 0.5 & 0.073 & 0.0 & 0.107 & 0.0 \\
        BEATs \cite{chen_beats_2022}       & 0.5 & 0.034 & 0.0 & 0.062 & 0.0 \\
        M2D \cite{niizumi_masked_2023}               & 0.5 & 0.182 & 0.0 & 0.301 & 0.039 \\
        \midrule
        \multicolumn{6}{l}{\textbf{Ours}} \\
        ATST-Clip  & 0.5 & 0.120 & 0.0 & 0.201 & 0.001 \\
        ATST-Frame & 0.5 & \textbf{0.207} & \textbf{0.008} & \textbf{0.304} & \textbf{0.048} \\
        \midrule
        \midrule
        \multicolumn{6}{l}{\textbf{Finetuning}} \\
        BYOL-A-V2-40ms \cite{niizumi_byol_2023}      & 0.5 & 0.110 & 0.0 & 0.243 & 0.027 \\
        SSAST-PATCH     \cite{gong_ssast_2022}       & 0.5 & 0.243 & 0.017 & 0.411 & 0.122 \\
        MAE-AST-PATCH   \cite{baade_mae-ast_2022}    & 0.5 & 0.274 & 0.039 & 0.481 & 0.187 \\
        Audio-MAE (local) \cite{huang_masked_2023}   & 0.5 & 0.276 & 0.038 & 0.476 & 0.182 \\
        BEATs \cite{chen_beats_2022}       & 0.5 & 0.290 & 0.045 & 0.491 & 0.186 \\
        M2D \cite{niizumi_masked_2023}               & 0.1 & 0.292 & 0.042 & 0.509 & 0.199 \\
        \midrule
        \multicolumn{6}{l}{\textbf{Ours}} \\
        ATST-Clip  & 0.5 & 0.328 & 0.083 & 0.478 & 0.178 \\
        ATST-Frame & 0.5 & 0.347 & 0.069 & 0.538 & 0.152 \\
        ATST-C2F   & 0.5 & \textbf{0.374} & \textbf{0.125} & \textbf{0.572} & \textbf{0.266} \\
        \textcolor{mygray}{ATST-F2C} & \textcolor{mygray}{0.5} & \textcolor{mygray}{0.323} & \textcolor{mygray}{0.075} & \textcolor{mygray}{0.470} & \textcolor{mygray}{0.163} \\
        \bottomrule
        \end{tabular}
    }
  \caption{Results on the frame-level downstream task, SED of the strongly-labeled AudioSet. `var-pen' stands for the performance variance penalty term.   } 
  \label{tab:audioset_strong}
\end{table}

\subsection{Results on Frame-level Downstream Task - Strongly-labeled AudioSet}
\label{sec:audioset_strong}

Table \ref{tab:audioset_strong} shows the results on the strongly-labeled AudioSet. According to the DESED performances, only the best-performing model for each comparison method is evaluated. 
Considering the large data size of strongly-labeled AudioSet, we only search over 3 different learning rates for each model, i.e. 0.05, 0.1 and 0.5. 
As mentioned in Sec.~\ref{sec:frame_metric}, we evaluate the models by the PSDSs with or without applying the performance variance penalty term. 

For all the models, the scores with variance penalty are much lower than the ones without variance penalty, which means the performance variance across classes for all models are very large. The scores with variance penalty for linear evaluation could be reduced to 0 for most of the models. This reflects the data imbalance and task difficulty of the strongly-labeled AudioSet. 


After finetuning, The BYOL-A-v2 model has a large performance gap comparing with other Transformer-based models. 
With better learned frame-level representations, the proposed ATST-Frame model has an obvious advantage over the comparison models and ATST-Clip, when variance penalty is not applied. However, ATST-Clip has a better stability of performance across classes, and thus outperforms ATST-Frame when applying variance penalty. When combining ATST-Frame and ATST-Clip, the performance measures are largely improved by ATST-C2F, and the model is improved in terms of both classification accuracy and performance stability.  




\subsection{Results on HEAR benchmark} We also evaluate the proposed models on the HEAR benchmark \cite{turian_hear_2022}, which includes 17 clip-level and 2 frame-level tasks. We successfully downloaded 18 tasks. The Hear benchmark trains a shallow MLP classifier on top of frozen embeddings. We use the official hear-eval-kit \footnote{https://github.com/hearbenchmark/hear-eval-kit}, and our embeddings are extracted in the same way as we did in our linear evaluation experiments except that for frame-level tasks, we concatenate outputs of all the blocks. 
Table \ref{tab:hear} shows the results. As a baseline, we quote the best result for each task from the HEAR leaderboard \footnote{https://hearbenchmark.com/hear-leaderboard.html}, denoted as `Best' in the table. \addnote[hear]{1}{It is worth noting that there are two frame-level tasks, i.e. DCASE 2016 and Maestro 5h. On DCASE 2016, both ATST-Clip and ATST-Frame perform better than the best baseline. However, the best baseline performs much better than the proposed models for Maestro 5h.  Maestro 5h is a piano music transcription task, aiming to extract pitch and onset from raw audio. The data augmentation of RRC and frequency warping in ATST encourage the model to learn frequency-changing-invariant representations, which may be not suitable for pitch learning, as pitch is sensitive to frequency changing. 
This phenomenon is also observed on the clip-level pitch estimation task, i.e. NSynth Pitch 5h.} Overall, both the proposed ATST-Clip and ATST-Frame achieve better performance than the best baseline on  five tasks. This is remarkable considering the fact that the best baseline results quoted here for different tasks are achieved by 14 different submissions. Moreover, some of the best baseline results are obtained by the model especially trained for the specific tasks, as HEAR benchmark does not limit the pre-training methods (supervised or unsupervised) and pre-training datasets. 

\begin{table}[!t]
  \centering
  \begin{threeparttable}


    \begin{tabular}{l|ccc}
      \toprule
                         & ATST-Clip & ATST-Frame & Best \\ 
      \midrule
      Beehive                    & 58.3       &    64.6  &  87.8  \\ 
      Beijing Opera              & 95.3          & 95.8   & 97.5 \\ 
      CREMA-D                   & \underline{76.0}            &\underline{76.7} & 75.2  \\ 
      DCASE 2016 *               & \underline{93.7}             &\underline{95.7}  & 92.5 \\
      ESC-50                    & 91.2        & 89.0   & 96.1 \\  
      FSD50K                     & 59.5         & 55.7 & 64.1 \\ 
      Gunshot                    & \underline{98.8}         &94.3  & 96.7  \\ 
      GTZAN Genre                & 87.7           & 88.3  & 90.8 \\ 
      GTZAN Music/Speech            & 99.2          &\underline{ 100.0} & 99.2\\ 
      Libricountl                & 78.2         & 78.1  & 78.5 \\ 
      Maestro 5h *                & 18.9         &24.4 & 46.9 \\ 
      Mridangam Stroke           & \underline{97.7}            & 97.5 & 97.5\\ 
      Mridangam Tonic            & \underline{96.7}           & \underline{96.9}  & 94.1\\ 
      NSynth Pitch 5h            & 67.8         &68.6  & 87.8 \\ 
      Speech command 5h        & 93.1          & 92.6 & 97.6 \\ 
      Speech command full         & 95.5         &95.1  & 97.8 \\ 
      
      
      Vocal Imitation           & 18.5           & \underline{22.3} & 21.5 \\ 
      VoxLingua107 top 10          & 53.9            & 66.9 & 72.2 \\ 

      \bottomrule
    \end{tabular}
      \begin{tablenotes}
\item{*} frame-level task
      \end{tablenotes}
  \end{threeparttable}
  \caption{Results on the HEAR benchmark. `Best' denotes the best result in the HEAR leaderboard. Underlined scores denote better performance than `Best'. }
  \label{tab:hear}
\end{table}




