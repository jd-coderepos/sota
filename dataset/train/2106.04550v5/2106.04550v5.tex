\section{Experiments}
We first describe the implementation details and datasets used for our experimentation. We then report how \model performs on the object detection tasks when finetuned on full and low-data regimes, including few-shot learning, and semi-supervised learning. Finally, we conclude with ablations, analyses, and visualizations from \model.
\vspace{0.1cm}
\\
\noindent\textbf{Implementation.} Based on the ablations presented in \Cref{ss:analysis}, the default experiment settings are as follows (see the Suppl. for all details). We initialize the ResNet50 backbone of \model with SwAV~\cite{caron2020unsupervised}, which was pretrained with multi-crop views for 800 epochs on \imagenetthousand, and fix it throughout the pretraining stage. In the object embedding branch,  and  are MLPs with  hidden layers of size  followed by a ReLU~\cite{nair2010rectified} nonlinearity. The output sizes of  and  are  and .  is implemented as a single fully-connected layer with  outputs. Unless otherwise noted, we use the \model Top-K region selection variant (see \Cref{ss:object_localization_task}) and set  proposals per-image.
\vspace{0.1cm}
\\
\noindent\textbf{Datasets.} We use the following datasets: \textbf{ImageNet ILSRVC 2012} (\imagenetthousand) dataset contains M images with  class categories. As done in prior work~\cite{chen2020simple, xiao2021region, caron2020unsupervised, xie2021detco}, we use the unlabeled \imagenetthousand data for pretraining. Similar to other works~\cite{xiao2021region, he2019momentum,he2020momentum}, we use a subset of \imagenetthousand called \textbf{\imagenethundred} that contains 125K images and  class categories for several ablation studies.
\textbf{MS~COCO}~\cite{lin2014microsoft} is a popular object detection benchmark that contains K labeled images, where objects from 80 object categories are annotated with bounding boxes. 
\textbf{PASCAL~VOC}~\cite{everingham2010pascal} contains 20K natural images where the objects from 21 classes are annotated. 
To explore a dataset with different visual properties than the typical object-centric benchmarks, we use the \textbf{Airbus Ship Detection} dataset~\cite{airbus}, which contains 231K satellite images annotated with bounding boxes of ships. Following~\cite{nie2020attention}, we convert the segmentation masks to bounding boxes and use a 42.5K image subset, with 3K test/val splits.

\begin{table}[t]
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lcclll}
\toprule
Pretraining & Detector & Epochs & AP   &  & \\
\midrule\midrule
Supervised & \multirow{4}{*}{DETR} & \multirow{4}{*}{150} & 39.5 & 60.3 & 41.4\\
SwAV~\cite{caron2020unsupervised} &  & & 39.7 & 60.3 & 41.7\\
UP-DETR &  & & 40.5 & 60.8 & 42.6 \\
 \textbf{DETReg} &  & & \textbf{41.9}  & \textbf{61.9} & \textbf{44.1} \\
\midrule
Supervised & \multirow{4}{*}{DETR} & \multirow{4}{*}{300} & 40.8 & 61.2 & 42.9\\
SwAV~\cite{caron2020unsupervised} &  &  & 42.1 & {63.1} & 44.5\\
UP-DETR &  &  & 42.8 & 63.0 & 45.3\\
\textbf{DETReg} & &  & \textbf{43.7}  & \textbf{63.7} & \textbf{46.6} \\
\midrule
Supervised & \multirow{4}{*}{DDETR} & \multirow{4}{*}{50} & 44.5 & 63.6 & 48.7 \\
SwAV~\cite{caron2020unsupervised} &  &  & 45.2 & 64.0 & 49.5\\
UP-DETR &  &  & 44.7 & 63.7 & 48.6 \\
\textbf{DETReg} &  &  & \textbf{45.5}  & \textbf{64.1} & \textbf{49.9}\\
\bottomrule
\end{tabular}
}
\captionof{table}{\textbf{Object detection results when trained on MS~COCO \texttt{train2017} and evaluated on \texttt{val2017}}. Both DETReg and UP-DETR are pretrained on \imagenetthousand under comparable settings, while supervised and SwAV only pretrain  the backbone of the object detector. We explore both the DETR and \defdetr (DDETR) architectures; for compatibility with prior work, we finetuned the DETR for 150/300 epochs and DDETR for 50 epochs.\vspace{-2mm}}
\label{table:coco}
\end{table}


\subsection{Object Detection in Full Data Regimes}
\label{subsec:fulldata}
These experiments test how well \model performs when a fully annotated dataset is available for finetuning.
\vspace{0.1cm}
\\
\noindent\textbf{Pretraining.} We pretrain two variants of \model based on DETR~\cite{carion2020end} and \defdetr~\cite{zhu2020deformable} detectors for  and  epochs on \imagenetthousand and \imagenethundred, respectively, where the pretraining schedules are set by proportionally adjusting the schedules used in UP-DETR to equate to the more efficient \defdetr schedules~\cite{zhu2020deformable}.
\vspace{0.1cm}
\\
\noindent\textbf{Baselines.} We compare \model to several closely related state-of-the-art pretraining approaches for object detection with transformers: using a SwAV~\cite{caron2020unsupervised} backbone, a fully pretrained UP-DETR~\cite{dai2020up}, and a supervised baseline backbone.
\vspace{0.1cm}
\\
\noindent\textbf{Experiments.} To evaluate \model, we finetuned it on three different datasets: MS~COCO~\cite{lin2014microsoft}, PASCAL~VOC~\cite{everingham2010pascal}, and Airbus Ship Detection~\cite{airbus}. We perform an extensive comparison on MS~COCO and finetune using similar training schedules as previously reported in~\cite{dai2020up, zhu2020deformable}, using \texttt{train2017} for finetuning and \texttt{val2017} for evaluation. On PASCAL~VOC and Airbus we use \model \defdetr based variant, which is faster to train. On PASCAL~VOC we finetune on \texttt{trainval07+12} for  epochs, dropping the learning rate after  epochs and use the \texttt{test07} for evaluation. For Airbus, we finetune for  epochs, dropping the learning rate after  epochs. 
\vspace{0.1cm}
\\
\noindent\textbf{Results.} \Cref{table:coco} shows that \model consistently outperforms other pretraining strategies using both DETR and \defdetr. For example, \model improves the COCO AP score by 1.4 points compared to UP-DETR when trained for 150 epochs, and in fact, outperforms the  epoch supervised variant after  epochs. Interestingly, using \model pretraining with DETR is competitive with supervised \defdetr, which achieves only  points (AP) more, despite significant architectural modifications.

\begin{table}[t]
\centering
\small
\resizebox{.9\linewidth}{!}{
\begin{tabular}{lllllll}

\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{PASCAL~VOC} & \multicolumn{3}{c}{Airbus Ship} \\
 & AP & AP & AP  & AP & AP & AP\\
\midrule\midrule
Supervised & 59.5  &  82.6 &  65.6 & 79.8 & 95.8 & 89.4 \\ SwAV~\cite{caron2020unsupervised} & 61.0 & 83.0 & 68.1 & 78.3 & 95.7 & 88.7  \\
\textbf{\model} & \textbf{63.5} & \textbf{83.3} & \textbf{70.3} & \textbf{81.0}  & 95.9 & \textbf{89.7} \\
\bottomrule                            
\end{tabular} 
}
\caption{\textbf{Object detection finetuned on PASCAL~VOC and Airbus Ship data.} The model is finetuned on PASCAL~VOC \texttt{trainval07+2012} and evaluated on \texttt{test07} (left), and Airbus Ship Detection finetuned on the train split and evaluated on the 3k test images (right). All models are based on \defdetr~\cite{zhu2020deformable}. Bold values indicate an improvement  AP.
}
\label{table:pascal_main}
\end{table}

\Cref{table:pascal_main} shows that \model improves by  (AP) points over SwAV on PASCAL~VOC and by  (AP) on Airbus. For reference, by using a specialized architecture for ship detection that builds on a ResNet50 backbone, as well as leveraging the pixel-level annotations, \cite{nie2020attention} obtains a box AP score of  on this dataset,  points lower than \model, which only uses the bounding box annotations.


\subsection{Object Detection in Low-Data Regimes}
\label{sec:low_data}
These experiments test how \model performs when a small amount of annotated data is available for finetuning.
\\
\noindent\textbf{Pretraining.} We pretrain \model based on \defdetr~\cite{zhu2020deformable} for  epochs on ImageNet (\imagenetthousand). 
\\
\noindent\textbf{Baselines.} We consider recent approaches for pretraining ResNet50 backbone for object detection: InstLoc~\cite{yang2021insloc},  ReSim~\cite{xiao2021region} and SwAV~\cite{caron2020unsupervised}, for each we use the publicly released checkpoint. We report the  w.r.t. the supervised variant, which utilizes a ResNet50 pretrained on \imagenetthousand.
\\
\noindent\textbf{Experiments.} We test the representations learned by \model when transferring with limited amounts of labeled data (up to  labeled images), randomly sampled from MS~COCO \texttt{train2017} and use \texttt{val2017} for evaluation. We train all methods for up to  epochs or until the validation performance stops improving. 
\vspace{0.1cm}
\\
\noindent\textbf{Results.} \cref{fig:main_coco} shows the results, where the y-axis reports the difference in AP compared to the supervised variant. The results indicate that \model consistently outperforms other pretraining strategies, when using \defdetr on low data regime. For example, when using only  images, \model improves the average precision (AP) score by  points compared to  for SwAV and  for ReSim.
\begin{figure}[t]
\centering
\resizebox{1\linewidth}{!}{
\includegraphics[width=\textwidth]{figures/delta-figure.pdf}}
\captionof{figure}{\textbf{Model comparison in low-data regimes.}  improvement over the supervised baseline, where the x-axis shows the total number of images used during training. We fix the \defdetr architecture across all methods and finetuned it using publically released ResNet50 weights of different methods on MS~COCO \texttt{train2017} and evaluate on \texttt{val2017}. 
    }


\label{fig:main_coco}
\end{figure}


\begin{table}
\makeatletter\def\@captype{table}
    \centering
    \resizebox{.97\linewidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Detector} & \multirow{2}{*}{Backbone} &\multicolumn{2}{c}{Novel AP} & \multicolumn{2}{c}{Novel AP}\\
    & & & 10 & 30 & 10 & 30 \\ \midrule\midrule
    \multicolumn{4}{l}{\small\bf\it Methods utilizing small backbones}\\
    \midrule
    YOLO-ft-full\small{~\cite{redmon2017yolo9000,kang2019few}} & \multirow{2}{*}{YOLOv2} & DarkNet-19 & 3.1 & 1.7 & 7.7 & 6.4 \\ 
    FSRW\small{~\cite{kang2019few}} &  & DarkNet-19 & 5.6 & 9.1 & 4.6 & 7.6 \\ 
    \midrule
    FRCN-ft-full\small{~\cite{yan2019meta}} &\multirow{2}{*}{FRCN} & VGG16 & 3.3  &  7.8 & 1.9 & 6.0 \\
    MetaDet\small{~\cite{wang2019meta}} & & VGG16 & 7.1 & 11.3 & 6.1 & 8.1 \\
    \midrule
    FRCN-ft-full\small{~\cite{yan2019meta}} &\multirow{2}{*}{FRCN} & ResNet50 &  6.5 & 11.1 & 5.9 & 10.3\\
    Meta R-CNN\small{~\cite{yan2019meta}} & & ResNet50 & 8.7 & 12.4 & 6.6 & 10.8\\ \midrule
    DDETR-ft-full & \multirow{2}{*}{DDETR} & ResNet50 & 12.4 & 20.4 & 13.3 & 21.8  \\ 
    \textbf{DETReg-ft-full} & & ResNet50 & \textbf{13.7} & \textbf{22.6} & \textbf{15.1} & \textbf{24.3} \\

    \midrule
    \multicolumn{4}{l}{\small\bf\it Methods utilizing large backbones}\\
    \midrule
    FRCN-ft-full~\cite{wang2020frustratingly} &\multirow{3}{*}{FRCN}& ResNet101 & 9.2 & 12.5  & 9.2 & 12.0  \\ 
    TFA~\cite{wang2020frustratingly} & & ResNet101 & {10.0} & {13.7}  & {9.3} & {13.4}\\
    DeFRCN~\cite{zhu2020deformable} & & ResNet101 & {18.5} & \textbf{22.6}  & {-} & {-} \\
\midrule
    DDETR-ft-full~\cite{zhang2021meta} & \multirow{2}{*}{DDETR*} & ResNet101 & 11.7 & 16.3 & 12.1 & 16.7 \\
    Meta-DETR~\cite{zhang2021meta} & &  ResNet101 &  \textbf{19.0} & {22.2}  & \textbf{19.7} & {22.8}\\
    \midrule
    \textbf{DETReg-ft-full} & DDETR & ResNet50 & 13.7 & \textbf{22.6} & 15.1 & \textbf{24.3} \\
    \bottomrule
    \multicolumn{6}{l}
    \text{\footnotesize \it DDETR is the customized single scale  deformable DETR model used in ~\cite{zhang2021meta}}. 
    \end{tabular}
}
\caption{\textbf{Few-shot detection evaluation on COCO.} We trained the model on the 60 base classes and then evaluate the model performance on the 20 novel categories, following the data split used in~\cite{wang2020frustratingly}. Through simple-fine tuning, \model outperforms previous few-shot object detectors with ResNet50 backbones. Using , \model achieves similar or improved performance compared to approaches that utilize larger backbones.}
\label{table:fewshot}
\end{table}




\vspace{0.1cm}
\subsection{Few-Shot Object Detection}
These experiments test how \model extends to the few-shot settings established in existing literature.
\vspace{0.1cm}
\\
\noindent\textbf{Pretraining.} We pretrain \model based on \defdetr~\cite{zhu2020deformable} for  epochs on ImageNet (\imagenetthousand). 
\\
\noindent\textbf{Baselines.} We consider \defdetr with a supervised pretrained backbone as the most direct baseline as its architecture and training strategy mirror \model. We also report the results of recent few-shot approaches, which utilize different underlying object detectors. Concurrent to our work, Meta-DETR~\cite{zhang2021meta} proposed a new method based on \defdetr. However, unlike \model, it uses a ResNet101 backbone and a single image scale, but we include its results to encourage unified reporting, even when experimental settings are not perfectly aligned.
\vspace{0.1cm}
\\
\noindent\textbf{Experiments.} Following the standard few-shot protocol for object detection~\cite{wang2020frustratingly}, we finetune \model on the full data of 60 base classes, which contain around K labeled images. Then, we finetune on a balanced set of all 80 classes, where every class has  object instances. We use the splits from~\cite{wang2020frustratingly} and report the performance on the novel 20 classes. The results are shown in~\cref{table:fewshot}.

\cref{table:fewshot_nobase} shows an extreme few-shot setup where \model is finetuned on the balanced few-shot set without the intermediate finetuning on base classes. We consider finetuning the decoder only (\textit{ft-decoder}) and the full model (\textit{ft-full}). \vspace{0.1cm}
\\
\noindent\textbf{Results.} \cref{table:fewshot} shows that \model improves over supervised pretraining and achieves state-of-the-art -shot results compared to approaches that utilize larger backbones. \model only uses a simple fine-tuning strategy, while other methods may include more complicated episodic training. 

\cref{table:fewshot_nobase} shows that \model achieves competitive few-shot performance even when the model is not trained on the abundant base class data. As a reference point, TFA~\cite{wang2020frustratingly} is a previous fine-tuning method that trains on the abundant base class data, and we can see that \model outperforms it without additional supervision from the base class data. 



\begin{table}[t]
\makeatletter\def\@captype{table}
    \centering
    \resizebox{.85\linewidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Model} 
    & \multirow{2}{*}{Detector} &\multicolumn{2}{c}{Novel AP} & \multicolumn{2}{c}{Novel AP}\\
    & & 10 & 30 & 10 & 30 \\ \midrule\midrule
    TFA~\cite{wang2020frustratingly} (w/base) & FRCN & {10.0} & {13.7}  & {9.3} & {13.4}\\
    \midrule
DDETR-ft-full & \multirow{4}{*}{DDETR} &  6.7 &  12.5 &  6.0 &  12.4 \\
    DDETR-ft-decoder & & 5.9 &  13.6 & 4.6 & 13.6 \\
    DETReg-ft-decoder & &  7.6 & 15.7 & 7.7 & 16.6 \\
    \textbf{DETReg-ft-full} & & 9.1 & 16.5 & 9.6 & 17.3  \\

    \bottomrule
    \end{tabular}}
\caption{\textbf{Few-shot object detection without training on the COCO base classes.} To test \model's performance on extreme few-shot scenarios, we conduct an evaluation where \model is finetuned only on the K-shot COCO subsets. \model performs slightly worse () or better () compared to TFA~\cite{wang2020frustratingly} without using base class data while also using a smaller backbone. \vspace{4.5mm}}
\label{table:fewshot_nobase}
\end{table}



\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{

\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Detector} & \multicolumn{4}{c}{COCO}\\
 & & 1\% & 2\% & 5\% & 10\% \\
\midrule\midrule
Supervised & \multirow{4}{*}{DDETR} &  11.31  0.3 &  15.22  0.32 &  21.33   0.2 &  26.34  0.1 \\
SwAV & &  11.79  0.3  &  16.02  0.4 &  22.81  0.3 &  27.79  0.2 \\
ReSim & &  11.07  0.4  & 15.26  0.26 & 21.48  0.1 &  26.56  0.3
\\
\textbf{\model} &  & \textbf{ 14.58  0.3} & \textbf{ 18.69  0.2}    & \textbf{24.80  0.2} & \textbf{ 29.12  0.2} \\
\bottomrule                            
\end{tabular}
}
\caption{\textbf{Object detection using k\% of the labeled data on COCO.} The models are trained on \texttt{train2017} using k\% and then evaluated on \texttt{val2017}.}
\label{tab:semi}
\end{table}


\subsection{Semi-supervised Learning}
These experiments test how \model compares to semi-supervised methods, where small amounts of labeled data and large amounts of unlabeled data are used during training.

\noindent\textbf{Pretraining.} We pretrain \model (\defdetr) for  epochs on MS~COCO \textit{train2017} without labels. 
\vspace{0.1cm}
\\
\noindent\textbf{Baselines.} We compare \model with a 
\defdetr model initialized with a supervised backbone from \imagenetthousand pretraining, which is the most direct baseline as all experiments are carried out on the same architecture and training data. We consider recent approaches for pretraining ResNet50 backbone for object detection like ReSim~\cite{xiao2021region} and SwAV~\cite{caron2020unsupervised}, for each we use the publicly released checkpoint.
\vspace{0.1cm}
\\
\noindent\textbf{Experiments.} We finetuned \model on random  of \texttt{train2017} data for , until convergence (validation performance stopped improving). In each setting, we train  different models with different random seeds and report the mean and standard deviation.
\vspace{0.1cm}
\\
\noindent\textbf{Results.} \Cref{tab:semi} shows that \model outperforms existing pretraining methods, including a consistent improvement over the supervised pretraining baseline. We include a more broad comparison in the Supplementary~\Cref{tab:full_semi}, where we also compare to approaches that leverage both the labeled and unlabeled data via auxiliary losses~\cite{jeong2019consistency,sohn2020simple, liu2021unbiased, xu2021end}.



\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.07\linewidth}
\vspace{-0.57in}
\centering

\end{minipage}\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7278_bx_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7816_bx_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_20247_bx_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_13923_bx_p3_thresh50.jpg}

\begin{minipage}[t]{0.07\linewidth}
\vspace{-0.57in}
\centering

\end{minipage}\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7278_by_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7816_by_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_20247_by_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_13923_by_p3_thresh50.jpg}

\begin{minipage}[t]{0.07\linewidth}
\vspace{-0.57in}
\centering

\end{minipage}\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7278_embed_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_7816_embed_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_20247_embed_p3_thresh50.jpg}
\hfill
\includegraphics[width=0.22\textwidth, height=1in]{figures/image-grads/img_13923_embed_p3_thresh50.jpg}
\caption{\textbf{\model visualization.} We show the gradient norms from the unsupervised DETReg detection with respect to the input image  for \textbf{(top)} the  coordinate of the object center, \textbf{(middle)} the  coordinate of the object center, \textbf{(bottom)} the feature-space embedding, .}
\label{fig:qualitative}
\end{figure*}
 
\subsection{\model Analysis} \label{sec:visualize}
This section further explores and justifies the architectural and algorithmic choices used in the main experiments.
\label{ss:analysis}


\begin{table}[t]
\centering
\small

\resizebox{1\linewidth}{!}{
    \begin{tabular}{ccc|ccc}
    \toprule
    Proposals &   & Frozen BB &   &   & AP \\
    \midrule\midrule
    Shuffle &  & & 11.3 & .044 & 32.0 \\
    Top-K &  &  & 9.50 & .037 &  43.3 \\
    Top-K &  &  & 8.81 & .037 & 45.1 \\ 
    Top-K &  &  & 9.14 & .039 & 43.8 \\
    Top-K &  &  \checkmark & \textbf{8.61} & \textbf{.037} & \textbf{45.4} \\
\bottomrule
\end{tabular}}
\caption{\textbf{Ablation studies.} This tables ablates region proposal sampling strategies, values of , and whether to freeze backbones with \model trained on \textit{\imagenethundred} and finetuned on MS~COCO. 
Shuffling the region proposals across images led to a  AP drop,  has a consistent performance, and freezing the backbone does not significantly change the performance.}
\label{table:ablations}
\end{table}


\begin{table}[t]
\centering
\small
\resizebox{1\linewidth}{!}{
    \begin{tabular}{lccccccc}
\toprule
    Method & AP & AP & AP &  &  & \\
    \midrule\midrule
    UP-DETR~\cite{dai2020up} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.4\\
Rand. Prop. & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.8\\
    Selective Search~\cite{uijlings2013selective} & 0.2 & 0.5 & 0.1 & 0.2& 1.5 & 10.9 \\
    \midrule
{ImpSamp (ours)} & 0.7 & 2.0 & 0.1 & 0.3 & 1.8 & 9.0 \\
{Random-K (ours)} & 0.7 & 2.4 & 0.2 & 0.5 & 2.9 & 11.7\\
    {Top-K (ours)} & \textbf{1.0} & \textbf{3.1} & \textbf{0.6} & \textbf{0.6} & \textbf{3.6} & \textbf{12.7} \\
    \bottomrule
    \end{tabular}
}
\caption[hypcap=false]{\textbf{Class agnostic object proposal evaluation on MS~COCO \texttt{val2017}}. The models are trained on IN100 and for each method, we consider the top 100 proposals. We show \model identifies objects more effectively than the previous methods.}
\label{table:unsupervised}
\end{table}


\noindent\textbf{Design Ablations.} \Cref{table:ablations} examines the contribution of the object localization and object embedding tasks in \model. 
To quantify the importance of using object-centric region proposals, we train \model while randomly shuffling the proposal box locations across images, as indicated by ``Shuffle'' in the ``Proposals'' column. Second, to assess the contribution of the embedding loss , we evaluate \model with different coefficients . Finally, we validate that performance does not drop when freezing the backbone during training, i.e. that the performance benefits stem from the core \model contributions. All models are trained on \imagenethundred for  epochs and finetuned on MS~COCO. 

\Cref{table:ablations} justifies our design choices: shuffling the region proposals across images led to a  AP drop indicating that the object-centric proposal are important. We further see that the embedding loss  has a relatively consistent performance improvements with changes of  AP for all setting, and we select  based on these results. Finally, the performance of \model with and without freezing the backbone encoder is relatively consistent with changes of  AP points between the two settings.
\vspace{0.1cm}
\\
\noindent\textbf{Class Agnostic Object Detection.} We examine the class agnostic performance of \model variants discussed in~\Cref{sec:model}, as well as region proposal and pretraining approaches. The results reported in~\Cref{table:unsupervised} indicate that \model variants achieve improved performance over other pretraining approaches including solely using Selective Search. This indicates that coupling the object embedding and localization components in the \model model improves the localization ability. In addition, we observe that the Top-K region proposal selection strategy performs best in these ablations.
\vspace{0.1cm}
\\
\noindent\textbf{Robustness to different proposal methods.} We test how \model performs when pretrained with Selective Search proposals compared to Edge Box region proposals~\cite{zitnick2014edge}. Specifically, we pretrain \model on \imagenethundred and finetune on MS~COCO with 2\% and 10\% of random data. We find that both variants perform similarly well with AP of  vs.  for  and a similar result of  for .
\vspace{0.1cm}
\\
\noindent\textbf{Visualizing \model.} \Cref{fig:qualitative} shows qualitative examples of \model unsupervised box predictions with \defdetr. Additionally, it shows the Saliency Map~\cite{simonyan2013deep} of the  bounding box center and the object embedding with respect to the input image . The first three columns show the attention focusing on the object edges for the  predictions and  for the predicted object embedding. The final column shows a case where the background plays a more important role than the object in the embedding. We believe this may be due to the CNN-based encoder focusing on the textures rather than the shapes in the region as discussed in~\cite{geirhos2018imagenet}, and we view further exploration of such characteristics as an intriguing direction for future work.
























