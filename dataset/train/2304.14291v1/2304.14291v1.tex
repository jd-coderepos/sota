\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{afterpage}

\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{color}
\usepackage{bbding}
\usepackage{array,multirow,textcomp}
\usepackage{rotating}
\usepackage[skip=0.5\baselineskip]{caption}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{balance}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{placeins}
\usepackage{float}

\usepackage[hashEnumerators,hybrid]{markdown}
\usepackage{pgf}
\usepackage{subcaption}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}

\iccvfinalcopy 

\def\iccvPaperID{9261} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}




\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{1.0ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\newcommand{\SB}[1]{\textbf{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\inlinetodo}[1]{{\textcolor{red}{[#1]}}}
\newcommand{\todo}[1]{{\textcolor{red}{[#1]}}}
\newcommand{\highlight}[1]{{\textcolor{blue}{#1}}}

\newcommand{\sem}{\mathrm{sem}}
\newcommand{\msk}{\mathrm{msk}}
\newcommand{\bbox}{\mathrm{box}}
\newcommand{\cntr}{\mathrm{cntr}}
\newcommand{\offs}{\mathrm{offs}}

\def\quads{\hskip0.5em\relax}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[2]{>{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}l<{\egroup}}
\newcommand*\rots{\multicolumn{1}{R{60}{1em}}}\newcommand{\rpm}{\raisebox{.3ex}{}}
\newcommand{\spm}[1]{\tiny{#1}}

\def\toptitlebar{
  \hrule height4pt
  \vskip .25in
}

\def\bottomtitlebar{
  \vskip .25in
  \hrule height1pt
  \vskip .25in
}

\begin{document}

\title{EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation}


\author{
    Suman Saha \\
    ETH Zurich
    \and
    Lukas Hoyer \\
    ETH Zurich
    \and
    Anton Obukhov \\
    ETH Zurich
    \and
    Dengxin Dai \\
    MPI for Informatics
    \and
    Luc Van Gool \\
    ETH Zurich, KU Leuven
}

\maketitle


\begin{abstract}
With autonomous industries on the rise, domain adaptation of the visual perception stack is an important research direction due to the cost savings promise.
Much prior art was dedicated to domain-adaptive semantic segmentation in the synthetic-to-real context.
Despite being a crucial output of the perception stack, panoptic segmentation has been largely overlooked by the domain adaptation community.
Therefore, we revisit well-performing domain adaptation strategies from other fields, adapt them to panoptic segmentation, and show that they can effectively enhance panoptic domain adaptation.
Further, we study the panoptic network design and propose a novel architecture (EDAPS) designed explicitly for domain-adaptive panoptic segmentation. It uses a shared, domain-robust transformer encoder to facilitate the joint adaptation of semantic and instance features, but task-specific decoders tailored for the specific requirements of both domain-adaptive semantic and instance segmentation. As a result, the performance gap seen in challenging panoptic benchmarks is substantially narrowed. EDAPS significantly improves the state-of-the-art performance for panoptic segmentation UDA by a large margin of 25\% on SYNTHIA-to-Cityscapes and even 72\% on the more challenging SYNTHIA-to-Mapillary Vistas.
The implementation is available at {\footnotesize\url{https://github.com/susaha/edaps}}.
\end{abstract}

 \section{Introduction}
\label{sec:intro}
Panoptic segmentation~\cite{kirillov2019panoptic} of images is a core computer vision task that jointly solves two related problems -- semantic segmentation and instance segmentation. 
With the rise of robotics and emerging autonomous driving markets, efficient visual perception stacks are in high demand.
However, large-scale supervised learning of panoptic segmentation~\cite{cheng2019panoptic,kirillov2019panoptic,kirillov2019panopticFP,porzi2019seamless,xiong2019upsnet,mohan2021efficientps} 
is prohibitively expensive as it requires dense annotations for both semantics and instances, which require time-consuming manual labeling.

A promising alternative to circumvent this issue is  
to learn from abundantly available photo-realistic synthetic images~\cite{ros2016synthia,richter2016playing} as their ground truth annotations can be automatically generated by the rendering engine.
However, often models trained on synthetic data (\emph{source domain})
fail to generalize well on the real data (\emph{target domain}) due to differences in data distribution, known as the \textit{domain gap}.

\begin{figure}[t]
  \centering
   \includegraphics[width=1.\linewidth]{figures/teaser.pdf}
   \caption{
   EDAPS is an architecture and a collection of recipes, designed specifically for Domain-Adaptive Panoptic Segmentation. 
   It demonstrates a significant improvement over the prior art on the challenging synthetic-to-real benchmarks.
   As shown above, it is better than CVRN~\cite{huang2021cross} on SYNTHIA  Cityscapes by  mPQ. 
   }
   \label{fig:teaser}
\end{figure}


A common remedy to this problem is to minimize the domain gap using Unsupervised Domain Adaptation (UDA).
This field is actively studied for 
image classification~\cite{
long2015learning,ganin2016domain,long2018conditional,saito2018maximum,pan2019transferrable}, 
object detection~\cite{
chen2018domain,saito2019strong, xu2020cross, chen2021scale, li2022cross}, 
and semantic segmentation~\cite{
hoffman2016fcns,tsai2018learning,hoffman2018cycada,li2019bidirectional,saha2021learning,tranheden2021dacs,hoyer2021daformer}.
However, UDA for panoptic segmentation is often overlooked and there are only two works, namely CVRN~\cite{huang2021cross} and UniDAPS~\cite{zhang2022hierarchical}, which address this problem from the synthetic-to-real point of view. 
Compared to the related semantic segmentation UDA, these approaches achieve only subpar performance. 
Specifically, the relative performance of the best UDA and fully-supervised learning approaches to panoptic segmentation (64\% in UniDAPS~\cite{zhang2022hierarchical}) is much smaller than that of semantic segmentation (88\% in DAFormer~\cite{hoyer2021daformer}).

To understand the root cause of the identified performance gap, we revisit the progress of UDA in semantic segmentation, lift the well-performing UDA strategies to panoptic segmentation, and show that they can effectively enhance panoptic segmentation UDA.


Further, we revisit the panoptic network design and conduct a study of principal architecture designs for panoptic segmentation with respect to their UDA capabilities. We show that previous UDA methods took sub-optimal design choices. While separating the networks for both tasks prevents the network from jointly adapting task-shared features from the source to the target domain (see Fig.~\ref{fig:architecture_comparison}\,a), a shared encoder-decoder cannot accommodate the different needs when adapting both tasks (see Fig.~\ref{fig:architecture_comparison}\,b). 
To address these problems, we propose EDAPS, a network architecture that is particularly designed for domain-adaptive panoptic segmentation. It uses a shared transformer~\cite{vaswani2017attention} encoder to facilitate the joint adaptation of semantic and instance features, but task-specific decoders tailored for the specific requirements of 
both domain-adaptive semantic segmentation and domain-adaptive instance segmentation (see Fig.~\ref{fig:architecture_comparison}\,c).


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/netflavors}
  \caption{
    Overview of network architectures employed in prior panoptic UDA works and the proposed architecture. CVRN~\cite{huang2021cross} 
    (a) does not share parameters between panoptic branches, whereas UniDAPS~\cite{zhang2022hierarchical} 
    (b) resorts to the opposite extreme and shares everything. 
    With EDAPS (c), we propose to share the encoder but to use task-specific decoders, which facilitates panoptic domain adaptation.
  }
  \label{fig:architecture_comparison}
\end{figure}

Utilizing the enhanced panoptic UDA and the enhanced domain-adaptive panoptic network design, EDAPS shows significant performance gains over the prior works on a number of standard perception benchmarks (Fig.~\ref{fig:teaser}).
EDAPS improves the state-of-the-art mPQ from  to  on On SYNTHIA  Cityscapes and from  to  on SYNTHIA  Mapillary Vistas. 
EDAPS trains semantic segmentation and instance segmentation tasks together using a joint training optimization. Therefore, EDAPS can be trained end-to-end in a single stage and the training only requires 21 hours on a single RTX 2080 Ti GPU, which improves its applicability for the community.

The main contribution of EDAPS is the novel combination of network components and UDA strategies on a system level, which results in a  relative gain in the state-of-the-art panoptic segmentation UDA performance of 25\% on SYNTHIA-to-Cityscapes and even 72\% on the more challenging SYNTHIA-to-Mapillary Vistas.
In particular, we carefully study various principal panoptic architectures for their UDA capability and identify a network design that is particularly suited for panoptic UDA. It improves the UDA performance over strong baselines, while being parameter-efficient and fast at inference.
Further, this is the first paper that lifts recent UDA techniques to panoptic segmentation and systematically studies their effectiveness for panoptic UDA.












 \section{Related Work}
\label{sec:relwork}
We outline two cornerstone classes of works related to UDA in Semantic and Panoptic Segmentation settings.

\paragraph{UDA for Semantic Segmentation}
Methods in this class take input images from source and target domains along with the source semantic ground truth label.
A supervised cross-entropy loss is computed on the source image semantic prediction. 
A UDA semantic loss (e.g., adversarial \cite{vu2019advent} or pseudo-label-based self-training \cite{zou2018unsupervised} loss) is used for domain alignment that operates on the semantic feature space. 
UDA for semantic segmentation is one of the most popular tasks in dense prediction, and a large number of works in this category exist in the literature~\cite{
vu2019advent,tsai2018learning,hoffman2018cycada,li2019bidirectional,kim2020learning,zhou2021context,tranheden2021dacs,melas2021pixmatch,choi2019self,araslanov2021self,zhang2021prototypical,zou2018unsupervised,yang2020fda,wang2020differential,luo2019taking,du2019ssf,chen2018road,hoffman2016fcns,kang2020pixel,zhang2019category,liu2021learning,saha2021learning}. 
The first approach of using an adversarial loss helps to align the source and target domain distributions at the input~\cite{
hoffman2018cycada,gong2021dlow}, 
at the feature-level~\cite{
tsai2018learning,hoffman2016fcns}, 
at the output~\cite{
tsai2018learning,vu2019advent},
or at the patch level~\cite{tsai2019domain}.
In the second approach of using self-training~\cite{
zou2018unsupervised,li2019bidirectional,zhang2019category,zou2019confidence}, 
pseudo-labels are generated on the unannotated target domain images either offline~\cite{
sakaridis2018model,yang2020fda,zou2018unsupervised,zou2019confidence}
or online~\cite{
zhang2021prototypical,wang2021domain,tranheden2021dacs,zhou2021context,hoyer2021daformer,hoyer2022hrda}.
The pseudo-labels can be stabilized with consistency regularization~\cite{
tarvainen2017mean,sohn2020fixmatch} based on pseudo-label prototypes~\cite{zhang2021prototypical}, different data augmentation schemes~\cite{
araslanov2021self,choi2019self,melas2021pixmatch,hoyer2023mic}, cross-domain mixup strategies~\cite{
tranheden2021dacs,zhou2021context}, and multiple resolutions~\cite{hoyer2022hrda}.

\paragraph{UDA for Panoptic Segmentation}
Considering the used network architectures, there are distinct shortcomings in both CVRN~\cite{huang2021cross} and UniDAPS~\cite{zhang2022hierarchical}.
On the one side, CVRN requires the training of two separate networks for semantic segmentation and instance segmentation, which is time-consuming, parameter-inefficient, and slow during inference (Fig.~\ref{fig:architecture_comparison}). 
Furthermore, no task-agnostic knowledge can be shared during the adaptation process by separating the semantic from the instance network.
The approach relies on expensive multi-stage training (i.e., training, pseudo-label generation, and retraining with fused labels).
On the other side, UniDAPS proposed to adapt panoptic segmentation within a unified network~\cite{carion2020end}, which directly predicts panoptic segments instead of separate semantic and instance masks.
While this simple concept is intriguing and performs well in a supervised setting~\cite{carion2020end, cheng2021per, li2022panoptic}, unified network architectures are not inherently suited for UDA as shown in~\cite{zhang2022hierarchical}. Even with specific UDA strategies to compensate for that, UniDAPS achieves only a small improvement over CVRN.

The authors of \cite{hoyer2021daformer} pointed out that architectures and training schemes that work for supervised learning might not work as desirably for UDA, and special care is required (in network design and training recipes) to address the domain shift problem effectively.
Outside of the synthetic-to-real domain, the authors of~\cite{pdam} proposed a panoptic UDA approach to microscopy image analysis.
In this paper, we propose a domain-adaptive panoptic segmentation framework that carefully selects the network design and training recipes tailored explicitly for UDA.
 \section{Method}
\label{sec:method}


In this section, we first recap panoptic image segmentation. Second, we present our enhanced panoptic segmentation UDA pipeline. Third, we define principal panoptic architectures for a systematic analysis of their UDA capabilities. And finally, we introduce our EDAPS network architecture, specifically designed for panoptic UDA.


\subsection{Supervised Panoptic Segmentation}

Panoptic segmentation is commonly approached by decomposing the task into a semantic segmentation and an instance segmentation component so that the panoptic segmentation loss  is composed of a semantic and instance loss. . The semantic segmentation loss typically uses a pixel-wise cross-entropy loss to assign each pixel of an image to one class from a pre-defined set.
Instance segmentation further distinguishes instances within classes with countable entities, the so-called thing-classes such as \emph{car} or \emph{person} (as opposed to uncountable stuff-classes such as \emph{road} or \emph{sky}). Instance segmentation can be approached in two ways. In top-down approaches such as Mask R-CNN~\cite{he2017mask}, instances are predicted based on proposals in the form of bounding boxes and instances masks. In bottom-up approaches such as Panoptic-DeepLab~\cite{cheng2019panoptic}, the instances are grouped on a pixel-level without proposals, for example using pixel-wise instance center and offset heatmaps.
During inference, semantic and instance segmentations are deterministically fused into a panoptic segmentation representation.

\subsection{Enhanced Panoptic UDA}
In the UDA setting, a neural network  is trained on annotated source domain images 
 
and unannotated target domain images 
 
with 
with the objective of achieving good performance on the target domain.
As panoptic segmentation ground truth  is only available on the source domain, the panoptic model can only be trained with source data in a supervised fashion. In particular, the source loss term .

Models trained using the supervised loss  on the source domain often exhibit poor generalization on the target domain due to the ``domain gap'' between the distributions of source and target data.
To adapt the model to the target domain, an additional unsupervised loss term  is computed on the target domain.
The overall loss combines both the source and target loss terms .

Various unsupervised target loss terms were proposed in the literature, mostly based on adversarial training~\cite{tsai2018learning,tsai2019domain,wang2020classes} or self-training~\cite{zou2018unsupervised,zhang2019category,mei2020instance,tranheden2021dacs,zhang2021prototypical,hoyer2021improving}.

\paragraph{Self-Training on Target Domain}
In this work, we resort to self-training for adapting the network to the target domain similar to previous panoptic UDA methods~\cite{xu2020cross, zhang2022hierarchical}. In self-training, the model  is trained with high-confidence pseudo-labels~\cite{lee2013pseudo} on the target domain using a weighted cross-entropy loss

which helps the network gradually adapt to the target domain. Here,  denotes the pseudo-label and  its confidence estimate.

The pseudo-labels can be generated using the predictions of a teacher network . 
As the output of  is a map of per-pixel probabilities, a single class assignment is done using the mode of categorical distribution, which could be converted back to one-hot categorical form via the Iverson bracket :

Since the pseudo-labels are the predictions of a network,
they are not always correct, and their quality can be estimated by a confidence estimate ~\cite{zou2018unsupervised,mei2020instance,tranheden2021dacs,hoyer2021improving} 


To stabilize the quality of the pseudo-labels, we resort to the mean teacher framework~\cite{tarvainen2017mean}, which is commonly used in semantic segmentation UDA~\cite{araslanov2021self,tranheden2021dacs,liu2021bapa,hoyer2021daformer}. The parameters of the teacher network  are updated with the exponential moving average of the parameters of the student network  at every training step :


Alongside self-training, we adopt a consistency training~\cite{sajjadi2016regularization, tarvainen2017mean, sohn2020fixmatch},
in which the student network  is trained on augmented target images,
whereas, the mean teacher network  predicts pseudo-labels for actual target images.
The augmented images are generated by mixing pixels of the source and target images.
The mixing is done following the Class-Mix strategy~\cite{olsson2021classmix}, which has been successfully applied to semantic segmentation UDA~\cite{tranheden2021dacs,liu2021bapa,hoyer2021daformer}: first, we randomly select  semantic classes among  classes present in the source image.
Next, the pixels belonging to these selected  semantic classes are pasted into the target image, resulting in a new augmented image.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/overview.pdf}
  \caption{
    The proposed EDAPS (Enhanced Domain-Adaptive Panoptic Segmentation) network architecture.
    EDAPS is built with many design choices tailored to UDA in mind. 
    It achieves competitive results on challenging synthetic-to-real panoptic segmentation benchmarks.
  }
  \label{fig:overview}
\end{figure*}

Furthermore, we leverage the recent findings in semantic segmentation UDA~\cite{hoyer2021improving} to further enhance the performance of the panoptic UDA.
More specifically, we adopt rare class sampling and a feature regularization loss based on ImageNet features to learn domain-invariant representations for panoptic UDA.

\paragraph{Rare Class Sampling (RCS)}
Most datasets are imbalanced, so certain classes are underrepresented. It can hurt the adaptation process if a certain class is sampled with a low frequency~\cite{hoyer2021daformer}. Therefore, we follow DAFormer~\cite{hoyer2021daformer} and sample images with rare classes more frequently. The sampling probability of class  is

where  denotes the class frequency in the source dataset and  is a temperature parameter. Given a sampled class , a source sample is drawn from the subset containing class : .

\paragraph{ImageNet Feature Distance (FD)}
The encoder network in panoptic segmentation UDA is usually pre-trained using ImageNet classification. 
Given that ImageNet is a real-world dataset, its features can be valuable when adapting to a real-world target domain. 
However, it was observed that some relevant features could be forgotten during the adaptation process~\cite{hoyer2021daformer}. 
Therefore, it can be useful to regularize the adaptation process with an ImageNet feature distance loss term~\cite{hoyer2021daformer}

where  denotes the frozen ImageNet model. 
As ImageNet annotates thing classes, it is beneficial to constraint the feature distance to image regions that are labeled as thing classes~\cite{hoyer2021daformer}.


\subsection{Principal Panoptic Architectures for UDA}
\label{sec:methods_principal_architectures}

To further facilitate panoptic UDA, we aim to design a network architecture particularly tailored for domain-adaptive panoptic segmentation.
Several different network architectures have been proposed for panoptic segmentation in the supervised setting. However, the effect of the different designs on the performance in a domain adaptation setting is not well studied. Therefore, we analyze four principal panoptic architectures in a fair comparison using the same network building blocks and the same enhanced panoptic UDA strategy in \S\ref{sec:network_study}. The systematic comparison is the foundation for our EDAPS architecture.

\paragraph{M-Net}

uses two separate encoder-decoder networks for semantic and instance segmentation (see Fig.~\ref{fig:architecture_comparison}\,a). This design was deployed in CVRN~\cite{huang2021cross}. The two encoders utilize the same architecture but the decoder architecture are task-specific. A bottom-up decoder is used for semantic segmentation while a top-down decoder is used for instance segmentation. The design of the network building blocks is further detailed in \S\ref{sec:method_edaps_architecture}. A potential disadvantage of M-Net is the use of separate encoders, which prevents the network from jointly adapting task-shared features from the source to the target domain.

\paragraph{S-Net}

shares the encoder and decoder for both tasks (see Fig.~\ref{fig:architecture_comparison}\,b), which is similar to the architecture of UniDAPS~\cite{zhang2022hierarchical}. As the decoder is shared, it is not possible to have task-specific designs, which can be disadvantageous if different decoder architectures have better domain adaptation properties for semantic and instance segmentation. Also, shared decoders can favor negative transfer of task-specific knowledge.

\paragraph{M-Dec-BU}

shares the encoder but splits the decoders to enable task-specific knowledge. Both decoders use a bottom-up design.

\paragraph{M-Dec-TD (EDAPS)}

shares the encoder but splits the decoders to enable task-specific knowledge. The instance decoder uses a top-down approach in contrast to the bottom-up semantic decoder (see Fig.~\ref{fig:architecture_comparison}\,c). We adopt this design in our EDAPS architecture. In the following section, we motivate and detail the design choices of the EDAPS architecture.

\subsection{EDAPS Network Architecture}
\label{sec:method_edaps_architecture}

An overview of the proposed EDAPS architecture is presented in Fig.~\ref{fig:overview}. On the one side, EDAPS utilizes a shared transformer backbone, which has an improved domain-robustness compared to CNNs \cite{naseer2021intriguing,hoyer2021daformer}. Sharing the encoder is advantageous compared the separate networks as it can facilitate the joint adaptation of semantic and instance features from the source to the target domain. On the other side, EDAPS uses task-specific decoders with different design tailored for the specific requirements of domain-adaptive semantic segmentation and domain-adaptive instance segmentation. In that way, EDAPS combines the strength of previous methods (see Fig.~\ref{fig:architecture_comparison}).

\paragraph{Shared Hierarchical Transformer Encoder} 
Transformers have shown domain-robust properties beneficial for UDA~\cite{hoyer2021daformer,xu2021cdtrans,sun2022safe}.
Therefore, EDAPS uses a Mix Transformer (MiT-B5)~\cite{xie2021segformer} as its encoder network, which is tailored for dense predictions by generating coarse (high-resolution) and fine-grained (low-resolution) features at different levels. 
Unlike  patches of ViT~\cite{dosovitskiy2020image}, MiT divides an image into relatively smaller patches of shape ; this facilitates preserving finer details helpful in improving semantic segmentation.
The patches are processed with four transformer blocks into multi-level features at  scales of the original image shapes.
Moreover, an efficient self-attention is used to cope with the computation bottleneck of transformer encoders, i.e., the computational complexity of the self-attention operation is minimized by reducing the input sequence length  using a reduction ratio as in \cite{wang2021pyramid}.
Overlapping patch merging is used to downsample feature maps which helps preserve local continuity.
These properties of MiT make it possible to learn robust domain-invariant features for UDA panoptic segmentation using limited computing resources (i.e., on a single GPU).

\paragraph{Bottom-Up Semantic Decoder} 
We adopt a domain-robust context-aware feature fusion decoder~\cite{hoyer2021improving} as our semantic decoder, which has two main benefits over traditional DeepLab decoders~\cite{chen2017deeplab,george2018encoder} or MLP-based decoders~\cite{wang2021pyramid,xie2021segformer,zheng2021rethinking}.
Firstly, it allows exploiting context information alongside local information in the decoder, which is beneficial for domain-robust semantic segmentation~\cite{kamann2021benchmarking}.
Secondly, instead of only using the context information coming from the bottleneck features~\cite{chen2017deeplab,george2018encoder}, it fuses context encoded by features at different levels of the backbone network.
The high-resolution features from the earlier layers encode low-level concepts helpful in better semantic segmentation.
The multi-level context-aware feature fusion is learned using a series of  depthwise separable convolutions~\cite{chollet2017xception} with different dilation rates.

\paragraph{Top-Down Instance Decoder} 

Based on the findings of the network study in \S\ref{sec:exp}, EDAPS resorts to a proposal-based top-down instance decoder, which exhibits better domain-adaptive properties compared to a bottom-up decoder.


Specifically, we follow Mask R-CNN~\cite{he2017mask} and use a Region Proposal Network (RPN) to predict candidate object bounding boxes. It is trained using a binary cross-entropy (CE) loss for bounding box classification (object or no object), and an  loss for box regression.
Given the bounding boxes predicted by RPN, a Region-of-Interest Alignment (RoIAlign) layer is used to extract features from each box. 
The extracted features are used as inputs to the box and mask heads.
The box head is trained to predict the bounding boxes and class labels, and the mask head is trained to predict the class-agnostic binary instance masks.
The box head is trained using CE loss for object classification and  loss for box regression over all thing classes.
The mask head predictions are penalized using the binary CE loss.

\paragraph{Bottom-Up Instance Decoder (Baseline)}

For the network studies with a bottom-up instance decoder, we adapt the strong domain-robust decoder from~\cite{hoyer2021improving} from semantic segmentation to instance segmentation by replacing the classification head (for semantic prediction) with two  convolutional layers that predict the instance centers and offsets. It is trained using the losses proposed in~\cite{cheng2019panoptic}.

\begin{figure*}[t!]
  \centering   
   \includegraphics[width=1.\linewidth]{figures/qualitatives.pdf}
   \caption{
   Visual comparison of the panoptic segmentation quality of EDAPS (our method) with the prior works CRST \cite{zou2019confidence}, FDA \cite{yang2020fda}, AdvEnt \cite{vu2019advent}, and CVRN \cite{huang2021cross} on the two UDA benchmarks: SYNTHIA  Cityscapes (top) and SYNTHIA  Mapillary Vistas (bottom).
   }
   \label{fig:qualitatives}
\end{figure*}

\paragraph{Feature Fusion}

We follow Panoptic-DeepLab~\cite{cheng2019panoptic} to fuse semantic and instance predictions into the final panoptic segmentation upon inference.
Class-agnostic instance segmentation maps are generated by selecting the top-k instance predictions with a detection score above a certain threshold. 
The resulting instance segmentation is fused with the predicted semantic map by a majority-voting rule to generate the final panoptic segmentation.





 \section{Experiments}
\label{sec:exp}

\subsection{Implementation Details}

\paragraph{Datasets}
We evaluate EDAPS for the common setting of synthetic-to-real adaptation. As a source dataset, we use Synthia~\cite{ros2016synthia}, which contains 9,400 synthetic images and panoptic labels. As target datasets, we use Cityscapes~\cite{cordts2016cityscapes}  and Mapillary Vistas~\cite{neuhold2017mapillary}.
Cityscapes~\cite{cordts2016cityscapes} consists of 2,975 training and 500 validation images of European street scenes.
Mapillary Vistas~\cite{neuhold2017mapillary} is a large-scale autonomous driving dataset consisting of 18,000 training and 2,000 validation images.


\paragraph{Training}
We follow DAFormer~\cite{hoyer2021daformer} and train EDAPS with AdamW~\cite{loshchilov2018decoupled} for 40k iterations, a batch size of 2, a crop size of 512512, a learning rate of  with a linear warmup for 1.5k iterations and polynomial decay afterward, and a weight decay of 0.01.
The experiments are conducted on a RTX 2080 Ti GPU with 11 GB memory.
We use the same data augmentation parameters as in DACS~\cite{tranheden2021dacs}.
To encourage the under-represented classes (in the source domain) to be sampled more frequently, we use RCS~\cite{hoyer2021daformer}
and set the RCS temperature to .
Following DAFormer~\cite{hoyer2021daformer}, we also use the ImageNet feature distance loss to preserve 
information about certain thing classes encoded in the ImageNet features.
We set this loss weight .
For the bottom-up instance head, we use the center loss weight , offset loss weight .
For the top-down instance head, we set the following loss weights to :
,
,
,
,
.

\paragraph{Evaluation Metrics}
For evaluating panoptic segmentation, we use the panoptic quality (PQ) metric \cite{kirillov2019panoptic}
that captures performance for all classes (including stuff and things) in an interpretable and unified manner.
PQ can be seen as the multiplication of a segmentation quality (SQ) term and 
a recognition quality (RQ) term, i.e., PQ = SQ  RQ.
We report PQ for each category.
Mean SQ (mSQ), mean RQ (mSQ), and mean PQ (mPQ) are average scores over all categories.
For evaluating instance and semantic segmentation, we use the mIoU and mAP metrics respectively. 


\begingroup
\setlength{\tabcolsep}{4pt} \renewcommand{\arraystretch}{1.3}
\begin{table*}[ht!]
\normalsize
\centering
\caption{
Comparison with state-of-the-art methods on SYNTHIA  Cityscapes benchmark for UDA panoptic segmentation. 
For clarity, per class PQs are reported. The results of EDAPS are averaged over 3 random seeds.
}
\footnotesize
\begin{tabular}{l @{\quad} cccccccccccccccc @{\quad} c @{\quad} c @{\quad} c}
\toprule 
UDA Method & \rots{road} & \rots{sidewalk\quads\quads} & \rots{building} & \rots{wall} & \rots{fence} & \rots{pole} & \rots{light} & \rots{sign} & \rots{veg} & \rots{sky} & \rots{person} & \rots{rider} & \rots{car} & \rots{bus} & \rots{m.bike} & \rots{bike} & mSQ & mRQ & mPQ \\

\midrule
FDA \cite{yang2020fda}                  & 79.0      & 22.0      & 61.8      & 1.1  & 0.0 & 5.6  & 5.5   & 9.5  & 51.6 & 70.7 & 23.4 & 16.3 & 34.1 & 31.0 & 5.2  & 8.8  & 65.0 & 35.5 & 26.6 \\
CRST \cite{zou2019confidence}           & 75.4      & 19.0      & 70.8      & 1.4  & 0.0 & 7.3  & 0.0   & 5.2  & 74.1 & 69.2 & 23.7 & 19.9 & 33.4 & 26.6 & 2.4  & 4.8  & 60.3 & 35.6 & 27.1 \\
AdvEnt \cite{vu2019advent}              & 87.1      & 32.4      & 69.7      & 1.1  & 0.0 & 3.8  & 0.7   & 2.3  & 71.7 & 72.0 & 28.2 & 17.7 & 31.0 & 21.1 & 6.3  & 4.9  & 65.6 & 36.3 & 28.1 \\
CVRN \cite{huang2021cross}              & \B{86.6}  & 33.8      & 74.6      & 3.4  & 0.0 & 10.0 & 5.7   & 13.5 & 80.3 & 76.3 & 26.0 & 18.0 & 34.1 & 37.4 & 7.3  & 6.2  & 66.6 & 40.9 & 32.1 \\
UniDAPS \cite{zhang2022hierarchical}    & 73.7      & 26.5      & 71.9      & 1.0 & 0.0 & 7.6 & 9.9 & 12.4 & 81.4 & 77.4 & 27.4 & 23.1 & \B{47.0} & \B{40.9} & 12.6 & \B{15.4} & 64.7 & 42.2 & 33.0 \\
\midrule
EDAPS (Ours)                            & 77.5      & \B{36.9}  & \B{80.1} & \B{17.2} & \B{1.8} & \B{29.2} & \B{33.5} & \B{40.9} & \B{82.6} & \B{80.4} & \B{43.5} & \B{33.8} & 45.6 & 35.6 & \B{18.0} & 2.8 & \B{72.7} & \B{53.6} & \B{41.2} \\

\bottomrule
\end{tabular}
 \label{table:s2c_pq}
\end{table*}
\endgroup

\begingroup
\setlength{\tabcolsep}{4pt} \renewcommand{\arraystretch}{1.3}
\begin{table*}[ht!]
\normalsize
\centering
\caption{
Comparison with state-of-the-art methods on SYNTHIA  Mapillary Vistas benchmark for UDA Panoptic Segmentation.
For clarity, per class PQs are reported. The results of EDAPS are averaged over 3 random seeds.
}
\footnotesize
\begin{tabular}{l @{\quad} cccccccccccccccc @{\quad} c @{\quad} c @{\quad} c}
\toprule 
UDA Method & \rots{road} & \rots{sidewalk\quads\quads} & \rots{building} & \rots{wall} & \rots{fence} & \rots{pole} & \rots{light} & \rots{sign} & \rots{veg} & \rots{sky} & \rots{person} & \rots{rider} & \rots{car} & \rots{bus} & \rots{m.bike} & \rots{bike} & mSQ & mRQ & mPQ \\

\midrule
FDA \cite{yang2020fda}          & 44.1 & 7.1 & 26.6 & 1.3 & 0.0 & 3.2 & 0.2 & 5.5 & 45.2 & 61.3 & 30.1 & 13.9 & 39.4 & 12.1 & 8.5 & 7.0 & 63.8 & 26.1 & 19.1 \\
CRST \cite{zou2019confidence}   & 36.0 & 6.4 & 29.1 & 0.2 & 0.0 & 2.8 & 0.5 & 4.6 & 47.7 & 68.9 & 28.3 & 13.0 & 42.4 & 13.6 & 5.1 & 2.0 & 63.9 & 25.2 & 18.8 \\
AdvEnt \cite{vu2019advent}      & 27.7 & 6.1 & 28.1 & 0.3 & 0.0 & 3.4 & 1.6 & 5.2 & 48.1 & 66.5 & 28.4 & 13.4 & 40.5 & 14.6 & 5.2 & 3.3 & 63.6 & 24.7 & 18.3 \\
CVRN \cite{huang2021cross}      & 33.4 & 7.4 & 32.9 & 1.6 & 0.0 & 4.3 & 0.4 & 6.5 & 50.8 & 76.8 & 30.6 & 15.2 & 44.8 & 18.8 & 7.9 & \B{9.5} & 65.3 & 28.1 & 21.3 \\
\midrule
EDAPS (Ours)                    & \B{77.5} & \B{25.3} & \B{59.9} & \B{14.9} & 0 & \B{27.5} & \B{33.1} & \B{37.1} & \B{72.6} & \B{92.2} & \B{32.9} & \B{16.4} & \B{47.5} & \B{31.4} & \B{13.9} & 3.7 & \B{71.7} & \B{46.1} & \B{36.6} \\

\bottomrule
\end{tabular}

 \label{table:s2m_pq}
\end{table*}
\endgroup

\begin{table}
\caption{Comparison of the mPQ for source-only training, UDA (SYNTHIA  Cityscapes), and supervised oracle training (Cityscapes) along with the relative UDA performance .
}
\label{tab:relative_uda}
\centering
\renewcommand{\arraystretch}{1.3}
\footnotesize
\begin{tabular}{lcccc}
\toprule
UDA Method  & mPQ & mPQ & mPQ & mPQ \\
\midrule
CVRN~\cite{huang2021cross}        & 20.1 & 32.1    & 47.7    & 67.3\%   \\
UniDAPS~\cite{zhang2022hierarchical}     & 18.3 & 33.0    & 51.9    & 63.6\%   \\
EDAPS (Ours) & \textbf{21.6} & \textbf{41.2}    & \textbf{56.6}    & \textbf{72.7\%}   \\
\bottomrule
\end{tabular}
\end{table} 
\subsection{Comparison with the State of the Art}

First, we compare our EDAPS with the state-of-the-art methods for panoptic segmentation UDA on SYNTHIA  Cityscapes (Tab.~\ref{table:s2c_pq}) and SYNTHIA  Mapillary (Tab.~\ref{table:s2m_pq}). It can be seen that EDAPS consistently outperforms previous methods with a large margin in all aggregated metrics (mSQ, mRQ, mPQ). Specifically, EDAPS improves the mPQ from 33.0 to 41.2 on SYNTHIA  Cityscapes and from 21.3 to 36.6 SYNTHIA  Mapillary, which is a respective improvement of remarkable 25\% and 72\% over previous works. 
EDAPS particularly improves the performance of classes that previous methods struggled with, such as wall, pole, traffic light, traffic sign, person, rider on Cityscapes, and road, sidewalk, building, wall, pole, traffic light, traffic sign, vegetation, sky, bus on Mapillary.

As the different panoptic UDA methods resort to different network architectures, it has to be considered that they have a varying inherent capability to learn panoptic segmentation. Therefore, we compare both the UDA performance and the supervised performance (as indicators of the capability of a network architecture in Tab.~\ref{tab:relative_uda}. Further, we normalize the UDA performance by the supervised performance () for a more fair comparison. 
It can be seen that even though UniDAPS uses a more powerful network than CVRN, as indicated by the higher supervised performance, this does not translate well to UDA performance. Therefore, UniDAPS has a significantly lower relative UDA performance than CVRN. The lower relative performance of UniDAPS could be caused by the additional transformer encoder-decoder architecture added to the backbone, which is not pre-trained on ImageNet and might overfit to the source domain more easily.
In contrast, our method can increase the relative UDA performance alongside with the best UDA and the best supervised performance. This means that EDAPS can effectively narrow the domain gap to supervised learning.

\begingroup
\setlength{\tabcolsep}{3.5pt} 
\renewcommand{\arraystretch}{1.3}
\begin{table}
\centering
\caption{
Efficiency comparison on an RTX 2080 Ti.
}
\footnotesize
\begin{tabular}{l @{\quad} ccccc}

\toprule 

Network architecture & \#Parameters & Inference Speed  \\
\midrule
CVRN~\cite{huang2021cross}                  & 185.5 M   & 0.36 fps \\
UniDAPS~\cite{zhang2022hierarchical}        & 58.7 M    & 7.24 fps \\
EDAPS (Ours)                                & 104.9 M   & 5.84 fps \\
\bottomrule
\end{tabular} \label{tab:efficiency}
\end{table}
\endgroup

We compare EDAPS with previous works from an efficiency viewpoint in Tab.~\ref{tab:efficiency}. EDAPS increases the inference speed by a factor of 16 compared to CVRN, demonstrating the efficiency of EDAPS. Compared to UniDAPS, EDAPS reaches 80\% of its inference speed, which is due to the additional instance decoder of EDAPS. However, given the large gains in the quality of the adapted panoptic segmentation, this is an acceptable trade-off.


\subsection{Study of Panoptic Architectures for UDA}
\label{sec:network_study}
\begingroup
\setlength{\tabcolsep}{3.5pt} 
\renewcommand{\arraystretch}{1.3}
\begin{table}
\centering
\caption{
Network topology study as detailed in \S\ref{sec:methods_principal_architectures}. Mean and standard deviation are computed over 3 random seeds.
}
\footnotesize
\setlength\tabcolsep{1px}
\begin{tabular}{l @{\quad} ccccc}

\toprule 

Network
& mAP & mIoU & mSQ & mRQ & mPQ \\

\midrule

S-Net
& 7.0\spm{0.4} 
& 57.8\spm{1.1}  
& 72.0\spm{0.4} 
& 43.8\spm{0.8} 
& 34.0\spm{0.7} 
\\

M-Net
& 23.3\spm{0.3} 
& 56.6\spm{0.6} 
& 71.7\spm{0.3} 
& 50.0\spm{0.5} 
& 38.1\spm{0.3} 
\\

M-Dec-BU      
& 17.6\spm{1.5} 
& 60.3\spm{0.6} 
& 73.9\spm{0.4} 
& 49.9\spm{0.3} 
& 39.0\spm{0.2} 
\\

M-Dec-TD (EDAPS)
& 34.4\spm{0.5} 
& 57.5\spm{0.0} 
& 72.7\spm{0.2} 
& 53.6\spm{0.5} 
& 41.2\spm{0.4} 
\\
\bottomrule
\end{tabular}



 \label{tab:network_study}
\end{table}
\endgroup

\begingroup
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1.0}
\begin{table}
\centering
\caption{Class-wise PQ comparison of EDAPS and M-Net.}
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrrrrrrrrrrrrr}
\toprule 

Network & \rots{road} & \rots{sidew.\quads\quads} & \rots{build.} & \rots{wall} & \rots{fence} & \rots{pole} & \rots{light} & \rots{sign} & \rots{veg.} & \rots{sky} & \rots{pers.} & \rots{rider} & \rots{car} & \rots{bus} & \rots{m.bike} & \rots{bike} \\

\midrule
M-Net     & 75.2 & 37.5 & 80.4 & 15.7 & 2.0  & 27.1 & 36.2 & 38.0 & 79.8 & 80.0 & 38.2 & 25.8 & 38.9 & 28.0 & 6.1  & 0.3 \\
EDAPS                 & 77.5      & 36.9  & 80.1 & 17.2 & 1.8 & 29.2 & 33.5 & 40.9 & 82.6 & 80.4 & 43.5 & 33.8 & 45.6 & 35.6 & 18.0 & 2.8 \\
\hline
Gain & +2.3 & -0.6 & -0.3 & +1.5 & -0.2 & +2.1 & -2.7 & +2.9 & +2.8 & +0.4 & \textbf{+5.3} & \textbf{+8.0} & \textbf{+6.7} & \textbf{+7.6} & \textbf{+11.9} & +2.5 \\

\bottomrule
\end{tabular} }
\label{tab:class_wise_pq_comparison}
\end{table}
\endgroup


To gain insights into the influence of the network architecture on the UDA performance, we compare the four principal architectures described in \S\ref{sec:methods_principal_architectures}.
For all network variants, we use the enhanced network components of EDAPS, i.e., the transformer encoder and the domain-robust decoder as well as the same enhanced UDA strategy for a fair comparison.

Tab.~\ref{tab:network_study} shows that S-Net achieves the lowest mPQ of 34.0. In contrast, M-Net achieves 38.1 mPQ, which is +4.1 mPQ better. This demonstrates the importance of task-specific networks (components) to learning task-specific knowledge in the UDA setting. Still, sharing features across tasks in a common encoder for UDA is useful, as shown by M-Dec-BU, which gains a performance improvement of +0.9 mPQ over M-Net. However, the symmetric architecture with bottom-up decoders for both semantics and instances is sub-optimal, as can be seen when comparing M-Dec-BU with M-Dec-TD. With a top-down decoder for instance segmentation, a significant improvement of +2.2 mPQ can be achieved over M-Dec-BU. In particular, the instance mAP improves by +16.8. This shows that the top-down approach is more domain-robust for instance segmentation than the bottom-up approach.

The network study in Tab.~\ref{tab:network_study} also allows a fair comparison of EDAPS and CVRN as M-Net uses the
CVRN architecture with the enhanced components of EDAPS, i.e., Transformer encoder, Mean Teacher, RCS, and FD. Therefore, M-Net has a higher 38.1 mPQ compared to the original CVRN 32.1 mPQ. Still, EDAPS outperforms M-Net by +3.1 mPQ. These improvements come from the joint training of EDAPS for semantic and instance segmentation (compared to disjoint M-Net training), allowing the shared encoder to learn better domain-robust instance and semantic features. In particular, EDAPS improves the instance mAP by +11.1 over M-Net demonstrating that particularly the instance head benefits from the shared features. The class-wise comparison in Tab.~\ref{tab:class_wise_pq_comparison} reveals that EDAPS mostly benefits difficult thing-classes such as rider or motorbike, showing that shared features are particularly important for instances that are difficult to adapt.


\subsection{UDA Ablation Study}

\begingroup
\setlength{\tabcolsep}{3.5pt} 
\renewcommand{\arraystretch}{1.3}
\begin{table}
\centering
\caption{
Ablation study of the UDA strategies Self-Training (Self-Tr.), Mean Teacher (MT), ImageNet Feature Distance (FD), and Rare Class Sampling (RCS). Mean and standard deviation are provided over 3 random seeds.
}
\footnotesize
\setlength\tabcolsep{1.8px}
\begin{tabular}{cccc @{\quad} ccccc}
\toprule 
Self-Tr. & MT & FD & RCS & mAP & mIoU & mSQ & mRQ & mPQ \\
\midrule

& 
& 
& 
& 22.0\spm{1.0}
& 33.0\spm{1.8}
& 60.9\spm{2.6} 
& 29.3\spm{1.7} 
& 21.6\spm{1.2} 
\\


\checkmark 
& 
& 
& 
& 34.7\spm{0.9} 
& 54.0\spm{0.1} 
& 71.5\spm{0.4} 
& 49.1\spm{0.4} 
& 37.5\spm{0.2}
\\



\checkmark 
& 
\checkmark
& 
& 
& 35.7\spm{1.7} 
& 56.3\spm{0.9} 
& 69.3\spm{1.6} 
& 50.7\spm{1.5} 
& 38.9\spm{1.2} 
\\

\checkmark 
& 
\checkmark
& 
\checkmark
& 
& 35.2\spm{0.9} 
& 56.7\spm{0.9} 
& 70.7\spm{1.8} 
& 52.1\spm{1.1} 
& 39.7\spm{0.9} 
\\

\checkmark 
& 
\checkmark
& 
\checkmark
& 
\checkmark
& 34.4\spm{0.5} 
& 57.5\spm{0.0} 
& 72.7\spm{0.2} 
& 53.6\spm{0.5} 
& 41.2\spm{0.4} 
\\

\bottomrule
\end{tabular}


















 \label{tab:uda_ablation}
\end{table}
\endgroup

To better understand the influence of the UDA components that we newly introduced to panoptic UDA, we ablate them in Tab.~\ref{tab:uda_ablation}. While the source-only model performs at 21.6 mPQ, the UDA baseline with basic self-training achieves a performance of 37.5 mPQ. This is already higher than previous state-of-the-art methods, emphasizing the strength of the proposed EDAPS architecture. The self-training can be improved by +1.4 mPQ using an EMA teacher for pseudo-label generation. ImageNet Thing-Class Feature Distance (FD) further increases performance by +0.8 mPQ. When further integrating Rare Class Sampling (RCS), the performance gains another +1.5 mPQ. This shows that the EMA teacher, FD and RCS are valuable components for panoptic UDA.
 \section{Conclusions}

Previous approaches to domain-adaptive panoptic segmentation either follow inefficient and expensive adaptation techniques, or their network architecture and training strategies are influenced by supervised learning.
In this work, we addressed these issues by carefully selecting the network design and training schemes tailored for UDA and proposed EDAPS, an efficient domain-adaptive panoptic segmentation network that demonstrates state-of-the-art performance and surpasses prior art by a large margin. 
Furthermore, we provided a detailed analysis of the various design choices for enhancing the panoptic UDA performance.
When compared with previous methods on UDA panoptic segmentation, EDAPS shows significant performance gains of 25\% on SYNTHIA-to-Cityscapes and even 72\% on the more challenging SYNTHIA-to-Mapillary Vistas.
We believe that EDAPS as a network architecture will facilitate benchmarking future UDA strategies in panoptic segmentation, making domain-adaptive panoptic segmentation more usable in practice.



 
\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\noindent\textbf{\Large Supplementary Material}

\makeatletter
\renewcommand{\theHsection}{papersection.\number\value{section}} \renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{section}{0}

\setcounter{figure}{0}
\setcounter{table}{0}
\makeatother

\section{Overview}
This supplementary material provides a more detailed analysis of the experiments presented in the paper.
In particular,
Sec.~\ref{sec:further_impl_details} provides further implementation details,
Sec.~\ref{sec:ablation_study} presents an ablation study showing the significance of different instance losses on the adaptation process,
Sec.~\ref{sec:qualitative_analysis} analyzes additional qualitative example predictions,
and Sec.~\ref{sec:mdecbu-analysis} offers a visual comparison of the predictions made by EDAPS and M-Dec-BU.

\section{Further Implementation Details}
\label{sec:further_impl_details}
EDAPS is implemented in PyTorch \cite{paszke2017automatic} based on the DAFormer framework~\cite{hoyer2021daformer}.
We will release the source code to ensure easy reproducibility and promote research in domain-adaptive panoptic segmentation.
We follow CVRN \cite{huang2021cross} and consider  stuff-classes and  thing-classes.
The stuff classes are road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, and sky;
the thing classes are person, rider, car, truck, bus, train, motorcycle, and bicycle. 

We use a threshold of  to select the top-k binary masks predicted by the EDAPS instance head.
We use these top-k predicted masks to generate the class-agnostic instance segmentation maps, which are then fused with the predicted semantic segmentation maps by a majority-voting rule.

\textbf{M-Dec-BU (Baseline).}
Since the bottom-up instance decoder (used in the M-Dec-BU) does not directly predict instance masks,
a post-processing step is required to generate the class-agnostic instance segmentation maps 
from the predicted center and offset heatmaps. 
The post-processing step includes selecting the top-k instance centers and
grouping pixels based on these selected centers.
We pick the top-k predicted centers by first applying hard thresholding to filter out the low-confident center predictions following a 2D max pooling on the predicted center heatmap.
In all our experiments, 
we set the threshold to ,
max-pooling kernel size to ,
and  as in \cite{cheng2019panoptic}.

Once the top-k instance centers are selected, 
we assign each pixel an instance id based on the predicted offset heatmap. 
More specifically, the instance id for a pixel is the index of the closest instance center 
after moving the pixel location by the offset.
We filter out the stuff pixels based on the predicted semantic segmentation.
Once the instance ids are computed, 
we generate the class-agnostic instance segmentation maps and 
fuse them with the predicted semantic segmentation maps by a majority-voting rule \cite{cheng2019panoptic}.

\begingroup
\setlength{\tabcolsep}{3.5pt} 
\renewcommand{\arraystretch}{1.3}
\begin{table*}[h!]
\centering
\caption{
EDAPS instance head losses ablation study on the SYNTHIA  Cityscapes UDA panoptic benchmark.
The results of the trained models are averaged over 3 random seeds.
}
\footnotesize
\begin{tabular}{cccccc @{\quad} ccccc}
\toprule 
&

& 
& 
& 
& 
& mAP 
& mIoU 
& mSQ 
& mRQ 
& mPQ \\
\midrule
Model 1
&
& & \checkmark
& \checkmark
& \checkmark
& 9.3\spm{4.4} 
& 57.5\spm{0.4} 
& 70.9\spm{7.3} 
& 43.2\spm{5.8} 
& 30.8\spm{1.3} 
\\
Model 2
&
\checkmark
& & \checkmark
& \checkmark
& \checkmark
& 4.8\spm{2.1} 
& 57.9\spm{1.1} 
& 62.9\spm{2.4} 
& 38.8\spm{0.3} 
& 29.6\spm{0.2} 
\\
Model 3
&
& \checkmark
& \checkmark
& \checkmark
& \checkmark
& 16.9\spm{5.9} 
& 57.8\spm{0.4} 
& 72.5\spm{1.6} 
& 45.1\spm{3.2} 
& 34.7\spm{2.4} 
\\
Model 4
&
\checkmark
& \checkmark
& & & \checkmark
& 0.5\spm{0.3} 
& 57.3\spm{0.4} 
& 45.5\spm{0.1} 
& 38.4\spm{0.7} 
& 29.5\spm{0.5} 
\\
Model 5
&
\checkmark
& \checkmark
& & \checkmark
& \checkmark
& 2.3\spm{2.0} 
& 57.9\spm{0.5} 
& 45.6\spm{0.1} 
& 38.4\spm{0.4} 
& 29.5\spm{0.3}
\\
Model 6
&
\checkmark
& \checkmark
& \checkmark
& & \checkmark
& 29.6\spm{0.4} 
& 57.5\spm{0.4} 
& 71.7\spm{0.5} 
& 50.4\spm{0.7} 
& 38.2\spm{0.7} 
\\
Model 7
&
\checkmark
& \checkmark
& \checkmark
& \checkmark
& & 9.7\spm{1.9} 
& 57.0\spm{1.3} 
& 65.3\spm{3.8} 
& 43.7\spm{1.6} 
& 32.7\spm{0.9} 
\\
Model 8
&
\checkmark
& \checkmark
& \checkmark
& \checkmark
& \checkmark
& 34.4\spm{0.5} 
& 57.5\spm{0.0} 
& 72.7\spm{0.2} 
& 53.6\spm{0.5} 
& 41.2\spm{0.4} 
\\

\bottomrule
\end{tabular}
 \label{tab:mask_rcnn_ablation}
\end{table*}
\endgroup

\section{Ablation Study of Instance Losses}
\label{sec:ablation_study}
EDAPS uses a top-down instance decoder, trained using  losses. Even though the effect of these losses is well explored for supervised panoptic segmentation, the influence of the different losses on UDA panoptic segmentation has not been studied so far. Therefore, we present a detailed ablation study analyzing the effect of each instance loss on the domain-adaptive panoptic performance (mPQ). Furthermore, we provide the domain-adaptive instance segmentation performance (mAP), which helps to understand the significance of each instance loss towards the adaptation process for instance segmentation.

We ablate all  instance losses, including the losses in the RPN and RoI heads.
There are  losses in the RPN head, RPN bounding-box classification, and regression losses (, ),
and  losses in the RoI head, RoI bounding-box classification, regression, and RoI mask classification losses (, 
, ).
For this ablation, we train  models with different combinations of the instance losses on the SYNTHIA  Cityscapes benchmark.
The models are trained following the same setup as EDAPS.

The results of the ablation study in Table \ref{tab:mask_rcnn_ablation} provide interesting observations:
Without RPN losses, the mPQ decreases from  to .
At a closer look, we note that instance segmentation (mAP) and recognition quality (mRQ) are adversely affected the most.
That implies that in the absence of good-quality region proposals, the network struggles to generate correct instance segmentation masks, and there is an increase in false detections (false positives and false negatives).
Besides, the RPN box regression loss contributes more to the overall performance improvement than the RPN box classification loss. 

Without the RoI head's box classification and regression losses,
the model shows the lowest mPQ, mAP, mRQ, and mSQ of , , , , respectively.
It implies that the RoI-pooled features play a vital role; the instance head trained without losses on the RoI features struggles to achieve high-quality instance segmentation.
Interestingly, the RoI head's box classification loss contributes more to the overall performance gain than the box regression loss.
Since the RPN box regression loss already helps the network to learn better instance bounding boxes, even if the RoI head box regression loss is turned off, it achieves an mPQ of , which is already better than the  mPQ of the prior work CVRN~\cite{huang2021cross}.
However, it is crucial for the RoI head to learn the correct box label classification; since the RPN box classification loss is only responsible for providing correct binary labels (object vs. no-object) for the region proposals, the RoI box classification loss helps the instance head to learn correct instance class labels (i.e., the  thing object classes) for the RoI-predicted boxes.
Finally, in the absence of the RoI mask classification loss, the mPQ goes down from  to  , which shows that the network must learn the correct binary instance masks to achieve better panoptic segmentation quality.

\section{Qualitative Analysis}
\label{sec:qualitative_analysis}

In this section, we provide additional qualitative prediction results to visually compare the proposed EDAPS and the prior art CVRN \cite{huang2021cross}. 
The visualizations for models trained on SYNTHIA  Cityscapes are presented in Fig. \ref{fig:pred_person}-\ref{fig:pred_miscellaneous}.
The major improvements come from better panoptic segmentation of the thing classes \emph{person} (Fig. \ref{fig:pred_person}), \emph{rider}  (Fig. \ref{fig:pred_rider_motorbike}), 
\emph{car} (Fig. \ref{fig:pred_car}); and stuff classes \emph{traffic light}, \emph{traffic sign}, \emph{pole} (Fig. \ref{fig:pred_rider_motorbike}, \ref{fig:pred_car}, and \ref{fig:pred_miscellaneous})
across different object scales, appearance, and viewing angles. 
In general, EDAPS can better delineate object boundaries, resulting in better-quality pixel-level panoptic segmentation.
Note that the detected object shapes (e.g., \emph{person}, \emph{rider}, \emph{car}) predicted by the EDAPS resemble more real-world object shapes when compared to CVRN \cite{huang2021cross}.
Thanks to the domain-robust Mix Transformer (MiT-B5)~\cite{xie2021segformer} backbone, 
EDAPS can learn a richer set of domain-invariant semantic and instance features helpful in better segmentation of fine structures.
EDAPS can better segment the occluded object instances in a crowded scene such as 
\emph{person} (Fig. \ref{fig:pred_person} row -), 
\emph{rider} (Fig. \ref{fig:pred_rider_motorbike} row ),
\emph{car} (Fig. \ref{fig:pred_car} row -).
Moreover, the \emph{person} segments predicted by EDAPS preserve finer details of the human body even when instances are occluded.
Similar observations can be made for the \emph{rider} and \emph{car} classes.
For large object instances (such as \emph{bus}), EDAPS can segment out the entire object, whereas CVRN fails to do so (Fig. \ref{fig:pred_rider_motorbike} row , Fig. \ref{fig:pred_miscellaneous} row ).
EDAPS can provide better segmentation for the \emph{traffic light} 
(Fig. \ref{fig:pred_rider_motorbike}  row , ; Fig. \ref{fig:pred_miscellaneous} row , ), and 
\emph{traffic sign} 
(Fig. \ref{fig:pred_rider_motorbike}  row , ; Fig. \ref{fig:pred_miscellaneous} row , , ).

In addition, we show visual qualitative results on SYNTHIA  Mapillary Vistas UDA panoptic benchmark 
(Fig. \ref{fig:pred_set8}-\ref{fig:pred_set10}).
EDAPS segments better the \emph{pole} instance (Fig. \ref{fig:pred_set8} row ).
In Fig. \ref{fig:pred_set9} and \ref{fig:pred_set10}, we present a visual comparison 
with the Source-Only model.
It can be observed that the Source-Only model struggles to learn the correct class labels and instance masks. In contrast, EDAPS successfully bridges the domain gap by learning the correct semantics and instances.
EDAPS produces better panoptic segmentation for 
the \emph{bus} (Fig. \ref{fig:pred_set9} row 1, Fig. \ref{fig:pred_set10} row 1, 2, 3),
\emph{rider} (Fig. \ref{fig:pred_set9} row 2, 3, Fig. \ref{fig:pred_set10} row 5),
\emph{motorbike} (Fig. \ref{fig:pred_set9} row 3, Fig. \ref{fig:pred_set10} row 6),
\emph{car} (Fig. \ref{fig:pred_set10} row 3, 4),
\emph{traffic sign} (Fig. \ref{fig:pred_set9} row 6, Fig. \ref{fig:pred_set10} row 1).

\section{Visual Comparison: EDAPS vs. M-Dec-BU}
\label{sec:mdecbu-analysis}
This section offers a visual comparison of the predictions made by EDAPS and M-Dec-BU on the SYNTHIA  Cityscapes benchmark, as depicted in Fig. \ref{fig:edaps_vs_mdecbu_01}.
We observed that the M-Dec-BU baseline model tends to segment objects (like 
pedestrians, cars, buses, and riders) into smaller parts than necessary (i.e., over-segmentation).
Notice that the pedestrian, car, and bus instances in Figs.~\ref{fig:edaps_vs_mdecbu_01}~(a-d) are over-segmented.
This over-segmentation problem is more prominent in scenes  with large and occluded objects.


The M-Dec-BU model adopts a bottom-up approach for instance segmentation \cite{cheng2019panoptic}.
Unlike top-down methods \cite{he2017mask}, M-Dec-BU's instance head does not directly predict instance segmentation masks.
Rather, it predicts instance centers and offsets.
An additional post-processing step is required to generate the class-agnostic instance segmentation masks 
from these predicted centers and offsets.
We found that the center predictions are not sufficiently robust under a domain shift (even with domain adaptation) to support reliable post-processing on the target domain which leads to an over-segmentation problem as discussed above.
In contrast, we noticed that EDAPS's top-down instance segmentation head 
predicts highly generalizable instance masks on the target domain
resulting an improved instance segmentation performance (mAP 34.4\%) as
compared to 17.6\% mAP of M-Dec-BU.

\afterpage{\clearpage}
\begin{figure*}[p]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & CVRN~\cite{huang2021cross} & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} % \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_18_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_16_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_17_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_12_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_10_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_7_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_14_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_5_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_6_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions showing better panoptic segmentation for \emph{person} on SYNTHIA  Cityscapes.}
\label{fig:pred_person}
\end{figure*}  

\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & CVRN~\cite{huang2021cross} & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} % \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_24_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_19_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_8_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_31_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_15_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_25_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_26_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_27_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions showing better panoptic segmentation for \emph{rider}, \emph{motorbike}, \emph{bus}, and \emph{sign} classes on SYNTHIA  Cityscapes.}
\label{fig:pred_rider_motorbike}
\end{figure*}

\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & CVRN~\cite{huang2021cross} & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} % \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_20_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_23_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_9_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_13_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_2_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_3_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_28_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_4_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_11_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions showing better panoptic segmentation for thing (\emph{car}) and stuff (\emph{wall}, \emph{sign}, \emph{light}) classes on 
SYNTHIA  Cityscapes.}
\label{fig:pred_car}
\end{figure*}


\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & CVRN~\cite{huang2021cross} & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} % \begin{tikzpicture}
\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_1_pdf.pdf}};
\end{tikzpicture} \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_22_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_30_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_29_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/city_vis_21_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions showing better panoptic segmentation for \emph{bus}, \emph{traffic sign} and \emph{traffic light} on SYNTHIA  Cityscapes.}
\label{fig:pred_miscellaneous}
\end{figure*}


\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & CVRN~\cite{huang2021cross} & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} % \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_3_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_5_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_1_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_4_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_2_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions on SYNTHIA  Mapillary Vistas.}
\label{fig:pred_set8}
\end{figure*}

\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & Source-Only & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_9_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_10_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_11_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_8_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_6_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_7_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions on SYNTHIA  Mapillary Vistas.}
\label{fig:pred_set9}
\end{figure*}

\begin{figure*}[h!]
\centering
{\footnotesize
\begin{tabularx}{\linewidth}{*{4}{Y}}
Image & Source-Only & EDAPS (Ours) & Ground Truth \\
\end{tabularx}
} \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_16_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_12_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_17_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_13_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_14_pdf.pdf}};



\end{tikzpicture}%
 \begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/map_vis_15_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Example predictions on SYNTHIA  Mapillary Vistas.}
\label{fig:pred_set10}
\end{figure*}
\begin{figure*}[h!]
\centering
\begin{tikzpicture}


\draw (0.0, 0.0) node[inner sep=0pt] (image) {\includegraphics[width=\linewidth]{preds/edaps_vs_mdecbu_01_comp_pdf.pdf}};



\end{tikzpicture}%
 \scriptsize
\setlength\tabcolsep{1pt}
{
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{@{}*{20}{P{0.09\columnwidth}}@{}}
     {\cellcolor[rgb]{0.5,0.25,0.5}}\textcolor{white}{road}
     &{\cellcolor[rgb]{0.957,0.137,0.91}}sidew.
     &{\cellcolor[rgb]{0.275,0.275,0.275}}\textcolor{white}{build.}
     &{\cellcolor[rgb]{0.4,0.4,0.612}}\textcolor{white}{wall}
     &{\cellcolor[rgb]{0.745,0.6,0.6}}fence
     &{\cellcolor[rgb]{0.6,0.6,0.6}}pole
     &{\cellcolor[rgb]{0.98,0.667,0.118}}tr. light
     &{\cellcolor[rgb]{0.863,0.863,0}}tr. sign
     &{\cellcolor[rgb]{0.42,0.557,0.137}}veget.
     &{\cellcolor[rgb]{0.596,0.984,0.596}}terrain
     &{\cellcolor[rgb]{0.275,0.510,0.706}}sky
     &{\cellcolor[rgb]{0.863,0.078,0.235}}\textcolor{white}{person}
     &{\cellcolor[rgb]{1,0,0}}\textcolor{white}{rider}
     &{\cellcolor[rgb]{0,0,0.557}}\textcolor{white}{car}
     &{\cellcolor[rgb]{0,0,0.275}}\textcolor{white}{truck}
     &{\cellcolor[rgb]{0,0.235,0.392}}\textcolor{white}{bus}
     &{\cellcolor[rgb]{0,0.392,0.471}}\textcolor{white}{train}
     &{\cellcolor[rgb]{0,0,0.902}}\textcolor{white}{m.bike}
     & {\cellcolor[rgb]{0.467,0.043,0.125}}\textcolor{white}{bike}
     &{\cellcolor[rgb]{0,0,0}}\textcolor{white}{n/a.}
\end{tabular}
} \vspace{-0.15cm}
\caption{Visual comparison of EDAPS and M-Dec-BU (baseline) predictions on SYNTHIA  Cityscapes.}
\label{fig:edaps_vs_mdecbu_01}
\end{figure*} 
\end{document}