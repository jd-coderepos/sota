\section{Experiments}\label{experiments}
\subsection{Shrinking the gap on in-context learning}\label{eval_ker}
We begin by empirically motivating the  design, including the choice of long convolution parametrization. We consider the suite of tasks described in Table \ref{table:grok}.
\begin{table}[b]
    \small
    \centering
    \caption{A selection of our \textit{mechanistic design} benchmarks.}
    \setlength{\tabcolsep}{5.8pt}
    \label{table:grok}
    \begin{tabular}{@{}c|cc@{}}
    \toprule
    \textbf{Task} &\multicolumn{1}{c}{\textbf{Prompt}}&\multicolumn{1}{c}{\textbf{Target}}\\
    \midrule 
    Associative Recall & a, , b, e, , f, b & e\\  
    Majority & a, g, g, g, e, f, g & g \\
    Counting & a, b, b, b, a, c, b & 4\\
    ICL of Functions & , ,  &  \\ 
    Arithmetic & , , , , , ,  & , ,  \\
    \bottomrule
    \end{tabular}
\end{table}
\begin{figure*}[t]
  \centering
  \begin{tikzpicture}[font=\small]

\definecolor{coral25314160}{RGB}{253,141,60}
\definecolor{cornflowerblue107174214}{RGB}{107,174,214}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{lightsteelblue158202225}{RGB}{158,202,225}
\definecolor{orangered2308513}{RGB}{230,85,13}
\definecolor{powderblue198219239}{RGB}{198,219,239}
\definecolor{steelblue49130189}{RGB}{49,130,189}
\definecolor{brown}{RGB}{165,42,42}
\definecolor{cornflowerblue}{RGB}{100,149,237}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{indianred}{RGB}{205,92,92}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{teal}{RGB}{0,128,128}
\definecolor{lightseagreen}{RGB}{32,178,170}

\begin{groupplot}[group style={group size=4 by 1, horizontal sep=.8cm}]
\nextgroupplot[
width=.28\linewidth,
xlabel={}, 
title={:~}, title style={at={(.5,.92)}},
xlabel={}, xlabel style={at={(.5,-.15)}},
xmin=0, xmax=4,
ymin=0, ymax=100,
xtick={0,1,2,3,4}, xticklabels={,,,,},
ytick={0,20,40,60,80,100}, yticklabels={0,20,40,60,80,100},
grid=both,
]
\addplot [line width=1pt, cornflowerblue107174214, dashed, mark=diamond*, mark size=3, mark options={solid}]
table {0 100
1 100
2 100
3 100
4 100
};
\addplot [line width=1pt, lightsteelblue158202225, dotted, mark=*, mark size=3, mark options={solid}]
table {0 98
1 99.8
2 99.4
3 99.4
4 98.8
};
\addplot [line width=1pt, powderblue198219239, mark=triangle*, mark size=3, mark options={solid,rotate=180}]
table {0 97.8
1 99.4
2 98.4
3 99.4
4 99.2
};
\addplot [line width=1pt, lightseagreen, mark=triangle*, mark size=3, mark options={solid}]
table {0 85.4
1 93
2 91.8
3 93.6
4 97.6
};
\addplot [line width=1pt, teal, mark=star, mark size=3, mark options={solid}]
table {0 62.6
1 98
2 97.2
3 50.4
4 56.8
};
\addplot [line width=1pt, indianred, mark=mystar, mark size=3, mark options={solid}]
table {0 93
1 99
2 99.6
3 100
4 100
};

\nextgroupplot[
width=.28\linewidth, 
title={:~}, title style={at={(.5,.92)}},
xlabel={}, xlabel style={at={(.5,-.15)}},
xmin=0, xmax=4,
ymin=0, ymax=100,
xtick={0,1,2,3,4}, xticklabels={,,,,},
ytick={0,20,40,60,80,100}, yticklabels={},
grid=both,
]

\addplot [line width=1pt, cornflowerblue107174214, dashed, mark=diamond*, mark size=3, mark options={solid}]
table {0 91
1 98
2 99.4
3 99.8
4 100
};
\addplot [line width=1pt, lightsteelblue158202225, dotted, mark=*, mark size=3, mark options={solid}]
table {0 71.6
1 88.8
2 93.9
3 97
4 94
};
\addplot [line width=1pt, powderblue198219239, mark=triangle*, mark size=3, mark options={solid,rotate=180}]
table {0 71.5
1 84.4
2 92.8
3 93.6
4 91.4
};
\addplot [line width=1pt, lightseagreen, mark=triangle*, mark size=3, mark options={solid}]
table {0 20
1 65
2 59.8
3 39.6
4 26.2
};
\addplot [line width=1pt, teal, mark=star, mark size=3, mark options={solid}]
table {0 16.2
1 18.2
2 35.2
3 16.4
4 15.8
};
\addplot [line width=1pt, indianred, mark=mystar, mark size=3, mark options={solid}]
table {0 93
1 99
2 99.6
3 100
4 100
};

\nextgroupplot[
width=.28\linewidth, 
title={:~}, title style={at={(.5,.92)}},
xlabel={}, xlabel style={at={(.5,-.15)}},
xmin=0, xmax=4,
ymin=0, ymax=100,
xtick={0,1,2,3,4}, xticklabels={,,,,},
ytick={0,20,40,60,80,100}, yticklabels={},
grid=both,
]
\addplot [line width=1pt, cornflowerblue107174214, dashed, mark=diamond*, mark size=3, mark options={solid}]
table {0 25.7
1 85
2 93.1
3 91.2
4 95.2
};
\addplot [line width=1pt, lightsteelblue158202225, dotted, mark=*, mark size=3, mark options={solid}]
table {0 14.3
1 44.1
2 18.4
3 25.4
4 13.4
};
\addplot [line width=1pt, powderblue198219239, mark=triangle*, mark size=3, mark options={solid,rotate=180}]
table {0 13.2
1 23.2
2 28.1
3 16.7
4 8.4
};
\addplot [line width=1pt, lightseagreen, mark=triangle*, mark size=3, mark options={solid}]
table {0 12.7
1 21.6
2 12.7
3 15.2
4 11.4
};
\addplot [line width=1pt, teal, mark=star, mark size=3, mark options={solid}]
table {0 9.6
1 9.3
2 10.3
3 7.4
4 9.7
};
\addplot [line width=1pt, indianred, mark=mystar, mark size=3, mark options={solid}]
table {0 38.8
1 98
2 98.8
3 99.2
4 99.6
};

\nextgroupplot[
width=.28\linewidth, 
title={:~}, title style={at={(.5,.92)}},
xlabel={}, xlabel style={at={(.5,-.15)}},
xmin=0, xmax=4,
ymin=0, ymax=100,
xtick={0,1,2,3,4}, xticklabels={,,,,},
ytick={0,20,40,60,80,100}, yticklabels={},
grid=both,
legend style = {
    draw=black,
    fill=none,
     /tikz/column 6/.style={column sep=5pt,},
    at={(0.45,-.35)},
},
legend columns=6
]
\addplot [line width=1pt, indianred, mark=mystar, mark size=3, mark options={solid}]
table {0 12.4
1 66.5
2 99.1
3 100
4 100
};\addlegendentry{};

\addplot [line width=1pt, cornflowerblue107174214, dashed, mark=diamond*, mark size=3, mark options={solid}]
table {0 20.4
1 38.6
2 52
3 51.2
4 49.7
};\addlegendentry{};
\addplot [line width=1pt, lightsteelblue158202225, dotted, mark=*, mark size=3, mark options={solid}]
table {0 10
1 16.4
2 8.4
3 4.3
4 4.2
};\addlegendentry{};
\addplot [line width=1pt, powderblue198219239, mark=triangle*, mark size=3, mark options={solid,rotate=180}]
table {0 10.2
1 12.2
2 8.6
3 4.1
4 4
};\addlegendentry{};
\addplot [line width=1pt, lightseagreen, mark=triangle*, mark size=3, mark options={solid}]
table {0 20
1 9.1
2 10.4
3 12
4 7.99
};\addlegendentry{};
\addplot [line width=1pt, teal, mark=star, mark size=3, ]
table {0 14.2
1 6.7
2 10.8
3 4.1
4 4.8
};\addlegendentry{};
\end{groupplot}

\draw ({}|-{}) node[
  scale=1.1,
  anchor=north,
  text=black,
  rotate=0.0
]{\textbf{Associative~Recall}};
\end{tikzpicture}
   \caption{Benchmark of long convolution parametrizations in order   operators on associative recall (\%). Our results show that implicit parametrizations scale more favorably in vocabulary size (number of possible values of tokens in the input) and length of the sequence.}
  \label{fig:synthetics1}
\end{figure*}
Our evaluation is grounded in recent work on mechanistic interpretability of Transformers \citep{elhage2021mathematical,power2022grokking,olsson2022context,zhang2022unveiling}. Recently, associative recall, in particular, has been successfully used to guide the design of H3 \citep{dao2022hungry}. We extend the suite of tasks from these works and include benchmarking more challenging versions of each task . For example, solving associative recall with a vocabulary size of only  reveals whether a model is structurally capable of performing recall. Testing on much longer sequences and larger vocabularies reveals additional gaps in performance that are otherwise hidden.
\paragraph{How to parametrize long convolutions}
We compare the performance of the following long convolution parametrizations for  and  in an order  Hyena:
\begin{itemize}[leftmargin=0.1in]
    \item Conv1d: Explicit convolutions (regular convolution layers with fixed filter size).
    \item FNO: Filters parametrized explicitly in the frequency-domain \citep{li2020fourier}.
    \item H3: Implicit parametrization using state-space models (SSMs), in particular the standard S4 \citep{gu2021efficiently}. 
    \item TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization of SSMs\footnote{Transfer functions roughly correspond to a frequency-domain representation of SSMs.} 
    \item CKConv: Implicit parametrization using {s} \citep{romero2021ckconv}. 
    \item : Combination of implicit parametrizations via {s} (with exponential decay modulation as shown in Figure \ref{fig:modul}), and short explicit filters.
\end{itemize}
All models have the same width and  layers. Figure \ref{fig:synthetics1} shows implicit approaches based on {\sf FFNs} outperform other long convolutions, with the gap widening on longer sequences and larger vocabulary sizes. We train a different model on each setting of sequence length and vocabulary size. The ranking is correlated with the ability to decouple sequence length from parameter count (, CKConv, TransferFunc, H3) and expressivity (Hyena, CKConv). We observe similar trends on the other tasks.
\paragraph{Pushing sequence length to the limit}
Next, we evaluate associative recall performance on extremely long sequences of length k. To the best of our knowledge, these represent the first empirical display of attention-free in-context learning on sequences of this length. The gap between parametrization schemes widens as shown in Appendix A, with  outperforming CKConv by  points.
\paragraph{Comparing operators}
We repeat our associative recall experiment, this time benchmarking different  layer models rather than changing the convolution parametrization: an order  Hyena, GSS \citep{mehta2022long}, H3 \citep{dao2022hungry}, AFT-conv \citep{zhai2021attention}, RWKV \citep{PENG_RWKV-LM_2021}, and a standard GPT \citep{brown2020language} using FlashAttention \citep{dao2022flashattention}. As shown in Table \ref{table:synthetic2},  is the only operator able to solve the task. Our results challenge the observation that only Transformers are capable of challenging in-context learning.
\begin{table}[t]
\small
\centering
\caption{Test accuracy (\%) for associative recall on longer sequences, vocabulary size . The symbol \xmark~is used to mark settings where the model does not fit in memory.}
\label{table:synthetic2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|c|c|c|ccccccc@{}}
\toprule
Sequence length &  & FlashTransformer & Transformer &\multicolumn{1}{c}{GSS}  &\multicolumn{1}{c}{H3} & \multicolumn{1}{c} {AFT} & \multicolumn{1}{c}{RWKV} \\
\midrule 
k &  &  & \xmark &  &  &  &  \\
k &  &  & \xmark &  &  &  &  \\
k &  & \xmark & \xmark &  &  &  &  \\
\bottomrule
\end{tabular}
\end{table}
Surprisingly, rankings of model performance at a fixed sequence length on {\sf The Pile} are consistent with rankings on aggregate scores on our synthetics (Appendix \ref{app:add_results}).
\paragraph{Generality of  operators and filters}
 operators and filters can also applied successfully beyond language tasks. We experiment on sequential CIFAR, where pixels are flattened as a sequence, and use the same operator defined for language. We reach the accuracy of standard S4 \citep{gu2021efficiently} with same model size (). In Section \ref{sec:image_classification} and Appendix \ref{appendix:experiment-details}, we discuss larger-scale image classification experiments with Hyena.
\subsection{Language Modeling}\label{res:lm}
Next, we verify the scaling of  on autoregressive language modeling. We evaluate the perplexity on {\sc WikiText103} (Table \ref{wt103}) and {\sc The Pile} (Table \ref{pile}). On the {\sc The Pile}, we train different models for  billion tokens (different runs), adjusting the learning rate scheduler.  is the first attention-free, convolution architecture to match GPT quality with a \footnote{The FLOP reduction consists in the \textit{non-parametric} FLOPs of {} devoted to attention matrix computation. The ratio of parametric to non-parametric FLOPs (and hence the gains) depend on the ratio of model width  and sequence length  used in training.} reduction in total FLOPs. Preliminary scaling laws are shown in Figure \ref{fig:hyena_laws}, collecting the training runs at  billion tokens. Each curve represents a different training run.
In Appendix \ref{appendix:experiment-details}, we provide results on the PG-19 long-range benchmark \citep{raecompressive2019}.



\begin{figure}[t]
    \centering
    \begin{tikzpicture}[font=\small]

\definecolor{brown}{RGB}{165,42,42}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{firebrick}{RGB}{178,34,34}
\definecolor{indianred}{RGB}{205,92,92}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{lightseagreen}{RGB}{32,178,170}
\definecolor{teal}{RGB}{0,128,128}
\definecolor{turquoise}{RGB}{64,224,208}

\begin{axis}[
width=8.3cm, height=6cm,
title={},
title style = {at={(.5, .94)}},
xlabel={}, ylabel={}, 
ylabel style={at={(-.12,.5)}},
xmin=7e+18, xmax=4.9e+19, ymin=2.2, ymax=2.45,
major grid style={lightgray,thin},
minor grid style={lightgray,very thin},
grid=both,
xtick={1.311e+19,1.61e+19,2.623e+19,3.185e+19,3.93e+19,4.77e+19},
xticklabels={
  ,
  ,
  ,
  ,
  ,
  
},
ytick={2.44, 2.29, 2.21},
yticklabels={
  ,
  ,
  
}
]
\addplot [mark=mystar, line width=1pt, firebrick]
table{x  y
1.311e+19 2.43
2.623e+19 2.30
3.934e+19 2.22
};\addlegendentry{};
\addplot [draw=black, mark=diamond*, turquoise]
table{x  y
1.592e+19 2.44
3.185e+19 2.29
4.777e+19 2.21
};\addlegendentry{};
\end{axis}

\end{tikzpicture}
     \vspace{-4mm}
    \caption{Preliminary "scaling law" of language models on {\sc The Pile}. Comparison of our approach (red) based on long convolutions and gating () and a standard GPT (blue) \citep{brown2020language}. We reach perplexity of GPT with a smaller training FLOP budget.}
    \label{fig:hyena_laws}
\end{figure}


\begin{table}[t]
    \small
    \centering
    \begin{minipage}{.38\textwidth}
        \centering
        \caption{Perplexity on {\sc WikiText103} (same tokenizer).  are results from \citep{dao2022hungry}. Deeper and thinner models (Hyena-slim) achieve lower perplexity.}
        \vspace{2mm}
        \label{wt103}
        \setlength{\tabcolsep}{4pt}
        \begin{tabular}{@{}c|cc@{}}
        \toprule
        Model &\multicolumn{1}{c}{{\sc Perplexity}} \\
        \midrule 
        Transformer (M) &  & \\
        Hybrid H3 (M) &  \\
        Performer (M) &   \\ 
        Reformer  (M)&   \\ 
        \midrule
        AFT-conv (M) &  \\
        Linear Attention (M) &  \\
        \midrule
        - (M) &  \\
        --slim (M) &  \\
        \bottomrule
        \end{tabular}
    \end{minipage}
\hspace{0.7cm}
\begin{minipage}{.55\textwidth}
        \centering
        \vspace{-18mm}
        \caption{Perplexity on {\sc The Pile} for models trained until a total number of tokens e.g.,  billion (different runs for each token total). All models use the same tokenizer (GPT2). FLOP count is for the  billion token run.}
        \vspace{2mm}
        \label{pile}
        \setlength{\tabcolsep}{4pt}
        \begin{tabular}{@{}c|cccc@{}}
        \toprule
        Model &\multicolumn{1}{c}{{\sc B}} & \multicolumn{1}{c}{{\sc B}} & \multicolumn{1}{c}{{\sc B}} & \multicolumn{1}{c}{{\sc FLOPs ()}}\\
        \midrule 
        GPT (M) &  &  &  &  \\
        - (M)&  &  &  &  \\
        \midrule
        GPT (M) &  &  &  &  \\
        - (M) &  &  &  &  \\
        \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}



\subsection{Downstream Evaluation}
We perform a downstream evaluation on SuperGLUE \citep{wang2019superglue} tasks. We compare  (trained for  billion tokens) with the best available pre-trained attention-free model, RWKV \citep{PENG_RWKV-LM_2021} (trained for  billion tokens), and a reference GPTNeo \citep{gpt-neo} (trained for  billion tokens) of the same size. Tables \ref{supergluezero} and \ref{supergluefew} summarize the results.  performs similarly to other models despite having been trained on less than half the number of total tokens. We observe  to display characteristic few-shot capabilities of standard Transformers, with some tasks e.g., MultiRC seeing a lift of more than  accuracy over zero-shot when the model is provided additional prompts as context. The improvements are more noticeable in generation tasks, where the additional prompts can instruct the model on how it should be responding to the questions. We report an additional downstream evaluation on the LAMBADA  task \citep{paperno2016lambada} in Appendix \ref{appendix:experiment-details}.
\begin{table}[!h]
\small
\centering
\caption{Zero-shot accuracy () on {\sc SuperGLUE} tasks for small models.}
\label{supergluezero}
\vspace{2mm}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Model &\multicolumn{1}{c}{{\sc WSC}} & \multicolumn{1}{c}{{\sc WIC}} & \multicolumn{1}{c}{{\sc RTE}} & \multicolumn{1}{c}{{\sc CB}} & \multicolumn{1}{c}{{\sc MultiRC}} &  \multicolumn{1}{c}{{\sc ReCoRD}} &  \multicolumn{1}{c}{{\sc BoolQ}} &  \multicolumn{1}{c}{{\sc COPA}} & \multicolumn{1}{c}{{\sc Average}} \\
\midrule 
GPTNeo \citep{gpt-neo} &  &  &  &  &  &  &  &  &  \\ 
RWKV \citep{PENG_RWKV-LM_2021} &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[!h]
\small
\centering
\caption{Few-shot () accuracy () on {\sc SuperGLUE} tasks for small models.}
\label{supergluefew}
\vspace{2mm}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Model &\multicolumn{1}{c}{{\sc WSC}} & \multicolumn{1}{c}{{\sc WIC}} & \multicolumn{1}{c}{{\sc RTE}} & \multicolumn{1}{c}{{\sc CB}} & \multicolumn{1}{c}{{\sc MultiRC}} &  \multicolumn{1}{c}{{\sc ReCoRD}} &  \multicolumn{1}{c}{{\sc BoolQ}} &  \multicolumn{1}{c}{{\sc COPA}} & \multicolumn{1}{c}{{\sc Average}} \\
\midrule 
GPTNeo \citep{gpt-neo} &  &  &  &  &  &  &  &  &  \\ 
RWKV \citep{PENG_RWKV-LM_2021} &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Benchmarking}
We benchmark runtime of an order   operator compared to attention and FlashAttention layers \citep{dao2022flashattention}.  uses a fused CUDA kernel to perform  \citep{dao2022hungry}. We set batch size to  and measure runtime (in milliseconds). Results are provided in Figure \ref{fig:benchmarking_plot}.  speedups reach  at sequence length K. Crossover points for  and attention is at length , and for  and FlashAttention is between  and . Despite the absolute reduction in FLOPs, speedups are achieved only on longer sequences when the gap grows sufficiently large. This occurs because hardware utilization of  is lower than FlashAttention. We expect the gap between theoretical maximum speedup to shrink with improved implementations of  and specialized hardware.
\begin{figure}[t]
    \centering
    \begin{tikzpicture}[font=\footnotesize
]
\definecolor{brown}{RGB}{165,42,42}
\definecolor{cornflowerblue}{RGB}{100,149,237}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{indianred}{RGB}{205,92,92}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{teal}{RGB}{0,128,128}
\definecolor{lightseagreen}{RGB}{32,178,170}
 \begin{axis}[
   width=.5\linewidth, height=4cm,
xmode=log,
   xmin=1000, xmax = 100000,
   ymin=0,
   xlabel=In,
   ylabel=out,
   name=ax1,
   xlabel={},
   ylabel={},
 ]
\addplot [line width=1pt, indianred]
table {1024 0.9
2048 1.16
4096 1.47
8192 1.5
16384 2.84
32768 5.41
65536 11.32
};
\addplot [line width=1pt, cornflowerblue]
table {1024 0.4
2048 1.25
4096 2.16
8192 6.17
16384 21.74
32768 90.71
};
\addplot [line width=1pt, lightseagreen, dashed]
table {1024 0.29
2048 0.3
4096 0.63
8192 2.1
16384 8.33
32768 32.85
65536 129.07
};


\coordinate (c1) at (axis cs:1024,0);
  \coordinate (c2) at (axis cs:1024,7);
\draw (c1) rectangle (axis cs:8196,7);


\end{axis}
\begin{axis}[
    width=.5\linewidth, height=4cm,
    name=ax2,
    xmin=1000,xmax=8192,
    ymin=0,ymax=7,
    xlabel={},
xmode=log,
    xminorgrids=true,
    legend style={font=\footnotesize, draw=none, fill=none, align=left},
    legend columns = 3,
    legend cell align={left},
at={()},
clip=false
 ]
    \addplot [line width=1pt, indianred, opacity=.9]
        table {1024 0.9
        2048 1.16
        4096 1.47
        8192 1.5
    };\addlegendentry{}
    \addplot [line width=1pt, cornflowerblue]
        table {1024 0.4
        2048 1.25
        4096 2.16
        8192 6.17
    };\addlegendentry{}
    \addplot [line width=1pt, lightseagreen, dashed]
        table {1024 0.29
        2048 0.3
        4096 0.63
        8192 2.1
    };\addlegendentry{}
\end{axis}

\draw [dashed] (c1) -- (ax2.south west);
\draw [dashed] (c2) -- (ax2.north west);

\draw ({}|-{}) node[
  scale=1,
  anchor=north,
  text=black,
  rotate=0.0
]{\textbf{Benchmarking~Hyena}};
\end{tikzpicture}     \vspace{-5mm}
    \caption{Benchmarking runtime of Hyena, Attention and FlashAttention with varying sequence lengths. Batch size is set to . The figure on the right is an inset showing a zoomed-in portion of the figure on the left.}
    \label{fig:benchmarking_plot}
\end{figure}

\subsection{Large-Scale Image Classification}\label{benchm}
\label{sec:image_classification}
Finally, we demonstrate the potential of  as a general deep learning operator by applying it to image classification. On {\sf ImageNet}, we drop-in replace attention layers in the \textit{Vision Transformer} (ViT) \citep{dosovitskiy2020image} with the  operator (without changes from its language counterpart) and match performance with ViT. We also show that using smaller image patches boosts performance in both attention and . Since this results in longer sequence lengths, we expect  to outperform in speed as patches get more fine-grained approaching pixel-level. On CIFAR-2D, we test a 2D version of  long convolution filters in a standard convolutional architecture, which improves on the 2D long convolutional model S4ND \citep{nguyen2022s4nd} in accuracy with a  speedup and 25\% fewer parameters. See Appendix \ref{appendix:image-classification} for additional vision architectures and training procedure details.



\begin{table}[h]
\small
\centering
\caption{Image classification top-1 accuracy.}
\label{image_results}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|cccc@{}}
\toprule
Model & \multicolumn{1}{c} {{\sc Patch Size}} &  \multicolumn{1}{c} {{\sc Seq Len}} & \multicolumn{1}{c} {{\sc Dataset}} & \multicolumn{1}{c}{{\sc Acc (\%)}} \\
\midrule 
ViT (M) & 16x16 & 196 & ImageNet-1k & 78.5 \\
Hyena-ViT (M) & 16x16 & 196 & ImageNet-1k & 78.5 \\
\midrule
ViT (M) & 8x8 & 1024 & ImageNet-1k & 80.0 \\
Hyena-ViT (M) & 8x8 & 1024 & ImageNet-1k & 79.8 \\
\midrule
S4ND-ISO (k) & - & - & CIFAR-10 & 89.9 \\
Hyena-ISO (k) & - & - & CIFAR-10 & 91.2 \\
\bottomrule
\end{tabular}
\end{table}