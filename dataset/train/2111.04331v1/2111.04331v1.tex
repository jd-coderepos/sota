\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithmicx,algorithm}
\usepackage{pifont}
\usepackage{array}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{color}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}

\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}



\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Enhancing Prototypical Few-Shot Learning by Leveraging the Local-Level Strategy}
\name{Junying~Huang,~Fan~Chen,~Keze~Wang,~Liang~Lin,~Dongyu~Zhang*\thanks{*Corresponding author}}
\address{Sun Yat-Sen University}





\begin{document}
\maketitle
\begin{abstract}
Aiming at recognizing the samples from novel categories with few reference samples, few-shot learning (FSL) is a challenging problem. We found that the existing works often build their few-shot model based on the image-level feature by mixing all local-level features, which leads to the discriminative location bias and information loss in local details. To tackle the problem, this paper returns the perspective to the local-level feature and proposes a series of local-level strategies. Specifically, we present (a) a local-agnostic training strategy to avoid the discriminative location bias between the base and novel categories, (b) a novel local-level similarity measure to capture the accurate comparison between local-level features, and (c) a local-level knowledge transfer that can synthesize different knowledge transfers from the base category according to different location features. Extensive experiments justify that our proposed local-level strategies can significantly boost the performance and achieve 2.8–7.2 improvements over the baseline across different benchmark datasets, which also achieves the state-of-the-art accuracy.
\end{abstract}
\begin{keywords}
Few-shot Learning, Image Classification, Local-level Feature, Knowledge Transfer, Neural Network.
\end{keywords}
\section{Introduction}
\label{sec:intro}
Few-shot learning aims to recognize samples from novel categories with only a few labeled samples. Existing methods can be roughly divided into two main streams: (a) Metric-based approaches \cite{hou2019cross, kye2020transductive, snell2017prototypical, vinyals2016matching, zhang2020deepemd,wang2019simpleshot, multi-pro} focus on learning an appropriate feature space for both base and novel categories, and then classify the query data according to their distance between support data on the feature space. ProtoNet \cite{snell2017prototypical} proposes to build a prototype for each category and recognize by the nearest neighbor rule. The recent FEAT \cite{ye2020set} also proposes to refine the prototype by a Transformer module, and DeepEMD proposes a high-complexity distance. (b) Optimization-based approaches \cite{finn2017model, hu2020empirical, lee2019meta, gidaris2018dynamic, liu2019large} focus on improving the transferability of the fully trained model and transferring the knowledge of base categories to novel categories. For example, \cite{gidaris2018dynamic, liu2019large} propose an attention module to refine query features/prototypes by the knowledge of the base category.


\begin{figure}[t]
	\begin{center}
\includegraphics[width=\columnwidth]{heatmap.pdf}
	\end{center}
	\vspace{-15pt}
	\caption{Feature heat map of novel category samples extracted from models with global pooling or local-agnostic training (LAT): The color overlaid on the original image represents the -norm of the location feature (Red means high and blue means low). The higher -norm can represent more model attention to the corresponding location because the corresponding feature has greater weight for the classification.}
	\label{fig: heatmap}
	\vspace{-10pt}
\end{figure}


Although these methods have achieved encouraging success, we observe that they often follow the common routine in the traditional image classification task, i.g., extracting the image-level feature vector by global pooling \cite{lin2013network} and then building their models on the image-level feature space. Under the fully supervised training by a large amount of labeled data, global pooling can improve the performance by driving the model to focus attention on the local features considered discriminative and suppress the features on the insignificant location. For example, in Fig. \ref{fig: heatmap} (b), the high-weight local features are only distributed at concentrated locations. However, these discriminative features are distributed in different locations for different categories. We call this difference \textit{discriminative location bias}. Few-shot learning requires the model to recognize the object belonging to the novel category. Hence, the discriminative location bias will cause the model to over-pay attention to the location beneficial to the base category while ignoring the discriminative features of the novel category. As shown in Fig. \ref{fig: heatmap} (b), the model trained with global pooling only pays attention to the insignificant location on the novel category images, resulting in the wrong classification.

 
To avoid the discriminative location bias, we first propose a local-level classification loss and a local-level regularization loss to realize local-agnostic training (LAT), which forces the model to pay attention to all locations instead of only the discriminative location. As shown in Fig. \ref{fig: heatmap} (c), the high-weight local features from the model trained with the LAT strategy can cover the complete object of novel categories.

Furthermore, during the inference phase, the image-level features of the novel category are either misled by the discriminative location bias (from the model trained with global pooling) or confused by too much information (from the model trained with LAT strategy). Meanwhile, the image-level feature vector also destroys the local details in the image. Therefore, we further propose two local-level strategies during inference for a comprehensive understanding of novel category images. First, we propose a hybrid local-level similarity measure (LSM) to measure the similarity between local-level features accurately, composed of local distance and matching distance. Then, we propose a local-level knowledge transfer (LKT), which can synthesize the different knowledge transfers from base categories for different location features.

Extensive experiments justify that our proposed local-level strategies can significantly boost the few-shot performance. On the ProtoNet \cite{snell2017prototypical} baseline, it achieve 2.8–7.2 improvements. Compared with the competitive works, it doesn't require the complex algorithm \cite{zhang2020deepemd} or additional model \cite{multi-pro, ye2020set} but still achieves the state-of-the-art accuracy.


\section{Background Knowledge}
\label{sec:preliminary}

Given disjoint category sets , ,  and the corresponding datasets , , , the objective of few-shot learning (FSL) is to train a robust model on  which can generalize well on  with few reference samples per category. The reference data is called the support set and the recognized data is called the query set. Generally, the model is evaluated by the -way -shot episode classification \cite{vinyals2016matching} in FSL. In each episode, a support set  and a query set  are sampled from  with the same  categories, where  contains  examples for each category with available labels. Then, the evaluation metric is defined as the average accuracy on  under multiple episodes. In this work, we use the setting in \cite{vinyals2016matching}, i.e., ,  and  randomly samples 15 examples for each category.

We build our model on top of ProtoNet \cite{snell2017prototypical}, which is a popular metric-based method due to its simplicity and effectiveness.
In each episode, the ProtoNet first computes a prototype (center) for each class in ,

where  means the embedding extractor,  means the subset of  with label  and  means the set size.
After that, the probability distribution of the query image  is predicted by the distance function  and softmax function ,




\begin{figure}[h]
    \begin{center}
    \includegraphics[width=\columnwidth]{overview_small3.pdf}
    \end{center}
    \vspace{-15pt}
  \caption{Overview of framework: The red and blue arrows mean the process in the training and inference phase respectively. We also mark the proposed enhancement in yellow.
    }
    \vspace{-10pt}
\label{fig:overview}
\end{figure}

\section{Methodology}
\label{approach}
In this section, we describe the model architecture with local-agnostic training objective (Sec. \ref{sec:inference}), the local-level similarity measure (Sec. \ref{sec:distance}), and the local-level knowledge transfer (Sec. \ref{sec:adaption}) in order.
The overview of our framework is depicted in Fig. \ref{fig:overview}. In particular, we mark the position of three proposed strategies in the framework by yellow, and the local-agnostic training and local-level knowledge transfer are visualized in Fig. \ref{fig:loss} and Fig. \ref{fig:adaption} in detail.





\subsection{Framework}
\label{sec:inference}
\noindent\textbf{Model Architecture}: Our model consists of an embedding extractor  and a classifier . Specifically, we use ResNet-12 \cite{he2016deep} with four blocks as the extractor and extract the feature map from the last block  for classification. The classifier  consists of a 1x1 convolution function  without bias and a softmax function , which classifies each local feature on the last feature map for the local-level classification:



\noindent\textbf{Training Objective}: 
We adopt the episode-based training \cite{snell2017prototypical} to avoid the over-fitting on . In each training, an -way -shot episode  that is considered as both support set and query set was sampled. As shown in Fig. \ref{fig:loss}, the training objective consists of the image-level similarity loss , as well as the local-level classification  and regularization loss  for local-agnostic training. The image-level similarity loss aims to minimize the intra-class distance and maximize the inter-class distance in the feature space, which is defined as

where  is cross-entropy function,  is softmax function, the  is the prototype of category  computed by Eq. \ref{eq:pro_int}, and the distance function  is described in Sec. \ref{sec:distance}.

Then we consider local-agnostic training: In the image-level classification loss, the gradient of low-discrimination locations is diluted by the global pooling function, which leads to the discriminative location bias. To avoid the problem, we first replace the image-level classification loss with the local-level classification loss (Eq. \ref{eq: lc}). It optimizes the classification loss for each location on the feature map, which encourages the model to focus on all location features, whether high or low discrimination. Specifically, we use a 1x1 convolution function described above as the classifier, and the local-level classification loss is computed by the cross-entropy :



Then, we propose a local-level regularization loss , which further avoids the discriminative location bias by punishing the model for paying too much attention to some location features. Specifically, we adopt the -norm of each local feature to evaluate the attention of the model to this location. Thus the local-level regularization loss is defined as


where  means the variance across all local features. The final optimization objective is the sum of three losses:




\begin{figure}[t]
	\begin{center}
	    \vspace{-10pt}
		\includegraphics[width=\columnwidth]{loss.pdf}
	\end{center}
	\vspace{-15pt}
	\caption{ The complete training objective, in which the learnable vectors are the weights of the 1x1 convolution classifier. }
	\label{fig:loss}
	\vspace{-8pt}
\end{figure}






\subsection{Local-level Similarity Measure}
\label{sec:distance}
Different from the image-level feature, the local-level feature is structural, so the similarity between local-level features should also consider their spatial structures. DeepEMD \cite{zhang2020deepemd} proposed to compute the minimal spatial matching between local-level features, but this destroys the original spatial structure. MetaOptNet \cite{lee2019meta} and MCT \cite{kye2020transductive} directly compare the spatial similarity, but it may not be accurate when the object posture and position vary. Different from them, we consider both the original spatial structures and the minimal spatial matching, which are visualized in Fig. \ref{fig:overview}. For each feature map , after normalizing them by Eq. (\ref{eq:normalize}), we first directly compare the spatial similarity of local-level features between query sample and prototype  by local distance :


Then we propose a simple minimum matching distance  between local-level features to capture the correct comparison when object position and posture vary, which is defined as

The final distance between the feature map and prototype is defined as the weighted sum of the two distances: 



\begin{figure}[h]
	\begin{center}
        \vspace{-10pt}
	    \includegraphics[width=0.7\columnwidth]{transfer.pdf}
	\end{center}
	\vspace{-15pt}
	\caption{ The process of LKT. ``Transfer'' means to replace each local feature as the weighted sum of the base knowledge. }
	\label{fig:adaption}
	\vspace{-10pt}
\end{figure}

\subsection{Local-level Knowledge Transfer}
\label{sec:adaption}
In FSL, the semantic information of novel datasets is unavailable during training, which leads the distribution of the novel category samples difficult to converge in the feature space. \cite{gidaris2018dynamic, liu2019large} propose an attention module to refine the distribution of novel samples by the similarity with the base category. Different from them, we argue that the image-level feature cannot correctly describe the semantic information of the novel category sample (Sec. \ref{sec:intro}).
Thus we introduce the local-level knowledge transfer during inference that refines all the location features separately. Specifically, we adopt the convergent classifier weight  (described in Sec. \ref{sec:inference}) as the knowledge (prototype) of base categories. As shown in Fig. \ref{fig:adaption}, for each sample from , we first compute a  similar-version  for the feature map , on which each location feature is computed by the probability-weighted sum of , 

Then, the local-level feature is refined as the weighted sum of the similar-version and original-version after normalization:






\begin{table*}[htp]
    \small
    \setlength{\abovecaptionskip}{0.0cm}
    \setlength{\abovecaptionskip}{0.0cm}
    \setlength{\belowcaptionskip}{0.1cm}
    \setlength{\tabcolsep}{0.8mm}{
      \centering
        \begin{tabular}{cccC{2.0cm}C{2.0cm}C{2.0cm}C{2.0cm}C{2.0cm}C{2.0cm}}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{MiniImageNet}  & \multicolumn{2}{c}{TieredImageNet} & \multicolumn{2}{c}{Cifar-FS}  \\
        \cline{3-8}
        & & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot  \\
        \midrule
MetaOptNet-SVM \cite{lee2019meta} & ResNet-12 
        & 62.64  0.61 & 78.63  0.46 & 65.99  0.72 & 81.56  0.53 & 72.00  0.70 & 84.20  0.50        \\
        Simple-Shot \cite{wang2019simpleshot} & WRN-28-10
        & 63.50  0.20 & 80.33  0.14 & 69.75  0.20 & 85.50  0.14 &       ---        &       ---        \\
        CAN \cite{hou2019cross} & ResNet-12
        & 63.85  0.48 & 79.44  0.34 & 69.89  0.51 & 84.23  0.37 &       ---        &       ---       \\
        R2-D2+Aug \cite{liu2020task} & ResNet-12
        & 64.79  0.45 & 81.08  0.32 &       ---        &       ---       & 76.51  0.47 & 87.63  0.34    \\
        S2M2R \cite{mangla2020charting} & WRN-28-10
        & 64.93  0.18 & \textbf{83.18}  0.11 &       ---        &       --- & 74.81  0.19 & 87.47  0.13           \\
        MCT (Inst) \cite{kye2020transductive} & ResNet-12 
        & 65.34  0.63 & 82.15  0.45 & 69.66  0.81 & 85.29  0.49 & 77.84  0.64 & 89.11  0.45  \\
        DeepEMD \cite{zhang2020deepemd} & ResNet-12
        & 65.91  0.82 & 82.41  0.56 & 71.16  0.87 & 86.03  0.58 &       ---        &       ---          \\
        FEAT \cite{ye2020set} & ResNet-12
        & 66.78  0.20 & 82.05  0.14 & 70.80  0.23 & 84.79  0.16 &       ---        &       ---        \\
        FEAT+Multi-Proto \cite{multi-pro} & ResNet-12
        & 67.24  0.58 & 82.51  0.66 &    ---    &    ---    &       ---        &       ---        \\
        \bottomrule
        ProtoNet* (baseline) & ResNet-12
        & 60.85  0.69 & 79.50  0.46 & 67.67  0.73 & 83.67  0.55 & 71.65  0.68 & 86.15  0.44  \\
        ProtoNet+LLS & ResNet-12 
        & \textbf{68.01}  0.63 & \textbf{83.26}  0.43 & \textbf{72.27}  0.71 & \textbf{86.50}  0.46 & \textbf{78.76}  0.67 & \textbf{89.60}  0.43  \\
        \bottomrule
         \cr 
        \end{tabular}
        \vspace{-10pt}
        \caption{ Average classification performance over 1,000 episodes with 95 confidence interval. ``---" means that the experiment is unavailable. * means the results is re-implemented.}
        \label{tab:performance_comparison_imagenet}
        }
        \vspace{-10pt}
\end{table*}





\section{Experiments}
\label{sec:experiments}
In this section, we evaluate the proposed local-level strategies (LLS), composed of the comparison with related works (Sec. \ref{sec:SOTA}) and the ablation studies (Sec. \ref{sec:ablation}).


\subsection{Experimental Setup}
\label{sec:experimental_setup}

\noindent\textbf{Datasets}:
We validate our framework on \textit{MiniImageNet} \cite{vinyals2016matching}, \textit{TieredImageNet} \cite{ren2018meta}, and \textit{CIFAR-FS} \cite{bertinetto2018meta} datasets. MiniImageNet and TieredImageNet are subsets of ILSVRC-2012 \cite{russakovsky2015imagenet}, while CIFAR-FS is a variant of CIFAR-100 dataset. They contain 64/16/20, 351/97/160, and 64/16/20 categories for training/validation/test, respectively.




\noindent\textbf{Implementation Details}: 
We use Pytorch to implement all our experiments on one NVIDIA TITAN GPU. All input images are resized to  and the channel size of each model block output is 64-128-256-512, respectively. In addition, we also adopt horizontal flip and random crop as data augmentation in the training phase. During training, the model is trained by an SGD optimizer for 50,000 episodes, in which  and  are set to 15 and 9. For TieredImageNet, we also adopt a batch-based pre-training step with only local-level classification loss followed by the episode-based training, in which the learning rates are initialized to 0.1 and 0.01 with the decay of factor 0.1 at every 20,000 episodes.  For other datasets, the learning rate is initialized to 0.1 and declines to 0.006 and 0.0012 at 30,000 episodes and 45,000 episodes, respectively. The balanced factor in Eq. (\ref{eq:l_all}) is set to . Other hyper-parameters are selected by the dataset validation, resulting in  for MiniImage,  for TieredImageNet, and  for CIFAR-FS.


\subsection{Comparison with the State-of-the-art Methods}
\label{sec:SOTA}
Tab. \ref{tab:performance_comparison_imagenet} shows the comparison with the latest methods. For a fair comparison, we report the model backbone and the average accuracy with 95 confidence interval over 1,000 episodes. As shown in Tab. \ref{tab:performance_comparison_imagenet}, LLS can significantly boost the performance of the baseline ProtoNet with up 2.8-7.2 on all benchmark datasets. Compared with competitive FEAT \cite{ye2020set}, Multi-Proto \cite{multi-pro}, and DeepEMD \cite{zhang2020deepemd} adopting extra Transformer module, BERT module, and high-complexity distance metric respectively, we only adopt the simple ProtoNet framework and light-weight strategies, but still achieve the state-of-the-art accuracy on all benchmark datasets. 



\subsection{Ablation Studies}
\label{sec:ablation}
Tab. \ref{tab:ablation_over} shows the ablation studies of three local-level strategies: local-agnostic training (LAT), local-level similarity measure (LSM), and local-level knowledge transfer (LKT). The performance when none of them is adopted can be regarded as our \textbf{baseline}, which is re-implemented from ProtoNet \cite{snell2017prototypical}. As can be seen, the proposed LLS can achieve performance improvements of 3.7-7.2 on the two datasets. The specific improvements are 2.3-4.4 by LAT, 0.8-2.1 by LSM, and 0.4-1.3 by LKT, respectively.








\begin{table}[h]
    \small
	\setlength{\abovecaptionskip}{0.0cm}
	\setlength{\abovecaptionskip}{0.0cm}
	\setlength{\belowcaptionskip}{0.1cm}
	\setlength{\tabcolsep}{0.8mm}{
		\centering
		\begin{tabular}{cccC{0.9cm}C{0.9cm}C{0.9cm}C{1.35cm}}
			\toprule
			\multirow{2}{*}{LAT} & \multirow{2}{*}{LSM} & \multirow{2}{*}{LKT} & \multicolumn{2}{c}{MiniImageNet} & \multicolumn{2}{c}{CIFAR-FS} \\
			\cline{4-7}
			& & & 1-shot & 5-shot & 1-shot & 5-shot  \\
			\ding{55} & \ding{55} & \ding{55} &  60.85 & 79.50 & 71.65 & 85.45 \\
			Cls       & \ding{55} & \ding{55} &  63.04 & 81.34 & 75.44 & 87.29 \\
			Cls+Reg   & \ding{55} & \ding{55} &  64.69 & 81.82 & 76.05 & 88.21 \\
			Cls+Reg   & Loc       & \ding{55} &  66.19 & 82.30 & 76.40 & 88.99 \\
			Cls+Reg   & Loc+Mat   & \ding{55} &  66.72 & 82.87 & 77.49 & 89.08\\
			Cls+Reg   & Loc+Mat   & \ding{51} &  \textbf{68.01} & \textbf{83.26}     & \textbf{78.76} & \textbf{89.60}     \\
\bottomrule
			\cr 
		\end{tabular}
    \vspace{-15pt}
	\caption{Ablation studies of three local-level strategies with 1,000 episodes on MiniImageNet and CIFAR-FS.}
    \label{tab:ablation_over}
    \vspace{-15pt}
	}
\end{table}











\section{Conclusion}
In this paper, we focus on the local-level feature and propose a series of local-level strategies to enhance the few-shot image classification by avoiding the discriminative location bias and information loss in local details. Extensive experiments show that the proposed local-level strategies can achieve significant improvements with 2.8–7.2 over the baseline, which also achieves the state-of-the-art accuracy. We hope our proposed new perspective on the local-level feature can inspire future works in few-shot learning.

\begin{thebibliography}{10}

\bibitem{hou2019cross}
Ruibing Hou, Hong Chang, MA~Bingpeng, Shiguang Shan, and Xilin Chen,
\newblock ``Cross attention network for few-shot classification,''
\newblock in {\em Advances in Neural Information Processing Systems}, 2019, pp.
  4003--4014.

\bibitem{kye2020transductive}
Seong~Min Kye, Hae~Beom Lee, Hoirin Kim, and Sung~Ju Hwang,
\newblock ``Transductive few-shot learning with meta-learned confidence,''
\newblock {\em arXiv preprint arXiv:2002.12017}, 2020.

\bibitem{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel,
\newblock ``Prototypical networks for few-shot learning,''
\newblock in {\em Advances in neural information processing systems}, 2017, pp.
  4077--4087.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.,
\newblock ``Matching networks for one shot learning,''
\newblock in {\em Advances in neural information processing systems}, 2016, pp.
  3630--3638.

\bibitem{zhang2020deepemd}
Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen,
\newblock ``Deepemd: Few-shot image classification with differentiable earth
  mover's distance and structured classifiers,''
\newblock in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, 2020, pp. 12203--12213.

\bibitem{wang2019simpleshot}
Yan Wang, Wei-Lun Chao, Kilian~Q Weinberger, and Laurens van~der Maaten,
\newblock ``Simpleshot: Revisiting nearest-neighbor classification for few-shot
  learning,''
\newblock {\em arXiv preprint arXiv:1911.04623}, 2019.

\bibitem{multi-pro}
Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, and Steven Schockaert,
\newblock ``Few-shot image classification with multi-facet prototypes,''
\newblock in {\em ICASSP 2021 - 2021 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, 2021, pp. 1740--1744.

\bibitem{ye2020set}
Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha,
\newblock ``Few-shot learning via embedding adaptation with set-to-set
  functions,''
\newblock in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine,
\newblock ``Model-agnostic meta-learning for fast adaptation of deep
  networks,''
\newblock {\em International Conference on Machine Learning}, 2017.

\bibitem{hu2020empirical}
Shell~Xu Hu, Pablo~G Moreno, Yang Xiao, Xi~Shen, Guillaume Obozinski, Neil~D
  Lawrence, and Andreas Damianou,
\newblock ``Empirical bayes transductive meta-learning with synthetic
  gradients,''
\newblock {\em arXiv preprint arXiv:2004.12696}, 2020.

\bibitem{lee2019meta}
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto,
\newblock ``Meta-learning with differentiable convex optimization,''
\newblock in {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2019, pp. 10657--10665.

\bibitem{gidaris2018dynamic}
S.~Gidaris and N.~Komodakis,
\newblock ``Dynamic few-shot visual learning without forgetting,''
\newblock in {\em CVPR}, 2018, pp. 4367--4375.

\bibitem{liu2019large}
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella~X
  Yu,
\newblock ``Large-scale long-tailed recognition in an open world,''
\newblock in {\em CVPR}, 2019, pp. 2537--2546.

\bibitem{lin2013network}
Min Lin, Qiang Chen, and Shuicheng Yan,
\newblock ``Network in network,''
\newblock {\em arXiv preprint arXiv:1312.4400}, 2013.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,
\newblock ``Deep residual learning for image recognition,''
\newblock in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2016, pp. 770--778.

\bibitem{liu2020task}
Jialin Liu, Fei Chao, and Chih-Min Lin,
\newblock ``Task augmentation by rotating for meta-learning,''
\newblock {\em arXiv preprint arXiv:2003.00804}, 2020.

\bibitem{mangla2020charting}
Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji
  Krishnamurthy, and Vineeth~N Balasubramanian,
\newblock ``Charting the right manifold: Manifold mixup for few-shot
  learning,''
\newblock in {\em The IEEE Winter Conference on Applications of Computer
  Vision}, 2020, pp. 2218--2227.

\bibitem{ren2018meta}
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky,
  Joshua~B. Tenenbaum, Hugo Larochelle, and Richard~S. Zemel,
\newblock ``Meta-learning for semi-supervised few-shot classification,''
\newblock in {\em Proceedings of 6th International Conference on Learning
  Representations {ICLR}}, 2018.

\bibitem{bertinetto2018meta}
Luca Bertinetto, Joao~F. Henriques, Philip Torr, and Andrea Vedaldi,
\newblock ``Meta-learning with differentiable closed-form solvers,''
\newblock in {\em International Conference on Learning Representations}, 2019.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.,
\newblock ``Imagenet large scale visual recognition challenge,''
\newblock {\em International journal of computer vision}, vol. 115, no. 3, pp.
  211--252, 2015.

\end{thebibliography}
 
\end{document}
