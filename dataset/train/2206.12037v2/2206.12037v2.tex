\documentclass{article} 










\usepackage{hyperref}
\hypersetup{
     colorlinks   = true,
     citecolor    = blue,
pdfborderstyle={/S/U/W 1}
}
\usepackage{url}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath,amsfonts,amsthm}       \usepackage{subcaption}
\usepackage{bm}
\usepackage{bbm}
\usepackage{multirow}
\usepackage[inline]{enumitem}
\usepackage{diagbox}
\usepackage[capitalise]{cleveref}  \usepackage{comment}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}         


\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}



\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{claim}{Claim}

\newcommand{\dt}{\Delta}
\newcommand{\paren}[1]{\left (#1 \right)}
\newcommand{\dd}{\mathop{}\!d}
\DeclareMathOperator{\hippo}{\mathsf{hippo}}
\DeclareMathOperator*{\diag}{diag}

\newcommand{\methodabbrv}{S4}


\newcommand{\discont}{t_0}
\newcommand{\twpa}{a}
\newcommand{\twpb}{b}
\newcommand{\twpc}{c}
\newcommand{\phid}{d}

\newtoggle{arxiv}
\toggletrue{arxiv}

\newcommand{\para}[1]{\paragraph{#1}}

\usepackage[square,numbers,sort]{natbib}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}
\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}
\setlength{\marginparwidth}{0.8in}
\setlength{\parskip}{5pt}\setlength{\parindent}{0pt}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\bibliographystyle{plainnat} \title{How to Train Your HiPPO: State Space Models with \\ Generalized Orthogonal Basis Projections}
\usepackage{authblk}
\author[]{Albert Gu\thanks{Equal contribution.}}
\author[]{Isys Johnson\samethanks}
\author[]{Aman Timalsina}
\author[]{Atri Rudra}
\author[]{Christopher R{\'e}}
\affil[]{Department of Computer Science, Stanford University}
\affil[]{{\texttt{albertgu@stanford.edu}, \texttt{chrismre@cs.stanford.edu}}}
\affil[]{Department of Computer Science and Engineering, University at Buffalo}
\affil[]{{\texttt{\{isysjohn,amantima,atri\}@buffalo.edu}}}
\date{}


\begin{document}


\maketitle



\begin{abstract}
  Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4).
  A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix,
  which was empirically important for S4's ability to handle long sequences.
  However, the specific matrix that S4 uses was actually derived in previous work for a particular \emph{time-varying} dynamical system,
and the use of this matrix as a \emph{time-invariant} SSM had no known mathematical interpretation.
Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained.
We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.
Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter.
These insights improve S4's performance to 86\% on the Long Range Arena benchmark, with 96\% on the most difficult Path-X task.
\end{abstract}
 
\section{Introduction}
\label{sec:intro}


The Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks \citep{gu2022efficiently}.
It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties \citep{gu2021lssl}:
as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time.
These properties give S4 remarkable computational efficiency and performance, especially when modeling continuous signal data and long sequences.

Despite its potential, several aspects of the S4 model remain poorly understood.
Most notably, \citet{gu2022efficiently} claim that the long range effects of S4 arise from instantiating it with a particular matrix they call the \textbf{HiPPO matrix}.
However, this matrix was actually derived in prior work for a particular \emph{time-varying} system \citep{gu2020hippo},
and the use of this matrix in a \emph{time-invariant} SSM did not have a mathematical interpretation.
Consequently, the mechanism by which S4 truly models long-range dependencies is actually not known.
Beyond this initialization, several other aspects of parameterizing and training S4 remain poorly understood.
For example, S4 involves an important timescale parameter ,
and suggests a method for parameterizing and initializing this parameter, but does not discuss its meaning or provide a justification.




This work aims to provide a comprehensive theoretical exposition of several aspects of S4.
The major contribution of this work is a cleaner, more intuitive, and much more general formulation of the HiPPO framework. This result directly generalizes all previous known results in this line of work \citep{voelker2019legendre,gu2020hippo,gu2021lssl,gu2022efficiently}.
As immediate consequences of this framework:
\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item We prove a theoretical interpretation of S4's state matrix , explaining S4's ability to capture long-range dependencies via decomposing the input with respect to an infinitely long, exponentially-decaying measure (\cref{fig:1} (\emph{Left})).

  \item We derive new HiPPO matrices and corresponding S4 variants that generalize other nice basis functions.
    For example, our new method S4-FouT produces \emph{truncated Fourier basis functions}. This method thus automatically
    captures sliding Fourier transforms (e.g. the STFT and spectrograms) which are ubiquitous as a hand-crafted signal processing tool,
    and can also represent any \emph{local convolution}, thus generalizing conventional CNNs (\cref{fig:1} (\emph{Middle})).

  \item We provide an intuitive explanation of the timescale , which has a precise interpretation as controlling the length of dependencies that the model captures. Our framework makes it transparent how to initialize  for a given task, as well as how to initialize the other parameters (in particular, the last SSM parameter ) to make a deep SSM variance-preserving and stable.
\end{itemize}


\begin{figure}[!t]
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/basis_legs_clean.png}
  \end{subfigure}
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/basis_fout_1024.png}
  \end{subfigure}
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/delay_fout_1024.png}
  \end{subfigure}
  \caption{
    In this work, we focus on a more intuitive interpretation of state space models as a convolutional model where the convolution kernel is a linear combination of basis functions.
    We introduce a generalized framework that allows deriving state spaces  that produce particular basis functions, leading to several generalizations and new methods.
    (\emph{Left}: \textbf{LegS}) We prove that the particular  matrix chosen in S4 produces Legendre polynomials under an exponential re-scaling, resulting in smooth basis functions with a closed form formula.
    This results in a simple mathematical interpretation of the method as orthogonalizing against an exponentially-decaying measure, granting the system better ability to model long-range dependencies.
    (\emph{Middle, Right}: \textbf{FouT})
    We derive a new SSM that produces approximations to the \textbf{truncated Fourier} basis, perhaps the most intuitive and ubiquitous set of basis functions. This method generalizes sliding Fourier Transforms and local convolutions (i.e. CNNs), and can also encode spike functions to solve classic memorization tasks.
  }
  \label{fig:1}
\end{figure}

Empirically, we validate our theory on synthetic function reconstruction and memorization tasks, showing that empirical performance of state space models in several settings is predicted by the theory.
For example, our new S4-FouT method, which can provably encode a spike function as its convolution kernel, performs best on a continuous memorization task compared to other SSMs and other models, when  is initialized correctly.
Finally,
we show that the original S4 method is still best on very long range dependencies, achieving a new state of the art of \textbf{86\%} average on Long Range Arena, with \textbf{96\%} on the most difficult Path-X task that even the other S4 variants struggle with.









 
\section{Background}
\label{sec:background}



\subsection{State Space Models: A Continuous-time Latent State Model}
\label{sec:ss-continuous}

The state space model (\textbf{SSM}) is defined by the simple differential equation \eqref{eq:ssm-1} and \eqref{eq:ssm-2}.
It maps a 1-D input signal  to an -D latent state 
before projecting to a 1-D output signal .

\begin{minipage}{.5\linewidth}
  \noindent
  \vspace*{-8pt}
  
\end{minipage}
\begin{minipage}{.5\linewidth}
  \vspace*{4pt}
  
\end{minipage}

For the remainder of this paper, we will assume  and omit it for simplicity, unless explicitly mentioned. 

SSMs can in general have dynamics that change over time, i.e. the matrices  are a function of  in \eqref{eq:ssm-1} and \eqref{eq:ssm-2}.
However, when they are constant
the system is \textbf{linear time invariant (LTI)}, and is equivalent to a convolutional system \eqref{eq:ssm-convolution}.
The function  is called the \textbf{impulse response} which can also be defined as the output of the system when the input  is the impulse or Dirac delta function.
We will call these \textbf{time-invariant state space models} (\textbf{TSSM}).
These are particularly important because the equivalence to a convolution makes TSSMs parallelizable and very fast to compute, which is critical for S4's efficiency.
\looseness=-1

\looseness=-1
Our treatment of SSMs will consider the  parameters separately from .
We will refer to an SSM as either the tuple  (referring to \eqref{eq:ssm-convolution}) or  (referring to \cref{def:ssm-basis}) when the context is unambiguous.
We also drop the T in TSSM when the context is clearly time-invariant.

\begin{definition}\label{def:ssm-basis}
  Given a TSSM ,
   is a vector of  functions
  which we call the \textbf{SSM basis}. The individual basis functions are denoted ,
  which satisfy . Here  is the one-hot basis vector.
\end{definition}
This definition is motivated by noting that the SSM convolutional kernel is a linear combination
of the SSM basis controlled by the vector of coefficients ,
.



\paragraph{Discrete SSM with Timescales.}
To be applied on a discrete input sequence  instead of continuous function ,
\eqref{eq:ssm-1} must be discretized by a \textbf{step size}  that represents the resolution of the input.
Conceptually, the inputs  can be viewed as sampling an implicit underlying continuous signal , where .
Analogous to the fact that the SSM has equivalent forms either as an \emph{dynamical system} \eqref{eq:ssm-1} or a \emph{continuous convolution} \eqref{eq:ssm-convolution},
the discrete-time SSM can be computed either as a \emph{recurrence} or a \emph{discrete convolution}.
The mechanics to compute the discrete-time SSM has been discussed in previous works \citep{gu2021lssl,gu2022efficiently}.
For our purposes, we only require the following fact:
for standard discretization methods used in prior work, discretizing the state space  at a step size  is exactly equivalent to discretizing the state space  at a step size .
This allows thinking of  simply as modulating the SSM parameters  instead of representing a step size.

A poorly understood question from prior work is how to interpret and choose this  parameter, especially when the input 
does not actually arise from uniformly sampling an underlying continuous signal. S4 specifies to log-uniformly initialize  in the range , but do not provide a concrete justification.
In \cref{sec:hippo:timescale} we show a simpler interpretation of  directly in terms of the length of dependencies in a discrete input sequence.









\subsection{HiPPO: High-order Polynomial Projection Operators}
\label{sec:background:hippo}


\linepenalty=1000
\looseness=-1
S4 is defined as a TSSM where  is initialized with a particular formula \eqref{eq:legs}.
This was called the HiPPO matrix in \citep{gu2022efficiently}, but is actually just one of several such special matrices derived in \citep{gu2020hippo}.
To disambiguate other variants of S4, we refer to the full S4 method using this HiPPO SSM as \textbf{S4-LegS}.
Other cases considered in this work include LegT from prior work \eqref{eq:legt} and FouT that we introduce \eqref{eq:fout}.

\begin{minipage}{0.5\linewidth}
  \small


\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
  \small

\end{minipage}

These matrices were originally motivated by the question of `online memorization' of an input signal.
The key idea is that for a suitably chosen SSM basis , then at any time , the current state  can be used to approximately reconstruct the entire input  up to time  (\cref{fig:reconstruction-background}).

The main theoretical idea is as follows.
Suppose that the basis functions satisfy \cref{def:hippo}.
\begin{definition}\label{def:hippo}
  We call an SSM  an \textbf{orthogonal SSM (OSSM)} for the basis 
  and measure  if the functions
  
  satisfy, at all times ,
  
  In the case of a \textbf{time-invariant OSSM (TOSSM)},  (depends only on  giving us \cref{def:ssm-basis} with measure  and basis .
\end{definition}


To be more specific about terminology,  and  are called the \emph{basis and measure} for \emph{orthogonal} SSMs (\cref{def:hippo}), while  are called the \emph{SSM basis kernels} which applies more generally to all SSMs (\cref{def:ssm-basis}).
The distinction will be made clear from context, notation, and the word ``kernel'' referring to .

For OSSMs,  and  are uniquely determined by each other, so we can refer to an OSSM by either. One direction is obvious:  determine  via .
\begin{proposition}\label{prop:basis-uniqueness}
  If a set of kernel functions satisfies  where the functions  are complete and orthogonal w.r.t.  (equation \eqref{eq:basis} right),  and  are unique.
\end{proposition}


\looseness=-1
Equation \eqref{eq:basis} is equivalent to saying that for every fixed , ,
or that  are an orthonormal basis with respect to measure .
More formally, defining  and  similarly, then  are orthonormal in the Hilbert space with inner product .
By equation \eqref{eq:basis},  where .
Thus at all times , the state vector  is simply \emph{the projections of  onto a orthonormal basis},
so that the history of  can
be reconstructed from .
HiPPO called this the \textbf{online function approximation} problem~\citep{gu2020hippo}.


\begin{proposition}\label{prop:hippo-reconstruction}
  \looseness=-1
  Consider an OSSM that satisfies \eqref{eq:basis} and fix a time .
  Furthermore suppose that in the limit ,
  the  are a complete basis on the support of .
Then  for all .
\end{proposition}

\begin{figure}[!t]
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/hippo_legt.png}
\end{subfigure}
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/hippo_legs_scale.png}
\end{subfigure}
\caption{
  Given an input function  (black), HiPPO compresses it online into a state vector  via equation \eqref{eq:ssm-1}. Specific cases of HiPPO matrices  are derived so that at every time , the history of  up to time  can be reconstructed linearly from  (red), according to a measure (green).
  (\emph{Left}) The HiPPO-LegT method orthogonalizes onto the Legendre polynomials against a time-invariant uniform measure, i.e. sliding windows.
  (\emph{Right}) The original HiPPO-LegS method is \emph{not} time-invariant system. When used as a time-varying ODE ,  represents the projection of the entire history of  onto the Legendre polynomials. It was previously unknown how to interpret the time-invariant version of this ODE using the same  matrices.
}
\label{fig:reconstruction-background}
\end{figure}


The main barrier to using \cref{prop:hippo-reconstruction} for function reconstruction is that
SSMs are in general not OSSMs.
For example, even though we will show that \eqref{eq:legs} is an TOSSM, its diagonalization is not.
\begin{proposition}\label{prop:diag-not-ossm}
  There is no TOSSM with the diagonal state matrix .
\end{proposition}


HiPPO can be viewed as a framework for deriving specific SSMs that do satisfy \eqref{eq:basis}.
The original HiPPO methods and its generalizations \citep{gu2020hippo,gu2021lssl} primarily focused on the case when the  are orthogonal polynomials,
and specifically looked for solutions to \eqref{eq:basis}, which turn out to be SSMs.
We have rephrased the HiPPO definition in \cref{def:hippo} to start directly from SSMs.

We discuss the two most important cases previously introduced.

\looseness=-1
\para{HiPPO-LegT.}
\eqref{eq:legt} is a TOSSM that approximates the truncated Legendre polynomials (\cref{fig:legt}).
\looseness=-1

\begin{definition}
  Let  be the indicator function for the unit interval .
  We denote the Legendre polynomials rescaled to be orthonormal on  as , satisfying .
\end{definition}

\begin{proposition}\label{prop:legt}
  As , the SSM with  in \eqref{eq:legt} is a TOSSM with
  
\end{proposition}

\begin{figure}[!t]
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legt_1024.png}
\end{subfigure}
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/delay_legt_1024.png}
\end{subfigure}
\caption{
  (\textbf{HiPPO-LegT.})
  (\emph{Left}) First 4 basis functions  for state size  (\cref{prop:legt}).
  (\emph{Right}) Choosing a particular  produces a spike kernel or ``delay network'' (\cref{thm:delay-legt}).
}
\label{fig:legt}
\end{figure}

This particular system was the precursor to HiPPO and has also been variously called the Legendre Delay Network (LDN) or Legendre Memory Unit (LMU) \citep{voelker2019dynamical,voelker2019legendre}.
The original motivation of this system was not through the online function approximation formulation of HiPPO, but through finding an optimal SSM approximation to the \textbf{delay network} that has impulse response  representing a time-lagged output by 1 time unit (\cref{fig:legt}).
We state and provide an alternate proof of this result in \cref{sec:hippo:finite}, \cref{thm:delay-legt}.

\para{HiPPO-LegS.}
Unlike the HiPPO-LegT case which is an LTI system \eqref{eq:ssm-1} (i.e. TOSSM),
the HiPPO-LegS matrix \eqref{eq:legs} was \emph{meant to be used in a time-varying system}  \citep{gu2020hippo}.
In contrast to HiPPO-LegT, which reconstructs onto the truncated Legendre polynomials in sliding windows , HiPPO-LegS reconstructs onto Legendre polynomials on ``scaled'' windows ; since the window changes across time, the system is not time-invariant (\cref{fig:reconstruction-background}).

\begin{theorem}\label{thm:hippo-legs}
  The SSM  for  in \eqref{eq:legs} is an OSSM with
  
\end{theorem}

However, the S4 model applies the exact same formula \eqref{eq:legs} inside the \emph{time-invariant} SSM \eqref{eq:ssm-1}, i.e. dropped the  term, which had no mathematical interpretation.
In other words, while  is an OSSM, it was not known whether the TSSM  is a TOSSM.
Given that the performance of SSM models is very sensitive to these matries  \citep{gu2022efficiently,gupta2022diagonal}, it remained a mystery why this works.
In \cref{sec:hippo} we will prove that \eqref{eq:legs} actually does correspond to a TOSSM.


\para{LSSL.}
While HiPPO originally showed just the above two cases involving Legendre polynomials (and another case called LagT for Laguerre polynomials, which will not be a focus of this work),
follow-up work showed that there exist OSSMs corresponding to all families of orthogonal polynomials .
Our more general framework will also subsume these results.

\looseness=-1
\para{Naming convention.}
We use HiPPO-[SSM] to refer to a fixed OSSM  suitable for online function approximation, where [SSM] is a suffix (e.g. LegS, LegT) that abbreviates the corresponding basis functions (e.g. scaled Legendre, truncated Legendre).
S4-[SSM] refers to the corresponding trainable layer  with randomly initialized , trained with S4's representation and computational algorithm \citep{gu2022efficiently}.
\looseness=-1



 
\section{Generalized HiPPO: General Orthogonal Basis Projections}
\label{sec:hippo}


\looseness=-1
In \cref{sec:hippo:legs}, we prove that the LTI HiPPO-LegS is actually a TOSSM and show closed formulas for its basis functions.
In \cref{sec:hippo:finite}, we include more specific results on finite-window SSMs, including introducing a new method HiPPO-FouT based on truncated Fourier functions, and proving previously established conjectures.
\cref{sec:hippo:timescale} shows more general properties of TOSSMs, which establish guidelines for interpreting and initializing SSM parameters such as the timescale .


Our main, fully general, result is \cref{thm:gen-hippo-t0} in \cref{sec:proofs:general}, which describes a very general way to derive OSSMs for various SSM basis functions .
This result can be instantiated in many ways to generalize all previous results in this line of work.

\subsection{Explanation of S4-LegS}
\label{sec:hippo:legs}

We show the matrices  in \eqref{eq:legs}
are deeply related to the Legendre polynomials  defined in \cref{thm:hippo-legs}.


\begin{corollary}\label{cor:legs}
  Define  for any differentiable function . The SSM 
  is an OSSM with
  
\end{corollary}

As more specific corollaries of \cref{cor:legs}, we recover both the original time-varying interpretation of the matrix in  \eqref{eq:legs},
as well as the instantiation of LegS as a time-invariant system.
If we set , then we recover the scale-invariant HiPPO-LegS OSSM in \cref{thm:hippo-legs},
\begin{corollary}[Scale-Invariant HiPPO-LegS, \cref{thm:hippo-legs}]The SSM
   is a TOSSM for basis functions   and measure  where 
    and  are defined as in \eqref{eq:legs}.
\end{corollary}
And if , this shows a new result for the time-invariant HiPPO-LegS TOSSM:
\begin{corollary}[Time-Invariant HiPPO-LegS]\label{cor:legs-time}
 The SSM  is a TOSSM with
  
\end{corollary}

This explains why removing the  factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential ``warping'' or change of basis on the time axis (\cref{fig:1}).


\subsection{Finite Window Time-Invariant Orthogonal SSMs}
\label{sec:hippo:finite}


\looseness=-1
For the remainder of this section, we restrict to the time-invariant SSM setting \eqref{eq:ssm-convolution}.
 A second important instantiation of \cref{thm:gen-hippo-t0} covers cases with a discontinuity in the SSM basis functions ,
which requires infinite-dimensional SSMs to represent.
The most important type of discontinuity occurs when  is supported on a finite window, so that these TSSMs represent sliding window transforms.

We first derive a new sliding window transform based on the widely used Fourier basis (\cref{sec:fout}).
We also prove results relating finite window methods to delay networks (\cref{sec:delay})


\subsubsection{S4-FouT}
\label{sec:fout}

\looseness=-1
Using the more general framework (\cref{thm:gen-hippo-t0}) that does not necessarily require polynomials as basis functions, we derive a TOSSM that projects onto \textbf{truncated Fourier functions}.


\begin{theorem}\label{thm:hippo-fourier}
  As , the SSM for \eqref{eq:fout} is a TOSSM with 
  and  are the truncated Fourier basis functions  orthonormal on , ordered in the form , where 
  and 
\end{theorem}
This SSM corresponds to Fourier series decompositions, a ubiquitous tool in signal processing, but represented as a state space model.
The basis is visualized in \cref{fig:1} (middle) for state size .




A benefit of using these well-behaved basis functions is that we can leverage classic results from Fourier analysis.
For example, it is clear that taking linear combinations of the truncated Fourier basis can represent any function on ,
and thus S4-FouT can represent any local convolution (i.e. the layers of modern convolutional neural networks).

\begin{theorem}\label{thm:fourier-kernel-approx}
  Let  be a differentiable kernel on , and let  be its representation by the FouT system (Theorem \ref{thm:hippo-fourier}) with state size  If  is Lipschitz, then for , we have  If  has derivatives bounded by , then we can take  
\end{theorem}



\subsubsection{Approximating Delay Networks}
\label{sec:delay}


An interesting property of these finite window TSSMs is that they can approximate \textbf{delay functions}.
This is defined as a system with impulse response : then , which means the SSM outputs a time-lagged version of the input.
This capability is intuitively linked to HiPPO, since in order to do this, the system must be remembering the entire window  at all times , in other words perform an \emph{approximate function reconstruction}.
Any HiPPO method involving finite windows should have this capability, in particular, the finite window methods LegT and FouT.


\begin{theorem}\label{thm:delay}

For the FouT system  and , let  be (twice) the vector of evaluations of the basis functions  and let .
For the LegT system  and , let  be the vector of evaluations of the basis functions  and let .

Then the SSM kernel  limits to  as .
\end{theorem}

\cref{thm:delay} is visualized in \cref{fig:1,fig:legt} (right).
Further, the result for LegT can be characterized even more tightly for finite .
In fact, this was the original motivation for the LDN/LMU~\citep{voelker2019dynamical,voelker2019legendre},
which worked backward from the transfer function of the desired delay function impulse response ,
and noticed that the SSM for Pad\'e approximations to this were linked to Legendre polynomials.
This was not fully proven, and we state it here and provide a full proof in \cref{sec:proofs:finite}.


\begin{theorem}\label{thm:delay-legt}
  For  in the LegT system described in \cref{thm:delay}, the transfer function  is the  Pad\'e approximant to .
\end{theorem}

\looseness=-1
We remark that although LegT (LMU) is designed to be an ``optimal'' approximation to the delay function via Pad\'e approximants,
it actually produces a weaker spike function than FouT (\cref{fig:legt} vs. \cref{fig:1}) and empirically performs slightly worse on synthetic tasks testing this ability (\cref{sec:experiments:delay}).
This may be because Pad\'e approximation in the Laplace domain does not necessarily translate to localization in the time domain.





\subsection{Properties of Time-invariant Orthogonal SSMs: Timescales and Normalization}
\label{sec:hippo:timescale}

\looseness=-1
We describe several general properties of TOSSMs,
which let us answer the following questions:
\begin{itemize}[leftmargin=*,itemsep=0pt]\item How should all parameters  be initialized for an SSM layer to be properly normalized?
  \item What does  intuitively represent, and how should it be set in an SSM model?
\end{itemize}


It turns out that for TOSSMs, these two questions are closely related and have intuitive interpretations.

\para{Closure properties.}
First, several basic transformations preserve the structure of TOSSMs.
  Consider a TOSSM  with basis functions  and measure .
  Then, for any scalar  and unitary matrix , the following are also TOSSMs with the corresponding basis functions and measure (\cref{sec:proofs:timescale}, \cref{prop:closure}):
\begin{center}
  \small
  \begin{tabular}{@{}lllll@{}}
    \toprule
    Transformation & Matrices & Interpretation & Basis & Measure \\
    \midrule
    Scalar Scaling &  & Timescale change &  &  \\
    Identity Shift &  & Exponential tilting &  &  \\
    Unitary Transformation &  & Identity &  &  \\
    \bottomrule
  \end{tabular}
   \label{tab:closure}
\end{center}




\paragraph{Normalization.}
A standard aspect of training deep learning models, in general, concerns the scale or variance of activations.
This has been the subject of much research on training deep learning models,
touching on deep learning theory for the dynamics of training such as
the exploding/vanishing gradient problem~\citep{hochreiter1991untersuchungen},
and a large number of normalization methods to ensure properly normalized methods,
from the simple Xavier/He initializations~\citep{glorot2010understanding,he2015delving} to BatchNorm and LayerNorm \citep{ioffe2015batch,ba2016layer} to many modern variants and analyses of these~\citep{davis2021catformer}.


The following proposition follows because for a TOSSM,  can be interpreted as projecting onto orthonormal functions in a Hilbert space (\cref{prop:hippo-reconstruction}).
\begin{proposition}[Normalization of TOSSM]\label{prop:ssm-normalization}
  Consider an (infinite-dimensional) TOSSM.
  For any input ,
  .
\end{proposition}


\begin{corollary}\label{cor:ssm-normalization}
  For a TOSSM with a probability measure (i.e. 
  and  any constant input ,
  the state has norm  and the output  has mean 0, variance  if the entries of  are mean  and variance .
\end{corollary}
  Note that the probability measure requirement can be satisfied by simply rescaling .
\cref{cor:ssm-normalization} says the TOSSM preserves the variance of inputs, the critical condition for a properly normalized deep learning layer.
Note that the initialization of  is different than a standard Linear layer in deep neural networks, which usually rescale by factor depending on its dimensionality such as  \citep{glorot2010understanding}.


\para{Timescales.}
As discussed in \cref{sec:background}, converting from continuous to discrete time involves a parameter  that represents the step size of the discretization.
This is an unintuitive quantity when working directly with discrete data, especially if it is not sampled from an underlying continuous process.

We observe the following fact:
for all standard discretization methods (e.g. Euler, backward Euler, generalized bilinear transform, zero-order hold \citep{gu2021lssl}), the discretized system depends on , and  \emph{only through their products} .
This implies that the SSM  discretized at step size  is computationally equivalent to the SSM  discretized at step size .

Therefore,  can be viewed just as a scalar scaling of the base SSM instead of changing the rate of the input.
In the context of TOSSMs, this just scales the underlying basis and measure (Scalar Scaling).
More broadly, scaling a general SSM simply changes its \emph{timescale} or rate of evolution.
\begin{proposition}The ODE  evolves at a rate  times as fast as the SSM ,
  in the sense that the former maps  if the latter maps .
\end{proposition}



The most intuitive example of this is for a finite window TOSSM such as LegT or FouT. Discretizing this system with step size  is equivalent to considering the system  with step size , which produces basis functions supported exactly on .
The interpretation of the timescale  lends to simple discrete-time corollaries of the previous continuous-time results.
For example, LegT and FouT \emph{represent sliding windows of  elements} in discrete time.
\begin{corollary}\label{cor:delay}
  By \cref{thm:delay}, as , the discrete convolutional kernel , i.e.\ the discrete delay network with lag .
\end{corollary}

\begin{corollary}\label{cor:fourier-kernel}
  For HiPPO-FouT matrices ,
  by \cref{thm:hippo-fourier}, as , the discrete convolutional kernel  (over the choice of  can represent any local convolution of length .
\end{corollary}

This discussion motivates the following definition.
Properly normalized TOSSMs  will model dependencies of expected length ,
and  modulates it to model dependencies of length ,
allowing fine-grained control of the context size of a TOSSM.
\begin{definition}[Timescale of TOSSM]\label{def:timescale}
  Define  to be the \emph{timescale} of a TOSSM having measure .
  A TOSSM is \emph{timescale normalized} if it has timescale .
\end{definition}
By this definition, HiPPO-LegS is timescale normalized. This motivates S4's initialization of  log-uniformly in , covering a geometric range of sensible timescales (expected length  to .
In \cref{sec:experiments} we show that the timescale can be chosen more precisely when lengths of dependencies are known.




We finally remark that HiPPO-LegT and -FouT were derived with measures . However, to properly normalize them by \cref{def:timescale}, we choose to halve the matrices to make them orthogonal w.r.t. .
The S4-FouT and S4-LegT methods in our experiments use these halved versions. 

\subsection{Discussion}

\cref{tab:kernels} summarizes the results for TOSSMs presented in this section, including both original HiPPO methods defined in \citet{gu2020hippo} as well as our new methods.
\begin{table}
  \caption{Summary of time-invariant orthogonal SSMs. Original HiPPO results (\emph{Bottom}) and new results (\emph{Top}).}
  \small
    \begin{tabular}{@{}llllll@{}}
        \toprule
        Method & SSM Matrices                     & SSM kernels       & Orthogonal basis           & Measure           & Timescale    \\
        \midrule
        LegS   & equation \cref{eq:legs}                    &                &     &             & 1            \\
        FouT   & equation \cref{eq:fout}                    &  &  &  & 1/2            \\
        \midrule
        LegT   & equation \cref{eq:legt}                    &         &          &  & 1/2            \\
        LagT   & see \citep{gu2020hippo}                    &        &                      &   &  \\
        \bottomrule
    \end{tabular}
    \label{tab:kernels}
\end{table}

\paragraph{HiPPO-LagT}
We note that the original HiPPO paper also included another method called LagT based on Laguerre polynomials.
Because Laguerre polynomials are orthogonal with respect to , this system was supposed to represent an exponentially decaying measure.
However, this method was somewhat anomalous; it generally performed a little worse than the others, and it was also empirically found to need different hyperparameters.
For example, \citet{gu2020hippo} found that on the permuted MNIST dataset, setting  to around  for most HiPPO methods was indeed optimal, as the theory predicts.
However, HiPPO-LagT performed better when set much higher, up to .
It turns out that this method changes the basis in a way such that it is \emph{not} orthogonal with respect to an exponentially decaying measure, but rather the constant measure , and has a timescale of ; this explains why the hyperparameters for  need to be much higher.

In summary, we do not recommend using the original HiPPO-LagT, which despite the original motivation does not represent orthogonalizing against an exponentially decaying measure.
Instead, HiPPO-LegS (as a time-invariant SSM) actually represents an exponentially decaying measure.


\paragraph{Timescales}
For a timescale normalized orthogonal SSM (i.e.  and :
\begin{itemize}\item  exactly represents the range of dependencies it captures.
    For example, S4-FouT can represent any finite convolution kernel of length  (so the expected length of a random kernel is .
  \item A random vector  with independent mean , variance  entries is a variance-preserving SSM, i.e. produces outputs matching the variance of the input.
\end{itemize}

\paragraph{LSSL and General Polynomials}

The Linear State Space Layer \citep{gu2021lssl} succeeded HiPPO by incorporating it into a full deep SSM model, and also generalized the HiPPO theory to show that all orthogonal polynomials can be defined as the SSM kernels for some .
Our framework is even stronger and immediately produces the main result of LSSL as a corollary (Appendix), and can also work for non-polynomial methods (e.g. FouT).


These results show that all orthogonal polynomial bases, including truncated and scaled variants, have corresponding OSSMs with polynomial kernels.
If we define this special case as polynomial OSSMs (POSSMs), we have therefore deduced that all of the original HiPPOs are POSSMs.
 

\section{Experiments}
\label{sec:experiments}


We study the empirical tradeoffs of our proposed S4 variants. We compare several S4 variants based on the TOSSMs introduced in this work,
as well as to simpler diagonal SSMs called S4D that are not orthogonal SSMs~\citep{gu2022s4d}.
Corresponding to our main contributions, we hypothesize that
\begin{itemize}[leftmargin=*]\item S4-LegS excels at sparse memorization tasks because it represents very smooth convolution kernels that memorize the input against an infinitely-long measure (\cref{cor:legs-time}, \cref{fig:1}). Conversely, it is less appropriate at short-range tasks with dense information because it smooths out the signal.
  \item S4-FouT excels at dense memorization tasks because it can represent spike functions that pick out past elements at particular ranges (\cref{sec:delay}). However, it is less appropriate at very long range tasks because it represents a finite (local) window.
  \item  can be initialized precisely based on known time dependencies in a given task to improve performance.
\end{itemize}

\subsection{Long Range Arena}

The Long Range Arena (LRA) benchmark is a suite of sequence classification tasks designed to stress test sequence models on modeling long sequences.
We improve S4's previous state of the art by another 6 points (\cref{tab:lra}).
Validating our hypothesis, S4-LegS is extremely strong at the hardest long-range task (Path-X) involving sparse dependencies of length 16384, which FouT cannot solve because it is a finite window method.

The Path-X task also serves as a validation of the theory of timescales in \cref{sec:hippo:timescale}.
To set these results,
we lowered the initialization of  in accordance with known length of dependencies in the task.
\cref{fig:pathx} illustrates the importance of setting  correctly.

\begin{table}[t!]
  \small
  \caption{
    (\textbf{Long Range Arena}) Accuracy (std.) on full suite of LRA tasks. Hyperparameters in \cref{sec:experiment-details}.
  }
    \centering
    \begin{tabular}{@{}llllllll@{}}
        \toprule
        \textsc{Model} & \textsc{ListOps}         & \textsc{Text}            & \textsc{Retrieval}       & \textsc{Image}           & \textsc{Pathfinder}      & \textsc{Path-X}       & \textsc{Avg}      \\ \midrule
        S4-LegS       & 59.60 (0.07) & 86.82 (0.13) & 90.90 (0.15) & \underline{88.65} (0.23) & \underline{94.20} (0.25) & \textbf{96.35} (-) & \textbf{86.09} \\
        S4-FouT       & 57.88 (1.90) & 86.34 (0.31) & 89.66 (0.88) & \textbf{89.07} (0.19)    & \textbf{94.46} (0.24)    & \xmark             & 77.90          \\
        S4-LegS/FouT  & 60.45 (0.75) & 86.78 (0.26) & 90.30 (0.28) & 89.00 (0.26)             & 94.44 (0.08)             & \xmark             & 78.50          \\
        \midrule
        S4 (original) & 58.35        & 76.02        & 87.09        & 87.26                    & 86.05                    & 88.10              & 80.48          \\ Transformer   & 36.37        & 64.27        & 57.46        & 42.44                    & 71.40                    & \xmark             & 53.66          \\ \bottomrule
    \end{tabular}
    \label{tab:lra}
\end{table}


\begin{figure}[!ht]
  \begin{subfigure}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/pathx_1.png}
  \end{subfigure}
  \begin{subfigure}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/pathx_2.png}
  \end{subfigure}

  \caption{
    (\textbf{Validation curves on Path-X.})
    (\emph{Left}) Setting  too small can solve the task, but is inconsistent.
    (\emph{Right}) A good setting of  can consistently solve the task. Note that the timescale of each feature is up to , which is on the order of (but not exceeding) the length of the task .
    Empirically, performance is best when spreading out the range of  with a larger  that covers a wider range of timescales and can potentially learn features at different resolutions, which are combined by a multi-layer deep neural network.
    We also show a diagonal variant of S4-LegS called S4D-Inv introduced in \citep{gu2022s4d} which approximates S4-LegS, but is still worse.
  }
    \label{fig:pathx}
\end{figure}

\subsection{Theory: Function Reconstruction, Timescales, Normalization}
\label{sec:experiments:reconstruction}


\begin{figure}[!t]
  \begin{subfigure}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/hippo_legs.png}
  \end{subfigure}
  \begin{subfigure}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/hippo_fourier.png}
  \end{subfigure}
  \caption{
    Function reconstruction predicted by our general theory.
    An input signal of length  is processed sequentially,
    maintaining a state vector of size only ,
    which is then used to approximately reconstruct the entire history of the input.
    (\emph{Left}) HiPPO-LegS (as an LTI system) orthogonalizes on the Legendre polynomials warped by an exponential change of basis, smoothening them out. This basis is orthogonal with respect to an exponentially decaying measure.
    Matching the intuition, the reconstruction is very accurate for the recent past and degrades further out, but still maintains information about the full history of the input, endowing it with long-range modeling capacity.
    This is the same as S4.
    (\emph{Right}) HiPPO-FouT orthogonalizes on the truncated Fourier basis, similar to the original HiPPO-LegT or LMU.
  }
  \label{fig:reconstruction-new}
\end{figure}

\looseness=-1
\cref{fig:reconstruction-new} confirms the HiPPO theory of online function reconstruction (\cref{prop:hippo-reconstruction}) for the proposed TOSSMs LegS and FouT.

We additionally construct a synthetic \textbf{Reconstruction Task} (for a uniform measure) to test if S4 variants can learn to reconstruct.
The input is a white noise sequence .
We use a single layer linear S4 model with state size  and  hidden units.
Models are required to use their output at the last time step, a vector ,
to reconstruct the last 1000 elements of the input with a linear probe.
Concretely, the loss function is to minimize ,
where  is a learned matrix.


\cref{fig:synthetic-reconstruct} shows that S4-LegT and S4-FouT, the methods that theoretically reconstruct against a uniform measure, are far better than other methods.
We include the new diagonal variants (S4D) proposed in \citep{gu2022s4d}, which are simpler SSM methods that generally perform well but do not learn the right function on this task.
We also include a method \textbf{S4-(LegS/FouT)} which combines both LegS and FouT measures by simply initializing half of the SSM kernels to each.
Despite having fewer S4-FouT kernels, this still performs as well as the pure S4-FouT initialization.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.6\linewidth]{tables/reconstruct.pdf}
  \caption{Log-MSE after training on the Reconstruction Task. (\emph{Left}) When the timescales  are set appropriately for this task, the methods that theoretically reconstruct against a uniform measure (LegT and FouT) are much better than alternatives, achieving MSE more orders of magnitude lower than other SSM initializations.
  (\emph{Right}) Interestingly, when the timescales  are not set correctly, these methods (LegT and FouT) actually perform worst and the diagonal methods introduced in \citep{gu2022s4d} perform best.}
  \label{fig:synthetic-reconstruct}
\end{figure}

Finally, we validate the theory of normalization in \cref{sec:hippo:timescale}, which predicts that for a properly normalized TOSSM, the projection parameter  should be initialized with unit variance, in contrast to standard initializations for deep neural networks which normalize by a factor related to the size of  (in this case .
\cref{tab:init-std} shows classification results on datasets Sequential CIFAR (sCIFAR) and Speech Commands (SC), using models of size at most 150K parameters.
This replicates the setup of the ``ablation models'' of \citep[Section 5]{gu2022s4d}.
Results show that using standard deviation  for  is slightly better than alternatives, although the difference is usually minor.

\begin{table}[!t]
  \caption{Ablation of the initialization standard deviation of  for S4-LegS on classification datasets.}
  \centering
  \begin{tabular}{@{}llllll@{}}
    \toprule
    Init std.  of  & 0.01         & 0.1          & 1.0                   & 10.0         & 100.0        \\
    \midrule
    Sequential CIFAR                       & 85.91 (0.41) & 86.33 (0.01) & \textbf{86.49} (0.51) & 84.40 (0.16) & 82.05 (0.61) \\
    Speech Commands                        & 90.27 (0.31) & 90.00 (0.11) & \textbf{90.67} (0.19) & 90.30 (0.36) & 89.98 (0.51) \\
    \bottomrule
  \end{tabular}
  \label{tab:init-std}
\end{table}



\subsection{Memorization: the Delay (continuous copying) Task}
\label{sec:experiments:delay}

Next, we study how the synthetic reconstruction ability transfers to other tasks.
The \textbf{Delay Task} requires models to learn a sequence-to-sequence map whose output is the input lagged by a fixed time period (\cref{fig:delay-task}).
For recurrent models, this task can be interpreted as requiring models to maintain a memory buffer that continually remembers the latest elements it sees.
This capability was the original motivation for the Legendre Memory Unit, the predecessor to HiPPO-LegT, which was explicitly designed to solve this task because it can encode a spike kernel (\cref{fig:legt}).
In \cref{fig:delay-results}, we see that our new S4-FouT actually outperforms S4-LegT,
which both outperform all other methods when the timescale  is set correctly.
We note that this task with a lag of just 1000 time steps is too hard for baselines such as an LSTM and Transformer, which empirically did not learn better than random guessing (RMSE 0.43).


\begin{figure}[!ht]
\begin{subfigure}{\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/delay_task.png}
    \caption{
      Models perform a mapping from  where the target output is lagged by  steps, with error measured by RMSE.
      The input is a white noise signal bandlimited to .
      We use single layer SSMs with state size .
    }
    \label{fig:delay-task}
\end{subfigure}
\begin{subfigure}{\linewidth}\centering
    \includegraphics[width=\linewidth]{tables/delay.pdf}
    \caption{
      (\emph{Left}) Setting  appropriately makes a large difference.
      For FouT , which encode \emph{finite window} basis functions (\cref{fig:1}),
      the model can see a history of length up to .
      For example, setting  too large means the model cannot see  steps in the past, and performs at chance.
      Performance is best at the theoretically optimal value of  which can encode a spike kernel at distance exactly  steps (\cref{cor:delay}).
      (\emph{Right}) When  is set optimally, the proposed S4-FouT method is the best SSM as the theory predicts.
      When  is not set optimally, other methods perform better, including the simple diagonal methods proposed in~\citep{gu2022s4d}.
    }
    \label{fig:delay-results}
\end{subfigure}
\caption{(\textbf{Delay Task.)} A synthetic memorization task: definition (\cref{fig:delay-task}) and results (\cref{fig:delay-results}).}
\label{fig:delay}
\end{figure}

 
\section{Summary: How to Train Your HiPPO}

\begin{itemize}\item SSMs represent convolution kernels that are linear combinations (parameterized by  of basis functions (parameterized by  and .
  \item HiPPO is a general mathematical framework for producing matrices  and  corresponding to prescribed families of well-behaved basis functions. We derive HiPPO matrices corresponding to exponentially-scaled Legendre families (LegS) and the truncated Fourier functions (FouT).
    \begin{itemize}\item HiPPO-LegS corresponds to the original S4 method and produces a very smooth, long-range family of kernels (\cref{fig:1}) that is still the best method for long-range dependencies among all S4 variants
      \item HiPPO-FouT is a finite window method that subsumes local convolutions (e.g. generalizing vanilla CNNs, \cref{cor:fourier-kernel}) and captures important transforms such as the sliding DFT or STFT
    \end{itemize}
  \item Independently of a notion of discretization, the timescale  has a simple interpretation as controlling the length of dependencies or ``width'' of the SSM kernels. Most intuitively, for a finite window method such as FouT, the kernels have length exactly , and generalize standard local convolutions used in deep learning.
\end{itemize}

A companion paper to this work builds on the theory introduced here to define a simpler version of S4 using \emph{diagonal} state matrices (S4D), which are approximations to the orthogonal SSMs we introduce and can inherit S4's strong modeling abilities \citep{gu2022s4d}.
It also includes experiments on more datasets comparing various state space models, including the S4 variants (S4-LegS and S4-FouT) introduced here.
 
\subsubsection*{Acknowledgments}
We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program,  Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.
Atri Rudra and Isys Johnson are supported by NSF grant CCF-1763481.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.


\newpage
\bibliography{biblio}

\newpage

\appendix

\section{Related Work}
\label{sec:related}

We discuss in more detail the differences between this work and the previous results in this line of work.

\para{Legendre Memory Unit (Legendre Delay Network).}
The HiPPO-LegT matrix \eqref{eq:legt} was first introduced as the LMU \citep{voelker2019dynamical,voelker2019legendre}.
The original motivation was to produce a state space model that approximates the \textbf{Delay Network},
which can be defined as the LTI system that transforms  into , i.e. lags the input by 1 time unit.
This can also be defined as the system with impulse response , i.e. convolves by the convolutional kernel with a  spike at time .

The connection between the Delay Network and Legendre polynomials was made in two steps.
First, the transfer function of the ideal system is  and must be approximated by a proper rational function to be represented as an SSM.
Taking Pad\'e approximants of this function yields ``optimal'' approximations by rational functions, which can then be distilled into a SSM  whose transfer function  matches it.
Second, the SSM basis  for this system can be computed and found to match Legendre polynomials.
However, despite making this connection and writing out formulas for this SSM, \citet{voelker2019dynamical} did not provide a complete proof of either of these two connections.

The preceding two steps that motivated the LDN can be informally written as the chain of transformations (i) transfer function   (ii) SSM   (iii) Legendre polynomials .
The HiPPO framework in a sense proceeded in the opposite direction.
\citet{gu2020hippo} started by defining the system that convolves with truncated Legendre polynomials, and with a particular differentiation technique showed that it could be written as a particular SSM which they called HiPPO-LegT.
This SSM turned out to be the same (up to a minor change in scaling) as the original  defined by the LMU, thus proving the second of the two steps relating this particular SSM to the Legendre polynomials.

In this work, we show the final piece in this reverse chain of equivalences.
In particular, we start from the LegT SSM  and directly prove that its transfer function produces Pad\'e approximants of the exponential.
Our proof introduces new techniques in an inductive argument that can be applied to HiPPO SSMs beyond the LegT case,
and relates them to continued fraction expansions of the exponential.


We comment on a minor difference between the parameterization of HiPPO-LegT and the LMU.
The LMU is originally defined as

where  is a hyperparameter that controls the length of the window.
However, we point out that such constant scaling of the SSM is also controlled by the step size  as discussed in \cref{sec:hippo:timescale}.
Therefore  is redundant with , so the LegT matrices defined in \citep{gu2020hippo} and in this work do not have a concept of .
Additionally, in this work we redefine the LegT matrices  to be scaled by a factor of  to make them properly timescale normalized,
using the theory developed in \cref{sec:hippo:timescale}.


\paragraph{HiPPO and LSSL.}
As discussed in \cref{sec:background:hippo}, HiPPO can be thought of as a framework for deriving state space models corresponding to specific polynomial bases.
The original paper did not explicitly draw the connection to state space models, and also developed systems only for a few particular cases which were called LegS (a time-varying system involving Legendre polynomials), LegT (a time-invariant system with the truncated Legendre polynomials), and LagT (involving Laguerre polynomials).

A follow-up paper on Linear State Space Layers (LSSL) generalized these results to \emph{all} orthogonal polynomial families, and also generalized the flexibility of the time-varying component.
They produced SSMs  where at all times ,  can be viewed as the projection of the history of  onto orthogonal polynomials  rescaled onto the interval , where  is an arbitrary factor.
This generalized all 3 cases of the original HiPPO paper.

Compared to these works, our framework (\cref{def:hippo}) simplifies and generalizes the concepts directly in terms of (time-varying) state space models.
We define a more natural concept of \textbf{orthogonal SSM}, derive very general instantiations of it (\cref{sec:hippo:legs}), and flesh out its properties (\cref{sec:hippo:timescale}).
Our general result subsumes all prior cases including all cases of the LSSL as a direct corollary.
Some concrete advantages include:
\begin{itemize}\item It allows more flexible transformations of polynomial bases, such as including a change-of-basis inside the polynomials. The previously expained case of LegS is an instance of this, which has basis functions  with an exponential change of basis, instead of vanilla polynomials.
  \item It can be applied to non-polynomial bases, such as the truncated Fourier basis FouT.
  \item It does not require considering multiple cases depending on where the basis functions are supported. Instead, we handle this by considering discontinuities in the basis functions.
\end{itemize}


\paragraph{S4.}
While the preceding discussion covers theoretical interpretations of SSMs,
S4 (and its predecessor LSSL) are the application of these SSMs to deep learning.
In comparison to prior works such as the LMU and HiPPO which require a pre-determined system  and incorporate them naively into an RNN,
LSSL and S4 use a full state space model  as a completely trainable deep learning layer. Doing this required resolving computational problems with the SSM, which was the main focus of S4.
In this work, we make a distinction between HiPPO, which is the theoretical derivation and interpretation of particular SSMs ,
and S4, which is the incorporation of those SSMs as a trainable deep learning layer with a particular algorithm.
 \section{Experiment Details and Additional Experiments}
\label{sec:experiment-details}


\subsection{Delay (Continuous Copying) Task}
The Delay Task consists of input-output pairs where the input is a white noise signal of length 4000 bandlimited to 1000 Hz.
The output is the same signal shifted by 1000 steps (\cref{fig:delay-task}).
We use single layer linear SSMs with  hidden units and state size .
Models are trained with the Adam optimizer with learning rate 0.001 for 20 epochs.

\subsection{Long Range Arena}

The settings for LRA use the same hyperparameters in~\citep{gu2022s4d}.
A more detailed protocol can be found in~\citep{gu2022s4d}.
To be self-contained, we recreate the same table of parameters in \cref{tab:lra-hyperparameters}.



\begin{table*}[!t]
  \caption{
    The values of the best hyperparameters found for LRA.
    LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization.
  }
  \small
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{@{}llllllllllll@{}}
      \toprule
                          & \textbf{Depth} & \textbf{Features } & \textbf{Norm} & \textbf{Pre-norm} & {\bf Dropout} & {\bf LR} & {\bf Batch Size} & {\bf Epochs} & \textbf{WD} &  \\
      \midrule
      \textbf{ListOps}    & 8              & 128                       & BN            & False             & 0             & 0.01     & 50               & 40           & 0.05        &            \\
      \textbf{Text}       & 6              & 256                       & BN            & True              & 0             & 0.01     & 16               & 32           & 0.05        &            \\
      \textbf{Retrieval}  & 6              & 256                       & BN            & True              & 0             & 0.01     & 64               & 20           & 0.05        &            \\
      \textbf{Image}      & 6              & 512                       & LN            & False             & 0.1           & 0.01     & 50               & 200          & 0.05        &            \\
      \textbf{Pathfinder} & 6              & 256                       & BN            & True              & 0             & 0.004    & 64               & 200          & 0.03        &            \\
      \textbf{Path-X}     & 6              & 256                       & BN            & True              & 0             & 0.0005   & 32               & 50           & 0.05        &          \\
      \bottomrule
    \end{tabular}}
  \label{tab:lra-hyperparameters}
\end{table*}

 
\newcommand{\bsigma}{\overline{\sigma}}
\newcommand{\Aprime}{\bm{A'}}




\section{Proof Details}
\label{sec:theory-details}

We furnish the missing proofs from \cref{sec:background} in \cref{sec:proofs:background}.
We will describe our general framework and results in \cref{sec:proofs:general}, and prove the results in \cref{sec:hippo:legs,sec:hippo:finite,sec:hippo:timescale}
in \cref{sec:proofs:legs,sec:proofs:finite,sec:proofs:timescale} respectively.

\subsection{Proofs from Background}
\label{sec:proofs:background}

This corresponds to results from \cref{sec:background}.

\begin{proof}[Proof of \cref{prop:basis-uniqueness}]Suppose for the sake of contradiction that there is a second basis and measure  such that  is complete and orthogonal w.r.t. , and .
  By completeness, there are coefficients  such that
  
  Then
  
  But , so
  
  So  which implies that  for all , as desired.
\end{proof}

\begin{proof}[Proof of \cref{prop:diag-not-ossm}]The SSM kernels are . Assume  so that the kernels are not degenerate.

  Suppose for the sake of contradiction that this was a TOSSM with measure .
  Then we must have
  
  Plugging in  and  gives
  
  This is clearly a contradiction.
\end{proof}


\subsection{General theory}
\label{sec:proofs:general}



Consider a measure supported on  with density , where  is the indicator function for membership in the  interval . Let the measure be equipped with a set of orthonormal basis functions , i.e.

where the integrals in this paper are over the range , unless stated otherwise. 

This is sufficient to derive an OSSM based on the HiPPO technique. The generalized HiPPO framework demonstrates how to build (T)OSSMs utilizing \emph{time warping} to shape the time interval and \emph{tilting} to construct new sets of orthogonal basis functions. 

Given an general interval , we will use the notation  to denote the indicator function for the interval -- we will drop the interval if .

We will need the notion of a ``\emph{time warping}'' function  as follows:
\begin{definition}
A {\em time warping function} is defined as

such that .



We will be using a special case of time-warping function, which we say {\em has a discontinuity at}  for some :
  

such that










We allow for , in which case we think of the interval  as .
\label{def:twf}
\end{definition}



Before proceeding, let us clarify our notation. We will use  and  to denote the partial derivatives  and  respectively. We will drop the parameters  and use  instead of  when it is clear from context to reduce notational clutter. Further, we will extend this notation to function composition, i.e. write  as  and function product, i.e. use  instead of . Finally, we'll shorten  as .

We also define the tilting  and show that regardless of warping, we can construct a new orthogonal basis (note that the result holds for warping functions as in~\eqref{eq:sigma-ind} and not just those as in ~\eqref{eq:twf-partial-sigs}).





\begin{lemma}
\label{lem:new-ortho}
For the set of orthonormal functions  orthogonal over measure , the set of basis functions

are orthogonal over the measure

for time-warping function  satisfying~\eqref{eq:sigma-ind} and any  that is non-zero in its support.

\end{lemma}



\begin{proof}


Consider the following sequence of equalities:

In the above, the second equality follows from the substitution  and hence  and the final equality follows from~\eqref{eq:OP-ortho}.
Then since  is always non-zero, we have

as desired.
\end{proof}

Without loss of generality, we can split  into a product


of one part that depends on  and another arbitrary component.

\paragraph{Time Warped HiPPO.}

Since we have an orthonormal basis and measure, we can try to derive the  (T)OSSM. For a given input signal , the HiPPO coefficients are defined as the projections.




defined as inner product of  with the tilted basis functions  with respect to the measure  as defined in  Lemma~\ref{lem:new-ortho}. For additional convenience, we  use the decomposition  from ~\eqref{eq:chi-def} to get:






The HiPPO technique is to differentiate through this integral in a way such that it can be related back to  and other .  We require for every , we require that there are a set of coefficients  such that



and for tilting component 







\begin{theorem}
Consider a set of basis functions  orthogonal over , time warping  as in~\eqref{eq:sigma-ind}, ~\eqref{eq:twf-partial-sigs}, and tilting  as in~\eqref{eq:chi-def} and \eqref{eq:tilting-phi} with the functions  obeying~\eqref{eq:cpw-linear}. If , further assume that for some vector ,  we have
as ,


Then  is an OSSM for basis functions  with measure  where

with  as in \eqref{eq:cpw-linear},
 and

\label{thm:gen-hippo-t0}
\end{theorem}
\begin{proof}


Applying the Leibniz rule to~\eqref{eq:hippo-x}, we get

where

and the  terms capture the term we get when differentiating .

Let us consider each term separately. The first term

 corresponds to the differentiation of the basis functions and measure. In order to relate this to , it suffices that  satisfies (\ref{eq:cpw-linear}) which implies that when we vectorize this, we get .

For additional warping and tilting terms, we consider



To reduce this term to , recall from ~\eqref{eq:twf-partial-sigs} that  








Then the above and ~\eqref{eq:tilting-phi} imply



where  are defined as in ~\eqref{eq:twf-partial-sigs} and ~\eqref{eq:tilting-phi}.

We will end up with . This leads to the the vectorized form . 

We now need to handle


For the above note that

where  is the ``heaviside step function." It is know that , which implies

Using the above in RHS of~\eqref{eq:x2+x3}, we separate out  and  as follows.
First, define


In the last equality, we have used the fact that   by definition.
 It follows that in vectorized form we have .

 Finally, define



If , then we have  and hence we have 

If , then as , from \eqref{eq:ut-approx}, the above comes out to

 

 It follows that in vectorized form we have . The result follows after combining the terms.


\end{proof}

We see that the behavior of is the model is dictated by . In particular, in this paper, we will consider two special cases.




\begin{corollary}[ independent of ]
The SSM   satisfying conditions of Theorem \ref{thm:gen-hippo-t0} with  independent of ,  is an OSSM for basis functions  with measure  where
    as in \eqref{eq:cpw-linear}
 and .

\label{cor:hippo-t0-infty}
\end{corollary}
\begin{proof}
Follows from Theorem \ref{thm:gen-hippo-t0}. Since
  is independent of , then  , and .
\end{proof}


\begin{corollary}[]
The SSM   satisfying conditions of Theorem \ref{thm:gen-hippo-t0} with   for a fixed , is an OSSM with basis functions  with measure  where  as in (\ref{eq:cpw-linear}),  , and .
\label{cor:hippo-gen-fin}
\end{corollary}
\begin{proof}
This follows directly from Theorem \ref{thm:gen-hippo-t0} by setting .
\end{proof}












\subsection{LegS (and LSSL?)}
\label{sec:proofs:legs}
\subsubsection{Explanation of S4-LegS}
Consider the case when



 i.e. the measure is completely ``tilted'' away, and let

Let's consider the special case of \eqref{eq:twf-partial-t} where . This is most generally satisfied by


Note that the condition  forces . Hence, we have


We now consider the following special case of \cref{cor:hippo-t0-infty}:


\begin{corollary}Let . The SSM , where  is independent of , is an OSSM for basis functions and measure
  
  where  satisfies \eqref{eq:exp-twf},
   such that

 and


\label{cor:exp-twf}
\end{corollary}




\begin{proof}
 Given a orthonormal basis  with respect to a measure . Note that time-warping function  satisfying \eqref{eq:exp-twf} implies that .

 We fix tilting  , which in turn follows by setting
 
 We show shortly that we satisfy the pre-conditions of \cref{cor:hippo-t0-infty}, which implies (with our choice of  and ) that we have an OSSM with basis functions   and measure





To complete the proof, we show that out choice of paramters above satisfies the conditions of  \cref{cor:hippo-t0-infty} (by showing they satisfy the conditions of \cref{thm:gen-hippo-t0}). We verify that  and  satisfy~\eqref{eq:twf-partial-sigs} and~\eqref{eq:tilting-phi}, noting that


and

 This implies that setting  and  is enough to satisfy~\eqref{eq:twf-partial-sigs} and~\eqref{eq:tilting-phi}.

 Further, note that ~\eqref{eq:exp-twf} and the fact that  imply that



It follows that~\eqref{eq:cpw-linear} is satisfied as long as



for some set of coefficients , which is exactly~\eqref{eq:pn-lin-comb}. This implies the  in \cref{cor:hippo-t0-infty} satisfy.

Let  be the matrix such that  and then note that  is exactly the first parameter of the SSM in \cref{cor:hippo-t0-infty}.  Similarly, recall in \cref{cor:hippo-t0-infty}



where the final equality follows since in our case, . Overloading notation and letting , all conditions of \cref{cor:hippo-t0-infty} hold, from which the claimed result follows.
\end{proof}

We are particularly interested in the following two special cases of Corollary \ref{cor:exp-twf}.



\begin{corollary}The SSM  is a OSSM for basis functions  with measure  where
    as in \eqref{eq:pn-lin-comb} and .
 \label{cor:scale_inv_hippo_legs}
\end{corollary}
\begin{proof}
Letting  implies that . Then we can observe that is a case of Corollary \ref{cor:exp-twf} with time warping



We set  in \cref{cor:exp-twf}, which in turn sets . This gives the tilting



Then by \cref{cor:exp-twf}, it follows that that we can use  and  to build an OSSM with basis functions


with measure



Then the result follows.
\end{proof}


\begin{corollary}The SSM  is a OSSM for basis functions  with measure  where
    as in \eqref{eq:pn-lin-comb}
 and .
 \label{cor:time_inv_hippo_legs}
\end{corollary}
\begin{proof}
This is a case of Corollary \ref{cor:exp-twf} where , , and we pick , implying that . It follows that


Utilizing \cref{cor:exp-twf}, we can use  and  to build an OSSM with basis functions


with measure



 This gives us our final result.
\end{proof}





Next we instantiate \cref{cor:exp-twf} to prove \cref{cor:legs}. (Even though strictly not needed, we instantiate \cref{cor:time_inv_hippo_legs} and \cref{cor:scale_inv_hippo_legs} to prove \cref{thm:hippo-legs} and \cref{cor:legs-time}.) To that end, we will need the following result:

\begin{lemma}
Let the Legendre polynomials orthonormal over the interval  be denoted as . Then





and

   
\end{lemma}

\begin{proof}
The Legendre polynomials satisfy the following orthogonality condition over :

Let us denote the normalized Legendre polynomials orthogonal over  as  where . To orthogonalize them over , let .  It follows that , . Note that we then have



This implies that



Then if we let


then we have an a set of functions over  such that



From \cite[(2.8), (2.9)]{chihara}, note that  and  . This implies that



Finally note that \eqref{eq:norm-shifted-leg} implies:



From \cite[7]{gu2020hippo}, we get


Using \eqref{eq:norm-shifted-leg} on the above, we get \eqref{eq:leg-comb-just-derv}.

We now consider



From \cite[8]{gu2020hippo}, we get



 Then the above becomes
 

\eqref{eq:norm-shifted-leg} implies that , thus



\end{proof}

We now re-state and prove \cref{cor:legs}:
\begin{corollary}[\cref{cor:legs}, restated]
  Let  be the Legendre polynomials orthonormal over the interval . Define . The SSM 
  is an OSSM with
  
where  and  are defined as in \eqref{eq:legs}.
\end{corollary}
\begin{proof}
We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke \cref{cor:exp-twf} with . Further, here we have  and . Now we have an SSM:
  where
    as in \eqref{eq:pn-lin-comb} and .

  From \eqref{eq:leg-val-at+-1} observe that . From \eqref{eq:leg-comb}, we have

 We write that  . Indeed,


Thus the  and  match those in \eqref{eq:legs}, which completes our claim.
\end{proof}



We now re-state and prove \cref{thm:hippo-legs}:
\begin{corollary}[\cref{thm:hippo-legs}, restated]
  Let  be the Legendre polynomials orthonormal over the interval . Then the SSM
   is a OSSM for basis functions   and measure  where
    and  are defined as in \eqref{eq:legs}.
   \label{cor:appn-legs}
\end{corollary}
\begin{proof}
We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke Corollary \ref{cor:scale_inv_hippo_legs} with . Now we have
  where
    as in \eqref{eq:pn-lin-comb} and .

  From \eqref{eq:leg-val-at+-1} observe that . From \eqref{eq:leg-comb}, we have

 We write that  . Indeed,





which completes our claim.



\end{proof}


We now restate and prove \cref{cor:legs-time}.

\begin{corollary}[\cref{cor:legs-time}, restated]
  Let  be the Legendre polynomials orthonormal over the interval . Then the SSM  is a TOSSM for basis functions  with measure  where
    are defined as in \eqref{eq:legs}.
\end{corollary}
 \begin{proof}
We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure, warping function , and with tilting . We note that  satisfies \eqref{eq:exp-twf} with, . This allows us to invoke Corollary \ref{cor:scale_inv_hippo_legs}.

  Then  orthogonalizes against the basis functions  with measure  where
    as in \ref{eq:pn-lin-comb}. Note that the SSM basis functions , hence we get the claimed SSM form utilizing the same argument for  as in the proof of \cref{cor:appn-legs}
\end{proof}

This explains why removing the  factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential ``warping''.



\subsection{Finite Windows}
\label{sec:proofs:finite}
\subsubsection{LegT Derivation}

\begin{corollary}
 Let  be the Legendre polynomials orthonormal over the interval  and let  for a constant . Then the SSM  is a OSSM for basis functions  with measure  where
    are defined as in \eqref{eq:legt}.
\label{cor:hippo-gen-legt}
\end{corollary}
\begin{proof}
Out plan is to apply Corollary \ref{cor:hippo-gen-fin}, for which we must show that the basis functions , time warping , and tilting  satisfy \eqref{eq:cpw-linear}, \eqref{eq:twf-partial-sigs}, and \eqref{eq:tilting-phi}, respectively. We first set some parameters--  note that because  and  set .


The above implies that we have





The above along with \eqref{eq:leg-comb-just-derv}, we see that the Legendre polynomials satisfy \eqref{eq:cpw-linear} with



We also note that .

It follows that


satisfying \eqref{eq:twf-partial-sigs} trivially by setting . Similarly, since  \eqref{eq:tilting-phi} is also satisfied trivially by setting . Finally we note that the  forms a complete basis over , hence as , we have

 


The above defines  by setting  (as well as  and .) Now by Corollary \ref{cor:hippo-gen-fin}, we have an SSM



where , and by \eqref{eq:leg-comb}  (as in \eqref{eq:gamma-Leg}) and .


From \eqref{eq:leg-val-at+-1}, we have   and .

Thus, we have



The proof is complete by noting that  and .

\end{proof}

We note that \cref{cor:hippo-gen-legt} implies \cref{prop:legt}. More specifically, \cref{prop:legt} follows by setting  in \cref{cor:hippo-gen-legt} and noticing that the OSSM there is actually a TOSSM. (Technically we get basis function  for measure  but this is OK since .)


We first give a proof of Theorem \ref{thm:hippo-fourier}. Then, we prove  Theorem \ref{thm:fourier-kernel-approx} as a function approximation result pertaining to S4-FouT.

\subsubsection{Explanation of S4-FouT}



\begin{proof}[Proof of Theorem \ref{thm:hippo-fourier}]
 We seek to derive  and  from \eqref{eq:fout} using Corollary \ref{cor:hippo-gen-fin}: 
 
 We use the time-warping function , which implies that we have 
 
 Thus, we can take 
 
 
 We then have  as we set
 
 So, we can take 
 
 
 We also have  and we order our bases in the form \footnote{Note that this is 0-indexed.}, where the basis functions have derivatives:


Consequently, we can define   as follows: 


Further, the discontinuity is at  which implies that  We now seek to use the stored approximation to  at time  to compute .

First, denote the latent state  with coefficients  and define the functions  and  such that we have

Now, let  and  denote the reconstruction of  and  where we have
Then, we claim that 


Towards that end, we examine the sine and cosine coefficients of  and  as follows:

Here, for \eqref{approx:cov1} and \eqref{approx:cov2}, we use the change of variables  which gives us 

Then, we use the fact that  but  That is, both  and  have the same cosine coefficients but negated sine coefficients of each other. But, we know that both  and , and hence, the reconstruction of  at the endpoints  and  depends only on the cosine coefficients, whence we assert that the reconstruction  agrees with  at both endpoints. Therefore, we have  implying that .

Note that  is continuous and periodic, for which the basis  is complete, and hence, we know that as   Thus, at  we have  which completes the proof of the claim in \eqref{fou:approx}.



Recall from \eqref{fou: ustore1} that we can express the stored approximation of , given by  as follows:

For the value at  the approximation  is then given by

Due to \eqref{fou:approx}, we know , which combined with the above yields:

    
Finally, with regards to Corollary  \ref{cor:hippo-gen-fin}, for Theorem \ref{thm:gen-hippo-t0}, \eqref{eq:twf-partial-sigs-val} satisfies \eqref{eq:twf-partial-sigs} and \eqref{eq:tilting-phi-val} satisfies \eqref{eq:tilting-phi} with \eqref{eq:cpw-linear-val} satisfying \eqref{eq:cpw-linear} for  Moreover, from \eqref{eq:fou-utm1}, we can take  and  to satisfy \eqref{eq:ut-approx}.

Invoking Corollary \ref{cor:hippo-gen-fin} now yields the following OSSM:\footnote{
Recall that, like the coefficients, the matrices are 0-indexed.}
  
 where  with  and  specified as follows:

Here, the values are derived from the expressions of  Corollary \ref{cor:hippo-gen-fin}:  Recall that we have , and from \eqref{eq:sigma-val} and \eqref{eq:phi-psi-val},  with  
 Thus, \eqref{fou: utm1} is due to  but   Similarly, \eqref{fou: ut} is because  but again 
 
 
Now, we have
 

As  we define  and , given by

\end{proof}


 \subsubsection{Function Approximation Error}
\begin{proof}[Proof of Theorem \ref{thm:fourier-kernel-approx}]
First, the state size being  dictates that there are   and  basis functions each. We fix time  and denote  and  to be the respective coefficients for  and  basis corresponding to S4-Fou. Since  forms an orthonormal basis, by Parseval's identity, we have

Thus, in order to bound the error, it suffices to bound the high-order coefficients by integration by parts as follows:

The quantity in the bracket vanishes as  is periodic. Therefore

where we use the fact that  is Lipshitz. 
For , a similar argument holds and we get:

Due to \eqref{coeff: pars}, this then implies that 

We use \eqref{coeff: lip} to get the following estimate on 

Thus, it suffices for  to satisfy the following inequality:



We now use the same argument as above to the fact that  has order- bounded derivative. By iteration, we get: 

Again, due to \eqref{coeff: pars}, this then gives us the following estimate on the square error:

If  has order bounded derivatives, then we use \eqref{coeff: kder} to get the following estimate on 

Again, it suffices for  to satisfy the following inequality:

\end{proof}
 

\subsubsection{Delay Network}

Finally, we prove \cref{thm:delay-legt}.
Note that this is a stronger version of the LegT portion of \cref{thm:delay}, while the FouT portion is a corollary of the proof of \cref{thm:hippo-fourier}.


We start by working out some calculations concretely to provide an example.
The SSM corresponding to HiPPO-LegT is


The transfer function is

(In the RHS and for the rest of this part, we will redefine  to be the  matrix found above for convenience.)


\paragraph{Case N=1.}

We have , and the transfer function is .

\paragraph{Case N=2.}

The transfer function is


It can be verified that this is indeed .



\paragraph{A General Recursion.}
We will now sketch out a method to relate these transfer functions recursively.

We will redefine  to be the vector that ENDS in .

The main idea is to write

Now we can use the block matrix inversion formula.\footnote{\url{https://en.wikipedia.org/wiki/Block_matrix\#/Block_matrix_inversion}}
Ideally, this will produce a recurrence where the desired transfer function  will depend on .
However, looking at the block matrix inversion formula, it becomes clear that there are also dependencies on terms like  and .

The solution is to track all of these terms simultaneously.

We will compute the 4 transfer functions


\begin{lemma}\label{lmm:block-ldu}
Instead of using the explicit block matrix inversion formula, it will be easier to work with the following factorization used to derive it (block LDU decomposition\footnote{\url{https://en.wikipedia.org/wiki/Schur_complement\#/Background}}).

\end{lemma}

Using \cref{lmm:block-ldu}, we can factor the inverse as


Now we compute

and



Now we can derive the full recurrence for all these functions.


Now we'll define a few transformations which will simplfy the calculations.
Define


These satisfy the following recurrences:


We can analyze each term separately.

\textbf{Case} .

This will be the most important term, as it determines the denominator of the expressions.
Simplifying the recurrence slightly gives

Now let  where  are polynomials.
Clearing the denominator  yields


This results in the recurrence


But this is exactly the \emph{fundamental recurrence formula} for continuants of the continued fraction

Therefore  are the denominators of the Pade approximants.

Note that by definition of ,


Going forward we will also suppress the superscript of , , as it will be evident that all terms have the same denominator 


\textbf{Case} .

First note that  is straightforward from the fact that their recurrences are identical.
The recurrence is

Therefore


\textbf{Case} .

Define


This term satisfies the formula

By definition of ,


But note that this is exactly satisfied by the Pad\'e approximants, by the determinantal formula of continued fractions.
This shows that  are the Pad\'e approximants of , as desired.
























 
\subsection{Normalization and Timescales}
\label{sec:proofs:timescale}



\begin{proposition}[Closure properties of TOSSMs]\label{prop:closure}
  Consider a TOSSM  for basis functions  and measure .
  Then, the following are also TOSSMs with the corresponding basis functions and measure:
  \begin{enumerate}
      \item Constant scaling changes the timescale:
       is a TOSSM with basis  and measure .
      \item Identity shift tilts by exponential:
       is a TOSSM with basis  and measure .
      \item Unitary change of basis preserves measure:
         is a TOSSM with basis  and measure .
  \end{enumerate}
\end{proposition}
\begin{proof}

We define  to be the vector of basis functions for the OSSM ,

Recall that the SSM kernels are  so that .

1. The SSM kernels are


It remains to show that the  are orthonormal with respect to measure :

which follows immediately from the change of variables formula.



2. Using the commutativity of  and , the SSM kernels are


It remains to show that  are orthonormal with respect to measure :


3. The SSM basis is


It remains to show that the basis functions  are orthonormal with respect to .
Note that orthonormality of a set of basis functions can be expressed as ,
so that



\end{proof}
  

\end{document}
