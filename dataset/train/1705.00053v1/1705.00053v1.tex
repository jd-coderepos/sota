\begin{figure}
\centering
\includegraphics[trim={0.2in 0in 6.5in 0in},clip,width=3.0in, height=3.5in]{figures/Sequential.pdf} 
\caption{{\bf Past Encoder-Decoder Network.} This portion of Pose-VAE encodes the past input deterministically. The Past Encoder reads in image features from $X_{t}$, corresponding past poses $P_{1..t}$, and their corresponding velocities $Y_{1...t}$. The Past Decoder replays the pose velocities in reverse order. The Past Decoder is only used for training and is discarded during testing. }
\label{fig:SequentialPast}
\end{figure}

In this paper we break down the process of video forecasting into two steps. We first predict the high-level movement in pose space using the \textbf{Pose-VAE}.
Then we use this structure to predict a final pixel level video with the \textbf{Pose-GAN}. 

\subsection{Pose-VAE}
The first step in our pipeline is forecasting in pure pose space. At time $t$, given a series of past poses $P_{1..t}$ and the last frame of in input video $X_{t}$, we want to predict the future poses up to time step $T$, $P_{t+1..T}$. $P_t \in \mathcal{R}^{36}$ is a 2D pose as timestep $t$ represented by the $(x,y)$ locations of $18$ key-points. We actually predict a series of pose velocities $Y_{t+1..T}$.
Given the pose velocities and an initial pose we can then construct the future pose sequence.





To accomplish this forecasting task, we build upon ideas related to sequential encoder-decoder networks ~\cite{Srivastava15, Fragikiadaki15}. As in these papers we can use an LSTM to encode the past information sequence. We call this the \textbf{Past Encoder} which takes in the past information $X_t$, $P_{1..t}$, and $Y_{1..t}$ and encodes it in a hidden representation $H_t$. We also have \textbf{Past Decoder} module to reconstruct the past information from the hidden state. Given this encoding $H_t$ of the past, it would be tempting to use another LSTM to simply produce the future sequence of poses similar to~\cite{Srivastava15}. However, forecasting the future is not a deterministic problem; there may be multiple plausible outcomes of a video. Forecasting actually requires estimating a probability distribution over possible events. To solve this problem, we use a probabilistic Future Decoder. Our probabilistic decoder is nothing but a conditional variational autoencoder where the future velocity $Y_{t+1}$ is predicted given the past information $H_t$, the current pose $P_{t+1}$ (estimated from $P_t$ and $Y_t$), and the random latent vector $z_{t+1}$. The hidden states of the Future Decoder are updated using the standard LSTM update rules.

\noindent{\bf Variational Autoencoders:} A Variational Autoencoder~\cite{Kingma14a} attempts to estimate the probability distribution $P(Y|z)$ of its input data $Y$ given latent variables $z$. An encoder $Q(z|Y)$  learns to encode the inputs into a stochastic latent variable $z$. The decoder $P(Y|z)$ then reconstructs the inputs based on what is sampled from $z$. During training, $z$ is regularized to match $\mathcal{N}(0,1)$ through KL-Divergence. During testing we can then sample our distribution of $Y$ by first sampling $z \sim \mathcal{N}(0,1)$ and then feeding our sample through a neural network $P(Y|z)$ to create a sample from the distribution of Y. Another interpretation is that the decoder $P$ transforms the latent random variable $z \sim \mathcal{N}(0,1)$ into random variable $Y \sim P(Y|z)$.

In our case, we want to estimate a distribution of future pose velocities given the past. Thus we aim to ``encode'' the future into latent variables $z = [z_{t+1}, z_{t+2}, ... z_T]$. Concretely, we wish to learn a way to estimate the distribution $P(Y_{t+1..T}|z,H_{t})$ of future pose velocities $Y_{t+1..T}$ given our encoded knowledge of the past $H_{t}$. Thus we need to train a ``Future Encoder'' that learns an encoding for latent variables $z \sim Q(z|Y_{t+1..T},H_{t})$, where $Q$ is trained to match $\mathcal{N}(0,1)$ as closely as possible. 
During testing, as in~\cite{Walker16}, we sample $z \sim \mathcal{N}(0,1)$ and feed sampled $z$ values into the future decoder network to output different possible forecasts.

\begin{figure}
\centering
\includegraphics[trim={0.5in 0in 6.5in 0in},clip,width=3.0in, height=3.5in]{figures/Forecasting.pdf} 
\caption{{\bf Future Encoder-Decoder Network.} This portion of Pose-VAE encodes the future stochastically. The Future Encoder is a Variational Autoencoder which takes the past $H_{t}$ and the future pose information $Y_{t+1...T}, P_{t+1...T}$ as input and outputs a Normal Distribution $Q$. The Future Decoder then samples $z$ from $Q$ to reconstruct the pose motions $Y_{t+1...T}$ given past $H_{t}$ and poses $P_{t+1....T}$. During testing, the future is not known, so the Future Encoder is discarded, and only the Future Decoder is used with $z \sim \mathcal{N}(0,1)$. }
\label{fig:SequentialFuture}
\end{figure}

\noindent{\bf Past Encoder-Decoder:} Figure~\ref{fig:SequentialPast} shows the Past Encoder-Decoder. The Past Encoder takes as input a frame $X_t$, a series of previous poses $P_{1..t}$, and the previous pose velocities $Y_{1..t}$. We apply a convolutional neural network on $X_t$. The units from the pose information and the image features are concatenated and then fed into an LSTM. After encoding the entire sequence, we use the hidden state of the LSTM at step $t$, $H_t$ to condition the Future Decoder. To enforce that $H_t$ encodes the pose velocity, the hidden state of of the encoding LSTM is fed into a decoder LSTM, the Past Decoder, which is trained through Euclidean loss to reconstruct $Y_{1..t}$ in reverse order. This enforces that the network learns a ``memory'' of past inputs~\cite{Srivastava15}. The Past Decoder exists only as an aid for training, and at test time, only the Past Encoder is used.

\noindent{\bf Future Encoder-Decoder:} Figure~\ref{fig:SequentialFuture} shows the Future Encoder-Decoder. The Future Encoder-Decoder is composed of a VAE encoder (Future Encoder) and a VAE decoder (Future Decoder) both conditioned on past information $H_{t}$. The Future Encoder takes the future pose velocity $Y_{t+1..T}$ and the past information $H_t$ and encodes it as a mean and variance $\mu(Y_{t+1..T}, H_t)$ and $\sigma(Y_{t+1..T}, H_t)$. We then sample a latent variable $z \sim Q(z|Y_{t+1..T}, H_t) = \mathcal{N}(\mu,\sigma)$. During testing, we 
sample $z$ from a standard normal, so during training we incorporate a KL-divergence loss such that $Q$ matches $\mathcal{N}(0,1)$ as closely as possible. Given the latent variable $z$ and the past information $H_t$, the Future Decoder recovers an approximation of the future pose sequence $\hat{Y}_{t+1..T}(z, H_t)$. The training loss for this network is the usual VAE Loss. It is Euclidean distance from the pose trajectories combined with KL-divergence loss of $Q$ from $\mathcal{N}(0,1)$.

\vspace{-.3cm}
\begin{equation}
\begin{split}
L(\hat{Y}_{t+1..T},Y_{t+1..T}) = ||Y_{t+1..T} - \hat{Y}_{t+1..T}||^{2} +
\\
\lambda\mathcal{KL}\left[Q(z|Y_{t+1..T},H_t)\|\mathcal{N}(0,1)\right]
\label{eq:VAE}
\end{split}
\end{equation}
\vspace{-.3cm}

At every future step $t_f$, the Future Decoder takes in $z_{t_f}$, as well as the current pose $P_{t_f}$ and outputs the pose motion $Y_{t_f}$. At training time, we use the ground truth poses, but at test time, we recover the future poses by simply adding the pose trajectory information $P_{t_f+1}=P_{t_f}+Y_{t_f}$. 

\noindent{\bf Implementation Details:} We train our network with Adam Solver at a learning rate of 0.001 and $\beta_{1}$ of 0.9. For the KL-divergence loss we set $\lambda=0.00025$ for 60000 iterations and then set $\lambda=0.0005$ for an additional 20000 iterations of training. Every timestep $t$ represents 0.2 second. We conditioned the past on 2 timesteps and predict for 5 timesteps. For the convolutional network over the image network, we used an architecture almost identical to AlexNet~\cite{Krizhevsky12} with the exception of a smaller (7x7) receptive field at the bottom layer and the addition of batch normalization layers. All layers in the entire network were trained from scratch. The LSTM units consist of two layers, both 1024 units. The Future Encoder is a simple single hidden layer network with ReLU activations and a hidden size of $512$. 



\subsection{Pose-GAN}

\noindent{\bf Generative Adversarial Networks:} Once we sample a pose prediction from our Pose-VAE, we can then render a video of a moving skeleton. Given an input image and a
video of the skeleton, we train a Generative Adversarial Network to predict a pixel level video of future events. As described in ~\cite{Wang16}, GANs consist of two models pitted against each other: a generator $G$ and a discriminator $D$. The generator $G$ takes the input skeleton video and image and attempts to generate a realistic video. The discriminator $D$, trained as a binary classifier, attempts to classify videos as either real or generated. During training, $G$ will try to generate videos which fool $D$, while $D$ will attempt to distinguish the fake videos generated by $G$ from ones sampled from the future video frames.
Following the work of ~\cite{Isola16, Vondrick16} we do not use any noise variables for the adversarial network. All the noise is contained in the Pose-VAE through $z$.


The loss for discriminator $D$ is:

\vspace{-.3cm}
\begin{equation}
\begin{split}
L_{D} = \sum_{i=1}^{M/2} l(D(V_{i}),l_r)+\hspace{-.3cm}
\sum_{i=M/2+1}^{M} l(D(G(I, S_{T})),l_f)
\end{split}
\end{equation}
\vspace{-.3cm}

Where $V$ are videos, $M$ is the batch size, $I$ is an input image, and $S_{T}$ is a video of a pose skeleton, $l_r$ is the real label ($1$), and $l_f$ is the fake label ($0$). Inside the batch $M$, half of videos $V$ are generated, and the rest are real. The loss function $l$ here is the binary entropy loss. 




The loss for generator $G$ is:

\vspace{-.5cm}
\begin{equation}
\begin{split}
L_{G} \hspace{-.1cm} = \hspace{-.4cm} \sum_{i=M/2+1}^{M} \hspace{-.2cm} l(D(G(I, S_{T})),l_r) +
\vspace{-10.0cm} \alpha||G(I, S_{T}) - V_{i}||_1
\end{split}
\end{equation}
\vspace{-.5cm}

Given our Pose-VAE, we can now generate plausible pose motions given a very short clip input. For each sample, we can render a video of a skeleton visualizing how a human will deform over the last frame of the input image. Recent work in adversarial networks has shown that GANs benefit from given structure~\cite{Isola16, Pathak16, Reed16}. In particular,~\cite{Reed16} showed that GANs improve on generating humans when given initial keypoints. In this paper we build on this work by extending this idea to Conditional Video GANs. Given an image and a generated skeleton video, we train a GAN to generate a realistic video at the pixel level. Figure~\ref{fig:GANArchitecture} shows the Pose-GAN network. The architecture of the discriminator $D$ is nearly identical to that of~\cite{Vondrick16}. 

\noindent{\bf Implementation Details:} The Pose-GAN consists of five volumetric convolutional layers with receptive fields of 4, stride of 2, and padding of 1. At each layer LeakyReLU units and Batch Normalization are used. The only difference is that the input is a 64x80 video. For the generator $G$, we first encode the input using a series of five Volumentric Convolutional Layers with receptive fields of 4, stride of 2, and padding of 1. We use LeakyReLU and Batch Normalization at each layer. In order to handle the modified aspect ratio of the input (80x64), the fifth layer has a receptive field of 6 in the spatial dimensions. The top five layers are the same but in reverse, gradually increasing the spatial and temporal resolution to 64x80 pixels at 32 frames. Our training parameters are identical to ~\cite{Vondrick16}, except that we set our regularization parameter $\alpha=1000$. Similar to~\cite{Isola16}, we utilize skip layers for the top part of the network. For the top five layers, ReLU activation and Batch Normalization is used. The final layer is sent through a TanH function in order to scale the outputs. 

\begin{figure}
\centering
\includegraphics[trim={1in 1.2in 1in 1.5in},clip,width=3.25in, height=1.5in]{figures/Generator.pdf} 
\vspace{-0.2in}
\caption{{\bf Generator Architecture}. We use volumetric convolutions at each layer. Receptive field size represents time, width, and length. For each frame in the input pose video we stack the input frame as an extra 3 channels, making each input frame 80x64x6. The number of input and output frames is 32. The output consists of 32 frames, 80x64 pixels.}
\vspace{-0.2in}
\label{fig:GANArchitecture}
\end{figure}
