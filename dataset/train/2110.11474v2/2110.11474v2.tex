\documentclass{bmvc2k}
\usepackage{pifont}
\usepackage{float}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{booktabs}
\usepackage{soul}
\usepackage[inline]{enumitem}
\usepackage{arydshln}
\usepackage[ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}



\title{AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation}

\addauthor{Khoa Vo}{khoavoho@uark.edu}{1}
\addauthor{Hyekang Joo}{hkjoo@cs.umd.edu}{2}
\addauthor{Kashu Yamazaki}{kyamazak@uark.edu}{1}
\addauthor{Sang Truong}{sangt@uark.edu}{1}
\addauthor{Kris Kitani}{kkitani@cs.cmu.edu}{3}
\addauthor{Minh-Triet Tran}{tmtriet@hcmus.edu.vn}{4,5}
\addauthor{Ngan Le}{thile@uark.edu}{1}

\addinstitution{
 AICV Lab, University of Arkansas\\
 Fayetteville, AR, USA
}
\addinstitution{
 University of Maryland\\
 College Park, MD, USA
}
\addinstitution{
 Carnegie Mellon University\\
 Pittsburgh, PA, USA
}
\addinstitution{
 University of Science VNU-HCM\\
 Ho Chi Minh City, Vietnam
}
\addinstitution{
 Vietnam National University\\
 Ho Chi Minh City, Vietnam
}

\runninghead{Khoa Vo et al.}{AEI with Adaptive Attention}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}



\begin{document}

\maketitle

\begin{abstract}
Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. An action only starts when the main actor in the video begins to interact with the environment, while it ends when the main actor stops the interaction. Despite the great progress in temporal action proposal generation, most existing works ignore the aforementioned fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of a human by proposing Actor Environment Interaction (\textbf{AEI}) network to improve the video representation for temporal action proposals generation. AEI contains two modules, i.e., perception-based visual representation (PVR) and boundary-matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed \textbf{adaptive attention mechanism}. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary-matching architectures (i.e., CNN-based and GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly outperforms the state-of-the-art methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection. Source code is available at \footnote{\url{https://github.com/vhvkhoa/TAPG-AgentEnvInteration.git}}.
\blfootnote{These authors contributed equally.}


\end{abstract}



\vspace*{-\baselineskip}

\begin{figure}[t]
\centering
  \includegraphics[width=0.7\linewidth]{Figures/bmvc_figure1.pdf}
  \vspace*{-0.1cm}
  \caption{TAPG comparison. The existing approaches (e.g., BMN \cite{bmn}) apply a network backbone to the entire spatial domain (red boxes); Our AEI takes main actor(s) into consideration via AAM. Our AEI is implemented with CNN-based BMM (AEI-B) and GCN-based BMM (AEI-G).}
  \vspace*{-0.4cm}
  \label{qualitative}
\end{figure}



\section{Introduction}
\label{sec:intro}


Temporal action proposals generation (TAPG) is one of the most important problems in video analysis and video understanding \cite{anchor_2, Jiyang2017, CTAP, Gao_2018_CVPR}. Particularly, TAPG is the fundamental step for other downstream tasks, including temporal action detection \cite{caba2015activitynet, THUMOS14}, action recognition \cite{Kinetics}, and video dense captioning \cite{krishna2017dense}. Given an untrimmed video, TAPG aims to propose temporal segments with specific starting and ending timestamps for each action of interest appearing in the video.

Recently, state-of-the-art (SOTA) methods \cite{lin2018bsn, bmn, BSN++, dbg} follow a paradigm where a set of possible starting and ending timestamps of all actions are detected separately, then a proposal evaluation module is employed to evaluate every possible pair of starting and ending timestamps by predicting its confidence score. A non-maximum suppression (NMS) \cite{NMS, SoftNMS} function is finally used to eliminate redundant candidate proposals based on their confidence scores and overlapping metrics.

As we observe, a human has a capacity to perceive an action being established in a video \cite{cognitive2, cognitive_vision} in two steps. First, the main actors at each temporal period are identified; then, the interactions between main actors and the environment are observed to specify when the action starts and ends. Despite good achievements on benchmarking datasets \cite{caba2015activitynet, THUMOS14}, the SOTA approaches \cite{bmn, BSN++, dbg} disregard the above perception process of humans by only applying a backbone network (pre-trained on action recognition task) to extract the video representation, leading to a potential loss of some proposals. For instance, in Fig.~\ref{qualitative}, the works in the literature take the whole spatial region of video frames (e.g., red boxes) to propose action intervals; this, however, may lead to inaccurate results because the background occupies much bigger region than the actor who performs the action (e.g., blue boxes). In Fig.~\ref{qualitative}, the "rope skipping" action can trick an action proposal model into missing the time at which this action starts or ends due to a subtle difference in shape between between "jumping" and "standing".











In this paper, we propose a novel \textbf{Actor Environment Interaction network (AEI)} in an attempt to simulate and explore the capability of human-perception process. Our AEI consists of a visual representation module (PVR) and a boundary-matching module (BMM). The PVR is comprised of three components: (i) environment spectator; (ii) actors spectator; and (iii) actors-environment interaction spectator. As illustrated in Fig.~\ref{qualitative}, the environment spectator processes the entire spatial dimensions of a snippet (red boxes) to capture the global environmental information. The actors spectator focuses on actors (blue and light gray boxes around humans) to capture local appearance and motion information of actors. Additionally, we introduce a novel \textbf{adaptive attention mechanism (AAM)} in the actors spectator to select the main actors (blue boxes) who mainly commit the action. Given a video snippet, the features corresponding to the global environment and the local main actors are first extracted by the first two spectators. The relationship between the environment and the main actors is then modeled by the third component (i.e., the actors-environment interaction spectator).





\textbf{Our contributions are summarized as follows:}
\begin{itemize}[leftmargin=*,noitemsep,topsep=-3pt] \item We propose a video representation network, AEI, which follows the human-perception process to understand human action.
    \item We introduce a novel \emph{adaptive attention mechanism (AAM)} that simultaneously selects main actors and eliminates inessential actor(s) and then extracts semantic relations between main actors.
    \item We investigate the effectiveness of the proposed AEI by implementing the BMM under two network architectures: CNN-based and GCN-based.
\item Our proposed AEI network achieves the SOTA performance on common benchmarking datasets of ActivityNet-1.3 and THUMOS-14 in both TAPG and TAD tracks with a large margin compared to the previous works.
\end{itemize}

\section{Related Works}

\noindent
\textbf{Temporal Action Proposal Generation (TAPG)} 

TAPG aims to propose intervals that tightly contain actions in a long untrimmed video. Previous works can be divided into two main groups: anchor-based and boundary-based. Anchor-based methods \cite{actionproposal_2016, FasterR_CNN_Action, anchor_1, anchor_2, anchor_3}, which are inspired by anchor-based object detection methods in 2D images \cite{FasterRCNN, RetinaNet, yolov3}, predefine a set of fixed segments and try to fit them into ground-truth action segments in the video. Although a regression network may be applied in some of those methods to refine the proposals, a finite number of anchors cannot fit all ground-truth actions with diverse lengths. Boundary-based methods \cite{lin2018bsn, BSN++, bmn, dbg, xu2020gtad, KhoaVo_ICASSP, KhoaVo_Access} address this problem by localizing the starting and ending timestamps of all actions appearing in the video and matching them by a boundary-matching module. Our boundary-matching module belongs to the second group.











\vspace{0.2cm}
\noindent
\textbf{Attention Networks}

Attention Networks (AN) have a long history in the artificial neural networks literature~\cite{itti1998model}. We can divide AN into two main groups: Soft-Attention Networks (Soft-AN) and Hard-Attention Networks (Hard-AN). \cite{bahdanau2014neural} was one of the first Soft-AN that was applied to machine translation. Because of its differentiable architecture, which helps the whole model learn in an end-to-end fashion, Soft-AN has become an essential component in a large number of applications (e.g., speech \cite{cho2015describing}, NLP \cite{galassi2020attention}, computer vision \cite{chaudhari2019attentive}).
Hard-AN was first introduced in \cite{xu2015show} and \cite{Saccader_NIPS2019} for digit and object classifications, respectively. Hard-AN aims to mask out irrelevant elements of the inputs to reduce the distractions. This is an advanced benefit over Soft-AN; however, Hard-AN in \cite{xu2015show} is indifferentiable. Recently, \cite{patro2018differential} proposes a Hard-AN that can be trained by normal gradient back-propagation, with a fundamental observation that the L2-norm values of more important features are usually higher than those of less important features in a feature map.
In this work, we propose AAM, a module to leverage both the differentiable Hard-AN \cite{patro2018differential} and the self-attention network \cite{attention_is_all_you_need} to select the main actors of the video and to learn the relations between main actors, respectively.






\section{Our Proposed AEI}


Given an input video , where  is the number of frames, we follow the common paradigm from previous works to divide it into a sequence of \textit{snippets}, each of which consists of  consecutive frames from the video, resulting in a total of  snippets. Let  be an encode function to extract visual representation of a -frame snippet , the entire video is presented as:




Prior works \cite{bmn, lin2018bsn, xu2020gtad} employ a pre-trained backbone network (e.g., C3D network \cite{C3D} or Two-Stream network \cite{2_stream_1}) to model .
However, simply applying those networks for video representation may have some drawbacks as mentioned in Section \ref{sec:intro}. In Section \ref{subsec:pvr}, our proposed perception-based visual representation (PVR) is discussed as an alternative to the former strategy. Then, boundary-matching module for temporal action proposals generation is discussed in Section \ref{subsec:bmm}. 







\begin{figure}[t]
\centering
  \includegraphics[width=0.8\linewidth]{Figures/BMVC-Fig2.pdf}
  \vspace*{0.2cm}
  \caption{The architecture of our proposed PVR. Given a -snippet, the corresponding snippet visual representation is obtained by three modules: (i) environment spectator to extract global environment feature; (ii) actors spectator to extract local actor representation; and (iii) actors-environment interaction spectator to model the relationship between the environment feature and the actor feature.}
  \vspace*{-0.4cm}
  \label{video_representation}
\end{figure}

\subsection{Perception-based Visual Representation (PVR)}
\label{subsec:pvr}
The PVR module aims to extract video visual representation based on how a human perceives an action, i.e., identifying the main actors at each temporal period and interactions between main actors and the environment to specify when the action starts and ends. PVR consists of three main components: (i) environment spectator; (ii) actors spectator; and (iii) actors-environment interaction spectator. The overall architecture of PVR is shown in Fig. \ref{video_representation}.



\vspace{0.2cm}
\textbf{(i) Environment Spectator}
\label{subsubsec:env}
aims to extract global semantic information of the input -frame snippet. To extract both the spatial and temporal details of the snippet, we adopt a 3D network pre-trained on action recognition benchmarking datasets as a backbone feature extractor. The snippet is processed through all convolutional blocks of the 3D network to obtain a feature map ; then, an average pooling operator is employed to produce a spatio-temporal feature vector .






\vspace{0.2cm}
\textbf{(ii) Actors Spectator}
\label{subsubsec:actors}
aims to semantically extract main actor(s) representation. An action cannot happen in the absence of a human (actor). However, when an action occurs, it does not necessarily signal that every actor in the scene has committed the action. First, the actors spectator detects all existing actors in the snippet by an \textit{actor localization} module. Then, an \textit{adaptive attention mechanism (AAM)} is proposed to adaptively select an arbitrary number of main actor(s) and extract their mutual relationships to represent them as a single feature vector.

\begin{figure}[t]
\centering
  \includegraphics[width=0.9\linewidth]{Figures/BMVC-Fig3.pdf}
  \vspace*{-0.1cm}
  \caption{Illustration of our proposed AAM. Given an environment feature  and a set of actor features , this module aims to select main actor features, followed by fusing arbitrary main actor features to obtain an actor visual representation .}
  \vspace*{-0.4cm}
  \label{adaptive_attention}
\end{figure}

\noindent
\textit{\textbf{Actor Localization}}: 
To localize all actors in a -frame snippet, we apply a human detector onto the middle frame of it with the assumption that, with a small , the actors would not move fast enough to be mis-located. We denote   as a set of detected human bounding boxes, where . Afterwards, each of the detected bounding boxes, , is aligned onto feature map  (obtained by the 3D network backbone from environment spectator) using RoIAlign \cite{MaskRCNN_ICCV17} and then average-pooled into a single feature vector . Finally, we obtain a set of actor features . \\
\noindent
\textit{\textbf{Adaptive Attention Mechanism (AAM)}}:
Given  detected actors, there are only a few of detected actors (called main actors) who actually contribute to the action if it presents. Because the number of main actors is unknown and continuously changes throughout the input video, we propose an adaptive attention mechanism (AAM) that inherits the merits from adaptive hard attention to select an arbitrary number of main actors and a soft self-attention mechanism \cite{attention_is_all_you_need} to extract relationships among them. AAM is described by pseudocode in Algorithm \ref{algo:aam} and illustrated in Fig. \ref{adaptive_attention}; more details are provided in the supplementary.

\begin{algorithm}[t]
\footnotesize
\DontPrintSemicolon
\SetNoFillComment
\KwData{Feature vector  and features set  represent environment and all actors that appear in input snippet, respectively.}
\KwResult{Feature vector  represents main actors in input snippet.}
 \tcp*{embed  to common space with every  in }
set  and  to empty list \tcp*{ will store scores of every actor}
set  to empty list \tcp*{ will store selected main actors}
\For{each  in }{
     \tcp*{embed  to common space with }
     to  \tcp*{compute main actor score corresponding to }
    append  to 
}

 \tcp*{rescale scores to sum up to }
 \tcp*{compute adaptive threshold }

\For{each  in }{
    \If{}
    {
        append  to  \tcp*{select  if its score higher than }
    }
}
 \tcp*{fuse main actor features by self-attention \cite{attention_is_all_you_need}}
\caption{Adaptive Attention Mechanism (AAM) to extract representation of main actors in a snippet.}
\label{algo:aam}
\end{algorithm}





\begin{figure*}[t]
\centering
  \includegraphics[width=0.8\linewidth]{Figures/BMVC-Fig4.pdf}
  \vspace*{-0.1cm}
  \caption{The overall architecture of our proposed AEI, consisting of perception-based visual representation module (PVR), and boundary-matching module (BMM). }
  \vspace*{-0.4cm}
  \label{full_architecture}
\end{figure*}






\vspace{0.2cm}
\textbf{(iii) Actors-Environment Interaction Spectator}
This module aims to model the relations between environment feature  and actors representation feature , and then combine them into a single feature . Herein, we employ the self-attention model \cite{attention_is_all_you_need} where  and  are the inputs. We denote  as a visual representation for snippet .





\subsection{Boundary-Matching Module (BMM)}
\label{subsec:bmm}

BMM is responsible for generating action proposals, which are boundary-pairs of every possible action of interest appearing in the video. Our BMM contains three components: base module, temporal evaluation module, and proposal evaluation module as illustrated in Fig. \ref{full_architecture}. The base module aims to model the semantic relationship between snippets. The temporal evaluation module assesses each snippet  in the video to estimate probabilities that any action starts or ends there, corresponding to  and , respectively. Meanwhile, the proposal evaluation module evaluates every interval  in the video to estimate its actionness score, corresponding to , where .














At the inference stage, we search through  and  to select temporal locations  whose  or  are local maximums to form sets of potential starting and ending temporal locations, respectively. Then, starting and ending locations  from those lists that satisfy timing constraint (e.g. ) are paired and become a candidate proposal with a score . 
Based on the timestamps and scores of candidate proposals, we finally apply NMS \cite{SoftNMS, NMS} to produce the final set of action proposals.







In our paper, we have conducted BMM under two different network architectures: CNN-based and GCN-based. Our CNN-based BMM, called \textbf{AEI-B}, is leveraged by \cite{bmn} where the base module is comprised of 1D convolutional layers to learn and extract the temporal relations between snippets. On the other hand, our GCN-based BMM, called \textbf{AEI-G}, is leveraged by \cite{xu2020gtad} to extract not only local relations, but also the relations of snippets that share close semantic features.










\subsection{Training Methodology}
\label{subsec:train_method}









Given a list of  ground-truth action segments  of input video , we generate the ground-truth starting labels , ending labels  and actionness labels  ( is a pre-defined maximum proposal length).  (or ) carrying a value of  means that there is a ground-truth starting (or ending) boundary of any action at  and vice versa. Likewise,  carrying a value of  means that there is a ground-truth action starts at  with a length of .
To train our AEI network with the ground-truth labels, we define the loss function  as follows:

We use weighted binary log-likelihood loss  for  and , which is defined as follows:


where  and  are the number of positives and negatives in ground-truth labels, respectively. Conversely, , where  is the mean squared error loss and  is set to .

To reduce time cost in training phase of our proposed AEI network, actors features set  and environment feature  for actors spectator and environment spectator, respectively, are extracted in advance. Then, the AAM and Interaction Spectator of PVR module is trained with BMM module in an end-to-end fashion.

\section{Experiments}
\noindent
\textbf{Datasets}

\noindent
We conduct experiments on ActivityNet-1.3 \cite{caba2015activitynet} and THUMOS-14 \cite{THUMOS14} datasets. The former contains 20,000 videos with 200 annotated activities while the latter consists of 414 videos with 20 actions. For both datasets, we follow previous works \cite{lin2018bsn,bmn, dbg} to preprocess videos with the snippet length set to .









\begin{table}[t]
\centering
\caption{\textbf{TAPG} comparisons in terms of AR@AN and AUC between our AEI and other SOTA methods on \textbf{ActivityNet-1.3}.}
\vspace*{0.2cm}
\resizebox{\linewidth}{!}{
\begin{tabular}{c||ccccccccc|cc}
\centering
\multirow{2}{*}{Metrics} & BSN & MGG & MR & BMN & DBG & BSN++ & TSI++ & AEN & ABN & \multirow{2}{*}{AEI-B} & \multirow{2}{*}{AEI-G} \\
& \cite{lin2018bsn} & \cite{liu2019multi} & \cite{MR_eccv2020} & \cite{bmn} & \cite{dbg} & \cite{BSN++} & \cite{tsi_accv} & \cite{KhoaVo_ICASSP} & \cite{KhoaVo_Access} & &  \\ \hline \hline
AR@100 (val)& 74.16 & 74.54 & 75.27 & 75.01 & 76.65 & 76.52 & 76.31 & 75.65 & 76.72 & \textbf{77.25} & \underline{\textit{77.24}} \\
AUC (val)   & 66.17 & 66.43 & 66.51 & 67.10 & 68.23 & 68.26 & 68.35 & 68.15 & 69.16 & \underline{\textit{69.43}} & \textbf{69.47} \\
AUC (test)  & 66.26 & 66.47 & - & 67.19 & 68.57 & - & 68.85 & 68.99 & 69.26 & \underline{\textit{69.94}} & \textbf{70.09} \\
\bottomrule
\end{tabular}
}
\label{activitynet_proposal1}
\end{table}









\noindent
\textbf{Metrics}
\newline 
In TAPG, AR@AN and AUC are popular metrics to benchmark the performance of competing methods. The former is the average recall (AR) computed with the average number of proposals (AN) kept by each video. The latter is the score of the area under AR versus the AN curve. In ActivityNet-1.3, AR@100 and AUC are mainly used. On the other hand, in THUMOS-14, only AR@AN is used to compare between methods; however, multiple AN is selected from a list of [50, 100, 200, 500, 1000].
\newline\indent
In TAD, both ActivityNet-1.3 and THUMOS-14 use mean Average Precision (mAP) to benchmark methods in this problem. ActivityNet-1.3 uses tIoU thresholds of \{0.5, 0.75, 0.95\} and average mAP, while THUMOS-14 uses tIoU thresholds of \{0.3, 0.4, 0.5, 0.6, 0.7\} for evaluation.

\noindent
\textbf{Implementation Details}

\noindent
For all experiments on both ActivityNet-1.3 \cite{caba2015activitynet} and THUMOS-14 \cite{THUMOS14}, we employ C3D \cite{C3D} network pre-trained on Kinetics-400 \cite{Kinetics} as the backbone network to extract features from video snippets. The features extracted from C3D backbone have 2048 dimensions.
\newline \indent
In the actors spectator, for actor localization, we employ a Faster-RCNN model \cite{FasterRCNN} pre-trained on COCO \cite{cocodataset} dataset to detect humans as discussed in \ref{subsubsec:actors}. To train our AEI network, we utilize Adam optimizer with the initial learning rate set to 0.0001 and 0.001 for ActivityNet-1.3 and THUMOS-14, respectively. 
\newline \indent
In TAPG, Soft-NMS \cite{SoftNMS} is applied in post-processing for all experiments on ActivityNet-1.3, while on THUMOS-14, both Soft-NMS \cite{SoftNMS} and NMS \cite{NMS} are evaluated. In TAD, following \cite{lin2018bsn, bmn}, we use NMS \cite{NMS} for post-processing on both datasets.
\newline \indent
In the following experiments, we highlight the best performance in \textbf{bold} and the second-best performance in \underline{\textit{italic}}.

\subsection{Temporal Action Proposal Generation (TAPG)}

Table \ref{activitynet_proposal1} demonstrates the comparison on ActivityNet-1.3 validation and testing sets. Based on the results, it can be observed that the performances of our methods, \text{AEI-B} and \text{AEI-G}, are standing out against those of SOTA methods in terms of AR@100 and AUC by large margins. Table \ref{thumos_proposal} presents the comparison of SOTA TAPG methods on THUMOS-14 dataset. Compared to SOTA approaches, our AEI obtains better performance on all AR@ANs metrics regardless of the architecture of BMM. From Table \ref{activitynet_proposal1}, \ref{thumos_proposal}, we empirically observe that AEI-G, which employs GCN to model the relationship between snippets, marginally surpasses  AEI-B on TAPG.



\begin{table}[t]
\centering
\caption{\textbf{TAD} comparisons on \textbf{ActivityNet-1.3} in terms of mAP@tIoU and mAP, where the proposals are combined with video-level classification results generated by \cite{action_protocol}.}
\vspace*{0.2cm}
\resizebox{\linewidth}{!}{
\begin{tabular}{c||cccccccc|cc}
\multirow{2}{*}{Metrics} & BSN & BMN & GTAD & P-GCN & MR & ABN & TSI++ & GTAN & \multirow{2}{*}{AEI-B} & \multirow{2}{*}{AEI-G} \\
& \cite{lin2018bsn} & \cite{bmn} & \cite{xu2020gtad} & \cite{pgcn_cvpr2020} & \cite{MR_eccv2020} & \cite{KhoaVo_Access} & \cite{tsi_accv} & \cite{gtan_cvpr2019} &  &  \\
\hline \hline
0.5 & 46.5 & 50.1 & 50.4 & 42.9 & 43.5 & 51.8 & 51.2 & \textbf{52.6} & 52.3 & \underline{\textit{52.4}}\\
0.75 & 30.0 & \underline{\textit{34.8}} & 34.6 & 28.1 & 33.9 & 34.2 & \textbf{35.0} & 34.1 & 34.5 & 34.5 \\
0.95 & 8.0 & 8.3 & 9.0 & 2.5 & 9.2 & \textbf{10.3} & 6.6 & 8.9 & \underline{\textit{9.7}} & 9.6 \\
Average & 30.0 & 33.9 & 34.1 & 27.0 & 30.1 & 34.2 & 34.2 & \underline{\textit{34.3}} & \textbf{34.7} & \textbf{34.7} \\
\bottomrule
\end{tabular}
}
\label{activitynet_detection}
\end{table}



Generalizability is also considered as an important criterion to evaluate a method in TAPG. 
Following the same experiment setup in \cite{bmn, dbg}, our AEI is trained on \textit{Unseen+Seen} and \textit{Seen} training sets, separately, and then is evaluated on the \textit{Seen} and \textit{Unseen} validation sets, separately as illustrated in Fig.~\ref{fig:Generalizability}. The performances on \textit{Seen} validation set is shown in the first two charts, whereas the performances on \textit{Unseen} validation set is given in the last two charts.
Fig. \ref{fig:Generalizability} shows that our AEI achieves good performances on \textit{Seen} validation set with an acceptable drop on \textit{Unseen} validation set on both training configurations, suggesting that our AEI is highly generalizable to unseen action types.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/generalizability.pdf}
  \vspace*{-0.1cm}
  \caption{\textbf{Generalizability} evaluation and comparisons with BMN \cite{bmn} and DBG \cite{dbg} on ActivityNet 1.3 in terms of AR@100 and AUC. Methods are trained on \textit{Unseen+Seen} (blue columns) and \textit{Seen} training sets (orange columns), respectively; and are evaluated on \textit{Seen} (first two charts) and \textit{Unseen} (last two charts) validation sets.}
  \label{fig:Generalizability}
  \vspace*{-0.4cm}
\end{figure}










\subsection{Temporal Action Detection (TAD)}

Following the experiment settings in \cite{lin2018bsn, bmn, xu2020gtad}, we adopt top-1 video-level classification results generated by the method in \cite{action_protocol} on ActivityNet-1.3 to label the proposals produced by our method. We use top-2 video-level classification results generated by UntrimmedNet \cite{untrimmetNet} to label proposals generated by our method on THUMOS-14.

Table \ref{activitynet_detection} illustrates TAD performance comparison between AEI and other SOTA methods on ActivityNet-1.3 validation set. The results emphasize that our methods outperform SOTA methods in spite of CNN-based BMM or GCN-based BMM. The experiment results on THUMOS-14 test set in Table \ref{thumos_detection} demonstrate that our AEI-B and AEI-G are superior to other SOTA methods on most of the metrics regardless of UntrimmedNet \cite{untrimmetNet} or P-GCN \cite{pgcn_cvpr2020} classifiers. From Table \ref{activitynet_detection} and \ref{thumos_detection}, we empirically notice that both AEI-B and AEI-G obtain comparable TAD performance.

\begin{table}[tb]
    \begin{minipage}[t]{.55\linewidth}
        \centering
        \caption{\textbf{TAPG} comparisons on \textbf{THUMOS-14} in terms of AR@AN, where SNMS represents Soft-NMS \cite{SoftNMS}.}\vspace{0.2cm}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c c c c c c }
        Method & @50 & @100 & @200 & @500 & @1000 \\
        \hline \hline
        T-TAG \cite{anchor_3}    & 18.55 & 29.00 & 39.61 & -     & -     \\
        CTAP  \cite{CTAP}        & 32.49 & 42.61 & 51.97 & -     & -     \\
        BSN \cite{lin2018bsn}    & 37.46 & 46.06 & 53.21 & 60.64 & 64.52 \\
        MGG  \cite{liu2019multi} & 39.93 & 47.75 & 54.65 & 61.36 & 64.06 \\
        BMN \cite{bmn}           & 39.36 & 47.72 & 54.70 & 62.07 & 65.49 \\
        DBG+NMS\cite{dbg}           & 40.89 & 49.24 & 55.76 & 61.43 & 61.95 \\
        DBG+SNMS \cite{dbg}      & 37.32 & 46.67 & 54.50 & 62.21 & 66.40 \\ 
        TSI++\cite{tsi_accv}     & 42.30 & 50.51 & 57.24 & 63.43 & -     \\
        MR\cite{MR_eccv2020}     & 44.23 & 50.67 & 55.74 & -     & -     \\
        ABN+NMS\cite{KhoaVo_Access}  & 44.89 & 51.86 & 57.36 & 61.67 & 62.59 \\
        ABN+SNMS\cite{KhoaVo_Access}  & 40.87 & 49.09 & 56.24 & 63.53 & 67.29 \\
        \hline
        \textbf{AEI-B+NMS} & \textbf{45.74} & \underline{\textit{52.39}} & 57.74 & 62.49 & 63.38 \\
        \textbf{AEI-G+NMS} & 45.12 & \textbf{52.81} & \underline{\textit{57.94}} & 62.11 & 63.17 \\ 
        \textbf{AEI-B+SNMS} & 44.97 & 50.13 & 57.34 & \underline{\textit{64.43}} & \underline{\textit{67.78}} \\
        \textbf{AEI-G+SNMS} & \underline{\textit{45.31}} & 51.12 & \textbf{58.19} & \textbf{64.58} & \textbf{67.96} \\
        \bottomrule
        \end{tabular}
        }
        \label{thumos_proposal}
    \end{minipage}\hspace{0.02\linewidth}
    \begin{minipage}[t]{.44\linewidth}
        \centering
        \caption{\textbf{TAD} comparisons on \textbf{THUMOS-14} in term of mAP@tIoU.  indicates P-GCN classifier ( group); otherwise, UntrimmedNet classifier.}\vspace{0.2cm}
\resizebox{\linewidth}{!}{
        \begin{tabular}{l l l l l l l} 
        Method  & 0.7   & 0.6  & 0.5   & 0.4   & 0.3 \\ \hline
        \hline
T-TAP\cite{SST_CVPR2017}   & 6.3  & 14.1 & 24.5 & 35.3 & 46.3 \\
        BSN \cite{lin2018bsn}      & 20.0 & 28.4 & 36.9 & 45.0 & 53.5 \\
        BMN \cite{bmn}             & 20.5 & 29.7 & 38.8 & 47.4 & 56.0 \\
        MGG \cite{liu2019multi}    & 21.3 & 29.5 & 37.4 & 46.8 & 53.9 \\
        DBG \cite{dbg}             & 21.7 & 30.2 & 39.8 & 49.4 & 57.8 \\
        GTAD \cite{xu2020gtad}     & \textbf{23.4} & 30.8 & 40.2 & 47.6 & 54.5 \\
        TSI++\cite{tsi_accv}       & 22.4 & 33.2 & 42.6 & \underline{\textit{52.1}} & \textbf{61.0} \\
        GTAN~\cite{gtan_cvpr2019}  & -- & -- & 38.8 & 47.2 & 57.8 \\
\hline
        
\textbf{AEI-B} & \textbf{23.4} & \textbf{35.9} & \textbf{44.7} & \textbf{52.7} & \underline{\textit{58.7}} \\\textbf{AEI-G} & \underline{\textit{22.9}} & \underline{\textit{34.2}} & \underline{\textit{43.4}} & 51.6 & 57.4 \\\hline \hline
        BSN\cite{lin2018bsn}   & --   & --   & 49.1 & 57.8 & 63.6 \\
        GTAD \cite{xu2020gtad} & \underline{\textit{22.9}} & 37.6 & 51.6 & \underline{\textit{60.4}} & 66.4 \\
        MR\cite{MR_eccv2020}     & \textbf{28.5} & \textbf{38.0} & 45.4 & 50.7 & 53.9 \\
        \hline
\textbf{AEI-B} & 22.4 & 37.8 & \textbf{52.1} & \textbf{60.6} & \underline{\textit{67.3}} \\
        \textbf{AEI-G} & 22.3 & \underline{\textit{37.9}} & \underline{\textit{52.0}} & \underline{\textit{60.4}} & \textbf{67.6} \\
        \bottomrule
        \end{tabular}
        }
        \label{thumos_detection}
    \end{minipage} 
\end{table}

\subsection{Ablation Study}

We further conduct a detailed ablation study on THUMOS-14 dataset to evaluate the contributions of different components of the proposed AEI framework. We conduct two ablation studies as shown in Fig.~\ref{ablation_fig} on TAPG and THUMOS-14 in terms of AR@ANs.

First, we evaluate the contribution of each spectator to the overall performance of our proposed PVR (described in Section~\ref{subsec:pvr}), i.e., environment spectator, actors spectator, and interaction spectator. As illustrated in Fig.~\ref{ablation_fig} (a), "Environment spectator only", which only focuses on global information, plays an important role in TAPG, whereas "Actors spectator only", which takes only local information of the main actor(s) into account, achieves adequate performance. "W/o interaction spectator", which withdraws the interaction spectator by simply fusing global and local information using an averaging operation, gives an undesired performance that is even lower than "Environment only". The complete proposed model, e.g., "AEI (all spectators)", gives the best result thanks to the interaction spectator adaptively fusing global feature from environment spectator and local feature from actors spectator.

In addition, we also evaluate the effectiveness of main actor selection and feature fusion in our proposed AAM. Fig.~\ref{ablation_fig} (b) shows the performance of the network without each of these components. In the "AEI w/o feature fusion" settings, we use an average pooling layer to fuse features obtained from main actor selection component. As illustrated, both configurations achieve similar performance with AN below 600, while main actor selection component plays a slightly more significant role than feature fusion component. This implies that having an appropriate main actor selection contributes significantly to the entire network.










\begin{figure}[t]
\centering
  \includegraphics[width=0.75\linewidth]{Figures/ablation_study_new.png}
  \vspace*{-0.1cm}
  \caption{TAPG comparisons on different AEI configurations: (a) either only environment or only actors spectator or both; (b) either only main actor selection or only feature fusion or both.}.
\label{ablation_fig}
  \vspace*{-0.4cm}
\end{figure}

\section*{Conclusion}
In this paper, we proposed a novel actors-environment interaction (AEI) network to simulate human perceiving ability in the temporal action proposals generation. Our AEI contains two modules: perception-based visual representation (PVR) and boundary-matching (BMM). PVR aims to extract visual representation of each snippet. To achieve the human perceiving ability, PVR is equipped with three spectators, each of which learns to perceive input snippet at a unique aspect, e.g. environment, main actors, and actors-environment interactions. An adaptive attention mechanism (AAM) is proposed in actors spectator to select an arbitrary number of main actor(s) in the snippet as well as learning the relationships between them.

Extensive experiments are conducted on ActivityNet-1.3 and THUMOS-14 datasets on TAPG and TAD tasks, which demonstrate that our proposed AEI outperforms SOTA methods regardless of BMM architecture (e.g., CNN-based or GCN-based). These results prove that replicating human perceiving ability in video understanding is a promising track to follow and further explore in the future. 

Beside three proposed spectators in PVR, we can include additional spectators to observe human body parts and the interaction between them with objects to better handle egocentric videos, in which the main actor who perform the action does explicitly appear.

\newpage






















































\section*{Acknowledgment}
This material is based upon work supported by the National Science Foundation under Award No. OIA-1946391; partially funded by Gia Lam Urban Development and Investment Company Limited, Vingroup and supported by Vingroup Innovation Foundation (VINIF) under project code VINIF.2019.DA19.

\bibliography{egbib}
\end{document}
