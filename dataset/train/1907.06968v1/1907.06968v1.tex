\documentclass{bmvc2k}

\title{A Unified Deep Framework for Joint 3D Pose Estimation and Action Recognition from a Single RGB Camera}

\addauthor{Huy Hieu PHAM}{huy-hieu.pham@cerema.fr}{5}
\addauthor{Houssam Salmane}{houssam.salmane@cerema.fr}{1}
\addauthor{Louahdi~Khoudour}{louahdi.khoudour@cerema.fr}{1}
\addauthor{Alain~Crouzil}{alain.crouzil@irit.fr}{2}
\addauthor{Pablo~Zegers}{pablozegers@gmail.com}{3}
\addauthor{Sergio~A.~Velastin}{sergio.velastin@ieee.org}{4}


\addinstitution{
 Cerema Research Center, Toulouse, France
}
\addinstitution{
 Informatics Research Institute of Toulouse (IRIT), Toulouse, France
}

\addinstitution{
 Aparnix, Chile
}

\addinstitution{
 Cortexica Vision Systems Ltd., London, UK
}

\addinstitution{
Vingroup Big Data Institute (VinBDI), Hanoi, Vietnam
}



\runninghead{A Preprint}{Hieu Pham \textit{et al.} 2019}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{float}
\usepackage{mathtools}

\begin{document}
\maketitle

\begin{abstract}
We present a deep learning-based multitask framework for joint 3D human pose estimation and action recognition from RGB video sequences. Our approach proceeds along two stages. In the first, we run a real-time 2D pose detector to determine the precise pixel location of important keypoints of the body. A two-stream neural network is then designed and trained to map detected 2D keypoints into 3D poses. In the second, we deploy the Efficient Neural Architecture Search (ENAS) algorithm to find an optimal network architecture that is used for modeling the spatio-temporal evolution of the estimated 3D poses via an image-based intermediate representation and performing action recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction datasets verify the effectiveness of the proposed method on the targeted tasks. Moreover, we show that our method requires a low computational budget for training and inference.
\end{abstract} 

\section{Introduction \0.15cm]
\hspace*{0.5cm}  First, we present a two-stream, lightweight neural network to recover 3D human poses from RGB images provided by a monocular camera. Our proposed method achieves state-of-the-art result on 3D human pose estimation task and benefits action recognition.\0.15cm]
The rest of this paper is organized as follows. We present a review of the related work in Section \ref{sect:2}. The proposed method is explained in Section \ref{sect:3}. Experiments are provided in Section \ref{sect:4} and Section \ref{sect:5} concludes the paper.\-0.2cm]} \label{sect:2}
This section reviews two main topics that are directly related to ours, \textit{i.e.} 3D pose estimation from RGB images and 3D pose-based action recognition. Due to the limited size of a conference paper, an extensive literature review is beyond the scope of this section. Instead, the interested reader is referred to the surveys of Sarafianos \etal \cite{Sarafianos20163DHP} for recent advances in 3D human pose estimation and Presti \etal \cite{presti20163d} for 3D skeleton-based action recognition.\-0.1cm]}
The problem of 3D human pose estimation has been intensively studied in the recent years. Almost all early approaches for this task were based on feature engineering \cite{4020735,10.1007/978-3-642-33765-9_41,6682899}, while the current state-of-the-art methods are based on deep neural networks \cite{li20143d,tekin2016direct,pavlakos2017coarse,pavllo20183d,VNect_SIGGRAPH2017,Katircioglu2018}. Many of them are regression-based approaches that directly predict 3D poses from RGB images via 2D/3D heatmaps. For instance, Li \etal \cite{li20143d} designed a deep convolutional network for human detection and pose regression. The regression network learns to predict 3D poses from single images using the output of a body
part detection network. Tekin \etal \cite{tekin2016direct} proposed to use a deep network to learn a regression mapping that directly estimates the 3D pose in a given frame of a sequence from a spatio-temporal volume centered on it. Pavlakos \etal \cite{pavlakos2017coarse} used multiple fully convolutional networks to construct a volumetric stacked hourglass architecture, which is able to recover 3D poses from RGB images. Pavllo \etal \cite{pavllo20183d} exploited a temporal dilated convolutional network \cite{Yu2016MultiScaleCA} for estimating 3D poses. However, this approach led to a significant increase in the number of parameters as well as the required memory. Mehta \etal \cite{VNect_SIGGRAPH2017} introduced a real-time approach to predict 3D poses from a single RGB camera. They used ResNets \cite{7780459} to jointly predict 2D and 3D heatmaps as regression tasks. Recently, Katircioglu \etal \cite{Katircioglu2018} introduced a deep regression network for predicting 3D human poses from monocular images via 2D joint location heatmaps. This architecture is in fact an overcomplete autoencoder that learns a high-dimensional latent pose representation and accounts for joint dependencies, in which a Long Short-Term Memory (LSTM) network \cite{Hochreiter1997LongSM} is used to enforce temporal consistency on 3D pose predictions.

To the best of our knowledge, several studies \cite{pavlakos2017coarse,VNect_SIGGRAPH2017,Katircioglu2018} stated that regressing the 3D pose from 2D joint locations is difficult and not too accurate. However, motivated by Martinez \etal \cite{martinez_2017_3dbaseline}, we believe that a simple neural network can learn effectively a \textit{direct 2D-to-3D mapping}. Therefore, this paper aims at proposing a simple, effective and real-time approach for 3D human pose estimation that benefits action recognition. To this end, we design and optimize a two-stream deep neural network that performs 3D pose predictions from the 2D human poses. These 2D poses are generated by a state-of-the-art 2D detector that is able to run in real-time for multiple people. We empirically show that although the proposed approach is computationally inexpensive, it is still able to improve the state-of-the-art.\-0.1cm]}
Human action recognition from skeletal data or 3D poses is a challenging task. Previous works on this topic can be divided into two main groups of method. The first group \cite{Lv2006RecognitionAS,Wang2012MiningAE,Vemulapalli2014HumanAR} extracts hand-crafted features and uses probabilistic graphical models, \textit{e.g.} Hidden Markov Model (HMM) \cite{Lv2006RecognitionAS} or Conditional Random Field (CRF) \cite{Han2010DiscriminativeHA} to recognize actions. However, almost all of these approaches require a lot of feature engineering. The second group \cite{Liu2016SpatioTemporalLW,Du2015HierarchicalRN,Shahroudy2016NTURA} considers the 3D pose-based action recognition as a time-series problem and proposes to use Recurrent Neural Networks with Long-Short Term Memory units (RNN-LSTMs) \cite{Hochreiter1997LongSM} for modeling the dynamics of the skeletons. Although RNN-LSTMs are able to model the long-term temporal characteristics of motion and have advanced the state-of-the-art, this approach feeds raw 3D poses directly into the network and just considers them as a kind of low-level feature. The large number of input features makes RNNs very complex and may easily lead to overfitting. Moreover, many RNN-LSTMs act merely as classifiers and cannot extract high-level features for recognition tasks \cite{Sainath2015ConvolutionalLS}.

In the literature, 3D human pose estimation and action recognition are closely related. However, both problems are generally considered as two distinct tasks \cite{cheronICCV15}. Although some approaches have been proposed for tackling the problem of jointly predicting 3D poses and recognizing actions in RGB images or video sequences \cite{5540235,7298734,luvizon20182d}, they are data-dependent and require a lot of feature engineering, except the work of Luvizon \etal \cite{luvizon20182d}. Unlike in previous studies, we propose a multitask learning framework for 3D pose-based action recognition by reconstructing 3D skeletons from RGB images and exploiting them for action recognition in a joint way. Experimental results on public and challenging datasets show that our framework is able to solve the two tasks in an effective way. \-0.2cm]} \label{sect:3}
We explain the proposed method is this section. First, our approach for 3D human pose estimation is presented. We then introduce our solution for 3D pose-based action recognition. \-0.1cm]}
Given an RGB video clip of a person who starts to perform an action at time  and ends at , the problem studied in this work is to generate a sequence of 3D poses , where ,  at the estimation stage. The generated  is then used as input for the recognition stage to predict the corresponding action label  by a supervised learning model. See Figure \ref{fig:1} for an illustration of the problem. \-0.1cm]}
Given an input RGB image , we aim to estimate the body joint locations in the 3-dimensional space, noted as  . To this end, we first run the state-of-the-art human 2D pose detector, namely OpenPose \cite{cao2017realtime}, to produce a series of 2D keypoints . To recover the 3D joint locations, we try to learn a \textit{direct 2D-to-3D mapping} : . This transformation can be implemented by a deep neural network in a supervised manner

where  is a set of trainable parameters of the function . To optimize , we minimize the prediction error over a labelled dataset of  poses by solving the optimization problem

Here  and  are the input 2D poses and the ground truth 3D poses, respectively;  denotes a loss function. In our implementation the robust Huber loss \cite{huber1992robust} is used to deal with outliers. \-1cm]
\subsection{3D pose-based action recognition \-0.6cm]
\begin{figure}[ht]
\begin{center}
 \includegraphics[width=13cm,height=2.2cm]{proposed-model.png}
 \vspace*{-0.8cm}
\end{center}
   \caption{Illustration of the proposed approach for 3D pose-based action recognition.}
\label{fig:4}
\end{figure}  \-0.2cm]} \label{sect:4}
\subsection{Datasets and settings \0.15cm]
\textbf{Human3.6M} \cite{6682899}: This is a very large-scale dataset containing 3.6 million different 3D articulated poses captured from 11 actors for 17 actions, under 4 different viewpoints. For each subject, the dataset provides 32 body joints, from which only 17 joints are used for training and computing scores. In particular, 2D joint locations and 3D poses ground truth are available for evaluating supervised learning models.\0.15cm]
\textbf{SBU Kinect Interaction} \cite{yun2012two}: This dataset contains a total of 300 interactions, performed by 7 participants for 8 actions. This is a challenging dataset due to the fact that it contains pairs of actions that are difficult to distinguish such as \textit{exchanging objects} -- \textit{shaking hands} or \textit{pushing} -- \textit{punching}. We randomly split the whole dataset into 5 folds, in which 4 folds are used for training and the remaining 1 fold is used for testing.\-0.1cm]}
The proposed networks were implemented in Python with Keras/TensorFlow backend. The two streams of the 3D pose estimator are trained separately with the same hyperparameters setting, in which we use mini-batches of 128 poses with 0.25 dropout rate. The weights are initialized by the He initialization \cite{He2015DelvingDI}. Adam optimizer \cite{Kingma2014AdamAM} is used with default parameters. The initial learning rate is set to 0.001 and is decreased by a factor of 0.5 after every 50 epochs. The network is trained for 300 epochs from scratch on the Human3.6M dataset \cite{6682899}. For action recognition task, we run OpenPose \cite{cao2017realtime} to generate 2D detections on MSR Action3D \cite{li2010action} and SBU Kinect Interaction \cite{yun2012two}. The pre-trained 3D pose estimator on Human3.6M \cite{6682899} is then used to provide 3D poses. We use standard data pre-processing and augmentation techniques such as randomly cropping and flipping on these two datasets due to their small sizes. To discover optimal recognition networks, we use ENAS \cite{pmlr-v80-pham18a} with the same parameter setting as the original work. Concretely, the shared parameters   are trained with Nesterov's accelerated gradient descent \cite{Nesterov} using Cosine learning rate \cite{Loshchilov2016SGDRSG}. The candidate architectures are initialized by He initialization \cite{He2015DelvingDI} and trained by Adam optimizer \cite{Kingma2014AdamAM} with a learning rate of 0.00035. Additionally, each search is run for 200 epochs.\-0.1cm]} \label{sect:4.3}
\subsubsection{Evaluation on 3D pose estimation}
We evaluate the effectiveness of the proposed 3D pose estimation network using the standard protocol of the Human3.6M dataset \cite{6682899,pavlakos2017coarse,martinez_2017_3dbaseline,VNect_SIGGRAPH2017}. Five subjects S1, S5, S6, S7, S8 are used for training and the rest two subjects S9, S11 are used for evaluation. Experimental results are reported by the average error in millimeters between the ground truth and the corresponding predictions over all joints. Much to our surprise, our method outperforms the previous best result from the literature \cite{martinez_2017_3dbaseline} by 3.1mm, corresponding to an error reduction of 6.8\% even when combining the ground truth 2D locations with the 2D OpenPose detections. This result proves that our network design can learn to recover the 3D pose from the 2D joint locations with a remarkably low error rate, which to the best of our knowledge, has established a new state-of-the-art on 3D human pose estimation (see Table \ref{tab:1} and Figure \ref{fig:5}). \-0.75cm]                
\end{tabular}
\end{center}
\scriptsize{\hspace*{0.25cm} The symbol  denotes that a 2D detector was used and the symbol  denotes the ground truth 2D joint locations were used.} \-0.1cm]}
Table~\ref{table2} reports the experimental results and comparisons with state-of-the-art methods on the MSR Action3D dataset \cite{li2010action}. The ENAS algorithm \cite{pmlr-v80-pham18a} is able to explore a diversity of network architectures and the best design is identified based on its validation score. Thus, the final architecture achieved a total average accuracy of 97.98\% over three subset AS1, AS2 and AS3. This result outperforms many previous studies \cite{li2010action,Chen2013RealtimeHA,Vemulapalli2014HumanAR,Du2015HierarchicalRN,Liu2016SpatioTemporalLW,Wang2016GraphBS,WengSpatioTemporalNN,Xu2015SpatioTemporalPM,lee2017ensemble}, and among them, many are depth sensor-based approaches. Figure \ref{fig:6} provides a schematic diagram of the best cells and optimal architecture found by ENAS on the AS1 subset \cite{li2010action}. For the SBU Kinect Interaction dataset \cite{yun2012two}, the best model achieved an accuracy of 96.30\%, as shown in Table~\ref{table3}. Our reported results indicated an important observation that by using only the 3D predicted poses, we are able to outperform previous works reported in \cite{Song2017AnES,Liu2016SpatioTemporalLW,8322204,ke2017new,DBLP:conf/bmvc/TasK18,Wang2017ModelingTD,liu2018skeleton} and reach state-of-the-art results provided in \cite{zhang2019view,s19081932}, which deploy accurate skeletal data provided by Kinect v2 sensor.
\begin{table}
\parbox{.45\linewidth}{
\centering
\caption{Test accuracies (\%) on the MSR Action3D dataset \cite{li2010action}.}
\vspace*{-0.3cm}
\begin{tabular}{ccccc}
\label{table2}\\
\hline
{\scriptsize \hspace*{0.01cm} \textbf{Method}} & {\scriptsize \hspace*{0.01cm}  \textbf{AS1}} & {\scriptsize \hspace*{0.01cm} \textbf{AS2}} & {\scriptsize \hspace*{0.01cm} \textbf{AS3}} &  {\scriptsize \hspace*{0.01cm} \textbf{Aver.}}  \\
 \hline
 {\scriptsize \hspace*{0.01cm} Li \etal \cite{li2010action}} & {\scriptsize \hspace*{0.01cm} 72.90} & {\scriptsize \hspace*{0.01cm} 71.90}  & {\scriptsize \hspace*{0.01cm} 71.90} &  {\scriptsize \hspace*{0.01cm} 74.70} \\
{\scriptsize \cellcolor{gray!30}  Chen \etal \cite{Chen2013RealtimeHA}} & {\scriptsize \cellcolor{gray!30} 96.20} & {\scriptsize \cellcolor{gray!30} 83.20}  & {\scriptsize \cellcolor{gray!30} 92.00} &  {\scriptsize \cellcolor{gray!30} 90.47} \\
{\scriptsize \hspace*{0.01cm}  Vemulapalli \etal \cite{Vemulapalli2014HumanAR}} & {\scriptsize \hspace*{0.01cm}  95.29} & {\scriptsize \hspace*{0.01cm}  83.87}  & {\scriptsize  \hspace*{0.01cm} 98.22} &  {\scriptsize \hspace*{0.01cm} 92.46} \\
{\scriptsize \cellcolor{gray!30} Du \etal \cite{Du2015HierarchicalRN}} & {\scriptsize \cellcolor{gray!30}    99.33} & {\scriptsize \cellcolor{gray!30}  94.64}  & {\scriptsize \cellcolor{gray!30}   95.50} &  {\scriptsize \cellcolor{gray!30}  94.49} \\
{\scriptsize  \hspace*{0.01cm} Liu \etal \cite{Liu2016SpatioTemporalLW}} & {\scriptsize \hspace*{0.01cm} N/A} & {\scriptsize  \hspace*{0.01cm} N/A}  & {\scriptsize  \hspace*{0.01cm} N/A} &  {\scriptsize  \hspace*{0.01cm} 94.80}
 \\
{\scriptsize \cellcolor{gray!30} Wang \etal \cite{Wang2016GraphBS}} & {\scriptsize \cellcolor{gray!30}   93.60} & {\scriptsize  \cellcolor{gray!30}  95.50}  & {\scriptsize  \cellcolor{gray!30}   95.10} &  {\scriptsize \cellcolor{gray!30}   94.80}
 \\
{\scriptsize  \hspace*{0.01cm} Wang \etal \cite{WengSpatioTemporalNN}} & {\scriptsize  \hspace*{0.01cm} 91.50} & {\scriptsize   \hspace*{0.01cm} 95.60}  & {\scriptsize   \hspace*{0.01cm} 97.30} &  {\scriptsize  \hspace*{0.01cm} 94.80} \\
{\scriptsize \cellcolor{gray!30} Xu \etal \cite{Xu2015SpatioTemporalPM}} & {\scriptsize\cellcolor{gray!30} 99.10} & {\scriptsize \cellcolor{gray!30} 92.90}  & {\scriptsize  \cellcolor{gray!30} 96.40} &  {\scriptsize \cellcolor{gray!30} 96.10} \\
{\scriptsize \hspace*{0.01cm} Lee \etal \cite{lee2017ensemble}} & {\scriptsize  \hspace*{0.01cm} 95.24} & {\scriptsize  \hspace*{0.01cm} 96.43}  & {\scriptsize   \hspace*{0.01cm}  100.0} &  {\scriptsize  \hspace*{0.01cm} 97.22} \\
{\scriptsize \cellcolor{gray!30} \hspace*{0.01cm} Pham \etal \cite{s19081932}} & {\scriptsize \cellcolor{gray!30}  \hspace*{0.01cm} 98.83} & {\scriptsize  \cellcolor{gray!30} \hspace*{0.01cm} 99.06}  & {\scriptsize  \cellcolor{gray!30} \hspace*{0.01cm}  99.40} &  {\scriptsize  \cellcolor{gray!30} \hspace*{0.01cm} 99.10} \\
\hline
{\scriptsize  \hspace*{0.01cm} \textbf{Ours}} & {\scriptsize   \hspace*{0.01cm} \textbf{97.87} } & {\scriptsize   \hspace*{0.01cm} \textbf{96.81}}  & {\scriptsize   \hspace*{0.01cm} \textbf{99.27}} &  {\scriptsize  \hspace*{0.01cm} \textbf{97.98}} \\    
\hline
\hline
\end{tabular}}
\hfill
\parbox{.45\linewidth}{
\centering
\caption{Test accuracies (\%) on the SBU Kinect Interaction dataset \cite{yun2012two}.}
\vspace*{-0.3cm}
\begin{tabular}{cc}
\label{table3}\\
\hline
\hspace*{0.01cm} {\scriptsize \textbf{Method}} & {\scriptsize \hspace*{0.3cm} \textbf{Acc.}} \\
\hline
{\scriptsize \hspace*{0.01cm} Song \etal \cite{Song2017AnES}} & {\scriptsize \hspace*{0.32cm} 91.51} \\
{\scriptsize \cellcolor{gray!30} Liu \etal \cite{Liu2016SpatioTemporalLW}} & {\scriptsize \cellcolor{gray!30}  \hspace*{0.25cm} 93.30}\\
{\scriptsize  \hspace*{0.01cm}  Weng \etal \cite{8322204}} & {\scriptsize   \hspace*{0.32cm} 93.30} \\
{\scriptsize   \cellcolor{gray!30} Ke \etal \cite{ke2017new}} & {\scriptsize \cellcolor{gray!30}  \hspace*{0.25cm} 93.57} \\
{\scriptsize   \hspace*{0.01cm} Tas \etal \cite{DBLP:conf/bmvc/TasK18}} & {\scriptsize  \hspace*{0.32cm} 94.36} \\
{\scriptsize \cellcolor{gray!30}   \hspace*{0.01cm} Wang \etal  \cite{Wang2017ModelingTD}} & {\scriptsize  \cellcolor{gray!30} \hspace*{0.32cm} 94.80} \\
{\scriptsize   \hspace*{0.01cm}  Liu \etal \cite{liu2018skeleton}} & {\scriptsize  \hspace*{0.32cm} 94.90}  \\
{\scriptsize \cellcolor{gray!30}   \hspace*{0.01cm} Zang \etal  \cite{zhang2019view} (using VA-RNN)} & {\scriptsize  \cellcolor{gray!30} \hspace*{0.32cm} 95.70} \\
{\scriptsize \hspace*{0.01cm} Zhang \etal  \cite{zhang2019view} (using VA-CNN)} & {\scriptsize   \hspace*{0.32cm} 97.50} \\
{\scriptsize \cellcolor{gray!30} \hspace*{0.01cm} Pham \etal  \cite{s19081932}} & {\scriptsize \cellcolor{gray!30}  \hspace*{0.32cm} 97.86} \\
\hline
{\scriptsize    \textbf{Ours} } & {\scriptsize   \hspace*{0.4cm} \textbf{96.30}}  \\
\hline
\hline
\end{tabular}}
\end{table}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=5.5cm,height=4.5cm]{normall_cell.png}\includegraphics[width=5.5cm,height=4.5cm]{reduction_cell.png} \hspace{0.1cm} \includegraphics[width=1.5cm,height=4.5cm]{model.png}\\
\scriptsize{ \hspace*{2cm} (a) \hspace{5.2cm} (b) \hspace{3cm} (c)}
\vspace*{-0.5cm}
\end{center}
   \caption{Diagram of the top performing \textit{normal cell} (a) and \textit{reduction cell} (b) discovered by ENAS \cite{pmlr-v80-pham18a} on AS1 subset \cite{li2010action}. They were then used to construct the final network architecture (c). We recommend the interested readers to \cite{pmlr-v80-pham18a} to better understand this procedure.} 
\label{fig:6}
\end{figure} 
\subsection{Computational efficiency evaluation  \-0.2cm]} \label{sect:5}
In this paper, we presented a unified deep learning framework for joint 3D human pose estimation and action recognition from RGB video sequences. The proposed method first runs a state-of-the-art 2D pose detector to estimate 2D locations of body joints. A deep neural network is then designed and trained to learn a direct 2D-to-3D mapping and predict human poses in 3D space. Experimental results demonstrated that the 3D human poses can be effectively estimated by a simple network design and training methodology over 2D keypoints. We also introduced a novel action recognition approach based on a compact image-based representation and automated machine learning, in which an advanced neural architecture search algorithm is exploited to discover the best performing architecture for each recognition task. Our experiments on public and challenging action recognition datasets indicated that the proposed framework is able to reach state-of-the-art performance, whilst requiring less computation budget for training and inference. Despite that, our method naturally depends on the quality of the output of the 2D detectors. Hence, a limitation is that it cannot recover 3D poses from 2D failed output. To tackle this problem, we are currently expanding this study by adding more visual evidence to the network in order to further gains in performance. The preliminary results are encouraging. 
\bibliography{egbib}
\end{document}
