\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{nips15submit_e,times}
\usepackage[square,sort,numbers]{natbib}
\usepackage[small,tight]{bibhacks}
\usepackage{hyperref}
\usepackage{url}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{ifthen}
\usepackage{pbox}
\usepackage{floatrow}
\usepackage{sansmath}




\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepgfplotslibrary{external}



\title{Generative Image Modeling Using Spatial LSTMs}

\author{
Lucas Theis \\
University of Tübingen \\
72076 Tübingen, Germany \\
\texttt{lucas@bethgelab.org} \\
\And
Matthias Bethge \\
University of Tübingen \\
72076 Tübingen, Germany \\
\texttt{matthias@bethgelab.org} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\renewcommand{\refname}{\normalsize References}

\setlength{\textfloatsep}{15pt}

\nipsfinalcopy 

\begin{document}
	\maketitle

	\begin{abstract}
	Modeling the distribution of natural images is challenging, partly because of
	strong statistical dependencies which can extend over hundreds of pixels.
	Recurrent neural networks have been successful in capturing long-range dependencies
	in a number of problems but only recently have found their way into generative image
	models. We here introduce a recurrent image model based on multi-dimensional long short-term
	memory units which are particularly suited for image modeling due to their
	spatial structure. Our model scales to images of arbitrary size and its likelihood is
	computationally tractable. We find that it outperforms the state of the art in quantitative
	comparisons on several image datasets and produces promising results when used for texture
	synthesis and inpainting.
	\end{abstract}

	\section{Introduction}
The last few years have seen tremendous progress in learning useful image representations
		\cite{Donahue:2014}. While early successes were often achieved through the use of generative
		models \cite[e.g.,][]{Hinton:2006,Lee:2009,Ranzato:2011}, recent breakthroughs were mainly
		driven by improvements in supervised techniques \cite[e.g.,][]{Krizhevsky:2012,Simonyan:2015}.
		Yet unsupervised learning has the potential to tap into the much larger source of unlabeled
		data, which may be important for training bigger systems capable of a more
		general scene understanding. For example, multimodal data is abundant but often unlabeled,
		yet can still greatly benefit unsupervised approaches \cite{Srivastava:2014}.

Generative models provide a principled approach to unsupervised learning.
		A perfect model of natural images would be able to optimally predict parts of an image given other parts
		of an image and thereby clearly demonstrate a form of scene understanding.
		When extended by labels, the Bayesian framework can be used to perform semi-supervised learning
		in the generative model \cite{Ngiam:2011,Kingma:2014b} while it is less clear how
		to combine other unsupervised approaches with discriminative learning.
		Generative image models are also useful in more traditional applications such as
		image reconstruction \cite{Roth:2009,Zoran:2011,Sohl-Dickstein:2015} or compression \cite{VanDenOord:2014b}.

Recently there has been a renewed strong interest in the development of generative image models
		\cite[e.g.,][]{VanDenOord:2014b,Kingma:2014a,Uria:2014,Gregor:2014,Goodfellow:2014,Ranzato:2014,Gregor:2015,Sohl-Dickstein:2015,Li:2015,Denton:2015}.
		Most of this work has tried to bring to bear the flexibility of deep neural networks on the
		problem of modeling the distribution of natural images. One challenge in this endeavor is to
		find the right balance between tractability and flexibility. The present article contributes
		to this line of research by introducing a fully tractable yet highly flexible image model.

Our model combines multi-dimensional recurrent neural networks \cite{Graves:2009} with
		mixtures of experts. More specifically, the backbone of our model is formed by a spatial variant of
		\textit{long short-term memory} (LSTM) \cite{Hochreiter:1997}.
		One-dimensional LSTMs have been particularly successful in modeling text and speech
		\cite[e.g.,][]{Sundermeyer:2010,Sutskever:2014}, but have also been used to model the progression of frames in video \cite{Srivastava:2014} and very
		recently to model single images \cite{Gregor:2015}. In contrast to earlier work on modeling
		images, here we use \textit{multi-dimensional LSTMs} \cite{Graves:2009} which naturally lend themselves to the task
		of generative image modeling due to their spatial structure and ability to capture long-range
		correlations.

To model the distribution of pixels conditioned on the hidden states of the neural network, we use
		\textit{mixtures of conditional Gaussian scale mixtures} (MCGSMs) \cite{Theis:2012a}.
		This class of models can be viewed as a generalization of Gaussian mixture models, but
		their parametrization makes them much more suitable for natural images. By treating
		images as instances of a stationary stochastic process, this model allows us to sample and capture the
		correlations of arbitrarily large images.

	\section{A recurrent model of natural images}
		\label{sec:ride}
		\begin{figure}
			\centering
			\tikzset{hunit/.style={circle,draw,minimum size=0.2cm,inner sep=0cm,fill=white}}
\tikzset{vunit/.style={circle,draw,minimum size=0.2cm,inner sep=0cm,fill=black!20!white}}
\tikzset{aunit/.style={circle,minimum size=0.2cm,inner sep=0cm,fill=blue!40!green,opacity=.5}}
\tikzset{munit/.style={circle,minimum size=0.2cm,inner sep=0cm,fill=red,opacity=.5}}
\tikzset{tunit/.style={circle,minimum size=0.2cm,inner sep=0cm,fill=black}}
\tikzset{connection/.style={-,black,opacity=.4}}
\tikzset{arrow/.style={-stealth',opacity=1}}
\tikzset{pixel/.style={rectangle,draw,minimum size=0.2cm,inner sep=0cm,fill=black!20!white}}
\tikzset{parent/.style={rectangle,draw,minimum size=0.2cm,inner sep=0cm,fill=blue!40!green,opacity=.5}}

\begin{tikzpicture}
  \def\zoffset{.16};
  \def\xoffset{.45};
  \def\yoffset{.22};



  \node[align=left, text width=1.5cm] at (7 * \xoffset, 3 + \yoffset) {\scriptsize\textsf{Pixels}};
  \node[align=left, text width=1.5cm] at (7 * \xoffset, 2 + \yoffset) {\scriptsize\textsf{SLSTM units}};
  \node[align=left, text width=1.5cm] at (7 * \xoffset, 1 + \yoffset) {\scriptsize\textsf{SLSTM units}};
  \node[align=left, text width=1.5cm] at (7 * \xoffset, 0 + \yoffset) {\scriptsize\textsf{Pixels}};

  \node at (2 * \xoffset + \zoffset, 3.8) {\scriptsize\textsf{\textbf{RIDE}}};

  \foreach \z in {2, 1, 0} {
    \foreach \x in {0, 1, 2, 3, 4} {
\node[draw,vunit] (v0\x\z) at (\x * \xoffset + \z * \zoffset, \z * \yoffset) {};
      \node[draw,hunit] (h0\x\z) at (\x * \xoffset + \z * \zoffset, \z * \yoffset + 1) {};
      \node[draw,hunit] (h1\x\z) at (\x * \xoffset + \z * \zoffset, \z * \yoffset + 2) {};
      \node[draw,vunit] (v1\x\z) at (\x * \xoffset + \z * \zoffset, \z * \yoffset + 3) {};

\draw [connection] (h0\x\z.north) to (h1\x\z.south);
      \draw [connection] (h1\x\z.north) to (v1\x\z.south);

\ifthenelse{\z > 0}{
        \draw [connection] (\x * \xoffset + \z * \zoffset, \z * \yoffset + 1) to (\x * \xoffset + \z * \zoffset - \zoffset, \z * \yoffset - \yoffset + 1);
        \draw [connection] (\x * \xoffset + \z * \zoffset, \z * \yoffset + 2) to (\x * \xoffset + \z * \zoffset - \zoffset, \z * \yoffset - \yoffset + 2);}{};
    }

    \ifthenelse{\z = 1}{
     \draw [connection,arrow] (2 * \xoffset + \z * \zoffset, \z * \yoffset + 1) to (2 * \xoffset + \z * \zoffset - \zoffset, \z * \yoffset - \yoffset + 1);
     \draw [connection,arrow] (2 * \xoffset + \z * \zoffset, \z * \yoffset + 2) to (2 * \xoffset + \z * \zoffset - \zoffset, \z * \yoffset - \yoffset + 2);
\foreach \x in {1, 2, 3} {
        \node [draw,aunit] at (\x * \xoffset + 1 * \zoffset, 1 * \yoffset) {};
      }
    }{}

    \ifthenelse{\z = 0}{
\node[draw,aunit] at (1 * \xoffset, 0) {};

\foreach \x in {1, 2, 3}
        \draw [connection] (v0\x1) to (h020.south);
      \draw [connection,arrow] (v010) to (h020.south);

\node [draw,tunit] at (2 * \xoffset + 0 * \zoffset, 0 * \yoffset + 3) {};
    }{}

    \ifthenelse{\z = 3 \OR \z = 2}{
      \foreach \x in {0, 1, 2, 3} {
        \node [draw,munit] at (\x * \xoffset + \z * \zoffset, \z * \yoffset) {};
      }
    }{}

    \ifthenelse{\z = 1 \OR \z = 0}{
      \foreach \x in {0} {
        \node [draw,munit] at (\x * \xoffset + \z * \zoffset, \z * \yoffset) {};
      }
    }{}

    \foreach \x in {0, 1, 2, 3} {
\pgfmathparse{\x + 1};
      \pgfmathtruncatemacro\r{\pgfmathresult};
      \draw [connection] (h0\x\z.east) to (h0\r\z.west);
      \draw [connection] (h1\x\z.east) to (h1\r\z.west);
    }
  }
  \draw [connection,arrow] (h120.north) to (v120.south);
  \draw [connection,arrow] (h020.north) to (h120.south);
  \draw [connection,arrow] (h010.east) to (h020.west);
  \draw [connection,arrow] (h110.east) to (h120.west);



  \begin{scope}[xshift=-4cm,yshift=1.2cm]
    \def\zoffset{0};
    \def\xoffset{.45};
    \def\yoffset{.45};

    \node[tunit] (l3) at (5 * \xoffset, 2 * \yoffset) {};
    \node[vunit] (l4) at (5 * \xoffset, 1.2 * \yoffset) {};
    \node[aunit] at (5 * \xoffset, 1.2 * \yoffset) {};
    \node[right=0.03cm of l3,yshift=-.04cm] {\footnotesize };
    \node[right=0.03cm of l4,yshift=-.04cm] {\footnotesize };

    \node at (2 * \xoffset, 3 * \yoffset) {\scriptsize\textsf{\textbf{MCGSM}}};

\foreach \z in {2, 1, 0, -1} {
      \foreach \x in {0, 1, 2, 3, 4}
        \node[draw,vunit] (v\x\z) at (\x * \xoffset + \z * \zoffset, \z * \yoffset) {};

\ifthenelse{\z = 1}{
        \foreach \x in {1, 2, 3}
          \node [draw,aunit] at (\x * \xoffset + 1 * \zoffset, 1 * \yoffset) {};
      }{}
    }
    \node [draw,aunit] at (1 * \xoffset, 0) {};
    \node [draw,tunit] at (2 * \xoffset, 0) {};

\foreach \z in {1, 0, -1}
      \foreach \x in {0, 1, 2, 3, 4} {
        \pgfmathparse{\z + 1};
        \pgfmathtruncatemacro\r{\pgfmathresult};
        \draw [connection,arrow,opacity=.2] (v\x\r) to (v\x\z) {};
      }

    \foreach \z in {2, 1, 0, -1}
      \foreach \x in {0, 1, 2, 3} {
        \pgfmathparse{\x + 1};
        \pgfmathtruncatemacro\r{\pgfmathresult};
        \draw [connection,arrow,opacity=.2] (v\x\z) to (v\r\z) {};
      }

    \foreach \z in {1, 0, -1}
      \foreach \x in {0, 1, 2, 3} {
        \pgfmathparse{\x + 1};
        \pgfmathtruncatemacro\r{\pgfmathresult};
        \pgfmathparse{\z + 1};
        \pgfmathtruncatemacro\s{\pgfmathresult};
        \draw [connection,arrow,opacity=.2] (v\x\s) to (v\r\z) {};
        \draw [connection,arrow,opacity=.2] (v\r\s) to (v\x\z) {};
      }

    \draw [connection,arrow] (v10) to (v20);
    \draw [connection,arrow] (v11) to (v20);
    \draw [connection,arrow] (v21) to (v20);
    \draw [connection,arrow] (v31) to (v20);
  \end{scope}

  \node at (-7.9cm,2.5cm) {\small\textsf{\textbf{A}}};
  \node at (-4.3cm,2.5cm) {\small\textsf{\textbf{B}}};
  \node at (-0.4cm,3.4cm) {\small\textsf{\textbf{C}}};

  \begin{scope}[xshift=-7.6cm,yshift=.7cm]
    \node[pixel,fill=black] (l1) at (9.5 * 0.2cm, 7 * 0.2cm) {};
    \node[pixel] (l2) at (9.5 * 0.2cm, 5 * 0.2cm) {};
    \node[parent] at (9.5 * 0.2cm, 5 * 0.2cm) {};
    \node[right=0.03cm of l1,yshift=-.04cm] {\footnotesize };
    \node[right=0.03cm of l2,yshift=-.04cm] {\footnotesize };

    \foreach \y in {0, 1, 2, 3, 4, 5, 6, 7}
      \foreach \x in {0, 1, 2, 3, 4, 5, 6, 7}
        \node[pixel] at (\x * 0.2cm, \y * 0.2cm) {};
    \foreach \y in {3, 4, 5, 6, 7}
      \foreach \x in {0, 1, 2, 3, 4, 5, 6, 7}
        \node[parent] at (\x * 0.2cm, \y * 0.2cm) {};
    \foreach \x in {0, 1, 2, 3}
      \node[parent] at (\x * 0.2cm, 2 * 0.2cm) {};
    \node[pixel,fill=black] at (4 * 0.2cm, 2 * 0.2cm) {};
  \end{scope}
\end{tikzpicture}
 			\caption{(\textbf{A}) We factorize the distribution of images such that the prediction
				of a pixel (black) may depend on any pixel in the upper-left green region. (\textbf{B}) A graphical model representation of an MCGSM with a
				causal neighborhood limited to a small region. (\textbf{C}) A visualization of our recurrent image model with two layers of spatial LSTMs.
				The pixels of the image are represented twice and some
				arrows are omitted for clarity. Through feedforward connections, the prediction of a pixel depends
				directly on its neighborhood (green), but through recurrent connections it has
				access to the information in a much larger region (red).}
			\label{fig:rim}
		\end{figure}
		In the following, we first review and extend the MCGSM \cite{Theis:2012a} and
		multi-dimensional LSTMs \cite{Graves:2009} before explaining how to combine them into
		a recurrent image model. Section~\ref{sec:experiments} will demonstrate
		the validity of our approach by evaluating and comparing the model on a number of image datasets.

		\subsection{Factorized mixtures of conditional Gaussian scale mixtures}
		\label{sec:mcgsm}
		One successful approach to building flexible yet tractable generative models has been to
		use fully-visible belief networks \cite{Neal:1992,Larochelle:2011}. To apply
		such a model to images, we have to give the pixels an ordering and specify the
		distribution of each pixel conditioned on its parent pixels. Several parametrizations
		have been suggested for the conditional distributions in the context of natural images \cite{Domke:2008,Hosseini:2010,Theis:2012a,Uria:2013,Uria:2014}.
		We here review and extend the work of Theis et al. \cite{Theis:2012a} who proposed
		to use \textit{mixtures of conditional Gaussian scale mixtures} (MCGSMs).

		Let  be a grayscale image patch and  be the intensity of the pixel at location
		. Further, let  designate the set of pixels  such that 
		or  and  (Figure~\ref{fig:rim}A). Then
		
		for the distribution of any parametric model with parameters . Note that this
		factorization does not make any independence assumptions but is simply an
		application of the probability chain rule. Further note that the conditional distributions
		all share the same set of parameters. One way to improve the
		representational power of a model is thus to endow each conditional distribution with its own set of parameters,
		
		Applying this trick to mixtures of Gaussian scale mixtures (MoGSMs) yields the MCGSM \cite{Theis:2011}.
		Untying shared parameters can drastically increase the number of parameters. For images, it
		can easily be reduced again by adding assumptions. For example, we can limit
		 to a smaller neighborhood surrounding the pixel by making a Markov
		assumption. We will refer to the resulting set of parents as the pixel's \textit{causal neighborhood} (Figure~\ref{fig:rim}B).
		Another reasonable assumption is stationarity or shift invariance, in which case we only have
		to learn one set of parameters  which can then be used at every pixel
		location. Similar to convolutions in neural networks, this allows the model to easily scale to images of arbitrary size.
		While this assumption reintroduces parameter sharing constraints into the model, the constraints are
		different from the ones induced by the joint mixture model.

		The conditional distribution in an MCGSM takes the form of a mixture of experts,
		where the sum is over mixture component indices  corresponding to different covariances
		and scales  corresponding to different variances.
		The gates and experts in an MCGSM are given by
		
		where  is positive definite. The number of parameters of an MCGSM still grows quadratically with
		the dimensionality of the causal neighborhood. To further reduce the number of parameters,
		we introduce a factorized form of the MCGSM with additional parameter sharing by replacing
		 with .
		This \textit{factorized MCGSM} allows us to use larger neighborhoods and more mixture components.
		A detailed derivation of a more general version which also allows for multivariate pixels is given in
		Supplementary Section~1.

		\subsection{Spatial long short-term memory}
		In the following we briefly describe the \textit{spatial LSTM} (SLSTM), a special case of the
		multi-dimensional LSTM first described by Graves \& Schmidhuber \cite{Graves:2009}.
		At the core of the model are memory units  and hidden units
		. For each location  on a two-dimensional grid, the operations
		performed by the spatial LSTM are given by \\
		\begin{minipage}{0.5\textwidth}
			
		\end{minipage}
		\begin{minipage}{0.5\textwidth}
			
		\end{minipage} \
			\mathbf{\hat x} &= \textstyle\mathbf{C}_\mathbf{xx}^{-\frac{1}{2}} \left( \mathbf{x} - \mathbf{m}_\mathbf{x} \right), &
			\mathbf{\hat y} &= \textstyle \mathbf{W} (\mathbf{y} - \mathbf{C}_{\mathbf{yx}} \mathbf{C}_\mathbf{xx}^{-\frac{1}{2}} \mathbf{\hat x} - \mathbf{m}_\mathbf{y}), &
			\mathbf{W} &= (\mathbf{C}_\mathbf{yy} - \mathbf{C}_{\mathbf{yx}} \mathbf{C}_\mathbf{xx}^{-1} \mathbf{C}_{\mathbf{yx}}^\top)^{-\frac{1}{2}},
		
			\lim_{N \rightarrow \infty} \mathbb{E}\left[ \log p(\mathbf{x})/N^2 \right]
			&= \mathbb{E}[\log p(x_{ij} \mid \mathbf{x}_{<ij})],
		
			\textstyle\alpha = \min\left\{ 1, \frac{p(\mathbf{x}')}{p(\mathbf{x})} \frac{p(\mathbf{x}_{ij} \mid \mathbf{x}_{<ij})}{p(\mathbf{x}_{ij}' \mid \mathbf{x}_{<ij})} \right\},
		
		where here  represents a 5 by 5 pixel patch and  its
		proposed replacement. Since evaluating the joint and conditional densities on the entire image is
		costly, we approximated  using RIDE applied to a 19 by 19 pixel patch surrounding .
		Randomly flipping images vertically or horizontally in between the sampling further helped.
		Figure~\ref{fig:inpainting} shows results after 100 Gibbs sampling sweeps.
		\vspace{-.15cm}
		\begin{figure}
			\vspace{-.2cm}
			\centering
			\begin{tikzpicture}
	\def\size{2.2cm};
	\def\x{1.05};
	\node at (0 * \x * \size, 0) {\includegraphics[width=\size]{figures/D12_original.png}};
	\node at (1 * \x * \size, 0) {\includegraphics[width=\size]{figures/D12_noisy.png}};
	\node at (2 * \x * \size, 0) {\includegraphics[width=\size]{figures/D12_inpainted.png}};
	\node at (3 * \x * \size + .1cm, 0) {\includegraphics[width=\size]{figures/D34_original.png}};
	\node at (4 * \x * \size + .1cm, 0) {\includegraphics[width=\size]{figures/D34_noisy.png}};
	\node at (5 * \x * \size + .1cm, 0) {\includegraphics[width=\size]{figures/D34_inpainted.png}};
\end{tikzpicture}
 			\vspace{-.6cm}
			\caption{The center portion of a texture (left and center) was reconstructed by sampling from the
			posterior distribution of RIDE (right).}
			\label{fig:inpainting}
		\end{figure}

	\section{Conclusion}
		We have introduced RIDE, a deep but tractable recurrent image model based on spatial LSTMs.
		The model exemplifies how recent insights in deep learning can be exploited for generative
		image modeling and shows superior performance in quantitative comparisons. RIDE is able to capture many
		different statistical patterns, as demonstrated through its application to textures. This is an
		important property considering that on an intermediate level of abstraction natural images
		can be viewed as collections of textures.

		We have furthermore introduced a factorized version of
		the MCGSM which allowed us to use more experts and larger causal neighborhoods. This
		model has few parameters, is easy to train and already on its own performs very
		well as an image model. It is therefore an ideal building block and may be used to extend other models such as DRAW
		\cite{Gregor:2015} or video models \cite{Ranzato:2014,Srivastava:2015}.

		Deep generative image models have come a long way since deep belief
		networks have first been applied to natural images \cite{Osindero:2008}. Unlike convolutional neural networks in object recognition,
		however, no approach has as of yet proven to be a likely solution to the problem of
		generative image modeling. Further conceptual work will be necessary to come up with a model
		which can handle both the more abstract high-level as well as the low-level statistics of natural images.

	\subsubsection*{Acknowledgments}
		The authors would like to thank Aäron van den Oord for insightful discussions and Wieland Brendel,
		Christian Behrens, and Matthias Kümmerer for helpful input on this paper. This study was
		financially supported by the German Research Foundation (DFG; priority program 1527, BE 3848/2-1).

	\bibliographystyle{plainnat}
	\bibliography{references}
\end{document}
