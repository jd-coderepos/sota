
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{subcaption}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{algorithm} 
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage[inkscapelatex=false]{svg}
\usepackage{wrapfig}
\usepackage{changes}
\usepackage{pdfpages}
\usepackage{xcolor,soul,framed} \usepackage{float}
\usepackage{svg}
\usepackage{marvosym}
\usepackage{booktabs}
\usepackage{hhline}
\usepackage{diagbox}

\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}
\newcommand{\zb}[1]{{\color{brown}{#1}}}
\newcommand{\xrq}[1]{{\color{green}{#1}}}
\newcommand{\yjc}[1]{{\color{red}{#1}}}

\definecolor{green}{RGB}{0,150,10}
\definecolor{blue}{RGB}{0,148,181}
\definecolor{orange}{RGB}{194,153,107}

\title{StructChart\raisebox{-0.6em}{\includegraphics[height=1.2cm]{images/SC.png}}: Perception, Structuring, Reasoning for Visual Chart Understanding}



\author{Renqiu Xia$^{1,2}$, Bo Zhang$^{2}$\textsuperscript{\Letter}, Haoyang Peng$^{2}$, Ning Liao$^{1,2}$ \\ 
\bf{Peng Ye}$^{3}$, Botian Shi$^2$, Junchi Yan$^{1,2}$\textsuperscript{\Letter}, Yu Qiao$^2$ \\[2mm]
$^1$ MoE Key Laboratory of Aritificial Intelligience, Shanghai Jiao Tong University, \\ $^2$ Shanghai Artificial Intelligence Laboratory \quad  $^3$ Fudan University \\
{\normalsize \Letter \ Corresponding Authors}, \\
\texttt{\small {\{xiarenqiu, yanjunchi\}@sjtu.edu.cn, zhangbo@pjlab.org.cn}}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle
\thispagestyle{fancy}
\fancyhead{}
\fancyhead[L]{preprint}
\renewcommand{\headrulewidth}{0.7pt}

\begin{abstract}
Charts are common in literature across different scientific fields, conveying rich information easily accessible to readers. Current chart-related tasks focus on either chart perception which refers to extracting information from the visual charts, or performing reasoning given the extracted data, \textit{e.g.} in a tabular form. In this paper, we aim to establish a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works. Specifically, StructChart first reformulates the chart information from the popular tubular form (specifically linearized CSV) to the proposed Structured Triplet Representations (STR), which is more friendly for reducing the task gap between chart perception and reasoning due to the employed structured information extraction for charts. We then propose a Structuring Chart-oriented Representation Metric (SCRM) to quantitatively evaluate the performance for the chart perception task. To enrich the dataset for training, we further explore the possibility of leveraging the Large Language Model (LLM), enhancing the chart diversity in terms of both chart visual style and its statistical information. Extensive experiments are conducted on various chart-related tasks, demonstrating the effectiveness and promising potential for a unified chart perception-reasoning paradigm to push the frontier of chart understanding. Our simulation chart dataset is available at: \textcolor{teal}{\url{https://github.com/UniModal4Reasoning/SimChart9K}}, and the pre-training code will be released for reproduction.
\end{abstract}

\vspace{-8pt}
\section{Introduction}
\label{sec_introd}
\vspace{-8pt}

Charts are common tools for visualizing large amounts of information. Automatically extracting the underlying information from the visual charts has become an emerging research topic in learning~\citep{Masry2022ChartQAAB, nam2023stunt} and vision communities~\citep{Luo2021ChartOCRDE, Obeid2020CharttoTextGN, Rane2021ChartReaderAP}, which can ultimately help better acquiring the data from the existing massive multi-modal corpus beyond raw texts or numbers. 

Visual Chart Understanding (CU) aims to extract the statistical information contained within a given \textbf{visual chart} and perform the corresponding downstream tasks (\textit{e.g.}, chart question answering or chart redrawing) according to the extracted information, which is practical in many fields including medical tabular analysis~\citep{ulmer2020trust}, chart Optical Character Recognition (OCR)~\citep{Luo2021ChartOCRDE, hegselmann2023tabllm, Obeid2020CharttoTextGN, Masry2022ChartQAAB}, and knowledge data extraction for Large Language Models (LLMs)~\citep{Brown2020gpt, chung2022scaling, he2022masked}. Recently, chart-related research works can be categorized into two classes: \textbf{(1) Chart Perception (CP) class} that focuses on recognizing valuable information from a chart, converting the chart from a visual-level image to text-level representation, and \textbf{(2) Chart Reasoning (CR) class} that aims to understand the chart information by a tabular form. Although these works~\citep{Masry2022ChartQAAB,raffel2020exploring,Luo2021ChartOCRDE,chung2022scaling} are inspiring and have achieved promising performance gains on chart perception or chart reasoning task, joint perception-reasoning is still under-explored and challenged by the following aspects.

\textbf{(1) Large perception-reasoning task gap:} Perception task tries to extract as accurate chart information as possible, ignoring the subtle relations between data columns and rows. However, the task of reasoning is often required to consider the complicated data relations to output the right answer or summarize the chart information, especially for chart data that combine both numerical and textual information. \textbf{(2) Incomplete metric evaluation:} There lacks a comprehensive metric to evaluate chart perception performance from the perspective of structured information extraction with data relations. Besides, the current performance metric~\citep{Masry2022ChartQAAB} only covers a single type of chart data such as bars~\citep{Choi2019VisualizingFT}, pie~\citep{Liu2019DataEF}, and line~\citep{Luo2021ChartOCRDE}, which is hard to be generalized to different chart domains when scaling up the number of chart data. \textbf{(3) Expensive chart data:} Acquiring charts from different fields and manually annotating these charts are highly dependent on professionals from different fields, which makes the chart data acquisition and annotation more difficult, labor-intensive, and time-consuming~\citep{ulmer2020trust}.

To tackle the above challenges, we propose StructChart, a novel approach for a unified and label-efficient learning paradigm for joint perception and reasoning tasks. Firstly, to \textbf{alleviate the task gap}, StructChart adopts an image-encoder and text-decoder to facilitate the representation transformation from chart images to text format using Linearized Comma-Separated Values Tokens (LCT). But we argue that LCT ignores the entity relation within the chart, and thus, propose to reformulate the chart from the commonly-used LCT format to a well-designed Structured Triplet Representations (STR) format. Secondly, to \textbf{unify the metric evaluation}, we develop a Structuring Chart-oriented representation Metric (SCRM) based on the proposed STR, which evaluates the chart perception ability from the STR (structured information description), enabling the perception evaluation process for different types of chart data. Finally, to \textbf{expand the chart data}, we propose an LLM-based self-inspection data production scheme that generates more chart data with different domain distributions by statistical data query and drawing code generation leveraging the LLMs. We found that the chart perception and reasoning ability can be enhanced by the LLMs-based simulation method.

Experiments are conducted on both chart perception and reasoning tasks, including chart perception, chart question answering, chart summarization, and chart redrawing. Besides, we produce a synthetic chart dataset termed SimChart9K. We observe that the SimChart9K significantly boosts the chart perception performance, even obtaining a high performance under the few-shot condition. Overall, experimental results verify that the proposed StructChart paradigm is able to achieve a high-performance chart perception and unify the chart understanding.

\textbf{Contribution.} \textbf{(1)} For robust chart perception and reasoning, we propose the so-called Structured Triplet Representations (STR), which replaces the widely-used linearized CSV tokens for chart-related tasks. \textbf{(2)} Based on the proposed STR format, we design a novel Structuring Chart-oriented Representation Metric (SCRM) applicable to various chart-related perception tasks whose evaluation sensitivity can be flexibly tuned by a preset hyper-parameter. \textbf{(3)} We perform data augmentation for chart perception and reasoning by leveraging an LLMs-based self-inspection data production scheme, producing the SimChart9K dataset. Besides, we observe that StructChart continuously improves the chart perception performance as more simulated charts are used for pre-training.



\vspace{-8pt}
\section{Related Works}
\vspace{-8pt}
Our work is focused on Chart Understanding (CU). We discuss works in this emerging area in several aspects, and leave the literature on Vision Language Pre-trained Models (VLPMs) in Appendix~\ref{app:related_work}.  

\textbf{Chart Perception} refers to obtaining the numerical and textual values (often in the tabular) from the charts. Earlier works are based on manually designed feature extraction (\textit{e.g.} color continuous searching and edge extraction). ReVision~\citep{Savva2011ReVisionAC} employs a set of hand-crafted features and rules (color continuous searching) to extract salient marks for chart values inferring. ChartReader~\citep{Rane2021ChartReaderAP} takes a combined approach, using rule or heuristic-based edge extraction supported by OCR for text elements. To avoid brittle hand-crafted features, some works leverage object detection methods and OCR to extract chart value for chart perception task. For Bar Chart, \citep{Choi2019VisualizingFT} adopts the idea of general object detection to detect the bar components by treating each bar as an object. For Pie Chart, \citep{Liu2019DataEF} proposes to use the recurrent network and feature rotation mechanism to extract the data. ChartOCR~\citep{Luo2021ChartOCRDE} employs a modified version of CornerNet~\citep{Law2018CornerNetDO} backbone for keypoint detection to reconstruct the chart components (\textit{e.g.} bars and sectors), and present values in chart components supported by OCR.

\textbf{Chart Reasoning} seeks to leverage chart image information in order to execute logical or mathematical reasoning processes, where Question Answering (QA) is a representative task for showing the chart reasoning ability. 
T5-OCR~\citep{Masry2022ChartQAAB} and TaPas-OCR~\citep{Masry2022ChartQAAB} first employ ChartOCR~\citep{Luo2021ChartOCRDE} to extract data table from a given chart image, and then conduct QA task with questions powered by T5~\citep{Nan2021FeTaQAFT} and TaPas~\citep{Herzig2020TaPasWS} respectively. Some works adopt the two-stage reasoning pipeline, improving the TableQA models~\citep{pasupat2015compositional}. VL-T5-OCR~\citep{Masry2022ChartQAAB} and VisionTaPas-OCR~\citep{Masry2022ChartQAAB} extend cross-modality encoder in T5~\citep{raffel2020exploring} and TaPas~\citep{Herzig2020TaPasWS} to consider chart image features. Besides, Pix2Struct~\citep{Lee2022Pix2StructSP} tries to use the screenshot parsing input to perform the self-supervised pre-training from abundant website data.

\textbf{Chart Understanding} is at a wider level than chart reasoning (at least by the scope of this paper), covering more open-ended and high-level tasks. Besides question answering task, chart understanding contains a wider variety of generative tasks, such as chart summarization, chart redrawing, \textit{etc}. Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV} are the pioneering attempts for chart understanding, with both carrying out the QA and summarization tasks. Matcha~\citep{Liu2022MatChaEV} pre-trains a Pix2Struct~\citep{Lee2022Pix2StructSP} with chart derendering and math reasoning tasks, while Deplot~\citep{Liu2022DePlotOV} harnesses Vision Language Pre-trained Model (VLPM) to extract chart information, and subsequently employs LLMs to conduct inference for the QA and summarization tasks.



\textbf{Evaluation Metrics for Chart Understanding.} In previous works~\citep{Savva2011ReVisionAC,Rane2021ChartReaderAP,Liu2019DataEF,Choi2019VisualizingFT}, researchers usually borrow evaluation metrics from object detection task (IoU) or information retrieval task (EM) to evaluate individual component in chart image, \textit{e.g.} area occupied by each bar, text, and numerical value, \textit{etc}. Such a component-level chart image evaluation method tends to treat the entire chart image (or all components) as a whole, ignoring the entity relations within the chart image.
Besides, for chart perception task, different quantitative metrics are still employed for addressing charts from different domains such as line, pie, bar -- see more details in~\citep{Luo2021ChartOCRDE} for their definitions. 

\textbf{Data Augmentation for Chart Understanding.} 
As an emerging task, there are also rare public tools for chart generation to augment the training examples. Relevant synthetic corpus \citep{Kahou2017FigureQAAA,Methani2019PlotQARO} are limited by template-based simulation drawing code, which can only support very limited types and styles of charts. The synthetic chart dataset in Matcha~\citep{Liu2022MatChaEV} is closed-source with an unknown simulation process. By comparison, the proposed LLM-based self-inspection data production scheme ensures the diversity of both data and styles.


\begin{table}[tb!]
\vspace{-12pt}
\centering
\caption{Comparisons of different research works on chart data, where CP, CR, and CU represent the chart perception, chart reasoning, and chart understanding works, respectively. S. and R. denote the summarization and redrawing downstream task, respectively.}
\vspace{-8pt}
\small
\scalebox{0.72}{
\begin{tabular}{c|l|ccc|c|c|cc|ccc}
\hline
&\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Chart Types} & \multirow{2}{*}{Perception} & Reasoning / & \multicolumn{2}{c|}{Perception}  & \multicolumn{3}{c}{Downstream Tasks}         \\ 
\cline{3-5} \cline{8-12}
&  & Line & Bar & Pie& & Understanding & \multicolumn{1}{c|}{Format} & Metric & QA & S. & R.  \\ \hline
\multirow{5}{*}{\rotatebox[origin=b]{90}{\textbf{CP}}} & ReVision~\citep{Savva2011ReVisionAC}& & \checkmark &\checkmark &\checkmark & \multirow{5}{*}{\fontsize{30.0pt}{\baselineskip}\selectfont $\ \backslash$ }& \multicolumn{1}{c|}{\multirow{5}{*}{JSON}}& & \multicolumn{3}{c}{\multirow{5}{*}{\fontsize{100.0pt}{\baselineskip}\selectfont $\ \backslash$ }}  \\
& ChartReader~\citep{Rane2021ChartReaderAP}  & & \checkmark &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Component}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
& \citep{Liu2019DataEF} & & \checkmark &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{}  &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{-level} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
& \citep{Choi2019VisualizingFT} & & & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}  &\multicolumn{1}{c|}{} &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
\cline{9-9}
& ChartOCR~\citep{Luo2021ChartOCRDE}   & \checkmark & \checkmark  & \multicolumn{1}{c|}{\checkmark}& \multicolumn{1}{c|}{\checkmark}  &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Type-level}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline
\multirow{4}{*}{\rotatebox[origin=b]{90}{\textbf{CR}}}  & T5-OCR~\citep{Masry2022ChartQAAB}  & \checkmark  & \checkmark & \multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\multirow{4}{*}{LCT} }  & \multirow{4}{*}{\fontsize{30.0pt}{\baselineskip}\selectfont $\backslash$} & \checkmark&  & \\
& TaPas-OCR~\citep{Masry2022ChartQAAB} & \checkmark & \checkmark   & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}  &\multicolumn{1}{c|}{\checkmark}  &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}  & \checkmark  &  &   \\
& VL-T5-OCR~\citep{Masry2022ChartQAAB}   & \checkmark  & \checkmark  & \multicolumn{1}{c|}{\checkmark}& \multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{} & \checkmark & & \\
& VisionTaPas-OCR~\citep{Masry2022ChartQAAB} & \checkmark & \checkmark & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{\checkmark} &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}  & \checkmark & &  \\ \hline
\multirow{3}{*}{\rotatebox[origin=b]{90}{\textbf{CU}}} & Matcha~\citep{Liu2022MatChaEV}  & \checkmark  & \checkmark & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\textbf{--}}  & \multicolumn{1}{c|}{\checkmark}   &\multicolumn{1}{c|}{\textbf{--}}   & \multicolumn{1}{c|}{\textbf{--}}  & \checkmark  & \checkmark  & \\
& Deplot~\citep{Liu2022DePlotOV} & \checkmark   & \checkmark & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}&\multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{LCT}  &\multicolumn{1}{c|}{\textbf{--}}   & \checkmark & \checkmark  & \\
\cline{2-12} 
& StructChart (Ours)  & \checkmark  & \checkmark  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}&\multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{STR}  & \multicolumn{1}{c|}{SCRM} & \checkmark  & \checkmark & \checkmark \\ \hline
\end{tabular}
}
\label{tab:rw_compare}
\vspace{-14pt}
\end{table}


\vspace{-2pt}
Overall, Table~\ref{tab:rw_compare} summarizes the differences between our StructChart and other chart-related works.


\vspace{-8pt}
\section{The Proposed Method}
\vspace{-8pt}
StructChart includes four key components: \textbf{\textit{(1) Transformer-based Chart-oriented Information Extractor (CIE)}}. It incorporates an image-encoder and text-decoder to facilitate the transformation from chart images to text format using Comma-Separated Values (CSV). \textbf{\textit{(2) Structured Representation Transformation.}} The extracted intermediate CSV text is structured into a triplet form to elucidate the intricate position relationship between the header and index. \textbf{\textit{(3) Structuring Chart-oriented Representation Metric
(SCRM).}} We further design a metric that comprehensively evaluates the quality of the transformed triplets, which facilitates the subsequent reasoning. \textbf{\textit{(4) LLM-based Self-inspection Data Production Scheme.}} We devise a novel methodology for chart data simulation to enhance zero-shot and few-shot perception and reasoning ability, achieving continuous performance gains when scaling up the simulated charts. The whole paradigm is illustrated in Fig.~\ref{fig:framework}.


\begin{figure}[tb!]
\vspace{-8pt}
\centering
\includegraphics[width=0.85\linewidth]{images/StructChart_framework.pdf}
\vspace{-6pt}
\caption{StructChart overview: (1) LLM-based production scheme for providing more chart data; (2) CIE training for chart perception; (3) Representation transformation for bridging the task gap; (4) Downstream reasoning tasks, including Question Answering, Summarization and Redrawing.}
\label{fig:framework}
\vspace{-12pt}
\end{figure}

\vspace{-8pt}
\subsection{Design for Two-stage StructChart}
\vspace{-8pt}
Different from end-to-end multi-modal reasoning tasks~\citep{Masry2022ChartQAAB,Obeid2020CharttoTextGN}, we fulfill CU by solving two independent tasks: perception and reasoning, with the Structured Triplet Representations (STR) data representation serving as a bridge between them.


\noindent\textbf{Perception Stage.}
We propose a CIE that utilizes a pixel-level encoder and text-level decoder, both based on the Vision Transformer (ViT)~\citep{Dosovitskiy2020AnII}. Instead of standard ViT, which scales the input images to a predefined resolution, we propose to always scale the input image to a fixed number of patches that can fit within the longest given sequence length, according to the original resolution of the input image. Moreover, we add 2-dimensional absolute positional embeddings for the input patches, allowing the perception module to handle variable resolutions. At this stage, the chart at pixel level can be converted to text-level Linearized CSV Tokens (LCT) through supervised training.

\noindent\textbf{Reasoning Stage.}
\label{sec:structchart model}
Before performing the reasoning process, we structure the LCT into the designed STR to facilitate the module's understanding of chart-oriented information (see Sec.~\ref{subbsection:reformulation} for details). This structuring process enables us to reason over the text, providing a better understanding of the entity relation within a chart. Considering the difficulty of evaluating downstream tasks, the reasoning process is performed on the QA task using GPT-3.5~\citep{Brown2020gpt} with few-shot prompting inference. Additionally, in order to verify the effectiveness of the proposed STR and compare with previous research works~\citep{Luo2021ChartOCRDE,Masry2022ChartQAAB,Liu2022DePlotOV,Liu2022MatChaEV}, Chain-of-Thoughts (CoT)~\citep{Wei2022ChainOT} is employed to perform reasoning process using both LCT and STR (as demonstrated in Sec.~\ref{sec:exp_qa}, the STR representations achieve better performance).


\vspace{-8pt}
\subsection{Structured Triplet Representations (STR) for Chart Understanding}
\vspace{-8pt}
\label{subbsection:reformulation}
Visual charts often contain rich textual and numerical information. Generally, the chart information is represented by long-form texts in the form of CSV, \textit{i.e.} previously-mentioned LCT. However, the LCT format is sensitive to positional variations of entities from charts due to that it is used in a linear form. 
Thus, we propose to reformulate the LCT format in order to effectively and robustly represent the positional relations between row and column headers of a given chart.


\noindent\textbf{Task Definition and Structuring.} 
Given a chart image, the extracted LCT can be described as:
\begin{equation}
\small
\begin{aligned}
C_{csv} := & \ none, \ \ \ \ \ Entity_{c_1},Entity_{c_2},...,Entity_{c_m},...Entity_{c_M} \ /n\\
&Entity_{r_1},Value_{r_1}^{c_1},Value_{r_1}^{c_2},...,Value_{r_1}^{c_m},...,Value_{r_1}^{c_M} \ /n\\
&......\\
&Entity_{r_n},Value_{r_n}^{c_1},Value_{r_n}^{c_2},...,Value_{r_n}^{c_m},...,Value_{r_n}^{c_M} \ /n\\
&......\\
&Entity_{r_N},Value_{r_N}^{c_1},Value_{r_N}^{c_2},...,Value_{r_N}^{c_m},...,Value_{r_N}^{c_M} \ /n \ ,
\end{aligned}
\label{equ:r_csv}
\end{equation}
where {\small $/n$} refers to line break, and {\small $Entity_{r_n}$} and {\small $Entity_{c_m}$} indicate {\small $n$-th} row header entity and {\small $m$-th} column header entity, respectively, where {\small ($M,N\in \mathbb{{N}^+}$)}. {\small $Value_{r_n}^{c_m}$} in Eq.~\ref{equ:r_csv} contains the  positional information of {\small $Entity_{c_m}$} and {\small $Entity_{r_n}$}. However, the LCT still faces two issues: \textit{(1) The evaluation process for the predicted long-form texts containing positional information for perception model selection is non-trivial.} \textit{(2) Highly position-sensitive LCT format increases the inference difficulty of different downstream chart tasks.}



Considering that chart information has matrix-like row-column transformation invariance and transpose transformation invariance, the LCT is structured into a well-designed triplet, with higher granularity for evaluation and downstream. Given LCT tokens {\small $C_{csv}$} shown in Eq.~\ref{equ:r_csv}, the structured triplet representations can be obtained as follows:
\begin{equation}
\small
\begin{aligned}
C_{tri} := &(Entity_{r_1}, Entity_{c_1},Value_{r_1}^{c_1}), \\
&(Entity_{r_1}, Entity_{c_2},Value_{r_1}^{c_2}), \\
&........, \\
&(Entity_{r_n}, Entity_{c_m},Value_{r_n}^{c_m}),\\
&........, \\
&(Entity_{r_N}, Entity_{c_M},Value_{r_N}^{c_M}). \\
\end{aligned}
\label{equ:r_tri}
\end{equation}

\noindent\textbf{Evaluation Metric Design.}
\label{sec:metric}
Furthermore, we design a \textbf{Structuring Chart-oriented Representation Metric (SCRM)} to comprehensively evaluate the extracted chart information represented using the proposed STR. When comparing the predicted STR and Ground Truth (GT) STR $C_{tri}$, we treat {\small $Entity_{r_n}, Entity_{c_m}$} as strings and {\small$ Value_{r_n}^{c_m}$} as floats, respectively. 

\textbf{1) Image-wise}. Suppose that there are totally {\small $\mathbf{P}$} triplets from the model prediction and {\small $\mathbf{Q}$} triplets from GT ({\small $\mathbf{P}, \mathbf{Q}\in \mathbb{{N}^+}$}), the evaluation process is shown as follows:

\begin{enumerate}[-]
\item For {\small $Entity$}, we obtain the edit distance of the $p$-th prediction string and the $q$-th GT string: 
\begin{equation}
\small
J(p,q)=\frac{\mid Entity _{pred}^p \cup Entity_{GT}^q|-| Entity_{pred}^p \cap Entity_{GT}^q \mid}{\mid Entity_{pred}^p \cup Entity_{GT}^q \mid} \ .
\end{equation}

\item For {\small $Value$}, we calculate the relative error between the $p$-th prediction value and the $q$-th GT value: 
\begin{equation}
\small
e(p,q)=\left|\frac{Value_{pred}^{p}-Value_{GT}^{q}}{Value_{GT}^q}\right| \ .
\end{equation}

\item To achieve a comprehensive evaluation, we design three levels of tolerance for fine-grained judgment, aiming to measure the similarity between the predicted triplets and GT triplets, by calculating the Intersection over Union {\small $IoU|_{tol}$}, under the given tolerance level {\small $tol$} as follows:
\begin{equation}
\small
l(p,q)|_{tol}=\left\{\begin{matrix}
 1, & if:J(p,q) \le J_{thr}|_{tol} \ \ \wedge \ \ e(p,q)\le e_{thr}|_{tol} \\
 0, & else
\end{matrix}\right.,
\end{equation}
\begin{equation}
\small
\begin{aligned}
 tol:=&\{ strict, slight, high\}, \quad
strict:=\left\{J_{thr}|_{tol}=0   \wedge   e_{thr}|_{tol}=0\right\}, \\
  slight:=& \left\{J_{thr}|_{tol}=2   \wedge  e_{thr}|_{tol}=0.05\right\}, \quad
  high:=\left\{J_{thr}|_{tol}=5   \wedge  e_{thr}|_{tol}=0.1\right\},
\end{aligned}
\end{equation}
\begin{equation}
\small
IoU|_{tol}=\frac{\sum_{q=1}^{\mathbf{Q}}\sum_{p=1}^{\mathbf{P}}l(p,q)|_{tol}}{\mathbf{P}+\mathbf{Q}-\sum_{q=1}^{\mathbf{Q}}\sum_{p=1}^{\mathbf{P}}l(p,q)|_{tol}}.
\end{equation}
\end{enumerate}

\textbf{2) Dataset-wise}. Given the dataset with {\small $\mathbf{L}$} chart images ({\small $\mathbf{L}\in \mathbb{{N}^+}$}), the Intersection over Union of the {\small $i$-th} image can be denoted as {\small $IoU(i)$}. Besides, given a preset similarity threshold {\small $IoU_{thr}$}, the corresponding discriminant function towards the positive and negative images can be written as:
\begin{equation}
\small
d(i)|_{IoU_{thr},tol}=\left\{\begin{matrix}
  1,& if: IoU(i)|_{tol} \ge IoU_{thr} \\
  0,& else
\end{matrix}\right..
\end{equation}

When the preset similarity threshold {\small $IoU_{thr}$} becomes a variable (denoted as $t$), it changes to:
\begin{equation}
\small
d(i, t)|_{tol}=\left\{\begin{matrix}
  1,& if: IoU(i)|_{tol} \ge t \\
  0,& else
\end{matrix}\right..
\end{equation}

The proposed metric SCRM consists of two indicators ($Precision$ with a fixed similarity threshold and $mPrecision$ with a varying one in the range {\small $(0.5:0.05:0.95)$}):
\begin{equation}
Precision|_{IoU_{thr},tol}=\frac{\sum_{i=1}^{L}d(i)|_{IoU_{thr},tol}}{L},
\quad 
\label{eqa:mprecison}
mPrecision|_{tol}=\frac{\sum_{t=10}^{19}\sum_{i=1}^{L}d(i,0.05t)|_{tol}}{10L} .
\end{equation}


\vspace{-8pt}
\subsection{Simulating Charts with Enhanced Diversity for Pretraining-Finetuning}
\vspace{-8pt}
\label{sec:simulate}
Considering the difficulty and cost of chart data acquisition and labeling, we introduce an LLM-based text-to-chart level data production scheme, dividing into: \textit{(1) statistical data query to ensure the data-level diversity}, and \textit{(2) drawing code generation to ensure the drawing style diversity.} The complete schematic simulation paradigm is shown in Appendix~\ref{app:simulation}.

\noindent\textbf{Statistical Data Query.} 
Given the chart dataset {\small $\mathbb{D}_{ori}=\{\mathbf{I}_{ori},\mathbf{T}_{ori}\}$ }, where {\small $\mathbf{I}_{ori}=\{I_{ori}^1,I_{ori}^2,I_{ori}^3,...,I_{ori}^n,...,I_{ori}^N\}$ } are the images and {\small $\mathbf{T}_{ori}=\{T_{ori}^1,T_{ori}^2,T_{ori}^3,...,T_{ori}^n,...,T_{ori}^N\}$ } are the corresponding labeled texts in CSV format. At this stage, we define the task as the imitation of labeled CSV texts {\small $\mathbf{T}_{ori}$ } in origin chart dataset $\mathbb{D}_{ori}$ through an LLM (specifically GPT-3.5) \underline{\textbf{to generate simulated CSV labels}} {\small $\mathbf{T}_{sim}$}. In detail, We employ the few-shot prompting, leveraging generative model~\citep{Brown2020gpt} to complete the imitation task. For diversity and effectiveness of the imitation data, we impose three restrictions on the demonstration and instruction: \textit{(1) The simulated content {\small $T_{sim}^n$ } must be in CSV format}, \textit{(2) The scale of {\small $T_{sim}^n$ } can be altered compared with {\small $T_{ori}^n$ }, including the number of rows}, and \textit{(3) The combination of text in {\small $T_{sim}^n$ } must be reasonable, even though it may be highly irrelevant to {\small $T_{ori}^n$.}}

\noindent\textbf{Drawing Code Generation.}
Having obtained the simulated texts (in CSV format) {\small $\mathbf{T}_{sim}=\{T_{sim}^1,T_{sim}^2,T_{sim}^3,...,T_{sim}^n,...,T_{sim}^N\}$}, the next step is to create images {\small$\mathbf{I}_{sim}= \{I_{sim}^1,I_{sim}^2,I_{sim}^3,...,I_{sim}^n,...,I_{sim}^N\}$} based on {\small $\mathbf{T}_{sim}$}. We still employ the advanced GPT-3.5 to directly generate the drawing code, and draw the simulated chart {\small $I_{sim}^n$ } based on the aforementioned statistical data {\small $T_{sim}^n$}, by means of our developed instruction prompting. To guarantee the diversity of chart distribution at the image level, we implement the following limitations within the instruction prompting: \textit{(1) Random selection of a chart type that is appropriate for {\small $T_{sim}^n$}}, {\textit{comprising histograms, scatterplots, line charts, and pie charts}, \textit{(2) Random choice of drawing style}, \textit{including fonts, colors, line styles, backgrounds, \textit{etc}}, and \textit{(3) Transformation of scale with different coordinate axes.}

To guarantee that the generated drawing code can be executed correctly, we design a \textbf{self-inspection mechanism} to iteratively skip the non-executable code generated by GPT-3.5, until all the drawing code that matches the corresponding text can be executable. Benefiting from the diversities of the texts generated by GPT-3.5, we can generate more diverse Statistical Data Queries and Drawing Codes, according to the given {\small $T_{ori}^n$}. As a result, the proposed LLM-based text-to-chart level data production scheme can be used to simulate \textbf{scalable}, \textbf{data-rich} and \textbf{style-diverse} chart dataset {\small $\mathbb{D}_{sim}=\{\mathbf{I}_{sim},\mathbf{T}_{sim}\}$ } based on scale-invariant or few-shot original chart datasets {\small $\mathbb{D}_{ori}=\{\mathbf{I}_{ori},\mathbf{T}_{ori}\}$ }


\vspace{-8pt}
\section{Experiments}
\vspace{-8pt}
\label{sec:experiment}

\subsection{Evaluation Datasets and Implementation Details}
\label{subsec:datasets}
\vspace{-8pt}

\noindent\textbf{Datasets.} 
We evaluate our StructChart on three real-world chart benchmarks and our simulated dataset with image-CSV pairs. \textbf{ChartQA}~\citep{Masry2022ChartQAAB} is a large-scale visual reasoning dataset with 20,882 charts collected from online sources, which can be divided into an augmented set generated synthetically and a human set written by humans. \textbf{PlotQA}~\citep{Methani2019PlotQARO} is a synthetic dataset that covers a wide range of topics from real-world sources, consisting of 28.9 million question-answer pairs spread across 224,377 plots, with the questions based on crowd-sourced question templates. \textbf{Chart2Text}~\citep{Obeid2020CharttoTextGN} is a dataset for automatic summarization of statistical charts crawled from \texttt{statista.com}, yielding total 8,305 charts with underlying data table and associated summaries. \textbf{SimChart9K} is the proposed simulated dataset composed of 9,098 charts and associated data annotations in CSV format. We have previously elucidated the simulation methodology in Sec.~\ref{sec:simulate}, where it was generated via ChartQA using a generative model.

\noindent\textbf{Implementation and Evaluation Metrics.} 
We design various data consolidation settings based on both real and simulated data, when training StructChart using the architecture mentioned in Sec.~\ref{sec:structchart model}. \textbf{For perception}, we employ SCRM (Sec.~\ref{sec:metric}) for perception model selection, and obtain STR of input chart from the well-trained StructChart model. \textbf{For reasoning}, we use GPT-3.5 to perform different downstream tasks based on the proposed STR. Please refer to Appendix~\ref{app:different_task} for more results of chart summarization and redrawing tasks.

\vspace{-8pt}
\subsection{Chart Perception Results on Real-world and Simulation Data}
\vspace{-8pt}
We first conduct the experiments on real-world datasets including ChartQA, PlotQA, and Chart2Text, and further merge them for joint-dataset training, to better evaluate the performance scalability given more training data. From Table~\ref{tab:real}, StructChart continuously improves the perception performance of chart data on each domain given more training samples. Moreover, Fig.~\ref{fig:tsne_real} visualizes the feature distributions via t-SNE~\citep{van2008visualizing}, where features from different datasets are basically consistent.


\begin{figure}[tb!]
\vspace{-12pt}
\centering
\begin{subfigure}{0.42\textwidth}
    \includegraphics[width=\textwidth]{images/t-SNE_a.pdf}
    \caption{Real Datasets.}
    \label{fig:tsne_real}
\end{subfigure}
\begin{subfigure}{0.42\textwidth}
    \includegraphics[width=\textwidth]{images/t-SNE_b.pdf}
    \caption{Real \& Simulated Datasets.}
    \label{fig:tsne_simulated}
\end{subfigure}
\vspace{-8pt}
\caption{Feature distributions of ChartQA, PlotQA, Chart2Text, and SimChart9K visualized by t-Distributed Stochastic Neighbor Embedding (t-SNE)~\citep{van2008visualizing}.}
\label{fig:tsne}
\vspace{-10pt}
\end{figure}


\begin{table*}[!tb]
\centering
\caption{Results on the validation set of ChartQA, PlotQA, and Chart2Text in two settings: 1) single-set that the model is trained and evaluated on the same dataset; and 2) merging-set that training samples are merged from the real datasets and the trained model is evaluated on the validation set. \textbf{Note that} Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV} employ additional Closed-source Data (C.D.) for training, while our StructChart trains on the public dataset and the proposed SimChart9K data using the proposed simulation method.}
\scalebox{0.7}{
\renewcommand{\arraystretch}{1}
\begin{tabular}{ccccrcccccccccc}
\toprule

&\multirow{2}*{Val Set} &\multirow{2}*{Model}  & \multirow{2}*{Train Set} 
& $IoU_{thr}$$\rightarrow$ &  &\multicolumn{2}{c} {mPrecision} & & \multicolumn{4}{c} {Precision} & \\ \cmidrule{7-8} \cmidrule{10-14}
& & & &Tolerance $\downarrow$ & & \multicolumn{2}{c} {0.5:0.05:0.95} &  &0.5& 0.75 & 0.95 & 1 (EM) \\
\midrule

& \multirow{12}{*}{\rotatebox[origin=r]{90}{\multirow{2}*{ChartQA}}}
&\multirow{3}*{Matcha~\citep{Liu2022MatChaEV}} & ChartQA+ & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.5160} & &0.5814 &0.5114 &0.4678 &0.4460 &\\
& & & PlotQA+ &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.6598} & &0.7045&0.6572 &0.6250 &- &\\
& & & C.D. &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7161} & &0.7519 &0.7150 &0.6894 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Deplot~\citep{Liu2022DePlotOV}} & ChartQA+ & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.6331} & &0.7008 &0.6326 &0.5814 &0.5663 &\\
& & & PlotQA+ &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7666} & &0.8229&0.7661 &0.7282 &- &\\
& & & C.D. &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8150} & &0.8759 &0.8087 &0.7812 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Our StructChart} &\multirow{3}*{ChartQA} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.6770} & &0.7273 &0.6714 &0.6458 &0.6326 &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7792} & &0.8220&0.7746 &0.7519 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8274} & &0.8703 &0.8210 &0.8011 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Our StructChart} &\multirow{3}*{Merging} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.7017} &&0.7547 &0.6998 &0.6610 &\textbf{0.6506} &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.8227} &&0.8674 &0.8201 &0.7926 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{\textbf{0.8591}} &&\textbf{0.8987} &\textbf{0.8551} &\textbf{0.8362} &- &\\

\midrule

& \multirow{12}{*}{\rotatebox[origin=b]{90}{PlotQA}}
&\multirow{3}*{Matcha~\citep{Liu2022MatChaEV}} & ChartQA+  & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.0048} & &0.0089 &0.0048 &0.0036 &0.0036 &\\
& & & PlotQA+ &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.0752} & &0.0909&0.0754 &0.0635 &- &\\
& & & C.D. &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.0823} & &0.1093 &0.0837 &0.0719 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Deplot~\citep{Liu2022DePlotOV}} & ChartQA+  & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.0997} & &0.1532 &0.1021 &0.0641 &0.0629 &\\
& & & PlotQA+ &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.6969} & &0.8664&0.7435 &0.5463 &- &\\
& & & C.D. &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7471} & &0.9679 &0.8034 &0.5992 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Our StructChart} &\multirow{3}*{PlotQA} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.1995} &&0.2500 &0.1931 &0.1765 &0.1736 &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7848} &&0.8519 &0.7784 &0.7405 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8271} &&0.8922 &0.8223 &0.7861 &- &\\  \cmidrule{3-14}

& &\multirow{3}*{Our StructChart} &\multirow{3}*{Merging} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.4549} &&0.5855 &0.4525 &0.3521 &\textbf{0.3385} &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.8589} &&0.9210 &0.8569 &0.8118 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{\textbf{0.8921}} &&\textbf{0.9466} &\textbf{0.8860} &\textbf{0.8557} &- &\\

\midrule

& \multirow{6}{*}{\rotatebox[origin=b]{90}{Chart2Text}}
&\multirow{3}*{Our StructChart} &\multirow{3}*{Chart2Text} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.1936} &&0.2473 &0.1892 &0.1533 &0.1442 &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.5524} &&0.6603 &0.5529 &0.4672 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.6945} &&0.7676 &0.6934 &0.6356 &- &\\ \cmidrule{3-14}

& &\multirow{3}*{Our StructChart} &\multirow{3}*{Merging} & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.3156} &&0.4002 &0.3123 &0.2509 &\textbf{0.2318} &\\
& & & &slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7141} &&0.7938 &0.7205 &0.6426 &- &\\
& & & &high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{\textbf{0.8085}} &&\textbf{0.8595} &\textbf{0.8090} &\textbf{0.7673} &- &\\
\bottomrule 
\end{tabular}
}
\label{tab:real}
\end{table*}


We randomly divide the proposed SimChart9K dataset into four subsets with different amounts (0.1K, 1K, 6K, 9K), and train the StructChart on the mixed dataset (including ChartQA training set and SimChart with different amounts). All evaluations are conducted on the ChartQA validation set based on the proposed SCRM metrics. In Table~\ref{tab:sim}, it can be seen that the introduction of simulation dataset significantly improves the CIE performance of StructChart; that is, \textbf{the larger the simulation dataset, the greater the performance gains in CIE}. Furthermore, we also illustrate feature distributions of SimChart9K in Fig.~\ref{fig:tsne_simulated}. It can be observed from Fig.~\ref{fig:tsne_simulated} that the feature distribution of the simulated data (SimChart9K) can be better matched with that of the real data such as ChartQA, PlotQA, and Chart2Text. As a result, the simulated charts are beneficial to boost the CIE performance of the model towards the real-world charts.

\begin{table}[!tb]
\vspace{-8pt}
\centering
\caption{ChartQA perception results by scaling up the simulation data (from 0.1K to 9K).}
\vspace{-6pt}
\scalebox{0.75}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{clrccccccccccc}
\toprule

\qquad &\multirow{2}*{Train Set} & $IoU_{thr}$ $\rightarrow$ &  &\multicolumn{2}{c} {mPrecision} & & \multicolumn{4}{c} {Precision} & \\ \cmidrule{5-6} \cmidrule{8-12}
& & Tolerance $\downarrow$ & & \multicolumn{2}{c} {0.5:0.05:0.95} &  &0.5& 0.75 & 0.95 & 1 (EM) \\
\midrule

& \multirow{3}*{ChartQA (w/o simulation data)}  
& strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.6770} & &0.7273 &0.6714 &0.6458 &0.6326 &\\
& & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7792} & &0.8220&0.7746 &0.7519 &- &\\
& & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8274} & &0.8703 &0.8210 &0.8011 &- &\\

\midrule 
\multirow{14}{*}{\rotatebox[origin=b]{90}{ Real \& Simulated Source}}
 & \multirow{3}*{ChartQA+SimChart 0.1K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.6804} & &0.7282 &0.6752 &0.6468 &0.6383 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7893} & &0.8305 &0.7850 &0.7595 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8326} & &0.8731 &0.8277 &0.8087 &-&\\ \cmidrule{2-12}

 & \multirow{3}*{ChartQA+SimChart 1K}
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.6871} &&0.7367 &0.6828 &0.6544 &0.6458 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7938} &&0.8371 &0.7907 &0.7661 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.8394} &&0.8788 &0.8362 &0.8116 &- &\\ \cmidrule{2-12}



 & \multirow{3}*{ChartQA+SimChart 6K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7040} &&0.7491  &0.7036 &0.6686 &0.6591 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8128} &&0.8580  &0.8116 &0.7794 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8450 } &&0.8835  &0.8428 &0.8182 &- &\\ \cmidrule{2-12}

 & \multirow{3}*{ChartQA+SimChart 9K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7116} &&0.7595 &0.7074 &0.6809 &\textbf{0.6686} &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8182} &&0.8674 &0.8144 &0.7850 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{\textbf{0.8527}} &&\textbf{0.8958} &\textbf{0.8532} &\textbf{0.8220} &- &\\
\bottomrule 
\end{tabular}
}
\label{tab:sim}
\end{table}

\vspace{-8pt}
\subsection{Achieving 100\% Performance by Only 20\% Real Data.}
\vspace{-8pt}
The purpose of this part is to answer two questions: 1) Can we achieve a high-performance CIE only leveraging few-shot real samples? 2) With the help of SimChart9K, how many real-world samples can obtain the CIE performance that is achieved on the full training set? To answer these questions, we split the real chart dataset ChartQA into subsets with different sizes, including subsets with 1\%, 10\%, 20\% and 50\% original real-world samples. We demonstrate zero-shot and few-shot results in Table~\ref{tab:few-shot}, obtaining the following observations: (1) When the model is trained on the real dataset ChartQA alone without any simulated samples, the CIE performance is still positively correlated with the number of real-world training samples. (2) Training only on the simulated dataset without any real samples (zero-shot training) fails to achieve a satisfactory CIE performance, due to the insufficiency in real-world charts. (3) By leveraging the proposed method to generate many simulated charts (SimChart9K), only 20\% real-world charts can basically achieve equal CIE performance under the 100\% real-world training samples. Besides, it can be observed from Table~\ref{tab:few-shot} that the CIE performance obtained using 50\% real-world charts significantly outperforms that obtained using the full-set training examples. We further illustrate the above observations in Fig.~\ref{fig:heat}.




\begin{figure}[tb!]
\vspace{-3pt}
\begin{minipage}{0.42\linewidth}
\centering
\includegraphics[width=\textwidth]{images/Fig3.pdf}
\vspace{-18pt}
\caption{mPrecision by various amounts of charts in ChartQA \& SimChart9K.}
\vspace{-10pt}
\label{fig:heat}
\end{minipage}
\hfill
\begin{minipage}{0.58\linewidth}
\captionof{table}{QA results on ChartQA, where LCT and our STR are two different text representation formats. Exact Match (EM) metric is employed for evaluation.}
\vspace{-4pt}
\centering
\scalebox{0.56}{
\begin{tabular}{llcccccccccccc}
\toprule
&\multirow{2}*{Model} &\multirow{2}*{Train Set \ }
& \multicolumn{3}{c}{ChartQA val} \\  \cmidrule{4-6}    
& & & aug. \textcolor{blue!30}{\rule{0.7em}{0.55em}} & human \textcolor{red!30}{\rule{0.7em}{0.55em}} & avg. & \\

\midrule
  \multirow{5}{*}{\rotatebox[origin=c]{90}{ \textbf{Baseline} }} 
& VL-T5-OCR~\citep{Masry2022ChartQAAB}  &ChartQA & - & - & 41.6 & \\
& Tapas-OCR~\citep{Masry2022ChartQAAB} &ChartQA& - & - & 45.5 &\\ 
& PaLI-17B~\citep{Chen2022PaLIAJ} &ChartQA& 64.9 & 30.4 & 47.6 &\\ 
& Pix2Struct~\citep{Lee2022Pix2StructSP} &ChartQA & 81.6 & 30.5 & 56.0 & \\ 
& MatCha~\citep{Liu2022MatChaEV} & ChartQA/PlotQA/C.D. & 90.2  & 38.2 & 64.2 & \\
\midrule 

 \multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{LCT}}}
 &\multirow{4}*{StructChart+GPT3.5}
 &ChartQA &64.2 &37.1 &50.7 &    \\
& &Merging &69.9 &39.1 &54.5 &    \\
& &ChartQA0.2+SimChart9K &62.6 &36.0 &49.3 &    \\
& &ChartQA+SimChart9K &71.3 &41.2 &56.3 &    \\

\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{STR}}}
&\multirow{4}*{StructChart+GPT3.5}
&ChartQA &78.5 &42.8 &60.7 &    \\
& &Merging &80.2 &44.3 &62.2 &    \\
& &ChartQA0.2+SimChart9K &76.3 &40.7 &58.5 &    \\
& &ChartQA+SimChart9K &\textbf{83.9} &\textbf{46.7} & \textbf{65.3}&    \\
\bottomrule 
\end{tabular}
}
\label{tab:qa}
\end{minipage}
\end{figure}


\begin{table*}[!tb]
\centering
\caption{Zero-shot and few-shot on real-world chart dataset by means of the proposed SimChart9K.}
\vspace{-8pt}
\scalebox{0.7}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{clrccccccccccc}
\toprule
\qquad \qquad \qquad \qquad &\multirow{2}*{Train Set} & $IoU_{thr}$ $\rightarrow$ &  &\multicolumn{2}{c} {mPrecision} & & \multicolumn{4}{c} {Precision} & \\ \cmidrule{5-6} \cmidrule{8-12}
& & Tolerance $\downarrow$ & & \multicolumn{2}{c} {0.5:0.05:0.95} &  &0.5& 0.75 & 0.95 & 1 (EM) \\
\midrule
 \multirow{14}{*}{\rotatebox[origin=b]{90}{No Simulated Source}}
 & \multirow{3}*{ChartQA0.01}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.1797} &&0.2121 &0.1771 &0.1591 &0.1534 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.2384} &&0.2850 &0.2339 &0.2178 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.2686} &&0.3277 &0.2633 &0.2424 &-&\\ \cmidrule{2-12}
 & \multirow{3}*{ChartQA0.1}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.3616} &&0.4015 &0.3598 &0.3277 &0.3210 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.4242} &&0.4612 &0.4195 &0.4025 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.4653} &&0.5038 &0.4631 &0.4422 &-&\\ \cmidrule{2-12}
  & \multirow{3}*{ChartQA0.5}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.5389} &&0.5786 &0.5350 &0.5085 &0.4981 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.6150} &&0.6525 &0.6089 &0.5890 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.6603} &&0.6951 &0.6553 &0.6392 &-&\\  \cmidrule{2-12}
& \multirow{3}*{ChartQA}  
& strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & &\multicolumn{2}{c}{0.6770} & &0.7273 &0.6714 &0.6458 &0.6326 &\\
& & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7792} & &0.8220&0.7746 &0.7519 &- &\\
& & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.8274} & &0.8703 &0.8210 &0.8011 &- &\\
\midrule 
\multirow{2}{*}{\rotatebox[origin=b]{90}{Zero-shot}}
 & \multirow{3}*{SimChart9K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.0688} &&0.0890 &0.0691 &0.0483 &0.0483 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.1577} &&0.1979 &0.1562 &0.1288 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.2527} &&0.3021 &0.2491 &0.2235 &-&\\ 
\midrule 
\multirow{12}{*}{\rotatebox[origin=b]{90}{Few-shot}}
 & \multirow{3}*{ChartQA0.01+SimChart9K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.3074} &&0.3797 &0.3078 &0.2500 &0.2348 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.4672} &&0.5227 &0.4706 &0.4138 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.5402} &&0.5909 &0.5398 &0.4981 &-&\\ \cmidrule{2-12}
 & \multirow{3}*{ChartQA0.1+SimChart9K}  
 & strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.5973} &&0.6619 &0.6013 &0.5483 &0.5294 &\\
 & & slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} & & \multicolumn{2}{c}{0.7466} &&0.7936 &0.7500 &0.7112 &- &\\
 & & high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} & &  \multicolumn{2}{c}{0.7980} &&0.8419 &0.8002 &0.7661 &-&\\ \cmidrule{2-12}
 &\cellcolor{gray!10}&\cellcolor{gray!10} strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} &  \cellcolor{gray!10} &  \multicolumn{2}{c}{ \cellcolor{gray!10} 0.6465} & \cellcolor{gray!10}& \cellcolor{gray!10}0.7055 & \cellcolor{gray!10}0.6468 & \cellcolor{gray!10}0.5956 & \cellcolor{gray!10}0.5814 &\cellcolor{gray!10}\\
 &\cellcolor{gray!10} & \cellcolor{gray!10} slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} &\cellcolor{gray!10} & \multicolumn{2}{c}{\cellcolor{gray!10}0.7787} &\cellcolor{gray!10}&\cellcolor{gray!10}0.8229 &\cellcolor{gray!10}0.7775 &\cellcolor{gray!10}0.7443 &\cellcolor{gray!10}-&\cellcolor{gray!10} \\
 &\multirow{-3}*{\cellcolor{gray!10}ChartQA0.2+SimChart9K}& \cellcolor{gray!10} high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} &\cellcolor{gray!10} &  \multicolumn{2}{c}{\cellcolor{gray!10}0.8206} &\cellcolor{gray!10}&\cellcolor{gray!10}0.8646 &\cellcolor{gray!10}0.8201 &\cellcolor{gray!10}0.7888 &\cellcolor{gray!10}-&\cellcolor{gray!10}\\ \cmidrule{2-12}
 &\cellcolor{gray!10} & \cellcolor{gray!10} strict \ \textcolor{red!60}{\rule{0.5em}{0.5em}} &\cellcolor{gray!10} & \multicolumn{2}{c}{\cellcolor{gray!10} 0.6902} &\cellcolor{gray!10}&\cellcolor{gray!10}0.7434&\cellcolor{gray!10}0.6913 &\cellcolor{gray!10}0.6487 &\cellcolor{gray!10}0.6420 &\cellcolor{gray!10}\\
 &\cellcolor{gray!10} & \cellcolor{gray!10} slight\ \textcolor{yellow!60}{\rule{0.5em}{0.5em}} &\cellcolor{gray!10} & \multicolumn{2}{c}{\cellcolor{gray!10}0.8015} &\cellcolor{gray!10}&\cellcolor{gray!10}0.8466 &\cellcolor{gray!10}0.8002 &\cellcolor{gray!10}0.7642 &\cellcolor{gray!10}- &\cellcolor{gray!10}\\
 & \multirow{-3}*{\cellcolor{gray!10}ChartQA0.5+SimChart9K}   & \cellcolor{gray!10} high \ \textcolor{green!60}{\rule{0.5em}{0.5em}} &\cellcolor{gray!10} &  \multicolumn{2}{c}{\cellcolor{gray!10}0.8380} &\cellcolor{gray!10}&\cellcolor{gray!10}0.8788 &\cellcolor{gray!10}0.8400 &\cellcolor{gray!10}0.8040 &\cellcolor{gray!10}-&\cellcolor{gray!10}\\ 
\bottomrule 
\end{tabular}
}
\label{tab:few-shot}
\vspace{-8pt}
\end{table*}
\vspace{-8pt}
\subsection{From Perception to Reasoning: Structured Triplet Representations (STR)}
\label{sec:exp_qa}
\vspace{-8pt}
We conduct experiments to verify the effectiveness of STR as the intermediate representations of chart for various downstream reasoning tasks. We compare STR with commonly-used LCT. Besides, we leverage GPT-3.5~\citep{Brown2020gpt} as the reasoning module in a one-shot prompting way.


\noindent\textbf{For QA task}, we evaluate on ChartQA~\citep{Masry2022ChartQAAB} using the Exact Match (EM) metric for the text answer, where a 5\% tolerance for the numerical answer is allowed to make a fair comparison. Table~\ref{tab:qa} shows that: (1) SCRM can effectively reflect the quality of CIE, as verified on the QA task, (2) By comparing LCT, the proposed STR facilitates a better understanding of LLMs, yielding higher answer accuracies. In our analysis, this is mainly due to that the STR is designed using a structural description, with a row-column matching relation compared with the previous LCT format. (3) Our StructChart+GPT3.5 pipeline surpasses all baselines on the ChartQA validation set and achieves comparable QA performance with the recently-proposed method~\citep{Liu2022MatChaEV} trained on a closed-source dataset (actually covers a large number of charts in the wild).

\noindent\textbf{For summarization and redrawing tasks}, due to the lack of public datasets and annotations, it is difficult to provide quantitative results. Thus we present qualitative results by employing different intermediate representations (LCT and STR) in Appendix~\ref{app:compare_lct_str}, which further shows that the STR has a stronger ability to represent chart information and help chart understanding.





\vspace{-8pt}
\section{Conclusion and Outlook}
\vspace{-8pt}
This work has addressed the task of extracting and understanding the structured information from a visual chart. We propose a plot-to-triplet transformation to achieve objectivity and precision for chart perception. Besides, we leverage the LLM to generate more query data and drawing codes to enhance the generalization ability under practical settings \textit{e.g.} few-shot chart perception, chart redrawing, and question answering. For future work, we may seek an end-to-end framework with our techniques, which is still an open question.





\clearpage





\bibliographystyle{iclr2024_conference}
\bibliography{egbib}


\newpage
\appendix
\section{Related Works on Vision Language Pre-trained Models (VLPMs)}
\label{app:related_work}
Here, we also discuss related works on Vision Language Pre-trained Models (VLPMs). According to the way of aggregating information from different modalities, VLPMs can be categorized into \textbf{fusion-encoder based}, \textbf{dual-encoder based}, and \textbf{combination based} models.

\noindent\textbf{Fusion-encoder based} VLPMs take text embeddings and image features
as input and involve several fusion approaches to model Vision Languate (VL) interaction. VL-BERT~\citep{Su2019VLBERTPO}, UNIMO~\citep{Li2020UNIMOTU,Li2022UNIMO2EU}, SOHO~\citep{Huang2021SeeingOO}, VL-T5~\citep{Cho2021UnifyingVT}, SimVLM~\citep{Wang2021SimVLMSV}, and ViLT~\citep{Kim2021ViLTVT} assume that the potential correlation and alignment between modalities can be learned by a single transformer encoder. Thus, the text embeddings and image features are concatenated with additional embeddings that indicate position and modalities, and fed into a transformer-based encoder. 
Further, ViLBERT~\citep{Lu2019ViLBERTPT}, Visual Parsing~\citep{Xue2021ProbingIV}, ALBEF~\citep{Li2021AlignBF}, and WenLan~\citep{Huo2021WenLanBV} adopt a cross-attention mechanism to model the interaction between VL modalities, where the query vectors originate from one modality and the key and value vectors from the other. Typical pre-training tasks for fusion-encoder based VLPMs include: masked language/vision modeling, image-text matching, masked region classification, masked region feature regression, visual grounding, visual question answering, and grounded captioning. Thus, fusion-encoder based VLPMs can be effective in VL understanding downstream tasks.

\noindent\textbf{Dual-encoder based} VLPMs utilize two individual single-modal encoders to encode each modality separately, then convert the image and text embeddings into the same semantic space to calculate the VL similarity scores. 
CLIP~\citep{Cho2021UnifyingVT}, ALIGN~\citep{Jia2021ScalingUV} and DeCLIP~\citep{Li2021SupervisionEE} leverage large-scale image-text pairs to learn transferable visual representations for retrieval tasks and exhibit surprising zero-shot transfer to image classification tasks.

\noindent\textbf{Combination based} VLPMs combine the benefits of fusion-encoder based and dual-encoder based architectures. FLAVA~\citep{Singh2021FLAVAAF} firstly utilizes a dual-encoder to acquire single-modal representations. Then, the single-modal embeddings are processed by a fusion-encoder to obtain cross-modal representations. VLMo~\citep{Wang2021VLMoUV} introduces a novel approach called Mixture-of-Modality Expert (MoME), combining a dual-encoder and a fusion-encoder into one unified framework, which can be fine-tuned on both VL understanding and image-text retrieval tasks.

\section{LLM-based Self-inspection Data Production Scheme}
\label{app:simulation}
The complete schematic simulation process can be divided into two stage, containing (1) Data simulation stage for label generation. (2) Image simulation stage for chart generation. We demonstrate two examples in Figs.~\ref{fig:simulation_1} and~\ref{fig:simulation_2}.

\begin{figure}[hp]
    \centering
    \small
    \scalebox{0.78}{
    \begin{tabular}{p{0.95\linewidth}}
    \toprule 
        \hspace{5.1cm}
        \textbf{Data Simulation Stage} 
        \\ \midrule 
        \textbf{Prompt:}   \\
Copying the following table information can be expanded and adapted as appropriate, The imitation is as irrelevant as possible to the original text.
         \\ \midrule
\textbf{Input LCT data:}\\ 
Year,It matters wins the really who presidential election,Things will be pretty much the same regardless of who is elected/n 2000,50,44/n \\2004,67,29/n\\ 2008,63,32/n \\2012,63,34/n \\2016,74,22/n \\2020,83,16
\\ \midrule 
\textbf{Simulated LCT data (completed by GPT-3.5):} \\
Year,People would be better off if the government would just stay out of it,People would be worse off if the government did not get involved/n \\2001,45,48/n\\ 2005,53,42/n \\2009,60,36/n \\2013,63,31/n \\2017,71,25/n \\2021,76,19
\\ \midrule 
\hspace{5.1cm}
\textbf{Image Simulation Stage} 
\\ \midrule 
\textbf{Prompt:}   \\
            Consider you are a professional Python grapher.\\
            Please draw and save a chart based on the following data using python, and images must be clear and intuitive.\\  
            Choose a plot type that best suits the value, for example line, column, scatter and pie charts.\\ 
            Drawing techniques such as background grids can be used.\\
            \[......\]
\\ \midrule
\textbf{Code generated by GPT3.5:} \\
import matplotlib.pyplot as plt\\
data = [[2001, 45, 48],[2005, 53, 42],[2009, 6, 36],[2013, 63, 31],[2017, 71, 25],[2021, 76, 19]]
year, better\_off,worse\_off = zip(*data)\\
fig,ax = plt.subplots(figsize=(15,7))\\
ax.plot(year,better off,color='green",label-'People would be better off if the government would just stay out of it')\\
ax.plot(year.worse off, color='red', label='people would be worse off if the government did not get involved')\\
ax.legend(loc='upper left",fontsize=12,ncol=2)\\
ax.set\_xlabel('Year',fontsize=14)\\
ax.set\_ylabel("Number',fontsize=14)\\
ax.set\_title('Government Involvement', fontsize=14fontweight='bold')\\
ax.set\_xticks(year)\\
plt.tight\_layout()\\
plt.savefig("34.png",dpi-400)\\
plt.cla()
\\ \midrule 
\textbf{Chart image simulated:}\\
\begin{center}
    \includegraphics[width=0.99\linewidth]{appendix/image/simchart_1.png}
\end{center}
\\ \bottomrule
    \end{tabular}
    }
    \caption{Example One of label-efficient chart simulation process including: (1) data simulation stage for labels generation. (2) image simulation stage for charts generation.}
    \label{fig:simulation_1}
\end{figure}


\begin{figure}[hp]
    \centering
    \small
    \scalebox{0.78}{
    \begin{tabular}{p{0.95\linewidth}}
    \toprule 
        \hspace{5.1cm}
        \textbf{Data Simulation Stage} 
        \\ \midrule 
        \textbf{Prompt:}   \\
Copying the following table information can be expanded and adapted as appropriate. The imitation is as irrelevant as possible to the original text.
         \\ \midrule
\textbf{Input LCT data:}\\ 
Entity,Value/n \\Have not heard of Latinx,76.0/n \\Have not heard of Latinx,76.0/n \\Do not use Latinx,20.0/n \\Do not Latinx u,20.0/n
\\ \midrule 
\textbf{Simulated LCT data (completed by GPT-3.5):} \\
Characteristic, Share of engineers/n\\
Very satisfied,48\% /n\\
Satisfied,41\% /n\\
Neither satisfied nor dissatisfied,7\% /n\\
Dissatisfied,3\% /n\\
Very dissatisfied,1\% /n
\\ \midrule 
\hspace{5.1cm}
\textbf{Image Simulation Stage} 
\\ \midrule 
\textbf{Prompt:}   \\
            Consider you are a professional Python grapher.\\
            Please draw and save a chart based on the following data using python, and images must be clear and intuitive.\\  
            Choose a plot type that best suits the value, for example, line, column, scatter and pie charts.\\ 
            Drawing techniques such as background grids can be used.\\
            \[......\]
\\ \midrule
\textbf{Code generated by GPT3.5:} \\
import matplotlib.pyplot as plt\\
Char= ['Very Satisfied', 'Satisfied', 'Neither Satisfied not Dissatisfied', 'Dissatisfied', 'Very Dissatisfied']\\
Share=[48, 41, 7, 3, 1]\\
plt.figure(figsize=(5,5)) \\
plt.pie(Share,labels=Char,autopct='\%1.2f\%\%',textprops={'fontsize': 10})\\ 
plt.title('Engineers\' Satisfaction Level')\\
plt.tight\_layout()\\
plt.savefig('86.png')\\
plt.cla()
\\ \midrule 
\textbf{Chart image simulated:}\\
\begin{center}
    \includegraphics[width=0.8\linewidth]{appendix/image/simchart_2.png}
\end{center}
\\ \bottomrule
    \end{tabular}
   }
    \caption{Example Two of label-efficient chart simulation process including: (1) data simulation stage for labels generation. (2) image simulation stage for charts generation.}
    \label{fig:simulation_2}
\end{figure}


\section{Visualization Comparisons with Matcha and Deplot}
\label{app:compare_matcha}
We compare the proposed Structchart with Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV} in Chart Information Extraction (CIE) task in Figs.~\ref{fig:vis_compare_1},~\ref{fig:vis_compare_2} and~\ref{fig:vis_compare_3}. Matcha~\citep{Liu2022MatChaEV} cannot perform CIE task and can only output the content in the form of HTML format during the pre-training phase, and Deplot~\citep{Liu2022DePlotOV} is affected by noisy backgrounds (\textit{e.g.} web page environment containing other irrelevant text). By comparison, our Structchart can extract accurate information of the chart from complicated background, such as information from the website.

\section{More Results of StructChart on Different Downstream Tasks}
\label{app:different_task}
We visually demonstrate StructChart on downstream tasks in Figs.~\ref{fig:multi_task_1} and~\ref{fig:multi_task_2},  including Question Answering (QA), summarization, and redrawing. For QA task, quantitative evaluation results are shown in Tab~\ref{tab:qa}, and here, we further give many visualization results. For summarization task, some open-ended summary descriptions can be conducted beyond the basic numeric description. For redrawing, different types of charts can be obtained by redrawing chart image given the statistical data (\textit{e.g.}, line chart $\to$ bar chart, bar chart $\to$ pie chart, \textit{etc.})

\section{Demonstrations on Downstream tasks with Linear CSV Tokens (LCT) v.s. Structured Triplet Representations (STR) }
\label{app:compare_lct_str}
We respectively use Linear CSV Tokens (LCT) and Structured Triplet Representations (STR) as intermediate representations of chart information for different downstream tasks. Figs.~\ref{fig:lct_str_1} and \ref{fig:lct_str_2} show that STR used in StructChart has better robustness compared to LCT. When noise is introduced into the highly position-sensitive LCT (a comma is introduced as noise as illustrated in Fig.~\ref{fig:lct_str_1}, and separator comma itself is included as illustrated in Fig.~\ref{fig:lct_str_2}), all downstream tasks will be affected negatively. By comparison, our StructChart achieves better performance on QA, summarization, and redrawing tasks, owing to the proposed STR.

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/matcha_deplot_1.pdf}
\vspace{-5pt}
\caption{Comparison of the proposed StructChart, Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV}, where the Golden Table represents the ground truth of the parsed chart information.}
\label{fig:vis_compare_1}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/matcha_deplot_2.pdf}
\vspace{-5pt}
\caption{Comparison of the proposed StructChart, Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV}, where the Golden Table represents the ground truth of the parsed chart information.}
\label{fig:vis_compare_2}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/matcha_deplot_3.pdf}
\vspace{-5pt}
\caption{Comparison of the proposed StructChart, Matcha~\citep{Liu2022MatChaEV} and Deplot~\citep{Liu2022DePlotOV}, where the Golden Table represents the ground truth of the parsed chart information.}
\label{fig:vis_compare_3}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/multi_task_1.pdf}
\vspace{-5pt}
\caption{Visualization results using the proposed StructChart on different chart-related reasoning tasks including Question Answering (QA), Summarization, and Redrawing.}
\label{fig:multi_task_1}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/multi_task_2.pdf}
\vspace{-5pt}
\caption{Visualization results using the proposed StructChart on different chart-related reasoning tasks including Question Answering (QA), Summarization, and Redrawing.}
\label{fig:multi_task_2}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/lct_str_1.pdf}
\vspace{-5pt}
\caption{Visualization results (a comma introduced in LCT) using the proposed StructChart on downstream tasks with Linear CSV Tokens (LCT) v.s. Structured Triplet Representations (STR).}
\label{fig:lct_str_1}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.92\linewidth]{appendix/image/lct_str_2.pdf}
\vspace{-5pt}
\caption{Visualization results (the separator comma itself is included in chart image) using the proposed StructChart on downstream tasks with Linear CSV Tokens (LCT) v.s. Structured Triplet Representations (STR).}
\label{fig:lct_str_2}
\end{figure}

\section{Discussion About How To Obtain General Chart Large Model (CLM)}
Here, we discuss one possible direction for training a Chart Large Model (CLM), which is challenging due to: (1) Scarce chart data covering as comprehensive scientific fields as possible; (2) Perception-reasoning task gap. This paper provides a preliminary attempt to tackle the above challenges. In the future, we intend to collect more chart data from different scientific subjects and perform the proposed LLM-based self-inspection data production scheme to enhance chart data diversity. This would be the foundation for training a general CLM with a large amount of simulated chart data which are rendered using real-world chart data from multiple fields.
 \appendix

\end{document}