

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage[dvipsnames,svgnames,x11names]{xcolor} 

\usepackage[colorlinks=True,
            linkcolor=Maroon,
            anchorcolor=Maroon,  
            pagebackref,
            urlcolor=icmlblue,
            ]{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\eric}[1]{\textcolor{blue}{Eric:#1}}



\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
  \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
  \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\usepackage{multirow}
\usepackage{amsthm,amsmath,amssymb,lipsum}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{textcomp}

\usepackage{color}
\def\eg{{\it{e.g.}}}
\def\etal{{\it{et al.}}}
\def\ie{{\it{i.e.}}}

\def\recon{{\scshape ReCon}}

\definecolor{pretty-blue}{RGB}{0, 113, 188}
\definecolor{linecolor}{gray}{.91} \definecolor{linecolor2}{gray}{.95} \definecolor{linecolor1}{gray}{.97} 

\definecolor{reconcolor}{HTML}{412F8A}
\definecolor{vitcolor}{HTML}{fc8e62}
\newcommand{\reconcolor}[1]{\textcolor{reconcolor}{#1}}
\newcommand{\vitcolor}[1]{\textcolor{vitcolor}{#1}}
\newcommand{\br}{\reconcolor{\,}} \newcommand{\bv}{\vitcolor{\,}}  \newcommand{\bs}{\vitcolor{\,}} \newcommand{\bh}{\reconcolor{\,}} 


\icmltitlerunning{~ \hfill \recon~: Contrast with Reconstruct \hfill \thepage}
\begin{document}

\twocolumn[
\icmltitle{
Contrast with Reconstruct: Contrastive 3D Representation Learning \\Guided by Generative Pretraining}

\icmlsetsymbol{equal}{}
\icmlsetsymbol{intern}{}


\begin{icmlauthorlist}
\icmlauthor{Zekun Qi}{equal,xjtu}
\icmlauthor{Runpei Dong}{equal,xjtu,intern}
\icmlauthor{Guofan Fan}{xjtu}
\icmlauthor{Zheng Ge}{megvii}
\icmlauthor{Xiangyu Zhang}{megvii}
\icmlauthor{Kaisheng Ma}{thu}
\icmlauthor{Li Yi}{thu,shai,qizhi}
\end{icmlauthorlist}

\icmlaffiliation{xjtu}{Xi'an Jiaotong University}
\icmlaffiliation{megvii}{MEGVII Technology}
\icmlaffiliation{thu}{Tsinghua University}
\icmlaffiliation{shai}{Shanghai AI Laboratory}
\icmlaffiliation{qizhi}{Shanghai Qi Zhi Institute}

\icmlcorrespondingauthor{Kaisheng Ma}{kaisheng@mail.tsinghua.edu.cn}
\icmlcorrespondingauthor{Li Yi}{ericyi@mail.tsinghua.edu.cn}

\icmlkeywords{Machine Learning, ICML, Representation Learning, GPT, Contrastive Learning, Generative Modeling, 3D Point Clouds}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution \icmlInternNotice} 

\begin{abstract}
    Mainstream 3D representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics:
    (i) contrastive models are data-hungry that suffer from a \textit{representation over-fitting} issue;
    (ii) generative models have a \textit{data filling} issue that shows inferior data scaling capacity compared to contrastive models.
    This motivates us to learn 3D representations by sharing the merits of both paradigms, which is non-trivial due to the \textit{pattern difference} between the two paradigms.
    In this paper, we propose \textit{Contrast with Reconstruct} (\recon) that unifies these two paradigms.
    \recon~is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student.
    An encoder-decoder style \recon-block is proposed that 
    transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues.
    \recon~achieves a new state-of-the-art in 3D representation learning, \eg, \textbf{91.26}\% accuracy on ScanObjectNN.
    Codes have been released at \url{https://github.com/qizekun/ReCon}.
\end{abstract}


\section{Introduction}\label{sec:intro}
\vspace{-2pt}
Self-supervised representation learning (SSRL) has witnessed a booming era of \textit{foundational models}~\citep{FoundationModel21}, significant advancements are being made in natural language processing (NLP)~\citep{GPTv1_18,BERT,GPT3_20,CoT22,InstructGPT22}, 2D machine vision~\citep{MoCo,MAE}, and both (vision-language, VL)~\citep{CLIP,StableDiffusion22,Flamingo22}.
While this great course toward foundational machine intelligence is trending, the success of these methods generally demands training on data of \textit{extreme} size.
However, compared to 2D vision and NLP, 3D vision is faced with a challenging \textit{data desert} issue~\citep{ACT23}
due to collection difficulty.

\begin{figure}[t!]
    \begin{center}
    \vspace{-2pt}
    \includegraphics[width=\linewidth]{fig/src/scaling.pdf}
    \vspace{-22pt}
    \caption{\textbf{Data efficiency comparison and attention distance visualization}. (a) Fine-tuning overall accuracy on ScanObjectNN PB\_T50\_RS with models pretrained with different methods on ShapeNet of different data ratios. (b) Averaged attention distance of models pretrained on ShapeNet with \textit{generative} masked point modeling (MPM)~\citep{PointMAE} and cross-modal \textit{contrastive} (CMC) modeling~\citep{CLIP} (texts, images, and point clouds are used), respectively. SUP: supervised classification pretraining on ShapeNet. SMC: single-modal contrastive pretraining~\citep{SCL20}. \recon-SMC and \recon-CMC represent our proposed \recon~with single-modal and cross-modal contrastive modeling variants, respectively. MPM+SMC and MPM+CMC represent vanilla multi-task learning.
    }\label{fig:data_scaling}
    \vspace{-24pt}
    \end{center}
\end{figure} \begin{figure*}[t!]
    \begin{center}
    \vspace{-5pt}
    \includegraphics[width=0.90\linewidth]{fig/src/concept_compare.pdf}
    \vspace{-12pt}
    \caption{\textbf{Concept comparison of \textit{contrastive}, \textit{generative}, and our \recon~paradigms}. Here, we illustrate the methods in a unified view of knowledge distillation (see Sec.~\ref{sec:kd_view}). (a) Contrastive students are trained to learn \textit{invariance} from the teacher. (b) Generative masked modeling encourages students to reconstruct clean signals provided by the teacher. (c) \recon~unifies the two paradigms by learning from multi-teachers, where the generative local student is also a ``teacher" that guides the contrastive global student.
    }\label{fig:concept} 
    \end{center}
    \vspace{-18pt}
\end{figure*} Though under this \textit{low-data} regime, numerous 3D SSRL methods have been developed, which can be grouped into 
two categories, \ie, \textit{contrastive (single/cross-modal)}~\citep{PointContrast20,DepthContrast21,4DContrast22,TupleInfoNCE21,CrossPoint22} and \textit{generative (reconstruct/predict)}~\citep{OcCo,PointBERT,PointMAE} methods.
However, we investigate the pretraining efficiency of the two paradigms by scaling the pretraining data of ShapeNet ranging from 1\% to 100\%, and we find that these two paradigms have their issues (see \cref{fig:data_scaling}(a)):
\vspace{-10pt}
\begin{itemize}
    \item \textbf{Representation over-fitting (contrastive)}. Contrastive models fail to bring generalization when the pretraining data is lacking (), while generative models bring significant improvements with only  data. It shows that contrastive models can easily find shortcuts with trivial representations that \textit{over-fit} the limited data~\citep{MoCo}, and generative models are less data-hungry that learn decent initialization with very few data~\citep{InvariantMIM22}.
    \vspace{-12pt}
    \item \textbf{Data filling (generative)}. Contrastive models present a better potential with scaled-up data, while generative models only provide a little improvement. 
    It shows that contrastive learning may bring superior \textit{data-scaling} capacity when the pretraining data is sufficient. 
    This is observed in 2D where contrastive models surpass generative models~\citep{TuneCLIP22} that have less scaling capability~\citep{ScaleMIM22}.
\end{itemize}
\vspace{-10pt}

These observations motivate the design that shares both merits without mentioned issues.
However, as shown in \cref{fig:data_scaling}(a), simply combining these two paradigms as multi-task learning leads to unsatisfactory results -- lower than the generative model baseline and the \textit{representation over-fitting} issue remains. 
To understand why, we visualize the averaged attention distance\footnote{The attention distance are conducted by averaging per attention head for each layer following~\citet{ViT}, which reveals the relative receptive field of the learned attention.} of the contrastive and generative models in \cref{fig:data_scaling}(b).
A \textit{pattern difference} issue is observed that the attention of contrastive models is mainly paid to a \textit{global} field, while generative models have an appetite for focused \textit{local} attention, which is consistent with the observations by \citet{DarkMIM22} in 2D. 
We conject that this \textit{pattern difference} issue causes a task conflict in the naive multi-task representation learning setting, indicating it is \textit{non-trivial} to combine the merits of contrastive and generative modeling.

To address the above-mentioned issues, we propose \textit{Contrast with Reconstruct} (\recon) that trains generative modeling as guidance for contrastive learning while sharing both merits.
As shown in \cref{fig:concept}, from the perspective of knowledge distillation (Sec.~\ref{sec:kd_view}), contrastive and generative methods can be viewed as vanilla \textit{student-teacher} paradigms (\cref{fig:concept}(a-b)). 
In contrast, our \recon\ unifies the two paradigms as ensemble distillation from multi-teachers, while the generative student is also a ``teacher" which guides the contrastive learning (\textit{student-teacher} knowledge distillation with \textit{student-student} assistance).

In particular, inspired by~\citet{AttentionIsAllYouNeed}, a novel encoder-decoder style \recon-block is proposed where cross attention with stop-gradient is used to transfer guidance from reconstruction to contrastive modeling.
In this fashion, the knowledge from multi-teachers is disentangled and reinforced, which addresses the \textit{representation over-fitting} issue and avoids the learned \textit{pattern difference}.
Further, our \recon~utilizes \textit{single-modal} or \textit{cross-modal} contrastive learning of 3D point clouds, 2D RGB images, and languages that significantly enlarge the pretraining data diversity and capacity. 
Meanwhile, the \textit{data-filling} issue of the generative student is alleviated due to the promising scaling capacity of the contrastive student.
\cref{fig:data_scaling}(b) shows that our \recon~learns 3D representations with high generalization capacity.
By transferring the learned representations to various benchmarks, a new state-of-the-art in self-supervised 3D learning is achieved. For example, an average improvement of \textbf{+9.2\%} and \textbf{+2.9\%} accuracy are achieved on ScanObjectNN and ModelNet40, respectively.
These results show that \recon\ learns foundational geometric understanding, and this simple and general framework successfully unifies contrastive and generative modeling. \section{Related Works}
\textbf{Contrastive Representation Learning}~
is one of the mainstream self-supervised learning paradigms~\citep{LeCunContrastive}, which learns potential semantics from constructed \textit{invariance} or \textit{equivaiance}~\citep{EquivariantSSL22}.
Generally, Instance Discrimination~\citep{InstanceDiscrimination18} is widely adopted to align and distinguish representations of views with the same high-level semantics or not. 
The views could be constructed by augmentations to single-modal~\citep{SimCLR,MoCo,MoCoThree21} or multi-modal data~\citep{CLIP,GLIP22}.
Most works use global features for processing. 
For example, SimCLR~\citep{SimCLR} uses samples with different augmentation policies to construct positive and negative pairs. 
CLIP~\citep{CLIP} proposes a two-tower network that aligns the global representation of languages and images. 
Driven by \textit{InforMAX principle}~\citep{DeepInfoMax19}, they generally use InfoNCE~\citep{InfoNCE} as the loss function to maximize mutual information. 
In 3D, PointContrast~\citep{PointContrast20} proposes geometric augmentation to generate positive and negative pairs. 
CrossPoint~\citep{CrossPoint22} uses both inter and intra-modal contrastive learning. 
PointCLIP~\citep{PointCLIP22} realizes image-point alignment by projecting point clouds to 2D depth images. 
In this work, we focus on single/cross-modal contrastive learning by discriminative contrast~\citep{SCL20} or global feature alignment like~\citet{CLIP}, which is guided by masked generative modeling.

\textbf{Generative Masked Representation Learning}~ 
has emerged as another paradigm of self-supervised learning from NLP~\citep{BERT} to Vision~\citep{MAE}.
It requires the models to learn structured knowledge by \textit{reconstructing masked} input data, which encourages the association of different local patches.
In NLP, it has been a dominant approach to probing knowledge by recovering or predicting words in sentences~\citep{BERT,GPT3_20}. 
With the rapid development of Transformers in vision~\citep{ViT,SwinT}, abundant works have been proposed to realize mask image modeling (MIM). 
\citet{MAE} propose masked autoencoder (MAE) to reconstruct RGB pixels. 
\citet{BEiT} reconstructs the VQVAE~\citep{DALL-E} codebook with encoded semantics. 
Some works propose to reconstruct online teacher tokens~\citep{iBoT} or HOG features~\citep{MaskFeat}. 
In 3D, PointMAE~\citep{PointMAE} extends MAE~\citep{MAE} by reconstructing masked point clouds. PointM2AE~\citep{PointM2AE22} uses a hierarchical Transformer and designs the corresponding masking strategy. MaskPoint~\citep{MaskPoint} proposes to add some noise points and classify whether they belong to masking tokens. Recently, ACT~\citep{ACT23} uses a cross-modal autoencoder as the reconstruction target to acquire dark knowledge from other modalities.  \section{\recon : Contrast with Reconstruct}\label{sec:method}
\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{fig/src/cross_block.pdf}
    \vspace{-5pt}
    \caption{\textbf{Illustration of the proposed \recon-block for pretraining}. The \textit{local} reconstruction task is used to train the encoder, while the \textit{global} contrastive task is used to train the decoder guided by reconstruction-oriented embeddings with cross attention (CA). Stop-gradient (stop-grad) is applied to every CA connection to avoid misleading gradient flow from \textit{global} to \textit{local}.}\label{fig:recon_block}
    \end{center}
    \vspace{-10pt}
\end{figure} We begin with a review in a unified view of knowledge distillation for the two mainstream representation learning methods: masked \textit{generative} modeling and \textit{contrastive} modeling. We then introduce \recon~that unifies these two representation learning methods by reconstruction-guided contrastive learning using an encoder-decoder style \recon-block based architecture, where the overall representation learning is formulated as distillation with both teachers with student-student assistance.

\begin{figure*}[ht!]
    \begin{center}
    \vspace{-4pt}
    \includegraphics[width=0.84\linewidth]{fig/src/framework.pdf}
    \vspace{-12pt}
    \caption{\textbf{Overview of \recon}. \recon\ can be applied to \textit{either} single-modal 3D point clouds inputs \textit{or} cross-modal inputs with rendered RGB images and text descriptions, which will be encoded as sequential tokens. The 3D token embeddings are then masked for \textit{generative} reconstruction for the local 3D encoder, where the encoded intermediate embeddings are fed to the global 3D decoder with stop-gradient (stop-grad) through cross-attention. The \textit{global queries} are learnable and supervised by global \textit{contrastive} learning. 
    }\label{fig:framework}
    \vspace{-15pt}
    \end{center}
\end{figure*} \subsection{Knowledge Distillation: A Unified View of Generative and Contrastive Learning}\label{sec:kd_view}
\paragraph{Contrastive Modeling}
The key insight of contrastive learning lies in \textit{invariance learning}~\citep{InvarianceTheory11,MoCo,InvariantMIM22}, where the abstraction of semantics is generally invariant or equivariant~\citep{EquivariantSSL22} to multiple transformed views like augmentations~\citep{SimCLR} or modalities~\citep{CMC20}.
From the perspective of knowledge distillation~\citep{HintonKD15}, it can be viewed as a student network learning the \textit{invariance} knowledge transferred from the encoded views of the teacher. 
Formally, given input data  with distribution , the student network is  with parameters  and  is the teacher network with parameters . The optimization target can be written:

where
\begin{itemize}
    \item The teacher parameters  can be the same as  with~\citep{SimSiam} or without~\citep{SimCLR} \textit{stop-gradient},  can also be the \textit{momentum counterpart} updated as  where  is the momentum coefficient~\citep{MoCo,MoCoThree21}. For multi-modal inputs\footnote{Note that there are some methods~\citep{CMC20,CLIP} that optimizes both  and .},  can be \textit{different} from  that encodes views from other modalities~\citep{CMC20,CLIP,CenterLoss21}.
    \item  are two transformations of the input data that belongs to constructed transformation pairs , where  constructs the positive \textit{or} negative\footnote{For methods use positive samples only~\citep{BYOL,SimSiam},  only generates positive views.} invariance targets, respectively.
    In general, this transformation generates copied views by data augmentations~\citep{DINO21,StrongAugContrast21}, we extend it to \textit{multi-modal} inputs where the transformation becomes generating views from different modalities that share the same high-order concept~\citep{CLIP,TupleInfoNCE21}.
    \item  is the distance function defined in some metric space .
    To avoid representation collapsing~\citep{MoCo}, \textit{InfoMax-Principle}~\citep{DeepInfoMax19,AugDeepInfoMax19} based metrics like MINE~\citep{MINE18}, InfoNCE~\citep{InfoNCE} are often used~\citep{MoCo,GoodViewContrast20,CDS22}. For methods using \textit{positive-only} transformations, the metric function can be feature correlation measurement~\citep{BarlowTwins21},  distance~\citep{WhiteContrast21}, or cosine similarity~\citep{BYOL,SimSiam}.
\end{itemize}
\paragraph{Generative Masked Modeling} Similarly, given input data  with distribution , the student network as  with parameters  and  as the teacher network\footnote{Note that for methods like MAE~\citep{MAE,PointMAE}, the teacher network is an identity mapping with no parameters, and therefore  is an empty set, see~\citet{ACT23}.} with parameters . 
generative masked modeling can be formulated as follows:

where 
\begin{itemize}
    \item  is a distance function defined in some metric space . It can be  distance~\citep{MAE}, cross-entropy~\citep{BERT,BEiT}, or Chamfer-Distance~\citep{ChamferDistance17,PointMAE}.
    \item  are the masking corruptions where  samples a subset of the input tokens and performs masking~\citep{BERT}, and  correspondingly samples the subset while without masking.
\end{itemize}

\begin{table*}[ht!]
\caption{
\textbf{Classification results on the ScanObjectNN and ModelNet40 datasets}. The inference model parameters \#P (M), FLOPS \#F (G), and overall accuracy (\%) are reported. 
The dagger () denotes that the model was reproduced using our proposed \br\recon-block and the fine-tuning techniques used in \recon.
We compare with methods using the \bh hierarchical Transformer architectures (\eg, Point-M2AE~\citep{PointM2AE22}), \bv plain Transformer architectures, and \bs dedicated architectures for 3D understanding.
\texttt{PT}: pretrained teacher is used, \texttt{MD}: multi-modal data is used. Single-Modal means that \textbf{only point clouds} are used as pre-training data.
}
\label{tab:scanobjectnn}
\begin{center}
\vspace{-14pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccccccc}
\toprule[0.95pt]
\multirow{2}{*}[-0.5ex]{Method} & \multirow{2}{*}[-0.5ex]{\#P} & \multirow{2}{*}[-0.5ex]{\#F} & \multirow{2}{*}[-0.5ex]{\texttt{PT}} & \multirow{2}{*}[-0.5ex]{\texttt{MD}} & \multicolumn{3}{c}{ScanObjectNN} & \multicolumn{2}{c}{ModelNet40}\\
\cmidrule(lr){6-8}\cmidrule(lr){9-10} & & & & & OBJ\_BG & OBJ\_ONLY & PB\_T50\_RS& 1k P & 8k P\\
\midrule[0.6pt]
\multicolumn{10}{c}{\textit{Supervised Learning Only}}\\
\midrule[0.6pt]
\bs PointNet~\citep{PointNet} & 3.5 & 0.5 &  &  & 73.3 & 79.2 & 68.0 & 89.2 & 90.8\\
\bs PointNet++~\citep{PointNet++} & 1.5 & 1.7 &  &  & 82.3 & 84.3 & 77.9 & 90.7 & 91.9\\
\bs DGCNN~\citep{DGCNN} & 1.8 & 2.4 &  &  & 82.8 & 86.2 & 78.1 & 92.9 & -\\
\bs PointCNN~\citep{PointCNN} & 0.6 & - &  &  & 86.1 & 85.5 & 78.5 & 92.2 & -\\
\bs SimpleView~\citep{SimpleView} & - & - &  &  & - & - & 80.50.3 & 93.9 & -\\
\bs MVTN~\citep{MVTN} & 11.2 & 43.7 &  &  & 92.6 & 92.3 & 82.8 & 93.8 & -\\
\bh PCT~\citep{PCT} & 2.88 & 2.3 &  &  & - & - & - & 93.2 & -\\
\bs PointMLP~\citep{PointMLP} & 12.6 & 31.4 &  &  & - & - & 85.40.3 & 94.5 & -\\
\bs PointNeXt~\citep{PointNext} & 1.4 & 3.6 &  &  & - & - & 87.70.4 & 94.0 & -\\
\bs P2P-HorNet~\citep{P2P} & - & 34.6 &  &  & - & - & 89.3 & 94.0 & -\\
\midrule[0.6pt]
\multicolumn{10}{c}{\textit{with Single-Modal Self-Supervised Representation Learning} ({\scshape Full})}\\
\midrule[0.6pt]
\bv Transformer~\citep{AttentionIsAllYouNeed} & 22.1 & 4.8 &  &  & 83.04 & 84.06 & 79.11 & 91.4 & 91.8\\
\br Transformer~\citep{AttentionIsAllYouNeed} & 43.6 & 5.3 &  &  & 84.90 & 86.12 & 81.64 & 91.6 & 92.0\\
\bv Point-BERT~\citep{PointBERT} & 22.1 & 4.8 &  &  & 87.43 & 88.12 & 83.07 & 93.2 & 93.8\\
\bv Point-MAE~\citep{PointMAE} & 22.1 & 4.8 &  &  & 90.02 & 88.29 & 85.18 & 93.8 & 94.0\\
\bh Point-M2AE~\citep{PointM2AE22} & 15.3 & 3.6 &  &  & 91.22 & 88.81 & 86.43 & 94.0 & -\\
\br Point-MAE~\citep{PointMAE} & 43.6 & 5.3 &  &  & 92.60 & 91.91 & 88.42 & 93.8 & 94.0\\
\rowcolor{linecolor2}\br\recon~\textbf{w/o vot.} & 43.6 & 5.3 &  &  & \textbf{94.15} & \textbf{93.12} & \textbf{89.73} & 93.6 & 93.8\\
\rowcolor{linecolor}\br\recon~\textbf{w/ vot.} & 43.6 & 5.3 &  &  & \textbf{94.49} & \textbf{93.29} & \textbf{90.35} & \textbf{93.9} & \textbf{94.2}\\
\midrule[0.6pt]
\multicolumn{10}{c}{\textit{with Cross-Modal Self-Supervised Representation Learning} ({\scshape Full})}\\
\midrule[0.6pt]
\bv ACT~\citep{ACT23} & 22.1 & 4.8 &  &  & 93.29 & 91.91 & 88.21 & 93.7 & 94.0\\
\rowcolor{linecolor1}\br\recon-Tiny~\textbf{w/o vot.} & 11.4 & 2.4 &  &  & \textbf{93.80} & \textbf{92.94} & \textbf{89.10} & 93.3 & 93.6\\
\rowcolor{linecolor2}\br\recon-Small~\textbf{w/o vot.} & 19.0 & 3.2 &  &  & \textbf{94.15} & \textbf{93.12} & \textbf{89.52} & 93.5 & 93.8\\
\rowcolor{linecolor1}\br\recon~\textbf{w/o vot.} & 43.6 & 5.3 &  &  & \textbf{94.66} & \textbf{93.29} & \textbf{90.32} & \textbf{94.0} & \textbf{94.2}\\
\rowcolor{linecolor2}\br\recon~\textbf{w/o vot.} & 43.6 & 5.3 &  &  & \textbf{95.18} & \textbf{93.63} & \textbf{90.63} & \textbf{94.1} & \textbf{94.3}\\
\rowcolor{linecolor}\br\recon~\textbf{w/ vot.} & 43.6 & 5.3 &  &  & \textbf{95.35} & \textbf{93.80} & \textbf{91.26} & \textbf{94.5} & \textbf{94.7}\\
\midrule[0.6pt]
\multicolumn{10}{c}{\textit{with Self-Supervised Representation Learning} ({\scshape Mlp-Linear})}\\
\midrule[0.6pt]
\br Point-MAE~\citep{PointMAE} & 43.6 & 5.3 &  &  & 82.770.30 & 83.230.16 & 74.130.21 & 90.220.09 & 90.730.09 \\
\bv ACT~\citep{ACT23} & 22.1 & 4.8 &  &  & 85.200.83 & 85.840.15 & 76.310.26 & 91.360.17 & 91.750.18 \\
\rowcolor{linecolor}\br\recon~\textbf{w/o vot.} & 43.6 & 5.3 &  &  & \textbf{89.50}0.20 & \textbf{89.72}0.17 & \textbf{81.36}0.14 & \textbf{92.47}0.22 & \textbf{92.68}0.07\\
\midrule[0.6pt]
\multicolumn{10}{c}{\textit{with Self-Supervised Representation Learning} ({\scshape Mlp-3})}\\
\midrule[0.6pt]
\br Point-MAE~\citep{PointMAE} & 43.6 & 5.3 &  &  & 85.780.31 & 85.510.16 & 80.380.21 & 91.250.24 & 91.680.19 \\
\bv ACT~\citep{ACT23} & 22.1 & 4.8 &  &  & 87.140.22 & 87.900.40 & 81.520.19 & 92.690.18 & 92.950.10 \\
\rowcolor{linecolor}\br\recon~\textbf{w/o vot.} & 43.6 & 5.3 &  &  & \textbf{90.62}0.22 & \textbf{90.71}0.30 & \textbf{83.80}0.42& \textbf{93.00}0.10 & \textbf{93.39}0.05\\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\vspace{-16pt}
\end{table*}
 
\textbf{Ensemble Representation Distillation}~~
Ensemble distillation\footnote{Term taken from~\citet{CRD20}.} is known to be more informative and instructive~\citep{CRD20,Feed20,MultiExpertKD,KDOverview22}, which encourages the student network to learn ensembled and disentangled knowledge representation. We motivate our method as an ensembled knowledge distillation of Eq.~(\ref{eq:kd_contrast}) and Eq.~(\ref{eq:kd_mdm}), where the student is trained with merits from both \textit{contrastive} and \textit{generative} aspects. The overall loss  is defined as follows:


\subsection{Reconstruction Guided Contrastive Learning}\label{sec:method_impl}
\vspace{-6pt}
\textbf{Network Architecture}~~ As discussed in Sec.~\ref{sec:intro}, since the two distillation result in different learning patterns, it is \textit{non-trivial} to learn from both targets jointly. 
To tackle this issue, we propose \textit{contrast with reconstruct} (\recon). The reconstruction-oriented representations that focus on \textit{local} patterns are used as semantic guidance for \textit{global} contrastive learning. 
Inspired by the Transformer encoder-decoder architecture~\citep{AttentionIsAllYouNeed}, we conduct dense masked modeling with an encoder, which produces features to guide global contrastive learning through a sparse query-based decoder. 
The encoder and decoder share the same Transformer architecture, and they are layer-wisely associated with cross attention (CA). 
Due to limited 3D data, the contrastive model can easily learn trivial representations as shortcuts, and this could lead to noisy training signals, which may harm generative student learning.
Hence, to avoid the task conflicts between these two students, we use \textit{stop-gradient} for every CA connection to cut the misleading training signal from global contrast to local reconstruction.
We call the proposed network architecture \recon-block, which is illustrated in~\cref{fig:recon_block}. 
In this fashion, the ensemble ``student-teacher" distillation from multi-teacher is learned jointly with ``student-student" assistance where the contrastive student is guided by the generative ``teacher".
As a result, the contrastive student is trained with a good data scaling capacity without the risk of representation over-fitting, and the pattern difference issue is avoided with no task conflicts.
\begin{center}
\begin{table}[t!]
    \vspace{-2.5pt}
    \centering
    \setlength\tabcolsep{8pt}
    \caption{\textbf{Few-shot classification results on ModelNet40}.  represent results of our proposed \br\recon-block built backbone architecture. Overall accuracy (\%) without voting is reported.}\label{tab:few-shot}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule[0.95pt]
    \multirow{2}{*}[-0.5ex]{Method}& \multicolumn{2}{c}{5-way} & \multicolumn{2}{c}{10-way}\\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5} & 10-shot & 20-shot & 10-shot & 20-shot\\
    \midrule[0.6pt]
    \bs DGCNN &31.6  2.8 &  40.8  4.6&  19.9   2.1& 16.9  1.5\\
    \bs OcCo &90.6  2.8 & 92.5  1.9 &82.9  1.3 &86.5  2.2\\
    \midrule[0.6pt]
    \multicolumn{5}{c}{\textit{with Self-Supervised Representation Learning} ({\scshape Full})}\\
    \midrule[0.6pt]
    \bv Transformer & 87.8  5.2& 93.3  4.3 & 84.6  5.5 & 89.4  6.3\\
    \br Transformer & 90.2  5.9 & 94.3  4.4 & 85.2  5.9 & 89.9  6.1\\
    \bs OcCo & 94.0  3.6& 95.9  2.3 & 89.4  5.1 & 92.4  4.6\\
    \bv Point-BERT & 94.6  3.1 & 96.3  2.7 &  91.0  5.4 & 92.7  5.1\\
    \bv MaskPoint & 95.0  3.7 & 97.2  1.7 & 91.4  4.0 & 93.4  3.5\\
    \bv Point-MAE & 96.3  2.5&97.8  1.8 & 92.6  4.1 & 95.0  3.0\\
    \bh Point-M2AE & 96.8  1.8&98.3  1.4 & 92.3  4.5 & 95.0  3.0\\
    \br Point-MAE & 96.4  2.8&97.8  2.0 & 92.5  4.4 & 95.2  3.9\\
    \bv ACT & 96.8  2.3 & 98.0  1.4 & 93.3  4.0 & 95.6  2.8\\
    \rowcolor{linecolor}\br\recon & \textbf{97.3  1.9} & \textbf{98.9  1.2} & \textbf{93.3  3.9} & \textbf{95.8  3.0} \\
    \midrule[0.6pt]
    \multicolumn{5}{c}{\textit{with Self-Supervised Representation Learning} ({\scshape Mlp-Linear})}\\
    \midrule[0.6pt]
    \br Point-MAE & 91.1  5.6 & 91.7  4.0 & 83.5  6.1 & 89.7  4.1\\
    \bv ACT & 91.8  4.7 & 93.1  4.2 & 84.5  6.4 & 90.7  4.3\\
    \rowcolor{linecolor}\br\recon & \textbf{96.9  2.6} & \textbf{98.2  1.4} & \textbf{93.6  4.7} & \textbf{95.4  2.6}\\
    \midrule[0.6pt]
    \multicolumn{5}{c}{\textit{with Self-Supervised Representation Learning} ({\scshape Mlp-})}\\
    \midrule[0.6pt]
    \br Point-MAE & 95.0  2.8 & 96.7  2.4 & 90.6  4.7 & 93.8  5.0\\
    \bv ACT & 95.9  2.2 & 97.7  1.8 & 92.4  5.0 & 94.7  3.9\\
    \rowcolor{linecolor}\br\recon&\textbf{97.4  2.2 }& \textbf{98.5  1.4} & \textbf{93.6  4.7}& \textbf{95.7  2.7}\\
    \bottomrule[0.95pt]
    \end{tabular}
    }
\vspace{-10pt}
\end{table}
\end{center} 
\vspace{-20pt}
\textbf{Implementation}~
We use the standard plain Transformer~\citep{AttentionIsAllYouNeed} built with 12 \recon-blocks with dimension 384 and a lightweight PointNet patch embedding~\citep{PointNet,PointNet++,PointBERT} as the 3D representation learner.
ShapeNet~\citep{ShapeNet15} is used to pretrain \recon, which contains 51K unique 3D CAD models covering 55 object categories. 
We follow~\citet{SCL20} for \textit{single-modal} contrastive modeling.
For \textit{cross-modal} setting, we utilize point clouds, RGB images, and free-form languages, where the limited 3D data is enlarged with rich texture and semantic knowledge within images and languages~\citep{CrossPoint22,ACT23}.
We obtain RGB images by rendering from 3D meshes and language descriptions by concatenating text prompts with category descriptions. 
We use by default an ImageNet~\citep{ImageNet09} pretrained Vision Transformer (ViT-B)~\citep{ViT} as the image view teacher, and we use the text encoder from CLIP~\citep{CLIP} as the text view teacher.
The image and text teacher encoders are frozen during pretraining, and Smooth -based positive-only distillation~\citep{SimSiam,WhiteContrast21} is used.
The masked generative modeling follows~\citet{PointMAE}, where the reconstruction metric is  Chamfer-Distance~\citep{ChamferDistance17}.
\cref{fig:framework} shows an overall illustration and more details can be found in Appendix~\ref{app:impl_detail}. \vspace{-3pt}
\section{Experiments}
\vspace{-3pt}
\subsection{Transfer Learning on Downstream Tasks}
\vspace{-5pt}
\paragraph{Transfer Protocol}
We use the same classification heads and transfer learning protocols following~\citet{ACT23}: {\scshape Full}, {\scshape Mlp-Linear}, and {\scshape Mlp-}. 

\textbf{3D Real-World Object Recognition}~
ScanObjectNN~\citep{ScanObjectNN19} is one of the most challenging 3D datasets, which covers 15K real-world objects from 15 categories.
For a fair comparison, we report the results with and without the voting strategy~\citep{RSCNN} separately. 
Note that we only use simple \textit{Rotation} as data augmentation in training following~\citet{ACT23}.
We report \textit{single-modal} (\recon-SMC) and \textit{cross-modal} (\recon-CMC, default if not otherwise specified) results on three model variants (see \cref{app:impl_detail}.) 
The results are shown in \cref{tab:scanobjectnn}, it is observed that: 
(i) With a comparable GFLOPS, the performance of our \recon-block (\textit{from scratch}) is improved by~2.5\% compared with that of standard Transformer under {\scshape Full} tuning protocol. 
Further, after the pre-training of \textit{reconstruction guided contrastive learning}, \recon\ gains a significant improvement of +11.3\% accuracy averaged on the three variant ScanObjectNN benchmarks.
(ii) Compared to other self-supervised learning (SSL) methods, our \recon\ achieves the best generalization across both single-modal and cross-modal on all transferring protocols. 
\eg, \recon\ outperforms Point-MAE by +5.6\% on three ScanObjectNN variants. 
(iii) Compared with any supervised or self-supervised method, our \recon\ achieves a new state-of-the-art that outperforms existing methods by a large margin. 
\begin{figure*}[h!]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{fig/src/ablation.pdf}
    \vspace{-15pt}
    \caption{\textbf{Ablation study of masking ratio, decoder depth, and the 2D teacher encoder choice used during \recon~pretraining.} The masking ratio and decoder depth represent the ablation for the generative masked modeling stream, and the 2D teachers are used for contrastive cross-modal learning. }\label{fig:ablation}
    \end{center}
    \vspace{-22pt}
\end{figure*} 
\textbf{3D Synthetic Object Recognition}~
ModelNet~\citep{ModelNet15} is one of the most classical datasets for synthetic 3D object recognition. It contains 12K meshed 3D CAD objects of 40 (ModelNet40) or 10 (ModelNet10) categories.
We conduct the evaluation on the ModelNet40 dataset, including fine-tuning and few-shot learning. 
We use \textit{Scale\&Translate} as data augmentation in training following~\citet{PointNet,PointNet++}. 
The results are shown in \cref{tab:scanobjectnn} and \cref{tab:few-shot}, respectively. 
It can be observed that our \recon\ achieves 94.7\% classification accuracy of ModelNet40 under {\scshape Full} protocol, improved by 2.7\% compared with the Transformer baseline. 
In the few-shot task, our \recon\ has also achieved the best performance under all protocols, especially under the {\scshape Mlp-} and {\scshape Mlp-Linear} protocols.

\begin{table}[!t]
\caption{\textbf{Zero-shot 3D object classification domain transfer on ModelNet40 (MN-40) and ModelNet10 (MN-10)}. Top-1 accuracy (\%) is reported. Ensemb. denotes whether to use the ensemble strategy with multiple text inputs.} \label{tab:zeroshot}
\vspace{-8pt}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule[0.95pt]
Method  & Backbone & Ensemb. & MN-10 & MN-40\\
\midrule[0.6pt]
\bs PointCLIP~\cite{PointCLIP22} & ResNet-50 &  & 30.2 & 20.2 \\
\bv CLIP2Point~\cite{CLIP2Point22} & Transformer &  & 66.6 & 49.4 \\
\rowcolor{linecolor1}\br\recon & Transformer &  & \textbf{74.2} & \textbf{60.6} \\
\rowcolor{linecolor}\br\recon & Transformer &  & \textbf{75.6} & \textbf{61.7} \\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\vspace{-25pt}
\end{table} \textbf{3D Zero-Shot Recognition}~
Similar to CLIP~\citep{CLIP}, our model aligns the feature space of languages and other modalities. 
Therefore, our model has a strong zero-shot capability. 
We use ModelNet~\citep{ModelNet15} dataset to conduct zero-shot evaluation, including ModelNet10 and ModelNet40. The results are shown in \cref{tab:zeroshot}.
Following PointCLIP~\citep{PointCLIP22}, We use prompt templates with the category label to generate text features. 
From \cref{tab:zeroshot}, it can be seen that our \recon\ surpasses all the zero-shot methods with CNN-based or Transformer-based backbones. 
Further, by using ensemble methods such as multi-prompt templates~\citep{CLIP2Point22}, our \recon\ achieves a Top-1 accuracy of 61.7\% on ModelNet40, which significantly outperforms PointCLIP and CLIP2Point by 41.5\% and 12.3\%, respectively. 

\begin{table}[!t]
\caption{\textbf{Ablation study on pretraining targets}. Overall accuracy (\%) without voting is reported.} \label{tab:target}
\begin{center}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule[0.95pt]
\multirow{2}{*}[-0.5ex]{Reconstruction} & \multicolumn{3}{c}{Contrastive} & \multirow{2}{*}[-0.5ex]{ScanObjectNN}\\
\cmidrule(lr){2-4} & text & image & self\\
\midrule[0.6pt]
 &  &  &  & 85.60 \\
 &  &  &  & 86.02 \\
 &  &  &  & 85.57 \\
 &  &  &  & 86.49 \\
\midrule[0.6pt]
 &  &  &  & 88.42 \\
 &  &  &  & 89.77 \\
 &  &  &  & 90.18 \\
 &  &  &  & 89.50 \\
 &  &  &  & \cellcolor{linecolor}\textbf{90.63} \\
 &  &  &  & 89.98 \\
\bottomrule[0.95pt]
\end{tabular}
}
\vspace{-15pt}
\end{center}
\end{table} \vspace{-3pt}
\subsection{Ablation Study}\label{sec:ablation}
\textbf{Hyper Parameter}~
In \cref{fig:ablation}, We show the ablation study on masking ratio, decoder depth, and the selection of the 2D image teacher during \recon\ pretraining. 
It can be observed that the optimal masking ratio and decoder depth is consistent with Point-MAE~\citep{PointMAE}, indicating that the pretraining model, which is friendly to downstream tasks, is also helpful for the guidance of contrastive learning.
The results also show that ViT~\citep{ViT}, as a 2D teacher, is superior to CLIP~\citep{CLIP}, Swin-Transformer (Swin)~\citep{SwinT}, ResNet~\citep{ResNet16} and BEiT~\citep{BEiT}. 
In addition, we find that CLIP, which is already aligned with languages, brings inferior performance to ViT.
This may be due to the reduction of diversity caused by the high similarity of features from the pre-aligned text teacher, which can be considered degenerating to only a single vision-language pretrained teacher.

\textbf{Pretraining Targets}~
To analyze the importance of the pretraining targets and verify the effect of reconstruction-guided contrastive modeling, we conduct an ablation study on the pretraining target. 
The results are shown in \cref{tab:target}.
It can be seen that: 
(i) When reconstruction guidance is not used, the performance of the contrastive model is very poor due to over-fitting on the limited 3D data.
(ii) The performance of single-modal contrastive learning is slightly weaker than that of cross-modal contrastive learning, and the improvements can not be shared.
Besides, we find that both 2D and text teachers can help improve performance without contradictions, and 2D teachers bring better improvement in learned representations generalization.

\begin{table}[!t]
\caption{\textbf{Ablation study on the contrastive metric}. Overall accuracy (\%) without voting is reported.} \label{tab:contrastive_loss}
\vspace{-5pt}
\begin{center}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcc}
\toprule[0.95pt]
Contrastive Metric & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
InfoNCE & 90.11 & 93.8\\
 Distance & 89.64 & 93.6\\
Smooth  & \cellcolor{linecolor}\textbf{90.63} & \cellcolor{linecolor}\textbf{94.1}\\
Cosine Similarity & 90.17 & 93.8\\
\bottomrule[0.95pt]
\end{tabular}
}
\vspace{-18pt}
\end{center}
\end{table}

 \textbf{Contrastive Metric}~
\cref{tab:contrastive_loss} shows the ablation study on the contrastive metric.
Smooth  distance achieves the best results in both tasks and is higher than the commonly used InfoNCE~\citep{InfoNCE}.
We argue that the reasons are two-fold:
(i) The cross-modal positive-only contrastive learning with the frozen teachers (stop-grad) has no risk of representation collapsing~\citep{SimSiam}, and it is not necessary to introduce negative samples.
(ii) ShapeNet dataset is full of household objects with limited semantic diversity, unlike ImageNet, which makes the negative samples noisy and confusing. 
These hard negatives are generally not easy for mining and may bring unsatisfactory optimization challenges~\citep{VSEPP}. 
 \section{Discussions}\label{Discuss}
\subsection{What role does the reconstruction guidance play in contrastive learning?}
\vspace{-6pt}
To analyze the reason why \recon\ works, we record the contrastive loss on the \textit{test} split of ShapeNet (not used for pretraining) using cross-modal contrastive (CMC) and \textit{reconstruction guidance contrastive} (\recon-CMC), along with the corresponding fine-tuning accuracy on ScanObjectNN. 
\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=0.95\linewidth]{fig/src/loss_curve.pdf}
    \vspace{-10pt}
    \caption{\textbf{Pretraining contrastive loss on ShapeNet \textit{test} split vs. finetuning accuracy (\%) on ScanObjectNN}. The pretraining \textit{test} loss (left) is plotted in orange, and the corresponding fine-tuning accuracy (right) is plotted in blue. LR: pretraining learning rate.
    }\label{fig:loss_curve}
    \vspace{-18pt}
    \end{center}
\end{figure} The results are shown in Figure~\ref{fig:loss_curve}. 
It can be seen that the \textit{test} contrastive loss of our \recon-CMC is consistently lower than vanilla CMC, which converges to a lower value more stably (0.034 vs. 0.052), indicating that our \recon~brings superior generalization performance of the pretraining contrastive task. 
As a result, \recon-CMC learns to contrast without falling into shortcuts of trivial solutions, and the over-fitting issue during pretraining is alleviated.
The corresponding fine-tuning accuracy demonstrates this point, where a significantly superior generalization performance with better training efficiency of our \recon-CMC compared to vanilla CMC is achieved (90.63\% vs. 82.48\%).
Besides, we find that a lower learning rate improves vanilla CMC performance, demonstrating that contrastive models are prone to over-fit (see Appendix~\ref{app:add_ablation}).

\begin{table}[!t]
\caption{\textbf{Study of the \textit{stop-gradient} operation in \recon-block}. Overall accuracy (\%) without voting is reported.} \label{tab:detach}
\begin{center}
\resizebox{0.72\linewidth}{!}{
\begin{tabular}{ccc}
\toprule[0.95pt]
Stop-grad & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
 & 81.60 & 89.7\\
 & \cellcolor{linecolor}\textbf{90.63} & \cellcolor{linecolor}\textbf{94.1}\\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\vspace{-15pt}
\end{table}

 \vspace{-4pt}
\subsection{Can contrastive learning guide reconstruction?}
\vspace{-3pt}
As discussed in Sec.~\ref{sec:intro}, the learned patterns of contrastive and generative modeling are different, and we build \recon\ where the generative student guides the contrastive student.
What if we use global contrastive learning to guide the generative masked modeling?
To answer this question, we analyze the role of \textit{stop-gradient} in \recon-block.
The results are shown in Table~\ref{tab:detach}. 
It can be seen that \textit{without} stop-gradient, the performance of \recon~is seriously degraded (-9.03\% on ScanObjectNN and -4.4\% on ModelNet40).
We argue that the contrastive task can easily converge to a degenerated solution due to the limited 3D data, as discussed before, which in turn leads to noisy gradient and training signal to the generative modeling part during pretraining.
As a result, the guidance itself is disturbed and harmed, while
the model fails to learn representations with valid semantics.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/src/vis.pdf}
    \vspace{-20pt}
    \caption{\textbf{Attention distribution visualization of local tokens and global queries learned by \recon}. We randomly select three local tokens, which are highlighted in red dashed circles.}\label{fig:atten_vis}
    \end{center}
    \vspace{-25pt}
\end{figure} \vspace{-5pt}
\subsection{What is learned in \recon?}
\vspace{-4pt}
According to the design, \recon\ should have learned both locally focused and global 3D geometric understandings. 
In Figure~\ref{fig:atten_vis}, we visualize the attention distribution of randomly selected local tokens and global queries. 
The red and yellow parts are the key areas of attention, while the blue and purple parts are the areas of less attention.
It can be seen that the local tokens in 3D point clouds focus more on the geometric structure around the tokens themselves, while the global queries focus on the whole parts of the object. 
Interestingly, the local tokens may have learned some geometric understanding of symmetry.
For example, the token on the left wing of the \texttt{airplane} also noticed the right wing. 
In addition, we find that global image and text queries may have learned some complementary knowledge. \vspace{-6pt}
\section{Conclusions}
\vspace{-3pt}
In this paper, we propose \textit{contrast with reconstruct} (\recon), which enjoys the merits of both generative masked modeling and contrastive modeling, while scalable to multimodal data to facilitate stronger 3D representation learning.
Our results show high-capacity data efficiency and generalizations on both pretraining and downstream representation transferring.
In particular, \recon\ achieves a new state-of-the-art on challenging real-world 3D object recognition.
By diving deeply into \recon, we emphasize the importance of reconstruction, which avoids the contrastive over-fitting issue due to limited 3D data.
Visualizations show that \recon\ indeed learns decoupled local and global representations in the proposed \recon-block.
\recon\ is a simple framework, and we hope more \recon-style models to be produced in the multi-modal learning community.
 


{
\small\bibliographystyle{icml2023}
\bibliography{main}
}

\newpage
\appendix
\onecolumn
\begin{figure*}[ht]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{fig/src/downstream.pdf}
    \caption{\textbf{Pipeline of \recon\ executing zero-shot and fine-tuning}. We fuse the pretrained global image (\texttt{[IMG]}) and text (\texttt{[TXT]}) query features by summation for zero-shot prediction. During fine-tuning, a new \texttt{[CLS]} token is added and fused with pretrained global \texttt{[IMG]} and \texttt{[TXT]} queries by concatenation for fine-tuning prediction. No stop-grad is used in CA connections during fine-tuning.}\label{fig:zeroshot}
    \end{center}
    \vspace{-10pt}
\end{figure*} \section{Additional Related Works}
\textbf{Contrastive-Generative Representation Learning}~
In 2D vision and NLP, some works have been proposed for contrastive-generative representation learning. \citet{iBoT} propose to use an online tokenizer to distill local tokens and global class tokens, respectively. 
\citet{sim22} further proposes to perform three tasks at the same time: reconstruction, intra-view matching, and intra-image contrastive learning. 
This paradigm is also trending in the vision-language (VL) community, \citet{albef21} propose ALBEF that uses contrastive learning before modality fusion to make the reconstructed features encoded with more semantics. Through cross attention of different modalities, CoCa~\citep{CoCa22} fuses visual-language features while performing mask language modeling, where contrastive learning aligns the multi-modal global features. Recently, \citet{FLIP22} propose to use masked signals for contrastive VL learning, which greatly improves the training efficiency without losing performance. 
Different from these methods, our \recon\ is built for 3D representation learning, which is faced with a unique and serious \textit{low-data} challenge. 
Besides, we propose a novel \recon-block that models contrastive-generative representation learning as a generative pretraining \textit{guided} contrastive learning.
\begin{center}
\begin{table}[h!]
\vspace{-12pt}
\caption{\textbf{Training recipes for pretraining and downstream fine-tuning}.}
\label{tab:hyper_params}
\vskip 0.10in
\centering
\begin{tabular}{lcccc}
 & \texttt{Pretraining} & \multicolumn{2}{c}{\texttt{Classification}} & \texttt{Segmentation}\\
 \toprule[0.95pt]
 Config & ShapeNet & ScanObjectNN & ModelNet & ShapeNetPart\\
 \midrule[0.6pt]
 optimizer & AdamW & AdamW & AdamW & AdamW\\
 learning rate & 5e-4 & 2e-5 & 1e-5 & 2e-4 \\
 weight decay & 5e-2 & 5e-2 & 5e-2 & 5e-2 \\
 learning rate scheduler & cosine & cosine & cosine & cosine \\
 training epochs & 300 & 300 & 300 & 300\\
 warmup epochs & 10 & 10 & 10 & 10\\
 batch size & 128 & 32 & 32 & 16\\
 drop path rate & 0.1 & 0.2 & 0.2 & 0.1 \\
 \midrule[0.6pt]
 image resolution & 224224 & - & - & -\\
 image patch size & 16 & - & - & -\\
 number of points & 1024 & 2048 & 1024/8192 & 2048 \\
 number of point patches & 64 & 128 & 64/512 & 128 \\
 point patch size & 32 & 32 & 32 & 32 \\
 \midrule[0.6pt]
 augmentation & Rotation & Rotation & Scale\&Trans & - \\
 \midrule[0.6pt]
 GPU device & PH402 SKU 200 & RTX 2080Ti & RTX 2080Ti & RTX 2080Ti \\
\bottomrule[0.95pt]
\end{tabular}
\end{table}
\end{center} 
\section{Additional Implementation Details}\label{app:impl_detail}
\subsection{Loss Function}
In this section, we detail the loss functions in our implementation of \textit{contrastive} models, \textit{generative} models, and our \recon\ models. Given a randomly sampled multimodal minibatch  with  paired samples, where  is the -th 3D point cloud sample, and ,  are the corresponding multimodal views, \ie, rendered RGB image (rendered from the single view of the default pose) and text category description (\eg, \texttt{chair}), respectively.
By dividing the text category descriptions into  groups  where  is the -th unique text description, we obtain a fine-grained label set  of ShapeNet. Hence, the minibatch becomes , and we can use part of them or all of them to perform different self-supervised representation learning methods.

\textbf{Single-Modal Contrastive Loss}~
For the single-modal contrastive models (SMC), we use a \textit{supervised contrastive loss} following~\citet{SCL20}, where the supervision is generalized from the constructed label set.
Let  be the index of the minibatch samples where  is the index set for all samples other than the -th \textit{anchor} sample.  is the index for the \textit{positive} samples, and other samples with different labels are considered as the \textit{negative} samples.
Given the 3D student network , where .
The loss  can be written as:

where  is the scalar temperature parameter which is set as 0.1~\citep{SCL20} for all contrastive models.

\textbf{Cross-Modal Contrastive Loss}~
For the cross-modal contrastive models (CMC), we use the contrastive InfoNCE loss~\citep{InfoNCE} following~\citet{CLIP}.
Given the minibatch, we can construct two groups of  pairs, \ie, point-image pairs  and point-text pairs .
Then the encoded representation of another modality from the same sample is used as \textit{positive}, while the others in the minibatch are used as \textit{negative} samples. 
Given a pretrained 2D teacher network  and a pretrained language teacher network , where  and  represent the decoded representations, respectively.
Similar to Eq. (\ref{eq:smc_loss}), the loss  is the summation of multimodal point-image loss  and point-text loss :

where  is the scalar temperature parameter, and  is the \textit{stop-gradient} operation. Here, no gradient is back-propagated to the image or text teachers, and hence the two cross-modal teachers are \textit{frozen}.

\textbf{Masked Point Modeling Reconstruction Loss}~
For the masked point modeling reconstruction (MPM), we use the  Chamfer-Distance~\citep{ChamferDistance17} following~\citet{PointMAE}.
Denote  as the masking operation. Let  and  be the reconstructed point clouds and ground truth point clouds, respectively. 
The reconstruction loss  can be written as:


\textbf{\recon\ Loss}~
The \recon\ loss is the ensemble distillation as defined in \cref{eq:ensemble_kd}. The first term  is the same as \cref{eq:mpm}, and the contrastive loss can be either single-modal contrastive loss defined in \cref{eq:smc_loss} or the cross-modal contrastive loss defined in \cref{eq:cmc_loss}.
Similar to SimSiam~\citep{SimSiam}, we find that the 3D student learning from the \textit{frozen} cross-modal teachers with \textit{stop-gradient} does not fall into the representation collapsing trap.
Therefore, we use the positive-only representation learning with Smooth  loss , which we find achieves the best performance (see Sec.~\ref{sec:ablation}). In this case, the cross-modal contrastive term  can be written as follows:


\subsection{Experimental Details}
\textbf{Pretraining Details}~
We use ShapeNetCore from ShapeNet~\citep{ShapeNet15} as the pretraining dataset. ShapeNet is a clean set of 3D CAD object models with rich annotations, including 51K unique 3D models from 55 common object categories. 
For the generation of paired data, we take surface point samples of the 3D object model to generate 3D point clouds, the 3D models are lighted with textures for rendering 2D RGB images. 
Specifically, we use the MacOS Preview\footnote{\url{https://en.wikipedia.org/wiki/Preview_(macOS)}} to generate high-quality rendered images. 
The text description comprises category labels and manually designed prompt templates.
During pretraining, a unique image query \texttt{[IMG]} token and text query \texttt{[TXT]} token are trained to align the global representation from the image teacher and the text teacher, respectively. 
The overall pretraining includes 300 epochs, and we use a cosine learning rate~\citep{CosineLRSGDR} of 5e-4 warming up for 10 epochs. AdamW optimizer~\citep{AdamW19} is used, and the batch size is 128.
More details are shown in \cref{tab:hyper_params}.

\textbf{Downstream Transferring Details}~
\cref{fig:zeroshot} shows the pipeline when \recon\ transfers to downstream tasks, including zero-shot and fine-tuning. For zero-shot, we use simple summation to fuse multi-modal features, and the cosine similarity is used as the classification metric~\citep{CLIP}. 
During fine-tuning, we concatenate the pooled representation of local tokens and the learned global query tokens as model features.
For classification, we add a new global classification \texttt{[CLS]} token and concatenate it with the other queries before being fused to the classification head.
It is worth noting that due to the consistency of the optimization objective during fine-tuning, we cancel the stop gradient of the cross attention connections.
Without specifications, we report overall accuracy (OA) results without voting on the most challenging ScanObjectNN PB\_T50\_RS benchmark using 2,048 input points and ModelNet40 using 1,024 input points (1k P), and the zero-shot classification results are reported on the \textit{test} split.
More detailed training configurations are shown in \cref{tab:hyper_params}.

\textbf{Model Variants}~
\cref{tab:recon_variants} summarizes the \recon\ model configurations, which are grounded in a similar fashion of ViT variants~\citep{ViT}.
The default version ``\recon" (or \recon-Base) is directly adopted from previous works~\citep{PointMAE,ACT23}, except that the network is configured as two-stream rather than single-stream. We add the smaller ``Tiny" and ``Small" models, which have the same number of layers but with reduced channel dimension.
\vspace{-8pt}

\begin{table}[!t]
\caption{\textbf{Details of \recon\ model variants}. This table format follows~\citet{ViT}.} \label{tab:recon_variants}
\begin{center}
\begin{tabular}{lccccc}
\toprule[0.95pt]
Model & Layers & Hidden size & MLP size & Heads & \#Params\\
\midrule[0.6pt]
\br\recon-Tiny & 12 & 192 & 768 & 3 & 11.4M\\
\br\recon-Small & 12 & 256 & 1024 & 4 & 19.0M\\
\br\recon & 12 & 384 & 1536 & 6 & 43.6M \\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\vspace{-25pt}
\end{table} 

\section{Additional Baselines}\label{app:add_baseline}
We show two additional simple fusion methods~\citep{MTLOverview}, including Vanilla Multi-task Learning and a Two-Tower network. Here, we make an analysis of these two baseline methods.

\textbf{Vanilla Multi-task Learning Fusion}~
As shown in \cref{fig:add_baseline}(a), Vanilla Multi-task Learning directly shares a standard Transformer as the encoder. The input embedding tokens take masked reconstruction as the pretext task, and the global tokens take the global contrast as the pretext task. Vanilla Multi-task Learning doesn't consider the pattern difference issue of the two tasks (see \cref{fig:data_scaling} and \cref{fig:atten_vis}). The transfer performances of Vanilla Multi-task Learning on ScanObjectNN and ModelNet40 are reported in \cref{tab:addition_baseline}. It is observed that this vanilla design leads to limited performance, which only improves the \textit{from scratch} OA by +0.89\% on ScanObjectNN, and no improvement on ModelNet40 is achieved.
This indicates task conflicts, and it is consistent with the analysis in Sec.~\ref{sec:intro} that it is \textit{non-trivial} for joint learning of these two tasks.
\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{fig/src/baseline_fig.pdf}
    \vspace{-15pt}
    \caption{\textbf{Illustration of the vanilla multi-task learning and two-tower network baselines}.
    }\label{fig:add_baseline} 
    \end{center}
\end{figure*} 
\textbf{Two-Tower Network}~
To verify whether the performance improvement of \recon\ comes from the form of a two-tower architecture, we design a simple Two-Tower network, shown in \cref{fig:add_baseline}(b). The Two-Tower network uses standard Transformers as the encoder for masked reconstruction and global contrastive learning, respectively.
During fine-tuning, it concatenates features from both streams for an ensemble (similar to \cref{fig:zeroshot}(b)). 
Clearly, the Two-Tower network doesn't suffer the \textit{pattern difference} issue.
The transfer performances of the Two-Tower network on ScanObjectNN and ModelNet40 are reported in \cref{tab:addition_baseline}. It can be seen that the Two-Tower network brings unsatisfactory performance, \ie, only +3.41\% and +0.5\% accuracy improvements by the \textit{from scratch} baseline. 
In comparison, our \recon\ uses the reconstruction task as guidance for global contrastive learning. 
As a result, \recon\ successfully disentangles the two tasks while preserving both merits, and significantly better improvements are achieved.
\begin{table}[!h]
\vspace{-15pt}
\caption{\textbf{Study of the additional baseline}. Overall accuracy (\%) without voting is reported.} \label{tab:addition_baseline}
\begin{center}
\begin{tabular}{lcc}
\toprule[0.95pt]
Method & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
Vanilla Multi-task Learning & 82.53 & 91.6\\
Two-Tower Network & 85.05 & 92.1\\
\rowcolor{linecolor}\recon & \textbf{90.63} & \textbf{94.1}\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\vspace{-20pt}
\end{table} 
\section{Additional Experiments}\label{app:add_exp}
\subsection{Additional Evaluations}\label{app:add_eva}
\vspace{-5pt}
\textbf{Linear SVM Evaluation}~
Linear SVM evaluation~\citep{MarginSVM92,SLTheory98} can be used to evaluate the discriminative quality of pretrained features~\citep{BenchmarkSSL19}. The results on ModelNet40 are shown in \cref{tab:linear}. It shows that our \recon\ outperforms Point-BERT, which also uses plain Transformers, by a clear margin of +6.0\%. 
Compared to methods using hierarchical Transformers, our \recon\ outperforms PointM2AE~\citep{PointM2AE22} by +0.5\%.
\begin{table}[h!]
\vspace{-10pt}
\caption{\textbf{Linear SVM classification on ModelNet40}. Overall accuracy (\%) without voting is reported.} \label{tab:linear}
\begin{center}
\begin{tabular}{lcc}
\toprule[0.95pt]
Method & Hierachical & ModelNet40\\
\midrule[0.6pt]
\bv Point-BERT~\citep{PointBERT} &  & 87.4\\
\bs OcCo~\citep{OcCo} &  & 89.2\\
\bs CrossPoint~\citep{CrossPoint22} &  & 91.2\\
\bh PointM2AE~\citep{PointM2AE22} &  & 92.9\\
\rowcolor{linecolor}\br \recon  &  & \textbf{93.4}\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table} 
\textbf{3D Part Segmentation}~
To evaluate the geometric understanding performance within objects, we conduct the part segmentation experiment on ShapeNetPart~\citep{ShapeNetPart16}. 
Specifically, we concatenate the cross-modal feature of \recon\ into the global feature and use the same segmentation head as Point-MAE for a fair comparison. 
From \cref{tab:partseg}, it can be observed that \recon\ improves the \textit{from scratch} baseline by +1.4\% and +1.7\% Cls. mIoU and Inst. mIoU, respectively. 
Besides, \recon\ outperforms the SSL counterpart Point-MAE~\citep{PointMAE} by +0.4\% Inst. mIoU. 
It shows that the cross-modal global knowledge from \recon~cross-modal pretraining can still play a certain role in part segmentation.
\begin{center}
\begin{table}[h!]
\vspace{-5pt}
\caption{\textbf{Part segmentation on ShapeNetPart dataset}. The mIoU over all classes (Cls.) and the mIoU over all instances (Inst.) are reported.
 denotes results with our proposed \br \recon-block built backbone architecture.
}
\label{tab:partseg}
\centering
\begin{tabular}{lcc}
\toprule[0.95pt]
Methods & Cls. mIoU (\%) & Inst. mIoU (\%)\\
\midrule[0.6pt]
\bs PointNet~\citep{PointNet}  & 80.4 & 83.7\\
\bs PointNet++~\citep{PointNet++} & 81.9 & 85.1\\
\bs DGCNN~\citep{DGCNN} & 82.3 & 85.2\\
\bs PointMLP~\citep{PointMLP} & 84.6 & 86.1\\
\midrule[0.6pt]
\bv Transformer~\citep{AttentionIsAllYouNeed} & 83.4 & 84.7\\
\br Transformer~\citep{AttentionIsAllYouNeed} & 83.6 & 85.2\\
\bs PointContrast~\citep{PointContrast20} & - & 85.1\\
\bs CrossPoint~\citep{CrossPoint22} & - & 85.5\\
\bv Point-BERT~\citep{PointBERT} & 84.1 & 85.6\\
\bv Point-MAE~\citep{PointMAE} & - & 86.1\\
\br Point-MAE~\citep{PointMAE} & 84.4 & 86.1\\
\bv ACT~\citep{ACT23} & 84.7 & 86.1\\
\rowcolor{linecolor}\br\recon & \textbf{84.8} & \textbf{86.4} \\
\bottomrule[0.95pt]
\end{tabular}
\end{table}
\end{center} 
\textbf{Zero-Shot Recognition on Real-World Dataset}~
In \cref{tab:zeroshot_scan}, we show the zero-shot evaluation results on the real-world ScanObjectNN dataset. 
Though our \recon\ is pretrained on the synthetic dataset ShapeNet, it outperforms PointCLIP and CLIP2Point, which leverage depth images, by a clear margin. 
For example, on the most challenging PB\_T50\_RS benchmark, \recon\ achieves a Top-1 accuracy of 30.5\%, which is +7.2\% and +15.1\% higher than CLIP2Point and PointCLIP, respectively.  

\begin{table}[!h]
\caption{\textbf{Zero-shot 3D object classification on ScanObjectNN dataset}. Top-1 accuracy (\%) is reported. Ensemb. denotes whether to use the ensemble strategy with multiple text inputs.} \label{tab:zeroshot_scan}
\begin{center}
\begin{tabular}{lccccc}
\toprule[0.95pt]
Method  & Backbone & Ensemb. & OBJ\_ONLY & OBJ\_BG & PB\_T50\_RS\\
\midrule[0.6pt]
\bs PointCLIP~\cite{PointCLIP22} & ResNet-50 &  & 21.3 & 19.3 & 15.4 \\
\bv CLIP2Point~\cite{CLIP2Point22} & Transformer &  & 35.5 & 30.5 & 23.3\\
\rowcolor{linecolor1}\br\recon & Transformer &  & \textbf{39.6} & \textbf{38.0} & \textbf{29.5}\\
\rowcolor{linecolor}\br\recon & Transformer &  & \textbf{43.7} & \textbf{40.4} & \textbf{30.5} \\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\end{table} 
\subsection{Additional Ablation Study}\label{app:add_ablation}
\textbf{Data Augmentation}~
We study the impact of using different data augmentations (DA) when fine-tuning \recon\ pretrained models on different downstream tasks, as shown in \cref{tab:augmentation}. The results show that \textit{Rotation} has the best performance improvement on ScanObjectNN, and \textit{Scale\&Translate} has the highest performance improvement on ModelNet40.
Therefore, we use by default \textit{Rotation} on ScanObjectNN and \textit{Scale\&Translate} on ModelNet40 if without specifications.
\begin{table}[!h]
\caption{
\textbf{Ablation study of data augmentations (DA) during fine-tuning}.
We report the fine-tuning overall accuracy (\%) without voting of \recon\ pretrained models.} \label{tab:augmentation}
\begin{center}
\begin{tabular}{lcc}
\toprule[0.95pt]
DA Strategy & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
- & 87.44 & 93.4\\
Scale \& Translate & 87.02 & \cellcolor{linecolor}\textbf{94.1}\\
Jitter & 90.22 & 92.9\\
Rotation & \cellcolor{linecolor}\textbf{90.63} & 92.3\\
Dropout & 87.40 & 93.5\\
Horizontal Flip & 87.27 & 93.6\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table}
 
\textbf{Paired Data Ablation}~
During pretraining, we use the point cloud, rendered image, and text inputs generated from ShapeNet~\citep{ShapeNet15}, which makes the data contain clear matching attributes. 
To verify the dependence of \recon\ on paired data, we shuffle the rendered images under the same category for pretraining. 
The results are shown in \cref{tab:nonpair}. 
It shows that \recon\ has a performance degradation of less than 1\% in fine-tuning tasks and 6.4\% in the zero-shot tasks. 
\begin{table}[!h]
\caption{
\textbf{Ablation study on the paired data during pretraining}. Paired denotes the paired 3D point clouds, rendered images, and category text descriptions; unpaired data denotes data with shuffled images under the same category.
Overall accuracy (\%) without voting is reported. ModelNet40-FT represents the fine-tuning accuracy, and ModelNet40-ZS is the zero-shot result on the \textit{train}+\textit{test} split.} \label{tab:nonpair}
\begin{center}
\begin{tabular}{cccc}
\toprule[0.95pt]
Paired &  ScanObjectNN & ModelNet40-FT & ModelNet40-ZS\\
\midrule[0.6pt]
 & 90.22 & 93.8 & 60.4\\
 & 90.63 & 94.5 & 66.8\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\end{table} 
\textbf{Prompt Ablation}~
For zero-shot evaluation, we construct the prompt by concatenating the prefix prompt \texttt{[P]} with the category text description \texttt{[C]}, followed by a suffix \texttt{[S]}, and the final prompt is \texttt{[P]}+\texttt{[C]}+\texttt{[C]}. 
For example, \texttt{A point cloud of a chair with white background}.
\cref{tab:prompts_ablation} shows the ablation study of results with only the prefix or suffix prompts we used, \ie, 20 prefixes and 4 suffixes.
Note that when we use the ensemble strategy, the results are ensembled by all the combinations of prefixes and suffixes.
The prompts can also be constructed from rendered images by powerful Vison-Language Foundation Models such as BLIP-2~\citep{BLIP223}, which may further bring improvements.
\begin{table}[!t]
\caption{
\textbf{Ablation study on the prompts for zero-shot learning}. \texttt{[C]} denotes the category text description, \texttt{[P]} denotes the prefix prompt, and \texttt{[S]} denotes the suffix prompt. Acc. (\%) represents the ModelNet40 zero-shot Top-1 Accuracy (\%).} \label{tab:prompts_ablation}
\begin{center}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lclc}
\toprule[0.95pt]
\texttt{[P]}+\texttt{[C]} & Acc. (\%) & \texttt{[C]}+\texttt{[S]} & Acc. (\%)\\
\midrule[0.6pt]
\texttt{`'} + \texttt{[C]} & - & \texttt{[C]} +\texttt{`'} & -\\
\texttt{`A '} + \texttt{[C]} & 52.27 & \texttt{[C]} +\texttt{`.'} & 49.11\\
\texttt{`A model of '} + \texttt{[C]} & 53.04 & \texttt{[C]} +\texttt{` with white background.'} & 56.93\\
\texttt{`A model of a '} + \texttt{[C]} & 54.05 & \texttt{[C]} +\texttt{` with black context.'} & 60.57\\
\texttt{`An image of '} + \texttt{[C]} & 57.50 & - & -\\
\texttt{`An image of a '} + \texttt{[C]} & 56.36 & - & -\\
\texttt{`A 3D model of '} + \texttt{[C]} & 55.63 & - & -\\
\texttt{`A 3D model of a '} + \texttt{[C]} & 55.71 & - & -\\
\texttt{`A rendered model of '} + \texttt{[C]} & 56.48 & - & -\\
\texttt{`A rendered model of a '} + \texttt{[C]} & 56.52 & - & -\\
\texttt{`A point cloud of '} + \texttt{[C]} & 52.71 & - & -\\
\texttt{`A point cloud of a '} + \texttt{[C]} & 55.27 & - & -\\
\texttt{`A point cloud model of'} + \texttt{[C]} & 56.65 & - & -\\
\texttt{`A point cloud model of a '} + \texttt{[C]} & 54.46 & - & -\\
\texttt{`A 3D rendered model of '} + \texttt{[C]} & 55.83 & - & -\\
\texttt{`A 3D rendered model of a '} + \texttt{[C]} & 54.78 & - & -\\
\texttt{`A rendered image of '} + \texttt{[C]} & 58.79 & - & -\\
\texttt{`A rendered image of a '} + \texttt{[C]} & 56.48 & - & -\\
\texttt{`A 3D rendered image of '} + \texttt{[C]} & 59.07 & - & -\\
\texttt{`A 3D rendered image of a '} + \texttt{[C]} & 57.70 & - & -\\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\end{table} 
\paragraph{Pretraining Learning Rate Ablation on Contrastive Models}
As discussed before, contrastive models can easily fall into the representation over-fitting trap. We find that it generally leads to a sensitivity to the pretraining learning rate. 
As shown in \cref{tab:lr_ablation}, when using the default learning rate for \recon\ pretraining (\ie, 5e-4), the contrastive model fails to generalize with unsatisfactory results. And when the learning rate is small, where the model learns slowly, a relatively better generalization performance is achieved with improved training stability (see \cref{fig:loss_curve} and the discussions).
We speculate that it is due to the \textit{low-data} challenge, which largely makes the model easily learn \textit{over-fitted} representations. As a result, the contrastive models are very sensitive to the pretraining learning rate, which has to be carefully adjusted. Hence, without specifications, we use this adjusted smaller learning rate for contrastive models.
\begin{table}[!h]
\caption{
\textbf{Ablation study on the pretraining learning rate (LR) of contrastive models}. We show the results of fine-tuned cross-modal contrastive models that are pretrained with different LRs. Overall accuracy (\%) without voting is reported.} \label{tab:lr_ablation}
\begin{center}
\begin{tabular}{cccc}
\toprule[0.95pt]
LR & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
5e-5 & 85.11 & 92.1\\
1e-4 & 86.49 & 92.4\\
5e-4 & 82.48& 90.3\\
1e-3 & 67.45 & 86.3\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\end{table} 
\paragraph{Ablation Study on Freezing Cross-Modal Teachers}
Unlike CLIP~\citep{CLIP}, \recon\ uses frozen cross-modal teachers in contrastive learning to acquire the dark knowledge of other modalities. We show the influence of freezing parameters on downstream tasks in \cref{tab:freeze_ablation}. It can be seen that if the model uses the InfoNCE~\citep{InfoNCE} loss function containing negative pairs, the impact of unfrozen parameters during pretraining is not significant. In contrast, for contrastive learning with only positive pairs, unfrozen parameters will cause serious performance degradation on ScanObjectNN and ModelNet.
\begin{table}[!h]
\caption{
\textbf{Ablation study on the freezing cross-modal teachers}. Freeze denotes whether both image and text teachers are frozen or not. Overall accuracy (\%) without voting is reported.} \label{tab:freeze_ablation}
\begin{center}
\begin{tabular}{cccc}
\toprule[0.95pt]
Freeze & Contrastive Metric & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
 & InfoNCE  & 89.87 & 93.7\\
 & Smooth  & 84.77 & 91.2\\
\midrule[0.6pt]
 & InfoNCE & 90.11 & 93.8\\
 & Smooth  & 90.63 & 94.1\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\end{table} 
\paragraph{Training Costs}
We report the singe-GPU without distributed training GPU hours and GPU memory usage of \recon\ and Point-MAE during pretraining and fine-tuning. We study \recon with three network configurations: \recon, \recon-Small, and \recon, which are described in \cref{tab:recon_variants}. Although the parameter count of \recon\ is almost doubled compared to Point-MAE, the memory consumption is mainly for storing intermediate variables of the model. However, \recon\ only adds three global queries to its intermediate variables compared to Point-MAE. Therefore, the memory consumption of \recon\ has mostly stayed the same compared to Point-MAE.
\begin{table}[!h]
\caption{
\textbf{Training costs comparison}.
We report the single-GPU training GPU hours and GPU memory costs during pretraining on ShapeNet and fine-tuning on ScanObjectNN.} \label{tab:traincost}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.95pt]
Method & \#Params & GFLOPS & Pretrain GPU Hours & Fine-tune GPU Hours & GPU Memory & ScanObjectNN\\
\midrule[0.6pt]
\bv Point-MAE & 22.1M & 4.8 & \textbf{32.7h} & 5.3h & 7766MB & 85.18\\
\rowcolor{linecolor1}\br\recon-Tiny & 11.4M & \textbf{2.4} & 40.9h & \textbf{3.8h} & \textbf{4578MB} & 89.10\\
\rowcolor{linecolor2}\br\recon-Small & 19.0M & 3.2 & 46.3h & 4.5h & 4710MB & 89.52\\
\rowcolor{linecolor}\br\recon & 43.6M & 5.3 & 54.5h & 6.1h & 7906MB & \textbf{90.63}\\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\end{table}
 
\paragraph{Edge Device Deployments}
We deploy the models using ONNX\footnote{\url{https://onnx.ai/}} and tested it on several common edge devices, including laptops, pads, smartphones, and single chip microcomputers (SCM), all of which are tested on CPU. \cref{tab:deploy} shows the number of frames per second (FPS) the model can make with inputs from the ModelNet40 dataset, which uses 1K point clouds per sample. The results demonstrate that our \recon\ network is easy to be deployed on various edge devices. For example, our small variant \recon-Tiny is even more efficient that surpasses the widely used PointNet++.
\begin{table}[!h]
\caption{
\textbf{Real-time deployments on edge devices}.
We report the number of frames per second (FPS).} \label{tab:deploy}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.95pt]
\multirow{3}{*}{Device} & Macbook Air & HUAWEI MatePad Pro & Honor X30 Android & Raspberry Pi 3B\\
& Laptop & Pad & Smartphone & Single Chip Microcomputer\\
& Apple M2 Silicon & Hisilicon Kirin 990 & Qualcomm Snapdragon 695 & Broadcom BCM2837\\
\midrule[0.6pt]
\bs PointNet++ & 83.6 & 25.6 & 16.7 & 2.5\\
\bv Point-MAE & 58.7 & 17.8 & 12.8 & 2.1\\
\rowcolor{linecolor1}\br\recon-Tiny & \textbf{103.5} & \textbf{29.8} & \textbf{20.2} & \textbf{3.7}\\
\rowcolor{linecolor2}\br\recon-Small & 78.1 & 24.5 & 14.6 & 2.7\\
\rowcolor{linecolor}\br\recon & 51.7 & 15.6 & 8.1 & 1.8\\
\bottomrule[0.95pt]
\end{tabular}
}
\end{center}
\end{table}
 
\newpage
\section{Discussions on Cross-Modal Teachers and Multimodal Training}
In \cref{tab:teacher_study}, we conduct an ablation study on \textit{pretrained teachers} and \textit{multimodal data} during pretraining. 
This analysis clearly demonstrates that 
(i) \recon+SMC that leverages single-modal contrastive learning still exhibits excellent performance on downstream tasks without including any other modality data or pretrained teachers. It achieves an overall accuracy of 89.73\% on ScanObjectNN, which is +1.31\% better than the generative-only baseline Point-MAE and +0.72\% better than the generative method ACT that leverages a pretrained 2D teacher. It demonstrates that our design of generative guidance for contrastive modeling is \textit{critical} and \textit{essential} for combining the merits of these two paradigms, which already yields superior results compared to other methods and has well tackled the raised issues. 
(ii) \recon+CMC (\textit{from scratch}) uses cross-modal contrastive learning on multimodal data while without any pretrained teachers, further bringing an improvement of +0.59\% to a remarkable 90.32\% overall accuracy on ScanObjectNN. It demonstrates that multimodal data is \textit{beneficial} since 3D data are seriously lacking.
(iii) \recon+CMC uses cross-modal contrastive learning with both multimodal data and pretrained teachers, further leading to an improvement of +0.31\% on ScanObjectNN. It demonstrates that pretrained teachers from other modality data and the usage of multimodal data in \recon\ (\textit{not} for other methods) can indeed further improve the performance for tackling the data dessert issue.
(iv) Vanilla Multi-Task Learning and Two-Tower Network baselines that simply transfer cross-modal knowledge from pretraining weights do not produce satisfactory results. We speculate that this is due to the pattern differences issue demonstrated in \cref{fig:data_scaling}, which is precisely the motivation behind our \recon-block. In contrast, our \recon+CMC outperforms these two baselines by a large margin. This shows that the benefits do not merely come from pretrained teachers but rather the fact that \textit{\recon\ design is an effective framework that guides contrastive learning with generative modeling}. It also shows that pretrained teachers are \textit{not} all you need, and the benefits of pretrained teachers or multimodal data can \textit{not} be obtained without our proposed \recon.
\begin{table}[!t]
\caption{
\textbf{Ablation study on pretrained teachers and multimodal data}. Pretrained Teacher denotes whether a pretrained teacher is used, and Multimodal Data denotes whether multimodal data is used during pretraining. SMC and CMC denote single-modal and cross-modal contrastive modeling methods, respectively. All results except the Vanilla Multi-Task Learning and Two-Tower Network baselines are conducted with our proposed \br\recon-block built backbone architecture during fine-tuning for a fair comparison. Overall accuracy (\%) without voting is reported.} \label{tab:teacher_study}
\begin{center}
\begin{tabular}{lcccc}
\toprule[0.95pt]
Method & Pretrained Teacher & Multimodal Data & ScanObjectNN & ModelNet40\\
\midrule[0.6pt]
\multicolumn{4}{l}{\textit{Contrastive} Methods}\\
\midrule[0.6pt]
\br SMC Only &  &  & 81.70 & 91.2\\
\br CMC Only &  & & 82.48 & 91.4\\
\midrule[0.6pt]
\multicolumn{4}{l}{\textit{Generative} Methods}\\
\midrule[0.6pt]
\br Point-MAE~\citep{PointMAE} &  & & 88.42 & 93.5\\
\br ACT~\citep{ACT23} &  &  & 89.01 & 93.5\\
\midrule[0.6pt]
\multicolumn{4}{l}{\textit{Generative} + \textit{Contrastive} Methods}\\
\midrule[0.6pt]
Vanilla Multi-task Learning &  &  & 82.53 & 91.6\\
Two-Tower Network &  &  & 85.05 & 92.1\\
\rowcolor{linecolor1}\br \recon + SMC &  &  & 89.73 & 94.0\\
\rowcolor{linecolor2}\br \recon + CMC (\textit{from scratch}) &  &  & 90.32 & 94.0\\
\rowcolor{linecolor}\br \recon + CMC &  &  & \textbf{90.63} & \textbf{94.1}\\
\bottomrule[0.95pt]
\end{tabular}
\end{center}
\end{table}
 
\section{Limitations and Future Works}
\recon\ is a general multimodal representation learning framework that leverages both merits of contrastive and generative modeling, which is demonstrated effective in 3D but is also general to any other modalities. However, there are some limitations of \recon, which may be two-fold. (i) The first limitation may come from the multimodal data and domain. This paper mainly explores \recon\ in 3D representation learning, and future explorations on multimodal problems like 2D Vision-Language may be intriguing. (ii) Another limitation may come from the architecture design, \ie, the \recon-block proposed in this work. It is our future exploration to extend \recon\ to be architecture-agnostic. 

\section*{Broader Impact}
The proposed \recon\ is a general framework that can be used for not only 3D representation learning but also all multimodal learning problems.
For example, by leveraging large-scale multimodal data like from the web~\citep{LAION5B2022}, one may obtain a foundational VL \recon\ that shares a similar property of CLIP~\citep{CLIP} since a multimodal alignment contrastive learning is used.
Besides, with the rapid development of Large Language Models (LLMs), \recon\ may also enable the potential for leveraging LLM like ChatGPT~\citep{ChatGPT} for LLM-assisted multimodal understanding.
Since \recon\ successfully unifies generative and contrastive modeling in a decent fashion, future applications may also involve AI-generated content (AIGC) but with cross-modal discriminative capability. For example, \recon\ can be trained for generative modeling that could be extended to generate contents based on input text instructions or other modalities.
We hope this work could motivate and facilitate future explorations on representation learning with multimodal or low-data inputs, which is critical for AI deployments in real-life.
However, all the potential impacts of the aforementioned applications should be taken into consideration while developing AI systems in human society.
 
\end{document}
