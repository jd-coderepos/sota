

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{breqn}
\usepackage{textcomp}
\usepackage{multicol}



\usepackage{systeme}
\usepackage[ruled,vlined]{algorithm2e}

\definecolor{RED}{rgb}{0.5,0.1,0}
\definecolor{vert}{rgb}{0.1,0.5,0.2}
\definecolor{BLUE}{rgb}{0,0.2,0.6}
\definecolor{GREEN}{rgb}{0,0.6,0}
\definecolor{PURPLE}{rgb}{0.69,0,0.8}
\definecolor{BLACK}{rgb}{0.2,0.2,0.2}

\definecolor{Green}{rgb}{0.6,0.8,0.6}
\definecolor{Blue}{rgb}{0.1,0.3,0.6}
\definecolor{Orange}{rgb}{0.9,0.7,0.6}

\newcommand{\amine}[1]{\textcolor{blue}{#1}\marginpar{\footnotesize{\textcolor{blue}{am}}}}

\newcommand{\abdallah}[1]{\textcolor{red}{#1}\marginpar{\footnotesize{\textcolor{red}{ab}}}}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\def\restr#1#2{\mathchoice
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}}
\def\restraux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}} 
\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rvlambda{{\mathbf{\lambda}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vkappa{{\bm{\kappa}}}
\def\vtheta{{\bm{\theta}}}
\def\vlambda{{\bm{\lambda}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\vsigma{{\bm{\sigma}}}
\def\vdelta{{\bm{\delta}}}
\def\vgamma{{\bm{\gamma}}}
\def\vchi{{\bm{\chi}}}
\def\vphi{{\bm{\phi}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}
\def\mDelta{{\bm{\Delta}}}
\def\mGamma{{\bm{\Gamma}}}
\def\mTheta{{\bm{\Theta}}}
\def\mPhi{{\bm{\Phi}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}


\def\restr#1#2{\mathchoice
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}}
\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} \newcommand{\bQ}{\bar \mQ} \newcommand{\tmW}{\tilde \mW}
\newcommand{\un}{\mathbbm{1}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\tx}{\tilde \vx}
\newcommand{\tz}{\tilde z}
\newcommand{\tmX}{\tilde \mX}
\newcommand{\tmC}{\tilde \mC}
\newcommand{\tm}{\tilde \vm}
\newcommand{\bvm}{\bar \vm}
\newcommand{\bS}{\bar \mS}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\bR}{\bar \mR}
\DeclareMathOperator{\diag}{Diag}
\def\restrictionn#1#2{\mathchoice
              {\setbox1\hbox{}
              \restrictionnaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionnaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionnaux{#1}{#2}}
              {\setbox1\hbox{}
              \restrictionnaux{#1}{#2}}}
\def\restrictionnaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}} 

\let\ab\allowbreak
 




\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{4339} \def\confYear{CVPR 2021}



\begin{document}

\title{Deep Miner: A Deep and Multi-branch Network which Mines Rich\\ and Diverse Features for Person Re-identification}

\author{Abdallah Benzine, Mohamed El Amine Seddik, Julien Desmarais\\
Digeiz AI Lab \\
Ecole Polytechnique\\
}





\maketitle





\begin{abstract}
Most recent person re-identification approaches are based on the use of deep convolutional neural networks (CNNs).  These networks, although effective in multiple tasks such as classification or object detection, tend to focus on the most discriminative part of an object rather than retrieving all its relevant features. This behavior penalizes the performance of a CNN for the re-identification task, since it should identify diverse and fine grained features. It is then essential to make the network learn a wide variety of finer characteristics in order to make the re-identification process of people effective and robust to finer changes. 

In this article, we introduce Deep Miner, a method that allows CNNs to ``mine'' richer and more diverse features about people for their re-identification. Deep Miner is specifically composed of three types of branches: a Global branch (G-branch), a Local branch (L-branch) and an Input-Erased branch (IE-branch). G-branch corresponds to the initial backbone which predicts global characteristics, while L-branch retrieves part level resolution features. The IE-branch for its part, receives partially suppressed feature maps as input thereby allowing the network to ``mine'' new features (those ignored by G-branch) as output. For this special purpose, a dedicated suppression procedure for identifying and removing features within a given CNN is introduced. This suppression procedure has the major benefit of being simple, while it produces a model that significantly outperforms state-of-the-art (SOTA) re-identification methods. Specifically, we conduct experiments on four standard person re-identification benchmarks and witness an absolute performance gain up to 6.5\% mAP compared to SOTA.


\end{abstract}

\section{Introduction}
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\linewidth]{deep_mina.pdf}
\caption{\textbf{Deep Miner Model Architecture.} Given a standard CNN backbone, several branches are created to enrich and diversify the features for the purpose of person re-identification. The proposed Deep Miner model is made of three types of branches: \textbf{(i)} The main branch G (in {\color{Orange} \textbf{orange}}) is the original backbone and predicts the standard \textit{global features} ; \textbf{(ii)} Several Input-Erased (IE) branches (in {\color{Green} \textbf{green}}) that takes as input erased feature maps and predict \textit{mined features}  and ; \textbf{(iii)} The local branch (in {\color{Blue} \textbf{blue}}) that outputs local features  and in which a uniform partition strategy is employed for \textit{part level feature} resolution as proposed by \cite{xie2020learning}. In the global branch, and the bottom IE Branch, attention modules are used in order to  improve their feature representation.}
\label{fig:deep_mina}
\end{figure}


\begin{figure}[htbp]
    \centering
    
    
        \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/7778/img.png}
    \end{subfigure}
   ~
    \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/7778/hmp.png}
    \end{subfigure}
   ~
        \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/7778/hme3.png}
    \end{subfigure}
   ~
          \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/7778/hme4.png}
    \end{subfigure}
   ~
             \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/7778/hmplr.png}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/16616/img.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/16616/hmp.png}
    \end{subfigure}
~
        \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/16616/hme3.png}
    \end{subfigure}
  ~
          \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/16616/hme4.png}
    \end{subfigure}
    ~
             \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/16616/hmplr.png}
    \end{subfigure}

    \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/15945/img.png}
        \caption*{Input}
    \end{subfigure}
   ~
    \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/15945/hmp.png}
        \caption*{Global}
    \end{subfigure}
    \texttildelow 
        \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/15945/hme3.png}
        \caption*{IE 1}
    \end{subfigure}
    ~
          \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/15945/hme4.png}
        \caption*{IE 2}
    \end{subfigure}
  ~
             \begin{subfigure}[t]{0.15\linewidth} \centering \includegraphics[width=\textwidth]{heatmaps/15945/hmplr.png}
        \caption*{Local}
    \end{subfigure}
\caption{\textbf{Feature visualization for three examples.} Warmer color denotes higher value. The global branch (second column) focuses only on some features but ignores other important ones. Thanks to the Input Erased branches (third and fourth columns), Deep Miner discovers new important features (localized by the black boxes). For instance, in the first row, the IE-branches are more attentive to the person pant.  In the second row, they discover some patterns on the coat and get attentive to its cap. In the third row, they find out the plastic bag. The local branch (last column) helps the network to focus on local features such as the shoes of the subject or the object handled by the subject in the second row.}
\label{fig:deep_miner_viss}

\end{figure}


In recent years, person re-identification (Re-ID) has attracted major interest due to its important role in various computer vision applications: video surveillance, human authentication, human-machine interaction etc. The main objective of person Re-ID is to determine whether a given person has already appeared over a network of cameras, which technically implies a robust modelling of the global appearance of individuals. The Re-ID task is particularly challenging because of significant appearance changes -- often caused by variations in the background, the lightening conditions, the body pose and the subject orientation w.r.t. the recording camera.

In order to overcome these issues, one of the main goals of person Re-ID models is to produce rich representations of any input image for person matching. Notably, CNNs are known to be robust to appearance changes and spatial location variations, as their global features are invariant to such transformations. Nevertheless, the aforementioned global features are prone to ignore detailed and potentially relevant information for identifying specific person representations. To enforce the learning of detailed features, attention mechanisms and aggregating global part-based representations were introduced in the literature, yielding very promising results \cite{chen2019mixed, chen2019abd, sun2018beyond}. Specifically, attention mechanisms allow to reduce the influence of background noise and to focus on relevant features, while part-based models divide feature maps into spatial horizontal parts thereby allowing the network to focus on fine-grained and local features.

Despite their observed effectiveness in various tasks, these approaches do not provide ways to enrich and diversify an individual's representation. In fact, deep learning models are shown to exhibit a biased learning behavior \cite{chen2019hybrid, chen2019energy, tamaazousti2019universal}; in the sense that they retrieve sufficiently partial attributes concepts which contribute to reduce the training loss over the seen classes, rather than learning all-sided details and concepts. Basically, deep networks tend to focus on surface statistical regularities rather than more general abstract concepts. This behavior is problematic in the context of re-identification, since the network is required to provide the richest and most diverse possible representations.\\

In this paper, we propose to address this problem by adding Input Erased Branches (IE-Branch) into a standard backbone. Precisely,  an IE-Branch takes partially removed feature maps as input in the aim of producing (that is ``mining'') more diversified features as output (as depicted in Figure \ref{fig:deep_miner_viss}). In particular, the removed regions correspond quite intuitively to areas where the network has strong activations and are determined by a simple suppression operation (see subsection \ref{ss:ie_branch}).  The proposed Deep Miner model is therefore made as the combination of IE-branches with local and global branches. The multi-branch architecture of Deep Miner is depicted on Figure \ref{fig:deep_mina}.\\

The main contributions brought by this work may be summarized in the following items:  We provide a multi-branch model allowing the mining of rich and diverse features for people re-identification. The proposed model includes three types of branches: a Global branch (G-branch), a Local branch (L-branch) and an Input-Erased Branch (IE-Branch); the latter being responsible of mining extra features;
 IE-Branches are constructed by adding an erase operation on the global branch feature maps, thereby allowing the network to discover new relevant features;
 Extensive experiments were conducted on Market1501 \cite{zheng2015scalable}, DukeMTMC-ReID \cite{ristani2016performance}, CUHK03 \cite{li2014deepreid} and MSMT17\cite{wei2018person}. We demonstrate that our model significantly outperforms the existing SOTA methods on all benchmarks.

\section{Related Work}
There has been an extensive amount of works around the problem of people re-identification. This section particularly recalls the main advances and techniques to tackle this task, which we regroup subsequently in terms of different approaches:

\textbf{Part-level features} which essentially pushes the model to discover fine-grained information by extracting features at a part-level scale. Specifically, this approach consists in dividing the input image into multiple overlapping parts in order to learn part-level features \cite{yi2014deep, li2014deepreid}. In the same vein, other methods of body division were also proposed; Pose-Driven Deep Convolutional (PDC) leverages human pose information to transform a global body image into normalized part regions, Part-based Convolution Baseline (PCB) \cite{sun2018beyond} learns part level features by dividing the feature map equally -- Essentially, the network has a 6-branch structure by dividing the feature maps into six horizontal stripes and an independent loss is applied to each strip. Based on PCB, stronger part based methods were notably developed \cite{zheng2019pyramidal, quan2019auto, wang2018learning}. However, theses division strategies usually suffer from misalignment between corresponding parts -- because of large variations in poses, viewpoints and scales. In order to avoid this misalignment, \cite{xie2020learning} suggest to concatenate the part-level feature vectors into a single vector and then apply a single loss to the concatenated vector. This strategy is more effective than applying individual loss to each part-level feature vector. As shall be seen subsequently, we particularly employ this strategy to learn the local branch of our Deep Miner model.



\textbf{Metric learning} methods contribute also to enhance the representations power of Re-ID features as notably shown in \cite{chen2018group, hermans2017defense, sun2017svdnet, ristani2016performance}. For instance, the batch hard triplet loss introduced in \cite{hermans2017defense} retrieves the hardest positive and the hardest negative samples for each pedestrian image in a batch. Moreover, the soft-margin triplet loss\footnote{This loss function is used for our Deep Miner model training.} \cite{hermans2017defense} extends the hard triplet loss by using the \textsc{softplus} function to remove the margin hyper-parameter. Finally, authors in \cite{shen2018deep} improve the training and testing processes by gallery-to-gallery affinities and through the use of a group-shuffling random walk network. 

\textbf{Attention Modules} were also suggested to improve the feature representation of Re-ID models \cite{chen2019mixed, chen2019abd, hou2019interaction, tay2019aanet, si2018dual, li2018harmonious, xu2018attention}. Specifically, a dual attention matching network with inter-class and intra-class attention module that captures
the context information of video sequences was proposed in \cite{si2018dual}. In \cite{li2018harmonious}, a multi-task  model jointly learns soft pixel-level and hard region-level attention to improve the discriminative feature representations. In \cite{xu2018attention}, the final feature embedding is obtained by combining global and part features through the use of pose information to learn attention masks.\\

In the aim of learning fine grained features, a series of works focus on using erasing methods -- in various contexts beyond the Re-ID paradigm. We briefly recall the main works following this approach in the following paragraph and we particularly emphasize that our proposed Deep Miner model relies on a feature erasing approach.

\textbf{Feature erasing methods} were commonly used for weakly-supervised object localization \cite{zhang2018adversarial,choe2019attention, benassou2020hierarchical}. Technically, these methods rely on erasing the most discriminative part to encourage the CNN to detect other parts of an object. Our Deep Miner Model relies on the same principal whereas it fundamentally differs in terms of the targeted purpose and from the implementation standpoint. Indeed, Deep Miner aims to enrich the feature representation of deep neural networks in order to properly distinguish a person from another one. Similar to the proposed Deep Miner model, authors in \cite{chen2020salience} introduce a Salient Feature Extraction unit which suppresses the salient features learned in the previous cascaded stage thereby extracting other potential salient features. Still, our method differs from \cite{chen2020salience} through different aspects:  the erasing operation used in Deep Miner is conceptually simpler -- since it consists in averaging and thresholding operations yielding an erasing mask -- while being very efficient in terms of outcome;  the obtained erasing mask is then multiplied by features maps of the initial CNN backbone allowing to create an Input-Erased branch that will discover new relevant features;  by simply incorporating such simple Input-Erased branches into the standard \texttt{Resnet50} backbone, Deep Miner achieves the same \textit{mAP} in Market1501 as \cite{chen2020salience} which include many other complex modules (e.g. attention modules);  Deep Miner contains a local branch with part level resolution that is absent in \cite{chen2020salience} and the two models differ in the used attention module;  finally, \cite{chen2020salience} integrates a non-local multistage feature fusion which we found unnecessary for Deep Miner to achieve high re-identification performance. 



\section{Proposed Method}
This section presents in more details our proposed method. As previously discussed, Deep Miner aims to enable the learning of more rich and fine grained features in the context of person re-identification. Specifically, Deep Miner relies on a given CNN backbone (e.g., \texttt{Resnet50}), and enriching it with new branches (each of them is described subsequently) to allow the network to mine richer and more diverse features.  The overall architecture of the network is described in Figure \ref{fig:deep_mina}. 


\subsection{Global Branch}
The Global branch (G-branch) corresponds to a standard CNN backbone like \texttt{Resnet50}. This backbone is composed of  convolutional blocks\footnote{As a standard residual network block.}  for \footnote{The notation .}. The output of each block is denoted . We particularly apply a global max pooling to the last convolutional layer (with stride being set to ) yielding to an output vector . The global feature representation of a person is then obtained as a linear transformation of .


\subsection{Input Erased Branch}
\label{ss:ie_branch}
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\linewidth]{erasing-operation.pdf}
\caption{Erasing Operation (\textit{ErO}).}
\label{fig:seo}
\end{figure}
As we discussed earlier, recent works \cite{chen2019hybrid, chen2019energy,tamaazousti2019universal} have  demonstrated that CNNs tend to focus only on the most discriminative parts of an image. In the context of person re-identification, this behavior is problematic since the network may not use important information to predict valuable features to increase person identification. 

To this end though, we propose to add new branches to the initial backbone network to mine a larger diversity of features from the images (yielding our Deep Miner proposed method). Specifically, we add Input-Erased branches (IE-branches) to the initial backbone. An IE-Branch can be added after any convolutional block \footnote{If an attention module is used after block , the IE-branch is added after this attention module.} of the global branch and provide a feature vector in the same way as the global branch. The convolutional block  of the G-branch takes as input the unerased feature maps \footnote{If an attention module is used after block ,  stands for the output of this attention module.}, while the corresponding block in the IE-branch takes as input a partially erased version of it.  An \textit{Erasing Operation (ErO)} (illustrated in Figure \ref{fig:seo}) is particularly applied to the feature maps  in order to obtain the erased features denoted . The \textit{ErO} operation consists first in compressing  through channel-wise average pooling so to get the average features maps . A min-max normalization is then applied to  to obtain . Given a thresholding parameter , an erasing mask  is computed as follows:

where  stands for the pixel intensity at position  of a 2D map . As such, the erasing mask  is simply obtained through averaging and thresholding operations. Furthermore, the erased features  are then obtained by element-wise multiplication between  and each channel of , i.e.,  for  indexing the channels.

As depicted in Figure \ref{fig:deep_mina}, creating an IE branch  at block  of the backbone will result in an additional branch composed of  convolutional blocks which we denote  for , each of them being identical to the corresponding convolutional block of the main branch (G-branch).  In Figure \ref{fig:deep_mina},  an IE-Branch  is created after the block  (bottom IE-Branch). This IE-Branch is composed of the blocks  and  which have the same layers architecture as  and  and are initialised with same pretrained weights. If an attention module (subsection \ref{ss:attention_module}) is added to the backbone, the same attention module is added to the IE-branch. In the same way, another IE-Branch is created after block  . We stress however that the weights are not shared between the different branches during training, so to let the network discover new features. 

Like in the global branch, we apply a global max pooling to the last convolutional layer of an IE Branch  yielding to an output vector  followed by a linear layer. 

Note that a similar erasing operation was already introduced in \cite{chen2020salience}. Nevertheless, the IE-branch in our proposed Deep Miner model differs from \cite{chen2020salience} in two fundamental ways: \textbf{(i)} the erasing mask is computed differently involving only a thresholding parameter . Indeed, \textit{ErO} is simply based on average pooling and thresholding operations, while \cite{chen2020salience} uses a complex salient feature extractor that divides the features maps into several stripes,  the selector guiding each stripe to mine important information; \textbf{(ii)} the erasing operation is performed in a cascaded way at the end of the \texttt{Resnet50} backbone in \cite{chen2020salience} , while it is performed at different stages of the backbone in our Deep Miner model. This notably allows our model to mine new relevant features at different resolutions and semantic levels. 

\subsection{Local Branch}
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.8\linewidth]{local_branch.pdf}
\caption{Local Branch of Deep Miner.}
\label{fig:local_branch}
\end{figure}

While the IE-erased branches help the network to mine more diverse features, these branches are still based on the global appearance of a person. To help the network to mine local and more precise features, a local branch is added to Deep Miner. This branch is placed after the convolutional block  and is composed of the convolutional block  which have the same layers architecture as  and is initialised with the same pretrained weights. Like in the IE-Branches, the weights are not shared between the different branches during training, so to let the network discover new localized features.

As illustrated in Figure \ref{fig:local_branch}, the local branch outputs features maps that are then partitioned into  horizontal stripes. A global average pooling is then applied to each strip to obtain  local features vectors. The  local vectors are then concatenated yielding to an output vector  as performed in \cite{xie2020learning}.  




\subsection{Attention Modules}
\label{ss:attention_module}

Attention modules are commonly used in various deep learning application tasks and specifically in the context of person re-identification. The proposed Deep Miner model is also compatible with attention modules, which notably yield an enhancement of the model ability to retrieve more relevant features. To stress out the effectiveness of attention modules on the proposed method, we implement a simple attention module composed of a Spatial Attention Module (\textsc{Sam}) and a CHannel Attention Module (\textsc{Cham}) which we describe subsequently. Features maps  are first processed by \textsc{Sam}, the result of which is then processed by \textsc{Cham}. The obtained features after \textsc{Sam} and \textsc{Cham} are denoted .

\textsc{Sam} focuses on the most relevant features within the spatial dimension which is essentially based on \cite{chen2019abd, xie2020learning}. Indeed, \textsc{Sam} captures and aggregates related features in the spatial domain. An illustration of this module is depicted in Figure \ref{fig:sam}. The input feature maps  of dimension  corresponding to the output of the convolutional block  are fed into two convolutional layers to get two features maps  and  both of dimension . The tensor  is transposed and reshaped to shape  and  is reshaped to , with . An affinity matrix  is computed by a matrix multiplication between the reshaped tensors  and  denoted respectively as  and . A Softmax activation is then applied to  leading to . After a reshaping operation,  is multiplied (map-wise) by  and a Batch Normalization layer is applied to the resulting tensor followed by a multiplication with a learnable scalar . This parameter adjusts the importance of the \textsc{Sam} transformation. The result is then added to the input  to get , the resulting tensor of the Spatial Attention Module. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{sam.pdf}
\caption{Spatial Attention Module (\textsc{Sam}). }
\label{fig:sam}
\end{figure}




In contrast, \textsc{Cham} as illustrated in Figure \ref{fig:cham}, explores the correlation and the inter-dependencies between
channel features. It is specifically based on the Squeeze-and-Exitation block \cite{hu2018squeeze}. Besides, compared to \cite{hu2018squeeze}, the global average pooling at the beginning of the block is removed to preserve spatial information into the attention block. A convolutional layer is applied to  to obtain feature maps of size  for which a second convolution layer is applied to obtain feature maps of size . A Softmax activation is applied to the result that is then multiplied element-wise by  to obtain the result of \textsc{Cham}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{cham.pdf}
\caption{CHannel Attention Module (\textsc{Cham}).}
\label{fig:cham}
\end{figure}

As depicted in Figure \ref{fig:deep_mina}, the attention module is placed right after the blocks  and  of the G-branch as well as after the block  of IE-branch  (bottom IE branch).






\subsection{Loss Functions}

Each branch  (among global branch, local branch and IE branches) outputs a corresponding feature vector . We apply the sames losses for each feature vector, i.e., an ID loss with label smoothing , a soft margin triplet loss  and a center loss   yielding a global loss for each branch \texttt{Br} as 

where  is an hyper-parameter ( in all our experiments). The ID loss is specifically defined as

with  standing for the number of samples,  denotes the predicted probability for identity , while  stands for the (ground-truth) smoothed label, and is defined as 

where  is the (hard) ground-truth identity and  is a precision parameter ( in practice) used to enforce the model to be less confident on the training set. Note that we apply the BNNeck \cite{luo2019bag} to the feature vector  right before the linear layer predicting the IDs probabilities.  

The soft margin triplet loss  is employed to enhance the final ranking performance of the Re-ID model.  is formally defined as 

\begin{dmath}
     \sum_{i=1}^{P} \sum_{k=1}^{K} \textsc{Softplus} ( \max_{\ell\in [K]} \left\Vert \vf_{\texttt{Br}}(\vx_k^i) - \vf_{\texttt{Br}}(\vx^i_\ell) \right\Vert_2 - \min_{ \substack{j\in [P]\setminus\{i\} \\ \ell\in [K] }} \left\Vert \vf_{\texttt{Br}}(\vx^i_k) - \vf_{\texttt{Br}}(\vx^j_\ell) \right\Vert_2 )
\end{dmath}
where  stands for the number of identities per batch,  is the number of samples per identity,  is the -th image of person  and  is the corresponding predicted feature vector (extracted among the branches of Deep Miner). 

Following \cite{luo2019bag}, the Center Loss  is also considered for training our Deep Miner model. It simultaneously learns a center
for deep features of each identity and penalizes the distances
between the deep features and their corresponding identity
centers. As such, intra-class compactness is increased. The Center Loss is particularly defined as

where  is the center of identity  for the branch .

\section{Experiments}
\subsection{Experimental details} Deep Miner is implemented in PyTorch and trained on a single Nvidia GV100 GPU. All images are resized into  pixels. Random Horizontal flipping and erasing are used during training. Each mini batch consists of  images where  is the number of randomly selected identities and  is the number of samples per identity. We take  and . We employ Adam as the optimizer with a warm-up strategy for the learning rate.  We spent  epochs linearly increasing the learning rate from  to . Then, the learning rate is decayed to 
and  at -th epoch and -th epoch respectively. The model is trained until convergence. The feature vector of an input image produced by Deep Miner corresponds the concatenation of the feature vectors predicted by each branch after application of the BNNeck. A \texttt{Resnet50} backbone is used in all experiments. 

\subsection{Evaluation Metrics}

To compare the performance of the proposed method with previous SOTA methods, the Cumulative Matching Characteristics (CMC) at \textit{Rank-1} and mean Average Precision (\textit{mAP}) are adopted. 
\subsection{Datasets}
We considered four standard re-identification benchmarks, the details of which are provided subsequently.

\textbf{Market1501} \cite{zheng2015scalable} consists of  identities and  images.  images of  subjects are used for training while  images of  subjects are used for testing with  query images and  gallery images. The images are shot by six cameras. The Deformable Part Model is used to generate the bounding boxes \cite{felzenszwalb2008discriminatively}.

\textbf{DukeMTMC-ReID} \cite{ristani2016performance} contains  images of  identities captured by more than  cameras. The training subset contains  identities with  images and the testing subset has other  identities. The gallery set contains  images and the query set contains  images. 

\textbf{CUHK03} \cite{li2014deepreid} contains  identities and a total of   labeled images and  detected captured by two camera views.  identities are used for training and  identities are used for testing. The labeled dataset contains  training images,  gallery and  query images for testing, while the detected dataset contains  images for training,  gallery, and  query images for test.

\textbf{MSMT17} \cite{wei2018person} is the largest and more challenging person Re-ID dataset.  identities and   images are captured by a -camera network ( outdoor and  indoor). Faster RCNN \cite{ren2016faster} is used to annotate the bounding boxes. 

\subsection{Ablation Studies}

To demonstrate the effectiveness of the IE and Local branches on the performance of Deep Miner, we incrementally evaluate each module on \textbf{Market-1501}. First, we evaluate the effect of the IE-branch. 

\textbf{Erasing threshold:} We evaluate a model with a single IE branch and vary the erasing threshold (Figure \ref{tab:erasing_th}).  Compared to the baseline,  adding the IE Branch improves the \textit{mAP} with all the evaluated thresholds. Nevertheless, a careful choice of the optimal value of the threshold is needed for an optimal performance. With a low threshold, too much features are erased and the IE-Branch cannot mine new significant features. With a high threshold, no enough features are removed are the IE-branch does not discover new ones. The highest scores are obtained with  with  \textit{Rank-1} score and  \textit{mAP} score.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.34\textwidth]{figure.pdf}
    \caption{Influence of the erasing threshold  with a single IE-Branch. The IE-Branch is placed after ,  the third \texttt{Resnet50} convolution block. The baseline (only global branch) has  \textit{Rank-1} and  \textit{mAP}.}
    \label{tab:erasing_th}
\end{figure}


\textbf{IE-Branch Position:} We now evaluate the importance of the position of the IE-branch (see Table \ref{tab:erasing_pos}). The IE-branch has a significant impact on the model performance, whatever its position in the CNN backbone. Still, the optimal scores are obtained at a particular position of the backbone. In fact, the best scores are obtained when the IE-branch is placed after the third convolution block with a  \textit{Rank-1} and  \textit{mAP}. 

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{@{}lll@{}}
\toprule
IE Branch Position             & \textit{mAP} & \textit{Rank-1}     \\ \midrule
baseline (only global branch)    &  &     \\
After block 1                    &   &      \\
After block 2                    &  &     \\
After block 3                  &  &     \\ \bottomrule
\end{tabular}
\caption{Influence of IE-Branch position ().}
\label{tab:erasing_pos}
\end{table}

\textbf{Multiple IE-Branches:} In this paragraph we evaluate whether the incorporation of multiple IE-branches into Deep Miner improves the results compared to a single IE-branch. Table \ref{tab:multiple_ie_branches} shows that adding IE-branches after the convolution block  and the convolutions block  or  improves the re-identification performance. Indeed, multiple IE-branches ensure that Deep Miner discovers new features at different semantics levels of the initial backbone. In particular, the optimal scores are obtained by adding IE-branches after the blocks  and . However, adding IE-branches after three different convolutional blocks does not yield a significant improvement compared to the  IE-branches architecture. 

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{@{}lll@{}}
\toprule
IE Branch Positions   & \textit{mAP} & \textit{Rank-1}  \\ \midrule
After block 3           &   & \\
After blocks 2 and 3   &   & \\
After blocks 1 and 3   &  &   \\
After blocks 1,2,3      &  &  \\ \bottomrule
\end{tabular}
\caption{Influence of multiple IE-Branches ().}
\label{tab:multiple_ie_branches}
\end{table}

\textbf{Local Branch:} Now we evaluate the effect of adding a local branch. Table \ref{tab:local_branch} shows that adding this branch to the optimal Multiple IE-branches (obtained by the previous ablation study, i.e., adding IE-branches after blocks  and ) significantly improves the \textit{mAP} ( versus ).

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{@{}lll@{}}
\toprule
Local Branch  & \textit{mAP} & \textit{Rank-1} \\ \midrule
No              &  &  \\
Yes            &  & \\ \bottomrule



\end{tabular}
\caption{Influence of the Local Branch.  IE-branches are placed after blocks  and  ().}
\label{tab:local_branch}
\end{table}

\textbf{Attention Modules:} As previously discussed, attention modules are shown to increase the performance of modern person re-identification models. We therefore evaluate in this part the effect of such modules on Deep Miner. Table \ref{tab:attention_module} shows that the attention module described in Subsection \ref{ss:attention_module} helps the proposed model to mine relevant features with a  absolute \textit{mAP} improvement. 

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{@{}lll@{}}
\toprule
Attention Module  & \textit{mAP}  & \textit{Rank-1} \\ \midrule
No                 &  &  \\
\textsc{Sam}                 &  &  \\
\textsc{Cham}                 &  &  \\
\textsc{Sam} \& \textsc{Cham}            &   &  \\ \bottomrule
\end{tabular}
\caption{Influence of the Attention Module. IE-branches are placed after blocks  and  () and a Local branch is used.}
\label{tab:attention_module}
\end{table}

In the next section, we consider the optimal Deep Miner model obtained through this ablation studies which we compare to State-of-the-art methods in terms of \textit{mAP} and \textit{Rank-1} score. Specifically, we define the Deep Miner model as a \texttt{Resnet50} with two IE-branches placed after blocks  and , a local branch and attention modules (see Figure \ref{fig:deep_mina}).



\subsection{Comparison with State-of-the-art Methods}

\begin{table}[]
\centering
\footnotesize
\begin{tabular}{@{}l|l|ll@{}}
\toprule
Method                                                                 & Backbone    & \textit{mAP}            & \textit{Rank-1} \\ \midrule
Deep Miner & \texttt{Resnet50}    & \textbf{90.40} & \textbf{95.70 } \\ \midrule
 PLR OSNet \cite{xie2020learning}                                                              & \texttt{OSNet}       & 88.90          & 95.60  \\

\texttildelow  HOReID \cite{wang2020high} & \texttt{Resnet50} & 84.9 & 94.2 \\

 SCSN \cite{chen2020salience}                                                                   & \texttt{Resnet50}    & 88.50          & \textbf{95.70}  \\
  ABDNet \cite{chen2019abd}                                                                 & \texttt{Resnet50}    & 88.28          & 95.60  \\
 Pyramid      \cite{zheng2019pyramidal}                                                          & \texttt{Resnet101}   & 88.20          & \textbf{95.70}  \\
DCDS \cite{alemu2019deep}                                                                   & \texttt{Resnet101}   & 85.80          & 94.81  \\
 MHN \cite{chen2019mixed}                                                                    & \texttt{Resnet50}    & 85.00          & 95.10  \\
 MGN \cite{lin2019improving}                                                          & \texttt{Resnet50}    & 86.9           & 95.70  \\
BFE  \cite{dai2019batch}                                                                           & \texttt{Resnet50}    & 86.20          & 95.30  \\
CASN \cite{zheng2019re}                                                                   & \texttt{Resnet50}   & 82.80          & 94.40  \\
AANet \cite{tay2019aanet}                                                                 & \texttt{Resnet152}   & 83.41          & 93.93  \\
 IANet \cite{hou2019interaction}                                                                 & \texttt{Resnet50}    & 83.10          & 94.40  \\
 VPM  \cite{sun2019perceive}                                                                  & \texttt{Resnet50}    & 80.80          & 93.00  \\
\texttildelow  PSE+ECN \cite{saquib2018pose}                                                               & \texttt{Resnet50}   & 80.50          & 90.40  \\
 PCB+RPP \cite{sun2018beyond}                                                               & \texttt{Resnet50}    & 81.60          & 93.80  \\
 PCB \cite{sun2018beyond}                                                                    & \texttt{Resnet50}    & 77.40          & 92.30  \\
\texttildelow  Pose-transfer \cite{liu2018pose}                                                         & \texttt{DenseNet169} & 56.90          & 78.50  \\
\texttildelow  SPReID  \cite{kalayeh2018human}                                                               & \texttt{Resnet152}  & 83.36          & 93.68  \\ \midrule

* RGA-SC \cite{zhang2020relation} & \texttt{Resnet50} & 88.4 & 96.1 \\
SNR \cite{jin2020style} & \texttt{Resnet50} & 84.70 & 94.40 \\
OSNet \cite{zhou2019omni} &\texttt{OSNet} & 84.9 & 94.8 \\
Tricks \cite{luo2019bag}                                                                 & \texttt{SEResNet101} & 87.30          & 94.60  \\
 Mancs \cite{wang2018mancs}                                                                 & \texttt{Resnet50}    & 82.30          & 93.10  \\
PAN  \cite{zheng2018pedestrian}                                                                  & \texttt{Resnet50}    & 63.40          & 82.80  \\
SVDNet \cite{sun2017svdnet}                                                                 & \texttt{Resnet50}    & 62.10          & 82.30  \\ \bottomrule

\end{tabular}
\\ *Attention related, +Stripes Related, \texttildelow Pose or human pose related
\caption{Comparison with state-of-the-art person Re-ID methods
on the Market1501 dataset.}
\label{tab:market}
\end{table}



\begin{table}[]
\centering
\footnotesize
\begin{tabular}{ll|l|l}
\hline
Method                                                                 & Backbone    & \textit{mAP}   & \textit{Rank-1} \\ \hline
Deep Miner & \texttt{Resnet50}   & \textbf{81.80} & 91.20  \\ \hline
 PLR OSNet \cite{xie2020learning}                                 & \texttt{OSNet}       & 81.20 &\textbf{ 91.60}  \\
\texttildelow  HOReID \cite{wang2020high} & \texttt{Resnet50} & 75.60 & 86.90 \\
 SCSN \cite{chen2020salience}                                     & \texttt{Resnet50}    & 79.00 & 91.00  \\
 ABDNet \cite{chen2019abd}                                          & \texttt{Resnet50}    & 78.60 & 89.00  \\
 Pyramid      \cite{zheng2019pyramidal}                             & \texttt{Resnet101}   & 79.00 & 89.00  \\
DCDS \cite{alemu2019deep}                                              & \texttt{Resnet101}   & 75.50 & 87.50  \\
 MHN \cite{chen2019mixed}                                         & \texttt{Resnet50}    & 77.20 & 89.10  \\
 MGN \cite{lin2019improving}                                        & \texttt{Resnet50}    & 78.40 & 88.70  \\
BFE  \cite{dai2019batch}                                               & \texttt{Resnet50}    & 75.90 & 88.90  \\
CASN \cite{zheng2019re}                                           & \texttt{Resnet50}    & 73.70 & 87.70  \\
AANet \cite{tay2019aanet}                                         & \texttt{Resnet50}   & 74.29 & 87.65  \\
 IANet \cite{hou2019interaction}                                    & \texttt{Resnet50}    & 73.40 & 83.10  \\
 VPM  \cite{sun2019perceive}                                      & \texttt{Resnet50}    & 72.60 & 83.60  \\
\texttildelow  PSE+ECN \cite{saquib2018pose}                                      & \texttt{Resnet50}    & 75.70 & 84.50  \\
 PCB+RPP \cite{sun2018beyond}                                       & \texttt{Resnet50}    & 69.20 & 83.30  \\
\texttildelow  Pose-transfer \cite{liu2018pose}                                   & \texttt{Densenet169} & 56.90 & 78.50  \\
\texttildelow  SPReID  \cite{kalayeh2018human}                                    & \texttt{Resnet152}   & 73.34 & 85.95  \\ \hline
SNR \cite{jin2020style} & \texttt{Resnet50} & 72.9 & 84.4 \\
OSNet \cite{zhou2019omni}                                              & \texttt{OSNet}       & 73.50  & 88.60   \\
Tricks \cite{luo2019bag}                                               & \texttt{SeResnet101} & 78.00 & 87.50  \\
 Mancs \cite{wang2018mancs}                                         & \texttt{Resnet50}    & 71.80 & 84.90  \\
PAN  \cite{zheng2018pedestrian}                                        & \texttt{Resnet50}    & 51.51 & 71.59  \\
SVDNet \cite{sun2017svdnet}                                            & \texttt{Resnet50}    & 56.80 & 76.70  \\ \hline
\end{tabular}


*Attention related, +Stripes Related, \texttildelow  Pose or human pose related
\caption{Comparison with state-of-the-art person Re-ID methods
on the DukeMTMC-ReID dataset.}
\label{tab:duke}
\end{table}


\begin{table}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|ll|ll}
\hline
\multirow{2}{*}{Method}                                                & \multirow{2}{*}{Backbone} & \multicolumn{2}{l|}{Labeled} & \multicolumn{2}{l}{Detected} \\ \cline{3-6} 
                                                                       &                           & \textit{mAP}          & \textit{Rank-1}        & \textit{mAP}          & \textit{Rank-1}        \\ \hline
Deep Miner& \texttt{Resnet50}                  & \textbf{84.7 }            & 86.6              & \textbf{81.4}         & 83.5          \\ \hline
 PLR OSNet \cite{xie2020learning}                                 & \texttt{OSNet}                     & 80.5         & 84.6          & 77.2         & 80.4          \\
 SCSN \cite{chen2020salience}                                     & \texttt{Resnet50}                  & 84.00        & \textbf{86.80}         & 81.00        & \textbf{84.70}         \\
 Pyramid      \cite{zheng2019pyramidal}                             & \texttt{Resnet101}                 & 76.90        & 78.90         & 74.80        & 78.90         \\
 MHN \cite{chen2019mixed}                                         & \texttt{Resnet50}                  & 72.40        & 77.20         & 65.40        & 71.70         \\
 MGN \cite{lin2019improving}                                        & \texttt{Resnet50}                  & 67.40        & 68.00         & 66.00        & 68.00         \\
BFE  \cite{dai2019batch}                                               & \texttt{Resnet50}                  & 76.60        & 79.40         & 73.50        & 76.40         \\
CASN \cite{zheng2019re}                                           & \texttt{Resnet50}                  & 68.00        & 73.70         & 64.40        & 71.50         \\
 PCB+RPP \cite{sun2018beyond}                                       & \texttt{Resnet50}                  & -            & -             & 57.50        & 63.70         \\ \hline
OSNet \cite{zhou2019omni}                                              & \texttt{OSNet}                     & -            & -             & 67.8         & 72.3          \\
Tricks \cite{luo2019bag}                                               & \texttt{SeResnet101}               & 70.40        & 72.00         & 68.00        & 69.60         \\
 Mancs \cite{wang2018mancs}                                         & \texttt{Resnet50}                  & 63.90        & 69.00         & 60.50        & 65.50         \\  \hline
\end{tabular}

}
*Attention related, +Stripes Related, 
\caption{Comparison with state-of-the-art person Re-ID methods
on the the CUHK03 dataset with the 767/700 split.}
\label{tab:cuhk}
\end{table}




\begin{table}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|ll}
\hline
Method                                                                 & Backbone    & \textit{mAP}   & \textit{Rank-1} \\ \hline
Deep Miner & \texttt{Resnet50}    & \textbf{67.30 }     & \textbf{85.60 }      \\ \hline
 SCSN \cite{chen2020salience}                                     & \texttt{Resnet50}    & 58.50 & 83.80  \\
 ABDNet \cite{chen2019abd}                                          & \texttt{Resnet50}    & 60.80 & 82.30  \\
BFE  \cite{dai2019batch}                                               & \texttt{Resnet50}    & 51.50 & 78.80  \\
 IANet \cite{hou2019interaction}                                    & \texttt{Resnet50}    & 46.80 & 75.50  \\
GLAD\cite{wang2018learning}                                                               & \texttt{Resnet50}    & 34.00 & 61.40  \\
PDC\cite{su2017pose}                                                                & \texttt{GoogLeNet}   & 29.70 & 58.00  \\ \hline
\end{tabular}

}
*Attention related, +Stripes Related
\caption{Comparison with state-of-the-art person Re-ID methods
on the MSMT17 dataset.}
\label{tab:msmt17}
\end{table}


Now we compare our proposed Deep Miner model with recent SOTA methods to demonstrate its effectiveness and robustness compared to more advanced methods.


\textbf{Market1501:} Table \ref{tab:market} shows the results on the \textbf{Market1501} dataset.  We divide the methods into two groups: methods that uses local features (top of the table) and methods that uses only global features (bottom). Deep Miner significantly outperforms previous methods in terms of the \textit{mAP} score and has the same \textit{Rank-1} score as the SOTA methods SCSN and Pyramid while being conceptually much simpler than the former.
We also stress that Deep Miner surpasses methods that use stronger backbones while we only consider \texttt{Resnet50}. In fact, authors  in \cite{zheng2019pyramidal} and \cite{alemu2019deep} use \texttt{Resnet101} while authors in \cite{tay2019aanet} and \cite{kalayeh2018human} use a \texttt{Resnet152}. Moreover, Deep Miner -- with a local branch that uses only  stripes -- surpasses more complex methods like Pyramid that uses a pyramidal feature set and a large number of stripes. Overall, the proposed Deep Miner model surpasses all SOTA methods of the global features group demonstrating its ability to mine more diverse and richer features for person re-identification. 

\textbf{DukeMTMC-ReID:} Similar to \textbf{Market1501}, results in Table \ref{tab:duke} show that Deep Miner achieves optimal \textit{mAP} scores compared to all SOTA methods on this dataset. 



\textbf{CUHK03:} The results are shown in Table \ref{tab:cuhk}.
Note that this dataset is more challenging than the previous ones in the sense that it contains fewer number of samples and has limited viewpoint variations. 
The proposed Deep Miner model is shown to achieve the optimal \textit{mAP} score compared to SOTA methods. Indeed, Deep Miner exceeds PLR OSNet by  in \textit{mAP} on the labeled dataset and exceeds SCSN by . On the detected dataset, it surpasses PLR OSNet by  and SCSN by . 
These experimental results notably express the benefits of the feature mining method, even under the condition of limited training samples.

\textbf{MSMT17:} Lastly, Table \ref{tab:msmt17} shows the results on the \textbf{MSMT17} dataset, which is the more recent and largest dataset. Note that Deep Miner significantly outperforms all SOTA methods in terms of both \textit{mAP} and \textit{Rank-1} scores. Notably, we obtain a \textit{significant gain} of  \textit{mAP} compared to the best SOTA method on this dataset.

\section{Conclusion}

This paper introduced Deep Miner, a multi-branch network that mines rich and diverse features for person re-identification. This model is composed of three types of branches that are all complementary to each other: the global branch extracts general features of the person; the Input-Erased branches mine richer and more diverse features; the Local branch looks for fine grained details. Specifically, our main insight is to add to a given backbone Input-Erased Branches which take as input partially erased features maps (through a simple erasing strategy) and find new features ignored by the backbone branch. Extensive experiments have demonstrated the effectiveness of the proposed Deep Miner model and its superiority in terms of performance compared to much more complex SOTA methods. It is worth noting that the proposed Deep Miner model makes  \textit{mAP} improvement on the \textbf{MSMT17} dataset -- the largest and more complex existing person re-identification dataset.  


\onecolumn
\begin{multicols}{1}
\vfill\pagebreak
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{multicols}
\end{document}
