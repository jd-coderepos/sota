
\documentclass[fleqn]{llncs}
\usepackage{amssymb}
\usepackage{sdna}

\pagestyle{plain} \raggedbottom

\title{Network Algebra for Synchronous Dataflow}
\author{J.A. Bergstra\inst{1} \and C.A. Middelburg\inst{1} \and 
        Gh. \c{S}tef\u{a}nescu\inst{2}}
\institute{Informatics Institute, Faculty of Science, \\
           University of Amsterdam,
           Science Park~904, 1098~XH Amsterdam, the Netherlands \\
           \email{J.A.Bergstra@uva.nl,C.A.Middelburg@uva.nl}
           \and
           Department of Computer Science, Faculty of Mathematics and
           Computer Science, \\ 
           University of Bucharest, 
           Strada Academiei 14, Bucharest, Romania \\
           \email{gheorghe.stefanescu@fmi.unibuc.ro}}

\begin{document}
\maketitle

\begin{abstract}
We develop an algebraic theory of synchronous dataflow networks.
First, a basic algebraic theory of networks, called BNA (Basic Network 
Algebra), is introduced.
This theory captures the basic algebraic properties of networks.
For synchronous dataflow networks, it is subsequently extended with 
additional constants for the branching connections that occur between 
the cells of synchronous dataflow networks and axioms for these 
additional constants.
We also give two models of the resulting theory, the one based on stream 
transformers and the other based on processes as considered in process 
algebra.
\begin{keywords}
network algebra, dataflow network, synchronous dataflow, 
stream transformer, process algebra
\end{keywords}\begin{classcode}
F.1.1, F.1.2
\end{classcode}
\end{abstract}

\section{Introduction}
\label{introduction}
In this paper we pursue an axiomatic approach to the theory of dataflow
networks.
Network algebra is presented as a general algebraic setting for the
description and analysis of dataflow networks.
A network can be any labelled directed hypergraph that represents some
kind of flow between the components of a system.
For example, flowcharts are networks concerning flow of control and
dataflow networks are networks concerning flow of data.
Assuming that the components have a fixed number of input and output
ports, such networks can be built from their components and (possibly
branching) connections using parallel composition (),
sequential composition () and feedback ().
The connections needed are at least the identity () and
transposition () connections, but branching connections may
also be needed for specific classes of networks -- e.g.\ the binary
ramification () and identification () connections
and their nullary counterparts ( and ) for flowcharts.

An equational theory concerning networks that can be built using the
above-mentioned operations with only the identity and transposition
constants for connections, called BNA (Basic Network Algebra), is
presented.
The axioms of BNA are sound and complete for such networks modulo graph
isomorphism.
BNA is the core of network algebra; for the specific classes of networks
covered, there are additional constants and axioms.
Flowcharts constitute one such class.
BNA is essentially a part of the algebra of flownomials of
C\u{a}z\u{a}nescu and \c{S}tef\u{a}nescu~\cite{CS90} which was developed
for the description and analysis of flowcharts.

In addition to BNA, an extension of BNA for synchronous dataflow
networks is presented.
Process algebra models of BNA and this extension of BNA are given.
These models provide for a very straightforward connection between
network algebra and process algebra.
Unlike process algebra, network algebra is used for describing systems 
as a network of interconnected components.
A clear connection between process algebra and network algebra appears 
to be useful.

For the process algebra models, ACP (Algebra of Communicating Processes)
of Bergstra and Klop~\cite{BK84b} is used, with the silent step and
abstraction, as well as the following additional features: renaming,
conditionals, iteration, prefixing and communication free merge.
Besides, a discrete-time extension of ACP is used to model synchronous 
dataflow networks.

There are strong connections between the work presented in this paper
and other work.
SCAs (Synchronous Concurrent Algorithms), introduced by Thompson and
Tucker in~\cite{TT91}, can be described in the extension of BNA for
synchronous dataflow networks.
In~\cite{BWM94}, Barendregt et al.\ present a model of computable
processes which is essentially a model of BNA; but a slightly different
choice of primitive operations and constants is used.

The paper starts with an outline of network algebra
(Section~\ref{overview}) and some process algebra preliminaries
(Section~\ref{preliminaries}).
Next the signature, the axioms and two models of BNA, including a
process algebra model, are presented (Section~\ref{bna}).
Thereafter the signature, the axioms and two models of the extension of 
BNA for synchronous dataflow networks, including a process algebra 
model, are presented (Section~\ref{na-s}).
Finally, some closing remarks are made (Section~\ref{conclusions}).

The current paper complements~\cite{BMS97a}.
The latter paper is a revision of~\cite{BMS95a} in which the part on 
synchronous dataflow networks has been left out due to space limitations
imposed by the journal.
The current paper is a revision of~\cite{BMS95a} in which the part on 
asynchronous dataflow networks has been left out instead.

\section{Overview of network algebra}
\label{overview}
This section gives an idea of what network algebra is.
The meaning of its operations and constants is explained informally
making use of a graphical representation of networks.
Besides, dataflow networks are presented as a specific class of
networks and the further subdivision into synchronous and asynchronous
dataflow networks is explained in broad outline.
The formal details will be treated in subsequent sections.

\subsection{General}
\label{overview-general}
First the meaning of the operations and constants of BNA
mentioned in Section~\ref{introduction} (, , ,
 and ) is explained and then the meaning of the 
additional constants for branching connections mentioned in 
Section~\ref{introduction} (, ,  and 
) is explained.

It is convenient to use, in addition to the operations and constants of
BNA, the extensions ,  and  of the
feedback operation and the identity and transposition constants.
These extensions are defined by the equations that occur as axioms 
R5--R6, B6 and B8--B9, respectively, of BNA 
(see Section~\ref{bna-sig-axioms}, Table~\ref{tbl-bna}).
They are called the block extensions of the feedback operation and these
constants.
The block extensions of additional constants for branching connections
can be defined in the same vein.

In Figure~\ref{fig-bna}, the meaning of the operations and constants of
BNA (including the block extensions) is illustrated by means of a
graphical representation of networks.
\begin{figure}[tb]
\setlength{\unitlength}{0.005in}
\thicklines
\begin{center}
\begin{minipage}[b]{3.75cm}
\begin{center}
\begin{picture}(70,100)(0,0)
\multiput( 20,100)( 20,  0){3}{\vector( 0,-1){ 30}}
\put( 10, 30){\framebox(60,40){{\em f}}}
\put( 40, 30){\vector( 0,-1){ 30}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{2em}
\begin{minipage}[b]{3.75cm}
\begin{center}
\begin{picture}(70,100)(0,0)
\multiput( 25,100)( 30,  0){2}{\vector( 0,-1){ 30}}
\put( 10, 30){\framebox(60,40){{\em g}}}
\multiput( 20, 30)( 20,  0){3}{\vector( 0,-1){ 30}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\end{center}

\begin{center}
\begin{minipage}[b]{3.25cm}
\begin{center}
\begin{picture}(160,190)(0,-45)
\multiput( 20,100)( 20,  0){3}{\vector( 0,-1){ 30}}
\put( 10, 30){\framebox(60,40){{\em f}}}
\put( 40, 30){\vector( 0,-1){ 30}}
\multiput(105,100)( 30,  0){2}{\vector( 0,-1){ 30}}
\put( 90, 30){\framebox(60,40){{\em g}}}
\multiput(100, 30)( 20,  0){3}{\vector( 0,-1){ 30}}
\put(  0, 20){\dashbox{4}(160,60){}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{1em}
\begin{minipage}[b]{3.25cm}
\begin{center}
\begin{picture}(160,190)(-40,0)
\multiput( 25,190)( 30,  0){2}{\vector( 0,-1){ 30}}
\put( 10,120){\framebox(60,40){{\em g}}}
\multiput( 20,120)( 20,  0){3}{\vector( 0,-1){ 25}}
\multiput( 20, 95)( 20,  0){3}{\vector( 0,-1){ 25}}
\put( 10, 30){\framebox(60,40){{\em f}}}
\put( 40, 30){\vector( 0,-1){ 30}}
\put(  0, 20){\dashbox{4}(80,150){}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{1em}
\begin{minipage}[b]{3.25cm}
\begin{center}
\begin{picture}(160,190)(-30,-25)
\put( 30,140){\vector( 0,-1){ 50}}
\put( 55,110){\vector( 0,-1){ 20}}
\put( 15, 50){\framebox(60,40){{\em g}}}
\multiput( 25, 50)( 20,  0){2}{\vector( 0,-1){ 50}}
\put( 65, 50){\vector( 0,-1){ 20}}
\put( 65, 30){\line( 1, 0){ 20}}
\put( 85, 30){\line( 0, 1){ 80}}
\put( 85,110){\line(-1, 0){ 30}}
\put(  0, 20){\dashbox{4}(100,100){}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\end{center}

\begin{center}
\begin{minipage}[b]{3.75cm}
\begin{center}
\begin{picture}(60,60)(0,0)
\multiput(  0, 60)( 20,  0){4}{\vector( 0,-1){ 60}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{2em}
\begin{minipage}[b]{3.75cm}
\begin{center}
\begin{picture}(60,60)(0,0)
\multiput(  0, 60)( 15,  0){2}{\vector( 3,-4){ 45}}
\put( 60, 60){\vector(-1,-1){ 60}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\end{center}
\caption{Operations and constants of BNA}
\label{fig-bna}
\end{figure}
We write  to indicate that network  has  input ports
and  output ports;  is called the sort of .
The input ports are numbered  and the output ports
.
In the graphical representation, they are considered to be numbered
from left to right.
The networks are drawn with the flow moving from top to bottom.
Note that the symbols for the feedback operation and the constants fit
with this graphical representation.
In Figure~\ref{fig-na}, the meaning of (block extensions of) the
additional constants for branching connections mentioned in
Section~\ref{introduction} is illustrated by means of a graphical 
representation.
\begin{figure}[tb]
\setlength{\unitlength}{0.005in}
\thicklines
\begin{center}
\begin{minipage}[b]{2.00cm}
\begin{center}
\begin{picture}(110,60)(0,0)
\multiput( 30,  0)( 20,  0){3}{
\begin{picture}(40,60)(0,0)
\put(  0, 60){\vector(-3,-4){ 45}}
\put(  0, 60){\vector( 3,-4){ 45}}
\end{picture}
}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{1em}
\begin{minipage}[b]{2.00cm}
\begin{center}
\begin{picture}(110,60)(-25,0)
\multiput( 10, 60)( 20,  0){3}{\vector( 0,-1){ 40}}
\put(  0,  0){\dashbox{4}(60,40){}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{1em}
\begin{minipage}[b]{2.00cm}
\begin{center}
\begin{picture}(110,60)(0,0)
\put(  0, 60){\vector( 3,-4){ 45}}
\put( 90, 60){\vector(-3,-4){ 45}}
\put( 20, 60){\vector( 3,-4){ 45}}
\put(110, 60){\vector(-3,-4){ 45}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\hspace*{1em}
\begin{minipage}[b]{2.00cm}
\begin{center}
\begin{picture}(110,60)(-35,0)
\put(  0, 20){\dashbox{4}(40,40){}}
\multiput( 10, 40)( 20,  0){2}{\vector( 0,-1){ 40}}
\end{picture}
\\ 
\end{center}
\end{minipage}
\end{center}
\caption{Additional constants for branching connections}
\label{fig-na}
\end{figure}
The symbols for these additional constants fit with the graphical
representation as well.

The operations and constants illustrated above allow to represent all
networks (cf.~\cite{Ste86}).
For example,
\begin{center}

\end{center}
where  and , represent a regular network
(some abbreviations are used here: iterated sequential composition
 and parallel
composition to the th 
( times)).
The instance  is illustrated in Figure~\ref{fig-network}.
\begin{figure}[tb]
\setlength{\unitlength}{0.005in}
\begin{center}
\begin{picture}(360,280)(0,0)
\thicklines
\multiput( 5,60)( 0,80){3}{\vector(1, 0){40}}
\multiput(40, 0)(80, 0){4}{
\begin{picture}(80,280)(0,0)
\multiput( 0,40)( 0,80){3}{\framebox(40,40){{\em f}}}
\multiput(40,60)( 0,80){3}{\vector(1, 0){40}}
\multiput(20,40)( 0,80){4}{\vector(0,-1){40}}
\put(20,140){\oval(80,280)[br]}
\put(20,140){\oval(80,280)[tr]}
\end{picture}
}
\end{picture}
\end{center}
\caption{A regular network}
\label{fig-network}
\end{figure}

The graphical illustration of the meaning of the operations and
constants of BNA in Figure~\ref{fig-bna} gives intuitive grounds for
the soundness of the axioms of BNA (see Section~\ref{bna-sig-axioms},
Table~\ref{tbl-bna}) for the intended network model.
Similarly, the illustration of the meaning of the additional constants
for branching connections in Figure~\ref{fig-na} makes most additional
axioms for these constants (see Section~\ref{bna-sig-axioms},
Table~\ref{tbl-na}) plausible.

\subsection{Dataflow networks}
In the case of dataflow networks, the components are also called cells.
The identity connections are called wires and the transposition
connections are viewed as crossing wires.
The cells are interpreted as processes that consume data at their input
ports, compute new data, deliver the new data at their output ports,
and then start over again.
The wires are interpreted as queues of some kind.
The classical kinds considered are firstly queues that deliver data with
a neglectible delay and never contain more than one datum, and secondly
unbounded, delaying queues.
In this paper, they are called {\em minimal stream delayers\/} and
{\em stream delayers}, respectively.
A stream is a sequence of data consumed or produced by a component of a
dataflow network.
A flow (of data) is a transformation of a tuple of streams into a tuple
of streams.
A wire behaves as an identity flow.
If the wire is a stream delayer, data pass through it with a time delay.
If the wire is a minimal stream delayer, data enter and leave it with a
neglectible delay -- i.e.\ within the same time slice in case time is
divided into time slices with the length of the time unit used.

In synchronous dataflow networks, the wires are minimal stream delayers.
Basic to synchronous dataflow is that there is a global clock.
On ticks of the clock, cells can start up the consumption of exactly
one datum from each of their input ports and the production of exactly
one datum at each of their output ports.
A cell that started up with that completes the production of data before
the next tick, and it completes the consumption of data as soon as a
new datum has been delivered at all input ports.
On the first tick following the completion of both, the cell concerned
starts up again.
In order to start the synchronous dataflow network, every cell has, for
each of its output ports, an initial datum available to deliver on the
initial tick.
The underlying idea of synchronous dataflow is that computation takes a
good deal of time, whereas storage and transport of data takes a
neglectible deal of time.
Phrased differently, data always pass through a wire between two
consecutive ticks of the global clock.
So minimal stream delayers fit in exactly with this kind of dataflow
networks.
The semantics of synchronous dataflow networks turns out to be rather
simple and unproblematic.

In asynchronous dataflow networks, the wires are stream delayers.
The underlying idea of asynchronous dataflow is that computation as
well as storage and transport of data takes a good deal of time, which
is sometimes more realistic for large systems.
In such cases, it is favourable to have computation driven by the
arrival of the data needed -- instead of by clock ticks.
Therefore, there is no global clock in an asynchronous dataflow network.
Cells may independently consume data from their input ports,
compute new data, and deliver the new data at their output ports.
Because it means that there may be data produced by cells but not yet
consumed by other cells, this needs wires that are able to buffer an
arbitrary amount of data.
So stream delayers fit in exactly with this kind of dataflow networks.
However, the semantics of asynchronous dataflow networks turns out to
be rather problematic.
The main semantic problem is a time anomaly, known as the
Brock-Ackermann anomaly.
With feedback, timing differences in producing data may become
important and the time anomaly actually shows that delaying queues do
not perfectly fit in with that.
Besides, the unbounded queues needed to keep an arbitrary amount of data
are unrealistic.
Note that a synchronous dataflow network can be viewed as a extreme
case of an asynchronous one, where the queues never contain more than
one datum.

Dataflow networks also need branching connections.
Their branching structure is more complex than the branching structure
of flowcharts.
In case of flowcharts, there is a flow of control which is always at
one point in the flowchart concerned.
In consequence, the interpretation of the branching connections is
rather obvious.
However, in case of dataflow networks, there is a flow of data which is
everywhere in the network.
Hence, the interpretation of the branching connections is not
immediately clear.
In this paper, two kinds of interpretation are considered.
For the binary branching connections, they are the
{\em copy}/{\em equality~test\/} interpretation and the
{\em split}/{\em merge\/} interpretation.
The first kind of interpretation fits in with the idea of permanent
flows of data which naturally go in all directions at branchings.
Synchronous dataflow reflects this idea most closely.
The second kind of interpretation fits in with the idea of intermittent
flows of data which go in one direction at branchings.
Asynchronous dataflow reflects this idea better.
In order to distinguish between the branching constants with these
different interpretations, different symbols for  and
 are used:  and  for the copy/equality~test
interpretation,  and  for the split/merge
interpretation.
Likewise, different symbols for the nullary counterparts  and
 are used:  and  versus  and
.
 and  are called {\em sink\/} and
{\em dummy sink}, respectively; and  and  are
called {\em source} and {\em dummy source}, respectively.


In the synchronous case, with minimal stream delayers as identity
connections and the copy/equality~test interpretation of the branching
connections, it turns out that two axioms for  and 
 are not valid.
Fortunately the others together with two new axioms give a complete set 
of axioms.
The asynchronous case is somewhat problematic owing to the time anomaly
that occurs in the model outlined above.
The asynchronous case is treated separately in~\cite{BMS97a}.

Dataflow networks have been extensively studied, see
e.g.~\cite{BWM94,Boh84,BA81,Bro88,Jon89,Kah74,Kok87,Rus89}.

\section{Process algebra preliminaries}
\label{preliminaries}
This section gives a brief summary of the ingredients of process
algebra which make up the basis for the process algebra models
presented in Sections~\ref{bna} and \ref{na-s}.
We will suppose that the reader is familiar with them.
Appropriate references to the literature are included.

We will make use of \acpt, which is an extension of ACP~\cite{BK84b} 
with abstraction based on branching bisimulation~\cite{GW96}.
In \acpt, processes can be composed from actions, the inactive process 
() and the silent step () by sequential composition 
(), alternative composition (), parallel composition 
(), encapsulation (), and abstraction ().
For a systematic introduction to \acpt, the reader is referred
to~\cite{BW90}.
We will use the following abbreviation.
Let  be a indexed set of process expressions
where .
Then, we write  for
 if  and 
if .

We will also use some of the features added to ACP in~\cite{BB94a}:
\begin{description}
\item[\it Renaming]
We will use the renaming operator .
Here  is a function that renames actions into actions,  or
.
The expression  denotes the process  with every occurrence
of an action  replaced by .
So the most crucial equation from the axioms for the renaming operator is 
.
\item[\it Conditionals]
We will use the two-armed conditional operator .
The expression , is to be read as
.
The most important equations derivable from the axioms for the two-armed 
conditional operator are  and 
.
\item[\it Early input prefixing]
We will use the early input action prefixing operators \sloppy
() and their generalization to a process prefixing 
operator ().
The most important equation derivable from the axioms for the early 
input action prefixing operators is 
 (it is assumed 
that a fixed but arbitrary finite set  of data has been given).
\item[\it Process prefixing]
We will use the process prefixing operator mainly to express parallel input:
.
We have:
\begin{center}
\footnotesize

\end{center}
\item[\it Communication free merge]
We will use the communication free merge operator (). 
This operator is in fact one of the synchronisation merge operators 
 of CSP, which are also added to ACP in~\cite{BB94a}, viz.\  
.
Communication free merge can also be expressed in terms of parallel
composition, encapsulation and renaming.
The most crucial equations from the axioms for the communication free 
merge operator are  and
.
\end{description}

Moreover, we will make use of \acpdrtt, which is an extension of 
\acpdrt\ with abstraction based on branching bisimulation.
\acpdrt\ in turn is an extension of ACP with discrete relative timing.
In \acpdrtt, time is considered to be divided into slices indexed by 
natural numbers.
These time slices represent time intervals of a length which corresponds
to the time unit used.
In \acpdrtt, we have the additional constants  (for each action 
),  and , and the delay operator .
The process  is  performed in any time slice and 
is  performed in the current time slice.
Similarly,  is a silent step performed in the current time
slice and  is inaction in the current time slice.
The expression  denotes the process  delayed one time 
slice.
The process  is recursively defined by the equation
.
In a parallel composition  the
transition to the next time slice is a simultaneous transition of 
.
For example,  will never perform
 because  can neither be delayed nor performed, so
.
However, .
For a systematic introduction to \acpdrtt, the reader is referred
to~\cite{BM02a}.

We will also use the above-mentioned features in the setting of \acpdrtt.
The integration of renaming, conditionals, and communication free merge 
in the discrete time setting is obvious.
The integration of early input prefixing and process prefixing may seem 
less clear at first sight, but the relevant equations are simply
 and
.

\section{Basic network algebra}
\label{bna}
BNA is essentially the part of the algebra of flownomials~\cite{CS90}
that is common to various classes of networks.
In particular, it is common to flowcharts and dataflow networks.
The additional constants, needed for branching connections, differ
however from one class to another.
In this section, BNA is presented.
First of all, the signature and axioms of BNA are given.
The extension of BNA to the algebra of flownomials is also addressed
here.
In addition, two models of BNA are described: a data transformer model
and a process algebra model.
In subsequent sections, an extension of BNA for synchronous dataflow 
networks is provided.

\subsection{Signature and axioms of BNA}
\label{bna-sig-axioms}
\subsubsection*{Signature}
In network algebra, networks are built from other networks -- starting
with atomic components and a variety of connections.
Every network  has a sort , where , associated
with it.
To indicate this,  we use the notation .
The intended meaning of the sort  is the set of networks with
 input ports and  output ports.
So  expresses that  has  input ports and  output
ports.

The sorts of the networks to which an operation of network algebra is
applied determine the sort of the resulting network.
In addition, there are restrictions on the sorts of the networks to
which an operation can be applied.
For example, sequential composition can not be applied to two networks
of arbitrary sorts because the number of output ports of one should
agree with the number of input ports of the other.

The signature of BNA is as follows:
\begin{center}
\footnotesize
\begin{tabular}{l@{\quad}c@{\quad}l}
 Name  & Symbol & Arity \-1.25ex]
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
  &  \\
   &                      \\
 &                  \\
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}} \\
\end{tabular}
\end{center}

\subsubsection*{Axioms}
The axioms of BNA are given in Table~\ref{tbl-bna}.
\begin{table}[tb]
\caption{Axioms of BNA}
\label{tbl-bna}
\rule{.99\textwidth}{.125mm}
\begin{center}
\footnotesize
\begin{tabular}{l@{\quad}ll}
  B1 &  \\
  B2 &  \\
  B3 &   \\
  B4 &  \\
  B5 &  \\
  B6 &  \\
  B7 &  \\
  B8 &  \\
  B9 &  \\
 B10 & 
     & \ \ \  for  \svsp \\
  R1 &  \\
  R2 &  \\
  R3 &   \\
  R4 &  
     & \ \ \  for  \\
  R5 &  \\
  R6 &  \svsp \\
  F1 &  \\
  F2 &  \\
\end{tabular}
\end{center}
\rule{.99\textwidth}{.125mm}
\end{table}
The axioms B1--B6 for ,  and  define a
strict monoidal category; and together with the additional axioms B7--B10
for , they define a {\em symmetric\/} strict monoidal
category (ssmc for short).
The remaining axioms R1--R6 and F1--F2 characterize .
The axioms R5--R6, B6 and B8--B9 can be regarded as the defining
equations of the block extensions of ,  and ,
respectively.

The axioms of BNA are sound and complete for networks modulo graph
isomorphism (cf.~\cite{Ste86}).
Using the graphical representation of Section~\ref{overview-general},
it is easy to see that the axioms in Table \ref{tbl-bna} are sound.
By means of the axioms of BNA, each expression can be brought into a
normal form

where the  ()\footnote{We write , where , for .}
are the atomic components of the network and
 is a bijective
connection.
A network is uniquely represented by a normal form expression up to a
permutation of .
The completeness of the axioms of BNA now follows from the fact that
these permutations in a normal form expression are deducible from the
axioms of BNA as well.

As a first step towards the stream transformer and process algebra
models for synchronous dataflow networks described in 
Section~\ref{na-s}, a data transformer model and a process algebra 
model of BNA are provided immediately after the connection with the 
algebra of flownomials has been addressed.

\subsubsection*{Extension to the algebra of flownomials}
The algebra of flownomials is essentially\footnote{
For naming ports, an arbitrary monoid is used in the algebra of
flownomials whereas the monoid of natural numbers is used in BNA.}
a conservative extension of BNA.
Recall that the algebra of flownomials was not developed for dataflow
networks, but for flowcharts.
The signature of the algebra of flownomials is obtained by extending the
signature of BNA as follows with additional constants for branching
connections:
\begin{center}
\footnotesize
\begin{tabular}{l@{\quad}c@{\quad}l@{\quad}l}
 Name  & Symbol & Arity & Instances \
\begin{array}[t]{lll}
\rmf{}{k+1} & = & \rmfii{} \scomp (\rmf{}{k} \pcomp \idn{})\;, \svsp \\
\idf{k+1}{} & = & (\idf{k}{} \pcomp \idn{}) \scomp \idfii{}\;.
\end{array}
-1.25ex]
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
 &   \\
  &    \\
 &   \\
  &    \\
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}} \\
\end{tabular}
\end{center}
The axioms for the additional constants of the algebra of flownomials
are given in Table~\ref{tbl-na}.
\begin{table}[tb]
\caption{Additional axioms for flowcharts}
\label{tbl-na}
\rule{.99\textwidth}{.125mm}
\begin{center}
\footnotesize
\begin{tabular}{l@{\quad}l}
  A1 &  \\
  A2 &  \\
  A3 &  \\
  A4 &  \1.5ex]
  A9 &  \\
  A10 &  \\
  A11 &  \1.5ex]
  A16 &  \\
  A17 &  \\
  A18 &  \\
  A19 &  \
f \subseteq S^m \x  S^n\;,
-1.25ex]
\multicolumn{3}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
  & 
            & for ,  \\
       & 
            & for ,  \\
       & 
            & for  \vsp \\
     &               \\
   &    \\
\multicolumn{3}{l}{\rule{.99\textwidth}{.125mm}}
\end{tabular}
\svsp \\

\end{center}
\footnotetext
{Let  and  be
 tuples.
 Then we write  for the tuple
 .
 Moreover, we often write  instead of .}
\edfn

The definitions of the operations and constants of BNA given above are 
very straightforward.
Note that the data transformer model defined here has a global crash 
property: if a component of a network fails to produce output, the whole 
network fails to produce output.

\bthm
\label{thm-rel}
 is a model of BNA.
\ethm
\bproof
The proof is a matter of straightforward calculation using only
elementary set theory.
\eproof

Additional branching constants can be defined such that the resulting
expanded model satisfies most axioms of the algebra of flownomials
(Tables~\ref{tbl-bna} and~\ref{tbl-na}).
One such set of branching constants is closely related to the one that
is used in the design of (nondeterministic) SCAs~\cite{TT91}.
The corresponding expanded model is principally the stream transformer
model for synchronous dataflow networks described in Section~\ref{na-s}
where an abstraction is made from the internals of the transformers:
arbitrary data is transformed instead of streams of data.
However,  must be interpreted as  in this data
transformer model, to keep up relationships with SCAs, whereas it is
interpreted as  in the stream transformer model for
synchronous dataflow networks.

\subsection{Process algebra model of BNA}
\label{bna-proc}
Network algebra can be regarded as being built on top of process
algebra.

Let  be a fixed, but arbitrary, finite set of data.
 is a parameter of the model.
The processes use the standard actions ,  and
 for  only.
They stand for read, send and communicate, respectively, datum  at
port .
On these actions, communication is defined such that
\linebreak[2] (for all  and ).
In all other cases, it yields .

We write , where , for the set

and  for .
In addition, we write
 for ,
 for  and
 for .
The abbreviations ,  and  are
used analogously.

 denotes the renaming function defined by

So  renames port  into  in read actions.
 is defined analogously, but renames send actions.
We write  for
 and
 for .
The abbreviations  and  are
used analogously.
\bdfn (process algebra model of BNA)
\label{dfn-proc}

\noindent
A {\em network\/}  is a triple

where  is a process with actions in
.
 denotes the indexed family of sets
.

A {\em wire\/} is a network , where 
satisfies for all networks  and : 
\begin{center}
\begin{tabular}{lll}
(P1) &
 \vsp \\
(P2) &

     & for all  \vsp \\
(P3) &

     & for all  
\end{tabular}
\end{center}
where .
\pagebreak[2]


The operations and constants of BNA are defined on  as follows:
\begin{center}
\small
\begin{tabular}{lll}
\multicolumn{3}{l}{Notation} \-1.25ex]
\multicolumn{3}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
(m,n,P) \pcomp (p,q,Q)  & = & (m+p,n+q,R)\;, \\
\multicolumn{3}{l}{
\mbox{where } R = P \cfm \rnm{in([p]/m+[p])}(\rnm{out([q]/n+[q])}(Q))}
\vsp \\
(m,n,P) \scomp (n,p,Q)  & = & 
(m,p,\abstr{I(u+[n],v+[n])}(\encap{H(u+[n],v+[n])}(R)))\;, \\ 
\multicolumn{3}{l}{\mbox{where } u = \max(m,p), v = u + n, \mbox{ and}} \\
\multicolumn{3}{l}{
R = (\rnm{out([n]/u+[n])}(P) \cfm \rnm{in([n]/v+[n])}(Q)) \parc
    w^{u+1}_{v+1} \parc \dots \parc  w^{u+n}_{v+n}} 
\vsp \\
(m+p,n+p,P) \feed{p}    & = & 
(m,n,\abstr{I(u+[p],v+[p])}(\encap{H(u+[p],v+[p])}(R)))\;, \\
\multicolumn{3}{l}{\mbox{where } u = \max(m,n), v = u + p, \mbox{ and}} \\
\multicolumn{3}{l}{
R = \rnm{in(m+[p]/v+[p])}(\rnm{out(n+[p]/u+[p])}(P)) \parc
    w^{u+1}_{v+1} \parc \dots \parc  w^{u+p}_{v+p}}
\end{array}

\begin{array}{l@{}l@{}ll@{}l@{}ll}
\idn{n} & \Eq & 
(n,n,w^1_1 \cfm \dots \cfm w^n_n)                      & \mbox{if } n > 0 \\
& & 
(0,0,\abstr{I(1,2)}(\encap{H(1,2)}(w^1_2 \parc w^2_1))) & \mbox{otherwise}
\vsp \\
\tr{m}{n} & \Eq & 
(m+n,n+m,
 w^1_{n+1} \cfm \dots \cfm w^m_{n+m} \cfm
 w^{m+1}_1 \cfm \dots \cfm w^{m+n}_n)              & \mbox{if } m + n > 0 \\
& &
(0,0,\abstr{I(1,2)}(\encap{H(1,2)}(w^1_2 \parc w^2_1))) & \mbox{otherwise}
\\
\multicolumn{7}{l}{\rule{.99\textwidth}{.125mm}}
\end{array}
\idn{0} \pcomp f = f = f \pcomp \idn{0}\idn{m} \scomp f = ff = f \scomp \idn{n}\abstr{I(1,2)}(\encap{H(1,2)}(w^1_2 \parc w^2_1))\dead\tau \seqc \dead\csl{\tau} \seqc \dead(\proc(D),\pcomp,\scomp,\feed{},\idn{},\tr{}{})\idn{n}\msd\cp{m}m  \to 2m\ssink{m}m  \to  0\eq{m}2m \to  m\asour{m}0  \to  m\cp{m}\ssink{m}\eq{m}\asour{m}\ssour{m}(\eq{m} \pcomp \idn{m}) \scomp \eq{m}
        = (\idn{m} \pcomp \eq{m}) \scomp \eq{m}\tr{m}{m} \scomp \eq{m} = \eq{m}^\circ(\asour{m} \pcomp \idn{m}) \scomp \eq{m}
        = \ssink{m} \scomp \asour{m}\eq{m} \scomp \ssink{m} = \ssink{m} \pcomp \ssink{m}\cp{m} \scomp (\cp{m} \pcomp \idn{m})
        = \cp{m} \scomp (\idn{m} \pcomp \cp{m})\cp{m} \scomp \tr{m}{m} = \cp{m}\cp{m} \scomp (\ssink{m} \pcomp \idn{m}) = \idn{m}\asour{m} \scomp \cp{m} = \asour{m} \pcomp \asour{m}\asour{m} \scomp \ssink{m} = \idn{0}\eq{m} \scomp \cp{m}
               = (\cp{m} \pcomp \cp{m}) \scomp
                 (\idn{m} \pcomp \tr{m}{m} \pcomp \idn{m}) \scomp
                 (\eq{m} \pcomp \eq{m})\cp{m} \scomp \eq{m} = \idn{m}\asour{0} = \idn{0}\asour{m+n} = \asour{m} \pcomp \asour{n}\eq{0} = \idn{0}\eq{m+n}
         = (\idn{m} \pcomp \tr{n}{m} \pcomp \idn{n}) \scomp
           (\eq{m} \pcomp \eq{n})\ssink{0} = \idn{0}\ssink{m+n} = \ssink{m} \pcomp \ssink{n}\cp{0} = \idn{0}\cp{m+n}
         = (\cp{m} \pcomp \cp{n}) \scomp
           (\idn{m} \pcomp \tr{m}{n} \pcomp \idn{n})\eq{m} \feed{m} = \ssink{m}\cp{m}\feed{m}  = \asour{m}^\circ((\idn{m} \pcomp \cp{m}) \scomp
         (\tr{m}{m} \pcomp \idn{m}) \scomp
         (\idn{m} \pcomp \eq{m})) \feed{m}
        = \ssink{m} \scomp \asour{m}^\circ^\circ\rel(S)D\tick \notin Dx \in Sk \in \Natx(0..k)xk + 1x(k)xkx(k) \in D\tickx(k) = \tickxk\eq{1}kf \in \rel(S)(m,n)
\begin{array}[t]{l}
\forall x  \in S^m  \st  \forall x' \in S^m  \st {} \\ \quad
  \set{y(0) \where y \in S^n, \pair{x}{y} \in f}
  = \set{  y'(0) \where y' \in S^n, \pair{x'}{y'} \in f} \And {} \\ \quad\,
 \forall k  \in \Nat \st x(0..k) = x'(0..k) \hsp \Implies {} \\ \qquad\,
  \set{y(0..k+1) \where y \in S^n, \pair{x}{y} \in f}
  = \set{  y'(0..k+1) \where y' \in S^n, \pair{x'}{y'} \in f}\;.
\end{array}
\idn{}\tr{}{}\cp{}\eq{}\idn{}\tr{}{}i \in [m]j \in [n]f \in {\sf \rel }(S)(m,n)
\begin{array}[t]{l}
\forall (x_1,\dots,x_m) \in S^m \st
\forall (y_1,\dots,y_n) \in S^n \st {} \\ \hsp
 \pair{(x_1,\dots,x_m)}{(y_1,\dots,y_n)} \in f \hsp \Implies \hsp
 x_i = y_j\;.
\end{array}
dc(f)\set{(i,j) \where
      i \mbox{ is directly connected with } j \mbox{ via } f}f \in \rel(S)(m,n)\rel(S)(m,n)
\begin{array}[t]{l}
h \scomp (\idn{k} \pcomp \cp{m-(k+l)} \pcomp \idn{l}) \scomp
(f \pcomp g) \scomp
(\idn{k'} \pcomp \eq{n-(k'+l')} \pcomp \idn{l'}) \scomp h'\;,
\end{array}
f  \in \rel(S)(m-l,n-l')g  \in \rel(S)(m-k,\linebreak[2]n-k')h  \in \rel(S)(m,m)h' \in \rel(S)(n,n)\cp{n} \in \rel(S)(n,n+n)\eq{n} \in \rel(S)(n+n,n)\rel(S)\qrel(S)\qrel(S)\qfn(S)\qfn(S)\qrel(S)SS = \iseqof{(D \union \set{\tick})}D\rel(S)\qfn(S)\qrel(S)\qfn(S)\qrel(S)\rel(S)\rel(S)\qfn(S)\qrel(S)\rel(S)\qrel(S)\qrel(S)\rel(S)\qrel(S)f \feed{p}\in \qrel(S)(m,n)f \in \qrel(S)(m+p,n+p)
\begin{array}{lll}
\multicolumn{3}{l}{\mbox{Definition}} \-1.25ex]
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
      &            \\
   &             \\
      &            \\
   &             \\
\multicolumn{2}{l}{\rule{.99\textwidth}{.125mm}}
\end{tabular}
\svsp \\
(x \eqt y)(k) = x(k)x(k) = y(k)(x \eqt y)(k) = \tick
\end{center}
\edfn

In Definition~\ref{dfn-rel}, the feedback operation was defined such
that, for each data transformer , the feedback loop behaves as the
greatest fixpoint of  relative to the input stream of .
In case of proper stream transformers, there is always a unique
fixpoint provided the transformer is a function or a continuous
relation (with respect to the prefixes of streams).
It means that the feedback loop is also the least fixpoint.
This is needed to model feedback in synchronous dataflow networks
properly; for otherwise it does not agree with the operational
understanding that it is iteratively feeding the network concerned with
data produced by it in the previous step.
The adaptation of the feedback operation given in
Definition~\ref{dfn-qrel} is needed to get a unique fixpoint in
case of quasiproper stream transformers as well.
It also guarantees that  is closed under feedback.
Because  now produces a dummy stream, it equals the
dummy source.
For this reason,  is used instead of  as constant
for synchronous dataflow.
Note that this stream transformer model does not have the global crash
property of the data transformer model from Section~\ref{bna-rel}: if a
component of a network fails to produce output on some tick of the
global clock, the effect is merely that the components connected to the
port(s) concerned will fail to produce output on some future tick.


\bthm
 is a model of BNA.
The constants  satisfy the additional
axioms for synchronous dataflow networks (Table~\ref{tbl-na-s}).
\ethm
\bproof
For the first part, it is enough to prove R1--R4 and F1--F2.
According to~\cite{CS88,CS89}, it suffices to prove R1--R4 for ,
and R4 additionally for  and .
The proofs concerned are straightforward proofs by case distinction --
the cases depending on whether the ports relevant to the feedback loop
are directly connected or not.
The second part is a matter of tedious, but simple calculation.
\eproof

\subsection{Process algebra model for synchronous dataflow}
\label{na-s-sproc}
In this subsection, the specialization of the process algebra model of
BNA (Section~\ref{bna-proc}) for synchronous dataflow networks is given.
In this case, we will make use of \acpdrtt.
Recall that \acpdrtt\ is \acpdrt\ -- the discrete relative time
extension of ACP -- extended with abstraction based on branching
bisimulation.

In Section~\ref{bna-proc}, only a few assumptions about wires and atomic
cells were made.
Here it is first explained how these ingredients are actualized for
synchronous dataflow networks.
Because of the crucial role of the time slices determined by the ticks
of a global clock, discrete-time process algebra is used.
\bdfn (wires and atomic cells in synchronous dataflow networks)
\label{dfn-wires-cells-s}

\noindent
In the synchronous case, the identity constant, called the
{\em minimal stream delayer}, is the wire 
where  is defined by

The constants , for , and  are defined by
the equations occurring as axioms B6 and B8--B9, respectively, of
Table~\ref{tbl-bna}.

In the synchronous case, the deterministic cell computing a function
, and having  as its
initial output tuple, is the network 
where  is defined by
\begin{center}

\end{center}
The non-deterministic cell computing a (finitely branching) relation
, and having  as its set of
possible initial output tuples, is the network 
where  is defined by
\begin{center}

\end{center}

The restriction of  to the processes that can be built under
this actualization is denoted by .
\edfn

The definition of  given above expresses the following.
The process  waits until a datum is offered at its input port.
When a datum is available at the input port,  delivers the datum
at its output port in the same time slice.
{From} the next time slice, it proceeds with repeating itself.

\noindent
The definition of  expresses the following.
In the current time slice  produces the data
 at the output ports , respectively.
In parallel,  waits until one datum is offered at each of
the input ports .
The waiting may last into subsequent time slices.
When data are available at all input ports,  proceeds with
repeating itself from the next time slice with a new output tuple, viz.\
the value of the function  for the consumed input tuple.
The non-deterministic case () is similar.

For , the operations and constants of BNA as defined on
 can be taken with  as wire.
This means that only the additional constants for synchronous dataflow
have to be defined.

\bdfn (process algebra model for synchronous dataflow)
\label{dfn-sproc}

\noindent
The operations , ,  on  are the
instances of the ones defined on  for  as wire.
Analogously, the constants  and  in  are
the instances of the ones defined on  for  as wire.

The additional constants in  are defined as follows:
\begin{center}
\footnotesize
\begin{tabular}{ll}
\multicolumn{2}{l}{Notation} \-1.25ex]
\multicolumn{4}{l}{\rule{.99\textwidth}{.125mm}} \svsp \\
\cp{1}    & = & (1,2,copy^1),   &
\multicolumn{1}{l}{\mbox{where }
 copy^1 =
  \csl{\tau} \seqc
  (\asl{er}_1(x) \pref (\csl{s}_1(x) \parc \csl{s}_2(x))) \seqc
  \delay(copy^1)} \\
\ssink{1} & = & (1,0,sink^1),   &
\multicolumn{1}{l}{\mbox{where }
 sink^1 =
  \csl{\tau} \seqc
  (\asl{er}_1(x) \pref \csl{\tau}) \seqc \delay(sink^1)} \\
\eq{1}    & = & (2,1,eq_1),     &
\multicolumn{1}{l}{\mbox{where }
 eq_1 =
  \csl{\tau} \seqc
  (\asl{er}_1(x_1) \pref P_2(x_1) \altc
   \asl{er}_2(x_2) \pref P_1(x_2)) \mbox{ and}} \\
\multicolumn{4}{r}{
 P_i(x) =
      \delay(eq_1) \altc
      \csl{{er}}_i(y) \pref (\csl{s}_1(x) \cond{x = y} \csl{\tau}) \seqc
      \delay(eq_1)
 \mbox{ for }} \\
\asour{1} & = & (0,1,source_1), &
\multicolumn{1}{l}{\mbox{where }
 source_1 = \csl{\tau} \seqc \asl{\dead}} \vsp \\
\multicolumn{4}{l}{
\mbox{for , these constants are defined by the equations
occurring as axioms A12--A19}} \\
\multicolumn{4}{l}{\mbox{in Table~\ref{tbl-na-s}}} \\
\multicolumn{4}{l}{\rule{.99\textwidth}{.125mm}}
\end{array}
\eq{1}eq_1eq_1\ol{\eq{}}_1 = (2,1,\ol{eq}_1)\ol{\eq{}}_1\feed{1}\idn{1} = (1,1,\msd)f = (m,n,P)\sproc(D)\idn{m} \scomp f = f = f \scomp \idn{n}\sproc(D)\idn{n} \scomp \idn{n} = \idn{n}\tr{m}{n} \scomp \idn{n} = \tr{m}{n} = \idn{m} \scomp \tr{m}{n}\idn{1} \scomp \idn{1} = \idn{1}\idn{1} \scomp \idn{1} = \idn{1}\idn{n}\tr{m}{n}(\sproc(D),\pcomp,\scomp,\feed{},\idn{},\tr{}{})\cp{}\ssink{}\eq{}\asour{}\idn{0} \pcomp f = f = f \pcomp \idn{0}f \in \sproc(D)\iter\,^{\wedge}\,\cp{}\tr{}{}D\Nat\cp{}\ssink{}\eq{}\cp{}\ssink{}\asour{}^*f^*       = \feed{1} f,                  \hsp f : 1+m \to 1+n\mu\mu f     = (f \scomp \rmfii{m}) \feed{m}, \hsp f : n+m \to m^*f^*       = \rmfii{1} \scomp
                (\idn{1} \pcomp
                 (\idfii{1} \scomp f \scomp \rmfii{1}) \feed{1}) \scomp
                \idfii{1},                     \hsp f:1 \to 1\daggerf^\dagger = \feed{m} (\idfii{m} \scomp f), \hsp f : m \to m+n^*f^*g      = \rmfii{1} \scomp
                (\idn{1} \pcomp
                 \feed{1} (\idfii{1} \scomp f \scomp \rmfii{1})) \scomp
                \idfii{1} \scomp g,            \hsp f,g : 1 \to 1$
 & \cite{Kle56}  \\ 
\multicolumn{4}{l}{\rule{.99\textwidth}{.125mm}} 
\end{tabular}
\end{center}

\subsubsection*{Acknowledgements}
The understanding on dataflow computation of the third author was much
clarified by discussions with M.~Broy and K.~St{\o}len.
The first author acknowledges discussions with J.V.~Tucker on SCAs.

\bibliographystyle{splncs03}
\bibliography{NA}

\end{document}
