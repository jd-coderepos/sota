\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}





\newcommand{\km}[1]{\textcolor{blue}{#1}}
\newcommand{\kso}[1]{\textcolor{magenta}{#1}}
\newcommand{\jy}[1]{\textcolor{red}{#1}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\wacvfinalcopy 

\def\wacvPaperID{495} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

\title{Skeleton-based Action Recognition of People Handling Objects
}
\author{Sunoh Kim \hspace{1.5cm} Kimin Yun \hspace{1.5cm} Jongyoul Park \hspace{1.5cm} Jin Young Choi \\
ASRI, Dept. of Electrical and Computer Eng., Seoul National University, South Korea\\
Electronics and Telecommunications Research Institute (ETRI), South Korea\\
{\tt\small \{suno8386, jychoi\}@snu.ac.kr, \{kimin.yun, jongyoul\}@etri.re.kr}
}


\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

\begin{abstract}
In visual surveillance systems, it is necessary to recognize the behavior of people handling objects such as a phone, a cup, or a plastic bag.
In this paper, to address this problem, we propose a new framework for recognizing object-related human actions by graph convolutional networks using human and object poses. 
In this framework, we construct skeletal graphs of reliable human poses by selectively sampling the informative frames in a video, which include human joints with high confidence scores obtained in pose estimation.
The skeletal graphs generated from the sampled frames represent human poses related to the object position in both the spatial and temporal domains, and these graphs are used as inputs to the graph convolutional networks.
Through experiments over an open benchmark and our own data sets, we verify the validity of our framework in that our method outperforms the state-of-the-art method for skeleton-based action recognition. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Human action recognition has attracted extensive attention in recent years, due to its many potential applications in video surveillance, human-robot interaction, and so on. 
Recognizing actions, however, is difficult, not only because of the common challenges of the domain, such as viewpoint changes and occlusions, but also because of the ambiguities of some actions.
Therefore, human action recognition is still one of the most challenging tasks for computer vision.
\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth, bb= 0 0 1500 500]{gcns.pdf}
\end{center}
   \caption{ Action recognition via object-related human poses. 
The pose graph is constructed by using
spatial human poses~(black dots and lines), spatial object poses~(red dots and lines), and temporal connections~(blue lines). 
In spatial and temporal domains, the graph is used as the input to GCNs.
}
\label{fig:gcns}
\end{figure}
To tackle these challenges in human action recognition, many approaches using various modalities like appearance, optical-flows, depth, and skeleton have been proposed.
Among them, pose-based representation using the skeleton simplifies learning of the actions by extracting the relevant high-level features.
Moreover, the pose-based representation suffers relatively little from the intra-class variances, as the action from actor to actor varies less than in other representations~\cite{yao2012coupled}.
The development of low-cost depth sensors such as Microsoft Kinect and pose estimation algorithms~\cite{cao2017realtime,newell2016stacked,pavlakos2017coarse,shotton2011real} provides an easier way to obtain skeletal information.
It also contributes to the success of data-driven methods of skeleton-based action recognition.

The applications of skeleton-based action recognition in real-world scenarios are difficult due to two major problems.
First, conventional studies~\cite{kim2017interpretable, liu2016spatio,shahroudy2016ntu,stgcn2018aaai} for skeleton-based action recognition usually use constrained datasets, assuming that skeletal data are provided. 
Visual information in real-world scenarios, however, is captured in an unconstrained environment where human skeletons are not given.
Thus, body movement should be interpreted through pose estimation algorithms to obtain skeletal data.
Pose estimation is a challenging task, however, due to the rarity of appearance or missing or overlapping body parts.
Given that pose estimation results are not always accurate, it becomes crucial to develop approaches dealing with inaccurate poses. Second, most approaches focusing only on human skeletons ignore contextual information like objects and scenes. 
Most meaningful actions in the real world, though, involve more than one object or person~\cite{moeslund2006survey}.
Therefore, previous methods cannot fully understand the real-world actions.


Motivated by the aforementioned problems, we develop a framework 
for object-related human action recognition based on graph convolutional networks~(OHA-GCN) as depicted in Figure~\ref{fig:gcns}.
To understand the object-related human actions, our method builds the graph structure of both human and object poses.
Human poses are extracted using the pose estimation method~\cite{cao2017realtime}, and object poses are obtained by the pose heatmap and background subtraction.
Using the obtained human and object poses, we model:
 the human pose graph, where the movement of human body is considered; 
 the object-related human pose graph, where the relationship between human and object poses is represented.
Our framework employs the two graphs of human poses and object-related human poses, generated in both the spatial and temporal domains.

To make the graphs more structurally complete, we explore a strategy of selecting informative frames, which discards ambiguous frames.  
After dividing the entire video into segments of equal length, we make a sequence of informative frames by sampling one informative frame from each segment.
Using the confidence scores calculated by the pose estimation algorithm, we can decide which human poses have a more complete structure than the others.
Then, the two types of the graphs are constructed from the sequence of the informative frames for pose-based representation.
Using the human pose graph and object-related human pose graph, OHA-GCN runs in the architecture of the two stream to boost action recognition.
The GCNs in each stream of our framework apply convolution filters to each graph.
The nodes and their neighbors in each graph are applied to the convolutional filters. 
In experiments, our framework is validated using an open benchmark and our own data sets that includes an illegal rubbish dumping~(IRD) dataset, which compiles realistic data in unconstrained environments.









\section{Related works}
\subsection{Graph convolutional neural networks}


Recent advancements in deep neural networks have led to the development of graph convolutional networks~(GCNs) to understand the form of graph structures~\cite{bruna2013spectral,defferrard2016convolutional,duvenaud2015convolutional, kipf2016semi, li2015gated,niepert2016learning}. 
GCNs generalize convolutional neural networks~(CNNs) from low-dimensional grids of images to high-dimensional domains represented by arbitrarily structured graphs.
These tasks are categorized into two main categories: spectral perspective and spatial perspective methods. 
Spectral perspective methods convert graph data into a spectrum and apply CNNs to the spectral domain~\cite{defferrard2016convolutional,duvenaud2015convolutional, li2015gated}.
Different from the spectral perspective methods, spatial perspective methods directly use graph convolutions to define parameterized filters~\cite{bruna2013spectral,niepert2016learning}.
The convolution operation in the spatial perspective resembles the convolution operation on images.
We propose a model using GCNs for action recognition following the concepts of the spatial perspective.

\subsection{Action recognition}
Video-based action recognition is a challenging task because the concepts of the actions are highly abstract and consider both spatial and temporal dimensions.
Conventional deep-learning-based approaches for action recognition rely mainly on 3D CNNs and two-stream CNNs.
The methods based on the 3D CNNs propose a model that directly applies 3D convolution filters to RGB video sequences~\cite{carreira2017quo,ji20133d, tran2015learning}.
3D CNNs, however, have problems in training due to the explosion of parameters, and their performance is only marginally improved compared to the traditional method~\cite{wang2013action}.
Methods based on the two-stream architecture of CNNs are proposed to solve these problems~\cite{feichtenhofer2016convolutional,simonyan2014two, wang2016temporal}.
In these methods, two CNNs are applied to process appearance and motion~(optical flows) independently.
These methods outperform traditional methods using hand-crafted features~\cite{dalal2005histograms, laptev2008learning, wang2013action}.
But, the heavy computation requirements for optical flows become a main limitation of the methods.
Our approach is based only on pose information that can be extracted from RGB images. 
The pose-based representation can reduce computation time and simplify training for the recognition.

\begin{figure*}
\begin{center}
\includegraphics[width = 1.0 \linewidth, bb= 0 0 950 250]{pipeline.pdf}
\end{center}
   \caption{The overall scheme of the proposed framework that includes pipeline modules of tracking and detection for extracting informative human and object poses, pose estimation, graph construction of human and object-related human poses, and GCNs for recognition of object-related human actions.} 
\label{fig:pipeline}
\end{figure*}
\subsection{Skeleton-based action recognition}
Some methods for action recognition are based on skeletal data because human poses are highly relevant to human action.
The methods can be categorized mainly into those based on hand-crafted features and those based on deep-learning features.
The former methods design several features for understanding human joint motions~\cite{fernando2015modeling, vemulapalli2014human, wang2016graph}. 
For example, the human skeleton is represented as a point in the Lie group~\cite{vemulapalli2014human}, encompassing rotations and translations between body parts.
With the recent success of deep learning, many methods for skeleton-based action recognition have been proposed.
Deep-learning-based methods usually use well-established neural models like recurrent neural networks~(RNNs)~\cite{liu2016spatio,shahroudy2016ntu,zhang2017geometric} or CNNs~\cite{ke2017new,kim2017interpretable}.
RNNs use their internal memory to process sequences of input data, making them applicable to skeleton-based action recognition.
Zhang~\textit{et al.}~\cite{zhang2017geometric} select geometric features based on distances between human joints and use an RNN-based model.
CNN-based methods propose applying the CNN directly by considering the skeleton sequence as an image array.
Qiuhong~\textit{et al.}~\cite{ke2017new} transform the skeleton sequence into three clips and learn the spatio-temporal feature based on deep CNNs.
Still, generalizing the CNNs and RNNs to work on graph structures is challenging because they cannot fully understand the graphs. 
Since human poses are represented by a graph structure physically, the methods based on the CNNs and RNNs are limited in their ability to use the poses.
Recently, the GCN-based methods have been suggested to understand the human poses without loss of graph information~\cite{tang2018deep,stgcn2018aaai}.
Yan~\textit{et al.}~\cite{stgcn2018aaai} propose a ST-GCN that applies the GCNs for skeleton-based action recognition.
This method can capture the motion information in dynamic skeleton sequences by constructing spatio-temporal graphs.
However, ST-GCN does not consider how to cope with inaccurate poses and cannot understand contextual information.
By contrast, our method can build the pose graphs with the contextual information using the reliable poses through the informative joint selection.




\section{Object-related  human  action recognition}

\subsection{Overall scheme}
\label{sec:overall}
The overall scheme of the proposed framework is depicted in Figure~\ref{fig:pipeline}. Pose estimation and detection algorithm are applied to patches extracted from RGB video through a tracking algorithm. 
Then, we make a sequence of informative frames by selecting frames containing relatively accurate poses in the patches. 
Using a spatial-temporal graph of human poses and object-related human poses from the informative frames, the GCNs perform action recognition
in the architecture of the two stream. 
We revisit the ST-GCN~\cite{stgcn2018aaai} in Section~\ref{sec:gcn-review}.
We then present how our method detects the objects handled by humans in Section~\ref{sec:obj-detect} and give detailed descriptions of the object-related human action recognition~(OHA-GCN) framework.
Section~\ref{sec:graph-construct} and Section~\ref{sec:GCNs} explain how to construct graphs of object-related human poses and apply the graphs on the GCNs, respectively.
Finally, we present the strategy of selecting informative frames in Section~\ref{sec:inf-select}.
\subsection{ST-GCN \cite{stgcn2018aaai}}
\label{sec:gcn-review}
ST-GCN uses a spatial temporal graph to form hierarchical representation of the human skeleton. 
The spatial temporal graph consists of nodes and edges where each node corresponds to a joint of the human body, and each edge corresponds to the spatio-temporal connection of the node.
In one single frame, they formulate a spatial graph which is represented by human body joints, and the spatial edge of the graph becomes natural connection between human body joints.
Then, temporal edges are made by connecting the corresponding joints in consecutive frames.

The skeletal graph with  joints in one single frame and  frames of a video is represented by , where  is the node set.
In terms of spatial domain, the graph convolution is written as 

where  and  are the input and output feature map, respectively. 
 is the neighbor set of a node , where the 1-distance neighbors of the target node  are considered.
 is the weight function which provides a weight vector to compute inner product with the input feature map.
Because the number of the weight vectors is fixed, strategies to partition  into a fixed number of subsets have to be designed. 
 is a function mapping each node in neighborhood of  to its subset label. 
To prevent the different subsets from unbalancing the output, the cardinality of the subset  is used as the normalizing term.
In ST-GCN, many partitioning strategies are introduced, such as uni-labeling, distance partitioning, and spatial configuration.
Among them, spatial configuration has achieved best performance, as it consider the gravity center of the human body.
This strategy divide the neighboring nodes of target node into three subsets: 
 the target node itself; 
 centripetal subset: the nodes that are closer to the gravity center; 
 otherwise centrifugal subset.

The implementation of the graph convolution~\cite{kipf2016semi} is adopted for ST-GCN. 
 is the  adjacency matrix where a graph structure is represented as matrix form by indicating connections between nodes.
To implement ST-GCN with the spatial configuration partitioning, the Eq.(~\ref{eq:graph_conv}) is transformed into

where the input feature map  is represented as a tensor  dimensions,  being the number of input channels.
Then, the adjacency matrix is divided into three matrices: 
 , the self-connection of each node; 
 , the connections of the centripetal subset; 
 , the connections of the centrifugal subset.
 denotes the weight matrix, where weight vectors of multiple output channels are stacked.
 whose element can be formulated as  is the diagonal node degree matrix of . 
 is set to 0.001 to avoid empty rows in .


\subsection{Detection of objects handled by humans}
\label{sec:obj-detect}

ST-GCN can capture dynamics of human poses by learning both the spatial and temporal patterns of skeletal data. 
ST-GCN has limitations, however, in understanding object-related human actions because it focuses on only the human poses.
For example, the object-related actions, such as dumping, phoning, texting, and smoking require a specific object that acts as a main cue for recognition.
Inspired by the issue, we create a new graph structure to form a hierarchical representation of the human and object poses.
Before constructing the graphs, it is essential to detect the objects a human is handling.

An easy approach is to find objects that overlap with humans through the object detector, but this approach has several problems.
First, objects in the background that are not related to humans, such as parked cars, are regarded as related objects.
To solve this problem, we define a class of objects handled by humans and develop a new detector, but the shapes of the objects handled by humans vary widely.
The objects handled by humans have a variety of shapes and are small compared to humans. 
Hence, conventional detectors cannot easily detect the objects handled by humans.
Several methods have been presented to use detection networks to detect humans, objects, and their interaction~\cite{Hu:2018vj,Gkioxari:2018uv}. 
Still, a pair consisting of a human and an object is required to train the model on each action, and the training operates independently for each frame, not for the whole video.

In this paper, we use a simple and efficient method to find an object handled by a human through the temporal property of a video.
Since CCTV cameras are generally fixed and limited to the motions of Pan-Tilt-Zoom, it is possible to find a moving area effectively through background modeling and camera motion compensation~\cite{Kim:2012bo,Yi:2013gp,Yun:2017jd}.
This moving area includes the areas of both the human and the object carried by the humans. 
Then, object area can then be detected by subtracting the human area from the moving area regardless of the shape of the object. Fortunately, a human area can be obtained by using Openpose, a 2D pose estimation algorithm providing the location of each joint. The algorithm provides not only the location but also the confidence scores of the joints and affinities between joints as a heatmap, which can then be used as a human area.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth, bb= 0 0 700 500]{object-detection.pdf}
\end{center}
   \caption{The procedure for detecting the object handled by a human. A moving area is obtained by background subtraction with camera motion compensation and the human area is found by estimated joint and heatmap. Then, the object area is detected by subtracting the human area from the moving area.
   }
\label{fig:object-det}
\end{figure}

Figure~\ref{fig:object-det} shows the procedure for detecting an object handled by a human.
In order to detect the moving region, we use the ViBE algorithm~\cite{Barnich:2011ks} which efficiently builds the background model by storing a set of pixel values from the past.
Then, through the  algorithm, we get each coordinate of the joint and the joint heatmaps.
We aggregate the heatmap for all joints and all joint association into a human region map, which is subtracted from the moving area.
The regions around each human hand are selected.
Finally, we can construct a graph structure by connecting the center position of this region with the joints of the closest person.

\subsection{Graph for object-related human poses}
\label{sec:graph-construct}
We use our detection algorithm and  to obtain the coordinates of the human joints and objects for creating a graph structure of the human and object poses.
For human skeletal data, the pose estimation algorithm produces 25 joints in a single frame.
We then formulate a human pose graph using all the human joints in a video sequence.
In addition, the object position obtained in Section~\ref{sec:obj-detect} is added to the human pose graph for modeling our object-related human pose graph.
As a result, the object-related human pose graph consists of 26 nodes that represent human and object poses.
We can model a spatial graph of the physical connection between joints in the human body and position of the object in a frame.
The corresponding nodes in consecutive frames are then connected to formulate a temporal graph.


To avoid confusion, we use notation similar to that used in the ST-GCN.
In our skeletal graph  for object-related human poses, the node set is denoted as  with  joints in one single frame. 
 is the number of informative frames sampled from the video.
The details of the informative frames are explained in Section~\ref{sec:inf-select}.
The edge set is , which depicts two subsets for spatial and temporal connection.
The subset for spatial connection is 

where  is the set of the connections for natural human body and the connections between an object and the human body joints.
The subset for temporal connection

connects the same nodes which is temporally consecutive.
An example of our spatial-temporal skeleton graph is shown in Figure~\ref{fig:skeleton_graph}.

We consider how to connect the object with the human body.
Many strategies of the connection are explored empirically, but we discuss three strategies which are representative of many cases in our view.
First, we use the relationship between the whole joint and the object.
This method, however, makes the graph-based network inefficient because the graph includes redundant information.
Thus, we build a graph structure that is denoted as limbs, focusing on the specific body parts used for interactions. 
We also introduce a strategy using only human hands, where most object-related human actions occur.
The three strategies are denoted as follows: 
 Body: connections between all the human joints and the object; 
 Limbs: connections between the human joints belonging to arms or legs and the object; 
 Hands: connections between the human joints of both hands and the object.
Detailed experiments with each strategy are described in Section~\ref{sec:ablation}.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth, bb= 0 0 800 500]{skeleton_graph.pdf}
\end{center}
   \caption{ For the construction of our skeletal graph, all of the dots are used as nodes and all of the lines as edges. In each frame, nodes and edges based on the natural human body are denoted as green.   \textbf{Left:} The graph structure of human poses in terms of both the spatial and temporal domains. Blue lines represent the temporal connections of corresponding nodes in consecutive frames.
   \textbf{Right:} The example of the graph structure for object-related human poses in each frame. 
   A red dot denotes the position of a human-related object. 
   Red lines are the connections between all human body joints and the object.
   }
\label{fig:skeleton_graph}
\end{figure}



\subsection{Two stream GCNs with human and object poses}
\label{sec:GCNs}
Using only the human pose graph is limited to express only human body motion, ignoring human-object interactions.
Based on the graph structure of both human and object-related human poses, we propose a framework of two-stream GCNs called OHA-GCN for object-related human action recognition.
The OHA-GCN can boost the recognition for object-related actions, as the human poses and object-related human poses are jointly used.
Human-related objects do not, however, appear in every scene. 
In this regard, the two-stream GCN is designed to use the object-related human pose stream only if human-related objects are found.
In the absence of the objects, OHA-GCN uses only the human pose stream. If the object is properly detected, OHA-GCN recognizes actions through both the human pose stream and the object-related human pose stream.


The GCN in each stream takes the graph constructed in Section~\ref{sec:graph-construct} as input and produces a video-level prediction for action recognition.
The model of GCN is composed of 9 GCN layers. Each layer has 64, 64, 64, 128, 128, 128, 256, 256, and 256 channels, respectively.
The global average pooling layer then follows and produces the feature vector. The final outputs are produced by Softmax for action prediction.
Categorical cross-entropy loss is adopted for training of the GCN.
We adopt evenly averaging Softmax scores for final predictions through empirical evaluation of different aggregation methods such as averaging and maximum.

\subsection{Informative frame selection strategy}
\label{sec:inf-select}

The conventional skeleton-based action recognition methods assume that accurate human skeletons are provided by datasets. 
Real-world scenarios, however, do not provide skeletal data, and the estimated skeletal data might be inaccurate due to an unusual appearance or missing or overlapping body parts.
Likewise, since our methods are based only on RGB sequences, the pose estimation results may be inaccurate.
This issue motivates us to introduce a new sampling strategy that selects informative frames. 
Our informative frame selection strategy extracts a short sequence of sampled frames from a video sequence. 
The sampled frames preserve the joints enough to understand human poses with high confidence score from the  algorithm.
Based on confidence scores, we selectively discard frames containing ambiguous human poses.
Then our method can select only those sequences with reliable human pose information. 


Initially, the entire video is divided into  segments  at equal interval.
We construct sequences of  frames by sampling one frame from each segment.
Since consecutive frames have little variation and provide redundant pose information, the sampled sequences can reduce the redundancy.
Each segment has  frames, so -th segment  has frames , where  is -th frame in the -th segment.After applying the pose estimation algorithm to each frame , we can obtain the confidence score  of each human joint , where  and  is the number of joints.
Then, we select the frame with the highest sum of confidence scores in the estimated joints within a segment because the higher the sum is, the more accurately the human joints are detected.
Finally, the sequence of the sampled frames is used directly to construct the human pose graph on both the spatial and temporal domains.
The node set of our skeletal graph can be written as

To demonstrate that our frame selection algorithm performs better than the existing sampling strategy, we conduct an ablation study in Section~\ref{sec:ablation}.
As a result, OHA-GCN has a relatively complete form of graph that includes reliable joints.












\section{Experiments}

\subsection{Datasets}

\begin{table*}[t]\centering
\caption{Ablation study of the proposed framework over the IRD and ICVL-4 datasets. The baseline of our approach is ST-GCN~\cite{stgcn2018aaai} which is the state of the art for the skeleton-based action recognition using GCN.} 
\vspace{3mm}
\centering
\begin{tabular}{p{10.5cm}|cc} 

\hline
Method & IRD~() & ICVL-4~()   \\\hline
ST-GCN~\cite{stgcn2018aaai} (HP stream  uniform samples) & 74.03 & 80.23   \\ \hline
OHA-GCN~(OHP-body stream  informative samples) & 71.27  & 72.09   \\ 
OHA-GCN~(OHP-limbs stream  informative samples) &  73.48 & 76.74  \\ 
OHA-GCN~(OHP-hands stream  uniform samples) & 74.59  & 79.06  \\ 
OHA-GCN~(OHP-hands stream  informative samples) &  76.24 & 81.40  \\ 
OHA-GCN~(HP stream  informative samples) & 79.56 & 88.37  \\
 \hline 
OHA-GCN~(Two stream; HP  OHP-hands  informative samples) & \textbf{80.11} & \textbf{91.86}   \\ 
\hline
\end{tabular}
\\{HP: Human Pose,  OHP: Object-related Human Pose}
\label{tabular:all_res}
\end{table*}


The existing datasets~\cite{hu2015jointly,shahroudy2016ntu,xia2012view} are not suitable for use in evaluating our framework to recognize the object-related actions in real-world scenes for two reasons.
First, the existing datasets for skeleton-based action recognition were captured in constrained environments.
These datasets acquired from sensors are limited to indoor environments.
Therefore, the constrained datasets are insufficient for modeling realistic scenes because these datasets cover only a few variations in dynamic environments.
Second, these datasets usually include one actor in each image, which is not reflective of the scenes from real-world surveillance.
In our evaluation, instead of using the existing datasets, we use two realistic datasets captured in unconstrained real-world environments, which contain only raw video clips without skeleton data. The two datasets are described below.

\textbf{IRD dataset:} 
As for our own dataset, we have constructed an illegal rubbish dumping~(IRD) dataset that contains videos from CCTV cameras.
The dataset is used for the purpose of the general surveillance, especially monitoring illegal dumping.
The original videos are about 10 minutes long, and the resolution of the original video is 1280720 pixels~(HD).
We made, however, several video clips for annotations from the original videos by extracting the region of the event.
Each video clip, averaging 690 frames in length, is divided into two classes:  a garbage dumping action and  a normal event.
Humans tagged the ground truth, such as the person, the object the person carries, the start time of the action of dumping trash, and the end time.
Unlike other existing datasets for skeleton-based action recognition, this dataset is not limited to indoor environments and captured in unconstrained environments.
There is a wide variation of conditions in dynamic environments, including viewpoint changes, illumination variance, and occlusions.
Most of all, this dataset is suitable for evaluating our method for recognizing object-related human actions.
We made a total of 1374 video clips and use 1193 of them for training and 181 for validation.

\textbf{ICVL-4 dataset:} 
The ICVL dataset~\cite{Jin:2018hda} includes dynamic sub-actions of multiple people at multiple locations in the surveillance scenes.
The actions are divided into 13 categories. 
The dataset contains 497 original videos, and one actor who is one of the people appearing in the video is marked by three sub-actions. 
For example, one person in the surveillance scene is standing, walking, and smoking.
The original video is divided into several video clips containing a single actor.
We use only these specific classes related to object-related actions, such as phoning, smoking, texting, and dumping, which will be called ICVL-4 in this paper.
The reason we choose the specific classes is that we have to verify the effectiveness of our method in recognizing object-related human actions. 
The evaluation using the entire class is meaningless, as the proposed strategies are the only major differences between previous skeleton-based approach and OHA-GCN.
Therefore, we only conduct experiments with only the data from 4 classes, focusing on the object-related actions.
A training set of 805 video clips and a validation set of 86 video clips are used for our experiments.


Both the IRD and ICVL-4 datasets provide only RGB video clips and action labels.
As our approach for action recognition is based on the skeleton, we use the pose estimation algorithm~\cite{cao2017realtime} and our detection algorithm.
Through the algorithms, 25 human joints are extracted, and the position of an object is obtained. There is a total of 26 nodes including both human joints and an object.
We make each video clip as a~ tensor, where the first dimension of the tensor represents 3 channels for the 2D coordinates~ and confidence scores.
If more than two people exist in one video clip, we select the person with the highest sum of confidence scores as the actor for the recognition.
 is the length of the frames, and 26 is the number of nodes here.
We use the top-1 accuracy, which is a conventional evaluation method for both datasets.
\subsection{Experimental result}
\subsubsection{Ablation study}
\label{sec:ablation}
\quad In the ablation study, we first investigate the role of 
the proposed strategy of selecting informative frames for skeleton-based action recognition. Second, we examine the strategy for how to connect the object to the human body in OHA-GCN in formulating object-related pose graphs.
Finally, we verify the effectiveness of the object-related human pose in recognizing object-related actions. 
We will show improvement using the proposed components in our framework compared with the state-of-the-art method, ST-GCN~\cite{stgcn2018aaai}, for skeleton-based action recognition.

\textbf{Frame selection strategies.} 
We compare the strategies of selecting frames on the IRD and ICVL-4 datasets: the proposed informative frame sampling and the existing uniform sampling.
Informative samples are selected from the informative frame selection strategy, and uniform samples are obtained from the uniform sampling strategy used in ST-GCN ~\cite{stgcn2018aaai}.
The uniform sampling selects  frames at the same interval from a video.
The comparisons between uniform and informative samples are performed using the human pose~(HP) stream and the object-related human pose with hands connection~(OHP-hands) stream.
Our results, as observed in Table~\ref{tabular:all_res}, show that the informative frame selection is a better strategy than uniform sampling. 
In particular, the informative frame selection in HP stream achieves a 5.5\%~(on IRD) and a 8\%~(on ICVL-4)  improvement compared to ST-GCN using uniform samples.
We argue that the improved performance comes from building more meaningful graphs by ignoring redundant information and focusing on accurate poses.
Our methods use the informative frame selection strategy for the following experiments.

\textbf{Graph construction strategies.} 
Approaches to connecting the object to the human body are explored empirically. `OHP-body' indicates connections between all the human joints and the object; `OHP-limbs' indicates connections between the human joints belonging to arms or legs and the object; `OHP-hands' denotes connections between the human joints of both hands and the object.
`OHP-body' shows lower performance than ST-GCN because the graph structure becomes complex and introduces redundant information.
It means that the more abstracted form the object-related pose graph takes, the better its performance.
As shown in Table~\ref{tabular:all_res}, `OHP-hands' shows the best performance among all the strategies.
We believe this result is reasonably good because object-related human actions usually occur in human hands. 
Consequently, we use the connections between human hands and an object in other experiments.


\textbf{HP stream and OHP stream.} 
We also compare the human pose stream with the object-related human pose stream in the Table~\ref{tabular:all_res}. 
The method using the object-related poses with the uniform sampling strategy~(OHP-hands stream  uniform samples) does not improve performance compared to the human poses~(HP stream) with the same strategy~(ST-GCN; HP stream  uniform samples).
While, 
The usage of object-related poses with the informative frame selection~(OHP-hands stream  informative samples) achieves a 2.2\%~(on IRD) and a 1.2\%~(on ICVL-4) improvements compared to ST-GCN. The improvement is minimal, however, in the case of `HP stream  informative samples'.
This difference in the performances is the result of the fact that the frame selection strategy is more informative to the human pose graph than the object-related one. By combining the HP and OHP streams, the two-stream OHA-GCN 
achieves the best performance of a 80.11\% and a 91.86\% on the IRD and ICVL-4 datasets, respectively, which 
is a significant improvement over the state-of-the-art, ST-GCN for skeleton-based action recognition.


\subsubsection{Comparison with CNN methods using images}
\quad Our method is compared with other CNN-based methods, evaluated on the ICVL-4 datasets.
Baseline CNNs are models of RGB modality, using only RGB images as input for the network.
In fact, this is not a fair comparison because our approach is based on pose modality while CNN methods are based on RGB modality.
Still, we want to show that even without the appearance information that is high dimensionality, our method can achieve a comparable performance.
We establish baseline models whose architectures are LeNet~\cite{lecun1998gradient}, VGG16~\cite{simonyan2014very}, and ResNet50~\cite{he2016deep}, respectively.
As shown in Table~\ref{tabular:res_icvl}, the accuracy of our method is lower than some of CNN-based methods, as expected. 
This result is explained by the difference in the modalities.
RGB modality has more information for actions while pose-based representation is a compressed representation of the human body. 
Still, we argue that our pose-based approach achieves not only fairly high performance but also faster speed than RGB-based models.
To examine this argument, we perform speed comparisons with the RGB-based methods.
The test is performed on GTX TITAN X GPU, and other details of our implemented environments are described in Section~\ref{application}.
Using the ICVL-4 datasets, we measure the average runtime that the networks take to process one frame.
Our method can operate at around 3571 frames per second~(FPS), much faster speed than the CNN-based methods.
The LeNet works relatively quickly with light computation, but its accuracy is inferior to that of our method.
The ResNet achieves better results than our methods, but cannot operate quickly due to heavy computation.
Given these speed/accuracy trade-offs, our method has more advantages than other methods because it achieves the comparable results at high speed even without the RGB modality.

\begin{table}
\caption{Accuracy and speed comparisons with the CNN-based methods using RGB image inputs on the ICVL-4 dataset.}
\vspace{3mm}
\centering
\begin{tabular}{p{4cm}|cc} 

\hline 

Method & Acc.() & Speed~()  \\ \hline 

CNN (LeNet~\cite{lecun1998gradient}) &  87.21 &  3.66 \\ 
CNN (VGG16~\cite{simonyan2014very}) &  95.35 &  24.56  \\ 
CNN (ResNet50~\cite{he2016deep}) &  \textbf{96.51} & 17.26 \\  \hline
ST-GCN~\cite{stgcn2018aaai} & 80.23 & 0.1321 \\ \hline
OHA-GCN (HP stream) & 88.37 & \textbf{0.1306} \\ 
OHA-GCN (OHP stream) & 81.40 & 0.1319 \\ 
OHA-GCN (Two Stream) & 91.86  & 0.2447 \\ \hline 
\end{tabular}
{HP: Human Pose, OHP: Object-related Human Pose}
\label{tabular:res_icvl}
\end{table}





\subsection{Applications}
\label{application}

Not only do our methods show promising results, but they also open up the possibility of many applications, building a real-world surveillance system. 
As a number of people appear throughout the original video in both datasets, there are many actors and actions to be recognized.
To prevent multiple people from obstructing an action of one actor in a video, we apply a tracking algorithm to extract the regions where a single person exists.
For the tracking, we adopt the online tracking-by-detection method based on the Hungarian matching algorithm~\cite{Yoo:2016cf} using pose information.
If more than two people overlap in one patch obtained from tracking, we use the person whose joints have the highest sum of confidence scores as an actor for action recognition.

To examine the real-time performance of our proposed method in the surveillance systems, we analyze the runtime of the overall pipeline.
Our method has been implemented with Pytorch deep learning framework, and the runtime is measured on GTX TITAN X GPU and Intel Core i7-6700K 4.0Ghz.
For applications in real-world surveillance, we also use tracking algorithms whose runtime is 12.87 ms using only CPU. In our evaluation on the IRD dataset, foreground detection~\cite{cao2017realtime} and object detection operate at up to 45.78 ms and 14.93 ms using only CPU, respectively.
The frame size in the original videos is 19201080, but we resize it to 656368 which is the size recommended by ~\cite{cao2017realtime}.
The pose estimation algorithm takes 74.62 ms using one GPU, 38.02 ms using two GPUs, and 20.92 ms using three GPUs.
The overall pipeline, including the runtime of OHA-GCN, takes around 94.78 ms~(10.6 FPS) on average, which can be applied to real-time monitoring.
The experimental results and runtime analysis imply that our approach can be successfully implemented in real-world scenarios, achieving real-time performance.


\section{Conclusion}
Our main contributions are summarized as follows. 
First, we propose a new framework, object-related human action recognition~(OHA-GCN), which uses the graphs of human poses and the object-related human poses to understand object-related human actions.
Second, to overcome the difficulties of skeleton-based action recognition in real-world scenarios, we explore good strategies, including informative frame selection and construction of the object-related human poses. 
Third, we introduce a new dataset of illegal rubbish dumping~(IRD), which compiles realistic data in unconstrained environments. 
Fourth, our framework can run in real time while achieving significantly better performance than the state-of-the-art algorithm for skeleton-based action recognition.
It is noted that the proposed strategies offer meaningful insights into how to incorporate contextual information into human-object interaction recognition in the future.



\vspace{2mm}
\noindent\textbf{Acknowledgements}
\ This work was partly supported by  
Next-Generation ICD Program through NRF funded by Ministry of S\&ICT [2017M3C4A7077582], 
the ICT R\&D program of MSIT/IITP. (No.B0101-15-0266, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis and No.B0101-15-0552, Predictive Visual Intelligence Technology).
\normalsize

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
