\section{Experiments}
\label{sec:results}

We first discuss the experimental setup and design choices of our implementation and then present  quantitative results on the test sets of Flickr 30k Entities (\tablesref{tbl:testset_flickr},\ref{tbl:testset}) and ReferItGame (\tableref{tbl:testset_referit}) datasets. We find our best results to outperform state-of-the-art on both datasets by a significant margin. \Figsref{fig:qualitative_flickr} and \ref{fig:qualitative_referit} show qualitatively how well we can ground phrases in images.

\subsection{Experimental Setup}
\label{sec:expsetup}

We evaluate GroundeR on the datasets Flickr 30k Entities \cite{plummer15iccv} and ReferItGame \cite{kazemzadeh14emnlp}. Flickr 30k Entities \cite{plummer15iccv} contains over 275K bounding boxes from 31K images associated with natural language phrases. Some phrases in the dataset correspond to multiple boxes, \eg ``two men''. For consistency with \cite{plummer15iccv}, in such cases we consider the union of the boxes as ground truth. We use 1,000 images for validation, 1,000 for testing and 29,783 for training. 
The ReferItGame \cite{kazemzadeh14emnlp} dataset contains over 99K regions from 20K images. Regions are associated with natural language expressions, constructed to disambiguate the described objects. We use the bounding boxes provided by \cite{hu16cvpr} and the same test split, namely 10K images for testing; the rest we split in 9K training and 1K validation images.



We obtain 100 bounding box proposals for each image using Selective Search \cite{uijlings2013selective} for Flickr 30k Entities and Edge Boxes \cite{zitnick2014eccv} for ReferItGame dataset. For our semi-supervised and fully supervised models we obtain the ground-truth attention by selecting the proposal box which overlaps most with the ground-truth box, while the overlap IOU (intersection over union) is above 0.5. Thus, our fully supervised model is not trained with all available training phrase-box pairs, but only with those where such proposal boxes exist.

On the Flickr 30k Entities for the visual representation we rely on the VGG16 network \cite{simonyan2014very} trained on ImageNet \cite{deng09cvpr}. For each box we extract a 4,096 dimensional feature from the fully connected fc7 layer. We also consider a VGG16 network fine-tuned for object detection on PASCAL \cite{everingham2010pascal}, trained using Fast R-CNN \cite{girshick2015fast}. In the following we refer to both features as VGG-CLS and VGG-DET, respectively. We do not fine-tune the VGG representation for our task to reduce computational and memory load, however, our model trivially allows back-propagation into the image representation which likely would lead to further improvements. For the ReferItGame dataset we use the VGG-CLS features and additional spatial features provided by \cite{hu16cvpr}. We concatenate both and refer to the obtained feature as VGG+SPAT. For the language encoding and decoding we rely on the LSTM variant implemented in Caffe \cite{jia2014caffe} which we initialize randomly and jointly train with the grounding task.

At test time we compute the accuracy as the ratio of phrases for which the attended box overlaps with the ground-truth box by more than 0.5 IOU.
 
\subsection{Design choices and findings}

In all experiments we use the Adam solver \cite{kingma2014adam}, which adaptively changes the learning rate during training. We train our models for about 20/50 epochs for the Flickr 30k Entities/ReferItGame dataset, respectively, and pick the best iteration on the validation set. 

Next, we report our results for optimizing hyperparmeters on the validation set of Flickr 30k Entities while using the VGG-CLS features.

\myparagraph{Regularization.}
Applying L2 regularization to parameters (weight decay) is important for the best performance of our unsupervised model. By introducing the weight decay of  we improve the accuracy from  to . In contrast, when supervision is available, we introduce batch normalization \cite{ioffe2015batch} for the phrase encoding LSTM and visual feature, which leads to a performance improvement, in particular from 37.42\% to 40.93\% in the supervised scenario.

\myparagraph{Layer initialization.}
We experiment with different ways to initialize the layer parameters. The configuration which works best for us is using uniform initialization for LSTM, MSRA \cite{he2015delving} for convolutional layers, and Xavier \cite{glorot2010understanding} for all other layers. Switching from Xavier to MSRA initialization for the convolutional layers improves the accuracy of the unsupervised model from  to .






\subsection{Experiments on Flickr 30k Entities dataset}
We report the performance of our approach with multiple levels of supervision in Table \ref{tbl:testset_flickr}. 
In the last line of the table we report the proposal upper-bound accuracy, namely the presence of the correct box among the proposals (which overlaps with the ground-truth box with ).




\newcommand{\midruleValShort}{\cmidrule(rr){1-1} \cmidrule(rr){2-4}}
\begin{table*}[t]
\scriptsize
\center
\begin{tabular}{lccc}
\toprule
Approach & \multicolumn{3}{c}{Accuracy} \\
         & Other & VGG-CLS & VGG-DET \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Unsupervised training}} \\
Deep Fragments [6] & 21.78 & - & - \\
GroundeR & - & 24.66 & 28.94 \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Supervised training}} \\
CCA \cite{plummer15iccv} & - & 27.42 & - \\
SCRC \cite{hu16cvpr} & - & 27.80 & - \\
DSPE \cite{wang2016cvpr} & - & - & 43.89 \\
GroundeR & - & 41.56 & 47.81 \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Semi-supervised training}} \\
GroundeR 3.12\% annot. & - & 33.02 & 42.32 \\
GroundeR 6.25\% annot. & - & 37.10 & 44.02 \\
GroundeR 12.5\% annot. & - & 38.67 & 44.96 \\
GroundeR 25.0\% annot. & - & 39.31 & 45.32 \\
GroundeR 50.0\% annot. & - & 40.72 & 46.65 \\
GroundeR 100.0\% annot. & - & 42.43 & 48.38 \\
\midruleValShort
Proposal upperbound & 77.90 & 77.90 & 77.90 \\
\bottomrule\\
\end{tabular}
\caption{Phrase localization performance on Flickr 30k Entities with different levels of bounding box supervision, accuracy in \%.}
\label{tbl:testset_flickr}
\end{table*}
 


\myparagraph{Unsupervised training.}
We start with the unsupervised scenario, \ie no phrase localization ground-truth is used at training time.
Our approach, which relies on VGG-CLS features, is able to achieve 24.66\% accuracy. Note that the VGG network trained on ImageNet has not seen any bounding box annotations at training time. VGG-DET, which was fine-tuned for detection, performs better and achieves 28.94\% accuracy. We can further improve this by taking a sentence constraint  into account. Namely, it is unlikely that two different phrases from one sentence are grounded to the same box. Thus we post-process the attended boxes: we jointly process the phrases from one sentence and greedily select the highest scoring box for each phrase, while the same box cannot be selected twice. This allows us to reach the accuracy of 25.01\% for VGG-CLS and 29.02\% for VGG-DET. While we currently only use a sentence constraint as a simple post processing step at test time, it would be interesting to include a sentence level constraint during training as part of future work. 
We compare to the unsupervised Deep Fragments approach of \cite{karpathy14nips}. 
Note, that \cite{karpathy14nips} does
not report the grounding performance and does not allow for direct comparison with our work. With our best case evaluation\footnote{We train the Deep Fragments model \cite{karpathy14nips} on the the Flickr 30k dataset and evaluate with the Flickr 30k Entities ground truth phrases and boxes. Our trained Deep Fragments model achieves 11.2\%/16.5\% recall@1 for image annotation/search compared to 10.3\%/16.4\% reported in \cite{karpathy14nips}. As there is a large number of dependency tree fragments per sentence (on average 9.5) which are matched to proposal boxes, rather than on average 3.0 noun phrases per sentence in Flickr 30k Entities, we make a best case study in favor of \cite{karpathy14nips}. For each ground-truth phrase we take the maximum overlapping dependency tree fragments (w.r.t. word overlap), compute the IOU between their matched boxes and the ground truth, and take the highest IOU.}
of Deep Fragments \cite{karpathy14nips}, which also relies on detection boxes and features, we achieve an accuracy of 21.78\%. 
Overall, the ranking objective in \cite{karpathy14nips} can be seen complimentary to our reconstruction objective. It might be possible, as part of future work, to combine both objectives to learn even better models without grounding supervision.

\myparagraph{Supervised training.}
Next we look at the fully supervised scenario. The accuracy achieved by \cite{plummer15iccv} is 27.42\%\footnote{The number was provided by the authors of \cite{plummer15iccv}, while in \cite{plummer15iccv} they report 25.30\% for phrases automatically extracted with a parser.} and by  
SCRC \cite{hu16cvpr} is 27.80\%. Recent approach of \cite{wang2016cvpr} achieves 43.89\% with VGG-DET features. Our approach, when using VGG-CLS features achieves an accuracy of 41.56\%, significantly improving over prior works that use VGG-CLS. We further improve our result to impressive 47.81\% when using VGG-DET features.

\myparagraph{Semi-supervised training.}
Finally, we move to the semi-supervised scenario. The notation ``\% annot.'' means that \% of the annotated data (where ground-truth attention is available) is used. As described in Section \ref{sec:reconstruct} we have a parameter  which controls the weight of the attention loss  vs. the reconstruction loss . We estimate the value of  on validation set and fix it for all iterations. We found that we need higher weight on  when little supervision is available. E.g. for 3.12\% of supervision  and for 12.5\% supervision . This is due to the fact that in these cases only 3.12\% / 12.5\% of labeled instances contribute to , while all instances contribute to .

When integrating 3.12\% of the available annotated data into the model we significantly improve the accuracy from 24.66\% to 33.02\% (VGG-CLS) and from 28.94\% to 42.32\% (VGG-DET). The accuracy further increases when providing more annotations,  reaching 42.43\% for VGG-CLS and 48.38\% for VGG-DET when using all annotations.
As ablation of our semi-supervised model we evaluated the supervised model while only using the respective \% of annotated data. We observed consistent improvement of our semi-supervised model over the supervised model. Intrestingly, when using all available supervision,  still helps to improve performance over the  supervised model (42.43\% vs. 41.56\%, 48.38\% vs. 47.81\%). Our intuition for this is that  only has a single correct bounding box (which overlaps most with the ground truth), while  can also learn from overlapping boxes with high but not best overlap.

\newcommand{\midruleValLong}{\cmidrule(rr){1-1} \cmidrule(rr){2-9} \cmidrule(rr){10-10}}
\begin{table}[t]
\scriptsize
\center
\begin{tabular}{lccccccccc}
\toprule
Phrase type & peo- & clo- & body- & ani- & vehi- & instru- & scene & other & novel \\
  & ple & thing & parts & mals & cles & ments &  &  &  \\
\midruleValLong
Number of instances & 5,656 & 2,306 & 523 & 518 & 400 & 162 & 1,619 & 3,374 & 2,214 \\
\midruleValLong
\multicolumn{10}{l}{\textbf{Unsupervised training}} \\
GroundeR (\tiny{VGG-DET}) & 44.32 & 9.02 & 0.96 & 46.91 & 46.00 & 19.14 & 28.23 & 16.98 & 25.43 \\
\midruleValLong
\multicolumn{10}{l}{\textbf{Supervised training}} \\
CCA embedding \cite{plummer15iccv} & 29.58 & 24.20 & 10.52 & 33.40 & 34.75 & 35.80 & 20.20 & 20.75 & n/a \\
GroundeR (\tiny{VGG-CLS}) & 53.80 & 34.04 & 7.27 & 49.23 & 58.75 & 22.84 & 52.07 & 24.13 & 34.28 \\
GroundeR (\tiny{VGG-DET}) & 61.00 & 38.12 & 10.33 & 62.55 & 68.75 & 36.42 & 58.18 & 29.08 & 40.83 \\
\midruleValLong
\multicolumn{2}{l}{\textbf{Semi-supervised training}} \\
GroundeR (\tiny{VGG-DET}) \scriptsize{3.12\% annot.} & 56.51 & 29.84 & 9.18 & 57.34 & 59.75 & 28.40 & 50.71 & 24.48 & 34.28 \\
GroundeR (\tiny{VGG-DET}) \scriptsize{100.0\% annot.} & 60.24 & 39.16 & 14.34 & 64.48 & 67.50 & 38.27 & 59.17 & 30.56 & 42.37 \\
\midruleValLong
Proposal upperbound & 85.93 & 66.70 & 41.30 & 84.94 & 89.00 & 70.99 & 91.17 & 69.29 & 79.90 \\
\bottomrule\\
\end{tabular}
\caption{Detailed phrase localization, Flickr 30k Entities, accuracy in \%.}
\label{tbl:testset}
\end{table}


\myparagraph{Results per phrase type.}
Flickr 30k Entities dataset provides a ``type of phrase'' annotation for each phrase, which we analyze in Table \ref{tbl:testset}. Our unsupervised approach does well on phrases like ``people'', ``animals'', ``vehicles'' and worse on ``clothing'' and ``body parts''. This could be due to confusion between people and their clothing or body parts. To address this, one could jointly model the phrases and add spatial relations between them in the model. Body parts are also the most challenging type to detect, with the proposal upper-bound of only . The supervised model with VGG-CLS features outperforms \cite{plummer15iccv} in all  types except ``body parts'' and ``instruments'', while with VGG-DET it is better or similar in all types. Semi-supervised model brings further significant performance improvements, in particular for ``body parts''.
In the last column we report the accuracy for novel phrases, \ie the ones which did not appear in the training data. On these phrases our approach maintains high performance, although it is lower than the overall accuracy. This shows that learned language representation is effective and allows transfer to unseen phrases.

\myparagraph{Summary Flickr 30k Entities.}
Our unsupervised approach performs similar (VGG-CLS) or better (VGG-DET) than the fully supervised methods of \cite{plummer15iccv} and \cite{hu16cvpr} (\tableref{tbl:testset_flickr}). Incorporating a small amount of supervision (\eg 3.12\% of annotated data) allows us to outperform \cite{plummer15iccv} and \cite{hu16cvpr} also when VGG-CLS features are used. Our best supervised model achieves 47.81\%, surpassing all the previously reported results, including \cite{wang2016cvpr}. Our semi-supervised model efficiently exploits the reconstruction loss  which allows it to outperform  the supervised model.

\begin{table*}[t]
\scriptsize
\center
\begin{tabular}{lccc}
\toprule
Approach & \multicolumn{3}{c}{Accuracy} \\
         & Other & VGG & VGG+SPAT \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Unsupervised training}} \\
LRCN \cite{donahue15cvpr} (reported in \cite{hu16cvpr}) & 8.59 & - & - \\
CAFFE-7K \cite{guadarrama2014open} (reported in \cite{hu16cvpr}) & 10.38 & - & - \\
GroundeR & - & 10.69 & 10.70 \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Supervised training}} \\
SCRC \cite{hu16cvpr} & - & - & 17.93 \\
GroundeR & - & 23.44 & 26.93 \\
\midruleValShort
\multicolumn{4}{l}{\textbf{Semi-supervised training}} \\
GroundeR 3.12\% annot. & - & 13.70 & 15.03 \\
GroundeR 6.25\% annot. & - & 16.19 & 19.53 \\
GroundeR 12.5\% annot. & - & 19.02 & 21.65 \\
GroundeR 25.0\% annot. & - & 21.43 & 24.55 \\
GroundeR 50.0\% annot. & - & 22.67 & 25.51 \\
GroundeR 100.0\% annot. & - & 24.18 & 28.51 \\
\midruleValShort
Proposal upperbound & 59.38 & 59.38 & 59.38 \\
\bottomrule\\
\end{tabular}
\caption{Phrase localization performance on ReferItGame with different levels of bounding box supervision, accuracy in \%.}
\label{tbl:testset_referit}
\end{table*}


\subsection{Experiments on ReferItGame dataset}
Table \ref{tbl:testset_referit} summarizes results on the ReferItGame dataset. We compare our approach to the previously introduced fully supervised method SCRC \cite{hu16cvpr}, as well as provide reference numbers for two other baselines: LRCN \cite{donahue15cvpr} and CAFFE-7K \cite{guadarrama2014open} reported in \cite{hu16cvpr}. The LRCN baseline of \cite{hu16cvpr} is using the image captioning model LRCN \cite{donahue15cvpr} trained on MSCOCO \cite{coco2014} to score how likely the query phrase is to be generated for the proposal box. CAFFE-7K is a large scale object classifier trained on ImageNet \cite{deng09cvpr} to distinguish 7K classes. \cite{guadarrama2014open} predicts a class for each proposal box and constructs a word bag with all the synonyms of the class-name based on WordNet \cite{fellbaum:wordnet}. The obtained word bag is then compared to the query phrase after both are projected to a joint vector space. Both approaches are unsupervised w.r.t. the phrase bounding box annotations. Table \ref{tbl:testset_referit} reports the results of our approach with VGG, as well as VGG+SPAT features of \cite{hu16cvpr}.

\myparagraph{Unsupervised training.}
In the unsupervised scenario our GroundeR performs competitive with the LRCN and CAFFE-7K baselines, achieving 10.7\% accuracy. We note that in this case VGG and VGG+SPAT perform similarly.

\myparagraph{Supervised training.}
In the supervised scenario we compare to the best prior work on this dataset, SCRC \cite{hu16cvpr}, which reaches 17.93\% accuracy. Our supervised approach, which uses identical visual features, significantly improves this performance to 26.93\%. 

\myparagraph{Semi-supervised training.}
Moving to the semi-supervised scenario again demonstrates performance improvements, similar to the ones observed on Flickr 30k Entities datset. Even the small amount of supervision (3.12\%) significantly improves performance to 15.03\% (VGG+SPAT), while with 100\% of annotations we achieve 28.51\%, outperforming the supervised model.  

\myparagraph{Summary ReferItGame dataset.}
While the unsupervised model only slightly improves over prior work, the semi-supervised version can effectively learn from few labeled training instances, and with all supervision it achieves 28.51\%, improving over \cite{hu16cvpr} by a large margin of 10.6\%. Overall the performance on ReferItGame dataset is significantly lower than on Flickr 30k Entities. We attribute this to two facts. First, the training set of ReferItGame is rather small compared to Flickr 30k (9k vs. 29k images). Second, the proposal upperbound on ReferItGame is significantly lower than on Flickr 30k Entities (59.38\% vs 77.90\%) due to the complex nature of the described objects and ``stuff" image regions.

\subsection{Qualitative results}

\begin{figure*}[t]
\center
\begin{tabular}{c@{\ \ \ \ \ \ }c@{\ \ \ \ }c}
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/2089_.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/1638.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/woman-lake.jpg} \\
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/girl-shirt-doll.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/woman-bike.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/woman-lake_s.png} \\
\multicolumn{1}{m{2.5cm}}{\small{
\textcolor{red}{A little girl} in \textcolor{blue}{a pink shirt} is looking at \textcolor{green}{a toy doll}.\newline ~
}} &
\multicolumn{1}{m{3.5cm}}{\small{
\textcolor{red}{A woman} is riding \textcolor{blue}{a bicycle} on \textcolor{green}{the pavement}.\newline
}} & 
\multicolumn{1}{m{3.7cm}}{\small{
\textcolor{red}{A girl} with \textcolor{blue}{a red cap}, \textcolor{green}{hair} tied up and \textcolor{magenta}{a gray shirt} is fishing in \textcolor{cyan}{a calm lake}.
}} 
\end{tabular}
\caption{Qualitative results on the test set of Flickr 30k Entities. Top : GroundeR (VGG-DET) unsupervised, bottom: GroundeR (VGG-DET) supervised.}
\label{fig:qualitative_flickr}
\end{figure*}

We provide qualitative results on Flickr 30K Entities dataset in Figure \ref{fig:qualitative_flickr}. We compare our unsupervised and supervised approaches, both with VGG-DET features. The supervised approach visibly improves the localization quality over the unsupervised approach, which nevertheless is able to localize many phrases correctly.
Figure \ref{fig:qualitative_referit} presents qualitative results on ReferItGame dataset. We show the predictions of our supervised approach, as well as the ground-truth boxes. One can see the difficulty of the task from the presented examples, including two failures in the bottom row. One requires good language understanding in order to correctly ground such complex phrases. In order to ground expressions like ``hut to the nearest left of the person on the right'' we would need to additionally model relations between objects, an interesting direction for future work.

\begin{figure*}[t]
\center
\begin{tabular}{c@{\ \ }c@{\ \ }c}
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/16_.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/585_.jpg} &
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/6827_.jpg} \\
\multicolumn{1}{m{3.5cm}}{\small{
two people on right}} &
\multicolumn{1}{m{3.5cm}}{\small{
picture of a bird flying above sand}} &
\multicolumn{1}{m{3.5cm}}{\small{
dat alpaca up in front, total coffeelate swag}} \\
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/6199_.jpg} &
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/1160_.jpg} & 
\includegraphics[clip=true,width=0.28\textwidth,height=0.16\textheight,keepaspectratio]{fig/2490_.jpg} \\
\multicolumn{1}{m{3.5cm}}{\small{
palm tree coming out of the top of the building}} &
\multicolumn{1}{m{3.5cm}}{\small{
guy with blue shirt and yellow shorts}} &
\multicolumn{1}{m{3.5cm}}{\small{
hut to the nearest left of the person on the right}} \\
\end{tabular}
\caption{Qualitative results on the test set of ReferItGame: GroundeR (VGG+SPAT) supervised. Green: ground-truth box, red: predicted box.}
\label{fig:qualitative_referit}
\end{figure*}

