



\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{array}
\usepackage[font={footnotesize}]{caption}





\aclfinalcopy 

\title{Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference}

\author{Yi Tay, Luu Anh Tuan, Siu Cheung Hui \\
  Nanyang Technological University, Singapore \\
  Institute for Infocomm Research, A*Star Singapore \\
  {\tt ytay017@e.ntu.edu.sg, at.luu@i2r.a-star.edu.sg} \\
  {\tt asschui@ntu.edu.sg} \\
  }





\newcommand\BibTeX{B{\sc ib}\TeX}
\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a  times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.
 \end{abstract}

 \section{Introduction}
Natural Language Inference (NLI) is a pivotal and fundamental task in language understanding and artificial intelligence. More concretely, given a premise and hypothesis, NLI aims to detect whether the latter \textit{entails} or \textit{contradicts} the former. As such, NLI is also commonly known as Recognizing Textual Entailment (RTE). NLI is known to be a significantly challenging task for machines whose success often depends on a wide repertoire of reasoning techniques.

In recent years, we observe a steep improvement in NLI systems, largely contributed by the release of the largest publicly available corpus for NLI - the Stanford Natural Language Inference (SNLI) corpus \cite{DBLP:conf/emnlp/BowmanAPM15} which comprises  hand labeled sentence pairs. This has improved the feasibility of training complex neural models, given the fact that neural models often require a relatively large amount of training data.

Highly competitive neural models for NLI are mostly based on soft-attention alignments, popularized by \cite{DBLP:conf/emnlp/ParikhT0U16,rocktaschel2015reasoning}. The key idea is to learn an alignment of sub-phrases in both sentences and learn to compare the relationship between them. Standard feed-forward neural networks are commonly used to model similarity between aligned (decomposed) sub-phrases and then aggregated into the final prediction layers.

Alignment between sentences has become a staple technique in NLI research and many recent state-of-the-art models such as the Enhanced Sequential Inference Model (ESIM) \cite{DBLP:conf/acl/ChenZLWJI17} also incorporate the alignment strategy. The difference here is that ESIM considers a non-parameterized comparison scheme, i.e., \textit{concatenating} the subtraction and element-wise product of aligned sub-phrases, along with two original sub-phrases, into the final comparison vector. A bidirectional LSTM is then used to aggregate the compared alignment vectors.

This paper presents a new neural model for NLI. There are several new novel components in our work. Firstly, we propose a \textit{compare, compress and propagate} (ComProp) architecture where compressed alignment features are propagated to upper layers (such as a RNN-based encoder) for enhancing representation learning. Secondly, in order to achieve an efficient propagation of alignment features, we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature. Each scalar valued feature is used to augment the base word representation, allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information.

There are several major advantages to our proposed architecture. Firstly, our model is relatively compact, i.e., we compress alignment feature vectors and augment them to word representations instead. This is to avoid large alignment (or match) vectors being propagated across the network. As a result, our model is more parameter efficient compared to ESIM since the width of the middle layers of the network is now much smaller. To the best of our knowledge, this is the first work that explicitly employs such a paradigm.

Secondly, the explicit usage of compression enables improved interpretabilty since each alignment pair is compressed to a scalar and hence, can be easily visualised. Previous models such as ESIM use subtractive operations on alignment vectors, edging on the intuition that these vectors represent contradiction. Our model is capable of visually demonstrating this phenomena. As such, our design choice enables a new way of deriving insight from neural NLI models.

Thirdly, the alignment factorization layer is expressive and powerful, combining ideas from standard machine learning literature \cite{rendle2010factorization} with modern neural NLI models. The factorization layer tries to decompose the alignment vector (constructed from the variations of ,  and ), learning higher-order feature interactions between each compared alignment. In other words, it models the second-order (pairwise) interactions between \textit{each} feature in \textit{every} alignment vector using factorized parameters, allowing more expressive comparison to be made over traditional fully-connected layers (FC). Moreover, factorization-based models are also known to be able to model low-rank structure and reduce risks of overfitting. The effectiveness of the factorization alignment over alternative baselines such as feed-forward neural networks is confirmed by early experiments.

The major contributions of this work are summarized as follows:
\begin{itemize}
\item We introduce a \textit{Compare, Compress and Propagate} (ComProp) architecture for NLI. The key idea is to use the myriad of generated comparison vectors for augmentation of the base word representation instead of simply aggregating them for prediction. Subsequently, a standard compositional encoder can then be used to learn representations from the augmented word representations. We show that we are able to derive meaningful insight from visualizing these augmented features.
\item For the first time, we adopt expressive factorization layers to model the relationships between soft-aligned sub-phrases of sentence pairs. Empirical experiments confirm the effectiveness of this new layer over standard fully connected layers.
\item Overall, we propose a new neural model - \textsc{CAFE} (\textbf{C}omProp \textbf{A}lignment-\textbf{F}actorized \textbf{E}ncoders) for NLI. Our model achieves state-of-the-art performance on SNLI, MultiNLI and the new SciTail dataset, outperforming existing state-of-the-art models such as ESIM. Ablation studies confirm the effectiveness of each proposed component in our model.
\end{itemize}

\section{Related Work}

Natural language inference (or textual entailment recognition) is a long standing problem in NLP research, typically carried out on smaller datasets using
traditional methods \cite{Maccartney:2009:NLI:1751277,Dagan:2005:PRT:2100045.2100054,MacCartney:2008:MSC:1599081.1599147,Iftene:2007:HTS:1654536.1654562}.

The relatively recent creation of  human annotated sentence pairs \cite{DBLP:conf/emnlp/BowmanAPM15} have spurred on many recent works that use neural networks for NLI. Many advanced neural architectures have been proposed for the NLI task, with most exploiting some variants of neural attention which learn to pay attention to important segments in a sentence \cite{DBLP:conf/emnlp/ParikhT0U16,DBLP:conf/acl/ChenZLWJI17,DBLP:conf/naacl/WangJ16,rocktaschel2015reasoning,DBLP:conf/eacl/YuM17a}.

Amongst the myriad of neural architectures proposed for NLI, the ESIM \cite{DBLP:conf/acl/ChenZLWJI17} model is one of the best performing models. The ESIM, primarily motivated by soft subphrase alignment in \cite{DBLP:conf/emnlp/ParikhT0U16}, learns alignments between BiLSTM encoded representations and aggregates them with another BiLSTM layer. The authors also propose the usage of subtractive composition, claiming that this helps model contradictions amongst alignments.

Compare-Aggregate models are also highly popular in NLI tasks. While this term was coined by \cite{DBLP:journals/corr/WangJ16b}, many prior NLI models follow this design \cite{DBLP:conf/ijcai/WangHF17,DBLP:conf/emnlp/ParikhT0U16,DBLP:journals/corr/abs-1709-04348,DBLP:conf/acl/ChenZLWJI17}. The key idea is to aggregate matching features and pass them through a dense layer for prediction. \cite{DBLP:conf/ijcai/WangHF17} proposed BiMPM, which adopts multi-perspective cosine matching across sequence pairs. \cite{DBLP:journals/corr/WangJ16b} proposed a one-way attention and convolutional aggregation layer. \cite{DBLP:journals/corr/abs-1709-04348} learns representations with highway layers and adopts ResNet for learning features over an interaction matrix.

There are several other notable models for NLI. For instance, models that leverage directional self-attention \cite{DBLP:journals/corr/abs-1709-04696} or Gumbel-Softmax \cite{choi2017unsupervised}. DGEM is a graph based attention model which was proposed together with a new entailment challenge dataset, SciTail \cite{scitail}. Pretraining have been known to also be highly useful in the NLI task. For instance, contextualized vectors learned from machine translation \cite{mccann2017learned} (CoVe) or language modeling \cite{peters2018deep} (ELMo) have showned to be able to improve performance when integrated with existing NLI models.

Our work compares and compresses alignment pairs using factorization layers which leverages the rich history of standard machine learning literature. Our factorization layers incorporate highly expressive factorization machines (FMs) \cite{rendle2010factorization} into neural NLI models. In standard machine learning tasks, FMs remain a very competitive choice for learning feature interactions \cite{xiao2017attentional} for both standard classification and regression problems. Intuitively, FMs are adept at handling data sparsity (typically interactions) by using factorized parameters to approximate a feature matching matrix. This makes it suitable in our model architecture since feature interaction between subphrase alignment pairs is typically very sparse as well.

A recent work \cite{beutel2018latent} reports an interesting empirical study pertaining to the ability of standard FC layers and their ability to model `cross features' (or multiplicative features). Their overall finding suggests that while standard ReLU FC layers are able to approximate 2-way or 3-way features, they are extremely inefficient in doing so (requiring either very wide or deep layers). This further motivates the usage of FMs in this work and is well aligned with our empirical results, i.e., strong competitive performance with reasonably small parameterization.

\section{Our Proposed Model}
In this section, we provide a layer-by-layer description of our model architecture. Our model accepts two sentences as an input, i.e.,  (premise) and  (hypothesis). Figure \ref{fig:high_level} illustrates a high-level overview of our proposed model architecture.


\begin{figure}[ht]
  \centering
    \includegraphics[width=0.8\linewidth]{images/comp_prop2.pdf}
    \caption{High level overview of our proposed architecture (\textit{best viewed in color}). Alignment vectors are compressed and then propagated to upper representation learning layers (RNN encoders). Intra-attention is omitted in this diagram due to the lack of space. }
    \label{fig:high_level}
\end{figure}


\subsection{Input Encoding Layer}{}
 This layer aims to learn a -dimensional representation for each word. Following \cite{DBLP:journals/corr/abs-1709-04348}, we learn feature-rich word representations by concatenating word embeddings, character embeddings and syntactic (part-of-speech tag) embeddings (provided in the datasets). Character representations are learned using a convolutional encoder with max pooling function and is commonly used in many relevant literature \cite{DBLP:conf/ijcai/WangHF17,DBLP:conf/repeval/ChenZLWJI17}.
\paragraph{Highway Encoder}
 Subsequently, we pass each concatenated word vector into a two layer highway network \cite{DBLP:journals/corr/SrivastavaGS15} in order to learn a -dimensional representation. Highway networks are gated projection layers which learn adaptively control how much information is being carried to the next layer. Our strategy is similar to \cite{DBLP:conf/emnlp/ParikhT0U16} which trains the projection layer in place of tuning the embedding matrix. The usage of highway layers over standard projection layers is empirically motivated. However, an intuition would be that the gates in this layer adapt to learn the relative importance of each word to the NLI task. Let  and  be single layered affine transforms with ReLU and sigmoid activation functions respectively. A single highway network layer is defined as:
 
 where  and  Notably, the dimensions of the affine transform might be different from the size of the input vector. In this case, an additional nonlinear transform is used to project  to the same dimensionality. The output of this layer is  (premise) and  (hypothesis), with each word converted to a -dimensional vector.

\subsection{Soft-Attention Alignment Layer}
This layer describes two soft-attention alignment techniques that are used in our model.
\paragraph{Inter-Attention Alignment Layer}
This layer learns an alignment of sub-phrases between  and . Let  be a standard projection layer with ReLU activation function. The alignment matrix of two sequences is defined as follows:

where  and ,  are the -th and -th word in the premise and hypothesis respectively.

where  is the sub-phrase in  that is softly aligned to .
Intuitively,  is a weighted sum across , selecting the most relevant parts of  to represent .

\paragraph{Intra-Attention Alignment Layer}
This layer learns a \textit{self-alignment} of sentences and is applied to both  and  independently. For the sake of brevity, let  represent either  or , the intra-attention alignment is computed as:

where  and  is a nonlinear projection layer with ReLU activation function. The intra-attention layer models similarity of each word with respect to the entire sentence, capturing long distance dependencies and `global' context of the entire sentence.

\subsection{Alignment Factorization Layer}
This layer aims to learn a scalar valued feature for each comparison between aligned sub-phrases. Firstly, we introduce our factorization operation, which lives at the core of our neural model.
\paragraph{Factorization Operation}
Given an input vector , the factorization operation \cite{rendle2010factorization} is defined as:

where  is a scalar valued output,  is the dot product between two vectors and  is the global bias. Factorization machines model low-rank structure within the matching vector producing a scalar feature.  The parameters of this layer are  and . The first term  is simply a linear term. The second term  captures all pairwise interactions in  (the input vector) using the factorization of matrix .

\paragraph{Inter-Alignment Factorization}
This operation compares the alignment between inter-attention aligned representations, i.e.,  and . Let  represent an alignment pair, we apply the following operations:

where ,  is the factorization operation,  is the concatenation operator and  is the element-wise multiplication. The intuition of modeling subtraction is targeted at capturing contradiction. However, instead of simply concatenating the extra comparison vectors, we compress them using the factorization operation. Finally, for each alignment pair, we obtain three scalar-valued features which map precisely to a word in the sequence.
\paragraph{Intra-Alignment Factorization}
Next, for each sequence, we also apply alignment factorization on the intra-aligned sentences. Let  represent an \textit{intra-aligned} pair from either the premise or hypothesis, we compute the following operations:

where  and  is the factorization operation. Applying alignment factorization to intra-aligned representations produces another three scalar-valued features which are mapped to each word in the sequence. Note that each of the \textit{six} factorization operations has its own parameters but shares them amongst all words in the sentences.

\subsection{Propagation and Augmentation}
Finally, the \textit{six} factorized features are then aggregated\footnote{Following \cite{DBLP:conf/emnlp/ParikhT0U16}, we may also concatenate the intra-aligned vector to  which we found to have speed up convergence.} via concatenation to form a final feature vector that is propagated to upper representation learning layers via augmentation of the word representation  or .

where  is -th word in  or , and  and   are the intra-aligned   and inter-aligned  features for the -th word in the sequence respectively.
Intuitively,  augments each word with global knowledge of the sentence and  augments each word with cross-sentence knowledge via inter-attention.
\subsection{Sequential Encoder Layer}
For each sentence, the augmented word representations  are then passed into a sequential encoder layer. We adopt a standard vanilla LSTM encoder.

where  represents the maximum length of the sequence. Notably, the parameters of the LSTM are \textit{siamese} in nature, sharing weights between both premise and hypothesis. We do not use a bidirectional LSTM encoder, as we found that it did not lead to any improvements on the held-out set. A logical explanation would be because our word representations are already augmented with global (intra-attention) information. As such, modeling in the reverse direction is unnecessary, resulting in some computational savings.
\paragraph{Pooling Layer}
Next, to learn an overall representation of each sentence, we apply a pooling function across all hidden outputs of the sequential encoder. The pooling function is a concatenation of temporal max and average (avg) pooling.

where  is a final -dimensional representation of the sentence (premise or hypothesis). We also experimented with \textit{sum} and \textit{avg} standalone poolings and found \textit{sum} pooling to be relatively competitive.
\subsection{Prediction Layer}
Finally, given a fixed dimensional representation of the premise  and hypothesis , we pass their concatenation into a two-layer -dimensional highway network. Since the highway network has been already defined earlier, we omit the technical details here. The final prediction layer of our model is computed as follows:

where  are highway network layers with ReLU activation. The output is then passed into a final linear softmax layer.

where  and . The network is then trained using standard multi-class cross entropy loss with L2 regularization.

\section{Experiments}

\begin{table*}[ht]
  \centering

    \begin{tabular}{|l|ccc|}
    \hline
    Model & Params & Train& Test\\
    \hline
    \multicolumn{4}{|c|}{\textbf{Single Model (w/o Cross Sentence Attention)}} \\
    \hline
300D Gumbel TreeLSTM \cite{choi2017unsupervised} & 2.9M&91.2 &85.6 \\
    300D DISAN \cite{DBLP:journals/corr/abs-1709-04696}&2.4M &91.1 & 85.6\\
    300D Residual Stacked Encoders \cite{DBLP:conf/repeval/NieB17} & 9.7M & 89.8& 85.7\\
    600D Gumbel TreeLSTM \cite{choi2017unsupervised}& 10M & 93.1 & \textbf{86.0}\\
    300D \textsc{CAFE} (w/o CA)  & 3.7M& 87.3 & \underline{85.9}\\
    \hline
    \multicolumn{4}{|c|}{\textbf{Single Models}} \\
    \hline
    100D LSTM with attention \cite{rocktaschel2015reasoning} & 250K  & 85.3  & 83.5 \\
300D mLSTM  \cite{DBLP:conf/naacl/WangJ16}& 1.9M  & 92.0    & 86.1 \\
    450D LSTMN + deep att. fusion \cite{DBLP:conf/emnlp/0001DL16} & 3.4M  & 88.5  & 86.3 \\
200D DecompAtt + Intra-Att \cite{DBLP:conf/emnlp/ParikhT0U16}  & 580K  & 90.5  & 86.8 \\
    300D NTI-SLSTM-LSTM \cite{DBLP:conf/eacl/YuM17} & 3.2M  & 88.5  & 87.3 \\
    300D re-read LSTM \cite{DBLP:conf/coling/ShaCSL16} & 2.0M  & 90.7  & 87.5 \\
    BiMPM \cite{DBLP:conf/ijcai/WangHF17} & 1.6M & 90.9 & 87.5 \\
     448D DIIN \cite{DBLP:journals/corr/abs-1709-04348} & 4.4M  & 91.2  & 88.0 \\
    600D ESIM \cite{DBLP:conf/acl/ChenZLWJI17} & 4.3M  & 92.6  & 88.0 \\

    \hline
    150D \textsc{CAFE} (SUM+2x200D MLP)  & 750K & 88.2 & 87.7 \\
    200D \textsc{CAFE} (SUM+2x400D MLP)  & 1.4M & 89.4 & 88.1 \\
    300D \textsc{CAFE} (SUM+2x600D MLP)  & 3.5M & 89.2&  \underline{88.3}\\
    300D \textsc{CAFE} (AVGMAX+300D HN)  & 4.7M &89.8 &  \textbf{88.5}\\
    \hline
    \multicolumn{4}{|c|}{\textbf{Ensemble Models}} \\
    \hline
    600D ESIM + 300D Tree-LSTM \cite{DBLP:conf/acl/ChenZLWJI17}  & 7.7M & 93.5 & 88.6 \\
    BiMPM \cite{DBLP:conf/ijcai/WangHF17}  & 6.4M  & 93.2 & 88.8 \\
    448D DIIN \cite{DBLP:journals/corr/abs-1709-04348} & 17.0M   & 92.3 &  88.9 \\
    300D \textsc{CAFE} (Ensemble) & 17.5M   & 92.5 & \textbf{89.3} \\
    \hline
     \multicolumn{4}{|c|}{\textbf{External Resource Models}} \\
     \hline
      BiAttentive Classification + CoVe + Char \cite{mccann2017learned} & 22M & 88.5 & 88.1 \\
     KIM \cite{chen2017natural} & 4.3M& 94.1 & 88.6\\
     ESIM + ELMo \cite{peters2018deep} & 8.0M & 91.6 & 88.7 \\
         200D \textsc{CAFE} (AVGMAX + 200D MLP) + ELMo & 1.4M & 89.5 & \textbf{89.0} \\
\hline


    \end{tabular}\caption{Performance comparison of all published models on the SNLI benchmark.}
  \label{tab:snli_results}\end{table*}

In this section, we describe our experimental setup and report our experimental results.
\subsection{Experimental Setup}
To ascertain the effectiveness of our models, we use the SNLI \cite{DBLP:conf/emnlp/BowmanAPM15} and MultiNLI \cite{DBLP:journals/corr/WilliamsNB17} benchmarks which are standard and highly competitive benchmarks for the NLI task. We also include the newly released SciTail dataset \cite{scitail} which is a binary entailment classification task constructed from science questions. Notably, SciTail is known to be a difficult dataset for NLI, made evident by the low accuracy scores even though it is binary in nature.

\paragraph{SNLI} The state-of-the-art competitors on this dataset are the BiMPM \cite{DBLP:conf/ijcai/WangHF17}, ESIM \cite{DBLP:conf/acl/ChenZLWJI17} and DIIN \cite{DBLP:journals/corr/abs-1709-04348}. We compare against competitors across three settings. The first setting disallows cross sentence attention. In the second setting, cross sentence is allowed. The last (third) setting is a comparison between model ensembles while the first two settings only comprise single models. Note that we consider the 1st setting to be relatively less important (since our focus is not on the encoder itself) but still report the results for completeness.


\paragraph{MultiNLI} We compare on two test sets (\textit{matched} and \textit{mismatched}) which represent in-domain and out-domain performance. The main competitor on this dataset is the ESIM model, a powerful state-of-the-art SNLI baseline. We also compare with ESIM + Read \cite{DBLP:journals/corr/Weissenborn17}.
\paragraph{SciTail} This dataset only has one official setting. We compare against the reported results of ESIM \cite{DBLP:conf/acl/ChenZLWJI17} and DecompAtt \cite{DBLP:conf/emnlp/ParikhT0U16} in the original paper. We also compare with DGEM, the new model proposed in \cite{scitail}.

Across all experiments and in the spirit of fair comparison, we only compare with works that (1) do not use extra training data and (2) do not use external resources (such as external knowledge bases, etc.). However, for the sake of completeness, we still report their scores\footnote{Additionally, we added ELMo \cite{peters2018deep} to our CAFE model at the embedding layer. We report CAFE + ELMo under external resource models. This was done post review after EMNLP. Due to resource constraints, we did not train CAFE + ELMo ensembles but a single run (and single model) of CAFE + ELMo already achieves 89.0 score on SNLI. } \cite{mccann2017learned,chen2017natural,peters2018deep}.

\subsection{Implementation Details}
We implement our model in TensorFlow \cite{tensorflow2015-whitepaper} and train them on Nvidia P100 GPUs. We use the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with an initial learning rate of . L2 regularization is set to . Dropout with a keep probability of  is applied after each fully-connected, recurrent or highway layer. The batch size is tuned amongst . The number of latent factors  for the factorization layer is tuned amongst . The size of the hidden layers of the highway network layers are set to . All parameters are initialized with xavier initialization.  Word embeddings are pre-loaded with  GloVe embeddings \cite{DBLP:conf/emnlp/PenningtonSM14} and fixed during training. Sequence lengths are padded to batch-wise maximum. The batch order is (randomly) sorted within buckets following \cite{DBLP:conf/emnlp/ParikhT0U16}.


\subsection{Experimental Results}
Table \ref{tab:snli_results} reports our results on the SNLI benchmark. On the cross sentence (single model setting), the performance of our proposed CAFE model is extremely competitive. We report the test accuracy of CAFE at different extents of parameterization, i.e., varying the size of the LSTM encoder, width of the pre-softmax hidden layers and final pooling layer. CAFE obtains  accuracy on the SNLI test set, an extremely competitive score on the extremely popular benchmark. Notably, competitive results can be also achieved with a much smaller parameterization. For example, CAFE also achieves  and  test accuracy with only  and  parameters respectively. This outperforms the state-of-the-art ESIM and DIIN models with only a fraction of the parameter cost. At , our model has about three times less parameters than ESIM/DIIN (i.e., 1.4M versus 4.3M/4.4M). Moreover, our lightweight adaptation achieves  with only  parameters, which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ().



Finally, an ensemble of  CAFE models achieves  test accuracy, the best test scores on the SNLI benchmark to date\footnote{As of 22nd May 2018, the deadline of the EMNLP submisssion.}. Overall, we believe that the good performance of our CAFE can be attributed to (1) the effectiveness of the ComProp architecture (i.e., providing word representations with global and local knowledge for better representation learning) and (2) the expressiveness of alignment factorization layers that are used to decompose and compare word alignments. More details are given at the ablation study. Finally, we emphasize that CAFE is also relatively lightweight, efficient and fast to train given its performance. A single run on SNLI takes approximately  minutes per epoch with a batch size of . Overall, a single run takes  hours to get to convergence.


\begin{table}[htbp]
  \centering
\small
    \begin{tabular}{|l|cc|c|}
    \hline
    & \multicolumn{2}{c}{MultiNLI} & SciTail \\
    \hline
    Model & \multicolumn{1}{l}{Match} & \multicolumn{1}{|l|}{Mismatch} & - \\
    \hline
    Majority & 36.5 & 35.6 & 60.3 \\
    NGRAM & - & - & 70.6 \\
CBOW & 65.2 & 64.8 & - \\
    BiLSTM  & 69.8 & 69.4 & -\\
    \hline
     ESIM & 72.4  & 72.1 & 70.6 \\
    DecompAtt - & - & - &72.3\\
    DGEM & - & - &70.8 \\
    DGEM + Edge & -  & - &77.3 \\
   \hline
    ESIM  & 76.3  & 75.8 & -\\
    ESIM + Read  & 77.8 & 77.0 & - \\
    \hline
    \textsc{CAFE} & 78.7  & 77.9 & \textbf{83.3} \\
    CAFE Ensemble & \textbf{80.2}& \textbf{79.0} & - \\
    \hline
    \end{tabular}\caption{Performance comparison (accuracy) on MultiNLI and SciTail. Models with ,  and  are reported from \cite{DBLP:journals/corr/Weissenborn17}, \cite{scitail} and \cite{DBLP:journals/corr/WilliamsNB17} respectively.}\label{multinli_results}
\end{table}

Table \ref{multinli_results} reports our results on the MultiNLI and SciTail datasets. On MultiNLI, CAFE significantly outperforms ESIM, a strong state-of-the-art model on both settings. We also outperform the ESIM + Read model \cite{DBLP:journals/corr/Weissenborn17}. An ensemble of CAFE models achieve competitive result on the MultiNLI dataset. On SciTail, our proposed CAFE model achieves state-of-the-art performance. The performance gain over strong baselines such as DecompAtt and ESIM are  in terms of accuracy. CAFE also outperforms DGEM, which uses a graph-based attention for improved performance, by a significant margin of . As such, empirical results demonstrate the effectiveness of our proposed CAFE model on the challenging SciTail dataset.




\subsection{Ablation Study}
\begin{table}[htbp]
  \centering
  \small

    \begin{tabular}{|p{4cm}|cc|}
    \hline
          & \multicolumn{1}{l}{Match} & \multicolumn{1}{l|}{Mismatch} \\
          \hline
    Original Model & 79.0    & 78.9 \\
    \hline
    (1a) Rm FM for 1L-FC & 77.7  & 77.9 \\
(1b) Rm FM for 1L-FC (ReLU) & 77.3  & 77.5 \\
    (1c) Rm FM for 2L-FC (ReLU) & 76.6  & 76.4 \\
    \hline
    (2) Remove Char Embed & 78.1  & 78.3 \\
        (3) Remove Syn Embed & 78.3  & 78.4 \\
    (4) Remove Inter Att & 75.2  & 75.6 \\
(5) Replace HW Pred. with FC & 77.7  & 77.9 \\
    (6) Replace HW Enc. with FC & 78.7  & 78.7 \\
    (7) Remove Sub Feat & 77.9  & 78.3 \\
    (8) Remove Mul Feat & 78.7  & 78.6 \\
    (9) Remove Concat Feat & 77.9  & 77.6 \\
    (10) Add Bi-directional & 78.3  & 78.4 \\
    \hline
    \end{tabular}\caption{Ablation study on MultiNLI development sets. HW stands for Highway.}
  \label{tab:ablation}\end{table}Table \ref{tab:ablation} reports ablation studies on the MultiNLI development sets. In (1), we replaced all FM functions with regular full-connected (FC) layers in order to observe the effect of \textbf{FM versus FC}. More specifically, we experimented with several FC configurations as follows: (a) 1-layer linear, (b) 1-layer ReLU (c) 2-layer ReLU. The 1-layer linear setting performs the best and is therefore reported in Table \ref{tab:ablation}. Using ReLU seems to be worse than nonlinear FC layers. Overall, the best combination (option a) still experienced a decline in performance in both development sets.

In (2-3), we explore the utility of using character and syntactic embeddings, which we found to have helped CAFE marginally. In (4), we remove the inter-attention alignment features, which naturally impact the model performance significantly. In (5-6), we explore the effectiveness of the highway layers (in prediction layers and encoding layers) by replacing them to FC layers. We observe that both highway layers have marginally helped the overall performance. Finally, in (7-9), we remove the alignment features based on their composition type. We observe that the \textit{Sub} and \textit{Concat} compositions were more important than the \textit{Mul} composition. However, removing any of the three will result in some performance degradation. Finally, in (10), we replace the LSTM encoder with a BiLSTM, observing that adding bi-directionality did not improve performance for our model.




\subsection{Linguistic Error Analysis}



We perform a linguistic error analysis using the supplementary annotations provided by the MultiNLI dataset. We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma \cite{DBLP:journals/corr/WilliamsNB17}.
Table \ref{tab:error} reports the result of our error analysis. We observe that our CAFE model generally outperforms ESIM on \textit{most categories}.

\begin{table}[htbp]
  \centering
  \small

    \begin{tabular}{|l|cccc|}
    \hline
          & \multicolumn{2}{c}{Matched}        & \multicolumn{2}{c|}{Mismatched}  \\

          & \multicolumn{1}{c}{ESIM} & \multicolumn{1}{c}{CAFE} & \multicolumn{1}{c}{ESIM} & \multicolumn{1}{c|}{CAFE} \\
          \hline
    Conditional & 100   & 70    & 60    & \textbf{85} \\
    Word overlap & 50    & \textbf{82} & 62    & \textbf{87} \\
    Negation & \textbf{76}    & \textbf{76}    & 71    & \textbf{80} \\
    Antonym & 67    & \textbf{82} & 58    & \textbf{80} \\
    Long Sentence & 75    & \textbf{79} & 69    & \textbf{77} \\
    Tense Difference & 73    & \textbf{82} & 79    & \textbf{89} \\
    Active/Passive & 88    & \textbf{100} & \textbf{91}    & 90 \\
    Paraphrase & \textbf{89}    & 88    & 84    & \textbf{95} \\
    Quantity/Time  & 33    & \textbf{53} & 54    & \textbf{62} \\
    Coreference & \textbf{83}    & 80    & 75    & \textbf{83} \\
    Quantifier & 69    & \textbf{75} & 72    & \textbf{80} \\
    Modal & 78    & \textbf{81} & 76    & \textbf{81} \\
    Belief & 65    & \textbf{77} & 67    & \textbf{83} \\
    \hline
    \end{tabular}\caption{Linguistic Error Analysis on MultiNLI dataset. }
  \label{tab:error}\end{table}

On the mismatched setting, CAFE outperforms ESIM in 12 out of 13 categories, losing only in one percentage point in \textit{Active/Passive} category. On the matched setting, CAFE is outperformed by ESIM very marginally on coreference and paraphrase categories. Despite generally achieving much superior results, we noticed that CAFE performs poorly on \textit{conditionals\footnote{This only accounts for  of samples.}} on the matched setting. Measuring the absolute ability of CAFE, we find that CAFE performs extremely well in handling linguistic patterns of \textit{paraphrase detection} and \textit{active/passive}. This is likely to be attributed by the alignment strategy that CAFE and ESIM both exploits.


\subsection{Interpreting and Visualizing with CAFE}

Finally, we also observed that the propagated features are highly interpretable, giving insights to the inner workings of the CAFE model.
\begin{figure}[ht!]
  \centering
 \begin{subfigure}[b]{0.44\textwidth}
   \includegraphics[width=1\linewidth, height=0.6\textwidth]{images/snli_river_p.pdf}
   \label{fig:ex1a}
\end{subfigure}
\vspace{-1em}
\begin{subfigure}[b]{0.44\textwidth}
   \includegraphics[width=1\linewidth,height=0.6\textwidth]{images/snli_river_h.pdf}
   \label{fig:ex1bb}
\end{subfigure}
\caption{Visualization of \textit{six} Propgated Features (\textit{Best viewed in color}). Legend is denoted by inter,intra followed by the operations \textit{mul}, \textit{sub} or \textit{cat} (concat).} \label{cafe_analysis}
\end{figure}
Figure \ref{cafe_analysis} shows a visualization of the feature values from an example in the SNLI test set. The ground truth is \textit{contradiction}. Based on the above example we make several observations. Firstly, \textit{inter\_mul} features mostly capture identical words (or semantically similar words), i.e., \textit{inter\_mul} features for \textit{`river'} spikes in both sentences. Secondly, \textit{inter\_sub} spikes on conflicting words that might cause contradiction, e.g., \textit{`sedan'} and \textit{`land rover'} are not the same vehicle. Another interesting observation is that we notice the \textit{inter\_sub} features for \textit{driven} and \textit{stuck} spiking. This also validates the observation of \cite{DBLP:conf/acl/ChenZLWJI17}, which shows what the \textit{sub} vector in the ESIM model is looking out for contradictory information. However, our architecture allows the inspection of these vectors since they are compressed via factorization, leading to larger extents of explainability - a quality that neural models inherently lack. We also observed that intra-attention (e.g., intra\_cat) features seem to capture the more important words in the sentence (\textit{`river'}, \textit{`sedan'}, \textit{`land rover'}).



\section{Conclusion}
We proposed a new neural architecture, CAFE for NLI. CAFE achieves very competitive performance on three
benchmark datasets. Extensive ablation studies confirm the effectiveness of FM layers over FC layers. Qualitatively, we show how different compositional operators (e.g., sub and mul) behave in NLI task and shed light on why subtractive composition helps in other models such as ESIM.



\bibliographystyle{acl_natbib}
\bibliography{acl2018}


\end{document}
