\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
\label{sec:datasets}

\begin{table}
\setlength\extrarowheight{1pt}
\begin{center}
\small
\begin{tabular}{l|r|r|r}
    \hline
    \textbf{Dataset} & \textbf{\#Images} &  \textbf{\#Classes} & \textbf{\#Avg}\\
    \hline\hline
    iNaturalist Train~\cite{vms+18} &  &  &  \\
    iNaturalist Test~\cite{vms+18} &  &  &  \\
    VehicleID Train~\cite{ltw+16} &  &  &  \\
    VehicleID Test~\cite{ltw+16} &  &  &  \\
    SOP Train~\cite{ohb16} &  &   &  \\
    SOP Test~\cite{ohb16} &  &   &  \\
    Cars196 Train~\cite{ksd+13} &  &  &  \\
    Cars196 Test~\cite{ksd+13} &  &  &  \\
    \hline
    Oxford~\cite{rit+18} &  &  & n/a \\
    Paris~\cite{rit+18} &  &  & n/a \\
    GLDv1~\cite{nas+17} &  &  &  \\
    \hline
\end{tabular}
\end{center}
\vspace{-1.5em}
\caption{Dataset composition for training and evaluation.}
\label{tab:datasets}
\vspace{-1em}
\end{table}

The training and evaluation is performed on four widely used image retrieval benchmarks, namely iNaturalist~\cite{vms+18}, PKU VehicleID~\cite{ltw+16}, Stanford Online Products~\cite{ohb16} (SOP) and Stanford Cars~\cite{ksd+13} (Cars196). Recall at top  retrieved images,  denoted by r@k, is one of the standard evaluation metrics in these benchmarks.  Metric r@k is 1 if at least one positive image appears in the top  list, otherwise 0. The metric is averaged across all queries. Note that this is different from the standard definition of recall in \equ{recall}. 

iNaturalist~\cite{vms+18} is firstly used by Brown~\etal~\cite{bxk+20}, whose setup we follow:  classes for training and  classes for testing. For VehicleID, according to the standard setup~\cite{ltw+16},  classes are used for training, and the evaluation is conducted on the predefined small ( classes), medium ( classes) and large ( classes) test sets. For SOP~\cite{ohb16} and Cars196~\cite{ksd+13}, the standard experimental setup of Song~\etal~\cite{sxj+15} is followed. The first half of the classes are used for training and the rest for testing, resulting in  classes for SOP and  for Cars196.

The method is evaluated for instance-level search on Revisited Oxford (Oxford) and Paris (Paris) benchmark~\cite{rit+18}, where the evaluation metric is mean Average Precision (mAP). The training uses the Google Landmarks dataset (GLDv1)~\cite{nas+17} to perform a comparison with the work of Revaud~\etal~\cite{rar+19} and their AP loss. The validation is performed according to the work of Tolias~\etal~\cite{tjc20}. 

The number of examples, classes, and average number of examples per class can be found in Table. \ref{tab:datasets}. Note that these datasets are diverse in the number of training examples, the number of classes, and the number of examples per class, ranging from class balanced~\cite{ksd+13} to long-tailed~\cite{vms+18}.



\subsection{Implementation details}
\label{sec:implementation_details}

Implementation details are identical for the four image retrieval benchmarks but differ for  Oxford/Paris to follow and compare to prior work~\cite{rar+19}. Differences are clarified when needed.

\paragraph{Architecture.} An ImageNet~\cite{dsl+09} pre-trained ResNet-50~\cite{hzr+16} is used as the backbone for deep image embeddings. Building on the standard implementation of~\cite{rms+20}, the BatchNorm parameters are kept frozen during the training. After the convolutional layers, Generalized mean pooling~\cite{rtc19} and layer normalization~\cite{bkh+16} are used,  similar to~\cite{tdt20}. For vision transformers~\cite{dbk+21} ViT-B/32 and ViT-B/16 with an ImageNet-21k initialization from the timm library~\cite{rw2019timm} are used. The last layer of the model is a  dimensional fully connected (FC) layer with  normalization. In the case of Oxford/Paris, ResNet-101~\cite{hzr+16} is used, layer normalization is not added, while the FC layer is initialized with the result of whitening~\cite{rtc19}.

\paragraph{Training hyper-parameters.} For ResNet architectures, Adam optimizer~\cite{kb15} is used and for vision transformers, AdamW~\cite{lh+19} is used. This paper follows the standard class-balanced-sampling~\cite{mbl20,bxk+20,tdt20} with  samples per class for all the datasets, while classes with less than  samples are not used for training. Unless stated otherwise, the batch size for training is set  for all datasets but Cars196 where it is equal to . Following the setup of ProxyNCA++~\cite{tdt20}, the training set is split into training and validation by using the first half of the classes for training and the other half for validation. With this split, a grid search determines the learning rate, decay steps, decay size and the total number of epochs. Once the hyper-parameters are fixed, training is conducted once on the entire training set and evaluated on the test set. When training on GLDv1 and testing on Oxford/Paris, the batch size is set to ~\cite{rar+19}, and training is performed for 500 batches, while other training hyper-parameters are set as in the work and GitHub implementation of Radenovic \etal~\cite{rtc19}. Note that the hyper-parameters for each dataset will be released with the implementation.

\paragraph{RS@k hyper-parameters.} The proposed Recall@k Surrogate (RS@k) loss \equ{smooth_recall} contains three hyper-parameters: sigmoid temperature  - applied on similarity differences, sigmoid temperature  - applied on ranks and the set of values for  for which the loss is computed. Both sigmoid temperatures are kept fixed across all the experiments as  (same as~\cite{bxk+20}) and . The values of  are kept fixed as  without SiMix and  with SiMix. For GLDv1~\cite{nas+17}, this is , and , respectively. The values of  are studied in the supplementary materials and the sigmoid temperature  are investigated in Section \ref{sec:effect_hyperparams}, where it is observed that the method is not very sensitive to these hyper-parameters. 

\paragraph{Large batch size.} To dispense with the GPU hardware constraints and manage to train with the large batch size, we follow the multistage back-propagation of Revaud~\etal~\cite{rar+19}. A forward pass is performed to obtain all embeddings while intermediate tensors are discarded from memory. Then, the loss is computed, and so are the gradients \wrt the embeddings. Finally, each of the embeddings is recomputed, this time allowing the propagation of the gradients. Note that there is no implementation online of this approach and that the code of this work will become publicly available. Algorithm \ref{alg:main} does not include such implementation details, but it is compatible with such an extension. The batch-size impact for the proposed RS@k loss function is validated in Section \ref{sec:effect_hyperparams}.

\paragraph{Discussion.} The methods in the literature use different embedding sizes, , therefore, the models for the RS@k loss are trained with two embedding sizes of  and  for image retrieval benchmarks \cite{vms+18,ltw+16,ohb16,ksd+13}, and  for Oxford/Paris \cite{rit+18}, to allow a fair comparison. In the standard split, the image retrieval benchmarks~\cite{ksd+13,ohb16,ltw+16,vms+18} do not contain an explicit validation set; as a result, image retrieval methods often tune the hyper-parameters on the test set, leading to the issue of training with test set feedback. This issue has been studied in~\cite{mbl20}, which proposes to train different methods with identical hyper-parameters. The setup of~\cite{mbl20} is not directly usable for experiments with the RS@k loss, as large batch sizes are crucial to estimate recall@k accurately. Furthermore, their setup does not allow mixup. Therefore, instead of following~\cite{mbl20}, the issue is eliminated by using a part of the training set for validation as described above. 


\subsection{Evaluation}
\label{sec:evaluation}

\begin{table*}[t]
\begin{center}
\footnotesize
\setlength{\tabcolsep}{1.5pt}
    \setlength\extrarowheight{-2pt}
    \begin{tabular}{@{\zsp}l@{\xssp}l@{\xssp}|cccc|cccc|cc|cc|cc|cccc}
    \hline
    \multirow{3}[1]{*}{Method}& \multirow{3}[1]{*}{Arch.} & \multicolumn{4}{c|}{iNaturalist \cite{vms+18}} & \multicolumn{4}{c|}{SOP \cite{ohb16}} & \multicolumn{6}{c|}{VehicleID \cite{ltw+16}} & \multicolumn{4}{c}{Cars196 \cite{ksd+13}}\\
    \cline{3-20}
    & & \multicolumn{15}{c}{r@k}\\ \cline{3-20}
    & & & & & & & & & & \multicolumn{2}{c|}{Small} & \multicolumn{2}{c|}{Medium} & \multicolumn{2}{c|}{Large} & & & & \\
    & & 1 & 4 & 16 & 32 &  &  &  &  & 1 & 5 & 1 & 5 & 1 & 5 & 1 & 2 & 4 & 8\\
    \hline \hline
    
	ProxyNCA \cite{mtl+17} & {\scriptsize} &
	 & 
	 & 
	 & 
	 &

	 & 
	- & 
	- & 
	- &
	
	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	Margin \cite{wms+17} & {\scriptsize} &
	 & 
	 & 
	 & 
	 &

	 &
	 & 
	 & 
	 &
	
	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	Divide \cite{skp+15} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 & 
	 & 
	 &
	 &

     &
     &
     &
     &
     &
     &
    
    - &
    - &
    - &
    -
	\\
	
	MIC \cite{rbo+19} & {\scriptsize} &
	- &
	- &
	- &
	- &

	 &
	 &
	 &
	-  &

     &
     &
    - &
    - &
     &
     &
    
    - &
    - &
    - &
    -

	\\
	
	Cont. w/M \cite{wzh+20} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &
    
    - &
    - &
    - &
    -
	\\
	
	\hline
	RS@k\textsuperscript{\dag} & {\scriptsize} &
	  &
	  &
	  &
	 &
	
	 &
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &
    
     &
     &
     &
    
	\\



	RS@k\textsuperscript{\dag} +SiMix & {\scriptsize} &
	 & 
	 & 
	 & 
	 &
	
	 &
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &
    
     &
     &
     &
     
	\\

	& & 
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &

	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &

	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}}
	\\
	




    \hline \hline
	FastAP \cite{chx+19} & {\scriptsize} &
	 & 
	 & 
	 & 
	 &

	 &
	 & 
	 & 
	 &

     &
     &
     &
     &
     &
     &
    
    - &
    - &
    - &
    -
	\\
	
	MS \cite{whh+19} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	 &

	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	NormSoftMax \cite{zw18} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	- &

	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	Blackbox AP \cite{rmp+20} & {\scriptsize} &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 &
	 &
	
	- &
	- &
	- &
	- &
	- &
	- &

    - &
    - &
    - &
    -
	\\
	
	Cont. w/M \cite{wzh+20} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &

    - &
    - &
    - &
    -
	\\
	
	HORDE \cite{jph+19} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	- &

	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	ProxyNCA++ \cite{tdt20} & {\scriptsize} &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	 &

	- &
	- &
	- &
	- &
	- &
	- &
	
	 &
	 &
	 &
	
	\\
	
	
	SAP \cite{bxk+20} & {\scriptsize} &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 & 
	 &

     &
     &
     &
     &
     &
     &
    
     &
     &
     &
    
	\\
	
	SAP\textsuperscript{\dag} \cite{bxk+20} {\tiny +GeM +LN} & {\scriptsize} &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &
    
     &
     &
     &
    
	\\

























    
	\hline







	RS@k\textsuperscript{\dag} & {\scriptsize} &
	 &
	 &
	 &
	 &
	
	 & 
	 & 
	 & 
	 &
	
     &
     &
     &
     &
     &
     &
    
     &
     &
     &
    
	\\




	RS@k\textsuperscript{\dag} +SiMix & {\scriptsize} &
	 &
	 &
	 &
	 &
	
	 & 
	 &
	 &
	 &

     &
     &
     &
     &
     &
     &
    
     &
     &
     &
    
	\\
	
	& & 
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &

	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &

	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{blue}{}} &
	\tiny{\textcolor{red}{}}
	\\



    \hline \hline
    
    SAP\textsuperscript{\dag} \cite{bxk+20} & {\scriptsize ViT-B/32} &
     &
	 &
	 &
	 &
	
     &
	 &
	 &
	 &
	
	 &
	 &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 &
	
	\\

    RS@k\textsuperscript{\dag} & {\scriptsize ViT-B/32} & 
     & 
     &
     &
     &

     & 
     &
     &
     &

     & 
     &
     &
     &
     &
     &
    
     &
     &
     &
    
    \\



	\hline
	
    SAP\textsuperscript{\dag} \cite{bxk+20} & {\scriptsize ViT-B/16} &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 &
	 &

	 &
	 &
	 &
	 &
	 &
	 &
	
	 &
	 &
	 &
	
	\\

    RS@k\textsuperscript{\dag} & {\scriptsize ViT-B/16} & 
     & 
     &
     &
     &
    
     & 
     &
     &
     &
    
     & 
     &
     &
     &
     &
     &
    
     &
     &
     &
    
    \\

\hline
    \end{tabular}     \vspace{-1em}
    \caption{Recall@ on iNaturalist~\cite{vms+18}, Stanford Online Products (SOP)~\cite{ohb16}, PKU VehicleID~\cite{ltw+16} and Stanford Cars (Cars196)~\cite{ksd+13}. Best results are shown with \textbf{bold}, previous state-of-the-art with \underline{underline} and relative gains over the state-of-the-art in \% of error reduction with \textcolor{blue}{blue} and relative declines in \textcolor{red}{red}. Methods marked with  were trained using the same pipeline by the authors of this paper.}
    \label{tab:metriclearning}
    \vspace{-2em}
\end{center}
\end{table*}


Unless otherwise stated, the results of the competing methods are taken from the original papers. Methods marked with a  were trained by the authors of this paper, using the same implementation as used for the RS@k loss. The results on image retrieval benchmarks~\cite{ksd+13,ohb16,ltw+16,vms+18} are compared with the methods that use either ResNet-50~\cite{hzr+16} or Inception network~\cite{slj+15}. ResNet-50~\cite{hzr+16} is represented as  in the tables and the standard Inception network~\cite{slj+15} as , the Inception network with BatchNorm as   (same as ~\cite{tdt20}). Here  is the embedding size. On all the datasets, the performance of the baseline, Smooth-AP (SAP)~\cite{bxk+20}, is also reported with Generalized mean pooling~\cite{rtc19} and layer normalization~\cite{bkh+16}, shown as SAP\textsuperscript{\dag} (+Gem +LN). This is to eliminate any performance boost in the comparisons that were caused by the architecture. Note that unless otherwise stated in our experiments, the batch size for SAP is set as , the same as the original implementation~\cite{bxk+20}. Further, we demonstrate the performance of SAP and RS@k on ViT-B architectures. The variant of ViT-B that uses a patch size of  is denoted by ViT-B/32 and the one that uses a patch size of  by ViT-B/16.

\paragraph{iNaturalist.} The results on iNaturalist~\cite{vms+18} species recognition are presented in Table \ref{tab:metriclearning}. The performances of the competing methods are taken from~\cite{bxk+20}, which uses the official implementations of these methods. It can be clearly seen that the RS@k outperforms classification and pairwise losses, including the three AP approximation losses, reaching the recall@1 score of  with SiMix, an error reduction of .

\paragraph{SOP.} The performance on SOP~\cite{ohb16} is presented in Table \ref{tab:metriclearning}, along with the comparisons with the competing methods. The proposed RS@k loss demonstrates clear state-of-the-art results, surpassing ProxyNCA++~\cite{tdt20} by  on recall@1, an error reduction of . If a smaller batch size, equal to , is used for RS@k, it reaches a performance of , ,  and  on r@, r@, r@ and r@ respectively. This result shows that large batch size helps in improving the performance, but RS@k outperforms the competing methods even with smaller batch size.

\paragraph{VehicleID.} The results on VehicleID~\cite{ltw+16} are presented in Table \ref{tab:metriclearning}. RS@k outperforms the competing methods both with and without SiMix. Better results were observed without SiMix where RS@k reaches recall@1 performance of ,  and  on the small, medium, and large test sets, respectively.


\begin{table*}[t!]
\tablestyle{3pt}{1.3}
\begin{center}
\setlength\extrarowheight{-2pt}
\newcommand{\xdagger}{^{\dagger}}
\def\arraystretch{1.0}\small
\begin{tabular}	{l@{\msp}r@{\msp}r@{\msp}r@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c@{\msp}c}
\hline
\multirow{2}{*}{Arch.}        &    \multirow{2}{*}{Loss} & \multirow{2}{*}{Train-set} & \multirow{2}{*}{} & \multicolumn{2}{c}{Mean}  &   \multicolumn{2}{c}{\ro} & \multicolumn{2}{c}{\ro\hspace{-3pt}+\rdis} & \multicolumn{2}{c}{\rpa} & \multicolumn{2}{c}{\rp\hspace{-3pt}+\rdis}     \\ \cline{5-14}
															 & &&    												     & all & \rdis &   med  &   hard         &      med  &   hard          &    med  &   hard      &           med  &   hard         \\ \hline \hline
GeM                                         & AP \cite{hls18} & Landmarks-clean~\cite{bsc+14}\cite{gar+17} & ~\cite{rar+19}/~\cite{tjc20}    & 49.7 & 36.7 & 67.1 & 42.3 & 47.8 & 22.5 & 80.3 & 60.9 & 51.9 & 24.6 \\             GeM                                         & AP \cite{hls18} & GLDv1~\cite{nas+17} & ~\cite{rar+19}/github    & - & -  & 66.3 & 42.5 & - & -  & 80.2 & 60.8  & -  & - \\             GeM\dag                & SAP \cite{bxk+20} & GLDv1~\cite{nas+17} & ~\cite{bxk+20}    & 52.7 & 40.6 & 67.9 & 46.3 &  49.5 & 25.8 & 81.7 & 63.3 & 57.4 & 29.8 \\     
GeM\dag                & RS@k & GLDv1~\cite{nas+17} & ours    & 53.1 & 41.0 & 68.3 & 46.1 & 50.1 & 25.8 & 82.1 & 63.9 & 57.9 & 30.2 \\            
GeM+SiMix\dag                & RS@k & GLDv1~\cite{nas+17} & ours    & 53.1 & 41.8 & 68.4 & 45.3 & 51.0 & 26.4 & 81.2 & 62.4 & 58.7 & 31.1\\            
 \hline
\end{tabular}
 \vspace{-1em}
\caption{Performance comparison (mAP\%) on Oxford and Paris with 1m distractor images (1m). Mean performance is reported across all setups or the large-scale setups only.  denotes that the FC layer is not part of the training but is added afterward to implement whitening. Batch size is 4096 for all methods; SiMix virtually increases it to 10240. ResNet101 is used as a backbone for all methods.
\label{tab:rop}}
\vspace{-2em}
\end{center}
\end{table*}

\paragraph{Cars196.} Evaluation on a small scale dataset, Cars196~\cite{ksd+13} is presented in the Table \ref{tab:metriclearning}. We train SAP with a batch size of ; it provides a performance of , , , and  and when combined with SiMix a performace of , ,  and  on r@, r@, r@ and r@ respectively. SiMix makes a large difference in performance for both RS@k and SAP~\cite{bxk+20}, primarily because of a smaller batch size (), as constrained by the low number of classes. With SiMix, RS@k reaches the state-of-the-art results on three out of four recall@k values. If the batch size is further increased to  by changing the number of samples per class from  to , then RS@k provides a larger gain with performance , ,  and .

\paragraph{Results with ViT-B.} The results by replacing the ResNet-50~\cite{hzr+16} backbone with a ViT-B~\cite{dbk+21} for SAP~\cite{bxk+20} and the proposed RS@k are also shown in Table \ref{tab:metriclearning}. With an exception of ViT-B/32 on VehicleID and Cars196 datasets, the use of ViT-B backbone leads to better performance for both methods, compared to the ResNet counterpart. It can be clearly seen that RS@k outperforms SAP~\cite{bxk+20} on all datasets. ViT-B/16 when trained with RS@k shows unprecedented performance on all datasets reaching recall@ score of  on iNaturalist~\cite{vms+18},  on SOP~\cite{ohb16},  on VehicleID~\cite{ltw+16} (small) and  on Cars196~\cite{ksd+13}. Note that while ResNet-50 has  M parameters and operates with  GMac/image, ViT-B has  M parameters and operates with  and  GMac/image for ViT-B/32 and ViT-B/16 respectively.

\paragraph{Concurrent work.} The method of learning intra-batch connections for deep metric learning~\cite{sel21} achieves r@1 of  on the SOP and  on Cars196 dataset. The approach for Grouplet embedding learning~\cite{zlx+21} obtains r@1 of  on SOP and  on Cars196. The metric mixup approach~\cite{vpa+21} reports the best results of  r@1 on SOP in combination with ProxyNCA++~\cite{tdt20} and  on Cars196 which is in combination with MS~\cite{whh+19}.

\paragraph{Oxford/Paris.} Table~\ref{tab:rop} summarizes a comparison with AP-based losses in the literature on Oxford/Paris with and without distractor images. The comparison is performed with GLDv1 as a training set whose performance is reported for the work of Revaud \etal~\cite{rar+19} in their GitHub page, while the \emph{landmarks-clean dataset} is avoided as all initial images are not publicly available at the moment. During the training performed by us, training images are down-sampled to have a maximum resolution of . 
The inference is performed with multi-resolution descriptors at three scales with up-sampling and down-sampling by a factor of .
Note that SAP is not evaluated on these datasets in the original work and this experiment is performed by us, which outperforms the previously used AP loss~\cite{hls18}. RS@k, with or without the SiMix, increases the performance by a small margin. 


\subsection{Effect of hyper-parameters}
\label{sec:effect_hyperparams}

We study the impact of hyper-parameter on the Cars196 dataset~\cite{ksd+13} since it is the smallest compared to the others and has the lowest training time. 

\paragraph{Sigmoid temperature  - applied on ranks.}
The effect of the sigmoid temperature  is summarized in Figure \ref{fig:ab_tau1_bs} (left). For both setups of with and without SiMix,  gives best results while higher and lower values lead to a decline.

\begin{figure}
\centering
\clearpage{}\raisebox{3pt}{
\pgfplotstableread{
 		temp	sop_r1		cars_r1    sop_simix_r1     cars_simix_r1
 		0.1		81.00	76.7	81.41       86.79
 		0.5		82.67	80.6	81.83       86.42
 		1.0		82.83	80.8	82.13       88.17
 		2.0		82.80	80.6	82.15       86.15
 		5.0		82.43	80.3	82.00       86.66
 	}{\yfccLambda}
 \begin{tikzpicture}
\begin{axis}[width=0.5\linewidth,
	height=0.5\linewidth,
	xlabel={\small },
	ylabel={\small r@1},
	title={varying temperature},
	legend cell align={left},
	legend pos=south east,
    legend style={cells={anchor=east}, font =\tiny, fill opacity=0.7, row sep=-2.5pt},
   	xtick={0.1,0.5,1.0,2.0,5.0},
   	xticklabels={0.1,0.5,1.0,2.0,5.0},
   	xmode=log,
   	ymin=70,
    grid=both,
]
	\addplot[color=magenta,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=temp, y expr={\thisrow{cars_r1}}] 
	\yfccLambda;
	\addlegendentry{RS@k};
	\addplot[color=magenta,     dashed, mark=*,  mark size=1.5, line width=1.0] table[x=temp, y expr={\thisrow{cars_simix_r1}}] \yfccLambda;
	\addlegendentry{RS@k+SiMix};
\end{axis}
\end{tikzpicture}
}
\pgfplotstableread{
 		bs_sop	sop_r1     bs_cars     cars_r1      cars_simix_r1   cars_r1_sap
 		16		72.9	    16          74.60       86.27           68.9
 		32		77.3	    32          77.04       87.46           71.91
 		64		79.0	    64          76.67       87.31           75.64
 		128		79.7	    128         78.92       87.43           77.92
 		256		80.5        256         79.14       87.06           78.48
 		512     81.2        392         80.72       88.17           79.50
 		1024    82.0        nan         nan         nan             nan
 		2048    82.5        nan         nan         nan             nan
 		4096    82.8        nan         nan         nan             nan
 		8192    82.4        nan         nan         nan             nan
}{\yfccLambda}
 \begin{tikzpicture}
\begin{axis}[width=0.5\linewidth,
	height=0.5\linewidth,
	xlabel={\small batch size},
	ylabel={\small r@1},
	title={varying batch size},
	legend cell align={left},
	legend pos=south east,
    legend style={cells={anchor=east}, font =\tiny, fill opacity=0.7, row sep=-2.5pt},
   	xtick={16,32,64,128,256,512,1024,2048,4096},
   	xmode=log,
   	log basis x={2},
    grid=both,
]
	\addplot[color=blue,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=bs_cars, y expr={\thisrow{cars_r1_sap}}]
	\yfccLambda;
	\addlegendentry{SAP};
	\addplot[color=magenta,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=bs_cars, y expr={\thisrow{cars_r1}}] 
	\yfccLambda;
	\addplot[color=magenta,     dashed, mark=*,  mark size=1.5, line width=1.0] table[x=bs_cars, y expr={\thisrow{cars_simix_r1}}]
	\yfccLambda;
\end{axis}
\end{tikzpicture}

\clearpage{}
\vspace{-3.0em}
\caption{The effect of sigmoid temperature  applied on ranks (left) and of batch size (right). Results are shown on Cars196~\cite{ksd+13}.}
\label{fig:ab_tau1_bs}
\vspace{-1.0em}
\end{figure}



\paragraph{Batch size.}
The effect of the varying batch size is shown in Figure \ref{fig:ab_tau1_bs} (right). It demonstrates that large batch size leads to better results. A significant performance boost is observed with the use of SiMix, especially in the small batch size regime, which comes at a small extra computation. A comparison with SAP~\cite{bxk+20} is also shown in this figure. Note that on smaller batch sizes, the proposed RS@k outperforms SAP with a larger margins.