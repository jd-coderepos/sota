\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{macro}
\usepackage{url}
\usepackage{qtree}
\usepackage{tikz-dependency}



\def\emnlppaperid{***}



\newcommand\BibTeX{B{\sc ib}\TeX}


\title{ Appendix: Structural Embedding of Syntactic Trees for Machine Comprehension }



\date{}
\begin{document}
\maketitle

We detail the neural network architecture that we used in Section 4, focusing on describing the attention layer and the output layer.

\section{Attention Layer}
Notice that our proposed structural embedding of syntactic trees can be easily applied to any attention approaches~\cite{xiong2016dynamic,seo2016bidirectional,wang2016multi,wang2016machine}. In this paper, we use the Bi-directional Attention flow~\cite{seo2016bidirectional} which performs the context-to-question and question-to-context attentions in both directions.

The inputs to the attention layer are the embeddings of context words and question words obtained from the embedding layer.  We denote the embeddings of  context words as  where , and denote the embeddings of  question words as  where . We first calculate the similarity matrix  where  measures the similarity between the -th context word and the -th question word by Equation~\ref{eq:sim}. 


where  is a trainable parameter,  is the elementwise multiplication and  is the vector concatenation along rows.

The context-to-question attention performs a soft attention over all question words for each context word. The attention weight is computed by  where  is the -th row of . Then each attended vector for each context word is calculated by .

The question-to-context attention first calculates the attention weight over the context words by , where max returns the maximum values across the columns in a matrix. Then the summarized context vector can be calculated by .  

Finally, we combine three input vectors, i.e., ,  and , for the -th context word by . The question-aware representation of the context, i.e., , is further encoded by a stacked Bi-directional LSTM with two layers to obtained .

\section{Output Layer}
For the machine comprehension task, the begin and end indices over the entire context is predicted by Equation~\ref{eq:start} and~\ref{eq:end} respectively. 

where  and  are trainable parameters and  is obtained by passing  to another bidirectional LSTM.

\section{Training}
We 
\bibliography{ref}
\bibliographystyle{emnlp_natbib}
\end{document}