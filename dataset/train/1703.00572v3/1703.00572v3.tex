\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{macro}
\usepackage{url}
\usepackage{qtree}
\usepackage{tikz-dependency}



\def\emnlppaperid{***}



\newcommand\BibTeX{B{\sc ib}\TeX}


\title{ Appendix: Structural Embedding of Syntactic Trees for Machine Comprehension }



\date{}
\begin{document}
\maketitle

We detail the neural network architecture that we used in Section 4, focusing on describing the attention layer and the output layer.

\section{Attention Layer}
Notice that our proposed structural embedding of syntactic trees can be easily applied to any attention approaches~\cite{xiong2016dynamic,seo2016bidirectional,wang2016multi,wang2016machine}. In this paper, we use the Bi-directional Attention flow~\cite{seo2016bidirectional} which performs the context-to-question and question-to-context attentions in both directions.

The inputs to the attention layer are the embeddings of context words and question words obtained from the embedding layer.  We denote the embeddings of $T$ context words as $\Hv=[\hv_1; \cdots; \hv_T]\in\Rb^{d\times T}$ where $\hv_t\in\Rb^d~\forall t$, and denote the embeddings of $J$ question words as $\Uv=[\uv_1; \cdots; \uv_J]\in\Rb^{d\times J}$ where $\uv_j\in\Rb^d~\forall j$. We first calculate the similarity matrix $\Sv\in\Rb^{T\times J}$ where $S_{tj}$ measures the similarity between the $t$-th context word and the $j$-th question word by Equation~\ref{eq:sim}. 

\begin{align}\label{eq:sim}
    S_{tj} = \Wv_{\Sv}^T[\hv_t; \uv_j; \hv_t \circ \uv_j]
\end{align}
where $\Wv_{\Sv}\in\Rb^{3d}$ is a trainable parameter, $\circ$ is the elementwise multiplication and $[;]$ is the vector concatenation along rows.

The context-to-question attention performs a soft attention over all question words for each context word. The attention weight is computed by $\av_t=\textnormal{softmax}(\Sv_{t:})\in\Rb^J$ where $\Sv_{t:}$ is the $t$-th row of $\Sv$. Then each attended vector for each context word is calculated by $\tilde{\hv}_t=\sum_{j=1}^J a_{tj} \uv_j$.

The question-to-context attention first calculates the attention weight over the context words by $\bv = \textnormal{softmax}(\textnormal{max}_{col}(\Sv)) \in \Rb^T$, where max$_{col}$ returns the maximum values across the columns in a matrix. Then the summarized context vector can be calculated by $\hat{\hv}=\sum_{t=1}^T b_t \hv_t\in \Rb^{d}$.  

Finally, we combine three input vectors, i.e., $\hv_t$, $\tilde{\hv}_t$ and $\hat{\hv}$, for the $t$-th context word by $\gv_t = [\hv_t; \tilde{\hv}_t; \hv_t \circ \tilde{\hv}_t; \hv_t \circ \hat{\hv}]\in \Rb^{4d}$. The question-aware representation of the context, i.e., $\Gv=[\gv_1, \cdots, \gv_T]\in\Rb^{4d\times T}$, is further encoded by a stacked Bi-directional LSTM with two layers to obtained $\Mv=[\mv_1; \cdots, \mv_T]in \Rb^{d\times T}$.

\section{Output Layer}
For the machine comprehension task, the begin and end indices over the entire context is predicted by Equation~\ref{eq:start} and~\ref{eq:end} respectively. 
\begin{align}\label{eq:start}
    \pv^1 = \textnormal{softmax}(\Wv^T_{\pv^1} [G;M]) \\\label{eq:end}
    \pv^2 = \textnormal{softmax}(\Wv^T_{\pv^2} [G;M^2])
\end{align}
where $\Wv_{\pv^1}$ and $\Wv_{\pv^2}$ are trainable parameters and $\Mv^2$ is obtained by passing $\Mv$ to another bidirectional LSTM.

\section{Training}
We 
\bibliography{ref}
\bibliographystyle{emnlp_natbib}
\end{document}