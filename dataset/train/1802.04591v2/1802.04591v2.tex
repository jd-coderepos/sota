\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{amsmath,mathtools}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{rotating}      \usepackage{bbding}

\usepackage{microtype}
\usepackage{booktabs} \usepackage[marginal]{footmisc}
\setlength{\footnotemargin}{1mm}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2018}

\icmltitlerunning{First Order Generative Adversarial Networks}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\conv}{Conv}
\DeclareMathOperator{\nconv}{NConv}
\DeclareMathOperator{\opt}{OPT}
\DeclareMathOperator{\oc}{OC}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator*{\myexpectation}{\mathbb{E}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{requirement}{Requirement}

\includeonly{appendix}

\begin{document}


\twocolumn[
\icmltitle{First Order Generative Adversarial Networks}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Calvin Seward}{zr,jku}
\icmlauthor{Thomas Unterthiner}{jku}
\icmlauthor{Urs Bergmann}{zr}
\icmlauthor{Nikolay Jetchev}{zr}
\icmlauthor{Sepp Hochreiter}{jku}
\end{icmlauthorlist}

\icmlaffiliation{zr}{Zalando Research, M\"uhlenstra√üe 25, 10243 Berlin, Germany}
\icmlaffiliation{jku}{LIT AI Lab \& Institute of Bioinformatics, Johannes Kepler University Linz, Austria}

\icmlcorrespondingauthor{Calvin Seward}{calvin.seward@zalando.de}


\icmlkeywords{Machine Learning, ICML, Generative Adversarial Networks}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective.
Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP.
To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction,
with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent.
We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic's first order information.
Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates.
We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task.
Code to reproduce experiments is available.
\end{abstract}

 \section{Introduction}
\setlength{\footnotemargin}{5mm}
 Generative adversarial networks (GANs) \cite{goodfellow2014generative} excel at learning generative models of complex
 distributions, such as images \cite{radford2015unsupervised,ledig2016photo},
 textures \cite{jetchev2016texture,BergmannJV17,jetchev2017ganosaic}, and
 even texts \cite{gulrajani2017improved,heusel2017gans}.

 GANs learn a generative model  that maps samples from multivariate random noise into a high dimensional space.
 The goal of GAN training is to update  such that the generative model approximates a target probability distribution.
 In order to determine how close the generated and target distributions are, a class of divergences, the so-called adversarial
 divergences was defined and explored by \cite{liu2017approximation}. This class is broad enough to encompass most popular GAN methods
 such as the original GAN \cite{goodfellow2014generative}, -GANs \cite{nowozin2016f}, moment matching networks \cite{li2015generative},
 Wasserstein GANs \cite{arjovsky2017wasserstein} and the tractable version thereof, the WGAN-GP \cite{gulrajani2017improved}.

 GANs learn a generative model with distribution  by minimizing an objective function
  measuring the similarity between target and generated distributions  and .
 In most GAN settings, the objective function to be minimized is an adversarial divergence \cite{liu2017approximation}, where a critic function is learned
 that distinguishes between target and generated data.
 For example, in the classic GAN \cite{goodfellow2014generative}
 the critic  classifies data as real or generated, and the generator  is encouraged
 to generate samples that  will classify as real.

 Unfortunately in GAN training, the generated distribution often fails to converge to the target distribution.
 Many popular GAN methods are unsuccessful with toy examples, for example failing to generate all modes of a
 mixture of Gaussians \cite{srivastava2017veegan,metz2016unrolled}
 or failing to learn the distribution of data on a one-dimensional line in a high dimensional space \cite{fedus2017many}.
 In these situations, updates to the generator don't significantly reduce the divergence between generated and target distributions;
 if there always was a significant reduction in the divergence then the generated distribution would converge to the target.

 The key to successful neural network training lies in the ability to efficiently obtain unbiased estimates of the gradients of a network's
 parameters with respect to some loss.
 With GANs, this idea can be applied to the generative setting. There, the generator  is parameterized by some
 values . If an unbiased estimate of the gradient of the divergence between target and generated distributions
 with respect to  can be obtained during mini-batch learning, then SGD can be applied to learn .

 In GAN learning, intuition would dictate updating the generated distribution by moving  in the direction of steepest descent
 . Unfortunately, 
 is generally intractable, therefore  is updated according to a tractable method; in most cases
 a critic  is learned and the gradient of the expected critic value
  is used as the update direction for .
 Usually, this update direction and the direction of steepest descent ,
 don't coincide and therefore learning isn't optimal.
 As we see later, popular methods such as WGAN-GP \cite{gulrajani2017improved} are affected by this issue.

 Therefore we set out to answer a simple but fundamental question:
 Is there an adversarial divergence and corresponding method that produces unbiased estimates of the direction of steepest descent in a mini-batch setting?

 In this paper, under reasonable assumptions, we identify a path
 to such an adversarial divergence and accompanying update method. Similar to the WGAN-GP,
 this divergence also penalizes a critic's gradients, and thereby ensures that
 the critic's first order information can be used directly to obtain an update direction
 in the direction of steepest descent.


 This program places four requirements
 on the adversarial divergence and the accompanying update rule for calculating the update direction
 that haven't to the best of our knowledge been formulated together.
 This paper will give rigorous definitions of these requirements, but for now we suffice with intuitive and informal definitions:
 \begin{enumerate}
  \item[A.] the divergence used must decrease as the target and generated distributions approach each other.
  For example, if we define the trivial distance between two probability distribution to be  if the distributions
  are equal, and  otherwise, i.e.\
  
  then even as  gets close to ,  doesn't change.
  Without this requirement,  and every direction is a ``direction of steepest descent,''
  \item[B.] critic learning must be tractable,
  \item[C.] the gradient  and the result of an update rule must be well defined,
  \item[D.] the optimal critic enables an update which is an estimate of .
 \end{enumerate}

 In order to formalize these requirements, we define in Section \ref{S:notation} the notions of adversarial divergences and
 optimal critics. In Section \ref{S:related_work} we will apply
 the adversarial divergence paradigm and begin to formalize the requirements above and better understand existing GAN methods.
 The last requirement is defined precisely in Section \ref{S:update_rule} where we explore criteria for an update rule guaranteeing a low
 variance unbiased estimate of the true gradient .

 After stating these conditions, we devote Section \ref{S:pen_divergence} to defining
 a divergence, the Penalized Wasserstein Divergence that fulfills the first
 two basic requirements. In this setting, a critic is learned, that similarly to the WGAN-GP critic, pushes real and generated
 data as far apart as possible while being penalized if the critic violates a Lipschitz condition.

 As we will discover, an optimal critic for the Penalized Wasserstein Divergence between two distributions need not be unique.
 In fact, this divergence only specifies the values that the optimal critic assumes on the supports of generated and target distributions.
 Therefore, for many distributions, multiple critics with different gradients on the support of the generated distribution can all be optimal.

 We apply this insight in Section \ref{S:fo_divergence} and add a gradient penalty to define the
 First Order Penalized Wasserstein Divergence. This divergence enforces not just correct values for the critic, but also
 ensures that the critic's gradient, its first order information, assumes values that allow for an easy formulation of an update rule.
 Together, this divergence and update rule fulfill all four requirements.

 We hope that this gradient penalty trick will be applied to other popular GAN methods and ensure that they too return better generator updates.
 Indeed, \cite{fedus2017many} improves existing GAN methods by adding a gradient penalty.

 Finally in Section \ref{S:experiments}, the effectiveness of our method is demonstrated by generating
 images and texts.

 \section{Notation, Definitions and Assumptions}\label{S:notation}

 In \cite{liu2017approximation} an adversarial divergence is defined:
\begin{definition}[Adversarial Divergence]\label{D:adversarial_divergence_old}
 Let  be a topological space,  the set of all continuous real valued functions over the Cartesian product 
 of  with itself and set , . An adversarial divergence  over  is a function
 
\end{definition}

The function class  must be carefully selected if  is to be reasonable.
For example, if  then the divergence between two Dirac distributions , and if , i.e.\ 
contains only the constant function which assumes zero everywhere, then .

Many existing GAN procedures can be formulated as an adversarial divergence. For example, setting
 
 
results in ,
the divergence in Goodfellow's original GAN \cite{goodfellow2014generative}.
See \cite{liu2017approximation} for further examples.

For convenience, we'll restrict ourselves to analyzing a special case of the adversarial divergence
(similar to Theorem 4 of \cite{liu2017approximation}), and use the notation:

\begin{definition}[Critic Based Adversarial Divergence]\label{D:adversarial_divergence}
 Let  be a topological space, , .
 Further let , ,  and .
 Then define
 
 and set .
\end{definition}

For example, the  from above can be equivalently defined by setting , ,  and . Then
 
is a critic based adversarial divergence.

An example with a non-zero  is the WGAN-GP \cite{gulrajani2017improved}, which is a critic based adversarial divergence when ,
the set of all differentiable real functions on ,
,  and

Then the WGAN-GP divergence  is:


While Definition \ref{D:adversarial_divergence_old} is more general, Definition \ref{D:adversarial_divergence} is more in line with most GAN models.
In most GAN settings, a critic in the simpler  space is learned that separates real and generated data
while reducing some penalty term  which depends on both real and generated data. For this reason, we use exclusively the notation from Definition \ref{D:adversarial_divergence}.

One desirable property of an adversarial divergence is that  obtains its infimum if and only if
, leading to the following definition adapted from \cite{liu2017approximation}:

\begin{definition}[Strict adversarial divergence]
 Let  be an adversarial divergence over a topological space .
  is called a strict adversarial divergence if for any ,
 
\end{definition}

In order to analyze GANs that minimize a critic based adversarial divergence, we introduce the set of optimal critics.
\begin{definition}[Optimal Critic,\; ]\label{D:opt_critic}
Let  be a critic based adversarial divergence over a topological space  and
, , .
 Define  to be the set of critics in  that maximize .
 That is

\end{definition}
Note that  is possible, \cite{arjovsky2017towards}. 
In this paper, we will always assume that if , 
then an optimal critic  is known. Although is an unrealistic assumption,
see \cite{binkowski2018demystifying}, it is a good starting point for a rigorous GAN analysis.
We hope further works can extend our insights to more realistic cases of approximate critics.



Finally, we assume that generated data is distributed according to a probability distribution 
parameterized by  satisfying the mild regularity Assumption \ref{A:1}. Furthermore, we assume that  and  both have
compact and disjoint support in Assumption \ref{A:2}. Although we conjecture that weaker assumptions can be made, we decide for the stronger assumptions
to simplify the proofs.

\begin{assumption}[Adapted from \cite{arjovsky2017wasserstein}]\label{A:1}
 Let . We say , 
 satisfies assumption \ref{A:1} if there is a locally Lipschitz function 
 which is differentiable in the first argument
 and a distribution  with bounded support in  such that
 for all  it holds 
 where .
\end{assumption}

\begin{assumption}[Compact and Disjoint Distributions]\label{A:2}
 Using  from Assumption \ref{A:1}, we say that  and 
 satisfies Assumption \ref{A:2} if for all
  it holds that the supports of
  and  are compact and disjoint.
\end{assumption}


\section{Requirements Derived From Related Work}\label{S:related_work}
With the concept of an Adversarial Divergence now formally defined, we can investigate
existing GAN methods from an Adversarial Divergence minimization standpoint.
 During the last few years, weaknesses in existing GAN frameworks have been highlighted and new frameworks have been
 proposed to mitigate or eliminate these weaknesses. In this section we'll trace this history and
 formalize requirements for adversarial divergences and optimal updates.

 Although using two competing neural networks for unsupervised learning isn't a new concept \cite{schmidhuber1992learning},
 recent interest in the field started when \cite{goodfellow2014generative} generated images with the divergence 
 defined in Eq.\ \ref{E:goodfellow_gan}.
 However, \cite{arjovsky2017towards} shows if  have compact disjoint support then
 , preventing the use of gradient based learning methods.

 In response to this impediment, the Wasserstein GAN was proposed in \cite{arjovsky2017wasserstein} with the divergence:
 
 where  is the Lipschitz constant of .
 The following example shows the advantage of . Consider a series of Dirac measures .
 Then  while . As  approaches
 , the Wasserstein divergence decreases while  remains constant.

 This issue is explored in \cite{liu2017approximation} by creating a weak ordering, the so-called strength, of divergences.
 A divergence  is said to be stronger than  if for any sequence of probability measures  and
 any target probability measure  the convergence
 
 implies .
 The divergences  and  are equivalent if  is stronger than  and  is stronger than .
 The Wasserstein distance  is the weakest divergence in the class of strict adversarial divergences \cite{liu2017approximation},
 leading to the following requirement:

 \begin{requirement}[Equivalence to ]\label{R:nonzero}
 An adversarial divergence  is said to fulfill Requirement \ref{R:nonzero} if  is a strict adversarial divergence
 which is weaker than .
 \end{requirement}

 The issue of the zero gradients was side stepped in \cite{goodfellow2014generative}
 (and the option more rigorously explored in \cite{fedus2017many})
 by not updating with  but instead using the gradient
 .
 As will be shown in Section \ref{S:update_rule}, this update direction doesn't generally move  in the direction of steepest descent.

 Although using the Wasserstein distance as a divergence between probability measures solves many theoretical problems,
 it requires that critics are Lipschitz continuous with Lipschitz constant .
 Unfortunately, no tractable algorithm has yet been found that is able to learn the optimal Lipschitz continuous critic
 (or a close approximation thereof).

 This is due in part to the fact that if the critic is
 parameterized by a neural network , , then the set of admissible parameters
  is highly non-convex.
 Thus critic learning is a non-convex optimization problem (as is generally the case in neural network learning)
 with non-convex constraints on the parameters.
 Since neural network learning is generally an unconstrained optimization problem, adding complex non-convex constraints makes learning intractable with current methods.
 Thus, finding an optimal Lipschitz continuous critic is a problem that can not yet be solved,
 leading to the second requirement:

 \begin{requirement}[Convex Admissible Critic Parameter Set]\label{R:convex_admissible}
 Assume  is a critic based adversarial divergence where critics are chosen from a set .
 Assume further that in training, a parameterization
  of the critic function  is learned.
 The critic based adversarial divergence  is said to fulfill requirement \ref{R:convex_admissible} if
 the set of admissible parameters  is convex.
 \end{requirement}

 It was reasoned in \cite{gulrajani2017improved} that since a Wasserstein critic must have gradients of norm at most  everywhere,
 a reasonable strategy would be to transform the constrained optimization into an unconstrained optimization problem
 by penalizing the divergence when a critic has non-unit gradients.
 With this strategy, the so-called Improved Wasserstein GAN or WGAN-GP divergence defined in Eq.\ \ref{E:WGAN_GP} is obtained.

 The generator parameters are updated by training an optimal critic  and updating with .
 Although this method has impressive experimental results, it is not yet ideal.
 \cite{petzka2017regularization} showed that an optimal critic for  has undefined gradients
 on the support of the generated distribution . Thus,
 the update direction  is undefined;
 even if a direction was chosen from the subgradient field (meaning the update direction is defined but random)
 the update direction won't generally point in the direction of steepest gradient descent.
 This naturally leads to the next requirement:

 \begin{requirement}[Well Defined Update Rule]\label{R:differentiable}
 An update rule is said to fulfill Requirement \ref{R:differentiable} on a target distribution 
 and a family of generated distributions  if
 for every  the update rule at  and  is well defined.
  \end{requirement}

 Note that kernel methods such as \cite{dziugaite2015training} and \cite{li2015generative} provide exciting theoretical guarantees
 and may well fulfill all four requirements. Since these guarantees come at a cost in scalability, we won't analyze them further.

\begin{table}
\newcommand{\nox}{\textcolor{red!70!black}{no}}\newcommand{\yescheck}{\textcolor{green!50!black}{\checkmark}}
\centering
  \caption{Comparing existing GAN methods with regard to the four Requirements formulated in this paper.
  The methods compared are the classic GAN \cite{goodfellow2014generative},
  WGAN \cite{arjovsky2017wasserstein}, WGAN-GP \cite{gulrajani2017improved},
  WGAN-LP \cite{petzka2017regularization}, DRAGAN \cite{dragan}, PWGAN (our method) and FOGAN (our method).
  }\label{Ta:whats_what}
  \begin{tabular}{lcccc}\toprule
  & Req. \ref{R:nonzero} & Req.\ \ref{R:convex_admissible} & Req.\ \ref{R:differentiable} & Req.\ \ref{R:fo_divergence} \tabularnewline \midrule
   GAN & \nox & \yescheck & \yescheck & \nox \tabularnewline
   WGAN & \yescheck & \nox & \yescheck & \yescheck \tabularnewline
   WGAN-GP & \yescheck & \yescheck & \nox & \nox \tabularnewline
   WGAN-LP & \yescheck & \yescheck & \nox & \nox \tabularnewline
   DRAGAN & \yescheck & \yescheck & \yescheck & \nox  \tabularnewline
   PWGAN & \yescheck & \yescheck & \nox & \nox  \tabularnewline
   FOGAN & \yescheck & \yescheck & \yescheck & \yescheck \tabularnewline \bottomrule
  \end{tabular}
 \end{table}

 \section{Correct Update Rule Requirement}\label{S:update_rule}

 In the previous section, we stated a bare minimum requirement for an update rule (namely that it is well defined). In this section, we'll go further
 and explore criteria for a ``good'' update rule. For example in Lemma \ref{L:wgan_counterexample} in Section \ref{S:proof_of_things} of Appendix,
 it is shown that there exists a target  and
 a family of generated distributions  fulfilling Assumptions \ref{A:1} and \ref{A:2} such that for the optimal critic
  there is no  so that
 
 for all 
 if all terms are well defined.
 Thus, the update rule used in the WGAN-GP setting, although well defined for this specific  and ,
 isn't guaranteed to move  in the direction of steepest descent. In fact, \cite{mescheder2018which} shows that
 the WGAN-GP does not converge for specific classes of distributions.
 Therefore, the question arises what well defined update rule also moves  in the direction of steepest descent?

 The most obvious candidate for an update rule is simply use the direction ,
 but since in the adversarial divergence setting  is the supremum over a set of infinitely many possible critics,
 calculating  directly is generally intractable.

 One strategy to address this issue is to use an envelope theorem \cite{milgrom2002envelope}.
 Assuming all terms are well defined, then for every optimal critic 
 it holds .
 This strategy is outlined in detail in \cite{arjovsky2017wasserstein} when proving the Wasserstein GAN update rule,
 and explored in the context of the classic GAN divergence  in \cite{arjovsky2017towards}.

 Yet in many GAN settings,
 \cite{goodfellow2014generative,arjovsky2017wasserstein,salimans2016improved,petzka2017regularization},
 the update rule is to train an optimal critic  and then take a step in the direction of .
 In the critic based adversarial divergence setting (Definition \ref{D:adversarial_divergence}), a direct result of Eq.\ \ref{E:adversarial_divergence}
 together with Theorem 1 from \cite{milgrom2002envelope} is that for every 
 
 when all terms are well defined. Thus, the update direction  only points in the direction of steepest descent for special choices of  and .
 One such example is the Wasserstein GAN where  and .

 Most popular GAN methods don't employ functions  and  such that the update direction  points in the direction of steepest descent.
 For example, with the classic GAN,  and , so the update direction  clearly is not oriented
 in the direction of steepest descent .
 The WGAN-GP is similar, since as we see in Lemma \ref{L:wgan_counterexample} in Appendix, Section \ref{S:proof_of_things},
  is not generally
 a multiple of .

 The question arises why this direction is used instead of directly calculating the direction of steepest descent?
 Using the correct update rule in Eq.\ \ref{E:update_rule} above involves estimating 
 , which requires sampling from both  and .
 GAN learning happens in mini-batches, therefore  isn't calculated directly, but estimated
 based on samples which can lead to variance in the estimate.

 To analyze this issue, we use the notation from \cite{bellemare2017cramer} where  are samples from  and
 the empirical distribution  is defined by
.
Further let  be the element-wise variance.
Now with mini-batch learning we get\footnote{Because the first expectation doesn't depend on , . In the same way,
 because the second expectation doesn't depend on the mini-batch  sampled,  .}

Therefore, estimation of  is an extra source of variance.

Our solution to both these problems chooses the critic based adversarial divergence 
in such a way that there exists a 
so that for all optimal critics  it holds

In Theorem \ref{T:fogan_theorem} we see conditions on  such that equality holds. Now using Eq.\ \ref{E:first_order_assumption} we see that

making  a low variance update approximation of the direction of steepest descent.

We're then able to have the best of both worlds.
On the one hand, when  serves as a penalty term, training of a critic neural network can happen in
an unconstrained optimization fashion like with the WGAN-GP.
At the same time, the direction of steepest descent can be approximated by calculating ,
and as in the Wasserstein GAN we get reliable gradient update steps.

With this motivation, Eq.\ \ref{E:first_order_assumption} forms the basis of our final requirement:
\begin{requirement}[Low Variance Update Rule]\label{R:fo_divergence}
 An adversarial divergence  is said to fulfill requirement \ref{R:fo_divergence} if
  is a critic based adversarial divergence and
 for every optimal critic  fulfills Eq.\ \ref{E:first_order_assumption}.
 \end{requirement}
 
It should be noted that the WGAN-GP achieves impressive experimental results; we conjecture that in
many cases  close enough to the true direction of steepest descent.
Nevertheless, as the experiments in Section \ref{S:experiments} show, our gradient estimates lead to better convergence in
a challenging language modeling task.

\section{Penalized Wasserstein Divergence}\label{S:pen_divergence}
 We now attempt to find an adversarial
 divergence that fulfills all four requirements. We start by formulating an adversarial divergence  and a corresponding update rule
 than can be shown to comply with Requirements \ref{R:nonzero} and \ref{R:convex_admissible}.
 Subsequently in Section \ref{S:fo_divergence},  will be refined to make its update rule
 practical and conform to all four requirements.

 The divergence  is inspired by the Wasserstein distance, there for an optimal critic between two Dirac distributions
  it holds . Now if we look at
 
 it's easy to calculate that , which is the same up to a constant (in this simple setting)
 as the Wasserstein distance, without being a constrained optimization problem. See Figure \ref{F:example} for an example.

 This has another intuitive explanation. Because Eq.\ \ref{E:simple_pwgan} can be reformulated as
 
 which is a tug of war between the objective  and the squared Lipschitz penalty  weighted by .
 This  term is important (and missing from \cite{gulrajani2017improved}, \cite{petzka2017regularization})
 because otherwise the slope of the optimal critic between  and  will depend on .

 The penalized Wasserstein divergence  is a straight-forward adaptation of  to the multi dimensional case.
 \begin{definition}[Penalized Wasserstein Divergence]\label{D:pen_divergence}
  Assume  and  are probability measures over  
  and . Set
  
  Define the penalized Wasserstein divergence as
  
  This divergence is updated by picking an optimal critic 
  and taking a step in the direction of .
 \end{definition}

 This formulation is similar to the WGAN-GP \cite{gulrajani2017improved}, restated here in Eq.\ \ref{E:WGAN_GP}.
 \begin{theorem}\label{T:pwgan_theorem}
  Assume , and  are probability measures over
   fulfilling Assumptions \ref{A:1} and \ref{A:2}.
  Then for every  the Penalized Wasserstein Divergence with it's corresponding update direction
  fulfills Requirements \ref{R:nonzero} and \ref{R:convex_admissible}.
  
  Further, there exists an optimal critic  that
  fulfills Eq.\ \ref{E:first_order_assumption} and thus Requirement \ref{R:fo_divergence}.
 \end{theorem}
 \begin{proof}
  See Appendix, Section \ref{S:proof_of_things}.
 \end{proof}

 Note that this theorem isn't unique to .
 For example, for the penalty in Eq.\ 8 of \cite{petzka2017regularization} we conjecture that a similar result can be shown.
 The divergence  is still very useful because, as will be shown in the next section,
  can be modified slightly to obtain a new critic , where every optimal critic
 fulfills Requirements \ref{R:nonzero} to \ref{R:fo_divergence}.
 
 Since  only constrains the value of a critic on the supports of  and , 
 many different critics are optimal, and in general  depends on the optimal critic choice and is thus is not well defined.
 With this, Requirements \ref{R:differentiable} and \ref{R:fo_divergence} are not fulfilled.
 See Figure \ref{F:example} for a simple example.
 
 \begin{figure}
\begin{minipage}[b]{.5\linewidth}
\centering
\includegraphics[width=\linewidth]{first_order_critic_font.pdf}
\subcaption{\footnotesize{First order critic}}\label{F:first_order_critic}
\end{minipage}\begin{minipage}[b]{.5\linewidth}
\centering
\includegraphics[width=\linewidth]{normal_critic_font.pdf}
\subcaption{\footnotesize{Normal critic}}\label{F:normal_critic}
\end{minipage}
\caption{
Comparison of  update rule given different optimal critics.
Consider the simple example of divergence  from Definition \ref{D:pen_divergence} between Dirac measures 
with update rule 
(the update rule is from Lemma \ref{L:first_order} in Appendix, Section \ref{S:proof_of_things}).
Recall that , and that so .
Let ; our goal is to calculate  via our update rule.
Since multiple critics are optimal for , we will explore how the choice of optimal critic affects the update.
In Subfigure \ref{F:first_order_critic}, we chose the first order optimal critic , and

and the update rule is correct (see how the red, black and green lines all intersect in one point). In Subfigure \ref{F:normal_critic},
the optimal critic is set to  which is not a first order critic
resulting in the update rule calculating an incorrect update.
}\label{F:example}
\end{figure}
 
 In theory, 's critic could be trained with a modified sampling procedure so that 
 is well defined and Eq.\ \ref{E:first_order_assumption} holds, as is done in both \cite{dragan} and \cite{unterthiner2017coulomb}.
 By using a method similar to \cite{bishop1998gtm}, one can minimize the divergence 
 where  is data equal to  where  is sampled from  and  is some zero-mean
 uniform distributed noise. In this way the support
 of  lives in the full space  and not the submanifold .
 Unfortunately, while this method works in theory,
 the number of samples required for accurate gradient estimates scales with the dimensionality of the underlying space ,
 not with the dimensionality of data or generated submanifolds  or .
 In response, we propose the First Order Penalized Wasserstein Divergence.
 
 \section{First Order Penalized Wasserstein Divergence}\label{S:fo_divergence}
 As was seen in the last section, since  only constrains the value of optimal critics on the supports of  and ,
 the gradient  is not well defined. A natural method to refine  to achieve
 a well defined gradient is to enforce two things:
 \begin{itemize}
  \item  should be optimal on a larger manifold, namely the manifold  that is created by
 ``stretching''  bit in the direction of  (the formal definition is below). 
  \item The norm of the gradient of the optimal critic,  on  should be equal to the norm of the maximal directional
  derivative in the support of  (see Eq.\ \ref{E:g_zero} in Appendix).
 \end{itemize}
 By enforcing these two points, we assure that  is well defined and points towards the real data . Thus, the following
 definition emerges (see proof of Lemma \ref{L:first_order} in Appendix, Section \ref{S:proof_of_things} for details).
 \begin{definition}[First Order Penalized Wasserstein Divergence (FOGAN)]\label{D:fo_divergence}
  Assume  and  are probability measures over .
  Set ,  and
  \begin{small}
  
  \end{small}
  Define the First Order Penalized Wasserstein Divergence as
  
  This divergence is updated by picking an optimal critic 
  and taking a step in the direction of .
 \end{definition}

 In order to define a GAN from the First Order Penalized Wasserstein Divergence, we must define a slight modification of
 the generated distribution  to obtain . Similar to the WGAN-GP setting,
 samples from  are obtained by  where  and .
 The difference is that , with  chosen small, making 
 and  quite similar. Therefore updates to  that reduce  also reduce
 .
 
 Conveniently, as is shown in Lemma \ref{L:adversary_subset} in Appendix, Section \ref{S:proof_of_things},
 any optimal critic for the First Order Penalized Wasserstein divergence
 is also an optimal critic for the Penalized Wasserstein Divergence.  
 The key advantage to the First Order Penalized Wasserstein Divergence is that for any ,  fulfilling Assumptions \ref{A:1}
 and \ref{A:2},  with its corresponding update rule
  on the slightly modified probability distribution 
  fulfills requirements \ref{R:differentiable} and \ref{R:fo_divergence}.
 

 \begin{theorem}\label{T:fogan_theorem}
  Assume , and  are probability measures over
   fulfilling Assumptions \ref{A:1} and \ref{A:2} and  is  modified
  using the method above.
  Then for every  there exists at least one optimal critic 
  and  combined with update direction  fulfills 
  Requirements \ref{R:nonzero} to \ref{R:fo_divergence}. If  are such that 
  it holds  for some constant , then equality holds for Eq.\ \ref{E:first_order_assumption}.
 \end{theorem}
 \begin{proof}
  See Appendix, Section \ref{S:proof_of_things}
 \end{proof}


 Note that adding a gradient penalty, other than being a necessary step for the WGAN-GP \cite{gulrajani2017improved}, 
 DRAGAN \cite{dragan} and Consensus Optimization GAN \cite{mescheder2017numerics},
 has also been shown empirically to improve the performance the original GAN method (Eq.\ \ref{E:goodfellow_gan}),
 see \cite{fedus2017many}. In addition, using stricter assumptions on the critic, \cite{nagarajan2017gradient} provides a theoretical justification for 
 use of a gradient penalty in GAN learning.
 The analysis of Theorem \ref{T:fogan_theorem} in Appendix, Section \ref{S:proof_of_things} provides a theoretical understanding
 why in the Penalized Wasserstein GAN setting adding a gradient penalty causes 
 to be an update rule that points in the direction of steepest descent, and may provide a path for other GAN methods to make similar assurances.

 \section{Experimental Results}\label{S:experiments}

 \subsection{Image Generation}
 We begin by testing the FOGAN on the CelebA image generation task \cite{liu2015deep},
 training a generative model with the DCGAN  architecture \cite{radford2015unsupervised} and obtaining Fr\'echet Inception Distance (FID) scores \cite{heusel2017gans}
 competitive with state of the art methods without doing a tuning parameter search.
 Similarly, we show competitive results on LSUN \cite{yu15lsun} and CIFAR-10 \cite{krizhevsky2009learning}.
 See Table \ref{Ta:results}, Appendix \ref{SS:appendix_celeba_words} and released code
 \footnote{\scriptsize{\url{https://github.com/zalandoresearch/first_order_gan}}}.

 \subsection{One Billion Word}
 Finally, we use the First Order Penalized Wasserstein Divergence to train a character level generative language model on the
 One Billion Word Benchmark \cite{chelba2013one}. In this setting, a 1D CNN deterministically transforms a latent
 vector into a  matrix, where  is the number of possible characters.
 A softmax nonlinearity is applied to this output, and given to the critic.
 Real data is one-hot encoding of 32 character texts sampled from the true data.
 
  \begin{table}
\caption{Comparison of different GAN methods for image and text generation.
We measure performance with respect to the FID on the image datasets
and JSD between -grams for text generation.}\label{Ta:results}
\centering
\begin{scriptsize}
  \begin{tabular}{cccccc}\toprule
  Task & BEGAN & DCGAN & Coulomb & WGAN-GP & FOGAN \\ \midrule
  CelebA & 28.5 & 12.5 & 9.3 & 4.2 & 6.0 \\
  LSUN & 112 & 57.5 & 31.2 &  9.5 & 11.4 \\
  CIFAR-10 & - & - & 27.3 & 24.8 & 27.4 \\
  4-gram & -    & -    & -   &  &  \\
  6-gram & - & - & -   &  &  \\ \bottomrule
  \end{tabular}
 \end{scriptsize}
 \end{table}

 We conjecture this is an especially difficult task for GANs, since data in the target distribution lies in just a few corners of
 the  dimensional unit hypercube. As the generator is updated, it must push mass from one corner to another, passing through
 the interior of the hypercube far from any real data. Methods other than Coulomb GAN \cite{unterthiner2017coulomb}
 WGAN-GP \cite{gulrajani2017improved,heusel2017gans} and the Sobolev GAN \cite{mroueh2017sobolev}
 have not been shown to be successful at this task.
 
 We use the same setup as in both \cite{gulrajani2017improved,heusel2017gans}
 with two differences.
 First, we train to minimize our divergence
 from Definition \ref{D:fo_divergence} with parameters  and  instead of the WGAN-GP divergence.
 Second, we use batch normalization in the generator, both for training our FOGAN method and the benchmark WGAN-GP;
 we do this because batch normalization improved performance and stability of both models.
 
 As with \cite{gulrajani2017improved,heusel2017gans} we use the Jensen-Shannon-divergence (JSD) between -grams from the model and the real world distribution
 as an evaluation metric.
 The JSD is estimated by sampling a finite number of 32 character vectors,
 and comparing the distributions of the -grams from said samples and true data.
 This estimation is biased; smaller samples result in larger JSD estimations.
 A Bayes limit results from this bias;
 even when samples are drawn from real world data and compared with real world data, small sample sizes results in large JSD estimations.
 In order to detect performance difference when training with the FOGAN and WGAN-GP, a low Bayes limit is necessary.
 Thus, to compare the methods, we sampled  32 character vectors in contrast with
 the  vectors sampled in past works. Therefore, the JSD values in those papers are higher than the results here.
 
 For our experiments we trained both models for  iterations in  independent runs, estimating the JSD between -grams
 of generated and real world data every  training steps, see Figure \ref{F:jsd_6}.
 The results are even more impressive when
 aligned with wall-clock time. Since in WGAN-GP training an extra point between real and generated distributions must be sampled, it is slower
 than the FOGAN training; see Figure \ref{F:jsd_6}
 and observe the significant () drop in estimated JSD.
 
 \begin{figure}
\begin{minipage}{\linewidth}
\centering
\includegraphics[width=1\linewidth]{js6_iter_full_font.pdf}
\end{minipage}
\begin{minipage}{\linewidth}
\centering
\includegraphics[width=1\linewidth]{js6_iter_zoom_font.pdf}
\end{minipage}
\begin{minipage}{\linewidth}
\centering
\includegraphics[width=\linewidth]{js6_wallclock_zoom_font.pdf}
\end{minipage}
\caption{
Five training runs of both WGAN-GP and FOGAN, with the average of all runs plotted in bold and
the  error margins denoted by shaded regions.
For easy visualization, we plot the moving average of the last three -gram JSD estimations.
The first two plots both show training w.r.t.\ number of training iterations; the second plot starts at iteration 50.
The last plot show training with respect to wall-clock time, starting after 6 hours of training.
}\label{F:jsd_6}
\end{figure}
\clearpage

 \section*{Acknowledgements}
 This work was supported by Zalando SE with Research Agreement 01/2016.
 
 \bibliography{bibi}
 \bibliographystyle{icml2018_style/icml2018}
 \clearpage{}\appendix
 \onecolumn
 \section{Proof of Things}\label{S:proof_of_things}

\begin{proof}[Proof of Theorem \ref{T:pwgan_theorem}]
 The proof of this theorem is split into smaller lemmas that are proven individually.
 \begin{itemize}
  \item That  is a strict adversarial divergence which is equivalent to  is proven in Lemma \ref{L:equavent},
  thus showing that  fulfills Requirement \ref{R:nonzero}.
  \item  fulfills Requirement \ref{R:convex_admissible} by design.
  \item The existence of an optimal critic in  follows directly from Lemma \ref{L:existence}.
  \item That there exists a critic  that fulfills Eq.\ \ref{E:first_order_assumption} is because
  Lemma \ref{L:existence} ensures that a continuous differentiable  exists in  which
  fulfills Eq.\ \ref{E:slope_strict}. Because Eq.\ \ref{E:slope_strict} holds for ,
  the same reasoning as the end of the proof of Lemma \ref{L:first_order} can be used to show Requirement \ref{R:fo_divergence}
 \end{itemize}
\end{proof}

We prepare by showing a few basic lemmas used in the remaining proofs

\begin{lemma}[concavity of ]\label{L:pen_concave}
 The mapping ,  is concave.
 \end{lemma}

 \begin{proof}
  The concavity of  is trivial. Now consider , then
  
  thus showing concavity of .
 \end{proof}


\begin{lemma}[necessary and sufficient condition for maximum]\label{L:necessary_for_maximum}
  Assume  fulfill assumptions \ref{A:1} and \ref{A:2}. Then for any  it must hold that
  
  and
  
  Further, if  and fulfills Eq.\ \ref{E:slope} and \ref{E:slope_p}, then 
 \end{lemma}

 \begin{proof}
  Since in Lemma \ref{L:pen_concave} it was shown that the the mapping  is concave,
   if and only if   and  is a local maximum of .
  This is equivalent to saying that all  with  and  it holds
  
  which holds if and only if
  
  and
  
  proving that Eq.\ \ref{E:slope} and \ref{E:slope_p} are necessary and sufficient.
 \end{proof}

 \begin{lemma}\label{L:existence}
  Let  be probability measures fulfilling Assumptions \ref{A:1} and \ref{A:2}.
  Define an open subset of , , such that  and .
  Then there exists a  such that
  
  and
  
  and .
 \end{lemma}

 \begin{proof}
  Since  for any  and is only affected by
  values of  on  we first start by considering
  
  Observe that Eq.\ \ref{E:slope_strict} holds if
 
 and similarly for Eq.\ \ref{E:slope_strict_p}
 
 Now it's clear that if the mapping  defined by
 
  admit a fix point , i.e.\ , then  is a solution to Eq.\ \ref{E:slope_strict} and \ref{E:slope_strict_p},
  and with that a solution to Eq.\ \ref{E:slope} and \ref{E:slope_p} and .

  Define the mapping  by
  
  Then
  
  and
  
  making  a projection.
  By the same reasoning, if 
  then  is a fix-point of , i.e.\ .
  Assume  is such a function, then by definition  of  in Eq.\ \ref{E:T_def}
  
  Therefore, . We can define  and see that .
  Further, since  only multiplies with a scalar, .

  Let . From Eq.\ \ref{E:s_equality} we get
  
  Now since for every  it holds by design that 
  and since  we see that  that
  
  Using this with the continuity of , there must exist  with
  
  With this (and compactness of our domain),
   must have mass in both positive and negative regions of  and exists a constant  such that for all
   it holds
  

  To show the existence of a fix-point for  in the Banach Space  we use the Banach fixed-point theorem
  to show that  has a fixed point in the metric space  (remember that  and ).
  If  then
  
 The same trick can be used to find some some  and show
 
 thereby showing
 
 The Banach fix-point theorem then delivers the existence of a fix-point  for .

 Finally, we can use the Tietze extension theorem to extend 
 to all of , thus finding a fix point for  in  and proving the lemma.
 \end{proof}


 \begin{lemma}\label{L:equavent}
   is a strict adversarial divergence and  and  are equivalent.
 \end{lemma}

 \begin{proof}
  Let  be two probability measures fulfilling Assumptions \ref{A:1} and \ref{A:2} with .
  It's shown in \cite{sriperumbudur2010hilbert} that ,
  meaning there exists a function ,  such that
 
 The Stone‚ÄìWeierstrass theorem tells us that there exists a  such that 
 and thus .
 Now consider the function  with , it's clear that
 
 and so for a sufficiently small  we'll get 
 meaning  and  is a strict adversarial divergence.

 To show equivalence, we note that
 
 therefore for any optimum it must hold , and thus (similar to Lemma \ref{L:necessary_for_maximum}) any optimal solution will
 be Lipschitz continuous with a the Lipschitz constant independent of . Thus  for
 , from which we directly get equivalence.
 \end{proof}

 \begin{proof}[Proof of Theorem \ref{T:fogan_theorem}]
 We start by applying Lemma \ref{L:adversary_subset} giving us
 \begin{itemize}
  \item .
  \item For any  fulfilling Assumptions \ref{A:1} and \ref{A:2}, it holds that
  , meaning  is like  a strict adversarial divergence
  which is equivalent to , showing Requirement \ref{R:nonzero}.
  \item  fulfills Requirement \ref{R:convex_admissible} by design.
  \item Every  is in ,
  therefore  the gradient  exists. Further Lemma \ref{L:first_order}
  shows that the update rule  is unique, thus showing Requirement \ref{R:differentiable}.
  \item Lemma \ref{L:first_order} gives us
 every  with the corresponding update rule fulfills Requirement \ref{R:fo_divergence}, thus proving Theorem \ref{T:fogan_theorem}.
 \end{itemize}
 \end{proof}

 Before we can show this theorem, we must prove a few interesting lemmas about . The following lemma is quite powerful;
 since  and 
 any property that's proven for  automatically holds for .

 \begin{lemma}\label{L:adversary_subset}
If let  and  be probability measures fulfilling Assumptions \ref{A:1} and \ref{A:2}. Then
\begin{enumerate}
 \item there exists  so that ,
 \item ,
 \item ,
 \item .
\end{enumerate}
Clain (4) is especially helpful, now anything that has been proven for all 
automatically holds for all 
\end{lemma}

\begin{proof}
For convenience define

( is for gradient penalty) and note that

Therefore it's clear that 

\noindent\textbf{Claim (1). }
Let  be an open set such that  and .
Then Lemma \ref{L:existence} tells us there is a  (and thus ) such that

and thus, because  open and ,

Now taking the gradients with respect to  gives us

meaning

thus , showing the claim.

\noindent\textbf{Claims (2) and (3). }
The claims are a direct result of Claim (1);
for every  there exists a

such that . Therefore

thus showing both  and .

\noindent\textbf{Claim (4). }
This claim is a direct result of claim (2); since ,
that means that if , then

thus  and .
\end{proof}

\begin{lemma}\label{L:first_order_helper}


For every  it holds





\end{lemma}

\begin{proof}
 Set

and note that due to construction of  and ,  is such that for almost all  there exists an
 where for all  it holds .

Since  it holds

Using Eq.\ \ref{E:slope} we see,

which means

Therefore,

Now from the proof of Lemma \ref{L:adversary_subset} claim (4), we know that since  we get

and since for  and  it holds  we discover
 and thus

and with 

Plugging this into Eq.\ \ref{E:long_equation} gives us

\end{proof}



\begin{lemma}\label{L:first_order}
 Let  and  in  and fulfill Assumptions \ref{A:1} and \ref{A:2},
 further let  be as defined in introduction to Theorem \ref{T:fogan_theorem},
 then for any 
 
 thus  fulfills Eq.\ \ref{E:first_order_assumption} and  fulfills Requirement \ref{R:fo_divergence}.
 Further, if  are such that there exits an  with  for all 
 and  then
 
\end{lemma}

\begin{proof}
Start off by noting that for some , Theorem 1 from \cite{milgrom2002envelope} gives us

Further, since for  it holds

the gradient of the gradient penalty part is zero, i.e.


One last point needs to be made before the main equation, which is for 

This is from the motivation of the penalized Wasserstein GAN where for an optimal critic it should hold that
 is close to  for some constant . Note that if  and 
are such that  is possible everywhere, then this term is equal to zero.


Since  fulfills Assumption \ref{A:1},  where  is differentiable in the first argument and 
( was defined in Assumption \ref{A:1}).
Therefore if we set  we get

  Now if we look at the \ref{E:big_baddy_1} term of the equation, we see that it's equal to:
  
  and term \ref{E:big_baddy_2} of the equation is equal to
  
  thus showing
  
\end{proof}
 


 \begin{lemma}\label{L:wgan_counterexample}
  Let  be the WGAN-GP divergence defined in Eq.\ \ref{E:WGAN_GP}, let the target distribution be the Dirac distribution 
  and the family of generated distributions be the uniform distributions  with . Then there is no
   that fulfills Eq.\ \ref{E:first_order_assumption} for all .
 \end{lemma}

 \begin{proof}
  For convenience, we'll restrict ourselves to the  case, for  the proof is similar. Assume that  and . Since  is an optimal critic, for any function  and any 
  it holds . Therefore  is a maximum of the continuously differentiable function
  , and . Therefore
  
  multiplying by  and deriving with respect to  gives us
  
  Since we already made the assumption that  and since  for any constant , we can assume that .
  This gives us  and thus
  
  Therefore, for the optimal critic it holds , and since  the optimal critic is . Now
  
  and
  
  Therefore there exists no  such that Eq.\ \ref{E:first_order_assumption} holds for every distribution in the WGAN-GP context.
 \end{proof}

 \section{Experiments}

 \subsection{CelebA}\label{SS:appendix_celeba_words}
 The parameters used for CelebA training were:
 \begin{verbatim}
 'batch_size': 64,
 'beta1': 0.5,
 'c_dim': 3,
 'calculate_slope': True,
 'checkpoint_dir': 'logs/1127_220919_.0001_.0001/checkpoints',
 'checkpoint_name': None,
 'counter_start': 0,
 'data_path': 'celebA_cropped/',
 'dataset': 'celebA',
 'discriminator_batch_norm': False,
 'epoch': 81,
 'fid_batch_size': 100,
 'fid_eval_steps': 5000,
 'fid_n_samples': 50000,
 'fid_sample_batchsize': 1000,
 'fid_verbose': True,
 'gan_method': 'penalized_wgan',
 'gradient_penalty': 1.0,
 'incept_path': 'inception-2015-12-05/classify_image_graph_def.pb',
 'input_fname_pattern': '*.jpg',
 'input_height': 64,
 'input_width': None,
 'is_crop': False,
 'is_train': True,
 'learning_rate_d': 0.0001,
 'learning_rate_g': 0.0005,
 'lipschitz_penalty': 0.5,
 'load_checkpoint': False,
 'log_dir': 'logs/0208_191248_.0001_.0005/logs',
 'lr_decay_rate_d': 1.0,
 'lr_decay_rate_g': 1.0,
 'num_discriminator_updates': 1,
 'optimize_penalty': False,
 'output_height': 64,
 'output_width': None,
 'sample_dir': 'logs/0208_191248_.0001_.0005/samples',
 'stats_path': 'stats/fid_stats_celeba.npz',
 'train_size': inf,
 'visualize': False
 \end{verbatim}
 The learned networks (both generator and critic) are then fine-tuned with learning rates divided by 10.
 Samples from the trained model can be viewed in figure \ref{F:fogan_faces}.
 \begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{celeba_sample.png}
  \caption{Images from a First Order GAN after training on CelebA data set.}\label{F:fogan_faces}
 \end{figure}
\clearpage

 \subsection{CIFAR-10}\label{SS:appendix_cifar}
 The parameters used for CIFAR-10 training were:
 \begin{verbatim}
  BATCH_SIZE: 64
  BETA1_D: 0.0
  BETA1_G: 0.0
  BETA2_D: 0.9
  BETA2_G: 0.9
  BN_D: True
  BN_G: True
  CHECKPOINT_STEP: 5000
  CRITIC_ITERS: 1
  DATASET: cifar10
  DATA_DIR: /data/cifar10/
  DIM: 32
  D_LR: 0.0003
  FID_BATCH_SIZE: 200
  FID_EVAL_SIZE: 50000
  FID_SAMPLE_BATCH_SIZE: 1000
  FID_STEP: 5000
  GRADIENT_PENALTY: 1.0
  G_LR: 0.0001
  INCEPTION_DIR: /data/inception-2015-12-05
  ITERS: 500000
  ITER_START: 0
  LAMBDA: 10
  LIPSCHITZ_PENALTY: 0.5
  LOAD_CHECKPOINT: False
  LOG_DIR: logs/
  MODE: fogan
  N_GPUS: 1
  OUTPUT_DIM: 3072
  OUTPUT_STEP: 200
  SAMPLES_DIR: /samples
  SAVE_SAMPLES_STEP: 200
  STAT_FILE: /stats/fid_stats_cifar10_train.npz
  TBOARD_DIR: /logs
  TTUR: True
 \end{verbatim}
 The learned networks (both generator and critic) are then fine-tuned with learning rates divided by 10.
 Samples from the trained model can be viewed in figure \ref{F:cifar_samples}.
 \begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{cifar_samples.png}
  \caption{Images from a First Order GAN after training on CIFAR-10 data set.}\label{F:cifar_samples}
 \end{figure}
\clearpage

 \subsection{LSUN}\label{SS:appendix_lsun}
 The parameters used for LSUN Bedrooms training were:
 \begin{verbatim}
 BATCH_SIZE: 64
  BETA1_D: 0.0
  BETA1_G: 0.0
  BETA2_D: 0.9
  BETA2_G: 0.9
  BN_D: True
  BN_G: True
  CHECKPOINT_STEP: 4000
  CRITIC_ITERS: 1
  DATASET: lsun
  DATA_DIR: /data/lsun
  DIM: 64
  D_LR: 0.0003
  FID_BATCH_SIZE: 200
  FID_EVAL_SIZE: 50000
  FID_SAMPLE_BATCH_SIZE: 1000
  FID_STEP: 4000
  GRADIENT_PENALTY: 1.0
  G_LR: 0.0001
  INCEPTION_DIR: /data/inception-2015-12-05
  ITERS: 500000
  ITER_START: 0
  LAMBDA: 10
  LIPSCHITZ_PENALTY: 0.5
  LOAD_CHECKPOINT: False
  LOG_DIR: /logs
  MODE: fogan
  N_GPUS: 1
  OUTPUT_DIM: 12288
  OUTPUT_STEP: 200
  SAMPLES_DIR: /samples
  SAVE_SAMPLES_STEP: 200
  STAT_FILE: /stats/fid_stats_lsun.npz
  TBOARD_DIR: /logs
  TTUR: True
 \end{verbatim}
 The learned networks (both generator and critic) are then fine-tuned with learning rates divided by 10.
 Samples from the trained model can be viewed in figure \ref{F:lsun_samples}.
 \begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{lsun_samples.png}
  \caption{Images from a First Order GAN after training on LSUN data set.}\label{F:lsun_samples}
 \end{figure}
\clearpage
 
 \subsection{Billion Word}\label{SS:appendix_billion_words}
 The parameters used for the Billion Word training were one run with the following settings, followed by
 a second run using initialized with the best saved model from the first run and learning rates divided by 10.
 Samples from our method and the WGAN-GP baseline can be found in figure \ref{F:billion_samples}
 
 \begin{verbatim}
  'activation_d': 'relu',
 'batch_norm_d': False,
 'batch_norm_g': True,
 'batch_size': 64,
 'checkpoint_dir': 'logs/checkpoints/0201_181559_0.000300_0.000100',
 'critic_iters': 1,
 'data_path': '1-billion-word-language-modeling-benchmark-r13output',
 'dim': 512,
 'gan_divergence': 'FOGAN',
 'gradient_penalty': 1.0,
 'is_train': True,
 'iterations': 500000,
 'jsd_test_interval': 2000,
 'learning_rate_d': 0.0003,
 'learning_rate_g': 0.0001,
 'lipschitz_penalty': 0.1,
 'load_checkpoint_dir': 'False',
 'log_dir': 'logs/tboard/0201_181559_0.000300_0.000100',
 'max_n_examples': 10000000,
 'n_ngrams': 6,
 'num_sample_batches': 100,
 'print_interval': 100,
 'sample_dir': 'logs/samples/0201_181559_0.000300_0.000100',
 'seq_len': 32,
 'squared_divergence': False,
 'use_fast_lang_model': True
 \end{verbatim}


\begin{figure}
\centering
\begin{minipage}[b]{.5\linewidth}
\begin{verbatim}
Change spent kands that the righ
Qust of orlists are mave hor int
Is that the spens has lought ant
If a took and their osiy south M
Willing contrased vackering in S
The Ireas's last to vising 5t ..
The FNF sicker , Nalnelber once 
She 's wast to miblue as ganemat
threw pirnatures for hut only a 
Umialasters are not oversup on t
Beacker it this that that that W
Though 's lunge plans wignsper c
He says : WalaMurka in the moroe
\end{verbatim} 
\end{minipage}\begin{minipage}[b]{.5\linewidth}
\begin{verbatim}
Dry Hall Sitning tven the concer
There are court phinchs hasffort
He scores a supponied foutver il
Bartfol reportings ane the depor
Seu hid , it ‚Äôs watter ‚Äôs remold
Later fasted the store the inste
Indiwezal deducated belenseous K
Starfers on Rbama ‚Äôs all is lead
Inverdick oper , caldawho ‚Äôs non
She said , five by theically rec
RichI , Learly said remain .‚Äò‚Äò‚Äò‚Äò
Reforded live for they were like
The plane was git finally fuels
\end{verbatim} 
\end{minipage}
\caption{Samples generated by First Order GAN trained on fhe One Billion Word benchmark with FOGAN
(left) the original TTUR method (right).}\label{F:billion_samples}
\end{figure}\clearpage{}
\end{document}
