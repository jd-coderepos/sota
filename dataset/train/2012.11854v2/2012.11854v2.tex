
\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{1407} \def\confYear{CVPR 2021}


\usepackage{mathrsfs}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts,dsfont}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{enumitem}



\usepackage{threeparttable}
\usepackage{multirow}

\newcommand{\estD}{{\hat D}}
\newcommand{\estDg}{{\hat D^{\tau}}}
\newcommand{\estDisg}{{\hat {\mathcal D}^{\tau}}}
\newcommand{\estDis}{{\hat {\mathcal D}}}

\newcommand{\PP}{\mathbb P}
\newcommand{\BR}{\mathds 1}
\newcommand{\F}{\mathcal F}
\newcommand{\E}{\mathbb E}
\newcommand{\BPeer}{\BR_\text{peer}}
\newcommand{\lPeer}{\ell_\text{peer}}
\newcommand{\AlphaPeer}{\BR_\text{-peer}}
\newcommand{\AlphaStar}{\BR_\text{-peer}}

\newcommand{\lAlphaPeer}{\ell_\text{-peer}}
\newcommand{\lAlphaStar}{\ell_\text{-peer}}

\newcommand{\squishlist}{
\begin{list}{{{\small{}}}}
{\setlength{\itemsep}{3pt}      \setlength{\parsep}{1pt}
\setlength{\topsep}{1pt}       \setlength{\partopsep}{0pt}
\setlength{\leftmargin}{1em} \setlength{\labelwidth}{1em}
\setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{  \end{list}  }
\newcommand{\SPL}{CORES}


\def\BState{\State\hskip-\ALG@thistlm}

\makeatletter
\makeatother
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\usepackage{hyperref}


\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
\ifodd 0
\newcommand{\rev}[1]{{\color{blue}#1}}
\newcommand{\com}[1]{\textbf{\color{red}(Yang: #1)}}
\newcommand{\clar}[1]{\textbf{\color{green}(NEED CLARIFICATION: #1)}}
\newcommand{\response}[1]{\textbf{\color{magenta}(RESPONSE: #1)}}
\newcommand{\zzw}[2]{\textbf{\color{blue}(Zhaowei: #1)}{\color{blue}#2}}
\newcommand{\tl}[2]{\textbf{\color{red}(Tongliang: #1)}{\color{red}#2}}
\else
\newcommand{\rev}[1]{#1}
\newcommand{\clar}[1]{}
\newcommand{\response}[1]{}
\fi
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\newcounter{list}
\newcommand{\covpeer}{\text{CAL}}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\algcom}[1]{\textsl{\color{blue}{\footnotesize #1}}}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \begin{document}

\title{A Second-Order Approach to Learning with Instance-Dependent Label Noise}



\author{Zhaowei Zhu \quad Tongliang Liu \quad  Yang Liu\\
Computer Science and Engineering, University of California, Santa Cruz \\ Trustworthy Machine Learning Lab, The University of Sydney\\
{\tt\small \{zwzhu,yangliu\}@ucsc.edu}
\quad
{\tt\small tongliang.liu@sydney.edu.au}
}

\maketitle
\pagestyle{empty}  \thispagestyle{empty} 

\begin{abstract}
The presence of label noise often misleads the training of deep neural networks. Departing from the recent literature which largely assumes the label noise rate is only determined by the true label class, the errors in human-annotated labels are more likely to be dependent on the difficulty levels of tasks, resulting in settings with instance-dependent label noise. We first provide evidences that the heterogeneous instance-dependent label noise is effectively down-weighting the examples with higher noise rates in a non-uniform way and thus causes imbalances, rendering the strategy of directly applying methods for class-dependent label noise questionable. Built on a recent work peer loss \cite{liu2019peer}, we then propose and study the potentials of a second-order approach that leverages the estimation of several covariance terms defined between the instance-dependent noise rates and the Bayes optimal label. We show that this set of second-order statistics successfully captures the induced imbalances. We further proceed to show that with the help of the estimated second-order statistics, we identify a new loss function whose expected risk of a classifier under instance-dependent label noise is equivalent to a new problem with only class-dependent label noise. This fact allows us to apply existing solutions to handle this better-studied setting. We provide an efficient procedure to estimate these second-order statistics without accessing either ground truth labels or prior knowledge of the noise rates. Experiments on CIFAR10 and CIFAR100 with synthetic instance-dependent label noise and Clothing1M with real-world human label noise verify our approach. Our implementation is available at \url{https://github.com/UCSC-REAL/CAL}.
\end{abstract}


\vspace{-10pt}
\section{Introduction}
Deep neural networks (DNNs) are powerful in revealing and fitting the relationship between feature  and label  when a sufficiently large dataset is given. However, the label  usually requires costly human efforts for accurate annotations. With limited budgets/efforts, the resulting dataset would be noisy, and the existence of label noise may mislead DNNs to learn or memorize wrong correlations \cite{han2020sigua,han2020survey,wang2021learning,xia2021robust,zhang2016understanding}.
To make it worse, the label noise embedded in human annotations is often instance-dependent, e.g., some difficult examples are more prone to be mislabeled \cite{wang2021tackling}. This hidden and imbalanced distribution of noise often has a detrimental effect on the training outcome \cite{jiang2020beyond,liu2021importance}. It remains an important and challenging task to learn with instance-dependent label noise. 

Theory-supported works addressing instance-dependent label noise mostly rely on loss correction, which requires estimating noise rates \cite{xia2020parts}. Recent work has also considered the possibility of removing the dependency on estimating noise rates \cite{sieve2020}. The proposed solution uses a properly specified regularizer to eliminate the effect of instance-dependent label noise. The common theme of the above methods is the focus on learning the underlying clean distribution by using certain forms of \underline{first-order statistics} of model predictions. In this paper, we propose a second-order approach with the assistance of additional \underline{second-order statistics} and explore how this information can improve the robustness of learning with instance-dependent label noise.
Our main contributions summarize as follows. 


\begin{enumerate}[itemsep = -5pt, topsep = -7pt, leftmargin = 12pt]
    \item Departing from recent works \cite{sieve2020,liu2019peer,natarajan2013learning,patrini2017making,vahdat2017toward,xia2020parts,xiao2015learning} which primarily rely on the first-order statistics (i.e. expectation of the models' predictions) to improve the robustness of loss functions, we propose a novel second-order approach and emphasize the importance of using second-order statistics \rev{(i.e. several covariance terms)} when dealing with instance-dependent label noise. \item 
    \rev{With the perfect knowledge of the covariance terms defined above, we identify a new loss  function that transforms the expected risk of a classifier under instance-dependent label noise to a risk with only class-dependent label noise, which is an easier case and can be handled well by existing solutions. Based on peer loss \cite{liu2019peer}, we further show the expected risk of class-dependent noise is equivalent to an affine transformation of the expected risk under the Bayes optimal distribution.
    Therefore we establish that our new loss function for \emph{Covariance-Assisted Learning (CAL)} will induce the same minimizer as if we can access the clean Bayes optimal labels.}
    \item  We show how the second-order statistics can be estimated efficiently using existing sample selection techniques. For a more realistic case where the covariance terms cannot be perfectly estimated, we prove the worst-case performance guarantee of our solution. 
    \item In addition to the theoretical guarantees, the performance of the proposed second-order approach is tested on the CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and the Clothing1M dataset with real-world human label noise. 
\end{enumerate}




\subsection{Related Works}


Below we review the most relevant literature. 

\noindent\textbf{Bounded loss functions}~~
Label noise encodes a different relation between features and labels. A line of literature treats the noisy labels as outliers.
However, the convex loss functions are shown to be prone to mistakes when outliers exist \cite{long2010random}.
To handle this setting, the cross-entropy (CE) loss can be generalized by introducing temperatures to  logarithm functions and exponential functions \cite{amid2019robust,amid2019two,zhang2018generalized}.
Noting the CE loss grows explosively when the prediction  approaches zero, some solutions focus on designing bounded loss functions \cite{ghosh2017robust,gong2018decomposition,shu2020learning,wang2019symmetric}.
These methods focus on the numerical property of loss functions, and most of them do not discuss the type of label noise under treatment.

\noindent\textbf{Learning clean distributions}~~
To be noise-tolerant \cite{manwani2013noise}, it is necessary to understand the effect of label noise statistically.
With the class-dependent assumption, the loss can be corrected/reweighted when the noise transition  is available, which can be estimated by discovering anchor points \cite{liu2015classification,patrini2017making,xia2020extended}, exploiting clusterability \cite{zhu2021clusterability}, regularizing total variation \cite{zhang2021learning}, or minimizing volume of  \cite{li2021provably}.
The loss correction/reweighting methods rely closely on the quality of the estimated noise transition matrix.
To make it more robust, an additive slack variable  \cite{xia2019anchor} or a multiplicative dual  \cite{dualT2020nips} can be used for revision.
Directly extending these loss correction methods to instance-dependent label noise is prohibitive since the transition matrix will become a function of feature  and the number of parameters to be estimated is proportional to the number of training instances. 
Recent follow-up works often introduce extra assumption \cite{xia2020parts} or measure  \cite{berthon2020confidence}.
Statistically, the loss correction approach is learning the underlying clean distribution if a perfect  is applied. 
When the class-dependent noise rate is known, surrogate loss \cite{natarajan2013learning}, an unbiased loss function targeting on binary classifications, also learns the clean distribution.
Additionally, the symmetric cross-entropy loss \cite{wang2019symmetric}, an information-based loss  \cite{xu2019l_dmi}, a correlated agreement (CA) based loss peer loss \cite{liu2019peer}, and its adaptation for encouraging confident predictions \cite{sieve2020} are proposed to learn the underlying clean distribution without knowing the noise transition matrix.



\noindent\textbf{Other popular methods}~~
Other methods exist with more sophisticated training framework or pipeline, including sample selection \cite{sieve2020,han2018co,jiang2017mentornet,lee2018cleannet,wei2020combating,yu2019does,yao2020searching}, label correction \cite{han2019deep,li2017learning,veit2017learning}, and semi-supervised learning \cite{Li2020DivideMix,nguyen2019self}, etc.





 \section{Preliminaries}\label{Sec:pre}


This paper targets on a classification problem given a set of  training examples with Instance-Dependent label Noise (IDN) denoted by , where  is the set of indices.
The corresponding noisy data distribution is denoted by .
Examples  are drawn according to random variables . 
Our goal is to design a learning mechanism that is guaranteed to be robust when learning with only accessing .
Before proceeding, we summarize important definitions as follows.

\noindent\textbf{Clean distribution }~~
Each noisy example  corresponds to a clean example , which contains one \emph{unobservable} ground-truth label, a.k.a. clean label.
Denote by  the clean distribution. Clean examples  are drawn from random variables .

\noindent\textbf{Bayes optimal distribution }~~
Denote by  the Bayes optimal label given feature , that is:

The distribution of  is denoted by . Note the Bayes optimal distribution  is different from the clean distribution  when .
Due to the fact that the information encoded between features and labels is corrupted by label noise, and both clean labels and Bayes optimal labels are unobservable, inferring the Bayes optimal distribution  from the noisy dataset  is a non-trivial task.
\rev{Notably there exist two approaches \cite{sieve2020,cheng2017learningdistill} that provide guarantees on constructing the Bayes optimal dataset. We would like to remind the readers that the noisy label , clean label , and Bayes optimal label  for the same feature  may disagree with each other.}

Most of our developed approaches will focus on dealing with the Bayes optimal distribution . 
{By referring to , as we shall see later, we are allowed to estimate the second-order statistics defined w.r.t. .} 

\noindent\textbf{Noise transition matrix }~~
Traditionally, the noise transition matrix is defined based on the relationship between clean distributions and noisy distributions \cite{sieve2020,liu2019peer,Patrini_2017_CVPR,xia2020parts}.
In recent literature \cite{cheng2017learningdistill}, the Bayes optimal label (a.k.a. distilled label in \cite{cheng2017learningdistill}) also plays a significant role.
In the image classification tasks where the performance is measured by the clean test accuracy, predicting the Bayes optimal label achieves the best performance. 
This fact motivates us to define a new noise transition matrix based on the Bayes optimal label as follows:

where  denotes the -th element of the matrix . 
Its expectation is defined as , with the -th element being .

\noindent\textbf{Other notations}~~
Let  and \footnote{We focus on the closed-set label noise, i.e. , , and  share the same space .} be the space of feature  and label , respectively. 
The classification task aims to identify a classifier  that maps  to  accurately.
One common approach is minimizing the empirical risk using DNNs with respect to the \emph{cross-entropy (CE) loss} defined as:

where  denotes the -th component of  and  is the number of classes.
Let  be the indicator function taking value  when the specified condition is satisfied and  otherwise. Define the \emph{0-1 loss} as 

Define the Bayes optimal classifier  as 

Noting the CE loss is classification-calibrated \cite{bartlett2006convexity}, given enough clean data, the Bayes optimal classifier can be learned using the CE loss: 


\noindent\textbf{Goal}~~
Different from the goals in surrogate loss \cite{natarajan2013learning},  \cite{xu2019l_dmi}, peer loss \cite{liu2019peer}, and CORES \cite{sieve2020}, which focus on recovering the performance of learning on clean distributions, we aim to learn a classifier  from the noisy distribution  which also minimizes . 
Note  holds for the Bayes optimal classifier . Thus, in the sense of searching for the Bayes optimal classifier, our goals are aligned with the ones focusing on the clean distribution. 
 \section{Insufficiency of First-Order Statistics}\label{sec:cal}
Peer loss \cite{liu2019peer} and its inspired confidence regularizer \cite{sieve2020} are two recently introduced robust losses that operate without the knowledge of noise transition matrices, which presents them as preferred solutions for more complex noise settings. 
In this section, we will first review the usages of first-order statistics in peer loss and the confidence regularizer (Section~\ref{sec:first-order-info}), and then analyze the insufficiency of using only the first-order statistics when handling the challenging IDN (Section~\ref{sec:pure_peer_IDN}).
Besides, we will anatomize the down-weighting effect of IDN and provide intuitions for how to make IDN easier to handle (Section~\ref{sec:down-weight}).

 We formalize our arguments using peer loss, primarily due to 1) its clean analytical form, and 2) that our later proposed solution will be built on peer loss too. Despite the focus on peer loss, we believe these observations are generally true when other existing training approaches meet IDN. 

For ease of presentation, the following analyses focus on binary cases (with classes ).
Note the class  should be mapped to class  following the notations in Section~\ref{Sec:pre}. 
For a clear comparison with previous works, we follow the notation in \cite{liu2019peer} and use class  to represent classes  when .
The error rates in  are then denoted as , . Most of the discussions generalize to the multi-class setting. 
 
\subsection{Using First-Order Statistics in Peer Loss}\label{sec:first-order-info}

It has been proposed and proved in peer loss \cite{liu2019peer} and CORES \cite{sieve2020} that the learning could be robust to label noise by considering some first-order statistics related to the model predictions.
For each example , peer loss \cite{liu2019peer} has the following form:

where  and  are two randomly sampled peer samples for .
The first-order statistics related to model predictions characterized by the peer term  are further extended to a confidence regularizer in CORES \cite{sieve2020}:

where  is a hyperparameter controlling the ability of regularizer, and {\small } is the marginal distribution of {\small } given dataset {\small }.
Although it has been shown in \cite{sieve2020} that learning with an appropriate  would be robust to instance-dependent label noise theoretically, in real experiments, converging to the guaranteed optimum by solving a highly non-convex problem is difficult.







\subsection{Peer Loss with IDN}\label{sec:pure_peer_IDN}


Now we analyze the possible performance degradation of using the binary peer loss function proposed in \cite{liu2019peer} to handle IDN. 
Denote by  the optimal classifier learned by minimizing 0-1 peer loss, where  represents  with 0-1 loss (could also be generalized for  with 0-1 loss).
\rev{Let .}
With a bounded variance in the error rates, supposing 

the worst-case performance bound for using pure peer loss is provided in Theorem~\ref{thm:peerIDN} and proved in Appendix~\ref{proof:peerIDN}.

\begin{theorem}[Performance of peer loss]\label{thm:peerIDN}
With the peer loss function proposed in \cite{liu2019peer}, we have

\end{theorem}
Theorem~\ref{thm:peerIDN} shows the ratio of wrong predictions given by  includes two components. The former term  is directly caused by IDN, indicating the error is increasing when the instance-dependent noise rates have larger mean (larger ) and larger \rev{variation} (larger ).
The latter term  shows possible errors induced by an unbalanced .
Theorem~\ref{thm:peerIDN} generalizes peer loss where , i.e., the error rates are homogeneous across data instances, and there is no need to consider any second-order statistics that involve the distribution of noise rates.

\subsection{Down-weighting Effect of IDN}\label{sec:down-weight}

\rev{We further discuss motivations and intuitions by studying}
how IDN affects the training differently from the class-dependent one. Intuitively, a high noise rate reduces the informativeness of a particular example , therefore ``down-weighting" its contribution to training. We now analytically show this under peer loss. 

As a building block, the invariant property (in terms of the \textit{clean distribution }) originally discovered by peer loss on class-dependent label noise is first adapted for the \textit{Bayes optimal distribution }. Define  and .
Focusing on a particular class-dependent , we provide Lemma~\ref{lem:invariant_d} and its proof in Appendix~\ref{proof:invariant_d}.
\begin{lemma}[Invariant property of peer loss \cite{liu2019peer}]\label{lem:invariant_d}
Peer loss is invariant to class-dependent label noise:
{\small }
\end{lemma}

Then we discuss the effect of IDN.
Without loss of generality, consider a case where noisy examples are drawn from two noisy distributions  and , and the noise rate of  is higher than , i.e. , where . \rev{Assume a particular setting of IDN that the noise is class-dependent (but not instance-dependent) only within each distribution, and different between two distributions, i.e. \textit{part-dependent} \cite{xia2020parts}.}
Let  and  be the Bayes optimal distribution related to  and . 
For simplicity, we write  as .
Then  and .
Note  and  Then we have the following equality:
{
}
where  indicates \emph{down-weighting} examples drawn from  (compared to the class-dependent label noise).

\noindent\textbf{What can we learn from this observation?}~~
First, we show the peer loss is already down weighting the importance of the more noisy examples. However, simply dropping examples with potentially high-level noise might lead the classifier to learn a biased distribution. \rev{Moreover, subjectively confusing examples are more prone to be mislabeled and critical for accurate predictions \cite{wang2021tackling}, thus need to be carefully addressed.} Our second observation is that if we find a way to compensate for the ``imbalances'' caused by the down-weighting effects shown above, the challenging instance-dependent label noise could be transformed into a class-dependent one, which existing techniques can then handle. More specifically, the above result shows the down-weighting effect is characterized by , implying only using the first-order statistics of model predictions without considering the distributions of the noise transition matrix  is insufficient to capture the complexity of the learning task. 
However, accurately estimating  is prohibitive since the number of parameters to be estimated is almost at the order of  -- recall  is the number of training examples and  is the number of classes.
Even though we can roughly estimate , applying element-wise correction relying on the estimated  may accumulate errors.
Therefore, to achieve the transformation from the instance-dependent to the easier class-dependent, we need to resort to other statistical properties of .


 \section{Covariance-Assisted Learning (CAL)}\label{sec:theory}


From the analyses in Section~\ref{sec:down-weight}, we know the instance-dependent label noise will ``automatically'' assign different weights to examples with different noise rates, thus cause imbalances.
When the optimal solution does not change under such down-weighting effects, the first-order statistics based on peer loss \cite{sieve2020,liu2019peer} work well. 
However, for a more robust and general solution, using additional information to ``balance'' the effective weights of different examples is necessary.
Although the Bayes optimal distribution is not accessible in real experiments, we first assume its existence for theoretical analyses in the ideal case, then we will discuss the gap to this optimal solution when we can only use a proxy  that can be constructed efficiently. 

\subsection{Extracting Covariance from IDN}\label{sec:extension}




Again consider an instance-dependent noisy distribution  with binary classes where .
Define the following two random variables \rev{to facilitate analyses}:

Recall  and .
Let  be the covariance between random variables  and  w.r.t. the distribution .
The exact effects of IDN on peer loss functions are revealed in Theorem~\ref{thm:covpeerBinary} and proved in Appendix~\ref{proof:covpeerBinary}.

\begin{theorem}[Decoupling binary IDN]
\label{thm:covpeerBinary}
In binary classifications, the expected peer loss with IDN writes as:

\end{theorem}


Theorem~\ref{thm:covpeerBinary} effectively divides the instance-dependent label noise into two parts.
As shown in Eq.~(\ref{eq:binaryCAP}), the first line is the same as Eq.~(\ref{eq:peerBinary}) in Lemma~\ref{lem:invariant_d}, indicating the average effect of instance-dependent label noise can be treated as a class-dependent one with parameters . The additional two covariance terms in the second and the third lines of Eq.~(\ref{eq:binaryCAP}) characterize the additional contribution of examples due to their differences in the label noise rates. The covariance terms will become larger for a setting with more diverse noise rates, capturing a more heterogeneous and uncertain learning environment. Interested readers are also referred to the high-level intuitions for using covariance terms at the end of Section~\ref{sec:down-weight}. 


\rev{We now briefly discuss one extension of Theorem~\ref{thm:covpeerBinary} to a -class classification task.
Following the assumption adopted in \cite{liu2019peer}, we consider a particular setting of IDN whose the expected transition matrix satisfies .}
Denote by .
\rev{Corollary~\ref{cor:covpeerMul} decouples the effects of IDN in multi-class cases and is proved in Appendix~\ref{proof:covpeerMul}.}
\begin{corollary}[Decoupling multi-class IDN]
\label{cor:covpeerMul}
In multi-class classifications, when the expected transition matrix satisfies ,  the expected peer loss with IDN writes as:

where  is the marginal distribution of  and  is the conditional distribution of  given .  \end{corollary}




\subsection{Using Second-Order Statistics}

Inspired by Theorem \ref{thm:covpeerBinary}, if  is available, we can subtract two covariance terms and make peer loss invariant to IDN.
Specifically, define 
{\small
}
We have the following optimality guarantee and its proof is deferred to Appendix~\ref{proof:optimal}. \begin{theorem}
\label{thm:optimal}

\end{theorem}

For a -class classification problem, a general loss function for our \textit{Covariance-Assisted Learning (\covpeer{})} approach is given by

Eq.~(\ref{eq:cov_def}) shows the Bayes optimal distribution  is critical in implementing the proposed covariance terms.
However,  cannot be obtained trivially, and only imperfect proxy constructions of the dataset (denoted by ) could be expected.
Detailed constructions of  are deferred to Section~\ref{sec:alg}. 


\noindent\textbf{Advantages of using covariance terms}~~
There are several advantages of using the proposed covariance terms.
Unlike directly correcting labels according to , the proposed covariance term can be viewed as a ``soft'' correction that maintains the information encoded in both original noisy labels and the estimated Bayes optimal labels.
Keeping both information is beneficial as suggested in \cite{han2019deep}.
Moreover, compared to the direct loss correction approaches \cite{Patrini_2017_CVPR,xia2020parts,xia2019anchor}, we keep the original learning objective and apply ``correction'' using an additional term.
Our method is more robust in practice compared to these direct end-to-end loss correction approaches due to two reasons: 
1) The covariance term summarizes the impact of the complex noise using an average term, indicating that our approach is less sensitive to the estimation precision of an individual example; 2) As will be shown in Section~\ref{sec:imperfectCov}, the proposed method is tolerant with accessing an imperfect .





Estimating the covariance terms relies on samples drawn from distribution .
Thus, we need to construct a dataset , which is similar or unbiased w.r.t. .
We will first show the algorithm for constructing , then provide details for DNN implementations.

\subsubsection{Constructing }\label{sec:alg}

To achieve unbiased estimates of the variance terms, the high-level intuition for constructing  is determining whether the label of each example in  is Bayes optimal or not by comparing the likelihood, confidence, or loss of classifying the (noisy) label to some thresholds.
There are several methods for constructing : distillation \cite{cheng2017learningdistill}, searching to exploit \cite{yao2020searching}, and sample sieve \cite{sieve2020}.
If the model does not overfit the label noise and learns the noisy distribution, both methods in \cite{cheng2017learningdistill} and \cite{yao2020searching} work well. However, for the challenging instance-dependent label noise, overfitting occurs easily thus techniques to avoid overfitting are necessary. In this paper, we primarily adapt the sample sieve proposed in \cite{sieve2020}, which uses a confidence regularizer to avoid overfitting, to construct . 
{Specifically, as shown in \cite{sieve2020}, in each epoch , the regularized loss for each example is adjusted by the parameter , which \rev{can be calculated based on model predictions in linear time}. In the ideal cases assumed in \cite{sieve2020}, any example with a positive adjusted loss is corrupted (with a wrong label).}

\begin{algorithm}[t]
\LinesNumbered
\SetAlgoLined
\KwIn{Noisy dataset . Thresholds . Number of epochs . .}
Train the sample sieve in \cite{sieve2020} for  epochs and get the model \;
    \For{}{
        Calculate  following \cite{sieve2020}\;
        \uIf{}{
           \; \label{line:small_loss}}
           \uElseIf{}
           {\; \label{line:large_loss}}
           \uElse{ (drop example )\;\label{Line:moderate_loss}}
     } 
 \KwOut{}
 \caption{Constructing }
 \label{alg:D}
\end{algorithm}
We summarized the corresponding procedures in Algorithm~\ref{alg:D}, where the critical thresholds for comparing losses are denoted by  and .
At Line~\ref{line:small_loss}, if the loss adjusted by  is small enough (\underline{smaller than the threshold }), we assume  is the Bayes optimal label.
Accordingly, at Line~\ref{line:large_loss}, if the adjusted loss is too large (\underline{larger than the threshold }), we treat  as a corrupted one and assume the class with maximum predicted probability to be Bayes optimal one.
For the examples with moderate adjusted loss, we drop it as indicated in Line~\ref{Line:moderate_loss}.
In ideal cases with infinite model capacity and sufficiently many examples (as assumed in \cite{sieve2020}), we can set thresholds  to guarantee a separation of clean and corrupted examples, thus  will be an unbiased proxy to  \footnote{In the ideal case as assumed in Corollary 1 of \cite{sieve2020}, we have .}.
However, in real experiments, when both the model capacity and the number of examples are limited, we may need to tune  and  to obtain a high-quality construction of .
In this paper, we set  to ensure  and reduce the effort to tuning both thresholds simultaneously.

{Note that using  to estimate the covariance terms could be made theoretically more rigorous by applying appropriate re-weighting techniques \cite{cheng2017learningdistill,fang2020rethinking,huang2007correcting}.
See Appendix~\ref{dis:L} for more discussions and corresponding guarantees. We omit the details here due to the space limit}. Nonetheless,
our approach is tolerant of an imperfect , which will be shown theoretically in Section \ref{sec:imperfectCov}.

\subsubsection{Implementations}

For implementations with deep neural network solutions, we need to estimate the transition matrix  relying on  and estimate the covariance terms along with stochastic gradient descent (SGD) updates.

\noindent\textbf{Covariance Estimation in SGD}~~
As required in (\ref{eq:cov_def}), with a particular , \rev{each computation for  requires only one time check of the associated noisy label} as follows:

When  is unbiased w.r.t. , the estimation in (\ref{eq:est_T}) is also unbiased because

Noting 
the covariance can be estimated empirically as

\rev{For each batch of data, the above estimation has  complexities in computation and space. To reduce
both complexities, with the cost of the estimation quality, we use  examples to estimate the covariance in each batch, where  is the set of sample indices of batch-. Per sample wise, Eq.~(\ref{eq:cov_def}) can be transformed to}

\rev{With the above implementation, the estimation is done locally for each point in  complexity.}
\subsection{\covpeer{} with Imperfect Covariance Estimates} 
\label{sec:imperfectCov}


As mentioned earlier,  cannot be perfectly obtained in practice. Thus, there is a performance gap between the ideal case (with perfect knowledge of ) and the actually achieved one.
We now analyze the effect of imperfect covariance terms (Theorem~\ref{thm:imperfect}).

Denote the imperfect covariance estimates by , where  is the expected ratio (a.k.a. probability) of correct examples in :

With , the minimizer of the 0-1 \covpeer{} loss is given by: 
{\small
}
Theorem~\ref{thm:imperfect} reports the error bound produced by . See Appendix~\ref{proof:imperfect} for the proof.
\begin{theorem}[Imperfect Covariance]\label{thm:imperfect}
With , when , we have

\end{theorem}

Theorem~\ref{thm:imperfect} shows the quality of  controls the scale of the worst-case error upper-bound.
Compared with Theorem~\ref{thm:peerIDN} where no covariance term is used, we know the covariance terms will always be helpful when . That is, the training with the assistance of covariance terms will achieve better (worst-case) accuracy on the Bayes optimal distribution when the construction  is better than a dataset that includes each instance in  randomly with 50\% chance.



 \section{Experiments}
We now present our experiment setups and results. 
\subsection{General Experiment Settings}

\noindent\textbf{Datasets and models}~~
The advantage of introducing our second-order approach is evaluated on three benchmark datasets: CIFAR10, CIFAR100 \cite{krizhevsky2009learning} and Clothing1M \cite{xiao2015learning}. Following the convention from \cite{sieve2020,xu2019l_dmi}, we use ResNet34 for CIFAR10 and CIFAR100 and ResNet50 for Clothing1M.
Noting the expected peer term   (a.k.a. \textit{confidence regularizer (CR)} as implemented in \cite{sieve2020}) is more stable and converges faster than the one with peer samples, we train with .
It also enables a fair ablation study since  is constructed relying on \cite{sieve2020}.
For numerical stability, we use a cut-off version of the cross-entropy loss .
Specifically, we use  for the traditional cross-entropy term, use  for the CR term, and the covariance term.
All the experiments use a momentum of .
The weight decay is set as  for CIFAR experiments and  for Clothing1M.

\noindent\textbf{Noise type}~~
For CIFAR datasets, the instance-dependent label noise is generated following the method from \cite{sieve2020,xia2020parts}.
\rev{The basic idea is randomly generating one vector for each class ( vectors in total) and project each incoming  feature  onto  these  vectors.
The label noise is added by jointly considering the clean label and the projection  results.
See Appendix \ref{sec:instance_noise_gen} for details.}
In expectation, the noise rate  is the overall ratio of examples with a wrong label in the entire dataset.
For the Clothing1M dataset, we train on 1 million noisy training examples that encode the real-world human noise. 

\subsection{Baselines}
We compare our method with several related works, where the cross-entropy loss is tested as a common baseline.
Additionally, the generalized cross-entropy \cite{zhang2018generalized} is compared as a generalization of mean absolute error and cross-entropy designed for label noise.
Popular loss correction based methods \cite{patrini2017making,xia2020parts,xia2019anchor}, sample selection based methods \cite{sieve2020,han2018co,wei2020combating,yu2019does}, and noise-robust loss functions \cite{liu2019peer,xu2019l_dmi} are also chosen for comparisons.
All the compared methods adopt similar data augmentations, including standard random crop, random flip, and normalization.
Note the recent work on part-dependent label noise \cite{xia2020parts} did not apply random crop and flip on the CIFAR dataset.
For a fair comparison with \cite{xia2020parts}, we remove the corresponding data augmentations from our approach and defer the comparison to Appendix~\ref{sec:noaug}.
The semi-supervised learning based methods with extra feature-extraction and data augmentations are not included.
\rev{All the CIFAR experiments are repeated  times with independently synthesized IDN.}
The highest accuracies on the clean testing dataset are \rev{averaged over  trials to show the best generalization ability of each method.}




\subsection{Performance Comparisons}
\subsubsection{CIFAR}



\begin{table*}[!t]
		\caption{Comparison of test accuracies () using different methods.}
		\begin{center}
		\scalebox{.8}{{\begin{tabular}{c|cccccc} 
				\hline 
				 \multirow{2}{*}{Method}  & \multicolumn{3}{c}{\emph{Inst. CIFAR10} } & \multicolumn{3}{c}{\emph{Inst. CIFAR100} } \\ 
				 & && & &&\\
				\hline\hline
			     CE (Standard)  &85.450.57 & 76.231.54 &  59.751.30 &  57.791.25 & 41.150.83 & 25.681.55 \\
				 Forward  \cite{patrini2017making} & 87.221.60 & 79.372.72 & 66.564.90  & 58.191.37 & 42.801.01 & 27.913.35\\
				  \cite{xu2019l_dmi}  &88.570.60 & 82.821.49 & 69.941.31 & 57.901.21 & 42.700.92 & 26.962.08\\
				  \cite{zhang2018generalized}   & 85.810.83 & 74.661.12 & 60.763.08  & 57.030.27 & 39.811.18 & 24.872.46\\
				 Co-teaching \cite{han2018co} & 88.870.24 & 73.001.24 & 62.511.98 & 43.300.39 & 23.210.57 & 12.580.51\\
				 Co-teaching+ \cite{yu2019does}  & 89.800.28 & 73.781.39 & 59.226.34 & 41.710.78 & 24.450.71 & 12.580.51\\
				JoCoR \cite{wei2020combating} & 88.780.15 & 71.643.09 & 63.461.58 & 43.661.32 & 23.950.44 & 13.160.91\\
				Reweight-R \cite{xia2019anchor} & 90.040.46 & 84.112.47 & 72.182.47 & 58.000.36 & 43.838.42 & 36.079.73\\
				Peer Loss \cite{liu2019peer} &89.120.76 & 83.260.42 & 74.531.22  & 61.160.64 & 47.231.23 & 31.712.06\\
				\SPL{} \cite{sieve2020} & 91.140.46 & 83.671.29 & 77.682.24 &66.470.45 & 58.991.49 & 38.553.25\\
				\covpeer{}  & \textbf{92.010.75} & \textbf{84.961.25} & \textbf{79.822.56}  & \textbf{69.110.46} & \textbf{63.171.40} & \textbf{43.583.30}\\
			   \hline
			\end{tabular}}}
		\end{center}
		\vspace{-8pt}
		\label{table:cifar-inst}
\end{table*}






In experiments on CIFAR datasets, we use a batch size of , an initial learning rate of , and reduce it by a factor of  at epoch .

\noindent\textbf{Construct }~~
To construct , we update the DNN for  epochs by minimizing  (without dynamic sample sieve) and apply Algorithm~\ref{alg:D} with .\footnote{Theoretically, we have  if both the CE term and the CR term use a log loss without cut-off (). Current setting works well (not the best) for CIFAR experiments empirically.}
For a numerically stable solution, we use the square root of the noise prior for the CR term in  as 
The hyperparameter  is set to  for CIFAR10 and  for CIFAR100.

\noindent\textbf{Train with \covpeer{}}~~
With an estimate of , we re-train the model  epochs.
The hyper-parameter  is set to  for CIFAR10 and  for CIFAR100.
Note the hyperparameters (, , ) can be better set if a clean validation set is available.

\noindent\textbf{Performance}~~
Table~\ref{table:cifar-inst} compares the means and standard deviations of test accuracies on the clean test dataset when the model is trained with synthesized instance-dependent label noise in different levels. 
All the compared methods use ResNet34 as the backbone.
On CIFAR10, with a low-level label noise (), all the compared methods perform well and achieve higher average test accuracies than the standard CE loss.
When the overall noise rates increase to high, most of the methods suffer from severe performance degradation while \covpeer{} still achieves the best performance.
There are similar observations on CIFAR100.
By comparing \covpeer{} with \SPL{}, we conclude that the adopted second-order statistics do work well and bring non-trivial performance improvement. 
Besides, on the CIFAR100 dataset with  and , we observe Reweight-R \cite{xia2019anchor} has a large standard deviation and a relatively high mean, indicating it may perform as well as or even better than \covpeer{} in some trials. 
It also shows the potential of using a revised transition matrix  \cite{xia2019anchor} in severe and challenging instance-dependent label noise settings.



\subsubsection{Clothing1M}
For Clothing1M, we first train the model following the settings in \cite{sieve2020} and construct  with the best model.
Noting the overall accuracy of noisy labels in Clothing1M is about  \cite{xiao2015learning}, we set an appropriate  such that  of training examples satisfying .
With , we sample a class-balanced dataset by randomly choosing  noisy examples for each class and continue training the model with  and an initial learning rate of  for  epochs.
Other parameters are set following \cite{sieve2020}.
See Appendix~\ref{sec:detailImplement} for more detailed experimental settings.
Table~\ref{table:c1m} shows \covpeer{} performs well in the real-world human noise.


\subsection{Ablation Study}

Table~\ref{table:analysis_component} shows either the covariance term or the peer term can work well individually and significantly improve the performance when they work jointly.
Comparing the first row with the second row, we find the second-order statistics can work well (except for ) even without the peer (CR) term. 
In row 4, we show the performance at epoch  since the second-order statistics are estimated relying on the model prediction at this epoch.
By comparing row 4 with row 5, we know the second-order statistics indeed lead to non-trivial improvement in the performance.
Even though the covariance term individually can only achieve an accuracy of  when , it can still contribute more than  of the performance improvement (from  to ) when it is implemented with the peer term.
This observation shows the robustness of \covpeer{}.

\begin{table}[!t]
	\caption{The best epoch (clean) test accuracies on Clothing1M. }
	\vspace{-3pt}
	\begin{center}
	\scalebox{.8}{{
		\begin{tabular}{c|c} 
			\hline 
			Method & Accuracy \\
			\hline \hline 
			CE (standard) &  68.94\\
			Forward   \cite{patrini2017making} & 70.83\\
			Co-teaching \cite{han2018co}  &69.21 \\
			JoCoR \cite{wei2020combating} &70.30 \\
			 \cite{xu2019l_dmi} & 72.46\\
			PTD-R-V\cite{xia2020parts} & 71.67 \\
			\SPL{} \cite{sieve2020} & 73.24\\
			\covpeer{}  & \textbf{74.17} \\
			\hline 
		\end{tabular}
		}}
	\end{center}
	\vspace{-5pt}
	\label{table:c1m}
\end{table}
\begin{table}[!t]
	\caption{Analysis of each component of \covpeer{} on CIFAR10. The result of a particular trial is presented. \textit{Cov.}: the covariance term. \textit{Peer}: the CR term \cite{sieve2020} (a.k.a. expected peer term \cite{liu2019peer}).}
	\vspace{-5pt}
	\begin{center}	
	\scalebox{.8}{
	\begin{tabular}{cccc|ccc} 
			\hline 
			 row \# & \emph{Cov.}  & \emph{Peer} & Epoch  & &&\\
			\hline\hline
			1 & \xmark&\xmark & Best & 90.47 & 82.56 & 64.65 \\
			2 & \cmark&\xmark & Best & 92.10  & 78.49 & 73.55\\
			3 & \xmark&\cmark & Best &  91.85 & 84.41 &  78.74\\
			4 & \xmark&\cmark & Fixed@65 & 90.73 & 82.76 &  77.70\\
			5 & \cmark &\cmark & Best  &92.69 & 85.55& 81.54\\
			\hline
		\end{tabular}}
	\end{center}
	\vspace{-8pt}
	\label{table:analysis_component}
\end{table}






 \section{Conclusions}
\vspace{-2pt}
This paper has proposed a second-order approach to transforming the challenging instance-dependent label noise into a class-dependent one such that existing methods targeting the class-dependent label noise could be implemented.
Currently, the necessary information for the covariance term is estimated based on a sample selection method.
Future directions of this work include extensions to other methods for estimating the covariance terms accurately.
We are also interested in exploring the combination of second-order information with other robust learning techniques.

\noindent\textbf{Acknowledgements}~~
This research is supported in part by National Science Foundation (NSF) under grant IIS-2007951, and in part by Australian Research Council Projects, i.e., DE-190101473.
 
\clearpage
\newpage
\bibliographystyle{ieee_fullname}
\bibliography{main.bib}


\clearpage
\newpage
\appendix
\onecolumn
\noindent{\Large \bf Appendix}

\section{Proof for Lemmas}



\subsection{Proof for Lemma~\ref{lem:invariant_d}}\label{proof:invariant_d}

\begin{proof}

We try to build the connection between noisy distribution  and the underlying Bayes optimal distribution  by the noise rates  and . 
The primary difference from the proof of Lemma 2 in \cite{liu2019peer} is the usage of .
Note:

Similarly, following the proof of Lemma 2 in \cite{liu2019peer}, we can prove this lemma.
\end{proof}

\subsection{Proof for Lemma~\ref{lem:peerBayes}}\label{proof:peerBayes}

\paragraph{Peer Loss on the Bayes Optimal Distribution}

Recall our goal is to learn a classifier  from the noisy distribution  which also minimizes the loss on the corresponding Bayes optimal distribution , i.e.
. 
Before considering the case with label noise, we need to prove peer loss functions induce the Bayes optimal classifier when minimizing the 0-1 loss on  as in Lemma~\ref{lem:peerBayes}.


\begin{lemma}\label{lem:peerBayes}
Given the Bayes optimal distribution , the optimal peer classifier defined below:

also minimizes .
\end{lemma}
See the proof below.
It has been shown in \cite{liu2019peer} that Lemma~\ref{lem:peerBayes} holds for the clean distribution  when the clean dataset is class-balanced, i.e. . For the Bayes optimal distribution , as shown in Lemma~\ref{lem:peerBayes}, there is \emph{no requirement} for the prior .

\begin{proof}

Recall  is the Bayes optimal label defined as 

We need to prove that the ``optimal peer classifier" defined below:

is the same as the Bayes optimal classifier . To see this, suppose the claim is wrong. Denote by (notations  and  are defined only for this proof): 

and denote by .
Then 

contradicting the optimality of . Thus our claim is proved.
\end{proof}
\section{Proof for Theorems}

\subsection{Proof for Theorem~\ref{thm:peerIDN}}\label{proof:peerIDN}


\begin{proof}
The covariance  in this proof is taken over the Bayes optimal distribution .
The following proof is built on the result of Theorem~\ref{thm:covpeerBinary}, i.e. Eq.~(\ref{eq:binaryCAP}).
First note

Similarly, one can show that 


Now with bounded variance in the error rates, suppose: 

Note

Then
Thus 

Noting , we finish the proof.

\end{proof}


\subsection{Proof for Theorem~\ref{thm:covpeerBinary}}\label{proof:covpeerBinary}
\begin{proof}


The covariance  in this proof is taken over the Bayes optimal distribution .
Recall 

and 

We first have the following equality:

Term-B can be transformed to:

Similarly, Term-D turns to

Define two random variables

Then Term-A becomes

Similarly, Term-B can be further transformed to

Combining the above results, we have

\end{proof}


\subsection{Proof for Theorem~\ref{thm:optimal}}\label{proof:optimal}






\begin{proof}
From Theorem~\ref{thm:covpeerBinary}, we know

With Lemma~\ref{lem:peerBayes}, we can finish the proof.
\end{proof}

\subsection{Proof for Theorem~\ref{thm:imperfect}}\label{proof:imperfect}
\begin{proof}


Recall  is the expected ratio (a.k.a. probability) of correct examples in , i.e.

With , the classifier learned by minimizing the 0-1 \covpeer{} loss is 
{
}
Note

Similarly,

When ,  and  have the same feature set, we have

Therefore, 

The rest of the proof can be accomplished by following the proof of Theorem~\ref{thm:peerIDN}.
\end{proof}


\section{Proof for Corollaries}




\subsection{Proof for Corollary~\ref{cor:covpeerMul}}\label{proof:covpeerMul}

\begin{proof}



The first term in (\ref{Eq:peerlossExpSup}) is


The rest of proofs can be done following standard multi-class peer loss derivations \cite{liu2019peer}. 

\end{proof}

\section{More Discussions}

\subsection{Setting Thresholds  and }\label{dis:L}

In a high level, there are two strategies for setting  and : {1)  and 2) }.


\paragraph{Strategy-1: :}
This strategy may provide a higher ratio of true Bayes optimal labels among feasible examples in  since some ambiguous examples are dropped. However, dropping examples changes the distribution of  (as well as the distribution of the unobservable ), a.k.a. covariate shift \cite{huang2007correcting,cheng2017learningdistill}. 
Importance re-weighting with weight  is necessary for correcting the covariate shift, i.e. the weight of each feasible example  should be changed from  to .
Let  and  be the marginal distributions of  and  on .
With a particular kernel , the optimization problem is:

The optimal solution is supposed to be .
Note the selection of kernel  is non-trivial, especially for complicated features \cite{fang2020rethinking} in DNN solutions.
Using this strategy, with appropriate  and  such that all the examples in  are Bayes optimal, the covariance could be guaranteed to be optimal when each example in  is re-weighted by . 


\paragraph{Strategy-2: :}
Compared with Strategy-1, we effectively lose one degree of freedom for getting a better . However, this is not entirely harmful since  and  have the same feature set, indicating estimating  is no longer necessary and  is an optimal solution for (\ref{eq:opt_gamma}) with this strategy.



\paragraph{Strategy selection}
When we can get a high-quality  by fine-tuning  and  or  is already provided from other sources, we may solve the optimization problem in (\ref{eq:opt_gamma}) to find the optimal weight .
However, considering the fact that estimating  introduces extra computation and potentially extra errors, we focus on Strategy-2 in this paper.
Using Strategy-2 also reduces the effort on tuning hyperparameters.
Besides, the proposed \covpeer{} loss is tolerant of an imperfect  (shown theoretically in Section~\ref{sec:imperfectCov}).



\subsection{Generation of Instance-Dependent Label Noise}\label{sec:instance_noise_gen}

Pseudo codes for generate instance-based label noise are provided in Algorithm \ref{alg_noise}.
This algorithm follows the state-of-the-art method \cite{xia2020parts}.
Define the overall noise rate as . 


\begin{algorithm}[!h]
\LinesNumbered
\SetAlgoLined
\KwIn{Clean examples ; Noise rate: ; Number of classes: ; Shape of each feature : .}
Sample instance flip rates  from the truncated normal distribution ; \hfill \algcom{// mean , variance , range }\\
Sample    from the standard normal distribution ;\\
    \For{}{
        \hfill \algcom{//  Generate instance dependent flip rates. The size of  is .}\\
        \hfill  \algcom{//  Only consider entries that are different from the true label}\\
       \hfill \algcom{//  Let  be the probability of getting a wrong label}
			\\
	    \hfill \algcom{//  Keep clean w.p. } 
			\\
	   Randomly choose a label from the label space as noisy label  according to ;
     } 
 \KwOut{Noisy examples .}
 \caption{Generating Instance-Dependent Label Noise}
 \label{alg_noise}
\end{algorithm}

Note Algorithm~\ref{alg_noise} cannot ensure  when . To generate an informative dataset, we set  as the upper bound of  and distribute the remaining probability to other classes.
	
	
\subsection{Performance without Data Augmentations}\label{sec:noaug}
For a fair comparison with the recent work on instance-dependent label noise \cite{xia2020parts}, we adopt the same data augmentations as \cite{xia2020parts} and re-produce their results using the same noise file as we employed in Table~\ref{table:cifar-inst}.
Each noise rate is tested  times with a different generation matrix  (defined in Algorithm~\ref{alg_noise}).
Table~\ref{table:noaug} shows the advantages of our second-order approach.
\begin{table}[!h]
	\caption{Performance comparisons without data augmentations}
	\begin{center}
	\scalebox{.85}{{
		\begin{tabular}{c|cc} 
			\hline 
			Method &  &  \\
			\hline \hline 
			PTD-R-V\cite{xia2020parts} &  &  \\
			\covpeer{}  &   &  \\
			\hline 
		\end{tabular}
		}}
	\end{center}
	\label{table:noaug}
\end{table}
\subsection{More Implementation Details on Clothing1M}\label{sec:detailImplement}

\paragraph{Construct }
We first train the network for 120 epochs on 1 million noisy training images using the method in \cite{sieve2020}.
The batch-size is set to 32. The initial learning rate is set as 0.01 and reduced by a factor of 10 at 30, 60, 90 epochs. 
We sample 1000 mini-batches from the training data for each epoch while ensuring the (noisy) labels are balanced. 
Mixup \cite{zhang2018mixup} is adopted for data augmentations.
Hyperparameter  is set to 0 at first 80 epochs, and linearly increased to 0.4 for next 20 epochs and kept as 0.4 for the rest of the epochs. 
We construct  with the best model.


\paragraph{Train with \covpeer{}}
We change the loss to the \covpeer{} loss after getting  and continue training the model (without mixup) with an initial learning rate of  for 120 epochs (reduced by a factor of 10 at 30, 60, 90 epochs).
We also tested re-train the model with  and get an accuracy of .
A randomly-collected balanced dataset with  noisy examples in each class is employed in training with \covpeer{}.
Examples that are not in this balanced dataset are removed from  for ease of implementation. 





\end{document}
