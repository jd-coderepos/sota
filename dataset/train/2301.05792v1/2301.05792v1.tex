\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2021}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}      
\usepackage{microtype}      
\usepackage{xcolor}         
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{listing}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{enumitem}

\usepackage{amssymb}
\usepackage{setspace}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{booktabs} 
\usepackage{array}
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{todonotes}

\definecolor{BlueBlack}{RGB}{36, 113, 163}

\hypersetup{colorlinks=true, citecolor=BlueBlack, linkcolor=BlueBlack, urlcolor=BlueBlack}

\definecolor{mygray}{gray}{0.6}
\definecolor{mygray-bg}{gray}{0.9}
\newcommand{\myparagraph}[1]{\vspace{0.1em}\noindent\textbf{#1}}
\newcommand{\myparagraphsupp}[1]{\vspace{0.1em}\noindent{\textcolor{red}{#1}}}
\newcommand{\redt}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\redtext}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\cotronlvsapce}{\vspace{0.0cm}}
\newcommand{\cotronlcaptionvsapce}{\vspace{0.0cm}}
\newcommand{\cotronlvsapcetop}{\vspace{0.0cm}}
\newcommand{\cotronlvsapcebottom}{\vspace{0.0cm}}
\newcommand{\beginsupp}{\setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}}
\newcommand{\mycaptionsupp}[1]{{\textcolor{red}{#1}}}

\title{RMM: Reinforced Memory Management
\\for Class-Incremental Learning}

\author{Yaoyao Liu 
\quad Bernt Schiele 
\quad Qianru Sun\\
\\
\small Max Planck Institute for Informatics, Saarland Informatics Campus\\
\small School of Computing and Information Systems, Singapore Management University\\
\small {\texttt{\{yaoyao.liu, schiele\}@mpi-inf.mpg.de}}  \quad  {\texttt{qianrusun@smu.edu.sg}}
}

\begin{document}

\maketitle

\begin{abstract}

Class-Incremental Learning (CIL)~\cite{rebuffi2017icarl} trains classifiers under a strict memory budget: in each incremental phase, learning is done for new data, most of which is abandoned to free space for the next phase. The preserved data are exemplars used for replaying. However, existing methods use a static and ad hoc strategy for memory allocation, which is often sub-optimal. In this work, we propose a dynamic memory management strategy that is optimized for the incremental phases and different object classes. We call our method reinforced memory management (RMM), leveraging reinforcement learning. RMM training is not naturally compatible with CIL as the past, and future data are strictly non-accessible during the incremental phases. We solve this by training the policy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data of the -th phase, and then applying it to target tasks. RMM propagates two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. In essence, it is an optimizable and general method for memory management that can be used in any replaying-based CIL method. For evaluation, we plug RMM into two top-performing baselines (LUCIR+AANets and POD+AANets~\cite{Liu2020AANets}) and conduct experiments on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets by , , and  in the -Phase settings of the above benchmarks, respectively.  The
code is available at \texttt{\href{https://class-il.mpi-inf.mpg.de/rmm/}{https://class-il.mpi-inf.mpg.de/rmm/}}.

\end{abstract}
 \section{Introduction}
\label{sec_introduction}

Ideally, AI systems should be adaptive to ever-changing environments---where the data are continuously observed by sensors. Their models should be capable of learning new concepts from data while maintaining the ability to recognize previous ones. In practice, the systems often have constrained memory budgets because of which most of the historical data have to be abandoned~\cite{hu2021cil}. However, deep-learning-based AI systems, when continuously updated using new data and limited historical data, often suffer from catastrophic forgetting, as the updates can override knowledge acquired from previous data~\cite{mccloskey1989catastrophic, McRae1993Catastrophic, Ratcliff1990catastrophic}.

To encourage research on the forgetting problem, Rebuffi et al.~\cite{rebuffi2017icarl} defined a standard protocol of class-incremental learning (CIL) for image classification, where the training data of different object classes come in phases. In each phase, the classifier is evaluated on all classes observed so far. As the total memory size is limited~\cite{rebuffi2017icarl}, CIL systems abandon the majority of the data and only preserve a small number of exemplars, e.g.,  exemplars per class, which will be used for replaying in subsequent phases. Replaying usually happens for multiple epochs~\cite{douillard2020podnet,hou2019lucir,Liu2020AANets,rebuffi2017icarl}, so both the old class exemplars and new class data need to be stored in the limited memory. Existing CIL methods allocate memory between the old and new classes in an arbitrary and static fashion, e.g.,  per old class \emph{vs.}  per new class for the ImageNet-Full dataset. This causes a serious imbalance between the old and new classes and can exacerbate the problem of catastrophic forgetting.

\begin{figure}[t]
\centering
\includegraphics[width=5.4in]{files/figure1.pdf}
\cotronlcaptionvsapce
\caption{(a) Existing CIL methods~\cite{hou2019lucir,Liu2020AANets,rebuffi2017icarl} allocate memory between old and new classes in an arbitrary and frozen way, causing the data imbalance between old and new classes and exacerbating the catastrophic forgetting of old knowledge in the learned model. (b) Our proposed method---Reinforced Memory Management (RMM)---is able to learn the optimal and class-specific memory sizes in different incremental phases. Please note we use \textcolor[rgb]{0.87, 0.49, 0.35}{orange}, \textcolor[rgb]{0.27, 0.47, 0.69}{blue}, and \textcolor[rgb]{0.35, 0.58, 0.53}{green} dots to denote the samples observed in the (-1)-th, -th, and (+1)-th phases, respectively.
}
\vspace{-0.2cm}
\label{fig_1}
\cotronlvsapce
\end{figure} 
To address this, we propose to learn an optimal memory management policy for each incremental phase with continuously reinforced model performance and call our method reinforced memory management (RMM). Detailed actions include 1) allocating the memory between the existing (old) and the coming (new) data for each phase, and 2) specifying the memory for each old class according to its recognition difficulty before abandoning any of its data. To this end, we leverage reinforcement learning~\cite{li2019online,li2019online3,li2019online2,williams1992simple,ZophL17} and design a new policy function to contain two sub-functions that propagate two levels of actions in a hierarchical way. Level-1 function determines how to split memory between the old and new data. Its output action is then inputted into the Level-2 function to determine how to allocate memory for each old class. The overall objective of the function is to maximize the cumulative evaluation accuracy across all incremental phases. However, this is not naturally compatible with the standard protocol of CIL~\cite{rebuffi2017icarl} where neither past nor future data are accessible for evaluation. 
To tackle this issue, we propose to pre-train the function on pseudo CIL tasks and then adopt it in the learning process of our target task. 
In principle, we can build such pseudo tasks using any available categorical data, e.g., the data in the -th phase of the target CIL task or the data from another dataset. Even though this is a non-stationary reinforcement learning problem, we can regard the pseudo and target CIL tasks as a sequence of stationary tasks and train the policy function to exploit the dependencies between these consecutive tasks. Such continuous adaptation in non-stationary environments is feasible based on the empirical analysis given in~\cite{ShedivatBBSM18}.

Technically, we propose the following method to guarantee the transferability of policy functions between pseudo and target CIL tasks. We take a Level-1 action based on the ratio of the number of new classes to the total number of classes observed so far. A lower (higher) ratio will result in weakening the stability (plasticity) of the classification model. Then, we take a Level-2 action for each individual class conditioned on both the Level-1 action and the training entropy of that class. A higher entropy denotes a more difficult class, leading to more memory allocated to the class. For evaluation, we conduct extensive CIL experiments by plugging RMM into two top-performing methods (LUCIR+AANets, POD+AANets) and testing them on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results show the clear and consistent superiority of RMM, e.g., it boosts the state-of-the-art POD+AANets by , , and  in the -Phase settings of the above benchmarks, respectively.

Our technical contribution is three-fold. 1) A hierarchical reinforcement learning algorithm called RMM to manage the memory in a way that can be conveniently modified through incremental phases and for different classes. 2) A pseudo task generation strategy that requires only in-domain available data (small-scale) or cross-domain datasets (large-scale), relieving the data incompatibility between reinforcement learning and class-incremental learning. 
3) Extensive experiments, visualization, and interpretation for RMM in three CIL benchmarks and using two top models as baselines. \section{Related Work}
\label{sec2}

\myparagraph{Incremental Learning}~\cite{iscen2020memory,rajasegaran2019random,simon2021learning,wu2021incremental,zhu2021prototype} continuously updates the model using data coming in a sequence of phases. 
Similar tasks are also referred to as continual learning~\cite{de2019continualsurvey,lopez2017gradient} and lifelong learning~\cite{aljundi2017expert,chen2018lifelong}.
Recent papers are either task-incremental learning---each phase corresponds to a task (dataset) that contains new data of all seen classes~\cite{chaudhry2018efficient,davidson2020sequential,hu2018overcoming,Li18LWF,riemer2018learning,shin2017continual,zhao2020maintaining}, 
or class-incremental learning (CIL)---each phase contains data of a new set of classes, i.e., classes are unseen~\cite{belouadah2019il2m,Castro18EndToEnd,hou2019lucir,Kukleva_2021_ICCV,liu2020mnemonics,rajasegaran2020itaml,rebuffi2017icarl,ren2018incremental,Tao2020topology,Wu2019LargeScale,yu2020CVPRsemantic,Zhang_2021_ICCV,zhao2020maintaining}. 
This paper is concerned with CIL. 
The key challenge of CIL is the forgetting problem---older classes are forgotten in later phases. 
Existing methods tackling this can be divided into three categories:
memory-based, regularization-based, and network-architecture-based~\cite{de2019literaturereview,prabhu12356gdumb}. 
\myparagraph{\emph{Memory-based}} methods preserved a small subset of the old class data (exemplars) to replay the model on them (together with the new class data), in order to relieve the forgetting of the old classes. 
Some work~\cite{hou2019lucir,rebuffi2017icarl} proposed heuristic strategies to select more representative exemplars from the old class data, and  others~\cite{liu2020mnemonics,shin2017continual} tried to generate exemplars in optimizable frameworks. None of them changed the allocation of memory for different classes, i.e., all used an arbitrary and static scheme for memory allocation.
\myparagraph{\emph{Regularization-based}} methods introduce regularization terms in the loss function to consolidate previous knowledge when training the model on new data. The key idea is to enforce predicted label logits~\cite{Li18LWF,rebuffi2017icarl}, features maps~\cite{douillard2020podnet,hou2019lucir}, or the topology in the feature space~\cite{Tao2020topology} of the new model to be close to that of the previous model.
\myparagraph{\emph{Network-architecture-based}} methods aim to design ``incremental network architectures''. Some work~\cite{rusu2016progressive,xu2018reinforced} gradually extended the network capacity for new data, while others proposed to freeze partial network parameters~\cite{abati2020conditional,Liu2020AANets} to preserve the knowledge of the old classes.

\myparagraph{Reinforcement Learning} defines an agent that needs to decide its actions in an unknown environment by maximizing the expected cumulative reward. 
It has been widely applied to many optimization problems, e.g., neural architecture search~\cite{xu2018reinforced,ZophL17} and neural machine translation~\cite{RanzatoCAZ15,ShenCHHWSL16}. 
Reinforcement learning has also been introduced to solve incremental learning problems.
Xu et al.~\cite{xu2018reinforced} proposed to increase convolution filters once a new task arrives and optimize the increased number by reinforcement learning.
Gao et al.~\cite{gao2020efficient} proposed an improved version that makes the minimal expansion of the network, reducing memory and computing overheads.
Veniat et al.~\cite{abs-2012-12631} introduced a modular architecture, where each module represents a different atomic skill, and used the REINFORCE algorithm~\cite{williams1992simple} to optimize it.
Huang et al.~\cite{huang2019neural} combined reinforcement learning with Net2Net~\cite{ChenGS15ICLR} and designed a NAS-based CIL method.
In our work, we also use the REINFORCE algorithm~\cite{williams1992simple}, but \textbf{differ} 
in three aspects. 
First, we are the first to optimize memory allocation for CIL in a reinforced way.
Second, we learn the policy functions on generated pseudo CIL tasks, where we can access both past, and future data (for each incremental phase) and thus are able to compute the cross-phase (long-term) rewards.
In contrast, the related work~\cite{gao2020efficient,xu2018reinforced} could use only current-phase data to estimate a short-term reward. 
Third, our reinforcement learning has a hierarchical structure that specially fits the nature of the data stream in the CIL settings. \section{Preliminaries}
\label{sec_preliminaries}

\myparagraph{Class-Incremental Learning (CIL)} usually assumes (+) learning phases: an initial phase and  incremental phases during which the number of classes gradually increases till the maximum~\cite{douillard2020podnet,hou2019lucir,hu2021cil,liu2020mnemonics}. We assume that total memory  is bounded and fixed for all incremental phases~\cite{rebuffi2017icarl}. 
 is used to store the exemplars and new coming data as both kinds of data need to be loaded repeatedly during training epochs.
In the initial (-th) phase, data , containing the training samples of  classes, are used to learn the initial classification model . In the -th incremental phase, we split  into two dynamic partitions: 
the exemplar memory  and new data memory . We select  as representative samples of the data seen in the -th phase, and denote total exemplars  shortly as . We save  into  and free . Then, we observe new data that contain  new classes.
We randomly load new data into  until  is full, and all the other new data are discarded. We denote the loaded new data as .
Then, we initialize  with , and train it using . The resulting model  will be evaluated with a test set containing all classes observed so far. We repeat this training and testing, and report the average accuracy across all phases.


\myparagraph{Reinforcement Learning (RL)} aims to learn an optimal policy function  for an agent interacting in an unknown environment~\cite{williams1992simple,xu2018reinforced,ZophL17}. In the CIL scenario, in each incremental phase, the agent observes the current state  from the environment, and then takes an action  (how to allocate memory) according to the policy function . Subsequently, the environment is updated to a new state , and the reward  is calculated to optimize the parameters of  through back-propagation. 
Specifically, the learning objective of  is to maximize the expected cumulative reward , where  is a discounting factor that determines the weights of future rewards. Please note that in our case, the (+)-phase CIL task is a finite horizon problem~\cite{glorennec2000reinforcement,ZophL17}, so we remove the discounting factor and use , which is actually
the cumulative validation accuracy of all training CIL tasks. In Section~\ref{sec_method}, we discuss the proposed RL algorithm for memory allocation and how to generate pseudo tasks for training its policy function. \section{Reinforced Memory Management (RMM)}
\label{sec_method}

Our RMM approach learns policy functions that propagate two levels of actions in a hierarchical way, specially designed for CIL. As illustrated in Figure~\ref{fig_1} (b), Level-1 determines the memory split between exemplars and new data, and Level-2 allocates the memory for each individual class. We motivate and introduce the formulation of RMM, including the definitions of states, actions, rewards, and hierarchical policy functions in Section~\ref{subsec_RMM_formulation}. In Section~\ref{subsec_optimization}, we detail the steps of creating pseudo CIL tasks on which we learn the policy functions. In Section~\ref{subsec_algo}, we summarize the algorithm.

\subsection{Formulation}
\label{subsec_RMM_formulation}

In the -th incremental phase CIL, we manage the memory for two kinds of data: exemplars  and new data . For the former, 
we have access to their images and labels
so we can allocate a different memory size to a different class, e.g., based on its recognition difficulty.
For the latter, we do not have such access before loading the data (otherwise, causing a violation of the CIL protocol), so we are only able to learn a total memory size, i.e., the memory size for all new classes (and then split it evenly for each individual class).
Therefore, memory management in CIL settings is inherently hierarchical: 1) coarse memory allocation between exemplars and new data; and then 2) fine-grained memory allocation among specific classes. To this end, we modify the standard reinforcement learning into a hierarchical structure.

As illustrated in Figure~\ref{fig_2} (a), in the -th incremental phase of  CIL (i.e., the environment), the argent receives a state value .
Level-1 policy  takes  as the input to produce an action . 
 determines how to split memory between the exemplars and new data. 
After that, Level-2 policy  takes  and  as inputs to produce the second action  that distributes the exemplar memory for each individual class.

\begin{figure}[t]
\centering
\includegraphics[height=2.21in]{files/figure2.pdf}
\cotronlcaptionvsapce
\caption{(a) In the -th phase of the -th pseudo CIL task, Level-1 policy  takes  as the input, and produces action . Level-2 policy  takes  and  as the inputs, then produces action . 
(b) For the -th pseudo CIL task, we allocate memory for  times (i.e., in  phases) using the policies  and , and compute the cumulative reward .
}
\label{fig_2}
\cotronlvsapce
\end{figure} \myparagraph{States}, defined for our CIL settings, should have two properties. 1) Being transferable between CIL tasks, e.g., from a small-scale CIL task including  classes (in total) to a large one including  classes.
The reason is that we need to transfer the policy functions learned from pseudo CIL tasks (defined in Section~\ref{subsec_optimization}) to the target task. The states, the inputs of policy functions, should also be transferable.
2) Being distinct in each incremental phase.
This is to enable the state variable to represent a specific forgetting or data imbalance degree at each different learning phase of the CIL model.
To fulfill these properties, we formulate the state in the -th phase as 
, where  denotes the number of classes in ,  denotes the memory allocated to exemplars , and  is the total memory.


\myparagraph{Level-1 Actions.} In the -st incremental phase, our Level-1 policy function produces an action to allocate the memory for exemplars  and new data . We denote this action as  
and assign its value with the ratio of the number of the exemplars  to the memory size , so we have . In the -th phase (), 
the definition of  is different to  
as it is a relative change over . Specifically,  is the ratio of increased (if its value is positive) or decreased (if negative) memory size of  compared to the (-)-th phase. Using this definition aims for smooth and continuous memory management. In the formulation, the memory sizes of exemplars  and new data  are, respectively,

We set a constrain  for . Otherwise, if  is too big, there are not enough exemplars to fill the memory, as most old-class data has been abandoned. If  is too small, many exemplars will be permanently deleted in this phase, making it hard or even impossible to adjust  back to a high value in the future phases.
If ,  will be negative. So, we force  by rejection sampling~\cite{bishop2006pattern}, i.e., using  
to output another action until it is feasible to execute. Note that this situation rarely happens in real training, because when  becomes very low,  tends to produce an action to increase it.

\myparagraph{Level-2 Actions.} 
Here, we elaborate on how to get class-specific memory allocation. In the -th phase, we split the classes for  into two groups evenly according to training entropy values: classes with higher values (difficult classes) are in one group and the rest in the other group.
Therefore, Level-2 action  determines how to split memories between harder and easier classes. 
During initial experiments, we observed that using two groups already yields improved results and using more groups causes a decrease.

Let  and  denote the memory allocated for the high-entropy and low-entropy groups, respectively, in the -th phase (): 

Then, we allocate memory evenly to the classes within the group, e.g., if the high-entropy group has  classes, each class will have a memory size of .


\myparagraph{Rewards.} The objective of CIL is that the trained model (in any phase) should be efficient to recognize all classes seen so far.
It is intuitive and convenient to use the validation accuracy as the reward in each phase.
In the -th phase, the objective of RMM is to maximize the expected cumulative reward,
i.e., , 
where  denotes the validation accuracy in the -th phase.

\clearpage
\subsection{Optimization}
\label{subsec_optimization}

\begin{wrapfigure}{r}{0.47\textwidth}
\vspace{-0.5cm}
\centering
\includegraphics[height=1.9in]{files/figure3.pdf}
\cotronlcaptionvsapce
\vspace{0.1cm}
\caption{Updating  and  in one epoch. To get stable gradients for , we create  different pseudo CIL tasks, and run each task for  times.}
\label{fig_3}
\cotronlvsapce
\vspace{-0.3cm}
\end{wrapfigure} 
In the CIL protocol, it is impossible to see past or future data in any incremental phase. It is thus not intuitive how to compute cumulative rewards till the last phase.
We propose to solve the issue by generating pseudo CIL tasks (where all data are accessible).

\myparagraph{Pseudo CIL Tasks} should meet two requirements:
1) their training and validation data are fully accessible for computing cumulative rewards, and 2) they have the same format (e.g., the same number of phases) of the target CIL task.
\textbf{\emph{Data Sources:}} 
For requirement 1, 
an intuitive solution is to use  (available in the -th phase). Based on the CIL protocol~\cite{douillard2020podnet,hou2019lucir,hu2021cil,Liu2020AANets},  contains half of the classes of the whole dataset, e.g.,  classes on CIFAR-100, which supplies enough data to build downsized CIL tasks.
When building the tasks, we randomly choose  training samples of each class (from ) to compose a pseudo validation set (note that we are not allowed to use the original validation set in training). 
When aiming for larger-scale data in CIL, 
we can leverage smaller datasets. For example, the pseudo tasks for ImageNet-Subset can be built on the data of CIFAR-100.
This is also meaningful to evaluate the transferability of RMM policy functions (discussed in the Ablation Study).
\textbf{\emph{Task Generation Protocol}} is based on requirement 2. 
If using another dataset, we simply follow its original CIL protocol. 
If using the data accessed in the -th phase (i.e., ), we can reduce the number of classes (in each phase) by half.
For example, for CIFAR-100, we use -class  to generate a -phase pseudo CIL task as follows: loading  classes in the -th phase, and after that, five classes per phase. 
To generate another pseudo task, we simply change the order of classes.

\textbf{Training.}
We elaborate the steps of learning Level-1 policy  and Level-2 policy  in the following.
The goal is to optimize the parameters  and  by maximizing the expected cumulative reward .
We denote any pseudo CIL task and its cumulative reward as  and , respectively, and have,

\textbf{\emph{Policy Gradient Estimation}}.
According to the policy gradient theorem~\cite{williams1992simple}, we can compute the gradients for  as follows,

Following the REINFORCE algorithm~\cite{williams1992simple}, we replace the expectations  and  with sample averages using the Monte Carlo method~\cite{hammersley2013monte}.
Specifically, in each epoch, we create  pseudo tasks and run each task for  times, as shown in Figure~\ref{fig_3}.
Thus we can derive the empirical approximation of  as,

where  denotes the -th reward for the -th pseudo task , and  denotes the baseline function---the moving average of previous rewards. 
Using this baseline function is a common trick in RL to reduce the variance of estimated policy gradients~\cite{kool2019buy,rennie2017self,ZophL17}.

\textbf{\emph{Updating Parameters}}.
We update  and  in each epoch according to the gradient ascent rule~\cite{xu2018reinforced,ZophL17}:

where  and  are the learning rates. We iterate this update for  epochs in total. 



\clearpage
\begin{wrapfigure}{r}{0.53\textwidth}

\newcommand{\myvl}{\vrule height .75\baselineskip depth .25\baselineskip}
\newcommand{\myvld}{\vrule height .75\baselineskip depth .25\baselineskip}

\scalebox{0.85}{
    \begin{minipage}{.61\textwidth}
    \vspace{-0.9cm}
\begin{algorithm}[H]
    \captionof{algorithm}{Learning policy functions in RMM} \label{alg_rmm}
        \begin{spacing}{1.30}
        \begin{algorithmic}[1]
        \STATE {\bfseries Input:} Data  for generating pseudo CIL tasks.
        \STATE {\bfseries Output:} Policy functions , .
    	
    	\STATE Initialize  and ;
        \STATE \textbf{for}  epochs \textbf{do} 

        \STATE \myvl \ \textbf{for}  \textbf{in}  \textbf{do} 
    	\STATE \myvl \ \myvl \ Create a new pseudo task  using ;
    	\STATE \myvl \ \myvl \ \textbf{for}  \textbf{in}  \textbf{do}
        \STATE \myvl \ \myvl \ \myvl \ Initialize classification model ;
        \STATE \myvl \ \myvl \ \myvl \ \textbf{for}  \textbf{in}  \textbf{do}
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ \textbf{if}  \textbf{do}
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ \myvl \ Observe  and produce ;
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ \myvl \ Allocate  and  using Eq.~\ref{eq_high_action};
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ \myvl \ Produce ;
        \STATE \myvld \ \myvld \ \myvld \ \myvld \ \myvld \ Allocate  and  using Eq.~\ref{eq_low_action};
        \vspace{-0.05cm}
        \STATE \myvld \ \myvld \ \myvld \ \myvld \ \myvld \ Update  using herding~\cite{rebuffi2017icarl};
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ \myvl \ Save  in  and free ;
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ Observe new data and load  into  randomly;
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ Initialize  with  and train it using ;
        \STATE \myvl \ \myvl \ \myvl \ \myvl \ Compute validation accuracy ;
\STATE \myvl \ \myvl \ \myvl \ Compute  and update ;
\STATE \myvl \ Compute  using Eq.~\ref{eq_empirical_approximation_with_b};
		\STATE \myvl \ Update  and  using Eq.~\ref{eq_gd}.
\vspace{-0.20cm} 
    	

    \end{algorithmic}
    \end{spacing}
    \end{algorithm}
    \vspace{-1.8cm}    
    \end{minipage}
}
\end{wrapfigure} 
\subsection{Algorithm}
\label{subsec_algo}
Algorithm~\ref{alg_rmm} summarizes the overall training steps of the proposed RMM. 
There are four loops in the algorithm: 1) we train the RMM agent for  epochs; 2) we create  pseudo CIL tasks in each epoch; 3) we run each pseudo CIL task for  times; and 4) there are + learning phases each time. Specifically, Line 3 initializes the parameters of policy functions. Line 6 creates the -th pseudo CIL task. Line 8 initializes the classification model. Lines 10-16 allocate the memory according to the actions produced by RMM policy. Line 17 loads new data. Lines 18-19 train the classification model and compute the accuracy. Line 20 estimates the -th cumulative reward. Lines 21-22 compute the gradients and update policy functions.
 \section{Experiments}
\label{sec_exp}

We evaluate the proposed RMM method on three CIL benchmarks: CIFAR-100 \cite{krizhevsky2009learning}, ImageNet-Subset \cite{rebuffi2017icarl}, and ImageNet-Full \cite{russakovsky2015imagenet}, and use two top-performing methods LUCIR+AANets and POD+AANets~\cite{Liu2020AANets} as baselines.
Below we introduce the datasets and implementation details (Section~\ref{subsec_datasets}), followed by the experimental results and analyses (Section~\ref{subsec_exp_analyses}).

\subsection{Datasets and Implementation Details}
\label{subsec_datasets}

\myparagraph{Datasets.}
We use three benchmarks based on two datasets, CIFAR-100~\cite{krizhevsky2009learning} and ImageNet~\cite{russakovsky2015imagenet}, following common settings~\cite{douillard2020podnet,hou2019lucir,rebuffi2017icarl,Liu2020AANets}. 
\textbf\emph{{CIFAR-100}}~\cite{krizhevsky2009learning} contains  samples of  color images from  classes. There are  training and  test samples for each class.
\textbf\emph{{ImageNet}} (ILSVRC 2012)~\cite{russakovsky2015imagenet} contains around  million
samples of  color images from  classes. There are about  training and  test samples for each class. 
ImageNet has two CIL settings: ImageNet-Subset is based on a subset of  classes; ImageNet-Full uses the full set of  classes. 
The -class data for the ImageNet-Subset
are sampled from ImageNet. For the experiments on PODNet~\cite{douillard2020podnet} and POD-AANets~\cite{Liu2020AANets}, we use the same class orders and hyperparameters as~\cite{douillard2020podnet}. For the experiments on LUCIR~\cite{hou2019lucir} and LUCIR-AANets~\cite{Liu2020AANets}, we use the same class orders and hyperparameters as~\cite{hou2019lucir}.

\myparagraph{Benchmarks.}
We follow the benchmark protocol used in \cite{douillard2020podnet,hou2019lucir,Liu2020AANets,liu2020mnemonics}.
Given a dataset, the initial (the -th phase) model is trained on the data of half of the classes.
Then, it learns the remaining classes evenly in the subsequent  phases. 
Assume there is an initial phase and  incremental phases in the CIL system. The total number of incremental phases  is set to be , , or  (for each, the setting is called ``-phase'' setting).
At the end of each individual phase, the learned model in each phase is evaluated on the test set containing all seen classes. In the tables, we report average accuracy over all phases and the last-phase accuracy, where the latter indicates the degree of forgetting.

\myparagraph{Network Architectures.}
Following~\cite{hou2019lucir,Liu2020AANets,rebuffi2017icarl,Wu2019LargeScale}, we use a -layer ResNet~\cite{rebuffi2017icarl} for CIFAR-100 and an -layer ResNet~\cite{He_CVPR2016_ResNet} for ImageNet. Please note that it is standard to use a shallower ResNet for ImageNet.
The -layer ResNet consists of an initial convolution layer and three residual blocks (in a single branch). Each block has ten convolution layers with  kernels. The number of filters starts from  and is doubled every next block. After these three blocks, there is an average-pooling layer to compress the output feature maps to a feature embedding. The -layer ResNet follows the standard settings in \cite{He_CVPR2016_ResNet}. We deploy AANets using the same parameters as its original paper~\cite{Liu2020AANets}. For policy functions  and , we use two-layer FC networks. All actions are discretized at  intervals to reduce the search space and get a tolerable training overhead.

\myparagraph{Hyperparameters and Configuration.}
The training of the classification model  exactly follows the uniform setting in~\cite{douillard2020podnet,hou2019lucir,Liu2020AANets,liu2020mnemonics}. On CIFAR-100 (ImageNet-Subset/Full), we train it for  () epochs in each phase, and divide the learning rate by  after  () and then after  () epochs. 
Then, we fine-tune the model for  epochs using only exemplars (including the preserved exemplars of the new data to be used in future phases).
We use an SGD optimizer and an ADAM optimizer for the classification model and policy functions, respectively. 
More details are given in the supplementary.

\myparagraph{Memory Budget.}
There are two popular settings for memory budget in related work. One uses a bounded memory budget with a fixed capacity for all phases~\cite{hou2019lucir,liu2020mnemonics,rebuffi2017icarl}. Another one allows the memory budget to grow along with phases~\cite{hou2019lucir,hu2021cil,Tao2020topology}.  
The first one is more strict and thus used as the major setting in our paper (note that the results and analyses using the second setting are given in the supplementary materials).
In every benchmark, the total budget of memory depends on the phase number .
For example, on CIFAR-100, the total memory budget is set as  samples when = ( samples =  classes/phase   samples/class +  samples). Please note that  is a bounded memory budget allocated since the -th phase for saving exemplars. 
More clarifications about the memory budget are given in the supplementary.
For fair comparison, we re-implement related methods and report the results in Table~\ref{table_sota} if their original results (in the respective papers) were obtained in a different setting of memory budget. 

\subsection{Results and Analyses}
\label{subsec_exp_analyses}

\newcommand{\highest}[1]{\textbf{#1}}
\setlength{\tabcolsep}{1.8mm}{
\renewcommand\arraystretch{1.2}
\begin{table*}\small
  \centering
  \cotronlvsapcetop
  \begin{tabular}{lccccccccccc}
  \toprule
   \multirow{2.5}{*}{Method} & \multicolumn{3}{c}{\emph{CIFAR-100}} && \multicolumn{3}{c}{\emph{ImageNet-Subset}} && \multicolumn{3}{c}{\emph{ImageNet-Full}}\\
  \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
   & =5 & 10  & 25 && 5 & 10 & 25 && 5 & 10 & 25 \\
    \midrule
LwF~\cite{Li18LWF} & 56.79 & 53.05 & 50.44 && 58.83 & 53.60 & 50.16 && 52.00 & 47.87 & 47.49\\
    iCaRL~\cite{rebuffi2017icarl} & 60.48 & 56.04 & 52.07 && 67.33 & 62.42 & 57.04 && 50.57 & 48.27 & 49.44 \\
    LUCIR~\cite{hou2019lucir}  & 63.34 & 62.47 & 59.69 && 71.21 & 68.21 & 64.15 && 65.16 & 62.34 & 57.37 \\
    Mnemonics~\cite{liu2020mnemonics} & 64.59 & 62.59 &  61.02 &&  72.60 &  71.66 &  70.52 &&  65.40 &  64.02 &  62.05 \\
    PODNet~\cite{douillard2020podnet} & 64.60 & 63.13 &  61.96 &&  76.45 &  74.66 &  70.15 &&  66.80 &  64.89 & 60.28 \\
\midrule
    LUCIR-AANets~\cite{Liu2020AANets} & 66.88 & 65.53 &  63.92 &&  72.80 &  69.71 &  68.07 && 65.31 &  62.99 & 61.21 \\
   \cellcolor{mygray-bg}{\ \ \emph{w/} RMM (ours)}  & \cellcolor{mygray-bg}{68.42} & \cellcolor{mygray-bg}{67.17} &  \cellcolor{mygray-bg}{64.56} &\cellcolor{mygray-bg}{}& \cellcolor{mygray-bg}{73.58} &  \cellcolor{mygray-bg}{72.83} &  \cellcolor{mygray-bg}{72.30} &\cellcolor{mygray-bg}{}& \cellcolor{mygray-bg}{65.81} &  \cellcolor{mygray-bg}{64.10} &  \cellcolor{mygray-bg}{62.23} \\
      \-14pt]
  \bottomrule

\end{tabular}
\cotronlcaptionvsapce
\vspace{0.2cm}
  \caption{
  Average accuracies (\%) across all phases using two state-of-the-art methods (LUCIR+AANets and POD+AANets~\cite{Liu2020AANets}) \emph{w/} and \emph{w/o} our RMM plugged in. The upper block is for recent CIL methods. For fair comparison, we re-implement these methods using our strict memory budget (see ``\textbf{Memory Budget}'' in Section~\ref{subsec_datasets}) based on the public code. 
The results of using another common budget setting and the detailed numbers (confidence intervals and last-phase accuracies) are provided in the supplementary materials.
}
  \label{table_sota}
  \cotronlvsapce
  \cotronlvsapcebottom
\end{table*}
} 
Table~\ref{table_sota} presents the results of two state-of-the-art methods (LUCIR+AANets and POD+AANets~\cite{Liu2020AANets}) \emph{w/} and \emph{w/o} our RMM plugged in, and some recent CIL work~\cite{douillard2020podnet,hou2019lucir,Li18LWF,liu2020mnemonics,rebuffi2017icarl}. Table~\ref{table_ablation} shows the ablation study in  settings. Figure~\ref{figure_action_plots} plots the changes of the average number of exemplars per old/new class for the incremental phases.

\myparagraph{Comparing to the State-of-the-Art.} From Table~\ref{table_sota}, we make the following observations. 1) Our RMM consistently improves the two top baselines LUCIR+AANets and POD+AANets~\cite{Liu2020AANets} in all settings. 
E.g., LUCIR-AANets \emph{w/} RMM and POD-AANets \emph{w/} RMM respectively get  and  average improvements on the ImageNet-Subset.
2) Our POD-AANets \emph{w/} RMM achieves the best performances. 
Interestingly, we find that our RMM can boost performance more when the number of phases is larger. For example, when =25, RMM improves POD-AANets by  and  on CIFAR-100 and ImageNet-Subset, respectively. These two numbers are  and  when =5. This indicates that the superiority of our RMM is more obvious in challenging settings (where the forgetting problem is more serious due to the more frequent model re-training through phases).

\setlength{\tabcolsep}{0.65mm}{
\renewcommand\arraystretch{1.2}
\begin{table}\small
  \centering
  \cotronlvsapcetop
  \begin{tabular}{llcccccccccccccccccccc}
  \toprule
\multicolumn{2}{l}{\multirow{4}{*}{Ablation Setting}} & \multicolumn{8}{c}{\emph{CIFAR-100}} && \multicolumn{8}{c}{\emph{ImagNet-Subset}} \\
   \cmidrule{3-10} \cmidrule{12-19} 
   &&  \multicolumn{2}{c}{=5} && \multicolumn{2}{c}{10}  && \multicolumn{2}{c}{25} && \multicolumn{2}{c}{5} && \multicolumn{2}{c}{10}  && \multicolumn{2}{c}{25}  \\
   \cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10} \cmidrule{12-13} \cmidrule{15-16} \cmidrule{18-19}
   &&Avg&Last&&Avg&Last&&Avg&Last&&Avg&Last&&Avg&Last&&Avg&Last\\
   \midrule
    1 & BaseRow & 66.61 & 57.81 && 64.61 & 55.70 && 62.63 & 52.53 && 77.36 & 70.02 && 75.83 & 68.97 && 72.18 & 63.89\\
    \midrule
    2 & One-level RL & 67.92 & 58.61 && 66.94 & 58.31 && 65.95 & 56.44 && 78.50 & 72.00 && 78.15 & 71.00 && 75.47 & 67.47\\
    \cellcolor{mygray-bg}{3} & \cellcolor{mygray-bg}{Two-level RL (Used)} & \cellcolor{mygray-bg}{68.86} & \cellcolor{mygray-bg}{59.00} &\cellcolor{mygray-bg}{ }& \cellcolor{mygray-bg}{67.61} & \cellcolor{mygray-bg}{59.03} &\cellcolor{mygray-bg}{ }& \cellcolor{mygray-bg}{66.21} & \cellcolor{mygray-bg}{56.50} &\cellcolor{mygray-bg}{ }& \cellcolor{mygray-bg}{79.52} & \cellcolor{mygray-bg}{73.80} &\cellcolor{mygray-bg}{ }& \cellcolor{mygray-bg}{78.47} & \cellcolor{mygray-bg}{71.40} &\cellcolor{mygray-bg}{ }& \cellcolor{mygray-bg}{76.54} & \cellcolor{mygray-bg}{68.84}\\ 
    \multicolumn{2}{c}{\emph{margin}} & +2.3 & +1.2 && +3 & +3.3 && +3.6 & +4 && +2.1 & +3.8 && +2.6 & +2.4 && +4.4 & +5 \\
    \midrule
    4 & Two-level RL (T.P.) & 68.62 & 59.40 && 67.22 & 58.20 && 65.82 & 56.20 && 78.81 & 72.42 && 77.68 & 70.77 && 75.29 & 68.81\\
    \multicolumn{2}{c}{\emph{margin}} & +2 & +1.6 && +2.6 & +2.5 && +3.2 & +3.7 && +1.5 & +2.4 && +1.9 & +1.8 && +3.1 & +4.9 \\
    \midrule
    5 & UpperBound RL& 70.00 & 61.12 && 68.36 & 60.00 && 66.56 & 56.74 && 80.01 & 74.31 && 78.95 & 71.97 && 76.99 & 69.14\\
    6 & CrossVal Fixed& 67.50 & 58.48 && 66.69 & 57.19 && 65.73 & 55.51 && 77.96 & 70.31 && 76.70 & 69.08 && 74.18 & 66.10 \\
  \bottomrule
\end{tabular}
\vspace{0.4cm}
  \caption{The evaluation results in the ablation study (). ``T.P.'' denotes our results using the \textbf{P}olicy functions \textbf{T}ransferred from another dataset. 
``Avg'', ``Last'', and ``Used''
 denote the average accuracy over all phases, the last-phase accuracy, and the results used as ours in Table~\ref{table_sota},  respectively.
  BaseRow is from the sota method POD-AANets~\cite{Liu2020AANets}.
  Row 2 is for learning Level-1 policy.
  Row 3 is for learning Level-1 and Level-2 policies in a hierarchical way. 
  Row 4 is for using Transferred Policies (from the other dataset in the table), when RL is costly or impossible on target CIL tasks. 
The bottom lines are two oracles: training the RL model on the target CIL task (Row 5) and using cross-validation to find the best fixed memory allocation between old and new classes (Row 6).
  }
  
  \label{table_ablation}
\cotronlvsapcebottom
  \vspace{-0.2cm}
\end{table}
} 
\myparagraph{Ablation Settings.} Table~\ref{table_ablation} shows the results of our ablation study. 
Row 1 is for the baseline method POD-AANets~\cite{Liu2020AANets}. 
Row 2 is for learning only Level-1 policy  (where each class gets an even split of the memory). 
Row 3 is for learning both Level-1 policy  and Level-2 policy  in our proposed hierarchical method, and its results are used in Table~\ref{table_sota} as ``ours''. 
Row 4 is for using Policy functions Transferred from another dataset (T.P.), which means on the target CIL dataset there is no training of RMM.
Here, for CIFAR-100, we use the policy functions learned on ImageNet-Subset, and vice versa.
On the last two rows, we show two oracle settings. 
Row 5 is the upper bound that assumes all past and future data are accessible during training RMM on the target CIL dataset. 
Row 6 is for using cross-validation (i.e., all past, future, and validation data are accessible) to find the best fixed memory split between old and new class data, e.g.,  =  is chosen and then used in all phases. The details of chosen split rates are given in the supplementary materials. 

\myparagraph{Ablation Results.} \textbf{\emph{Hierarchical}}: In Table~\ref{table_ablation}, when comparing Row 2 to Row 1, it is clear that leveraging reinforcement learning yields better results as it can derive adaptive memory allocation between old and new data.
Using class-specific memory management further increases the model performance (i.e., comparing Row 3 to Row 2), even though we divide the classes into only two groups. 
\textbf{\emph{T.P.}} (\textbf{T}ransferred \textbf{P}olicy functions): 
Comparing Row 4 to Row 3, we can see that using transferred policy functions (trained on another dataset) achieves comparable performance, and Row 4 does not require any reinforcement learning on the target CIL dataset.
\textbf{\emph{Oracle}}: Comparing Row 3 to Row 5, we see that learning RMM on pseudo CIL tasks is comparable to the upper bound case where all training and validation data are accessible, given the fact that the latter needs higher computational overhead and violates the standard CIL protocol.
Row 6 results are consistently lower than ours in Row 3, although cross-validation has access to all past, future, and validation data.

\myparagraph{Allocated Memory.}
Figure~\ref{figure_action_plots} shows the change of the average number of samples per class in three ablative settings. 
Solid and dashed lines represent old and new classes, respectively.
From the plots, we have two observations. 1) Learning RMM on the pseudo or target CIL tasks (green and orange lines), we can obtain similar memory management results (i.e., actions). 
This means the learned policy is transferrable in non-stationary continuous environments.
This matches the conclusion of continuous adaptation
in~\cite{ShedivatBBSM18}.
2) Using our RMM method achieved more balanced memory sizes between exemplars and new data.
For example, in the -st phase of the -phase setting, ``UpperBound RL'' and ``Two-level RL'' allocate around  samples for both exemplars and new data. 
While the baseline setting has  and  samples for them, respectively.
It thus addresses the data imbalance problem for CIL in a learnable way.

\begin{figure}
\newcommand{\newincludegraphics}[1]{\includegraphics[height=1.27in]{#1}}
\centering
\cotronlvsapcetop
\includegraphics[height=0.26in]{files/actions/legend.pdf}
\newincludegraphics{files/actions/cifar100_5phase_final_v2.pdf}
\hspace{1mm}
\newincludegraphics{files/actions/cifar100_25phase_final_v2.pdf}
\vspace{0.3cm}
\caption{
The memory allocated for ``Old'' and ``New'' across different phases on CIFAR-100. 
The second and fourth plots are enlarged versions of the first and third plots, respectively.
Solid and dashed lines denote old and new classes, respectively. 
The baseline is POD-AANets~\cite{Liu2020AANets}. ``Two-level RL'' and ``UpperBound RL'' correspond to Row 3 and Row 5 in Table~\ref{table_ablation}, respectively. 
}
\cotronlvsapcebottom
\label{figure_action_plots}
\end{figure}  \section{Conclusions}
\label{sec_conclusion}

We propose the reinforced memory management (RMM) method specially for tackling CIL tasks. 
The hierarchical reinforcement learning (RL) framework (two levels) in RMM is capable of making more adaptive memory allocation actions than using standard RL (one level).
Using the generated pseudo tasks in RMM solves the issue of data incompatibility between CIL and RL. Corresponding experimental results show that the policy trained on these pseudo tasks can be directly applied to target tasks without any computational overhead.
Our overall method of RMM is generic, and its trained policy (with or without using an in-domain dataset) can be easily incorporated into exemplar replaying-based CIL methods to boost  performance. \section*{Limitations and Societal Impact}

We analyze the limitations and potential negative societal impact in the following three aspects.
\begin{itemize}[leftmargin=*]
    \item 
 \textbf{\emph{Complexity.}}
   Training RMM takes an additional time cost. According to Algorithm~\ref{alg_rmm}, the cost is  times higher than the time used for the target CIL task. 
   However, the training of RMM policy is offline and can use a different dataset (see Table~\ref{table_ablation}) --- RMM pre-learns a robust policy from synthesized pseudo tasks and can be directly applied for memory management in real CIL tasks. The overhead of applying this policy is very little, e.g., \% and \% of the total training time respectively on CIFAR-100 and ImageNet (Subset and Full), taking POD+AANets as the baseline.
    \item 
    \textbf{\emph{Technical assumptions.}} We build the framework of RMM based on a series of technical assumptions, which might not directly hold for all real-world continual-learning applications. When applying our method to mission-critical problems, particular care is required when modeling the system.
    \item 
    \textbf{\emph{Privacy issues.}} Keeping the old class exemplars has the issue of data privacy. This calls for future research that explicitly forgets or mitigates the identifiable feature of the data.
\end{itemize}

\section*{Acknowledgments and Disclosure of Funding}

This research was supported by A*STAR under its AME YIRG Grant (Project No. A20E6c0101), Alibaba Innovative Research (AIR) program, and Max Planck Institute for Informatics. 
\clearpage
\bibliography{egbib.bib}{}
\bibliographystyle{plain}
\clearpage

\section*{Supplementary Materials}

The supplementary materials are available here:

\href{https://proceedings.neurips.cc/paper/2021/file/1cbcaa5abbb6b70f378a3a03d0c26386-Supplemental.pdf}{https://proceedings.neurips.cc/paper/2021/file/1cbcaa5abbb6b70f378a3a03d0c26386-Supplemental.pdf}

\end{document}