\section{Baselines}
We first introduce several \emph{self-training} based baselines that we have constructed for our joint person pose estimation and instance segmentation task by extending representative approaches. We extend pseudo-label \citep{lee2013pseudo, sohn2020simple}, data-distillation \citep{radosavovic2018data}, and ORPose \citep{srivastav2020human} as our baselines approaches. We refer to the extended version of pseudo-label, data-distillation, and ORPose as KM-PL, KM-DDS, and KM-ORPose, respectively. The KM as a prefix signifies that these approaches have been extended for the joint pose (keypoint) estimation and instance (mask) segmentation tasks. The baselines approaches are two-stage approaches where the first stage generates the pseudo labels on the unlabeled data. The second stage jointly trains the model using the pseudo and the ground truth labels. \emph{AdaptOR} on the other hand generates the pseudo labels on the unlabeled data \emph{on-the-fly} during the training. For a fair comparison, we train all the baseline methods with the same training strategy, data augmentation pipeline, and \emph{kmrcnn++} model. We give a brief overview of extended baseline approaches as follows.

\begin{table}[t!]
	\centering
	\caption{\small{{{\blue An overview of the source and the target domain datasets used in this work.}}}}
	\vspace{-2mm}
	\scalebox{0.85}{
		\begin{tabular}{l|c|c|c}
			\toprule
			\textbf{Dataset}                           & \textbf{type} & \textbf{\# images} & \textbf{\# instances} \\
			\textit{Source domain labeled dataset}     &               &                                            \\
			COCO                                       & train         & 57,000             & 150,000               \\
			\textit{COCO-val}                          & test          & 5,000              & 10,777                \\
			\hline\bottomrule
			\textit{Target domain unlabelled datasets} &               &                                            \\
			MVOR                                       & train         & 80,000             & -                     \\
			\textit{MVOR+}                             & test          & 2,196              & 5,091                 \\\hline
			TUM-OR                                     & train         & 1,500              & -                     \\
			\textit{TUM-OR-test}                       & test          & 2,400              & 11,611                \\
			\hline\bottomrule
		\end{tabular}
	}
	\label{table:datasets}
	\vspace{-1mm}
\end{table}

\begin{table*}[t]
	\centering
	\caption{\small{Results on the source domain \emph{COCO-val} dataset with 100\% labeled supervision. The \emph{kmrcnn+} model using GN \citep{wu2018group} and initialized using self-supervised MoCo-v2 approach \citep{chen2020simple,he2020momentum} perform equally well with the model using Cross-GPU BN \citep{peng2018megdet} but using less training time. The first row result for the \emph{kmrcnn} model is obtained from the paper \citep{he2017mask}. Rest of the results correspond to the models that we train. Inference is performed on a single-scale of 800 pixels following \citep{he2017mask}. Automatic mixed precision (AMP) uses single- and half-precision (32 bits and 16 bits) floating operation to speed up the training while trying to maintain single-precision (32 bits) model accuracy.}}
	\vspace{-2mm}
	\scalebox{1.0}{
		\begin{tabular}{l|c|c|c|c|cccc}
			\toprule
			\textbf{{Model}} & \textbf{Initialization} & \textbf{Normalization} & \textbf{AMP} & \textbf{$\approx$ Training-time} & $\mathit{AP_{person}^{bb}}$ & $\mathit{AP_{person}^{kp}}$ & $\mathit{AP_{person}^{mask}}$ \\
			\toprule
			\emph{kmrcnn}    & Supervised-Imagenet     & Frozen BN              & \xmark       & 32 hours                         & 52.0                        & 64.7                        & 45.1                          \\
			\emph{kmrcnn}    & Supervised-Imagenet     & Frozen BN              & \checkmark   & 16 hours                         & 56.4                        & 65.7                        & 49.1                          \\
			\emph{kmrcnn}    & MoCo-v2                 & Cross-GPU BN           & \checkmark   & 22 hours                         & 57.5                        & 66.6                        & 49.8                          \\
			\emph{kmrcnn+}   & MoCo-v2                 & GN                     & \checkmark   & 18 hours                         & 57.5                        & 66.2                        & 49.9                          \\
			\hline\bottomrule
		\end{tabular}
	}
	\label{table:source}
	\vspace{-1mm}
\end{table*}

\begin{table*}[t]
	\centering
	\caption{\small{Results for the baseline approaches and \emph{AdaptOR}. We see improvements in all three metrics on both the target domain datasets, especially on the low-resolution images making the proposed approach suitable for the deployment inside the privacy-sensitive OR environment. The \emph{source-only} results correspond to the model trained on the labeled source domain without any training on the target domain images. The KM-PL, KM-DDS, and KM-ORPose are strong baselines proposed in this work.}}
	\vspace{-2mm}
	\scalebox{0.92}{
		\begin{tabular}{l|cccc|cccc}
			\toprule
			\multirow{3}{*}{Methods} & \multicolumn{4}{c|}{\textbf{\emph{MVOR+}}} \Tstrut \Bstrut                                            & \multicolumn{4}{c}{\textbf{\emph{TUM-OR-test}}} \Tstrut \Bstrut                                                                                                                                                                     \\
			\cline{2-9}
			                         & \textbf{~1x~}                                                                                         & \textbf{~8x~}                                                   & \textbf{~10x~}          & \textbf{~12x~}          & \textbf{~1x~}           & \textbf{~8x~}           & \textbf{~10x~}          & \textbf{~12x~}   \Tstrut        \\
			\cline{2-9}
			                         & \multicolumn{8}{c}{$\mathit{AP_{person}^{bb}}$ (mean$\pm$std)}   \Tstrut \Bstrut                                                                                                                                                                                                                                                   \\
			\hline
			\emph{source-only}       & 56.61$\pm$0.34                                                                                        & 40.42$\pm$2.17                                                  & 34.87$\pm$2.47          & 29.61$\pm$2.69          & 68.61$\pm$1.54          & 41.84$\pm$2.33          & 31.08$\pm$2.83          & 24.00$\pm$2.90 \Tstrut          \\\hline
			KM-PL                    & 60.21$\pm$0.51                                                                                        & 57.14$\pm$0.34                                                  & 55.88$\pm$0.39          & 54.26$\pm$0.41          & 72.28$\pm$1.51          & 65.44$\pm$1.45          & 62.84$\pm$1.02          & 62.42$\pm$1.55 \Tstrut          \\
			KM-DDS                   & 60.79$\pm$0.47                                                                                        & 57.88$\pm$0.39                                                  & 56.74$\pm$0.37          & 55.12$\pm$0.45          & 72.51$\pm$1.45          & 65.98$\pm$1.18          & 63.87$\pm$0.99          & 62.68$\pm$1.32 \Tstrut          \\
			KM-ORPose                & 58.88$\pm$0.69                                                                                        & 55.14$\pm$0.56                                                  & 53.81$\pm$0.52          & 51.96$\pm$0.47          & 69.73$\pm$1.22          & 63.46$\pm$0.93          & 60.71$\pm$0.73          & 60.14$\pm$0.94 \Tstrut          \\ \hline
			\emph{\textbf{AdaptOR}}  & \textbf{61.41$\pm$0.40}                                                                               & \textbf{59.48$\pm$0.35}                                         & \textbf{58.55$\pm$0.36} & \textbf{57.33$\pm$0.43} & \textbf{72.75$\pm$0.88} & \textbf{67.33$\pm$0.78} & \textbf{65.53$\pm$0.57} & \textbf{65.65$\pm$0.66} \Tstrut
			\\\hline
			                         & \multicolumn{8}{c}{$\mathit{AP_{person}^{kp}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                                                                                                                                                                                     \\
			\hline
			\emph{source-only}       & 50.55$\pm$0.39                                                                                        & 23.99$\pm$2.25                                                  & 16.86$\pm$2.16          & 11.31$\pm$1.91          & 65.60$\pm$4.55          & 27.21$\pm$1.49          & 19.41$\pm$1.86          & 13.18$\pm$1.81 \Tstrut          \\\hline
			KM-PL                    & 58.72$\pm$0.44                                                                                        & 55.19$\pm$0.43                                                  & 52.81$\pm$0.55          & 49.53$\pm$0.46          & 77.49$\pm$1.87          & 67.57$\pm$1.03          & 63.46$\pm$0.89          & 58.24$\pm$1.05 \Tstrut          \\
			KM-DDS                   & 59.83$\pm$0.40                                                                                        & 55.60$\pm$0.49                                                  & 53.16$\pm$0.48          & 50.02$\pm$0.46          & 78.39$\pm$1.76          & 69.24$\pm$1.07          & 65.29$\pm$0.93          & 60.56$\pm$1.21 \Tstrut          \\
			KM-ORPose                & \textbf{62.50$\pm$0.53}                                                                               & 57.18$\pm$0.60                                                  & 54.59$\pm$0.59          & 51.24$\pm$0.47          & \textbf{80.49$\pm$1.74} & 69.90$\pm$1.03          & 65.64$\pm$0.94          & 60.67$\pm$0.73 \Tstrut          \\ \hline
			\emph{\textbf{AdaptOR}}  & 60.86$\pm$0.38                                                                                        & \textbf{57.35$\pm$0.61}                                         & \textbf{55.42$\pm$0.66} & \textbf{52.60$\pm$0.60} & 77.84$\pm$1.24          & \textbf{70.65$\pm$1.04} & \textbf{67.36$\pm$0.96} & \textbf{63.27$\pm$1.21} \Tstrut \\
			\hline
			                         & \multicolumn{8}{c}{$\mathit{AP_{person}^{bb\ (from\ mask)}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                                                                                                                                                                       \\
			\hline
			\emph{source-only}       & 54.95$\pm$0.37                                                                                        & 37.98$\pm$2.21                                                  & 32.58$\pm$2.37          & 27.56$\pm$2.48          & 69.33$\pm$1.46          & 40.38$\pm$2.30          & 30.11$\pm$2.79          & 22.97$\pm$2.93  \Tstrut         \\\hline
			KM-PL                    & 56.50$\pm$0.60                                                                                        & 54.06$\pm$0.44                                                  & 52.90$\pm$0.48          & 51.33$\pm$0.46          & 71.93$\pm$1.34          & 65.43$\pm$1.46          & 63.16$\pm$0.89          & 62.67$\pm$1.11 \Tstrut          \\
			KM-DDS                   & 57.12$\pm$0.47                                                                                        & 54.76$\pm$0.50                                                  & 53.78$\pm$0.49          & 52.06$\pm$0.67          & 71.99$\pm$1.18          & 65.96$\pm$1.07          & 64.02$\pm$0.70          & 63.01$\pm$1.02 \Tstrut          \\
			KM-ORPose                & 55.46$\pm$0.76                                                                                        & 52.37$\pm$0.62                                                  & 51.23$\pm$0.55          & 49.34$\pm$0.46          & 68.05$\pm$1.13          & 61.15$\pm$1.09          & 58.53$\pm$0.86          & 57.89$\pm$1.00 \Tstrut          \\\hline
			\emph{\textbf{AdaptOR}}  & \textbf{59.34$\pm$0.40}                                                                               & \textbf{57.44$\pm$0.42}                                         & \textbf{56.62$\pm$0.41} & \textbf{55.39$\pm$0.51} & \textbf{72.13$\pm$0.91} & \textbf{66.55$\pm$0.80} & \textbf{65.04$\pm$0.52} & \textbf{65.15$\pm$0.65} \Tstrut \\
			\hline
			\bottomrule
		\end{tabular}
	}
	\label{table:source-target-da}
	\vspace{-1mm}
\end{table*}

\begin{figure*}[htb!]
	\includegraphics[clip, trim=0.0cm 1.0cm 0.0cm 0.0cm, width=1.01\linewidth]{figures/result_graphs}
	\caption{\small{Bounding box detection \emph{$AP_{person}^{bb}$}, pose estimation \emph{$AP_{person}^{kp}$}, and instance segmentation \emph{$AP_{person}^{bb}$ (from mask)} results for unsupervised domain adaptation experiments on four downsampling scales (1x, 8x, 10x, and 12x) and nine target resolution (480, 520, 560, 600, 640, 680, 720, 760, and 800) corresponding to the shorter side of the image for \emph{MVOR+} and \emph{TUM-OR-test} datasets. We see an increase in the accuracy with the increase in target resolution for the \emph{TUM-OR-test} dataset.  We also observe an increase in accuracy for the \emph{MVOR+} dataset but only up to around 680 pixels.}}
	\label{fig:results_graphs}
\end{figure*}
\begin{table*}[t!]
	\centering
	\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1.5pt}\setlength\tabcolsep{0.5pt}\scalebox{0.95}{
		\begin{tabular}{cccc}
			\centering
			                                                                                                                & 1x                                                                                    & 8x                                                                                    & 12x                                                                                    \\
			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Source-only}}}             & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/mvor_d1c1_000016_1x.png}}     & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/mvor_d1c1_000016_8x.png}}     & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/mvor_d1c1_000016_12x.png}}     \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Pseudo-label}}}            & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/mvor_d1c1_000016_1x.png}} & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/mvor_d1c1_000016_8x.png}} & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/mvor_d1c1_000016_12x.png}} \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Data-distillation}}}       & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/mvor_d1c1_000016_1x.png}}   & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/mvor_d1c1_000016_8x.png}}   & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/mvor_d1c1_000016_12x.png}}   \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{ORPose}}}                  & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/mvor_d1c1_000016_1x.png}}       & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/mvor_d1c1_000016_8x.png}}       & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/mvor_d1c1_000016_12x.png}}       \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{\emph{\textbf{AdaptOR}}}}} & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/mvor_d1c1_000016_1x.png}}      & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/mvor_d1c1_000016_8x.png}}      & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/mvor_d1c1_000016_12x.png}}      \\
		\end{tabular}
	}
	\captionof{figure}[]{\small{Qualitative results for bounding box detection, pose estimation, and instance segmentation on a sample \emph{MVOR+} image for the baseline approaches and \emph{AdaptOR}. Results are displayed on the for original image and corresponding downsampled images with downsampling factor 8 and 12. The red arrows show either missed detections or localization errors. Localization errors are noticeable on the low-resolution images}}
	\label{tab:table-qual1}
\end{table*}

\begin{table*}[t!]
	\centering
	\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1.5pt}\setlength\tabcolsep{0.5pt}\scalebox{0.95}{
		\begin{tabular}{cccc}
			\centering
			                                                                                                                & 1x                                                                                & 8x                                                                                & 12x                                                                                \\
			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Source-only}}}             & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/c6_img000057_1x.png}}     & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/c6_img000057_8x.png}}     & \fbox{\includegraphics[width=2.3in]{figures/or/baseline/c6_img000057_12x.png}}     \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Pseudo-label}}}            & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/c6_img000057_1x.png}} & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/c6_img000057_8x.png}} & \fbox{\includegraphics[width=2.3in]{figures/or/pseudo-label/c6_img000057_12x.png}} \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{Data-distillation}}}       & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/c6_img000057_1x.png}}   & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/c6_img000057_8x.png}}   & \fbox{\includegraphics[width=2.3in]{figures/or/data-distl/c6_img000057_12x.png}}   \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{ORPose}}}                  & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/c6_img000057_1x.png}}       & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/c6_img000057_8x.png}}       & \fbox{\includegraphics[width=2.3in]{figures/or/orpose/c6_img000057_12x.png}}       \\

			\raisebox{5.0\normalbaselineskip}[0pt][0pt]{\parbox[b]{3mm}{\rotatebox[origin=c]{90}{\emph{\textbf{AdaptOR}}}}} & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/c6_img000057_1x.png}}      & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/c6_img000057_8x.png}}      & \fbox{\includegraphics[width=2.3in]{figures/or/adaptor/c6_img000057_12x.png}}      \\
		\end{tabular}
	}
	\captionof{figure}[]{\small{{\blue Qualitative results for bounding box detection, pose estimation, and instance segmentation on a sample \emph{TUM-OR-test} image for the baseline approaches and \emph{AdaptOR}.}}}
	\label{tab:table-qual-tum-or}
\end{table*}

\subsection{KM-PL} We modify the pseudo-labeling \citep{lee2013pseudo} approach to generate the pseudo labels on a single-scale image on the unlabeled target domain data. The authors in \citep{sohn2020simple} recently use a similar approach with advanced data augmentations for the object detection task.

\subsection{KM-DDS} KM-DDS \citep{radosavovic2018data} is also a pseudo-labeling approach, but instead of generating pseudo labels on a single scale, it aggregates the labels from multiple scales with random horizontal flipping transformations. Authors use the approach for multi-class object detection and human pose estimation. We further extend it to generate pseudo labels for the masks. Similar to the authors, we use scaling and random horizontal flipping transformations on nine predefined image sizes ranging from 400 to 1200 pixels with a step size of 100. Here, the image size corresponds to the shorter side of the image; the size of the longer side of the image is computed by maintaining the same aspect ratio.
\subsection{KM-ORPose} KM-ORPose \citep{srivastav2020human} uses the \emph{teacher-student} learning paradigm for the domain adaptation in the OR for joint person detection and 2D/3D human pose estimation. It combines the knowledge-distillation \citep{hinton2015distilling, zhang2019fast} - using complex three-stage models - along with data-distillation \citep{radosavovic2018data} to generate accurate pseudo labels. In the first stage, it uses cascade-mask-rcnn \citep{cai2019cascade} with the deformable convolution \citep{dai2017deformable} based resnext-152 backbone \citep{xie2017aggregated} to generate the person bounding boxes. We use the same network to get the pseudo masks as well. In the second stage, it uses the HRNet-w48 model (384x288 input size) \citep{SunXLW19} to get the pseudo labels for the poses. KM-ORPose is a strong baseline as it uses a complex multi-stage teacher model to generate accurate pseudo labels for the training.


\section{Experiments and results}
\subsection{Datasets and evaluation metrics}
We use COCO \citep{lin2014microsoft} as source domain dataset. It contains 57k images, and the ground truth labels have 150k instances of person bounding box, segmentation mask, and 17 body keypoints. The test dataset of COCO, called \emph{COCO-val}, contain 5k images with 10777 person instances.

We train and evaluate our approach on the two target domain OR datasets: MVOR \citep{srivastav2018mvor,srivastav2020human} and TUM-OR \citep{belagiannis2016parsing}. MVOR contains data captured during real surgical interventions, whereas TUM-OR contains OR images from simulated surgical activities. The unlabelled training datasets of MVOR and TUM-OR contain 80k and 1.5k images, respectively. The testing dataset of MVOR, called \emph{MVOR+}, and TUM-OR, called \emph{TUM-OR-test}, contain 2196 images with 5091 person instances and 2400 images with 11611 person instances, respectively. The \emph{MVOR+} dataset is extended from the public \emph{MVOR} dataset \citep{srivastav2018mvor,srivastav2020human}. Before the extension, it consists of 4699 person bounding boxes, 2926 2D upper body poses with 10 keypoints, (and 1061 3D upper body poses). The fully-annotated extension called \emph{MVOR+} consists of 5091 person bounding boxes, and 5091 body poses with 17 keypoints in the COCO format. The original \emph{TUM-OR-test} consists of only the upper-body bounding boxes with six common COCO keypoints. These annotations are not suitable for our evaluation purpose; hence we annotate the \emph{TUM-OR-test} using a semi-automatic approach. We first use a state-of-the-art person detector \citep{cai2019cascade} to get the person bounding boxes and manually correct all the bounding boxes. We then run the HRNet model \citep{SunXLW19} on all the corrected bounding boxes to get the poses. The predicted poses are corrected using the keypoint annotation tool\footnote{\url{https://github.com/visipedia/annotation_tools}}. 
{\blue An overview of the datasets used in this work is shown in the Table \ref{table:datasets}}.

The image sizes of \emph{MVOR+} and \emph{TUM-OR-test} datasets are 640x480 and 1280x720, respectively. We also conduct experiments with downsampled images using the scaling factors 8x, 10x, and 12x, yielding images of size 80x64, 64x48, and 53x40 for the \emph{MVOR+} dataset and 160x90, 128x72, and 107x60 for the \emph{TUM-OR-test} dataset.

We use the Average Precision \emph{$AP_{0.5:0.95}$} metric from COCO \citep{lin2014microsoft} for the evaluation. The bounding box evaluation metric \emph{$AP_{person}^{bb}$} uses intersection over union (IoU) over boxes, and the pose estimation evaluation metric \emph{$AP_{person}^{kp}$} uses the object keypoint similarity (OKS) over person keypoints to compare the ground-truth and the predictions. Both \emph{MVOR+} and \emph{TUM-OR-test} do not have a ground-truth for the person instance segmentation masks. Hence, we evaluate the mask predictions by computing a tight bounding box on the prediction masks and comparing them with ground-truth bounding boxes called \emph{$AP_{person}^{bb}$ (from mask)}. We also show extensive qualitative results for the instance segmentation and pose estimation in the supplementary video. The instance segmentation on the source domain COCO images is evaluated using the \emph{$AP_{person}^{mask}$} which uses IoU over masks to compare the ground-truth and the predictions.

\subsection{Experiments}
\subsubsection{Source domain fully supervised training} \label{sdfs}
The models are trained on the source domain COCO dataset in a fully supervised manner for three experiments: supervised ImageNet initialization with Frozen batch normalization (BN) \citep{he2016deep}, self-supervised MOCO-v2 initialization \citep{chen2020simple,he2020momentum} with Cross-GPU BN \citep{peng2018megdet}, and self-supervised MOCO-v2 initialization \citep{chen2020simple,he2020momentum} with group normalization (GN) \citep{wu2018group}. The goal of these experiments is to obtain one suitable \emph{source-only} baseline as an initialization model for the UDA experiments. The last model with self-supervised MOCO-v2 initialization and GN, called \emph{kmrcnn+}, is further used in the SSL experiments and extended in UDA experiments.
\subsubsection{AdaptOR: unsupervised domain adaptation (UDA) on target domains} \label{stduda}
The UDA experiments on source domain COCO datasets and target domains MVOR and TUM-OR datasets are conducted to train the \emph{kmrcnn++} model for eight sets of experiments. The first four experiments are for the target domain MVOR and the last four for TUM-OR. For each target domain, the first three experiments train the \emph{kmrcnn++} model on three constructed baseline methods: KM-PL, KM-DDS, and KM-ORPose, respectively, and the fourth experiment trains the \emph{kmrcnn++} model on our \emph{AdaptOR} method. Eleven ablation experiments are conducted with the source domain COCO dataset and the target domain MVOR dataset: the first experiment evaluates the contribution of disentangled feature normalization, the next five different types of strong augmentations, and the {\blue last five different unsupervised loss weights loss values $\lambda$}.
\subsubsection{AdaptOR-SSL: semi-supervised learning (SSL) on source-domain} \label{sdssl}
The SSL experiments on the source domain COCO dataset are conducted for four experiments where we train the \emph{kmrcnn+} model using 1\%, 2\%, 5\%, and 10\% of COCO dataset as the labeled set and the rest of the data as the unlabeled set. The \emph{kmrcnn+} model uses the regular GN layers instead of disentangled feature normalization layers. We use the same labeled and unlabeled images and training iterations as used by Unbiased-teacher \citep{liu2021unbiased}, the current state-of-the-art in SSL for object detection. 

{\blue \subsubsection{Domain adaptation on AdaptOR-SSL model} \label{sdudaonssl}
\emph{AdaptOR} assumes it has access to all the source-domain labels in the previous experiments. We conduct a final experiment to see how  \emph{AdaptOR} performs when initialized from a source-domain model trained with less source domain data. We take a \emph{AdaptOR-SSL} model trained using 10\% labeled and 90\% unlabeled source domain data and use it to initialize \emph{AdaptOR}.}

\subsection{Implementation details}
The source domain fully supervised training experiments, explained in section \ref{sdfs}, are conducted with batch size 16 and learning rate 0.02 for 270k iterations with multi-step (210k and 250k) learning rate decay on eight V100 GPUs. 

The \emph{AdaptOR} and the \emph{AdaptOR-on-AdaptOR-SSL} experiments explained in section \ref{stduda}, \ref{sdudaonssl}, respectively, are conducted on four V100 GPUs with a labeled and unlabeled batch size of eight (four images/GPU) and a learning rate of 0.001. The experiments are conducted for 65k iterations for the MVOR dataset and 10k iterations for the TUM-OR dataset. Finally, the \emph{AdaptOR-SSL} experiments explained in \ref{sdssl} are conducted on four V100 GPUs following the linear learning rate scaling rule \citep{goyal2017accurate}.

The spatial augmentations from rand-augment \citep{cubuk2020randaugment} consist of ``inversion'', ``auto-contrast'', ``posterize'', ``equalize'', ``solarize'', ``contrast-variation'', ``color-jittering'', ``sharpness-variations'', and  ``brightness-variations'' {\blue implemented using a python image library\footnote{\url{https://github.com/jizongFox/pytorch-randaugment}}}. 
The random cut-out \citep{devries2017improved} augmentation places square boxes of random sizes chosen between 40 to 80 pixels at random locations in the image. The random-resize operation for the \emph{weakly} and \emph{strongly} augmented images resize the image to a size randomly sampled from 600 to 800 pixels for SSL experiments following \citep{he2017mask}. For the UDA experiments, we choose the random-resize range from 480 to 800 pixels to provide more size variability in the data augmentation and match the original size of the MVOR dataset (640x480). The image size corresponds to the shorter side of the image.

We use a detectron2 framework \citep{wu2019detectron2} to run all the experiments with automatic mixed precision (AMP) \citep{micikevicius2017mixed}. We use bounding box threshold $\delta_{bbox} = 0.7$, keypoint threshold $\delta_{kp} = 0.1$, mask threshold $\delta_{mask} = 0.5$, EMA decay rate $\alpha = 0.9996$, unsupervised loss weight $\lambda = 3.0$ for \emph{AdaptOR}, and $\lambda = 2.0$ for \emph{AdaptOR-SSL}.

\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.8\textwidth}
		\centering
		\resizebox{\linewidth}{!}{
			\begin{tabular}{ccc}
				\raisebox{15mm}{\multirow{2}{*}{\includegraphics[width=.4\linewidth]{figures/results_analysis/baseline/localization_errors/keypoints_breakdown/overall_keypoint_errors.pdf}}}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/baseline/localization_errors/keypoints_breakdown/Miss_kpt_breakdown.pdf}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/baseline/localization_errors/keypoints_breakdown/Jitter_kpt_breakdown.pdf}    \\
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/baseline/localization_errors/keypoints_breakdown/Inversion_kpt_breakdown.pdf}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/baseline/localization_errors/keypoints_breakdown/Swap_kpt_breakdown.pdf}      \\
			\end{tabular}
		}
		\caption{{\small\emph{source-only}}}
	\end{subfigure}
	\begin{subfigure}[t]{0.8\textwidth}
		\centering
		\resizebox{\linewidth}{!}{
			\begin{tabular}{ccc}
				\raisebox{15mm}{\multirow{2}{*}{\includegraphics[width=.4\linewidth]{figures/results_analysis/adaptor/localization_errors/keypoints_breakdown/overall_keypoint_errors.pdf}}}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/adaptor/localization_errors/keypoints_breakdown/Miss_kpt_breakdown.pdf}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/adaptor/localization_errors/keypoints_breakdown/Jitter_kpt_breakdown.pdf}    \\
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/adaptor/localization_errors/keypoints_breakdown/Inversion_kpt_breakdown.pdf}
					& \includegraphics[width=.99\linewidth]{figures/results_analysis/adaptor/localization_errors/keypoints_breakdown/Swap_kpt_breakdown.pdf}      \\
			\end{tabular}
		}
		\caption{{\small\emph{AdaptOR}}}
	\end{subfigure}
	\caption{\small{{\blue Localization errors at individual keypoint level for the pose estimation task before and after the domain adaptation. ``Jitter'', ``Inversion'', ``Swap'', and ``Miss'' are various localization errors defined in \citep{ruggero2017benchmarking}: ``Jitter'' error is the error in predicted keypoint w.r.t close proximity of the correct ground truth, ``Inversion'' error is due to the right-left swap of the body part, ``Swap'' is the error in assigning predicted keypoint to a wrong person, and ``Miss'' error is due to completely missing the correct ground truth location. We use the author's code repository \citep{ruggero2017benchmarking}\protect\footnotemark ~ for plotting the results.}}}	
	
	\label{figure:result-analysis}
\end{figure*}
\footnotetext{\url{https://github.com/matteorr/coco-analyze}}

\begin{figure*}[t!]
\centering
	\includegraphics[width=.99\linewidth]{figures/dfn_analysis/figure_dfn_analysis.pdf}
	\caption{\small{{\blue t-sne feature visualization \citep{van2008visualizing} of the \emph{layer5} resnet features of the backbone model on random 200 images of the source and the target domain test datasets. The \emph{source-only} model uses only the \emph{GN(S)} layers whereas the \emph{AdapOR} uses separate \emph{GN(S)} and \emph{GN(T)} layers for the source and the target domain images, respectively. The \emph{AdapOR} model appropriately segregates the source and the target domain image features from the two domains helping in improving the domain adaptation for the downstream heads.}}}
	\label{figure:dfn-comp}
\end{figure*}

\section{Results}
\subsection{Source domain fully supervised training}
Table \ref{table:source} shows the results of \emph{kmrcnn} and \emph{kmrcnn+} models trained on the source domain COCO dataset. The \emph{kmrcnn} trained using self-supervised MoCo-v2 weights with Cross-GPU BN \citep{peng2018megdet} obtains improvement of approximately 1\% in all the three metrics compared to supervised ImageNet weights using frozen BN. The \emph{kmrcnn+} using GN performs equally well but with less training time. The \emph{kmrcnn+} model is therefore further used in the SSL experiments and extended in UDA experiments.

\subsection{AdaptOR: unsupervised domain adaptation (UDA) on target domains}
Table \ref{table:source-target-da} and figure \ref{fig:results_graphs} show the result of our unsupervised domain adaptation experiments using \emph{AdaptOR}. The first and the second half in table \ref{table:source-target-da} show the results for \emph{MVOR+} and \emph{TUM-OR-test} datasets, respectively. We evaluate the models at four downsampling scales (1x, 8x, 10x, and 12x). As the model is trained on unlabeled image sizes from 480 to 800 pixels (shorter side), we evaluate the model on nine target resolutions (480, 520, 560, 600, 640, 680, 720, 760, and 800), i.e., for a given downsampling scale, we down-sample the image with the scale and up-sample it to the given target resolution. The target resolution also corresponds to the shorter size of the image to maintain the aspect ratio. We use bilinear interpolation for the downsampling and up-sampling. The results in Table \ref{table:source-target-da} show the mean and standard deviation of the results computed on all the target resolutions for bounding box detection \emph{$AP_{person}^{bb}$}, pose estimation \emph{$AP_{person}^{kp}$}, and instance segmentation \emph{$AP_{person}^{bb}$ (from mask)} on a given downsampling scale.

The first row shows the \emph{source-only} results for the \emph{kmrcnn+} model trained on source domain images and evaluated on the target domain. The significant decrease in the low-resolution results of the  \emph{kmrcnn+} is likely because such heavily downsampled images are not present in the source domain. The improved result for the KM-DDS approach compared to KM-PL shows the effects of generating pseudo labels using the multi-scale and flipping transformation. The bounding box and segmentation results for the KM-ORPose are slightly worse than the KM-PL and KM-DDS. It may be because KM-ORPose uses a state-of-the-art object detector trained on all the 80 class categories from COCO whereas, KM-PL and KM-DDS use the model trained specifically for the person class. The \emph{AdaptOR} performs significantly better compared to baseline approaches, especially on the low-resolution at different target resolutions, see figure \ref{fig:results_graphs}, suggesting the potential of our approach for low-resolution images in the privacy-sensitive OR environment. We observe a slight decrease in the accuracy for \emph{$AP_{person}^{kp}$} metric on original size, likely due to the use of the multi-stage complex teacher model to generate the pseudo poses. Instead, our approach improves the given model in a model agnostic way without relying on an external teacher model to generate the pseudo labels. We also plot the results at individual scales in the figure \ref{fig:results_graphs}. The figure \ref{tab:table-qual1} and \ref{tab:table-qual-tum-or} show qualitative results comparing our approach with the baseline approaches. 

{\blue We further analyze the impact of different localization errors at the keypoint level before and after the domain adaptation using an approach described in \citep{ruggero2017benchmarking}. As shown in Fig. ~\ref{figure:result-analysis}, after domain adaptation, our approach correctly detects more keypoints while reducing the impact of different localization errors.} Additional qualitative results for the UDA experiments on \emph{MVOR+} and \emph{TUM-OR-test} are presented in the supplementary video\footnote{\url{https://youtu.be/gqwPu9-nfGs}}


\subsubsection{Ablation experiments}
\emph{6.2.1.1 Disentangled feature normalization}\\
{\blue Fig.~\ref{figure:dfn-comp} shows t-sne feature visualization \citep{van2008visualizing} of the \emph{layer5} resnet features of the backbone model illustrating the appropriate segregation of the features after the domain adaptation}. We also conduct experiments to quantify the use of two separate GN layers, \emph{GN(S)} and  \emph{GN(T)}, in the feature extractor for domain-specific normalization compared to either a single GN layer or a single frozen BN layer. The first row in Table \ref{table:splitgn-design} shows the results for the \emph{krcnn} \citep{he2017mask,wu2019detectron2kpn} model using frozen BN \citep{he2016deep} layers for joint bounding box detection and pose estimation. We take the source domain COCO trained weights from detectron2 \citep{wu2019detectron2} library and train it on the MVOR dataset. The second row shows the results for the \emph{kmrcnn+} model using a single GN layer for both domains. We also evaluate \emph{kmrcnn++} where we use the GN layers corresponding to source domain \emph{GN(S)} to evaluate on the target domain (\emph{kmrcnn++ GN(S)}). We obtain significantly better results by using our design of the two separate GN layers for feature normalization.\\
\begin{table}[t!]
	\centering
	\caption{\small{Ablation study comparing the \emph{kmrcnn++} model using the two GN layer-based design for feature normalization with the \emph{kmrcnn+} that uses only a single layer. We also compare it with a \emph{krcnn} model using single frozen BN, and \emph{kmrcnn++ GN(S)}, the same \emph{kmrcnn++} model but using the GN layers corresponding to the source domain.}}
	\vspace{-2mm}
	\scalebox{0.78}{
		\begin{tabular}{l|cccc}
			\toprule
			\multirow{3}{*}{Models} & \multicolumn{4}{c}{\textbf{\emph{MVOR+}}} \Tstrut \Bstrut                                                                                                                                               \\
			\cline{2-5}
			                        & \textbf{~1x~}                                                                                         & \textbf{~8x~}           & \textbf{~10x~}          & \textbf{~12x~}  \Tstrut                     \\
			\cline{2-5}
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb}}$ (mean$\pm$std)}   \Tstrut \Bstrut                                                                                                               \\
			\hline
			\emph{krcnn}            & 59.00$\pm$0.35                                                                                        & 56.78$\pm$0.37          & 55.87$\pm$0.34          & 54.43$\pm$0.36 \Tstrut                      \\
			\emph{kmrcnn+}          & 60.71$\pm$0.16                                                                                        & 58.75$\pm$0.33          & 58.03$\pm$0.31          & 56.97$\pm$0.39 \Tstrut                      \\
			\emph{kmrcnn++ GN(S)}   & 59.64$\pm$0.46                                                                                        & 55.86$\pm$0.48          & 53.84$\pm$0.64          & 51.61$\pm$0.74 \Tstrut                      \\\hline
			\emph{kmrcnn++}         & \textbf{61.41$\pm$0.40}                                                                               & \textbf{59.48$\pm$0.35} & \textbf{58.55$\pm$0.36} & \textbf{57.33$\pm$0.43} \Tstrut
			\\\hline
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{kp}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                                                 \\
			\hline
			\emph{krcnn}            & 57.96$\pm$0.32                                                                                        & 55.48$\pm$0.62          & 53.34$\pm$0.55          & 50.50$\pm$0.44  \Tstrut                     \\
			\emph{kmrcnn+}          & 47.15$\pm$0.30                                                                                        & 45.27$\pm$0.44          & 43.89$\pm$0.44          & 42.01$\pm$0.44                      \Tstrut \\
			\emph{kmrcnn++ GN(S)}   & 58.64$\pm$0.40                                                                                        & 52.37$\pm$0.41          & 49.51$\pm$0.46          & 46.08$\pm$0.51 \Tstrut                      \\\hline
			\emph{kmrcnn++}         & \textbf{60.86$\pm$0.38}                                                                               & \textbf{57.35$\pm$0.61} & \textbf{55.42$\pm$0.66} & \textbf{52.60$\pm$0.60} \Tstrut             \\
			\hline
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb\ (from\ mask)}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                                   \\
			\hline
			\emph{krcnn}            & -                                                                                                     & -                       & -                       & - \Tstrut                                   \\
			\emph{kmrcnn+}          & 55.18$\pm$0.25                                                                                        & 53.85$\pm$0.42          & 53.28$\pm$0.5           & 52.44$\pm$0.58 \Tstrut                      \\
			\emph{kmrcnn++ GN(S)}   & 58.22$\pm$0.46                                                                                        & 54.77$\pm$0.62          & 53.06$\pm$0.67          & 50.70$\pm$0.72 \Tstrut                      \\\hline
			\emph{kmrcnn++}         & \textbf{59.34$\pm$0.40}                                                                               & \textbf{57.44$\pm$0.42} & \textbf{56.62$\pm$0.41} & \textbf{55.39$\pm$0.51} \Tstrut             \\
			\hline
		\end{tabular}
	}
	\label{table:splitgn-design}
	\vspace{-1mm}
\end{table}
\begin{table}[t!]
	\centering
	\caption{\small{Ablation study quantifying the different augmentations on the strongly transformed image used by the student model for the training. Here, sr: \emph{strong-resize}, ra: random-augment, rc: random-cut, and geom: geometric transformations consisting of random-resize and random-flip.}}
	\vspace{-2mm}
	\scalebox{0.67}{
		\begin{tabular}{lccc|cccc}
			\toprule
			           &            &            &            & \multicolumn{4}{c}{\textbf{\emph{MVOR+}}} \Tstrut \Bstrut \Tstrut \Bstrut                                                                                                                    \\
			\cline{5-8}
			sr         & ra         & rc         & geom       & \textbf{~1x~}                                                                                         & \textbf{~8x~}           & \textbf{~10x~}          & \textbf{~12x~}  \Tstrut          \\
			\cline{5-8}
			           &            &            &            & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb}}$ (mean$\pm$std)}   \Tstrut \Bstrut                                                                                                    \\
			\hline
			           &            & Baseline   &            & 56.61$\pm$0.34                                                                                        & 40.42$\pm$2.17          & 34.87$\pm$2.47          & 29.61$\pm$2.69 \Tstrut           \\\hline
			\xmark     & \xmark     & \xmark     & \xmark     & 58.06$\pm$0.28                                                                                        & 45.14$\pm$1.70          & 40.19$\pm$2.09          & 35.45$\pm$2.28 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \xmark     & 58.34$\pm$0.34                                                                                        & 58.03$\pm$0.31          & 57.25$\pm$0.33          & 55.97$\pm$0.33 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \checkmark & 59.64$\pm$0.34                                                                                        & 58.74$\pm$0.30          & 58.01$\pm$0.36          & 56.80$\pm$0.32 \Tstrut           \\
			\checkmark & \checkmark & \xmark     & \xmark     & 58.43$\pm$0.31                                                                                        & 57.72$\pm$0.31          & 56.99$\pm$0.33          & 55.58$\pm$0.29 \Tstrut           \\
			\checkmark & \checkmark & \checkmark & \xmark     & 59.79$\pm$0.54                                                                                        & 58.38$\pm$0.44          & 57.48$\pm$0.45          & 56.21$\pm$0.46 \Tstrut           \\\hline
			\checkmark & \checkmark & \checkmark & \checkmark & \textbf{61.41$\pm$0.40}                                                                               & \textbf{59.48$\pm$0.35} & \textbf{58.55$\pm$0.36} & \textbf{57.33$\pm$0.43}  \Tstrut
			\\\hline
			           &            &            &            & \multicolumn{4}{c}{$\mathit{AP_{person}^{kp}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                                      \\
			\hline
			           &            & Baseline   &            & 50.55$\pm$0.39                                                                                        & 23.99$\pm$2.25          & 16.86$\pm$2.16          & 11.31$\pm$1.91 \Tstrut           \\\hline
			\xmark     & \xmark     & \xmark     & \xmark     & 52.32$\pm$0.30                                                                                        & 31.33$\pm$1.56          & 24.48$\pm$2.07          & 18.19$\pm$1.97 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \xmark     & 54.22$\pm$0.39                                                                                        & 53.53$\pm$0.63          & 51.65$\pm$0.58          & 49.13$\pm$0.58 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \checkmark & 57.07$\pm$0.31                                                                                        & 55.41$\pm$0.62          & 53.68$\pm$0.55          & 51.19$\pm$0.48 \Tstrut           \\
			\checkmark & \checkmark & \xmark     & \xmark     & 54.51$\pm$0.24                                                                                        & 52.67$\pm$0.62          & 50.74$\pm$0.68          & 47.97$\pm$0.50 \Tstrut           \\
			\checkmark & \checkmark & \checkmark & \xmark     & 57.44$\pm$0.37                                                                                        & 54.73$\pm$0.47          & 52.64$\pm$0.47          & 49.96$\pm$0.49 \Tstrut           \\\hline
			\checkmark & \checkmark & \checkmark & \checkmark & \textbf{60.86$\pm$0.38}                                                                               & \textbf{57.35$\pm$0.61} & \textbf{55.42$\pm$0.66} & \textbf{52.60$\pm$0.60} \Tstrut
			\\\hline
			           &            &            &            & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb\ (from\ mask)}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                                        \\
			\hline
			           &            & Baseline   &            & 54.95$\pm$0.37                                                                                        & 37.98$\pm$2.21          & 32.58$\pm$2.37          & 27.56$\pm$2.48  \Tstrut          \\\hline
			\xmark     & \xmark     & \xmark     & \xmark     & 56.08$\pm$0.32                                                                                        & 42.12$\pm$1.78          & 37.19$\pm$2.13          & 32.56$\pm$2.27 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \xmark     & 55.81$\pm$0.38                                                                                        & 55.66$\pm$0.46          & 54.94$\pm$0.43          & 53.73$\pm$0.51 \Tstrut           \\
			\checkmark & \xmark     & \xmark     & \checkmark & 57.14$\pm$0.35                                                                                        & 56.52$\pm$0.38          & 55.84$\pm$0.42          & 54.62$\pm$0.41 \Tstrut           \\
			\checkmark & \checkmark & \xmark     & \xmark     & 56.06$\pm$0.32                                                                                        & 55.50$\pm$0.33          & 54.70$\pm$0.41          & 53.30$\pm$0.40 \Tstrut           \\
			\checkmark & \checkmark & \checkmark & \xmark     & 57.58$\pm$0.50                                                                                        & 56.34$\pm$0.45          & 55.48$\pm$0.50          & 54.21$\pm$0.62 \Tstrut           \\\hline
			\checkmark & \checkmark & \checkmark & \checkmark & \textbf{59.34$\pm$0.40}                                                                               & \textbf{57.44$\pm$0.42} & \textbf{56.62$\pm$0.41} & \textbf{55.39$\pm$0.51} \Tstrut
			\\\hline
		\end{tabular}
	}
	\label{table:ablation}
	\vspace{-1mm}
\end{table}


\begin{figure}[t!]
\centering
	\includegraphics[width=.99\linewidth]{figures/figure_lambda_abl.png}
	\caption{\small{{\blue Results for different values of unsupervised loss weight ($\lambda$) on the \emph{MVOR+} dataset. Results show the mean and confidence interval computed using different downsampling scales (1x, 8x, 10x, and 12x) and target resolutions (480, 520, 560, 600, 640, 680, 720, 760, and 800).}}}
	\label{figure:lambda-weights}
\end{figure}

\begin{table*}[t!]
	\centering
	\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1.5pt}\setlength\tabcolsep{0.5pt}\vspace{-2mm}
	\scalebox{0.95}{
		\begin{tabular}{cc}
			\centering
			\fbox{\includegraphics[width=3.6in]{figures/coco_ssl/000000410456_1p.jpg}} & \fbox{\includegraphics[width=3.6in]{figures/coco_ssl/000000410456_2p.jpg}}   \\
			1\% supervision                                                            & 2\% supervision                                                              \\
			\fbox{\includegraphics[width=3.6in]{figures/coco_ssl/000000410456_5p.jpg}} & \fbox{\includegraphics[width=3.6in]{figures/coco_ssl/000000410456_100p.jpg}} \\
			5\% supervision                                                            & 100\% supervision                                                            \\
		\end{tabular}
	}
	\captionof{figure}[]{\small{Qualitative results on a sample image from \emph{COCO-val} dataset with x\%(x=1,2,5,100) of labeled supervision. We use the \emph{AdaptOR-SSL} for 1\%, and 5\% labeled supervision with the rest of the data as the unlabeled data. We see comparable qualitative results with 1\% of labeled supervision to 100\% of labeled supervision. The red arrows show either missed detections or localization errors.}}
	\vspace{-1mm}
	\label{tab:table-qual3}
\end{table*}

\begin{table*}[htb!]
	\centering
	\caption{\small{Results for \emph{AdaptOR-SSL} on \emph{COCO-val} dataset under the semi-supervised learning setting with x\%(x=1,2,5,10) of labeled supervision. We compare it with the fully supervised baselines trained on the same labeled data without using any unlabeled data. The \emph{supervised} baseline uses only the random resize and random-flip data augmentations as used in \citep{he2017mask} whereas \emph{supervised++} uses the same data augmentation pipeline as in \emph{AdaptOR-SSL} containing \emph{weakly} and \emph{strongly} augmented labeled images. We also compare it with the current state-of-the-art SSL object detector Unbiased-Teacher \citep{liu2021unbiased} for the person bounding box detection task. The inference is performed on a single scale of 800 pixels (shorter side) following the same settings as used in \citep{he2017mask, liu2021unbiased}.}}
	\vspace{-2mm}
	\scalebox{0.94}{
		\begin{tabular}{l|cccc|cccc|cccc}
			\toprule
			\multirow{2}{*}{Methods}    & \multicolumn{4}{c|}{$\mathit{AP_{person}^{bb}}$} \Tstrut \Bstrut & \multicolumn{4}{c|}{$\mathit{AP_{person}^{kp}}$} \Tstrut \Bstrut & \multicolumn{4}{c}{$\mathit{AP_{person}^{mask}}$} \Tstrut \Bstrut                                                                                                                                                                  \\
			\cline{2-13}
			                            & ~1\%~                                                            & ~2\%~                                                            & ~5\%~                                                             & ~10\%~         & ~1\%~          & ~2\%~          & ~5\%~          & ~10\%~         & ~1\%~          & ~2\%~          & ~5\%~          & ~10\%~ \Tstrut         \\
			\hline
			\emph{supervised}           & 22.09                                                            & 28.43                                                            & 34.52                                                             & 37.88          & 15.91          & 23.58          & 30.96          & 37.77          & 18.13          & 23.93          & 29.34          & 33.47  \Tstrut         \\
			\emph{supervised++}         & 28.59                                                            & 34.27                                                            & 41.18                                                             & 43.60          & 25.78          & 32.14          & 41.45          & 46.51          & 24.18          & 29.14          & 35.40          & 37.83  \Tstrut         \\\hline
			Unbiased-Teacher            & 39.18                                                            & 40.76                                                            & 43.72                                                             & 46.64          & -              & -              & -              & -              & -              & -              & -              & - \Tstrut              \\
			\emph{\textbf{AdaptOR-SSL}} & \textbf{42.57}                                                   & \textbf{45.37}                                                   & \textbf{49.90}                                                    & \textbf{52.70} & \textbf{38.22} & \textbf{44.08} & \textbf{49.79} & \textbf{56.65} & \textbf{36.06} & \textbf{38.96} & \textbf{43.10} & \textbf{45.46} \Tstrut \\ \hline
			\bottomrule
		\end{tabular}
	}
	\label{table:source:ssl}
	\vspace{-1mm}
\end{table*}

\begin{table}[bt!]
	\centering
	\caption{\small{{\blue Performance comparison when applying \emph{AdaptOR-SSL} models trained with 1\%, 2\%, 5\%, and 10\% source domain labels to the target domain of \emph{MVOR+} (see ``Before UDA'' results). When we apply the \emph{AdaptOR} approach on the \emph{AdaptOR-SSL} model (trained using 10\% source domain labels), we observe an improvement in the performance (see ``After UDA'' results). Results corresponding to 100\% source domain labeled supervision in ``Before UDA'' and ``After UDA'' are obtained from Table \ref{table:source} and \ref{table:source-target-da}, respectively.}}}
	\vspace{-2mm}
	\scalebox{0.78}{
		\begin{tabular}{l|cccc}
			\toprule
			\multirow{3}{*}{models} & \multicolumn{4}{c}{\textbf{\emph{MVOR+}}} \Tstrut \Bstrut                                                                                                         \\
			\cline{2-5}
			                        & \textbf{~1x~}                                                                                         & \textbf{~8x~}  & \textbf{~10x~} & \textbf{~12x~}  \Tstrut \\
			\cline{2-5}
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                           \\
			\hline
			Before UDA              &                                                                                                       &                & \Tstrut                                  \\
			\emph{1\%}              & 48.33$\pm$0.67                                                                                        & 39.89$\pm$1.97 & 34.46$\pm$2.44 & 28.63$\pm$3.25  \Tstrut \\
			\emph{2\%}              & 48.28$\pm$0.64                                                                                        & 41.12$\pm$2.00 & 35.93$\pm$2.16 & 30.51$\pm$2.54 \Tstrut  \\
			\emph{5\%}              & 51.27$\pm$0.48                                                                                        & 43.11$\pm$2.08 & 37.95$\pm$2.14 & 31.75$\pm$2.72 \Tstrut  \\
			\emph{10\%}             & 53.95$\pm$0.65                                                                                        & 44.74$\pm$1.92 & 39.83$\pm$2.03 & 34.13$\pm$2.60 \Tstrut  \\
			\emph{100\%}            & 56.61$\pm$0.34                                                                                        & 40.42$\pm$2.17 & 34.87$\pm$2.47 & 29.61$\pm$2.69 \Tstrut  \\\hline
			After UDA               &                                                                                                       &                & \Tstrut                                  \\
			\emph{10\%}             & 57.58$\pm$0.56                                                                                        & 55.80$\pm$0.60 & 54.70$\pm$0.51 & 53.44$\pm$0.38 \Tstrut  \\
			\emph{100\%}            & 61.41$\pm$0.40                                                                                        & 59.48$\pm$0.35 & 58.55$\pm$0.36 & 57.33$\pm$0.43 \Tstrut  \\
			\hline
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{kp}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                                           \\
			\hline
			Before UDA              &                                                                                                       &                & \Tstrut                                  \\
			\emph{1\%}              & 25.28$\pm$1.06                                                                                        & 16.64$\pm$1.17 & 12.72$\pm$1.90 & 08.34$\pm$1.94 \Tstrut  \\
			\emph{2\%}              & 30.16$\pm$0.58                                                                                        & 21.44$\pm$1.91 & 16.28$\pm$2.22 & 11.35$\pm$2.39 \Tstrut  \\
			\emph{5\%}              & 37.09$\pm$0.30                                                                                        & 25.93$\pm$2.22 & 20.12$\pm$2.38 & 13.84$\pm$2.50 \Tstrut  \\
			\emph{10\%}             & 41.51$\pm$0.58                                                                                        & 28.57$\pm$1.88 & 22.57$\pm$2.15 & 16.17$\pm$2.38 \Tstrut  \\
			\emph{100\%}            & 50.55$\pm$0.39                                                                                        & 23.99$\pm$2.25 & 16.86$\pm$2.16 & 11.31$\pm$1.91 \Tstrut  \\\hline
			After UDA               &                                                                                                       &                & \Tstrut                                  \\
			\emph{10\%}             & 48.52$\pm$0.50                                                                                        & 45.73$\pm$0.56 & 43.74$\pm$0.47 & 40.90$\pm$0.44 \Tstrut  \\
			\emph{100\%}            & 60.86$\pm$0.38                                                                                        & 57.35$\pm$0.61 & 55.42$\pm$0.66 & 52.60$\pm$0.60 \Tstrut  \\
			\hline
			                        & \multicolumn{4}{c}{$\mathit{AP_{person}^{bb\ (from\ mask)}}$ (mean$\pm$std)} \Tstrut \Bstrut                                                             \\
			\hline
			Before UDA              &                                                                                                       &                & \Tstrut                                  \\
			\emph{1\%}              & 47.54$\pm$0.78                                                                                        & 38.37$\pm$2.32 & 32.44$\pm$2.73 & 26.32$\pm$3.42 \Tstrut  \\
			\emph{2\%}              & 47.96$\pm$0.90                                                                                        & 39.32$\pm$2.29 & 33.54$\pm$2.30 & 27.87$\pm$2.44 \Tstrut  \\
			\emph{5\%}              & 50.55$\pm$0.74                                                                                        & 41.09$\pm$2.30 & 35.68$\pm$2.16 & 29.45$\pm$2.69 \Tstrut  \\
			\emph{10\%}             & 52.79$\pm$0.69                                                                                        & 42.63$\pm$2.17 & 37.18$\pm$2.10 & 31.41$\pm$2.60 \Tstrut  \\
			\emph{100\%}            & 54.95$\pm$0.37                                                                                        & 37.98$\pm$2.21 & 32.58$\pm$2.37 & 27.56$\pm$2.48 \Tstrut  \\\hline
			After UDA               &                                                                                                       &                & \Tstrut                                  \\
			\emph{10\%}             & 55.60$\pm$0.52                                                                                        & 54.07$\pm$0.58 & 53.00$\pm$0.49 & 51.55$\pm$0.35 \Tstrut  \\
			\emph{100\%}            & 59.34$\pm$0.40                                                                                        & 57.44$\pm$0.42 & 56.62$\pm$0.41 & 55.39$\pm$0.51 \Tstrut  \\
			\hline
		\end{tabular}
	}
	\label{table:init-exps}
	\vspace{-1mm}
\end{table}

\emph{6.2.1.2 Components of AdaptOR}\\
Table \ref{table:ablation} shows the ablation experiments to see the effect of using different types of augmentations on the strongly transformed images used by the student model during training. The results show that the \emph{strong-resize} augmentations are needed to adapt the model to the low-resolution OR images. The geometric transform exploiting the \emph{transformation equivariance constraints} significantly improves the results, especially for the pose estimation task, where we also utilize the chirality transforms to map the flipped keypoints to the horizontally flipped image. The results are further improved using the random-augment and random-cut augmentations.

{\blue \emph{6.2.1.3 Effect of unsupervised loss weight ($\lambda$) values}\\
Unsupervised loss weight ($\lambda$) controls the proportion of the total loss attributed to the unsupervised loss for the target domain. As the aim is to adapt the model to the target domain, higher value of $\lambda$ generally leads to better performance. Fig.~\ref{figure:lambda-weights} shows the ablation results for different values of unsupervised loss weight ($\lambda$). We observe that the increase in the $\lambda$ increases the accuracy; however, it starts to decrease after the $\lambda$ value of 4.0.}

\subsection{AdaptOR-SSL: semi-supervised learning (SSL) on source-domain}
Table \ref{table:source:ssl} shows the results of SSL experiments using \emph{AdaptOR-SSL} on the COCO dataset with 1\%, 2\%, 5\%, and 10\% labeled supervision. The results with 100\% labeled supervision are presented in Table \ref{table:source}. The first two rows in Table \ref{table:source:ssl} show the results of two fully supervised baselines: \emph{supervised} and \emph{supervised++}. The \emph{supervised} baseline uses random-resize and random-flip augmentations as used \citep{he2017mask}, whereas the \emph{supervised++} uses the our data augmentation pipeline containing \emph{weakly} and \emph{strongly} augmented labeled images. We observe significant improvement in the results by utilizing our data augmentation pipeline. We also compare our bounding box detection results with the current state-of-the-art SSL approach for object detection, Unbiased-teacher \citep{liu2021unbiased}: a multi-class object bounding box detection approach using \emph{self-training} and \emph{mean-teacher} based SSL approach. Different from ours, it uses fully supervised ImageNet weights for initialization and does not exploit the \emph{transformation equivariance constraints} using geometric augmentations. As the Unbiased-teacher performs bounding box detection on 80 COCO classes, we compare our results with their person category results \emph{$AP_{person}^{bb}$} from the model obtained from their GitHub repository\footnote{\url{https://github.com/facebookresearch/unbiased-teacher}}. We observe significant improvement in results attributed to our initialization using the self-supervised method, feature normalization using GN, exploitation of the geometric constraints on the unlabeled data, and single class training for person category exploiting mask and the keypoint annotations. Fig.~\ref{tab:table-qual3} shows the qualitative results from the models trained with 1\%, 2\%, 5\% and 100\% labels. We also show the qualitative results in the supplementary video on some YouTube videos and observe comparable qualitative results with 1\% of labeled supervision w.r.t 100\% of labeled supervision.

{\blue \subsection{Domain adaptation on AdaptOR-SSL model}
Table \ref{table:init-exps} shows results when we evaluate \emph{AdaptOR-SSL} models trained using 1\%, 2\%, 5\%, and 10\% source domain labels to our \emph{MVOR+} target domain. We observe a significant decrease in the results (see ``Before UDA'' results in the Table \ref{table:init-exps}). As a final experiment, we initialize our \emph{AdaptOR} approach with \emph{AdaptOR-SSL} model trained using 10\% source domain labels. We observe an increase in the performance after the domain adaptation. However, there still exists a gap of around 4\% for \emph{$AP_{person}^{bb}$} and \emph{$AP_{person}^{bb\ (from\ mask)}$}, and 12\% for \emph{$AP_{person}^{kp}$}. These results show the need to develop effective domain adaptation approaches in the presence of limited source domain labels.}



























































































%
