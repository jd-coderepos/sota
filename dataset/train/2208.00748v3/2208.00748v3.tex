\section{Experiments}
\label{sec:experiemnts}

We evaluate \sled{} on \SCROLLS{} \cite{Shaham2022SCROLLSSC}, a recently-proposed benchmark for evaluating long text understanding.
\SCROLLS{} contains seven datasets that span three different language understanding tasks:
\begin{enumerate}
    \item Summariazation: \emph{GovReport} \cite{huang2021govreport} is a summarization task over reports from the Congressional Research Service; \emph{SummScreenFD} \cite{chen2021summscreen} is a summarization dataset over TV scripts; \emph{QMSum} \cite{zhong2021qmsum} is a query-based summarization dataset over meeting transcripts from various domains. While GovReport and SummScreenFD do not contain a prefix, for QMSum we consider the query as the prefix.
    \item Question answering (QA): \emph{Qasper} \cite{dasigi2021qasper} is a QA benchmark that contains questions over NLP papers; \emph{NarrativeQA} \cite{kocisky2018narrativeqa}
    contains questions over entire books and movie scripts; \emph{QuALITY} \cite{pang2021quality} is a multiple-choice QA dataset over books and articles.
    For all QA datasets, we set the question as the prefix. For QuALITY, we consider the four answer options part of the question.
    \item Natural language inference: \emph{ContractNLI} \cite{koreeda-manning-2021-contractnli-dataset} contains short legal hypotheses (set as the prefix) and legal documents as the premise. Models are tasked to predict whether the premise entails, contradicts or is neutral w.r.t. to the hypothesis.
\end{enumerate}

For each task, we use the official evaluation metrics defined in \SCROLLS{}, which are based on the metrics from the original datasets.

\subsection{Settings}

We evaluate \sled{} with both BART \cite{Lewis2020BARTDS} and T5 \cite{Raffel2020ExploringTL} as backbone models.
For each backbone model, we compare performance with \sled{}, which can consume long sequences, vs. the backbone models alone that are fed with the first 1,024 tokens. For comparison, we also finetune \ledbase{}. In all \sled{} and \led{} experiments, we use a maximal sequence length of 16K tokens and chunk size of 256 to allow for a fair evaluation.

For each model-dataset pair, we run hyperparameter tuning (detailed in Appendix~\ref{app:experimental-details}) based on the development set.
Additionally, we submit generated predictions over the test set to \SCROLLS{} leaderboard,\footnote{\label{fn:leaderboard}\url{https://www.scrolls-benchmark.com/leaderboard}} and compare to the reported performance of other models at the time of submission.
\newcommand\leaderboardfootnote{\value{footnote}}


\subsection{Results}
\label{subsec:results}
\begin{table*}[th]
\footnotesize
\addtolength{\tabcolsep}{-2pt}  
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l@{}rccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{~(Chunk/Input)}} &
\multirow{2}{*}{\textbf{\#Params}} & \multirow{2}{*}{\textbf{Avg}} & \textbf{GovRep} & \textbf{SumScr} &  \textbf{QMSum} & \textbf{Qspr}  & \textbf{Nrtv}  & \textbf{QALT}  & \textbf{CNLI}  \\
&  & & & ROUGE-1/2/L  &  ROUGE-1/2/L  &  ROUGE-1/2/L  & F1  & F1 & EM-T/H &  EM \\
\midrule


\multicolumn{11}{c}{\textbf{Development Scores}} \\
\midrule

\primitiveinput{tables/main_results.txt}

\midrule
\multicolumn{11}{c}{\textbf{Test Scores}} \\
\midrule

\primitiveinput{tables/main_results_test.txt}\\[0.1cm]
\hdashline\\[-0.3cm]
\primitiveinput{tables/reported_main_results_test.txt}\\

\bottomrule
\end{tabular}}
\caption{Main results on the \SCROLLS{} benchmark. Chunk/Input refers to the chunk size used ($c$) and to the maximal input length ($n$). Avg is the average \SCROLLS{} score as described in \newcite{Shaham2022SCROLLSSC}. Development scores for QuALITY are only for the full set (T). $\dagger$ indicates reported results from \SCROLLS{} public leaderboard.\footnotemark[\leaderboardfootnote{}]
\ledbaseglobal{} scores were reported by \newcite{Shaham2022SCROLLSSC} and are lower than 
our LED$_{\mathtt{base}}$ implementation, presumably since our implementation uses all question tokens for global attention rather than just the first one. The results for LongT5  and UL2 were submitted to the SCROLLS leaderboard by their authors.}


\label{tab:main_results}
\end{table*}
 
Tab.~\ref{tab:main_results} reports results over \SCROLLS{} development and test sets.
Taking short-range pretrained LMs like BART and T5 and casting them into \sled{}'s framework allows them to process long documents effectively, improving the average \SCROLLS{} score by 4.8-7 points.
Examining \bartbase{}-\sled{}, we see a large improvement compared to \ledbase{} (33.6$\rightarrow$35.4), and competitive performance on multiple tasks compared to LongT5$_{\mathtt{base}}$ and UL2.
Moreover, adding \sled{} to \bartlarge{}
results in a high-performing model
with results that are comparable to LongT5$_{\mathtt{base}}$ and outperforming UL2, despite UL2's large parameter count (50x larger), and with no need for expensive pretraining geared towards long-range tasks. \bartlarge{}-\sled{}'s performance is moderately lower than the larger LongT5 models.


Barring QuALITY, \sled{} significantly improves performance across all tasks compared to the corresponding backbone models.
All summarization datasets (GovReport, SummScreenFD and QMSum) show impressive gains of up to 35\% compared to their baseline scores, across all metrics (Rouge-1/Rouge-2/Rouge-L \cite{Lin2004ROUGEAP}) and for all three backbone models. 
Similarly, on ContractNLI \cite{koreeda-manning-2021-contractnli-dataset} we see large relative improvements. As the performance of the baseline models was already high, this boost in performance is even more significant.
Finally, the QA datasets Qasper and NarrativeQA show the largest gains, improving by an average of 60\%.


\paragraph{QuALITY} In stark contrast to other datasets lies the multi-choice QA dataset QuALITY \cite{pang2021quality}. While the performance of \bartlarge{}-\sled{} is above chance, it barely improves the performance of its backbone model (\bartlarge{}), which observes only the first 1K tokens, with a similar trend in other backbone models. Analyzing test scores in Tab.~\ref{tab:main_results}, we see  that
increasing model size consistently improves performance (up to 46\% exact match), but increasing input length has a negligible effect. 
Since reported human accuracy on QuALITY is high (93.5\%), this hints that QuALITY might require commonsense reasoning and knowledge that are absent from models with a lower parameter count.

\paragraph{Summary}
We have shown that taking off-the-shelf pretrained LMs and embedding them into \sled{} leads to competitive performance on \SCROLLS{}. Importantly, any future pretrained LM can be easily plugged into \sled{}, without the need for an expensive pretraining step.


\subsection{Datasets analysis}
\sled{}'s simplicity and modularity allow it to be used as a useful tool for dataset analyses. Specifically, we can vary the chunk size, $c$, and the number of tokens, $n$, across datasets to analyze a) how local are individual pieces of relevant information, and b) 
how far into the document they are located.

\label{subsec:properties}
\paragraph{Locality of information}
\sled{} relies on an assumption that information can be contextualized locally at encoding time. To analyze locality, we vary the chunk size, $c$, which defines the attention window, and measure
the effect on \SCROLLS{} datasets with input length 16K. 
Fig.~\ref{fig:ab-chunk} shows the results of this experiment, where the y-axis shows the relative improvement compared to \bartbase{} on a target metric as a function of the chunk size $c$ for all datasets.
We observe that in all datasets the best performing chunk size is relatively small (up to 256), and further increasing $c$ even hurts the performance in some cases. However, the summarization datasets show a much larger gain in performance when increasing $c$ up to that threshold.
This coincides with a common hypothesis that QA and NLI require relatively local context, and thus increasing $c$ can add noise and hurt optimization, while summarization may require a more high-level view of information.



\begin{figure}[t]
\begin{center}


\centerline{
\includegraphics[width=\linewidth]{figures/ablations_chunk_size.pdf}
}
\setlength{\belowcaptionskip}{-20pt}
\caption{\bartbase{}-\sled{} relative improvement compared to \bartbase{} results, when varying the \sled{}'s chunk size (i.e. $c$), fixing the maximum input length to 16K. \textbf{Top}: Summarization datasets. The y-axis measures relative improvement of Rouge-2. \textbf{Bottom}: QA and NLI datasets. The y-axis measures relative improvement of exact match for QuALITY and ContractNLI and F$_1$ for NarrativeQA and Qasper.
}
 
\label{fig:ab-chunk}
\end{center}
\end{figure} 
\paragraph{Distance from start of document}
We now analyze whether indeed the entire document is required for tasks in \SCROLLS{} by varying the maximum document length, $n$.
Fig.~\ref{fig:ab-length} shows the results of this experiment, where the y-axis shows relative improvement of \bartbase{}-\sled{} compared to \bartbase{} as a function of the first $n$ tokens from the document (chunk size $c=256$). As expected, all datasets (except QuALITY) show a roughly monotonic improvement in performance with $n$. 
This shows that
(a) \sled{} is able to effectively use all of the information in a long sequence (up to 16K tokens),\footnote{For ContractNLI, the length of over 95\% of the tokenized examples is less than 8K.} and that (b) observing the entire inputs from \SCROLLS{} improves performance.
\begin{figure}[t]
\begin{center}


\centerline{
\includegraphics[width=\linewidth]{figures/ablations_max_len.pdf}
}
\setlength{\belowcaptionskip}{-20pt}
\caption{\bartbase{}-\sled{} relative improvement compared to \bartbase{} results, when varying the input length fed to \sled{}, fixing $c=256$. \textbf{Top}: Summarization datasets compared w.r.t. Rouge-2. \textbf{Bottom}: QA and NLI datasets. Relative improvment is measured w.r.t. exact match for QuALITY and ContractNLI and F$_1$ for NarrativeQA and Qasper.}
 

\label{fig:ab-length}
\end{center}
\end{figure} 
\subsection{Effect of context padding}
In all experiments thus far, we used a conservative padding value $\rho=0.5$, resulting in effective chunk size of $\frac{c}{2}$ and $\frac{c}{4}$ context padding tokens on each side.
Since both memory and, more importantly, the number of forwards passes through the encoder are linear in the number of chunks, a natural question is how much padding and overlap are necessary to achieve satisfactory results. 

To explore this, we finetune \bartbase{}-\sled{} on all six datasets  where \sled{} showed gains over its baseline model (i.e., all datasets except for QuALITY), varying the value of $\rho$, and fixing $c=256$. Tab.~\ref{tab:padding} shows the results of this experiment, where we compare relative gain compared to \bartbase{} across different $\rho$ values.

As expected, decreasing the padding factor and consequently the number of chunks reduces training time. When $\rho=0.05$ training can be faster by up to 2x compared to $\rho=0.5$ as the number of chunks drops to almost half. 
Moreover, relative gain (i.e., improvement relative to the baseline) is often close to or even higher with less padding (perhaps due to better encoding or more stable optimization). Nevertheless, there is no single $\rho$ value that consistently beats the conservative choice of $\rho=0.5$. In particular, in all six datasets, setting $\rho=0.5$ results in a top-2 performance, often by a large margin and never considerably worse then the best result.
Thus, we conclude that one may improve the efficiency and performance of \sled{} by tuning the hyperparameter $\rho$ for optimal behaviour w.r.t. a specific task, and we fix $\rho=0.5$ in our experiments.

Moreover, Tab.~\ref{tab:padding} demonstrates the importance of having chunks at least partially overlapping. In all six dataset, using non-overlapping chunks ($\rho=0$) results in a drop of at least 10\% gain compared to the best setting, where in some cases this gap grows to over 50\%. This supports our hypothesis that chunking inputs with no overlap may lead to crucial loss of information.

\begin{table}[th]
\footnotesize
\addtolength{\tabcolsep}{-3pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}lcccccc@{}}

\toprule

\multirow{2}{*}{\textbf{$\mathbf{\rho}$}} & \multicolumn{6}{c}{\textbf{Relative Gain}}\\
& \underline{GovRep} & \underline{SumScr} &  \underline{QMSum} & \underline{Qspr}  & \underline{Nrtv} & \underline{CNLI} \\

\rule{0pt}{2.5ex}\primitiveinput{tables/padding_gain.txt}


\bottomrule
\end{tabular}}
\caption{\bartbase{}-\sled{} relative improvement compared to \bartbase{} when varying the padding percentage ($\rho$).
In all cases the maximum input length is 16K and $c=256$. Relative gain is measured w.r.t. Rouge-2 for GovReport, SummScreenFD and QMSum, F$_1$ for Qasper and NarrativeQA and exact match for ContractNLI. In each column, \textbf{boldface} marks the top performing value and \underline{underline} the second-best.}
\label{tab:padding}
\end{table}

 
