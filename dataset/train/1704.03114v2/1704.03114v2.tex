\section{Experiments}
\label{sec:exprt}

\begin{table*}[t]
    \centering
    \small
\begin{tabular}{c|c|c|c|c|c|c|c}
        &  & \multicolumn{2}{c}{Predicate Recognition} & \multicolumn{2}{|c}{Union Box Detection} & \multicolumn{2}{|c}{Two Boxes Detection} \\
        \cline{3-8}
        & & Recall@50 & Recall@100 & Recall@50 & Recall@100 & Recall@50 & Recall@100 \\
	\hline
        \multirow{5}{*}{\rotatebox{90}{\textbf{VRD}}}  
        & VP \cite{sadeghi2011recognition}    & 0.97 & 1.91 & 0.04 & 0.07 & - & - \\
        & Joint-CNN\cite{fang2015captions}    & 1.47 & 2.03 & 0.07 & 0.09 & 0.07 & 0.09 \\
        & VR \cite{lu2016visual}              & 47.87 & 47.87 & 16.17 & 17.03 & 13.86 & 14.70 \\
        & DR-Net	       		      & \textbf{80.78} & \textbf{81.90} & 19.02 & 22.85 & 16.94 & 20.20 \\
	& DR-Net + pair filter                            & - & - & \textbf{19.93} & \textbf{23.45} & \textbf{17.73} & \textbf{20.88} \\
        \hline
        \hline
        \multirow{5}{*}{\rotatebox{90}{\textbf{sVG}}}  
        & VP \cite{sadeghi2011recognition}    & 0.63 & 0.87 & 0.01 & 0.01 & - & - \\
        & Joint-CNN\cite{fang2015captions}    & 3.06 & 3.99 & 1.24 & 1.60 & 1.21 & 1.58  \\
        & VR \cite{lu2016visual}              & 53.49 & 54.05 & 13.80 & 17.39 & 11.79 & 14.84  \\
        & DR-Net                             & \textbf{88.26} & \textbf{91.26} & 20.28 & 25.74 & 17.51 & 22.23 \\
	& DR-Net + pair filter			      & - & - & \textbf{23.95} & \textbf{27.57} & \textbf{20.79} & \textbf{23.76} 
\end{tabular}
    \caption{\small Comparison with baseline methods, using \emph{Recall@50} and \emph{Recall@100} as the metrics.
    We use ``-'' to indicate \emph{``not applicable''}. 
    For example, no results are reported for \emph{DR-Net + pair filter} on Predicate Recognition, as in this setting,
    pairs are given, and thus pair filtering can not be applied.
    Also, no results are reported for \emph{VP} on Two Boxes detection, as VP detects the entire instance as 
    a single entity.    
    }
    \label{tab:itr_rst}
\end{table*}

 \begin{table*}[t]
    \centering
    \small
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
	&  & A & A & S & AS & ASC & ASD & ASD &ASDF \\
	\hline
	\multirow{3}{*}{\rotatebox{90}{\textbf{VRD}}}
	& Predicate Recognition & 63.39 & 65.93 & 64.72 & 71.81 & 72.77 & 80.66 & \textbf{80.78} & -\\
	& Union Box Detection & 12.01 & 12.56 & 13.76 & 16.04 & 16.37 & 18.15 & \textbf{19.02} & \textbf{19.93} \\
	& Two Boxes Detection & 10.71 & 11.22 & 12.16 & 14.38 & 14.66 & 16.12 & \textbf{16.94} & \textbf{17.73} \\
	\hline
	\hline
	\multirow{3}{*}{\rotatebox{90}{\textbf{sVG}}}
	& Predicate Recognition & 72.13 & 72.54 & 75.18 & 79.10 & 79.18 & 88.00 & \textbf{88.26} & -\\
	& Union Box Detection & 13.24 & 13.84 & 14.01 & 16.04 & 16.08 & 20.21 & \textbf{20.28} & \textbf{23.95} \\
	& Two Boxes Detection & 11.35 & 11.98 & 12.07 & 13.77 & 13.81 & 17.42 & \textbf{17.51} & \textbf{20.79}
\end{tabular}
    \caption{\small Comparison of different variants of the proposed method, using \emph{Recall@50} as the metric.}
    \label{tab:cfg_rst}
\end{table*}

 \begin{table*}
    \centering
   \small
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{c|c|c|c|c}
         & \includegraphics[width=0.2\textwidth,height=0.2\textwidth]
        {imgs/predicate_sample_6.jpg} 
                    & \includegraphics[width=0.2\textwidth,height=0.2\textwidth]
        {imgs/predicate_sample_1.jpg}
                    & \includegraphics[width=0.2\textwidth,height=0.2\textwidth]
        {imgs/predicate_sample_5.jpg}
                    & \includegraphics[width=0.2\textwidth,height=0.2\textwidth]
        {imgs/predicate_sample_7.jpg}
        \\
        \hline
        VR\cite{lu2016visual} & (sky, \textbf{in}, water)
                            & (giraffe, \textbf{have}, tree)
                            & (woman, \textcolor{red}{ride}, bicycle)
                            & (cat, \textbf{have}, hat)
        \\
        A & (sky, \textbf{on}, water)
        & (giraffe, \textbf{have}, tree) 
        & (woman, \textbf{behind}, bicycle)
        & (cat, \textbf{on}, hat)
        \\
        S & (sky, \textcolor{red}{above}, water)
        & (giraffe, \textbf{in}, tree) 
        & (woman, \textbf{wear}, bicycle)
        & (cat, \textbf{have}, hat)
        \\
        AS & (sky, \textcolor{red}{above}, water)
        & (giraffe, \textcolor{red}{behind}, tree)
        & (woman, \textbf{wear}, bicycle)
        & (cat, \textbf{have}, hat)
        \\
        ASC & (sky, \textcolor{red}{above}, water)
        & (giraffe, \textcolor{red}{behind}, tree)
        & (woman, \textcolor{red}{ride}, bicycle)
        & (cat, \textbf{have}, hat)
        \\
        ASD & (sky, \textcolor{red}{above}, water)
        & (giraffe, \textcolor{red}{behind}, tree)
        & (woman, \textcolor{red}{ride}, bicycle)
        & (cat, \textcolor{red}{wear}, hat)
    \end{tabular}
    \caption{\small This table lists predicate recognition results for some object pairs. 
    Images containing these pairs are listed in the first row,
    where the red and green boxes respectively correspond to the subjects and the objects.
    The most probable predicate predicted by different methods are listed in the following rows,
    in which \textbf{black} indicates wrong prediction and \textcolor{red}{red} indicates correct prediction.}
    \label{tab:itr_sample}
\end{table*}

 
We tested our model on two datasets:
(1) \textbf{VRD}: the dataset used in~\cite{lu2016visual},
containing  images and
 visual relationship instances that belong to
 triplet types.
We follow the train/test split in~\cite{lu2016visual}.
(2) \textbf{sVG}: a substantially larger subset
constructed from Visual Genome~\cite{krishnavisualgenome}.
\emph{sVG} contains  images and  relationship instances
that belong to  triplet types.
All instances are randomly partitioned into disjoint training and testing sets,
which respectively contain  and  instances.

\subsection{Experiment Settings}

\paragraph{Model training.}
In all experiments, we trained our model using Caffe\cite{jia2014caffe}.
The appearance module is initialized with a model pre-trained on ImageNet,
while the spatial module and the DR-Net are initialized randomly.
After initialization, the entire network is jointly optimized using SGD.

\paragraph{Performance metrics.}
Following~\cite{lu2016visual}, we use \emph{Recall@K} as the major performance metric,
which is the the fraction of ground-truth instances that are correctly recalled in top  predictions.
Particularly, we report \emph{Recall@100} and \emph{Recall@50} in our experiments.
The reason of using \emph{recall} instead of \emph{precision} is that
the annotations are incomplete, where some true relationships might be missing.

\paragraph{Task settings.}
Like in~\cite{lu2016visual}, we studied three task settings:
\textbf{(1) Predicate recognition}: this task focuses on the accuracy of \emph{predicate} recognition,
where the labels and the locations of both the \emph{subject} and \emph{object} are given.
\textbf{(2) Union box detection}: this task treats the whole triplet as a union bounding box.
A prediction is considered correct if all three elements in a triplet  are
correctly recognized, and the IoU between the predicted box and the ground-truth is
above .
\textbf{(3) Two boxes detection}: this is similar to the one above,
except that it requires the IoU metrics for the subject and the object
are both above . This is relatively more challenging.


\subsection{Comparative Results}



\paragraph{Compare with baselines.}

We compared our method with the following methods under all three task settings outlined above.
(1) \textbf{Visual Phrase(VP)}~\cite{sadeghi2011recognition}:
a representative approach that treats each distinct triplet as a different class.
and employs a DPM detector~\cite{lsvm-pami} for each class.
(2) \textbf{Joint-CNN}~\cite{fang2015captions}:
a neural network~\cite{Simonyan14c} that has +-way outputs, jointly predicts the class responses
for subject, object, and relationship predicate.
(3) \textbf{Visual Relationship (VR)}~\cite{lu2016visual}:
This is the state-of-the-art and is the most closely related work.

Table~\ref{tab:itr_rst} compares the results. On both datasets, we observed:
(1) VP~\cite{sadeghi2011recognition} performs very poorly, failing in most cases,
as it is difficult to cope with such a huge and imbalanced class space.
(2) Joint-CNN~\cite{fang2015captions} also works poorly, as
it's hard for the CNN to learn a common feature representation for both
relationship predicates and objects.
(3) VR~\cite{lu2016visual} performs substantially better than the two above.
However, the performance remains unsatisfactory.
(4) The proposed method outperforms the state-of-the-art method \emph{VR}~\cite{lu2016visual} by a considerable margin
in all three tasks.
Compared to \emph{VR}, it improves the \emph{Recall@100} of \emph{predicate recognition} by over  on both datasets.
Thanks to the remarkably improved accuracy in recognizing the relationship predicates,
the performance gains on the other two tasks are also significant.
(5) Despite the significant gain compared to others,
the recalls on \emph{union box detection} and \emph{two boxes detection}
remains weak. This is primarily ascribed to the limitations of the object detectors.
As shown in Figure \ref{fig:iou_relax}, we observe that
the object detector can only obtain about  of object recall, measured by \emph{Recall@50}.
To improve on these tasks, a more sophisticated object detector is needed.

\begin{figure}
	\centering
\includegraphics[width=0.4\textwidth]{rebuttal/union_curve.pdf}
	\caption{\small This figure shows the performance on the \emph{union-box detection} task with different IoU thresholds.}
	\label{fig:iou_relax}
\end{figure}

\paragraph{Compare different configs.}

We also compared different variants of the proposed method, in order
to identify the contributions of individual components listed below:
(1)\textbf{Pair (F)ilter}: the pair filter discussed in section \ref{sec:ovalfrm},
used to filter out object pairs with trivial relationships.
(2)\textbf{(A)ppearance Module}: the appearance module, which has two versions,
\emph{A}: based on VGG16~\cite{Simonyan14c}, which is also the network used in VR~\cite{lu2016visual},
\emph{A}: based on ResNet101~\cite{he2015deep}.
(3)\textbf{(S)patial Module}: the network to capture the spatial configs, as mentioned in section \ref{sec:ovalfrm}.
(4)\textbf{(C)RF}: a classical CRF formulation, used as a replacement of the DR-Net to capture statistical dependencies.
(5)\textbf{(D)R-Net}: the DR-Net discussed in section \ref{sec:drnet}.
The name of a configuration is the concatenation of abbrevations of involved components,
\eg, the configuration named \emph{ASC} contains an appearance module based on VGG16, a spatial module, and a CRF.

In Table \ref{tab:cfg_rst},
we compared \emph{A}, \emph{A}, \emph{S}, \emph{AS}, \emph{ASC}, \emph{ASD}, \emph{ASD} and \emph{ASDF}.
The results show:
(1) Using better networks (ResNet-101 vs. VGG16) can moderately improve the performance.
However, even with state-of-the-art network \emph{A},
visual relationship detection could not be done effectively using appearance information alone.
(2) The combination of appearance and spatial configs considerably outperforms each component alone,
suggesting that visual appearances and spatial configurations are complementary to each other.
(3) The statistical dependencies are important. However, CRF is not able to effectively exploit them.
With the use of DR-Net, the performance gains are significant.
We evaluated the perplexities of the predictions for our model \emph{with} and \emph{without} DR-Net,
which are  and . These results show the benefit of exploiting statistical dependencies for joint recognition.

Table~\ref{tab:itr_sample} further shows the predicted relationships on several example images.
The first two columns show that the incorporation of spatial configuration can
help detect positional relationships.
The third column shows that the use of statistical dependencies can help
to resolve the ambiguities in the relationship predicates.
Finally, the fourth column shows that for subtle cases,
DR-Net can identify the relationship predicate more accurately than
the config that relies on CRF.


\begin{figure}
    \centering
\includegraphics[width=0.4\textwidth]{imgs/recall_curve.pdf}
    \caption{\small
	This figure shows the recall curves of two possible settings in DR-Net.
        In each setting, we change the number of inference units to see how the recall changes.}
    \label{fig:drnet_curve}
\end{figure}

\paragraph{Compare architectural choices.}

This study is to compare the effect of different choices in the DR-Net architecture.
The choices we study here include:
the number of inference units and whether the relational weights are shared
across these units.
The comparison is conducted on \emph{sVG}.

Figure~\ref{fig:drnet_curve} shows the resultant curves.
From the results we can see:
(1) On both settings, the recall increases as the number of inference units increases.
The best model can improve the recall from  to , as the number
of inference units increases.
With weight sharing, the recall saturates with  inference units;
while without sharing, the recall increases more rapidly, and saturates when it has  inference units.
(2) Generally, with same number of inference units,
the network without weight sharing performs relatively better,
due to the greater expressive power.




\begin{table}
    \centering
   \small
    \begin{tabular}{c|c|c|c|c}
	\toprule
	\multicolumn{5}{c}{Average Similarity} \\
	\midrule
        VR \cite{lu2016visual} & A & S & AS & ASD \\
	\hline
	    0.2076 & 0.2081 & 0.2114 & 0.2170 & \textbf{0.2271} \\
	\bottomrule
    \end{tabular}
    \caption{\small This table lists the average similarities between generated scene graphs and the ground truth.
	All methods are named after their visual relationship detectors.}
    \label{tab:sg_rst}
\end{table}

\begin{figure}
    \centering
\begin{tabular}{c}
    \toprule
    \includegraphics[width=0.42\textwidth]{imgs/sg_4.pdf} \\
    \hline
    \includegraphics[width=0.42\textwidth]{imgs/sg.pdf} \\
    \hline
    \includegraphics[width=0.42\textwidth]{imgs/sg_2.pdf} \\
\bottomrule
    \end{tabular}
    \caption{\small This figure illustrates some images and their corresponding scene graphs.
	The scene graphs are generated according to section \ref{sec:sg}.
        In the scene graphs, the \textbf{black} edges indicate wrong prediction,
        and the \textcolor{red}{red} edges indicate correct prediction.}
    \label{fig:sg_sample}
    \vspace{-1.5mm}
\end{figure}

\subsection{Scene Graph Generation}
\label{sec:sg}

Our model for visual relationship detection can be used for scene graph generation,
which can serve as the basis for many tasks,
\eg~image captioning\cite{anderson2016spice, aditya2015images},
visual question answering\cite{wu2016visual} and image retrieval\cite{johnson2015image}.

The task here is to generate a directed graph for each image
that captures objects, object attributes, and the relationships between them~\cite{johnson2015image}.
See Figure~\ref{fig:sg_sample} for an illustration.
We compared several configs of our method, including \emph{A}, \emph{S}, \emph{AS} and \emph{ASD},
with \emph{VR}~\cite{lu2016visual} on this task,
on a dataset \emph{sVG-a}, which extends \emph{sVG} with attribute annotations.
All methods are augmented with an attribute recognizer.


For each test image, we measure the similarity~\cite{champin2003measuring} between the generated scene graph and the ground truth.
We report average similarity over all test images as our metric.
Table \ref{tab:sg_rst} compares the results of these approaches,
where \emph{ASD} achieves the best result.
This comparison indicates that
with better relationship detection, one can obtain better scene graphs.
