\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{tikz}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[utf8x]{inputenc}
\usepackage{color}
\usepackage{xcolor}
\usepackage{booktabs,siunitx} \usepackage{makecell}
\usepackage{enumitem}
\usepackage[caption=false]{subfig}
\usepackage[noadjust]{cite}
\usepackage{rotating}
\usepackage[misc]{ifsym}
\usepackage{authblk}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}



\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother

\if@mathematic
   \def\vec#1{\ensuremath{\mathchoice
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}}}
\else
   \def\vec#1{\ensuremath{\mathchoice
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}
                     {\mbox{\boldmath}}}}
\fi


\definecolor{road}                {RGB}{128, 64,128}
\definecolor{sidewalk}            {RGB}{244, 35,232}
\definecolor{building}            {RGB}{ 70, 70, 70}
\definecolor{wall}                {RGB}{102,102,156}
\definecolor{fence}               {RGB}{190,153,153}
\definecolor{pole}                {RGB}{153,153,153}
\definecolor{traffic light}       {RGB}{250,170, 30}
\definecolor{traffic sign}        {RGB}{220,220,  0}
\definecolor{vegetation}          {RGB}{107,142, 35}
\definecolor{terrain}             {RGB}{152,251,152}
\definecolor{sky}                 {RGB}{ 70,130,180}
\definecolor{person}              {RGB}{220, 20, 60}
\definecolor{rider}               {RGB}{255,  0,  0}
\definecolor{car}                 {RGB}{  0,  0,142}
\definecolor{truck}               {RGB}{  0,  0, 70}
\definecolor{bus}                 {RGB}{  0, 60,100}
\definecolor{train}               {RGB}{  0, 80,100}
\definecolor{motorcycle}          {RGB}{  0,  0,230}
\definecolor{bicycle}             {RGB}{119, 11, 32}
\definecolor{void}                {RGB}{  0,  0,  0}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vecnorm}[1]{\left\|#1\right\|}
\newtheorem{thm}{Theorem}

\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\yes}{\checkmark}
\newcommand{\no}{}

\newcommand{\best}[1]{\textbf{#1}}
\newcommand{\scnd}[1]{\textit{#1}}

\newcommand{\fnn}[1]{}

\newcommand{\PAR}[1]{\vskip4pt \noindent{\bf #1~}}

\iccvfinalcopy 

\def\iccvPaperID{0921} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation}

\author[1]{Christos Sakaridis}
\author[1]{Dengxin Dai}
\author[1,2]{Luc Van Gool}

\affil[1]{ETH Z\"urich}
\affil[2]{KU Leuven}

\maketitle


\begin{abstract}
Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial \emph{uncertainty} of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions \emph{beyond human recognition capability} in the evaluation in a principled fashion; 3) the \emph{Dark Zurich} dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The state of the art in semantic segmentation is rapidly improving in recent years. Despite the advance, most methods are designed to operate at daytime, under favorable illumination conditions. However, many outdoor applications require robust vision systems that perform well at all times of day, under challenging lighting conditions, and in bad weather~\cite{vision:atmosphere}. Currently, the popular approach to solving perceptual tasks such as semantic segmentation is to train deep neural networks~\cite{pspnet,refinenet,dilated:convolution} using large-scale human annotations~\cite{pascal:2011,Cityscapes,Mapillary}. This supervised scheme has achieved great success for daytime images, but it scales badly to adverse conditions.  In this work, we focus on semantic segmentation at nighttime, both at the method level and the evaluation level. 

At the method level, this work adapts semantic segmentation models from daytime to nighttime, without annotations in the latter domain.
To this aim, we propose a new method called Guided Curriculum Model Adaptation (GCMA). The underpinnings of GCMA are threefold: power of time, power of place, and power of data. \textbf{Time}: environmental illumination changes continuously from daytime to nighttime. This enables adding intermediate domains between the two to smoothly transfer semantic knowledge. This idea is found to be effective in~\cite{SynRealDataFogECCV18,daytime:2:nighttime}; we extend it by adding two more modules. \textbf{Place}: images taken over different time but with the same 6D camera pose share a large portion of content. The shared content can be used to guide the knowledge transfer process from a favorable condition (daytime) to an adverse condition (nighttime). We formalize this observation and propose a solution for large-scale application. \textbf{Data}: GCMA takes advantage of the powerful image translation techniques to stylize large-scale real annotated daytime datasets to darker target domains in order to perform standard supervised learning.

The adversity of nighttime poses further challenges for perceptual tasks compared to daytime. The extracted features become corrupted due to visual hazards~\cite{cv:hazop} such as underexposure, noise, and motion blur. The degradation of affected input regions is often so intense that they are rendered \emph{indiscernible}, i.e.\ determining their semantic content is impossible even for humans. We term such regions as \emph{invalid} for the task of semantic segmentation. A robust model should predict with high \emph{uncertainty} on invalid regions while still being confident on valid (discernible) regions, and a sound evaluation framework should reward such behavior. The above requirement is particularly significant for safety-oriented applications such as autonomous cars, since having the vision system declare a prediction as invalid can help the downstream driving system avoid the fatal consequences of this prediction being false, e.g.\ when a pedestrian is missed.

To this end, we design a generic uncertainty-aware annotation and evaluation framework for semantic segmentation in adverse conditions which explicitly distinguishes invalid from valid regions of input images, and apply it to nighttime. On the annotation side, our novel protocol leverages privileged information in the form of daytime counterparts of the annotated nighttime scenes, which reveal a large portion of the content of invalid regions. This allows to reliably label invalid regions and to indeed \emph{include} invalid regions in the evaluation, contrary to existing semantic segmentation benchmarks~\cite{Cityscapes} which completely exclude them from evaluation. Moreover, apart from the standard class-level semantic annotation, each image is annotated with a mask which designates its invalid regions. On the evaluation side, we allow the \emph{invalid} label in predictions and adopt from~\cite{wilddash} the principle that for invalid pixels with legitimate semantic labels, both these labels and the \emph{invalid} label are considered correct predictions. However, this principle does not cover the case of valid regions. We address this by introducing the concept of false invalid predictions. This enables calculation of \emph{uncertainty-aware intersection-over-union (UIoU)}, a joint performance metric for valid and invalid regions which generalizes standard IoU, reducing to the latter when no invalid prediction exists. UIoU rewards predictions with confidence that is \emph{consistent} to human annotators, i.e.\ with higher confidence on valid regions than invalid ones, meeting the aforementioned requirement.

Finally, we present \emph{Dark Zurich}, a dataset of real images which contains corresponding images of the same driving scenes at daytime, twilight and nighttime. We use this dataset to feed real data to GCMA and to create a benchmark with 151 nighttime images for our uncertainty-aware evaluation. Our dataset and code are publicly available\footnote{\scriptsize{\url{https://trace.ethz.ch/projects/adverse/GCMA_UIoU}}}.


\section{Related Work} 
\label{sec:related} 

\PAR{Vision at Nighttime.}
Nighttime has attracted a lot of attention in the literature due to its ubiquitous nature. Several works pertain to human detection at nighttime, using FIR cameras~\cite{night:vision:pedestrian:05,pedestrian:detection:tracking:night:09}, visible light cameras~\cite{cnn:human:detection:nighttime:17}, or a combination of both~\cite{nighttime:pedestrian:detection:08}. In driving scenarios, a few methods have been proposed to detect cars~\cite{nighttime:object:proposal:18} and vehicles' rear lights~\cite{night:rear:lights:16}. Contrary to these domain-specific methods, previous work also includes both methods designed for robustness to illumination changes, by employing domain-invariant representations~\cite{road:detection:illumination:invariant,outdoor:transformation:labeling:iv15} or fusing information from complementary modalities and spectra~\cite{AdapNet:adverse:17}, and datasets with adverse illumination~\cite{Oxford,localization:benchmarking:adverse} for localization benchmarking.
A recent work~\cite{daytime:2:nighttime} on semantic nighttime segmentation shows that images captured at twilight are helpful for supervision transfer from daytime to nighttime. Our work is partially inspired by~\cite{daytime:2:nighttime} and extends it by proposing a guided curriculum adaptation framework which learns jointly from stylized images and unlabeled real images of increasing darkness and exploits scene correspondences.

\PAR{Domain Adaptation.}
Performance of semantic segmentation on daytime scenes has increased rapidly in recent years. As a consequence, attention is now turning to adaptation to adverse conditions~\cite{AdapNet:adverse:17,benchmark:sensor:adverse:weather:18,wulfmeier2017addressing,continuous:manifold:adaptation}. A case in point are recent efforts to adapt clear-weather models to fog~\cite{SFSU_synthetic,SynRealDataFogECCV18,CMAda:IJCV2019}, by using both labeled synthetic images and unlabeled real images of increasing fog density. This work instead focuses on the nighttime domain, which poses very different and---as we would claim---greater challenges than the foggy domain (e.g.\ artificial light sources casting very different illumination patterns at night). A major class of adaptation approaches, including~\cite{cyCADA,learning:synthetic:data:cvpr18,chen2018road,adapt:structured:output:cvpr18,incremental:adversarial:DA:18,conditional:GAN:adaptation,FCNs:adaptation,conservative:loss:adaptation,DCAN:adaptation,bidirectional:learning:adaptation}, involves adversarial confusion or feature alignment between domains.
The general concept of curriculum learning has been applied to domain adaptation by ordering tasks~\cite{curriculum:domain:adaptation:17} or target-domain pixels~\cite{self:training:adaptation}, while we order domains. Cross-domain correspondences as guidance have only been used very recently in~\cite{cross:season:correspondence}, which requires pixel-level matches, while we use more generic image-level correspondences.

\PAR{Semantic Segmentation Evaluation.}
Semantic segmentation evaluation is commonly performed with the IoU metric~\cite{pascal:2011}. Cityscapes~\cite{Cityscapes} introduced an instance-level IoU (iIoU) to remove the large-instance bias, as well as mean average precision for the task of instance segmentation. The two tasks have recently been unified into panoptic segmentation~\cite{panoptic:segmentation}, with a respective panoptic quality metric. The most closely related work to ours in this regard is WildDash~\cite{wilddash}, which uses standard IoU together with a fine-grained evaluation to measure the impact of visual hazards on performance. In contrast, we introduce UIoU, a new semantic segmentation metric that handles images with regions of uncertain semantic content and is suited for adverse conditions. Our uncertainty-aware evaluation is \emph{complementary} to uncertainty-aware methods such as~\cite{uncertainty:bayesian} that explicitly incorporate uncertainty in their model formulation and aims to promote the development of such methods, as UIoU rewards models that accurately capture heteroscedastic aleatoric uncertainty~\cite{uncertainty:bayesian} in the input images through the different treatment of invalid and valid regions.  



\section{Guided Curriculum Model Adaptation} 
\label{sec:gcma}


\subsection{Problem Formulation}
\label{sec:gcma:general}
GCMA involves a source domain , an ultimate target domain , and an intermediate target domain . In this work,  is daytime,  is nighttime, and  is twilight time with an intermediate level of darkness between  and . GCMA adapts semantic segmentation models through this sequence of domains , which is sorted in ascending order with respect to level of darkness. The approach proceeds progressively and adapts the model from one domain in the sequence to the next. The knowledge is transferred through the domain sequence via this gradual adaptation process. The transfer is performed using two coupled branches: 1) learning from labeled synthetic stylized images and 2) learning from real data without annotations, to jointly leverage the assets of both. Stylized images inherit the human annotations of their original counterparts but contain unrealistic artifacts, whereas real images have less reliable pseudo-labels but are characterized by artifact-free textures.

Let us use  as the index in . 
Once the model for the current domain  is trained, its knowledge can be distilled on unlabeled real data from , and then used, along with a new version of synthetic data from the next domain  to adapt the current model to .

Before diving into the details, we first define all datasets used. 
The inputs for GCMA consist of: 1) a labeled daytime set with  real images , e.g.\ Cityscapes~\cite{Cityscapes}, where  is the ground-truth label of pixel  of ; 2) an unlabeled daytime set of  images ; 3) an unlabeled twilight set of  images ; and 4) an unlabeled nighttime set of  images . In order to perform knowledge transfer with annotated data,  is rendered in the style of  and . We use CycleGAN~\cite{cycleGAN} to perform this style transfer, leading to two more sets:  and , where  and  are the stylized twilight and nighttime version of  respectively, and labels are copied. 
For , the semantic segmentation model  is trained directly on .
In order to perform knowledge transfer with unlabeled data, pseudo-labels for all three unlabeled real datasets need to be generated. The pseudo-labels for  are generated using the model  via . 
For , training  and generating  is performed progressively as GCMA proceeds, as is detailed in Sec.~\ref{sec:gcma:learning}. All six datasets are summarized in Table~\ref{tab:GCMA:notations}.

\setlength{\tabcolsep}{1pt}
\begin{table}
\caption{The training sets used in GCMA.  indicates an image and  its label map;  is a synthetic image and  a pseudo-label map. See the text for details.}
    \label{tab:GCMA:notations}
    \centering
    \footnotesize
    \begin{tabular}{lccc}
 \toprule
&  \multicolumn{2}{c}{Labeled}  & Unlabeled   \\
 & Real & Synthetic & Real  \\
 1. Daytime &  & &    \\
 2. Twilight time & &  &     \\
 3. Nighttime & &  &   \\ 
 \bottomrule
\end{tabular}
\end{table}

\subsubsection{Guided Curriculum Model Adaptation} 
\label{sec:gcma:learning}
Since the method proceeds in an iterative manner, we present the algorithmic details only for a single adaptation step from  to . The presented algorithm is straightforward to generalize to multiple intermediate target domains.
In order to adapt the semantic segmentation model  from the previous domain  to the current domain , we generate synthetic stylized data in domain : .   

For real unlabeled images, since no human annotations are available, we rely on a strategy of self-learning or curriculum learning. Our motivating assumption is that objects are generally easier to recognize in lighter conditions, so the tasks are solved in ascending order with respect to the level of darkness and the easier, solved tasks are used to re-train the model to further solve the harder tasks. This is in line with the concept of curriculum learning~\cite{curriculum:learning}.  
In particular, the model  for domain  can be applied to the unlabeled real images of domain  to generate supervisory labels for training . Specifically, the dataset of real images with pseudo-labels for adaptation to domain  is , where  denotes the predicted labels of image . A simple way to get these labels is by directly feeding  to , similar to the approach of~\cite{SynRealDataFogECCV18,CMAda:IJCV2019} for the case of fog. This choice, however, suffers from accumulation of substantial errors in the prediction of  into the subsequent training step if domain  is not the daytime domain. We instead propose a method to refine these errors by using \emph{guidance} from the semantics of a daytime image  that \emph{corresponds} to , i.e.\ depicts roughly the same scene as  (the difference in the camera pose is small):

where  is a guidance function which will be defined in Sec.~\ref{sec:gcma:guidance} and .  is the correspondence function giving the index of the daytime image that corresponds to .

Once we have the two training sets  (with labels inferred through \eqref{eq:guided:prediction}) and , learning  is performed by optimizing a loss function that involves both datasets:

where  is the cross entropy loss and  is a hyper-parameter balancing the contribution of the two datasets.

In order to leverage the \emph{place} prior at large scale to improve predictions through the guided label refinement defined in \eqref{eq:guided:prediction}, specific aligned datasets need to be compiled. With this aim, we collected the \emph{Dark Zurich} dataset by driving several laps in disjoint areas of Zurich; each lap was driven multiple times during the same day, starting from daytime through twilight to nighttime. The recordings include GPS readings and are split into three sets: daytime, twilight and nighttime (cf.\ Sec.~\ref{sec:dark:zurich}). Since different drives of the same lap correspond to the same route, the camera orientation at a certain point of the lap is similar across all drives. We implement the correspondence function  that assigns to each image in domain  its daytime counterpart using a GPS-based nearest neighbor assignment. The method presented in Sec.~\ref{sec:gcma:guidance} carefully handles the effects of misalignment and dynamic objects in paired images.


\subsection{Guided Segmentation Refinement}
\label{sec:gcma:guidance}

\begin{figure*}
    \centering
    \subfloat[Dark image ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0375_frame_000864_twilight.png}}\label{fig:refinement:twilight}
    \hfil
    \subfloat[Daytime image ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0375_frame_000864_day.png}}\label{fig:refinement:day}
    \hfil
    \subfloat[Initial prediction  for ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0375_frame_000864_labels_init.png}}\label{fig:refinement:init}
    \hfil
    \subfloat[Our refined prediction  for ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0375_frame_000864_labels_ours.png}}\label{fig:refinement:ours}
    \caption{Example pair of corresponding images from \emph{Dark Zurich}, initial prediction for the dark image and our refined prediction.}
    \label{fig:refinement}
\end{figure*}

In the following presentation of our guided segmentation refinement for dark images using corresponding daytime images, we drop for brevity the subscript which was used to indicate this correspondence. The guidance function  which models our refinement approach and was introduced in a  general form in \eqref{eq:guided:prediction} can be written more specifically as

i.e.\ as the composition of a cross bilateral filter  on the daytime predictions, which aligns them to the dark image, with a fusion function , which adaptively combines the aligned daytime predictions with the initial dark image predictions to refine the latter.

\subsubsection{Cross Bilateral Filter for Prediction Alignment}
\label{sec:gcma:guidance:bilateral}

The correspondences between real images that are used in GCMA are not perfect, in the sense that they are not aligned at a pixel-accurate level. Therefore, to leverage the prediction for the daytime image  as \emph{guidance} for refining the respective prediction for the dark image , it is necessary to first align the former prediction to . To this end, we operate on \emph{soft} predictions and define a cross bilateral filter on the initial soft prediction map  which uses the color of the dark image  as reference:

In \eqref{eq:cross:bilateral},  and  denote pixel positions,  is the neighborhood of ,  is the spatial-domain Gaussian kernel and  is the color-domain kernel. The definition of the filter implies that only pixels  with similar color to the examined pixel  in the dark image  contribute to the output , which shifts salient edges in the initial daytime prediction to their correct position in the dark image. For the color-domain kernel, we use the CIELAB version of , as it is more appropriate for measuring color similarity~\cite{bilateral:grid}. We set the spatial parameter  to  to account for large misalignment, and  to  following~\cite{bilateral:grid,SynRealDataFogECCV18}.

\subsubsection{Confidence-Adaptive Prediction Fusion}
\label{sec:gcma:guidance:fusion}

The final step in our refinement approach is to fuse the aligned prediction  for  with the initial prediction  for  in order to obtain the refined prediction , the hard version of which is subsequently used in training. We propose an adaptive fusion scheme, which uses the \emph{confidence} associated with the two predictions at each pixel to weigh their contribution in the output and addresses disagreements due to dynamic content by properly adjusting the fusion weights. Let us denote the confidence of the aligned prediction  for  at pixel  by  and respectively the confidence of the initial prediction  for  by . Our confidence-adaptive fusion is then defined as

where  may vary and we have completely dropped the pixel argument  for brevity. In this way, we allow the daytime image prediction to have a greater effect on the output at regions of the dark image which were not easy for model  to classify, while preserving the initial prediction  at lighter regions of the dark image where  is more reliable.

Our fusion distinguishes between dynamic and static scene content by regulating . In particular,  downweights  to induce a preference towards  when both predictions have high confidence. However, apart from imperfect alignment, the two scenes also differ due to dynamic content. Intuitively, the prediction of a dynamic object in the daytime image should be assigned an even lower weight in case the corresponding prediction in the dark image does not agree, since this object might only be present in the former scene. More formally, we denote the subset of  that includes dynamic classes by  and define

In our experiments, we manually tune ,  and  on a couple of training images (no grid search). A result of our guided refinement is shown in Fig.~\ref{fig:refinement}.



\section{Uncertainty-Aware Evaluation}
\label{sec:evaluation} 

Images taken under adverse conditions such as nighttime contain invalid regions, i.e.\ regions with indiscernible semantic content. Invalid regions are closely related to the concept of negative test cases which was considered in~\cite{wilddash}. However, invalid regions constitute intra-image entities and can co-exist with valid regions in the same image, whereas a negative test case refers to an entire image that should be treated as invalid. We build upon the evaluation of~\cite{wilddash} for negative test cases and generalize it to be applied uniformly to all images in the evaluation set, whether they contain invalid regions or not. Our annotation and evaluation framework includes invalid regions in the set of evaluated pixels, but treats them differently from valid regions to account for the high uncertainty of their content. In the following, we elaborate on the generation of ground-truth annotations using privileged information through the day-night correspondences of our dataset and present our UIoU metric.

\subsection{Annotation with Privileged Information}
\label{sec:evaluation:annotation}



\begin{figure*}
    \centering
    \subfloat[Input image ]{\includegraphics[width=0.24\textwidth]{figures/GP010364_frame_000074_image.png}}\label{fig:annotation:input}
    \hfil
    \subfloat[Auxiliary image ]{\includegraphics[width=0.24\textwidth]{figures/GP010364_frame_000074_daytime.png}}\label{fig:annotation:auxiliary}
    \hfil
    \subfloat[GT invalid mask ]{\includegraphics[width=0.24\textwidth]{figures/GP010364_frame_000074_gtInvalidBlended.png}}\label{fig:annotation:invalid}
    \hfil
    \subfloat[GT semantic labeling ]{\includegraphics[width=0.24\textwidth]{figures/GP010364_frame_000074_gt.png}}\label{fig:annotation:gt}
    \caption{Example input images from \emph{Dark Zurich-test} and output annotations with our protocol. Valid pixels in  are marked green.}
    \label{fig:annotation}
\end{figure*}

For each image , the annotation process involves two steps: 1) creation of the ground-truth invalid mask , and 2) creation of the ground-truth semantic labeling .

For the semantic labels, we consider a predefined set  of  classes, which is equal to the set of Cityscapes~\cite{Cityscapes} evaluation classes (). The annotator is first presented only with  and is asked to mark the valid regions in it as the regions which she can unquestionably assign to one of the  classes or declare as not belonging to any of them. The result of this step is the invalid mask , which is set to 0 at valid pixels and 1 at invalid pixels.

Secondly, the annotator is asked to mark the semantic labels of , only that this time she also has access to an \emph{auxiliary} image . This latter image has been captured with roughly the same 6D camera pose as  but under more favorable conditions. In our dataset,  is captured at daytime whereas  is captured at nighttime. The large overlap of static scene content between the two images allows the annotator to label certain regions in  with a legitimate semantic label from , even though the same regions have been annotated as invalid (and are kept as such) in . This allows joint evaluation on valid and invalid regions, as it creates regions which can accept both the \emph{invalid} label and the ground-truth label from  as correct predictions. Due to the imperfect match of the camera poses for  and , the labeling of invalid regions in  is done conservatively, marking a coarse boundary which may leave unlabeled zones around the true semantic boundaries in , so that no pixel is assigned a wrong label. The parts of  which remain indiscernible even after inspection of  are left unlabeled in . These parts as well as instances of classes outside  are not considered during evaluation. We illustrate a visual example of our annotation inputs and outputs in Fig.~\ref{fig:annotation}.

\subsection{Uncertainty-Aware Predictions}



The semantic segmentation prediction that is fed to our evaluation is expected to include pixels labeled as \emph{invalid}. Instead of defining a separate, explicit \emph{invalid} class, which would potentially require the creation of new training data to incorporate this class, we allow a more flexible approach for soft predictions with the original set of semantic classes by using a \emph{confidence threshold}, which affords an evaluation curve for our UIoU metric by varying this threshold.

In particular, we assume that the evaluated method outputs an intermediate soft prediction  at each pixel  as a probability distribution among the  classes, which is subsequently converted to a hard assignment by outputting the class  with the highest probability. In this case,  is the effective confidence associated with the prediction. This assumption is not very restrictive, as most recent semantic segmentation methods are based on CNNs with a softmax layer that outputs such soft predictions.

The final evaluated output  is computed based on a free parameter  which acts as a confidence threshold by invalidating those pixels where the confidence of the prediction is lower than , i.e.\  if  and \emph{invalid} otherwise. Increasing  results in more pixels being predicted as \emph{invalid}. This approach is motivated by the fact that ground-truth invalid regions are identified during annotation by the uncertainty of their semantic content, which implies that a model should ideally place lower confidence (equivalently higher uncertainty) in predictions on invalid regions than on valid ones, so that the former get invalidated for lower values of  than the latter. The formulation of our UIoU metric rewards this behavior as we shall see next. Note that our evaluation does not strictly require soft predictions, as UIoU can be normally computed for fixed, hard predictions .

\subsection{UIoU}



We propose UIoU as a generalization of the standard IoU metric for evaluation of semantic segmentation predictions which may contain pixels labeled as \emph{invalid}. UIoU reduces to standard IoU if no pixel is predicted to be invalid, e.g.\ when .

The calculation of UIoU for class  involves five sets of pixels, which are listed along with their symbols: true positives (TP), false positives (FP), false negatives (FN), true invalids (TI), and false invalids (FI). Based on the ground-truth invalid masks , the ground-truth semantic labelings  and the predicted labels  for the set of evaluation images, these five sets are defined as follows:

UIoU for class  is then defined as

Note that a true invalid prediction results in equal reward to predicting the correct semantic label of the pixel. Moreover, an invalid prediction does not come at no cost: it incurs the same penalty on valid pixels as predicting an incorrect label.

When dealing with multiple classes, we modify our notation to  (similarly for the five sets of pixels related to class ), which we avoided in the previous definitions to reduce clutter. The overall semantic segmentation performance on the evaluation set is reported as the mean UIoU over all  classes. By varying the confidence threshold  and using the respective output, we obtain a parametric expression . When , no pixel is predicted as invalid and thus .

We motivate the usage of UIoU instead of standard IoU in case the test set includes ground-truth invalid masks by showing in Th.~\ref{thm:UIoU:greater:iou} that UIoU is guaranteed to be larger than IoU for some  under the assumption that predictions on invalid regions are associated with lower confidence than those on valid regions, which lies in the heart of our evaluation framework. The proof is in Appendix~\ref{supp:sec:proof}.

\begin{thm} \label{thm:UIoU:greater:iou}
Assume that there exist ,  such that ,  and . If we additionally assume that , then .
\end{thm}


\section{The Dark Zurich Dataset}
\label{sec:dark:zurich}

\begin{table}[!tb]
    \caption{Comparison of \emph{Dark Zurich} against related datasets with nighttime semantic annotations. ``Night annot.'': annotated nighttime images, ``Invalid'': can invalid regions get legitimate labels?}
    \label{table:datasets:comparison}
    \centering
    \footnotesize
    \setlength\tabcolsep{1.2pt}
    \begin{tabular}{lccccc}
    \toprule
    Dataset & Night annot. & Classes & Reliable GT & Fine GT & Invalid \\
    \midrule
    WildDash~\cite{wilddash} & 13 & 19 & \yes & \yes & \no \\
    Raincouver~\cite{raincouver} & 95 & 3 & \yes & \no & \no \\
    BDD100K~\cite{BDD100K} & 345 & 19 & \no & \yes & \no \\
    Nighttime Driving~\cite{daytime:2:nighttime} & 50 & 19 & \yes & \no & \no \\
    Dark Zurich & 151 & 19 & \yes & \yes & \yes \\
    \bottomrule
    \end{tabular}
\end{table}

\emph{Dark Zurich} was recorded in Zurich using a 1080p GoPro Hero 5 camera, mounted on top of the front windshield of a car. The collection protocol with multiple drives of several laps to establish correspondences is detailed in Sec.~\ref{sec:gcma}.

We split \emph{Dark Zurich} and reserve one lap for testing. The rest of the laps remain unlabeled and are used for training. They comprise 3041 daytime, 2920 twilight and 2416 nighttime images extracted at 1 fps, which are named \emph{Dark Zurich}-\{\emph{day}, \emph{twilight}, \emph{night}\} respectively and correspond to the three sets in the rightmost column of Table~\ref{tab:GCMA:notations}. From the testing night lap, we extract one image every 50m or 20s, whichever comes first, and assign to it the corresponding daytime image to serve as the auxiliary image  in our annotation (cf.\ Sec.~\ref{sec:evaluation:annotation}). We annotate 151 nighttime images with fine pixel-level Cityscapes labels and invalid masks following our protocol and name this set \emph{Dark Zurich-test}. In total, 272.2M pixels have been annotated with semantic labels and 56.7M of these pixels are marked as invalid. We validate the quality of our annotations by having 20 images annotated twice by different subjects and measuring consistency. 93.5\% of the labeled pixels are consistent in the semantic annotations and respectively 95\% in the invalid masks.
We compare to existing annotated nighttime sets in Table~\ref{table:datasets:comparison}, noting that most large-scale sets for road scene parsing, such as Cityscapes~\cite{Cityscapes} and Mapillary Vistas~\cite{Mapillary}, contain few or no nighttime scenes. Nighttime Driving~\cite{daytime:2:nighttime} and Raincouver~\cite{raincouver} only include \emph{coarse} annotations. \emph{Dark Zurich-test} contains ten times more nighttime images than WildDash~\cite{wilddash}---the only other dataset with \emph{reliable} fine nighttime annotations. Detailed inspection showed that 70\% of the 345 densely annotated nighttime images of BDD100K~\cite{BDD100K} contain severe labeling errors which render them unsuitable for evaluation, especially in dark regions we treat as invalid (e.g.\ \emph{sky} is often mislabeled as \emph{building}). Our annotation protocol helps avoid such errors by properly defining invalid regions and using daytime images to aid annotation, and \emph{Dark Zurich-test} is an initial high-quality benchmark to promote our uncertainty-aware evaluation.


\section{Results}
\label{sec:exp}

\begin{figure*}[!tb]
    \centering
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000301_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000301_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000301_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000301_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000301_ours.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000755_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000755_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000755_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000755_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000755_ours.png}}
    \\
    \vspace{-0.3cm}
    \addtocounter{subfigure}{-10}
    \subfloat[Image]{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000143_image.png}}
    \hfil
    \subfloat[Semantic GT]{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000143_gt.png}}
    \hfil
    \subfloat[AdaptSegNet~\cite{adapt:structured:output:cvpr18}]{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000143_adaptsegnet.png}}
    \hfil
    \subfloat[DMAda~\cite{daytime:2:nighttime}]{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000143_dmada.png}}
    \hfil
    \subfloat[GCMA (Ours)]{\includegraphics[width=0.195\textwidth]{figures/GOPR0364_frame_000143_ours.png}}
    \caption{Qualitative semantic segmentation results on \emph{Dark Zurich-test}. ``AdaptSegNet'' adapts from Cityscapes to \emph{Dark Zurich-night}.}
    \label{fig:sem:seg}
\end{figure*}

\begin{table*}[!tb]
  \caption{Comparison on \emph{Dark Zurich-test}. AdaptSegNet-Cityscapes\emph{DZ-night} denotes adaptation from Cityscapes to \emph{Dark Zurich-night}.}
  \label{table:exp:main:dark_zurich}
  \centering
  \setlength\tabcolsep{1.9pt}
  \footnotesize
  \begin{tabular}{lcccccccccccccccccccc}
  \toprule
  Method & \ver{road} & \ver{sidew.} & \ver{build.} & \ver{wall} & \ver{fence} & \ver{pole} & \ver{light} & \ver{sign} & \ver{veget.} & \ver{terrain} & \ver{sky} & \ver{person} & \ver{rider} & \ver{car} & \ver{truck} & \ver{bus} & \ver{train} & \ver{motorc.} & \ver{bicycle} & mIoU\\
  \midrule
  RefineNet~\cite{refinenet} & 68.8&23.2&46.8&20.8&12.6&29.8&30.4&26.9&43.1&14.3&0.3&36.9&49.7&63.6&6.8&\best{0.2}&24.0&33.6&9.3 & 28.5\\
  AdaptSegNet-Cityscapes~\cite{adapt:structured:output:cvpr18} & 79.0 & 21.8 & 53.0 & 13.3 & 11.2 & 22.5 & 20.2 & 22.1 & 43.5 & 10.4 & 18.0 & 37.4 & 33.8 & 64.1 & 6.4 & 0.0 & \best{52.3} & 30.4 & 7.4     & 28.8\\
  \midrule
  AdaptSegNet-Cityscapes\emph{DZ-night}~\cite{adapt:structured:output:cvpr18} & \best{86.1} & 44.2 & 55.1 & \best{22.2} & 4.8 & 21.1 & 5.6 & 16.7 & 37.2 & 8.4 & 1.2 & 35.9 & 26.7 & 68.2 & \best{45.1} & 0.0 & 50.1 & 33.9 & 15.6       & 30.4\\
  DMAda~\cite{daytime:2:nighttime} & 75.5&29.1&48.6&21.3&14.3&34.3&36.8&29.9&49.4&13.8&0.4&43.3&\best{50.2}&69.4&18.4&0.0&27.6&\best{34.9}&11.9             & 32.1\\
  Ours: GCMA & 81.7&\best{46.9}&\best{58.8}&22.0&\best{20.0}&\best{41.2}&\best{40.5}&\best{41.6}&\best{64.8}&\best{31.0}&\best{32.1}&\best{53.5}&47.5&\best{75.5}&39.2&0.0&49.6&30.7&\best{21.0}             & \best{42.0}\\
  \bottomrule
  \\
  \end{tabular}
\end{table*}

\begin{table}[!tb]
  \caption{Comparison on Nighttime Driving~\cite{daytime:2:nighttime}. Read as Table~\ref{table:exp:main:dark_zurich}.}
  \label{table:exp:main:nighttime_driving}
  \centering
  \setlength\tabcolsep{4pt}
  \footnotesize
  \begin{tabular}{lc}
  \toprule
  Method & mIoU (\%)\\
  \midrule
  RefineNet~\cite{refinenet} & 31.5\\
  AdaptSegNet-Cityscapes~\cite{adapt:structured:output:cvpr18} & 32.6\\
  \midrule
  AdaptSegNet-Cityscapes\emph{DZ-night}~\cite{adapt:structured:output:cvpr18} & 34.5\\
  DMAda~\cite{daytime:2:nighttime} & 36.1\\
  Ours: GCMA & \best{45.6}\\
  \bottomrule
  \\
  \end{tabular}
\end{table}

Our architecture of choice for implementing GCMA is RefineNet~\cite{refinenet}. We use the publicly available \emph{RefineNet-res101-Cityscapes} model, trained on Cityscapes, as the baseline model to be adapted to nighttime. Throughout our experiments, we train this model with a constant learning rate of  on mini-batches of size 1.

\PAR{Comparison to Other Adaptation Methods.}
Our first experiment compares GCMA to state-of-the-art approaches for adaptation of semantic segmentation models to nighttime. To obtain the synthetic labeled datasets for GCMA, we stylize Cityscapes to twilight using a CycleGAN model that is trained to translate Cityscapes to \emph{Dark Zurich-twilight} (respectively to nighttime with \emph{Dark Zurich-night}). The real training datasets for GCMA are \emph{Dark Zurich-day}, instantiating , and \emph{Dark Zurich-twilight}, instantiating . Each adaptation step comprises 30k SGD iterations and uses . For the second step, we apply our guided refinement to the labels of \emph{Dark Zurich-twilight} that are predicted by model  fine-tuned in the first step, using the correspondences of \emph{Dark Zurich-twilight} to \emph{Dark Zurich-day}.

We evaluate GCMA on \emph{Dark Zurich-test} against the state-of-the-art adaptation approaches AdaptSegNet~\cite{adapt:structured:output:cvpr18} and DMAda~\cite{daytime:2:nighttime} and report standard IoU performance in Table~\ref{table:exp:main:dark_zurich}, including \emph{invalid} pixels which are assigned a legitimate semantic label in the evaluation. We have trained AdaptSegNet to adapt from Cityscapes to \emph{Dark Zurich-night}. For fair comparison, we also report the performance of the respective baseline Cityscapes models for each method. RefineNet is the common baseline of GCMA and DMAda. GCMA significantly outperforms the other methods for most classes and achieves a substantial 10\% improvement in the overall mIoU score against the second-best method. The improvement with GCMA is pronounced for classes which usually appear dark at nighttime, such as \emph{sky}, \emph{vegetation}, \emph{terrain} and \emph{person}, indicating that our method successfully handles large domain shifts from its source daytime domain. These findings are supported by visually assessing the predictions of the compared methods, as in the examples of Fig.~\ref{fig:sem:seg}. We repeat the above comparison on Nighttime Driving~\cite{daytime:2:nighttime} in Table~\ref{table:exp:main:nighttime_driving} and show that GCMA generalizes very well to different datasets.

\begin{table}[!tb]
  \caption{Ablations of GCMA on \emph{Dark Zurich-test}, reporting mIoU.}
  \label{table:exp:ablation:dark_zurich}
  \centering
  \setlength\tabcolsep{4pt}
  \footnotesize
  \begin{tabular}{lc}
  \toprule
  Daytime baseline: RefineNet~\cite{refinenet} & 28.5\%\\
  +direct CycleGAN adapt.\ (w/o real, w/o curriculum) & 37.1\%\\
  +GCMA w/o guided refinement & 39.4\%\\
  +GCMA w/ guided refinement & \best{42.0\%}\\
  \bottomrule
  \\
  \end{tabular}
\end{table}

\PAR{Ablation Study for GCMA.}
We measure the individual effect of the main components of GCMA in Table~\ref{table:exp:ablation:dark_zurich} by evaluating its ablated versions on \emph{Dark Zurich-test}. Direct adaptation to nighttime in a single step using only Cityscapes images stylized as nighttime with CycleGAN is a strong baseline, due to the reliable ground-truth labels that accompany the stylized Cityscapes, its high diversity and the limited artifacts of CycleGAN-based translation. Adding our real images to the training algorithm and applying our two-stage curriculum significantly improves upon this baseline. Finally, our guided segmentation refinement in the second step of GCMA brings an additional 2.6\% benefit, as it corrects a lot of errors in the pseudo-labels of the real twilight images, which helps compute more reliable gradients from the corrected loss during the subsequent training.

\begin{figure}[!tb]
    \centering
    \includegraphics[clip,width=\linewidth,trim=25mm 100mm 25mm 100mm]{figures/mUIoU_comparison_GCMA_new.pdf}
    \caption{Uncertainty-aware evaluation of RefineNet~\cite{refinenet}, DMAda~\cite{daytime:2:nighttime} and GCMA on \emph{Dark Zurich-test}. We evaluate mean UIoU across the entire range  of confidence threshold . For each method, the point at which mean UIoU is maximized is marked black and labeled with this maximum mean UIoU value.}
    \label{fig:exp:uiou}
\end{figure}

\PAR{Comparisons with UIoU.}
In Fig.~\ref{fig:exp:uiou}, we use our novel UIoU metric to evaluate GCMA against DMAda and our baseline RefineNet model on \emph{Dark Zurich-test} for varying confidence threshold  and plot the resulting mean UIoU curves. Note that standard mean IoU can be read out from the leftmost point of each curve. First, our expectation based on Th.~\ref{thm:UIoU:greater:iou} is confirmed for all methods, i.e.\ maximum UIoU values over the range of  are larger than IoU by ca.\ 3\%. This implies that on \emph{Dark Zurich-test}, these models generally have lower confidence on invalid regions than valid ones. Second, the comparative performance of the methods is the same across all values of  ---GCMA substantially outperforms the other two---which shows that UIoU is generally consistent with standard IoU and is a suitable substitute of the latter in adverse settings where declaring the input as invalid is relevant.


\section{Conclusion}
\label{sec:conclusion}

In this paper, we have introduced GCMA, a method to gradually adapt semantic segmentation models from daytime to nighttime with stylized data and unlabeled real data of increasing darkness, as well as UIoU, a novel evaluation metric for semantic segmentation designed for images with indiscernible content. We have also presented \emph{Dark Zurich}, a large-scale dataset of real scenes captured at multiple times of day with cross-time-of-day correspondences, and annotated 151 nighttime scenes of it with a new protocol which enables our evaluation. Detailed evaluation with standard IoU on real nighttime sets demonstrates the merit of GCMA, which substantially improves upon competing state-of-the-art methods. Finally, evaluation on our benchmark with UIoU shows that invalidating predictions is useful when the input includes ambiguous content.

\PAR{Acknowledgements.}
This work is funded by Toyota Motor Europe via the research project TRACE-Z\"urich. We thank Simon Hecker for his advice on decoding GoPro GPS data.

{\small
\bibliographystyle{ieee}
\bibliography{refs}
}

\appendix
\clearpage
\pagenumbering{roman}

\section{Proof of Theorem 1}
\label{supp:sec:proof}

\begin{proof}
For brevity in the proof, we drop the class superscript  which is used in the statement of the theorem.

Firstly, we draw an association between pixel sets related to the standard  and their counterparts for UIoU defined in \eqref{eq:tp}--\eqref{eq:fi}. In particular, the following holds true:

The first assumption of Th.~\ref{thm:UIoU:greater:iou} implies that , because  (including ) there exists no false invalid pixel for the examined class. Thus, applying \eqref{eq:proof:true:positive:false:negative:iou} for  leads to


Secondly, we plug the proposition of the first assumption of the theorem into the proposition of the second assumption to obtain

We further elaborate on \eqref{eq:proof:nonempty:improved:init} by observing that ,  and  to arrive at


Both terms on the left-hand side of \eqref{eq:proof:nonempty:improved} are nonnegative based on our previous observations, while at the same time \eqref{eq:proof:nonempty:improved} implies that at least one of the two is strictly positive. To complete the proof, we distinguish between the two corresponding cases.

\begin{figure*}[!tb]
    \centering
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_image.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_image.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_gtInvalid.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_gtInvalid.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_gtInvalid.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_gtInvalid.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_gtInvalid.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_gt.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_gt.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_adaptsegnet.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_adaptsegnet.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_dmada.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_dmada.png}}
    \\
    \vspace{-0.3cm}
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000382_ours.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000336_ours.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000188_ours.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000151_ours.png}}
    \hfil
    \subfloat{\includegraphics[width=0.195\textwidth]{figures/GP010364_frame_000018_ours.png}}
    \caption{Examples of our annotations and qualitative semantic segmentation results on \emph{Dark Zurich-test}. From top to bottom row: nighttime image, invalid mask annotation overlaid on the image (valid pixels are colored green), semantic annotation, AdaptSegNet~\cite{adapt:structured:output:cvpr18}, DMAda~\cite{daytime:2:nighttime}, and GCMA (ours).}
    \label{fig:sem:seg:extra}
\end{figure*}

In the first case, the first term in \eqref{eq:proof:nonempty:improved} is strictly positive, so \eqref{eq:proof:true:positive:iou:theta1} implies

We establish the inequality we are after by writing

where we have used the definition of IoU in the second line, \eqref{eq:proof:true:positive:false:negative:iou} in the third line,  in the fourth line, \eqref{eq:proof:correct:predict:ineq:strict} in the fifth line, and the definition of UIoU that has been introduced in \eqref{eq:uiou} in the last line.

In the second case, the second term in \eqref{eq:proof:nonempty:improved} is strictly positive, which implies that

Besides, applying the nonnegativity of the first term in \eqref{eq:proof:nonempty:improved} to \eqref{eq:proof:true:positive:iou:theta1} leads to

Similarly to the first case, we establish the inequality we are after by writing

where we have used the definition of IoU as well as \eqref{eq:proof:true:positive:false:negative:iou} in the second line, \eqref{eq:proof:fp:ineq} in the third line, \eqref{eq:proof:correct:predict:ineq} in the fourth line, and the definition of UIoU in the last line.
\end{proof}

\section{Additional Qualitative Results}
\label{supp:sec:results}

In Fig.~\ref{fig:sem:seg:extra}, we compare our GCMA approach against AdaptSegNet~\cite{adapt:structured:output:cvpr18} and DMAda~\cite{daytime:2:nighttime} on additional images from \emph{Dark Zurich-test}, further demonstrating the superiority of GCMA. For these images, we also present our annotations for invalid masks and semantic labels, which show that a significant portion of ground-truth invalid regions is indeed assigned a reliable semantic label through our annotation protocol and can thus be included in the evaluation.

\begin{figure}
    \centering
    \subfloat[: Cityscapes]{\includegraphics[height=0.115\textwidth]{figures/dusseldorf_000048_000019_leftImg8bit.png}}
    \hfil
    \subfloat[: \emph{Dark Zurich-day}]{\includegraphics[height=0.115\textwidth]{figures/GOPR0345_frame_000241_day.png}}
    \\
    \vspace{-0.3cm}
    \subfloat[: Cityscapes-twilight style]{\includegraphics[height=0.115\textwidth]{figures/dusseldorf_000048_000019_leftImg8bit_twilight.png}}
    \hfil
    \subfloat[: \emph{Dark Zurich-twilight}]{\includegraphics[height=0.115\textwidth]{figures/GOPR0348_frame_000120_twilight.png}}
    \\
    \vspace{-0.3cm}
    \subfloat[: Cityscapes-nighttime style]{\includegraphics[height=0.115\textwidth]{figures/dusseldorf_000048_000019_leftImg8bit_night.png}}
    \hfil
    \subfloat[: \emph{Dark Zurich-night}]{\includegraphics[height=0.115\textwidth]{figures/GOPR0351_frame_000130_night.png}}
    \caption{Sample images from the training sets used in GCMA.}
    \label{fig:training:datasets}
\end{figure}

\section{Configuration of Training Sets for GCMA}
\label{supp:sec:training:datasets}

In Fig.~\ref{fig:training:datasets}, we show examples from the six training sets we introduced in Sec.~\ref{sec:gcma:general}, which are used for implementing GCMA. Cityscapes is used to instantiate the labeled sets, while \emph{Dark Zurich} is used for the unlabeled sets.

More examples of Cityscapes images stylized to nighttime using a CycleGAN model~\cite{cycleGAN} that is trained to translate Cityscapes to \emph{Dark Zurich-night} are presented in Fig.~\ref{fig:stylized:night:examples}.

\begin{figure*}[!tb]
    \centering
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/aachen_000127_000019_leftImg8bit.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/stuttgart_000117_000019_leftImg8bit.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/aachen_000122_000019_leftImg8bit.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/stuttgart_000120_000019_leftImg8bit.png}}
    \\
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/aachen_000127_000019_leftImg8bit_night.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/stuttgart_000117_000019_leftImg8bit_night.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/aachen_000122_000019_leftImg8bit_night.png}}
    \hfil
    \subfloat{\includegraphics[width=0.24\textwidth]{figures/stuttgart_000120_000019_leftImg8bit_night.png}}
    \caption{Top row: Examples of images from Cityscapes ( in GCMA), bottom row: corresponding images from Cityscapes-nighttime style ( in GCMA).}
    \label{fig:stylized:night:examples}
\end{figure*}

\section{Parameter Selection for Prediction Fusion}
\label{supp:sec:params:fusion}

For our confidence-adaptive prediction fusion, we demonstrate the benefit of selecting ---the rationale of which is exposed in Sec.~\ref{sec:gcma:guidance:fusion}---through a visual example in Fig.~\ref{fig:params:fusion:comparison}.

\begin{figure*}[!tb]
    \centering
    \subfloat[Dark image ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0348_frame_000120_twilight_box.png}}
    \hfil
    \subfloat[]{\includegraphics[width=0.24\textwidth]{figures/GOPR0348_frame_000120_twilight_labels_aL1_aH1_crop.png}\label{fig:params:fusion:comparison:a1}}
    \hfil
    \subfloat[]{\includegraphics[width=0.24\textwidth]{figures/GOPR0348_frame_000120_twilight_labels_aL06_aH06_crop.png}\label{fig:params:fusion:comparison:a06}}
    \hfil
    \subfloat[, , ]{\includegraphics[width=0.24\textwidth]{figures/GOPR0348_frame_000120_twilight_labels_aL03_aH06_theta02.png}\label{fig:params:fusion:comparison:aL03aH06}}
    \caption{Dark image  from \emph{Dark Zurich} and our refined predictions  for the region indicated by the red box for different values of the parameters involved in the proposed confidence-adaptive prediction fusion. When , reducing  to a value lower than , e.g.\ \protect\subref{fig:params:fusion:comparison:a1}\protect\subref{fig:params:fusion:comparison:a06}, reduces false positives and/or false negatives both for static and dynamic classes, e.g.\ \emph{pole}, \emph{sidewalk}, \emph{road} and \emph{car}. When , reducing  to a value lower than , e.g.\ \protect\subref{fig:params:fusion:comparison:a06}\protect\subref{fig:params:fusion:comparison:aL03aH06}, improves accuracy on pixels that are assigned to a dynamic class in either prediction, e.g.\ \emph{car}, because of the formulation of \eqref{eq:alpha}. Best viewed with zoom.}
    \label{fig:params:fusion:comparison}
\end{figure*}

\end{document}
