
\documentclass{article} \usepackage{iclr_conference,times}

\usepackage{graphicx}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}



\title{Attend to who you are: Supervising self-attention for Keypoint Detection and Instance-Aware Association}


	\author{
		\centerline{Sen Yang\thanks{This work was done when Sen Yang was intern at MEGVII Tech.}, Zhicheng Wang, Ze Chen, Yanjie Li, Shoukui Zhang,}\\
		 \centerline{\textbf{Zhibin Quan, Shu-Tao Xia, Yiping Bao, Erjin Zhou, Wankou Yang}}\
    \mathcal{L}_{heatmap}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{M} \cdot\left\| \mathbf{\hat{H}}_k-\mathbf{H}_k\right\|,

  \mathbf{A}_p=\frac{1}{\sum_{i=1}^Kv_p^i}\sum_{k=1}^K  v_p^k\cdot\tA[int(y_p^k/r),int(x_p^k/r),:,:].

    \mathcal{L}_{mask}=\operatorname{MSE}(\operatorname{bilinear}(\mathbf{A}_p/\operatorname{max}(\mathbf{A}_p)),\mathbf{M}_p)=\frac{1}{N}\sum_{p=1}^N\left\|\operatorname{bilinear}(\mathbf{A}_p/\operatorname{max}({\mathbf{A}_p}))-\mathbf{M}_p\right\|.

    \mathcal{L}_{train}=\alpha\cdot\mathcal{L}_{heatmap}+\beta\cdot\mathcal{L}_{mask},

\operatorname{Attraction}(p_{c},\mathcal{S}_f)=\frac{1}{|\mathcal{S}_f|}\sum_{(x',y', s')\in \mathcal{S}_f}s'\cdot\tA[y,x,y',x'].

Thus the candidate point with the highest  is considered to belong to the current skeleton : 

We repeat the process above and record all the matched keypoints until all keypoints of this skeleton have been found.
Then we need to decode the next skeleton. 
We pop the first unmatched keypoint to seed a new skeleton  again. 
We follow the previous steps to find keypoints belonging to this instance. 
Note if the  is smaller than a threshold  (empirically set to 0.0025), this type of keypoint in this skeleton to be empty (zero-filling). 
It is also worth noting that we also consider the keypoints that have already been claimed by a previous skeleton , but only when , we assign the matched  to the current skeleton .

\textbf{Part-first view}. This view aims to decode all human skeletons part-by-part. 
Given all candidates for each keypoint type, we initialize multiple skeleton seeds  with the most easily detected keypoints such as nose. Then we follow a fixed order to connect the candidate parts to the current skeletons. These skeletons can be seen as multiple clusters consisting of found keypoints. 
 Like the \emph{body-first} view, we also use the mean attention attraction  from the found keypoints in the skeletons as the metric to assign the candidate parts (Figure~\ref{fig:group}). But in the \emph{part-first} view, we compute the pairwise distance matrix between the candidate parts and existing skeletons, and then we use the Hungarian algorithm~\citep{hungarian:kuhn1955hungarian} to solve this bipartite graph matching problem. Note, if an  that represents a matching in the solution is lower than a threshold , we use this corresponding candidate part to start a new skeleton seed. We repeat the process above until all types of candidate parts have been assigned. This part-first grouping algorithm can achieve the optimal solution for assigning local parts to the skeletons although it cannot guarantee the global optimal assignment. We choose the part-first grouping as the default. And we compare both algorithms on the performance, complexity and runtime in Appendix~\ref{runtime}. 
 
 
 \subsection{Mask Prediction}
The instance masks are easy to obtain after the detected keypoints have been grouped into skeletons. To produce the instance segmentation results, we sample the visible keypoint locations  of the -th instance from the supervised self-attention matrix:
. Then we achieve the estimated instance mask: , where  is a threshold (0.4 by default) to determine the mask region. When we obtain the initial skeletons and masks for all person instances, the joints of a person may fall in multiple incomplete skeletons, but their corresponding segments (sampled attention areas) may overlap. Thus we further perform non-maximum suppression to merge instances if the Intersection-over-Max (IoM) of two masks exceeds 0.3, where Max denotes the maximum area between two masks.



\section{Experiments}


\textbf{Dataset}. We evaluate our method on the COCO keypoint dectection challenge~\citep{coco:lin2014microsoft} and on the instance segmentation of the COCO person category.


\textbf{Model setup}.
 We follow the model architecture design of TransPose~\citep{transpose:yang2021transpose}\footnote{\url{https://github.com/yangsenius/TransPose}} to predict the keypoint heatmaps.
 The setup is built on top of pre-existing ResNet and Transformer Encoder. We use the Imagenet pre-trained ResNet-101 or ResNet-151 as the backbone whose final classification layer is replaced by a  convolution to reduce the channels from 2048 to  (192).
The normal output stride of ResNet backbone is 32 but we increase the feature map resolution of its final stage (C5 stage) by adding the dilation and removing the stride, i.e., the downsampling ratio  of ResNet is 16. 
We use a regular Transformer with 6 encoder layers with a single attention head for each layer. The hidden dimension of FFN is 384. See more training and inference details in Appendix~\ref{Appendix}.


\subsection{Results on COCO keypoint detection and person instance segmentation}



\begin{table}[t]
	\caption{Results on the COCO validation set. (res101, s16, i640) represents that we use ResNet-101; the output stride is 16; the input resolution is 640640. R\#1 represents only refining the keypoints without filling. R\#2 represents refining the keypoints with filling zero-score keypoints. }
	\label{table:coco-val}
	\begin{center}
		\resizebox{0.90\columnwidth}{!}{
			\begin{tabular}{lcccccc}
				\toprule
				\textbf{Method}  &AP &  &  &  &  &  \\\midrule
				OpenPose~\citep{openpose:cao2017realtime} & 58.4 & 81.5 & 62.6 & 54.4 & 65.1 & - \\

				OpenPose + Refinement~\citep{openpose:cao2017realtime} & 61.0 & 84.9 & 67.5 & 56.3 & 69.3 & - \\PersonLab (res101, s16, i601)~\citep{personlab:papandreou2018personlab}&53.2 &76.0 &56.3 &38.6 &73.1 &57.0 \\PersonLab (res101, s16, i801)~\citep{personlab:papandreou2018personlab} &60.0& 82.1 &64.3 &49.7 &74.6 &64.1 \\PersonLab (res101, s16, i1401)~\citep{personlab:papandreou2018personlab} & 65.6 & 85.9 & 71.4 & 61.1 & 72.8 & 70.1 \\
				\midrule
				Ours (res101, s16, i640)  &50.4 &78.5 &53.1 &41.6 &62.8 &56.9 \\
				Ours (res152, s16, i640)  &50.7 &77.7 &53.6 &41.1 &64.2 &56.9 \\Ours (res152, s16, i640) + R\#1 &  58.7 & 81.1 & 62.9 & 54.0 & 66.0 & 63.9 \\Ours (res152, s16, i640) + R\#2 & 65.3 &85.8 &71.3 &59.1 & 74.4 & 70.5 \\Ours (res101, s16, i800)  &51.6 &79.7 &55.1 &44.6 &61.2 &57.9 \\
				Ours (res101, s16, i800) + R\#1 &59.3 &82.1 &63.7 & 56.4 & 63.6 & 64.6 \\
				Ours (res101, s16, i800) + R\#2  &66.4 &86.1 & 72.6 & 61.1 & 74.0 & 71.2 \\
				\bottomrule
			\end{tabular}
		}
	\end{center}\vspace{-2em}
\end{table}

The standard evaluation metric for COCO keypoint localization is the object keypoint similarity (OKS) and the mean average precision (AP) over 10 thresholds (0.5,0.55,...,0.95) is regarded as the performance metric. 
We train our models on COCO train2017 set, and evaluate the model on the val2017 and test-dev2017 sets, as shown in Table~\ref{table:coco-val} and Table~\ref{table:coco-test}.
We mainly compare with the typical bottom-up models that have similar pipelines to our method: OpenPose~\citep{openpose:cao2017realtime}, PersonLab~\citep{personlab:papandreou2018personlab}, and AE~\citep{ae:newell2016associative}.
Following the works~\citep{openpose:cao2017realtime,ae:newell2016associative}, we also refine the grouped skeletons using a single pose estimator. 
We adopt the COCO pretrained TransPose-R-A4~\citep{transpose:yang2021transpose} that has a very similar architecture to our model and has only 6M parameters. 
We apply the single pose estimator to each single scaled person region achieved by the box containing the person mask. 
Note that the refinement results are highly dependent on the effect of the grouping and mask prediction, and we only update the keypoint estimates where the predictions of the two models are almost the same.
The concrete update rule is whether the keypoint similarity (KS) metric\footnote{We consider the per-keypoint standard deviation and object scale as the standard OKS metric does.} computing between two keypoints exceeds 0.75, indicating that the distance between two predicted locations is already very small.




\newcommand{\mysize}{0.2 \columnwidth}

\begin{figure}[hp]\centering    

\subfigure[OpenPose]{
\includegraphics[width=\mysize]{cmu_error.pdf}
}
\quad
\subfigure[TransPose-R]{
\includegraphics[width=\mysize]{tpr_error.pdf}
}
\quad
\subfigure[Ours (BU)]{
\includegraphics[width=\mysize]{error_pure.pdf}
\label{fig:error:bu}
}
\quad
\subfigure[Ours (BU+Refine)]{
\includegraphics[width=\mysize]{error_refine.pdf}
\label{fig:error:bu+refine}
}


\caption{Localization errors analysis on COCO validation set.}
\label{fig:error}
\end{figure}



{\bf Analysis}. We further analyze the differences between pure bottom-up results and the refined ones through the benchmarking and error diagnosis tool~\citep{error:matteo2017}. We compare our methods with the typical OpenPose model and the Transformer-based model. The yielded localization error bars (Figure~\ref{fig:error})
reveal the weaknesses and strengths of our model: (1) \emph{Jitter error}: The heatmap localization precision under the existence of multi-person is still not as accurate as the localization precision of single pose estimate. The \textit{small localization and quantization errors} of the pure bottom-up model reduce the precision under high thresholds;
(2) \emph{Missing error}: Since our algorithm does not ensure that the coordinate of every keypoint in a detected pose has been predicted, if the GT coordinate of a keypoint is annotated, \textit{zero-filling} coordinates will seriously pull down the calculated OKS value. Thus, for the evaluation, it is necessary to produce complete predictions. When we further use the single pose estimator to fill the missing joints with zero scores in the initially grouped skeletons, it achieves about 7 AP gains (Table~\ref{table:coco-val}) and reduces the missing error (shown in Figure~\ref{fig:error:bu+refine});
(3) \emph{Inversion error}: Forcing diverse keypoint types in an individual instance to have higher query-key similarity may make it difficult for the model to distinguish different keypoint types, especially the left and right inversion;
(4) \emph{Swap error}: We notice that our pure bottom-up model has fewer swap errors (1.2, shown in Figure~\ref{fig:error:bu}), which represents less confusion between semantically similar parts of different instances. It indicates that compared with OpenPose model, our attention-based grouping strategy performs relatively better in assigning parts to their corresponding instances. We show the qualitative human poses and instance segmentation results in Appendix~\ref{appendix:vis}.





\begin{table}[t]
\caption{Results on the COCO test-dev2017 set, compared with state-of-the-art methods. Our result is achieved based on ResNet-101 model with 16 output stride and 800 input resolution.}
\label{table:coco-test}
\begin{center}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Method}  &AP &  &  &  &  &  &  &  &  & \\ \midrule
Top-down\\ \midrule
G-RMI~\citep{grmi:papandreou2017towards} & 64.9 & 85.5 & 71.3& 62.3& 70.0 & 69.7 & 88.7 & 75.5 & 64.4 & 77.1\\
Mask-RCNN~\citep{mask:he2017mask} & 63.1 & 87.3 & 68.7 & 57.8 & 71.4 &- &- &- &-& -\\
SimpleBaseline~\citep{xiao2018simple}&73.7 &91.9 &81.1 &70.3 &80.8 &79.0& - & -& - &-\\
HRNet~\citep{hrnet:sun2019deep}&75.5 & 92.5 & 83.3 & 71.9 & 81.5 & 80.5 &- & -& - &- \\
\midrule

Bottom-up\\ \midrule
OpenPose~\citep{openpose:cao2017realtime}& 61.8 & 84.9 & 67.5 &57.1 &68.2 & -& -& -& - & -\\
AE~\citep{ae:newell2016associative}& 65.5 &86.8 &72.3 &60.6 &72.6 &70.2 &89.5 &76.0 &64.6 &78.1\\
PersonLab~\citep{personlab:papandreou2018personlab}&68.7 & 89.0 &75.4 &64.1 &75.5 &75.4 &92.7 &81.2 &69.7 &83.0\\
CenterNet~\citep{centernet:zhou2019objects} & 63.0 &86.8 &69.6& 58.9& 70.4 &- &- &- &- &-\\
SPM~\citep{spm:nie2019single} & 66.9 & 88.5 & 72.9 & 62.6 & 73.1 &- &- &- &- &- \\
HigherHRNet~\citep{higherhrnet:cheng2020higherhrnet} &70.5 &	89.3 & 	77.2 &	66.6 &	75.8 &- &- &- &- &- \\
DEKR~\citep{dekr:geng2021bottom} & 71.0	& 89.2&	78.0 &	67.1 & 76.9	&76.7 &93.2	& 83.0	&71.5 &83.9 \\
\midrule
\textbf{Ours} (\textbf{SSA}) &65.0& 86.2& 72.2& 60.1& 71.8& 70.1& 88.9& 76.2 & 64.2 & 78.2\\
 \bottomrule
\end{tabular}
}
\end{center} \vspace{-2em}
\end{table}




{\bf Person instance segmentation}. We evaluate the instance segmentation results on COCO val split (person category only). We compare our method with PersonLab~\citep{personlab:papandreou2018personlab}. In Table~\ref{table:coco-instance}, we report the results with a maximum of 20 person proposals due to the convention of the COCO person keypoint evaluation protocol. The results on the mean average precision (AP) show that our model still has a gap in the segmentation performance in comparison to PersonLab. We argue that this is mainly because we conduct the mask learning on low-resolution attention maps that have been downsampled 16 times w.r.t. the 640 or 800 input resolution, while the reported PersonLab result is based on 8 times downsampling w.r.t the 1401 input resolution. As shown in Table~\ref{table:coco-instance}, our model performs worse on small and medium scales but achieves comparable or even superior performance on large scale persons even if PersonLab uses a larger resolution. In this paper the instance segmentation is not our main goal, so we straightforwardly utilize 16 times \textit{bilinear interpolation} to upsample the attention maps as the final segmentation results. We believe further mask-specific optimization could improve the performance of instance segmentation.

\begin{table}[t]
\caption{Instance segmentation results (person class only) obtained with 20 proposals per image on the COCO  validation set.}
\label{table:coco-instance}
\begin{center}
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{lcccccccccccccc}\toprule
\textbf{Method} & \#Params & FLOPs &AP &  &  &  &  &  &  &  &  &  &  & \\ \midrule
PersonLab (res101, stride=8, input=1401) & 68.7M & 405.5G &33.8 &56.0 &36.8 &7.6 &45.9 &59.1 &15.6 &37.0 &38.3 &8.0 &51.4 &68.0\\
Ours (res152, stride=16, input=640) &60.6M&132.7G&20.7 &43.5 &16.9 &0.3 &24.5 &59.0 &12.9 & 29.4 & 30.3 & 1.0 & 36.1 & 68.5\\
Ours (res101, stride=16, input=800) &45.0M & 159.8G &22.0 &45.3 &18.8 &0.9 &27.7 &55.3 &13.2 & 30.8 & 32.0 & 1.8 & 41.1 & 66.9\\
 \bottomrule
\end{tabular}
}
\end{center}
 \vspace{-2em}
\end{table}


\subsection{Comparison between naive self-attention and supervised self-attention}

To study the differences in model learning when trained with and without supervising self-attention, we compare their convergences in the heatmap loss and instance mask loss, since the overfitting on COCO train data is usually not an issue.
As illustrated in Figure~\ref{fig:sup-vs-naive}, compared with training the naive self-attention model, supervising self-attention achieves a better fitting effect in the mask learning, while achieving an acceptable sacrifice on the fitting of heatmap learning. 
It is worth noting that the instance mask training loss curve of the naive self-attention model drops slightly, which suggests that the spontaneously formed attention pattern has a tendency to instance-awareness. 
To \textit{quantitatively} evaluate the performance of using naive self-attention patterns for keypoint grouping, we average the attentions from all transformer layers as the association reference (shown in Figure~\ref{fig:navie-vs-supervised}). When we use the totally same conditions (including model configuration, training \& testing settings and grouping algorithm) of the supervised self-attention model based on (res152, s16, i640), we achieve 29.0AP on COCO validation set, which is far from the 50.7AP result achieved by supervising self-attention. 


\begin{figure}
\begin{center}
	\includegraphics[width=0.98\linewidth]{loss_sup-vs-naive.pdf}
\end{center}
\caption{The convergences on the heatmap loss and instance mask loss when trained with and without supervising self-attention. We use moving average to visualize losses. Supervising self-attention sacrifices the heatmap loss a little but attains a good fitting in instance-awareness.}
\label{fig:sup-vs-naive}
\end{figure}

\section{Related Work}

{\bf Transformer.} We are now witnessing the applications of Transformer~\citep{vaswani2017attention} in various computer vision tasks due to its powerful visual relation modeling capability, such as image classification~\citep{vit:dosovitskiy2020image, deit:touvron2020deit}, object detection~\citep{detr:carion2020detr, deformdetr:zhu2020deformable}, semantic segmentation~\citep{setr:zheng2021rethinking}, tracking~\citep{transtrack:sun2020transtrack, trackformer:meinhardt2021trackformer}, human pose estimation~\citep{metro:lin2021end, prtr:li2021pose,transpose:yang2021transpose, tokenpose:li2021tokenpose, stoffl2021end} and etc. 
The common practice of these methods is to use the task-specific supervision signals such as class labels, object box coordinates, keypoint positions or semantic masks to supervise the final output of transformer-based models. They may visualize the attention maps to understand the model but few works directly use it as an explicit function in the inference process. 
Different from them, our work gives a successful example of explicitly using and supervising self-attention in vision Transformer for a specific purpose.

{\bf Human Pose Estimation \& Instance segmentation.} 
Multi-person pose estimation methods are usually classified into two categories: top-down (TD) or bottom-up (BU). TD models first detect persons, and then estimate single pose for each person, such as G-RMI~\citep{grmi:papandreou2017towards}, Mask-RCNN~\citep{mask:he2017mask}, CPN~\citep{cpn:chen2018cascaded}, SimpleBaseline~\citep{xiao2018simple}, and HRNet~\citep{hrnet:sun2019deep}. 
BU models need to detect the existence of various types of keypoints at any position and scale. And matching keypoints into instances requires the model to learn dense association signals pre-defined by human knowledge. OpenPose~\citep{openpose:cao2017realtime} proposes part affinity field (PAF) to measure the association between keypoints by computing the integral along the connecting line. Associative Embedding~\citep{ae:newell2016associative} abstracts an embedding as the human `tag' ID to measure the association. PersonLab~\citep{personlab:papandreou2018personlab} constructs mid-range offset as the geometric embedding to group keypoints into instances. In addition, single-stage methods~\citep{centernet:zhou2019objects, spm:nie2019single} also 
regress offset field to assign keypoints to their centers. Compared with them, we use Transformer to capture the \textit{intra-dependencies} within a person and\textit{ inter-dependencies} across different persons. And we explicitly exploit the intrinsic property of self-attention mechanism to solve the association problem, rather than \textit{regressing} highly abstracted offset fields or embeddings. The generic instance segmentation methods also can be categorized into top-down and bottom-up schemes. Top-down approaches predict the instance masks based on the object proposals, such as FCIS~\citep{fcis:li2017fully} and Mask-RCNN~\citep{mask:he2017mask}. Bottom-up approaches mainly cluster the semantic segmentation results to obtain instance segmentation using an embedding space or a discriminative loss to measure the pixel association like~\citep{ae:newell2016associative, de2017semantic, fathi2017semantic}. Compared with them, our method uses self-attention to measure the association and estimates instance masks based on instance keypoints.


\section{Discussion and Future Works}

This paper presents a new method to solve keypoint detection and instance association by using Transformer. 
We supervise the inherent characteristics of self-attention -- the feature similarity between any pair of positions -- to solve the grouping problem of the keypoints or pixels. 
Unlike a typical CNN-based bottom-up model, it no longer requires a pre-defined vector field or embedding as the associative reference, thus reducing the model redundancy and simplifying the pipeline. We demonstrate the effectiveness and simplicity of the proposed method on the challenging COCO keypoint detection and person instance segmentation tasks. 

The current approach also brings limitations and challenges. 
Due to the quadratic complexity of the standard Transformer, the model still struggles in simultaneously scaling up the Transformer capacity and the resolution of the input image.
The selection of loss criteria, model architecture, and training procedures can be further optimized. In addition, the reliance on the instance mask annotations also can be removed in future works, such as by imposing high and low attention constraints only on the pairs of keypoint locations. While, the current approach still has not yet beaten the sophisticated CNN-based state-of-the-art counterparts, we believe it is promising to exploit or supervise self-attention in vision Transformers to solve the detection and association problems in multi-person pose estimation, and other tasks or applications.






\bibliography{iclr_conference}
\bibliographystyle{iclr_conference}
\newpage
\appendix
\section{Appendix}



\subsection{Training details}
\label{Appendix}
In the training phase, we use data augmentation with random scale factor between 0.75 and 1.5, random flip with probability 0.5, random rotation with 30 degrees, and random translate with 40 pixels along the horizontal and vertical directions. The input size is 640 or 800 and thus the input sequence length of Transformer is 1600 or 2500. In our case the data processed by the transformer already belongs to the category of ultra-long sequences. We use the Post-Norm Transformer architecture and ReLU activation function in FFN. We supervise the 4-th self-attention layer by default (the total layer depth is 6). By convention, we use 2 transposed convolution layers to upsample the Transformer output size to 160160 or 200200. The standard deviation for the Gaussian kernel in the generated heatmaps is set to 2. We use Adam optimizer to train the model. The model is distributed across 8 NVIDIA Tesla V100-32G GPUs. The per-GPU batchsize is 1 or 2. The initial learning rate is set to , and decays 10 times at the 150-th and 200-th epochs respectively, with a total of 240 training epochs. 






\subsection{Inference details} 
The threshold score  for obtaining candidate keypoint from heatmaps is set to 0.0025. The final person masks are achieved by bilinear interploating the estimated  to the original image size. The skeleton kinematic tree used in the body-first grouping is defined as a graph structure: the vertices are all types of keypoints that are denoted as the numbers from 0 to 16 by the order defined by COCO dataset; the edges are defined as
    [(0, 1),
        (0, 2),
        (0, 3),
        (0, 4),
        (3, 5),
        (4, 6),
        (5, 7),
        (5, 11),
        (6, 8),
        (6, 12),
        (7, 9),
        (8, 10),
        (11, 13),
        (13, 15),
        (12, 14),
        (14, 16),
        (5, 6),  
        (15, 16),  
        (13, 14),  
        (11, 12)].





\subsection{Ablation on which attention layers should be supervised}
Supervising the self-attention matrix in different Transformer layer depths may have different effects on the heatmap and mask learning. To study such effects, we train a smaller proxy model to compare their differences in the fitting of heatmap and instance mask loss on a small subset (1/5) split of the COCO train set. The model configurations are: ResNet-50 based, 576 input resolution and 5 transformer layers with  and 320 hidden dimensions in FFN. 
Note that using a smaller model and small-scale training data inevitably reduces the overall performances of the model, but we only aim to find the relative differences in supervising at different Transformer layers. 
As illustrated in Figure~\ref{fig:layer-depth}, we do not observe significant differences in both heatmap loss and instance mask loss when leveraging the mask supervision in different Transformer layers. We further evaluate all these models on the COCO validation set. As shown in Table~\ref{tab:layers}, supervising one of the last three attention layers achieves better performance compared with supervising the first two layers. Especially, supervising the penultimate or third-to-last layer shows a better performance. This suggests that leveraging the instance mask loss in this layer depth is a better trade-off between heatmap learning and mask learning.

\begin{figure}[h]
	\centering
	\includegraphics[width=\columnwidth]{loss_layers.pdf}
	\caption{The convergences on the heatmap loss and mask loss when supervising the self-attention in different layer depths.}
	\label{fig:layer-depth}
\end{figure}

\begin{table}[h]\small
	\centering
	\begin{tabular}{cccccccc}
		\toprule
		Supervised layer & AP &  &  &  &  & & AP (with Refinement)\\
		\midrule
		1-th & 32.3 & 60.9& 29.8& 23.0& 45.5 & 38.8 & 52.1\\
		2-th & 33.7 & 62.1& 31.7& 22.9& 48.8 & 40.0 & 54.6\\
		3-th & 34.1 & 63.0& 31.4& 23.4& 49.0 & 40.4 & 54.6\\
		4-th & 34.1 & 63.0& 32.0& 23.3& 49.0 & 40.2 & 54.7\\
		5-th & 33.9 & 62.7& 31.4& 23.5& 48.5 & 40.5 & 54.7\\
		\toprule
	\end{tabular}
	\caption{Comparisons for different supervised layers on COCO validation set when using a small proxy model.}
	\label{tab:layers}
\end{table}


\subsection{Will an independent self-attention head be better than a shared one to leverage the instance mask loss?} 

Intuitively, using an independent self-attention head may be helpful to reduce the effect of introducing an intermediate instance mask loss on the standard Transformer forward. Thus we try to mitigate the negative effect on the heatmap localization by using an independent self-attention head to leverage the mask supervision. This design will need to insert an extra self-attention layer to the transformer intermediate output, as shown in Figure~\ref{fig:design_share-vs-ind}. However, by comparing the convergence of the training losses, we find no obvious difference in the heatmap loss fitting between using shared self-attention attention and independent self-attention, while, the independent self-attention performs relatively better in fitting the instance mask loss. 

When we test their performances on COCO validation set, we find both designs achieve similar performances, as shown in Table~\ref{tab:model_design_choice}.
Such results indicate that using an independent layer to the intermediate loss bring little gain, and introducing an intermediate instance mask loss may generate a weak effect on the prediction of keypoint heatmaps. We conjecture that \textit{the existence of the residual path parallel to the supervised self-attention layer} may also adaptively reduce the effect of the instance mask loss on the subsequent transformer layers, since we only leverage the sparse constraints to the self-attention matrix in a certain transformer layer.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\linewidth]{shared_vs_independent.pdf}
	\caption{The architecture designs for supervising shared self-attention and independent self-attention.}
	\label{fig:design_share-vs-ind}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{loss_share-vs-ind.pdf}
	\caption{The convergences on the heatmap loss and mask loss when trained with supervising shared self-attention and independent self-attention.}
	\label{fig:share-vs-ind}
\end{figure}

\begin{table}[t]\small
	\centering
	\begin{tabular}{ccccccccccc}
		\toprule
		 Supervision type & AP &  &  &  &  &   &
		 &  &
		 &  \\
		\midrule
		Shared & 50.7	&77.7&	53.5 &41.0	&64.2&	56.9&	80.0	&59.9&	43.3&	75.7 \\
		Independent & 50.7 &	77.0 &	53.6 &	40.9 &	64.6 &	56.7 &	79.7 &	59.4 &	42.9 &	75.9\\
		\toprule
	\end{tabular}
	\caption{Results on COCO validation set when using shared self-attention and independent self-attention designs.}
	\label{tab:model_design_choice}
\end{table}



 



\subsection{Runtime and complexity analysis} 
\label{runtime}
We take the ResNet-101 based model as the exemplar to test two types of grouping algorithm. We use the total 5000 images from COCO validation set. For each image, we run the model forward on a single GPU and the grouping algorithm on the CPU\footnote{NVIDIA Tesla V100 GPU and Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz.}, where the grouping runtime is far less than the model forward. In Table~\ref{tab:grouping}, we provide a controlled study to compare their differences in the model performance, theoretical complexity for per part assignment, \textbf{runtime for the whole inference pipeline (including keypoint detection, grouping and instance segmentation)}. Note that the complexity is a theoretical analysis based on the assumption that there are  existing skeletons and  candidates for a certain part type. We report the performances and runtime for the pure bottom-up result and the ones with refinement. 

\begin{table}[h]
    \centering
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{cccccccc}
    \toprule
       \multirow{2}{*} {Grouping Algorithm}  & Theoretical complexity for & \multirow{2}{*}{AP (BU)} & \multirow{2}{*}{Runtime (BU)} &
       \multirow{2}{*}{AP (BU+Refine)} & \multirow{2}{*}{Runtime (BU+Refine)}\\
       &per part assignment&& 
       \\
       \midrule
       Part-first view & 
       & 50.4
         & 8.45 img/sec & 65.3&  5.69 img/sec\\
Body-first view & 
       & 49.7
         & 8.94 img/sec  & 64.8& 5.72 img/sec\\
         \bottomrule 
    \end{tabular}}
    \caption{Comparison between the body-first and part-first grouping algorithm}
    \label{tab:grouping}
\end{table}

In Table~\ref{tab:params_flops} we compare our models with the mainstream bottom-up models, in terms of the number of model parameters and computational complexity of the model forward pass. The results of Hourglass~\citep{ae:newell2016associative}, PersonLab~\citep{personlab:papandreou2018personlab}, and HigherHRNet~\citep{higherhrnet:cheng2020higherhrnet} are taken from the HigherNet paper~\citep{higherhrnet:cheng2020higherhrnet}. We can see that compared with them, our models have fewer parameters and less computational complexity in the model forward pass.

\begin{table}[h]
	\centering
	\begin{tabular}{cccc}
		\toprule
		Model & Input Resolution & \#Param & FLOPs\\
		\midrule
	    Hourglass~\citep{ae:newell2016associative} & 512 & 277.8M & 206.9G \\
	    
	    PersonLab~\citep{personlab:papandreou2018personlab} & 1401 & 68.7M & 405.5G \\
	    
	    HigherHRNet~\citep{higherhrnet:cheng2020higherhrnet} & 640 & 63.8M & 154.3G \\
	    
	    DEKR~\citep{dekr:geng2021bottom} & 640 & 65.7M	& 141.5G \\
	    \midrule
	    
	    Ours (ResNet101+Transformer) & 640 & 45.0M & 102.3G \\
	    
	    Ours (ResNet152+Transformer) & 640 & 60.6M & 132.7G \\
	    Ours (ResNet101+Transformer) & 800 & 45.0M & 159.8G \\
		\bottomrule
	\end{tabular}
	\caption{Comparisons on the number of model parameters and model forward complexity.}
	\label{tab:params_flops}
\end{table}

\subsection{Visualization for human skeletons, instance masks and keypoint attention areas.}
\label{appendix:vis}
In Figure~\ref{fig:vis}, we visualize the qualitative results predicted by our pure bottom model based on ResNet-152 and 640 input resolution. Note that our algorithm is not limited to the number of the detected persons. Our model still can perform relatively well even in some hard cases, such as occluded persons and crowded scene with the existence of a large number of people (45) (shown in the 4-th row in Figure~\ref{fig:vis}). We also can see that the model is \textit{\textbf{instance-aware}}, i.e., the attention areas of the sampled keypoints belonging to a specific person can accurately and reasonably attend to the target person and not attend to the areas excluding the person.


\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{show_examples-5.pdf}
\end{center}
\caption{Qualitative visualization results predicted by our pure bottom-up model. For each image, we show the original image plotted with \textbf{human poses} and \textbf{masks}. And, for each image, we also show \textbf{the learned attention areas from the views of 4 sampled keypoints}, each location of which has been annotated by a white color pentagram. Redder areas mean higher attention scores.}
\label{fig:vis} \end{figure}

\end{document}
