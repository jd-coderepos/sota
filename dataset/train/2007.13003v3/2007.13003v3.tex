
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newcommand{\norms}[1]{\Vert#1\Vert}
\newcommand{\iprods}[1]{\langle #1\rangle}
\newtheorem*{remark}{Remark}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{forloop}
\usepackage{pgffor}
\usepackage{authblk}

\mathchardef\mhyphen="2D

\newcommand{\RandConv}{\texttt{RandConv}}

\usepackage{todonotes}
\newcommand{\mn}[1]{{\color{red}{#1}}}
\newcommand{\mnl}[1]{{\color{red}{~#1}}}
\newcommand{\mnr}[1]{{\color{red}{#1~}}}
\newcommand{\zx}[1]{{\color{blue}{#1}}}
\newcommand{\zxl}[1]{{\color{blue}{~#1}}}
\newcommand{\zxr}[1]{{\color{blue}{#1~}}}

\title{Robust and Generalizable Visual Representation Learning via Random Convolutions}

\author[1]{\textbf{Zhenlin Xu}}
\author[1]{\textbf{Deyi Liu}}
\author[2]{\textbf{Junlin Yang}}
\author[1]{\textbf{Colin Raffel}}
\author[1]{\textbf{Marc Niethammer}}
\affil[1]{
University of North Carolina at Chapel Hill }
\affil[2]{
Yale University}
\affil[1]{\footnotesize\texttt{\{zhenlinx, mn, craffel\}@cs.unc.edu, deyi@live.unc.edu}}
\affil[2]{\footnotesize\texttt{junlin.yang@yale.edu}}







\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
	While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation. \footnote{Code is available at \url{ https://github.com/wildphoton/RandConv}.}
\end{abstract}


\section{Introduction}
\label{Introduction}

Generalizability and robustness to out-of-distribution samples have been major pain points when applying deep neural networks (DNNs) in real world applications~\citep{volpi2018generalizing}. Though DNNs are typically trained on datasets with millions of training samples, they still lack robustness to domain shift, small perturbations, and adversarial examples~\citep{luo2019taking}.  Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large-scale datasets such as ImageNet~\citep{geirhos2018imagenettrained}. These superficial features can be local textures or even patterns imperceptible to humans but detectable to DNNs, as is the case for adversarial examples~\citep{ilyas2019adversarial}. In contrast, image semantics often depend more on object shapes rather than local textures. For image data, local texture differences are one of the main sources of domain shift, e.g., between synthetic virtual images and real data~\citep{sun2014virtual}. Our goal is therefore to learn visual representations that are invariant to local texture and that generalize to unseen domains. {While texture and color may be treated as different concepts, we follow the convention in \cite{geirhos2018imagenettrained} and include color when talking about texture.}


We address the challenging setting of robust visual representation learning from \emph{single domain data}. Limited work exists in this setting. Proposed methods include data augmentation~\citep{volpi2018generalizing, qiao2020learning, geirhos2018imagenettrained}, domain randomization~\citep{tobin2017domain,yue2019domain}, self-supervised learning~\citep{carlucci2019jigen}, and penalizing the predictive power of low-level network features~\citep{wang2019learning}. Following the spirit of adding inductive bias towards global shape information over local textures, we propose using random convolutions to improve the robustness to domain shifts and small perturbations. While recently \citet{lee2020network} proposed a similar technique for improving the generalization of reinforcement learning agents in unseen environments, we focus on visual representation learning and examine our approach on visual domain generalization benchmarks. Our method also includes the multiscale design and a mixing variant.
In addition, considering that many computer vision tasks rely on training deep networks based on ImageNet-pretrained weights (including some domain generalization benchmarks), we ask \emph{``Can a more robust pretrained model make the finetuned model more robust on downstream tasks?''} Different from~\citep{kornblith2019better,salman2020adversarially} who studied the transferability of a pretrained ImageNet representation to new tasks while focusing on in-domain generalization, we explore generalization performance on \emph{unseen domains} for new tasks.


\begin{figure}[t]
	\begin{center}
		\renewcommand{\arraystretch}{0.5}
		\setlength{\tabcolsep}{0.00cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}

			\begin{tabular}{ccccccc}
\multicolumn{7}{c}{\includegraphics[width=0.9\textwidth]{Fig/diagram3.png}} \\\\
				\newcounter{imgnum}
				\newcounter{sample_id}
				\newcounter{ks}
				Input &  &  &  &  &  & \\
				\forloop{imgnum}{2}{\value{imgnum} < 3}{
					\includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}.png} 
					\forloop{sample_id}{2}{\value{sample_id} < 3}{
& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel1_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel3_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel5_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel7_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel11_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel15_sample\arabic{sample_id}.png}
						\\
					}  
				}\
\label{eq:dist_preserve}
\|f(\mathbf{p}(x_i, y_i)) - f(\mathbf{p}(x_j, y_j))\| / \|\mathbf{p}(x_i, y_i) - \mathbf{p}(x_j, y_j)\| \approx r 
\label{eq:local_projection}
\mathbf{g}(x, y) & = \mathbf{U}\mathbf{p}(x,y)\,,

	\begin{aligned}
	P\Big(\sup_{i\neq j; i,j  \in [N]} \Big\{r_{i,j} :=\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} > \delta_1 \Big) \leq \epsilon,  \\\vspace{5ex}
	P\Big(\inf_{i\neq j; i,j  \in [N]} \Big\{r_{i,j} :=\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} < \delta_2 \Big) \leq \epsilon,  
	\end{aligned}
	
	\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} = \frac{1}{\sigma^2 } \frac{(\mathbf{z}_i-\mathbf{z}_j)^{\top}\mathbf{U}^{\top}\mathbf{U}(\mathbf{z}_i-\mathbf{z}_j)}{\norms{\mathbf{z}_i - \mathbf{z}_j}^2} = \sum_{k = 1}^{ m} \frac{\mathbf{v}_k^2}{\sigma^2} \sim \chi^2(m).
	
	P\Big(\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big) \leq \frac{2\epsilon}{N(N-1)}.
	
	\begin{array}{l}
	P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{ \frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\norms{\mathbf{z}_i - \mathbf{z}_j}^2}\Big\} > \sigma^2\chi^2_{\frac{2\epsilon}{N(N-1)}}(m)\Big) \vspace{1ex}\\
	= P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2}\Big\} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m)\Big) \vspace{1ex}\\
	= P\Big(\bigcup\limits_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big\}\Big) \vspace{1ex}\\
	\leq \sum\limits_{i\neq j; i,j  \in [N]} P\Big(\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big) \vspace{1ex}\\
	\leq \epsilon, \vspace{1ex}
	\end{array}
	
	P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}}\Big\} > \sigma\sqrt{\chi^2_{\frac{2\epsilon}{N(N-1)}}(m)}\Big) \leq \epsilon.
	
	P\Big(\inf_{i\neq j; i,j  \in [N]} \Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} < \sigma\sqrt{\chi^2_{1 - \frac{2\epsilon}{N(N-1)}}(m)} \Big) \leq \epsilon.
	
	
\end{proof}

\textbf{Simulation on Real Image Data} To better understand the relative distance preservation property of random linear projections in practice, we use Algorithm~\ref{algo:simulation} to empirically obtain a bound for real image data. We choose , ,  and  as in computing our theoretical bounds. We use   real images from the PACS dataset for this simulation. Note that the image patch size or  does not affect the bound. We use a patch size of  resulting in . This simulation tell us that applying linear projections with a randomly sampled  on  local images patches in every image, we have a  chance that  of  is in the range . 		

\begin{algorithm}[h]
	\small
	\caption{Simulate the range of central 80\% of  on real image data}
	\label{algo:simulation}
	\begin{algorithmic}[1]
		\State \textbf{Input}:  images , number of data points , projection output dimension , standard deviation  of normal distribution, confidence level .


		\For {}
		\State Sample images patches in  at 1,000 locations and vectorize them as 
		\State Sample a projection matrix  and 
		\For {}
		\For {}
		\State Compute , where 
		\EndFor	
		\EndFor
		\State  =  quantile of  for  
		\State 	=  quantile of  for  \Comment{Get the central 80\% of  in each image}
		\EndFor	
		\State  =  quantile of all  
		\State  =  quantile of all 
		\Comment{Get the  confident bound for  and }
		\State \textbf{return} , 		 
	\end{algorithmic}
\end{algorithm}


\section{Experimental Details}
\label{exp_detail}


\textbf{Digits Recognition} The network for our digits recognition experiments is composed of two \emph{Conv55-ReLU-MaxPool22} blocks with 64/128 output channels and three fully connected layer with 1024/1024/10 output channels. We train the network with batch size 32 for 10,000 iterations. During training, the model is validated every 250 iterations and saved with the best validation score for testing. We apply the \texttt{Adam} optimizer with an initial learning rate of 0.0001.   

\textbf{PACS} We use the official data splits for training/validation/testing; no extra data augmentation is applied. We use the official \texttt{PyTorch} implementation and the pretrained weights of AlexNet for our PACS experiments. AlextNet is finetuned for 50,000 iterations with a batch size 128. Samples are randomly selected from the training data mixed between the three domains. We use the validation data of  source domains only at every 100 iterations. We use the \texttt{SGD} optimizer for training with an initial learning rate of 0.001, Nesterov momentum, and weight decay set to 0.0005. We let the learning rate decay by a factor of 0.1 after finishing 80\% of the iterations.

\textbf{ImageNet} Following the \texttt{PyTorch} example \footnote{https://github.com/pytorch/examples/tree/master/imagenet} on training ImageNet models, we set the batch size to 256 and train AlexNet from scratch for 90 epochs. We apply the \texttt{SGD} optimizer with an initial learning rate of 0.01, momentum 0.9, and weight decay 0.0001. We reduce the learning rate via a factor of 0.1 every 30 epochs.

\section{More Experiments with ResNet-18}
\label{resnet_exp}
In this section, we demonstrate that {\RandConv} also works on other stronger backbone architectures, e.g. for a Residual Network~\citet{he2016deep}. Specifically, we run the PACS and ImageNet experiments with ResNet-18 as the baseline and {\RandConv}. As Table~\ref{table:imagenet_resnet} shows,  {\RandConv} improves the baseline using ResNet18 on ImageNet-sketch by 10.5\% accuracy. When using a {\RandConv} pretrained ResNet-18 on PACS, the performance of finetuning with DeepAll and {\RandConv} are both improved shown in Table~\ref{table:transfer_PACS_resnet}. The best average domain generalization accuracy is 84.09\%, with a more than 8\% improvement over our initial Deep-All baseline. A model pretrained with  generally performs better than when pretrained with . We also provide the ResNet-18 performance of JiGen~\citep{carlucci2019jigen} on PACS as reference. Note that JiGen uses extra data augmentation and a different data split than our approach and it only improves over its own baseline by 1.5\%. In addition, we test {\RandConv} trained ResNet-18 on ImageNet-R \citep{hendrycks2020many}, a domain generalization benchmark that contains images of artistic renditions of 200 object classes from the original ImageNet dataset. As Table \ref{table:imagenetR_resnet} shows, {\RandConv} also improve the generalization performance on ImageNet-R and reduce the gap between the in-domain (ImageNet-200) and out-of-domain (ImageNet-R) performance. 

\begin{table}[htp]
	\small
\caption{Accuracy of ImageNet-trained ResNet-18 on ImageNet-Sketch data.}
	\label{table:imagenet_resnet}
	\centering
	\begin{tabular}{c|cccc}
		\toprule
		      & Baseline &
		\tiny, =0.5, =10   & \tiny, =10   \\
		\toprule
		    Top1  & 20.23        & 
		28.79 &  30.70 \\
		 Top5 & 37.26 
		& 49.02 & 51.80 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htp]
	\small
\caption{Top 1 Accuracy of ImageNet-trained ResNet-18 on ImageNet-R data. ImageNet-200 are the original ImageNet data with the same 200 classes as ImageNet-R. }
	\label{table:imagenetR_resnet}
	\centering
	\begin{tabular}{c|cccc}
		\toprule
		      & Baseline &
		\tiny, =0.5, =10   & \tiny, =10   \\
		\toprule
		    ImageNet-200 (\%)    &  88.15         & 
		83.72 &  72.7 \\
		 ImageNet-R (\%) &  33.06   
		& 37.38 & 35.75 \\
		Gap & 55.09 & 46.34 & 36.95\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htp]
	\small
	\vspace{-2mm}
	\caption{Generalization results on PACS with {\RandConv} pretrained model using ResNet-18. ImageNet column shows how the pretrained model is trained on ImageNet (baseline represents training using only the classification loss); PACS column indicates the methods used for finetuning on PACS.  \textbf{Best} and \underline{second best} accuracy for each target domain are highlighted in bold and underlined. The performance of JiGen~\citep{carlucci2019jigen} and its baseline using ResNet-18 is also given. }
	\label{table:transfer_PACS_resnet}
	\centering
	\begin{tabular}{c|c|ccccc}
		\toprule
		PACS                & ImageNet & Photo & Art & Cartoon & Sketch          & Avg                  \\
		\toprule
		\multirow{3}{*}{Deep-All} & Baseline     & \textbf{95.45\tiny(0.43)} &        74.96\tiny(0.99) &        71.48\tiny(1.22) &        62.09\tiny(1.12) &        76.00\tiny(0.37)          \\
		&	  & 94.65\tiny(0.16) &        73.85\tiny(0.97) &        74.78\tiny(0.58) &        73.51\tiny(1.16) &        79.20\tiny(0.40)   \\
		&	    & 94.10\tiny(0.43) &        76.72\tiny(1.43) &        73.41\tiny(1.29) &        77.60\tiny(0.55) &        80.46\tiny(0.74)         \\
		\midrule
	
		\multirow{3}{*}{\shortstack{ \\ \tiny =0.5,=10}}   & Baseline     & 92.37\tiny(0.54) &        76.50\tiny(0.55) &        71.33\tiny(0.29) &        79.65\tiny(1.32) &        79.96\tiny(0.53)              \\

		&   & 94.43\tiny(0.22) &        79.80\tiny(1.03) &        73.40\tiny(0.37) &        81.51\tiny(0.85) &        82.28\tiny(0.38)         \\
		&       & 94.57\tiny(0.45) &        \textbf{81.32\tiny(1.00)} &        \textbf{76.28\tiny(0.82)} &        \textbf{84.18\tiny(0.94)} &        \textbf{84.09\tiny(0.61)}          \\
		\midrule
		\multirow{3}{*}{\shortstack{ \\\tiny =10 }}      & Baseline     & 93.57\tiny(0.40) &        77.73\tiny(0.91) &        71.24\tiny(0.91) &        75.53\tiny(2.17) &        79.52\tiny(0.61)          \\
		&  & {\ul95.23\tiny(0.30)} &        80.56\tiny(0.82) &        74.18\tiny(0.53) &        80.70\tiny(1.43) &        82.67\tiny(0.46)    \\
		&   & 95.01\tiny(0.32) &        {\ul81.09\tiny(1.24)} &        {\ul76.04\tiny(0.92)} &        {\ul83.02\tiny(0.93)} &        {\ul83.79\tiny(0.60)}         \\
		\midrule
		Deep-All & \multirow{2}{*}{Baseline} & 95.73 & 77.85 & 74.86 & 67.74  & 79.05 \\
		JiGen & &  96.03 & 79.42 & 75.25 & 71.35 & 80.51 \\
		\bottomrule
	\end{tabular}
	\vspace{-2mm}
\end{table}

\section{Hyperparameter Selections and Ablation Studies on Digits Recognition Benchmarks}

We provide detailed experimental results for the digits recognition datasets.  Table~\ref{digits-p} shows results for different hyperameters  for . Table~\ref{digits-scale} shows results for an ablation study on the multi-scale design for  and . Table~\ref{digits-consistency} shows results for studying the consistency loss weight  for  and . Tables~\ref{digits-p},~\ref{digits-scale},~and~\ref{digits-consistency} correspond to Fig. 2 (a)(b)(c) in the main text respectively.
\label{results}
\begin{table}[htp]
	\small
	\centering
\caption{Ablation study of hyperparameter  for  on digits recognition benchmarks. DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results are \textbf{bold}.}
	\label{digits-p}
	\begin{tabular}{l|c|ccccc|c}
		\toprule
		& MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		Baseline    & 98.40\tiny(0.84) & 58.87\tiny(3.73) & 33.41\tiny(5.28) & 79.27\tiny(2.70) & 42.43\tiny(5.46) & 53.50\tiny(4.23) & 88.20\tiny(2.10) \\
		\tiny, =0.9 & 98.68\tiny(0.06) & 83.53\tiny(0.37) & 53.67\tiny(1.54) & 80.38\tiny(1.41) & 59.19\tiny(0.85) & 69.19\tiny(0.34) & \textbf{89.79\tiny(0.44)} \\
		\tiny, =0.7 & 98.64\tiny(0.07) & 84.17\tiny(0.61) & 54.50\tiny(1.55) & \textbf{80.85\tiny(0.91)} & 60.25\tiny(0.85) & 69.94\tiny(0.50) & 89.20\tiny(0.60) \\
		\tiny, =0.5 & 98.72\tiny(0.08) & 85.17\tiny(1.12) & \textbf{55.97\tiny(0.54)} & 80.31\tiny(0.85) & \textbf{61.07\tiny(0.47)} & \textbf{70.63\tiny(0.42)} & 88.66\tiny(0.62) \\
		\tiny, =0.3 & 98.71\tiny(0.12) & 85.45\tiny(0.87) & 54.62\tiny(1.52) & 79.78\tiny(1.40) & 60.51\tiny(0.41) & 70.09\tiny(0.60) & {89.02\tiny(0.32)} \\
		\tiny, =0.1 & 98.66\tiny(0.06) & 85.57\tiny(0.79) & 54.34\tiny(1.52) & 79.21\tiny(0.44) & 60.18\tiny(0.63) & 69.83\tiny(0.38) & 88.53\tiny(0.38) \\
		\tiny, =0 & 98.55\tiny(0.13) & \textbf{86.27\tiny(0.42)} & 52.48\tiny(3.00) & 79.01\tiny(1.11) & 59.53\tiny(1.14) & 69.32\tiny(1.19) & 88.01\tiny(0.36) \\
		\bottomrule
	\end{tabular}
\end{table}
\begin{table}[htp]
	\small
	\centering
	\setlength{\tabcolsep}{3pt}
	\caption{Ablation study of multi-scale {\RandConv} on digits recognition benchmarks for  and . Best entries for each variant are \textbf{bold}.}
	\label{digits-scale}
	\begin{tabular}{l|c|ccccc|c}
		\toprule
		& MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		       & 98.62\tiny(0.06)          & 83.98\tiny(0.98)          & 53.26\tiny(2.59)          & 80.57\tiny(1.09)          & 59.25\tiny(1.38)          & 69.26\tiny(1.35)          & 88.59\tiny(0.38)          \\
		  & 98.76\tiny(0.02)          & 84.66\tiny(1.67)          & 55.89\tiny(0.83)          & 80.95\tiny(1.15)          & 60.07\tiny(1.05)          & 70.39\tiny(0.58)          & 89.80\tiny(0.94)          \\
		  & 98.76\tiny(0.06)          & 84.32\tiny(0.43)          & \textbf{56.50\tiny(2.68)} & 81.85\tiny(1.05)    & 60.76\tiny(1.02)          & 70.86\tiny(0.86)          & 90.06\tiny(0.80)          \\
		  & 98.82\tiny(0.06)    	& 84.91\tiny(0.68)         & 55.61\tiny(2.63)   & \textbf{82.09\tiny(1.00)} & \textbf{62.15\tiny(1.30)}   & \textbf{71.19\tiny(1.21)}        & 90.30\tiny(0.44)   \\
		 &  98.81\tiny(0.12) & \textbf{85.13\tiny(0.72)}          & 54.18\tiny(3.36)          & 82.07\tiny(1.28)          & 61.85\tiny(1.41)          & 70.81\tiny(1.24)          & \textbf{90.83\tiny(0.52)}   \\
		\midrule
		\tiny, =0.5   & 98.66\tiny(0.05) & 85.12\tiny(0.96)          & 55.59\tiny(0.29)          & 80.65\tiny(0.71)          & 60.85\tiny(0.48)          & 70.55\tiny(0.15)          & 89.00\tiny(0.45)          \\
		\tiny, =0.5 & 98.79\tiny(0.07) & 85.36\tiny(1.04)          & \textbf{55.60\tiny(1.09)} & 80.99\tiny(0.99)          & 61.26\tiny(0.80)          & 70.80\tiny(0.86)          & 89.84\tiny(0.70)          \\
		\tiny, =0.5 & 98.83\tiny(0.07) & \textbf{86.33\tiny(0.47)} & 54.99\tiny(2.48)          & 80.82\tiny(1.83)          & 62.61\tiny(0.75)          & 71.19\tiny(1.25)          & 90.70\tiny(0.43)          \\
		\tiny, =0.5 & 98.83\tiny(0.07) & 86.08\tiny(0.27)          & 54.93\tiny(1.27)          & \textbf{81.58\tiny(0.74)} & \textbf{62.78\tiny(0.86)} & \textbf{71.34\tiny(0.61)} & \textbf{91.18\tiny(0.38)} \\
		\tiny, =0.5 & 98.80\tiny(0.12) & 85.63\tiny(0.70)          & 52.82\tiny(2.01)          & 81.48\tiny(1.22)          & 62.55\tiny(0.74)          & 70.62\tiny(0.73)          & 90.79\tiny(0.48)              \\
\bottomrule
	\end{tabular}
\end{table}
\begin{table}[htp]
	\small
	\centering
	\setlength{\tabcolsep}{3pt}
	\caption{Ablation study of consistency loss weight  on digits recognition benchmarks for  and . DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results for each variant are \textbf{bold}.}
	\label{digits-consistency}
	\begin{tabular}{l|c|c|ccccc|c}
		\toprule
		& & MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		\multirow{6}{*}{} & 20   & 98.90 \tiny(0.05) & 87.18 \tiny(0.81)          & \textbf{57.68 \tiny(1.64)} & \textbf{83.55 \tiny(0.83)} & 63.08 \tiny(0.50)          & 72.87 \tiny(0.47)          & 91.14 \tiny(0.53)          \\
		& 10   & 98.85 \tiny(0.04) & \textbf{87.76 \tiny(0.83)} & 57.52 \tiny(2.09)          & 83.36 \tiny(0.96)          & 62.88 \tiny(0.78)          & \textbf{72.88 \tiny(0.58)} & \textbf{91.62 \tiny(0.77)} \\
		& 5    & 98.94 \tiny(0.09) & 87.53 \tiny(0.51)          & 55.70 \tiny(2.22)          & 83.12 \tiny(1.08)          & 62.37 \tiny(0.98)          & 72.18 \tiny(1.04)          & 91.46 \tiny(0.50)          \\
		& 1    & 98.95 \tiny(0.05) & 86.77 \tiny(0.79)          & 56.00 \tiny(2.39)          & 83.13 \tiny(0.71)          & \textbf{63.18 \tiny(0.97)} & 72.27 \tiny(0.82)          & 91.15 \tiny(0.42)          \\
		& 0.1  & 98.84 \tiny(0.07) & 85.41 \tiny(1.02)          & 56.51 \tiny(1.58)          & 81.84 \tiny(1.14)          & 61.86 \tiny(1.44)          & 71.41 \tiny(0.98)          & 90.72 \tiny(0.60)          \\
		& 0    & 98.82 \tiny(0.06) & 84.91 \tiny(0.68)          & 55.61 \tiny(2.63)          & 82.09 \tiny(1.00)          & 62.15 \tiny(1.30)          & 71.19 \tiny(1.21)          & 90.30 \tiny(0.44)          \\
		
		\midrule
		\multirow{6}{*}{} & 20  & 98.79 \tiny(0.04) & 87.53 \tiny(0.79)          & 53.92 \tiny(1.59)          & 81.83 \tiny(0.70)          & 62.16 \tiny(0.37)          & 71.36 \tiny(0.49)          & \textbf{91.20 \tiny(0.53)}          \\
		& 10  & 98.86 \tiny(0.05) & 87.67 \tiny(0.37)          & 54.95 \tiny(1.90)          & 82.08 \tiny(1.46)          & 63.37 \tiny(1.58)          & 72.02 \tiny(1.15)          & 90.94 \tiny(0.51)          \\
		& 5   & 98.90 \tiny(0.04) & \textbf{87.77 \tiny(0.72)} & \textbf{55.00 \tiny(1.40)} & \textbf{82.10 \tiny(0.55)} & \textbf{63.58 \tiny(1.33)} & \textbf{72.11 \tiny(0.62)} & {90.83 \tiny(0.71)}          \\
		& 1   & 98.86 \tiny(0.04) & 86.74 \tiny(0.32)          & 53.26 \tiny(2.99)          & 81.51 \tiny(0.48)          & 62.00 \tiny(1.15)          & 70.88 \tiny(0.93)          & 91.11 \tiny(0.62)          \\
		& 0.1 & 98.85 \tiny(0.14) & 86.85 \tiny(0.31)          & 53.55 \tiny(3.63)          & 81.23 \tiny(1.02)          & 62.77 \tiny(0.80)          & 71.10 \tiny(1.31)          & 91.13 \tiny(0.69)          \\
		& 0   & 98.83 \tiny(0.07) & 86.08 \tiny(0.27)          & 54.93 \tiny(1.27)          & 81.58 \tiny(0.74)          & 62.78 \tiny(0.86)          & 71.34 \tiny(0.61)          & 91.18 \tiny(0.38)          \\
		
\bottomrule
	\end{tabular}
\end{table}
\newpage

\section{More Examples of {\RandConv} Data Augmentation}
\label{examples}
We provide additional examples of {\RandConv} outputs for different convolution filter sizes in Fig.~\ref{fig:randconv_example_more} and for its mixing variants at scale  with different mixing coefficients in Fig.~\ref{fig:randconv_mix_example_more}. We observe that {\RandConv} with different filter sizes retains shapes at different scales. The mixing strategy can continuously interpolate between the training domain and a randomly sampled domain.  




\begin{figure}[htp]
	\begin{center}
\setlength{\tabcolsep}{0.01cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{ccccccc}
Input &  &  &  &  &  &  \\
				\includegraphics[width=\cwidth]{{Fig/examples/image2}.png}
				\forloop{sample_id}{1}{\value{sample_id} < 4}{ 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.9_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.7_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.5_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.3_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.1_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0_sample\arabic{sample_id}}.png} \\
				} 
			\end{tabular}
		\end{adjustbox}
	\end{center}
\caption{\small Examples of the {\RandConv} mixing variant  on images of size  with different mixing coefficients . When , the output is just the original image input;when , we use the output of the random convolution layer as the augmented image.}  
\label{fig:randconv_mix_example_more}
\end{figure}

\begin{figure}[htp]
	\begin{center}
\setlength{\tabcolsep}{0.01cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{ccccccc}


				Original image &  &  &  &  &  & \\
				\forloop{imgnum}{1}{\value{imgnum} < 4}{
					\includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}.png} 
					\forloop{sample_id}{0}{\value{sample_id} < 3}{
& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel1_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel3_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel5_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel7_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel11_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel15_sample\arabic{sample_id}.png}
\\
					}  
				}\\
				
\end{tabular}
		\end{adjustbox}
	\end{center}
\caption{\small {\RandConv} data augmentation examples on images of size . First column is the input image; following columns are convolution results using random filters of different sizes . We can see that the smaller filter sizes help maintain the finer shapes.}
	\label{fig:randconv_example_more}
\end{figure}

\end{document}
