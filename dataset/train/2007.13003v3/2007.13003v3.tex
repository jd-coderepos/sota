
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newcommand{\norms}[1]{\Vert#1\Vert}
\newcommand{\iprods}[1]{\langle #1\rangle}
\newtheorem*{remark}{Remark}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{forloop}
\usepackage{pgffor}
\usepackage{authblk}

\mathchardef\mhyphen="2D

\newcommand{\RandConv}{\texttt{RandConv}}

\usepackage{todonotes}
\newcommand{\mn}[1]{{\color{red}{#1}}}
\newcommand{\mnl}[1]{{\color{red}{$\leftarrow$~#1}}}
\newcommand{\mnr}[1]{{\color{red}{#1~$\rightarrow$}}}
\newcommand{\zx}[1]{{\color{blue}{#1}}}
\newcommand{\zxl}[1]{{\color{blue}{$\leftarrow$~#1}}}
\newcommand{\zxr}[1]{{\color{blue}{#1~$\rightarrow$}}}

\title{Robust and Generalizable Visual Representation Learning via Random Convolutions}

\author[1]{\textbf{Zhenlin Xu}}
\author[1]{\textbf{Deyi Liu}}
\author[2]{\textbf{Junlin Yang}}
\author[1]{\textbf{Colin Raffel}}
\author[1]{\textbf{Marc Niethammer}}
\affil[1]{
University of North Carolina at Chapel Hill }
\affil[2]{
Yale University}
\affil[1]{\footnotesize\texttt{\{zhenlinx, mn, craffel\}@cs.unc.edu, deyi@live.unc.edu}}
\affil[2]{\footnotesize\texttt{junlin.yang@yale.edu}}







\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
	While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation. \footnote{Code is available at \url{ https://github.com/wildphoton/RandConv}.}
\end{abstract}


\section{Introduction}
\label{Introduction}

Generalizability and robustness to out-of-distribution samples have been major pain points when applying deep neural networks (DNNs) in real world applications~\citep{volpi2018generalizing}. Though DNNs are typically trained on datasets with millions of training samples, they still lack robustness to domain shift, small perturbations, and adversarial examples~\citep{luo2019taking}.  Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large-scale datasets such as ImageNet~\citep{geirhos2018imagenettrained}. These superficial features can be local textures or even patterns imperceptible to humans but detectable to DNNs, as is the case for adversarial examples~\citep{ilyas2019adversarial}. In contrast, image semantics often depend more on object shapes rather than local textures. For image data, local texture differences are one of the main sources of domain shift, e.g., between synthetic virtual images and real data~\citep{sun2014virtual}. Our goal is therefore to learn visual representations that are invariant to local texture and that generalize to unseen domains. {While texture and color may be treated as different concepts, we follow the convention in \cite{geirhos2018imagenettrained} and include color when talking about texture.}


We address the challenging setting of robust visual representation learning from \emph{single domain data}. Limited work exists in this setting. Proposed methods include data augmentation~\citep{volpi2018generalizing, qiao2020learning, geirhos2018imagenettrained}, domain randomization~\citep{tobin2017domain,yue2019domain}, self-supervised learning~\citep{carlucci2019jigen}, and penalizing the predictive power of low-level network features~\citep{wang2019learning}. Following the spirit of adding inductive bias towards global shape information over local textures, we propose using random convolutions to improve the robustness to domain shifts and small perturbations. While recently \citet{lee2020network} proposed a similar technique for improving the generalization of reinforcement learning agents in unseen environments, we focus on visual representation learning and examine our approach on visual domain generalization benchmarks. Our method also includes the multiscale design and a mixing variant.
In addition, considering that many computer vision tasks rely on training deep networks based on ImageNet-pretrained weights (including some domain generalization benchmarks), we ask \emph{``Can a more robust pretrained model make the finetuned model more robust on downstream tasks?''} Different from~\citep{kornblith2019better,salman2020adversarially} who studied the transferability of a pretrained ImageNet representation to new tasks while focusing on in-domain generalization, we explore generalization performance on \emph{unseen domains} for new tasks.


\begin{figure}[t]
	\begin{center}
		\renewcommand{\arraystretch}{0.5}
		\setlength{\tabcolsep}{0.00cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}

			\begin{tabular}{ccccccc}
\multicolumn{7}{c}{\includegraphics[width=0.9\textwidth]{Fig/diagram3.png}} \\\\
				\newcounter{imgnum}
				\newcounter{sample_id}
				\newcounter{ks}
				Input & $k=1$ & $k=3$ & $k=5$ & $k=7$ & $k=11$ & $k=15$\\
				\forloop{imgnum}{2}{\value{imgnum} < 3}{
					\includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}.png} 
					\forloop{sample_id}{2}{\value{sample_id} < 3}{
& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel1_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel3_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel5_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel7_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel11_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel15_sample\arabic{sample_id}.png}
						\\
					}  
				}\\[-2mm]
Input & $\alpha=0.9$ & $\alpha=0.7$ & $\alpha=0.5$ & $\alpha=0.3$ & $\alpha=0.1$ & $\alpha=0$ \\
				\includegraphics[width=\cwidth]{{Fig/examples/image2}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.9_sample1}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.7_sample1}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.5_sample1}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.3_sample1}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.1_sample1}.png} 
				&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0_sample1}.png} 
			\end{tabular}
		\end{adjustbox}
	\end{center}
	\vspace{-3mm}
	\caption{\small \textbf{Top}: Illustration that {\RandConv} randomize local texture but preserve shapes in the image. \textbf{Middle:} First column is the input image of size $224^2$; following columns are convolutions results using random filters of different sizes $k$. \textbf{Bottom:} Mixing results between an image and one of its random convolution results with different mixing coefficients $\alpha$.}  
	\vspace{-5mm}
	\label{fig:randconv_example}
\end{figure}


We make the following contributions:
\begin{itemize}[leftmargin=2em]
	\vspace{-0mm}
	\setlength\itemsep{0em}
\item We develop {\RandConv}, a data augmentation technique \emph{using multi-scale random-convolutions to generate images with random texture while maintaining global shapes.} We explore using the {\RandConv} output as training images or mixing it with the original images. We show that a consistency loss can further enforce invariance under texture changes.
	\item We provide insights and justification on why {\RandConv} augments images with different local texture but the same semantics with the shape-preserving property of random convolutions.
	\item We validate {\RandConv} and its mixing variant in extensive experiments on synthetic and real-world benchmarks as well as on the large-scale ImageNet dataset. Our methods outperform single domain generalization approaches by a large margin on digit recognition datasets and for the challenging case of generalizing to the Sketch domain in PACS and to ImageNet-Sketch.
	\item  We explore if the robustness/generalizability of a pretrained representation can transfer. We show that transferring a model pretrained with {\RandConv} on ImageNet can further improve domain generalization performance on new downstream tasks on the PACS dataset.
\end{itemize}

\section{Related Work}
\textbf{Domain Generalization} (DG) aims at learning representations that perform well when transferred to unseen domains. Modern techniques range between feature fusion~\citep{shen2019situational}, meta-learning~\citep{li2018mldg, balaji2018metareg}, and adversarial training~\citep{shao2019multi, li2018domain}. Note that most current DG work~\citep{ghifary2016scatter, li2018mldg, li2018domain} requires a multi-source training setting to work well. However, in practice, it might be difficult and expensive to collect data from multiple sources, such as collecting data from multiple medical centers~\citep{raghupathi2014big}. 
Instead, we consider the more strict single-domain generalization DG setting, where we train the model on source data from a single domain and generalize it to new unseen domains~\citep{carlucci2019jigen, wang2018learning}.

\textbf{Domain Randomization} (DR) was first introduced as a DG technique by~\citet{tobin2017domain} to handle the domain gap between simulated and real data. As the training data in~\citep{tobin2017domain} is synthesized in a virtual environment, it is possible to generate diverse training samples by randomly selecting background images, colors, lighting, and textures of foreground objects. When a simulation environment is not accessible, image stylization can be used to generate new domains~\citep{yue2019domain,geirhos2018imagenettrained}. However, this requires extra effort to collect data and to train an additional model; further, the number of randomized domains is limited by the number of predefined styles.

\textbf{Data Augmentation} has been widely used to improve the generalization of machine learning models~\citep{simard2003best}. DR approaches can be considered a type of synthetic data augmentation. To improve performance on unseen domains, \citet{volpi2018generalizing} generate adversarial examples to augment the training data;  \citet{qiao2020learning} extend this approach via meta-learning. As with other adversarial training 
algorithms, significant extra computation is required to obtain adversarial examples.

\textbf{Learning Representations Biased towards Global Shape}
\citet{geirhos2018imagenettrained} demonstrated that convolutional neural networks (CNNs) tend to use superficial local features even when trained on large datasets. To counteract this effect, they proposed to train on stylized ImageNet, thereby forcing a network to rely on object shape instead of textures. Wang et al. improved out-of-domain performance by penalizing the correlation between a learned representation and superficial features such as the gray-level co-occurrence matrix~\citep{wang2018learning}, or by penalizing the predictive power of local, low-level layer features in a neural network via an adversarial classifier~\citep{wang2019learning}. Our approach shares the idea that learning representations invariant to local texture helps generalization to unseen domains. However, {\RandConv} avoids searching over many hyper-parameters, collecting extra data, and training other networks. It also scales to large-scale datasets since it adds minimal computation overhead.

\textbf{Random Mapping in Machine Learning}
Random projections have also been effective for dimensionality reduction based on the distance-preserving property of the Johnson–Lindenstrauss lemma~\citep{johnson1984extensions}. \citep{vinh2016training} applied random projections on entire images as data augmentation to make neural networks robust to adversarial examples. \citet{lee2020network} recently used random convolutions to help reinforcement learning (RL) agents generalize to new environments. Neural networks with \textit{fixed} random weights can encode meaningful representations \citep{saxe2011random} and are therefore useful for neural architecture search~\citep{gaier2019weight}, generative models~\citep{he2016powerful}, natural language processing~\citep{wieting2018no}, and RL~\citep{osband2018randomized, burda2018exploration}. In contrast, {\RandConv} uses \textit{non-fixed} randomly-sampled weights to generate images with different local texture.

\section{RandConv: Randomize Local Texture at Different Scales }

We propose using a convolution layer with non-fixed random weights as the first layer of a DNN during training. This strategy generates images with random local texture but consistent shapes, and is beneficial for robust visual representation learning. Sec.~\ref{sec:rc_shape_preserve} justifies the shape-preserving property of a random convolution layer. 
Sec.~\ref{sec:randconv} describes {\RandConv}, our data augmentation algorithm using a multi-scale randomized convolution layer and input mixing.

\subsection{A Random Convolution Layer Preserves Global Shapes}
\label{sec:rc_shape_preserve}
Convolution is the key building block for deep convolutional neural networks.
Consider a convolution layer with filters $\mathbf{\Theta}\in\mathbb{R}^{h\times w \times C_{in}\times C_{out}}$ with an input image $\mathbf{I}\in\mathbb{R}^{H\times W\times C_{in}}$, where $H$ and $W$ are the height and width of the input and $C_{in}$ and $C_{out}$ are the number of feature channels for the input and output, and $h$ and $w$ are the height and width of the layer's filter. The output (with appropriate input padding) will be $\mathbf{g} = \mathbf{I} *  \mathbf{\Theta}$ with $\mathbf{g}\in\mathbb{R}^{H\times W\times C_{out}}$.  

 In images, nearby pixels with similar color or texture can be grouped into primitive shapes that represent parts of objects or the background. A convolution layer linearly projects local image patches to features at corresponding locations on the output map using shared parameters. While a convolution with random filters can project local patches to arbitrary output features, the output of a random linear projection approximately preserves relative similarity between input patches, proved in Appendix \ref{theorem_proof}. In other words, since any two locations within the same shape have similar local textures in the input image, they tend to be similar in the output feature map. Therefore, shapes that emerge in the output feature map are similar to shapes in the input image provided that the filter size is sufficiently small compared to the size of a typical shape.

In other words, the size of a convolution filter determines the smallest shape it can preserve. For example, 1x1 random convolutions preserve shapes at the single-pixel level and thus work as a random color mapping; large filters perturb shapes smaller than the filter size that are considered local texture of a shape at this larger scale. See Fig.~\ref{fig:randconv_example} for examples. \textit{More discussion and a formal proof are in Appendix \ref{sec:shapes} and \ref{theorem_proof}}. 

\subsection{Multi-scale Image Augmentation with a Randomized Convolution Layer}
\label{sec:randconv}

\begin{algorithm}[h]
	\small
	\caption{Learning with Data Augmentation by Random Convolutions}
	\label{Algorithm:randconv}
	\begin{algorithmic}[1]
		\State \textbf{Input}: Model $\Phi$, task loss $\mathcal{L}_{task}$, training images $\{I_i\}_{i=1}^N$ and their labels $\{y_i\}_{i=1}^N$, pool of filter sizes $\mathcal{K}=\{1,...,n\}$, fraction of original data $p$, whether to $\mathtt{mix}$ with original images, consistency loss weight $\lambda$
\Function{$\text{{\RandConv}}$}{I, $\mathcal{K}$, $\mathtt{mix}$, $p$}
		\State Sample $p_0 \sim U(0, 1)$
		\If {$p_0$ < $p$ and $\mathtt{mix}$ is False} 
		\State return $I$ \Comment{When not in $\mathtt{mix}$ mode, use the original image with probability $p$}		
		\Else 
		\State Sample scale $k$ $\sim \mathcal{K}$
		\State Sample convolution weights $\mathbf{\Theta}\in\mathbb{R}^{k\times k \times 3\times 3} \sim N(0, \frac{1}{3 k^2})$
		\State $I_{rc} = I*\mathbf{\Theta}$ \Comment{Apply convolution on $I$}
		\If{$\mathtt{mix}$ is True}
		\State Sample $\alpha \sim U(0, 1)$
		\State return $\alpha I + (1-\alpha)I_{rc}$ \Comment{Mix with original images}
		\Else
		\State return $I_{rc}$
		\EndIf
		\EndIf
		\EndFunction
		\State \textbf{Learning Objective}:
		\For {$i = 1 \to N$}
		\For {$j = 1 \to 3$} 
\State $\hat{y}_{i}^j = \Phi(\text{{\RandConv}}(I_i))$ \Comment{Predict labels for three augmented variants of the same image}
		
		\EndFor
		\State $\mathcal{L}_{cons} = \lambda\sum_{j=1}^{3}\text{KL}(\hat{y}^j_{i}|| \bar{y}_{i})$ where $\bar{y}_i = \sum_{j=1}^{3}\hat{y}_{i}^j/3$ \Comment{Consistency Loss}
		\State  $\mathcal{L} = \mathcal{L}_{task}(\hat{y}_{i}^1, y_i) + \lambda\mathcal{L}_{cons}$ \Comment{Learning with the task loss and the consistency loss}	
		\EndFor	
\end{algorithmic}
\end{algorithm}

Sec.~\ref{sec:rc_shape_preserve} discussed how outputs of randomized convolution layers approximately maintain shape information at a scale larger than their filter sizes. Here, we develop our {\RandConv} data augmentation technique using a randomized convolution layer with $C_{out}=C_{in}$ to generate shape-consistent images with randomized texture (see Alg.~\ref{Algorithm:randconv}). {Our goal is not to use {\RandConv} to parameterize or represent texture as in previous filter-bank based texture models~\citep{heeger1995pyramid, portilla2000parametric}. Instead, we only use the three-channel outputs of {\RandConv} as new images with the same shape and different “style” (loosely referred to as "texture"). We also note that, a convolution layer is different from a convolution operation in image filtering. Standard image filtering applies the same 2D filter on three color channels separately. In contrast, our convolution layer applies three different \emph{3D} filters and each takes all color channels as input and generates one channel of the output.}
Our proposed {\RandConv} variants are as follows:

\textbf{{$\text{RC}_{\text{img}}$}: Augmenting Images with Random Texture} A simple approach is to use the randomized convolution layer outputs, $I*\mathbf{\Theta}$, as new images; where $\mathbf{\Theta}$ are the randomly sampled weights and $I$ is a training image. If the original training data is in the domain $D^0$, a sampled weight $\mathbf{\Theta}_k$ generates images with consistent global shape but random texture forming the random domain $D^k$. Thus, by random weight sampling, we obtain an infinite number of random domains $D^1,D^1,\dots, D^\infty$. Input image intensities are assumed to be a standard normal distribution $N(0, 1)$ (which is often true in practice thanks to data whitening). As the outputs of {\RandConv} should follow the same distribution, we sample the convolution weights from $N(0, \sigma^2)$ where $\sigma =1/\sqrt{C_{in}\times h \times w}$, which is commonly applied for network initialization~\citep{he2015delving}. We include the original images for training at a ratio $p$ as a hyperparameter.

\textbf{{$\text{RC}_{\text{mix}}$}: Mixing Variant}
As shown in Fig.~\ref{fig:randconv_example}, outputs from $\text{RC}_{\text{img}}$ can vary significantly from the appearance of the original images. Although generalizing to domains with significantly different local texture distributions is useful, we may not want to sacrifice much performance on domains similar to the training domain. Inspired by the AugMix ~\citep{hendrycks2020augmix} strategy, we propose to blend the original image with the outputs of the {\RandConv} layer via linear convex combinations $\alpha I + (1-\alpha)(I*\mathbf{\Theta})$, where $\alpha$ is the mixing weight uniformly sampled from $[0,1]$.In $\text{RC}_{\text{mix}}$, the {\RandConv} outputs provide shape-consistent perturbations of the original images. Varying $\alpha$, we continuously interpolate between the training domain and the randomly sampled domains of \texttt{$\text{RC}_{\text{img}}$}.

\textbf{Multi-scale Texture Corruption} 
As discussed in Sec.~\ref{sec:rc_shape_preserve},, image shape information at a scale smaller than a filter's size will be corrupted by {\RandConv}. Therefore, we can use filters of varying sizes to preserve shapes at various scales.
We choose to uniformly randomly sample a filter size $k$ from a pool $\mathcal{K}={1,3,...n}$ before sampling convolution weights $\mathbf{\Theta}\in\mathbb{R}^{k\times k \times C_{in}\times C_{out}}$ from a Gaussian distribution $N(0, \frac{1}{k^2C_{in}})$. Fig.~\ref{fig:randconv_example} shows examples of multi-scale {\RandConv} outputs.

\textbf{Consistency Regularization} To learn representations invariant to texture changes, we use a loss encouraging consistent network predictions for the same {\RandConv}-augmented image for different random filter samples. Approaches for transform-invariant domain randomization~\citep{yue2019domain}, data augmentation~\citep{hendrycks2020augmix}, and semi-supervised learning~\citep{berthelot2019mixmatch} use similar strategies. We use Kullback-Leibler (KL) divergence to measure consistency. However, enforcing prediction similarity of two augmented variants may be too strong. Instead, following ~\citep{hendrycks2020augmix}, we use {\RandConv} to obtain 3 augmentation samples of image $I$: $G_j = \text{{\RandConv}}^j(I)$ for $j=1,2,3$ and obtain their predictions with a model $\Phi$: $y^j = \Phi(G^j)$. We then compute the \emph{relaxed} loss as $\lambda\sum_{j=1}^{3}\text{KL}(y^j|| \bar{y})$, where $\bar{y} = \sum_{j=1}^{3}y^j/3$ is the sample average.

\section{Experiments}
Secs.~\ref{section:digits} to~\ref{section:imagenet-sketch} evaluate our methods on the following datasets: multiple digit recognition datasets, PACS, and ImageNet-sketch. Sec.~\ref{section:revisit_pacs} uses PACS to explore the out-of-domain generalization of a pretrained representation in transfer learning by checking if pretraining on ImageNet with our method improves the domain generalization performance in downstream tasks. All experiments are in the single-domain generalization setting where training and validation sets are drawn from one domain. \textit{Additional experiments with ResNet18 as the backbone are given in the Appendix}.



\subsection{Digit Recognition}
\label{section:digits}

The five digit recognition datasets (MNIST~\citep{lecun1998gradient}, MNIST-M~\citep{ganin2016domain}, SVHN~\citep{netzer2011reading}, SYNTH~\citep{ganin2014unsupervised} and USPS \citep{denker1989neural}) have been widely used for domain adaptation and generalization research~\citep{peng2019moment, Peng2019DomainAL,qiao2020learning}. Following the setups in~\citep{volpi2018generalizing} and~\citep{qiao2020learning}, we train a simple CNN with \emph{10,000} MNIST samples and evaluate the accuracy on the test sets of the other four datasets. We also test on MNIST-C~\citep{mu2019mnist}, a robustness benchmark with \emph{15 common corruptions} of MNIST and report the average accuracy over all corruptions.

\begin{figure}[b]
\begin{center}
\setlength{\tabcolsep}{0.00cm}
		\newcommand\cwidth{0.4\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{ccc}
\includegraphics[width=\cwidth]{Fig/error_bar_update/avg-RC_img_with_different_p.png} &
				\includegraphics[width=\cwidth]{Fig/error_bar_update/avg-multiscale.png} &
				\includegraphics[width=\cwidth]{Fig/error_bar_update/avg-lambda.png}\\
			\end{tabular}
		\end{adjustbox}
	\end{center}
	\vspace{-4mm}
	\caption{Average accuracy and 5-run variance of MNIST model on MNIST-M, SVHN, SYNTH and USPS. Studies for: (a) original data fraction $p$ for $\text{RC}_{\text{img}}$; (b) multiscale design (1-n refers to using scales {1,3,..,n}) for $\text{RC}_{\text{img},p=0.5}$ (orange) and $\text{RC}_{\text{mix}}$ (blue); (c) consistency loss weight $\lambda$ for  $\text{RC}_{\text{img}1-7,p=0.5}$ (orange) and $\text{RC}_{\text{mix}1-7}$ (blue).}  
\label{fig:digit_ablation}
\end{figure}

\textbf{Selecting Hyperparameters and Ablation Study.} Fig. \ref{fig:digit_ablation}(a) shows the effect of the hyperparameter $p$ on $\text{RC}_{\text{img}}$ with filter size 1. We see that  adding only $10\%$ {\RandConv} data ($p=0.9$) immediately improves the average performance (DG-Avg) on MNIST-M, SVHN, SYNTH and USPS performance from 53.53 to 69.19, outperforming all other approaches (see Tab.~\ref{table:digits_compare}) for every dataset. We choose $p=0.5$, which obtains the best DG-Avg. Fig.~\ref{fig:digit_ablation}(b) shows results for a multiscale ablation study. Increasing the pool of filter sizes up to $7$ improves DG-Avg performance. Therefore we use multi-scale $1\mhyphen7$ to study the consistency loss weight $\lambda$, shown in Fig. ~\ref{fig:digit_ablation}(c). Adding the consistency loss improves both {\RandConv} variants on 
DG-avg: $\text{RC}_{\text{mix}1-7}$ favors $\lambda=10$ while $\text{RC}_{\text{img}1-7,p=0.5}$ performs similarly for $\lambda=5$ and $\lambda=10$. We choose $\lambda=10$ for all subsequent experiments.

\textbf{Results.} Tab.~\ref{table:digits_compare} compares the performance of $\text{RC}_{\text{img}1-7,p=0.5,\lambda=10}$ and $\text{RC}_{\text{mix}1-7,\lambda=10}$ with other state-of-the-art approaches. We show results of the adversarial training based methods GUD~\citep{volpi2018generalizing}, M-ADA~\citep{qiao2020learning}, and PAR~\citep{wang2019learning}. The baseline model is trained only on the standard classification loss. 
To show {\RandConv} is more than a trivial color/contrast adjustment method, we also compare to ColorJitter\footnote{See PyTorch documentation for implementation details; all parameters are set to 0.5.} data augmentation (which randomly changes image brightness, contrast, and saturation) and GreyScale (where images are transformed to grey-scale for training and testing). {We also tested data augmentation with a fixed Laplacian of Gaussian filter (Band-Pass) of size=3 and $\sigma=1$ and the data augmentation pipeline (Multi-Aug) that was used in a recently proposed large scale study on domain generalization algorithms and datasets \citep{gulrajani2020search}. } {\RandConv} and its mixing variant outperforms the best competing method (M-ADA) by 17\% on DG-Avg and achieves the best 91.62\% accuracy on MNIST-C. While the difference between the two variants of {\RandConv} is marginal, $\text{RC}_{\text{mix}1-7,\lambda=10}$ performs better on both DG-Avg and MNIST-C. {When combined with Multi-Aug, {\RandConv} achieves improved performance except on MNIST-C.}
Fig~\ref{fig:feat_tsne} shows t-SNE image feature plots for unseen domains generated by the baseline approach and $\text{RC}_{\text{mix}1-7,\lambda=10}$. The {\RandConv} embeddings suggest better generalization to unseen domains.

\begin{table}[th]
\small
	\setlength{\tabcolsep}{4pt}
	\caption{Average accuracy and 5-run standard deviation (in parenthesis) of MNIST10K model on MNIST-M, SVHN, SYNTH, USPS and their average (DG-avg); and average accuracy of 15 types of corruptions in MNIST-C. Both {\RandConv} variants significantly outperform all other methods.}
	\label{table:digits_compare}
	\centering
	\begin{tabular}{l|l|lllll|l}
		\toprule
		& MNIST   & MNIST-M    & SVHN        & USPS        & SYNTH       & DG-Avg         & MNIST-C     \\
		\midrule
		Baseline    & 98.40\tiny(0.84) & 58.87\tiny(3.73) & 33.41\tiny(5.28) & 79.27\tiny(2.70) & 42.43\tiny(5.46) & 53.50\tiny(4.23) & 88.20\tiny(2.10) \\
		
		GreyScale   & 98.82\tiny(0.02) & 58.41\tiny(0.99)          & 36.06\tiny(1.48)          & 80.45\tiny(1.00)          & 45.00\tiny(0.80)          & 54.98\tiny(0.86)          & 89.15\tiny(0.44)          \\
		
		ColorJitter   & 98.72\tiny(0.05) & 62.72\tiny(0.66)          & 39.61\tiny(0.88)          & 79.18\tiny(0.60)          & 46.40\tiny(0.34)          & 56.98\tiny(0.39)          & 89.48\tiny(0.18)          \\
		
		{BandPass} & 98.65\tiny(0.11) &70.22\tiny(2.73) &	48.34\tiny(2.56)	& 78.60\tiny(0.82) &	57.17\tiny(2.01) &	63.58\tiny(1.89)	& 87.89\tiny(0.68) \\
		
		{MultiAug} & 98.80\tiny(0.05)	&62.32\tiny(0.66)	& 39.07\tiny(0.68)	&79.31\tiny(1.02)	 &46.48\tiny(0.80)&	56.79\tiny(0.34)	& 89.54\tiny(0.11) \\
		
		PAR (our imp)       & 98.79\tiny(0.05) & 61.16\tiny(0.21) & 36.08\tiny(1.27) & 79.95\tiny(1.18) & 45.48\tiny(0.35) & 55.67\tiny(0.33) & 89.34\tiny(0.45) \\

		
		GUD         & -           & 60.41       & 35.51       & 77.26       & 45.32       & 54.62       & -           \\
		M-ADA       & -           & 67.94       & 42.55       & 78.53       & 48.95       & 59.49       & -           \\
		\midrule

$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5, $\lambda$=5    & 98.86\tiny(0.05) & 87.67\tiny(0.37)          & 54.95\tiny(1.90)          & 82.08\tiny(1.46)          & {63.37\tiny(1.58)}          & 72.02\tiny(1.15)          & 90.94\tiny(0.51)\\
		$\text{RC}_{\text{mix}1\mhyphen7,\lambda=10}$ & 98.85\tiny(0.04) & 87.76\tiny(0.83) & {57.52\tiny(2.09)} & {83.36\tiny(0.96)} & 62.88\tiny(0.78) & {72.88\tiny(0.58)} & \textbf{91.62\tiny(0.77)} \\
		{$\text{RC}_{\text{mix}1\mhyphen7,\lambda=10}$ + \scriptsize MultiAug} & 98.82\tiny(0.06) & \textbf{87.89\tiny(0.29)} & \textbf{62.07\tiny(0.62)} & \textbf{84.39\tiny(1.02)} & \textbf{63.90\tiny(0.63)} & \textbf{74.56\tiny(0.46)} &  91.40\tiny(0.93) \\
		
		\bottomrule
	\end{tabular}
\end{table}



\begin{figure}[th]
	\begin{center}
		\small
\newcommand\cwidth{0.25\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{cccc}
MNIST-M    & SVHN        & USPS   & SYNTH \\
\includegraphics[width=\cwidth]{Fig/tSNE/mnist_m_seed5_baseline.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/svhn_seed5_baseline.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/usps_seed5_baseline.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/synth_seed5_baseline.png}
				\\
\includegraphics[width=\cwidth]{Fig/tSNE/mnist_m_seed5_proposed.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/svhn_seed5_proposed.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/usps_seed5_proposed.png}
				& \includegraphics[width=\cwidth]{Fig/tSNE/synth_seed5_proposed.png}
				\\
			\end{tabular}
		\end{adjustbox}
	\end{center}
\caption{t-SNE feature embedding visualization for digit datasets for models trained on MNIST without (top) and with our $\text{RC}_{\text{mix}{1\mhyphen7},\lambda=10}$ approach (bottom). Different colors denote different classes.}  
\label{fig:feat_tsne}
\end{figure}


\begin{table}[t]
	\small
	\caption{Mean and 5-run standard deviation (in parenthesis) results for domain generalization on PACS. Best results with our Deep-All baseline are in \textbf{bold}. The  domain name in each column represents the target domain. {Base column indicates different baselines and results under different baselines are not directly comparable. MLDG and CIDDF used domain labels for training.}}
	\label{table:PACS}
	\centering
	\begin{tabular}{c|lccccc}
		\toprule
		Base & Method & Photo & Art & Cartoon & Sketch & Average \\
		\toprule
		\multirow{10}{*}{\centering Ours}& Deep-All     & 86.77\tiny(0.42) & 60.11\tiny(1.33) & 64.12\tiny(0.32) & 55.28\tiny(4.71) & 66.57\tiny(1.36) \\
		
		& GreyScale        & 83.93\tiny(1.47) & 61.60\tiny(1.18) & 62.12\tiny(0.61) & 60.07\tiny(2.47) & 66.93\tiny(0.83) \\
		
		& ColorJitter & 84.61\tiny(0.83) & 59.01\tiny(0.24) & 61.43\tiny(0.68) & 62.44\tiny(1.68) & 66.88\tiny(0.33) \\
		
		&{BandPass} & 87.08\tiny(0.57) & 	59.46\tiny(0.27)	& \textbf{64.39\tiny(0.51)}	& 55.39\tiny(2.95)&	66.58\tiny(0.73) \\
		
		&{MultiAug} & 85.21\tiny(0.47)	& 59.51\tiny(0.38)&	62.88\tiny(1.01)&	61.67\tiny(0.76)&	67.32\tiny(0.23) \\
		
		&PAR (our imp.) & \textbf{87.21\tiny(0.42)} & 60.17\tiny(0.95)  & 63.63\tiny(0.88)  & 55.83\tiny(2.57) & 66.71\tiny(0.58)\\
		\cmidrule{2-7}
		
&$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5 & 86.50\tiny(0.72) & 61.10\tiny(0.38) &  {64.24\tiny(0.62)} & 68.50\tiny(1.83) & 70.09\tiny(0.43) \\
		
		&$\text{RC}_{\text{mix}{1\mhyphen7}}$ & 86.60\tiny(0.67) &  {61.74\tiny(0.90)} & 64.05\tiny(0.66) & 69.74\tiny(0.66) &  \textbf{70.53\tiny(0.25)} \\
		
		& {$\text{RC}_{\text{mix}{1\mhyphen7}}$ + \scriptsize MultiAug } & 86.23\tiny(0.74)&	\textbf{61.91\tiny(0.76)}&	62.69\tiny(0.76)&	67.74\tiny(1.21)&	69.64\tiny(0.49) \\
		
		&$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5, $\lambda$=10  & 81.15\tiny(0.76) & 59.56\tiny(0.79) & 62.42\tiny(0.59) & 71.74\tiny(0.43) & 68.72\tiny(0.58) \\	
		
		&$\text{RC}_{\text{mix}{1\mhyphen7}}$\tiny,$\lambda$=10 & 81.78\tiny(1.11) & 61.14\tiny(0.51) & 63.57\tiny(0.29) & \textbf{71.97\tiny(0.38)} & 69.62\tiny(0.24) \\

		
\midrule
		\multicolumn{7}{c}{{Results below are not directly comparable due to different Deep-All implementations.}} \\
		\midrule
		\multirow{3}{*}{\scriptsize	\citet{wang2019learning}} & Deep-All (our run) & 88.40 & 66.26 & 66.58 & 59.40 & 70.16 \\
		& PAR (our run) & 88.40 & 65.19 & 68.58  & 61.86 & 71.10 \\
		& PAR (reported) & 89.6 & 66.3 & 68.3 & 64.1 & 72.08 \\
\midrule
		\multirow{2}{*}{\scriptsize\citet{carlucci2019jigen}} & Deep-All & 89.98 & 66.68 & 69.41 & 60.02  & 71.52 \\
		& Jigen & 89.00 & 67.63& 71.71& 65.18 & 73.38 \\


		\midrule
		\multirow{2}{*}{\scriptsize\citet{li2018mldg}} & Deep-All &  86.67 & 64.91 & 64.28 & 53.08  & 67.24 \\
		& MLDG (\tiny use domain labels) & 88.00& 66.23 & 66.88 & 58.96  & 70.01 \\
\midrule
		\multirow{2}{*}{\scriptsize\citet{li2018ciddg}} & Deep-All& 77.98  &  57.55  & 67.04  & 58.52  & 65.27 \\
		& CIDDG (\tiny use domain labels)& 78.65 & 62.70 & 69.73 & 64.45  & 68.88 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{PACS Experiments}

\label{section:PACS}
The PACS dataset~\citep{li2018domain} considers 7-class classification on 4 domains: photo, art painting, cartoon, and sketch, with very different texture styles. Most recent domain generalization work studies the multi-source domain setting on PACS and uses domain labels of the training data. Although we follow the convention to train on 3 domains and to test on the fourth, we simply pool the data from the 3 training domains as in~\citep{wang2019learning}, without using domain labels during the training. 

\textbf{Baseline and State-of-the-Art}. Following~\citep{li2017deeper}, we use Deep-All as the baseline, which finetunes an ImageNet-pretrained AlexNet on 3 domains using only the classification loss and tests on the fourth domain. We test our {\RandConv} variants  $\text{RC}_{\text{img}{1\mhyphen7},p=0.5}$ and $\text{RC}_{\text{mix}{1\mhyphen7}}$ with and without consistency loss, and ColorJitter/GreyScale/{BandPass/MultiAug} data augmentation as in the digit datasets. We also implemented PAR~\citep{wang2019learning} using our baseline model. { $\text{RC}_{\text{mix}{1\mhyphen7}}$ combined with MultiAug is also tested.}
Further, we compare to the following state-of-the-art approaches: Jigen~\citep{carlucci2019jigen} using self-supervision, MLDG~\citep{li2018mldg} using meta-learning, 
and the conditional invariant deep domain generalization method CIDDG~\citep{li2018ciddg}. Note that {previous methods used different Deep-All baselines which make the final accuracy not directly comparable, and} MLDG and CIDDG use domain labels for training.

\textbf{Results.} Tab.~\ref{table:PACS} shows \emph{significant improvements on Sketch} for both {\RandConv} variants. Sketch is the most challenging domain with no color and much less texture compared to the other 3 domains. The success on Sketch demonstrates that our methods can guide the DNN to learn global representations focusing on shapes that are robust to texture changes. Without using the consistency loss, $\text{RC}_{\text{mix}{1\mhyphen7}}$ achieves the best overall result improving over Deep-All by $\sim$4\% {but adding MultiAug does not further improve the performance}.
Adding the consistency loss with $\lambda=10$, $\text{RC}_{\text{mix}{1\mhyphen7}}$ and $\text{RC}_{\text{img}{1\mhyphen7}, p=0.5}$ performs better on Sketch but degrades performance on the other 3 domains, so do GreyScale and ColorJitter. \textit{This observation will be discussed in Sec~\ref{section:revisit_pacs}}.



\subsection{{Generalizing an ImageNet Model to ImageNet-Sketch}}
\label{section:imagenet-sketch}

\begin{table}[htp]
	\vspace{-4mm}
	\small
\caption{Accuracy of ImageNet-trained AlexNet on ImageNet-Sketch (IN-S) data. Our methods outperform PAR by 5\% {and are on par with a Stylized-ImageNet (SIN) trained model. Note that PAR was built on top of a stronger baseline than our model, and both PAR and SIN fine-tuned the baseline model which helped the performance, while we train \texttt{RandConv} model from scratch.}}
	\label{table:imagenet}
	\centering
	\begin{tabular}{c|cc|ccc|c}
		\toprule
		      & \multirow{2}{*}{\shortstack{Baseline\\ \tiny\citep{wang2019learning}}} & \multirow{2}{*}{\shortstack{PAR\\ \tiny\citep{wang2019learning}}}   & \multirow{2}{*}{Baseline} &
\multirow{2}{*}{\shortstack{$\text{RC}_{\text{img}1\mhyphen7,}$ \\\tiny $p$=0.5,$\lambda$=10}}
		& \multirow{2}{*}{\shortstack{$\text{RC}_{\text{mix}1\mhyphen7,}$ \\\tiny $\lambda$=10 }} & 
		
		\multirow{2}{*}{{\shortstack{SIN\\\tiny\citep{geirhos2018imagenettrained}}}} \\
		\\
		\toprule
            Top1 & 12.04  & 13.06 & 10.28  &
18.09 & 16.91 & 17.62 \\
		Top5  & 25.60 & 26.27 & 21.60 
& 35.40 & 33.99 & 36.22\\ 
\bottomrule
	\end{tabular}
\end{table}


ImageNet-Sketch~\citep{wang2019learning} is an out-of-domain test set for models trained on ImageNet. We trained AlexNet from scratch with $\text{RC}_{\text{img}1\mhyphen7, p=0.5, \lambda=10}$ and $\text{RC}_{\text{mix}1\mhyphen7,\lambda=10}$. We evaluate their performance on ImageNet-Sketch. We use the AlexNet model trained without {\RandConv} as our baseline. Tab.~\ref{table:imagenet} compares \texttt{PAR} and its baseline model and {AlexNet trained with Stylized ImageNet (SIN) \citep{geirhos2018imagenettrained} on ImageNet-Sketch.}
Although \texttt{PAR} uses a stronger baseline, {\RandConv} achieves significant improvements over our baseline and outperforms \texttt{PAR} by a large margin. Our methods achieve more than a 7\% accuracy improvement over the baseline and surpass PAR by 5\%. 
 {SIN as an image stylization approach that can modify image texture in a hierarchical and realistic way. However, albeit its complexity, it still performs on par with RandConv. Note that image stylization techniques require additional data and heavy precomputation. Further, the images for the style source also need to be chosen. In contrast, RandConv is much easier to use: it can be applied to any dataset via a simple convolution layer. We also measure the shape-bias metric proposed by
\cite{geirhos2018imagenettrained} for \texttt{RandConv} trained AlexNet. $\text{RC}_{\text{img}1\mhyphen7, p=0.5, \lambda=10}$ and $\text{RC}_{\text{mix}1\mhyphen7,\lambda=10}$ improve the baseline from $25.36\%$ to $48.24\%$ and $54.85\%$ respectively.}



\subsection{Revisiting PACS with more Robust Pretrained Representations}
\label{section:revisit_pacs}
A common practice for many computer vision tasks (including the PACS benchmark) is transfer learning, i.e.\ finetuning a backbone model pretrained on ImageNet. Recently, how the accuracy on ImageNet \citep{kornblith2019better} and adversial robustness \citep{salman2020adversarially} of the pretrained model affect transfer learning has been studied in the context of domain generalization. Instead, we study how out-of-domain generalizability transfers from pretraining to downstream tasks and shed light on how to better use pretrained models.

\textbf{Impact of ImageNet Pretraining} A model trained on ImageNet may be biased towards textures~\citep{geirhos2018imagenettrained}. 
Finetuning ImageNet pretrained models on PACS may inherit this texture bias, thereby benefitting generalization on the Photo domain (which is similar to ImageNet), but hurting performance on the Sketch domain. Therefore, as shown in Sec.~\ref{section:PACS}, using {\RandConv} to correct this texture bias improves results on Sketch, but degrades them on the Photo domain. Since pretraining has such a strong impact on transfer performance to new tasks, we ask: \emph{"Can the generalizability of a pretrained model transfer to downstream tasks? I.e., does a pretrained model with better generalizability improve performance on unseen domains on new tasks?"} To answer this, we revisit the PACS tasks based on  ImageNet-pretrained weights where our two {\RandConv} variants of Sec.~\ref{section:imagenet-sketch} are used during ImageNet training. We study if this results in performance changes for the Deep-All baseline and for finetuning with {\RandConv}.


\begin{table}[t]
\small
	\caption{Generalization results on PACS with {\RandConv} and SIN pretrained AlexNet.  ImageNet column shows how the pretrained model is trained on ImageNet (baseline represents training the ImageNet model using only the classification loss); PACS column indicates the methods used for finetuning on PACS. \textbf{Best} and \underline{second best} accuracy for each target domain are highlighted in bold and underlined.}
	\label{table:transfer_PACS}
	\centering
	\begin{tabular}{c|c|ccccc}
		\toprule
		PACS                & ImageNet & Photo & Art & Cartoon & Sketch          & Avg                  \\
		\toprule
		\multirow{4}{*}{Deep-All} & 
		Baseline     & \textbf{86.77\tiny(0.42)}          & 60.11\tiny(1.33)          & 64.12\tiny(0.32)          & 55.28\tiny(4.71)          & 66.57\tiny(1.36)          \\
		
		&	$\text{RC}_{\text{img}1\mhyphen7,p=0.5,\lambda=10}$  & 84.48\tiny(0.52)          & 62.61\tiny(1.23)          & 66.13\tiny(0.80)          & 69.24\tiny(0.80)          & 70.61\tiny(0.53)          \\
		
		&	$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$    & 85.59\tiny(0.40)          & 63.30\tiny(0.99)          & 63.83\tiny(0.85)          & 68.29\tiny(1.27)          & 70.25\tiny(0.45)          \\
		& {SIN} & 85.33\tiny(0.66)	& \textbf{65.85\tiny(0.87)}&	65.39\tiny(0.62)	&65.75\tiny(0.59)&	70.58\tiny(0.21) \\
		
		\midrule
	
		\multirow{3}{*}{\shortstack{$\text{RC}_{\text{img}1\mhyphen7,}$ \\ \tiny $p$=0.5,$\lambda$=10}}   & Baseline     & 81.15\tiny(0.76)          & 59.56\tiny(0.79)          & 62.42\tiny(0.59)          & 71.74\tiny(0.43)          & 68.72\tiny(0.58)          \\

		&$\text{RC}_{\text{img}1\mhyphen7,p=0.5,\lambda=10}$   & 84.36\tiny(0.36)          & { 63.73\tiny(0.91)}      & \textbf{68.07\tiny(0.55)} & {\ul75.41\tiny(0.57)}          & {\ul72.89\tiny(0.33)}          \\
		&$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$       & 84.63\tiny(0.97)          & 63.41\tiny(1.22)          & 66.36\tiny(0.43)          & 74.59\tiny(0.84)          & 72.25\tiny(0.54)          \\
		\midrule
		\multirow{3}{*}{\shortstack{$\text{RC}_{\text{mix}1\mhyphen7}$ \\\tiny $\lambda$=10 }}      & Baseline     & 81.78\tiny(1.11)          & 61.14\tiny(0.51)          & 63.57\tiny(0.29)          & 71.97\tiny(0.38)          & 69.62\tiny(0.24)          \\
		&$\text{RC}_{\tiny\text{img}1\mhyphen7,p=0.5,\lambda=10}$  & {85.16\tiny(1.03)}    & 63.17\tiny(0.38)          & {\ul67.68\tiny(0.60)}        & \textbf{76.11\tiny(0.43)}    & \textbf{73.03\tiny(0.46)}    \\
		&$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$   & {\ul86.17\tiny(0.56)} & {\ul65.33\tiny(1.05)} & 65.52\tiny(1.13)          & 73.21\tiny(1.03)          & 72.56\tiny(0.50)         \\
		\bottomrule
	\end{tabular}
	\vspace{-5mm}
\end{table}

\textbf{Better Performance via RandConv pretrained model}  We start by testing the Deep-All baselines using the two {\RandConv}-trained ImageNet models of Sec.~\ref{section:imagenet-sketch} as initialization. Tab.~\ref{table:transfer_PACS} shows significant improvements on Sketch. Results are comparable to finetuning with {\RandConv} on a normal pretrained model. Art is also consistently improved. Performance drops slightly on Photo as expected, since we reduced the texture bias in the pretrained model, which is helpful for the Photo domain. {A similar performance improvement is observed when using the SIN-trained AlexNet as initialization.} 
Using {\RandConv} for \emph{both} ImageNet training and PACS finetuning, we achieve 76.11\% accuracy on Sketch. As far as we know, this is the best performance using an AlexNet baseline. This approach even outperforms Jigen~\citep{carlucci2019jigen} (71.35\%) with a stronger ResNet18 baseline model. Cartoon and Art are also improved. The best average domain generalization accuracy is 73.03\%, with a more than 6\% improvement over our initial Deep-All baseline. 

This experiment confirms that generalizability may transfer: removing texture bias may not only make a pretrained model more generalizable, but it may help generalization on downstream tasks. For similar target and pretraining domains like Photo and ImageNet, where learning texture bias may actually be beneficial, performance may degrade slightly.

\vspace{-1mm}
\section{Conclusion and Discussion}
Randomized convolution ({\RandConv}) is a simple but powerful data augmentation technique for randomizing local image texture. {\RandConv} helps focus visual representations on global shape information rather than local texture. We theoretically justified the approximate shape-preserving property of {\RandConv} and developed {\RandConv} techniques using multi-scale and mixing designs. We also make use of a consistency loss to encourage texture invariance. 
{\RandConv} outperforms state-of-the-art approaches on the digit recognition benchmark and on the sketch domain of PACS and on ImageNet-Sketch by a large margin. By finetuning a model pretrained with {\RandConv} on PACS, we showed that the generalizability of a pretrained model may transfer to and benefit a new downstream task. This resulted in a new state-of-art performance on PACS in the Sketch domain. 

{{\RandConv} can help computer vision tasks when a shape-biased model is helpful e.g. for object detection. {\RandConv} can also provide a shape-biased pretrained model to improve performance on downstream tasks when generalizing to unseen domains.}
However, local texture features can be useful for many computer vision tasks, especially for fixed-domain fine-grained visual recognition. In such cases, visual representations that are invariant to local texture 
may hurt in-domain performance. Therefore, important future work includes learning representations that disentangle shape and texture features and building models to use such representations in an explainable way. 

{Adversarial robustness of deep neural networks has received significant recent attention. Interestingly, \cite{zhang2019interpreting} find that adversarially-trained models are more shape biased; \cite{shi2020informative} show that their method for increasing shape bias also helps adversarial robustness, especially when combined with adversarial training. Therefore, exploring how {\RandConv} affects the adversarial robustness of models could be interesting future work. Moreover, recent biologically inspired models for improving adversarial robustness \citep{dapello2020simulating} use Gabor filters with fixed random configurations followed by a stochastic layer to add Gaussian noise to the network input, which may explain the importance of randomness in {\RandConv}. Exploring connections between {\RandConv} and biological mechanisms in the human visual system would be interesting future work. }

\textbf{Acknowledgments} We thank Zhiding Yu for discussions on initial ideas and the experimental setup. We also thank Nathan Cahill for advice on proving the properties of random convolutions.
\newpage






\bibliography{mybib}
\bibliographystyle{iclr2021_conference}


\newpage
\appendix
This supplementary material provides additional details. Specifically, in Sec.~\ref{sec:shapes} and \ref{theorem_proof}, we discuss definitions of shapes and textures in images and justify why random convolution preserves global shapes and disrupts local texture formally by proving Theorem~\ref{theorem1}. This theorem shows that random linear projections are approximately distance preserving. We also discuss our simulation-based bound based on 80\% distance rescaling on real image data. Sec.~\ref{exp_detail} provides more experimental details for the different datasets. Sec.~\ref{resnet_exp} shows experimental results with a stronger backbone architecture and on a new benchmark ImageNet-R~\citep{hendrycks2020many}.  Sec.~\ref{results} provides more detailed results regarding hyperparameter selection and ablation studies. Lastly, Sec.~\ref{examples} shows example visualizations of {\RandConv} outputs and for its mixing variant.

\section{Shapes and Texture in Images}
\label{sec:shapes}
\begin{figure}[t]
    \centering
\newcommand\cwidth{0.3\textwidth}
	\begin{tabular}{ccc}
		\includegraphics[width=\cwidth]{Fig/textures.png} & \includegraphics[width=\cwidth]{Fig/texture_kernel9_sample0.png}  & \includegraphics[width=\cwidth]{Fig/texture_kernel19_sample0.png} \\
	\end{tabular}
	\caption{\small \textbf{Left:} An image with texture and shapes at different scales; \textbf{Middle:} The output of {\RandConv} with a small filter size which largely preserves the shapes of the stones.  \textbf{Right:} The output of {\RandConv} with a large filter size distorts the shape of the stones as well.}
	\vspace{-5mm}
	\label{fig:texture_example}
\end{figure}
As discussed in the main text, we define shapes in images that are preserved by a random convolution layer as primitive shapes: spatial clusters of pixels with similar local texture. An object in a image can be a single primitive shape alone but in most cases it is the composition of multiple primitive shapes e.g. a car includes wheels, body frames, windshields. Note that the definition of texture is not necessarily opposite to shapes, since the texture of a larger shape can includes smaller shapes. For example, in Fig.\ref{fig:texture_example}, the left occluded triangle shape has texture composed by shapes of cobble stones while cobble stones have their own texture. Random convolution can preserve those large shapes that usually define the image semantics while distorting the small shapes as local texture.   

To formally define the shape-preserving property, we assume $(x_1, y_1)$, $(x_2, y_2)$ and $(x_3, y_3)$ are three locations on a image and $(x_1, y_1)$ has closer color and local texture with $(x_2, y_2)$ than $(x_3, y_3)$. For example, $(x_1, y_1)$ and $(x_2, y_2)$ are within the same shape while $(x_3, y_3)$ is located at a neighboring shape. Then we have $\|\mathbf{p}(x_1, y_1) - \mathbf{p}(x_2, y_2)\| < \|\mathbf{p}(x_1, y_1) - \mathbf{p}(x_3, y_3)\|$, where $\mathbf{p}(x_i, y_i)$ is the image patch at location $(x_i, y_i)$. A transformation $f$ is \emph{shape-preserving} if it \emph{maintains} such relative distance relations for most location triplets, i.e.
\begin{align}
\label{eq:dist_preserve}
\|f(\mathbf{p}(x_i, y_i)) - f(\mathbf{p}(x_j, y_j))\| / \|\mathbf{p}(x_i, y_i) - \mathbf{p}(x_j, y_j)\| \approx r \end{align}
for any two spatial location $(x_i, y_i)$ and $(x_j, y_j)$;  $r\geq 0$ is a constant. 

\section{Random Convolution is Shape-preserving as Random Linear Projection is Distance Preserving}
\label{theorem_proof}

We can express a convolution layer as a local linear projection:
\begin{align}
\label{eq:local_projection}
\mathbf{g}(x, y) & = \mathbf{U}\mathbf{p}(x,y)\,,
\end{align}
where $\mathbf{p}(x,y)\in\mathbb{R}^{d}$ ($d = h \times w \times C_{in}$) is the vectorized image patch centerized at location $(x,y)$, $\mathbf{g}(x,y)\in\mathbb{R}^{C_{out}}$ is the output feature at location $(x,y)$, and $\mathbf{U}\in\mathbb{R}^{C_{out}\times d }$ is the matrix expressing the convolution layer filters $\mathbf{\Theta}$. I.e., for each sliding window centered at $(x, y)$, a convolution layer applies a linear transform $f: \mathbb{R}^{d}\rightarrow\mathbb{R}^{C_{out}} $ projecting the $d$ dimensional local image patch $\mathbf{p}(x,y)$ to its $C_{out}$ dimensional feature $\mathbf{g}(x, y)$. When $\mathbf{\Theta}$ is independently randomly sampled, e.g. from a Gaussian distribution, the convolution layer preserves global shapes since that a random linear projection is \emph{approximately} distance-preserving by bounding the range of $r$ in Eq.~\ref{eq:dist_preserve} in Theorem~\ref{theorem1}.

\begin{theorem}
	\label{theorem1}
	Suppose we have N data points $\mathbf{z}_1,\cdots, \mathbf{z}_N \in \mathbb{R}^d$. Let $f(\mathbf{z}) = \mathbf{U} \mathbf{z}$ be a random linear projection $f: \mathbb{R}^{d}\rightarrow\mathbb{R}^{m} $ such that $\mathbf{U} \in \mathbb{R}^{m \times d}$ and $\mathbf{U}_{i,j} \sim N(0,\sigma^2)$. Then we have:
	\begin{equation}
	\begin{aligned}
	P\Big(\sup_{i\neq j; i,j  \in [N]} \Big\{r_{i,j} :=\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} > \delta_1 \Big) \leq \epsilon,  \\\vspace{5ex}
	P\Big(\inf_{i\neq j; i,j  \in [N]} \Big\{r_{i,j} :=\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} < \delta_2 \Big) \leq \epsilon,  
	\end{aligned}
	\end{equation}
	where $\delta_1 := \sigma\sqrt{\chi^2_{\frac{2\epsilon}{N(N-1)}}(m)}$ and $\delta_2 := \sigma\sqrt{\chi^2_{1 - \frac{2\epsilon}{N(N-1)}}(m)}$. Here, $\chi^2_{\alpha}(m)$ denotes the $\alpha$-upper quantile of the $\chi^2$ distribution with $m$ degrees of freedom.
	
\end{theorem}

Thm.~\ref{theorem1} tells us that for any data pair $(\mathbf{z}_i, \mathbf{z}_j)$ in a set of $N$ points, the distance rescaling ratio $r_{i,j}$ after a random linear projection is bounded by $\delta_1$ and $\delta_2$ with probability  $1 -\epsilon$. A Smaller $N$ and a larger output dimension $m$ give better bounds. E.g., when $m=3$, $N=1,000$, $\sigma=1$ and $\epsilon=0.1$, $\delta_1 = 5.8$ and $\delta_2 = 0.01$. Thm.~\ref{theorem1} gives a theoretical bound for \emph{all} the $N(N-1)/2$ pairs. However, in practice, preserving distances for a majority of $N(N-1)/2$ pairs is sufficient. To empirically verify this, we test
the range of central $80\%$ of $\{r_{i,j}\}$ on real image data. Using the same $(m,N,\sigma,\epsilon)$, $80\%$ of the pairs lie in 
$[0.56,2.87]$, which is significantly better than the strict bound: $[0.01,5.8]$. A proof of the theorem and simulation details are given in the following.

\begin{proof} 
	Let $\mathbf{U}_{k}$ represent to the $k$-th row of $\mathbf{U}$. It is easy to check that $\mathbf{v}_k := \iprods{\mathbf{U}_k, \mathbf{z}_i - \mathbf{z}_j}/\norms{\mathbf{z}_i - \mathbf{z}_j} \sim N(0, \sigma^2)$. Therefore,
	
	$$
	\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} = \frac{1}{\sigma^2 } \frac{(\mathbf{z}_i-\mathbf{z}_j)^{\top}\mathbf{U}^{\top}\mathbf{U}(\mathbf{z}_i-\mathbf{z}_j)}{\norms{\mathbf{z}_i - \mathbf{z}_j}^2} = \sum_{k = 1}^{ m} \frac{\mathbf{v}_k^2}{\sigma^2} \sim \chi^2(m).
	$$
	Therefore, for $0 < \epsilon < 1$, we have
	$$
	P\Big(\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big) \leq \frac{2\epsilon}{N(N-1)}.
	$$
	From the above inequality, we have
	$$
	\begin{array}{l}
	P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{ \frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\norms{\mathbf{z}_i - \mathbf{z}_j}^2}\Big\} > \sigma^2\chi^2_{\frac{2\epsilon}{N(N-1)}}(m)\Big) \vspace{1ex}\\
	= P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2}\Big\} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m)\Big) \vspace{1ex}\\
	= P\Big(\bigcup\limits_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big\}\Big) \vspace{1ex}\\
	\leq \sum\limits_{i\neq j; i,j  \in [N]} P\Big(\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}^2}{\sigma^2\norms{\mathbf{z}_i - \mathbf{z}_j}^2} > \chi^2_{\frac{2\epsilon}{N(N-1)}}(m) \Big) \vspace{1ex}\\
	\leq \epsilon, \vspace{1ex}
	\end{array}
	$$
	
	which is equivalent to
	$$
	P\Big(\sup_{i\neq j; i,j  \in [N]}\Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}}\Big\} > \sigma\sqrt{\chi^2_{\frac{2\epsilon}{N(N-1)}}(m)}\Big) \leq \epsilon.
	$$
	Similarly, we have
	$$
	P\Big(\inf_{i\neq j; i,j  \in [N]} \Big\{\frac{\norms{f(\mathbf{z}_i) - f(\mathbf{z}_j)}}{\norms{\mathbf{z}_i - \mathbf{z}_j}} \Big\} < \sigma\sqrt{\chi^2_{1 - \frac{2\epsilon}{N(N-1)}}(m)} \Big) \leq \epsilon.
	$$
	
\end{proof}

\textbf{Simulation on Real Image Data} To better understand the relative distance preservation property of random linear projections in practice, we use Algorithm~\ref{algo:simulation} to empirically obtain a bound for real image data. We choose $m=3$, $N=1,000$, $\sigma=1$ and $\epsilon=0.1$ as in computing our theoretical bounds. We use  $M=1,000$ real images from the PACS dataset for this simulation. Note that the image patch size or $d$ does not affect the bound. We use a patch size of $3\times3$ resulting in $d=27$. This simulation tell us that applying linear projections with a randomly sampled $U$ on $N$ local images patches in every image, we have a $1-\epsilon$ chance that $80\%$ of $r_{i,j}$ is in the range $[\delta_{10\%}, \delta_{90\%}]$. 		

\begin{algorithm}[h]
	\small
	\caption{Simulate the range of central 80\% of ${r_{i,j}}$ on real image data}
	\label{algo:simulation}
	\begin{algorithmic}[1]
		\State \textbf{Input}: $M$ images $\{I_i\}_{i=1}^{M}$, number of data points $N$, projection output dimension $m$, standard deviation $\sigma$ of normal distribution, confidence level $\epsilon$.


		\For {$m = 1 \to M$}
		\State Sample images patches in $I_m$ at 1,000 locations and vectorize them as $\{\mathbf{z}_l^m\}_{l=1}^{N}$
		\State Sample a projection matrix $\mathbf{U} \in \mathbb{R}^{m \times d}$ and $\mathbf{U}_{i,j} \sim N(0,\sigma^2)$
		\For {$i = 1 \to N$}
		\For {$j = i+1\to N$}
		\State Compute $r_{i,j}^m = \frac{\norms{f(\mathbf{z}_i^m) - f(\mathbf{z}_j^m)}}{\norms{\mathbf{z}_i^m - \mathbf{z}_j^m}}$, where $f(\mathbf{z}) = \mathbf{U} \mathbf{z}$
		\EndFor	
		\EndFor
		\State $q^m_{10\%}$ = $10\%$ quantile of $r_{i,j}^m$ for $I_m$ 
		\State $q^m_{90\%}$	= $90\%$ quantile of $r_{i,j}^m$ for $I_m$ \Comment{Get the central 80\% of $r_{i,j}$ in each image}
		\EndFor	
		\State $\delta_{10\%}$ = $\epsilon$ quantile of all $q^m_{10\%}$ 
		\State $\delta_{90\%}$ = $(1-\epsilon)$ quantile of all $q^m_{90\%}$
		\Comment{Get the $\epsilon$ confident bound for $q^m_{10\%}$ and $q^m_{90\%}$}
		\State \textbf{return} $\delta_{10\%}$, $\delta_{90\%}$		 
	\end{algorithmic}
\end{algorithm}


\section{Experimental Details}
\label{exp_detail}


\textbf{Digits Recognition} The network for our digits recognition experiments is composed of two \emph{Conv5$\times$5-ReLU-MaxPool2$\times$2} blocks with 64/128 output channels and three fully connected layer with 1024/1024/10 output channels. We train the network with batch size 32 for 10,000 iterations. During training, the model is validated every 250 iterations and saved with the best validation score for testing. We apply the \texttt{Adam} optimizer with an initial learning rate of 0.0001.   

\textbf{PACS} We use the official data splits for training/validation/testing; no extra data augmentation is applied. We use the official \texttt{PyTorch} implementation and the pretrained weights of AlexNet for our PACS experiments. AlextNet is finetuned for 50,000 iterations with a batch size 128. Samples are randomly selected from the training data mixed between the three domains. We use the validation data of  source domains only at every 100 iterations. We use the \texttt{SGD} optimizer for training with an initial learning rate of 0.001, Nesterov momentum, and weight decay set to 0.0005. We let the learning rate decay by a factor of 0.1 after finishing 80\% of the iterations.

\textbf{ImageNet} Following the \texttt{PyTorch} example \footnote{https://github.com/pytorch/examples/tree/master/imagenet} on training ImageNet models, we set the batch size to 256 and train AlexNet from scratch for 90 epochs. We apply the \texttt{SGD} optimizer with an initial learning rate of 0.01, momentum 0.9, and weight decay 0.0001. We reduce the learning rate via a factor of 0.1 every 30 epochs.

\section{More Experiments with ResNet-18}
\label{resnet_exp}
In this section, we demonstrate that {\RandConv} also works on other stronger backbone architectures, e.g. for a Residual Network~\citet{he2016deep}. Specifically, we run the PACS and ImageNet experiments with ResNet-18 as the baseline and {\RandConv}. As Table~\ref{table:imagenet_resnet} shows,  {\RandConv} improves the baseline using ResNet18 on ImageNet-sketch by 10.5\% accuracy. When using a {\RandConv} pretrained ResNet-18 on PACS, the performance of finetuning with DeepAll and {\RandConv} are both improved shown in Table~\ref{table:transfer_PACS_resnet}. The best average domain generalization accuracy is 84.09\%, with a more than 8\% improvement over our initial Deep-All baseline. A model pretrained with $\text{RC}_{\text{mix}{1\mhyphen7, \lambda=10}}$ generally performs better than when pretrained with $\text{RC}_{\text{img}1\mhyphen7,p=0.5,\lambda=10}$. We also provide the ResNet-18 performance of JiGen~\citep{carlucci2019jigen} on PACS as reference. Note that JiGen uses extra data augmentation and a different data split than our approach and it only improves over its own baseline by 1.5\%. In addition, we test {\RandConv} trained ResNet-18 on ImageNet-R \citep{hendrycks2020many}, a domain generalization benchmark that contains images of artistic renditions of 200 object classes from the original ImageNet dataset. As Table \ref{table:imagenetR_resnet} shows, {\RandConv} also improve the generalization performance on ImageNet-R and reduce the gap between the in-domain (ImageNet-200) and out-of-domain (ImageNet-R) performance. 

\begin{table}[htp]
	\small
\caption{Accuracy of ImageNet-trained ResNet-18 on ImageNet-Sketch data.}
	\label{table:imagenet_resnet}
	\centering
	\begin{tabular}{c|cccc}
		\toprule
		      & Baseline &
		$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5, $\lambda$=10   & $\text{RC}_{\text{mix}{1\mhyphen7}}$\tiny, $\lambda$=10   \\
		\toprule
		    Top1  & 20.23        & 
		28.79 &  30.70 \\
		 Top5 & 37.26 
		& 49.02 & 51.80 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htp]
	\small
\caption{Top 1 Accuracy of ImageNet-trained ResNet-18 on ImageNet-R data. ImageNet-200 are the original ImageNet data with the same 200 classes as ImageNet-R. }
	\label{table:imagenetR_resnet}
	\centering
	\begin{tabular}{c|cccc}
		\toprule
		      & Baseline &
		$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5, $\lambda$=10   & $\text{RC}_{\text{mix}{1\mhyphen7}}$\tiny, $\lambda$=10   \\
		\toprule
		    ImageNet-200 (\%)    &  88.15         & 
		83.72 &  72.7 \\
		 ImageNet-R (\%) &  33.06   
		& 37.38 & 35.75 \\
		Gap & 55.09 & 46.34 & 36.95\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htp]
	\small
	\vspace{-2mm}
	\caption{Generalization results on PACS with {\RandConv} pretrained model using ResNet-18. ImageNet column shows how the pretrained model is trained on ImageNet (baseline represents training using only the classification loss); PACS column indicates the methods used for finetuning on PACS.  \textbf{Best} and \underline{second best} accuracy for each target domain are highlighted in bold and underlined. The performance of JiGen~\citep{carlucci2019jigen} and its baseline using ResNet-18 is also given. }
	\label{table:transfer_PACS_resnet}
	\centering
	\begin{tabular}{c|c|ccccc}
		\toprule
		PACS                & ImageNet & Photo & Art & Cartoon & Sketch          & Avg                  \\
		\toprule
		\multirow{3}{*}{Deep-All} & Baseline     & \textbf{95.45\tiny(0.43)} &        74.96\tiny(0.99) &        71.48\tiny(1.22) &        62.09\tiny(1.12) &        76.00\tiny(0.37)          \\
		&	$\text{RC}_{\text{img}1\mhyphen7,p=0.5,\lambda=10}$  & 94.65\tiny(0.16) &        73.85\tiny(0.97) &        74.78\tiny(0.58) &        73.51\tiny(1.16) &        79.20\tiny(0.40)   \\
		&	$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$    & 94.10\tiny(0.43) &        76.72\tiny(1.43) &        73.41\tiny(1.29) &        77.60\tiny(0.55) &        80.46\tiny(0.74)         \\
		\midrule
	
		\multirow{3}{*}{\shortstack{$\text{RC}_{\text{img}1\mhyphen7,}$ \\ \tiny $p$=0.5,$\lambda$=10}}   & Baseline     & 92.37\tiny(0.54) &        76.50\tiny(0.55) &        71.33\tiny(0.29) &        79.65\tiny(1.32) &        79.96\tiny(0.53)              \\

		&$\text{RC}_{\text{img}1\mhyphen7,p=0.5,\lambda=10}$   & 94.43\tiny(0.22) &        79.80\tiny(1.03) &        73.40\tiny(0.37) &        81.51\tiny(0.85) &        82.28\tiny(0.38)         \\
		&$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$       & 94.57\tiny(0.45) &        \textbf{81.32\tiny(1.00)} &        \textbf{76.28\tiny(0.82)} &        \textbf{84.18\tiny(0.94)} &        \textbf{84.09\tiny(0.61)}          \\
		\midrule
		\multirow{3}{*}{\shortstack{$\text{RC}_{\text{mix}1\mhyphen7}$ \\\tiny $\lambda$=10 }}      & Baseline     & 93.57\tiny(0.40) &        77.73\tiny(0.91) &        71.24\tiny(0.91) &        75.53\tiny(2.17) &        79.52\tiny(0.61)          \\
		&$\text{RC}_{\tiny\text{img}1\mhyphen7,p=0.5,\lambda=10}$  & {\ul95.23\tiny(0.30)} &        80.56\tiny(0.82) &        74.18\tiny(0.53) &        80.70\tiny(1.43) &        82.67\tiny(0.46)    \\
		&$\text{RC}_{\text{mix}1\mhyphen7, \lambda=10}$   & 95.01\tiny(0.32) &        {\ul81.09\tiny(1.24)} &        {\ul76.04\tiny(0.92)} &        {\ul83.02\tiny(0.93)} &        {\ul83.79\tiny(0.60)}         \\
		\midrule
		Deep-All & \multirow{2}{*}{Baseline} & 95.73 & 77.85 & 74.86 & 67.74  & 79.05 \\
		JiGen & &  96.03 & 79.42 & 75.25 & 71.35 & 80.51 \\
		\bottomrule
	\end{tabular}
	\vspace{-2mm}
\end{table}

\section{Hyperparameter Selections and Ablation Studies on Digits Recognition Benchmarks}

We provide detailed experimental results for the digits recognition datasets.  Table~\ref{digits-p} shows results for different hyperameters $p$ for $\text{RC}_{\text{img}{1}}$. Table~\ref{digits-scale} shows results for an ablation study on the multi-scale design for $\text{RC}_{\text{mix}}$ and $\text{RC}_{\text{img}, p=0.5}$. Table~\ref{digits-consistency} shows results for studying the consistency loss weight $\lambda$ for $\text{RC}_{\text{mix}1\mhyphen7}$ and $\text{RC}_{\text{img}1\mhyphen7, p=0.5}$. Tables~\ref{digits-p},~\ref{digits-scale},~and~\ref{digits-consistency} correspond to Fig. 2 (a)(b)(c) in the main text respectively.
\label{results}
\begin{table}[htp]
	\small
	\centering
\caption{Ablation study of hyperparameter $p$ for $\text{RC}_{\text{img}{1}}$ on digits recognition benchmarks. DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results are \textbf{bold}.}
	\label{digits-p}
	\begin{tabular}{l|c|ccccc|c}
		\toprule
		& MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		Baseline    & 98.40\tiny(0.84) & 58.87\tiny(3.73) & 33.41\tiny(5.28) & 79.27\tiny(2.70) & 42.43\tiny(5.46) & 53.50\tiny(4.23) & 88.20\tiny(2.10) \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.9 & 98.68\tiny(0.06) & 83.53\tiny(0.37) & 53.67\tiny(1.54) & 80.38\tiny(1.41) & 59.19\tiny(0.85) & 69.19\tiny(0.34) & \textbf{89.79\tiny(0.44)} \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.7 & 98.64\tiny(0.07) & 84.17\tiny(0.61) & 54.50\tiny(1.55) & \textbf{80.85\tiny(0.91)} & 60.25\tiny(0.85) & 69.94\tiny(0.50) & 89.20\tiny(0.60) \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.5 & 98.72\tiny(0.08) & 85.17\tiny(1.12) & \textbf{55.97\tiny(0.54)} & 80.31\tiny(0.85) & \textbf{61.07\tiny(0.47)} & \textbf{70.63\tiny(0.42)} & 88.66\tiny(0.62) \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.3 & 98.71\tiny(0.12) & 85.45\tiny(0.87) & 54.62\tiny(1.52) & 79.78\tiny(1.40) & 60.51\tiny(0.41) & 70.09\tiny(0.60) & {89.02\tiny(0.32)} \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.1 & 98.66\tiny(0.06) & 85.57\tiny(0.79) & 54.34\tiny(1.52) & 79.21\tiny(0.44) & 60.18\tiny(0.63) & 69.83\tiny(0.38) & 88.53\tiny(0.38) \\
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0 & 98.55\tiny(0.13) & \textbf{86.27\tiny(0.42)} & 52.48\tiny(3.00) & 79.01\tiny(1.11) & 59.53\tiny(1.14) & 69.32\tiny(1.19) & 88.01\tiny(0.36) \\
		\bottomrule
	\end{tabular}
\end{table}
\begin{table}[htp]
	\small
	\centering
	\setlength{\tabcolsep}{3pt}
	\caption{Ablation study of multi-scale {\RandConv} on digits recognition benchmarks for $\text{RC}_{\text{mix}}$ and $\text{RC}_{\text{img}, p=0.5}$. Best entries for each variant are \textbf{bold}.}
	\label{digits-scale}
	\begin{tabular}{l|c|ccccc|c}
		\toprule
		& MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		$\text{RC}_{\text{mix}{1}}$       & 98.62\tiny(0.06)          & 83.98\tiny(0.98)          & 53.26\tiny(2.59)          & 80.57\tiny(1.09)          & 59.25\tiny(1.38)          & 69.26\tiny(1.35)          & 88.59\tiny(0.38)          \\
		$\text{RC}_{\text{mix}{1\mhyphen3}}$  & 98.76\tiny(0.02)          & 84.66\tiny(1.67)          & 55.89\tiny(0.83)          & 80.95\tiny(1.15)          & 60.07\tiny(1.05)          & 70.39\tiny(0.58)          & 89.80\tiny(0.94)          \\
		$\text{RC}_{\text{mix}{1\mhyphen5}}$  & 98.76\tiny(0.06)          & 84.32\tiny(0.43)          & \textbf{56.50\tiny(2.68)} & 81.85\tiny(1.05)    & 60.76\tiny(1.02)          & 70.86\tiny(0.86)          & 90.06\tiny(0.80)          \\
		$\text{RC}_{\text{mix}{1\mhyphen7}}$  & 98.82\tiny(0.06)    	& 84.91\tiny(0.68)         & 55.61\tiny(2.63)   & \textbf{82.09\tiny(1.00)} & \textbf{62.15\tiny(1.30)}   & \textbf{71.19\tiny(1.21)}        & 90.30\tiny(0.44)   \\
		$\text{RC}_{\text{mix}{1\mhyphen9}}$ &  98.81\tiny(0.12) & \textbf{85.13\tiny(0.72)}          & 54.18\tiny(3.36)          & 82.07\tiny(1.28)          & 61.85\tiny(1.41)          & 70.81\tiny(1.24)          & \textbf{90.83\tiny(0.52)}   \\
		\midrule
		$\text{RC}_{\text{img}{1}}$\tiny, $p$=0.5   & 98.66\tiny(0.05) & 85.12\tiny(0.96)          & 55.59\tiny(0.29)          & 80.65\tiny(0.71)          & 60.85\tiny(0.48)          & 70.55\tiny(0.15)          & 89.00\tiny(0.45)          \\
		$\text{RC}_{\text{img}{1\mhyphen3}}$\tiny, $p$=0.5 & 98.79\tiny(0.07) & 85.36\tiny(1.04)          & \textbf{55.60\tiny(1.09)} & 80.99\tiny(0.99)          & 61.26\tiny(0.80)          & 70.80\tiny(0.86)          & 89.84\tiny(0.70)          \\
		$\text{RC}_{\text{img}{1\mhyphen5}}$\tiny, $p$=0.5 & 98.83\tiny(0.07) & \textbf{86.33\tiny(0.47)} & 54.99\tiny(2.48)          & 80.82\tiny(1.83)          & 62.61\tiny(0.75)          & 71.19\tiny(1.25)          & 90.70\tiny(0.43)          \\
		$\text{RC}_{\text{img}{1\mhyphen7}}$\tiny, $p$=0.5 & 98.83\tiny(0.07) & 86.08\tiny(0.27)          & 54.93\tiny(1.27)          & \textbf{81.58\tiny(0.74)} & \textbf{62.78\tiny(0.86)} & \textbf{71.34\tiny(0.61)} & \textbf{91.18\tiny(0.38)} \\
		$\text{RC}_{\text{img}{1\mhyphen9}}$\tiny, $p$=0.5 & 98.80\tiny(0.12) & 85.63\tiny(0.70)          & 52.82\tiny(2.01)          & 81.48\tiny(1.22)          & 62.55\tiny(0.74)          & 70.62\tiny(0.73)          & 90.79\tiny(0.48)              \\
\bottomrule
	\end{tabular}
\end{table}
\begin{table}[htp]
	\small
	\centering
	\setlength{\tabcolsep}{3pt}
	\caption{Ablation study of consistency loss weight $\lambda$ on digits recognition benchmarks for $\text{RC}_{\text{mix}1\mhyphen7}$ and $\text{RC}_{\text{img}1\mhyphen7, p=0.5}$. DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results for each variant are \textbf{bold}.}
	\label{digits-consistency}
	\begin{tabular}{l|c|c|ccccc|c}
		\toprule
		& $\lambda$& MNIST-10k    & MNIST-M    & SVHN        & USPS        & SYNTH       & DG Avg         & MNIST-C     \\
		\midrule
		\multirow{6}{*}{$\text{RC}_{\text{mix}1\mhyphen7}$} & 20   & 98.90 \tiny(0.05) & 87.18 \tiny(0.81)          & \textbf{57.68 \tiny(1.64)} & \textbf{83.55 \tiny(0.83)} & 63.08 \tiny(0.50)          & 72.87 \tiny(0.47)          & 91.14 \tiny(0.53)          \\
		& 10   & 98.85 \tiny(0.04) & \textbf{87.76 \tiny(0.83)} & 57.52 \tiny(2.09)          & 83.36 \tiny(0.96)          & 62.88 \tiny(0.78)          & \textbf{72.88 \tiny(0.58)} & \textbf{91.62 \tiny(0.77)} \\
		& 5    & 98.94 \tiny(0.09) & 87.53 \tiny(0.51)          & 55.70 \tiny(2.22)          & 83.12 \tiny(1.08)          & 62.37 \tiny(0.98)          & 72.18 \tiny(1.04)          & 91.46 \tiny(0.50)          \\
		& 1    & 98.95 \tiny(0.05) & 86.77 \tiny(0.79)          & 56.00 \tiny(2.39)          & 83.13 \tiny(0.71)          & \textbf{63.18 \tiny(0.97)} & 72.27 \tiny(0.82)          & 91.15 \tiny(0.42)          \\
		& 0.1  & 98.84 \tiny(0.07) & 85.41 \tiny(1.02)          & 56.51 \tiny(1.58)          & 81.84 \tiny(1.14)          & 61.86 \tiny(1.44)          & 71.41 \tiny(0.98)          & 90.72 \tiny(0.60)          \\
		& 0    & 98.82 \tiny(0.06) & 84.91 \tiny(0.68)          & 55.61 \tiny(2.63)          & 82.09 \tiny(1.00)          & 62.15 \tiny(1.30)          & 71.19 \tiny(1.21)          & 90.30 \tiny(0.44)          \\
		
		\midrule
		\multirow{6}{*}{$\text{RC}_{\text{img}1\mhyphen7, p=0.5}$} & 20  & 98.79 \tiny(0.04) & 87.53 \tiny(0.79)          & 53.92 \tiny(1.59)          & 81.83 \tiny(0.70)          & 62.16 \tiny(0.37)          & 71.36 \tiny(0.49)          & \textbf{91.20 \tiny(0.53)}          \\
		& 10  & 98.86 \tiny(0.05) & 87.67 \tiny(0.37)          & 54.95 \tiny(1.90)          & 82.08 \tiny(1.46)          & 63.37 \tiny(1.58)          & 72.02 \tiny(1.15)          & 90.94 \tiny(0.51)          \\
		& 5   & 98.90 \tiny(0.04) & \textbf{87.77 \tiny(0.72)} & \textbf{55.00 \tiny(1.40)} & \textbf{82.10 \tiny(0.55)} & \textbf{63.58 \tiny(1.33)} & \textbf{72.11 \tiny(0.62)} & {90.83 \tiny(0.71)}          \\
		& 1   & 98.86 \tiny(0.04) & 86.74 \tiny(0.32)          & 53.26 \tiny(2.99)          & 81.51 \tiny(0.48)          & 62.00 \tiny(1.15)          & 70.88 \tiny(0.93)          & 91.11 \tiny(0.62)          \\
		& 0.1 & 98.85 \tiny(0.14) & 86.85 \tiny(0.31)          & 53.55 \tiny(3.63)          & 81.23 \tiny(1.02)          & 62.77 \tiny(0.80)          & 71.10 \tiny(1.31)          & 91.13 \tiny(0.69)          \\
		& 0   & 98.83 \tiny(0.07) & 86.08 \tiny(0.27)          & 54.93 \tiny(1.27)          & 81.58 \tiny(0.74)          & 62.78 \tiny(0.86)          & 71.34 \tiny(0.61)          & 91.18 \tiny(0.38)          \\
		
\bottomrule
	\end{tabular}
\end{table}
\newpage

\section{More Examples of {\RandConv} Data Augmentation}
\label{examples}
We provide additional examples of {\RandConv} outputs for different convolution filter sizes in Fig.~\ref{fig:randconv_example_more} and for its mixing variants at scale $k=7$ with different mixing coefficients in Fig.~\ref{fig:randconv_mix_example_more}. We observe that {\RandConv} with different filter sizes retains shapes at different scales. The mixing strategy can continuously interpolate between the training domain and a randomly sampled domain.  




\begin{figure}[htp]
	\begin{center}
\setlength{\tabcolsep}{0.01cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{ccccccc}
Input & $\alpha=0.9$ & $\alpha=0.7$ & $\alpha=0.5$ & $\alpha=0.3$ & $\alpha=0.1$ & $\alpha=0$ \\
				\includegraphics[width=\cwidth]{{Fig/examples/image2}.png}
				\forloop{sample_id}{1}{\value{sample_id} < 4}{ 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.9_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.7_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.5_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.3_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0.1_sample\arabic{sample_id}}.png} 
					&\includegraphics[width=\cwidth]{{Fig/examples/image2_kernel7_mix0_sample\arabic{sample_id}}.png} \\
				} 
			\end{tabular}
		\end{adjustbox}
	\end{center}
\caption{\small Examples of the {\RandConv} mixing variant $\text{RC}_{\text{mix}7}$ on images of size $224^2$ with different mixing coefficients $\alpha$. When $\alpha=1$, the output is just the original image input;when $\alpha=0$, we use the output of the random convolution layer as the augmented image.}  
\label{fig:randconv_mix_example_more}
\end{figure}

\begin{figure}[htp]
	\begin{center}
\setlength{\tabcolsep}{0.01cm}
		\newcommand\cwidth{0.14\textwidth}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{ccccccc}


				Original image & $k=1$ & $k=3$ & $k=5$ & $k=7$ & $k=11$ & $k=15$\\
				\forloop{imgnum}{1}{\value{imgnum} < 4}{
					\includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}.png} 
					\forloop{sample_id}{0}{\value{sample_id} < 3}{
& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel1_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel3_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel5_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel7_sample\arabic{sample_id}.png} 
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel11_sample\arabic{sample_id}.png}
						& \includegraphics[width=\cwidth]{Fig/examples/image\arabic{imgnum}_kernel15_sample\arabic{sample_id}.png}
\\
					}  
				}\\
				
\end{tabular}
		\end{adjustbox}
	\end{center}
\caption{\small {\RandConv} data augmentation examples on images of size $224^2$. First column is the input image; following columns are convolution results using random filters of different sizes $k$. We can see that the smaller filter sizes help maintain the finer shapes.}
	\label{fig:randconv_example_more}
\end{figure}

\end{document}
