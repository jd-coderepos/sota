\section{Experiments}
\label{section-experiments}
This section is dedicated to the evaluation of the \modelname{} (\modelacc{}) for document recognition. We evaluate the \modelacc{} on the RIMES 2009 and READ 2016 datasets. We provide a visualization of the attention process and of the predictions. We also provide an ablation study to highlight the key components that made it possible to achieve these results.

\subsection{Datasets}
We evaluated the \modelacc{} on two public handwritten document datasets: RIMES \cite{RIMES_page,RIMES} and READ 2016 \cite{READ2016}. We also evaluate the \modelacc{} on the MAURDOR dataset \cite{MAURDOR} for the text recognition task only, as detailed in the appendix.

\subsubsection{RIMES}
The RIMES dataset corresponds to French gray-scale handwritten page images at a resolution of 300 dpi. They have been produced in the context of writing mail scenarios. We used the page-level version of the dataset used during the 2009 evaluation campaign \cite{RIMES_page}, referred to as RIMES 2009. It is split into 1,050 pages for training, 100 pages for validation and 100 pages for test. We used two kinds of annotation from this page-level dataset: transcription ground truth and layout analysis annotations. Text regions are classified among 7 classes: sender (S), recipient (R), date \& location (W), subject (Y), opening (O), body (B) and PS \& attachment (P). We used these classes and associated them with the corresponding text part: we do not use any positional ground truth to train the proposed model. We corrected 3 annotations where at least one text line was missing. Sometimes, the annotators proposed two options for a given word or expression, we systematically selected the first one in every case. 

Since there is no other work evaluating their model on this dataset, we also evaluate the performance of the \modelacc{} on the RIMES 2009 dataset at paragraph-level, as well as on the RIMES 2011 dataset \cite{RIMES} at paragraph and line levels. The idea is to compare the recognition performance between the different segmentation levels and with the state-of-the-art approaches which rely on pre-segmented input at line or paragraph level. One should notice that the paragraphs from RIMES 2011 are not extracted from RIMES 2009, so we cannot directly compare the results on both datasets, but it is the nearest comparison we can make.

\subsubsection{READ 2016}
The READ 2016 dataset is a subset of the Ratsprotokolle collection (READ project). It was proposed for the ICFHR 2016 competition on handwritten text recognition and corresponds to Early Modern German handwritings. We used the page-level version of the dataset. We also generated a double-page version of this same dataset by concatenating the images of successive pages. Only few pages of the original dataset are not paired and have been removed. Based on their positions, we automatically added a class to each text region among the 5 following classes: page (P), page number (N), body (B), annotation (A) and section (S) (group of linked annotation(s) and body). For comparison purposes, we also evaluate the \modelacc{} on the READ 2016 dataset at line and paragraph levels.


For both RIMES 2009 and READ 2016 datasets at page or double-page level, the reading order is deduced automatically from the paragraph positions as follows. 
For READ 2016, the reading order is from page to page: first, the page number, then section by section. Among a section, all annotations are read before the body.
For RIMES 2009, text regions are read from top to bottom. If two text regions share the same vertical space, text regions are read from left to right.
An example of each dataset is depicted in Figure \ref{fig:dataset}. Text regions are represented by bounding boxes, colored given their class. We also represented the expected reading order as an oriented graph linking the different text regions. Both datasets have their own specificity. READ 2016 has a more regular layout compared to RIMES, but it includes hierarchical layout tokens: bodies are included in sections, themselves included in pages. In addition, it can contain multiple pages. RIMES 2009 provides more variability regarding the layout, but the layout tokens are sequential.
\begin{figure*}[h!]
\centering
    \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/read_sample.png}   
    \par\medskip
    \includegraphics[width=0.7\textwidth]{img/graph_read.png}   
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{img/rimes_sample.png}   
    \par\medskip 
    \includegraphics[width=0.9\textwidth]{img/graph_rimes.png}  
    \end{subfigure}

    \caption{Images from READ 2016 and RIMES 2009 and associated layout graph annotation.}
\label{fig:dataset}
\end{figure*}

Datasets are split into training, validation and test sets, as detailed in Table \ref{table:split}. It corresponds to the official splits, or the most used by the community. We also provide the number of characters for each dataset, as well as the number of layout tokens (two by class: begin and end). We also provide this information at line and paragraph levels for comparison purposes.

\begin{table}[h!]
    \caption{Datasets split in training, validation and test sets and associated number of tokens in their alphabet.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ c c c c c c c}
    \hline
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Level} & \multirow{2}{*}{Training} & \multirow{2}{*}{Validation} & \multirow{2}{*}{Test} & \# char  & \# layout\\ 
     & & & & & tokens & tokens\\
    \hline
    \hline     
    \multirow{2}{*}{RIMES 2011} & Line & 10,530 & 801 &  778 & 97 & \xmark\\
    & Paragraph & 1,400 & 100 & 100 & 98 & \xmark\\ 
    \hline  
    \multirow{2}{*}{RIMES 2009} & Paragraph & 5,875 & 540 &  559 & 108 & \xmark\\
    & Page & 1,050 & 100 & 100 & 108 & 14\\
    \hline  
    \multirow{4}{*}{READ 2016} & Line & 8,367 & 1,043 & 1,140 & 88 & \xmark\\
    & Paragraph & 1,602 & 182 & 199 & 89 & \xmark\\
    & Page & 350 & 50 & 50 & 89 & 10\\
    & Double page & 169 & 24 & 24 & 89 & 10\\

    \hline
    \end{tabular}
    }
    \label{table:split}
\end{table}

\subsection{Training details}
Pre-training and training are carried out with the same following configuration, no matter the dataset:
\begin{itemize}
    \item PyTorch framework with Automatic Mixed Precision.
    \item Training with a single GPU Tesla V100 (32 Gb).
    \item Adam optimizer with an initial learning rate of .
    \item We use exactly the same hyperparameters for both datasets.
    \item We do not use any external data, external language model nor lexicon constraints.
\end{itemize}

For the generation of synthetic documents, we set the maximum number of lines per page  to 30 for READ 2016 and to 40 for RIMES 2009 to match the dataset properties. Given a set of arbitrarily-chosen fonts, we only kept those for which all characters were supported, leading to 41 fonts for READ 2016 and 95 for RIMES 2009. The font lists are provided with the code for reproducibility purposes. Figure \ref{fig:syn} illustrates the curriculum process for the generation of synthetic documents for the READ 2016 dataset at double-page level. As one can note, these synthetic documents are far from being visually realistic compared to the original dataset. This does not matter, since the objective here is only to learn the reading order.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\linewidth}
    {
    \setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1pt}\fbox{\includegraphics[width=\linewidth]{img/syn_read_3.png}}}
    \caption{.}
    \end{subfigure}
    \par\medskip
    \begin{subfigure}[b]{\linewidth}
        {
    \setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1pt}\fbox{\includegraphics[width=\linewidth]{img/syn_read_15.png}}}
    \caption{.}
    \end{subfigure}
    \par\medskip
    \begin{subfigure}[b]{\linewidth}
        {
    \setlength{\fboxsep}{0pt}\setlength{\fboxrule}{1pt}\fbox{\includegraphics[width=\linewidth]{img/syn_read_30.png}}}
    \caption{ (end of curriculum stage, no crop).}
    \end{subfigure}
    
    \caption{Illustration of the curriculum learning strategy through the synthetic document image generation process for the READ 2016 dataset at double-page level. The number of lines per page  increases from 1 to 30 through the epochs. The cropping strategy is discarded when the curriculum phase ends.}
    \label{fig:syn}
\end{figure}

We also evaluate the \modelacc{} at paragraph and line levels for comparison purposes. For each training of the same dataset, at paragraph and page levels of RIMES 2009 for instance, the same pre-trained weights are used to initialize the model. Each evaluation corresponds to specific training. Evaluation at paragraph or line levels corresponds to training only on synthetic and real paragraphs or lines, respectively. 

\subsection{Evaluation}

To our knowledge, there is no work evaluating their system on the RIMES 2009 and READ 2016 datasets at page level. Comparison at paragraph and line levels is carried out with approaches under similar conditions, \textit{i.e.}, without external data nor external language model. In Table \ref{table:rimes}, we present the evaluation of the \modelacc{} on the RIMES 2009 dataset at paragraph and page levels. One can notice that we reach very satisfying results at page level for both text and layout recognition with a CER of 4.54\%, a WER of 11.85\%, a LOER of 3.82\% and a  of 93.74\%.
The closest dataset with which we can compare is the RIMES 2011 dataset at paragraph level \cite{RIMES}. The \modelacc{} achieves new state-of-the-art results at paragraph level and competitive results at line level on this dataset. One should keep in mind that, as said previously, the comparison between RIMES 2009 and RIMES 2011 at paragraph level is not fair because RIMES 2011 only contains body images whose content seems easier to recognize than that of RIMES 2009. Unique character sequences representing dates, postal codes, product and client references, or even proper nouns like names and places, are mainly in the other text regions. 
In addition, the body images from the RIMES 2011 dataset are not taken from the page images of RIMES 2009. This explains the CER difference between RIMES 2009 (5.46\%) and RIMES 2011 (1.82\%) at paragraph level. One can notice a CER improvement from line to paragraph level (for RIMES 2011) and from paragraph to page level (for RIMES 2009). This highlights the negative impact of using a prior segmentation step, which is prone to annotation variations or errors.

\begin{table*}[ht]
    \caption{Evaluation of the DAN on the test set of the RIMES datasets and comparison with the state-of-the-art approaches.}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l l c c c c c}
        \hline
        Dataset & Approach & CER  & WER  & LOER  &   & PPER \\ 
        \hline
        \hline
        \multirow{9}{*}{\shortstack{RIMES\\2011\\\cite{RIMES}}}& \textbf{Line level}\\
        & Coquenet \textit{et al.} \cite{van} (FCN) & 3.04\% & 8.32\% & \xmark & \xmark & \xmark \\
        & Puigcerver \textit{et al.} \cite{Puigcerver2017} (CNN+BLSTM\tnote{a} ) & \textbf{2.3\%} & 9.6\% & \xmark & \xmark & \xmark\\
        & Ours (DAN\tnote{c} ) & 2.63\% & \textbf{6.78\%} & \xmark & \xmark & \xmark\\
        & \textbf{Paragraph level}\\
        & Coquenet \textit{et al.} \cite{span} (FCN) & 4.17\% & 15.61\% & \xmark & \xmark & \xmark \\
        & Bluche \textit{et al.} \cite{Bluche2016} (CNN+MDLSTM\tnote{b} ) & 2.9\% & 12.6\% & \xmark & \xmark & \xmark \\
        & Coquenet \textit{et al.} \cite{van} (FCN+LSTM\tnote{b} )  & 1.91\% & 6.72\% & \xmark & \xmark & \xmark \\
        & Ours (DAN\tnote{c} ) & \textbf{1.82\%} & \textbf{5.03\%} & \xmark & \xmark & \xmark\\
        \hline
        \multirow{4}{*}{\shortstack{RIMES\\2009\\\cite{RIMES_page}}} & \textbf{Paragraph level}\\
        & Ours (DAN\tnote{c} ) & 5.46\% & 13.04\% & \xmark & \xmark & \xmark \\
        & \textbf{Page level}\\
        & Ours (DAN\tnote{c} ) & 4.54\% & 11.85\% & 3.82\% & 93.74\% & 1.45\%\\
        \hline
        \end{tabular}
        \begin{tablenotes}
            \item [a] This work uses a different split (10,203 for training, 1,130 for validation and 778 for test).
            \item [b] with line-level attention.
            \item [c] with character-level attention.
        \end{tablenotes}
    \end{threeparttable}
    }
    \label{table:rimes}
\end{table*}



Table \ref{table:read} provides an evaluation of the \modelacc{} on the READ 2016 dataset, at line, paragraph, single-page and double-page levels. As one can note, we achieve state-of-the-art results at line and paragraph levels. We also reach very interesting results whether it is at single-page or double-page level, with a CER of 3.43\% and 3.70\%, respectively. It corresponds to slightly higher CER compared to the paragraph level (3.22\%).
One can note that the LOER and the  are also satisfying, highlighting the good recognition of the layout. Moreover, these metrics are slightly better for the double-page level dataset. This could be explained by the higher necessity to understand the layout for complex samples. This is discussed in Section \ref{section:ablation}.

\begin{table*}[ht]
    \caption{Evaluation of the DAN on the test set of the READ 2016 dataset and comparison with the state-of-the-art approaches}
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c c}
        \hline
        Approach & CER  & WER  & LOER  &   & PPER \\ 
        \hline
        \hline
        \textbf{Line level}\\
        Michael \textit{et al.} \cite{Michael2019} (CNN+BLSTM\tnote{a} )  & 4.66\% & \xmark  & \xmark & \xmark & \xmark \\
        SÃ¡nchez \textit{et al.} \cite{READ2016} (CNN+RNN) & 5.1\% & 21.1\% & \xmark & \xmark & \xmark\\
         Coquenet \textit{et al.} \cite{van} (FCN+LSTM\tnote{b} )  & \textbf{4.10\%} & \textbf{16.29\%} & \xmark & \xmark & \xmark\\
        Ours (DAN\tnote{a} ) & \textbf{4.10\%} & 17.64\% & \xmark & \xmark & \xmark\\
        \textbf{Paragraph level}\\
         Coquenet \textit{et al.} \cite{span} (FCN) & 6.20\% & 25.69\% & \xmark & \xmark & \xmark\\
         Coquenet \textit{et al.} \cite{van} (FCN+LSTM\tnote{b} )  & 3.59\% & 13.94\% & \xmark & \xmark & \xmark\\
        Ours (DAN\tnote{a} ) & \textbf{3.22\%} & \textbf{13.63\%} & \xmark & \xmark & \xmark\\
        \textbf{Single-page level}\\
        Ours (DAN\tnote{a} ) & 3.43\% & 13.05\% & 5.17\% & 93.32\% & 0.14\%\\
        \textbf{Double-page level}\\
        Ours (DAN\tnote{a} ) & 3.70\% & 14.15\% & 4.98\% & 93.09\% & 0.15\%\\
        \hline
        \end{tabular}
        \begin{tablenotes}
            \item [a] with character-level attention.
            \item [b] with line-level attention.
        \end{tablenotes}
    \end{threeparttable}
    }
    \label{table:read}
\end{table*}


One should note that these CER values, for both RIMES 2009 and READ 2016 datasets, should not be compared directly to paragraph-level or line-level HTR approaches. Indeed, it is necessary to understand the impact of the reading order. CER is computed based on the edit distance between two one-dimensional sequences. This way, if the reading order is wrong, it impacts severely the CER. For instance, if the sender coordinates are read before the recipient coordinates while it is the opposite in the ground truth, the edit distance will be very important even if the text is well recognized. It means that part of this CER is due to a wrong reading order and not to wrong text recognition.
However,  would not be impacted: it is invariant to the reading order between the different text regions.

For both datasets, the PPER metric is very low. It indicates that the good results obtained for the layout recognition are mainly due to the \modelacc{} itself and not to the post-processing stage.

We now focus on the  metric with the READ 2016 dataset at double-page level. In Table \ref{tab:map_read}, we detailed this metric for each layout class and for each threshold of CER. As one can notice, pages, page numbers, sections, and bodies are very well recognized with at least 91\% for a CER as of 10\%. The \modelacc{} has more difficulty with the annotations, with an average of 80.28\%. We assume that this is due to two main points: it is the layout entity with the most variability. There can be zero, one or multiple annotations per body. In addition, they can be placed wherever along the body, from its beginning to its end. The second point is about the length of the annotations: they are very much shorter than bodies. This way, only few errors can lead to an important CER increase, leading to lower average precision.
\begin{table*}[ht]
    \caption{ detailed for each class and each CER threshold, for the READ 2016 double-page dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c c c c c c c}
    \hline
    & 
     &  &  &  &
     &  &  &
     &  &  & \\
    \hline
    \hline
    Page (P) & 96.56 & 71.88 & 93.75 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00\\
    Page number (N) & 97.50 & 95.83 & 95.83 & 95.83 & 95.83 & 95.83 & 95.83 & 100.00 & 100.00 & 100.00 & 100.00 \\
    Section (S) & 94.04 & 73.31 & 91.00 & 94.70 & 96.48 & 96.48 & 97.46 & 97.46 & 97.46 & 97.46 & 98.57 \\
    Annotation (A) & 80.28 & 37.84 & 60.07 & 75.49 & 88.24 & 88.24 & 90.20 & 90.20 & 90.20 & 90.20 & 92.16 \\
    Body (B) & 95.18 & 85.74 & 93.33 & 95.93 & 95.93 & 95.93 & 96.98 & 96.98 & 96.98 & 96.98 & 96.98\\
    \hline
    \end{tabular}
    }
    \label{tab:map_read}
\end{table*}

The average inference time per image is given in Table \ref{table:inference} for both RIMES 2009 and READ 2016 datasets. As one can note, the inference time grows linearly with the number of characters: 5.8s for an average of 588 characters for RIMES 2009, 4.3s for 468 characters for READ 2016 at single-page level and 9.7s for the double-page level. It has to be noted that the input size does not seem to have a significant impact on the inference time, which is mainly due to the recurrent, attention-based decoding process.
\begin{table}[h!]
    \caption{Prediction using a single GPU V100 (32 Gb). Times and properties are averaged for a sample of the test set.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l | c c c c }
    \hline
    Dataset & Time & Input size & \# lines & \# chars \\
    \hline
    \hline
    RIMES 2009 (page) & 5.8s &  & 18 & 588\\
    READ 2016 (single page) & 4.3s &  & 23 & 468\\
    READ 2016 (double-page) & 9.7s &  & 46 & 944\\ 
    \hline
    \end{tabular}
    }
    \label{table:inference}
\end{table}

\subsection{Visualization}
A visualization of the prediction for a test sample of the RIMES 2009 dataset is depicted in Figure \ref{fig:viz} \footnote{Full demo at  \url{https://www.youtube.com/watch?v=HrrUsQfW66E}}. On the left, attention weights of the last mutual attention layer are projected on the input image. The colors depend on the last predicted layout token. For visibility, the intensity of the colors is encoded for attention values between 0.02 and 0.25. The text prediction is added in red line by line, under the associated text line. The corresponding layout graph is depicted on the right, with each node corresponding to a text region in the input image. As one can note, even if the \modelacc{} is not trained using any segmentation label, it performs a kind of implicit segmentation in its process, which can be globally retrieved through the attention weights. 
As one can notice, the \modelacc{} performs a document recognition: it recognizes both text and layout.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{img/visual_pred.png}
    \caption{Visualization of the prediction on a RIMES 2009 test sample. The DAN predicts both text (printed in red under each text line) and layout entities (depicted as a graph on the right). Attention weights are colored given the last predicted layout token.}
    \label{fig:viz}
\end{figure*}

In addition, the \modelacc{} is able to deal with slanted lines through its character-level attention mechanism. A prediction visualization for such example is depicted in Figure \ref{fig:viz_slanted_lines}. This time, we used one color per line for visibility. As one can note, the attention mechanism correctly follows the slope of the lines, leading to no error in prediction. This is an improvement compared to approaches based on line-level attention, which cannot handle such close and slanted lines.


\subsection{Ablation study}
\label{section:ablation}

\begin{table*}[ht]
    \caption{Ablation study of the DAN on the RIMES 2009 and READ 2016 datasets. Results are given in percentages, for the test sets, and for a 2-day training.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l | c c c c | c c c c | c c c c}
    \hline
    & \multicolumn{4}{c|}{RIMES 2009 (single-page)}& \multicolumn{4}{c|}{READ 2016 (single-page)} & \multicolumn{4}{c}{READ 2016 (double-page)}\\
    & CER  & WER  & LOER  &    & CER  & WER  & LOER  &   & CER  & WER  & LOER  &  \\ 
    \hline
    \hline
    Baseline & 5.72 & 13.05 & \textbf{4.18} & \textbf{92.86} & \textbf{3.65} & 14.64 & 5.51 & 92.36 & 4.50 & 16.75 & 4.74 & \textbf{92.37}\\
    (1) No synthetic data  & 8.26 & 16.45 & 8.18 & 86.34 & 81.05 & 94.46 & 12.04 & 0.35 & 80.75 & 95.65 & 36.77 & 0.13\\
    (2) No curriculum for syn. data  & 7.59 & 16.48 & 6.63 & 88.92  & 4.28 & 15.41 & 5.62 & 91.66 & 78.89 & 92.05 & 15.42 & 0.00\\
    (3) No crop in curr. for syn. data & 5.84 & 13.73 & 4.42 & 91.94 & 100.00 & 100.00 &  100 & 0.00 & 100.00 & 100.00 &  100 & 0.00\\
    (4) No data augmentation  & 7.08 & 15.54 & 4.78 & 91.65  & 4.32 & 16.67 & 5.29 & 91.39 & 4.92 & 18.06 & 5.69 & 90.92\\
    (5) No curriculum dropout  & 5.83 & 14.41 & 4.36 & 92.09 & 3.92 & 14.85 & 5.51 & \textbf{93.13} & \textbf{4.23} & \textbf{16.12} & \textbf{3.68} & 92.26\\
    (6) No error in teacher forcing  & 8.09 & 15.12 & 5.91 & 89.24 & 7.51 & 21.87 & 4.95 & 83.51 & 85.78 & 99.51 & 42.35 & 10.73\\
    (7) No layout recognition & \textbf{5.30} & \textbf{12.46} & \xmark & \xmark  & 4.60 & 15.59 & \xmark & \xmark & 4.96 & 16.81 & \xmark & \xmark\\
    (8) No pre-training  & 71.42 & 87.48 & 18.46 & 12.72 & 4.47 & 16.32 & 4.72 & 90.52 & 5.84 & 20.47 & 5.81 & 88.24\\
    (9) No 1D positional encoding & 8.04 & 16.93 & 5.73 & 90.65 & 3.77 & \textbf{14.03} & 4.95 & 92.51 & 4.96 & 18.28 & 6.17 & 88.88\\
    (10) No 2D positional encoding & 12.43 & 20.83 & 8.42 & 89.81& 5.63 & 16.25 & \textbf{4.27} & 92.79 & 65.54 & 88.43 & 34.40 & 25.46\\
    \hline
    \end{tabular}
    }
    \label{table:ablation}
\end{table*}

We provide an extensive ablation study in Table \ref{table:ablation}. The evaluation is carried out on the test set of both RIMES 2009 and READ 2016 datasets, at single-page and double-page levels, after a 2-day training. We evaluated the proposed training approach through the independent removal of each of the training components we used. 

Experiments (1) to (3) are dedicated to the generation of synthetic documents at training time. In (1), we do not generate synthetic documents: the model is only trained on real documents. In (2), the synthetic documents do not follow a curriculum approach: the maximum number of lines per document is directly set to its upper bound (). In (3), the synthetic document images are not cropped below the lowest text line during the curriculum phase: they preserve the original image height. As one can note, the removal of either of these three aspects leads to poorer results for the RIMES 2009 dataset, for each metric. However, it prevents the model from adapting from training to evaluation for the READ 2016 dataset, except for (2) which only leads to degraded performance at single-page level. Indeed, when looking at the predictions, it turned out that the model learned the language from the training set: predictions are coherent (correct succession of words) but do not match the input document image. We assume that the complexity of the task, \textit{i.e.}, recognizing text and layout from whole documents directly, for (1) and (2), or from images of full size, for (3), combined with few training examples compared to RIMES 2009, lead to the learning of the training language as a shortcut to reduce the loss.
The removal of the data augmentation strategy during both pre-training and training, experimented in (4), leads to degraded results, for both datasets.

In (5), we do not use the curriculum dropout strategy either during pre-training or during training. This time, the results are rather mitigated, with an overall improvement at single-page level and a deterioration of the results at double-page level.
In experiment (6), we do not introduce any error in the teacher forcing strategy: we preserve the ground truth. The introduction of errors improves the results. We assume that training the model with errors helps it to deal with the errors made at prediction time. It also avoids learning the language from the training set in the case of the READ 2016 dataset at double-page level.

The layout tokens are removed from the ground truth in (7) leading to text recognition only. As one can notice, it leads to lower CER and WER for the RIMES 2009 dataset but higher ones for the READ 2016 dataset. This can be explained by the fact that the reading order is easier for RIMES 2009 than for READ 2016, especially at double-page level. We assume that recognizing the different layout entities enables to understand the spatial relationship between them and helps to learn the reading order. Annotations are always at the left of a body: after the prediction of an </annotation> token, the attention must be focused more on the right to predict a <body> token. 

In (8), the \modelacc{} is trained from scratch, without transfer learning from a prior pre-training step. Results are dramatically worse for the RIMES 2009 dataset. We assume that this is due to its irregular layout. Indeed, compared to READ 2016, RIMES 2009 shows a simpler reading order but with more variability in the layout. This way, we assume that for READ 2016, the reading order can be learned jointly with the feature extraction part, directly during the curriculum step. For RIMES 2009, the layout variability slows down the learning process of the feature extraction part, leading to slower convergence.

Finally, in (9) and (10), we evaluate the importance of the 1D and 2D positional encoding components. As one can notice, they both enable to improve the results, especially the 2D positional encoding. This is especially true for the READ 2016 dataset at double-page level. We assume this is due to the double-page nature of this dataset, which leads to a more complex layout, resulting in more important jumps from one character prediction to another. For example, from the last character of the left page to the first one of the right page.
